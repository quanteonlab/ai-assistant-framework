chapter,level,title,text,text.len,summary,summary.len,time
False,3,"filename,title,text,len","filename,title,text,len",23,"**Filename:**\n- Contains name of document or image file (e.g., ""example.txt"")\n\n**Title:**\n- Brief description of content for searchability and organization (e.g., ""Example Document"")\n\n**Text:**\n- Detailed explanation or summary of the provided information\n- Use headings and terms in bold to highlight key points\n- Break up text into bulleted notes for easy scanning (e.g., \n  **Importance of Filenames**:\n    - Contains important metadata used by operating systems\n    - Helps users quickly identify file contents\n    - Should be descriptive yet concise, with no spaces or special characters\n\n**Length:**\n- Indicates file size in bytes for binary files (e.g., ""1024 bytes"")\n- For text files, focus on character count instead of byte size (e.g., ""500 characters"")",763,19.461705684661865
False,3,"01-Half Title.pdf,01-Half Title,Living Documentation\nThis page intentionally left blank,56","01-Half Title.pdf,01-Half Title,Living Documentation\nThis page intentionally left blank,56",91,"**Essentials of Living Documentation:** Half Title\n\n**Living Documentation**: Half Title\n- This page intentionally left blank,56 (Half Title)\n- Repeat as needed.\n\n**Instructions**:\n- The provided text is a description of ""Living Documentation"" and its component, the ""Half Title"".\n- Emphasize concision by using headings and terms in bold for easier reference.\n\n**Essentials of Living Documentation:** Half Title\n\n**Description**:\n- Text describing ""Living Documentation"" and its element, ""Half Title""\n- This page left blank intentionally (56)\n\n**Components**:\n- Living Documentation\n  - Half Title\n\n**Instructions for Use**:\n- Repeat provided text as needed.",659,17.946789741516113
False,3,"02-Dedication.pdf,02-Dedication,""Living Documentation\nContinuous Knowledge Sharing by Design\nCyrille Martraire\nBoston • Columbus • New Y ork • San","02-Dedication.pdf,02-Dedication,""Living Documentation\nContinuous Knowledge Sharing by Design\nCyrille Martraire\nBoston • Columbus • New Y ork • San Francisco • Amsterdam • Cape T own\nDubai • London • Madrid • Milan • Munich • Paris • Montreal • T oronto • Delhi\nMexico City • São Paulo • Sydney • Hong Kong • Seoul • Singapore • Taipei • T okyo\nPublisher\nMark L. Taub\nDevelopment Editor\nChris Zahn\nManaging Editor\nSandra Schroeder\nSenior Project Editor \nT onya Simpson\nCopy Editor\nKitty Wilson\nIndexer \nErika Millen\nProofreader\nLinda Morris\nTechnical Reviewers\nRebecca Wirfs-Brock Woody Zuill Steve Hayes \nEditorial Assistant\nCindy Teeters\nCover Designer\nChuti Prasertsith\nCompositor \ncodeMantra\nGraphics\nY unshan XiaMany of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed with initial capital letters or in all capitals.\nThe author and publisher have taken care in the preparation of this book, but make \nno expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential damages in connection with or arising out of the use of the information or programs contained herein.\nFor information about buying this title in bulk quantities, or for special sales \nopportunities (which may include electronic versions; custom cover designs; and content particular to your business, training goals, marketing focus, or branding interests), please contact our corporate sales department at corpsales@pearsoned.com or (800) 382-3419.\nFor government sales inquiries, please contact governmentsales@pearsoned.com. For questions about sales outside the U.S., please contact intlcs@pearson.com. Visit us on the Web: informit.com/aw\nLibrary of Congress Control Number: 2019936806Copyright © 2019 Pearson Education, Inc.\nAll rights reserved. This publication is protected by copyright, and permission must \nbe obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, request forms and the appropriate contacts within the Pearson Education Global Rights & Permissions Department, please visit www .pearsoned.com/permissions/.\nISBN-13: 978-0-13-468932-6\nISBN-10: 0-13-468932-1\n1  19\nFor my wife, Yunshan, and our children, \nNorbert and Gustave.\nThis page intentionally left blank"",2634",2673,"**Living Documentation:** Continuous Knowledge Sharing by Design\n\n**Authors:** Cyrille Martraire, Mark L. Taub, Chris Zahn, Sandra Schroeder, Tonya Simpson, Kitty Wilson, Erika Millen, Linda Morris\n\n**Publisher:** N/A\n\n**Designations:**\n- Various designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.\n- Initial capital letters or all caps indicate trademarked designations that appear in the book.\n\n**Disclaimer:**\n- No express or implied warranty is given for errors or omissions.\n- No liability is assumed for incidental or consequential damages resulting from using the information or programs contained herein.\n\n**Special Sales Opportunities:**\n- Contact corporate sales department at corpsales@pearsoned.com or (800) 382-3419 for information about buying this title in bulk quantities or custom content tailored to your business, training goals, marketing focus, or branding interests.\n\n**Government Sales:**\n- Contact governmentsales@pearson.com for government sales inquiries.\n\n**Permissions Requests:**\n- Visit www.pearsoned.com/permissions/ for information about obtaining permissions to reproduce content from this book.\n\n**ISBN:**\n- 978-0-13-468932-6 (Hardcover)\n- ISBN-10: 0-13-468932-1\n\n**Dedication:**\n- To my wife, Yunshan, and our children, Norbert and Gustave.",1327,42.49533653259277
False,3,"03-Contents.pdf,03-Contents,""viiContents\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xx \","03-Contents.pdf,03-Contents,""viiContents\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xx \nAbout the Author  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxiiIntroduction  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xxiii\nChapter 1: Rethinking Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nA Tale from the Land of Living Documentation . . . . . . . . . . . . . . . . . . . .  1\nWhy This Feature?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  2\nT omorrow Y ou Won’t Need This Sketch Anymore . . . . . . . . . . . .  2Sorry , We Don’t Have Marketing Documents!  . . . . . . . . . . . . . . .  3Y ou Keep Using This Word, but This Is Not What It Means  . . . .  3Show Me the Big Picture, and Y ou’ll See What’s Wrong There . . .  4The Future of Living Documentation Is Now . . . . . . . . . . . . . . . .  5\nThe Problem with T raditional Documentation  . . . . . . . . . . . . . . . . . . . .  5\nDocumentation Is Not Cool, Usually . . . . . . . . . . . . . . . . . . . . . .  5The Flaws of Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . .  6The Agile Manifesto and Documentation  . . . . . . . . . . . . . . . . .  11It’s Time for Documentation 2.0  . . . . . . . . . . . . . . . . . . . . . . . .  11\nDocumentation Is About Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . .  12\nThe Origination of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . .  13How Does Knowledge Evolve?  . . . . . . . . . . . . . . . . . . . . . . . . . .  13Why Knowledge Is Necessary . . . . . . . . . . . . . . . . . . . . . . . . . . .  14\nDocumentation Is About T ransferring Knowledge . . . . . . . . . . . . . . . . .  17\nFocusing on What Matters  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  18\nCore Principles of Living Documentation  . . . . . . . . . . . . . . . . . . . . . . .  19\nReliable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  20Low Effort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  21Collaborative  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  21Insightful . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  22How Ants Exchange Knowledge: Stigmergy . . . . . . . . . . . . . . . .  23\nContents viii\nMost Knowledge Is Already There . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  23\nInternal Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\nInternal V ersus External Documentation  . . . . . . . . . . . . . . . . . .  26Examples of Internal and External Documentation . . . . . . . . . .  27Preferring Internal Documentation . . . . . . . . . . . . . . . . . . . . . . .  28\nIn Situ Documentation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  28Machine-Readable Documentation  . . . . . . . . . . . . . . . . . . . . . .  29\nSpecific V ersus Generic Knowledge  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  29\nLearning Generic Knowledge  . . . . . . . . . . . . . . . . . . . . . . . . . . .  29Focusing on Specific Knowledge . . . . . . . . . . . . . . . . . . . . . . . . .  30\nEnsuring Documentation Accuracy  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  30\nAccuracy Mechanism for Reliable Documentation  . . . . . . . . . .  31When Documentation Does Not Need an \nAccuracy Mechanism  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  32\nBig Questions to Challenge Y our Documentation  . . . . . . . . . . . . . . . . .  33\nQuestioning the Need for Documentation at All  . . . . . . . . . . . .  34Need for Documentation Because of Lack of T rust . . . . . . . . . .  34Just-in-Time Documentation, or a Cheap Option on \nFuture Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  35\nQuestioning the Need for T raditional Documentation . . . . . . . .  36Minimizing Extra Work Now . . . . . . . . . . . . . . . . . . . . . . . . . . .  37Minimizing Extra Work Later  . . . . . . . . . . . . . . . . . . . . . . . . . .  38\nMaking an Activity Fun  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  39Documentation Reboot  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  41\nLiving Documentation: The V ery Short V ersion . . . . . . . . . . . . .  47Approaches to Better Documentation  . . . . . . . . . . . . . . . . . . . .  47\nA Gateway to DDD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  49\nDomain-Driven Design in a Nutshell  . . . . . . . . . . . . . . . . . . . . .  49Living Documentation and Domain-Driven Design . . . . . . . . . .  50When Living Documentation Is an Application of DDD . . . . . .  51A Story of Mutual Roots Between BDD, DDD, XP , and \nLiving Documentation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  51\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  54\nChapter 2: Behavior-Driven Development as an Example of Living Specifications  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  55\nBDD Is All About Conversations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  55BDD with Automation Is All About Living Documentation  . . . . . . . . .  56\nRedundancy and Reconciliation . . . . . . . . . . . . . . . . . . . . . . . . .  56\nContents ix\nThe Anatomy of Scenarios in a File  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  58\nThe Intent of a Feature File . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  58\nFeature File Scenarios  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  58Specification Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  59Tags in Feature Files  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  60Scenarios as Interactive Living Documentation  . . . . . . . . . . . . .  61Scenarios in Boring Paper Documents . . . . . . . . . . . . . . . . . . . . .  62\nA Feature File Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  63A Canonical Case of Living Documentation in Every Aspect  . . . . . . . .  65Going Further: Getting the Best of Y our Living Documentation . . . . . .  66\nProperty-Based Testing and BDD . . . . . . . . . . . . . . . . . . . . . . . .  68\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  70\nChapter 3: Knowledge Exploitation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  71\nIdentifying Authoritative Knowledge  . . . . . . . . . . . . . . . . . . . . . . . . . . .  71Where Is the Knowledge Now? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  72Single-Source Publishing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  72\nSome Examples of Producing a Published Document  . . . . . . . .  73A Published Snapshot with a V ersion Number . . . . . . . . . . . . . .  75Remarks  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  75\nSetting Up a Reconciliation Mechanism (aka \nV erification Mechanism) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  75\nRunning Consistency Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  76Reconciliation on the Test Assumptions . . . . . . . . . . . . . . . . . . .  78Published Contracts  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  79\nConsolidating Dispersed Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  80\nHow Consolidation Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81Consolidation Implementation Considerations . . . . . . . . . . . . .  82\nReady-Made Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  83\nThe Power of a Standard V ocabulary  . . . . . . . . . . . . . . . . . . . . .  85Linking to Standard Knowledge  . . . . . . . . . . . . . . . . . . . . . . . . .  86More Than Just V ocabulary  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  87Using Ready-Made Knowledge in Conversation to \nSpeed Up Knowledge T ransfer  . . . . . . . . . . . . . . . . . . . . . . . .  88\nT ools History  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  92Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  93\nChapter 4: Knowledge Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95\nWhen Programming Languages Are Not Enough  . . . . . . . . . . . . . . . . .  95Documentation Using Annotations  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  97\nAnnotations as More Than Tags  . . . . . . . . . . . . . . . . . . . . . . . .  99\nContents x\nDescribing the Rationale Behind Decisions  . . . . . . . . . . . . . . .  100\nEmbedded Learning  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  100\nDocumentation by Convention  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  104\nLiving Documentation in Legacy Code with Conventions . . . .  105Documenting the Conventions . . . . . . . . . . . . . . . . . . . . . . . . .  105Consistently Adhering to Conventions . . . . . . . . . . . . . . . . . . .  106The Limitations of Conventions . . . . . . . . . . . . . . . . . . . . . . . .  107\nExternal Documentation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . .  107\nSidecar Files  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  108Metadata Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  108\nDesigning Custom Annotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  109\nStereotypical Properties  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  110Stereotypes and Tactical Patterns  . . . . . . . . . . . . . . . . . . . . . . .  110Using Meaningful Annotation Package Names  . . . . . . . . . . . .  112Hijacking Standard Annotations  . . . . . . . . . . . . . . . . . . . . . . .  113Standard Annotation: @Aspect and Aspect-Oriented \nProgramming  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  114\nAnnotation by Default or Unless Necessary . . . . . . . . . . . . . . .  115\nHandling Module-Wide Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . .  115\nDealing with Many Kinds of Modules . . . . . . . . . . . . . . . . . . .  116Module-Wide Augmentation In Practice  . . . . . . . . . . . . . . . . .  117\nIntrinsic Knowledge Augmentation  . . . . . . . . . . . . . . . . . . . . . . . . . . .  117Machine-Accessible Documentation  . . . . . . . . . . . . . . . . . . . . . . . . . .  119Recording Y our Rationale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  122\nWhat’s in a Rationale?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  123Making the Rationale Explicit  . . . . . . . . . . . . . . . . . . . . . . . . .  124Beyond Documentation: Motivated Design  . . . . . . . . . . . . . . .  124Avoid Documenting Speculation  . . . . . . . . . . . . . . . . . . . . . . .  125Skills as Pre-Documented Rationales  . . . . . . . . . . . . . . . . . . . .  125Recording the Rationale as an Enabler for Change . . . . . . . . . .  126\nAcknowledging Y our Influences (aka Project Bibliography) . . . . . . . . . . 126\nDeclaring Y our Style . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  127\nCommit Messages as Comprehensive Documentation  . . . . . . . . . . . .  128\nCommit Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  129\nSummary  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  133\nChapter 5: Living Curation: Identifying Authoritative Knowledge  . . . . . .  135\nDynamic Curation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  135\nExamples of Dynamic Curation . . . . . . . . . . . . . . . . . . . . . . . .  137Editorial Curation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  137\nContents xi\nLow-Maintenance Dynamic Curation  . . . . . . . . . . . . . . . . . . .  138\nOne Corpus of Knowledge for Multiple Uses . . . . . . . . . . . . . .  139Scenario Digests  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  139\nHighlighting the Core . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  141Highlighting Inspiring Exemplars  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  142Guided T ours and Sightseeing Maps  . . . . . . . . . . . . . . . . . . . . . . . . . .  144\nCreating a Sightseeing Map  . . . . . . . . . . . . . . . . . . . . . . . . . . .  146Creating a Guided T our  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  148Creating a Living Guided T our . . . . . . . . . . . . . . . . . . . . . . . . .  150A Poor Man’s Literate Programming  . . . . . . . . . . . . . . . . . . . .  153\nSumming Up: The Curator Preparing an Art Exhibition . . . . . . . . . . .  153\nSelecting and Organizing Existing Knowledge  . . . . . . . . . . . . .  154Adding What’s Missing When Needed . . . . . . . . . . . . . . . . . . .  155Accessibility for People Who Can’t Attend and \nfor Posterity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\nSummary  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  156\nChapter 6: Automating Documentation  . . . . . . . . . . . . . . . . . . . . . . . . . .  157\nLiving Documents  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  157\nSteps in Creating a Living Document  . . . . . . . . . . . . . . . . . . . .  158Presentation Rules  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  158\nLiving Glossaries  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  158\nHow a Living Glossary Works  . . . . . . . . . . . . . . . . . . . . . . . . .  160An Example Please!  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  160Information Curation for Living Documents  . . . . . . . . . . . . . .  162Creating a Glossary Within a Bounded Context . . . . . . . . . . . .  164Case Study of a Living Glossary . . . . . . . . . . . . . . . . . . . . . . . .  165\nLiving Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  170\nDiagrams Assist in Conversations . . . . . . . . . . . . . . . . . . . . . . .  171One Diagram, One Story  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  171Living Diagrams to Keep Y ou Honest  . . . . . . . . . . . . . . . . . . .  174The Quest for the Perfect Diagram . . . . . . . . . . . . . . . . . . . . . .  174Rendering a Living Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . .  175Visualization Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  178Example: Hexagonal Architecture Living Diagram  . . . . . . . . .  179Case Study: A Business Overview as a Living Diagram  . . . . . .  184Example: A Context Diagram  . . . . . . . . . . . . . . . . . . . . . . . . .  192The Challenges with Automated Generation of Design \nDocumentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  196\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  197\nContents xii\nChapter 7: Runtime Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  199\nExample: Living Services Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . .  200\nA Matter of Augmented Code but at Runtime  . . . . . . . . . . . . .  200\nDiscovering the Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . .  201The Magic That Makes This Work . . . . . . . . . . . . . . . . . . . . . .  202Going Further . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  202Visible Workings: Working Software as Its Own \nDocumentation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  202\nVisible Tests  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  203\nDomain-Specific Notation  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  203Generating Custom Domain-Specific Diagrams to \nGet Visual Feedback  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  205\nExample: A Visible Test When Using Event Sourcing  . . . . . . . . . . . . .  207\nA Concrete Example in Code  . . . . . . . . . . . . . . . . . . . . . . . . . .  208Living Diagrams from Event Sourcing Scenarios  . . . . . . . . . . .  209\nIntrospectable Workings: Code in Memory as a Source \nof Knowledge  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\nIntrospecting with Reflection  . . . . . . . . . . . . . . . . . . . . . . . . . .  213Introspecting Without Reflection  . . . . . . . . . . . . . . . . . . . . . . .  214\nSummary  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  215\nChapter 8: Refactorable Documentation . . . . . . . . . . . . . . . . . . . . . . . . . .  217\nCode as Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  218\nText Layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  219Coding Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  221\nNaming as the Primary Documentation . . . . . . . . . . . . . . . . . . . . . . . .  223\nComposed Methods: Y ou Need to Name Them . . . . . . . . . . . .  223Idiomatic Naming Is Contextual  . . . . . . . . . . . . . . . . . . . . . . .  224Coding Against a Framework . . . . . . . . . . . . . . . . . . . . . . . . . .  224\nType-Driven Documentation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  225\nFrom Primitives to Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  226Documented Types and Integrated Documentation . . . . . . . . .  226Types and Associations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  227Types over Comments  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  228\nThe Composed Method  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  230Fluent Style . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  232\nContents xiii\nUsing an Internal DSL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  232\nImplementing a Fluent Interface . . . . . . . . . . . . . . . . . . . . . . . .  233Fluent Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  234Creating a DSTL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  234When Not to Use a Fluent Style  . . . . . . . . . . . . . . . . . . . . . . . .  235\nCase Study: An Example of Refactoring Code, Guided \nby Comments  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  236\nIntegrated Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  238\nType Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  238Code Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  238Semantics Derived from Actual Usage  . . . . . . . . . . . . . . . . . . .  239\nUsing Plain-Text Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  239\nExample: Plain-Text Diagrams . . . . . . . . . . . . . . . . . . . . . . . . .  240Diagrams as Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  243\nSummary  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  244\nChapter 9: Stable Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  245\nEvergreen Content  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  245\nRequirements Are More Stable Than Design Decisions  . . . . . .  246High-Level Goals Tend to Be Stable  . . . . . . . . . . . . . . . . . . . . .  246A Lot of Knowledge Is Less Stable Than It Looks  . . . . . . . . . .  247Case Study: A README File  . . . . . . . . . . . . . . . . . . . . . . . . . .  247\nTips for Evergreen Documentation  . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nAvoiding Mixing Strategy Documentation with the \nDocumentation of Its Implementation . . . . . . . . . . . . . . . . .  251\nEnsuring Stability  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  252Using Perennial Naming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  253Organizing Artifacts Along Stable Axes . . . . . . . . . . . . . . . . . .  254\nLinked Knowledge  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  254\nV olatile-to-Stable Dependencies . . . . . . . . . . . . . . . . . . . . . . . .  255Broken Link Checkers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  255Link Registry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  256Bookmarked Searches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  257\nCategories of Stable Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  258\nEvergreen README . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  258\nVision Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  259\nDomain Vision Statements  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  260\nContents xiv\nGoals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  260\nImpact Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  261\nInvesting in Stable Knowledge  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  262\nDomain Immersion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  263Investigation Wall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  264Domain T raining  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  264Live-My-Life Sessions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  264Shadow Users  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  265A Long-Term Investment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  265\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  265\nChapter 10: Avoiding Traditional Documentation  . . . . . . . . . . . . . . . . . .  267\nConversations About Formal Documentation  . . . . . . . . . . . . . . . . . . .  268\nWiio’s Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  270The Rule of Three Interpretations  . . . . . . . . . . . . . . . . . . . . . .  271Obstacles to Conversations  . . . . . . . . . . . . . . . . . . . . . . . . . . .  271\nWorking Collectively for Continuous Knowledge Sharing . . . . . . . . . .  272\nPair Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  273Cross Programming  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  274Mob Programming  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  274The Three Amigos (or More)  . . . . . . . . . . . . . . . . . . . . . . . . . .  275Event Storming as an Onboarding Process . . . . . . . . . . . . . . . .  275Knowledge T ransfer Sessions  . . . . . . . . . . . . . . . . . . . . . . . . . .  276Continuous Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . .  276T ruck Factor  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  277\nCoffee Machine Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  277Idea Sedimentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  280Throw-Away Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  282On-Demand Documentation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  282\nJust-in-Time Documentation  . . . . . . . . . . . . . . . . . . . . . . . . . .  283Provoking Just-in-Time Learning Early  . . . . . . . . . . . . . . . . . .  284Astonishment Report  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  285Including Some Upfront Documentation  . . . . . . . . . . . . . . . . .  285\nInteractive Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  287Declarative Automation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  289\nDeclarative Style  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  290Declarative Dependency Management . . . . . . . . . . . . . . . . . . .  291Declarative Configuration Management  . . . . . . . . . . . . . . . . .  293\nContents xv\nDeclarative Automated Deployment  . . . . . . . . . . . . . . . . . . . .  296\nMachine Documentation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  299Remarks on Automation in General . . . . . . . . . . . . . . . . . . . . .  299\nEnforced Guidelines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  300\nSome Examples of Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  301Evolving the Guidelines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  302Enforcement or Encouragement . . . . . . . . . . . . . . . . . . . . . . . .  303Declarative Guidelines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  303A Matter of T ools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  304Guidelines or Design Documentation? . . . . . . . . . . . . . . . . . . .  305Warranty Sticker V oid if Tampered With  . . . . . . . . . . . . . . . . .  306T rust-First Culture  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  307\nConstrained Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  307\nMaking It Easy to Do the Right Thing  . . . . . . . . . . . . . . . . . . .  307Making Mistakes Impossible: Error-Proof API  . . . . . . . . . . . .  308\nDesign Principles for Documentation Avoidance . . . . . . . . . . . . . . . . .  309\nReplaceability First . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  310Consistency First  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  310\nExample: The Zero Documentation Game  . . . . . . . . . . . . . . . . . . . . .  312\nContinuous T raining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  312\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  313\nChapter 11: Beyond Documentation: Living Design  . . . . . . . . . . . . . . . . .  315\nListening to the Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  315\nWhat Happened to the Language of the Domain?  . . . . . . . . . .  316Programming by Coincidence Design . . . . . . . . . . . . . . . . . . . .  317\nDeliberate Decision Making . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  319\n“Deliberate Decision” Does Not Mean “Upfront Decision” . . .  321Documentation Is a Form of Code Review . . . . . . . . . . . . . . . .  321\nShameful Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  322\nExample: Shameful Documentation . . . . . . . . . . . . . . . . . . . . .  323The T roubleshooting Guide  . . . . . . . . . . . . . . . . . . . . . . . . . . .  324Shameful Code Documentation  . . . . . . . . . . . . . . . . . . . . . . . .  325Documenting Errors or Avoiding Errors? . . . . . . . . . . . . . . . . .  326\nDocumentation-Driven Development . . . . . . . . . . . . . . . . . . . . . . . . . .  326\nDocumentation to Keep Y ou Honest  . . . . . . . . . . . . . . . . . . . .  327The Apparent Contradiction Between Documentation Driven \nand “ Avoiding Documentation”  . . . . . . . . . . . . . . . . . . . . . .  328\nContents xvi\nAbusing Living Documentation (Anti-pattern)  . . . . . . . . . . . . . . . . . .  329\nProcrastination by Living Documentation . . . . . . . . . . . . . . . . . . . . . .  330Biodegradable Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  331Hygienic T ransparency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  332\nDiagnostic T ools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  333Positive Pressure to Clean the Inside . . . . . . . . . . . . . . . . . . . . .  337\nDesign Skills Everywhere  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  338Reporter Porter Interviewing Mr. Living Doc Doc . . . . . . . . . . . . . . . .  339Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  341\nChapter 12: Living Architecture Documentation . . . . . . . . . . . . . . . . . . . .  343\nDocumenting the Problem  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  344\nAn Example of a Problem Brief  . . . . . . . . . . . . . . . . . . . . . . . .  345\nExplicit Quality Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  346\nStake-Driven Architecture Documentation  . . . . . . . . . . . . . . .  347Explicit Assumptions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  349Brevity Suggests Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  349Evolving Continuously: Change-Friendly Documentation . . . .  350\nDecision Logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  350\nAn Example of a Structured Decision Log  . . . . . . . . . . . . . . . .  351Journals or Blogs as Brain Dumps  . . . . . . . . . . . . . . . . . . . . . .  355\nFractal Architecture Documentation  . . . . . . . . . . . . . . . . . . . . . . . . . .  355The Architecture Landscape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  356\nArchitecture Diagrams and Notations  . . . . . . . . . . . . . . . . . . .  358\nAn Architecture Codex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  359T ransparent Architecture  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  362\nArchitectural Annotations  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  363Enforced Design Decisions  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  365\nArchitectural Reality Check  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  366Test-Driven Architecture  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  367\nQuality Attributes as Scenarios . . . . . . . . . . . . . . . . . . . . . . . . .  368Quality Attributes at Runtime in Production  . . . . . . . . . . . . . .  369Other Quality Attributes  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  370From Fragmented Knowledge to Usable Documentation  . . . . .  370\nSmall-Scale Simulation as Living Architecture Documentation . . . . . . . 370\nThe Desirable Properties of a Small-Scale Simulation  . . . . . . .  372Techniques to Simplify a System . . . . . . . . . . . . . . . . . . . . . . . .  373Building a Small-Scale Simulation Is Half the Fun  . . . . . . . . . .  374\nContents xvii\nSystem Metaphor  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  374\nExplaining a System by Talking About Another System . . . . . .  375\nUseful Even Without Prior Knowledge  . . . . . . . . . . . . . . . . . . .  375A Metaphor in Another Metaphor . . . . . . . . . . . . . . . . . . . . . .  376\nSummary  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376\nChapter 13: Introducing Living Documentation to a New Environment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  377\nUndercover Experiments  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  377\nOfficial Ambition  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  378\nNew Things Have to Work and Have to Be Accepted  . . . . . . . . . . . . .  378\nStarting Gently  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  379Going Big and Visible  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  380\nCase Study: A Tale of Introducing Living \nDocumentation to a Team Member . . . . . . . . . . . . . . . . . . . . . . . . .  381\nConversations First . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  381The First Debriefing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  382Time to Talk About the Code . . . . . . . . . . . . . . . . . . . . . . . . . .  382Decision Logs and Guided T ours  . . . . . . . . . . . . . . . . . . . . . . .  383\nCommon Objections to Living Documentation . . . . . . . . . . . . . . . . . .  384\nAnnotations Are Not Meant for Documentation . . . . . . . . . . .  384 “We Do It Already”  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  385\nMigrating Legacy Documentation into Living Documentation  . . . . . .  385Marginal Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  386Case Study: Introducing Living Documentation in a Batch System . . . .  386\nREADME and Ready-Made Documentation . . . . . . . . . . . . . .  387Business Behavior  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  388Visible Workings and a Single Source of T ruth . . . . . . . . . . . . .  388Integrated Documentation for Developers and a Living \nGlossary for Other Stakeholders . . . . . . . . . . . . . . . . . . . . . .  389\nA Living Diagram to Show the Design Intent . . . . . . . . . . . . . .  389Contact Information and Guided T ours . . . . . . . . . . . . . . . . . .  390Microservices Big Picture  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  390\nSelling Living Documentation to Management  . . . . . . . . . . . . . . . . . .  390\nStarting with an Actual Problem  . . . . . . . . . . . . . . . . . . . . . . .  391A Living Documentation Initiative . . . . . . . . . . . . . . . . . . . . . .  392Contrasting the Current Situation with the Promise of a \nBetter World to Match People’s Aspirations . . . . . . . . . . . . .  394\nContents xviii\nCompliance in Spirit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  395\nCase Study: Compliance with ITIL . . . . . . . . . . . . . . . . . . . . . .  397\nThe ITIL Example  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  398\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  399\nChapter 14: Documenting Legacy Applications  . . . . . . . . . . . . . . . . . . . .  401\nDocumentation Bankruptcy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  401Legacy Application as Fossilized Knowledge  . . . . . . . . . . . . . . . . . . . .  402\nArcheology  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  403\nBubble Context  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  404Superimposed Structure  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  406Highlighted Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  408External Annotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  410Biodegradable T ransformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  410\nExample: Strangler Application  . . . . . . . . . . . . . . . . . . . . . . . .  411Example: Bankruptcy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  411\nAgree on Maxims  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  412Enforced Legacy Rules  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  413Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  415\nChapter 15: Extra: Conspicuous Documentation  . . . . . . . . . . . . . . . . . . .  417\nFocusing on Differences  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  417\nHow Is Y our Lemon? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  418\nTell Only What’s Unknown  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  418\nSegmenting by Known Audience . . . . . . . . . . . . . . . . . . . . . . . .  419Flexible Content  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  419Low-Fidelity Content  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  419Visual Facilitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  419\nSearch-Friendly Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  420Concrete Examples, T ogether, Now . . . . . . . . . . . . . . . . . . . . . . . . . . .  421\nIn Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  422Fast Media and Prior Preparation . . . . . . . . . . . . . . . . . . . . . . .  423T ogether, Now . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  424\nStack Overflow Documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  424Affordable and Attractive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  425\nContents xix\nSpecs Digest  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  425\nEaster Eggs and Fun Anecdotes  . . . . . . . . . . . . . . . . . . . . . . . .  426Promoting News . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  426\nUnorthodox Media . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  426\nMaxims . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  427Posters and Domestic Ads . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  427Meme-Based Posters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  428Information Radiators  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  429Humor and Cheap Media . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  430Goodies/Swag . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  431Comics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  432Infodecks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  432Visualizations and Animations . . . . . . . . . . . . . . . . . . . . . . . . .  432LEGO Bricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  433Furniture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  4333D Printed Stuff  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  433\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  433\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  435"",39825",39861,"**Living Documentation:** An Effective Approach to Documenting Software Development\n\n**Background:**\n- Living documentation is essential for successful software development projects\n- It focuses on creating a dynamic, evolving document that reflects the current state of the project\n\n**Benefits:**\n- Clear communication between team members and stakeholders\n- Improved understanding of system design intentions\n- Enhanced knowledge transfer between teams and generations\n\n**Best Practices:**\n1. **Start with a Problem**: Identify an actual problem or goal to work towards\n2. **Collaborative Documentation**: Encourage active participation from team members\n3. **Focus on Differences**: Emphasize what sets the project apart\n4. **Flexible Content**: Use various media types (text, images, videos) for documentation\n5. **Search-Friendly Documentation**: Organize content in a logical and accessible manner\n6. **Conspicuous Documentation**: Make it visually appealing with posters, infographics, etc.\n7. **Affordable and Attractive**: Use low-cost or free tools for documentation\n8. **Humor and Cheap Media**: Keep the environment lighthearted to foster engagement\n9. **Goodies/Swag**: Provide team members with branded merchandise as motivation\n10. **Comics and Infodecks**: Use visual aids to simplify complex concepts\n11. **Visualizations and Animations**: Illustrate relationships between components or processes\n12. **LEGO Bricks, Furniture, 3D Printed Stuff**: Use physical models for better understanding\n\n**Documenting Legacy Applications:**\n- Documentation is crucial for understanding and maintaining legacy systems\n- Approach documentation as an ""archeology project"" to uncover hidden knowledge\n\n**Conspicuous Documentation:**\n1. **Focus on Differences**: Identify unique aspects of the project or system\n2. **Fast Media and Prior Preparation**: Use tools that allow for quick creation and sharing\n3. **Together, Now**: Encourage real-time collaboration between team members\n4. **Infodecks, Posters, Domestic Ads, Memes**: Utilize various media types to engage audiences\n5. **Maxims**: Establish guiding principles for the project or organization\n6. **Information Radiators**: Display critical information in a visible and accessible format\n7. **Humor and Cheap Media**: Maintain a lighthearted environment to boost morale and creativity\n8. **Goodies/Swag, Comics**: Provide team members with branded merchandise or custom illustrations\n9. **Infodecks, Visualizations, Animations**: Use visual aids to simplify complex concepts\n10. **LEGO Bricks, Furniture, 3D Printed Stuff**: Create physical models for better understanding\n\n**Documentation Bankruptcy:**\n- Outdated or inadequate documentation can hinder progress and lead to misunderstandings\n- Documentation should be updated regularly and reviewed for relevancy\n11. **Easter Eggs and Fun Anecdotes, Promoting News**: Incorporate unique stories and news into the documentation\n12. **Unorthodox Media: Meme-Based Posters, Memes, Affordable and Attractive Documentation**: Use various media types to engage audiences\n13. **Maxims, Posters, Domestic Ads, Humor**: Keep the environment lighthearted to foster engagement\n14. **Information Radiators, Cheap Media, Goodies/Swag, Comics, Visualizations**: Utilize multiple formats for documentation\n15. **LEGO Bricks, Furniture, 3D Printed Stuff**: Create physical models for better understanding.",3401,172.3213973045349
False,3,"04-Acknowledgments.pdf,04-Acknowledgments,""Acknowledgments\nFirst, I’d like to give special thanks my official reviewers, Rebecca Wirfs-Brock, Steve \","04-Acknowledgments.pdf,04-Acknowledgments,""Acknowledgments\nFirst, I’d like to give special thanks my official reviewers, Rebecca Wirfs-Brock, Steve \nHayes, and Woody Zuill, for the insightful review of the manuscript in a very short period of time, which really helped improve and organize the material better.\nMany thanks to the Pearson team, starting with Chris Zahn, the developmental \neditor I’ve been lucky to work with regularly , Mark Taub, publisher, who led the whole publishing process, Kitty Wilson for the meticulous copy editing, and T onya Simpson, it was a pleasure working with you throughout the project. I also want to thank Chris Guzikowski, executive editor, for signing the book at Pearson back in 2016.\nThe ideas in this book originate from people I respect a lot. Dan North, Chris \nMatts, and Liz Keogh derived the practice called behavioral driven development (BDD), which is one of the best examples of living documentation at work. Eric Evans, in his book Domain-Driven Design, proposed many ideas that, in turn, inspired BDD . Gojko Adzic proposed the term living documentation in his book Specification  \nby Example. In this book, I elaborate on these ideas and generalizes them to other areas of a software project. DDD has emphasized how the thinking evolves during the life of a project, and its proponents have proposed unifying the domain model and code. Similarly , this book suggests unifying project artifacts and documentation.\nThe patterns movement and its authors, starting with Ward Cunningham and \nKent Beck, have made it increasingly obvious that it is possible to do better docu-mentation by referring to patterns, those already published or presented at Pattern Languages of Programs (PLoP) conferences.\nPragmatic Programmers, Martin Fowler, Ade Oshyneye, Andreas Rüping, Simon \nBrown, and many other authors have distilled nuggets of wisdom on how to do bet-ter documentation, in a better way . Rinat Abdulin first wrote on living diagrams and, indeed, coined the term. Thanks to you all of you!\nEric Evans, thanks for all the discussions, usually not on this book, and for your \nadvice.\nI would also like to thank Brian Marick for sharing his own work on visible work-\nings with me. As encouragement matters, discussions with Vaughn V ernon and Sandro Mancuso on writing a book did help me, so thanks, guys!\nSome discussions are more important than others; especially important are those \nthat generate new ideas, lead to better understanding, or are just exciting. Thanks to George Dinwiddie, Paul Rayner, Jeremie Chassaing, Arnauld Loyer, and Romeu Moura for all the exciting discussions and for sharing your own stories and experiments.\nxx\nAcknowledgments xxi\nThrough the writing of this book, I looked for ideas and feedback as much as \nI could, particularly during open-space sessions at software development confer-\nences. Maxime Sanglan gave me the first encouraging feedback, along with Franziska Sauerwein. Thanks, Franzi and Max! I want to thank all the participants of the sessions I have run on living documentation at these conferences and uncon-ferences, such as in Agile France, Socrates Germany , Socrates France, Codefreeze Finland, and the Meetup Software Craftsmanship Paris round tables and several Jams of Code at Arolla in the evening.\nI had been giving talks at conferences for some time but always concerning prac-\ntices that are already widely accepted in our industry . With more novel content like living documentation, I also had to test acceptance from various audiences, and I thank the first conferences that took the risk of select the topic: NCrafts in Paris, Domain-Driven Design eXchange in London, Bdx.io in Bordeaux, and ITAKE Bucharest. Thanks for hosting the first versions of the talk or workshop. It was very helpful to have great feedback to inspire more effort to create the book.\nI am very lucky at Arolla to have a community of passionate colleagues; thank \nyou all for your contributions and for being my very first audience, in particular Fabien Maury , Romeu Moura, Arnauld Loyer, Y van Vu, and Somkiane V ongnoukoun. Somkiane suggested adding stories to make the text “less boring,” and it was one of the best ideas to improve the book. Thanks to the coaches of the Craftsmanship center at SGCIB for all the lunch discussions and ideas and their enthusiasm to get better at how we do software. In particular, I want to thank Gilles Philippart, who is mentioned several times in this book for his ideas, and Bruno Boucard and Thomas Pierrain.\nI must also acknowledge Clémo Charnay and Alexandre Pavillon for providing \nearly support for some of the ideas as experiments in the SGCIB commodity trading department information system and Bruno Dupuis and James Kouthon for their help making it a reality . Many of the ideas in this book have been tried in previous compa-nies I’ve worked with: the Commodity department at SGCIB, the Asset Arena teams at Sungard Asset Management, all the folks at Swapstream, our colleagues at CME, and others.\nThanks to Café Loustic and all the great baristas there. It was the perfect place to \nwork as an author, and I wrote many chapters there, usually powered by an Ethio-pian single origin coffee from Caffenation. Merci papa et maman, for encouraging our free spirit. Finally , I want to thank my wife Y unshan, who’s always been support-ive and encouraging throughout the writing of the book. V ery importantly , you also made the book a more pleasant experience, thanks to your cute pictures! Chérie, your support was key , and I want to support your own projects the same way you have this book."",5616",5665,"**Acknowledgments:**\n* **Special thanks to official reviewers**: Rebecca Wirfs-Brock, Steve Hayes, and Woody Zuill for insightful manuscript review\n* **Pearson team acknowledgements**: Chris Zahn (developmental editor), Mark Taub (publisher), Kitty Wilson (copy editing), Tanya Simpson, Chris Guzikowski (executive editor) for support throughout the publishing process\n* **Origins of ideas**: Dan North, Chris Matts, Liz Keogh (Behavioral Driven Development), Eric Evans (Domain-Driven Design), Gojko Adzic (living documentation), Ward Cunningham and Kent Beck (patterns movement)\n* **Contributions from authors and colleagues**: Martin Fowler, Ade Oshyneye, Andreas Rüping, Simon Brown, Rinat Abdulin, Eric Evans, Brian Marick, Vaughn Vernon, Sandro Mancuso, George Dinwiddie, Paul Rayner, Jeremie Chassaing, Arnauld Loyer, and Romeu Moura\n* **Feedback from conferences and events**: Maxime Sanglan, Franziska Sauerwein, Agile France, Socrates Germany, Socrates France, Codefreeze Finland, Meetup Software Craftsmanship Paris, NCrafts in Paris, Domain-Driven Design eXchange in London, Bdx.io in Bordeaux, and ITAKE Bucharest\n* **Support from colleagues and community**: Fabien Maury, Romeu Moura, Arnauld Loyer, Yvan Vu, Somkiane Vongnoukoun, Gilles Philippart, Bruno Boucard, Thomas Pierrain, Clémo Charnay, Alexandre Pavillon, Bruno Dupuis, James Kouthon, and the Craftsmanship center at SGCIB\n* **Company support**: Commodity department at SGCIB, Asset Arena teams at Sungard Asset Management, Swapstream, CME, and other companies\n* **Appreciation for partners**: Café Loustic and Ethiopian single origin coffee from Caffenation\n* **Personal acknowledgements**: Parents for encouragement, wife Yunshan's support and love.",1726,62.452940702438354
False,3,"05-Introduction.pdf,05-Introduction,""About the Author\nCyrille Martraire (@cyriux on T witter) is CTO, co-founder, and partner at Arolla \n(@ArollaFr","05-Introduction.pdf,05-Introduction,""About the Author\nCyrille Martraire (@cyriux on T witter) is CTO, co-founder, and partner at Arolla \n(@ArollaFr on T witter), the founder of the Paris Software Crafters community , and a regular speaker at international conferences. Cyrille refers to himself as a developer, since he has designed software since 1999 for startups, software vendors, and corpo-rations as an employee and as a consultant.\nHe has worked and led multiple significant projects, mostly in capital finance, \nincluding the complete rewriting of a multilateral trading facility of interest rate swaps. In most cases he has to start from large and miserable legacy systems.\nHe’s passionate about software design in every aspect: test-driven development, \nbehavior-driven development, and, in particular, domain-driven design.\nCyrille lives in Paris with his wife, Y unshan, and children, Norbert and Gustave.\nxxii\nxxiiiIntroduction\nI never planned to write a book on living documentation. I didn’t even have in mind \nthat this topic was worth a book.\nLong ago, I had a grandiose dream of creating tools that could understand the \ndesign decisions we make when coding. I spent a lot of free time over several years trying to come up with a framework for that, only to find out it’s very hard to make such a framework suitable for everyone. However, I tried the idea whenever it was helpful in the projects I was working on.\nIn 2013 I was speaking at Øredev on refactoring specifications. At the end of the \ntalk I mentioned some of the ideas I’d been trying over time, and I was surprised at the enthusiastic feedback I received about the living documentation ideas. That is when I recognized the need for better ways to do documentation. I’ve done this talk other times since then, and the feedback has continued to be about the documenta-tion thing and how to improve it, how to make it real-time and automated, without manual effort.\nThe term living documentation was introduced in the book Specification by \nExample by Gojko Adzic, as one of the many benefits of specification by example. But living documentation is a good name for an idea that is not limited to specifications.\nI had many ideas to share about living documentation. I wrote down a list of all \nthese things I had tried, as well as other stuff I had learned about the topic. More ideas came from other people—people I actually know and people I know only from T witter. As all that was growing, I decided to make it into a book. Instead of offering a framework ready for use, I believe a book will be more useful to help you create quick and custom solutions to make your own living documentation.\nWhat This Book Is About\nThe book Specification by Example introduced the idea of living documentation, where an example of behavior used for documentation is promoted into automated test. Whenever the test fails, you know the documentation is no longer in sync with the code, and you can just fix it quickly . This idea has shown that it is possible to have \nIntroduction xxiv\nuseful documentation that doesn’t suffer the fate of becoming obsolete as soon as it \nis written. But we can go much further.\nThis book expands on Gojko’s idea of living documentation, embracing a kind of \ndocumentation that evolves at the same pace as the code for many aspects of a pro-ject, from the business goals to the business domain knowledge, architecture and design, processes, and deployment.\nThis book combines some theory and practice, with illustrations and concrete \nexamples. Y ou will learn how to start investing in documentation that is always up to date and has minimal extra cost, thanks to well-crafted artifacts and a reasonable amount of automation.\nY ou will see that you don’t necessarily have to choose between working software \nand extensive documentation!\nWho This Book Is For\nThis book is primarily for software developers or anyone who is not afraid of code in a source control system. It is code-centric and meant for developers, coding archi-tects, and those in senior roles who understand code. It also addresses some needs from other stakeholders, from business analysts to managers, but through the lens of software developers who change the source code and commit files into the source control system.\nThis book is not about producing user documentation. Specific skills such as \ntechnical writing are required to do user documentation well, and that is absolutely not the topic of this book.\nHow to Read This Book\nThis book is on the topic of living documentation, and it is presented as a network of related patterns. Each pattern stands on its own and can be read independently . However, to fully understand and situate each pattern in context, it’s often desirable to also look at related patterns. On the book’s website, you can find pattern dia-grams, which illustrate some of the relationships between the patterns.\nThe content of this book is organized as a progression from the problem of man-\naging knowledge, to the inspiration from BDD, to some preliminary theory , to the different paces of knowledge changes and the corresponding documentation tech-niques. It then expands beyond that, focusing on the application for architecture and legacy systems, and on how to introduce living documentation in your environment.\nIntroduction xxv\nI suggest starting with Chapter 1 and making sure you grasp the key ideas of \nChapters 3 and 4 before taking a look at Chapters 5 through 9, which cover general \npractical techniques. Then you should take a look at the Chapter 10 for the shift of perspective. Chapters 11 through 15 cover more specific topics and provide addi-tional examples.\nSome readers have enjoyed reading this book cover to cover; however, feel free to \nskim, dig into a particular area, and read the text in any other order.\nWhat This Book Covers\nChapter 1, “Rethinking Documentation,” takes a fresh look at documentation from first principles, providing a foundation for the rest of the book.\nChapter 2, “Behavior-Driven Development as an Example of Living Specifications,” \ndescribes how BDD is a key inspiration behind living documentation, although BDD itself is not the central topic of this book.\nChapter 3, “Knowledge Exploitation,” and Chapter 4, “Knowledge Augmentation,” \nlay a foundation on which other practices build. In particular, they discuss extracting knowledge that is there and augmenting it with what’s missing.\nChapter 5, “Living Curation: Identifying Authoritative Knowledge,” shows how \nto start turning knowledge into something useful through curation, while embracing that this knowledge is continuously changing.\nChapter 6, “ Automating Documentation,” expands on turning the knowledge \ninto documentation and diagrams that live at the pace of the knowledge, mirroring each change in the knowledge.\nChapter 7, “Runtime Documentation,” is an extension of the previous chapter, \ndiscussing how to use knowledge that is accessible only at runtime. \nChapter 8, “Refactorable Documentation,” is more code-centric and focuses on \nusing development tools to help keep documentation up-to-date.\nIn contrast to the previous chapters, Chapter 9, “Stable Documentation,” explores \nthe idea that you don’t need living techniques for knowledge that doesn’t change and discusses better methods of documenting such knowledge.\nChapter 10, “ Avoiding T raditional Documentation,” takes a more rebellious per-\nspective, with a focus on alternative means of documentation.\nAfter all the chapters on how to improve documentation by design, Chapter 11, \n“Beyond Documentation: Living Design,” takes a different view: how a focus on doc-umentation could help you improve the design itself.\nChapter 12, “Living Architecture Documentation,” applies the living documenta-\ntion ideas to software architecture and discusses some specific techniques.\nIntroduction xxvi\nChapter 13, “Introducing Living Documentation to a New Environment,” offers \nguidance on how to introduce living documentation in your environment, mostly as \na social challenge.\nBecause we are surrounded by legacy systems, Chapter 14, “Documenting Legacy \nApplications,” closes the book with a set of specific patterns for dealing with these tough legacy challenges.\nChapter 15, “Extra: Conspicuous Documentation,” is a bonus chapter on practi-\ncal advice to make all your living documentation initiatives more effective by making them more noticeable.\nRegister Your Book\nRegister your copy of Living Documentation at informit.com for convenient access to downloads, updates, and corrections as they become available. T o start the regis-tration process, go to informit.com/register and log in or create an account. Enter the product ISBN 9780134689326 and click Submit. Once the process is complete, you will find any available bonus content under “Registered Products.”\nxxviiFigure Credits\nCover image: Y unshan Xia\n Globally , LOL note image: Y unshan XiaFigures 1.1 through 1.6, 1.10 and 1.11: Y unshan XiaFigure 1.9: Y unshan XiaFigure 2.1: Y unshan XiaFigure 2.4: Screenshot of Pickles © Jeffrey Cameron 2011-2012Figures 3.3, 3.5, 3.8 through 3.11: Y unshan XiaFigures 4.3 and 4.4: Y unshan XiaFigure 4.5: Screenshot of Github © 2019 GitHub, Inc.Figures 5.1 and 5.4: Y unshan XiaFigure 5.2: Screenshot of Eclipse © Eclipse Foundation, Inc.Figure 6.2: Screenshot of Java © OracleFigure 6.7: Y unshan XiaFigure 6.10: Alistair CockburnFigure 6.15: Screenshot of Java © OracleFigure 7.1: Screenshot of Zipkin © Zipkin 2019Figure 8.5: Screenshot of Eclipse © Eclipse Foundation, Inc.Figure 8.6: Cyrille MartraireFigure 10.1, 10.5, and 10.9: Y unshan XiaFigure 10.2: Alistair CockburnFigure 10.3: Cyrille MartraireFigure 10.6: Screenshot of Bug Magnet © 2015-2017 Gojko AdzicFigure 10.8: John Arundel, http://bitfieldconsulting.com/puppet-dependency-graphsFigures 11.1, 11.2, 11.4, and 11.7: Y unshan XiaFigure 11.5 and 11.6: Screenshot of Word Cloud © ZygomaticFigures 12.3, 12.4, and 12.5: Y unshan XiaFigures 15.1, 15.4, and 15.5: Cyrille MartraireFigures 15.2 and 15.3: Y unshan Xia\nThis page intentionally left blank"",10147",10191,"**Cyrille Martraire's Book on Living Documentation**\n\n**About the Author**:\n- Cyrille Martraire: CTO, co-founder, and partner at Arolla; founder of Paris Software Crafters community; regular speaker at international conferences\n- Over 20 years experience in software development for startups, software vendors, and corporations as employee and consultant\n- Passionate about software design, including test-driven development, behavior-driven development, and domain-driven design\n- Lives in Paris with wife Yunshan and children Norbert and Gustave\n\n**Introduction**:\n- Cyrille initially wanted to create tools that could understand design decisions made during coding\n- Later recognized the need for better ways of documenting software development\n- Book expands on the idea of **living documentation**, which is about keeping documentation up-to-date as code evolves\n\n**What This Book Covers**:\n- **Rethinking Documentation**: Provides a foundation for the rest of the book\n- **Behavior-Driven Development as an Example of Living Specifications**: Discusses how BDD is a key inspiration behind living documentation\n- **Knowledge Exploitation** and **Knowledge Augmentation**: Lay a foundation for other practices\n- **Living Curation: Identifying Authoritative Knowledge**: Shows how to turn knowledge into something useful\n- **Automating Documentation**: Expands on turning knowledge into documentation that lives at the pace of the code\n- **Runtime Documentation**: Discusses using knowledge accessible only at runtime\n- **Refactorable Documentation**: Focuses on using development tools to keep documentation up-to-date\n- **Stable Documentation**: Examines documenting knowledge that doesn't change\n- **Avoiding Traditional Documentation**: Takes a rebellious perspective on alternative means of documentation\n- **Beyond Documentation: Living Design**: Explores how focusing on documentation can improve design\n- **Living Architecture Documentation**: Applies the living documentation ideas to software architecture\n- **Introducing Living Documentation to a New Environment**: Offers guidance on introducing living documentation\n- **Documenting Legacy Applications**: Provides patterns for dealing with legacy challenges\n- **Conspicuous Documentation**: A bonus chapter on practical advice to make all living documentation initiatives more effective\n\n**Registering the Book**:\n- Register the book at informit.com for convenient access to downloads, updates, and corrections",2476,69.0914945602417
False,3,"06-Chapter 1 Rethinking Documentation.pdf,06-Chapter 1 Rethinking Documentation,,0","06-Chapter 1 Rethinking Documentation.pdf,06-Chapter 1 Rethinking Documentation,,0",82,"**Rethinking Documentation: Chapter 1**\n\n**Key Points**:\n- Focus on understanding the provided document's purpose\n- Identify the main topic: ""Rethinking Documentation""\n- Break down the title into components for analysis\n\n**Title Components**:\n1. **""Rethinking""**: Suggests a shift in perspective or approach\n2. **""Documentation""**: Refers to recording information for future reference\n3. **""Chapter 1""**: Indicates this is an introductory piece, possibly part of a larger work\n4. **""Rethinking Documentation""**: The focus is on examining the concept of documentation and how it's approached\n\n**Understanding the Purpose**:\n- The given text doesn't provide much context regarding the purpose\n- Assessing the title can help glean some information about the topic and tone\n\n**Tips for Expanding Bulleted Notes**:\n- Use headings and terms in bold to organize your notes\n- Add terms and concepts to your own words to expand on the meaning\n- Provide examples or illustrations to reinforce key ideas.",993,25.28089427947998
False,3,"07-Why This Feature.pdf,07-Why This Feature,""1Chapter 1\nRethinking Documentation\nForget about documentation. Instead, focus on the speed of working","07-Why This Feature.pdf,07-Why This Feature,""1Chapter 1\nRethinking Documentation\nForget about documentation. Instead, focus on the speed of working on software. \nY ou want to deliver software faster. It isn’t just about going fast right now but about going sustainably fast in the long run. It is not just about you going fast but about the whole team or company going fast.\nWorking on software faster involves more productive programming languages and \nframeworks, better tools, and higher level of skills. But the more the industry makes progress on all these aspects, the more we have to look at the other bottlenecks.\nBeyond making use of technology , writing software is a lot about making deci-\nsions based on knowledge. When you don’t have enough knowledge, you have to make learning experiments and collaborate with other people to discover new knowledge. This takes time, which also means this knowledge is expensive and has value. Going fast is all about learning faster when you need new knowledge or about quickly recovering any prior valuable knowledge whenever there was some. Let us illustrate that point with a little story .\nA Tale from the Land of Living Documentation\nWe begin with a story . Imagine a software project to develop a new application as part of a bigger information system in your company . And imagine that you are a developer on this project. Y our task is to add a new kind of discount for recent loyal customers."",1413",1464,"**Rethinking Documentation**\n\n**Focus on Faster Software Development:**\n- Deliver software faster for long-term sustainability\n- Team or company wide focus on productivity\n- Faster programming languages, frameworks, tools, and skills\n\n**Bottlenecks Beyond Technology:**\n- Making decisions based on knowledge\n- Learning experiments and collaboration for new knowledge\n- Time-consuming and valuable when needed\n\n**Value of Learning Fast:**\n- Quickly recover prior valuable knowledge\n- Illustrated through a story: A Software Project\n\n**The Story:**\n- Developing a new application in a company project\n- Adding a new kind of discount for loyal customers\n- Importance of learning fast and making decisions based on knowledge.",721,18.14989995956421
False,3,"08-Sorry We Dont Have Marketing Documents.pdf,08-Sorry We Dont Have Marketing Documents,""Chapter 1 Rethinking Documentation 2\nWhy This Feature?\nY ou","08-Sorry We Dont Have Marketing Documents.pdf,08-Sorry We Dont Have Marketing Documents,""Chapter 1 Rethinking Documentation 2\nWhy This Feature?\nY ou meet Franck, from the marketing team, and Lisa, a professional tester, and the \nthree of you start talking about the new feature, asking questions, and looking for concrete examples. Lisa asks, “Why this feature?” Franck explains that the rationale is to reward recent loyal customers in order to increase customer retention, in a gam-ification approach, and suggests a link on Wikipedia about that topic. Lisa takes some notes about the main points and main scenarios.\nAll this goes quickly because everyone is around the table, and communication is \neasy . Also, the concrete examples make it easier to understand and clarify anything that starts out unclear. Once it’s all clear, everyone gets back to their desk. It’s Lisa’s turn to write down the most important scenarios and send them to everyone. (Last time it was Franck’s turn.) Now you can start coding from that.\nIn your previous work experience, the process did not work like this. Teams talked \nto each other through hard-to-read documents full of ambiguities. Y ou smile. Y ou quickly turn the first scenario into an automated acceptance test, watch it fail, and start writing code to make it pass to green.\nY ou have the nice feeling that you’ll be able to spend your valuable time on what \nmatters and nothing else.\nTomorrow You Won’t Need This Sketch Anymore\nThat afternoon, a pair of colleagues, Georges and Esther, ask the team about a design decision that needs to be made. Y ou meet around the whiteboard and quickly evaluate each option while sketching. Y ou don’t need much UML\n1 at this point, just \nsome custom boxes and arrows. You just want to make sure everybody understands it right now . A few minutes later a solution is chosen. The plan is to use two different topics in the messaging system; the rationale for this decision is the need for full iso-lation between the incoming orders and the shipment requests.\nEsther takes a picture of the whiteboard with her phone, just in case someone \nerases the whiteboard during the day . But she knows that in half a day , it will be implemented, and she can then safely delete the picture stored in her phone. One hour later, when she commits the creation of the new messaging topic, she takes care to add the rationale “isolation between incoming orders and shipment requests” in the commit comment.\nThe next day , Dragos, who was away yesterday , notices the new code and wonders \nwhy it’s like that. He runs \ngit blame  on the line and immediately gets the answer.\n1. Unified Modeling Language: http://www .uml.org/"",2622",2717,"**Rethinking Documentation**\n\n**Chapter 1**:\n- **Why This Feature?**\n  - Franck (marketing team) explains rationale to Lisa (professional tester) and others\n    - Reward recent loyal customers to increase customer retention in a gamification approach\n    - Suggests a link on Wikipedia about the topic\n  - Lisa takes notes on main points and scenarios\n  - Easy communication as everyone is around the table\n- **Concrete Examples**:\n  - Help clarify anything unclear\n  - Faster understanding than hard-to-read documents with ambiguities\n  - Allows team members to spend time on what matters\n\n**That Afternoon**:\n- Colleagues Georges and Esther ask about design decision\n- Team evaluates options on a whiteboard (no need for UML)\n- Quickly choose a solution: use two different topics in the messaging system\n  - Need for full isolation between incoming orders and shipment requests\n- Esther takes picture of whiteboard with her phone for reference\n- Solution implemented within half a day, so she deletes the saved image\n- Dragos (new team member) wonders why code is written that way and uses `git blame` to find out the rationale.\n\n**Unified Modeling Language**:\n- Mentioned as a resource for further reading.",1209,31.805680513381958
False,3,"09-The Future of Living Documentation Is Now.pdf,09-The Future of Living Documentation Is Now,""A T ale from the Land of Living Documentation 3\nSorry,","09-The Future of Living Documentation Is Now.pdf,09-The Future of Living Documentation Is Now,""A T ale from the Land of Living Documentation 3\nSorry, We Don’t Have Marketing Documents!\nA week later, a new marketing manager, Michelle, replaces Franck. Michelle is more \ninto customer retention than Franck. She wants to know what’s already implemented in the application in the area of customer retention, so she asks for the correspond-ing marketing document, and she is surprised to learn there is none.\n“Y ou can’t be serious!” she first says. But you quickly show her the website with \nall the acceptance tests produced during each build. There’s a search area on top so she can enter “customer retention” and search on it. She clicks submit and discovers the results: \n1  In order to increase customer retention\n2  As  a  marketing person3  I want to offer a discount to recent loyal customers   45    Scenario: 10$ off on next purchase for recent loyal customer6    ...78    Scenario: Recent loyal customers have bought 3 times in the last week9    ...\nThe result list displays many scenarios about the special discount for recent loyal \ncustomers. Michelle smiles. She didn’t even have to browse a marketing document to \nfind the knowledge she was looking for. And the level of precision of these scenarios well exceeds what she was expected.\n“Could we do the same discount for purchases in euro?” Michelle asks. Y ou reply , \n“I’m not sure the code manages currencies well, but let’s just try .” In your IDE\n2, you \nchange the currency in the acceptance test, and you run the tests again. They fail, so you know some work will need to be done to support that. Michelle has her answer within minutes. She begins to think that your team has something special compared to her former work environments.\nYou Keep Using This Word, but This Is Not What It Means\nThe next day Michelle has another question: What is the difference between a  purchase and an order?\n2. integrated development environment (IDE).\nChapter 1 Rethinking Documentation 4\nUsually she would just ask the developers to look in the code and explain the \ndifference. However, this team has anticipated the question, and the project’s web-\nsite includes a glossary . “Is this glossary up-to-date?” she asks. “Y es,” you reply . “It’s updated during every build—automatically from the code.” She’s surprised. Why doesn’t everybody do that? “Y ou need to have your code closely in line with the business domain for that,” you say simply , though you’re tempted to elaborate on the ubiquitous language from Eric Evans’s book Domain-Driven Design\n3 you’re so \nenthusiastic about.\nLooking at the glossary , Michelle discovers a confusion that nobody has spotted \nbefore in the naming, and she suggests fixing the glossary with the correct name. But this is not the way it works here. Y ou want to fix the name first and foremost in the code. Y ou rename the class and run the build again, and—voilà—the glossary is now fixed as well. Everybody is happy , and you just learned something new about the business of e-commerce.\nShow Me the Big Picture, and You’ll See What’s Wrong There\nNow you’d like to remove a toxic dependency between two modules, but you’re not very familiar with the full codebase, so you ask Esther for a dependency diagram, since she has the most knowledge of that. But even she does not remember every dependency . “I’ll generate a diagram of the dependencies from the code. It’s some-thing I’ve long wanted to do. This will take me a few hours, but then it’ll be done forever,” she says.\nEsther already knows about a few open-source libraries she can use to easily \nextract the dependencies from a class or a package, and she quickly wires one to Graphviz, the magical diagram generator that does the layout automatically . A few hours later, her little tool generates the diagram of dependencies. Y ou get what you wanted, and you’re happy . She then spends an extra half hour integrating this tool into the build.\nBut the funny thing is that when Esther first looks at the generated diagram, she \nnotices something intriguing: a dependency between the two modules that should not be there. By comparing her mental view of the system with the generated view of the actual system, it was easy to spot the design weakness.\nIn the next project iteration, the design weakness is fixed, and in the next build, \nthe dependency diagram is automatically updated. It becomes a much cleaner diagram.\n3. Evans, Eric. Domain-Driven Design: T ackling Complexity in the Heart of  Software. Hoboken: \n Addison-Wesley Professional, 2003."",4548",4649,"**Living Documentation**\n\n**A Tale from the Land of Living Documentation:**\n- Marketing manager Michelle replaces Franck\n- Michelle wants to know about customer retention\n- No marketing document exists, but she finds information through:\n  - Website search (customer retention scenarios)\n  - Asking team members for help\n  - Glossary on project's website (updates during every build)\n\n**Difference between Purchase and Order:**\n- Michelle asks about difference, usually would ask developers\n- Team has anticipating question and provides glossary\n- Glossary updated from code automatically\n\n**Dependency Diagram:**\n- Want to remove toxic dependency between modules\n- Ask Esther for dependency diagram\n- Esther generates diagram using open-source library and Graphviz\n- Spots a design weakness in the generated diagram\n- Weakness fixed during next project iteration, and diagram updated automatically.",899,26.195688247680664
False,3,"10-The Problem with Traditional Documentation.pdf,10-The Problem with Traditional Documentation,,0","10-The Problem with Traditional Documentation.pdf,10-The Problem with Traditional Documentation,,0",98,**The Issue of Traditional Documentation (10-The Problem with Traditional Documentation)**,90,4.185183763504028
False,3,"11-The Flaws of Documentation.pdf,11-The Flaws of Documentation,""The Problem with Traditional Documentation 5\nThe Future of Living Documentation Is N","11-The Flaws of Documentation.pdf,11-The Flaws of Documentation,""The Problem with Traditional Documentation 5\nThe Future of Living Documentation Is Now\nThis tale is not about the future. It is already here, right now , and it has been here for \nyears already . T o quote fiction writer William Gibson, this “future has arrived, it’s just not evenly distributed yet.” \nThe tools are here. The techniques are here. People have been doing all this for \nages, but it’s not yet mainstream. That’s a pity because these are powerful ideas for software development teams.\nIn the following chapters, we’ll go through all these approaches and many others, \nand you’ll learn how to implement them in your projects.\nThe Problem with Traditional Documentation\nDocumentation is the castor oil of programming—managers think it is good for \n programmers, and programmers hate it!\n—Gerald W einberg, Psychology of Computer Programming\nDocumentation is a boring topic. I don’t know about you, but in my work experi-\nence so far, documentation has mostly been a great source of frustration.\nWhen I’m trying to consume documentation, the information I need is always \nmissing. When it’s there, it is often obsolete and misleading, so I can’t even trust it.\nCreating documentation for other people is a boring task, and I’d prefer to be \ncoding instead. But it does not have to be this way.\nThere have been a number of times when I’ve seen, used, or heard about better \nways to deal with documentation. I’ve tried a lot of them. I’ve collected a number of \nstories, and you’ll find many of them in this book.\nThere’s a better way , but it requires adopting a new mindset about documentation. \nWith this mindset and the techniques that go with it, it is possible for documenta-tion to be as much fun as coding.\nDocumentation Is Not Cool, Usually\nWhat comes to mind when you hear the word documentation? Here are a few of the \nanswers you might give:\n •It’s boring.\n •It involves writing lots of text.\n •It means trying to use Microsoft Word without losing your sanity with picture placement.\nChapter 1 Rethinking Documentation 6\n •As a developer, I love dynamic, executable stuff that exhibits motion and \nbehavior. T o me, documentation is like a dead plant that’s static and dry .\n •It’s supposed to be helpful, but it’s often misleading.\n •Creating documentation is a boring chore. I’d prefer to be writing code instead of doing documentation (see Figure 1.1)! \nDocumentation takes a lot of time to write and to maintain, it becomes obsolete \nquickly , it is typically incomplete at best, and it is just not fun. Documentation is a fantastic source of frustration. And I’m sorry to bring you on this journey on such a dull topic.\nThe Flaws of Documentation\nLike cheap wine, paper documentation ages rapidly and leaves you with a bad headache.\n—@gojkoadzic on Twitter\nT raditional documentation suffers from many flaws and several common anti-\npatterns. An anti-pattern describes a common response to a recurring problem that is considered not a good idea and that should avoided.\nSome of the most frequent flaws and anti-patterns of documentation are described \nin the following sections. Do you recognize some of them in your own projects?\nSeparate Activities\nEven in software development projects that claim to be agile, deciding what to build and doing the coding, testing, and preparing documentation are too often separate activities, as illustrated in Figure 1.2. \nSeparate activities induce a lot of waste and lost opportunities. Basically , the \nsame knowledge is manipulated during each activity , but in different forms and Figure 1.1 Oh no…I’d better be coding!\nThe Problem with Traditional Documentation 7\nin different artifacts—and probably with some amount of duplication. In addi-\ntion, this “same” knowledge can evolve during the process itself, which may cause inconsistencies.\nManual Transcription\nWhen the time comes to do documentation, members of the team select some ele-ments of knowledge of what has been done and perform a manual transcription into a format suitable for the expected audience. Basically , it means writing another docu-ment about what has just been done in the code—like copyists before Gutenberg (see Figure 1.3). Test Doc Code Specs\nFigure 1.2 Separate activities in software development projects\nFigure 1.3 Manual transcription\nChapter 1 Rethinking Documentation 8\nRedundant Knowledge\nThe transcription just described leads to redundant knowledge: Y ou end up with the \noriginal source of truth (usually the code) and a bunch of copies that duplicate this knowledge in various forms. Unfortunately , when one artifact changes—for exam-ple, the code—it is hard to remember to update the other documents. As a result, the documentation quickly becomes obsolete, and you end up with incomplete docu-mentation that you cannot trust. How useful is that documentation?\nBoring Time Sink\nManagers want documentation for the users and also to cope with the turnover in the team. However, developers hate writing documentation. It is not fun compared to writing code or compared to automating a task. Dead text that gets obsolete quickly and that does not execute is not particularly exciting to write for a developer. When developers are working on documentation, they’d prefer to be working on the real working software instead. Paradoxically , when they want to reuse third-party software, they often wish it had more documentation available.\nTechnical writers like to do documentation and are paid for that. However, to get \naccess to the technical knowledge required, they usually need developers, and often they’re still doing manual transcription of knowledge. This is all frustrating and con-sumes a lot of precious time (see Figure 1.4). \nBrain Dump\nBecause writing documentation is not fun and is done because it simply has to be done, it is often done arbitrarily , without much thinking. The result is a random Figure 1.4 Documentation is often a time sink\nThe Problem with Traditional Documentation 9\nbrain dump of what the writer had in mind at the time of writing (see Figure 1.5). \nThe problem is that such a random brain dump is not helpful to anyone. \nPolished Diagrams\nThis anti-pattern is common with people who like to use CASE tools. These tools \nare not meant for sketching. Instead they encourage the creation of polished and large diagrams, with various layouts and validation against a modeling referential. All this takes a lot of time. Even with all the auto-magical layout features of these tools, it still takes too much time to create even a simple diagram.\nNotation Obsession\nIt is now increasingly obvious that the UML notation is not fashionable anymore, but in the decade following its adoption as a standard in 1997, it was the universal \nnotation for everything software, despite not being suited for all situations. No other notation has been popularized since that time, and teams around the world still use some UML to document stuff, even when it is not well suited for that. When all you know is UML, everything looks like one of its collection of standard diagrams.\nNo Notation\nIn fact, the opposite of notation obsession has been rather popular. Many people have simply ignored UML, drawing diagrams with custom notations that nobody understands the same way and mixing together random concerns like build depend-encies, data flow , and deployment concerns in a happy mess.Figure 1.5 A brain dump is not necessarily useful as documentation\nChapter 1 Rethinking Documentation 10\nInformation Graveyard\nEnterprise knowledge management solutions are the places where knowledge goes to \ndie. Consider these:\n •Enterprise wikis\n •SharePoint\n •Large Microsoft Office documents \n •Shared folders\n •Ticketing systems and wikis with poor search capabilities\nThese approaches to documentations often fail either because they make it too \nhard to find the right information or because it’s too much work to keep the infor-mation up-to-date or both. They promote a form of write-only documentation, or write-once documentation.\nOn a recent T witter exchange, the famous software developer Tim Ottinger \n(@tottinge) asked:\nProduct category: “Document Graveyard” - are all document management & wiki & \nSharePoint & team spaces doomed?\nJames R. Holmes (@James_R_Holmes) replied:\nOur standard joke is that “It’s on the intranet” leads to the response, “Did you just tell me to go ____ myself?” (Note: Edited because of  the original rough language; you get the idea.)\nMisleading Help\nWhenever documentation is not strictly kept up-to-date, it becomes misleading, as \npictured in Figure 1.6. Although it pretends to help, it is incorrect. As a result, such documentation may be interesting to read, but there’s an additional cognitive load involved in trying to find out what’s still correct versus what’s become incorrect. \nThere’s Always Something More Important Right Now\nWriting good documentation requires a lot of time, and maintaining it takes even more time. Those who are under time pressure often skip documentation tasks or do them quickly and badly ."",9136",9207,"**The Flaws of Traditional Documentation**\n\n**Documentation as a Problem**:\n- Quote from William Gibson: ""The future has arrived, it's just not evenly distributed yet.""\n- Tools and techniques are available, but documentation is not mainstream\n- Documentation is often seen as a ""boring topic"" and a source of frustration for developers\n\n**Flaws and Anti-Patterns in Traditional Documentation**:\n- **Separate Activities**: Documentation, coding, testing, etc. are treated as separate tasks\n- **Manual Transcription**: Manually transcribing information from code/specs into documentation\n- **Test Doc Code Specs**: Using documentation to test, code, and write specs\n- **Redundant Knowledge**: Duplication of knowledge between documentation and code\n- **Boring Time Sink**: Documentation is not as exciting or rewarding as writing code\n- **Brain Dump**: Documentation is a ""random brain dump"" without much thought or organization\n- **Polished Diagrams**: Overly formal diagrams that take too much time to create\n- **Notification Obsession/Notation Ignorance**: UML notation and other modeling approaches\n- **Information Graveyard**: Inadequate documentation management tools lead to outdated or poor quality documentation\n- **Misleading Help**: Outdated documentation that is no longer accurate or relevant\n- **There's Always Something More Important Right Now**: Documentation often takes a backseat to more pressing tasks",1420,47.525023460388184
False,3,"12-The Agile Manifesto and Documentation.pdf,12-The Agile Manifesto and Documentation,,0","12-The Agile Manifesto and Documentation.pdf,12-The Agile Manifesto and Documentation,,0",88,"**Agile Manifesto and Documentation**\n\n**The Agile Manifesto**:\n- Contains guiding principles for software development using an agile approach\n- Emphasizes values such as:\n  - Individuals and interactions over processes and tools\n  - Working software over comprehensive documentation\n  - Customer collaboration over customer satisfaction\n  - Responding to change over following a plan\n\n**Agile Documentation**:\n- Focuses on providing value to the development team and customers\n- Should not be excessive, but provide enough detail to support collaboration and transparency\n- Can include:\n  - User stories: brief descriptions of features from the perspective of the user\n  - Acceptance criteria: defining conditions for when a feature is considered complete\n  - Backlog items: prioritized list of features to be developed\n  - Scrum artifacts (e.g., sprint backlog, sprint burdened, and inventory): documentation used in the Agile framework to support effective planning and collaboration.",987,20.606099605560303
False,3,"13-The Origination of Knowledge.pdf,13-The Origination of Knowledge,""The Problem with Traditional Documentation 11\nThe Agile Manifesto and Documentat","13-The Origination of Knowledge.pdf,13-The Origination of Knowledge,""The Problem with Traditional Documentation 11\nThe Agile Manifesto and Documentation\nThe Agile Manifesto was written by a group of software practitioners in 2001. In it, \nthey list what they have come to value, including the following:\n •Individuals and interactions over processes and tools\n •Working software over comprehensive documentation\n •Customer collaboration over contract negotiation\n •Responding to change over following a plan\nThe second preference, “Working software over comprehensive documentation,” \nis frequently misunderstood. Many people believe that it disregards documentation completely . In fact, the Agile Manifesto does not say “don’t do documentation.” It’s only a matter of preference. In the words of the authors of the manifesto, “We embrace documentation, but not to waste reams of paper in never-maintained and rarely-used tomes.”\n4 Still, with agile approaches becoming mainstream in larger corporations, the \nmisunderstanding is still there, and many people neglect documentation.\nHowever, I’ve noticed recently that the lack of documentation is a big source of \nfrustration for my customers and colleagues, and this frustration is getting bigger. I was surprised to see some great appetite for the topic of documentation after I first mentioned living documentation at the Öredev conference in Sweden in 2013.\nIt’s Time for Documentation 2.0\nT raditional documentation is flawed, but now we know better. Since the end of the 1990s, practices like clean code, test-driven development (TDD), behavior-driven development (BDD), domain-driven design (DDD), and continuous delivery have \n4. Martin Fowler and Jim Highsmith, http://agilemanifesto.org/history .html\nPizza\nFigure 1.6 Documentation can be toxic when misleading\nChapter 1 Rethinking Documentation 12\nbecome increasingly popular. All these practices have changed the way we think \nabout delivering software.\nWith TDD, the tests are first considered as specifications. With DDD, we identify \nthe code and the modeling of the business domain, breaking with the tradition of models kept separately from the code. One consequence is that we expect the code to tell the whole story about the domain. BDD borrowed the idea of the business language and made it more literal, with tool support. Finally , continuous delivery is showing that an idea that looked ridiculous a few years ago (delivering several times a day in a non-event fashion) is actually possible and even desirable if we decide to follow the recommended practices.\nAnother interesting thing that is happening is due to the effect of time: Even \nthough old ideas like literate programing or HyperCard did not become mainstream, they remained slowly and quietly influential, especially in more recent programming languages communities such as F# and Clojure, which bring some of the old ideas to the foreground.\nNow at last we can expect an approach to documentation that is useful, always \nup-to-date, low cost, and fun to create. We acknowledge all the problems of the  traditional approach to documentation, and we also see that there is a need to be  fulfilled. This book explores and offers guidance on other approaches to meet the needs in more efficient ways. But first, let’s explore what documentation really is.\nDocumentation Is About Knowledge\nSoftware development is all about knowledge and decision-making based on that knowledge, which in turn creates additional knowledge. The given problem, the  decision that was made, the reason it was made that way , the facts that led to that decision, and the considered alternatives are all knowledge.\nY ou may not have ever thought about it this way , but each instruction typed in a \nprogramming language is a decision. There are big and small decisions, but no mat-ter the size, they are all decisions. In software development, there is no expensive con-struction phase following a design phase: The construction (running the compiler) is so cheap that there’s only an expensive—and sometimes everlasting—design phase.\nSoftware design can last a long time. It can last long enough to forget about pre-\nvious decisions made, as well as their contexts. It can last long enough for people to leave, taking with them their knowledge, and for new people to join, lacking  knowledge. Knowledge is central to a design activity like software development.\nMost of the time this design activity is, for many good reasons, a team effort \ninvolving more than one person. Working together means making decisions together or making decisions based on someone else’s knowledge."",4605",4680,"**Origination of Knowledge: Documentation**\n\n**Agile Manifesto and Documentation**:\n- Agile Manifesto written by software practitioners in 2001\n- Values:\n  - **Individuals and interactions over processes and tools**\n  - **Working software over comprehensive documentation**\n  - **Customer collaboration over contract negotiation**\n  - **Responding to change over following a plan**\n- ""Working software over comprehensive documentation"" is frequently misunderstood, not as ""don't do documentation"" but rather a matter of preference.\n\n**Documentation in Agile Approaches**:\n- Lack of documentation can be a source of frustration for customers and colleagues\n- Traditional documentation is flawed, but practices like **clean code**, **test-driven development (TDD)**, **behavior-driven development (BDD)**, and **domain-driven design (DDD)** have changed the way we think about delivering software.\n- Documentation can be ""toxic when misleading"" (Figure 1.6).\n\n**Documentation 2.0**:\n- Traditional documentation is flawed, but now we know better\n- Practices like **clean code**, **TDD/BDD**, and **continuous delivery** have changed the way we think about delivering software\n- Old ideas like ""literate programming"" or ""HyperCard"" have been slowly and quietly influential, especially in programming languages communities.\n\n**Documentation as Knowledge**:\n- Software development is all about **knowledge and decision-making based on that knowledge**\n- Each instruction typed in a programming language is a decision, no matter the size\n- **Software design can last a long time**, leading to forgetting previous decisions and contexts, or bringing new people who lack knowledge.",1672,90.56103849411011
False,3,"14-Why Knowledge Is Necessary.pdf,14-Why Knowledge Is Necessary,""Documentation Is About Knowledge 13\nSomething unique with software development is th","14-Why Knowledge Is Necessary.pdf,14-Why Knowledge Is Necessary,""Documentation Is About Knowledge 13\nSomething unique with software development is that the design involves not only \npeople but also machines. Computers are part of the picture, and many of the deci-\nsions made are simply given to the computer to execute. This is usually done through documents called source code. Using a formal language like a programming lan-guage, we pass knowledge and decisions to a computer in a form it can understand.\nHaving a computer understand source code is not the hard part, though. Even \ninexperienced developers usually manage to succeed at that. The hardest part is for other people to understand what has been done so that they can then do better and faster work.\nThe greater the ambition, the more documentation becomes necessary to ena-\nble a cumulative process of knowledge management that scales beyond what fits in our heads. When our brains and memories are not enough, we need assistance from technologies such as writing, printing, and software to help remember and organize larger sets of knowledge.\nThe Origination of Knowledge\nWhere does knowledge come from? Knowledge primarily comes from conversations. \nWe develop a lot of knowledge through conversations with other people. This hap-pen during collective work such as pair programming, or during meetings, or at the coffee machine, on the phone, or via a company chat or emails. Examples of conver-sations include BDD specification workshops and the three amigos in agile.\nHowever, as software developers, we have conversations with machines, too, and \nwe call these experiments. We tell something to a machine by using code in some programming language, and the machine runs it and tells us something in return: The test fails or goes green, the UI reacts as expected, or the result is not what we wanted, in which case we learn something new . Examples of experiments include TDD, emerging design, and Lean Startup experiments.\nKnowledge also comes from observation of the context. In a company , you learn \na lot by just being there, paying attention to other people’s conversations, behavior, and emotions. Examples of observation include domain immersion, obsession walls, information radiators, and Lean Startup “Get out of the building” observation.\nKnowledge comes from conversations with people and experiments with \nmachines in an observable context.\nHow Does Knowledge Evolve?\nSome knowledge can be stable for years, whereas other knowledge changes frequently over months or even hours.\nChapter 1 Rethinking Documentation 14\nAny form of documentation has to consider the cost of maintenance and make it \nas close to zero as possible. For stable knowledge, traditional methods of documen-\ntation work. But with frequently changing knowledge, writing text and updating it after every change is just not an option.\nThe effect of acceleration in the software industry is that we want to be in a posi-\ntion to evolve the software very quickly . The speed is such that it’s impossible to spend time writing pages and pages of documentation, and yet we want all the ben-efits of documentation.\nWhy Knowledge Is Necessary\nWhen creating software, we go through a lot of questions, decisions, and adjust-ments as we learn:\n •What problem are we trying to solve? Everyone should know it from now on.\n •What problem are we really  trying to solve? (We try to answer this when we \nrealize we got it wrong initially .)\n •We’ve been confusing trade and deal, but we eventually realized that they are not synonyms. We should not confuse them again.\n •We tried this new DB, and it doesn’t match our needs—for three reasons. No need to try again later as long as our needs remain the same.\n •We decided to decouple the shopping cart module and the payment module because we noticed that the changes to one had nothing to do with changes to the other. We should not couple them again.\n •We found out by chance that this feature is useless, so we plan to delete the code next month. But we are likely to forget our rationale, and if the code remains, it will be a mystery forever.\nWith existing software, when we miss the knowledge developed before, we end up \nredoing what’s already done because we don’t know it’s there already . We also end up putting a feature in an unrelated component because we don’t know where it should be, and this makes the software bloated. Or the code about the feature becomes frag-mented across various components.\nIf only we had the knowledge available to answer everyday questions like the \nfollowing:\n •Where can I fix that issue safely?\n •Where should I add this enhancement?\nDocumentation Is About Knowledge 15\n •Where would the original authors add this enhancement?\n •Is it safe to delete this line of code that looks useless?\n •I’d like to change a method signature, but what impacts will result if I do? \n •Do I really have to reverse engineer the code just to understand how it works?\n •Do I really have to spend time reading the source code each time the business \nanalysts need to know about the current business rules?\n •When a customer asks for a feature, how do we know if it’s already supported if it needs to be developed?\n •We have the feeling that the way we evolve the code is the best possible, but what if we lack a complete understanding of how it works?\n •How do we easily find the part of the code that deals with a particular feature?\nLack of knowledge manifests as two costs:\n •Wasted time: That time could have been better invested in improving some-thing else.\n •Suboptimal decisions: Other decisions could have been more relevant, or \ncheaper in the long term.\nThese two expenses compound over time: The time spent finding missing knowl-\nedge is time not spent on making better decisions. In turn, suboptimal decisions compound to make our life progressively more miserable, until we have no choice but to decide that the software is no longer maintainable and start again.\nIt sounds like a good idea to be able to access the knowledge that is useful to per-\nform the development tasks.\nSoftware Programming as Theory Building and Passing\nIn 1985, Peter Naur’s famous paper “Programming as Theory Building” per-fectly revealed the truth about programming as a collective endeavor: He said that it’s not so much about telling the computer what to do as it is about shar-ing with other developers the theory of the world (think “mental model”) that has been patiently elaborated by learning, experiment, conversations, and deep reflections. In his own words:\nChapter 1 Rethinking Documentation 16\nProgramming properly should be regarded as an activity by which the pro-\ngrammers form or achieve a certain kind of insight, a theory , of the matters \nat hand. This suggestion is in contrast to what appears to be a more common notion, that programming should be regarded as a production of a program and certain other texts.\n5\nThe problem is that most of the theory is tacit. The code only represents \nthe tip of the iceberg. It’s more a consequence of the theory in the mind of the developers than a representation of the theory itself. In Peter Naur’s view , this theory encompasses three main areas of knowledge:\n •The mapping between code and the world it represents: The programmer who has the theory of the program can explain how the solution relates to the affairs of the world that it helps to handle. \n •The rationale of the program: The programmer who has the theory of the program can explain why each part of the program is what it is; in other words, the programmer is able to support the actual program text with a justification of some sort.\n •The potential of extension or evolution of the program: The programmer who has the theory of the program is able to respond constructively to any demand for modification of the program in order to support the affairs of the world in a new manner.\nOver time, we’ve learned a number of techniques that enable people to pass \ntheories among themselves. Clean code and Eric Evans’s domain-driven design \nencourage programmers to find ways of expressing the theory in their heads more literally into the code. For example, DDD’s ubiquitous language bridges the gap between the language of the world and the language of the code, help-ing solve the mapping problem. I hope future programming languages will recognize the need to represent not only the behavior of the code but also the bigger mental model of the programmers, of which the code is a consequence.\nPatterns and pattern languages also come to mind, as literal attempts to \npackage nuggets of theories. The more patterns we know , the more we can encode the tacit theory , making it explicit and transferable to a wider extent. Patterns embody in the description of their forces the key elements of the rationale in choosing them, and they sometimes hint at how extension should happen. They might hint at the potential of the program; for example, a strat-egy pattern is meant to be extended by adding new strategies.\n5.  Peter Naur, “Programming as Theory Building,” Microprocessing and Microprogramming, \n V olume 15, Issue 5, 1985, pp. 253–261."",9189",9260,"**Why Knowledge is Necessary**\n\n**Documentation**:\n- Essential for recording and sharing knowledge about software development\n- Represents ""tip of the iceberg"" as code is a consequence of the underlying theory in developers' minds\n\n**Importance of Documenting Knowledge**:\n- Avoids wasting time and making suboptimal decisions\n- Helps maintain and evolve software\n- Facilitates understanding of the system for new team members or stakeholders\n\n**Origination of Knowledge**:\n- Primarily comes from conversations with people and experiments with machines\n- Conversations can be formal (pair programming, meetings) or informal (coffee machine discussions)\n- Machines provide feedback through tests and experiments (TDD, Lean Startup)\n- Observation of context is also a source of knowledge (domain immersion, information radiators)\n\n**Documenting Knowledge Costs**:\n- Stable knowledge can be documented using traditional methods\n- Frequently changing knowledge requires alternative documentation approaches\n\n**Programming as Theory Building**:\n- Programmers form theories about the world through learning, experimentation, and reflection\n- Clean code and DDD encourage expressing these theories more literally in the code\n- Patterns and pattern languages embody nuggets of theories, making them explicit and transferable.",1317,49.54576873779297
False,3,"15-Low Effort.pdf,15-Low Effort,""Documentation Is About Transferring Knowledge 17\nBut as we progress in the codification of our understanding, we als","15-Low Effort.pdf,15-Low Effort,""Documentation Is About Transferring Knowledge 17\nBut as we progress in the codification of our understanding, we also tackle \nmore ambitious challenges, so our frustration remains the same. I  believe \nNaur’s sentence from 1985 will still hold in the next decades:\nFor a new programmer to come to possess an existing theory of a program \nit is insufficient that he or she has the opportunity to become familiar with the program text and other documentation.\n6\nWe’ll never completely solve that knowledge transfer problem, but we can \naccept it as a fact and learn to live with it. The theory as a mental model in  programmers’ heads can never be fully shared with those who weren’t part of the thought process that led to building it.\nThe conclusion seems inescapable: At least in certain kinds of large pro-\ngrams, the continued adaption, modification, and correction of errors is dependent on a certain kind of knowledge possessed by a group of program-mers who are closely and continuously connected to each other.\nIt’s worth noting that permanent teams that regularly work collectively \ndon’t suffer too much from this issue of theory passing.\nDocumentation Is About Transferring Knowledge\nThe word documentation often brings a lot of connotations to mind: written docu-ments, Microsoft Word or PowerPoint documents, documents based on company templates, printed documents, big, heavy and boring text on a website or on a wiki, and so on. However, all these connotations anchor us to practices of the past, and they exclude a lot of newer and more efficient practices.\nFor the purposes of this book, we’ll adopt a much broader definition of \ndocumentation:\nThe process of transferring valuable knowledge to other people now and also to people \nin the future.\nThere’s a logistic aspect to documentation. It’s about transferring knowledge in \nspace between people and also about transferring it over time, which technical peo-\nple call persistence or storage. Overall, our definition of documentation looks like shipment and warehousing of goods, where the goods are knowledge.\n6.  Peter Naur, “Programming as Theory Building,” Microprocessing and Microprogramming, \n V olume 15, Issue 5, 1985, pp. 253–261.\nChapter 1 Rethinking Documentation 18\nT ransferring knowledge between people is actually transferring knowledge \nbetween brains (see Figure 1.7). From one brain to other brains, it’s a matter of trans-\nmission, or diffusion (for example, to reach a larger audience). From brains now to brains later, it’s about persisting the knowledge, and it’s a matter of memory . \n  Did you know?\nThe development tenure half-life is 3.1 years, whereas the code half-life is 13 years.\n7 Documentation has to help with this mismatch.\nT ransferring knowledge from the brain of a technical person to the brains of non-\ntechnical people is a matter of making the knowledge accessible. Another case of making knowledge accessible is to make it efficiently searchable.\nAnd there are other situations, such as needing to put knowledge into a specific \ndocument format for compliance reasons—because you just have to.\nFocusing on What Matters\nAs a means of transferring valuable knowledge, documentation can take many forms: written documents, face-to-face conversations, code, activity on social tools, or nothing at all when it’s not necessary .Store knowledge for the future.Transfer knowledge to other people.Space Time\nFigure 1.7 Documentation is about transferring and storing knowledge\n7.  Rob Smallshire, Sixty North blog, http://sixty-north.com/blog/predictive-models-of-development-\nteams-and-the-systems-they-build\nCore Principles of Living Documentation 19\nWith this definition of documentation, we can express some important principles:\n •Knowledge that is of interest for a long period of  time deserves to be \ndocumented.\n •Knowledge that is of interest to a large number of  people deserves to be documented.\n •Knowledge that is valuable or critical may also need to be documented.\nOn the other hand, you don’t need to care about documentation of knowledge \nthat isn’t in any of these cases. Spending time or effort on it would be a waste.\nThe value of the considered knowledge matters. There’s no need to make the \neffort to transfer knowledge that’s not valuable enough for enough people over a long enough period of time. If a piece of knowledge is already well known or is use-ful for only one person, or if it’s only of interest until the end of the day , then there’s no need to transfer or store it.\n The Default Is Don’t\nThere is no point in making any specific effort at documenting knowledge unless there’s a compelling reason to do it; otherwise, it’s a waste. Don’t feel bad about not documenting something that doesn’t need to be documented.\n \nHaving reconsidered what documentation really is, in terms of knowledge trans-\nmission and preservation, and some early consequences in how it should be man-aged, it is now time to introduce the central idea of living documentation and its core principles.\nCore Principles of Living Documentation\nThe term living documentation first became popular in the book Specification by \nExample by Gojko Adzic. Adzic described a key benefit of teams doing BDD: Their scenarios created for specifications and testing were also very useful as documenta-tion of the business behaviors. Thanks to the test automation, this documentation was always up-to-date, as long as the tests were all passing.\nIt is possible to get the same benefits of living documentation for all aspects of \na software development project: business behaviors, of course, but also business domains, project vision and business drivers, design and architecture, legacy strate-gies, coding guidelines, deployment, and infrastructure.\nChapter 1 Rethinking Documentation 20\nLiving documentation involves a set of four principles (see Figure 1.8):\n •Reliable: Living documentation is accurate and in sync with the software \nbeing delivered, at any point in time.\n •Low effort: Living documentation minimizes the amount of work to be done on documentation, even in case of changes, deletions, or additions. It requires only minimal additional effort—and only once.\n •Collaborative: Living documentation promotes conversations and knowledge sharing between everyone involved.\n •Insightful: By drawing attention to each aspect of the work, living documenta-tion offers opportunities for feedback and encourages deeper thinking. It helps reflect on the ongoing work and helps in making better decisions. \nLiving documentation also brings the fun back for developers and other team \nmembers. They can focus on doing a better job, and at the same time they get the liv-ing documentation out of this work.\nThe following sections briefly describe the four core principles of living docu-\nmentation that together will act as a guidance to unlock the most benefits from the approach. These important ideas are then elaborated on in the rest of the chapter and in the next three chapters.\nReliable\nT o be useful, documentation must be trustworthy; in other words, it must be 100% reliable. Since humans are never that reliable, we need discipline and tools to help with reliability .Low\nEffort\nReliable CollaborativeInsightful\nFigure 1.8 Principles of  living documentation"",7354",7393,"**Documentation**\n\n**Importance of Documenting Valuable Knowledge:**\n- Transferring knowledge between people is essential\n- Documentation helps with this process\n- Defined as the process of transferring valuable knowledge to others and future generations\n\n**Types of Documentation:**\n- Can take many forms: written documents, conversations, code, social tools, or none at all\n- Depending on the specific situation\n\n**Core Principles of Living Documentation:**\n1. **Reliable**: Accurate and in sync with software being delivered\n2. **Low Effort**: Minimizes work required for updates, deletions, or additions\n3. **Collaborative**: Promotes conversations and knowledge sharing among team members\n4. **Insightful**: Encourages feedback, deeper thinking, and decision making\n5. Helps bring back the fun for developers and other team members\n6. Trustworthy and reliable\n7. Eliminates human error\n\n**Reliability in Documentation:**\n- Depends on discipline and tools to ensure accuracy\n- Important for maintaining trust within a team or organization\n\n**Low Effort**:\n- Minimal additional effort required\n- Focuses on efficient methods of documenting knowledge\n\n**Collaborative**:\n- Encourages conversations and knowledge sharing\n- Fosters better communication among team members\n\n**Insightful**:\n- Provides opportunities for feedback and deeper thinking\n- Helps make informed decisions\n\n**Benefits of Living Documentation:**\n- Brings fun back for developers and team members\n- Focuses on doing a better job while documenting\n- Encourages effective documentation practices.",1565,59.03795647621155
False,3,"16-How Ants Exchange Knowledge Stigmergy.pdf,16-How Ants Exchange Knowledge Stigmergy,""Core Principles of Living Documentation 21\nT o achieve reliabl","16-How Ants Exchange Knowledge Stigmergy.pdf,16-How Ants Exchange Knowledge Stigmergy,""Core Principles of Living Documentation 21\nT o achieve reliable documentation, we rely on the following ideas:\n •Exploiting available knowledge: Most of the knowledge is already present \nin the artifacts of the project, it just needs to be exploited, augmented, and curated for documentation purposes.\n •Accuracy mechanism: An accuracy mechanism is needed to ensure that the knowledge is always kept in sync.\nLow Effort\nA living documentation must be low effort to be feasible and sustainable on ever-changing environments; you can achieve that thanks to the following ideas:\n •Simplicity: Documentation is best if there is nothing to declare, but it’s just obvious.\n •Standard over custom solutions: Standards are supposed to be known, and if that’s not the case, it is enough to just refer to a standard in an external refer-ence (for example, your favorite books, authors, or Wikipedia).\n •Evergreen content: There is always stuff that does not change or that changes very infrequently , and this material does not cost much to maintain.\n •Refactoring-proof knowledge: Some things don’t require human effort when there is a change. This can be because of refactoring tools that automatically propagate linked changes, or because knowledge intrinsic to something is col-located with the thing itself, and it changes and moves with that thing.\n •Internal documentation: Additional knowledge about a thing is best located on the thing itself, or as close as possible.\nCollaborative\nA living documentation must be collaborative through the following preferences:\n •Favor conversations over formal documentation: Nothing beats interactive, face-to-face conversations for exchanging knowledge efficiently . Don’t feel bad about not keeping a record of every discussion. Even though I usually favor conversations, there is knowledge that keeps on being useful repeatedly over a long period of time or for many people. It is important to pay attention to the \nChapter 1 Rethinking Documentation 22\nprocess of ideas sedimentation over time to decide what knowledge is worth \nthe effort of recording in a persistent form.\n •Accessible knowledge: In a living documentation approach, knowledge is often declared within technical artifacts in a source control system. This makes it difficult for nontechnical people to access it. Therefore, you should provide tools to make this knowledge accessible to all audiences without any manual effort.\n •Collective ownership: It’s not because all the knowledge is in the source con-trol system that developers own it. Developers don’t own the documentation; they just own the technical responsibility to deal with it.\nInsightful\nThe above-mentioned principles are useful, but to realize the full potential of a living documentation, it must be insightful:\n •Deliberate decision making: If you don’t know clearly what you’re doing, it shows immediately when you’re about to do living documentation. This kind of feedback encourages you to clarify your decisions so that what you do becomes easy to explain. By encouraging more deliberate decision-making, this will often raise the quality of the work.\n •Embedded learning: Y ou want to write code and other technical artifacts that are so good that your workmates can learn the design, the business domain, and everything else just by working with the system, learning through their interactions.\n •Reality check: Living documentation helps reveal the actual state of the sys-tem (for example, “I did not expect the implementation to be that messy ,” as in “I thought I was shaved correctly , but the mirror tells otherwise.”). This, again, can foster improvements by accepting reality for what it is, as opposed to what you’d like it to be.\nThe following sections describe these principles more in details, while the next \nchapters expand into the related patterns and practices to implement a successful living documentation approach. But before that, an important inspiration for living documentation can be found, surprisingly , in the way ants and other social insects collaborate and exchange knowledge."",4116",4209,"**Living Documentation Principles**\n\n**Exploiting Available Knowledge:**\n- Most knowledge is already present in project artifacts\n- Exploit, augment, and curate for documentation purposes\n\n**Accuracy Mechanism:**\n- Ensure knowledge stays synchronized\n\n**Low Effort:**\n- Simplicity: Documentation should be obvious and easy to maintain\n- Use standards: Refer to external sources instead of custom solutions\n- Evergreen content: Maintain infrequently changing material\n- Refactoring-proof knowledge: Tools automatically propagate changes\n- Internal documentation: Knowledge located on or near the thing itself\n\n**Collaborative:**\n- Favor conversations over formal documentation\n- Make knowledge accessible to all audiences\n- Collective ownership: Developers don't own documentation, but are responsible for technical aspects\n\n**Insightful:**\n- Encourage deliberate decision making through feedback\n- Write clear and effective code and artifacts for learning purposes\n- Reveal the actual state of the system to foster improvements.\n\n**Inspiration from Social Insects:**\n- Ants and other social insects collaborate and exchange knowledge, providing inspiration for living documentation principles.",1193,35.2664589881897
False,3,"17-Most Knowledge Is Already There.pdf,17-Most Knowledge Is Already There,""Most Knowledge Is Already There 23\nHow Ants Exchange Knowledge: Stigmergy\","17-Most Knowledge Is Already There.pdf,17-Most Knowledge Is Already There,""Most Knowledge Is Already There 23\nHow Ants Exchange Knowledge: Stigmergy\nMichael Feather (@mfeathers) recently shared a link to a fantastic article online by \nTed Lewis, who introduces the concept of stigmergy in relation to our work in  software as a team:\nThe French entomologist Pierre-Paul Grassé described a mechanism of insect coordina-\ntion he called “stigmergy”—work performed by an actor stimulates subsequent work by the same or other actors. That is, the state of a building, code base, highway , or other physical construction determines what needs to be done next without central plan-ning or autocratic rule. The actors—insects or programmers—know what to do next, based on what has already been done. This intuitive urge to extend the work of others becomes the organizing principle of modern software development.\nAnts use a special type of chemical marker—pheromones—to highlight the results \nof their activity .\n8\nSimilarly , programmers manufacture their own markers through emails, GitHub \nissues, and all kinds of documentation that augments code. As Lewis concludes:\nThe essence of modern software development is stigmergic intellect and markers embedded within the code base. Markers make stigmergy more efficient, by more reli-ably focusing a programmer’s attention on the most relevant aspects of the work that needs to be done.\n9\nStigmergy is already the prominent way we exchange knowledge between the \n people and the machines involved when doing software. One key idea of living docu-\nmentation is to acknowledge that this stigmergic effect is there, and to find ways to push it to the max. This starts by getting most of the knowledge out of the system you’re in, like ants do.\nMost Knowledge Is Already There\nThere is no need to record a piece of  knowledge when it is already recorded in the system itself.\nEvery interesting project is a learning journey that produces specific knowledge. \nWe usually expect documentation to give us the specific knowledge we need, but the \n8.  Ted Lewis, Ubiquity blog, http://ubiquity .acm.org/blog/why-cant-programmers-be-more-like-ants-or-\na-lesson-in-stigmergy\n9.  Ted Lewis, Ubiquity blog, http://ubiquity .acm.org/blog/why-cant-programmers-be-more-like-ants-or-\na-lesson-in-stigmergy\nChapter 1 Rethinking Documentation 24\nfunny thing is that all this knowledge is already there: in the source code, in the config-\nuration files, in the tests, in the behavior of the application at runtime, in memory of the various tools involved, and, of course, in the brains of all the people working on it.\nIn a software project, most of the knowledge is present in some form somewhere \nin the artifacts. It is similar to ants learning how to evolve their nest mostly from the nest itself.\nTherefore: Acknowledge that most of the knowledge is already in the system \nitself. When needed, identify where it is located and exploit it from there.\nEven if the knowledge is there somewhere, this does not mean that there is noth-\ning to do about it. There are a number of problems with the knowledge that’s already there:\n •Inaccessible: The knowledge stored in the source code and other artifacts is not accessible to nontechnical people. For example, source code is not readable by nondevelopers.\n •Too abundant: Huge amounts of knowledge are stored in the project arti-facts, which makes it impossible to use the knowledge efficiently . For example, each logical line of code encodes knowledge, but for a given question, only one or two lines may be relevant to the answer.\n •Fragmented: There is knowledge that we think of as one single piece but that is in fact spread over multiple places in the project’s artifacts. For example, a class hierarchy in Java is usually spread over multiple files, one for each subclass, even though we tend to think about the class hierarchy as a whole.\n •Implicit: A lot of knowledge is present implicitly in the existing artifacts. It may be, for example, 99% there but missing the 1% to make it explicit. For example, when you use a design pattern like a composite, the pattern is visible in the code only if you’re familiar with the pattern.\n •Unrecoverable: It may be that the knowledge is there but that there is no way to recover it because it’s excessively obfuscated. For example, business logic is expressed in code, but the code is so bad that nobody can understand it.\n •Unwritten: In the worst case, the knowledge is only in people’s brains, and only its consequences are there in the system. For example, there may a general business rule, but it may have been programmed as a series of special cases, so the general rule is not expressed anywhere."",4691",4772,"**Stigmergy: A Concept Relating to Software Development**\n- **Stigmergy**: described by French entomologist Pierre-Paul Grassé as a mechanism of insect coordination where the state of a building, code base, or other physical construction determines what needs to be done next\n- **Ants**: use chemical markers (pheromones) to highlight results of their activity\n- **Programmers**: manufacture their own markers through emails, GitHub issues, and documentation\n- **Stigmergic intellect and markers**: essential in modern software development\n- **Living documentation**: acknowledges the stigmergic effect and pushes it to the max\n\n**Most Knowledge Is Already There**\n- No need to record knowledge if it's already recorded in the system itself\n- Every interesting project is a learning journey that produces specific knowledge\n- Knowledge can be found in various forms within the project artifacts: source code, configuration files, tests, runtime behavior, tools, and people's brains\n\n**Problems with the Existing Knowledge**\n- **Inaccessible**: not accessible to non-technical people (e.g., source code)\n- **Too abundant**: huge amounts of knowledge stored in project artifacts, making it impossible to use efficiently\n- **Fragmented**: knowledge is spread over multiple places within the project's artifacts\n- **Implicit**: present but not explicitly stated\n- **Unrecoverable**: excessively obfuscated and cannot be recovered easily\n- **Unwritten**: only in people's brains, with no explicit representation in the system",1520,49.45942044258118
False,3,"18-Preferring Internal Documentation.pdf,18-Preferring Internal Documentation,""Internal Documentation 25\nInternal Documentation\nThe best place to st","18-Preferring Internal Documentation.pdf,18-Preferring Internal Documentation,""Internal Documentation 25\nInternal Documentation\nThe best place to store documentation is on the documented thing itself.\nY ou’ve probably seen pictures of the Google datacenters and of the Centre \n Pompidou in Paris (see Figure 1.9). They have in common a lot of color-coded pipes, \nwith additional labels printed or riveted on the pipes themselves. On the  Pompidou Center, air pipes are blue and water pipes are green. This logic of color-coding expands beyond the pipes: Electricity transport is yellow , and everything about  moving people is red, including the elevators and stairways. \nThis logic is also ubiquitous in datacenters, and even more documentation is \nprinted directly on the pipes. There are labels to identify the pipes, and there are \nFigure 1.9 The Centre Pompidou building is color coded\nChapter 1 Rethinking Documentation 26\narrows to show the direction of the water flow in them. In the real world, such color-\ncoding and ad hoc marking are often mandatory for fire prevention and firefighting: Water pipes for firefighters have very visible labels riveted on them, indicating where they come from. Emergency exits in buildings are made very visible above the doors. In airplanes, bright signs on the central corridors document where to go. In a situa-tion of crisis, you don’t have time to look for a manual; you need the answer in the most obvious place: right where you are, on the thing itself.\nInternal Versus External Documentation\nPersistent documentation comes in two flavors: external and internal.\nWith external documentation, knowledge is expressed in a form that has nothing \nto do with the chosen implementation technologies of the project. This is the case of the traditional forms of documentation, with separate Microsoft Office documents on shared folders or wikis with their own databases.\nAn advantage of external documentation is that it can take whatever format and \ntool is most convenient for the audience and for the writers. The drawback is that it’s extremely hard, if not impossible, to ensure that external documentation is up-to-date with respect to the latest version of the product. External documentation can also simply be lost.\nIn contrast, internal documentation directly represents knowledge by using the \nexisting implementation technology . Using Java annotations or naming conventions on the language identifiers to declare and explain design decisions is a good example of internal documentation.\nThe advantage of internal documentation is that it’s always up-to-date with any \nversion of the product, as it’s part of its source code. Internal documentation cannot be lost because it’s embedded within the source code itself. It’s also readily available and comes to the attention of any developers working on the code just because it’s under their eyes.\nInternal documentation also enables you to benefit from all the tools and all the \ngoodness of your fantastic IDE, such as autocomplete, instant search, and seamless navigation within and between elements. The drawback is that your expression of the knowledge is limited to the possible extension mechanisms built in to the language. For example, there’s little you can do to extend the Maven XML with additional knowledge about each dependency . Another big drawback is that knowledge expressed as internal documentation is not readily accessible to nondevelopers. However, it is possible to work around that limitation with automated mechanisms that extract the knowledge and turn it into documents that are accessible to the right audience.\nIf you’re familiar with the book Domain-Specific Languages by Martin Fowler \nand Rebecca Parsons, you’ll recognize the similar concept of internal versus external \nInternal Documentation 27\ndomain-specific languages (DSLs). An external DSL is independent from the cho-\nsen implementation technology . For example, the syntax of regular expressions has nothing to do with the programming language chosen for the project. In contrast, an internal DSL uses the regular chosen technology , such as the Java programming language, in a way that makes it look like another language in disguise. This style is often called a fluent style and is common in mocking libraries.\nExamples of Internal and External Documentation\nIt’s not always easy to tell whether documentation is internal or external, as it’s sometime relative to your perspective. Javadoc is a standard part of the Java pro-gramming language, so it’s internal. But from the Java implementors’ perspective, it’s another syntax embedded within the Java syntax, so it would be external.  Regular code comments are in a gray middle area. They’re formally part of the language but do not provide anything more than free text. Y ou’re on your own to write them with your writing talent, and the compiler will not help check for typos beside the default spell-checking based on the English dictionary .\nFrom the perspective of the developer, every standard technology used to build a \nsoftware product can be considered a host for internal documentation, including the following:\n •Feature files used for business-readable specifications and testing tools\n •Markdown files and images next to the code with a naming convention or linked to from the code or feature files\n •T ools manifests, including the dependency management manifest, automated deployment manifest, infrastructure description manifest, and so on\nWhenever we add documentation within these artifacts, we benefit from being \nable to use our standard toolset and have the advantage of being in the source con-trol, close to the corresponding implementation so that it can evolve together with it.\nExamples of potential media for internal documentation include the following:\n •Self-documenting code and use of clean code practices, including class and method naming, using composed methods and types\n •Annotations that add knowledge to elements of the programming language\n •Javadoc comments on public interfaces, classes, and main methods\n •Folder organization and decomposition and naming of modules and submodules"",6137",6222,"**Preferring Internal Documentation**\n\n**Internal Documentation**:\n- The best place to store documentation is on the documented thing itself\n- Example: Color-coded pipes in datacenters and buildings, indicating function\n- Advantage: Available in crisis situations when no manual is available\n\n**External vs. Internal Documentation**:\n- **External Documentation**: Separate documents not tied to implementation technology\n  - Can take any format, but not up-to-date with latest version of product\n  - Difficult to ensure accuracy and accessibility for non-developers\n- **Internal Documentation**: Part of the existing implementation technology\n  - Always up-to-date with product version\n  - Embedded within source code, readily available to developers\n  - Limited by language extension mechanisms\n\n**Automated Extraction of Internal Documentation**:\n- Possible to extract and make internal documentation accessible to non-developers\n\n**Examples of Internal Documentation**:\n- Javadoc (Java's standard part)\n- Regular code comments\n- Feature files, Markdown files, tools manifests\n\n**Media for Internal Documentation**:\n- Self-documenting code and clean code practices\n- Annotations on programming language elements\n- Javadoc on public interfaces, classes, main methods",1267,44.161389112472534
False,3,"19-Machine-Readable Documentation.pdf,19-Machine-Readable Documentation,""Chapter 1 Rethinking Documentation 28\nIn contrast, examples of external docu","19-Machine-Readable Documentation.pdf,19-Machine-Readable Documentation,""Chapter 1 Rethinking Documentation 28\nIn contrast, examples of external documentation include the following:\n •README and similar text files\n •Any HTML or Microsoft Office document about the project\nPreferring Internal Documentation\nRemember what I said earlier: The best place to put documentation about a thing is \non the thing itself.\nAs you’ll see throughout this book, I’m definitely in favor of internal documenta-\ntion, coupled with just enough automation for cases where it’s necessary to publish more traditional documents. I suggest choosing internal documentation by default, at least for all knowledge that’s at risk of changing regularly .\nEven for stable knowledge, I recommend internal documentation first, and I \nwould choose to do external documentation only when there’s clearly value added, such as with a documentation that must be maximally attractive (perhaps for mar-keting reasons). In that case, I suggest hand-crafted slides, diagrams with careful manual layout, and appealing pictures. The point of using external documentation would be to be able to add a human feel to the final document, so I’d use Apple Key-note or Microsoft PowerPoint, select or create beautiful quality pictures, and beta test the effectiveness of the documentation on a panel of colleagues to make sure it’s well received.\nNote that appeal and humor can be hard to automate or to encode into formal \ndocumentation, but it’s not impossible either.\nIn Situ Documentation\nInternal documentation, also an in-situ documentation, means documentation that is “in the natural or original position or place.”\n10\nThis implies that the documentation is not only using the same implementation \ntechnology , but is also directly mixed into the source code, within the artifact that built the product. In situ means bringing the additional knowledge about a thing where the thing is located, for example within the source code rather than in a remote location.\nThis type of documentation is convenient for developers. As in designing user \ninterfaces, where the term in situ means that a particular user action can be per-formed without going to another window , consuming and editing the documenta-tion can be performed without going to another file or to another tool.\n10.  By Permission. From Merriam-Webster.com © 2019 by Merriam-Webster, Inc.  https://www .merriam-\nwebster.com/dictionary/in situ."",2411",2490,"**Documentation Types**\n\n**Internal Documentation**:\n- Preferred over external documentation\n- Best place for documenting regularly changing knowledge\n- Can be handcrafted for a human feel using tools like Apple Keynote or Microsoft PowerPoint\n- Appeal and humor are hard to automate, but not impossible\n\n**In Situ Documentation**:\n- Internal documentation ""in the natural or original position or place""\n- Documentation mixed into the source code of the product\n- Convenient for developers as it's located with the code\n\n**Choosing Documentation Type**:\n- Default: internal documentation\n- External documentation only when there's clearly added value, like for marketing\n- For external documentation, use hand-crafted slides, diagrams, and appealing pictures.",759,21.448387145996094
False,3,"20-Specific Versus Generic Knowledge.pdf,20-Specific Versus Generic Knowledge,,0","20-Specific Versus Generic Knowledge.pdf,20-Specific Versus Generic Knowledge,,0",80,"**Specific vs. Generic Knowledge**\n\n**Key Point**: Differentiate between specific and generic knowledge.\n\n**Explanation**:\n- **Specific Knowledge**: Facts or information directly related to a particular subject or situation, often unique or unrepeated.\n- **Generic Knowledge**: Principles or concepts that apply broadly across multiple subjects or situations, not tied to any one specific context.\n\n**Example of Specific Knowledge**: The capital city of France is Paris.\n\n**Example of Generic Knowledge**: All living organisms are composed of cells.",549,12.7561616897583
False,3,"21-Focusing on Specific Knowledge.pdf,21-Focusing on Specific Knowledge,""Specific Versus Generic Knowledge 29\nMachine-Readable Documentation\nGood do","21-Focusing on Specific Knowledge.pdf,21-Focusing on Specific Knowledge,""Specific Versus Generic Knowledge 29\nMachine-Readable Documentation\nGood documentation focuses on high-level knowledge like the design decisions on \ntop of the code and the rationale behind these decisions. We usually consider this kind of knowledge to be of interest only to people, but even tools can take advantage of it. Because internal documentation is expressed using implementation technolo-gies, it can usually be parsed by tools. This opens new opportunities for tools to assist developers in their daily tasks. In particular, it enables automated processing of knowledge for curation, consolidation, format conversion, automated publishing, or reconciliation.\nSpecific Versus Generic Knowledge\nThere is knowledge that is specific to your company , your particular system, or your business domain, and there is knowledge that is generic and shared with many other people in many other companies in the industry . \nKnowledge about programming languages, developers’ tools, software patterns, \nand practices belongs to the generic knowledge category . Examples include DDD, patterns, continuous integration using Puppet and Git tutorials.\nKnowledge about mature business industry sectors is also generic knowledge. \nEven in very competitive areas like pricing in finance or supply chain optimization in e-commerce, most of the knowledge is public and available in industry-standard books, and only a small part of the business knowledge is specific and confidential—and only for a while at that.\nFor example, each business domain has its own essential reading list, and it \nmight have a book that is often referred to as “The Bible” of that field (for example, Options, Futures, and Other Derivatives by John C Hull, Logistics and Supply Chain Management by Martin Christopher).\nThe good news is that generic knowledge is already documented in the industry \nliterature. There are books, blog posts, and conference talks that describe it quite well. There are standard vocabularies to talk about it. There are trainings available to learn it faster from knowledgeable people.\nLearning Generic Knowledge\nY ou also learn generic knowledge by doing your job, as well as by reading books and attending trainings and conferences. This only takes a few hours, and you know beforehand what you’re going to learn, how long it will take, and how much it will cost. It’s as easy to learn generic knowledge as it is to go to the store to buy food."",2458",2537,"**Focusing on Specific Knowledge**\n\n**Importance of Documentation**:\n- Focuses on high-level knowledge (design decisions, rationale)\n- Can be parsed by tools for automated processing\n- Enables tools to assist developers in daily tasks\n\n**Specific Versus Generic Knowledge**:\n- **Specific knowledge**: Company-, system-, or business domain-specific\n- **Generic knowledge**: Shared with many other people and industries\n- Examples of generic knowledge:\n  - Programming languages, tools, software patterns, practices\n  - Business industry sectors (e.g., pricing in finance, supply chain optimization)\n- Generic knowledge is already documented in industry literature\n- Learning generic knowledge takes only a few hours through reading, training, and conferences",757,18.705155849456787
False,3,"22-Questioning the Need for Documentation at All.pdf,22-Questioning the Need for Documentation at All,""Chapter 1 Rethinking Documentation 30\nGeneric","22-Questioning the Need for Documentation at All.pdf,22-Questioning the Need for Documentation at All,""Chapter 1 Rethinking Documentation 30\nGeneric knowledge is a solved problem. This knowledge is ready-made, ready to \nbe reused by everyone. When you use it, you just have to link to an authoritative \nsource, and you’re done documenting. This is as simple as noting an Internet link or a bibliographic reference.\nFocusing on Specific Knowledge\nUse documentation for specific knowledge and learn the generic knowledge from trainings.\nSpecific knowledge is the knowledge your company or team has that is not (yet) \nshared with other peers in the same industry . This knowledge is more expensive to learn than generic knowledge; it takes time practicing and making mistakes. This is the kind of knowledge that deserves most attention.\nSpecific knowledge is valuable and cannot be found ready-made, so it’s the kind \nof knowledge you have to take care of. Specific knowledge deserves the biggest efforts from you and your colleagues. As a professional, you should know enough of the generic, industry standard knowledge to be able to focus on growing the knowledge that’s specific to your particular ambitions.\nTherefore: Make sure everyone is trained on the generic knowledge in your \nindustry. Then focus any documentation effort on specific knowledge.\nEnsuring Documentation Accuracy \nY ou can trust documentation only if  there is a mechanism to guarantee its accuracy.\nWhen it comes to documentation, the main problem is often that it’s not accu-\nrate, usually because of obsolescence. Documentation that is not 100% accurate all the time cannot be trusted. As soon as you know documentation can be misleading from time to time, it loses its credibility . It might still be a bit useful, but it will take more time to find out what’s right and what’s wrong in it. And when it comes to creating documentation, it’s hard to dedicate time to it when you know it won’t be accurate for long; its lifespan is a big motivation killer.\nBut updating documentation is one of the most unappreciated tasks ever. It is \nnot interesting and doesn’t seem rewarding. However, you can have nice documenta-tion if you take it seriously and decide to tackle it with a well-chosen mechanism to enforce accuracy at all times.\nTherefore: You need to think about how you address the accuracy of your \ndocumentation.\nEnsuring Documentation Accuracy 31\nAccuracy Mechanism for Reliable Documentation \nAs mentioned earlier, authoritative knowledge that can be trusted already exists \nsomewhere, usually in the form of source code. Therefore, duplicated knowledge is problematic because it multiplies the cost of keeping it updated to keep pace with changes. This applies to source code, of course, and this applies to every other arti-fact, too. We usually call “design” the discipline of making sure that changes remain cheap at any point in time. We need design for the code, of course, and we need the same design skills for everything about documentation.\nA good approach for documentation is a matter of design. It takes design \nskills to design documentation that is always accurate, without slowing down the software development work.\nWith knowledge that can change at any time, there are a number of approaches \nto keeping documentation accurate. They are described in the following sections, ordered from the most desirable to the least desirable, and Chapter 3, “Knowledge Exploitation,” expands on them.\nConsider a piece of knowledge kept in a single source that is authoritative. This \nknowledge is accessible only to the people who can read the files. For example, source code is a natural documentation of itself for developers, and with good code, there’s no need for anything else. For example, a manifest to configure the list of all dependencies for a dependency management tool like Maven or NuGet is natural authoritative documentation for the list of dependencies. As long as this knowledge is only of interest for developers, it’s just fine as it is; there’s no need for a publishing mechanism to make the knowledge accessible to other audiences.\nSingle Sourcing with a Publishing Mechanism\nSingle sourcing is the approach to favor whenever possible. With single sourcing, knowledge is kept in a single source that is authoritative. It’s made available in vari-ous forms as published and versioned documents, thanks to an automated publish-ing mechanism. Any time there’s a change, it’s updated there and only there.\nAs an example, source code and configuration files are often the natural authori-\ntative homes for a lot of knowledge. When necessary , the knowledge from such a single source of truth is extracted and published in another form, but it remains clear that there is only one place that is authoritative. The publishing mechanism should also be automated to be run frequently; automation prevents the introduction of errors that is common with manual documentation.\nEven without the additional comments, Javadoc is a good example of this \napproach: The reference documentation is the source code itself, as parsed by the Javadoc Doclet, and it’s published automatically as a website for everyone to browse the structure of interfaces, classes, and methods, including the class hierarchies, in a convenient and always accurate manner.\nChapter 1 Rethinking Documentation 32\nRedundant Sources with a Propagation Mechanism\nKnowledge may be duplicated in various places, but reliable tools can automatically \npropagate any change in one place to every other place. Automated refactorings in your IDE are the best examples of this approach. The class names, interface names, and method names are repeated everywhere in the code, but it’s easy to rename them because the IDE knows how to reliably chase every reference and update it correctly . This is far superior to and safer than using Find and Replace, where you run the risk \nof replacing random strings by mistake.\nSimilarly , documentation tools such as AsciiDoc offer built-in mechanisms to \ndeclare attributes that you can then embed everywhere in the text. Thanks to the built-in include and substitution features, you can rename and make changes in one place and propagate the change to many places at no cost.\nRedundant Sources with a Reconciliation Mechanism\nIf knowledge is declared in two sources, one source may change without the other changing—and that’s a problem. There’s a need for a mechanism to detect whenever the two sources don’t match. Such a reconciliation mechanism should be automated and run frequently to ensure permanent consistency .\nBDD with automation tools such as Cucumber is an example of this approach. In \nthis case, the code and the scenarios are the two sources of knowledge, and they both describe the same business behavior. Whenever a test running the scenarios fails, it’s a signal that the scenarios and the code are no longer in sync.\nAn Anti-Pattern: Human Dedication\nHuman dedication is an anti-pattern. If knowledge is duplicated in various places, sometimes it’s left to people on the team to make sure everything remains consistent at all times through a lot of dedication and hard grunt work. In practice, this does not work, and it is not a recommended approach.\nWhen Documentation Does Not Need an Accuracy Mechanism \nIn some cases, such as in the following sections, you don’t need an accuracy mecha-nism for your documentation. \nSingle-Use Knowledge\nSometimes accuracy is just not a concern because the knowledge recorded will be disposed of within hours or a few days after use. This kind of transient knowledge does not age and does not evolve, and hence there’s no concern about its \nBig Questions to Challenge Y our Documentation 33\nconsistency—as long as it’s used for only a short period of time and actually dis-\nposed of immediately after use. For example, conversations between the pair in pair programming and the code written during baby steps in TDD don’t matter once the task at hand is done.\nAccounts from the Past\nAn account of past events, such as a blog post, is not subject to issues of accuracy because it is clear for the reader that there is no promise of the text being accurate forever. The point of the post may be, for example, to describe a situation as it hap-pened, including the thinking at the time and the related emotions.\nSuch knowledge that is accurate at a point in time and that’s recorded in the con-\ntext of that point in time is not considered obsolete documentation. The knowledge in the blog post does get outdated over time, but this is not a problem as it’s clearly in the context of a blog post with a date and a story that’s clearly in the past. This is a smart way to archive episodes of work and the big idea behind a story in a persis-tent fashion, without pretending it is evergreen. A blog post won’t mislead anyone to think that it’s new information as it’s clear that it’s an account of a past reflection. As a story anchored in the past, it’s always an accurate story , even if you can’t trust the particular code or examples that may be quoted. It’s like reading a book on his-tory , and there are a lot of precious lessons to be learned, regardless of the context in which they happened.\nThe worst that can happen to an account from the past is that it might become \nirrelevant, when the concerns of that time are no longer concerns. \nBig Questions to Challenge Your Documentation \nEvery minute crafting documents is a minute lost to other things. Is this adding value? \nIs this most important?\n—@dynamoben on Twitter\nImagine that your boss or a customer asks for “more documentation.” A number \nof important questions need to be asked and answered to decide what to do next. \nThe goal behind these questions is to make sure you’re going to use your time as effi-ciently as possible in the long run.\nThe order in which you ask the important questions listed in the following sec-\ntions depends on the situation, and you may skip or rearrange the questions at will. The following sections explain the thought process involved in determining how to do documentation, and once you understand it, you can make the process your own."",10164",10274,"**Rethinking Documentation**\n\n**Chapter 1: Rethinking Documentation**\n- **Generic vs Specific Knowledge**:\n    - Generic knowledge is ready-made and easily reusable, requiring only a link to an authoritative source\n    - Specific knowledge is company/team-specific and more expensive to learn, deserves focus in documentation\n- Ensuring Documentation Accuracy:\n    - Documentation must be accurate to be trusted\n    - Obsolescence leads to misleading information\n    - Updating documentation is important but often overlooked\n- **Accuracy Mechanism for Reliable Documentation**:\n    - Single sourcing with automated publishing mechanism is preferred\n    - Redundant sources can be kept in sync with automatic reconciliation\n    - Human dedication is an anti-pattern\n- When Documentation Does Not Need an Accuracy Mechanism:\n    - Transient knowledge (e.g., conversations, code) does not need to be accurate forever\n    - Accounts of past events are not subject to issues of accuracy\n\n**Considerations for Documentation**:\n- Is this adding value? Is it the most important thing to focus on?\n- Important questions to ask when considering additional documentation:\n    1. What is the purpose and goal of the documentation?\n    2. Who is the intended audience?\n    3. What specific knowledge needs to be documented?\n    4. How will the documentation be shared and accessed (e.g., single sourcing, publishing mechanism)?\n    5. How often will the documentation need to be updated?\n    6. What is the expected lifespan of the knowledge being documented?",1547,54.48770761489868
False,3,"23-Making an Activity Fun.pdf,23-Making an Activity Fun,""Chapter 1 Rethinking Documentation 34\nQuestioning the Need for Documentation at All\nDocumen","23-Making an Activity Fun.pdf,23-Making an Activity Fun,""Chapter 1 Rethinking Documentation 34\nQuestioning the Need for Documentation at All\nDocumentation is not an end in itself; it’s a mean for a purpose that must be \n identified. Y ou won’t be able to make something useful unless you understand the goal. So the first question is:\nWhy do we need this documentation?\nIf no answer comes easily , then you’re definitely not ready to start investing effort \nin additional documentation. Y ou should put the topic on hold until you know \n better. Y ou don’t want to waste time on ill-defined objectives.\nThen the next question immediately follows:\nWho’s the intended audience?If the answer is unclear or sounds like “everyone,” then you’re not ready to start \ndoing anything. Efficient documentation must target an identified audience. Even \ndocumentation about things “that everyone should know” has to target an audi-ence, such as “nontechnical people with only a superficial knowledge of the business domain.”\nNow , still determined to avoid wasting time, you’re ready for the first question of \ndocumentation.\nThe First Question of Documentation\nDo we really need this documentation?\nSomeone might be tempted to create documentation on a topic that is only of \ninterest for himself or herself or that is relevant only for the time he or she is working on it. Perhaps it does not make much sense to even add a paragraph to the wiki. But there’s another, and worse, reason for asking for documentation.\nNeed for Documentation Because of Lack of Trust\nThe answer to the first question of documentation may sound something like “I need documentation because I’m afraid you don’t work as much as I’d like, so I need to see deliverables to make sure you work hard enough.” In this case, the main issue is not a matter of documentation. \nAs the duo Matt Wynne (@mattwynne) and Seb Rose (@sebrose) said at the BDD \neXchange conference in 2013: “Need for detail might indicate lack of trust.” In such \nBig Questions to Challenge Y our Documentation 35\na case, lack of documentation is just a symptom, and the root issue is lack of trust. \nThis is a serious enough issue that you should stop reading this book and try to find ways to improve the situation. No amount of documentation alone can fix lack of trust in the first place. However, since delivering value often is a good way to build trust, sensible documentation has a side role in a remediation. For example, making the work more visible may help build trust and is a form of documentation.\nJust-in-Time Documentation, or a Cheap Option on Future \nKnowledge\nIf you need documentation, you might not actually need it right away . Hence, there is \nanother first question of documentation.\n The Other First Question of Documentation\nDo we really need this documentation now? \nCreating documentation has a cost, and the benefit in the future is an uncertain \nbenefit. The benefit is uncertain when you cannot be sure someone will have the need for the information in the future.\nOne thing I’ve learned over the years in software development is that people are \nnotoriously bad at anticipating the future. Usually people can just bet, and their bets are often wrong. As a consequence, it is important to use a number of strategies to determine when it’s important to do documentation:\n •Just-in-time: Add documentation only when really needed.\n •Cheap upfront: Add a little documentation now , at a very low cost.\n •Expensive upfront: Add documentation now , even if it takes time to create it.\nJust-in-Time\nY ou may decide that the cost of documenting now is not worth it, given the uncer-tainty that it will be useful in the future. In such a case, you might put off doing documentation until it becomes really necessary . Typically , it is a good idea to wait for someone to initiate a documentation effort. On a big project with lots of stake-holders, you might even decide to wait for a second or third request before deciding it’s worth investing time and effort in creating documentation.\nNote that this assumes that you will still have the knowledge available somewhere \nin the team when the time comes to share it. It also assumes that the effort of docu-menting in the future will not be too great compared to what it would be right now .\nChapter 1 Rethinking Documentation 36\nCheap Upfront\nY ou might decide that the cost of documenting right now is so low that it’s not worth \ndeferring it for later, even if it’s never actually used. This is especially relevant when the knowledge is fresh in mind and you run the risk that it will be much harder later to remember all the stakes and important details. And, of course, creating documen-tation upfront makes sense if you have cheap ways to do it, as you’ll see later.\nExpensive Upfront\nY ou might decide that it’s worth it to bet on the future need for this knowledge and choose to create documentation right now , even if doing so is not cheap. There’s the risk it might be a waste, but you might be happy to take this risk—hopefully for some substantiated reason (for example, guidelines or compliance requirement, high confidence from more than one person that it’s necessary).\nIt’s important to keep in mind that any effort around documentation right now \nalso has an impact on the quality of the work because it put the focus on how it’s done and why and acts like a review . This means that even if it’s never used in the future, it can be useful at least once, right now , for the sake of thinking clearly about the decisions and the rationale behind them.\nQuestioning the Need for Traditional Documentation\nAssuming that there’s a genuine need for additional documentation, for an identified purpose, and for an identified audience, you’re now ready for the second question of documentation.\n The Second Question of Documentation\nCould we just share knowledge through conversations or working together? \nT raditional documentation should never be the default choice, as it’s too wasteful \nunless absolutely necessary . When there is a need for knowledge transfer from some people to other people, this is best done by simply talking—by asking and answering questions instead of exchanging written documents.\nWorking collectively , with frequent conversations, is a particularly effective form \nof documentation. Techniques such as pair programming, cross-programming, the three amigos in agile, and mob-programming totally change the game with respect to documentation, as knowledge transfer between people is done continuously and at the same time the knowledge is created or applied on a task.\nConversations and working collectively are the preferred forms of documenta-\ntion, although sometimes they are not enough. Sometime there’s a genuine need to have the knowledge formalized.\nBig Questions to Challenge Y our Documentation 37\nChallenging the Need for Formalized Documentation\nDoes it have to be persistent? Does it have to be shared to a large audience? Is \nit critical knowledge?\nIf the answer to each of these questions is “no,” conversations and work-\ning collectively should be enough, and there is no need for more formal documentation.\nOf course, if you ask a manager these questions, you’re likely to be answered \n“yes” just because it’s a safe choice. Y ou can’t be wrong by doing more, right? It’s a bit like setting the priority on tasks; many people put a high priority flag on everything, which then makes high priority meaningless. With documenta-tion, what seems to be the safe choice carries a higher cost, which can in turn endanger the project. The safe choice is really to consider these three questions in a balanced way rather than automatically answering “yes” or “no.”\nEven for the knowledge that must be shared to a large audience, that must be \nkept persistent for the long term or that is critical, there are several documentation options:\n •Plenary meeting with the full audience attending, or a lecture-style conference talk with the audience hopefully taking notes \n •Podcast or video, like a recorded conference talk or a recorded interview\n •Artifacts that are self-documented or that are augmented in an internal docu-mentation approach\n •Manually written document\nThe point is that, even with particularly important knowledge, manually written \ndocumentation does not have to be the default choice.\nMinimizing Extra Work Now\nSay that you have a legitimate need to keep some knowledge in a formal form. Because, as you’ve learned, most knowledge already exists somewhere, in some form, you need to answer another question.\n The Knowledge Location Question\nWhere’s the knowledge right now? \nChapter 1 Rethinking Documentation 38\nIf knowledge is only in the heads of the people, then it needs to be encoded \n somewhere—as text, code, metadata, or something else. If the knowledge is already \nrepresented somewhere, the idea is to use it (knowledge exploitation) or reuse it (knowledge augmentation) as much as possible.\nY ou might be able to use the knowledge that’s in the source code, in the configura-\ntion files, in the tests, in the behavior of the application at runtime, and perhaps in the memory of the various tools involved. This process, which is described in detail in the following chapters, involves asking the following questions:\n •Is the knowledge exploitable, or obfuscated, or unrecoverable?\n •Is the knowledge too abundant?\n •Is the knowledge accessible for the intended audience?\n •Is the knowledge in one single place or fragmented?\n •What is missing that would make the knowledge 100% explicit?\nWhen the knowledge is not fully there or is too implicit to be used, then the game \nbecomes finding a way to add the knowledge directly into the source of the product. That is the focus of Chapter 4, “Knowledge Augmentation.”\nMinimizing Extra Work Later\nIt’s not enough to create documentation once; you must consider how to keep it accurate over time. Therefore, an important question remains.\n The Knowledge Stability Question\nHow stable is this knowledge? \nStable knowledge is easy because you can ignore the question of its maintenance. \nOn the other end of the spectrum, living knowledge is challenging. It can change often or at any time, and you don’t want to update multiple artifacts and documents over and over.\nThe rate of change is the crucial criterion (see Figure 1.10). Knowledge that is \nstable over years can be taken care of with any traditional form, such as writing text manually and printing it on paper. Knowledge that is stable over years can even sur-vive some amount of duplication because it will not need to be updated. \nMaking an Activity Fun 39\nIn contrast, knowledge that changes every hour or more often just cannot afford \ntraditional forms of documentation. The key concerns to keep in mind are the cost \nof evolution and the cost of maintaining the documentation. Changing the source code and then having to update other documents manually is not an option.\nThis process, which is described in the next chapters, involves the following \nquestions:\n •If it changes, what changes at the same time?\n •If there’s redundant knowledge, how do we keep the redundant sources in sync?\nMaking an Activity Fun\nT o make an activity sustainable, make it fun. \nFun is important for sustainable practices. If something is not fun, you’ll not want \nto do it very often, and the practice will progressively disappear. For practices to last, they need to be fun. This is particularly important with a boring topic such as documentation.\nTherefore: Choose living documentation practices that are as fun as possible. If \nsomething is fun, do more of it, and if it’s totally not fun, look for alternatives, such as solving the problem in another way or through automation.\nThis preference for fun activities obviously assumes that working with people is \non the fun side, because there’s no good way around that. For example, if coding is fun for you, you’ll try to document as much as possible in code. That’s the idea behind many suggestions in this book. If copying information from one place to Figure 1.10 The rate of  change of  the knowledge is the key criterion\nChapter 1 Rethinking Documentation 40\nanother is a chore, then it’s a candidate for automation or for finding a way to avoid \nhaving to move data at all. Fixing a process and automating a part of a process tend to be fun, so these are also things you might feel like doing (see Figure 1.11)—and that’s lucky . \nMixing Fun and Professionalism\nThere’s nothing wrong with having fun at work, as long as you’re professional in your work. This means doing your best to solve the problems that matter, delivering value, and reducing risk. With that in mind, you’re free to choose the practices and tools that make your life more fun. After 18 years in program-ming, I’m now confident it’s always possible to do professional work while having fun. The idea that work should be boring and unpleasant because it’s work or because you’re paid for it to compensate for this very unpleasantness is just stupid. Y ou’re paid some money to deliver value that is worth even more money . Delivering value is fun, and behaving professionally is pleasant, too. And fun is essential for working efficiently as a team in a pleasant atmosphere.Figure 1.11 Fun starts with automating the chores"",13394",13458,"**Documentation: Rethinking Approach and Practices**\n* **Chapter 1:** Discussing documentation's purpose, audience, and various aspects\n* **Need for Documentation**: Understanding why it's essential and identifying the intended audience\n* **First Question of Documentation**: Do we really need this documentation now? (just-in-time, cheap upfront, expensive upfront)\n	+ Just-in-time: Add documentation only when necessary\n	+ Cheap upfront: Documentation with low cost and minimal effort\n	+ Expensive upfront: Documentation for future use despite the cost\n* **Questioning Traditional Documentation**: Can knowledge be shared through conversations or working together?\n* **Minimizing Extra Work Now:** Finding existing knowledge sources instead of creating new ones\n	+ Is the knowledge explicit, abundant, accessible, in one place, and complete?\n* **Minimizing Extra Work Later:** Keeping documentation accurate over time based on its stability\n	+ Stable knowledge: Traditional forms of documentation are sufficient\n	+ Living knowledge: Automation and reduction of manual updates are essential\n* **Making an Activity Fun**: Making sustainable practices enjoyable to maintain interest and efficiency.\n\n**The Need for Documentation:**\n* Understanding why documentation is necessary\n* Identifying the intended audience\n\n**Documentation Practices:**\n* Just-in-time: Adding documentation only when really needed\n* Cheap upfront: Low cost, minimal effort documentation\n* Expensive upfront: Documentation for future use despite the cost\n* Sharing knowledge through conversations or working together instead of traditional documentation\n* Minimizing extra work now by finding existing sources instead of creating new ones\n	+ Explicit vs implicit knowledge\n	+ Abundance and accessibility of knowledge\n	+ Accessible in one place and complete documentation\n* Minimizing extra work later by keeping documentation accurate over time based on its stability\n	+ Stable knowledge: Traditional forms sufficient\n	+ Living knowledge: Automation and reduction of manual updates essential.",2065,64.35873985290527
False,3,"24-Documentation Reboot.pdf,24-Documentation Reboot,""Documentation Reboot 41\nDocumentation Reboot\nThis book could be titled Documentation 2.0: Livin","24-Documentation Reboot.pdf,24-Documentation Reboot,""Documentation Reboot 41\nDocumentation Reboot\nThis book could be titled Documentation 2.0: Living Documentation, Continuous \nDocumentation, or No Documentation. Even with the shorter title Living Docu-\nmentation, the key driver of this book is to reconsider the way we do documentation, starting from the purpose. From there, the universe of applicable solutions is nearly infinite. This book explores the universe of practices and techniques in various cate-gories, organized into almost 100 patterns. Table 1.1 provides a summary of this language of patterns. \nTable 1-1 Patterns Summary\nPattern Brief Description\nRethinking Documentation\nMost knowledge is already \nthereThere is no need to record a piece of knowledge that is already recorded in the system itself.\nPrefer internal documentation The best place to store documentation is on the \ndocumented thing itself.\nFocus on specific knowledge Use documentation for the specific knowledge and \nlearn the generic knowledge from trainings.\nAccuracy mechanism Y ou can trust documentation only if there is a \nmechanism to guarantee its accuracy . \nFun activity T o make an activity sustainable, make it fun.\nKnowledge Exploitation\nSingle-source publishing Keep the knowledge in one single source of truth \nand publish from there when needed.\nReconciliation mechanism If knowledge is repeated in more than one place, \nset up a reconciliation mechanism to detect \ninconsistencies immediately .\nConsolidation of dispersed factsDiverse facts put together become useful knowledge.\nT ools history Y our tools record knowledge about your system.\nReady-made documentation Most of what you do is already documented in the \nliterature.\nChapter 1 Rethinking Documentation 42\nPattern Brief Description\nKnowledge Augmentation\nAugmented code When the code doesn’t tell the full story , add the \nmissing knowledge to make it complete.\nDocumentation by annotation Extend your programming language by using \nannotations for documentation purposes.\nDocumentation by convention Rely on code conventions to document knowledge.Module-wide knowledge \naugmentationKnowledge that spans a number of artifacts that have something in common is best factored out in one place.\nIntrinsic knowledge augmentationOnly annotate elements with knowledge that is intrinsic to them.\nEmbedded learning Putting more knowledge into the code helps its \nmaintainers learn while working on it.\nSidecar files When putting annotations within the code is not \npossible, put them into a file next to the code.\nMetadata database When putting annotations within the code is not \npossible, keep them in an outside database.\nMachine-accessible documentationDocumentation that is machine accessible opens new opportunities for tools to help at the design level.\nRecord your rationale The rationale behind a decision is one of the most \nimportant things to augment the code with.\nAcknowledge your influences The major influences of a team are the keys for \nunderstanding the system they’ve built.\nCommit messages as comprehensive documentationCarefully written commit messages make each line of code well documented.\nKnowledge Curation\nDynamic curation Even if all the works of art are already there in the \ncollection, there is still work to be done to make an \nexhibition out of it.\nHighlighted core\n11 Some elements of the domain are more important than others.\nInspiring exemplars The best documentation on how to write code is often just the best code that is already there.\n11.  Evans, Eric. Domain-Driven Design: T ackling Complexity in the Heart of  Software . Hoboken: \nAddison-Wesley Professional, 2003.\n43 Documentation Reboot\nPattern Brief Description\nGuided tour, sightseeing map12It is easier to quickly discover the best of a new \nplace with a guided tour or a sightseeing map.\nAutomating Documentation\nLiving document A document that evolves at the same pace as the \nsystem it describes.\nLiving glossary A glossary that evolves at the same pace as the \nsystem it describes, reflecting the domain language \nused in the code. \nLiving diagram A diagram that can be generated again on any \nchange so that it’s always up-to-date.\nOne diagram/one story One diagram should tell only one specific message.\nRuntime Documentation\nVisible test Tests can produce visual output for human review \nin domain-specific notation.\nVisible workings13Working software can be its own documentation \nat runtime.\nIntrospectable workings Y our code in memory can be a source of knowledge.\nRefactorable Documentation\nCode as documentation Most of the time, the code is its own \ndocumentation.\nIntegrated documentation Y our IDE already fulfills many documentation \nneeds.\nPlain-text diagrams Diagrams that cannot be genuine living diagrams \nshould be created from plain-text documents to \nmake their maintenance easier.\nStable Documentation\nEvergreen content Evergreen content is content that remains useful \nwithout change for a long time.\nPerennial naming Favor naming schemes that last longer than others.\nLinked knowledge Knowledge is more valuable when it is connected, \nprovided that the connections are stable.\nLink registry An indirection can be changed to fix broken links \nin a single place.\n12.  Brown, Simon. Software Architecture for Developers, V ol 2: Visualize, document, and explore your \nsoftware architecture. https://leanpub.com/visualising-software-architecture\n13.  Brian Marick, “Visible Workings”: https://web.archive.org/web/20110202132102/http://\nvisibleworkings.com/\nChapter 1 Rethinking Documentation 44\nPattern Brief Description\nBookmarked search A search made into a link is more stable than a \ndirect link.\nBroken link checker Detecting broken links as soon as possible helps \nkeep the documentation trusted.\nInvest in stable knowledge Stable knowledge is an investment that pays back \nover a longer period of time.\nHow to Avoid Traditional Documentation\nWorking collectively as \ncontinuous knowledge sharingWorking collectively is an opportunity for continuous knowledge sharing.\nCoffee machine communicationNot all exchange of knowledge has to be planned and managed. Spontaneous discussions in a relaxed environment often work better and must be encouraged.\nIdeas sedimentation It takes some time to find out whether a piece of \nknowledge was important or not.\nThrowaway documentation Some documentation is useful for only a limited \nperiod of time before it can be deleted.\nOn-demand documentation Document what you’ve seen is necessary to be \ndocumented.\nAstonishment report Newcomers’ superpower is bringing a fresh \nperspective.\nInteractive documentation Documentation can try to emulate the interactivity \nof a conversation.\nDeclarative automation Every time you automate a software task, you \nshould take the opportunity to make it a form of documentation as well.\nEnforced guidelines The best documentation does not even have to be \nread if it can alert you at the right time with the right piece of knowledge.\nConstrained behavior Influence or constrain the behavior instead of \ndocumenting.\nReplaceability first Designing for replaceability reduces the need to \nknow how things work.\nConsistency first Being consistent reduces the need for \ndocumentation.\n45 Documentation Reboot\nPattern Brief Description\nBeyond Documentation: Living Design\nListen to the documentation Documentation can be a signal to spot \nopportunities for improvements.\nShameful documentation The presence of a free comment is often a signal of \na shameful behavior in the code.\nDeliberate decision making The path to better design and better \ndocumentation starts by making more decisions \ndeliberately .\nHygienic transparency T ransparency leads to greater hygiene because the \ndirt cannot hide.\nWord cloud A word cloud of the identifiers in the code should \nreveal what the code is about.\nSignature survey14 Looking at the code at some level of detail can reveal its shape.\nDocumentation driven Start by explaining your goal or end result, such as \nhow the system will be used.\nAbusing living documentation (anti-pattern)Don’t be dogmatic about living documentation but focus on delivering value for your users.\nLiving documentation procrastinationHave fun in your living documentation tools to avoid having too much fun in your production code.\nBiodegradable documentation The goal of documentation should be to make \nitself redundant.\nDesign skills everywhere Learn and practice good design; it’s equally good \nfor your code and for your documentation.\nLiving Architecture\nDocument the problem It’s almost useless to document a solution without \nexplaining the problem it attempts to solve.\nStake-driven architecture Is your biggest challenge on the domain \nunderstanding a quality attribute or socio-\ntechnical aspects?\nExplicit quality attributes Friends don’t let friends guess the quality attributes \nfor which a system was designed.\n14.  Ward Cunningham, “Signature Survey: A Method for Browsing Unfamiliar Code”: https://c2.com/\ndoc/SignatureSurvey/\nChapter 1 Rethinking Documentation 46\nPattern Brief Description\nArchitecture landscape Organize multiple documentation mechanisms \ninto a consistent whole for easier navigation.\nDecision log Keep the major decisions in a decision log.\nFractal architecture \ndocumentationY our system is made of smaller systems; organize your documentation accordingly .\nArchitecture codex Documenting the way you make decisions enables \ndecentralized decision making.\nT ransparent architecture Architecture is for everyone, as long as they have \naccess to the information.\nArchitectural reality check Making sure the implementation of the \narchitecture matches its intent.\nTest-driven architecture The ultimate living architecture is test driven.\nSmall-scale simulation as \ndocumentationDocument a large system with a smaller version of itself.\nSystem metaphor\n15A concrete analogy shared by everyone—customers, programmers, and managers—can help you understand how the system works.\nIntroducing Living Documentation\nUndercover experiments Start with safe-to-fail experiments without much \npublicity .\nMarginal documentation New pr actices can usually only be applied to new work.\nCompliance in spirit A living documentation approach can comply with \neven the most demanding compliance requirements \nby aiming for the spirit instead of for the letter.\nDocumenting Legacy Applications\nFossilized knowledge Legacy systems should not be considered blindly \nas reliable documentation.\nBubble context\n16Create an isolated space where you can work \nwithout the constraint of the legacy system.\nSuperimposed structure Relate the desirable structure to the existing, less \ndesirable one.\n15.  Evans, Eric. Domain-Driven Design: T ackling Complexity in the Heart of  Software . Hoboken: \nAddison-Wesley Professional, 2003. \nalso Beck, Kent. Extreme Programming Explained. Hoboken: Addison-Wesley Professional, 2000.\n16.  Eric Evans, “Getting Started with DDD when Surrounded by Legacy Systems”: http://domainlanguage.com/\nwp-content/uploads/2016/04/GettingStartedWithDDDWhenSurroundedByLegacySystemsV1.pdf"",11242",11302,"**Rethinking Documentation: Patterns for Effective Knowledge Sharing**\n\n**Introduction:**\n- Book title options: Documentation 2.0, Living Documentation, Continuous Documentation, No Documentation\n- Purpose of the book: Explore ways to reconsider documentation practices\n- Overview of patterns in various categories (Rethinking Documentation and Knowledge Exploitation)\n\n**Table 1-1 Patterns Summary:**\n| Category   | Pattern                          | Description                                                                     |\n| ---        | ---                             | ---                                                                         |\n| Rethinking Documentation | Most knowledge is already there | No need to record existing information               |\n|              | Prefer internal documentation    | Store documentation on the documented thing itself       |\n|              | Focus on specific knowledge      | Use documentation for specific knowledge and learn generic knowledge from trainings |\n|              | Accuracy mechanism             | Ensure trust in documentation through a guarantee of accuracy |\n|              | Fun activity                     | Make activities fun to encourage participation           |\n| Knowledge Exploitation | Single-source publishing        | Keep all knowledge in one single source                |\n|              | Reconciliation mechanism         | Set up a mechanism to detect inconsistencies             |\n|              | Consolidation of dispersed facts | Combine diverse facts into valuable knowledge            |\n|              | Tools history                   | Utilize existing tools for recording knowledge about systems |\n|              | Ready-made documentation        | Use literature and readymade resources for reference      |\n\n**Chapter 1: Rethinking Documentation**\n- Patterns in the Knowledge Augmentation category: Augmented code, Documentation by annotation, Documentation by convention, Embedded learning, Sidecar files, Metadata database, Machine-accessible documentation, Record your rationale, Acknowledge your influences, Commit messages as comprehensive documentation.\n- Patterns in the Knowledge Curation category: Dynamic curation, Highlighted core, Inspiring exemplars.\n- Patterns in the Guided Tour and Sightseeing Map category: Automating Documentation - Living document, Living glossary, Living diagram; Runtime Documentation - Visible test, Visible workings, Visible workings; Refactorable Documentation - Code as documentation, Integrated documentation, Plain-text diagrams.\n\n**Pattern Brief Descriptions:**\n- Augmented code: Add missing knowledge to make the code complete\n- Documentation by annotation: Extend your programming language with annotations\n- Documentation by convention: Rely on code conventions for documentation\n- Embedded learning: Putting more knowledge into the code helps maintainers learn\n- Sidecar files: Store annotations next to the code when annotations cannot be embedded\n- Metadata database: Keep annotations in an outside database instead of code\n- Machine-accessible documentation: Opens new opportunities for tools to help at design level\n- Record your rationale: Important to augment the code with the decision-making process\n- Acknowledge your influences: Understanding system building blocks is key to understanding the system\n- Commit messages as comprehensive documentation: Carefully written commit messages make code well documented.",3481,92.06573462486267
False,3,"25-Living Documentation The Very Short Version.pdf,25-Living Documentation The Very Short Version,,0","25-Living Documentation The Very Short Version.pdf,25-Living Documentation The Very Short Version,,0",100,**Living Documentation: The Very Short Version**,48,3.2483842372894287
False,3,"26-Approaches to Better Documentation.pdf,26-Approaches to Better Documentation,""47 Documentation Reboot\nPattern Brief Description\nHighlighted struc","26-Approaches to Better Documentation.pdf,26-Approaches to Better Documentation,""47 Documentation Reboot\nPattern Brief Description\nHighlighted structure Make a superimposed structure visible in relation \nto the existing source code.\nExternal annotations Sometimes you don’t want to touch a fragile \nsystem just to add some knowledge to it.\nBiodegradable transformation Documentation of a temporary process should \ndisappear when that process is done.\nAgree on maxims17Big changes to legacy systems are made by a \nnumber of people who share common objectives; use maxims to share the vision.\nEnforced legacy rule Legacy transformations can last longer than the \npeople doing them; automate the enforcement of the big decisions to protect them.\nLiving Documentation: The Very Short Version\nIf you only want to spend a minute on what living documentation is all about, please remember the following big ideas:\n •Favor conversations and working together over every kind of document. Most knowledge is already there and just needs to break free.\n •Most of the knowledge is there already . Y ou just need to augment it with the missing context, intent, and rationale.\n •Pay attention to the frequency of change.\n •Thinking about documentation is a way to draw attention to the quality or lack thereof in a system.\nIf this list is clear enough, you’ve understood the key message of this chapter.\nApproaches to Better Documentation\nThere are many ways to consider the topic of documentation. These approaches cover a full spectrum that can be seen as a cycle that follows a progression from avoiding documentation to documentation to the max and then beyond, to \n17.  Demeyer, Serge, Stéphane Ducasse, Oscar Nierstrasz. Object Oriented Reengineering Patterns. \nSan Francisco: Morgan Kaufmann Publishers, Inc., 2002.\nChapter 1 Rethinking Documentation 48\nquestioning the need for documentation again and looping the cycle to less docu-\nmentation again. Y ou could also see this cycle as going from lightweight approaches to more heavyweight ones.\nThis cycle involves the rate of change (volatility) of the knowledge in question, \nfrom stable knowledge to knowledge that changes continuously .\nThe following describes the following categories of approaches to documenta-\ntion, which are discussed throughout this book:\n •Avoiding documentation: The best documentation is often no documenta-tion because the knowledge is not worth any particular effort beyond doing the work. Collaboration with conversation or collective work is key here. Some-times you can do even better and improve the underlying situation rather than work around it with documentation. Examples include automation and fixing the root issues.\n •Stable documentation: Not all knowledge changes all the time. When it’s stable enough, documentation becomes much simpler and much more useful at the same time. Sometimes it just takes one step to go from changing a piece of knowledge to a more stable one—and this is the type of opportunity you want to exploit.\n •Refactorable documentation: Code, tests, plain text, and so on present par-ticular opportunities to evolve continuously in sync, thanks to the refactor-ing capabilities of modern IDEs and tools. Refactorable documentation makes possible accurate documentation for little to no cost.\n •Automating documentation: Automating documentation is the geekiest area, involving using specific tools to produce documentation automatically in a liv-ing fashion, following the changes in the software construction. A particular flavor of automating documentation involves every approach that operates at runtime, when the software is running; this is in contrast to other approaches that work at build time.\n •Beyond documentation: Finally , we reach the beyond documentation area, \nwhere we have the opportunity to question everything and recognize that the topic of documentation can have benefits well beyond just transferring and storing knowledge. This is where we reach enlightenment and reconsider every other approach and technique in a more critical way . This aspect of living doc-umentation is more abstract, but important. Through living documentation practices that stimulate your attention to your work, the quality of the work can be improved as a side effect.\nThese categories structure the main chapters of this book—but in reverse order to \nfollow a progression from more technical and rather easy-to-grasp to more abstract"",4404",4491,"**Approaches to Better Documentation**\n\n**Documentation Approaches**:\n- Avoiding documentation: The best documentation is often no documentation because the knowledge is not worth any particular effort. Collaboration, conversation, or collective work are key here. Examples include automation and fixing the root issues.\n- Stable documentation: Not all knowledge changes all the time. When it's stable enough, documentation becomes much simpler and more useful at the same time.\n- Refactorable documentation: Code, tests, plain text present particular opportunities to evolve continuously in sync, thanks to the refactoring capabilities of modern IDEs and tools.\n- Automating documentation: Automating documentation is the geekiest area, involving using specific tools to produce documentation automatically in a living fashion, following the changes in the software construction.\n- Beyond documentation: This is where we question everything and recognize that the topic of documentation can have benefits well beyond just transferring and storing knowledge. This is where we reach enlightenment and reconsider every other approach and technique in a more critical way.\n\n**Documentation Categories**:\n- Avoiding documentation\n- Stable documentation\n- Refactorable documentation\n- Automating documentation\n- Beyond documentation\n\n**Progression of Documentation Approaches**:\n- The cycle involves the rate of change (volatility) of the knowledge in question, from stable knowledge to knowledge that changes continuously.\n- The approaches progress from lightweight to more heavyweight, or from avoiding documentation to more documentation and beyond.",1647,34.35842752456665
False,3,"27-A Gateway to DDD.pdf,27-A Gateway to DDD,,0","27-A Gateway to DDD.pdf,27-A Gateway to DDD,,0",46,"**Gateways to Deep Demon Dialogue (DDD)**\n\n**Background:** No context given\n\n**Keywords:** Gateway, Deep Demon Dialogue (DDD)\n\n**Phrase Meaning:** Unknown without additional context\n\n**Expansion:** This passage suggests that there may be a connection between the term ""Gateway"" and ""Deep Demon Dialogue (DDD)"" but provides no further information.",346,11.435898303985596
False,3,"28-When Living Documentation Is an Application of DDD.pdf,28-When Living Documentation Is an Application of DDD,""A Gateway to DDD 49\nand people-orien","28-When Living Documentation Is an Application of DDD.pdf,28-When Living Documentation Is an Application of DDD,""A Gateway to DDD 49\nand people-oriented considerations. This ordering means the chapters progress from \nthe less important to the more important.\nAcross these categories of approaches, this book discusses some core principles \nthat guide you in how to do documentation efficiently .\nA Gateway to DDD\nY ou can get closer to domain-driven design by investing in living documentation.\nLiving documentation can help guide a team or a set of teams in their adoptions \nof DDD practices. It helps make these practices more concrete and focuses some attention on the resulting artifacts. Of course, the way you work with the DDD mindset is much more important than the resulting artifacts. Still, the artifacts can at least help visualize what DDD is about, and they can help make visible any problem-atic practice and provide guidance on how well it’s done (or not).\nDomain-Driven Design in a Nutshell\nDomain-driven design is an approach to tackling complexity in the heart of software development. It primarily advocates a sharp focus on the particular business domain being considered. It promotes writing code that expresses the domain knowledge lit-erally , with no translation between the domain analysis and the executable code. As such, it calls for modeling directly in code written in a programming language , in \ncontrast with a lot of literature on modeling. This is possible only if there is the pos-sibility of frequent and close conversations with domain experts, with everyone using the same ubiquitous language—the language of the business domain.\nDomain-driven design calls for focusing the efforts on the core domain, the one \nbusiness area with the potential to make a difference against the competition. As such, DDD encourages developers not just to deliver code but to contribute as part-ners with the business in a constructive two-way relationship where the developers grow a deep understanding of the business and gain insights into the important stakes.\nDomain-driven design is deeply rooted in Kent Beck’s Extreme Programming \nExplained: Embrace Change. It is also built on top of the pattern literature, most notably Martin Fowler’s Analysis Patterns: Reusable Object Models and Rebecca Wirfs-Brock’s Object Design: Roles, Responsibilities, and Collaborations, the book \nthat began the practice of “xDD” naming.\nEric Evans’s Domain-Driven Design: T ackling Complexity in the Heart of  Soft-\nware  also includes numerous patterns for applying DDD successfully . One of the most \nimportant concepts is the notion of bounded context. A bounded context  defines \nChapter 1 Rethinking Documentation 50\nan area of a system where the language can be kept precise and without ambiguity . \nBounded contexts are a major contribution to system design; they simplify and parti-tion large, complicated systems into several smaller and simpler subsystems— without much downside. Splitting systems and work between teams efficiently is quite hard, and the notion of bounded contexts is a powerful design tool to help with this.\nBecause Evans’s Domain-Driven Design book was published in 2003, most \nof its  examples were proposed for application in object-oriented programming languages, but it has become clear since then that DDD applies just as well with functional programming languages. I often make the claim that DDD advocates a functional programming style of code even in object-oriented programming languages.\nLiving Documentation and Domain-Driven Design\nThis book focuses on several aspects of DDD:\n •It promotes the use of DDD in a project, in particular through the chosen examples.\n •It shows how documentation can support the adoption of DDD and how it can act as a feedback mechanism to improve your practice.\n •It is in itself an application of DDD on the subject of documentation and knowledge management, in the way this topic is approached.\n •In particular, many of the practices of living documentation are actually directly DDD patterns from Eric Evans’s book.\n •The point of writing this book is to actually draw attention to design, or lack of thereof, through documentation practices that make it visible when the team is doing a poor job of design.\nDo these factors make this book a book on DDD? I think so. As a fan of DDD, \nI would definitely love this to be the case.\nLiving documentation is all about making each decision explicit, with not only \nthe consequences in code but also the rationale, the context, and the associated busi-ness stakes expressed (or perhaps modeled) using all the expressiveness of the code as a documentation medium.\nA project is interesting if it addresses a problem for which there is no standard \nsolution. The project must discover how to solve the problem through continuous learning and a lot of knowledge crunching while exploring the domain. As a con-sequence, the resulting code will change all the time, from small changes to major breakthroughs."",4957",5076,"**Living Documentation and Domain-Driven Design (DDD)**\n\n**Background:**\n- Investing in living documentation as gateway to DDD\n- Helps make concrete the practices and focus attention on artifacts\n- Discusses core principles for efficient documentation\n\n**Domain-Driven Design (DDD):**\n- Approach to handling complexity in software development\n- Sharp focus on business domain\n- Advocates modeling directly in code using programming language\n- Close conversations with domain experts and use of same ubiquitous language\n- Encourages developers to contribute as partners, deepening understanding of business\n- Rooted in Extreme Programming (XP) and pattern literature\n\n**DDD Patterns:**\n- Numerous patterns for applying DDD successfully\n- Notion of bounded context: defines precise, unambiguous language and simplifies system design\n\n**Living Documentation and DDD:**\n- Focuses on several aspects of DDD in book\n  - Promotes use of DDD through chosen examples\n  - Shows how documentation supports adoption and improves practice\n  - Applies DDD principles to documentation and knowledge management\n  - Many practices of living documentation are directly DDD patterns\n  - Draws attention to design issues through documentation practices.\n\n**Living Documentation:**\n- Explicit decisions, consequences in code, rationale, context, business stakes\n- Discovering solutions to non-standard problems through continuous learning and knowledge crunching\n- Code changes all the time from small to major ones.",1495,36.86741232872009
False,3,"29-A Story of Mutual Roots Between BDD DDD XP and Living Documentation.pdf,29-A Story of Mutual Roots Between BDD DDD XP and Living Documentation,""A G","29-A Story of Mutual Roots Between BDD DDD XP and Living Documentation.pdf,29-A Story of Mutual Roots Between BDD DDD XP and Living Documentation,""A Gateway to DDD 51\n“T ry , T ry Again” requires change-friendly documentation. However, at all times it \nis important to keep the precious knowledge that took so much effort to learn. Once \nthe knowledge is there, you can turn it into valuable and deliverable software by writ-ing and refactoring source code and other technical artifacts. But you need to find ways to keep the knowledge through this process.\nDDD advocates “modeling with code” as the fundamental solution. The idea \nis that code itself is a representation of the knowledge. Only when the code is not enough do you need something else. Tactical patterns leverage the idea that code is the primary medium, and they guide developers in how to use it as such in practice, with their ordinary programming language.\nTherefore: Your investment in learning living documentation is also an invest-\nment in learning some aspects of domain-driven design. Learn one, and you learn half of the other for free!\nWhen Living Documentation Is an Application of DDD\nLiving documentation not only supports DDD but is also in itself an example of applying the DDD approach on the domain of managing knowledge throughout its lifecycle. And in many cases, living documentation is a directly applied case of DDD under a slightly different name.\nA Story of Mutual Roots Between BDD, DDD, XP, and Living \nDocumentation\nThe term living documentation was introduced by Gojko Adzic in Specification by \nExample, which is a book on behavior-driven development (BDD). BDD is an approach involving collaboration between everyone involved in software develop-ment that was proposed by Dan North, who introduced the idea by combining test-driven development (TDD) with the ubiquitous language of domain-driven design. As a consequence, even the term living documentation already has roots in domain-driven design!\nConsider that living documentation strongly adheres to the following tenets of DDD:\n •Code as the model: Code is the model (and vice versa), so you want to have as much knowledge of the model in the code as possible—and this is, by defini-tion, the documentation.\n •Tactical techniques to make the code express all the knowledge: Y ou want to exploit programming languages to the maximum of what they can express, to express even knowledge that is not executed at runtime.\nChapter 1 Rethinking Documentation 52\n •Evolving the knowledge all the time with the DDD whirlpool: Knowledge \ncrunching is primarily a matter of collaboration between business domain experts and the development team. Through this process, some of the most important knowledge becomes embodied into the code—and perhaps into some other artifacts. Because all the knowledge evolves or may evolve at any time, any documented knowledge must embrace change without impediments such as maintenance costs.\n •Making clear what’s important and what’s not: In other words, there needs to be a focus on curation. “Focus on the core domain” and “highlight the core concepts” are from Evans’s DDD book, but there’s much more you can do with curation to help keep the knowledge under control despite the limited human memory and cognition capabilities.\n •Attention to detail: Many DDD patterns emphasize that attention to detail is important. Decisions should be deliberate and not arbitrary , and they should be guided by concrete feedback. The living documentation approach has to encourage attention to detail by making it easier to document what’s deliber-ate and by giving insightful feedback throughout the process.\n •Strategic design and large-scale structures: DDD offers techniques to deal with evolving knowledge at the strategic level and on a large scale, providing opportunities for smarter documentation, too.\nIt is difficult to mention all the correspondences between the ideas of living docu-\nmentation and domain-driven design without rewriting parts of other books. But some examples are necessary to make the point (see Table 1.2). \nTable 1.2 Correspondence Between Living Documentation and DDD \nLiving Documentation \nPatternDDD Pattern (from Evans’s book or from later contributions) Notes\nReady-made knowledge; acknowledge bibliographyDraw on established formalisms, when you can; read the book; apply analysis patternsClearly declare all the ready-made knowledge used with references to the sources.\nEvergreen document Domain vision statement Higher-level knowledge is \na great example of stable knowledge that can be written in an evergreen document.\nA Gateway to DDD 53\nLiving documentation goes beyond traditional documentation and its limitations. \nIt elaborates on the DDD techniques and advice for knowledge about the business \ndomain and also for knowledge about the design, as well as the infrastructure and delivery process, which are technical domains, too, with respect to the project stake-holders. The ideas from domain-driven design are essential to guiding developers in how to invest in knowledge in a tactical and strategic way , dealing with change in the short term and in the long term as well. Therefore, as you are going the living docu-mentation route, you are learning domain-driven design, too.Living Documentation PatternDDD Pattern (from Evans’s book or from later contributions) Notes\nCode as documentation Model-driven design; \nintention-revealing interfaces; declarative design; the building blocks of a model-driven design (to enable expressive code)DDD is about modeling in plain code, with the purpose of having all the domain knowledge embodied in the code and its test.\nLiving glossary Ubiquitous language When the code literally \nfollows the ubiquitous language, it becomes the single reference for the glossary of the domain.\nListen to the documentationHands-on modelers Modeling in code with \nliving documentation extracted from it gives fast feedback on the quality of the design, in a hands-on fashion.\nChange-friendly documentationRefactoring toward deeper insight; try , try again“Embracing change” is a constant theme with XP , DDD, and living documentation\nCuration Highlighted core; flagged \ncore; segregated core; abstract coreSegregating what is particularly important from the rest is a key driver in DDD; the goal is to best allocate effort and cognitive attention."",6289",6442,"**Living Documentation and Domain-Driven Design (DDD)**\n\n**Overview:**\n- Living documentation is a concept introduced by Gojko Adzic in Specification by Example\n- It has roots in both Behavioral Driven Development (BDD) and Domain-driven Design (DDD)\n- Living documentation supports DDD principles and is an application of the DDD approach.\n\n**DDD Tenets:**\n- **Code as the model**: Code is a representation of knowledge, making it valuable for documentation\n- **Tactical techniques to make code express all the knowledge**\n- **Evolving the knowledge all the time with the DDD whirlpool**\n- **Making clear what’s important and what’s not (curation)**\n- **Attention to detail**\n- **Strategic design and large-scale structures**\n\n**Living Documentation Patterns:**\n- **Ready-made knowledge; acknowledge bibliography**: Identify established formalisms, read books, apply analysis patterns\n- **Evergreen document**: Document higher-level knowledge that remains stable\n- **Code as documentation**: Model-driven design with intention revealing interfaces and declarative design\n- **Listening to the documentation**: Hands-on modelers for fast feedback on design quality\n- **Change-friendly documentation**: Embrace change through refactoring towards deeper insight\n- **Curating knowledge**: Segregate core concepts, flagged and abstracted core.\n\n**DDD Patterns:**\n- **Model-driven design; intention-revealing interfaces; declarative design**: Embody domain knowledge in code\n- **Ubiquitous language**: Follow the ubiquitous language to create clear documentation\n- **Listen to the documentation (hands-on modelers)**: Gain fast feedback on design quality.\n\n**Living Documentation vs. DDD:**\n- Living documentation goes beyond traditional documentation, elaborating on DDD techniques for business and technical domains.\n- It supports DDD principles by dealing with change in both the short term and long term.",1902,49.65865659713745
False,3,"30-Chapter 2 Behavior-Driven Development as an Example of Living Specifications.pdf,30-Chapter 2 Behavior-Driven Development as an Example of Living S","30-Chapter 2 Behavior-Driven Development as an Example of Living Specifications.pdf,30-Chapter 2 Behavior-Driven Development as an Example of Living Specifications,""Chapter 1 Rethinking Documentation 54\nSummary\nThroughout this chapter, you have seen that documentation often suffers because of \nunchallenged traditional habits. This is good news, in a way , because it means there are many opportunities to deconstruct the topic in order to reconstruct it again from first principles, in the light of the fast-paced and change-friendly projects we deal with today .\nLiving documentation is all about paying attention to the knowledge involved in \nsoftware making. Some knowledge is more important than other knowledge, and the most important knowledge is almost surely already somewhere in a project’s arti-facts. The goal, and the fun, of living documentation is to recognize the valuable knowledge, where it already is, and determine what may be missing and how often it changes in order to best benefit from it with minimal expense. In other words, it is about designing a system of knowledge within your code base itself, and it requires design skills, just like coding!"",1014",1185,"**Behavior-Driven Development as an Example of Living Specifications**\n\n**Rethinking Documentation**:\n- Documentation often suffers due to traditional habits\n- Opportunity to deconstruct and reconstruct from first principles\n\n**Living Documentation**:\n- All about paying attention to the knowledge involved in software making\n- Some knowledge is more important than others\n- Goal is to recognize valuable knowledge, determine what's missing, and how often it changes\n- Requires design skills like coding\n- Designed system of knowledge within the code base itself",562,12.852927684783936
False,3,"31-BDD with Automation Is All About Living Documentation.pdf,31-BDD with Automation Is All About Living Documentation,""55Chapter 2\nBehavior-Driven De","31-BDD with Automation Is All About Living Documentation.pdf,31-BDD with Automation Is All About Living Documentation,""55Chapter 2\nBehavior-Driven Development \nas an Example of Living Specifications\nWhat about documenting the business behavior? (Because as you know, \nbusiness people never change their mind.)\nBehavior-driven development (BDD) was the first example of living documentation. In the book Specification by Example, Gojko Adzic explains that many teams doing BDD say that one of the biggest benefits is that they benefit from a living documen-tation that explains what the application is doing, and that can be trusted because it is always up-to-date.\nThe following sections quickly look at what BDD is and what it is not, in relation \nwith living documentation.\nBDD Is All About Conversations\nIf you think BDD is about testing, forget what you think you know about it. BDD is about sharing knowledge efficiently . This means that you can do BDD without any tools. Before anything else, BDD promotes deep conversations between the three (or more) amigos shown in Figure 2.1. BDD also relies on the use of concrete  scenarios—which must use the language of the business domain—to detect misunderstandings and ambiguities early ."",1130",1255,"**Behavior-Driven Development (BDD) as Living Documentation**\n* **BDD**: First example of living documentation in software development\n* Explained in ""Specification by Example"" book by Gojko Adzic\n* Benefit: Living documentation of application behavior\n* Up-to-date and trusted by all team members\n* Not about testing, but sharing knowledge efficiently\n* Conversations between stakeholders for effective communication\n* Can be done without any tools\n* Deep conversations between amigos (team members or stakeholders)\n* Use of concrete scenarios in business domain language\n  - Detect misunderstandings and ambiguities early.\n\n**BDD vs Living Documentation: Focus on Business Behavior**\n* BDD is focused on documenting the behavior of an application from a business perspective\n* Living documentation refers to the practice of keeping updated documentation related to software development projects\n* Both serve different purposes but can be interconnected in the context of Agile methodologies.\n\n**BDD as a Communication Tool:**\n* Encourages effective communication among team members and stakeholders\n* Facilitates understanding of application behavior from a business perspective\n* Helps identify misunderstandings or ambiguities early on in development process.\n\n**BDD vs Testing:**\n* BDD is not solely about testing, but rather focusing on conversations and documenting business requirements\n* It does involve automated tests as part of the process to ensure the desired behavior is met\n* The primary goal is better communication between developers, testers, and stakeholders.",1579,34.02506375312805
False,3,"32-Redundancy and Reconciliation.pdf,32-Redundancy and Reconciliation,""56 Chapter 2 Behavior-Driven Development as an Example of \n Living Specificati","32-Redundancy and Reconciliation.pdf,32-Redundancy and Reconciliation,""56 Chapter 2 Behavior-Driven Development as an Example of \n Living Specifications\nBDD with Automation Is All About Living \nDocumentation\nBDD with just conversations provides a lot of value. However, going to the addi-\ntional effort of setting up automation brings even more benefits. When using a tool like Cucumber, BDD still involves the use of a domain language between every stake-holder involved and, in particular, between the three amigos, a focus on the higher-level purpose, and frequent use of concrete examples, also known as scenarios. These scenarios then become tests in the tool, and they become living documentation at the same time.\nRedundancy and Reconciliation\nBDD scenarios describe the behavior of an application, but the source code of the application also describes this behavior: The scenarios and source code are redun-dant with each other, as shown in Figure 2.2. \nOn one hand, this redundancy is good news: Scenarios expressed in pure domain \nlanguage, if done properly , are accessible to nontechnical audiences such as busi-ness people who could never read code. However, this redundancy is also a problem: If some scenarios or parts of the code evolve independently , then you have two prob-lems: Y ou must determine whether to trust the scenarios or the code, and (a big-ger problem), you must in some way know that the scenarios and the code are not in sync.BIZ DEV TESTER\nFigure 2.1 The three amigos\nBDD with Automation Is All About Living Documentation 57\nThis is where a reconciliation mechanism is needed. In the case of BDD, you \ncan use tests and tools like Cucumber or SpecFlow . These tools act like a Roberval \nbalance between both redundant pieces of knowledge, as pictured in Figure 2.3. \nThese tools parse the scenarios in plain text and use some glue code provided by \nthe developers to drive the actual code. The amounts, dates, and other values in the “Given” and “When” sections of the scenarios are extracted and passed as param-eters when calling the actual code. The values extracted from the “Then” sections of the scenarios, on the other hand, are used for the assertions, to check what’s expected in the scenario, based on the result from the code matches.Redundancy!\nWhat if one changes and not the other?\nFigure 2.2 Scenarios and code both describe the same behavior\nCucumber\nspecflow\nPRAGMATIC BDD FOR .NET\nFigure 2.3 T ools check regularly that the scenarios and the code describe the same behavior"",2478",2555,"**Behavior-Driven Development (BDD) as Living Specifications:**\n* BDD uses a domain language for communication between stakeholders\n* Involves use of tools like Cucumber to create living documentation\n* Scenarios become tests and living documentation simultaneously\n\n**Redundancy and Reconciliation:**\n* Behavior described in scenarios and code is redundant\n* Accessible to non-technical audience through domain language\n* Problem arises when they evolve independently\n* Reconciliation mechanism needed: Cucumber or SpecFlow\n\n**Role of Tools:**\n* Parse scenarios in plain text\n* Glue code provided by developers for driving actual code\n* Extract values from ""Given"" and ""When"" sections as parameters\n* Use values from ""Then"" sections for assertions to check results.\n\n**Cucumber:**\n* Tool used for BDD with living documentation\n* Checks regularly that scenarios and code describe same behavior.",894,22.39351797103882
False,3,"33-The Anatomy of Scenarios in a File.pdf,33-The Anatomy of Scenarios in a File,,0","33-The Anatomy of Scenarios in a File.pdf,33-The Anatomy of Scenarios in a File,,0",82,"**Anatomy of Scenarios: Understanding ""The Anatomy of Scenarios in a File""**\n\n**Key Points:**\n- The term ""scenarios"" refers to specific situations or events\n- In ""The Anatomy of Scenarios in a File,"" we explore this concept further\n\n**Exploring the Concept of Scenarios**\n- This phrase is commonly used, but its anatomy or composition can be examined more deeply\n- Understanding the structure and elements that make up scenarios is valuable\n\n**Location:**\n- ""The Anatomy of Scenarios in a File"" is a term often found in various texts, presentations, or discussions related to planning, forecasting, and risk management.\n\n**Exploring Further:**\n- For a comprehensive analysis of the anatomy of scenarios, consider delving into relevant literature or resources on this topic.",773,17.95114231109619
False,3,"34-The Intent of a Feature File.pdf,34-The Intent of a Feature File,,0","34-The Intent of a Feature File.pdf,34-The Intent of a Feature File,,0",70,"**The Purpose of a Feature File**\n\n**Feature Files**: Understanding their intent\n\n**Key Points**:\n- Provide concise bulleted notes summarizing the provided text, with headings and terms in bold.\n\n**The Purpose of a Feature File**\n1. **Understanding the Intent of a Feature File**\n2. - Repeat the provided passage, with **Concision**.\n3. **Provide concise bulleted notes summarizing the provided text**, with headings and terms in bold.\n4. The Purpose of a Feature File:\n   - Discussing its significance and role in software development or testing\n5. Understanding its intent:\n   - Learning how it functions and benefits the project\n6. **Repeat the provided passage, with Concision**:\n   - Paraphrase the text to convey the same meaning using different words and phrasing.\n7. **Provide concise bulleted notes summarizing the provided text**, with headings and terms in bold:\n**The Purpose of a Feature File**:\n- Discussing its significance and role in software development or testing\n- Understanding its intent:\n  - Learning how it functions and benefits the project\n- **Repeat the provided passage, with Concision**:\n  - Paraphrase the text to convey the same meaning using different words and phrasing.\n\n**Note:** For additional information on feature files and their usage in Behavior Driven Development (BDD) testing, check out this article: [What is a Feature File in BDD?](https://www.guru99.com/feature-file-bdd.phtml)",1424,39.3506383895874
False,3,"35-A Feature File Example.pdf,35-A Feature File Example,""58 Chapter 2 Behavior-Driven Development as an Example of \n Living Specifications\nThe tools","35-A Feature File Example.pdf,35-A Feature File Example,""58 Chapter 2 Behavior-Driven Development as an Example of \n Living Specifications\nThe tools turn scenarios into automated tests. The nice thing is that these tests \nalso provide a way to detect when the scenarios and the code are no longer in sync. \nThis is an example of a reconciliation mechanism, a means to ensure that redundant sets of information always match.\nThe Anatomy of Scenarios in a File\nWhen using a tool like Cucumber or SpecFlow to automate scenarios into tests, you create files called feature files. These files are plain-text files that are stored in the source control, just like code. Usually they are stored near the tests or as Maven test resources. This means they are versioned like the code, and they are easy to diff.\nLet’s take a closer look at a feature file.\nThe Intent of a Feature File\nA feature file must start with a narrative that describes the intent of all the scenarios in the file. It usually follows the pattern “In order to…. As a…. I want.…” Starting with “In order to” helps you focus on the most important thing: the value you’re looking for.\nHere’s an example of a narrative for an application about detection of potential \nfrauds in the context of fleet management for parcel delivery:\n1  Feature: Fuel Card Transactions anomalies\n2  In order to detect potential fuel card abnormal behavior by drivers3  As a fleet manager4  I want to automatically detect anomalies in all fuel card transactions\nNote that the tools just consider the narrative as text; they don’t do anything with \nit except include it in the reports because they acknowledge that it’s important.\nFeature File Scenarios\nThe rest of the feature file usually lists all the scenarios that are relevant for the cor-\nresponding feature. Each scenario has a title, and scenarios almost always follow the “Given…. When…. Then…” pattern.\nThe Anatomy of Scenarios in a File 59\nHere’s an example of one of the many concrete scenarios for an application on \ndetection of potential frauds, in the context of fleet management for parcel delivery:\n1  Scenario: Fuel transaction with more fuel than the vehicle \ntank can hold2  Given that the tank size of the vehicle 23 is 48L3  When a transaction is reported for 52L on the fuel card 4  associated with vehicle 235  Then an anomaly """"The fuel transaction of 52L exceeds the tank size of 48L"""" is reported\nWithin one feature file there are typically between 3 and 15 scenarios, describing \nthe happy path, its variants, and the most important situations.\nThere are a number of other ways to describe scenarios, such as by using the out-\nline format, and there are also ways to factor out common assumptions between \nscenarios with background scenarios. (For more information on outline format and background scenarios, see for example https://docs.cucumber.io/gherkin/reference/.)\nSpecification Details\nThere are many cases in which scenarios alone are enough to describe the expected behavior, but in some rich business domains, such as accounting or finance, scenar-ios are definitely not enough. In such cases, you also need abstract rules and formulas.\nRather than putting all this additional knowledge in a Word document or in a \nwiki, you can directly embed it within the related feature file, between the intent and the list of scenarios. Here’s an example, from the same feature file as before:\n1  Feature: Fuel Card Transactions anomalies\n2  In order to detect potential fuel card abnormal behavior by drivers3  As a fleet manager4  I want to automatically detect anomalies in all fuel card transactions 56  Description:7  The monitoring detects the following anomalies:8  * Fuel leakage: whenever capacity > 1 + tolerance,9  where capacity = transaction fuel quantity / vehicle tank size\n60 Chapter 2 Behavior-Driven Development as an Example of \n Living Specifications\n10 * Transaction too far from the vehicle: whenever distance to \nvehicle > threshold,11 where distance to vehicle = geo-distance (vehicle coordinates, gas 12 station coordinates),13 and where the vehicle coordinates are provided by the GPS14 Tracking by (vehicle, timestamp),15 and where the gas station coordinates are provided by16 geocoding its post address. 1718 Scenario: Fuel transaction with no anomaly19 When a transaction is reported on the fuel card20 .../// more scenarios here\nThese specification details are just comments as free text though; the tools com-\npletely ignore it. However, the point of putting it there is to have co-located with the \ncorresponding scenarios. Whenever you change the scenarios or the details, you are more likely to update the specification details because they are so close. As we say , “out of sight, out of mind.” But there is no guarantee to do so.\nTags in Feature Files\nThe last significant ingredient in feature files is the ability to add tags. Each scenario can have tags, like the following:\n1  @acceptance-criteria   @specs   @wip   @fixedincome @interests \n2  Scenario: Bi-annual compound interests over one year3    Given a principal of USD 1000 4    ...//\nTags are documentation. Some tags describe project management knowledge, \nsuch as @wip, which stands for work in progress and signals that this scenario is \ncurrently being developed. Other similar tags may even name who’s involved in the development (for example, @bob, @team-red) or mention the sprint (for example, @sprint-23) or its goal (for example, @learn-about-reporting-needs). These tags are temporary and are deleted when the tasks are all done.\nSome tags describe how important the scenario is. For example, @acceptance-\ncriteria indicates that this scenario is part of the few user acceptance criteria. Other similar tags may help with curation of scenarios, such as @happy-path, @nominal, @variant, @negative, @exception, and @core.\nThe Anatomy of Scenarios in a File 61\nFinally , some tags describe categories and concepts from the business domain. For \nexample, in the example just shown, the tags @fixedincome and @interests describe \nthat this scenario is relevant to the fixed income and interest financial areas.\nTags should be documented, too. For example, a collocated text file can list all \nthe valid tags, along with a text description for each. And to make sure that every tag used in the feature files is documented there, my colleague Arnauld Loyer likes to add a separate unit test as yet another reconciliation mechanism.\nOrganizing Feature Files\nWhen the number of feature files grows, it’s necessary to organize them into folders. The organization you use is also a way to convey knowledge; the folders can tell a story .\nWhen the business domain is the most important thing, I recommend organizing \nthe folders by functional areas to show the overall business picture. For example, you might have the following folders:\n •Accounting\n •Reporting Rules\n •Discounts\n •Special Offers\nIf you have any additional content as text and pictures, you can also include it in \nthe same folders, so that it stays as close as possible to the corresponding scenarios.\nIn the book Specification by Example, Gojko Adzic lists three ways to organize \nstories into folders:\n •By functional areas\n •Along UI navigation routes (when documenting user interfaces)\n •Along business processes (when end-to-end use case traceability is required)\nWith this approach, the folders literally represent the chapters of your business \ndocumentation (as you can see in the example later in this chapter).\nScenarios as Interactive Living Documentation\nThe scenarios form the basis for living documentation. Even better, this documenta-tion is typically interactive, as a generated interactive website. For example, if you \n62 Chapter 2 Behavior-Driven Development as an Example of \n Living Specifications\nuse Pickles for SpecFlow , a specific one-page website is generated during each build \n(see Figure 2.4). The website shows a navigation pane that is organized by chapter, provided that your folders represent functional chapters. It displays all scenarios, together with the test results and their statistics. This is quite powerful—much more so than any paper documentation you’ve ever seen.  \nA built-in search engine in Pickles allows instant access to any scenario by key-\nword or by tag. This is the second powerful effect of tags: They make searching more efficient and accurate.\nScenarios in Boring Paper Documents\nAn interactive website like the one shown in the preceding section is convenient for a team, providing fast access to the business behavior knowledge. However, in some cases, such as when there are mandatory compliance requirements, you must provide a boring paper document (a “BPD,” as some call it).\nThere are tools for creating these documents. One of them, which was developed \nby my Arolla colleague Arnauld Loyer (@aloyer), is called Tzatziki\n1, because it’s a \nCucumber sauce. It exports a beautiful PDF document out of the feature files. It goes a bit further, also including Markdown files and images that are stored alongside the feature files into the document. It therefore helps create nice explanations at the beginning of each functional area chapter.\n1. Tzatziki, https://github.com/Arnauld/tzatziki\nFigure 2.4 Generated interactive documentation website, with Pickles\nA Feature File Example 63\n Note \nIf the tool you need in your context is missing, you should create it on top of or as \na derivation of existing tools. The sky’s the limit. Custom tools or extensions of tools can be created as fun projects, on hackathon days, or during slack time; they needn’t be made by vendors or other people.\n \nBDD is a great example of living documentation: It’s not additional work to be \ndone but is part of doing work properly . It’s always in sync, thanks to the tools that act as reconciliation mechanisms. And if the feature files in the source code are not enough, the generated website illustrates how documentation can be useful, interac-tive, searchable, and well organized.\nA Feature File Example\nThis section provides a full example of a fictitious yet realistic feature file in the busi-ness domain of finance. T o keep it short, this example contains only one outline sce-nario, along with a corresponding data table. It illustrates another style of using Cucumber, SpecFlow , and equivalent tools. The scenario is evaluated for each line of the table. Here is the complete feature file example:\n1  Feature: Calculate compound interests on a principal\n2  In order to manage the company money3  As a finance officer4  I want to calculate the compound interests on a principal on my account56  Description:7  Compound interest is when the bank pays interest on both the   principal (the original amount of money) and the interest an   account has already earned.8  9  To calculate compound interest use the formula below.1011 In the formula, A represents the final amount in the account    after t years compounded 'n' times at interest rate 'r' with   starting amount 'p'.1213\n64 Chapter 2 Behavior-Driven Development as an Example of \n Living Specifications\n14\n15   A = P*(1+(r/n))^n*t161718 Scenario: Bi-annual compound interests over one year19 Given a principal of USD 100020 And interests are compounded bi-annually at a rate of 5%21 When the calculation period lasts exactly 1 year22 Then the amount of money in the account is USD 1053.63 2324 Scenario: Quarterly compound interests over one year25 //... outline scenario \n2627 Examples:2829 | convention | rate | time | amount      | remarks            |\n30 |-------------------------------------------------------  |\n31 | LINEAR      | 0.05 | 2      | 0.100000  | (1+rt)-1          |\n32 | COMPOUND   | 0.05 | 2      | 0.102500  | (1+r)^t-1        |\n33 | DISCOUNT   | 0.05 | 2      | -0.100000 | (1 - rt)-1       |\n34 | CONTINUOUS   | 0.05 | 2      | 0.105171  | (e^rt)-1 (rare)   |\n35 | NONE        | 0.05 | 2       | 0            | 0                    |\n36 |-------------------------------------------------------  |\nWith the support of tools, all business scenarios become automated tests and \nliving documentation at the same time. The scenarios are just plain text in the \nfeature files. T o bridge the gap between the text in the scenarios and the actual production code, you create a little set of steps. Each step is triggered on a par-ticular text  sentence, matched by regular expressions, and calls the production code. The text sentence may have parameters that are parsed and used to call the production code in many different ways. Here is an example:\n1  For example:\n2  Given the VAT rate is 9.90%3  When I but a book at an ex-VAT price of EUR 254  Then I have to pay an inc-VAT price of EUR 2.49\nT o automate this scenario, you need to define what is called a step for each line \nused within scenarios. For example, you can define that any sentence like this:\n1  """"When I but a book at an ex-VAT price of EUR <exVATPrice>"""""",13001",13069,"**Behavior-Driven Development as an Example of Living Specifications**\n\n**Overview:**\n- Behavior Driven Development (BDD) uses feature files to document business behavior\n- Feature files are plain text files stored with source code, versioned, and easy to diff\n- Contains narrative describing intent, scenarios, tags, and specification details\n\n**Anatomy of Scenarios in a File:**\n- Starts with a narrative that describes the feature's intent\n- Followed by scenarios, each with title, ""Given..."", ""When..."", ""Then..."" pattern\n- May include additional knowledge as comments\n- Tags help organize and categorize scenarios\n- Interactive website generated from tool like Pickles for easy access to business behavior documentation\n\n**Example of a Feature File:**\n1. **Feature: Fuel Card Transactions anomalies**\n2. **In order to detect potential fuel card abnormalities by drivers**\n3. **As a fleet manager**\n4. **I want to automatically detect anomalies in all fuel card transactions**\n5. **Description:** Monitoring detects the following anomalies:\n   - Fuel leakage: capacity > 1 + tolerance\n   - Transaction too far from vehicle (distance to vehicle > threshold)\n6. **Scenario: Fuel transaction with no anomaly**\n7. **Given a principal of USD 1000**\n8. **When a transaction is reported on the fuel card**\n9. **Then an anomaly ""The fuel transaction of 52L exceeds the tank size of 48L"" is reported**\n10. **Tags: acceptance-criteria, specs, wip, fixedincome, interests**\n11. **Scenarios are evaluated for each line in a data table**\n12. **Automated tests created from feature files using tools like Pickles**\n13. **Business scenarios become living documentation**\n\n**Organizing Feature Files:**\n- Organize feature files into folders based on functional areas or business processes\n- Interactive website generated for easy access to business behavior documentation\n- Boring paper documents can be created using Tzatziki tool if needed.",1930,83.40673017501831
False,3,"36-Going Further Getting the Best of Your Living Documentation.pdf,36-Going Further Getting the Best of Your Living Documentation,""A Canonical Case of","36-Going Further Getting the Best of Your Living Documentation.pdf,36-Going Further Getting the Best of Your Living Documentation,""A Canonical Case of Living Documentation in Every Aspect 65\nWould trigger the following glue code:\n1  Book(number exVATPrice)\n2  Service  = LookupOrderService();3  Service.sendOrder(exVATPrice);\nIn this code snippet, the tool (Cucumber or SpecFlow) passes the variable exV AT-\nPrice to the glue code; the value of this variable is automatically extracted from the \nsentence in a scenario. For example, in the preceding scenario, the value of exV AT-Price would be 25.\nUsing this mechanism, the scenarios become automated tests that are driven by \nthe scenarios and the values they declare. If you change the rounding mode of the price in the scenario without changing the code, the test will fail. If you change the rounding mode of the price in the code without changing the scenario, the test will fail, too. This is a reconciliation mechanism to signal inconsistencies between the sides of the redundancy .\nA Canonical Case of Living Documentation in \nEvery Aspect\nBDD has shown that it is possible to have accurate documentation that is always in \nsync with the code by doing the specification work more carefully . BDD is a canoni-cal case of living documentation, and all the core principles of living documentation are already present in BDD:\n •Collaborative: The primary tool of BDD is talking among people, making sure that each role out of the three amigos (or more) is present. \n •Low-effort: The conversations around concrete examples are useful for agree-ing on what to build, and with some additional work they become automated tests and a living documentation: one activity , multiple benefits.\n •Reliable, thanks to a reconciliation mechanism: Because the business behav-iors are described both in text scenarios and in implementation code, tools like Cucumber and SpecFlow ensure that the scenarios and code remain always in sync (or at least show when they go out of sync). This is necessary whenever there is duplication of knowledge.\n66 Chapter 2 Behavior-Driven Development as an Example of \n Living Specifications\n •Insightful: The conversations provide feedback, as do writing and automating \nthe scenarios. For example, if a scenario is too long or awkward, it may suggest looking for the missing implicit concepts that would make the scenario shorter and simpler.\nIt also illustrates other ideas described later in this book:\n •Targeted audience: All this work is targeted for an audience that includes business people, hence the focus on clear, nontechnical language when discuss-ing business requirements.\n •Idea sedimentation: Conversations are often enough, and not everything needs to be written down. Only the most important scenarios, the key scenarios, need to be written for archiving or automation.\n •Plain-text documents: Plain text is very convenient for managing stuff that changes and for living alongside the source code in source control.\n •Accessible published snapshot: Not everyone has or wants access to the source control in order to read the scenarios. T ools like Pickles and Tzatziki offer a solution, exporting a snapshot of all the scenarios at a current point in time as an interactive website or as a PDF document that can be printed.\nNow that you’ve seen BDD as the canonical case of living documentation, you’re \nready to move on to other contexts where you can apply living documentation. Living documentation is not restricted to the description of business behaviors, as is BDD; it can help you in many other aspects of software development projects—and perhaps even outside software development.\nGoing Further: Getting the Best of Your Living \nDocumentation\nFeature files describing business scenarios are a great place to gather rich domain \nknowledge in an efficient way .\nMost tools that support teams in doing BDD understand the Gherkin syntax. \nThey expect feature files to follow a fixed format, as shown here:\n1  Feature: Name of the feature\n23  In order to... As a... I want...4\nGoing Further: Getting the Best of Y our Living Documentation 67\n5  Scenario: name of the first scenario\n6  Given...7  When...8  Then...910 Scenario: name of the second scenario 11 ...\nOver time, teams in rich domains like finance or insurance realized that they \nneeded more documentation than just the intent at the top and the concrete sce-\nnarios at the bottom. As a result, they started putting additional description of their business case in the middle area, and this is ignored by the tools. T ools like Pickles that generate living documentation out of the feature files adapted to this use and started to support Markdown formatting for what became called “the description area”:\n1  Feature: Investment Present Value\n23  In order to calculate the breakeven point of the investment opportunity4  As an investment manager5  I want to calculate the present value of future cash amounts678  Description9  ===========1011  We need to find the present value *PV* of the given future cash \ 12 amount *FV*. The formula for that can be expressed as:1314 - Using the negative exponent notation:1516         PV = FV * (1 + i)^(-n)1718 - Or in the equivalent form: 1920         PV = FV * (1 / (1 + i)^n)2122 Example23 -------2425     For example, n = 2, i = 8%"",5247",5384,"**Living Documentation: BDD as an Example**\n* **Behavior-Driven Development (BDD)** is a canonical case of living documentation\n* BDD involves:\n	+ Collaborative conversations between roles\n	+ Low-effort, with multiple benefits\n	+ Reliable reconciliation mechanism\n	+ Insightful feedback through scenarios and writing\n* Conversations around concrete examples help agree on what to build\n* Scenarios become automated tests and living documentation\n* BDD is targeted at an audience including business people\n* Idea sedimentation: Only key scenarios need be written down\n* Plain text documents are convenient for managing changing knowledge\n* Accessible published snapshot of scenarios as interactive website or PDF document\n\n**Living Documentation Beyond BDD:**\n* Feature files describe rich domain knowledge efficiently\n* Tools support Gherkin syntax and Markdown formatting in the ""description area""\n* Teams in finance and insurance need more documentation than just intent and concrete scenarios\n* Living documentation can be applied beyond BDD to other aspects of software development projects.",1095,28.62683629989624
False,3,"37-Property-Based Testing and BDD.pdf,37-Property-Based Testing and BDD,""68 Chapter 2 Behavior-Driven Development as an Example of \n Living Specifica","37-Property-Based Testing and BDD.pdf,37-Property-Based Testing and BDD,""68 Chapter 2 Behavior-Driven Development as an Example of \n Living Specifications\n26     PV?                          FV = $100\n27     |               |              |28     ---------------------------------------> t (years)29     0               1              2303233 Scenario: Present Value of a single cash amount34  Given a future cash amount of 100$ in 2 years35  And an interest rate of 8%36  When we calculate its present value37  Then its present value is $85.73\nThis documentation will be rendered in the living documentation website as a \npretty document called “Feature: Investment Present Value.”\nThis example illustrates how feature files provide the opportunity to gather a lot \nof documentation in the same place, directly within the source control. Note that \nthis descriptive area with text, formulas, and ASCII diagrams in the middle of the file is not really living; it’s is just co-located with the scenarios; if we change the scenarios, you’re likely to also need to update the description nearby , but there is no guarantee.\nThe best strategy would be to put knowledge that does not change very often in \nthe description section and to keep the volatile parts within the concrete scenarios. One way to do this is to clarify that the description uses sample numbers, not the numbers that are necessarily used for the configuration of the business process at any point in time.\nT ools like Pickle,\n2 Relish,3 and Tzatzikinow understand Markdown descriptions \nand even plain Markdown files located next to the feature files. This makes it easy to have an integrated and consistent approach for the domain documentation. And Tzatziki can export a PDF from all this knowledge, as expected by the regulators in finance.\nProperty-Based Testing and BDD\nRequirements often come naturally as properties (for example, “The sum of all amounts paid and received must be zero at all times” or “Nobody can ever be a law-yer and a judge at once”). When doing BDD or TDD, you must clarify these general properties into specific concrete examples, which will help with finding issues and building code incrementally .\n2. Pickle, http://www .picklesdoc.com\n3. Relish, http://www .relishapp.com\nGoing Further: Getting the Best of Y our Living Documentation 69\nIt’s a good idea to keep track of the general properties for their documentation \nvalue. Y ou usually do that as plain text comments in the feature file, as described ear-\nlier in this chapter. But it happens that the technique of property-based testing is pre-cisely about exercising these properties against randomly generated samples. This is performed with a property-based testing framework that runs the same test over and over, with inputs generated from generators of samples. The canonical framework is QuickCheck in Haskell, and there are now similar tools in most other programming languages.\nIntegrating property-based testing into your feature files eventually makes the \ngeneral properties executable, too. In practice, it’s a matter of adding special scenar-ios describing the general property and invoking the property-based testing frame-work underneath, as shown here:\n1  Scenario: The sum of all cash amounts exchanged must be zero \nfor derivatives2  3  Given any derivative financial instrument4  And a random date during its lifetime5  When we generate the related cash flows on this date for the payer and\6  the receiver7  Then the sum of the cash flows of the payer and the receiver is\8  exactly zero\nSuch scenarios typically use sentences like “given ANY shopping cart….” This \nwording is a code smell for regular scenarios, but it’s okay for property-oriented sce-\nnarios on top of property-based testing tools supplementing the regular concrete scenarios.\nCreating a Glossary\nAn ideal glossary is a living one, extracted directly from your code. However, in many cases it is not possible to create a living glossary , and you must manually create one. \nIt’s possible to create a glossary manually as a Markdown file and to co-locate it \nwith the other feature files. This way , it will be included in the living documentation website, too. Y ou could even do it as dummy empty feature file."",4213",4292,"**Behavior-Driven Development (BDD) and Property-Based Testing**\n\n**Background:**\n- Living specifications in feature files provide opportunity for documentation\n- Description section may contain co-located formulas, diagrams, text\n- Documentation rendered as ""Feature: Investment Present Value"" on living documentation website\n- Tools like Pickle, Relish, Tzatziki understand Markdown descriptions and integrated approach\n\n**BDD vs. Property-Based Testing:**\n- Requirements come naturally as properties (e.g., ""The sum of all amounts paid and received must be zero at all times"")\n- Clarify general properties into specific concrete examples for testing and code development\n- Property-based testing frames work with randomly generated samples to exercise these properties\n- Frameworks like QuickCheck in Haskell, similar tools in other languages\n- Add special scenarios describing the general property and invoke property-based testing framework\n\n**Glossary:**\n- Living glossary ideal but not always possible\n- Create manually as Markdown file co-located with feature files for inclusion in living documentation website.\n\n**Benefits of Property-Based Testing:**\n- Exercises properties against randomly generated samples\n- Identifies issues and builds code incrementally\n- Executable general properties when integrated into feature files using property-based testing frameworks.",1377,39.753577709198
False,3,"38-Chapter 3 Knowledge Exploitation.pdf,38-Chapter 3 Knowledge Exploitation,""70 Chapter 2 Behavior-Driven Development as an Example of \n Living Speci","38-Chapter 3 Knowledge Exploitation.pdf,38-Chapter 3 Knowledge Exploitation,""70 Chapter 2 Behavior-Driven Development as an Example of \n Living Specifications\nLinking to Nonfunctional Knowledge\nNot all the knowledge should be described in the same place. Y ou don’t want to mix \ndomain knowledge with UI-specific or legacy-specific knowledge, which is impor-tant and should be stored elsewhere. And when that language is related to the domain language, you should use links to represent the relationship and make it easy to find it.\nAs described elsewhere in this book, you can use different approaches to linking. \nY ou may link directly to a URL, as shown here, although you risk of having a broken link whenever it changes:\n1  https://en.wikipedia.org/wiki/Present_value\nY ou may go through a link registry that you maintain to manage links and to \nreplace broken links with working ones, as shown here:\n1  go/search?q=present+value\nY ou may also use bookmarked searches to link to places that include the related \ncontent, as shown here:\n1  https://en.wikipedia.org/w/index.php?search=present+value\nLinking to nonfunctional knowledge gives you a resilient way to link to related \ncontent, at the expense of letting the reader select the most relevant results each time.\nSummary\nBDD is the canonical example of living documentation. It primarily relies on fre-quent conversations between team members before anything else. It is a direct part of the necessary work to build software, and yet it preserves the knowledge collected during a project in a form that is accessible to both business people and developers. And even though it leads to redundant knowledge in the code and the scenarios, the accompanying tools make sure it all remains in sync. But BDD only deals with the business behaviors of the software. In subsequent chapters, we explore how to extrapolate these ideas for other activities related to software development."",1877",1960,**Knowledge Exploitation: Chapter 3**\n\n**Behavior-Driven Development (BDD)**\n- Canonical example of living documentation\n- Relies on frequent conversations between team members\n- Directly part of software building process\n- Preserves knowledge accessible to both business people and developers\n- Redundant knowledge in code and scenarios but kept in sync through accompanying tools\n\n**Living Specifications: Linking to Nonfunctional Knowledge**\n- Not all knowledge should be described in the same place\n- Domain knowledge vs. UI/legacy-specific knowledge: separate storage\n- Use links to represent relationships between related content\n  - Direct URL linking (risk of broken links)\n  - Link registry management\n  - Bookmarked searches for related content\n\n**Benefits of BDD Documentation**\n- Collects and preserves project knowledge\n- Accessible to both business people and developers\n- Preserves knowledge in a form that can be updated and refined over time.,959,29.530380487442017
False,3,"39-Where Is the Knowledge Now.pdf,39-Where Is the Knowledge Now,""71Chapter 3\nKnowledge Exploitation\nFor a given project or system, a lot of knowledg","39-Where Is the Knowledge Now.pdf,39-Where Is the Knowledge Now,""71Chapter 3\nKnowledge Exploitation\nFor a given project or system, a lot of knowledge already exists, and it’s everywhere: \nin the source code of the software, in the various configuration files, in the source code of the tests, in the behavior of the application at runtime, in various random files and as data within the various tools around, and in the brains of all the people involved.\nT raditional documentation attempts to gather knowledge into convenient doc-\numents, in paper form or online. These documents duplicate knowledge that was already present elsewhere. This is obviously a problem when the other document is the authority and can be trusted, but it evolves all the time.\nBecause knowledge already exists in many places, all you need to do is to set up \nmechanisms to extract the knowledge from where it’s located and bring it where it’s needed, when it’s needed. And because you don’t have much time for that, such mechanisms must be lightweight, reliable, and low effort.\nIdentifying Authoritative Knowledge\nIt’s important to learn to spot the authoritative sources of knowledge in your system. When knowledge is repeated in different places, you need to know where to find the knowledge that you can trust. When decisions change, where does the knowledge reflect the changes most accurately?\nTherefore: Identify all the places where authoritative knowledge is located. For \na given need, set up mechanisms such as automation to extract the knowledge and transform it into an adequate form. Make sure this mechanism remains  simple and does not become a distraction."",1597",1668,"**Knowledge Exploitation**\n- **Existing Knowledge**: Already present in various forms throughout a project or system:\n  - Source code\n  - Configuration files\n  - Test source code\n  - Runtime behavior\n  - Random files and tools\n  - Human brains\n- **Traditional Documentation**: Gathers knowledge into documents, duplicating what was already present\n  - Problems when the authoritative source is not the document itself\n- **Extracting Knowledge**:\n  - Set up mechanisms to gather knowledge from its various locations\n  - Transform it into an appropriate format for easy access\n  - Lightweight, reliable, and low-effort\n- **Identifying Authoritative Knowledge**:\n  - Determine the primary source of trustworthy information in a system\n  - When decisions change, find the most accurate reflection of that knowledge\n  - Establish mechanisms to extract this authoritative knowledge:\n    - Set up automation for automatic extraction and transformation\n    - Ensure the process remains simple and unobtrusive.",1001,27.9755802154541
False,3,"40-Some Examples of Producing a Published Document.pdf,40-Some Examples of Producing a Published Document,""Chapter 3 Knowledge Exploitation 72\nKnowle","40-Some Examples of Producing a Published Document.pdf,40-Some Examples of Producing a Published Document,""Chapter 3 Knowledge Exploitation 72\nKnowledge about how the software works is in the source code. In the ideal case, \nit’s easy to read, and there is no need for any other documentation. In a not-so-ideal \ncase, perhaps because the source code is naturally obfuscated, you just need to make this knowledge more accessible.\nWhere Is the Knowledge Now?\nImagine that a colleague or manager says to you, “Give me documentation on stuff X!” T o handle this request, you would first need to ask yourself or the team, “Where is this knowledge now?”\nThe answer is often obvious: The knowledge is in the code, in the functional tests, \nor in the document on project goals. Sometimes it’s less obvious: The knowledge is in people’s brains, whether they know it or not. It may even be between people, in which case you’ll need collective workshops to elucidate it. Some knowledge exists only dur-ing the evaluation of the working software, in the memory of the program at runtime.\nOnce you’ve found the location of the authoritative knowledge, how can you har-\nness that knowledge and make it living documentation?\nWhen the knowledge is there but in a form that is not accessible or not conveni-\nent for the target audience and for the desired purpose, it must be extracted from its  single source of  truth into a more accessible form. This process should be automated \nto publish a clearly versioned document, with a link to the latest version.\nSometimes the knowledge can’t be extracted. For example, perhaps business \nbehavior can’t simply be extracted as English business sentences from the code, so in this case, you write these sentences by hand as functional scenarios or tests. By doing so, you introduce redundancy to the knowledge, so you need a reconciliation \n mechanism to easily detect inconsistencies, as described in the previous chapter.\nWhen the knowledge is spread over many places, you need a way to do a consoli-\ndation of all the knowledge into one aggregated form. And when there is an excess of knowledge, a careful selection process—a curation process—is essential.\nSingle-Source Publishing\nIt’s important to keep the knowledge in one single source of truth and publish from there when needed. When the authoritative source of knowledge is source code in a programming language or a configuration file of a tool in a formal syntax, it’s often necessary to make this knowledge accessible to audiences that can’t read it. The stand-ard way to do this is to provide a document in a format everyone understands, like plain English in a PDF document, or as a Microsoft Office document, spreadsheet, or slide deck. However, if you directly create such a document and include all the relevant \nSingle-Source Publishing 73\nknowledge in a copy-and-paste fashion, you will have a difficult time when it changes. \nAnd on an active and healthy project, you should expect that it will change a lot.\nThe Pragmatic Programmer by Dave Hunt and David Thomas says that English \ncan be considered a programming language. They suggest: “Write documents as you would write code: honor the DR Y principle, use metadata, MVC, automatic genera-tion, and so on.” As an example of duplication, Hunt and Thomas mention that a database schema in a specification document is redundant with the database schema file in a formal language like SQL. One has to be produced out of the other. For example, a specification document could be produced by a tool that can convert the SQL or DDL file into plain text and diagram form.\nTherefore: Keep each piece of knowledge in exactly one place, where it’s author-\nitative. When it must be made available to audiences that can’t access it directly, publish a document out of that single source of knowledge. Don’t include the elements of knowledge into the document to be published by copying and past-ing but use automated mechanisms to create a published document straight from the single authoritative source of knowledge.\nFigure 3.1 illustrates how the authoritative knowledge that is already present can \nbe extracted by automated mechanisms in order to publish documents. \nSome Examples of Producing a Published Document\nThere are many tools available to produce documents out of source code and other technical artifacts. Here are some examples:\n •GitHub: GitHub takes the README.md file, which is a single source of knowledge about the goals of an overall project, and turns it into a web page that is rendered to be pretty .\nAutomated\nMechanismAuthor itative\nKnowledge\nPublished Document\nFigure 3.1 From authoritative knowledge to published documents\nChapter 3 Knowledge Exploitation 74\n •Javadoc: Javadoc extracts the structure and all the public or private API of the \ncode and publishes it to a website as reference documentation. Y ou can easily create a custom tool based on the standard Javadoc Doclet in order to gener-ate your own specific report, glossary , or diagram, as described in Chapter 6, “ Automating Documentation.”\n •Maven: Maven and some other tools have a built-in way of producing consist-ent documentation, usually as a website, by putting together a number of tool reports and rendered artifacts. For example, Maven collects test reports, static analysis tools reports, Javadoc output folders, and any Markdown documents and organizes it all into a standard website. Every Markdown document can be rendered in the process.\n •Leanpub: Leanpub, the publishing platform I used to write this book, is a canonical example of single sourcing with a publication mechanism: Every chapter is written as a separate Markdown file, images are kept outside, the code can be in its own source files, and even the table of contents is in its own file. In other words, the content is stored in the way it’s most convenient to work with. Whenever I ask for a preview , Leanpub’s publishing toolchain collates all files according to the table of contents and renders it through various tools for Markdown rendering, typesetting, and code highlighting in order to produce a good-quality book in multiple formats: PDF , MOBI, and ePUB. This is similar to how the manuscript of a novel in the publishing world can be published as a book, as a comics, and then later as a movie—all from the same initial manu-script (see Figure 3.2). Y ou could follow this basic pattern with any templating mechanism and a bit of custom code. For example, you could produce a PDF out of the resource file that lists every currency supported by the program.  \nManuscript\nFigure 3.2 One single source and possibly multiple documents"",6595",6708,"**Producing a Published Document**\n\n**Knowledge Exploitation:**\n- In an ideal case, knowledge is present in source code that's easy to read\n- When it's not, make the knowledge more accessible\n- Ask ""Where is the knowledge now?"" - Could be in code, functional tests, or people's brains\n- Once located, make it living documentation by extracting and publishing\n\n**Single Source Publishing:**\n- Keep authoritative knowledge in one place\n- Publish documents from single source using automated mechanisms\n- Avoid copying and pasting into published documents\n\n**Examples:**\n1. **GitHub**: Transforms README.md file into a webpage\n2. **Javadoc**: Extracts public/private API of code and publishes it as reference documentation\n3. **Maven**: Collects reports and artifacts to create consistent documentation (website)\n4. **Leanpub**: Single sources content for books, comics, or movies with custom tools\n\n**Automated Mechanism:**\n- Extract authoritative knowledge from single source\n- Publish documents using automated mechanisms\n- Prevent duplication of effort and inconsistencies.",1074,32.754082441329956
False,3,"41-A Published Snapshot with a Version Number.pdf,41-A Published Snapshot with a Version Number,,0","41-A Published Snapshot with a Version Number.pdf,41-A Published Snapshot with a Version Number,,0",98,"**Snapshot with Version Number**\n* Provide a published snapshot with an assigned version number: 41-A\n* Repeat: Snapshot with Version Number (41-A)\n\n**Note:** For more concise versions, consider saying ""Publish snapshot as Version 41-A"" or ""Version 41-A: Publish the snapshot.""",277,8.842773914337158
False,3,"42-Remarks.pdf,42-Remarks,,0","42-Remarks.pdf,42-Remarks,,0",28,"**Remarks**: Refer to ""42-Remarks"" document for further details.",64,4.163361072540283
False,3,"43-Running Consistency Tests.pdf,43-Running Consistency Tests,""Setting Up a Reconciliation Mechanism (aka Verification Mechanism) 75\nA Published Snap","43-Running Consistency Tests.pdf,43-Running Consistency Tests,""Setting Up a Reconciliation Mechanism (aka Verification Mechanism) 75\nA Published Snapshot with a Version Number\nAny document published from a single source of truth is a snapshot: It must there-\nfore be considered as strictly immutable and should never be edited. T o avoid the risk of having someone edit a published document, you should favor document formats that prevent editing—or at least that make editing difficult. For example, prefer PDF over Microsoft Office documents, which are very easy to change. What-ever the format, consider using the locking flags to prevent edits. It’s not about making it impossible for hackers to edit; rather, the idea is to make it just hard enough to make edits that it is easiest to change the authoritative source and have it published again.\nAny published document must clearly identify its version and should also \ninclude a link to the location where the latest version can be found.\nIf you produce a lot of paper documents to be printed, you may consider  putting on \neach of them a barcode with the link to the folder that always contains the latest version. This way , even a printed document can easily direct readers to the latest version.\nRemarks\nY ou should write by hand only what cannot be extracted from an already existing project artifact, and you should store such remarks in a file that has its own lifecycle. Ideally , this file will change much less frequently than the knowledge extracted from other places. On the other hand, if some information is missing from a document you need to publish, you should by all means try to add it to the artifact it is most related to, perhaps using annotations, tags, or naming convention, or make it a new collaborative artifact on its own.\nSetting Up a Reconciliation Mechanism (aka Verification \nMechanism)\nWhenever knowledge is repeated in more than one place, you should set up a recon-\nciliation mechanism to detect inconsistencies immediately . Duplication of knowl-edge about software is a bad thing because it necessitates recurring work to update all the places that are redundant to each other, and it also means there is a risk of getting into an inconsistent state when an update is forgotten.\nHowever, if you must have redundancy , you can relieve the pain by using a verifica-\ntion mechanism, such as an automated test that checks that two copies are always in \nChapter 3 Knowledge Exploitation 76\nsync. This does not remove the cost of making changes in more than one place, but at \nleast it ensures that you won’t forget a change somewhere.\nOne reconciliation mechanism everybody is familiar with is checking the bill in \na restaurant (see Figure 3.3). Y ou know what you ate (and the evidence may be still visible in the number of dishes), and you check each line on the bill to ensure that there’s no discrepancy . \nTherefore: When you want or have to accommodate a redundancy in the knowl-\nedge stored at various places, make sure all the redundant knowledge is kept consistent using a reconciliation mechanism. Use automation to make sure eve-rything remains in sync and that any discrepancy is detected immediately, and you get an alert prompting to fix it.\nRunning Consistency Tests\nAs mentioned in Chapter 2, “Behavior-Driven Development as an Example of Living Specifications,” with BDD, scenarios provide documentation of the behavior. When-ever a scenario and code disagree, you know it immediately because the test automa-tion fails, much like a Roberval balance (see Figure 3.4). \nThis mechanism is made possible thanks to tools that parse the scenario in natu-\nral domain language to drive their implementation code. The code is driven through a little layer of glue code that you write specifically for that purpose, usually called “step definitions.” These steps are adapters between the parsed scenario and the actual code being driven.\nImagine testing the following scenario:\n •Given party BARNABA is marked as bankrupt\n •And trade 42 is against BARNABA,Drinks...4\nSoup.....2Dishes...4\nTotal: $225\nReconciliation?=Error!\nFigure 3.3 Checking the restaurant bill is a reconciliation mechanism\nSetting Up a Reconciliation Mechanism (aka Verification Mechanism) 77\n •When the risk alerting calculation is run,\n •Then an alert occurs: T rade against the bankrupt party BARNABA is triggered.\nThe tool parses these lines of text and recognizes the sentence “Given party \nBARNABA is marked as bankrupt” as one it has a step definition for:\n1 Given(""""^party (.*) is marked as bankrupt$"""")\n2 public void partyMarkedAsBankrupt(string party){\n3  bankruptParties.put(party); 4 }\nThe tool does the same for each line. Typically sentences starting with When \ntrigger actual computation, and sentences starting with Then cue the tool to check \nassertions:\n1 Then(""""^an alert: (/*) is triggered$"""")2 public void anAlertIsTriggered(string expectedMessage){\n3  assertEquals(expectedMessage, actualMessage);4 }\nFor all of this to work, the sentences need to actually drive the code with param-\neters (the regular expression [ /*] in the middle of the sentence), and the assertions \nmust check against the expectations from the sentences as precisely as possible.\nOK\nKO\nClear Status\nAutomated\nMechanism\nFigure 3.4 Automated mechanism to verify that redundant knowledge is in sync"",5345",5418,"**Setting Up a Reconciliation Mechanism (aka Verification Mechanism)**\n\n**Document Formats**:\n- Prefer PDF over Microsoft Office documents due to ease of editing\n- Use ""locking flags"" to prevent edits\n- Any published document must clearly identify its version and include a link to the latest version\n- Consider adding a barcode on each printed document that links to the latest version\n\n**Storing Remarks**:\n- Write by hand only what cannot be extracted from existing project artifacts\n- Store remarks in a separate file with its own lifecycle\n- Add missing information to the related artifact using annotations, tags, or naming conventions\n\n**Reconciliation Mechanism**:\n- Whenever knowledge is repeated, set up a reconciliation mechanism to detect inconsistencies\n- Duplication of knowledge necessitates recurring work and risk of inconsistent state\n- Use automated testing to check if redundant knowledge remains in sync\n- Tools can parse scenarios in natural domain language to drive code implementation\n- Step definitions are adapters between parsed scenario and actual code\n\n**Running Consistency Tests**:\n- BDD scenarios provide documentation of behavior\n- Scenario and code disagreement results in test automation failure, like a RÃ´beval balance\n- Tools parse sentences and recognize ""Given"", ""When"", and ""Then"" keywords to drive code",1344,34.96065020561218
False,3,"44-Ready-Made Documentation.pdf,44-Ready-Made Documentation,""Chapter 3 Knowledge Exploitation 78\nAs a counter example, it would not make sense to cod","44-Ready-Made Documentation.pdf,44-Ready-Made Documentation,""Chapter 3 Knowledge Exploitation 78\nAs a counter example, it would not make sense to code the step without extract-\ning a parameter from the sentence, or you would run the risk of inconsistency after \na few changes:\n1 Then(""""^an alert: Trade against the bankrupt party BARNABA is \ntriggered$"""" )3 public void anAlertIsTriggered(){\n4  assertEquals(""""Trade against the bankrupt party ENRON"""",actualMessage); 5 }\nThe scenario would unfortunately still pass even when the hardcoded message \ndoes not fit the one in the scenario, and nobody would notice.\nReconciliation on the Test Assumptions\nUsually you use the Given (or the equivalent Arrange phase in plain xUnit code) to \ncreate mock objects or to inject data into the test database.\nWhen testing legacy systems, you often have to deal with a couple types of \nproblems:\n •It’s too hard to mock the database, so you must test in an end-to-end fashion.\n •Y ou can’t re-create or populate a database just for your tests, so you must work on a real shared database that can change anytime if someone else is working on it.\nDespite these problems, it’s still possible to use exactly the same declaration of an \nassumption as a When sentence or an Arrange phase in xUnit but with an implemen-tation that checks that the assumption still holds true instead of injecting the value into a mock object:\n1 Given(""""^party (.*) is marked as bankrupt$"""")\n2 public void partyMarkedAsBankrupt(string party){\n3  assertTrue(bankruptParties.isBankruptParty(party)); // calls the DB4 }\nThis not an assertion of the test; it’s just a prerequisite for the scenario (or test) \nto even have a chance to pass. If this assumption already fails, then the scenario \n“does not even fail.” I often call this kind of “tests before the tests” a canary test. \nSetting Up a Reconciliation Mechanism (aka Verification Mechanism) 79\nSuch tests tell that something’s wrong even outside the test focus, helping you know \nthat you don’t have to waste time investigating in the wrong place.\nPublished Contracts\nA reconciliation mechanism that I first saw used by my Arolla colleague Arnauld Loyer is used to reconcile contracts against third parties such as external services that call your services. If your services exposes a resource with the parameter  \nCreditDefaultType , which has the two possible values FAILURE_TO_PAY  and \nRESTRUCTURING , you can’t rename them as you wish once published. Y ou can there-\nfore use tests with a deliberate redundancy to enforce that these elements of the con-tract don’t change. Y ou can refactor and rename as you wish, but whenever you break the contract, the reconciliation tests will alert you with a test failure.\nThis is an example of enforced documentation. Ideally you would make the test \nthe reference documentation for the contract, in a readable form; some tools in the API sphere enable you to do this. Here you definitely don’t want to update the test through automated refactoring; rather, you want it out of reach of the refactoring so that it stays unchanged to represent the external consumer services.\nThe most naive implementation for this approach would be something like that \nthe following, assuming that the internal representation of the  \nCreditDefaultType  \nis a Java enum named CREDIT_DEFAULT_TYPE :\n1 @Test\n2 public void enforceContract_CreditDefaultType3  final String[] contract = {""""FAILURE_TO_PAY"""", """"RESTRUCTURING""""}; 45  for(String type : contract){6   assertEquals(type, CREDIT_DEFAULT_TYPE.valueOf(type).toString());7  }8 }\nBecause you want to make sure that the contract for the external calling code is \nrespected, you define this contract again as an array of strings, as if it’s being used \nfrom the outside. And because you want to check that the contract is being honed with incoming and outcoming values, you make sure the contractual string is recog-nized as an input with \nvalueOf()  and that it’s the one being sent as an output with \ntoString() .\nChapter 3 Knowledge Exploitation 80\n Note \nThis example is only meant to explain the idea of this type of reconciliation \nmechanism. In the real world, it’s bad practice to use a loop inside a test as the test reporting will not tell precisely in which loop the problem resides if there is an exception. Instead, you would use a parameterized test, making the collection of values that are part of the contract the source of parameters.\n \nWith this approach, when someone who has recently joined the team decides to \nrename a constant of the enum, the test immediately fails to signal that it’s not pos-sible to do that—in effect acting like defensive documentation. It’s a defense against misconduct, and at the same time it provides an opportunity for the violator to learn on the spot: When the test fails, they learn that this enum constant is part of a  contract that should not be changed.\nConsolidating Dispersed Facts\nDiverse facts put together become useful knowledge. Sometimes knowledge is spread over many places. For example, a type hierarchy with an interface and five imple-menting classes may actually be declared in six different files. The content of a pack-age or module can actually be stored in many files. The full list of dependencies of the project may actually be defined partially in its Maven manifest (POM) and also in its parent manifest. Therefore, there is a need to collect and aggregate many little bits of knowledge to get a full picture.\nFor example, the big picture of a system is the union of the black-box view of each \nof its part, as pictured in Figure 3.5. The overall knowledge here is derived through a consolidation mechanism. \nFigure 3.5 From fragmented authoritative knowledge to unified knowledge\nConsolidating Dispersed Facts 81\nEven if the knowledge is split into many little parts, it’s still desirable to consider \nall those little bits as the authoritative single sources of truth. The derived consoli-\ndated knowledge is therefore a special case of published document extracted from many places.\nTherefore: Design a simple mechanism to automatically consolidate all the dis-\npersed facts. This mechanism must be run as often as necessary to ensure that the information about the whole is up-to-date with respect to the parts. Avoid any storage of the consolidated information, unless there are technical concerns like caching.\nHow Consolidation Works\nBasically , consolidation is like a SQL GROUP BY : Y ou take many things that have \nsome properties in common, and you find a way to turn all those multiple things into a single thing. In practice, it’s done by scanning every element within a given  perimeter, while growing the result, as shown in Figure 3.6. \nFor example, to reconstitute the full picture of a class hierarchy within the lim-\nits of one project from its individual elements, it’s necessary to scan every class and every interface of the project. The scanning process keeps a growing diction-ary of every hierarchy under construction so far in the process (for example, with the mapping top of hierarchy > list of subclasses). Every time the scan encoun-\nters a class that extends another class or that extends an interface, it adds it to the dictionary .\nAutomated\nMechanism\nFragmented\nAuthor itative\nKnowledgePublished Document on\nConsolidated Kno wledge\nFigure 3.6 From dispersed facts to useful knowledge\nChapter 3 Knowledge Exploitation 82\nWhen the scan is done, the dictionary contains a list of all type hierarchies in the \nproject. Of course, it’s possible to reduce the process to only a subset of the hierar-\nchies of interest for a particular documentation need, such as restricting the scan to classes and interfaces that belong to a published API.\nAs another example, you can create a black-box living diagram of a system made \nof smaller components, each of which has its own set of inputs and outputs, as shown in Figure 3.7. \nA simple consolidation can just collect the union of every input and output from \neach component. A more sophisticated consolidation can try to remove every input and output that match each other internally . It’s up to you to decide how you want consolidation to happen for a particular need.\nConsolidation Implementation Considerations\nAs usual, if possible, you should reuse a tool you already have that can do a desired consolidation. Some parsers for Java code can provide type hierarchies, for example. If what you want is not already in the tool, you can add it, such as by writing another visitor\n1 on the programming language Abstract Syntax T ree (AST)2. Some more \npowerful tools even provide their own language to query the code base very  efficiently . Y ou might want to load the AST into a graph database if you have to do very com-plex queries, but if you do that, I’m afraid you’re becoming a software vendor of documentation tools.\nIf the derived knowledge is kept in a cache for performance issues, make sure it \ndoes not become a source of truth and that it can always be properly dropped and then rebuilt from scratch from all the sources of truth.\n1. “The Visitor Pattern” from the book Design Patterns: Elements of  Reusable Object-Oriented Software, \nby Erich Gamma, John Vlissides, Ralph Johnson, and Richard Helm. Addison-Wesley .\n2. A common tree structure used by parsers to represent and manipulate the structure of the source code.=>\nFigure 3.7 The black-box view of  the whole system can be derived through a consolidation \nof  the black-box view of  its components\nReady-Made Documentation 83\nFor most systems, it is possible to scan all parts in sequence in a batch-processing \nfashion. This is typically done during a build, and it produces a consolidation that is \nready for publication on the project website or as a report.\nFor a large system such as an information system, it is not practical to run calcula-\ntions that scan all parts in sequence. In such a case, the consolidation process may be done incrementally . For example, the build of each part might contribute a par-tial update by pushing data to an overall consolidation state somewhere in a shared place, such as a shared database. This consolidation state is derived information; it is less trusted than the information from each build. If anything goes wrong, you should drop it and let it grow again from the contributions of each build.\nReady-Made Documentation\nMost of what you do is already documented in the literature. Not all knowledge is specific to your context; a lot of knowledge is generic and shared with many other people in many other companies in the industry . Think about all the knowledge on programming languages, developer tools, and software patterns and practices; most of that is industry standard.\nMore and more of what we do every day gets codified by talented practitioners \ninto patterns, techniques, and practices. And all that knowledge is properly docu-mented in books, blog posts, and conference talks and workshops around the world. This is ready-made documentation that’s readily available, for free or for the price of a book or workshop. Here are a few examples: \n •T est-Driven Development, Kent Beck\n •Design Patterns: Elements of  Reusable Object-Oriented Software , Erich \nGamma, John Vlissides, Ralph Johnson, and Richard Helm \n •Patterns of  Enterprise Application Architecture, Martin Fowler \n •Domain-Driven Design, Eric Evans \n •Everything on the C2 wiki \n •Every book from Jerry Weinberg \n •Continuous Delivery, Jez Humber and Dave Farley \n •All the clean code literature \n •Git workflow strategies\nIt is probably safe to say that if you can think about it, somebody has already \nwritten about it. Patterns, standard names, and standard practices exist, even if you \nChapter 3 Knowledge Exploitation 84\ndon’t know them yet. The literature is growing, and it’s already so huge that you can-\nnot know it all, or you would spend so much time reading you would not have any time left to create software.\n Note\nIn the book Apprenticeship Patterns, Dave Hoover and Adewale Oshineye advo-cate studying the classics. For example, they suggest reading the oldest books in your reading list first.\n \nKnowledge about mature business industry sectors is also generic knowledge. \nEven in very competitive areas like pricing in finance or supply chain optimization in e-commerce, most of the knowledge is public and available in industry-standard books, and only a small part of the business knowledge is specific and confidential—and only for a while at that.\nFor example, each business domain has its own essential reading list, and it \nmight have a book that is often referred to as “The Bible” of that field (for example, Options, Futures, and Other Derivatives by John C Hull, Logistics and Supply Chain Management by Martin Christopher).\nThe good news is that generic knowledge is already documented in the industry \nliterature. There are books, blog posts, and conference talks that describe it quite well. There are standard vocabularies to talk about it. There are trainings available to learn it faster from knowledgeable people.\nGeneric knowledge in itself is a solved problem. This knowledge is ready-made, \nready to be reused by everyone. If you have applied this generic knowledge in your system, you just have to link to the right literature and you’re done documenting.\nTherefore: Consider that most knowledge is already documented somewhere in \nthe industry literature. Do your homework and look for the standard source of knowledge on the web or by asking other knowledgeable people. Don’t try to document again something that’s already been well written by someone else; link to it instead. And don’t try to be original; instead, adopt the standard practices and the standard vocabulary as much as possible.\nIn most cases, deliberately adopting industry standards is a win. What you’re \ndoing is almost certainly already covered somewhere. If you’re unlucky , it will be only in a blog or two. If you’re lucky , it’s industry standard. Either way , you need to find where it is covered, for several reasons:\n •Y ou can refer to other sources instead of doing the writing yourself.\n •Other sources might suggest improvements or alternatives that you haven’t considered."",14311",14389,"**Knowledge Exploitation**\n\n**Documentation**:\n- Ready-made documentation is important for ensuring consistency and accuracy\n- Counterexample: Avoid hardcoding messages without extracting parameters, as it can lead to inconsistencies\n\n**Reconciliation on Test Assumptions**:\n- Given phase is used to create mock objects or inject data into test database\n- When testing legacy systems, you may need to deal with:\n  - Difficulty in mocking the database, leading to end-to-end testing\n  - Inability to re-create/populate the database for tests, requiring use of real databases\n- Despite these challenges, you can still use the ""Given"" statement as a prerequisite for scenarios or test methods\n\n**Consolidating Dispersed Facts**:\n- Diverse facts need to be consolidated into useful knowledge\n- This can be done by automatically scanning elements and aggregating information\n- Examples: Consolidating class hierarchies, creating living diagrams of system components\n\n**Ready-Made Documentation**:\n- Much knowledge is already documented in industry literature (e.g., programming languages, design patterns)\n- Adopting industry standards can be beneficial, as it allows reusing existing solutions",1189,60.55617046356201
False,3,"45-Using Ready-Made Knowledge in Conversation to Speed Up Knowledge Transfer.pdf,45-Using Ready-Made Knowledge in Conversation to Speed Up Knowledge T","45-Using Ready-Made Knowledge in Conversation to Speed Up Knowledge Transfer.pdf,45-Using Ready-Made Knowledge in Conversation to Speed Up Knowledge Transfer,""Ready-Made Documentation 85\n •Other sources might describe the situation in a deeper way than you did, giving \nyou external insights.\n •Such a description can validate that your approach makes sense. If you cannot find any account, beware.\n •Most importantly , you will learn how the rest of the planet talks about this situation.\nThe Power of a Standard Vocabulary\nHe who controls vocabulary controls thought.\n—Ludwig Wittgenstein\nTalking using the same words as everybody else in the world is a fantastic advan-tage. It enables you to communicate using shorter sentences. Without a common vocabulary , you could spend several sentences trying to describe the design of a text editor:\nInline editing is done thanks to an interface with several subclasses. The text editor \ndelegates the actual processing to the interface, without having to care which subclass is actually doing the job. Depending on whether the inline editing is on or off, an instance of a different subclass is used.\nHowever, if you’re familiar with standard documented knowledge like design pat-\nterns, you can get your point across more concisely:\n“Inline editing is implemented as a State in the Controller.”3\nEach mature industry has its own rich jargon because using such understood \njargon is an efficient way to communicate. Every part in a car has a specific name, \ndepending on its role in the vehicle: A shaft is not just a shaft, it’s a camshaft or a crankshaft. There’s a piston in a cylinder, and there are pushrods and a timing chain. \nDomain-driven design advocates for carefully growing such ubiquitous language in the domain.\nThe software industry makes progress each time its standard vocabulary grows. \nFor example, whenever Martin Fowler coins another term for a patterns that we do without thinking about it, he is helping grow our own ubiquitous language for our industry .\n3. This example of sentence on the State Pattern is from Kent Beck, https://www .facebook.com/notes/\nkent-beck/entropy-as-understood-by-a-programmer-part-1-program-structure/695263730506494\nChapter 3 Knowledge Exploitation 86\nUbiquitous language is extremely helpful in documentation. If you know what \nyou’re doing and you know what it’s called in the industry , you can just insert a \n reference to the industry standard, and you have achieved extensive documentation at low cost.\nPatterns and pattern languages are particularly effective at packing ready-made \nknowledge into a reusable documentation. Patterns really are canned documen-tation. They create a standard vocabulary you can use and refer to for complete reference.\nDesign patterns are communication tools for experienced programmers. Not training \nwheels or scaffolding for beginners.\n—@nycplayer on Twitter \n100% Design Patterns?\nPatterns matter. But when I started learning about design patterns, I was \n trying to use them whenever I could; this is so common that some even call it  patternitis. Then I became reasonable and learned when not to use patterns. \nMany articles have harshly criticized code that is full of patterns. However, \nI think they miss the point: Y ou should learn as many patterns as you can. The idea is not to learn patterns in order to use them, though they can be useful; instead, the point is to know many patterns in order to know the standard names of what you’re doing. In this view , 100% of the code could, and should, be described by the means of patterns.\nKnowing the standard vocabulary also opens the door to even more knowledge: \nY ou can find books and buy training on the topic you’re interested in. Y ou can also pinpoint people with this knowledge and hire them.\nKnowing the standard vocabulary is not so much about finding a solution. \nEven when you have a perfect solution, it’s worth finding out what it’s called in the  industry . A standard vocabulary enables you to refer to the work of other people who describe a solution in a well-written, peer-reviewed, and time-tested fashion.\nLinking to Standard Knowledge\nGeneric knowledge is already documented in the industry literature, in books, and online. When you use it, link to the authoritative source via an Internet link or a bib-liographic reference. If something has been written well once, you should refer to it rather than redo a poorer job of documenting it again.\nReady-Made Documentation 87\nOf course, one big problem can be identifying the standard name for a piece of \nknowledge. Search engines like Google and community websites like C2 and Stack \nOverflow are your friends. Y ou might have to guess how others talk about a topic. Then you can quickly scan the first search engine results to determine a more accu-rate vocabulary that can help you make narrower queries. Through this exploration, you’ll quickly learn a lot and get to see how much of the topic is already codified and using what terminology .\nDon’t hesitate to ask around—in your team or on a forum—for suggestions. \nOther people may have experience and seniority and more time to index pointers to standard knowledge they’ve encountered, even superficially , over the years.\nFrom a given term, you can also browse Wikipedia and all the various links at the \nends of the articles. Keep an eye on the bottom “related” links as well, until you rec-ognize your situation. Wikipedia is a fantastic tool for mapping a standard vocabu-lary to your own mental universe.\nMore Than Just Vocabulary\nHaving a shared standard vocabulary is a key advantage in communicating effec-tively , both verbally and in writing. That said, even a standard description may involve refinements and alternatives you haven’t considered. That information is use-ful, too. Ready-made documentation is in fact reusable thinking, which is a great help. It’s a bit like having the author, usually a seasoned practitioner, close to you so you can think together. \nYOU STILL HA VE TO THINK. BUT YOU DONT HA VE TO DO IT ALONE.\n—@michelesliger on Twitter\nIf I say that “I create an adapter on top of the legacy subsystem,” this sentence \nimplies a lot of things in a few words, because there’s more than a name in the idea of the adapter pattern. For example, an important consequence in this pattern is that the adaptee—the legacy subsystem in our example—should not know about the adapter; only the adapter should know about the legacy subsystem.\nWhen I say that a package represents the presentation layer, whereas this other \npackage represents the domain layer, I also imply that only the former can depend on the latter—and never the other way around.\nIt’s the norm in mathematics to reuse theorems and shared abstract structures \nfrom the literature to go further without reinventing and having to prove the same results again and again. The same goes for a standard vocabulary .\nChapter 3 Knowledge Exploitation 88\nUsing Ready-Made Knowledge in Conversation to Speed Up \nKnowledge Transfer\nHere’s a little conversation I had with my friend Jean-Baptiste Dusseaut (“JB” for \nshort, @BodySplash on T witter) to illustrate how a common culture and vocabulary can help with sharing knowledge efficiently (see Figure 3.8).\nCM: Hello, JB. I heard you launched a new startup, Jamshake. What is it about? \nJB: It’s a social and collaborative tool for musicians. We provide both a light-\nweight social network to find other musicians and cool projects and an in-browser digital audio workstation, the Jamstudio, to collaborate in real time with other musicians. It’s a kind of Google Doc for music (see Figure 3.9). Hello JB, I heard you \nlaunched a new startup\nJamshake, what is it\nabout?\nFigure 3.8 Hello, JB. I heard you launched a new startup, Jamshake—what is it about?\nIt‛s a social and collaborative tool for musicians.\nWe provide both a lightweight social network to \nfind other musicians and cool projects and an\nin-browser digital audio workstation,\nthe Jamstudio, to \ncollaborate in real time\nwith other musicians. \nIt‛s a kind of Google\nDoc for music.\nFigure 3.9 It’s a social and collaborative tool for musicians\nReady-Made Documentation 89\nCM: Sounds really cool! On the technical side, how’s your system organized in a \nnutshell?\nJB: I know you’re familiar with software craftsmanship and design, and DDD in \nparticular, so you won’t be surprised to hear that our system is made of several subsystems, one by bounded context (see Figure 3.10). \nCM: Oh yes, makes perfect sense! Each one is a microservice then?\nJB: Y es and no. They begin as modules very strictly decoupled in terms of \n dependencies—that is, ready to be extracted into their own process at runtime. But we keep them within the same process until we really need the separate  processes, usually to scale with the increasing load.\nCM: Y es! I call that the “microservice-ready” style of code. Y ou don’t have to pay \nthe full cost of too many physical services up front, as long as you have the option to do it at any time. But it takes discipline.I know you‛re familiar\nwith software\ncraftsmanship and\ndesign, and DDD in\nparticular, so you won‛t \nbe surprised to hear\nthat our system is made\nof several subsystems,\none by bounded\ncontext.\nFigure 3.10 I know you’re familiar with software craftsmanship and design\nChapter 3 Knowledge Exploitation 90\nJB: Y es, and that’s easy when you’re just one or two developers, like we are at the \nmoment. In practice, we regularly use these options because of the increasing load.\nCB: Increasing load: that’s a good problem to have when you’re a startup growing \nand looking for financing!\nJB: Y es, absolutely .CB: I’d like to have an overview of the full system. Perhaps Bounded Context by \nBounded Context?\nJB: Sure. There are around five bounded contexts at the moment: Acquisition \n(registration of new users), Arrangement, Audio Rendering (mixing, limiter, and compression kinds of processing), Stems Management, and Reporting. They all rely on Spring Boot instances on top of separate Postgres databases, except for the Stems management built with Node.js on top of an S3 storage. Each Bounded Context is paying attention to its domain model, except  Registration, which is CRUD-y based on Hibernate. It’s a survivor from the early version of the system.\nCB: I now have a clear picture in mind of what it’s like (see Figure 3.11). Thanks \na lot, JB! \nFigure 3.11 Picturing the full system while listening to JB talking\nReady-Made Documentation 91\nDoes Working More Deliberately Oppose Gut Feeling and Spontaneity?\nWhen should knowing something consciously be considered less desirable \nthan gut feeling? Consider this information from an interesting post by Steve Hawley:\nThe use of patterns is like the use of literary device. There are (proba-\nbly) an infinite number of ways in which the same general thought can be expressed, but I doubt you will find a single quality writer who started off a chapter thinking, “I’m introducing a character here so it’s best to paint a picture of the character. That calls for simile. Y eah, simile will do it. I think I’ll also use some ironic juxtaposition.” This type of writing feels forced. I’ve read code where the application of design patterns also felt forced.\n4\nSteve has a point here. I must admit that gut feeling, if trained properly on \nexamples of good quality , may have advantages over a conscious quest for per-\nfection, perhaps because our brains are more powerful that we often realize. And yes, very often, we pretend that what we have done was intentional and deliberate, even though we’re just explaining, after the fact, a decision that was actually based on gut feeling.\nFrancois from Propel has raised this interesting question: Should develop-\ners know design patterns? ORM engines are rather sophisticated pieces of software, and they make big (and deliberate) use of patterns, in particular the Fowler PoEAA patterns. Francois discusses in a blog post the reasons to men-tion or not mention the various patterns used at the heart of the Propel ORM in the documentation of the engine:\nPropel, like other ORMs, implements a lot of common Design Patterns. \nActive Record, Unit Of Work, Identity Map, Lazy Load, Foreign Key Mapping, Concrete Table Inheritance, Query Object, to name a few , all appear in Propel. The very idea of an Object-Relational Mapping is indeed a Design Pattern itself.\nIf you know the patterns, you can understand Propel quickly; if you don’t, \nyou’ll need to go through a lot more explanation to reach a higher level of \nexpertise, and next time you encounter another ORM, you’ll have to redo this discovery effort. Of course, at some point you’ll recognize the patterns, and you just won’t know their names. Y ou’d just be half-conscious of the patterns.\n4.  https://www .atalasoft.com/cs/blogs/stevehawley/archive/2009/07/29/design-patterns-and-practice.aspx"",12884",13050,"**Using Ready-Made Knowledge in Conversation to Speed Up Knowledge Transfer**\n\n**Advantages of a Standard Vocabulary**:\n- Enables communication using shorter sentences\n- Allows for more concise documentation\n- Helps identify solutions by referring to industry standards\n\n**Patterns and Pattern Languages**:\n- Pack ready-made knowledge into reusable documentation\n- Provide a standard vocabulary for efficient communication\n- Help grow the ubiquitous language in an industry\n\n**Ready-Made Documentation**:\n- Enables reusable thinking, like having an experienced practitioner close by\n- Helps identify the standard vocabulary to describe solutions\n\n**Using Ready-Made Knowledge in Conversation**:\n- Illustrated through a conversation between two friends about their startup\n- Demonstrates how a common culture and vocabulary can help share knowledge efficiently\n\n**Organizing a System with Bounded Contexts**:\n- System is made up of several subsystems, each focused on a specific area\n- Subsystems begin as modules decoupled in terms of dependencies, but kept within the same process until scaling is needed\n\n**Discipline in Implementing ""Microservice-Ready"" Style**:\n- Avoid paying the full cost of separate processes up front, but require discipline to maintain\n\n**Design Patterns vs. Gut Feeling**:\n- Gut feeling can sometimes be more desirable than a conscious quest for perfection\n- Knowing design patterns can help understand sophisticated systems like ORM engines",1468,81.1239242553711
False,3,"46-Summary.pdf,46-Summary,""Chapter 3 Knowledge Exploitation 92\nTools History\nAs you’ve seen before, a lot of knowledge is already there, and some of","46-Summary.pdf,46-Summary,""Chapter 3 Knowledge Exploitation 92\nTools History\nAs you’ve seen before, a lot of knowledge is already there, and some of it is hidden in \nthe history of the tools you use. Source control systems are an obvious example. They know about every commit—when it was done, by whom, what changes were made—and remember each commit comment. Some tools, like Jira or even your email client, also know a lot about your project.\nHowever, this knowledge is not always readily accessible, and it is not used as \nmuch as it could be used. For example, if there’s no screen to conveniently retrieve the most commonly asked question on a chat, you may never know it.\nSometimes you have to re-enter the same knowledge in another form in another \ntool. For example, a commit to fix a bug may have a comment that states it fixed the bug, but in many companies, you have to go to the work tracker to declare that you’ve fixed the bug. Y ou also have to declare the time spent on the task, only to enter it again into the time tracking tool later in an aggregated form. This is a waste of time. Consider integrating the tools together.\nBetter integration between tools also helps simplify human tasks, which reduces \nthe need for manual documentation of the tasks. However, when an integration fails, you do need documentation. Ideally , the integration component should provide this documentation. For example, an integration script should be readable and as declar-ative as possible.\nTherefore: Exploit the knowledge stored in your tools. Decide what tool is the \nunique authority for each bit of knowledge. Search for plugins that can provide integration with other tools or specific reports for your documentation  purposes. \nLearn how to use the command-line interface for each of your tools to program-matically extract knowledge or integrate various tools with other tools. Discover the APIs provided by the tools, including the email or chat integration.\nAs a last resort, find out how to query the internal database of a tool, but beware \nthat a database may change at any time without prior notice, as it’s usually not part of the official API.\nThe following are some examples of tools and their knowledge:\n •Source control: T ools such as Git with the blame  command can tell you who \nchanged what and when, show you commit comments, and reveal pull request discussions.\n •Internal chat system: Systems such as Slack can reveal questions, launch build, release information, mentions of words, activity , moods, who, and when.\n •User directory mailing lists: These tools can list teams, team members, and team managers, which helps you know who to contact for support, who to contact for escalation, and so on.\nSummary 93\n •Console history: Such tools can tell you the most recently or frequently used \ncommands or sequences of commands.\n •Services registry: This tool can give you a list of every running service, its address, and any additional tags.\n •Configuration server: This tool can give you environment configuration details.\n •Company service catalog: This catalog lists services governance information, such as who to contact, time last updated, and so on.\n •Project registry: Even a spreadsheet file on a shared drive can tell you project names, codes, leader, sponsor identification, budget codes, and so on.\n •Sonar components: These tools can show groupings into logical units, met-rics, and their trends at various levels of details and across multiple reposito-ries and multiple technologies.\n •Project tracking tools history or release management tools history: These tools can tell you about changes, including who, what when, and current versions.\n •Email server: These tools are commonly used to archive the following for auditing (for example, by forwarding to the archiving address): manual reports, manual decisions such as go-live decisions, and the most knowledge-able collaborators.\nSummary\nMost, but not all, valuable knowledge already exists in some form in the artifacts of your system. The path to living documentation starts with acknowledging the vari-ous authoritative sources of knowledge that are already present. It also involves determining whether there is a single source of truth, which can be extracted into various kinds of documents, or whether there are redundant sources that require a reconciliation mechanism. If the knowledge is spread over many places, you may need a consolidation mechanism to put it back in one piece. \nMost knowledge is already there, but not 100% of it, which suggests that you \nneed to find ways to enrich, or augment, the system itself with the missing knowl-edge so that it becomes knowledge complete. That is the topic of the next chapter.\nThis page intentionally left blank"",4772",4805,"**Knowledge Exploitation: Leveraging Existing Tools**\n\n**Integrating Tools for Knowledge Access:**\n- History of tools holds valuable information (e.g., source control systems, chat logs)\n- Integration between tools simplifies tasks and reduces manual documentation\n- When integration fails, document the reasons and provide declarative integration scripts\n\n**Exploiting Tool-specific Knowledge:**\n- Determine unique authority for each bit of knowledge\n- Use plugins or APIs to access information from various tools\n- Utilize command-line interfaces for extraction purposes\n\n**Examples of Tools and Their Respective Knowledge:**\n1. **Source control**: Git, commit history, blame command, pull requests discussions\n2. **Internal chat system**: Slack, questions, launch builds, release information, mentions\n3. **User directory mailing lists**: Teams, team members, managers, contact information\n4. **Console history**: Recently or frequently used commands\n5. **Services registry**: List of running services, addresses, tags\n6. **Configuration server**: Environment configuration details\n7. **Company service catalog**: Governance information (contact, time last updated)\n8. **Project registry**: Project names, codes, leaders, sponsors\n9. **Sonar components**: Logical unit groupings, metrics, trends across multiple repositories and technologies\n10. **Project tracking tools/release management tools**: Changes, who, what, when, current versions\n11. **Email server**: Archived manual reports, decisions, collaborators\n\n**Leveraging Existing Knowledge:**\n- Determine if there's a single source of truth\n- Consolidate knowledge from various places if needed\n- Enrich the system with missing knowledge to make it ""knowledge complete""",1729,60.97273373603821
False,3,"47-Chapter 4 Knowledge Augmentation.pdf,47-Chapter 4 Knowledge Augmentation,,0","47-Chapter 4 Knowledge Augmentation.pdf,47-Chapter 4 Knowledge Augmentation,,0",78,**Chapter 4: Knowledge Augmentation**\n\n**Knowledge Augmentation**: This chapter explores the topic of knowledge augmentation.,125,6.239603757858276
False,3,"48-When Programming Languages Are Not Enough.pdf,48-When Programming Languages Are Not Enough,""95Chapter 4\nKnowledge Augmentation\nThe source code mi","48-When Programming Languages Are Not Enough.pdf,48-When Programming Languages Are Not Enough,""95Chapter 4\nKnowledge Augmentation\nThe source code might have code that is never executed, variable and proce-\ndure names that are lies, and is in general a poor way to learn the programmer’s intention. T o me, design is as much the decisions and the reasons as the results of  the decisions. Sometimes code makes that clear , but usually it doesn’t.\n—Ralph Johnson, http://c2.com/cgi/wiki?WhatIsSoftwareDesign\nSoftware is built from its source code. Does this mean that the source code tells eve-rything there is to know over the lifecycle of the application? Sure, the source code tells a lot—and it has to. The source code describes how to build the software so that the compiler can do it. Clean code goes further, aiming to make knowledge as clear as possible for the other developers working on it.\nStill, code is often not enough. When the code doesn’t tell the full story , you need \nto add the missing knowledge to make it complete.\nWhen Programming Languages Are Not Enough\nMost programming languages have no predefined way to declare the key decisions, to record the rationale, and to explain the choice made against the considered alter-natives. Programming languages can never tell everything. They focus on their key paradigm and rely on other mechanisms to express the rest: naming, comments, libraries, and so on.\nChapter 4 Knowledge Augmentation 96\nA Bridge Metaphor\nConsider the construction of a bridge as a metaphor here. A bridge is built \nbased on its technical drawings. However, if at some point it is necessary to replace its wood timbers with new ones in a stronger material, such as steel, the original technical drawings wouldn’t be enough. They would tell the dimensions chosen for the wooden timbers, but they wouldn’t tell where the dimensions came from. They wouldn’t tell about the calculations related to the resistance of materials, or fatigue of materials, or resistance against strong waters and extreme wind forces. They wouldn’t tell what was considered “extreme” at the time the drawings were made. Perhaps the design needs to be reconsidered now , to accommodate more extreme conditions in light of recent events. Perhaps at the time the bridge was originally built, no one though a tsunami would be pos-sible in this place, but now we know it could actually happen. \nWhen it comes to documenting design decisions and their rationale, program-\nming languages can’t help much beyond simple standard decisions such as the typical visibility of members or inheritance.\nWhen a language does not support a design practice, workarounds such as nam-\ning conventions can often do the job. Some languages with no way to express pri-vate methods prefix them with an underscore. Languages without objects adopt a convention of having a first function parameter called \nthis . Y et even with the best \nprogramming language, a lot of what’s in the developer’s head still cannot be fully expressed by the language alone.\nIt’s possible to add knowledge as code comments. But comments lack structure, \nunless you hijack structured comments like Javadoc. Also, refactoring does not apply to comments as well as it applies to code.\nTherefore: Augment your programming language so that the code can tell the \nfull story, in a structured way. Define your own way to declare the intentions and the reasoning behind each key decision. Declare the higher-level design intentions, the goals, and the rationales.\nDon’t rely on plain comments for this. Use strong naming conventions or use the \nextension mechanisms of the language, such as Java annotations and .Net attrib-utes; the more structured, the better. Don’t hesitate to write a little code solely for this documentation purpose. Create your domain-specific language (DSL) or reuse one if needed. Rely on conventions when suitable."",3831",3932,"**Knowledge Augmentation: The Need for More Than Programming Languages**\n\n**The Limits of Source Code**:\n- Source code may contain unexecuted code, misnamed variables/procedures, and is not a complete representation of the programmer's intentions\n- **Design** is about the decisions and their reasons, which may not be fully expressed in code\n\n**Programming Languages as Bridge Metaphors**:\n- Like bridge construction, programming requires more than just technical drawings - it needs rationale and context\n- Programming languages can't express all the decisions and reasoning behind code choices\n- Workarounds like naming conventions and comments can help, but are not ideal due to lack of structure\n\n**Extending Programming Languages**:\n- Adding custom code comments or using structured commenting styles (e.g., Javadoc) can help\n- Creating **domain-specific languages (DSLs)** or reusing existing ones can be more effective\n\n**Conclusion**:\n- Rely on conventions and language extensions, rather than plain comments, to document design intentions and reasoning.",1063,31.512125968933105
False,3,"49-Documentation Using Annotations.pdf,49-Documentation Using Annotations,""Documentation Using Annotations 97\nKeep the augmented knowledge as close a","49-Documentation Using Annotations.pdf,49-Documentation Using Annotations,""Documentation Using Annotations 97\nKeep the augmented knowledge as close as possible to the code it is related to. \nIdeally, they should be collocated to be totally refactoring proof. Have the com-piler check for any errors. Rely on the autocompletion of the IDE. Make sure the augmented knowledge is easily searchable in your editor or IDE and ensure that it can be easily parsed by tools to extract living documents out of the whole aug-mented code.\nAugmented code contains a lot of valuable hints for the future maintainers. \nOne important consideration when adding knowledge related to the code is how it evolves when the code changes. Code will change because that’s the way it is. As a consequence, it’s essential for the additional knowledge to either remain accurate or change at the same time as the code, with no or very little manual maintenance. What happens when a class or package is renamed? What happens when a class is deleted? The extra knowledge you want to add should be refactoring proof.\nAugmented code is great for making decisions explicit in the code and for adding \nthe rationale behind the decisions.\nBecause it is structured, augmented code is also easy to search and to navigate in \nthe IDE, without plugins. This means that it also works the other way: From a cho-sen rationale, you can find all the code that is related to it. This is quite valuable for traceability or impact analysis.\nAugmented code in practice can be done with several approaches:\n •Internal documentation \n •By annotation\n •By convention\n •External documentation\n •With sidecar files\n •With a metadata database\n •With a DSL\nDocumentation Using Annotations\nExtending a programming language by using annotations for documentation pur-poses is my favorite way to augment code in languages such as Java or C#. Annota-tions do not impose any constraint on naming or code structure, which means they work in most code bases. And because they are as structured as the programming \nChapter 4 Knowledge Augmentation 98\nlanguage itself, it’s possible to rely on the compiler to prevent errors and to rely on \nthe IDE for autocompletion, navigation, and searching.\nThe main strength of annotations is that they are refactoring-friendly: They are \nrobust when the element they are attached to is renamed, they move with it when it moves, and they get deleted when it’s deleted. This means no extra effort to maintain them, even if the code changes a lot.\nTherefore: Explain the design and its purpose using structured annotations. Cre-\nate, grow, and maintain a catalog of predefined annotations, and then simply include these annotations to enrich the semantics of the classes, methods, and modules.\nY ou can then create little tools that can exploit the additional information in the \nannotations, such as to enforce constraints or to extract knowledge into another format.\nWhen you have annotations and you know them, you can more quickly declare \na design decision: Just add the annotation. Annotations are like bookmarks for the thinking that has happened (see Figure 4.1). \nAnnotations can represent class stereotypes such as values, entities, domain ser-\nvices, and domain events. They can represent active pattern collaborators, such as composites or adapters. They can declare styles of coding and the default preference.\nIt’s important that your annotations correspond to standard techniques with \nstandard names as much as possible. If you need your own custom ones, be sure to document them in a place that everybody knows.\nPlacing an annotation to declare your decisions in terms of standard knowledge \nand standard practices encourages deliberate practice. Y ou have to know what you’re doing, and you have to know what it’s called in the industry literature. Using stand-ard design patterns and annotations can reduce the time required to complete a task.\nAnnotations are also searchable in the IDE, which is handy . For example, you can \nsearch for every class that is annotated by a selected annotation, which gives you a new way to navigate the design.\nStructured annotations are a powerful tool, but they are probably not enough to \ncompletely replace all other forms of documentation to describe all design decisions and their intentions. Y ou still need conversations between everyone involved. In addi-tion, some knowledge and insights are best explained through clear writing with a \n<code/> +\nFigure 4.1 Augmented Code = Code + Annotations"",4494",4575,"**Documentation Using Annotations**\n\n**Advantages of Augmented Code:**\n- Keep augmented knowledge close to related code\n- Collocated for easy refactoring\n- Compiler checks ensure accuracy\n- Autocompletion in IDEs\n- Easily searchable and parseable by tools\n- Valuable hints for future maintainers\n\n**Considerations:**\n- Augmented knowledge evolves with code changes\n- Remain accurate or change at the same pace as the code\n- No manual maintenance required\n- Decisions and rationale documented in annotations\n\n**Approaches to Augmented Code:**\n- Internal documentation\n- By annotation\n- By convention\n- External documentation\n- With sidecar files\n- With a metadata database\n- With a DSL\n\n**Annotations:**\n- Extend programming languages for documentation purposes\n- No constraint on naming or code structure\n- Structured and compiler-checked\n- Refactoring-friendly\n- Represent class steretypes, active patterns, coding styles, and preferences\n\n**Standardization:**\n- Correspond to standard techniques with standard names\n- Document custom annotations\n- Encourage deliberate practice and industry knowledge\n\n**Searchability:**\n- Searchable in IDEs for easy navigation\n- New way to navigate the design.",1197,35.00828552246094
False,3,"50-Describing the Rationale Behind Decisions.pdf,50-Describing the Rationale Behind Decisions,""Documentation Using Annotations 99\nsense of nuance—som","50-Describing the Rationale Behind Decisions.pdf,50-Describing the Rationale Behind Decisions,""Documentation Using Annotations 99\nsense of nuance—something that’s hard to do in annotations. Y ou may also find it \ndesirable to keep track of more nuanced aspects like emotions involved in decision-making, such as fears, taste, distaste, political pressure. Other media, such as plain text, are better for that.\nFinally , the knowledge declared using annotations is machine readable, which means \ntools can exploit this knowledge to help the team. Living diagrams and living glossa-ries, for example, rely on such possibilities. Imagine what you could do—or what you could have tools do for you—with tools that can understand your design intents!\nAnnotations as More Than Tags\nAnnotations in Java and attributes in .Net are genuine citizens of their programming language. They have a name and a module name (package or namespace). They also hold parameters and can themselves be annotated by other annotations. And because they are classes, they also benefit from the structured comments syntax used by doc-umentation generators like Javadoc. All this means you can convey a lot of knowl-edge through simple annotations.\nLet’s look at a technical example. Annotations use meta-annotations to describe \nwhere they can be applied. For example, here the annotation \nAdapter  can be applied \nto types and packages:\n1  @Target({ ElementType.TYPE, ElementType.PACKAGE })\n2  public @interface Adapter { 3  }\nThe following example involves annotations with parameters. If you were to \nannotate an instance of a builder pattern, you could describe the type that the builder \nproduces as a parameter of the annotation:\n1  public @interface Builder {\n2      Class[] products() default {}; 3  }45  @Builder(products = {Supa.class, Dupa.class})6  public class SupaDupaBuilder { 7   //...8  }\nOften the declared return types and implemented interfaces can already tell a lot \nof similar information, but they miss the precise semantics that additional anno-\ntations convey . In fact, more precise annotations open the door to more automa-tion because they give tools a way to interpret the source code with higher-level semantics."",2139",2240,"**Annotations in Software Development:**\n* Used for providing additional information about code elements\n* Can convey nuanced aspects of decision making: emotions, fears, political pressure (not machine readable)\n* Examples: Living diagrams and glossaries rely on this knowledge\n* Annotations are classes with name, module name, parameters, and can be annotated\n* Described using meta-annotations, e.g., @Target for application type and location\n* Can have parameters, as in the case of Builder annotation in Java\n* More precise annotations enable automation by interpreting source code with higher semantics.",609,19.712615966796875
False,3,"51-Embedded Learning.pdf,51-Embedded Learning,""Chapter 4 Knowledge Augmentation 100\nJust as the Semantic Web aims to transform unstructured data into","51-Embedded Learning.pdf,51-Embedded Learning,""Chapter 4 Knowledge Augmentation 100\nJust as the Semantic Web aims to transform unstructured data into a web of data, \na code base with annotations that clarify the semantics of the source code becomes a \nweb of data that machines can interpret.\nDescribing the Rationale Behind Decisions\nOne of the most important pieces of information worth recording for future genera-tions is the rationale behind each decision. What may seem like a stupid choice years later was not so stupid when it was decided. Most importantly , when the rationale is referring to a context at some point in time, and now the context is different, you are in a better position to reconsider the decision now .\nFor example, say that an expensive database was chosen long ago because it was \none of the few to be able to fully cache data in memory . Reading this rationale now , you may consider instead using NoSQL datastores for that purpose. As another example, say that an application has layers talking to each other through XML  everywhere, which makes your life cumbersome and causes performances issues. The rationale for this decision explains that this architecture was meant to be distributed physically between layers in order to scale. However, after many years, it has become clear that this will never happen, so you know now that you could remove all the extra complexity . Without a clear rationale, you would always wonder if you had missed something, and you would not dare reconsidering the whole thing.\nEmbedded Learning\nPutting more knowledge into code helps its maintainers learn while working on it. At a minimum, annotations should themselves be documented. If you have an annota-tion named \nAdapter , its comments should explain what Adapter  is. My favorite \nway to do this is to link to a clear online definition, such as the corresponding  Wikipedia page, along with a brief text description within the comment itself:\n1  /**\n2   * The adapter pattern is a software design pattern that allows the 3   * interface of an existing class to be used from another interface.4   * 5   * The adapter contains an instance of the class it wraps, and 6   * delegates calls to the instance of the wrapped object.7   * 8   * Reference: See <a href=""""http://en.wikipedia.org/wiki/\9   * Adapter_pattern"""">Adapter_pattern</a>10  */\nDocumentation Using Annotations 101\n11  public @interface Adapter { \n12  }\nThis is more important than it might seem. From now on, every class with this \nannotation is only a tooltip away from complete documentation of its design role.\nConsider the example of a random Adapter  class in a project, in this case on top \nof the RabbitMQ middleware:\n1 @Adapter2 public class RabbitMQAdapter { 3     //...4 }\nWhen this class is opened in any IDE, when the mouse hovers over it, the tooltip \ndisplays its documentation, as shown in Figure 4.2. \nThe tooltip description provides a brief explanation but is especially useful for \ndevelopers who already know the information but just need a refresher. Those who \nneed more information than the tooltip provides can click the link to be redirected to more information. They will probably ask questions in the process, but at least there’s an easy entry point to the learning. In this case, the annotations describe that the \nAdapter  class is an instance of the adapter pattern, and they act as a gateway to \nlearning more about the adapter pattern.\nTherefore: Putting more knowledge into the code is not just for documentation; \nit can also help deliberately increase the skills of the teams working on it. Con-sider this opportunity when deciding on your augmented code strategy. When augmenting code, think about how your colleagues will react when they dis-cover it.\nAn annotation could also link to the book or books that best explain the topic. Or \nit could link to a company e-learning program.\nFigure 4.2 The tooltip of  the annotation displays its documentation\nChapter 4 Knowledge Augmentation 102\nAs an alternative to including links in comments, every annotation from the same \nbook could have a meta-tag representing the book. In the following example, both \nAdapter  and Decorator  annotations represent design patterns from the Gang of \nFour book Design Patterns, and information about the book can be included in a \nmeta-annotation GoF specifically about the book:\n1  /**\n2  * Book: <a href=""""http://books.google.fr/books/about/Design_3  Patterns.html?id=6oHu\KQe3TjQC"""">Google Book</a>4  */5  @Target(ElementType.ANNOTATION_TYPE)6  public @interface GoF { 7  }89  @GoF10 public @interface Adapter { 11 }1213 @GoF14 public @interface Decorator { 15 }\nThis is only an example, and of course you are not limited to documenting design \npatterns! Feel free to elaborate your own scheme for organizing your knowledge \nbased on these ideas.\nUsing Structured Tags Within Comments\nIf you are using a programming language that does not have annotations, you can use structured tags within comments:\n1 /** @Adapter */\nIt’s a good idea in this case to conform to a common style of structured docu-\nmentation. The language may provide some tool support, such as autocompletion or code highlighting. The XDoclet library did that with great success in the early Java days, hijacking the Javadoc tags in order to use them as annotations.\nY ou may also use the good old marker interface pattern, which involves imple-\nmenting an interface with no method in order to mark the class. For example, to mark a class as serializable, you implement the \nSerializable  interface:\n1  public class MyDto implements Serializable { \n2    ...3  }\nDocumentation Using Annotations 103\nNote that this is quite an intrusive way to tag a class, and it pollutes the type \nhierarchy , but it provides a good illustration of what we’re discussing here.\nWhen Annotations Go Too Far\nGoogle Annotations Gallery1 is a retired open-source project from 2010 that \nproposed a collection of neat annotations to augment your code with your \ndesign decision, intentions, honest feelings, and even shame. \nDiscovering stupid code? Y ou can leave a @LOL  or @Facepalm  or @WTF  \nannotation:\n1  @Facepalm\n2  if(found == true){...}\nOr you can leave all of them with an explanation:\n1  @LOL @Facepalm @WTF(""""just use Collections.reverse()"""")2  <T> void invertOrdering(List<T> list) {...\nY ou can also use remark annotations to preemptively qualify your own \nmiserable code:\n1  @Hack public String 2  unescapePseudoEscapedCommasAndSemicolons(String url) {\n…or to justify it:\n1 @BossMadeMeDoIt2 String extractSQLRequestFromFormParameter(String params){...}\nY ou can warn your team members with the @CantTouchThis  annotation.\nStumble across code that somehow works beyond all reason? Life’s short. \nMark it with @Magic  and move on:\n1 @Magic public static int negate(int n) { 2   return new Byte((byte) 0xFF).hashCode() 3   / (int) (short) '\uFFFF' * ~0 4   * Character.digit ('0', 0)\n1. Google Annotations Gallery , https://code.google.com/p/gag/"",7039",7098,"**Embedded Learning**\n\n**Chapter 4: Knowledge Augmentation**\n\n**Transforming Data into Understandable Information**:\n- The Semantic Web aims to transform unstructured data into a web of data that machines can interpret\n- A codebase with annotations clarifying the semantics becomes a ""web of data"" that can be understood by machines\n\n**Documenting Decisions**:\n- Recording the rationale behind decisions is important for future generations\n- This allows reconsideration of decisions in different contexts\n- Example: Choosing an expensive database or using XML between layers\n\n**Embedded Learning**:\n- Putting more knowledge into code can help its maintainers learn\n- Annotations should document what the annotated element is\n- Tooltips can provide brief explanations and link to more information\n\n**Structured Tags Within Comments**:\n- Use a common style for structured documentation in comments\n- The XDoclet library provided autocompletion and code highlighting for this approach\n\n**When Annotations Go Too Far**:\n- The ""Google Annotations Gallery"" proposed annotations for design decisions, intentions, feelings, and more\n- These annotations could be used to preemptively qualify or justify code",1198,41.454734086990356
False,3,"52-Living Documentation in Legacy Code with Conventions.pdf,52-Living Documentation in Legacy Code with Conventions,""Chapter 4 Knowledge Augmentation","52-Living Documentation in Legacy Code with Conventions.pdf,52-Living Documentation in Legacy Code with Conventions,""Chapter 4 Knowledge Augmentation 104\n5   * n * (Integer.MAX_VALUE * 2 + 1) / (Byte.MIN_VALUE >> 7)\n6   1 * (~1 | 1); 7 }\nAnd when you’ve done a good job of design, you can let the world know \nyour brilliance with the literary annotations:\n1 @Metaphor public interface Life extends Box { }\nor:\n1  @Oxymoron public interface DisassemblerFactory { Disassembler \n2  createDisassembler(); }\n \nDocumentation by Convention\nUsing plain conventions to document your decisions is convenient. For example, in \nJava, every identifier that starts with an uppercase letter is a class, and every identi-fier that starts with a lowercase letter is a variable name.\nThere are conventions for many situations, in many technologies, and you can \nalways add your own conventions on top of any technical environment, be it code, XML, JSON, assembly , or SQL. Even old projects with old technologies rely on conventions to communicate knowledge, describe their structure, and help with navigation.\nHere are some examples of documentation by convention:\n •Package names by layer: Everything in a package named *.domain.*  might \nrepresent domain logic, whereas everything in a package named *.infra.*  \nmight represent infrastructure code.\n •Package names by technical class stereotype: It’s common in many code bases to group every data access object class in a package named after the abbreviations \n*.dao.* ; similarly for your Enterprise Java Beans in a package \n*.ejb.*  and your plain old Java objects used by your favorite framework in a \npackage *.pojo.* .\n •Commit comments: Y ou could use a convention such as [FIX] issue-\n12345 free text , where the square brackets categorize the type of commit \nas either FIX, REFACTOR , FEATURE , or CLEAN , and issue-xxx  references the \nticket ID in the bug tracker."",1818",1941,"**Living Documentation in Legacy Code with Conventions**\n\n**Using Conventions for Documenting Decisions**:\n- In Java:\n    - **Uppercase identifiers**: represent classes\n    - **Lowercase identifiers**: represent variables\n- There are conventions for many situations, technologies, and can be customized.\n- Examples of documentation by convention:\n  - **Package names by layer**: `*.domain.*` represents domain logic, `*.infra.*` represents infrastructure code\n  - **Package names by technical class stereotype**:\n    - `*.dao.*`: data access objects\n    - `*.ejb.*`: Enterprise Java Beans\n    - `*.pojo.*`: plain old Java objects used by a framework\n  - **Commit comments**: use conventions like `[FIX] issue-12345` to categorize the type of commit (e.g., FIX, REFACTOR, FEATURE, CLEAN) and reference the ticket ID in the bug tracker.",834,23.702744722366333
False,3,"53-The Limitations of Conventions.pdf,53-The Limitations of Conventions,""Documentation by Convention 105\n •The Ruby on Rails style of convention over","53-The Limitations of Conventions.pdf,53-The Limitations of Conventions,""Documentation by Convention 105\n •The Ruby on Rails style of convention over configuration: In this conven-\ntion, if a database table is named orders , the controller would be named \norders_controller .\nLiving Documentation in Legacy Code with Conventions\nWhenever you have an existing code base that follows conventions, you have an opportunity to go the living documentation route by exploiting all the existing con-ventions, without even having to touch the source code to add anything. (This is not possible with documentation by annotation.)\nFor example, say that an existing application follows a layered design. If you’re \nlucky , its package names represent the layers directly through naming conventions:\n1  /record-store-catalog/gui\n2  /record-store-catalog/businesslogic3  /record-store-catalog/dataaccesslayer4  /record-store-catalog/db-schema\nY our documentation is already there in the naming of the Java packages or in \nnamespaces or subprojects in C#.\nDocumenting the Conventions\nIf everyone on a team is familiar with the conventions used, then you don’t need any \nfurther documentation. Conventions published by another company are called ready-made documentation, and you can adopt such conventions and then just cre-ate a reference to the external documentation of the sets of conventions in the README file. In practice, though, I recommend always documenting the conven-tions in the README file. Here’s an example of documenting conventions in a real code base:\n1 README.txt\n23 This application follows a Layered Architecture.4 Each layer has its own package, with the following naming5 conventions:67 /gui/*8 /businesslogic/*9 /dataaccesslayer/*10 /db-schema/*\nChapter 4 Knowledge Augmentation 106\n11\n12 The GUI layer contains all the code about the graphical user13 interface.14 All code responsible for display & data entry must be there.1516 The business logic layer contains all the domain-specific17 logic and behavior. This is where the domain model is.18 Business logic should only be there and nowhere else.1920 The data access layer contains all the DAO (Data Access 21 Objects) responsible to interact with the database.22 Any change of storage technology should only impact23 this layer and no other layer (in theory at least :)24 25 The DB Schema contains all the SQL scripts to setup, delete26 or update the database.2728 Important Rule: Each layer can only depend on the layers29 below. No layer can depend on the layer or layers above,30 this is forbidden!\nSome conventions carry a cost, especially when they add noise to the naming. For \nexample, putting prefixes or suffixes on identifiers (for example, VATCalculation-\nService , DispatchingManager , DispatchingDTO ) is a standard practice, but \nit’s not clean code, and the names in your code do not belong to the business domain \nlanguage anymore!\nWhen every interface in a package is a service, adding the Service  prefix adds no \ninformation, just noise. Every class in a /dto/  package might not need the DTO suf-\nfix if it would be redundant information.\nConsistently Adhering to Conventions\nDocumentation by convention works only to the extent that everyone involved has enough discipline to adhere to the conventions consistently . The compiler does not care about your conventions and won’t help much in enforcing them.\nOne typo, and you’re already not following the convention! Y ou can, of course, \ntweak the compiler or your IDE parser, or you can use static analysis tools to detect some violations of conventions. Sometimes it’s a lot of work, but other times it’s surprisingly easy , so you may give it a try ."",3647",3726,"**Limitations of Conventions:**\n- **The Ruby on Rails style of convention over configuration**: If a database table is named ""orders"", the controller would be named ""orders\_controller"".\n- **Living Documentation in Legacy Code with Conventions**: When existing codebase follows conventions, you can exploit them for living documentation without modifying source code.\n  * Example: A layered design where package names represent layers through naming conventions.\n\n**Documenting the Conventions:**\n- Write comprehensive bulleted notes summarizing the provided text, with headings and terms in bold.\n\n**Convention Naming Conventions:**\n1. **README.txt**:\n   - Application follows a Layered Architecture:\n   - Each layer has its own package with specific naming conventions:\n     * GUI layer: /gui/\n     * Business logic layer: /businesslogic/\n     * Data access layer: /dataaccesslayer/\n     * DB schema: /db-schema/\n   - Each layer can only depend on the layers below, not on those above.\n2. **Costs of Conventions**:\n   - Adding prefixes or suffixes to identifiers (e.g., ""VATCalculation\_Service"") can add noise and deviate from clean code principles.\n   - Consistently adhering to conventions is essential as the compiler won't enforce them, requiring manual enforcement through static analysis tools.",1303,37.772878646850586
False,3,"54-Sidecar Files.pdf,54-Sidecar Files,""External Documentation Methods 107\nRelying on documentation by convention to help produce living documents suc","54-Sidecar Files.pdf,54-Sidecar Files,""External Documentation Methods 107\nRelying on documentation by convention to help produce living documents such \nas living diagrams encourages and rewards following the conventions: If you break \nthe convention, then your living documents will fail, which is nice.\nThe Limitations of Conventions \nConventions work well for categorizing sections of code, but they quickly show their limits when you try to enrich them with additional knowledge such as rationales, alternatives, and so on. In contrast, annotations may be better able to include such additional knowledge.\nConventions are often little more than free text meant for humans. However, you \nmay still get some tool support for your conventions:\n •Y ou can configure your IDE with templates for each convention. For example, you might be able to type a few characters and have the template print the full name properly to adhere to the convention; for a commit comment with a more complicated convention, the template might print a placeholder that you can just fill in.\n •Y ou can have your living document generators interpret the conventions to perform their work.\n •Y ou can enforce rules such as dependencies between layers based on the nam-ing conventions (for example, using JDepend, ArchUnit, or your own tool built on top of any code parser).\nCompared to annotations, conventions also have the advantage of not disrupting \nold habits. If your team and managers are very conservative, you may prefer going the documentation by convention route rather than the documentation by annota-tion route. Y ou can probably guess that I prefer documentation by annotation.\nExternal Documentation Methods\nDocumentation by annotation and documentation by convention are forms of inter-nal documentation that appear right into the code itself. In contrast, the techniques described in the following sections are forms of external documentation, as they reside at a location that is remote from the thing being documented."",1989",2034,"**Conventions vs. Annotations:**\n* **Conventions**:\n	+ Work well for categorizing sections of code\n	+ Limitations: unable to include additional knowledge like rationales or alternatives\n	+ Often little more than free text for humans\n	+ IDEs may support with templates and tool generation\n	+ Can enforce rules based on naming conventions\n	+ Disrupting old habits less likely due to familiarity\n* **Annotations**:\n	+ May be able to include additional knowledge like rationales, alternatives, etc.\n	+ More detailed documentation than conventions\n	+ Do not disrupt old habits as much due to personal preference and customization\n	+ Can provide more comprehensive documentation directly into the code itself.\n\n**Limitations of Conventions:**\n* Quickly show their limits when trying to include additional knowledge\n* Often just free text meant for humans\n* IDEs may offer support with templates or tool generation\n* Enforce rules based on naming conventions (e.g., dependencies between layers)\n\n**Advantages of Conventions:**\n* Less disruptive to old habits compared to annotations\n* Team and managers might prefer this approach over documentation by annotation.\n\n**External Documentation Methods:**\n* Different from internal documentation by convention or annotation\n* Techniques described in following sections reside at a location remote from the thing being documented.",1367,34.406708002090454
False,3,"55-Stereotypical Properties.pdf,55-Stereotypical Properties,""Chapter 4 Knowledge Augmentation 108\nSidecar Files\nWhen putting annotations within code","55-Stereotypical Properties.pdf,55-Stereotypical Properties,""Chapter 4 Knowledge Augmentation 108\nSidecar Files\nWhen putting annotations within code is not possible, you can put them into a file \nnext to the code. Sidecar files—also known as buddy files, companion files, or con-nected files—are files that store metadata that cannot be supported by the source file format. For each source file, there is typically one associated sidecar file with the same name but a different file extension.\nFor example, some web browsers save a web page as a pair of files: an HTML file \nand a sidecar folder of the same name but with a _folder prefix. As another example, a digital camera might have the capability to record a piece of audio while taking a picture, and the associated audio may be stored as a sidecar file with the same name as the .jpg file, but with a .wav extension.\nSidecar files are a type of external annotations. They can be used to add any kind \nof information, such as a classification tag or a free text comment, without having to touch the original source file on the file system.\nThe main problem with using sidecar files is that when the file manager is not \naware of the relationship between the source file and its sidecar file, it cannot prevent the user from renaming or moving only one of the files without the other, and the relationship is broken. For this reason, I don’t recommend using sidecar files unless you have no other choice.\n Note\nOld source control systems like Concurrent V ersions System (CVS) used a lot of sidecar files.\n \nMetadata Databases\nWhen putting annotations within code is not possible, you can keep them in an out-side database. A metadata database is a database that stores metadata referencing other source files or components. A well-known example is the iTunes database, which contains a lot of metadata associated with each song (for example, playlists, recent listening history) that doesn’t fit within the song’s audio file. The metadata might not fit within the file because the file format does not have a place to store the metadata or because it would not be a good idea to change the file at all.\nMetadata may also reference a file but not really be intrinsic to it, and hence it \nshould be stored somewhere else. For example, a photo should not store information about its being part of an album; it would be better to store the album somewhere \nDesigning Custom Annotations 109\nelse. In a similar fashion, the URL of the thumbnail of a photo is metadata that is \nof interest only for the photo application, and it would be intrusive to corrupt the photo file by including that kind of metadata in its own structure (assuming it were even possible).\nThe main problem with using metadata databases for annotations, just like the \nmain problem with using sidecar files, is that it’s easy for the metadata database and the corresponding files to get out of sync if one of these files is renamed, moved, or deleted without the database being updated.\nA metadata database should be the last-resort choice when it’s not possible to \ntouch the files at all and metadata has to be stored elsewhere. However, it is also a convenient approach when the management of the metadata is done in bulk across all files at the same time, by different people than the people managing the files  themselves. For example, if hundreds of photos are managed by a photographer, but the metadata database is a plain spreadsheet managed by a librarian, then it is easy for the librarian to quickly add all the metadata in a column, thanks to the copy/paste, interpolation, and calculation abilities of a modern spreadsheet applica-tion. The photographer does not have to be involved, and there is no risk of corrupt-ing the photo file by mistake.\nCommon examples of metadata databases are the various key/value stores \nembedded in discovery registries; deployment, configuration, and provisioning tools; service catalogs; bookmarking registries; and so on. Whenever you can reference something and add tags, you have a de facto metadata database!\nDesigning Custom Annotations\nOff-the-shelf literature is essential in quickly learning from the experience of others and sharing a common vocabulary , whatever the company , department, or continent. However, the problem with such literature is that in order to be shared with every-one, it has to give up what is specific to each particular context.\nY ou should use this standard body of knowledge, and you can also extend it to \nmake it even more expressive. Y ou can evolve your vocabulary of tags and annota-tions from the standard literature with additions and extensions to make it more specific to your own context.\nFor example, we more or less all agree on a standard circle of six colors, but in \nyour own visual charter, you certainly use custom variants of these colors, which are specific to you. Y our light blue is certainly a blue, but it is up to you to define what “light” means."",4951",5018,"**Sidecar Files:**\n- Sidecar files are metadata that cannot be supported by a source file's format\n- Typically have the same name as the source file but different file extensions (e.g., HTML and its associated sidecar folder)\n- Stored next to the source file, not within it\n- Used for additional information such as classification tags or free text comments\n- Problem: File manager is unaware of relationship between files, leading to potential issues if one file is renamed, moved, or deleted without the other\n\n**Metadata Databases:**\n- Store metadata referencing other source files or components\n- Examples include iTunes database and photo album information\n- Metadata may not fit within a file's format or structure\n- Problem: Easy for metadata database and corresponding files to get out of sync if one is renamed, moved, or deleted without the other being updated\n- Last-resort choice when touching the original files is not possible\n- Convenient approach for managing metadata in bulk across all files at once\n\n**Custom Annotations:**\n- Use standard literature as a foundation for specific contexts\n- Extend and customize vocabulary of tags and annotations to be more expressive\n- Example: Using custom variants of standard colors in your own visual charter.",1266,33.585341691970825
False,3,"56-Stereotypes and Tactical Patterns.pdf,56-Stereotypes and Tactical Patterns,""Chapter 4 Knowledge Augmentation 110\nStereotypical Properties\nWhen we","56-Stereotypes and Tactical Patterns.pdf,56-Stereotypes and Tactical Patterns,""Chapter 4 Knowledge Augmentation 110\nStereotypical Properties\nWhen we design code, we think in terms of working behavior and also in terms of \nproperties, desirable or not. Here are some examples of desirable properties:\n •NotNull : For a parameter that cannot be null. Life is so much easier when you \nuse it almost always!\n •Positive : For a parameter that has to be positive.\n •Immutable : For a class that remains unchanged.\n •Identity by value : Where equality is defined as the equality of data.\n •Pure : For a function or for every function of a class to avoid side effects.\n •Idempotent : For a function that has the same effect when called more than \nonce (which is very important in distributed systems).\n •Associative : For a function such as (a + b) + c = a + (b + c), when doing \n map-reduce kinds of things.\nWhen you use these properties, you need to make your usage clear in the code. \nY ou can do this with the type system whenever possible. For example, it is possible to express the possibility of having no result with \nOption  or Optional  if it is built in \nto the language or provided by a standard library . Using a Scala case class is in itself \na shorthand for (Immutable, Identity by value). When this is not possible, you can express the properties with comments or with custom annotations, along with auto-mated tests and property-based testing.\nStereotypes and Tactical Patterns\nIn a language like Java or C#, everything is a class, but not every class is of the same kind or has the same purpose. Note that in functional programming languages, eve-rything is a function, but not every function has the same purpose either. Domain-driven design proposes some fundamental categories of classes, such as value object, \nentity, domain service, and domain event. It also suggests borrowing from other pat-\nterns, such as using design patterns (for example, strategy and composite patterns). The point is that some (but not all) design patterns are also domain patterns.\nIt is interesting that these categories of classes offer compressed ways to express a \nlot of information. For example, when I say that the class \nFueldCardTransaction  \nis a value object, I mean that its identity is only defined by its values, that it is immu-table, that it should be without any side effects, and that it should be transferable. \nDesigning Custom Annotations 111\nIt is therefore natural to declare explicitly these patterns as a simple way to do \ndocumentation.\nY ou could introduce a set of custom annotations like the following into a project:\n •@ValueObject\n •@Entity  or @DomainEntity  (to prevent any ambiguity with the annotations \nof similar names from all the technical frameworks)\n •@DomainService\n •@DomainEvent\nAnd you can declare the consequences explicitly by using the properties.Each category of class comes with predefined properties. For example, a value \nobject should have identity by value, should be immutable, and should be side effect free. Y ou can easily make this explicit in a system of annotations by using annota-tions on annotations, as shown here:\n1  @Immutable\n2  @SideEffectFree3  @IdentityByValue4  public @interface ValueObject { 5  ...\nWhen you mark a class as being a value object, you indirectly mark it with the \nmeta annotations as well. This is a convenient way to group properties that go \ntogether into bundles, to declare them all with only one equivalent declaration. Of course, a bundle should have a clear name and meaning; it should not be just a random bunch of properties together.\nThis approach enables additional enforcement of the design and architecture. \nFor example \n@DomainEntity , @DomainService , and @DomainEvent  imply being \npart of a domain model and perhaps related restrictions on the allowed dependen-cies, which can all be enforced with static analysis.\nAs described later in this chapter, you can put annotations on packages in Java \nso that a declaration in one place collectively marks every element of the package. Y ou can take advantage of this in an “unless specified otherwise” fashion. For exam-ple, you could define a custom annotation named \n@FunctionalFirst , meant to \nbe put on whole packages, which would mean @Immutable  and @SideEffect-\nFree  by default for every type, unless something else were stated explicitly on a \nparticular type."",4387",4472,"**Stereotypes and Tactical Patterns**\n\n**Desirable Properties in Code Design**:\n- Not null: for parameters that cannot be null\n- Positive: for parameters that must be positive\n- Immutable: for classes that remain unchanged\n- Identity by value: where equality is defined as the equality of data\n- Pure: for functions or classes to avoid side effects\n- Idempotent: for a function that has the same effect when called multiple times\n- Associative: for functions like (a + b) + c = a + (b + c)\n\n**Expressing Properties in Code**:\n- Use comments, custom annotations, and automated tests to express properties\n- Use Scala case classes as shorthand for immutable and identity by value\n- When not possible, use domain-driven design categories like value object, entity, domain service, and domain event\n\n**Custom Annotations**:\n- Introduce custom annotations like @ValueObject, @Entity, @DomainService, and @DomainEvent to express patterns\n- Declare consequences explicitly using properties (e.g., immutable, side effect free)\n- Group related properties into bundles with a clear name and meaning\n\n**Implications of Custom Annotations**:\n- Enable enforcement of design and architecture, such as restricting dependencies for @DomainEntity, @DomainService, and @DomainEvent\n- Apply annotations to packages to mark all elements within them (e.g., @FunctionalFirst)",1353,36.820444107055664
False,3,"57-Annotation by Default or Unless Necessary.pdf,57-Annotation by Default or Unless Necessary,""Chapter 4 Knowledge Augmentation 112\nThere are many ot","57-Annotation by Default or Unless Necessary.pdf,57-Annotation by Default or Unless Necessary,""Chapter 4 Knowledge Augmentation 112\nThere are many other catalogs of patterns and stereotypes of interest to express \nefficiently a lot of design and modeling knowledge. They provide ready-made knowl-\nedge and vocabulary related to your job as a developer, about design, and about modeling and solving infrastructure problems. But you can go further and extend the standard categories into finer-grained categories.\nFor example, it is possible to refine the kind of value object. Martin Fowler wrote \nabout the quantity pattern, the null object pattern, the special case pattern, and the range pattern, which are all specialized cases of value objects. In addition, the money pattern is a special case of the quantity pattern. Y ou can use all these patterns, choos-ing the most specific one possible. For example, you could chose range over just value object if it applies, as it is common knowledge that a range is a value object. If you \ndid this, you could make it explicit that a range is a special case of a value object with an annotation on the annotation:\n1  @ValueObject\n2  public @interface Range { 3  ...\nY ou can also create your own variants. In one project, I had a lot of value objects, \nbut they were more than that. They were also instances of the policy pattern, the \ndomain pattern equivalent of the strategy pattern. More importantly , in the business domain of finance, we would usually call them standard market conventions. So I created my own \n@Convention  annotation and made it clear that it was at the same \ntime a value object and a policy:\n1  @ValueObject\n2  @Policy3  public @interface Convention { 4  ...\nUsing Meaningful Annotation Package Names\nWhen you create a custom annotation, you have to choose its package name. Y ou can \nchoose for a package name to have a particular meaning. I like to encode a reference place for an idea in the package name. When the annotation is drawn from a book, for example, I might use the book name or an abbreviation of the book name or authors, such as com.acme.annotation.gof for a book by the Gang of Four, com.\nacme.annotation.poeaa for the book Patterns of  Enterprise Application Architecture, \nand com.acme.annotation.ddd for the book Domain-Driven Design. For standard \nknowledge with no one golden book, I might name the package after the field (for example, com.acme.annotation.algebra).\nDesigning Custom Annotations 113\nHijacking Standard Annotations\nMany frameworks in the Java world use annotations as a form of configuration. For \nexample, JPA (Java Persistence API) and Spring Framework offer the infamous choice between XML and annotations. Even though I advocate using annotations for documentation purposes, I am not a big fan of using annotations as an alternative to writing code. I prefer the approach found in some .Net projects, such as Fluent  NHibernate, of using plain code to define the object-to-relational mappings.\nHowever, in Java at this time, you still have to use annotations, unless you prefer \nXML (which I do not). When you use annotations to drive the behavior of frame-works, the annotations are code indeed, and since most of them relate to infrastruc-ture concerns such as persistence or web service, they often have the annoying habit of polluting domain classes with non-domain noise.\nAside from my little rant, you probably wonder whether these standard anno-\ntations have any documentation value. Because they are code, at a minimum, the annotations document what they are doing—just like well-designed code. They tell the what.\nLet’s consider a few examples of particular documentation interest:\n •Stereotyping annotations (Spring): This set of annotations includes  \n@Service , @Repository , and @Controller . They are used to stereotype \nclasses, and you can declare them for registration into the dependency injec-tion mechanism. In fact, they alias the \n@Component  annotation with more \nmeaning, which is a nice way to hijack these noisy annotations for something more meaningful for humans rather than just for Spring.\n •Creating custom stereotypes (Spring): This approach also supports your own custom annotations, provided that you annotate them with the \n@Component  \nmeta annotation.\n •@Transactional  (Spring): The @Transactional  annotation is used to \ndeclare transactional boundaries and rules, typically on a service. If you have a hexagonal architecture, the transactional services should be your applica-tion services in their own thin layer on top of the domain model. Y ou could thus decide that this Spring annotation in itself also means \n@Application-\nService in the DDD sense. Because most Spring annotations are also meta \nannotations, you could actually define your own @ApplicationService \nannotation and mark it as @Transactional in order to express your intent in \na way that Spring can recognize for doing its magic, too.\nChapter 4 Knowledge Augmentation 114\n •@Inheritance  (JPA): The @Inheritance annotation and its friends can be \nused to directly document design decisions on how to do the mapping between \na class hierarchy and a corresponding database schema. This directly relates to the corresponding patterns from Martin Fowler’s book Patterns of  Enterprise Application Architecture. For example, \n@Inheritance(strategy=JOINED)  \ncorresponds to the single-table inheritance pattern2 (but, unfortunately , under \nanother name).\n •RESTful web service (JAX-RS annotations): This set of annotations is clearly declarative: \n@Path  identifies the URI path, @GET  declares the GET \nrequest method, and @Produces  defines the media type as a parameter. The \nresulting code is self-documented to a large extent. Furthermore, tools like Swagger can exploit these annotations to generate living documentation of the API.\nIt is possible to rely on the standard annotations for their particular documenta-\ntion value, but this is almost always limited to technical concerns, where the annota-tion is just like particularly declarative code in that it tells the what but not the why. \nAs mentioned earlier, it is sometimes possible to extend the standard mechanism to convey additional meaning while still playing nicely with the frameworks you depend on.\nStandard Annotation: @Aspect  and Aspect-Oriented Programming\nThe Spring Pet Clinic demonstrates aspect-oriented programming (AOP) by showing how to set up a simple aspect that monitors call count and call invocation time for \nevery repository.\n3\nIt is interesting here that the requirement “to monitor every repository” is \ndescribed literally in the aspect declaration, as shown in the following excerpt anno-\ntated with the @Aspect  annotation from Spring AOP . \n1  @Aspect\n2  public class CallMonitoringAspect { 3    ...4    @Around(""""within(@org.springframework.stereotype.Repository *)"""")\n2. Martin Fowler, “Single Table Inheritance,” ThoughtWorks, http://martinfowler.com/eaaCatalog/ \nsingleTableInheritance.html\n3. https://github.com/spring-petclinic/spring-framework-petclinic/blob/master/src/main/java/org/spring-\nframework/samples/petclinic/util/CallMonitoringAspect.java"",7151",7254,"**Patterns and Stereotypes for Expressive Design Knowledge**\n\n**Catalogs of Patterns and Stereotypes**:\n- Provide ready-made knowledge and vocabulary related to design, modeling, and infrastructure problems\n- Can be extended into finer-grained categories\n\n**Examples of Specialized Value Object Patterns**:\n- **Quantity pattern**, **null object pattern**, **special case pattern**, **range pattern**, **money pattern**\n    - Choosing the most specific pattern that applies\n    - Examples: range, value object, policy pattern (value object and policy)\n\n**Creating Custom Annotations**:\n- **Convention annotation** as a custom annotation for value objects and policies\n- Encoding meaning in package name (e.g., com.acme.annotation.gof, com.acme.annotation.poeaa, etc.)\n\n**Hijacking Standard Annotations**:\n- Frameworks like Spring and JPA use annotations as configuration\n- Not a fan of using annotations as alternatives to XML\n- In Java, still have to use annotations unless preferring XML\n- **Stereotyping annotations (Spring)**, **creating custom stereotypes (Spring)**, **@Transactional (Spring)**, **@Inheritance (JPA)**\n    - Document design decisions and relate to patterns from Enterprise Application Architecture\n    - Can be used to express intent for frameworks\n- **RESTful web service (JAX-RS annotations)**: Declarative, self-documenting\n\n**Standard Annotation: @Aspect and Aspect-Oriented Programming**:\n- **CallMonitoringAspect** in Spring Pet Clinic demonstrates aspect-oriented programming\n    - Description of the requirement ""to monitor every repository"" is literally declared in the aspect",1607,54.600566148757935
False,3,"58-Module-Wide Augmentation In Practice.pdf,58-Module-Wide Augmentation In Practice,""Handling Module-Wide Knowledge 115\n5    public Object invoke(Pro","58-Module-Wide Augmentation In Practice.pdf,58-Module-Wide Augmentation In Practice,""Handling Module-Wide Knowledge 115\n5    public Object invoke(ProceedingJoinPoint joinPoint) throws \nThrowable{ 6      ...7    }8  ...9  }\nThis expressiveness is possible because the code has been augmented with the \nmeaningful @Repository  stereotype. This illustrates perfectly how augmenting \nthe code with explicit design decisions makes it possible to talk to tools the way we \nhumans think.\nAnnotation by Default or Unless Necessary\nWhen designing custom annotations to express properties, you have a choice of cre-ating an annotation for the case when the property is met or when it’s not met:\n •@Immutable  or @Mutable\n •@NonNull  or @Nullable\n •@SideEffectFree  or @SideEffect\nY ou may create both and let individuals decide which one to choose, but that \nmight create inconsistencies, in which case the annotation means nothing at all.\nY ou may decide on an alternative that you want to promote so that having the \nannotation in many places becomes a marketing campaign; for example, using \n@NonNull  everywhere will encourage making everything non-null. No annotation \nthen suggests nullability .\nOn the other hand, you might consider that annotations are noise and think that \nthe fewer annotations the better. In this case, the default and preferred choice should be no annotation, and you would use an annotation only to declare a deviation from the default. If the team preference is to have every class immutable by default, you would annotate mutable classes because you want your colleagues to notice: “Oh, this class is exceptionally \n@Mutable !”\nHandling Module-Wide Knowledge\nIn a software project, a module contains a set of artifacts (essentially packages, classes, and nested modules) that can be manipulated together. Y ou can define prop-erties that apply to all the elements a module contains. Design properties and quality \nChapter 4 Knowledge Augmentation 116\nattribute requirements (for example, being read only , serializable, stateless) often \napply on a whole module, not just on distinct elements within the module.\nY ou can also define the primary programming paradigm at the module level: \nobject-oriented, functional, or even procedural or reporting style.\nA module is also ideal for declaring architecture constraints. For example, you \ncould have distinct areas for code written from scratch with high-quality standard and for legacy code with more tolerant standards. In each module, you could define preferences of style such as Checkstyle configuration, metrics thresholds, unit test coverage, and allowed or forbidden imports.\nTherefore: When a piece of knowledge spans a number of artifacts equally \nwithin a module, you should put this knowledge at the module level directly, so that it applies to all the contained elements.\nThis approach can also be applied to all elements satisfying a given predicate, \nas long as you can find a home for this declaration, such as the pointcuts in aspect-oriented programming.\nDealing with Many Kinds of Modules\nPackages are the most obvious modules in Java and in other languages. But a pack-age x.y .z actually defines more than one module: the module of its direct members (\nx.y.z.* ) and the module that also include every artifact included in its subpack-\nages (x.y.z.** ). Similarly , a class also represents a “module” for its member fields, \nmethods, and nested classes—for example, x.y.z.A#  and x.y.z.$ .\nThe “working sets” in an IDE like Eclipse also define another logical grouping, \nsimilar to modules, as simple collections of classes and other resources. T ools like Ant also define filesets using lists of files and regular expressions—for example, \n{x.y.z.A, x.y.z.B, x.y.*.A} . Like modules, working sets and filesets are usu-\nally named for easy reference.\nSource folders (for example, src/main/java or src/test/java) obviously define \ncoarse-grained groupings of elements. Maven modules define bigger modules, at the scale of subprojects. Pointcuts of aspect-oriented programming also defines logical groupings elements across various “real” modules.\nInheritance and implementation implicitly define modules, too, such as “every \nsubclass of a class or implementation of an interface” as \nx.y.z.A+ , and if it includes \nevery member of every nested member, it is x.y.z.A++ .\nA stereotype implicitly defines the set of its occurrences. For example, the value \nobject pattern implicitly defines the logical set of every class that is a value object.\nCollaboration patterns such as Model–View–Controller (MVC) and knowledge \nlevel also imply logical groupings such as the model part of MVC or each level of the \nknowledge level pattern (knowledge level or operational level)."",4726",4817,"**Module-Wide Augmentation in Practice**\n\n**Code Expressiveness**:\n- Code augmented with meaningful @Repository stereotype allows for better communication between humans and tools\n- Custom annotations can express properties such as:\n  - **@Immutable/@Mutable**, **@NonNull/@Nullable**, **@SideEffectFree/@SideEffect**\n- Default or preferred choice of annotation should be clear to avoid inconsistencies\n\n**Handling Module-Wide Knowledge**:\n- A module contains a set of artifacts (packages, classes, nested modules) that can be manipulated together\n- Design properties and quality attributes often apply to entire modules\n- Primary programming paradigm can be defined at the module level\n- Architecture constraints can be declared at the module level\n- When a piece of knowledge spans multiple artifacts within a module, it should be placed at the module level\n- Dealing with multiple kinds of modules:\n  - Packages define more than one module (direct members and included subpackages)\n  - Classes represent ""modules"" for their member fields, methods, and nested classes\n  - Tools like Eclipse, Ant, Maven, pointcuts in aspect-oriented programming define logical groupings of elements\n  - Inheritance and implementation implicitly define modules (subclasses, implementations)\n  - Stereotypes implicitly define the set of their occurrences\n  - Collaboration patterns like Model–View–Controller (MVC) and knowledge level patterns imply logical groupings",1450,38.15754246711731
False,3,"59-Intrinsic Knowledge Augmentation.pdf,59-Intrinsic Knowledge Augmentation,""Intrinsic Knowledge Augmentation 117\nDesign patterns also define logical","59-Intrinsic Knowledge Augmentation.pdf,59-Intrinsic Knowledge Augmentation,""Intrinsic Knowledge Augmentation 117\nDesign patterns also define logical groupings by the role played within the \npattern (for example, “Every abstract role in the Abstract Factory pattern” would be \n@AbstractFactory.Abstract.* ).\nThere are many other modules or quasi-modules implied by concepts like layers, \ndomains, bounded contexts, and aggregate roots.\nThe problem with large modules is that they contain huge numbers of items, \nwhich often necessitates aggressive filtering and may even require ranking to con-\nsider only the N most important elements out of many more.\nModule-Wide Augmentation In Practice\nAll the techniques to augment code with additional knowledge apply for module-wide knowledge: annotations, naming conventions, sidecar files, metadata database, and DSL.\nA common way to add documentation to a Java package is to use a special class \nnamed package-info.java as a location for the Javadoc and any annotation about the package. Note that this special pseudo-class with a magic name is actually an exam-ple of a sidecar file.\nC# modules often contain projects, which can have assembly information \ndescriptions:\n1  AssemblyInfoDescription(""""package comment"""")\nIn most programming languages, package or namespace naming conventions can \nalso be used to declare design decisions. For example, something.domain can be used to mark a package or namespace as a domain model.\nIntrinsic Knowledge Augmentation\n Caution\nThis section is more abstract than most others. The concepts discussed here are important but subtle. If abstract nonsense is definitely not your thing, you can safely skip this section and perhaps come back to it later.\n \nIt is important to make a distinction between what things really are for themselves \nand what they are for something else or for a purpose. A car may be red, may be a coupe, or may have a hybrid engine. These properties are really intrinsic to the car; \nChapter 4 Knowledge Augmentation 118\nthey are part of its identity . In contrast, the owner of the car, the car’s location at a \npoint in time, or the car’s role in a company fleet are extrinsic to the car. This extrin-\nsic knowledge is not really about the car in itself but about a relationship between the car and something else. As a consequence, it can change for many reasons other than the car itself. Thinking about intrinsic versus extrinsic knowledge has many benefits for design and for documentation.\nIf only intrinsic knowledge is attached to an element, the following would happen:\n •If you were to delete the element, the attached knowledge would go away with it, without regret and without modification anywhere else. For example, when the car is recycled, its serial number is crunched at the same time, and this is okay .\n •Any change that it is not intrinsically about the element would not modify the element or its artifacts at all. For example, selling the car would not modify its user manual.\nUnderstanding the Importance of Extrinsic Properties\nI first learned about the notion of intrinsic versus extrinsic in the Gang of Four book Design Patterns. The chapter that introduces the lightweight pattern considers a glyph used in a word processor. Each letter in the text is printed on the screen as a glyph, the rendered image of a character. A glyph has a size and style attributes such as italic or bold. A glyph also has an (x, y) position on the page. The core idea behind the lightweight pattern is to exploit the difference between intrinsic properties of the glyph (for example, its size, its style) and its extrinsic properties (for example, its position on the page) to enable reuse of the same instance of the glyph many times on the page.\nThis explanation has had a big influence on the way I design. It is a secret \ningredient in improving the long-term relevance of design decisions. \nTherefore: Only annotate elements with knowledge that is intrinsic to them. Conversely, consider attaching all intrinsic knowledge to the element itself. Avoid attaching knowledge that is extrinsic, as it will change often and for rea-sons unrelated to the element. A focus on intrinsic knowledge will reduce the maintenance efforts of the documentation over time."",4234",4319,"**Intrinsic vs Extrinsic Knowledge**\n\n**Concepts**:\n- Design patterns define logical groupings by role within the pattern (e.g., ""Every abstract role in the Abstract Factory pattern"")\n- Layers, domains, bounded contexts, and aggregate roots imply other modules or quasi-modules\n- Large modules contain many items, necessitating aggressive filtering and ranking to consider only the most important elements\n\n**Module-Wide Augmentation**:\n- Techniques for augmenting code with additional knowledge apply to module-wide knowledge: annotations, naming conventions, sidecar files, metadata database, and DSL\n- Java's ""package-info.java"" file can be used to add documentation and annotations about a package\n- C# modules contain assembly information descriptions (e.g., 1  AssemblyInfoDescription(""package comment""))\n- Namespace/package naming conventions can declare design decisions (e.g., something.domain)\n\n**Intrinsic vs Extrinsic Knowledge**:\n- **Intrinsic knowledge**: Properties that are part of the identity of an object, not related to any purpose or relationship (e.g., car's color, type)\n- **Extrinsic knowledge**: Knowledge about the relationship between an object and something else, subject to change for reasons unrelated to the object itself (e.g., owner, location, role in a company fleet)\n- Benefits of distinguishing intrinsic vs extrinsic:\n  - If only intrinsic knowledge is attached, deleting the element will not affect the attached knowledge\n  - Any changes not intrinsic to the element will not modify it or its artifacts\n\n**Extrinsic Properties in Design**:\n- Intrinsic versus extrinsic properties of a glyph (e.g., size, style) vs. position on the page allow for reuse and improved maintenance\n- Focusing on intrinsic knowledge reduces long-term documentation maintenance efforts",1800,48.397090911865234
False,3,"60-Machine-Accessible Documentation.pdf,60-Machine-Accessible Documentation,""Machine-Accessible Documentation 119\nKey Point\nY ou may think of this a","60-Machine-Accessible Documentation.pdf,60-Machine-Accessible Documentation,""Machine-Accessible Documentation 119\nKey Point\nY ou may think of this attention to intrinsic knowledge as a matter of more \nor less judicious coupling. The key question is “How would my declared knowledge have to evolve when I change the element?” The best approach is the one that requires you to do the least work when the element changes. \nThe common use of annotations by popular frameworks regularly does not con-\nsider whether they’re really intrinsic to the thing annotated. For example, say that you have a class that exists in itself and that can be used independently , but then you put annotations on it to declare how it is supposed to be mapped to the database or to declare that it is the default implementation for some interface. If you consider this class to really represent a domain responsibility , then this database mapping is an unrelated concern; having it attached only makes the class more likely to change for database reasons, too.\nImagine that you have a \nCatalogDAO  interface with two implementations: \nMongoDBCatalogDAO  and PostgresCatalogDAO . Marking the MongoDBCata-\nlogDAO  class as the default implementation of the CatalogDAO  interface would be \nan example of forcing an extrinsic concern on the class. A better alternative would be to annotate each \nDAO with an intrinsic attribute like @MongoDB  or @Postgres  and \nseparately make the selection indirectly via this intermediate attribute. For exam-ple, you could mark all \nMongoDBDAO  implementations with the @MongoDB  annota-\ntion and all PostgresDAO  implementations with the @Postgres  annotation. This \nis intrinsic knowledge with respect to the DAO. Separately , you could decide to inject \nevery implementation for the technology chosen for a particular deployment. If you deploy with Postgres, we want to inject every \n@Postgres  implementation. This \ndecision to inject one selected technology is knowledge, too, but the DAO hierarchy \nshouldn’t have to know .\nMachine-Accessible Documentation\nY ou code at the design level, not just the code level, but your tools cannot help you much at the design level. They cannot help because they have no idea, based on the code alone, what you are doing from a design perspective. If you make your design explicit, such as by using annotations attached to the code, then tools can begin to manipulate the code at the design level, too, and thus help you more.\nChapter 4 Knowledge Augmentation 120\nDesign knowledge that can make the code more explicit is worth adding. An anno-\ntation attached to the language element is often enough. For example, you can declare \nthe layers on each top-level package in the corresponding package-info.java file:\n1  @Layer(LayerType.INFRASTRUCTURE)\n2  package com.example.infrastructure;\nBy putting the annotation @Layer  on the package com.example.infrastructure, \nyou declare a particular instance of the layer pattern, where the layer is the package \nitself.\nAs usual, there are many options for designing a custom annotation, such as \ndeclaring an ID (which may be useful for referencing it later):\n1  @Layer(id = """"repositories"""")\n2  package com.example.domain;\nWith this design intent made explicit in the code itself, tools such as a dependency \nchecker could automatically derive forbidden dependencies between layers to detect \nwhen they are violated.\nY ou could do this with tools like JDpend, but you’d have to declare each package-\nto-package dependency restriction. This it tedious and does not directly describe the layering; it describes just the consequence of the layering.\nDeclaring every forbidden or acceptable package-to-package dependency is \ntedious, but imagine doing it between classes: It’s prohibitive! However, if classes are tagged—for example, as \n@ValueObject , @Entity , or @DomainService — \ndependency checkers can enforce your favorite dependency restrictions. For example, I like to enforce the following rules:\n •Value objects should never depend on anything other than other value objects.\n •Entities should never have any service instance as a member field.\nOnce the classes are augmented with these stereotypes explicitly , you can more \nliterally and more concisely tell the tools what you want.\nLiterate Programming\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us con-centrate rather on explaining to human beings what we want a computer to do.. \n—Donald Knuth4\n4. Donald Knuth, http://www .literateprogramming.com\nMachine-Accessible Documentation 121\nIt is hard not to mention literate programming in a book on living documen-\ntation. Literate programming is an approach to programming introduced by \nDonald Knuth. A literate program explains program logic in a natural lan-guage, such as English, along with snippets of macros and traditional source code. A tool processes the program, producing both a document for humans and source code that can be compiled, becoming an executable program.\nAlthough literate programming never became widely popular, it had a pro-\nfound and widespread influence on the industry , even if the idea was often distorted.\nLiterate programming introduced several important ideas:\n •Documentation interleaved with the code, in the same artifacts, with code inserted within the prose of the documentation: This should not be confused with documentation generation, where the documentation is extracted from comments inserted into the source code.\n •Documentation following the flow of thoughts of the programmer, as opposed to being constrained by the compiler-imposed order: Good documentation follows the order of the human logic.\n •A programming paradigm encouraging programmers to think deliberately about each decision: Literate programming goes well beyond documenta-tion: It is meant to force programmers to think deliberately , as they have to explicitly state their thoughts behind the program.\nKeep in mind that literate programming is not a way to do documentation \nbut a way to write programs.\nEven though its use is not widespread, literate programming is still alive today , \nwith tools available for all good programming languages, including Haskell, \nClojure, and F#. The focus now is on writing prose in Markdown, with snip-pets of programming language inserted. In Clojure you use Marginalia,\n5 in \nCoffeeScript you use Docco,6 and in F# you use T omas Petricek’s FSharp.\nFormatting.7\nT raditionally , documentation of a software program has involved a mix of \ncode and prose, which can be combined in several ways:\n •Code in prose: This is the method of literate programming as originally proposed by Donald Knuth. The primary document is prose that follows the human logic of the programmer. The author-programmer has full con-trol of the narration.\n5. Marginalia, https://github.com/gdeer81/marginalia\n6. Docco, http://jashkenas.github.io/docco/7. FSharp.Formatting, https://github.com/tpetricek/FSharp.Formatting"",7061",7146,"**Machine-Accessible Documentation**\n\n**Key Point**: Attention to intrinsic knowledge involves considering how declared knowledge would evolve when an element changes, choosing the approach that requires the least work.\n\n**Intrinsic vs. Extrinsic Concerns in Annotations**:\n- Annotations attached to classes or packages can declare design intentions and make the code more explicit, allowing tools to help at the design level.\n  - Example: Declaring a package as a particular layer type using an annotation.\n- Custom annotations can be used to document design intent, such as forbidding certain dependencies between classes or layers.\n\n**Literate Programming**:\n- Approach introduced by Donald Knuth that explains program logic in natural language with code snippets.\n- Tools process the program, generating both human-readable documentation and executable source code.\n- Introduced important ideas: interleaving code and documentation, following the flow of human thought, and encouraging deliberate programming.\n\n**Documentation Formatting**:\n- Traditionally, documentation involves a mix of code and prose that can be combined in various ways.\n- Examples include code-in-prose (literate programming), code commenting, and documentation generators.",1250,36.15510606765747
False,3,"61-Making the Rationale Explicit.pdf,61-Making the Rationale Explicit,""Chapter 4 Knowledge Augmentation 122\n •Prose in code: This is the documentatio","61-Making the Rationale Explicit.pdf,61-Making the Rationale Explicit,""Chapter 4 Knowledge Augmentation 122\n •Prose in code: This is the documentation generation approach offered by \nmost programming languages; Javadoc is an example of a tool that creates prose in code.\n •Separate code and prose, merged into one document by a tool: T ools are used to perform a merge in order to publish a document, such as a tutorial.\n •Code and prose as the same thing: In this approach, the programming language is so clear that it can be read as prose itself. Unfortunately , this Holy Grail is never reached, but some programming languages get closer than others. I’ve seen some F# code by Scott Waschlin that is impressively close to this ideal.\nSome tools, like Dexy ,\n8 offer a choice of how to organize the code and the \nprose with each other.\nRecording Your Rationale\nIn the book 97 Things Every Software Architect Should Know, Timothy High is \nquoted as saying, “ As explained in the axiom ‘ Architectural T radeoffs’, the definition of a software architecture is all about choosing the right tradeoffs between various quality attributes, cost, time, and other factors.” Replace the word architecture with \ndesign, or even with code, and the sentence still holds.\nThere are trade-offs everywhere in software, whenever a decision is being made. \nIf you believe you’re not making any trade-off, it just means the trade-off is out of sight.\nDecisions belong to stories. Humans love stories and tend to remember them. \nIt is important to preserve the context of a decision. The context of a past deci-sion is necessary to reevaluate that decisions in a new context. Past decisions are tools that can help learn from the thinking of the predecessors. Many decisions are also more compact to describe than their consequences and hence are easier to transfer from one brain to another than all the details that result from a decision. If you tell me your intent and the context shortly , provided that I’m a skilled pro-fessional, I may come up with the same decisions that you’ve made. Without the intent and context, however, you are left to wonder “What were they thinking?” (see Figure 4.3). \n8. Dexy , https://github.com/dexy/dexy\nRecording Y our Rationale 123\nTherefore: Record the rationale for each important decision in some form of \npersistent documentation. Include the context and the main alternatives. And listen to the documentation: If you find it hard to formalize the rationale and the alternatives, then it may be that the decision was not as deliberate as it should have been. You may be programming by coincidence!\nWhat’s in a Rationale?\nAny decision happens in a context and is one of the considered answers to a  problem. Therefore, a rationale is not only the reason behind a chosen decision, it is also all of the following:\n •The context at the time: The context includes the main stakes and concerns, for example the current load (“Only 1000 end users using the application once a week”) or the current priority (“Priority is exploring the market-product fit as quickly as possible”) or an assumption (“This is not expected to change”) or a people consideration (“The development teams don’t want to learn JavaScript”).\n •The problem or requirement behind the choice: Examples of problems are “The page must load in less than 800ms to not lose visitors” and “Decommis-sion the VB6 module.”\n •The decision itself instead of the chosen solution, with the main reason or reasons: Examples of decisions and reasons are “The ubiquitous lan-guage is expressed with English words only , as it’s simpler and every current What were\nthey thinking?\nFigure 4.3 What were they thinking?"",3645",3722,"**Documentation Approaches:**\n- **Prose in code**: Offered by most programming languages; Javadoc is an example of a tool that creates prose in code\n- **Separate code and prose, merged into one document by a tool**: Tools used to merge code and prose for publication, such as tutorials\n- **Code and prose as the same thing**: Some programming languages are clear enough to be read as prose; F# is an example of this approach\n- **Choices in organization**: Tools like Dexy offer choices on how to organize code and prose\n\n**Recording Your Rationale:**\n- Recording rationale for important decisions: Necessary to reevaluate past decisions in new contexts, preserve context, and help learn from predecessors\n- Documentation should include the context, main alternatives, and rationale behind chosen decision\n- If formalizing rationale is difficult, it may indicate that the decision was not deliberate\n\n**Content of a Rationale:**\n- Context: Includes current stakes and concerns (load, priority, assumptions, people considerations)\n- Problem or requirement behind the choice: Examples include performance requirements and decommissioning modules\n- Decision itself instead of chosen solution, with main reasons: Examples include language choices and design decisions.",1263,34.09717631340027
False,3,"62-Avoid Documenting Speculation.pdf,62-Avoid Documenting Speculation,""Chapter 4 Knowledge Augmentation 124\nstakeholder prefers it that way” and “Thi","62-Avoid Documenting Speculation.pdf,62-Avoid Documenting Speculation,""Chapter 4 Knowledge Augmentation 124\nstakeholder prefers it that way” and “This facade exposes the legacy system \nthrough a pretty API because there is no good reason to rewrite the legacy but we still want to consume it with the same convenience as if it were brand new .”\n •The main alternatives that were considered seriously and perhaps why they were not selected or why they would be selected if the context were  different: Examples of alternative are “Buying an off-the-shelves solution would be a better choice if the needs were more standard,” “ A graph structure would be more powerful but is harder to map with the Excel spreadsheets of the users,” and “ A NoSQL datastore would be a better choice if we didn’t have all this investment with our current Oracle database.”\nAs @CarloPescio suggested in a conversation on self-documenting code, gener-\nally speaking, design rationale is very much about discarded options and so is not typically in the code.\nMaking the Rationale Explicit\nY ou can record the rationale behind important decisions in a number of ways:\n •Ad hoc document: Y ou need an explicit document about the requirements, including all quality attributes. It needs to evolve slowly but still at least once a year; such a document is needed only for the main attributes that span large areas of the system, not for more local decisions. Chapter 12, “Living Architec-ture Documentation,” describes a decision log as an example of that approach for architectural matters.\n •Annotations: Annotations that document a decision could have fields to record the rationale: \n@MyAnnotation(rationale =""""We only know how \nto do that way"""") .\n •Blog post: A blog post takes more time to write than annotations or even an ad hoc document, and a good writing style is really helpful. However, you pro-vides a human account of the reasoning and the human context behind a deci-sion, even with the politics and personal agenda mentioned between the lines, and this makes it valuable. A blog post may also be searched and scanned when a question arises on a past decision.\nBeyond Documentation: Motivated Design\nRecording the rationale is not just for future generations or your future self; it is also useful right now , at the time you do it. Y ou need to listen to what’s difficult as a signal"",2315",2394,"**Avoid Documenting Speculation**\n\n**Importance of Explicit Rationale**:\n- Documenting design rationale is crucial for:\n    - Stakeholders\n    - Future developers\n    - Decision-making process\n\n**Ways to Make the Rationale Explicit**:\n1. **Ad hoc document**:\n   - Evolve slowly, at least once a year\n   - Record decisions related to major system attributes\n   - See ""Living Architecture Documentation"" in Chapter 12 for an example\n2. **Annotations**:\n   - Document decisions with rationale using annotations\n   - Example: `@MyAnnotation(rationale = ""We only know how to do it that way"")`\n3. **Blog post**:\n   - Time-consuming but provides a human account of reasoning and context\n   - Valuable for future reference, even with politics and personal agendas present",763,22.32579255104065
False,3,"63-Recording the Rationale as an Enabler for Change.pdf,63-Recording the Rationale as an Enabler for Change,""Recording Y our Rationale 125\nthat somet","63-Recording the Rationale as an Enabler for Change.pdf,63-Recording the Rationale as an Enabler for Change,""Recording Y our Rationale 125\nthat something could be improved. If it is hard to come up with a rationale, or a con-\ntext, perhaps the decision was not thought about seriously enough, and this should be an alert.\nIf it is hard to come up with two or three credible alternatives to a decision, \nthen perhaps the first solution that fit was chosen, without any work being done to explore simpler or better solutions. Y our current decision might not be optimal, and it can have consequences as lost opportunities in the future. Of course, one rationale may be “First solution that fit was chosen to go to market as quickly as possible,” but at least this decision is deliberate, and those involved understand the consequence and can be ready to reconsider it next time.\nIn the absence of deliberate design decisions, and with a complete lack of skills, \nyou will just have a random software structure. Y ou just end up with a pile of details, and the only way to deal with it is to guess the intentions involved. This is typically the issues you have to deal with in legacy code, which we discuss in detail in  Chapter 14, “Documenting Legacy Applications.” My point is that the focus on making a rationale explicit helps make better decisions and better software.\nAvoid Documenting Speculation\nIn the book Building Microservices, Sam Newman advises against documenting solutions to speculative needs. He paints a critical picture of traditional architecture documentation that explains with many pages and many diagrams how the perfect system will be but ignoring completely any unexpected future impediment when it comes to actually build it and make it work.\nIn contrast, rationales are decisions taken on actual needs that have been proven \nto be necessary . In incremental approaches, such as emerging design, we grow the solution slice by slice, and each slice is driven by the most important need at each instant. We often work in a just-in-time fashion, precisely because it is an antidote to speculation: We build it just when it becomes necessary to be built.\nOverall, you should document only what has been built, in response to actual \nneeds.\nSkills as Pre-Documented Rationales\nThe thinking process for many small decisions has already been solved and docu-mented in the literature. For example, the single responsibility principle says to split a class that does two things into two classes that do one thing each. There is no need to document each occurrence of a particular happening, but you may document once, in a single place, each of the principles you consistently follow; I call this the acknowledge your influences pattern and describe it a little later in this chapter."",2706",2821,"**Recording Rationale as an Enabler for Change**\n\n**Importance of Recording Rationale**:\n- Helps make better decisions and better software\n- Avoids ""random software structure"" and ""legacy code"" issues\n\n**Avoid Documenting Speculation**:\n- Don't document solutions to speculative needs (Newman in ""Building Microservices"")\n- Document only what has been built, in response to actual needs\n\n**Skills as Pre-Documented Rationales**:\n- Some decision-making processes are already documented and can be referenced\n- Examples: \n  - **Single Responsibility Principle**: Split a class into two that do one thing each\n  - **Acknowledge Your Influences Pattern**: Document the principles you consistently follow",699,22.066384077072144
False,3,"64-Commit Guidelines.pdf,64-Commit Guidelines,""Chapter 4 Knowledge Augmentation 126\nRecording the Rationale as an Enabler for Change\nKnowing all the","64-Commit Guidelines.pdf,64-Commit Guidelines,""Chapter 4 Knowledge Augmentation 126\nRecording the Rationale as an Enabler for Change\nKnowing all the reasons behind past decisions can enable you to more successfully \nmake changes because you can either respect or reject each of those decisions delib-erately . The best way to know about those decisions in a reliable way is to have them recorded; otherwise, the reasoning will be forgotten (see Figure 4.4). Without the explicit rationale behind each past decision, you might wonder if a change has unex-pected impacts with respect to a concern you don’t have in mind. Without knowing about past decisions, you may never be sure enough to decide to change, and the sta-tus quo will dominate, even though the opportunity to improve is there in front of your eyes. Alternatively , you may actually cause harm inadvertently if you make a change that triggers a forgotten concern that you cannot see because it was not recorded. \nAcknowledging Your Influences (aka Project \nBibliography)\nGood books care about their bibliography . For the reader, it’s a way to learn more, but \nit’s also a way to check the influences of the author. When a word has different mean-ings, looking at the bibliography helps find out how to interpret it. Read the book! \n—Eric Evans, Domain-Driven DesignWe eventually\ndecided to go\nthrough door C.AB CWhy? Next\ntime I‛ll try door\nA or B…\nFigure 4.4 Without the why, they will make the same mistake again\nAcknowledging Y our Influences (aka Project Bibliography) 127\nThe mindset of the team that worked on a project is stable knowledge that’s worth \nmaking explicit for future developers. It doesn’t require long sentences; you can just list your bibliography and list the main points of your style.\nA project bibliography provides a context for readers. It reveals the influences of \nthe team at the time that they built the software. A project bibliography is composed of links to books, articles, and blogs either crafted by hand or extracted from anno-tations and comments or through a mix of both.\nDeclaring Your Style\nAs painters tend to belong to specific painting schools (for example, Surrealism, Cubism), software developers align themselves with various schools of thoughts. Some painters can switch between styles from one work to another; similarly , a devel-oper might be able to create a module in a very functional programming style, with everything pure and immutable, and then create another module using semantic technologies and graph-oriented stores.\nT o provide context for documentation readers, it is useful to declare the style and \nthe main paradigm, if any , that you have chosen for some area of code—typically for a module or for a project. This overall statement might look similar to a resume for the team or teams:\n •Modeling paradigms (for example, DDD) \n •Authors the team members follow \n •Books the team members have read and blogs they often go read \n •Languages and frameworks the team members are familiar with \n •Any kind of inspiration that matters, such as “Stripe as an inspiration for developer-friendliness” \n •Typical kinds of projects the team members have mostly done so far (for exam-ple, web, server, embedded)\nT o be refactoring proof, this information should reside within the module or pro-\nject itself. It can be done with annotations such as \n@Style  (Styles.FP) on packages \n(Java), attributes on the AssemblyInfo (.Net), or by using a style.txt file with a key/value syntax at the root of the module or project.\nChapter 4 Knowledge Augmentation 128\n Note\nAn explicit style declaration is also useful for tools; for example, the declared style \ncan be used to select a specific rulesets for static analysis tools. \nDeclaring your style helps enforce consistency within the code base. \nLOL\nCoined Gierke’s law yesterday: from the structure of a software system you can derive the book the architect read most recently .\n—From Oliver Gierke, @olivergierke on Twitter\nCommit Messages as Comprehensive Documentation\nCarefully written commit messages make each line of code well documented. When committing files into source control, it is good practice to add a meaningful com-ment that includes the commit message. This is often neglected, and the result is time wasted opening files to discover what a change was about. When done carefully , commit messages are very valuable for several purposes, as yet another high-yield activity:\n •Thinking: Y ou have to think about the work done. Is it one single change or a mix of more than one that should be split? Is it clear? Is it really done? Are there new tests that should have been added or modified along with the changes?\n •Explaining: A commit message must make the intention explicit. It is a fea-ture, or a fix, and the reason should be written, even briefly , as in recording the rationale. This will save time for the readers.\n •Reporting: Commit messages can later be used for various kinds of reporting, published as a changelog, or integrated into the developer toolchain.\nThe big idea with commit messages is that on any given line of code, asking the \nsource control for its history gives you a detailed list of reasons and, hopefully , of rationales explaining why this line of code is what it is. As Mislav Marohnić writes in his blog post “Every Line of Code Is Always Documented,” “a project’s history is its most valuable documentation.”\n9\n9. Mislav Marohnić, “Every Line of Code Is Always Documented,” http://mislav .uniqpath.com/2014/02/\nhidden-documentation/\nCommit Messages as Comprehensive Documentation 129\nLooking at the history of a given line of code tells you who made a change, when, \nand what other files were changed at the same time: for example, the related tests. \nThis helps pinpoint to the new test cases that were added and acts as a built-in mech-anism for code to test traceability . In the history you would also find the commit mes-sage explaining the change and the reasons for the change.\nT o make the most of commit messages, it might be a good idea to agree on a \nstandard set of commit guidelines if the current quality of the messages is not satis-factory . Using a standard structure and standard keywords has several benefits. For one thing, it is more formal and, therefore, more concise. With a formal syntax, you can write:\nfix(ui): change the color of the submit button to green\nwhich is shorter to write and to read than the equivalent full English sentence:\n“This is a fix on the UI area to change the color of the submit button to green.”\nA structure message enforces that the required information, such as the type of \ncommit or the location of the change, will not be forgotten. And using formal syntax \nturns a messages into machine-accessible knowledge, for even more goodness!\nTherefore: Take care with commit messages. Agree on a set of commit guide-\nlines and use a semi-formal syntax and a standard dictionary of keywords. Work collectively or use peer pressure, code reviews, or enforcement tools to ensure that the guidelines are respected. Design the guidelines so that tools can use them to help you more.\nCommit messages provide comprehensive documentation for each line of code. \nThis information is available at the command line or on the graphical interface on top of your source control, as shown in Figure 4.5. \nCommit Guidelines\nA good example of commit guidelines is the Angular commit guidelines,10 which \nspecify strict rules for how the commit messages must be formatted. These rules, says the Angular website, lead “to more readable messages that are easy to follow when looking through the project history . But also, we use the git commit messages to generate the AngularJS change log.” According to this particular set of guidelines, \n10. AngularJS, https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit\nChapter 4 Knowledge Augmentation 130\nthe commit message must be structured as a header section, an optional body sec-\ntion, and an optional footer section, each separated by a blank line, as shown here:\n1  <type>(<scope>): <subject> \n2\n3  <body> 45  <footer>\nSpecifying the Type of a Change\ntype  must be one of the following:\n •feat : A new feature\n •fix: A bug fix\n •docs : Documentation-only changes\n •style : Changes that do not affect the meaning of the code, such as changes to \nwhitespace, formatting, missing semicolons, and so on\n •refactor : A code change that neither fixes a bug nor adds a feature\n •perf : A code change that improves performance\nFigure 4.5 The blame view on GitHub shows every contribution for each line, here for the \nfamous Junit project\nCommit Messages as Comprehensive Documentation 131\n •test : Changes that add missing tests\n •chore : Changes to the build process or auxiliary tools and libraries, such as \ndocumentation generation\nAll breaking changes must be declared in the footer, starting with the words \nbreaking change, followed by a space and a detailed explanation of the change and \nof the migration aspects.\nIf a commit is related to issues in a tracker, the issue should be referenced in the \nfooter as well, with the identifier of the issue in the tracker.\nHere is an example of a feature related to the scope “trade feeding”:\n1 feat(tradeFeeding): Support trade feeding for negative-coupon\n2 bonds3 4 Some bonds have negative coupon rates, e.g. -0.21 percent.5 Change the validation to not reject trades on bonds 6 with negative coupons.78 Closes #8125\nSpecifying the Scope of a Change\nThe commit syntax shown earlier is semiformal, with a combination of keywords \nand free text. The first keyword, type , denotes the type of change (feature, fix, and \nso on) out of a small list. The second keyword, scope , denotes the scope of the \nchange in the system or application and is specific to the context.\nscope  can cover various aspects of the system:\n •Environment: Examples include prod , uat, and dev\n •Technology: Examples include RabbitMq , SOAP , JSON , Puppet , build , \nand JMS\n •Feature: Examples include pricing , authentication , monitoring , \ncustomer , shoppingcart , shipping , and reporting\n •Product: Examples include books , dvd, vod, jewel , and toy\n •Integration: Examples include Twitter  and Facebook\n •Action: Examples include create , amend , revoke , and dispute\nChapter 4 Knowledge Augmentation 132\nA commit guideline could require a main scope, but you could add more, as \nshown here:\n1 feat(pricing, vod): increase the rate on prime time \n2 ...\nOf course, you have to define a list of scopes, ideally as a whole team and includ-\ning the three amigos, with everyone involved in the DevOps in close collaboration. \nEvery change that could be committed to the source control should be covered in at least one of the scopes.\nKeep in mind that a smart list of scopes opens the door to reasoning about \nimpacts.\nMachine-Accessible Information\nA semiformal syntax for commit messages has the benefit of making it possible for machines to use these messages to automate more chores, such as generating a change log document.\n11 Let’s take a closer look at Angular.js, which provides a neat \nexample in this area.\nUnder Angular.js conventions, the change log is made of three optional sections \nfor each version, and each section is shown only when it is not empty:\n •New features\n •Bug fixes\n •Breaking changes\nThe following is an excerpt from an Angular.js change log:\n## 0.13.5 (2015-08-04)\n### Bug Fixes- file-list: Ensure autowatchDelay is working. (655599a), closes #1520- file-list: use lodash find() (3bd15a7), closes #1533### Features- web-server: Allow running on https (1696c78)\nThis change log is in the Markdown format, which enables links for convenient \nnavigation between commits, versions, and ticketing systems. For example, each \n11.  “Keep a Changelog,” http://keepachangelog.com"",11940",11994,"**Commit Guidelines**\n\n**Recording the Rationale as an Enabler for Change**:\n- Knowing all the reasons behind past decisions can enable successful change\n- Best way to know about past decisions is to have them recorded, otherwise reasoning will be forgotten\n- Without explicit rationale, you may wonder if a change has unexpected impacts or make changes that cause harm\n\n**Acknowledging Your Influences (aka Project Bibliography)**:\n- Good books care about their bibliography to reveal the influences of the author\n- A project bibliography provides context for readers and reveals the influences of the team\n- The project bibliography is composed of links to books, articles, and blogs\n\n**Declaring Your Style**:\n- Software developers align themselves with various schools of thought (e.g., functional programming, semantic technologies)\n- Declaring your style helps enforce consistency within the code base\n- Can be documented through annotations, comments, or a ""style.txt"" file\n\n**Commit Messages as Comprehensive Documentation**:\n- Carefully written commit messages make each line of code well-documented\n- Commit messages should include the commit type (e.g., fix, feature), scope, subject, and reasoning behind the change\n- A standard set of commit guidelines can lead to more readable and easily followable messages",1322,47.38771891593933
False,3,"65-Summary.pdf,65-Summary,""Summary 133\nversion in the change log links to the corresponding compare view in GitHub, show-\ning the differences betwee","65-Summary.pdf,65-Summary,""Summary 133\nversion in the change log links to the corresponding compare view in GitHub, show-\ning the differences between this version and the previous one. Each commit message also links to their particular commits and also links to the corresponding issue(s), when applicable.\nThanks to this kind of structured commit guidelines, it is possible to extract and \nfilter commits through command-line magic, as shown in the following example, borrowed from the Angular.js documentation:\n1 List of all subjects (first lines in commit message) since \nlast release:2 >> git log <last tag> HEAD --pretty=format:%s 34 New features in this release5 >> git log <last release> HEAD --grep feature\nThe change log shown here can be generated by a script when doing a release. \nThere are many open-source projects to do this, such as the conventional-changelog \nproject.12 This change log automation script relies strongly on the chosen commit \nguidelines, and it already supports several of them, including Atom, Angular, and jQuery .\nSuch automation is convenient, although if a human should review and edit the \ngenerated change log skeleton before release to the public.\nSummary\nIt is often the case that elements of knowledge that are missing from a system are things that you want to be remembered. In particular, you should record the ration-ale behind decisions. Y ou need to augment the code of a system to make it knowledge complete. Annotations, conventions, and other techniques are instrumental in this augmented code approach for recording the most important pieces of knowledge. And this process of augmented code is also an opportunity to spread skills to your workmates as a form of embedded learning.\n12.  https://github.com/ajoslin/conventional-changelog\nThis page intentionally left blank"",1812",1845,"**Commit Guidelines for GitHub Projects:**\n* Each commit message follows a structured format:\n	+ Includes summary of changes between versions\n	+ Links to corresponding compare view and commits in GitHub\n	+ Links to related issues, when applicable\n* List of all subjects (first lines in commit message) since last release:\n	+ `git log <last tag> HEAD --pretty=format:%s`\n* New features in this release:\n	+ `git log <last release> HEAD --grep feature`\n* Change log automation scripts available for projects like Atom, Angular, and jQuery\n* Human review and editing of generated change log skeleton recommended before public release\n* Recording knowledge important for system development through annotations, conventions, etc.\n* Augmented code approach to spread skills among workmates as embedded learning.",804,20.576158046722412
False,3,"66-Chapter 5 Living Curation Identifying Authoritative Knowledge.pdf,66-Chapter 5 Living Curation Identifying Authoritative Knowledge,,0","66-Chapter 5 Living Curation Identifying Authoritative Knowledge.pdf,66-Chapter 5 Living Curation Identifying Authoritative Knowledge,,0",136,"**Identifying Authoritative Knowledge in Living Curation**\n\n**Living Curation**:\n- A concept or process within which knowledge is preserved and transmitted across generations\n\n**Importance of Identifying Authoritative Knowledge**:\n- Ensures accurate preservation and transmission of valuable information\n- Maintains continuity and consistency of beliefs, practices, and traditions over time\n- Essential for intellectual growth and development of individuals and communities\n\n**Criteria for Identifying Authoritative Knowledge**:\n1. **Consistency**: Information should agree with other reliable sources and not contradict each other.\n2. **Credibility**: Knowledge originates from reputable sources, usually recognized experts or trusted authorities within a particular field or community.\n3. **Corroboration**: Information is backed up by multiple forms of evidence, such as physical artifacts, witnesses, or experiments.\n4. **Transmissibility**: Knowledge can be effectively passed down through generations, often with the help of rituals and ceremonies that instill value and meaning.\n5. **Beneficial Impact**: Knowledge improves individual or collective well-being, resolves problems, addresses needs, or offers new opportunities for growth.\n\n**Challenges in Identifying Authoritative Knowledge**:\n1. **Misinformation**: False knowledge can be intentionally spread or accidentally shared, leading to confusion and potential harm.\n2. **Lack of Corroboration**: Some knowledge may not have sufficient evidence to support its validity, making it difficult to verify.\n3. **Cultural Relativism**: Different cultures may hold incompatible knowledge claims, causing tension and conflict between groups.\n4. **Hierarchies of Knowledge**: Authoritative knowledge can be controlled by those with power or privilege, limiting access for the marginalized or disempowered.\n5. **Rapid Technological Change**: Traditional knowledge may not keep pace with new information presented through technology, leading to confusion and disruption in value systems.\n\n**Strategies for Maintaining Living Curation**:\n1. Encourage a culture of open dialogue and critical thinking to challenge, verify, and correct misinformation.\n2. Foster interdisciplinary collaboration to bridge gaps between different types of knowledge and prevent silos from forming.\n3. Promote accessibility and inclusivity in preserving and transmitting knowledge, ensuring it is available to all members of a community.\n4. Embrace new technologies as tools for preserving and disseminating knowledge while maintaining traditional methods that foster connection and meaning-making.\n5. Continuously reassess and update knowledge in light of new information or changing circumstances.",2728,60.67514181137085
False,3,"67-Dynamic Curation.pdf,67-Dynamic Curation,""135Chapter 5\nLiving Curation: Identifying \nAuthoritative Knowledge\nThe Queen’s speech is like the rele","67-Dynamic Curation.pdf,67-Dynamic Curation,""135Chapter 5\nLiving Curation: Identifying \nAuthoritative Knowledge\nThe Queen’s speech is like the release notes for a minor new version of  the UK! \n—Matt Russell (@MattRussellUK) on T witter\nRemember that most of the knowledge related to a system is already there in that \nsystem—and there is a lot of it. One crucial way to exploit all that knowledge is through curation. The idea of curation is to select the few relevant bits of knowledge out of the ocean of data in the system, in order to help people working on it in their future work assignments. Because this system is always changing, it is safest to ensure that this curation evolves naturally , without any manual maintenance.\nDynamic Curation\nIn art exhibitions, the curator is as important as the director in a movie. In contem-porary art, the curator selects and often interprets works of art. For example, the curator searches for prior work and places that inspired the artist, and he or she pro-poses a narrative or a structured analysis that links the selected works together in a way that transcends each individual piece. When a work that is essential for the exhi-bition is not in the collection, the curator will borrow it from another museum or from a private collection or may even commission the artist to create it. In addition to selecting works, the curator is responsible for writing labels and catalog essays and overseeing the scenography of the exhibition to help convey the chosen messages.\nWhen it comes to documentation, we need to become our own curators, working \non all the knowledge that is already there to turn it into something meaningful and useful.\nChapter 5 Living Curation: Identifying Authoritative Knowledge 136\nCurators select works of art based on many objective criteria, such as the \nartist name, the date and place of creation of the works, or the private collectors who \nfirst bought the works. They also rely on more subjective criteria, such as the relation-ships to art movements or to major events in history , such as wars or popular scan-dals. The curator needs metadata about each painting, sculpture, or video performance. When the metadata is missing, the curator has to create it, sometimes by doing research.\nCuration is something that you already do, perhaps without being aware of it. For \nexample, when you are asked to demo an application to a customer or to a top man-ager, you have to choose just a few use cases and screens to show in order to convey a message, such as “everything is under control” or “buy our product because it will help you do your job.” If you have no underlying message, it’s likely that your demo with be an unconvincing mess.\nUnlike in art exhibitions, in software development what we need is more like a \nliving exhibition with content that adjusts according to the latest changes. As the knowledge evolves over time, we need to automate the curation on the most impor-tant topics.\nTherefore: Adopt the mindset of a curator to tell a meaningful story out of all \nthe available knowledge in the source code and artifacts. Don’t select a fixed list of elements. Instead, rely on tags and other metadata in each artifact to dynami-cally select the cohesive subset of knowledge that is of interest for the long term. Augment the code when the necessary metadata is missing and add any missing pieces of knowledge when they are needed for the story.\nCuration is the act of selecting relevant pieces out of a large collection to create a \nconsistent narrative that tells a story . It’s like a remix or a mashup. Curation is key for knowledge work like software development. Source code is full of knowledge about many facets of the development, and different parts of it are of varying degrees of importance. On anything bigger than a toy application, extracting knowledge from the source artifacts immediately overflows our regular cognitive capabilities with too many details, and the knowledge becomes meaningless and therefore useless (see Figure 5.1). \nFigure 5.1 T oo much information is as useless as no information"",4088",4139,"**Living Curation: Identifying Authoritative Knowledge**\n\n**Curation**:\n- Selecting relevant bits of knowledge from the ""ocean of data"" in a system\n- Helps people work on the system by providing meaningful and useful information\n- Evolves naturally without manual maintenance\n\n**Role of the Curator**:\n- In art exhibitions, curators select and interpret works of art\n- Write labels and catalog essays, oversee scenography to convey chosen messages\n- Use objective and subjective criteria to select works based on metadata\n\n**Curation in Software Development**:\n- Need a ""living exhibition"" with content that adjusts according to latest changes\n- Adopt the mindset of a curator to tell a meaningful story from available knowledge\n- Rely on tags and metadata to dynamically select cohesive subsets of interest\n- Augment code when necessary metadata is missing\n\n**Importance of Curation**:\n- Overwhelming amount of information in source code requires selection and curation\n- Too much information is as useless as no information",1025,27.245095252990723
False,3,"68-Examples of Dynamic Curation.pdf,68-Examples of Dynamic Curation,,0","68-Examples of Dynamic Curation.pdf,68-Examples of Dynamic Curation,,0",70,"**Examples of Dynamic Curation**\n\n**Keywords**: Examples, Dynamic Curation\n\n**Notes**:\n- Dynamic curation refers to actively updating and managing content on a website or platform\n- Here are some examples:",205,6.427196741104126
False,3,"69-One Corpus of Knowledge for Multiple Uses.pdf,69-One Corpus of Knowledge for Multiple Uses,""Dynamic Curation 137\nThe solution is to aggressively f","69-One Corpus of Knowledge for Multiple Uses.pdf,69-One Corpus of Knowledge for Multiple Uses,""Dynamic Curation 137\nThe solution is to aggressively filter the signal from the noise for a particular \ncommunication intent; as the little buffoon monster in Figure 5.1 says, “T oo much \ninformation is as useless as no information.” What would be the noise from a par-ticular perspective might be the signal from another perspective. For example, the method names are an unnecessary detail in an architecture diagram, but they might be important in a close-up diagram about how two classes interact, with one being an adapter to the other.\nCuration as its core is the selection of pieces of knowledge to include or to ignore, \naccording to a chosen editorial perspective. It’s a matter of scope. Dynamic curation goes one step further, with the ability to do the selection continuously on an ever-changing set of artifacts.\nExamples of Dynamic Curation\nA T witter search is an example of automated dynamic curation, and it is a resource in itself that you can follow just as you would follow any T witter handle. People on T witter also do a manual form of curation when they retweet content they have (more or less) carefully selected according to their own editorial perspective (perhaps). A Google search is another example of simple automated curation.\nAs another example, selecting an up-to-date subset of artifacts based on a \ncriterion is something we do every day when using an IDE:\n •Show every type with a name that ends with “DAO.”\n •Show every method that calls this method.\n •Show every class that references this class.\n •Show every class that references this annotation.\n •Show every type that is a subtype of this interface.\nWhen a tag is missing to help select the pieces, you should introduce it with anno-\ntations, naming conventions, or any other means. When a piece of knowledge is missing, in order to show a complete picture, you need to add it in a just-in-time fashion.\nEditorial Curation\nCuration is an editorial act. Deciding on an editorial perspective is the essential step. There should be one and only one message at a time. A good message is a statement with a verb, like “No dependency is allowed from the domain model layer to the other layers” rather than just “Dependencies between layer,” where there is no \nChapter 5 Living Curation: Identifying Authoritative Knowledge 138\n message, and it’s up to the reader to guess what is meant. At a minimum, dynamic \ncuration should be given an expressive name that reflects the intended message.\nLow-Maintenance Dynamic Curation\nSelecting subsets of knowledge can be hazardous if done in a rigid way . For example, a direct reference to a list of classes, tests, or scenarios will rapidly become obsolete and will require maintenance. It is a form of copy and paste, and it makes change more expensive; it also is subject to the risk of someone forgetting to update it. This is not a good practice, and it should be avoided at all cost.\n Caution\nAvoid directly referencing artifacts by name or URL. Instead, find mechanisms to select pieces of knowledge based on criteria that are stable over time so that the selection will remain up-to-date without any manual action.\n \n Key Concept\nSelect artifacts indirectly , based on stable criteria.\nY ou can describe the artifacts of interest in a stable way by using one of the stable \nselection criteria described here:\n •Folder organization: For example, “everything in the folder named ‘Return \nPolicy’”\n •Naming conventions: For example, “every test with ‘Nominal’ in its name”\n •Tags or annotations: For example, “every scenario tagged as ‘WorkInProgress’”\n •Links registry over which you have control (which may need some main-tenance from time to time, but at least it is in a central place): For example, “the URL registered under this shortlink” \n •Tool output: For example, “every file that has been processed by the compiler, as visible in its log”\nWhen you use stable criteria, the work is done by tools that automatically extract \nthe latest content that meets the criteria to insert it into the published output. Because it is fully automated, it can be run as often as possible—perhaps continu-ously on each build."",4190",4291,"**Dynamic Curation: Selecting Pieces of Knowledge for Specific Uses**\n\n**Concept:** Dynamic curation is the continuous selection and filtering of knowledge pieces according to a chosen editorial perspective.\n\n**Importance**:\n- Filter information to avoid noise and focus on specific communication intents\n- Selective inclusion or exclusion based on perspectives\n- Examples: Twitter searches, Google searches, IDE searches\n\n**Editorial Curation:**\n- Deciding on an editorial perspective is essential\n- One message at a time with clear intent\n- Avoid directly referencing artifacts by name or URL\n\n**Selecting Artifacts Indirectly:**\n- Use stable criteria for selection: folder organization, naming conventions, tags/annotations, links registry, tool output\n- Automated extraction of latest content meeting criteria\n- Continuous updating without manual intervention\n- Avoid direct referencing to prevent obsolete information and maintenance costs.",945,26.28682780265808
False,3,"70-Scenario Digests.pdf,70-Scenario Digests,""Dynamic Curation 139\nOne Corpus of Knowledge for Multiple Uses\nEverything can be curated—code, configur","70-Scenario Digests.pdf,70-Scenario Digests,""Dynamic Curation 139\nOne Corpus of Knowledge for Multiple Uses\nEverything can be curated—code, configuration, tests, business behavior scenarios, \ndatasets, tools, data, and so on. All the knowledge available can be considered as a huge corpus, accessible via automated means for analysis and curated extractions.\nProvided that the content of the knowledge corpus is adequately tagged, it is pos-\nsible to extract by curation out of it a business view of a glossary (that is, a living glossary), a technical view of the architecture (that is, a living diagram), and any other perspective you can imagine, including the following:\n •Audience-specific content, such as business-readable content only versus tech-nical details\n •Tasks-specific content, such as how to add one more currency\n •Purpose-specific content, such as an overview of content versus a references section\nCuration is possible only to the extent that metadata about the source knowledge \nis available to enable relevant selection of material of interest.\nScenario Digests\nCuration is not just about code; it’s also about tests and scenarios. A good example of dynamic curation is a scenario digest, in which the corpus of business scenarios is curated under various dimensions in order to publish reports tailored for particular audiences and purposes.\nWhen a team makes use of BDD together with an automated tool such as Cucum-\nber, a large number of scenarios are written in feature files. Not every scenario is equally interesting for everyone and for every purpose, so you need a way to do a dynamic curation of the scenarios, and for that you need to have the scenarios marked with a nicely designed system of tags. Remember from Chapter 2, “Behavior-Driven Development as an Example of Living Specifications,” that tags are documentation.\nEach scenario can have tags like the following:\n1  @acceptancecriteria @specs @returnpolicy @nominalcase \n@keyexample2  Scenario: Full reimbursement for return within 30 days 3  ...45  @acceptancecriteria @specs @returnpolicy @nominalcase6  Scenario: No reimbursement for return beyond 30 days  \nChapter 5 Living Curation: Identifying Authoritative Knowledge 140\n7  ...\n89  @specs @returnpolicy @controversial10 Scenario: No reimbursement for return with no proof of purchase11  ...1213 @specs @returnpolicy @wip @negativecase14 Scenario: Error for unknown return15  ...\nNote that almost all these tags are totally stable and intrinsic to the scenario they \nrelate to. I say almost because @controversial  and @wip  (work in progress) are \nactually not meant to last too long, but they are convenient for a few days or weeks \nfor easy reporting.\nThanks to all these tags, it is easy to extract only a subset of scenarios, by title \nonly or complete with step-by-step descriptions. The following are some examples:\n •When meeting business experts who have very limited time, perhaps you could focus only on the information tagged \n@keyexample  and @controversial :\n1 @keyexample or @controversial Scenarios:\n2 - Full reimbursement for return within 30 days3 - No reimbursement for return with no proof of purchase\n •When reporting to the sponsor about the progress, the @wip  and @pending  \nscenarios are probably more interesting for this audience, along with the pro-\nportion of @acceptancecriteria  passing green:\n1 @wip, @pending or @controversial Scenarios:\n2 - Error for unknown return\n •When onboarding a new team member, going through the @nominalcase  \nscenarios of each @specs  section may be enough:\n1 @nominalcase Scenarios:2 - Full reimbursement for return within 30 days3 - No reimbursement for return beyond 30 days\n •Compliance officers want everything that is not @wip . However, even in that \ncase, they might want to have the big document show a summary of the \n@acceptancecriteria  first and the rest of the scenarios in addendum."",3903",3954,"**Curation of Knowledge Corpus**\n- Everything can be curated: code, configuration, tests, business behavior scenarios, datasets, tools, data, etc.\n- Knowledge corpus is a huge resource accessible for automated analysis and curated extractions\n- **Metadata** about the source knowledge is essential to enable relevant selection of material of interest\n- Curated content can be tailored for different audiences and purposes: business view, technical view, audience-specific, tasks-specific, purpose-specific\n\n**Scenario Digests**\n- Curation is not just about code; it also applies to tests and scenarios\n- Scenario digest is a good example of dynamic curation, where the corpus of business scenarios is curated under various dimensions to publish tailored reports\n- When using Behavior-Driven Development (BDD) with an automated tool like Cucumber, a large number of scenarios are written in feature files\n- Each scenario can have **tags** for different purposes: acceptance criteria, specifications, return policy, key examples, audience, tasks, purpose, controversy, work in progress\n- Tags help in dynamic curation of scenarios by extracting subsets based on various criteria (e.g., @keyexample or @controversial, @wip and @pending, @nominalcase)\n- Different audiences may be interested in specific subsets of scenarios: business experts, sponsors, new team members, compliance officers.",1388,33.86576557159424
False,3,"71-Highlighting Inspiring Exemplars.pdf,71-Highlighting Inspiring Exemplars,""Highlighting the Core 141\nHighlighting the Core \nSome elements of a dom","71-Highlighting Inspiring Exemplars.pdf,71-Highlighting Inspiring Exemplars,""Highlighting the Core 141\nHighlighting the Core \nSome elements of a domain are more important than others. In the book Domain-\nDriven Design, Eric Evans explains that when a domain grows to a large number of \nelements, it becomes difficult to understand, even if only a small subset of the ele-ments are really important. A simple way to guide developers to focus on particular subsets is to highlight them in the code repository itself. He calls that subset the highlighted core.\nTherefore: Flag each element of the core domain within the primary repository \nof the model without particularly trying to elucidate its role. Make it effortless for a developer to know what is in or out of the core.\nUsing annotations to flag the core concepts directly in the code is a natural \napproach, and it evolves well over time. Code elements such as classes or interfaces get renamed, are moved from one module to another, and sometimes end up deleted.\nThe following is a perfect simple example of curation by annotations:\n1  /**\n2   * A fuel card with its type, id, holder name3   */4  @ValueObject5  @CoreConcept6  public class FueldCard {7    private final String id;8    private final String name; 9    ...\nIt is an internal documentation integrated into the search capabilities of an IDE. \nY ou can see the list of all core concepts just by searching every reference of the anno-\ntation in the project, which is always up-to-date (see Figure 5.2). \nFigure 5.2 The highlighted core is available instantly and at any time in the IDE through a \nsearch on all references to the @CoreConcept  annotation\nChapter 5 Living Curation: Identifying Authoritative Knowledge 142\nAnd, of course, tools can also scan the source and use the highlighted core as a \nconvenient and relevant way to improve curation. For example, a tool to generate a \ndiagram may show different levels of detail in different cases, such as showing every-thing when there are fewer than seven elements and focusing only on the highlighted core when there are many more than seven elements. A living glossary typically uses this technique to highlight the most important elements in the glossary by showing them first or by printing them in a bold font.\nHighlighting Inspiring Exemplars\nThe best documentation on how to write code is often the code that is already there. When I’m coaching teams on TDD, I pair-program randomly with developers on code bases I have never seen before. The developers pairing with me often behave as if they have never seen the code base before; for a new task, they might go looking for an example of something similar already there, and then they copy and paste it into a new case. A programmer might determine, for example, to find a service written by Fred, who is the team lead and is well respected by the rest of the team. However, Fred might not be great in every aspect of his code, and the flaws in his code may end up being replicated across the whole code base. In such situations, a good way to improve the code quality is to improve the examples of code that people imitate. Exemplary code should serve as a desirable model to imitate—or at least to inspire other developers. Sam Newman writes about this in his book Building Microservices:\nIf you have a set of standards or best practices you would like to encourage, then having \nexemplars that you can point people to is useful. The idea is that people can’t go far wrong just by imitating some of the better parts of your system.\n1\nY ou can point your colleagues to the exemplars during conversations and \nduring pair-programming or mob-programming: “Let’s look at the class \n ShoppingCartResource , which is the most well-designed class and is exactly in \nthe style of code we favor as a team.”\nConversations are perfect for sharing examplars, but some additional documen-\ntation can have benefits, too, when you are not present to point people in the right direction or when people are working on their own. Y ou can use documentation to provide the equivalent of a big loud sign to signal a good examples (see Figure 5.3). \nTherefore: Highlight directly in the actual production code the places that are \nparticularly good exemplars of a style or of a best practice you would like to encourage. Point your colleagues to these exemplars and advertise how to find \n1. Newman, Sam. Building Microservices. Sebastopol, CA: O’Reilly Media, Inc., 2015. \nHighlighting Inspiring Exemplars 143\nthem on their own. Take care of the exemplars so that they remain exemplary, \nfor everyone to imitate in a way that will improve the overall code base.\nAnnotations are, of course, a perfect fit here: Y ou can create a custom annotation \nto put on the few classes or methods that are the most exemplary . Of course, exem-plars are useful only if their numbers are limited to those that are very best.\nDecisions on what code is exemplary or not are best made collectively by the \nteam. Make it a team exercise to find a consensus on the few exemplars to highlight with a special annotation.\nExemplars should be actual code used in production, not tutorial code, as Sam \nNewman says in Building Microservices:\nIdeally , these should be real world services you have that get things right, rather than \nisolated services that are just implemented to be perfect examples. By ensuring your exemplars are actually being used, you ensure that all the principles you have actually make sense.\n2\nIn practice, an exemplar is hardly perfect in all aspects. It might be a very good \nexample of design, but the code style might be a bit weak—or the other way round. \nMy preferred solution would be to fix the weak aspect first. However, if that’s not pos-sible or desirable, you should at least clarify why the exemplar is good and what aspect of it should not be considered exemplary . Here are a few examples of exemplars:\n •On a class: @Exemplar(""""A very good example of a REST resource \nwith content negotiation and the use of URI-templates"""")\n •On a JavaScript file: @Exemplar(""""The best example of integrating \nAngular and Web Components"""")\n2. Newman, Sam. Building Microservices. Sebastopol, CA: O’Reilly Media, Inc., 2015.Good\nExample\nHere\n<code/> <code/> <code/>\nFigure 5.3 Good example of  code here!"",6293",6380,"**Highlighting Inspiring Exemplars**\n\n**Importance of Highlighting Core Domain Elements**:\n- Some elements of a domain are more important than others\n- Large number of elements makes it difficult to understand which are truly core\n- **Highlighting the core**:\n  - Flag core domain elements directly in the code repository\n  - Use annotations to mark core concepts\n  - Evolutionary approach: code elements change over time, but annotations remain consistent\n\n**Exemplars as Inspiring Documentation**:\n- Exemplars are inspiring pieces of code that serve as models for best practices\n- New developers often imitate existing code, so highlighting exemplars can improve code quality\n- **Exemplar documentation**:\n  - Highlight specific exemplars in the actual production code\n  - Provide supplementary documentation to point colleagues to these exemplars\n  - Limit the number of exemplars to only the very best examples\n\n**Deciding on Exemplars**:\n- Decisions on what constitutes an exemplar should be made collectively as a team\n- Exemplars should be actual production code, not tutorial or idealized examples\n- If an exemplar has weaknesses, clarify which aspects are exemplary and which are not",1192,39.90307283401489
False,3,"72-Guided Tours and Sightseeing Maps.pdf,72-Guided Tours and Sightseeing Maps,""Chapter 5 Living Curation: Identifying Authoritative Knowledge 144\n •O","72-Guided Tours and Sightseeing Maps.pdf,72-Guided Tours and Sightseeing Maps,""Chapter 5 Living Curation: Identifying Authoritative Knowledge 144\n •On a package or a key class of this part of design: @Exemplar(""""A nicely \ndesigned example of CQRS"""")\n •On a particular class: @Exemplar(pros = """"Excellent naming of the \ncode"""", cons = """"too much mutable state, we recommend  immutable \nstate"""")\nBasically , marking exemplars directly in the code enables you to then ask your IDE \nsomething like “What code is a good example of writing a REST resource?” In an \nintegrated documentation fashion, finding exemplars is only a matter of searching for all references of the \n@Exemplar  annotation in your IDE. Y ou can then just scroll \nthe short list of results to decide which code will be the inspiration for your task.\nOf course, there are caveats in the approach suggested before:\n •Software development is not supposed to be as much copying and pasting as thinking and solving problems. Highlighting exemplars does not give you license to copy and paste code.\n •Copying/pasting requires refactoring. As similar code accumulates, it must be refactored.\n •Marking the exemplars in the code is not meant to replace asking colleagues for exemplary code. Asking questions is good because it leads to conversations, and conversations are key for improving the code and the skills. Don’t reply “R TFM” (“read the flipping manual”) when asked for exemplars. Instead, go through the suggested exemplars in the IDE together to determine which one would be best for the task. Always take conversations as opportunities to improve something mutually .\nGuided Tours and Sightseeing Maps\nIt is easier to quickly discover the best of a new place with a guided tour or a sight-seeing map. In a city you have never visited before, you can explore randomly , hoping to bump into something interesting. This is something I love to do during an after-noon within a longer stay , to get a feel of the place. However, if I have only one day and I want to quickly enjoy the best of the city , I take a guided tour with a theme. For example, I have excellent souvenirs from a guided tour of the old sky-scrapers in Chicago, where the guide knew how to get us into the historical lobbies to enjoy the low light that was typical of early light bulbs. One year later, I enjoyed an \nGuided T ours and Sightseeing Maps 145\narchitecture boat tour of Chicago, from the river, which is another way to really \ngrasp the city . In Berlin, I booked a tour dedicated to Berlin’s street art, which was eye opening. For me, the same street art I see every day without really noticing gains another dimension when put in a context with one hint from a guide.\nBut guided tours start at a fixed hour on a few days a week only , often take a few \nhours, and may be expensive. If you happen to pass through a city on the wrong day , you are out of luck. But you can still get a tourist map or printed guided tours. And, of course, there is probably an app for that! Plenty of apps provide guided tours and sightseeing maps, classified by themes such as attractions, eat, drink, dance, and concerts. In Chicago, the Society of Architecture offers free architecture tours on leaflets, too. And the Internet is full of resources to help plan a visit, such as “T op 20 List of Must-See Highlights,” “Itineraries to Help Y ou Plan Y our Visit,” and “101 Things to Do in London.”\n Note\nSometimes these resources go a bit too far, as in the guided tour Unusual and Original Things to Do in London, for example, which has a stop for coffee in a public loo: As Timeout London says, “Don’t worry , these beautifully converted old Victorian toilets were given a good scrub down before the plates of cakes were laid out. Opened in 2013, Attendant has a small bank of tables where the porce-lain urinals once provided relief to gents about town.”\n3\n \nThe process of becoming familiar with a code base can be similar to the process \nof becoming familiar with a city . The best way for someone to discover it is with another human—a colleague. But if you want to provide an alternative to a human guide, you can take inspiration from the tourism industry and provide itineraries of guided tours and sightseeing maps. This tourism metaphor comes from Simon Brown, who writes the blog “Coding the Architecture” and also wrote the book  Software Architecture for Developers, V olume 2.\nOne important thing to realize is that all the tourism guidance in a city is highly \ncurated: Only a very small subset of all the possible content of the city is presented, \nfor various reasons ranging from the historical importance of different landmarks to more money-related reasons.\nOne important difference between a code base and a city is that a code base can \nchange more frequently than most cities. As a result, the guidance must be provided in such a way that the work to keep it up-to-date is minimized; of course, automa-tion is a good option here.\n3. Timeout London, “Unusual and Original Things to Do in London,” http://www .timeout.com/\nlondon/things-to-do/101-things-to-do-in-london-unusual-and-unique"",5091",5182,"**Guided Tours and Sightseeing Maps**\n\n**Exemplars in Code:**\n- Exemplars marked in code enable easy discovery of best practices\n- Search for @Exemplar annotations in IDE for inspiration\n- Highlighted exemplars not meant to replace asking colleagues\n- Conversations key for improving skills and updating code\n\n**Discovering New Places:**\n- Guided tours and sightseeing maps facilitate exploration\n- Fixed schedules, expense, or limited availability\n- Apps provide alternatives with various themes\n\n**Code Base Exploration:**\n- Similar to becoming familiar with a city\n- Human guide (colleague) recommended for discovery\n- Provide alternative guided tours and sightseeing maps if human guide not available\n\n**Curated Guidance:**\n- Tourism metaphor for code exploration\n- Only a small subset of content presented in city guides\n- Automation can help keep up-to-date with frequently changing code bases.",900,27.302507400512695
False,3,"73-Creating a Sightseeing Map.pdf,73-Creating a Sightseeing Map,""Chapter 5 Living Curation: Identifying Authoritative Knowledge 146\nTherefore: Provid","73-Creating a Sightseeing Map.pdf,73-Creating a Sightseeing Map,""Chapter 5 Living Curation: Identifying Authoritative Knowledge 146\nTherefore: Provide curated guides of a code base, each with a big theme. Aug-\nment the code with extra metadata about the guided tour or a sightseeing map, and set up an automated mechanism to publish as often as desired an updated guide from these metadata. A sightseeing map or a guided tour based on tags in the code is a perfect example of the augmented code approach.\nIf the code base does not change much, a guided tour or a sightseeing map can \nbe as simple as a bookmark with a list of the selected places of interest, each with a short description and a link to its location in the code. If the code is on a platform like GitHub, it is easy to link to any line of code directly . This bookmark can be cre-ated in HTML, Markdown, JSON, a dedicated bookmark format, or any other form you like.\nIf the code base changes frequently or may change frequently , a manually man-\naged bookmark would require too much effort to keep up-to-date, so you might choose dynamic curation instead: Place tags on the selected locations in the code and rely on the search features of the IDE to instantly display the bookmarks. If needed, you can add metadata to the tags to enable reconstruction of the complete guided tour, simply by scanning the code base.\nY ou might be worrying that adding tags about sightseeing maps or guided tours \ninto the code will pollute the code—and you are right. These tags are not really about the tagged element intrinsically but about how it is used, so use this approach sparingly .\nConsider your code base as a beautiful wilderness in the mountains where you go \nhiking. It is a protected area, and there are red-and-white hiking trail signs painted directly on the stones and on the trees. This paint does pollute the natural environ-ment in a small way , but we all accept it because it’s very useful and degrades the landscape only a limited amount.\nCreating a Sightseeing Map\nT o create a sightseeing map, you first create a custom annotation or attribute, and then you put it on the few most important places that you want to emphasize. T o be effective, you should keep the number of places of interest low—ideally 5 to 7 and certainly no more than 10.\nIt may well be that one of the most difficult decision here is to name each annota-\ntion or attribute. Here are some naming suggestions:\n •KeyLandmark  or Landmark\n •MustSee\nGuided T ours and Sightseeing Maps 147\n •SightSeeingSite\n •CoreConcept  or CoreProcess\n •PlaceOfInterest , PointOfInterest , or POI\n •TopAttraction\n •VIPCode\n •KeyAlgorithm  or KeyCalculation\nFor the approach to be useful, you also need to make sure everybody knows about \nthe tags and how to search them.\nA Sightseeing Map Example in C# and Java\nSay that in creating a custom attribute, you decide to put it into its own assembly to \nbe shared by other Visual Studio projects (which also means you don’t want anything to be specific to any particular project there). Here is how the attribute might look in C#:\n1  public class KeyLandmarkAttribute: Attribute \n2  {3  }\nY ou can now immediately use this attribute to tag your code:\n1  public class Foo2  {3    [KeyLandmark(""""The main steps of enriching the Customer4    Purchase from the initial order to a ready-to-confirm 5    purchase"""")]6    public void Enrich(CustomerPurchase cp)7    {8      //... interesting stuff here9    }10 }\nJava and C# are very similar. Here’s the same example, now in Java:\n1  package acme.documentation.annotations; 23  /**"",3564",3637,"**Creating a Sightseeing Map**\n\n**Approach to Creating a Sightseeing Map**:\n- Provide curated guides of a codebase, each with a big theme\n- Augment the code with extra metadata about the guided tour or sightseeing map\n- Set up an automated mechanism to publish updated guides from these metadata\n\n**Guided Tour and Sightseeing Map as Code Annotations**:\n- If the codebase does not change much, a guided tour or sightseeing map can be as simple as a bookmark with:\n  - Short description\n  - Link to location in the code\n- If the codebase changes frequently, use dynamic curation:\n  - Place tags on the selected locations in the code\n  - Rely on IDE search features to instantly display the bookmarks\n  - Add metadata to the tags to enable reconstructing the complete guided tour\n\n**Caution about Adding Tags**:\n- **Adding tags about sightseeing maps or guided tours into the code can ""pollute"" it.**\n- Use this approach sparingly, as these tags are not really about the tagged element but how it is used.\n\n**Naming Conventions for Custom Attributes/Annotations**:\n- **KeyLandmark**, **Landmark**, **MustSee**, **SightSeeingSite**, **CoreConcept**, **PlaceOfInterest**, **PointOfInterest**, **POI**, **TopAttraction**, **VIPCode**, **KeyAlgorithm**, or **KeyCalculation**\n\n**Making the Tags Effective**:\n- Ensure everybody knows about and can search them",1352,42.30542540550232
False,3,"74-Creating a Guided Tour.pdf,74-Creating a Guided Tour,""Chapter 5 Living Curation: Identifying Authoritative Knowledge 148\n4 * Marks this place in t","74-Creating a Guided Tour.pdf,74-Creating a Guided Tour,""Chapter 5 Living Curation: Identifying Authoritative Knowledge 148\n4 * Marks this place in the code as a point of interest\nworth listing on a sightseeing map.5 */6   7  @Retention(RetentionPolicy.RUNTIME)8  @Documented9  public @interface PointOfInterest { 1011     String description() default """"""""; 12  }\nAnd now we can use it as follows:\n1  @PointOfInterest(""""Key calculation"""")2  private double pricing(ExoticDerivative ...){ 3    ...\nAn alternative naming could look like this:\n1  @SightSeeingSite(""""This is our secret sauce"""")2  public SupplyChainAllocation optimize(Inventory ...){ 3    ...\nIn C# you would use the custom attribute as follows:\n1 public class CoreConceptAttribute : Attribute23 [CoreConcept(""""The main steps of enriching the Customer4 Purchase from the initial order to the ready to ship5 Shipment Request"""")]\nThe wording is up to you, and you can use one generic annotation with a generic \nname like PointOfInterest  and add the parameter Key calculation  to tell pre-\ncisely what it is about. Alternatively , you could decide to create one annotation for \neach kind of point of interest:\n1  @KeyCalculation()\n2  private double pricing(ExoticDerivative ...){ 3   ...\nCreating a Guided Tour \nIn the example shown in this section, the idea is to take a newcomer by the hand \nalong the complete chain of processing of an incoming transaction, from the event \nGuided T ours and Sightseeing Maps 149\nlistener on a message queue down to storing the outgoing report to the database. \nNote that even though it strictly separates the domain logic and the infrastructure logic, this guided tour spans both business logic elements with elements of the under-lying infrastructure in order to give a complete picture of a complete execution path.\nThis guided tour currently has six steps, each of which is anchored on a code ele-\nment that can be a class, a method, a field, or a package.\nThis example uses the custom annotation \n@GuidedTour  with some parameters:\n •The name of the guided tour: This is optional if there is only one tour, or if you prefer one annotation by guided tour, like \n@QuickDevTour .\n •A description of the step in the context of this tour:  This is in contrast to the \nJavadoc comment on the element, which describes the element for what it is and not necessarily for how it is used.\n •A rank: The rank can be expressed as a number or anything comparable, and it is used to order the steps when presenting them to the visitor.\nHere’s an example of a guided tour:\n1 /**\n2 * Listens to incoming fuel card transactions from the 3 * external system of the Fuel Card Provider4 */5 @GuidedTour(name = """"Quick Developer Tour"""", 6     description = """"The MQ listener which triggers a full 7 chain of processing"""", rank = 1)8 public class FuelCardTxListener {\nIt then goes through other steps, until the last one:\n1 @GuidedTour(name = """"Quick Developer Tour"""", 2    description = """"The DAO to store the resulting 3    fuel card reports after processing"""", rank = 7)4 public class ReportDAO {56 public void save(FuelCardTransactionReport report){7 ..."",3097",3176,"**Creating a Guided Tour: Identifying Authoritative Knowledge**\n\n**Using Custom Attributes**:\n- `PointOfInterest` annotation can be used to mark code elements as points of interest\n- Can also create custom attributes like `CoreConceptAttribute` and `SightSeeingSite`\n- Use generic attribute with specific parameter, e.g., `@KeyCalculation()`\n\n**Creating a Guided Tour**:\n- Takes newcomers through the complete chain of processing an incoming transaction\n- Separates domain logic and infrastructure logic, but spans both in one guided tour\n- Anchored on code elements (classes, methods, fields, packages)\n- Uses `@GuidedTour` custom attribute with:\n  - Name of the guided tour (optional if only one tour)\n  - Description of each step in the context of the tour\n  - Rank to order steps\n\n**Example**:\n- `FuelCardTxListener` class listens to incoming fuel card transactions from an external system\n- Other classes in the chain of processing, ending with `ReportDAO.save()` method to store the fuel card reports",1006,38.60916829109192
False,3,"75-Creating a Living Guided Tour.pdf,75-Creating a Living Guided Tour,""Chapter 5 Living Curation: Identifying Authoritative Knowledge 150\n  Note\nNot","75-Creating a Living Guided Tour.pdf,75-Creating a Living Guided Tour,""Chapter 5 Living Curation: Identifying Authoritative Knowledge 150\n  Note\nNote that the numbering here is not consecutive; it goes from 1 to 7, but there are \nonly 6 steps. In the good old BASIC line numbering style, you would number the \nlines 10, 20, 30, and so on to make it easier to add another step in between when you want to.\nIf you wanted to provide a simple selection of points of interest only for an \naudience of developers, you could stop here and rely on the user to do a search of the custom annotation to get the IDE to present the tour as a whole:\n1  Search results for 'flottio.annotations.GuidedTour' \n6 References: 23  flottio.fuelcardmonitoring.domain - (src/main/java/l...)4  - FuelCardMonitoring5   - monitor(FuelCardTransaction, Vehicle)6  - FuelCardTransaction7  - FuelCardTransactionReport 89  flottio.fuelcardmonitoring.infra - (src/main/java/l...)10 - FuelCardTxListener11 - ReportDAO\nThe recap is all here, but it is not pretty , and there is no ordering. This could be \nenough for a small list of the main landmarks that a developer can explore in any \norder desired, though, so do not discount the value of the integrated approach, as it is much simpler and may be more convenient than more sophisticated mechanisms.\nHowever, this first case is not enough for a guided tour that is meant to be visited \nin order, from start to finish. So the next step is to create a living document out of it so that it is a living guided tour.\nCreating a Living Guided Tour\nGoing further than in the preceding section, you can create a little mechanism to scan the code base to extract the information about each step of the guided tour and produce a synthetic report of the guided tour in the form of a ready-to-follow and ordered itinerary . \nFuelCardTxListener\nThe MQ listener which triggers a full chain of  processing.\nGuided T ours and Sightseeing Maps 151\nListens to incoming fuel card transactions from the external system of the fuel \ncard provider.\nFuelCardTransaction\nThe incoming fuel card transaction. \nA transaction, between a card and a merchant, as reported by the fuel card \nprovider.\nFuelCardMonitoring\nThe service that takes care of  all the fuel card monitoring. \nMonitoring of fuel card use to help improve fuel efficiency and detect fuel \nleakages and potential driver misbehaviors.\nmonitor(transaction, vehicle)\nThe method that does all the potential fraud detection for an incoming fuel card \ntransaction.\n1  public FuelCardTransactionReport monitor(FuelCardTransaction \n2  transaction, Vehicle vehicle) {3    List<String> issues = new ArrayList<String>(); 45    verifyFuelQuantity(transaction, vehicle, issues);6    verifyVehicleLocation(transaction, vehicle, issues); 78  MonitoringStatus status 9     = issues.isEmpty() ? VERIFIED : ANOMALY;9  return new FuelCardTransactionReport( 10    transaction, status, issues);11 }\nFuelCardTransactionReport\nThe report for an incoming fuel card transaction. \nThe fuel card monitoring report for one transaction, with a status and any \npotential issue found.\nReportDAO\nThe DAO to store the resulting fuel card reports after processing.\nNote that in this guided tour, each title is a link to the corresponding line of code \non GitHub. When the point of interest is a method (like the monitor()  method), \nI  include its block of code verbatim from GitHub, for convenience. In a similar \nChapter 5 Living Curation: Identifying Authoritative Knowledge 152\nfashion, when the point of interest is a class, I might include an outline of the non-\nstatic fields and the public methods if I find it convenient and relevant to the focus of the guided tour.\nThis living guided tour document is generated in Markdown, for convenience. \nThen a tool like Maven site (or sbt or any other similar tool) could do the rendering to a web page or in any other format. An alternative, as shown here, is to use a Java-Script library to render the Markdown in the browser, which requires no additional toolchain.\nAn alternative to using strings in the guided tour annotations would be to use \nenums, which take care of naming, descriptions, and ordering at the same time. However, this moves the descriptions of each step of the guided tour from the anno-tated code to the enum class, as you can see here:\n1 public enum PaymentJourneySteps {\n2   REST_ENDPOINT(""""The single page app call this endpoint withthe id of the shopping cart""""),3   AUTH_FILTER(""""The call is being authenticated""""),   4   AUDIT_TRAIL(""""The call is audit-trailed in case of disputeand to comply to regulation""""),56   PAYMENT_SERVICE(""""Now enter the actual service to performthe job""""),7   8   REDIRECT(""""The response from the payment is sent through aredirect"""");910 private final String description;11 }\nThis enum is then used as the value in the annotation:\n1  @PaymentJourney(PaymentJourneySteps.PAYMENT_SERVICE)2  public class PaymentService...\nThe Implementation of the Guided Tour\nIn Java you can use a Doclet-like library called QDox to do the implementation grunt \nwork, which allows you to access the Javadoc comments. If you don’t need Javadoc, then any parser and even pain reflection could work.\nQDox scans every Java file in src/main/java, and from the collection of parsed ele-\nments, you can do the filtering by annotation. When a Java element (class, method,"",5354",5441,"**Creating a Living Guided Tour**\n\n**Step 1: Identify Authoritative Knowledge**\n- Search results for 'flottio.annotations.GuidedTour'\n- References:\n  - `FuelCardMonitoring` domain (`src/main/java/l...`)\n    - `FuelCardMonitoring`\n    - `monitor(FuelCardTransaction, Vehicle)`\n  - `FuelCardTxListener`\n  - `ReportDAO`\n\n**Step 2: Extract Information from Code Base**\n- Create a living document to order and present the information as a guided tour\n\n**Recap of Points of Interest:**\n- `FuelCardTransactionMonitoring` service\n  - Listens to incoming fuel card transactions\n- `FuelCardTransaction`\n  - Transaction between card and merchant\n- `monitor(transaction, vehicle)` method\n  - Performs potential fraud detection for an incoming transaction\n- `FuelCardTransactionReport`\n  - Report for an incoming fuel card transaction\n- `ReportDAO`\n  - DAO to store the resulting fuel card reports after processing\n\n**Creating a Living Document:**\n- Use Markdown to generate a ready-to-follow and ordered itinerary\n- Include links to corresponding code snippets on GitHub for reference\n\n**Step 3: Implement Guided Tour Annotations**\n- Alternative: use enums instead of strings in the guided tour annotations\n  - Takes care of naming, descriptions, and ordering at the same time\n\n**Example Enum:**\n```java\npublic enum PaymentJourneySteps {\n  REST_ENDPOINT(""The single page app calls this endpoint with the id of the shopping cart""),\n  AUTH_FILTER(""The call is being authenticated""),\n  AUDIT_TRAIL(""The call is audit-trailed in case of dispute and to comply to regulation""),\n  PAYMENT_SERVICE(""Now enter the actual service to perform the job""),\n  REDIRECT(""The response from the payment is sent through a redirect"");\n\n  private final String description;\n\n  PaymentJourneySteps(String description) {\n    this.description = description;\n  }\n}\n```\n\n**Step 4: Implement Guided Tour using QDox**\n- Use JavaDoc comments or any parser to extract information from code base\n- Filter the parsed elements by annotation and implement the guided tour grunt work\n\n**Implementing with QDox:**\n- Scans every Java file in `src/main/java`\n- Performs filtering based on the annotations",2152,81.19177055358887
False,3,"76-A Poor Mans Literate Programming.pdf,76-A Poor Mans Literate Programming,,0","76-A Poor Mans Literate Programming.pdf,76-A Poor Mans Literate Programming,,0",78,"**76-A Poor Man's Literate Programming**\n\nThis phrase is a term used in programming and software development. It refers to an approach or methodology for writing high-quality code using plain English documentation integrated into the source code itself, making it easier for other developers to understand and work with. This approach became popular during the early days of computing when resources were limited, and formal documentation tools were not readily available.\n\n**Keywords:** Programming, Software development, Approach, Methodology, Code quality, Documentation, Plain English, Source code, Early computing.\n\n**Additional Information**\n- **Advantages of Poor Man's Literate Programming**: Helps developers understand the purpose and functionality of the code; promotes consistency in naming conventions and commenting style; allows for easy modification of existing code by providing context; reduces the need for separate documentation tools.\n- **Disadvantages of Poor Man's Literate Programming**: May lead to bloated source code with excessively lengthy comments; lacks a standard format or structure for documentation; can result in inconsistent commenting styles within large projects.\n- **Origin**: Named after the practice of using plain English comments in the source code itself, rather than relying on formal documentation tools. This approach was popular during the early days of computing when resources were limited and formal documentation tools were not readily available.",1499,33.1394681930542
False,3,"77-Adding Whats Missing When Needed.pdf,77-Adding Whats Missing When Needed,""Summing Up: The Curator Preparing an Art Exhibition 153\npackage, and so","77-Adding Whats Missing When Needed.pdf,77-Adding Whats Missing When Needed,""Summing Up: The Curator Preparing an Art Exhibition 153\npackage, and so on) has the custom GuidedTour  annotation, it is included in the \nguided tour. Y ou can extract the parameters of the annotation and also extract the \nname, Javadoc comment, line of code, and other information (including the code itself, when necessary). Y ou can then turn all that into fragments of Markdown for each step, stored in a map sorted by the step rank criteria. This way , when the scan is done, you can render the whole document by concatenating all the fragments in the rank ordering.\nOf course, the devil is in the details, and this kind of code can quickly grow hairy , \ndepending on how demanding you are with respect to the end result. Scanning code and traversing the Java or C# metamodel is not always nice. In the worst case, you could even end up with a visitor pattern. I expect that more mainstream adoption of these practices will lead to new small libraries which will take care of most of the grunt work for common use cases.\nA Poor Man’s Literate Programming\nA guided tour is reminiscent of literate programming but in reverse: Instead of hav-ing prose with code, a guided tour has code with prose. For a sightseeing map, you \nonly have to select the points of interest and group them by big themes. For a guided tour, you need to devise a linear ordering of the code elements. In literate program-ming, you also tell a linear story that progresses through the code and ends up with a document explaining the reasoning and the corresponding software at the same time.\nA guided tour or sightseeing map is not just a documentation concern but also \na way to encourage continuous reflection on your own work as you do it. It would therefore be a good idea to document a guided tour as soon as you are building the early walking skeleton of the application. This way , you will benefit from the thoughtful effect of doing the documentation at the same time of doing the work.\nSumming Up: The Curator Preparing an Art Exhibition\nAs a concluding note on the topic of living curation, let’s now go back to the approach of the curator in art exhibition, as illustrated in Figure 5.4. \nThe curator of an exhibition primarily decides on a key editorial focus, which \noften becomes the title of the event. Sometimes the focus is trivial, such as “Claude Monet, the Surrealist,” but even in this case, there is an opiniated decision—to exclude prior art from the artist that was not yet Surrealism. Similarly , any documen-tation initiative must clearly deliver one key message.\nChapter 5 Living Curation: Identifying Authoritative Knowledge 154\nGood exhibitions try to bring an element of surprise to create interest (for exam-\nple, “Y ou’ve always thought Kandinsky paintings are fully abstract, but we’ll show \nhow the abstract shapes evolved from his prior figurative paintings.”). Visitors come not just to see the art pieces but also to expand their cultural awareness and better understand relationships between artists, art pieces, and their era. Similarly , good documentation adds value and new knowledge, with emphasis on relationships, by offering a different perspective of things. \nSelecting and Organizing Existing Knowledge\nThe curator selects art works based on the chosen editorial focus. Most of the pieces available are left in the storage room, and only the few pieces of particular interest for the exhibit are on display . Similarly , documentation is a curation activity that involves deciding what’s most important in a given perspective. \nThe curator decides which pieces to display in each room. A room may be organ-\nized around a time period, a phase in the life of the artist, or a theme. Art pieces may be displayed side-by-side to suggest comparisons between them. They may be displayed with an ordering that tells a story , chronologically or through a succession of themes. Organization of knowledge is a key tool for adding meaning to a plain collection of pieces of knowledge. We group elements by named folders, tags, or naming conventions.Figure 5.4 The curator in the museum"",4123",4206,"**Guided Tours and Code Documentation:**\n* Extracting annotations from Java or C# code using guided tours\n* Converting extracts into Markdown fragments for document rendering\n* Achieving a linear ordering of code elements (guided tour)\n* Benefits: Encourages reflection on work, adds value with new knowledge\n* Curator in art exhibition as analogy to guided tour\n	+ Decides editorial focus for exhibition or documentation\n	+ Selects pieces based on the chosen perspective\n	+ Organizes pieces in exhibitions or documents for meaning\n\n**Guided Tours and Code Documentation (Continued):**\n* Extracting annotations from Java or C# code using guided tours\n* Converting extracts into Markdown fragments for document rendering\n* Achieving a linear ordering of code elements (guided tour)\n* Benefits: Encourages reflection on work, adds value with new knowledge\n* Curator in art exhibition as analogy to guided tour (Continued)\n	+ Decides editorial focus for exhibition or documentation\n	+ Selects pieces based on the chosen perspective\n	+ Organizes pieces in exhibitions or documents for meaning\n\n**Curating Art Exhibitions:**\n* Curator decides on key editorial focus for art exhibition\n* Curator selects artworks based on the chosen editorial focus\n* Curator organizes artworks by time period, artist phase, or theme\n* Artworks displayed side-by-side to suggest comparisons and tell a story.\n\n**Curating Documentation:**\n* Documentation is a curation activity that involves deciding what's most important\n* Organize knowledge in folders, tags, or naming conventions for added meaning\n* Select pieces based on chosen perspective, just as curators select artworks for exhibitions.",1672,68.2581741809845
False,3,"78-Chapter 6 Automating Documentation.pdf,78-Chapter 6 Automating Documentation,""Summing Up: The Curator Preparing an Art Exhibition 155\nAdding What’","78-Chapter 6 Automating Documentation.pdf,78-Chapter 6 Automating Documentation,""Summing Up: The Curator Preparing an Art Exhibition 155\nAdding What’s Missing When Needed\nThe curator writes a few bits of text explaining the big idea of each section of the \nexhibition. She or he also writes a small label for each piece of art that is displayed on the wall directly next to the appropriate art piece. Similarly , documentation needs knowledge augmentation, which can occur through annotations, DSL, or naming conventions. Some limited amount of text can be useful in some places, too. This knowledge is attached to the related code elements whenever possible.\nWhen a work considered essential for the art exhibition is not in the collection, it \nis borrowed or commissioned from the artist. The artist may also contribute to the organization of his or her pieces directly .\nSometimes some information is missing. The curator can have researchers con-\nduct investigations or may request chemical analysis on the painting or by looking at written archives to find the missing piece in the puzzle. For example, the Louvre uses research results on the style of brushing colors on the canvas in order to tell visitors how much Raphael really participated in each of his paintings. And it reveals that the famous master did not touch many of them! In a similar way , documentation is a feedback mechanism that helps you notice when something is missing or wrong in the code or in the related knowledge.\nAccessibility for People Who Can’t Attend and for Posterity\nThe curator creates a catalog of the exhibition, which recaps all the content dis-played: the explanative text by section, the art pieces as quality pictures, and their labels. The catalog as a book is usually organized in a way that is similar to the organization of the rooms in the exhibition venue.\nMuseums now sometimes offer expensive and heavyweight complete exhibition \ncatalogs, and they also offer catalogs in a shorter form, with just a digest of the major pieces. I usually buy the shorter catalog, which is a more attractive read by far!\nDocumentation also involves making knowledge accessible and ensuring that \nthe important pieces are persisted for the future. Y ou may , for example, publish content as documents and on an interactive website, targeted for different audi-ences and different needs—much like the different catalogs published by the art museum.\nChapter 5 Living Curation: Identifying Authoritative Knowledge 156\nSummary\nBecause the typical quantity of knowledge in any real-life code base is huge, any \nattempt at making use of it involves discarding most of it, through a process of cura-tion that itself adds value to the curated knowledge by focusing on the essentials.\nLiving curation, inspiring exemplars, highlighting the core, and providing guided \ntours and sightseeing maps are some possible approaches to curation that can high-light a subset of knowledge for a particular purpose."",2918",3005,"**Automating Documentation**:\n- Curator writes explanatory text for exhibition sections and labels for art pieces\n- Documentation requires knowledge augmentation through annotations, DSL, or naming conventions\n- Limited amount of text can be useful in some places\n- Missing essential works are borrowed or commissioned from the artist\n- Researchers can conduct investigations or request chemical analysis to fill in missing information\n- Catalog of exhibition includes explanatory text, art pieces as quality pictures, and labels\n- Catalog may be available in different forms: comprehensive and expensive, or shorter and more accessible\n- Documentation helps identify when something is missing or wrong\n\n**Living Curation**:\n- Curator identifies authoritative knowledge by discarding most of the existing knowledge base\n- Living curation involves:\n  - Inspiring exemplars\n  - Highlighting the core\n  - Providing guided tours and sightseeing maps\n- These approaches help curate a subset of knowledge for a particular purpose.",1024,25.479793548583984
False,3,"79-Steps in Creating a Living Document.pdf,79-Steps in Creating a Living Document,""157Chapter 6\nAutomating Documentation\nAs described earlier, livin","79-Steps in Creating a Living Document.pdf,79-Steps in Creating a Living Document,""157Chapter 6\nAutomating Documentation\nAs described earlier, living documentation does not necessarily require producing \nformal documents in order to deal with knowledge. However, there are a number of situations in which it is desirable to produce traditional-looking documents. In such a case, the most obvious example of documentation that is really “living” is docu-ments that evolve at exactly the same pace as the knowledge they describe. Y ou need automation to make living documents possible.\nThis chapter introduces two important, related concepts: using automation to \nhelp create living documents.\nLiving Documents\nA living document is a document that is evolving at the same pace as the system it describes. It’s prohibitively time-consuming to create a living document manually , so a living document is usually achieved through automation.\nAs the names suggests, living documentation relies a lot on living documents, \nwhich are required when other means of documentation cannot keep up with the pace of change, or are not accessible for the intended audience.\nA living document works like a reporting tool that produces a new report after \neach change. A change is usually a code change but could also be a key decision made during a conversation.\nThis chapter presents a few key examples of living documents, including living \nglossaries and living diagrams."",1387",1476,"**Living Documentation: Automating Document Production**\n\n**Chapter Overview:**\n- Description of living documentation\n- Benefits and necessity for automation\n- Examples of living documents (living glossaries, diagrams)\n- Use of living documents as reporting tools\n\n**Automating Living Documents**\n- Not all forms of living documentation require manual production\n- Time-consuming to create manually\n- Automation makes living documents possible\n\n**Examples of Living Documents:**\n- Code changes\n- Decisions made during conversations\n- Evolution at the same pace as system they describe\n\n**Living Document Examples:**\n- **Living Glossaries**: evolve with changes in terminology or meaning\n- **Living Diagrams**: updated after modifications to a system or process\n\n**Role of Automation:**\n- Enables real-time document updates\n- Reduces manual labor and potential errors.",867,22.249907970428467
False,3,"80-Presentation Rules.pdf,80-Presentation Rules,,0","80-Presentation Rules.pdf,80-Presentation Rules,,0",50,**Presentation Rules**:\n- Provide concise bulleted notes summarizing the provided text.\n- Headings and terms in boldface type.\n\n**Presentation Rules**:\n* The speaker should follow certain rules for effective presentation:\n* **Concision**: Provide concise bulleted notes summarizing the provided text.\n* Use headings and terms in **boldface type**.,347,10.115968227386475
False,3,"81-Living Glossaries.pdf,81-Living Glossaries,""Chapter 6 Automating Documentation 158\nSteps in Creating a Living Document\nCreating a living document","81-Living Glossaries.pdf,81-Living Glossaries,""Chapter 6 Automating Documentation 158\nSteps in Creating a Living Document\nCreating a living document typically involves four main steps:\n 1. Select a range of data stored somewhere, such as source code in source control.\n 2. Filter the data according to the objective of the document. 3. For each piece of data that made it out through the filter, extract the subset of \nits content that is of interest for the document. It can be seen as a projection, \nand it’s specific to the purpose of the diagram.\n 4. Convert the data and the relationships in the data into the target format to \nproduce the document. For a visual document, this target can be a sequence of calls to the API of the rendering library . For a text document, it can be a list of text snippets consumed by a tool to produce a PDF .\nIf the rendering is very complex, the step of converting into another model may \nbe multiple times—to create intermediate models that are then chained to drive the final rendering library .\nThe hard part in each step is the interplay between the editorial perspective and \nthe presentation rules. What data should be selected or ignored? What information should be added from another source? What layout should be used?\nPresentation Rules\nA good document must follow particular rules, such as showing or listing no more than five to nine items at a time. There are also rules for choosing a particular  layout—such as a list or a table or a chart—so that it is congruent with the structure of the problem. This is not a book on that topic, but some awareness of such presen-tation rules will help you make your documents more efficient.\nLiving Glossaries\nHow do you share the ubiquitous language of the domain with everyone involved in a project? The usual answer is to provide a complete glossary of every term that belongs to the ubiquitous language, together with a description that explains what you need to know about it. However, the ubiquitous language is an evolving creature, so the glossary needs to be maintained, and there is a risk that it will become  outdated compared to the source code.\nLiving Glossaries 159\nIn a domain model, the code represents the business domain, as closely as possible \nto the way the domain experts think and talk about it. In a domain model, great code \ntells the domain business: Each class name, each method name, each enum constant name, and each interface name is part of the ubiquitous language of the domain. But not everyone can read code, and there is almost always some code that is not very related to the domain model.\nTherefore: Extract the glossary of the ubiquitous language from the source code. \nConsider the source code as the single source of truth and take great care in the naming of each class, interface, and public method whenever they represent domain concepts. Add the description of the domain concept directly to the source code, as structured comments that can be extracted by a tool. When extracting the glossary, find a way to filter out code that is not expressing the domain.\nAs illustrated on Figure 6.1, the living glossary processor scans the source code \nand its annotations to generate a living glossary that will remain up-to-date because it can be regenerated as frequently as desired.\n \nFor a successful living glossary , the code must be declarative. The more the code \nlooks like a DSL of the business domain, the better the glossary . Indeed, for develop-ers there is no need for a living glossary because the glossary is the code itself.  A  living glossary is especially useful for nondevelopers who don’t have access to the source core in an IDE. It brings additional convenience in being all in a single document.\nA living glossary is also a feedback mechanism. If a glossary does not look good, \nor if you find it hard to make the glossary work, you know you have something to improve in the code.Living Diagram\nLiving\nDiagram\nProcessor Source Code\nand Annotations Living Diag ram\nAlways Up-to-Date\nFigure 6.1 Overview of  a living glossary"",4066",4119,"**Creating Living Documentation**\n- **Living documents**: Involve four main steps:\n  - Select data (e.g., source code) based on objective\n  - Filter data to extract relevant information for document\n  - Convert data and relationships into target format (e.g., API calls, text snippets)\n  - Complex rendering processes may require multiple conversion steps\n- **Hard part**: Interplay between editorial perspective and presentation rules\n\n**Presentation Rules**\n- Documents must follow particular rules: limit information to 5-9 items at a time\n- Choose layout (e.g., list, table, chart) that aligns with problem structure\n\n**Living Glossaries**\n- Share ubiquitous language of domain through glossary\n  - Complete description of terms and their meaning\n  - Maintain and update regularly to remain relevant to source code\n- **Code as single source of truth**: Extract glossary from source code\n  - Use class, interface, method names as part of ubiquitous language\n  - Add descriptions to source code as structured comments for extraction\n- Filter out irrelevant code when extracting glossary\n- Living glossaries are declarative and a single document for nondevelopers\n\n**Living Diagrams**\n- Process source code and annotations to generate living diagrams\n- Keep up-to-date with source code changes",1294,48.37144613265991
False,3,"82-How a Living Glossary Works.pdf,82-How a Living Glossary Works,,0","82-How a Living Glossary Works.pdf,82-How a Living Glossary Works,,0",68,"**Living Glossary: Understanding Its Functionality**\n\nA living glossary works by:\n- Maintaining an organized collection of terms, definitions, and related information\n- Updating dynamically to reflect changes or additions\n- Enabling efficient access to information, fostering understanding.",290,7.911496877670288
False,3,"83-An Example Please.pdf,83-An Example Please,""Chapter 6 Automating Documentation 160\nHow a Living Glossary Works\nIn many languages, documentation c","83-An Example Please.pdf,83-An Example Please,""Chapter 6 Automating Documentation 160\nHow a Living Glossary Works\nIn many languages, documentation can be embedded directly within the code as \nstructured comments, and it is good practice to write a description of what a class, an interface, or an important method is about. T ools like Javadoc can then extract the comments and create a report based on them. With Javadoc you can create your own Doclet (documentation generator) based on the provided Doclet, and it does not take a lot of effort. By using a custom Doclet, you can export custom documen-tation in any format.\nAnnotations in Java and attributes in C# are great for augmenting code. For \nexample, you can annotate classes and interfaces with custom domain stereotypes (\n@DomainService , @DomainEvent , @BusinessPolicy , and so on) or domain-\nirrelevant stereotypes ( @AbstractFactory , @Adapter , and so on). This makes it \neasy to filter out classes that do not contribute to expressing the domain language. Of course, you need to create a small library of annotations to augment your code.\nIf done well, these annotations also express the intention of the developer who \nwrote the code. They are part of a deliberate practice.\nIn the past I have used the approach just described to extract a reference busi-\nness document that I could then send directly to a customer abroad. I was using a custom Doclet to export an Excel spreadsheet with one tab for each category of busi-ness domain concepts. The categories were simply based on the custom annotations added to the code.\nAn Example Please!\nLet’s look at a brief and oversimplified example of a living glossary about a kitten, because everybody loves kittens. The following code base in pseudo-code represents the main activities of a cat:\n1  module com.acme.catstate\n23  // The set of the main activities of a cat4  @CoreConcept5  interface CatActivity67  // How the cat changes its activity in response to an event8  @CoreBehavior9  @StateMachine10 CatState nextState(Event)1112 // The cat is sleeping with its two eyes closed\n13 class Sleeping -|> CatActivity\nLiving Glossaries 161\n14\n15 // The cat is eating, or very close to the dish\n16 class Eating -|> CatActivity1718 // The cat is actively chasing, eyes wide open\n19 class Chasing -|> CatActivity2021 @CoreConcept22 class Event // Anything  that can happen that matters to the cat23 void apply(Object)2425 class Timestamp // technical boilerplate\nThis is just plain source code that describes the domain of the daily life of a cat. \nHowever, it is augmented with annotations that highlight what’s important in the \ndomain.\nA processor that builds a living glossary out of this code will print a glossary like \nthe following:\n1  Glossary\n2  --------34  CatActivity: The set of the main activities of a cat.5  - Sleeping: The cat is sleeping with its two eyes closed6  - Eating: The cat is eating, or very close to the dish7  - Chasing: The cat is actively chasing, eyes wide open89  nextState: How the cat changes its activity in response to10 an event1112 Event: Anything that can happen that matters to the cat\nNotice that the Timestamp  class and the Event  method have been ignored here \nbecause they don’t matter for the glossary . Also, each separate class that imple-\nments CatActivity  has been presented together with the interface they implement, \nbecause that’s the way we think about that particular construction.\n Note\nThis is the state design pattern, and here it is genuinely part of the business domain."",3532",3585,"**Living Glossary Example using Java Annotations:**\n* **Java Documentation**: embedded within code as structured comments, extracted using tools like Javadoc\n* **Custom Doclet**: created to generate custom documentation in desired format\n* **Annotations in Java and C#**: used for augmenting code with domain stereotypes or other metadata\n	+ Easy to filter out classes not expressing domain language\n	+ Expresses developer's intention\n* **Example of a Living Glossary**: based on a simple pseudo-code example about a cat's activities\n1. **Module com.acme.catstate**\n2. **CatActivity interface**: set of main activities of a cat\n3. **CatState nextState(Event) method**: how the cat changes its activity in response to an event\n4. **Classes implementing CatActivity:** Sleeping, Eating, Chasing\n5. **Event class**: anything that can happen that matters to the cat\n6. **Living Glossary Generation**: processor builds a glossary from code annotations\n	+ Prints out important terms and their definitions\n* **Ignored Classes**: Timestamp (technical boilerplate) and Event method apply(Object) as they don't matter for the glossary.\n7. **State Design Pattern**: genuinely part of business domain, implemented using the state pattern design concept.",1241,34.052507638931274
False,3,"84-Information Curation for Living Documents.pdf,84-Information Curation for Living Documents,""Chapter 6 Automating Documentation 162\nBuilding the gl","84-Information Curation for Living Documents.pdf,84-Information Curation for Living Documents,""Chapter 6 Automating Documentation 162\nBuilding the glossary out of the code is not an end to itself; from this first gener-\nated glossary you might notice that the entry nextState  is not as clear as you’d \nexpect it to be. (This is more visible in the glossary than in the code.) So you go back \nto the code and rename the method nextActivity() .\nAs soon as you rebuild the project, the glossary is updated because it is, after all, \na living glossary:\n1  Glossary\n2  --------34  CatActivity: The set of the main activities of a cat.5  - Sleeping: The cat is sleeping with its two eyes closed6  - Eating: The cat is eating, or very close to the dish7  - Chasing: The cat is actively chasing, eyes wide open89  nextActivity: How the cat changes its activity in response 10 to an event1112 Event: Anything that can happen that matters to the cat\nInformation Curation for Living Documents\nThe technique just described requires a parser for the programming language, and \nthe parser must not ignore the comments. For Java, there are many options, includ-ing like Antlr, JavaCC, Java annotation-processing APIs, and several open-source tools. However, the simplest option is to go with a custom Doclet, and this is the approach described here.\n Note\nEven if you don’t care about Java, you can still read on; the important informa-tion here is largely language agnostic.\n \nIn a simple project that covers only one domain, one single glossary is enough. \nThe Doclet is given the root of the Javadoc metamodel, and from this root it scans all programming elements, including classes, interfaces, and enums.\nFor each class, the main question is “Does this matter to the business enough to \nbe included in the glossary?” Java annotations can go a long way toward answering this question. If you use a “business meaningful” annotation, each class that has this annotation is a strong candidate for the glossary .\nLiving Glossaries 163\n Caution\nIt is preferable to avoid strong coupling between the code that processes annota-\ntions and the annotations themselves. T o avoid this coupling, annotations can be recognized just by their prefix (for example, \norg.livingdocumentation.* ) or \nby their unqualified name (for example, BusinessPolicy ). Another approach is \nto check annotations that are themselves annotated by a meta-annotation, such as \n@LivingDocumentation . Such a meta-annotation can itself be recognized by \nsimple name only to avoid direct coupling.  \nFor each class to be included, the Doclet then drills down the members of the class \nand prints everything that is of interest for the glossary , in a way that is appropriate for the glossary .\nSelectively showing and hiding the relevant portions of the source code and group-\ning the related elements are critically important. If it weren’t for this, the standard Javadoc would be enough. At the core of a living glossary are all the editorial deci-sions on what to show , what to hide, and how to present the information in the most appropriate way . It’s hard to make such decisions outside a context. I won’t tell how to do it step by step, but I do give some examples of selective curation:\n •An enum and its constants\n •A bean and its direct non-transient fields\n •An interface, its direct methods, and its main subclasses that are not technical and not abstract\n •A value object and its methods that are “closed under operation”1 (that is, \nmethods that only involve the type itself)\nFor a relevant glossary , a lot of details from the code usually must be hidden:\n •Y ou typically ignore all methods from the super-object, such as toString()  \nand equals() .\n •Y ou typically ignore all transient fields because they are there just for optimiza-tion purposes and seldom mean anything for the business.\n •Y ou typically ignore all constant fields, except the public static final of the type itself, if they represent important concepts of the business.\n •Marker interfaces often don’t need to list their subclasses, and the same may be true for any interface that has only one method.\n1. Evans, Eric. Domain-Driven Design: T ackling Complexity in the Heart of  Software. Hoboken: \n Addison-Wesley Professional, 2003. See the section “Closure of Operations.”"",4267",4368,"**Information Curation for Living Documents**\n\n**Automating Documentation**:\n- Building a glossary from code:\n  - Rename methods to make entries clearer\n  - Rebuild project to update glossary\n- For Java, use:\n  - Parser like Antlr or JavaCC\n  - Java annotation-processing APIs\n  - Open-source tools\n- Simplest option: custom Doclet approach\n\n**Creating a Living Glossary**:\n- Covers all elements that matter to the business\n- Scans classes, interfaces, and enums in Javadoc metamodel\n- Includes classes with ""business meaningful"" annotations\n- Avoids strong coupling between code and annotations\n\n**Selective Curation of Information**:\n- Show/hide relevant portions based on context\n- Examples: enum, bean, interface, value object\n- Hide details like methods from super-object, transient fields, constant fields (except public static final), marker interfaces, and their subclasses.\n\n**Important Concepts**:\n- Domain-Driven Design (DDD)\n  - Eric Evans: ""Domain-Driven Design: Tackling Complexity in the Heart of Software""\n  - Closure of Operations (closed under operation)",1072,34.004560708999634
False,3,"85-Case Study of a Living Glossary.pdf,85-Case Study of a Living Glossary,""Chapter 6 Automating Documentation 164\nThe selective filtering depends to","85-Case Study of a Living Glossary.pdf,85-Case Study of a Living Glossary,""Chapter 6 Automating Documentation 164\nThe selective filtering depends to a large extent on the style of the code. If con-\nstants are usually used to hide technical literals, then they should be mostly hidden, \nbut if they are usually used in the public API, they may be of interest for the glossary .\nDepending on the style of code, you can adjust the filtering so that it does most of the \nwork by default, even if it goes too far in some cases. T o supplement or deviate from this default filtering, you can use an override mechanism (for example, by using annotations).\nFor example, the selective filtering may ignore every method by default; in this \ncase, you have to define an annotation to distinguish the methods that should appear in the glossary . However, I would never use an annotation named \n@Glossary  because \nit would be noise in the context of the code. A class or method is not meant to belong to a glossary or not; it is meant to represent a concept of the domain or not. But a method can represent a core concept of the domain and be annotated as such with a \n@CoreConcept  annotation that can be used to include the method in the glossary .\nFor more on curation, see Chapter 5, “Living Curation: Identifying Authoritative \nKnowledge.” For more on the proper usage of annotations to add meaning to code, refer to Chapter 4, “Knowledge Augmentation.”\nCreating a Glossary Within a Bounded Context\nIn domain-driven design, a ubiquitous language can be defined with no ambiguity only within a given bounded context. If you are not comfortable with bounded con-texts, don’t worry; for this discussion you may replace the term bounded context with module about a set of  cohesive use-cases.\nIf the source code spans several bounded contexts, you need to segregate the \n glossary by bounded context. In order to do that, the bounded contexts must be explicitly declared.\nY ou can use annotations to declare the bounded contexts, but this time \nthe  annotations will be on modules. In Java they are package annotations, using the pseudo-class \npackage-info.java :\n1 package-info.java\n23 // Cats have many fascinating activities, and the way they   4 // switch from one to another can be simulated by Markov 5 // chains.6 @BoundedContext(name = """"Cat Activity"""")7 package com.acme.lolcat.domain\nThis is the first bounded context in the application, and you have another bounded \ncontext, again on cats but this time from a different perspective:\n1  package-info.java2\nLiving Glossaries 165\n3 // Cats moods are always a mystery. \n4 // Yet we can observe cats with a webcam and use image5 // processing to detect moods and classify them into mood 6 // families. 78 @BoundedContext(name = """"Cat Mood"""")9 package com.acme.catmood.domain\nWith several bounded contexts, the processing is a bit more complicated because \nthere will be one glossary for each bounded context. Y ou need to inventory all the \nbounded contexts and then assign each element of the code to the corresponding glossary . If the code is well structured, the bounded contexts are clearly defined at the roots of modules, so a class obviously belongs to a bounded context if it belongs to a particular module.\nThe processing then proceeds as follows:\n 1. Scan all packages and detect each context. \n 2. Create a glossary for each context.  3. Scan all classes, and for each class, find out what context it belongs to. This \ncan simply be done from the qualified class name (for example, \ncom.acme. \ncatmood.domain.funny.Laughing ) that starts with the module qualified \nname (for example, com.acme.catmood.domain ).\n 4. For each glossary , apply the selective filtering and curation process described \nabove for building a nice and relevant glossary .\nThis process can be enhanced to suit your taste. A glossary may be sorted by entry \nname or sorted by importance of concepts.\nCase Study of a Living Glossary\nLet’s take a close look at a sample project in the domain of music theory and MIDI. \nFigure 6.2 shows what you see when you open the project in an IDE. \nThere are two modules, each containing a single package. Each module defines a \nbounded context. The first one, which focuses on Western music theory , is shown in Figure 6.3.\n \nThe second bounded context, which focuses on MIDI, is shown in Figure 6.4. \nChapter 6 Automating Documentation 166\nFigure 6.2 Tree view of  the code base\nFigure 6.3 Declaration of  the first bounded context as a package annotation\nFigure 6.4 Declaration of  the second bounded context as a package annotation\nFrom the second bounded context, Figure 6.5 shows an example of a simple value \nobject with its Javadoc comment and annotation. \nFrom the first context, Figure 6.6 shows an example of an enum that is a value \nobject as well, with its Javadoc comments, the Javadoc comments on its constants, \nand the annotation. \nNote that there are other methods, but they will be ignored for the glossary .\nLiving Glossaries 167\nFigure 6.5 A value object with its annotation\nFigure 6.6 An enum with its annotation\nChapter 6 Automating Documentation 168\nStarting with Something and Adjusting Manually\nT o create the living glossary processor, you need to create a custom Doclet that cre-\nates a text file and prints the glossary title in Markdown:\n1 public class AnnotationDoclet extends Doclet {\n23   //...45   // doclet entry point6   public static boolean start(RootDoc root) {7     try {8       writer = new PrintWriter(""""glossary.txt"""");9       writer.println(""""# """" + """"Glossary"""");10      process(root);11      writer.close();12    } catch (FileNotFoundException e) {13      //...14    }15    return true;16  }\nWhat’s left to implement is the method process() , which enumerates all classes \nfrom the Doclet root and checks whether each class is meaningful for the business:\n1          public void process() {2                  final ClassDoc[] classes = root.classes();3                  for (ClassDoc clss : classes) {4                          if (isBusinessMeaningful(clss)) {5                                  process(clss);6                          }7                  }8          }\nHow do you check whether a class is meaningful for the business? Here you do \nit only through annotation. In this case, you can consider that all annotations from \norg.livingdocumentation.*  mark the code as meaningful for the glossary . This \nis a gross simplification, but here it’s enough:\n1  private boolean isBusinessMeaningful(ProgramElementDoc doc){2    final AnnotationDesc[] annotations = doc.annotations();3    for (AnnotationDesc annotation : annotations) {4      if (isBusinessMeaningful(annotation.annotationType())) {\nLiving Glossaries 169\n5        return true;\n6      }7    }8    return false;9  }1011 private boolean isBusinessMeaningful(AnnotationTypeDoc                                             annotationType) {12   return annotationType.qualifiedTypeName()           .startsWith(""""org.livingdocumentation.annotation."""");13 }\nIf a class is meaningful, then you must print it in the glossary:\n1  protected void process(ClassDoc clss) {2    writer.println("""""""");3    writer.println(""""## *"""" + clss.simpleTypeName() + """"*"""");4    writer.println(clss.commentText());5    writer.println("""""""");6    if (clss.isEnum()) {7      for (FieldDoc field : clss.enumConstants()) {8         printEnumConstant(field);9      }10     writer.println("""""""");11     for (MethodDoc method : clss.methods(false)) {12        printMethod(method);13     }14   } else if (clss.isInterface()) {15     for (ClassDoc subClass : subclasses(clss)) {16        printSubClass(subClass);17     }18   } else {19     for (FieldDoc field : clss.fields(false)) {20        printField(field);21     }22     for (MethodDoc method : clss.methods(false)) {23        printMethod(method);24     }25   }26 }"",7846",7949,"**Creating a Living Glossary for Code Documentation**\n\n**Background:**\n- Automating documentation through code analysis using Doclet in Java\n- Two modules, each defining a bounded context\n- Annotations from org.livingdocumentation.* mark the code as meaningful for glossary\n\n**Process:**\n1. Create custom Doclet: `AnnotationDoclet`\n   - Enumerates all classes from root document\n   - Checks if class is meaningful using annotations\n2. Processing a Class: `process(ClassDoc clss)`\n   - Print relevant information in Markdown format\n      * **Class name**\n      * **Comments**\n   - Check for specific types (enum, interface, etc.)\n      * Handle enum constants and methods\n      * Interface subclasses\n3. Annotation Analysis: `isBusinessMeaningful(ProgramElementDoc doc)`\n   - Check if class annotated with org.livingdocumentation.* annotations is meaningful\n4. Outputting the Glossary\n   - Write to file ""glossary.txt"" in Markdown format\n\n**Improvements:**\n- Sorting glossary by entry name or importance of concepts\n- Handling more complex cases manually.",1054,43.4603750705719
False,3,"86-Diagrams Assist in Conversations.pdf,86-Diagrams Assist in Conversations,""Chapter 6 Automating Documentation 170\nThis method is too big and should","86-Diagrams Assist in Conversations.pdf,86-Diagrams Assist in Conversations,""Chapter 6 Automating Documentation 170\nThis method is too big and should be refactored, but for the sake of this expla-\nnation I wanted to show it all on one page, as a script. As you can see, this method \ndecides how to print the living glossary for each kind of element in the Java/Doclet metamodel (class, interface, subclass, field, method, enum, enum constant):\n1  private void printMethod(MethodDoc m) {\n2    if (!m.isPublic() || !hasComment(m)) {3      return;4    }5    final String signature = m.name() + m.flatSignature()6       + """": """" + m.returnType().simpleTypeName();7    writer.println(""""- """" + signature + """" """" + m.commentText());8  }9101112 private boolean hasComment(ProgramElementDoc doc) {13   return doc.commentText().trim().length() > 0;14 }\nY ou get the idea. The point is to have something working as soon as possible so \nyou can get feedback on the glossary generator (your custom Doclet) and on the code \nas well. Then it’s all about iterating: Y ou change the code of the glossary generator to improve the rendering of the glossary and to improve the relevance of its selective filtering, and you change the actual code of the project so that it is more expressive by adding annotations and creating new annotations if needed so that the code tells the whole business domain knowledge. This cycle of iterations should not take a lot of time; however, it never really finishes and does not have an end state because it’s a living process. There is always something to improve in the glossary generator or in the code of the project.\nA living glossary is not a goal in itself. It’s above all a process that helps a team \nreflect on its code so it can improve its quality .\nLiving Diagrams\nAutomation should make it easier to change code safely , not harder. If it’s getting \nharder, delete some. And never automate stuff in flux.\n—Liz Keogh (@lunivore) on Twitter\nSome problems are difficult to explain with words but are much easier to explain \nwith a picture. This is why we frequently use diagrams in software development for static structures, sequences of actions, and hierarchies of elements."",2135",2224,"**Chapter 6 Automating Documentation**\n\n**Method for Printing Living Glossary**:\n- Decides how to print the living glossary for each element in Java/Doclet metamodel: class, interface, subclass, field, method, enum, enum constant\n- Method `printMethod`:\n    - Checks if method is public and has a comment\n    - If not, returns\n    - Creates signature string with method name, flat signature, and return type\n    - Writes method comment to writer\n\n**Living Glossary**:\n- Aims to help team reflect on code and improve quality\n- Not a goal in itself\n- Automation should make it easier to change code safely, not harder\n- Delete automation if it's getting harder or is in flux\n\n**Diagrams in Software Development**:\n- Diagrams used for:\n    - Static structures\n    - Sequences of actions\n    - Hierarchies of elements\n- Diagrams help explain problems that are difficult to describe with words",888,26.29794144630432
False,3,"87-One Diagram One Story.pdf,87-One Diagram One Story,""Living Diagrams 171\nMost of the time we only need diagrams for the duration of a conversation.","87-One Diagram One Story.pdf,87-One Diagram One Story,""Living Diagrams 171\nMost of the time we only need diagrams for the duration of a conversation. Quick \nsketches on a napkin are perfect for such a case. Once the idea has been explained or \nthe decision made, you don’t need the diagram anymore.\nBut you might want to keep some diagrams because they explain important parts \nof a design that everybody should know . Most teams create diagrams and keep them as separate documents, such as slides or Visio or CASE tools documents.\nThe problem, of course, is that a diagram will become outdated. The code of the \nsystem changes, and nobody has time or remembers to update the diagram. Conse-quently , it’s very common for diagrams to be a bit incorrect. People get used to this and learn not to trust diagrams too much. The diagrams become increasingly useless until someone has the courage to delete them. From that point, it will require a lot of skills to look at the system as it is and try to recognize how it was designed and why . It becomes a matter of reverse engineering.\nThis is all frustrating, but the worst part is that important knowledge is lost in the \nprocess—and that knowledge was there at the beginning. Enter the living diagram: a diagram that you can generate again after a change so that it’s always up-to-date.\nTherefore: Whenever a diagram will be useful for the long term—for example, if \nit has already been used several times—you should set up a mechanism to auto-matically generate the diagram from the source code without any manual effort. Have your continuous integration trigger it on each build or on a special build that is run on demand at the click of a button. Don’t re-create or update the dia-gram manually each time.\nDiagrams Assist in Conversations\nUnexpected side effect of having a living diagram of the system: it makes development \nmore tangible. Y ou can point to things in discussions. \n—@abdullin on Twitter\nConversations and diagrams are not incompatible. Always being able to refer to the \nlatest version of a diagram that reflects the current state of the software is a catalyzer of discussions.\nOne Diagram, One Story\nWhen you create and maintain diagrams manually , given the time it takes, it’s tempt-ing to put as much as possible onto the same diagram to save effort, even if doing so is detrimental to its users. However, once diagrams are automated, there is no reason to make them more complicated. Creating another diagram is not so much effort, so you can create another diagram for each clearly identified purpose for an identified audience.\nChapter 6 Automating Documentation 172\nDiagram real estate is in limited supply , and so are the time and cognitive \nresources of its audience, which are the main reasons a diagram should convey only \none message.\nA document whose purpose is to clarify the external actors of a system for a non-\ntechnical audience should hide everything except the system as a black box and each actor with its nontechnical name and the business relationship with the system. It should not show anything about JBoss, HTTP , or JSON. It should not show compo-nents or service names. This selective perspective is what makes a document relevant or not. A document that tries to show different things at the same time requires more work from its audience and does not convey a clear message.\nAs illustrated with the funny poster in Figure 6.7, a living diagram should tell only \na single story at a time. If you want to tell several stories, make one diagram for each of them. Remember: one diagram, one story .\n \nTherefore: Remember that each diagram should have one and only one purpose. Resist the temptation to add extra information to an existing diagram. Instead, create another diagram that focuses on the extra information and remove other information that is less valuable for this new different purpose. Filter superflu-ous information aggressively; only the essential elements deserve to make it onto a diagram.\nA related anti-pattern is showing what’s convenient rather than showing what’s \nrelevant to an identified purpose. Remember the reverse-engineering/round-trip tools of the end of the 1990s? It was magical at the beginning, but eventually we all ended up with diagrams like the one in Figure 6.8 (or worse).\n \nFigure 6.7 One diagram, one story\nLiving Diagrams 173\nT oo much information is like no information at all: It’s equally useless. It takes a \nlot of serious filtering for diagrams to be useful! But if you clearly know the point of \na diagram, you’re already halfway there.\nA challenge with living diagrams is filtering and extracting only the relevant data \nout of the mass of available data. In any real-world code base, a living diagram with-out filtering is close to useless; it’s just a mess of boxes and wires that don’t help anyone understand anything (refer to Figure 6.8).\nA useful diagram tells one thing. It has a clear focus. It might show dependencies \nor hierarchy or workflow . Or it might show a particular decomposition of modules or a particular collaboration between classes, as in a design pattern. Y ou name it, but you choose only one. Because a living document is automatically generated, it’s easy to create one diagram for each aspect you want to explain; there’s no need to try to mix them. Determining the focus of a diagram is an editorial decision.\nOnce the focus is chosen, the filtering step selects only the elements that really con-\ntribute to the focus and ignore the rest. Ideally there should be a maximum of seven to nine elements at this stage. Then, for each element, the extraction step extracts \nFigure 6.8 How useful is this diagram?"",5681",5742,"**Diagrams for Effective Communication**\n\n**Living Diagrams**:\n- Useful for long-term use, such as important design aspects\n- Automatically generated from source code without manual effort\n- Keep diagrams up-to-date and maintain consistency\n\n**Benefits of Living Diagrams**:\n- Make development more tangible for discussions\n- Catalyst for clearer communication between team members\n- Convey one message at a time, avoiding confusion\n\n**Creating Effective Diagrams**:\n- Each diagram should have one specific purpose\n- Filter irrelevant information aggressively\n- Focus on essential elements only\n\n**Avoiding Overloaded Diagrams**:\n- Resist the temptation to add extra information\n- Create new diagrams for additional information\n- Determine clear focus and filter elements accordingly.",784,25.68732714653015
False,3,"88-Living Diagrams to Keep You Honest.pdf,88-Living Diagrams to Keep You Honest,,0","88-Living Diagrams to Keep You Honest.pdf,88-Living Diagrams to Keep You Honest,,0",82,"**Living Diagrams: Staying Truthful**\n\n**Title:** Living Diagrams to Keep You Honest\n\n**Keywords:** living diagrams, honesty, truthfulness\n\n**Introduction:**\n- This text discusses the importance and benefits of ""living diagrams"" in maintaining a truthful life.\n\n**Living Diagrams:**\n- Concept that helps keep one honest\n- Not a physical diagram but an idea or way of living\n\n**Importance:**\n- Helps individuals stay truthful and honest in their actions and thoughts\n- Encourages self-reflection and growth\n\n**Conclusion:**\n- Living diagrams serve as a guide to live an authentic, honest life.",592,16.44854187965393
False,3,"89-Rendering a Living Diagram.pdf,89-Rendering a Living Diagram,""Chapter 6 Automating Documentation 174\nonly the minimal subset of data that is reall","89-Rendering a Living Diagram.pdf,89-Rendering a Living Diagram,""Chapter 6 Automating Documentation 174\nonly the minimal subset of data that is really relevant for the focus. Y ou should resist \nthe temptation to show everything. If you’ve ever tried UML tools with magic round-trip mechanisms, you’ve seen what overly complex diagrams you can get.\nLiving Diagrams to Keep You Honest\nIt’s important to store the code for a living diagram in the source control itself. Y ou want to run it again and again, so that when the code changes, it’s easy to generate an updated diagram. This generator could even become a plugin in the build tool that generates the latest versions of the diagrams during each build.\nWhen a living diagram is part of a build, it provides another way to look at the \ncurrent state of the code. Y ou may have a look at it during code review , or during design meetings, or just randomly to see if everything is as expected. The biggest benefit from this kind of diagram is that it shows the code as it is, which can be a bad surprise. It keeps you honest about the quality of the design. As I discussed with Rinat Abdullin on T witter, if you must code a new module on your own, an auto-generated diagram can be your first development feedback. And if you work with colleagues, another benefit of having a living diagram of the system is that, as Rinat said, “it makes development more tangible. Y ou can point to things in discussions.”\nThe Quest for the Perfect Diagram\nThere is a scale from traditional, manually crafted diagrams to perfect living dia-grams, and each point along the way has a different level of automatic adaptations to change and requires a different level of effort. The lower in the scale, the less effort is required to produce one diagram but the more effort is required to update the dia-gram in reaction to changes. Here’s what the scale of diagrams might look like:\n •Napkin sketch: These throw-away diagrams are created using pen and paper. This type of diagram is perfect for the instant but disposable. There is no need for anything beyond a pen and a random piece of paper: the back of a letter, a napkin, whatever.\n •Proprietary diagram: These diagrams look nice but take a lot of time to create and maintain. This is not the preferred option unless you want to do the lay-out manually , if you need more complete UML support, if you really want all the extra features the tools offer, or if you have to use these types of diagrams by law . They are time-consuming and editable only by people with the tool installed. They are hard to diff, they produce large files, and it takes time to adjust the layout and every graphical possibility .\nLiving Diagrams 175\n •Plain-text diagrams: Plain text is easy to maintain, source control friendly \nand diff friendly . It supports find and replace operations, but you still have to \nmaintain it. These diagrams are malleable, easy to change, and easy to diff. Some IDEs can propagate refactorings such as renaming to class names in text, which may help reduce maintenance, but this can constrain the text, too. ASCII diagrams are a particular flavor of plain-text diagrams.\n •Code-driven diagram: Y ou may author a diagram by using code rather than plain text. It could even be refactoring proof when a class is renamed (and the diagram is renamed, too) or removed (when the compiler tells you there is something wrong). These diagrams, which are more refactoring proof, are pro-grammatic diagrams with dedicated code and/or application code (for exam-ple, driven by a DSL that includes references to code).\n •Living diagram: The diagram is totally created from the code base or from the software system at runtime (see Chapter 7, “Runtime Documentation”).\nIf you need a diagram only once and then you can throw it away immediately \nafter use, choose a napkin sketch. On the other hand, if the knowledge is important enough to use over a period of time, choose another flavor of diagram from the spec-trum. Choose one you feel comfortable with. For simple diagrams that won’t change much except for a few additions, deletions, and renaming refactorings, I recommend plain-text diagram or code-driven diagrams.\nIf you need a beautiful diagram to convince or sell, then a generated diagram is \nprobably not a good fit. Generated diagrams seldom look particularly attractive. As soon as the diagram becomes a stake in itself, it becomes worth doing it well, using the right tools to make it shiny . Y ou may try commercial proprietary CASE tools, but you will eventually need to resort to graphical design tools or even calling a graphic \ndesigner to do the job.\n  \nLOL\n“I know this diagramming tool is not friendly and you hate it, but you must use \nit, we have already bought an unlimited enterprise license, and there’s a support team of four people to help!”\n \nRendering a Living Diagram\nThere are many possible ways to create diagrams using programming languages, and this topic could fill many other books, for various technologies and various contexts. \nChapter 6 Automating Documentation 176\nThis chapter doesn’t even try to cover them all but aims to give you a good idea of the \nprocess.\nRemember that a diagram should tell a story . One story . It should hide everything \nthat does not matter for the story . As a result, most of the work for a living diagram is in ignoring everything that is not central to the story . The story must be the sole focus of the diagram.\nThe generation of a living diagram depends on what kind of diagram you need to \ncreate, and it typically involves four steps:\n 1. Scan the source code.\n 2. Filter the relevant parts out of the huge number of elements. 3. Extract the relevant information from each part, including the few meaningful \nrelationships that are relevant for the focus of the diagram.\n 4. Render the information using a layout that matches the focus of the diagram.\nLet’s look at a simple example. Say that you have a code base with many classes, \nsome of which are related to the concept of order. Y ou’d like to see a diagram that \nfocuses only on the order-related classes and how they depend on each other.\nThe code base looks like this:\n1  ...\n2  Order3  OrderPredicates5  SimpleOrder6  CompositeOrder7  OrderFactory8  Orders9  OrderId10 PlaceOrder11 CancelOrder12 ... // many other classes\nFirst, you need a way to scan the code. Y ou can use reflection or dynamically load-\ning code for that. Starting from a package, you can then enumerate all its elements.\nThere are many classes in the domain model of this application, so you need a \nway to filter the elements you’re interested in. Here you’re interested in every class \nor interface related to the concept of order. For the sake of simplicity , you can do the filtering on all elements that contain “order” in their name.\nLiving Diagrams 177\nNow you need to decide the focus of the diagram. In this case, say that you’d like \nto show dependencies between the classes, perhaps to highlight those that may be \nundesired. T o do this, during the scan of all the classes and interfaces you will extract \nonly their name and the aggregated dependencies between them. For example, you can collect all field types, enum constant, method parameters types and return types, and super types that constitute the dependencies of a class. Y ou typically do this by using a simple parser for the Java language and with a visitor who walks through all declarations—imports, superclass, implemented interfaces, fields, methods, method parameters, method return, and exceptions—collecting all dependencies found into one set. Y ou may decide to ignore some of them.\nThe last step is to render the diagram, using a specialized library . If you use Graph-\nviz, you need to convert the model of classes with dependencies into the Graphviz text language. When that is finished, you run the tool and get a diagram.\n Note\nIn this example, for each class with a name containing Order , you would have its \nname and its list of dependencies. It is already a graph that you can map to any graph rendering library , such as Graphviz.\n \nThere are many tools available for rendering, but not many of them can do a \nsmart layout of an arbitrary graph. Graphviz is probably the best, but it’s a native tool. Fortunately , it now also exists as a JavaScript library and is easy to include into a web page to render a diagram in a browser. And this JavaScript library has also become a pure Java library , graphviz-java!\n2 I used to use my old little Java wrap-\nper dot-diagram3 on top of Graphviz dot, but graphviz-java now seems like a better \nalternative.\nA Word on Tools\nSome tools and technologies that can help render a living diagram include  Pandoc, D3.js, Neo4j, AsciiDoc, PlantUML, ditaa, Dexy , and many other  not-so-well-known tools on GitHub and SourceForge. Creating a plain SVG file is an option, too, but you have to do the layout yourself. However, it may be a good approach if you can use it as a template, too, as you would do dynamic HTML pages with a template. Simon Brown’s Structurizr is another tool.\n2. graphviz-java, https://github.com/nidi3/graphviz-java\n3. dot-diagram, https://github.com/cyriux/dot-diagram"",9223",9294,"**Rendering a Living Diagram**\n\n**Living Diagrams**:\n- Keep you honest about the quality of design\n- Part of the build process\n- Can be used for code review, design meetings, or casual inspection\n\n**Types of Diagrams**:\n- **Napkin sketch**: Disposable, hand-drawn diagrams\n- **Proprietary diagram**: Time-consuming to create and maintain\n- **Plain-text diagram**: Easy to maintain, source control friendly\n- **Code-driven diagram**: Dedicated code or application code for diagram generation\n- **Living diagram**: Totally generated from the code base or software system\n\n**Generating a Living Diagram**:\n1. Scan the source code\n2. Filter relevant parts\n3. Extract information (class names, dependencies)\n4. Render the information using a layout that matches the focus of the diagram\n\n**Tools for Rendering**:\n- Graphviz: Best for smart layout of arbitrary graphs\n- Pandoc, D3.js, Neo4j, AsciiDoc, PlantUML, ditaa, Dexy, and many others",934,39.32307314872742
False,3,"90-Example Hexagonal Architecture Living Diagram.pdf,90-Example Hexagonal Architecture Living Diagram,""Chapter 6 Automating Documentation 178\nT o sca","90-Example Hexagonal Architecture Living Diagram.pdf,90-Example Hexagonal Architecture Living Diagram,""Chapter 6 Automating Documentation 178\nT o scan the source code, you need parsers. Some parsers can only parse the \nmetamodel, while others have access to the code comments. For example, in \nJava, the Javadoc standard Doclet or alternative tools such as QDox give you access to the structured comments. On the other hand, the excellent Google Guava ClassPath only gives access to the programming language’s metamodel, which is enough in many cases.\nLet’s look at diagram types by layout complexity:\n •Tables (which are perhaps not really diagrams, but they have a strict layout)\n •Pins on a fixed background, like the markers on Google Map, which provide a way to map an (x, y) location for each element to pin on the background\n •Diagram templates (for example, SVG, DOT) that are evaluated with the actual content extracts from the source code\n •Simple one-dimensional flow diagrams (left-to-right or top-to-bottom), which are simple layouts you could even program yourself\n •Pipelines, sequence diagrams, and in-out ecosystem black boxes\n •T ree structures (left-to-right, top-to-bottom, or radial), which can be compli-cated but which you can do yourself if you really want to.\n •Inheritance trees and layers\n •Containment, which involves auto layout, for example using the cluster feature of Graphviz\n •Rich layout, with vertical and horizontal layout as well as containment\nOf course, if you want to be more creative, you can also try to turn a diagram into \na piece of art, such as a photo collage, or even turn it into something animated or interactive.\nVisualization Guidelines\nWhy do so many engineers think complicated system diagrams are impressive? What’s \ntruly impressive are simple solutions to hard problems.\n—@nathanmarz on Twitter\nThe ultimate rule of thumb: if there is at least one line crossing another in a diagram, the system is too complicated.\n—@pavlobaron on Twitter\nLiving Diagrams 179\nThere are rules for what makes a good document, such as showing or listing no more \nthan five to nine items and choosing a layout or list style or a table or chart that is congruent with the structure of the problem.\nT o get the most from your diagrams, consider making everything meaningful:\n •Make the left–right and top-down axes meaningful: Examples might include causality relations left-to-right, API on the left and SPI on the right, and dependencies top-to-bottom.\n •Make the layout meaningful: For example, proximity between elements could mean “similarity ,” while containment could mean “specialization.”\n •Make the size and color meaningful: For example, the size or color of a vis-ual element may reflect its importance, its severity , or the magnitude of some of its attributes.\nExample: Hexagonal Architecture Living Diagram\nThe hexagonal architecture is an evolution of the layered architecture and goes fur-ther with respect to dependency constraints. The hexagonal architecture has only two layers: an inside and an outside. And there’s a rule: Dependencies must go from the outside to the inside and never the other way round.\nAs shown in Figure 6.9, the inside is the domain model, clean and free from any \ntechnical corruption. The outside is the rest, in particular all the infrastructure required to make the software work in relation to the rest of the world. The domain is in the center, sometimes with a small application layer around it usually shown on the left. Around the domain model are adapters to integrate the domain model and ports that connect to the rest of the world: databases, middleware, servlets or REST resources, and so on.\n \nSay that you have to create documentation for a project that follows the hexago-\nnal architecture, perhaps because the boss asked for it or because you’d like to be able to explain this nice architecture to a colleague. How do you do that?InfrastructureDomain\nModel\nFigure 6.9 Hexagonal architecture in a nutshell\nChapter 6 Automating Documentation 180\nThe Architecture Is Already Documented\nThe first thing to realize is that this architecture is already documented in many \nplaces in the industry literature, starting with the website of Alistair Cockburn, who first described this pattern with the vintage diagram shown in Figure 6.10.\n \nThis architecture pattern is also described in many books, including Grow-\ning Object-Oriented Software, Guided by T ests by Steve Freeman and Nat Pryce (GOOS), and Implementing DDD by Vaughn V ernon (IDDD). This pattern is also known in the .Net circles as the onion architecture, proposed by Jeffrey Palermo.\nBecause there is so much information about hexagonal architecture, there is no \nneed for you to explain much about it yourself. Y ou can just link to an external refer-ence that already explains it well. Why try to rewrite what’s been already written by someone else? This is ready-made architecture documentation.\nThe Architecture Is Already in the Code\nThe architecture itself is already documented in the literature, but what about its particular implementation in your custom project?\nBecause you’re serious about your craft, the hexagonal architecture is already \nthere in the code: The domain model is in its own package (respectively , namespace or project in .Net), and the infrastructure is in one or several other packages, clearly segregated from the domain model.Notifications\nAppDatabaseAdministrationTrigger Data\nTest\nAdapterApp-to-App\nAdapterMock\nDatabaseHTTP\nAdapterHTTP\nFeedTest\nAdapterWire\nFeed\nMock\nTelephoneEmail\nAdapterAnswering\nMachine\nAdapter\nGUI\nDBApplication\nFigure 6.10 Hexagonal architecture diagram from Alistair Cockburn’s website\nLiving Diagrams 181\nWith some experience with this pattern, you can recognize it just by looking at \nthe packages and their content. Such clean and strict segregation never happens by \npure chance; it demonstrates a clear design intent. If you can recognize the hexago-nal architecture just by looking at the code, you’re done, right?\nWell, not really . Not everyone knows about hexagonal architecture, and architec-\nture is something everybody should be aware of. Y ou need to make the architecture explicit in some way . It’s 99% there already , but you need to add the missing 1% to make it fully visible to everyone. Y ou need to do some knowledge augmentation, using annotations or naming conventions, both of which would work well here.\nThe naming convention is, in fact, already there:\n •Every class, interface, and enum is in a package under the root package \n*.domain.* .\n •Every infrastructure code is under *.infra.* .\nY ou need this convention to be documented and, of course, stable.Y ou could use annotations instead of naming conventions. This would enable you \nor others to add more information, such as a rationale:\n1  @HexagonalArchitecture.DomainModel(\n2    rationale = """"Protect the domain model"""",3    alternatives = """"DDD.Conformist"""")4  package flottio.fuelcardmonitoring.domain;56  import flottio.annotations.hexagonalarchitecture7                                 .HexagonalArchitecture;\nKnowing What You Want for the Living Document\nY ou can start figuring out what you expect by doodling on a napkin. What you want \nhere is a diagram with a hexagon (or any other shape) at the center, representing the domain model with its most important elements inside. Outside and around this shape you expect to have every significant element of the infrastructure, with arrows showing their dependencies with the domain elements inside. It might look some-thing like the diagram in Figure 6.11.\n \nY ou want a layout that flows from left-to-right, from the calls to the API to \nthe domain, and then to the service providers and their implementations in the infrastructure.\nChapter 6 Automating Documentation 182\nWhere’s the Knowledge Right Now?\nAs you’ve seen, the bulk of the knowledge about the hexagonal architecture is in the \nnaming convention used with the packages. The rest of the knowledge is simply the list of every class, interface, and enum contained in these packages, along with their relationships.\nA convenient convention when drawing hexagonal architecture is to have every \nelement consuming the domain model on the left and every element providing ser-vices to the domain model on the right. How do you extract this information from the source code?\nIn the current application you have several opportunities for simplification: Every \nclass that calls the domain model does so through its member fields, and every ser-vice provider integrates with the domain model by implementing one of its inter-faces. This is a common situation, but it is not a rule; for example, a caller may be getting its response through a callback. In other cases, you may have to declare explicitly who’s on the API side and who’s on the SPI (service provider) side if you care about that in the diagram layout.\nFiltering Out Irrelevant Details\nEven in small projects, the source code contains a lot of information, so you always need to carefully decide what to keep out of the diagram. In this case, you want to exclude the following:\n •Every primitive\n •Every class that acts as a primitive (like the most basic value objects)\n •Every class that is not related to other classes mentioned in the diagramDomain Model\nFigure 6.11 A quick sketch of  the kind of  diagram you’d like to generate\nLiving Diagrams 183\nY ou want to include classes in the following fashion:\n •Include all classes and interfaces within the domain model (apart from quasi-\nprimitives such as units of measurement). Being in the domain model is a mat-ter of naming convention, or of being in a package annotated as such.\n •Include mutual relationships that make sense. Y ou may want to fold type hier-archies into their supertype to save diagram real estate.\n •Include infrastructure classes that have relationships with elements already included in the domain model.\n •For each infrastructure class, include its relationship to the domain classes and between infrastructure elements, too. In order to have a directed diagram in the API-to-SPI direction from the left to the right, you need to help the renderer. For example you need to ensure that your call and implement relationships are \nin opposite directions in your generated diagram description: A calls B and A implements B must be in opposite directions. If you don’t understand this now , that is no problem; you will understand it clearly as soon as you try to make it work by tweaking your rendering.\nAll this is just one example that works fine in one context. It is by no mean a \nuniversal solution for this kind of diagram. Y ou should expect to try various alter-natives, and you may have to filter more aggressively if your diagram gets too big. For example, you may decide to show only the core concepts, based on additional annotations.\nScanning the Source Code\nFor a living diagram like this one, all you need is a way to iterate through all the classes and the ability to introspect them. Any standard parser can do this, and you can even do it without any parser, just by using reflection. Because the focus is on the hexagonal architecture and nothing else, your focus is on segregating elements and highlighting the dependencies between them.\nThe example shown in Figure 6.12 uses standard Java reflection, with the help of the \nGoogle Guava ClassPath to scan the full class path conveniently . My own utility library DotDiagram is a convenience wrapper on top of the Graphviz DOT syntax to create the .dot files. Then it’s up to Graphviz dot to do the auto-magical layout and rendering.\n \nWorking with Changes \nSay that a month after you create the diagram shown in Figure 6.12, you are not happy with a name \nTrend  for the domain interface, and you decide to rename it"",11843",11957,"**Hexagonal Architecture: Documenting Living Diagrams**\n* **Automating Documentation**:\n    * Parsers required for scanning source code\n        - Some parsers can access comments, others only metamodel\n            * In Java: Javadoc Doclet or Google Guava ClassPath\n    * Diagram types by layout complexity:\n        1. Tables (strict layout)\n        2. Pins on a fixed background (Google Map)\n        3. Diagram templates (SVG, DOT)\n        4. Simple one-dimensional flow diagrams\n        5. Pipelines, sequence diagrams, and ecosystem black boxes\n        6. Tree structures (left-to-right, top-to-bottom, or radial)\n        7. Inheritance trees and layers\n        8. Containment (with auto layout)\n        9. Rich layout (vertical and horizontal, as well as containment)\n* **Hexagonal Architecture Living Diagram**\n    * Two layers: inside (domain model) and outside (infrastructure)\n        - Dependencies must go from the outside to the inside\n    * Documenting the architecture:\n        - Already documented in industry literature\n            * Websites, books (e.g., Growing Object-Oriented Software, Guided by Tests, Implementing DDD)\n            * Known as onion architecture in .Net circles\n        - Existing codebase follows the pattern\n            * Domain model in its own package (namespace or project)\n            * Infrastructure clearly segregated\n* **Living Diagrams: Making it Visible**\n    * Naming convention and annotations for documentation\n        - Every element consuming/providing services to domain model\n    * Filtering out irrelevant details when extracting information from source code\n    * Scanning the source code with standard parsers or reflection\n        - Focus on segregating elements and highlighting dependencies\n* **Working with Changes**: Adapting the diagram as needed\n    - Renaming domain interface in hexagonal architecture living diagram.",1885,74.42372751235962
False,3,"91-Case Study A Business Overview as a Living Diagram.pdf,91-Case Study A Business Overview as a Living Diagram,""Chapter 6 Automating Documentation 18","91-Case Study A Business Overview as a Living Diagram.pdf,91-Case Study A Business Overview as a Living Diagram,""Chapter 6 Automating Documentation 184\nSentencesAuditing . There is no need to update the diagram by hand; a new , \n up-to-date diagram showing the new name is generated on the next build.\nPossible Evolutions\nThe hexagonal architecture constrains dependencies: They can only go from the out-\nside to the inside and never the other way around. However, the living diagram shows all dependencies, even those that violate the rule. This is very useful for making vio-lations visible.\nIt’s possible to go even further and to highlight all violations in a different color, \nsuch as with big red arrows that indicate dependencies that are going the wrong direction. The line between a living diagram and static analysis to enforce guidelines is very thin.\nY ou may have noticed that it’s impossible to talk seriously about a living diagram \nwithout talking deeply about the purpose of the diagram—in other words, with-out talking about design. This is no coincidence. Useful diagrams must be relevant, and to be relevant when you’re supposed to describe a design intent, you must really understand the design intent. This suggests that doing design documentation well converges with doing design well.\nCase Study: A Business Overview as a Living Diagram\nSay that you work for an online shop that was launched a few years ago. The soft-ware system for this online shop is a complete e-commerce system made of several \nFigure 6.12 Hexagonal architecture living diagram generated from source code\nLiving Diagrams 185\ncomponents. This system has to deal with everything necessary for selling online, \nfrom the catalog and navigation to the shopping cart, the shipping, and some basic customer relationship management.\nY ou’re lucky because the founding technical team had good design skills. As a \nresult, the components match the business domains in a one-to-one fashion, as illus-trated in Figure 6.13. In other words, the software architecture is well aligned with the business it supports.\n \nBecause of its success, your online shop is growing quickly . As a result, there are \nan increasing number of new needs to support, which in turn means there are more features to add to the components. Because of this growth, you’ll probably have to add new components, redo some components, and split or merge existing compo-nents into new components that are easier to maintain, evolve, and test.\nY ou also need to hire new people in the development teams. As part of the neces-\nsary knowledge transmission for the new joiners, you want some documentation, starting with an overview of the main business areas, or domains, supported by the system. Y ou could create it manually , spending a couple of hours in PowerPoint or in some dedicated diagramming tool. But you want to trust your documentation, and you know you’ll likely forget to update a manually created document whenever the system changes—and you know it will change.\nFortunately , after you read a book on living documentation, you decided to auto-\nmatically generate the desired diagrams from the source code. Y ou don’t want to spend time on a manual layout; a layout based on the relationships between the domains will be perfectly fine—something like what is sketched in Figure 6.14.\n Business Domain\nSoftware System\nFigure 6.13 Software components match the business domains one-to-one\nChapter 6 Automating Documentation 186\nPractical Implementation: The Existing Source Code\nY our system is made of components that are simply Java packages, as shown in \n Figure 6.15. \nThe naming of these packages is a bit inconsistent because historically the com-\nponents were named after the development project code, as it is often the case. For example, the code that takes care of the shipping features is named Fast Delivery Pro because that’s the name the marketing team gave the automated shipping initiative two years ago. This name is not used anymore, though, except as a package name. Similarly , Omega is actually the component that takes care of the catalog and the current navigation features.\nY ou have a naming problem that is also a documentation problem: The code \ndoes not tell the business. For some reason you can’t rename the packages right now , although you hope to be able to do it next year. However, even with the right names, the packages won’t tell the relationships between them.\nAugmenting the Code\nAs a result of the current naming problems in code, you need extra information in order to make a useful diagram. As you’ve seen before, one great way to add Items of the catalog\nare placed into\nthe shopping cart.\nShopping\nCartCatalog\nFigure 6.14 Expected style of  diagram\nFigure 6.15 Overview of  the components as Java packages\nLiving Diagrams 187\nknowledge to code is to use annotations. At a minimum, you want to add the follow-\ning knowledge to the code to fix the naming:\n1  @BusinessDomain(""""Shipping"""")\n2  org.livingdocumentation.sample.fastdeliverypro  34  @BusinessDomain(""""Catalog & Navigation"""")5  org.livingdocumentation.sample.omega\nY ou introduce a custom annotation with just a name to declare a business domain:\n1  @Target({ ElementType.PACKAGE })2  @Retention(RetentionPolicy.RUNTIME)3  public @interface BusinessDomain {4          String value(); // the domain name5  }\nNow you’d like to express the relationships between the domains:\n •The catalog items are placed into the shopping cart before they are ordered.\n •Then the items in orders must be shipped.\n •These items are also analyzed statistically to inform the customer relationship \nmanagement.\nY ou then extend the annotation with a list of related domains. However, as soon \nas you refer to the same name several times, text names raise a little problem: If you change one name, then you must change it everywhere it is mentioned. T o remedy this, you want to factor out each name into a single place to be referenced. One pos-sibility is to use enumerated types instead of text. Y ou can then make references to the constants of the enumerated type. If you rename one constant, you’ll have noth-ing special to do to update its references everywhere. Because you also want to tell the story for each link, you add a text description for the link as well:\n1  public @interface BusinessDomain {\n2          Domain value();3          String link() default """""""";4          Domain[] related() default {};5  }67 // The enumerated type that declares each domain in one 8 // place9 public enum Domain {\nChapter 6 Automating Documentation 188\n9    CATALOG(""""Catalog & Navigation""""),\n10   SHOPPING(""""Shopping Cart""""),11   SHIPPING(""""Shipping""""), CRM(""""CRM"""");1213         private String value;1415         private Domain(String value) {16                 this.value = value;17         }1819         public String getFullName() {20                 return value;21         }22 }\nNow it’s just a matter of using the annotations on each package to explicitly add \nall the knowledge that was missing from the code:\n1  @BusinessDomain(value = Domain.CRM, 2           link = """"Past orders are used in statistical3           analysis for customer relationship management"""", 4           related = {Domain.SHOPPING}))5  org.livingdocumentation.sample.crm67  @BusinessDomain(value = Domain.SHIPPING, 8           link = """"Items in orders are shipped to the 9           shipping address"""", 10          related = {Domain.SHOPPING})11 org.livingdocumentation.sample.fastdeliverypro1213 //etc.\nGenerating the Living Diagram\nBecause you need a fully automatic layout that works like magic in all cases, you \ndecide to use the tool Graphviz for the layout and rendering of the diagram. This tool expects a text file with a .dot extension that conforms to the DOT syntax. Y ou need to create this plain-text file before running Graphviz to render it into a regular image file.\nThe generation process involves the following steps:\n 1. Scan the source code or the class files to collect the annotated packages and \ntheir annotation information.\nLiving Diagrams 189\n 2. For each annotated package, add an entry to the DOT file:\n •T o add a node that represents the module itself\n •T o add a link to each related node\n 3. Save the DOT file.\n 4. Run Graphviz dot at the command line by passing it the .dot filename and the \ndesired options to generate an image file.\nY ou’re done! The image is ready on disk.\nThe code to do all this can fit inside a single class of fewer than 170 lines of code. \nBecause you’re in Java, most of this code is about dealing with files, and the hardest part of it is about scanning the Java source code.\nAfter running Graphviz, you get the living diagram shown in Figure 6.16.\n \nFigure 6.16 Actual diagram generated from the source code\nAfter adding some additional style information, you get the diagram shown in \nFigure 6.17. \nFigure 6.17 Actual diagram generated from the source code, with style\n\nChapter 6 Automating Documentation 190\nAccommodating Changes\nAfter some time, the business has grown, and the supporting software system has to \ngrow as well. Several new components have appeared—some brand new and some as a result of splitting existing components. For example, now you have dedicated com-ponents for the following business domains:\n •Search & Navigation\n •Billing\n •Accounting\nEach new component has its own package and has to declare its knowledge in \nits package annotation, like any well-behaved component. Then, without any addi-tional effort, your living diagram will automatically adapt and produce the new , more complicated overview diagram shown in Figure 6.18.\n \nAdding Other Information\nNow you’d like to enrich the diagram with concerns such as quality attributes. Because this knowledge is missing from the code, you need to add it by augmenting the code. Y ou can once again use package annotations for that, as shown in Figure 6.19.\n \nFigure 6.18 The new diagram generated from the source code, some time later\nFigure 6.19 Package annotations in package-info.java\nLiving Diagrams 191\nY ou can now enhance the living diagram processor to extract the @Concern  infor-\nmation as well to include it in the diagram. After you do this, you get the diagram \nshown in Figure 6.20, which is obviously a little less clear than the previous diagrams. \nThis case study provides an example of what’s possible with a living diagram. \nThe main limits are your imagination and the time required to try ideas, some of which may not work. However, it’s worth the time to try playing with ideas from time to time or whenever there’s frustration about the documentation or about the design. Living documentation makes your code, its design, and its architecture trans-parent for everyone to see. If you don’t like what you see, you need to fix it in the source code.\nHow Does the Living Diagram Fit with the Patterns of Living \nDocumentation?\nThis diagram is a living document that is automatically refreshed whenever the sys-\ntem changes. If you were to add or delete a module, the diagram would adjust as quickly as the next build.\nThis case study provides an example of one diagram that tells a story from one \nnode to the next through links that display brief descriptions.\nThis diagram is an example of augmented code, using annotations to augment \neach main module with the knowledge of its corresponding business domain. This is also a case of information consolidation spread across many packages.\nFinally , the knowledge added to the source code can be used for an enforced guide-\nlines about architecture. Writing a verifier is similar to writing a living diagram gen-\nerator except that the relationships between nodes are used as a dependency whitelist to detect unexpected dependencies instead of generating a diagram.\nFigure 6.20 Actual diagram generated from the source code, with additional quality \nattributes"",11893",12031,"**Living Documentation: A Business Overview as a Living Diagram**\n\n**Background:**\n- Online shop system consists of e-commerce components\n- Software architecture is well aligned with business domains\n- Growing number of new needs and features necessitates documentation\n- Manual methods for documentation are time-consuming and prone to errors\n\n**Solution: Living Documentation via Automated Diagrams**\n- Generate diagrams from source code using Graphviz\n- Use custom annotations in Java packages to add business domain information\n- Scan source code, create .dot file for Graphviz, render image\n\n**Benefits:**\n- Fully automatic layout that works like magic\n- Adapts to changes in software system without additional effort\n- Enhances the diagram with concerns such as quality attributes\n\n**Implementation:**\n1. Scan source code for annotated packages and their annotation information\n2. Add node representations and links to related nodes in .dot file\n3. Save .dot file, run Graphviz to render image\n4. Code can fit into a single class with minimal effort\n\n**Example of Living Diagram:**\n- Represents business domains and their relationships\n- Shows which components are related to each other\n- Displays quality attributes as nodes in the diagram\n\n**Conclusion:**\n- Living documentation makes code, design, and architecture transparent for everyone to see\n- If unsatisfied with what is seen, fix it in source code.",1414,57.107431173324585
False,3,"92-Example A Context Diagram.pdf,92-Example A Context Diagram,""Chapter 6 Automating Documentation 192\nExample: A Context Diagram\nNo system is an isl","92-Example A Context Diagram.pdf,92-Example A Context Diagram,""Chapter 6 Automating Documentation 192\nExample: A Context Diagram\nNo system is an island; every system is part of a bigger ecosystem with other actors, \ntypically people and other systems. From a developer’s point of view , integration with other systems is sometimes considered obvious knowledge not worth docu-menting, especially in the early years of a system. But after a while the system grows and becomes deeply integrated with many other actors, and even people on the team no longer know about this ecosystem. T o reconstitute the whole picture, you have to review all the code manually and interview knowledgeable people (who also happen to be very busy).\nContext knowledge is essential for reasoning about impacts to or from other \nactors when considering changes in this system or in another external system. As such, it deserves to be made clearly visible and up-to-date at any time. Basically , a context diagram provides a recap of all actors using the system (API side) or used by the system (service providers side):\n1  Actors using * --> System --> * Actors used\n2  using the system               by the system\nThe context can be expressed as a simple list, like this:\n •API (actors using the system)\n •Fuelo Card API\n •Fleet Management Teams\n •Support & Monitoring\n •SPI (actors providing services to the system)\n •Google Geocoding\n •GPS T racking from Garmin\n •Legacy V ehicle Assignment\nBut a visual layout has advantages too, as shown in Figure 6.21. \nY ou can create such a diagram by hand each time you need it, tailoring it to the \nmatter at hand. Or you could generate it.\nThe diagram shown in Figure 6.21 was generated from the sample Flottio fleet \nmanagement system used in examples throughout this book. This diagram tells the \nstory of the system through its links to external actors, with some brief descriptions on some of them.\nLiving Diagrams 193\n Note\nThe name context diagram is borrowed from Simon Brown’s C4 model,4 a light-\nweight approach to architecture diagrams that is becoming increasingly popular \namong developers.  \nThis diagram is a living document that is automatically refreshed whenever the \nsystem changes. It is generated by scanning the augmented source code and calling a graph layout engine such as Graphviz. If you were to add or delete a module, the diagram would adjust as quickly as the next build. This diagram is also an example of a refactoring-proof diagram; if you want to rename a module in the code, the diagram will show it renamed, too, without extra effort. There is no need to fire up PowerPoint or a diagram editor each time.\nHyperlinks to the Corresponding Source Code Locations\nY our living document can feature hyperlinks to the accurate locations in the code base. With these links, a user can click on any external actor on the diagram to jump to the corresponding URL in the source code repository online. (For this you can use one of the patterns for stable links from Chapter 8, “Refactorable Documentation.”)\nFigure 6.21 A generated context diagram with three actors on the left and three actors on \nthe right\n4. Simon Brown, Coding the Architecture blog, http://www .codingthearchitecture.com/2014/08/24/\nc4_model_poster.html\nChapter 6 Automating Documentation 194\nNote that even without a link, the wording in the diagram can be used verbatim to \nperform a search in the code base. Because the wording came from the code, it would \nbe easy to find the corresponding location.\nApplying Augmented Code and Knowledge Consolidation\nThe problem, of course, is to identify automatically the external actors and their names, descriptions, and directions of use (using or being used). Unfortunately , I haven’t found a miracle solution for that.\nT o generate this diagram, the code has to be augmented with some annotations to \ndeclare the external actor. This is an example of augmented code and is also a case of consolidation of information spread across multiple packages and subpackages.\nFor example, the package \nflottio.fuelcardmonitoring.legacy  takes care \nof the integration with the legacy system for vehicle assignments to drivers, a pro-vider of services for the system under consideration:\n1  /**\n2  * Vehicle Management is a legacy system which manages which3  * drivers is associated to a vehicle for a period of time.3  */4 5  @ExternalActor(6    name = """"Legacy Vehicle Assignment System"""",7    type = SYSTEM,8    direction = ExternalActor.Direction.SPI)9  package flottio.fuelcardmonitoring.legacy;1011 import static flottio.annotations.ExternalActor12                                         .ActorType.SYSTEM;13 import flottio.annotations.ExternalActor;\nAnother example is the class listening to the incoming message bus, which basi-\ncally uses the system to check whether fuel card transactions have anomalies:\n1 package flottio.fuelcardmonitoring.infra;2 // more imports...34 /**5 * Listens to incoming fuel card transactions from the6 * external system of the Fuel Card Provider7 */8 @ExternalActor(\nLiving Diagrams 195\n9    name = """"Fuelo Fuel Card Provider"""",\n10   type = SYSTEM,11   direction  = Direction.API)12 public class FuelCardTxListener {13   //...\nY ou don’t have to use annotations. Y ou could also add sidecar files in the same \nfolder as the annotation code, with the same content as the annotation inside, as a \nY AML, JSON, or .ini file:\n1  ; external-port.ini\n2  ; this sidecar file is in the integration code folder3  name=Fuelo Fuel Card Provider4  type=SYSTEM5  direction=API\nSay that at some point, you want to add information to the context diagram, so \nyou add this information to the code itself, in the Javadoc of the integration code, \nand then the diagram gets updated as shown in Figure 6.22. \nLimitations and Benefits of This Living System Diagram\nBecause of the need for some code augmentation with annotation files, there is a risk of not knowing about some external actors.\nIf in your project you can enumerate only a few ways to perform integration, you \nmay try to detect them all and add them to the diagram unless silenced explicitly through code augmentation.\nFigure 6.22 A generated context diagram with three actors on the left and three actors on \nthe right"",6261",6334,"**Context Diagrams**\n* Essential for understanding system integration within ecosystem\n* Documentation often overlooked during early stages of system growth\n* Reconstitute ecosystem by manually reviewing code and interviewing knowledgeable people\n* Represents actors using and acting on the system (API side) and those providing services to it (SPI side)\n* Provides visual layout with advantages over hand-drawn diagrams\n* Can be generated automatically from augmented source code\n* Living document that updates as system changes\n* Incorporates hyperlinks to corresponding source code locations for easy access\n* Generated from scanning augmented source code and using graph layout engine like Graphviz\n* Identification of external actors requires code augmentation with annotations or sidecar files\n* Risk of overlooking some external actors if integration methods are not enumerated. (Figure 6.21)\n\n**Context Diagram: Advantages**\n* Clear visual representation of system's relationship with other actors\n* Facilitates understanding of impacts on and from external systems when making changes\n* Easy to update as the system evolves, thanks to automatic generation\n* Encourages consistency in naming conventions for external actors\n* Improves communication among team members about system dependencies\n* Enhances traceability by providing a single source of truth for external actor relationships. (Figure 6.22)",1411,38.66296625137329
False,3,"93-Summary.pdf,93-Summary,""Chapter 6 Automating Documentation 196\nIn any case, integration through the database will be hard to detect and docu-\nmen","93-Summary.pdf,93-Summary,""Chapter 6 Automating Documentation 196\nIn any case, integration through the database will be hard to detect and docu-\nment. Y ou may believe the database is a private detail of your system, but if another \nsystem queries or writes into it directly , it will be hard to find out without a conversa-tion with the culprits.\nOn the other hand, this diagram shows every potential integration, but it cannot \ntell whether the integrations are active in production. If the code base is a toolkit for a product line, it will show all the potential integrations—not just the one actually used in practice in a particular instance.\nAnother drawback of a generated diagram like the one shown in Figure 6.22 com-\npared to an ad hoc manual diagram is that it is not tailored for the particular matter at hand. However, it’s much faster to generate a diagram than to draw an ad hoc one.\nStill, you may want to tweak the diagram generator—for example, to focus on a \nsubset of the context.\nThe Challenges with Automated Generation of Design \nDocumentation\nProducing documentation of the design of a software project manually requires a lot \nof work and becomes obsolete very quickly after the next change or refactoring. Manually drawing meaningful UML diagrams is very time-consuming, and even choosing what to display takes a lot of time.\nAccording to domain-driven design, the code is itself the model, but code lacks \nthe ability to clearly express larger-than-class structures and collaborations. There-fore, some additional carefully selected design documentation is useful to show the bigger picture. And it can be generated from the code, as long as the code is aug-mented with the design intentions.\nUsing Patterns in Generating Design Documentation\nThe use of patterns to help with the process of generating design documentation is promising. Patterns naturally lie “on top” of the language elements. They address a particular problem within a context, discuss a solution, and have a clear intent. They involve a collaboration of several elements from the language, such as several classes and their protocols, or just relationships between fields and methods within a class. Each pattern is a chunk of design knowledge. When it comes the time to automate the description of the design, it seems natural to chunk the automation process by pattern as well.\nIn some projects, I’ve declared some of the patterns used in the code (using anno-\ntations) and created little tools to derive partial macro structures of the software design around these patterns. Each pattern come with a context, and this context \nSummary 197\nhelps in selecting what to show and how to show it. From the patterns declared in \nthe code, the tool can then generate better-than-generic design documentation (for example, a diagram) informed by the knowledge chunked pattern by pattern.\nSo far, all the examples of living diagrams in this book have been generated from \nthe source code at compile time, but this doesn’t have to be the case; it’s also possible to exploit runtime knowledge to produce them.\nSummary\nLiving documents are great fun to produce and help bridge the gap between fast-paced projects and traditional documents that may still be desirable in some cases. The design diagrams, glossary , overview diagrams, and domain-specific diagrams described in this chapter illustrate the use of automation to produce smart documents.\nBut it isn’t just automating diagrams that is important; deriving diagrams from \nknown sources is also important so that when a diagram changes, the documenta-tion does as well. As shown in this chapter, you need to augment the source code a little—for example, by declaring explicitly the patterns or design stereotype that you want to use through annotation or any other means of augmenting the code—to achieve reliable automation.\nThis page intentionally left blank"",3907",3940,"**Automating Documentation: Challenges and Solutions**\n\n**Challenges:**\n- Integration through databases hard to detect without conversation\n- Diagrams generated may not be tailored for specific use cases\n- Code lacks ability to express larger structures and collaborations\n- Manual documentation is time-consuming and becomes obsolete quickly\n\n**Solutions:**\n- Use pattern-based design documentation\n- Declare patterns in code, create tools to derive partial macro structures\n- Generate better-than-generic design documentation informed by patterns\n- Automate diagram generation at compile time or runtime\n\n**Benefits of Living Documents:**\n- Bridge gap between fast-paced projects and traditional documents\n- Illustrate use of automation for smart document production\n- Derive diagrams from known sources to keep them up-to-date.\n\n**Patterns in Design Documentation:**\n- Lie ""on top"" of language elements\n- Address specific problems within a context\n- Clear intent, collaboration of several language elements\n- Useful for generating design documentation\n\n**Automating Diagram Generation:**\n- From source code at compile time or runtime\n- Exploit runtime knowledge to produce them.",1181,33.3822226524353
False,3,"94-Example Living Services Diagram.pdf,94-Example Living Services Diagram,""199Chapter 7\nRuntime Documentation\nThe Agile Manifesto calls for “Working","94-Example Living Services Diagram.pdf,94-Example Living Services Diagram,""199Chapter 7\nRuntime Documentation\nThe Agile Manifesto calls for “Working Software over Comprehensive \nDocumentation.”\nWhat if the working software were itself a kind of documentation?It is already quite common to design the user experience so that the users can \nhave successful interactions with an application without ever having to open the user manual. However, it’s less common to design software so that its developers can understand it without even having to open the source code.\nIt is possible to learn the business domain just by using a related and well-designed \napplication. The software is by itself a piece of documentation about itself and its business domain. This is why all the developers on an application should at least know how to use their application for most standard use cases, even if it is a compli-cated application that deals with complicated ideas (such as financial instruments).\n Key Point\nAnything that can answer a question can be considered documentation. \nIf you can answer questions by using an application, then the application is part of the documentation.\nIn Chapter 6, “ Automating Documentation,” you saw several examples of living \ndiagrams based on source code, but living diagrams can also be built from knowl-edge available at runtime. Let’s look at this by using an example based on distributed tracing, which is typically used on distributed systems with multiple components."",1438",1519,"**Living Services Diagram: Documentation through Software**\n* Agile Manifesto prioritizes ""Working Software over Comprehensive Documentation""\n* Software can serve as documentation by providing successful user interactions\n* Developers can learn business domain from well-designed applications\n* Applications are self-documenting when they answer questions (Chapter 6)\n* Living diagrams can be built using runtime knowledge, such as distributed tracing in multi-component systems.\n\n**Key Point:**\n- Anything that answers a question is considered documentation\n- Software that responds to queries functions as part of the documentation.",634,18.237669944763184
False,3,"95-The Magic That Makes This Work.pdf,95-The Magic That Makes This Work,""Chapter 7 Runtime Documentation 200\nExample: Living Services Diagram\nDistri","95-The Magic That Makes This Work.pdf,95-The Magic That Makes This Work,""Chapter 7 Runtime Documentation 200\nExample: Living Services Diagram\nDistributed tracing, based on Google’s Dapper paper,1 is becoming a vital ingredient \nof a microservices architecture. It’s “the new debugger for distributed services,” a \nkey runtime tool for monitoring, typically to solve response time issues. But it’s also a fantastic ready-made living diagram tool that can discover the living architecture of your overall system, with all its services, on a given day .\nFor example, the Zipkin and Zipkin Dependencies provide a services dependency \ndiagram out-of- the-box, as shown in Figure 7.1. This view is nothing more than the aggregation of every distributed trace over some period (for example, for a day). \nA Matter of Augmented Code but at Runtime\nFor distributed tracing to work, you need to augment the system through instrumen-\ntation. Every service or component must use a tracer that conforms to the trace iden-tifiers to declare the reception of a request, and the sending of the response, along with annotations, and additional “baggage” as a key/value store.\nThe trace identifiers involve a context made of three identifiers that enable you to \nbuild the call tree as an offline process:\n •Trace ID: The correlation ID of a complete call tree\n •Span ID: The correlation ID of this single client/server call\n •Parent ID: The current parent call \n1. http://research.google.com/pubs/pub36356.html\nFigure 7 .1 Zipkin Dependencies diagram on screen\nExample: Living Services Diagram 201\nThe span name can be specified, for example, with Spring Cloud Sleuth, using an \nannotation:\n1  @SpanName(""""calculateTax"""")\nSome of the core annotations used to define the start and stop of a client/server \nrequest are as follows:\n •cs: Client start\n •sr: Server receive\n •ss: Server send\n •cr: Client receive\nThe annotations may be extended to classify your services or to perform filtering. \nHowever, the tools may not naturally support your own annotations.\nThe baggage, or “binary annotation,” goes further, capturing key runtime \ninformation:\n1  responsecode = 500\n2  cache.somekey = HIT3  sql.query = """"select * from users""""4  featureflag.someflag = FALSE5  http.uri = /api/v1/login6  readwrite = READONLY7  mvc.controller.class = Application8  test = FALSE\nHere, all the tagging with metadata and other live data happens in real time. Y ou \nmight recognize that this approach is similar to augmented code. Y ou need to inject \nsome knowledge for tools to help more, and this augmentation happens at runtime.\nDiscovering the Architecture\nThe ability to inspect a distributed system in real time isn’t just for front-end  developers. As Mick Semb Wever writes on his blog, aggregating traces over time into a runtime service dependency graph “goes a long way to help architects and management to understand accurately how things work, negating much of the need for higher level documentation.”\n2\n2. Mick Semb Wever, The Last Pickle blog, http://thelastpickle.com/blog/2015/12/07/using-zipkin-for-\nfull-stack-tracing-including-cassandra.html"",3088",3171,"**Distributed Tracing as Runtime Documentation**\n\n**Background:**\n- Distributed tracing becoming vital in microservices architecture\n- Discover living architecture of systems through aggregated traces\n- Google's Dapper paper inspiration for distributed tracing\n\n**Benefits:**\n- Identify response time issues\n- Monitor services dependency diagrams\n- Understand system components and interactions\n\n**Implementation:**\n- Augment system with tracers for request/response identification\n  - Use annotations: @SpanName, cs, sr, ss, cr\n  - Inject knowledge for tools to help more\n- Capture runtime information as ""baggage"" or metadata\n\n**Advantages:**\n- Real-time discovery of distributed systems\n- Insightful understanding for architects and management\n\n**Components:**\n- Trace ID: Correlation ID of complete call tree\n- Span ID: Correlation ID of single client/server call\n- Parent ID: Current parent call's correlation ID\n\n**Annotations:**\n- Client start (cs)\n- Server receive (sr)\n- Server send (ss)\n- Client receive (cr)\n\n**Baggage or Binary Annotation:**\n- Key/value store for runtime information\n  - Response code, cache hits/misses, SQL queries, HTTP URIs, read/write status, etc.",1181,48.39455461502075
False,3,"96-Going Further.pdf,96-Going Further,,0","96-Going Further.pdf,96-Going Further,,0",40,"**The Provided Passage**, repeated for concision:\n\n**""Going Further""**:\n- The title of a document or topic\n- Has no additional context provided\n\n**Concision**:\n- Repeating the passage as it is, preserves its original form and meaning\n- Avoids introducing new information or altering the original text",300,8.741634130477905
False,3,"97-Visible Tests.pdf,97-Visible Tests,""Chapter 7 Runtime Documentation 202\nThe Magic That Makes This Work\nThrough sampling, some requests get instru","97-Visible Tests.pdf,97-Visible Tests,""Chapter 7 Runtime Documentation 202\nThe Magic That Makes This Work\nThrough sampling, some requests get instrumented as they go through each node of \nthe system. The instrumentation generates span traces that are collected and stored in some central (logically) datastore. Individual traces can then be searched and dis-played. A daily \ncron  triggers then post-processes all the traces into aggregates repre-\nsenting the “dependencies” between services. The aggregation is something like this simplified example:\n1  select distinct span\n2  from zipkin_spans3  where span.start_timestamp between start and end4  and span.annotation in ('ca', 'cs', 'sr', 'sa')5  group by span\nThe UI then displays all the dependencies using some sort of automated nodes \nlayout.\nGoing Further\nBy getting creative on the tags and through test robots stimulating the system on pre-\ndefined scenarios, a distributed infrastructure like Zipkin has a lot of potential for living architecture diagrams:\n •Y ou can create “controlled” traces from a test robot driving one or more service(s), with a specific tag to flag the corresponding traces.\n •Y ou can display different diagrams for the “Cache = HIT” and the “cache = MISS” scenarios.\n •Y ou can display distinct diagrams for the “Write part” versus the “Read part” of an overall conversation across the system.\nT rying something in this area? Please let me know!\nVisible Workings: Working Software as Its Own Documentation \nAnother idea related to software as documentation is to rely on the software itself to explain how it works inside, which Brian Marick calls visible workings and which involves making internal mechanisms visible from the outside.\n3 There are many ways \n3. Brian Marick, “Visible Workings”: https://web.archive.org/web/20110202132102/http://visibleworkings.com/"",1835",1880,"**Zipkin Infrastructure**\n- **Instrumentation**: Some requests are instrumented as they go through each node of the system\n- **Span Traces**: Generated by instrumentation, collected and stored in a central datastore\n- **Processing**: Daily cron triggers post-processing of all traces into aggregates representing ""dependencies"" between services\n- **Aggregation**: Example query for distinct span data based on specific criteria:\n  - `SELECT DISTINCT span FROM zipkin_spans`\n  - `WHERE span.start_timestamp BETWEEN start AND end`\n  - `AND span.annotation IN ('ca', 'cs', 'sr', 'sa')`\n  - `GROUP BY span`\n- **Displaying Dependencies**: UI displays all dependencies using automated nodes layout\n\n**Expanding Zipkin's Potential**\n- **Creative Tags**: By getting creative with tags, a distributed infrastructure like Zipkin has potential for:\n  - Creating ""controlled"" traces from a test robot driving one or more service(s)\n  - Displaying different diagrams for ""Cache = HIT"" and ""cache = MISS"" scenarios\n  - Displaying distinct diagrams for the ""Write part"" versus the ""Read part"" of an overall conversation across the system\n\n**Visible Workings: Software as Documentation**\n- **Brian Marick's Concept**: Visible workings, where software itself explains how it works from the outside\n- **Related Idea**: Relying on the software to document its internal mechanisms.",1361,38.91050171852112
False,3,"98-Domain-Specific Notation.pdf,98-Domain-Specific Notation,""Visible Tests 203\nto achieve this, and they all have in common relying on the software i","98-Domain-Specific Notation.pdf,98-Domain-Specific Notation,""Visible Tests 203\nto achieve this, and they all have in common relying on the software itself to output \nthe desired form of documentation.\nFor example, many applications perform calculations for payroll or bank state-\nments or other forms of data crunching. It is often necessary to describe how the processing is done for external audiences such as business people or compliance auditors.\nY ou may think of visible workings approaches as an exporting or reporting fea-\nture that explains to end users the way it works internally . Y ou want to be able to ask the software “How do you compute that?” or “What’s the formula for this result?” and have it just tell you the answer at runtime. There should be no need to ask a developer to get the answer.\nVisible workings are not often requested by customers, but they’re a valid answer \nto a need for more documentation. Visible workings techniques are obviously very useful for development teams. Such a team should have full latitude to decide to add features to make its own life easier, since it’s obviously one of the key stakeholders of its project. The key is to spend just enough time for the expected benefit.\nVisible Tests\nGood tests check the code against predefined assertions all the time. They are silent unless something unexpected occurs, such as a failed assertion or an error. However, I’ve found that tests can sometimes also be used to produce visible output such as diagrams in various domain-specific notations.\nWhen starting in exploration mode, such as during a spike, when the problems \nare not clear and you’re not sure how to solve them, it’s hard to define accurate assertions. However, visible output provides fast feedback on whether it works as expected or not. Later, as the tests turn into non-regression tools, you can add actual assertions, but you may still decide to keep some of the visible outputs as a way to show what’s happening.\nDomain-Specific Notation\nMany business domains have grown their own specific notations over time. Domain experts are comfortable with notation, usually doing it with pen and paper.\nFor example, for a supply chain, we tend to draw trees from the upstream produc-\ners to the distributors downstream, as shown in Figure 7.2. \nChapter 7 Runtime Documentation 204\nFor a stock exchange, we often have to draw order books when it is time to decide \nhow the matching happens, as shown in Figure 7.3. \nIn finance, financial instruments pay and receive cash flows (amounts of money) \nover a timeline, which we draw using vertical arrows on a timeline, as shown in \nFigure 7.4. Downstream\nUpstream\nLegend:\nMaterial Flow/\nTransportationRaw Materials\nSupplierManufacturing\nPlantRetailers/\nCustomersDistribution\nCenter\nFigure 7 .2 Supply chain tree\nBid\nVolumeAsk\nVolumePrice\n105.00\n104.00103.00102.00101.00104.50\n103.504020\n5030\n10\n102.50101.501030\n5020\n40\nFigure 7 .3 Order book for matching orders"",2949",3016,"**Visible Workings Notation in Domain-Specific Contexts:**\n\n**Background:**\n- Approach for achieving desired documentation form using software output\n- Calculations in payroll or data crunching applications require explanation to external audiences\n- Useful for development teams and stakeholders\n- Can be requested infrequently but beneficial\n\n**Visible Workings Techniques:**\n- Explanation of internal processing methods at runtime\n- No need to consult developers for answers\n- Handy during exploration mode and spike sessions when problems aren't clear\n- Fast feedback on whether it works as expected or not\n\n**Domain-Specific Notations:**\n- Business domains have their own specific notations\n- Useful for supply chain, finance, etc.\n- Visual representation of information flow\n\n**Supply Chain Notation:**\n- Tree diagram from upstream producers to distributors\n- Shows material flow and transportation (Figure 7.2)\n\n**Order Book Notation for Matching Orders in Stock Exchange:**\n- Diagram used during order matching process\n- Bid, ask, volume, and price displayed visually (Figure 7.3)\n\n**Finance Domain Notation:**\n- Vertical arrows on a timeline represent cash flows from financial instruments\n- Downstream represents payers, upstream are receivers (Figure 7.4).",1267,35.959253549575806
False,3,"99-Generating Custom Domain-Specific Diagrams to Get Visual Feedback.pdf,99-Generating Custom Domain-Specific Diagrams to Get Visual Feedback,""Visible","99-Generating Custom Domain-Specific Diagrams to Get Visual Feedback.pdf,99-Generating Custom Domain-Specific Diagrams to Get Visual Feedback,""Visible Tests 205\nGenerating Custom Domain-Specific Diagrams to Get \nVisual Feedback\nLong ago, at the beginning of a new project I used to create simple tests with no asser-\ntions that simply generated basic, ugly SVG files like the one shown in Figure 7.5. \nEUR13469 at 20/06/2010 EUR13161 at 20/09/2010 EUR12715 at 20/12/2010 EUR12280 at 20/03/2011 EUR12247 at 20/06/2011EUR11939 at 20/09/2011EUR11507 at 20/12/2011 EUR11205 at 20/03/2012 EUR11021 at 20/06/2012EUR8266 at 20/09/2012 EUR5450 at 20/12/2012EUR2695 at 20/03/2013Amortized Bond Coupons\nFigure 7 .5 Generating an SVG file\nCompare the information shown in Figure 7.5 to the following spreadsheet table:\n1 EUR13469 20/06/2010\n2 EUR13161 20/09/2010\n3 EUR12715 20/12/2010\n4 EUR12280 20/03/2011\n5 EUR12247 20/06/2011\n6 EUR11939 20/09/2011\n7 EUR11507 20/12/2011\n8 EUR11205 20/03/2012\n9 EUR11021 20/06/2012\n10 EUR8266 20/09/2012\n11 EUR5450 20/12/2012\n12 EUR2695 20/03/2013Borrower Lender$1000\ni = 10% i = 10%\n1\n0\n$1002\n$1003\n$1004\n$1005\n$11000\n$1000Inflows\nOutflows$1100\n$100\n5 1$100\n2$100\n3$100\n4\nFigure 7 .4 Cash flows over a timeline\nChapter 7 Runtime Documentation 206\nIt’s much easier to check the evolution of the amounts paid over time visually , \nusing the diagram.\nOf course, you could also dump a CSV file and graph it in your favorite spread-\nsheet application. Or you could even generate an XLS file with the graph inside pro-\ngrammatically; in Java, for example, you could use Apache POI to do this.\nFigure 7.6 shows a more complicated example of a generated diagram, which \nshows how the cash flows are conditioned by market factors. \nAs you can see, I’m not an expert with SVG, and the figures here are just quick \ngraphs to get visual feedback during the initial spike of a bigger project. Y ou could use a modern JavaScript library to produce beautiful diagrams!\n \nA Complement to Gherkin Scenarios?\nI haven’t tried it yet, but I’d love to have some key scenarios in Cucumber or \nSpecFlow produce such domain-specific diagrams in addition to the test results for their assertions. This sounds quite feasible, so if you happen to try it, please let me know!\n Return Leg CashflowsCDS Cashflows\nFunding Leg Cashflows\n-270000.0\nEUR270000 at 20/04/2012270000.0-270000.0\n270000.0270000.0\nEUR27653 at 20/03/2013 EUR28111 at 20/09/2012 EUR27806 at 20/03/2012EUR27806 at 20/03/2012EUR27000 at 20/04/2012\nEUR28111 at 20/09/2011 EUR27653 at 20/03/2011 EUR23375 at 20/09/2010EUR28111 at 20/09/2011 EUR27653 at 20/03/2011 EUR23375 at 20/09/2010\nFigure 7 .6 Generating the SVG file for the cash flows of  a more complicated financial \ninstrument"",2662",2811,"**Generating Custom Domain-Specific Diagrams to Get Visual Feedback**\n\n**Background:**\n- In the beginning of new projects, the author used to create simple tests without assertions that generated basic SVG files (Figure 7.5) for visual feedback\n- Compare Figure 7.5 with following spreadsheet table:\n  1 EUR13469 20/06/2010\n  2 EUR13161 20/09/2010\n  3 EUR12715 20/12/2010\n  4 EUR12280 20/03/2011\n  5 EUR12247 20/06/2011\n  6 EUR11939 20/09/2011\n  7 EUR11507 20/12/2011\n  8 EUR11205 20/03/2012\n  9 EUR11021 20/06/2012\n  10 EUR8266 20/09/2012\n  11 EUR5450 20/12/2012\n  12 EUR2695 20/03/2013\n- Borrower Lender\n  1 $1,000\n  10%\n  1 $1002\n  1 $1003\n  1 $1004\n  1 $1005\n  1 $11,000\n  1 $10,000 (Inflows)\n  2 -$11,000\n  2 $100\n  2 $100\n  2 $100 (Outflows)\n\n**Visual Feedback**:\n- It is easier to check the evolution of amounts paid over time visually using diagrams like Figure 7.5 instead of CSV files or spreadsheet applications\n- Generate XLS file with graphs in Java using Apache POI or use modern JavaScript libraries for beautiful domain-specific diagrams\n\n**More Complicated Example**:\n- Figure 7.6 shows a more complicated example of generated SVG file that illustrates cash flows conditioned by market factors\n- The author is not an expert at SVG but acknowledges these are quick graphs to get visual feedback during project development\n- Consider generating diagrams from Cucumber or SpecFlow scenarios as a complement to test results.",1437,81.38747501373291
False,3,"100-Living Diagrams from Event Sourcing Scenarios.pdf,100-Living Diagrams from Event Sourcing Scenarios,""Example: A Visible Test When Using Event Sour","100-Living Diagrams from Event Sourcing Scenarios.pdf,100-Living Diagrams from Event Sourcing Scenarios,""Example: A Visible Test When Using Event Sourcing 207\nExample: A Visible Test When Using \nEvent Sourcing\nEvent sourcing is a way to capture all changes to an application state as a sequence of \nevents. In this approach, every change to the state of an application (an aggregate in \ndomain-driven design terminology) is represented by events that are persisted. The state at a current point in time can be built by applying all past events.\nWhen a user or another part of the system wants to change the state, it sends \na command to the corresponding state holder (the aggregate) through a command handler. The command can be accepted or rejected. In either case, one or more events are sent for everyone who is interested. Events are named as verbs in the past tense, using nothing but domain vocabulary . Commands are named with imperative verbs, also from the domain language.\nWe can represent all this in the following way:\n1  Given past events\n2  when we handle a command3  Then new events are emitted\nIn this approach, each test is a scenario of the expected business behavior, and \nthere is not much to do to make it a business-readable scenario in fluent English. So \nwe are back to typical BDD goodness—without Cucumber!\nTherefore: You need no “BDD framework” when you’re doing event sourcing. \nIn this approach, if the commands and events are named properly after the domain language, the tests are naturally business-readable scenarios. If you want additional reporting for nondevelopers, you can print the events and the command through simple text transformations in your event sourcing testing framework.\nThere are many benefits of using event sourcing, and one of them is that you \nget very decent automated tests and living documentation almost for free. This was initially proposed by Greg Y oung in various talks,\n4 and Greg has made his related \nSimple.Testing framework available on Github.5 This idea was later elaborated by \nJeremie Chassaing.6\n4. Greg Y oung, Skills Matter, http://skillsmatter.com/podcast/design-architecture/talk-from-greg-young\n5. Simple.Testing, https://github.com/gregoryyoung/Simple.Testing\n6. Jeremie Chassaing, https://twitter.com/thinkb4coding\nChapter 7 Runtime Documentation 208\nA Concrete Example in Code\nLet’s consider an example of making (and eating) batches of cookies, taken from \nBrian Donahue on the CQRS mailing list7 discussing Greg’s approach:\nGiven: Batch Created with 20 Cookies\nWhen: Eat Cookies: Amount = 10Then: Cookies Eaten: Amount Eaten = 10, AmountRemaining: 10\nFor illustration purpose, I’ve created a similar very simple framework in Java.\n8\nIn this approach, and using this framework, the scenario is written literally in \ncode, through the direct use of domain events and commands that form the event \nsourcing API:\n1  @Test\n2  public void eatHalfOfTheCookies() {3    scenario(""""Eat 10 of the 20 cookies of the batch"""")4      .Given(new BatchCreatedWithCookiesEvent(20))5      .When(new EatCookiesCommand(10))6      .Then(new CookiesEatenEvent(10, 10));7  }\nThis is a test, and the “then” clause is an assertion. If no CookiesEatenEvent  \nevent is emitted, then this test fails. But it’s more than just a test; it’s also a part of \nthe living documentation, since running the test also describes the corresponding business behavior in a way that is quite readable, even for nondevelopers:\n1  Eat 10 of the 20 cookies of the batch\n2          Given Batch Created With 20 Cookies3          When Eat Cookies 10 cookies4          Then 10 Cookies Eaten and 10 remaining cookies\nHere the framework just invokes and prints the toString()  method of each \nevent and command involved in the test (aka scenario). It is as simple as this.\nAs a result, this is not as beautiful and “natural language” as text scenarios \nwritten by hand in a tool like Cucumber or SpecFlow , but it is not bad.\n7. Brian Donahue, https://groups.google.com/forum/#!topic/dddcqrs/JArlssrEXIY\n8. jSimpleTesting, https://github.com/cyriux/jSimpleTesting\nExample: A Visible Test When Using Event Sourcing 209\nOf course, there can be more than one event in the prior history of the aggregate, \nand more than one event can be emitted as a result of applying the command:\n1  @Test\n2  public void notEnoughCookiesLeft() {3    scenario(""""Eat only 12 of the 15 cookies requested"""")4      .Given(5        new BatchCreatedWithCookiesEvent(20),6        new CookiesEatenEvent(8, 12))7      .When(new EatCookiesCommand(15))8      .Then(9         new CookiesEatenEvent(12, 0),10        new CookiesWereMissingEvent(3));11 }\nThis second scenario would print as the following text:\n1  Eat only 12 of the 15 cookies requested2          Given Batch Created With 20 Cookies3          And 8 Cookies Eaten and 12 remaining cookies4          When Eat Cookies 15 cookies5          Then 12 Cookies Eaten and 0 remaining cookies6          And 3 Cookies were missing (no more cookies)\nThis little framework is just a builder producing test cases using method chain-\ning between the three methods Given(Event...) , When(Command) , and \nThen(Event...) . Each method stores the events and command passed as param-\neters. Calling the Then()  method at the end runs the full test and prints its text sce-\nnario by calling the toString()  method of each event and command, prefixed with \nthe keyword Given , When , or Then . When a keyword is repeated, it is aliased by And.\nThe method scenario(title)  instantiates the SimpleTest  class of the frame-\nwork the way you want it to print and log. From there, you can elaborate to go fur-\nther than just tests. For example, you might also use the knowledge from these tests to document the possible behaviors as living diagrams.\nLiving Diagrams from Event Sourcing Scenarios\nIn the example shown in the previous section, the test checks the behavior and prints a description of the business behavior in plain text that is readable by anyone.\nChapter 7 Runtime Documentation 210\nThere are several tests, each with different incoming events, commands, and out-\ncoming events. The union of all these tests represents the use cases for the considered \naggregate. This is often enough.\nIf you want to turn this test into diagrams, the event sourcing–based testing \nframework can collect all these inputs and outputs across the test suite in order to print a diagram of the incoming commands and the outgoing events.\nEach test collects commands and events. When the test suite has completed, it’s \ntime to print the diagram in the following fashion:\n1  add the aggregate as the central node of the diagram\n2  add each command as a node3  add each event as a node 45  add a link from each command to the aggregate6  add a link from the aggregate to each command\nWhen this is rendered with Graphviz in the browser, you get something like what \nis shown in Figure 7.7. \nY ou may or may not find this kind of diagram useful, and you can make your \nown based on this approach. This example illustrates that automated tests are a \ndata source to be mined for valuable knowledge that can then be turned into a living document or a living diagram.\nFigure 7 .7 The generated living diagram of  commands, aggregate, and events for the cookies \ninventory aggregate"",7277",7392,"**Event Sourcing with Living Documentation**\n* Event sourcing: sequence of events representing changes to application state (aggregates)\n* Each change is persisted as an event, state built by applying past events\n* Users send commands to aggregates through command handlers\n* Commands can be accepted or rejected, emitting events for interested parties\n* Events named as verbs in past tense using domain vocabulary, commands imperative verbs\n* Each test is a scenario of expected business behavior, written in code using domain events and commands\n* Scenarios printed as readable text, double-quoted examples given below\n\n**Benefits of Event Sourcing:**\n* Decent automated tests and living documentation almost for free\n* Proposed by Greg Young, implemented in Simple.Testing framework\n* Elaborated by Jeremie Chassaing\n\n**Runtime Documentation with Event Sourcing:**\n* Example: Making and Eating Cookies using domain events and commands\n1. Given: Batch Created with 20 Cookies\n2. When: Eat Cookies: Amount = 10\n3. Then: Cookies Eaten: Amount Eaten = 10, Amount Remaining = 10\n* SimpleTesting framework prints events and commands involved in test as living documentation\n\n**Multiple Events in Test:**\n1. Given: Batch Created with 20 Cookies\n2. Given: 8 Cookies Eaten (previous event)\n3. When: Eat Cookies: 15 cookies requested\n4. Then: 12 Cookies Eaten, 0 remaining cookies, 3 Cookies missing\n* Scenarios printed as readable text with Batch Created, Commands, and Outgoing Events\n1. Eat only 12 of the 15 cookies requested\n2. Given: Batch Created With 20 Cookies (central node)\n3. Add each command as a node and add links from commands to aggregate\n4. Add each event as a node and add links from aggregate to events\n5. Render the diagram using Graphviz in browser for visual representation of living documentation.",1814,59.67611074447632
False,3,"101-Introspectable Workings Code in Memory as a Source of Knowledge.pdf,101-Introspectable Workings Code in Memory as a Source of Knowledge,""Introspec","101-Introspectable Workings Code in Memory as a Source of Knowledge.pdf,101-Introspectable Workings Code in Memory as a Source of Knowledge,""Introspectable Workings: Code in Memory as a Source of Knowledge 211\nNote that the same content shown in Figure 7.7 could also be rendered as a table:\nCookies Inventory Commands\nBakeCookiesCommand\nEatCookiesCommand\nCookies Inventory Events\nBatchCreatedWithCookiesEventCookiesEatenEventCookiesWereMissingEvent\nY ou might also want to avoid mixing scenarios together, or you might decide to \nadd more information to the same picture. Y ou might, for example, remove the noise \nof the Event  or Command  suffixes. Y ou can customize this idea for in your particular \ncontext.\nIntrospectable Workings: Code in Memory as a Source \nof Knowledge\nAt runtime, code often takes the form of an object tree, a tree of objects that you cre-\nate by using new operators, factories, or builders or dependency injection frame-works such as Spring or Unity .\nOften, the exact nature of the object tree depends on the configuration or even on \na by-request basis, as illustrated in Figure 7.8. \nCreates\nBuilder\nObject Tree\n(Configuration A)Creates\nBuilder\nObject Tree\n(Configuration B)\nFigure 7 .8 An object tree at runtime may vary depending on the configuration or the request\nChapter 7 Runtime Documentation 212\nHow do you know what the object tree really looks like at runtime for a given \nrequest? The regular way is to look at the source code and try to imagine how it will \nwire the object tree. But you would probably still like to check whether your under-standing is correct.\nTherefore: Introspect trees of objects at runtime in order to display the actual \narrangement of objects, their object types, and their actual structure.\nIn languages like Java or C#, this can be done through reflection or through meth-\nods on each member of the structure to be introspected, as shown in Figure 7.9. The simplest form of this idea is to rely on the \ntoString()  method of each element to \ntell about itself and about its own members with some indentation scheme. When using a Dependency Injection (DI) container, you might as well try to ask the con-tainer to tell what it constructed. \nLet’s look at an example of a little search engine of hip-hop beats loops. It’s made \nup of an engine, at the root, that itself queries a reverse index for fast search queries. For indexing purposes, it also browses a repository of links contributed by users of the service, using a loop analyzer to extract the significant features of each beats loop to put into the reverse index. The analyzer makes use of a waveform processor.\nThe engine, reverse index, links repository , and loop analyzer are all abstract inter-\nfaces with more than one implementation each. The exact wiring of the object tree is determined at runtime and changes according to the environment’s configuration.Figure 7 .9 Introspecting the object tree, from its root"",2848",2995,"**Introspective Workings: Code in Memory as a Source of Knowledge**\n\n**Concepts:**\n- Introspection of object trees at runtime\n- Object tree formation and variation based on configuration or request\n- Using reflection or methods to introspect objects\n\n**Object Tree Runtime Variation:**\n1. Can depend on configuration\n2. May change according to requests\n3. Object trees can be introspected for actual arrangement, object types, and structure\n4. In Java and C#: Reflection or methods used for introspection\n5. Example of hip-hop beats loops search engine with varying object tree configurations\n\n**Introspection Techniques:**\n1. Reflection in languages like Java or C#\n2. Methods on each member of the structure\n3. Simple form: relying on `toString()` method of elements\n4. Using Dependency Injection (DI) container for container construction introspection\n\n**Example:**\n- Hip-hop beats loops search engine with varying object tree configurations\n1. Engine (abstract interface)\n2. Reverse index (abstract interface)\n3. Links repository (abstract interface)\n4. Loop analyzer (abstract interface)\n5. Waveform processor\n6. The exact wiring of the object tree is determined at runtime and changes based on environment's configuration.",1228,38.444483518600464
False,3,"102-Summary.pdf,102-Summary,""Introspectable Workings: Code in Memory as a Source of Knowledge 213\nIntrospecting with Reflection\nIf it’s an object, w","102-Summary.pdf,102-Summary,""Introspectable Workings: Code in Memory as a Source of Knowledge 213\nIntrospecting with Reflection\nIf it’s an object, we can traverse it.\n—Arnold Schwarzenegger\nIntrospecting a tree of objects is nothing but a trivial recursive traversal. From the \ngiven (root) instance, you get its class and enumerate each declared field because that’s how classes store their injected collaborators here. For each collaborator, you carry out the traversal through a recursive call.\nAs you might suspect, you need to filter uninteresting elements that you don’t \nwant to include in the traversal—classes like strings and other low-level stuff. Here the filtering is just based on the qualified names of the classes. If an instance of a class that has nothing to do with the business logic is passed as a parameter, then you want to just ignore it. The following code snippet illustrates how to do that intro-spection while filtering uninteresting elements:\n1  final String prefix = org.livingdocumentation.\nvisibleworkings."""";23  public void introspectByReflection(final Object instance, int depth)\4  throws IllegalAccessException {5    final Class<?> clazz = instance.getClass();6    if (!clazz.getName().startsWith(prefix)) {7      return;8    }9    // System.out.println(indent(depth) + instance);10   for (Field field : clazz.getDeclaredFields()) {11     field.setAccessible(true);// necessary12     final Object member = field.get(instance);13     introspectByReflection(member, depth + 1);14   }15 }\nWith this code, if you just print each element with the proper indentation, the \nconsole displays the following:\n1  SingleThreadedSearchEngine2  ..InMemoryLinkContributions3  ..MockLoopAnalyzer4  ....WaveformEnergyProcessor5  ..MockReverseIndex\nChapter 7 Runtime Documentation 214\nThis engine is a single-threaded one, and it uses an in-memory repository of \ncontributed links, together with a mock of a loop analyzer and another mock of a \nreverse index.\nWith the same code, you can instead build a DOT diagram with each element and \nthe proper relationships between them, as shown in Figure 7.10. \nThis diagram shows the same information as the text in the console we had before, \nbut the visible relationship could show additional information.\nIntrospecting Without Reflection\nT o introspect an object tree without using reflection, all the objects in the tree must exhibit an accessible way to enumerate their collaborators. Y ou could accomplish this with public fields, but I do not recommend that. Instead, they can expose a pub-lic method that returns the list of their members.\nIn the simplest case, every element would implement some interface like \nIntrospectable , becoming an instance of the composite pattern:\n1  interface Introspectable {\n2    Collection<?> members();3  }\nThus the traversal of the tree would again be nothing but the recursive traversal of \nthe composite:\n1  private void traverseComposite(Object instance, int depth) {2    final String name = instance.getClass().getName();3    // Add this node to the diagram4    digraph.addNode(name).setLabel(instance.toString());WaveformEnergyProcessorSingleThreadedSearchEngine\nMockLoopAnalyzer MockReverseIndex InMemoryLinkContributionsIntrospectable Workings\nFigure 7 .10 Introspecting the object tree, in practice\nSummary 215\n5    if (!(instance instanceof Introspectable)) {\n6        return;7    }8    final Introspectable introspectable = (Introspectable) instance;9    for (Object member : introspectable.members()) {10     traverseComposite(member, depth + 1);11     // Add the relationship from this node to its member to the diagram12     digraph.addAssociation(name, member.getClass().getName());13   }14  }\nObviously , this approach produces exactly the same output you get by using \nreflection.\nWhich approach should you choose? If all the objects are created by the team \nand there aren’t too many of them, I recommend the composite flavor, as long as it \ndoesn’t pollute the classes too much.\nIn all other cases, the approach of introspection by reflection is the best or only \nchoice. This approach helps make the inner workings visible. In the case of a work-flow , decision tree, or decision table that is built on the fly for each given business request, introspectable workings is a way to make the particular structure that was built visible for users and developers alike.\nSometimes, however, you don’t need any introspection at all. When the processing \nis driven by a configuration, hardcoded, or from a file or from a database, displaying the workings may be simplified a lot, as this is just a way to display the configura-tion in a nice way . At a minimum, every workflow or processing that is driven by a configuration should be able to display the configuration that is used for a particular type of processing.\nSummary\nBecause some knowledge is much more easily accessible when the source code is exe-cuted at runtime than from the static artifacts in the source code repository , working software can, and should, be considered a source of truth for documentation pur-poses. And with distributed architecture and cloud infrastructure becoming more prevalent, so will the opportunities for exploiting runtime knowledge that is also accessible by machines for better living documentation.\nThis page intentionally left blank"",5361",5397,"**Introspecting Object Trees: Code in Memory as a Source of Knowledge**\n\n**Reflection-Based Introspection**:\n- Traverse the tree by following declared fields in objects\n- Filter uninteresting elements (e.g., strings, low-level classes)\n- Example code snippet illustrating recursive traversal\n- Output: Console display or DOT diagram\n\n**Introspecting Without Reflection**:\n- Objects must exhibit an accessible way to enumerate collaborators\n- Use public methods instead of reflection or public fields\n- Interface `Introspectable` and composite pattern implementation\n- Example code snippet illustrating recursive traversal of composite object tree\n\n**Choosing the Approach**:\n- If all objects are created by the team and not too many, use the composite approach\n- Otherwise, use reflection-based introspection for more flexibility\n- Displaying workflows or processing driven by configuration can be simplified\n\n**Importance of Runtime Documentation**:\n- Knowledge accessible at runtime is valuable for documentation purposes\n- Increasing prevalence of distributed architecture and cloud infrastructure",1100,33.68533182144165
False,3,"103-Text Layout.pdf,103-Text Layout,""217Chapter 8\nRefactorable Documentation\nLiving documents are documents that are automatically generated using u","103-Text Layout.pdf,103-Text Layout,""217Chapter 8\nRefactorable Documentation\nLiving documents are documents that are automatically generated using up-to-date \nknowledge from an authoritative source. In contrast, documents created using pro-prietary tools must be updated manually , which is tedious. Between these two extreme cases are refactorable documents, which must be manually updated but in a smart way , thanks to automation tools that reduce the labor-intensive burden.\nFor text documents, one example of a tool that enables refactorability is the Find \nand Replace feature of most text editors, which makes it easy to consistently change a word used in a large document. For example, this can be appealing for maintaining diagrams that change only from time to time, where each word is referenced multiple times because of the links between the words.\nBut the prominent example of a tool that enables refactorability is source code \nand the automated refactorings that apply on it. Developers spend a lot of time read-ing code and trying to get knowledge out of it. If you think of code specifically for its documentation value, then it becomes a key ingredient in your living documentation, an ingredient that embraces change very well.\nAutomated refactoring is one of the primary ways of changing a software sys-\ntem. In healthy projects, this process is used all the time. When renaming a class or a method somewhere, an automated refactoring tool updates the declaration and every instance of its use throughout the project—instantly . Moving a type to another module, adding a parameter to a function signature, and extracting a function out of a block of code are just a few transformations that tools do automatically . This refac-toring automation, combined with tests, enables teams to make changes, including big changes, very frequently because the tools remove the pain.\nChapter 8 Refactorable Documentation 218\nRefactoring all the time is a good thing, even if it can be seen as a challenge for \ntraditional documentation. Living documentation embraces continuous refactoring \nand involves documentation approaches that leverage the power of automated refac-torings instead of resisting them.\nCode as Documentation\nPrograms are meant to be read by humans and only incidentally for computers to \nexecute.\n—Harold Abelson, Gerald Jay Sussman, and Julie Sussman, Structure and \nInterpretation of  Computer Programs\nThat, yes, but more. The source code is also the ONLY document in whatever collection you may have that is guaranteed to reflect reality exactly . As such, it is the only design document that is known to be true. The thoughts, dreams, and fantasies of the designer are only real insofar as they are in the code. The pictures in the reams of UML are only veridical insofar as they are in the code. The source code is the design in a way that no other document can claim. One other thought: maybe gloss isn’t in the writing, maybe it’s in the reading.\n1\n—Ron Jeffries\nMost of the time, code is its own documentation. Code is written for machines, of \ncourse, but code is also written so that human beings can understand it for its main-tenance and evolution.\nIt takes a lot of skills and techniques to improve the ability of code to be quickly \nand clearly understood by people. It’s a core topic in the software crafters commu-nity , and there have been many books, articles, and conference talks on the topic, which this book is not meant to replace. Instead it focuses on a few practices and techniques that are especially relevant, typical, or original with respect to the idea of code being documentation itself. As Chris Epstein once said during a talk, “Be kind to your future self.” Learning how to make code easy to understand will be a big bonus for your future self.\nMany books have been written on writing code that is easy to read. Of particular \nimportance are Clean Code by Robert Cecil Martin (commonly referred to as Uncle Bob) and Implementation Patterns by Kent Beck. Kent advocates asking yourself, “What do I want to say to someone when they read this code?” and “What will the computer do with this code?” and “How can I communicate what I am thinking to people?”\n2\n1. http://wiki.c2.com/?WhatIsSoftwareDesign\n2. Beck, Kent. Implementation Patterns. Hoboken: Addison-Wesley Professional, 2007.\nCode as Documentation 219\nText Layout\nWe usually think of code as a linear medium, but code is itself a graphical arrange-\nment of characters in the two-dimensional space of a text editor. This 2D layout of the code can be used to express meaning.\nThe most common examples of text layout are the guidelines on the ordering of \nthe members of a class:\n •Class declaration\n •Fields\n •Constructors and methods\nWith this ordering, even as the class is declared as plain text, there is a visual \naspect implied by the layering of the blocks of text on the page. This is not that far from how a class is visually represented in UML (see Figure 8.1). The main difference between code layout and visual notation is the absence of the border lines around the blocks of text in the code.\n \nThe following sections look at other cases of code layout.\nTabular Code Layout\nTake the example of a socket considered as a state machine. This state machine can be fully described through its state transition table, which can be expressed literally as code. In this case, the layout really matters, including the vertical alignment of the current states, the transitions, and the next states, as shown in Figure 8.2.\n \nThis type of layout is easy to do with code, except that the automatic code \nformatting of the IDE may often break this alignment. Putting an empty comments section (\n/**/ ) at the beginning of a line can prevent the formatter from reordering \nthe lines, but it’s hard to preserve the whitespace. Of course, this all depends on your IDE and its capabilities to autoformat in a smarter way .Class Name\nFields\nMethods\nFigure 8.1 UML visual notation for a class\nChapter 8 Refactorable Documentation 220\nArrange–Act–Assert\nUnit tests offer examples of how the 2D graphical layout of code can be used to \nexpress meaning. The Arrange–Act–Assert convention advocates organizing code in \nthree different sections, each below the previous one, as shown in Figure 8.3. \nWhen you’re familiar with this convention, the vertical layout makes it graphi-\ncally obvious what each section is doing. Y ou can tell just by looking at the composi-tion of text versus whitespace.\nAnother convention in unit tests involves considering that a unit test matches a \ngiven expression on the left with another expression on the right. In this approach, the horizontal layout is meaningful: Y ou want the full assertion on one single line, with the two expressions on both sides of the assertion, as shown in the example in Figure 8.4.\n \nFigure 8.2 The transition table of  a socket as a state machine with its expressive code layout\nFigure 8.3 The convention Arrange–Act–Assert in a unit test"",7043",7086,"**Refactorable Documentation**\n\n**Living Documents**:\n- Automatically generated using up-to-date knowledge from an authoritative source\n- In contrast to documents created using proprietary tools that must be updated manually\n\n**Refactorable Documents**:\n- Must be manually updated, but with automation tools that reduce the labor-intensive burden\n\n**Text Layout**:\n- Refactorability tools for text documents:\n  - **Find and Replace feature** in text editors\n  - Automated refactoring of source code\n\n**Code as Documentation**:\n- Code is a key ingredient in living documentation, embracing change\n- Automated refactoring is the primary way to update a software system\n- Refactoring tools automatically update declarations and uses throughout the project\n\n**Code Layout**:\n- Code can be organized in a graphical layout to express meaning\n- Examples:\n  - **Tabular code layout** for state transitions in a socket\n  - **Arrange–Act–Assert convention** in unit tests",961,31.578566551208496
False,3,"104-Coding Conventions.pdf,104-Coding Conventions,""Code as Documentation 221\nMuch more could be said about various possible ways to organize code gra","104-Coding Conventions.pdf,104-Coding Conventions,""Code as Documentation 221\nMuch more could be said about various possible ways to organize code graphi-\ncally , but this section is just meant to draw your attention to this universe of \npossibilities.\nCoding Conventions\nProgramming has always relied on conventions to convey additional meaning in the code. The programming language syntax does a lot of the job. For example, in C# and Java, it’s easy to recognize the method \nplay()  from the variable play  because \nmethods end in parentheses. But the parentheses are not enough to indicate the dif-ference between class identifiers and variable identifiers. As a result, we rely on nam-ing conventions, such as particular uses of lowercase and uppercase, to quickly distinguish between class names and variable names. Such conventions are so ubiqui-tous that they can be considered mandatory .\nFor example, in Java, class names must be in mixed case, with the first letter of \neach internal word capitalized (for example, \nStringBuilder ). This convention is \nsometime called CamelCase. Instance variables follow the same convention except that they must start with a lowercase first letter (for example, \nmyStringBuilder ). \nConstants, on the other hand, should be all uppercase with words separated by underscores (for example, \nDAYS_IN_WEEK ). When you are familiar with such con-\nventions, you don’t need to think too hard about them, and you instantly recognize classes, variables, and constants based on their case.\nNote that the standard Java and C# notations are redundant with the coloring \nand syntax highlighting of your IDE (instance variables are blue, static variables are underlined, and so on). So, in theory , you should not even need the naming conven-tion any longer.\nThe Hungarian notation is an extreme example of using a naming convention \nto store information. In it, the name of a variable or function indicates the type or intended use of that variable or function. The idea is to encode the type into a short prefix, as in these examples:\n •lPhoneNum: The variable is a long integer ( l).\n •rgSamples: The variable is an array or a range of Sample elements ( rg).\nFigure 8.4 A test is about matching the expressions on the left with the expression \non the right\nChapter 8 Refactorable Documentation 222\nThe visible drawback of this notation is that it makes identifiers ugly , as if they \nwere obfuscated. I definitely do not recommend this convention.\nA convention is more than just a matter of convenience; it’s also a social con-\nstruct, a social contract between all developers in a community . When you are famil-\niar with a convention, you feel at home with it, and you may even feel disturbed when you encounter a different convention. Familiarity with notation makes it almost invisible, even if it’s very cryptic to those who don’t understand it.\nThe Hungarian notation originated in languages that lack a type system, and \nusing such notation helped you remember the type of each variable. However, unless you’re still coding in BCPL, it’s very unlikely that you need such notation because it impedes code readability too much, for almost no benefit.\n Caution\nIt’s unfortunate that C# has kept the convention of prefixing every interface with \nI, as this is reminiscent of Hungarian notation and has no benefit. From a \ndesign perspective, we should not even know whether a type is an interface or a class; it does not matter from a caller point of view . In fact, you might start with a class and later generalize it into an interface when really needed, and this should not change the code much. However, it’s part of the standard convention that should be followed, unless all developers involved in an application agree not to.\n \nIn languages with no built-in support for namespaces, it’s common practice to \nprefix all types with a module-specific prefix, as shown here:\n •ACMEParser_Controller: Module ACMEParser\n •ACMEParser_Tokenizer: Module ACMEParser\n •ACMECompiler_Optimizer: Module ACMECompiler\nThis is usually a bad practice, as it pollutes the class names with information that \ncould be factored out in their package (Java) or namespace (C#):\n •acme.parser: Controller\n •acme.parser: T okenizer\n •acme.compiler: Optimizer"",4270",4327,"**Coding Conventions**\n\n**Programming Conventions**:\n- Syntax alone is not enough to convey additional meaning\n- Rely on **naming conventions** to distinguish between different elements (e.g., class names, variable names)\n\n**Java Naming Conventions**:\n- Classes: **mixed case**, first letter of each internal word capitalized (e.g., **StringBuilder**)\n- Instance variables: same as classes but start with a lowercase first letter (e.g., **myStringBuilder**)\n- Constants: **all uppercase** with words separated by underscores (e.g., **DAYS_IN_WEEK**)\n\n**Hungarian Notation**:\n- Extreme example of using naming conventions to encode information\n- Example identifiers: \n  - **lPhoneNum**: variable is a long integer\n  - **rgSamples**: variable is an array or range of ""Sample"" elements\n\n**Conventions as Social Constructs**:\n- Conventions are more than just convenience, they're a social contract between developers\n- Familiarity with conventions makes code feel ""at home"", while unfamiliar conventions can be disturbing\n\n**Caution on C# Interface Naming Convention**:\n- C# convention of prefixing interfaces with ""I"" is reminiscent of Hungarian notation and has no benefit\n- It's better to use namespaces or packages instead of prefixing class/interface names",1257,36.68098545074463
False,3,"105-Naming as the Primary Documentation.pdf,105-Naming as the Primary Documentation,,0","105-Naming as the Primary Documentation.pdf,105-Naming as the Primary Documentation,,0",86,**Naming as the Primary Documentation**\n\n**105-Naming**: the primary documentation process.,91,4.369690179824829
False,3,"106-Idiomatic Naming Is Contextual.pdf,106-Idiomatic Naming Is Contextual,""Naming as the Primary Documentation 223\nAs you’ve seen, coding conventions","106-Idiomatic Naming Is Contextual.pdf,106-Idiomatic Naming Is Contextual,""Naming as the Primary Documentation 223\nAs you’ve seen, coding conventions try to extend the syntax of a programming \nlanguage to support features and semantics that are missing. When you have no type, \nyou must manage the type by hand with some help from the naming convention. On the other hand, types can be very helpful for documentation.\nNaming as the Primary Documentation\nSearching for just the right words is a valuable use of time while designing.\n—@Kent Beck and W ard Cunningham, “ A Laboratory for T eaching \nObject-Oriented Thinking”\nOne of the most important documentation tools is naming. Despite its unattractive-ness, naming should never be overlooked. More often than not, the names bestowed by the original authors are the only element of documentation available to retrieve those authors’ knowledge. Good naming is immensely important. But good naming is diffi-cult. Names as a social convention need agreement and shared connotations. Check-ing a thesaurus for alternative terms, listening actively to the words used in spontaneous conversations, and asking your workmates feedback about names can help.\nGood names are not just useful when you read them; they are also useful when \nyou’re searching for something. Good naming ensures that all names are searchable. The programming language named Go provides an example of naming that fails on the searchability front, which is especially interesting considering that it originates from the “search company” known as Google.\nComposed Methods: You Need to Name Them\nNames don’t live in isolation. In object-oriented programming languages, the set of class names form a language, and the words have various relationships to each other, gaining expressivity as a whole. In the paper “ A Laboratory for Teaching Object-Oriented Thinking” (1989), Kent Beck and Ward Cunningham wrote:\nThe class name of an object creates a vocabulary for discussing a design. Indeed, many \npeople have remarked that object design has more in common with language design than with procedural program design. We urge learners (and spend considerable time ourselves while designing) to find just the right set of words to describe our objects, a set that is internally consistent and evocative in the context of the larger design environment.\n3\n3. Kent Beck and Ward Cunningham, “ A Laboratory for Teaching Object-Oriented Thinking,” http://\nt.co/PjQfDzRZcX"",2418",2499,"**Naming as the Primary Documentation**\n\n**Coding Conventions**:\n- Extend programming language syntax to support missing features and semantics\n- When no type is available, manage it manually with naming conventions\n- Types can be helpful for documentation\n\n**Importance of Naming**:\n- Naming is a primary form of documentation\n- Good names are important, but difficult to achieve\n- Names need agreement and shared connotations\n- Checking thesaurus, listening actively, and asking workmates for feedback can help\n- Good names are useful when reading and searching\n\n**Composed Methods**:\n- Naming is not isolated, classes form a language in object-oriented programming\n- Class names create a vocabulary for discussing design\n- Finding the right set of words to describe objects is important\n\n**Example: The Go Programming Language**:\n- Provides an example of naming that fails on searchability\n- Despite being developed by Google, the ""search company""",950,32.99991512298584
False,3,"107-From Primitives to Types.pdf,107-From Primitives to Types,""Chapter 8 Refactorable Documentation 224\nFor more on naming and practical advices, I s","107-From Primitives to Types.pdf,107-From Primitives to Types,""Chapter 8 Refactorable Documentation 224\nFor more on naming and practical advices, I suggest reading the chapter on \nnaming written by Tim Ottinger in Robert C. Martin’s book Clean Code.\nIdiomatic Naming Is Contextual\nThe naming style does not have to be uniform throughout a large code base. \nDifferent areas of the system call for different idiomatic styles. For example, I always go for business domain names within a domain model or in the domain layer (for example, \nAccount , ContactBook , Trend ). But on the infrastructure layer or \nadapters (in the hexagonal architecture sense), I like to use prefixes and suffixes to qualify technologies and patterns being used in the corresponding implementing subclasses (for example, \nMongoDBAccount , SOAPBankHolidaysRepository , \nPostgresEmailsDAOAdapter , RabbitMQEventPublisher ). In this example of a \ndouble standard in naming, the names must tell what things are within a domain model, whereas outside of a domain model, in the infrastructure code, the names must tell how they are implemented.\nCoding Against a Framework\nBut if you write an app “without a framework,” you end up with an under-specified, \nun-documented, informal framework.\n—Hacker News, https://news.ycombinator .com/item?id=10839081\nCoding against a popular or opiniated framework has great value for strong docu-\nmentation. Code that is not written needs no documentation. When you use an off-the-shelf framework such as Spring Boot (a lightweight microservice framework) or Axon-Framework (a framework for event-sourced applications), a lot of code is already written, and your code has to conform to what the framework expects. Choosing such a framework may be a good idea for a team of limited maturity , where the framework will constrain the design to follow some structure. This might sound like a bad thing, but it’s a good thing from a knowledge transfer perspective: There is less room for surprise, and when you’re familiar with the framework, you can under-stand most of the code. In addition, such frameworks are well documented, and their use of annotations also provides documentation in the code, as shown in the follow-ing example:\n1  @CommandHandler\n2  void placePurchase(PlacePurchaseCommand c){...}\nT ype-Driven Documentation 225\nType-Driven Documentation\nTypes are powerful vehicles for storing and conveying knowledge for both developers \nand tools. With a type system, you need no Hungarian notation; the type system knows which type is there. It’s part of the documentation, whether it is a compile time (Java, C#) or runtime (TypeScript) system.\nIn a Java or C# IDE, you can see the type of everything by putting the mouse over \nit, and a tooltip will tell you about its type.\nPrimitives are types, but types really shine when you use custom types instead of \nprimitives. For example, the following code does not tell the whole story that this quantity is supposed to represent an amount of money , and you need a comment to tell the currency:\n1  int amount = 12000; // EUR\nBut if you create a custom type Money , as a class, for example, it becomes explicit. \nNow you know it’s an amount of money , and the currency is part of the code:\n1  Money amount = Money.inEUR(12000);\nThere are many advantages to creating types for different concepts, and docu-\nmentation is a very important one. This is not a random integer anymore, it’s an amount of money , the type system knows that and can tell you.\nY ou can also check the \nMoney  type to learn more about it. For example, here is the \nclass’s Javadoc comment description:\n1  /**\n2  * A quantity of money in Euro (EUR), 3  * for accounting purpose,4  * i.e. with an accuracy of exactly 1 cent.5  *6  * Not suited for amounts over 1 trillion EUR.7  */8  class Money {9  ...10 }\nThis is valuable information, and it’s best located in the code itself rather than in \nrandom document somewhere else."",3932",4001,"**Chapter 8: Refactorable Documentation - Naming Conventions**\n* **Idiomatic Naming**: Dependent on context within a large codebase\n* Business domain names in domain model or domain layer (e.g., `Account`, `ContactBook`)\n* Infrastructure layer or adapters: use prefixes and suffixes to qualify technologies (e.g., `MongoDBAccount`, `SOAPBankHolidaysRepository`)\n* Naming styles not uniform throughout a codebase\n* **Coding Against a Framework**:\n  * Writing without a framework results in unspecified, undocumented code\n  * Using popular frameworks (e.g., Spring Boot, Axon-Framework) provides documentation through annotations and well-documented use\n* **Type-Driven Documentation**\n  * Types serve as vehicles for storing and conveying knowledge\n  * Custom types convey more information than primitives (e.g., `int` vs. `Money`)\n  * Creating custom types explicitly defines the concept they represent\n  * Javadoc comments provide valuable information about custom types in code.",980,29.458256483078003
False,3,"108-Types over Comments.pdf,108-Types over Comments,""Chapter 8 Refactorable Documentation 226\nY our types are an essential part of your documentation","108-Types over Comments.pdf,108-Types over Comments,""Chapter 8 Refactorable Documentation 226\nY our types are an essential part of your documentation. Type everything and \nname the types carefully .\nTherefore: Use types whenever possible. The stronger the types are, the better. \nAvoid bare primitives and bare collections. Promote them into first-class types. Name your types carefully, according to the ubiquitous language, and add just enough documentation on the types themselves.\nFrom Primitives to Types\nIn the following example, the code switches on a String ; it’s a type, but a weak one, \nwhich in practice is almost like a primitive:\n1  validate(String status)\n2    if (status == """"ON"""")3    ...4    if (status == """"OFF"""")5    ...6    else7      // some error message\nThis kind of code is shameful. Because a String  can be anything, you need an \nadditional else  clause to catch any unexpected value. All this code describes the \nexpected behavior, but if this behavior were done by the type system—for example, \nby using a typed enum—there would simply be no code to write at all:\n1  switch (Status status){\n2    case: ON ...3    case: OFF ...4  }\nDocumented Types and Integrated Documentation\nA type is a perfect place to put documentation about a concept in a Javadoc section \nor its C# equivalent. Such documentation will evolve throughout the life of the type: It’s created when the type is created, and if the type is deleted, its documentation will go away with it. If the type is renamed or moved, its documentation remains attached to it, so there is no maintenance.\nT ype-Driven Documentation 227\nThe only risk is that if you change the definition of the type without changing its \ndocumentation, you might still end up with misleading documentation. However, \nthis risk is lower because the documentation is co-located with the type declaration.\nAn obvious benefit of using types with their documentation is that it gives you \nintegrated documentation directly within your IDE. When you mouse over a type name anywhere in the code, the IDE shows a small popup with the related documen-tation. When you’re using autocompletion, a brief excerpt of the documentation is shown in front of each autocompletion option.\nTypes and Associations\nAssociations in code are expressed as member fields to types. The code and its types can tell a lot, but sometimes you need something more. Let’s consider a few  examples. When the associations are one-to-one, and the member fields are well named, you need nothing more than this:\n1  // nothing to say\n2  private final Location from;3  private final Location to;\nThere is no need to tell much when types can also express meaning themselves. In \nthe following example, the annotation is redundant with the declared type, and it is \ncommon knowledge that a Set enforces unicity:\n1  @Unicity\n2  private final Set<Currency> currencies;\nSimilarly , the following code does not need the additional ordering declaration, as it \nis implied by the concrete type (but is this really the case from the caller point of view?):\n1  @Ordered2  Collection<Item> items = new TreeSet<Item>();\nY ou could refactor into a new declared type to make the documentation \nredundant:\n1  SortedSet<Item> items = new TreeSet<Item>();\nBut doing this exposes a lot of methods you may not want to expose. If you would \nlike to expose only Iterable<Item> , the ordering is an internal detail.\nY ou can see here that I prefer types over annotations.\nChapter 8 Refactorable Documentation 228\nTypes over Comments\nComments can and often do lie. So does naming, though to a lesser extent. But types \ndon’t lie; if they did, the program would not even compile.\nA method name may pretend to be the following:\n1  GetCustomerByCity()\nBut regardless of its name, if the signature and its types are actually as follows, \nyou get a much more accurate picture of what it really is:\n1  List<Prospect> function(ZipCode)\nAnd it could even be improved: List<Prospect>  could be a type in itself, some-\nthing like Prospects  or ProspectionPortfolio .\nWith just primitives, you’re on your own to decide whether you can trust the nam-\ning or not. What does the Boolean ignoreOrFail  mean? Enums, such as IGNORE  \nand FAIL , add accuracy .\nOptional<Customer>  expresses the possible absence of a result with total \n accuracy . In languages that support them, monads signal the presence of side effects with total accuracy . In these examples, the information is accurate because the com-piler enforces it.\nGenerics such as \nMap<User, Preference>  tell a lot, whatever the variable \nname.\nIn case you’re still not convinced, you can read this study on the topic: “What Do \nWe Really Know About the Differences Between Static and Dynamic Types?”4\nA Touch of Type-Driven Development\nWhen using types, even if you didn’t name the variables, you could still determine a lot about them, thanks to their type. Consider the following variable declaration:\n1  FuelCardTransactionReport x = ...\nThe type name tells it all. The variable name will be useful only if there’s more \nthan one instance in the scope at the same time.\nThe same goes for functions and methods. Even without knowing its name, you \ncan tell that a function that takes ShoppingCart  as an argument and returns Money  \nprobably has something to do with pricing or tax calculation. By just looking at the function signature, you can glean a good understanding of what the function can do.\n4. Stefan Hanenberg, http://www .slideshare.net/mobile/devnology/what-do-we-really-know-about-the-\ndifferences-between-static-and-dynamic-types\nT ype-Driven Documentation 229\nOn the other hand, if you’re trying to find the code doing the pricing of a shop-\nping cart, you have two options:\n •Guess how the class or method is named and perform a search based on your \nguess\n •Guess the signature in terms of type and perform a search by signature\nHaskell has a documentation tool called Hoogle that can show every function \nwith a given signature. In Java using Eclipse (Kepler), you can also search by method signature. In the search menu, you select the Java Search tab, select the radio buttons Search For: Method and Limit T o: Declarations, and then type in the search string (see Figure 8.5):\n1  *(int, int) int \nY ou get a lot of search results of methods that take two integers as parameters \nand return another integer, for example:\n1 com.sun.tools.javac.util.ArrayUtils\n                           .calculateNewLength(int, int) int2 com.google.common.math.IntMath.mean(int, int) int3 com.google.common.primitives.Ints.compare(int, int) int4 org.apache.commons.lang3.RandomUtils.nextInt(int, int) int5 org.joda.time.chrono.BasicChronology                           .getDaysInYearMonth(int, int) int6 ...\nFigure 8.5 Search by method signature in Eclipse"",6842",6905,"**Types Over Comments**\n\n**Importance of Types:**\n- Essential part of documentation\n- Use types instead of primitives or bare collections\n- Promote them into first-class types\n- Name your types carefully using ubiquitous language\n\n**From Primitives to Types:**\n- Code switching on String is weak, use typed enums instead\n- Documentation evolves with the type and stays attached to it\n\n**Documented Types and Integrated Documentation:**\n- Integrated documentation directly within IDEs\n- Preferred over comments as they don't lie or fade away\n\n**Types and Associations:**\n- Express associations as member fields to types\n- Well-named fields provide enough information for one-to-one associations\n- Use new declared types to make documentation redundant\n\n**Type-Driven Development:**\n- Types determine meaning of variables, functions, and methods\n- Determine function's purpose by looking at its signature\n\n**Searching Methods by Signature:**\n- Haskell has Hoogle for function search based on given signature\n- Java in Eclipse offers the same functionality using method signature search.",1084,42.715426445007324
False,3,"109-The Composed Method.pdf,109-The Composed Method,""Chapter 8 Refactorable Documentation 230\nThis works not just for primitives such as integers but","109-The Composed Method.pdf,109-The Composed Method,""Chapter 8 Refactorable Documentation 230\nThis works not just for primitives such as integers but for any type. For example, if \nyou were looking for a method to calculate the distance between two  Coordinates  \nobjects (Latitude , Longitude ), you would search for the following signature, \nusing the fully qualified type names:\n1  *(flottio.domain.Coordinates, flottio.domain.Coordinates) \ndouble\nThis would find the service you are looking for, without knowing its name:\n1  GeoDistance.distanceBetween(Coordinates, Coordinates) double\nY ou might have heard about type-driven development (TDD), or type-first devel-\nopment (TFD). These approaches have similar ideas about types.\nThe Composed Method \nClear code, like clear writing, is hard to do. Often you can only tell how to make it clear \nwhen someone else looks at it, or you come back to it at a later date.\nWard Cunningham explained it like this. Whenever you have to figure out what code \nis doing, you are building some understanding in your head. Once you’ve built it, you should move that understanding into the code so nobody has to build it from scratch in their head again.\n—Martin Fowler , “Refactoring”\nClear code does not happen by chance. Y ou have to make it emerge through continu-\nous refactoring, using all your design skills. For example, it could be a good idea to follow the four rules of simple design expressed by Kent Beck.\n5, 6\nAmong all the design skills, the composed method pattern is particularly relevant \nfor documentation purposes. For example, what’s this block of code doing?\n •It’s squishing the fibbly-bar.\n •So should we extract it into a squishFibblyBar  function?\nThe composed method is an essential technique for writing clear code. It involves \ndividing code into a number of small methods, each of which performs one task. Because each method is named, method names are the primary documentation.\n5. Martin Fowler, ThoughtWorks, http://martinfowler.com/bliki/BeckDesignRules.html\n6. Corey Haines, “Understanding the Four Rules of Simple Design,” https://leanpub.com/\n4rulesofsimpledesign\nThe Composed Method 231\nA common refactoring is to replace a block of code that requires a comment into \na composed method named after the comment. Consider the following example:\n1  public Position equivalentPosition(Trade... trades) {\n2          // if trades list has no trade3          if (trades.length == 0) {4                  // return position of quantity zero5                  return new Position(0);6          }7          // return quantity of first trade8          return new Position(trades[0].getQuantity());9  }\nHere the comments suggest that you can do better, such as by simplifying the \ncode or extracting methods into composed methods. Y ou could extract little cohesive \nblocks of code into their own composed method, as shown here:\n1  public Position equivalentPosition(Trade... trades) {\n2         if (hasNo(trades)) {3                 return positionZero();4         }5         return new Position(quantityOfFirst(trades));6  }78  //----910 private boolean hasNo(Trade[] trades) {11          return trades.length == 0;12 }1314 private Position positionZero() {15         return new Position(0);16 }1718 private static double quantityOfFirst(Trade[] trades) {19         return trades[0].getQuantity();20 }\nNotice that the first method now describes the overall processing, and the other \nthree methods underneath describe low-level parts of the code. This is another way \nto make code clear by organizing the methods into different levels of abstraction.\nHere the first method is one level of abstraction above the three other methods. \nUsually you can just read the code in the higher level of abstraction to understand"",3770",3829,"**Composed Method Pattern for Writing Clear Code**\n\n**Clear Code**:\n- Hard to achieve through chance\n- Requires continuous refactoring and use of design skills\n- Relevant for documentation purposes, especially the composed method pattern\n\n**Refactoring to Composed Methods**:\n- Replace a block of code with clear, named methods\n- Each method performs one task and is named accordingly\n- Helps make code more readable and understandable\n- Example:\n  - **Original Code**: `Position equivalentPosition(Trade[] trades)`\n    - Large, unnamed block of code\n  - **Refactored Code**:\n    - Extracted methods: `hasNo()`, `positionZero()`, `quantityOfFirst()`\n    - Each method performs a specific task and is named accordingly\n\n**Benefits of Composed Methods**:\n- Helps make code clearer by organizing into different levels of abstraction\n- Allows for better understanding of the overall processing without needing to read individual lines of low-level code.",949,28.29026198387146
False,3,"110-Fluent Style.pdf,110-Fluent Style,,0","110-Fluent Style.pdf,110-Fluent Style,,0",40,**Fluent Speaking Style:**\n- Maintain a smooth and consistent flow of speech\n- Speak at an understandable pace\n- Use natural gestures for emphasis\n- Make eye contact with the audience\n- Use facial expressions to convey emotions.,228,7.820881128311157
False,3,"111-Fluent Tests.pdf,111-Fluent Tests,""Chapter 8 Refactorable Documentation 232\nwhat it does without having to deal with all the code in the lower le","111-Fluent Tests.pdf,111-Fluent Tests,""Chapter 8 Refactorable Documentation 232\nwhat it does without having to deal with all the code in the lower levels of  abstraction. \nThis allows you to more efficiently read and navigate unknown code.\nThe code above also illustrates how the layout of text is meaningful: Y ou can \ngraphically see the two levels of abstraction one on top of the other, just through the ordering of the methods.\nFluent Style\nOne of the most obvious way to make code more readable is to make it mimic natu-ral language, using a style that is called a fluent interface. Let’s consider an example taken from a software application to calculate mobile phone billing:\n1  Pricing.of(PHONE_CALLS).is(inEuro().withFee(12.50).\natRate(0.35));\nY ou can read this pretty easily in English: “The pricing of phone calls is in euros, \nwith a fee of 12.50, at a rate of 0.35.”\nThe code can grow bigger while remaining readable as a quasi-English sentence:\n1  Pricing.of(PHONE_CALLS)2    .is(inEuro().withFee(12.50).atRate(0.35))3    .and(TEXT_MESSAGE)4      .are(inEuro().atRate(0.10).withIncluded(30));\nUsing an Internal DSL\nUsing an internal domain-specific language (DSL) usually relies heavily on method \nchaining, among other tricks. A fluent interface is an example of an internal DSL that is built on the programming language itself. The advantage is that you get the power of expression without giving up all the good things around your programming language: compiler checking, autocompletion, automated refactoring features, and  so on.\nCreating a nice fluent interface takes some time and effort, so I do not recommend \nmaking it the default programming style in all situations. It’s especially interesting for your published interface, the API you expose to all your users, anything about configuration, and for testing so that the tests become living documentation that is readable by anyone.\nFluent Style 233\nA famous example of a fluent interface in .Net is the LINQ syntax. It’s imple-\nmented through extension methods, and it manages to mimic SQL quite closely , as \nshown in this example:\n1  List<string> countries = new List<string> \n2   {""""USA"""", """"CANADA"""", """"FRANCE"""", """"ENGLAND"""",""""CHINA"""", """"RUSSIA""""};34  // Find every country containing the letter 'C', 5  // ordered by length6  IEnumerable<string> filteredOrdered = countries7                              .Where (c => c.Contains(""""C""""))8                              .OrderBy(c => c.Length);910 \nHere’s another example of a fluent interface for data validation, from \nFluentValidation:7\n1 using FluentValidation;23 public class CustomerValidator: AbstractValidator<Customer> {4   public CustomerValidator() {5     RuleFor(customer => customer.Surname).NotEmpty();6     RuleFor(customer => customer.Forename).NotEmpty()7                 .WithMessage(""""Please specify a first name"""");8     RuleFor(customer => customer.Discount).NotEqual(0)9                 .When(customer => customer.HasDiscount);10    RuleFor(customer => customer.Address).Length(20, 250);11    ...12  }\nImplementing a Fluent Interface\nAs in the first step of writing tests in TDD, when implementing a fluent interface you \nstart by dreaming. Write examples of using the ideal fluent interface by imagining that it’s there and perfect, even though you haven’t started to build it yet. Then take a subset of it and start to make it work. Y ou’ll encounter difficulties that will force you \n7. https://github.com/JeremySkinner/FluentValidation"",3465",3526,"**Refactoring Documentation with Fluent Interfaces**\n\n**Two Levels of Abstraction**:\n- Code is more readable when it's organized by levels of abstraction\n- The layout illustrates the two levels visually\n\n**Fluent Style**:\n- Makes code more readable by mimicking natural language\n- Example: Pricing.of(PHONE_CALLS).is(...), which reads like a sentence\n- Code can grow bigger while remaining readable\n\n**Using an Internal DSL**:\n- Fluent interface is an example of internal DSL built on the programming language\n- Provides power of expression without losing compiler checking and other features\n- Ideal for public interface, configuration, testing, and documentation\n\n**Fluent Interface in .Net**:\n- LINQ syntax is a fluent interface implemented through extension methods\n- Mimics SQL queries closely\n\n**Implementing a Fluent Interface**:\n- Start by ""dreaming"" or imagining the ideal interface\n- Make it work by addressing difficulties that arise",944,42.89750695228577
False,3,"112-Case Study An Example of Refactoring Code Guided by Comments.pdf,112-Case Study An Example of Refactoring Code Guided by Comments,""Chapter 8 Refac","112-Case Study An Example of Refactoring Code Guided by Comments.pdf,112-Case Study An Example of Refactoring Code Guided by Comments,""Chapter 8 Refactorable Documentation 234\nto reconsider alternative ways to express the same behavior. Martin Fowler has more \nto say about fluent interfaces.8 \nFluent Tests\nThe fluent style is particularly popular for testing. JMock, AssertJ, JGiven, and NFluent are well-known libraries that can help you write tests in a fluent style. When tests are easy to read, they become the documentation of the behaviors of the software.\nNFluent\n9 is a test assertion library in C# created by Thomas Pierrain. Using \nNFluent, you can write test assertions in a fluent way , like this:\n1 int? one = 1;\n2 Check.That(one).HasAValue().Which.IsPositive()3            .And.IsEqualTo(1);\nThrough method chaining and many other tricks—in particular around the C# \ngenerics—the library allows for a very readable style of tests, as shown here:\n1  var heroes = """"Batman and Robin"""";2  Check.That(heroes).Not.Contains(""""Joker"""")        .And.StartsWith(""""Bat"""")        .And.Contains(""""Robin"""");\nAn equivalent library in Java is AssertJ.10\nCreating a DSTL\nY ou can create your own domain-specific test language (DSTL) to enable writing \npretty scenarios in plain code. This involves test data builders.\nWhen using builders, it is not very difficult to create an internal DSL for creat-\ning test data. Nat Pryce calls this a test data builder. Y ou could extend the previous example with the use of a test data builder to create objects on the given section.\nTest data builders can be nested. For example, you can define bundled travel as \ntravel that groups flights, accommodations, and additional services into one basket \n 8. Martin Fowler, ThoughtWorks, http://martinfowler.com/bliki/FluentInterface.html\n 9.  http://www .n-fluent.net\n10.  AssertJ, http://joel-costigliola.github.io/assertj/\nFluent Style 235\nso that it’s more convenient to buy . Y ou can use a test data builder to create each \nelement independently:\n1 aFlight().from(""""CDG"""").to(""""JFK"""")\n2      .withReturn().inClass(COACH).build();34 anHotelRoom(""""Radisson Blue"""")5      .from(""""12/11/2014"""").forNights(3)6      .withBreakfast(CONTINENTAL).build();\nY ou could use another test data builder to create the bundle from each product:\n1  aBundledTravel(""""Blue Week-End in NY"""")2    .with(aFlight().from(""""CDG"""").to(""""JFK"""")3    .withReturn().inClass(COACH).build())4  .with(5     anHotelRoom(""""Radisson Blue"""")6     .from(""""12/11/2014"""").forNights(3)7     .withBreakfast(CONTINENTAL).build()).build();\nTest data builders can be so useful that you may decide to use them not just for \ntests. For example, I ended up moving them into the production code and made sure \nthey were no longer “test” data builders but just regular companion builders with nothing test-specific in them.\nSee Martin Fowler’s book Domain-Specific Languages for more on DSL.\nWhen Not to Use a Fluent Style\nFluent is not an end to itself, and coding with a fluent style is not always the right thing to do:\n •It makes it more complicated to create the API, and it’s not always worth spending the extra effort.\n •A fluent API is sometimes harder to use when writing code because of non-idiomatic use of the programming language. In particular, it can be confusing to know when to use method chaining or nested functions or object scoping.\n •The methods used as part of a fluent style have names that are not meaningful on their own, like \nNot() , And() , That() , With() , or Is() .\nChapter 8 Refactorable Documentation 236\nCase Study: An Example of Refactoring Code, Guided \nby Comments\nThis case study starts with a random class taken from a legacy C# application in the \ndomain of finance:\n1  public class Position : IEquatable<Position>\n2  {3      //could be just DealId4      private IEnumerable<Position> origin;56      // Position properties to be defined ...7      private double Quantity { get; set; }8      private double Price { get; set; }910     // MAGMA properties to dispatch a job11     public int Id { get; set; }12     public string ContractType { get; set; }13     public string CreationDate { get; set; }14     public string ModificationVersionDate { get; set; }15     public bool FlagDeleted { get; set; }16     public string IndexPayOffTypeCode { get; set; }17     public string IndexPayOffTypeLabel { get; set; }18     public string ScopeKey { get; set; }19     // end MAGMA properties to dispatch a job2021    #region constructors22 ...\nNotice that most comments delimit sections. For example, the last comment \nsays, in plain English, “from here to there, this is a subsection that is used only by \nthe application MAGMA.” Unfortunately , plain English is code for people, and it requires developers like you to deal with it time and time again.\nY ou can do better than these free-text comments to describe sections: Y ou can \nturn them into formal sections represented by distinct classes. This way , you turn the fuzzy knowledge in plain English into strict knowledge expressed in the program-ming language instead. Here’s how you can do this for the last section:\n1  public class MagmaProperties\n2  {3      public int Id { get; set; }4      public string ContractType { get; set; }\nCase Study: An Example of Refactoring Code, Guided by Comments 237\n5      public string CreationDate { get; set; }\n6      public string ModificationVersionDate { get; set; }7      public bool FlagDeleted { get; set; }8      public string IndexPayOffTypeCode { get; set; }9      public string IndexPayOffTypeLabel { get; set; }10     public string ScopeKey { get; set; }11 }\nY ou could apply this approach once or twice again here, on the subsets of the \nfields. For example, CreationDate  and ModificationVersionDate  probably go \ntogether as a versioning subsection that could become a generic shared class:\n1  public class AuditTrail2  {3      public string CreationDate { get; set; }4      public string ModificationVersionDate { get; set; }5  }\nDoing this opens opportunities to think more deeply about what you’re doing. \nFor example, when you use the name AuditTrail , it becomes obvious that this \nshould be immutable to prevent mutation of the history .\nIndexPayoffTypeCode  and IndexPayoffTypeLabel  also probably go \ntogether, as suggested by their similar naming:\n1  IndexPayoffTypeCode2  IndexPayoffTypeLabel\nThe prefix of the name acts like a module name or namespace. Again, this would \nbe well expressed as an actual class:\n1  public class IndexPayoffType2  {3      public string Code { get; set; }4      public string Label { get; set; }5  }\nY ou could go on and on, improving the code and its design purely guided by \ncomments and naming. When you do this, use the formal syntax of your languages \ninstead of fragile and ambiguous text comments.\nComments, sloppy naming, and other shameful signals suggest opportunities for \nimproving code. If you see any of this and don’t know the alternative techniques, you need some external help with clean code, object-orientated design, or functional programming style."",7004",7171,"**Fluent Interfaces and Refactoring Code Guided by Comments**\n\n**Refactorable Documentation (234-236)**\n\n**Fluent Interfaces**:\n- Popular for testing\n- Libraries like JMock, AssertJ, JGiven, and NFluent enable writing tests in a fluent style\n- Fluent style improves readability of tests, which serves as documentation of software behaviors\n- NFluent is a test assertion library in C# that allows writing tests in a fluent way\n- Example: Testing an int variable and hotel room reservation using the fluent style with NFluent\n\n**Creating Domain-Specific Test Languages (DSTL)**:\n- You can create your own DSTL to write pretty scenarios in plain code\n- This involves test data builders\n- Test data builders can be nested to create objects, such as flights and hotels, for testing\n\n**Fluent Style vs. Not Using It**:\n- Fluent is not an end to itself, and coding with a fluent style may not always be the right thing to do\n- Potential drawbacks:\n  - More complicated to create API\n  - Harder to use when writing code due to non-idiomatic use of language\n  - Non-meaningful method names used in fluent style (e.g., Not(), And(), That(), With(), Is())\n\n**Case Study: Refactoring Code Guided by Comments**\n- Starting with a legacy C# code snippet in the finance domain: Position class with comments\n- Most comments delimit sections, but they are written in plain English and not easily understood as code\n- Opportunity to turn these free-text comments into formal classes that express strict knowledge\n- Example: Refactoring the ""MagmaProperties"" section into separate classes (e.g., AuditTrail, IndexPayoffType)\n- This opens opportunities for improving the design and immutability of the code",1685,60.71795725822449
False,3,"113-Integrated Documentation.pdf,113-Integrated Documentation,,0","113-Integrated Documentation.pdf,113-Integrated Documentation,,0",64,**Important Note:** Integrated Documentation is crucial for comprehensive understanding. (No words removed),107,4.118680715560913
False,3,"114-Type Hierarchy.pdf,114-Type Hierarchy,,0","114-Type Hierarchy.pdf,114-Type Hierarchy,,0",44,"**The Hierarchy of Emotional Types**:\n* This concept is not explicitly stated in the passage\n* But it can be inferred from the given information\n* The passage describes how the brain processes emotions and distinguishes between them based on their intensity and facial expression\n* Therefore, we can assume that there is a hierarchy at play, with different emotional types being more prominent or basic than others.\n\n**The Role of Intensity and Facial Expression**:\n* The higher up in the hierarchy an emotion is, the more intense its physiological expression will be\n* This means that facial expressions are a good indicator of the dominant emotion being felt at a given moment\n* Lower-level emotions (like happiness or sadness) have weaker physiological expressions, while higher-level emotions (like fear or anger) have stronger ones.\n\n**The Hierarchy in Action**:\n* When two emotional signals conflict, the higher one in the hierarchy wins out\n* For example, if someone is experiencing both fear and happiness at once (e.g., seeing a potential romantic partner at an airport terminal), the fear response will be muted because happiness is a higher-level emotion\n* This helps explain why some emotions are more powerful or influential than others.",1250,29.797972440719604
False,3,"115-Semantics Derived from Actual Usage.pdf,115-Semantics Derived from Actual Usage,""Chapter 8 Refactorable Documentation 238\nIntegrated Documentatio","115-Semantics Derived from Actual Usage.pdf,115-Semantics Derived from Actual Usage,""Chapter 8 Refactorable Documentation 238\nIntegrated Documentation\nY our integrated development environment (IDE) already fulfills many documenta-\ntion needs. This documentation is even more integrated into your coding, thanks to \nthe autocompletion. This is sometimes called “intellisense” for its ability to guess what you need from the context. As you write code, the IDE shows what’s available.\nIf you write the name of a class and press the period key , instantly the IDE shows \na list of every method for the class. In fact, though, it’s not every method but is fil-tered to show only what you can really access in the context of your code under the cursor. It doesn’t show the private method if you’re not within the class, for example.\nThis is a form of documentation that is task oriented and highly curated for your \ncontext.\nTherefore: Acknowledge that your IDE is a key tool for documentation pur-\nposes. Learn how to use it well. Admit that the documentation use cases that your IDE handles don’t have to be addressed anywhere else.\nType Hierarchy\nA class hierarchy diagram is a classic element of a reference documentation. Because these diagrams usually use the UML notation, they take a lot of screen real estate. In contrast, your IDE can display a custom type hierarchy diagram on-the-fly from any selected class. The diagram is interactive: Y ou select whether to display the hierarchy above or below the selected type, and you can expand or fold branches of the hierar-chy . And because it’s not using UML, it’s quite compact, so you can see a lot in a frac-tion of the screen.\nIf, for example, you’re looking for a concurrent list with a fixed length, but you \ncan’t remember its name, you can select the standard \nList  supertype and ask the \nIDE for its type hierarchy . The IDE displays every type that is a list. Now you can examine each type by name, have a look at the Javadoc for each by mousing over, and select the one you want. Look, Ma, no documentation!\nIndeed, this is documentation. It’s just different. Again, this is a form of docu-\nmentation that is task oriented and interactively curated for your context.\nCode Searching\nIt would be unfair to talk about the IDE without mentioning the IDE’s searching capabilities."",2273",2364,"**Semantics Derived from Actual Usage: Documentation in Integrated Development Environments (IDEs)**\n\n**Autocompletion and Intellisense:**\n- IDEs provide task-oriented, curated documentation through autocompletion or ""intellisense""\n- Shows available methods and properties based on context\n- Filters results to only show accessible items\n- Integrated into coding process\n- Acknowledge IDE as key tool for documentation purposes\n\n**Type Hierarchy:**\n- Class hierarchy diagrams are a classic element of reference documentation\n- IDE can display custom type hierarchy diagrams on-the-fly from selected classes\n- Diagram is interactive, allowing users to expand or fold branches and view superclasses/subclasses\n- Compact compared to UML notation\n- Helps with discovering specific types in hierarchies during development\n\n**Code Searching:**\n- IDEs offer powerful code searching capabilities\n- Allows users to quickly find relevant classes, methods, or properties\n- Useful for task-oriented documentation discovery\n- Interactive and curated based on user context.",1059,44.33744764328003
False,3,"116-Example Plain-Text Diagrams.pdf,116-Example Plain-Text Diagrams,""Using Plain-Text Diagrams 239\nWhen you’re looking for a class but don’t remember","116-Example Plain-Text Diagrams.pdf,116-Example Plain-Text Diagrams,""Using Plain-Text Diagrams 239\nWhen you’re looking for a class but don’t remember its name, you can just type \nstems that belong to a class name, and the interval search engine will display a list of \nevery type that contains each stem. The same works with just initials of a stem. For example, you can type \nbis as a shortcut for BufferedInputStream .\nSemantics Derived from Actual Usage\nA colleague at Arolla, Mathieu Pauly , once told me about the idea that meaning comes from the associations between things. Therefore, one way to learn what a class means is by looking at its relationships with all other classes that you already know .\nSuperficially this is something that you probably do already . Imagine that you \nneed to find every transactional service within a code base. If the services use anno-tations like \n@Transactional , then it’s easy: Select the annotation anywhere and \nask the IDE to find all usages.\nAlternatively , suppose that transactions are done through the standard Java class \nTransaction  and its method Commit() . Y ou can ask the IDE to retrieve the call \nstack for this method. Every class that is directly or indirectly calling this transaction stuff should be a transactional service. So the IDE is an assessment tool. It’s far from perfect, though. Y ou have to translate your goal into what the IDE offers. Still, all the capabilities the IDE offers replace advantageously a lot of documentation that would otherwise be necessary . The IDE is a great integrated documentation tool.\nY ou can extend your code in an augmented code fashion by using your IDE as \na user interface. Guided tours and inspiring exemplars (see Chapter 5, “Living  Curation: Identifying Authoritative Knowledge”) illustrate this approach.\nUsing Plain-Text Diagrams\nMost diagrams are short-lived. A diagram may be useful for a particular discussion or to help reason on a specific design decision, but once the idea has been communi-cated or once the decision has been made, the diagram immediately loses most of its interest. This is why napkin sketches are the best choice for handmade diagrams. I use the word napkin sketches to actually refer to any low-tech visual and tangible technique. Whiteboarding, CRC cards, and event storming are of similar interest. \nThey’re all great tools for communicating, reasoning, and trying things in a visual fashion.\nSome diagrams do remain of interest for a longer term, and in such a case, you \nwant to persist the initial napkin sketch, set of cards or stickers, or whiteboard into something better suited for posterity . One way is to simply take a photograph of the \nChapter 8 Refactorable Documentation 240\noutcome and to store it in the wiki or directly in the source control, co-located with \nthe related artifacts. This works fine if the picture describes stable knowledge, but if it describes decisions that evolve regularly , the picture will be misleading after a while. Y ou could try to do a Living Diagram, but this might be too hard or too much work to do, given the expected benefits. This is when you need a plain-text diagram.\nTherefore: Take your initial napkin sketch or set of CRC cards and turn it into \nplain-text form. Use a text-to-diagram tool to render it into a visual diagram automatically. Then, on every change, maintain the plain-text description of the diagram and keep it in the source control area in the related code base.\nKeep in mind that a plain-text diagram favors content over the formatting. Y ou \nwant to focus on the content in plain text and let the tools take care of the format-ting, layout, and rendering as much as possible.\nExample: Plain-Text Diagrams\nLet’s consider the example of a fuel card fraud detection algorithm. Say that you start with a napkin sketch (see Figure 8.6) when thinking about the problem, listing every related responsibility needed and how the responsibilities interoperate to solve the problem.\n \nFigure 8.6 The initial napkin sketch for a fraud detection mechanism\nUsing Plain-Text Diagrams 241\nAfter a few days, your team might agree that you need to keep the napkin sketch \nas part of your documentation, and you need to make it easier to read and to main-\ntain as you expect it to change from time to time.\nThis diagram should tell one story . It should hide everything that does not matter \nfor the story . T o be story-oriented, you can use links as sentences:\n1  <actor A> """"does something to"""" <Actor B>\nSo, basically , you look at the napkin sketch and describe it by using sentences in \nthis format:\n1  FueldCardTransaction received into FuelCardMonitoring\n2  FuelCardMonitoring queries Geocoding3  FuelCardMonitoring queries GPSTracking4  Geocoding converts address into AddressCoordinates5  GPSTracking provides VehicleCoordinates6  VehicleCoordinates between GeoDistance7  AddressCoordinates between GeoDistance8  GeoDistance compared to DistanceThreshold9  DistanceThreshold delivers AnomalyReport\nThen you can turn this set of sentences into a nice diagram by using a rendering \ntool. \n Note\nFor the drawing shown in Figure 8.7 and 8.8, I used an online tool called Dia-\ngrammr that is not available anymore; however, Zoltán Nagy contributed a similar tool named diagrammr-lite (while attending my Living Documen-tation workshop at CraftConf Budapest): https://gist.github.com/abesto/a58a5e7155f38f4ac29d6c02f720a312.\n \nThe default layout of the rendered diagram is an activity-like diagram like the one \nshown in Figure 8.7. \nBut the same text sentences can also be rendered as a sequence diagram instead, \nas shown in Figure 8.8. \nA tool like that is in fact only a thin wrapper on top of an automatic layout tool \nlike Graphviz. Each sentence describes a relationship between two nodes. The first word of the sentence represents the start node, and the last word of the sentence rep-resents the target node. This is a rustic approach.\nChapter 8 Refactorable Documentation 242\nIt’s not difficult to create your own flavor of this approach, using different conven-\ntions to interpret the text sentences. However, the point is to keep it really rustic. If \nyou don’t keep the syntax simple, you may end up with syntax so complicated that you have to look at its syntax sheet all the time.\nFigure 8.7 A diagram rendered from the text\nFigure 8.8 Another layout to tell the same story in a different way"",6401",6478,"**Using Plain-Text Diagrams**\n\n**Napkin Sketches**:\n- Useful for short-lived diagrams during discussion or design decision making\n- Can be photographed and stored for longer term reference\n\n**Converting Napkin Sketches to Plain-Text Diagrams**:\n- Use a text-to-diagram tool to render the initial sketch into a visual diagram\n- Maintain the plain-text description of the diagram in source control\n- Focus on content over formatting\n\n**Example: Fuel Card Fraud Detection Algorithm**:\n1. Initial napkin sketch outlines responsibilities and interactions between components\n2. Convert sentences describing the sketch into a text format\n3. Use a rendering tool to create the visual diagram from the text\n4. Compare the activity-like diagram (Figure 8.7) and sequence diagram (Figure 8.8) renditions\n5. Keep the approach ""rustic"" to maintain simplicity",845,46.16088390350342
False,3,"117-Chapter 9 Stable Documentation.pdf,117-Chapter 9 Stable Documentation,""Using Plain-Text Diagrams 243\nWhen there are changes that make updates to","117-Chapter 9 Stable Documentation.pdf,117-Chapter 9 Stable Documentation,""Using Plain-Text Diagrams 243\nWhen there are changes that make updates to the diagram necessary , it’s easy to \nmake them in the text. Renaming can be done through Find and Replace. Depending \non your preferences, your IDE can probably have its refactoring automation reach the plain-text files, in which case you’re less at risk of forgetting to update the diagram.\nDiagrams as Code\nAn alternative flavor of a plain-text diagram is to use code in a programming lan-guage as the way to declare the nodes and their relationships. There are benefits to this method:\n •Y ou get the benefit of autocompletion.\n •Checks from the compiler or interpreter can catch invalid syntax.\n •Y ou can move along with any automated refactoring to remain in sync with all changes.\n •Y ou can programmatically generate many dynamic diagrams from data sources.\nThere are some drawbacks, too:\n •The code itself is less readable by nondevelopers than plain text would be.\n •The names of identifiers cannot contain whitespace.\n •It’s not really a living diagram, but just a diagram created from ad hoc code.\nHere is an example of a diagram generated from my little library DotDiagram,11 \nwhich is a wrapper on top of Graphviz:\n1 final DotGraph graph = new DotGraph(""""MyDiagram"""");\n2 final Digraph = graph.getDigraph();34 // Add the nodes5 digraph.addNode(""""Car"""").setLabel(""""My Car"""")         .setComment(""""This is a BMW"""");6 digraph.addNode(""""Wheel"""").setLabel(""""Its wheels"""")         .setComment(""""The wheels of my car"""");7 89 // Add the associations between the nodes\n11.  DotDiagram, https://github.com/LivingDocumentation/dot-diagram\nChapter 8 Refactorable Documentation 244\n10 digraph.addAssociation(""""Car"""", """"Wheel"""").setLabel(""""4*"""")\n         .setComment(""""There are 4 wheels"""")11       .setOptions(ASSOCIATION_EDGE_STYLE);1213 // Render everything14 final String actual = graph.render().trim();\nThe diagram created from this code should render as shown in Figure 8.9. \nOf course, the biggest benefit of diagrams as code is the ability to generate \ndiagrams from any source of data.\nSummary\nConsidering that many changes in a software system are done using automated \nrefactoring, it makes a lot of sense to leverage refactorings for updating the docu-mentation as well. In practice, this suggests a bias toward code-level techniques, from plain text to actual code, and it helps to learn many coding techniques to make the code more expressive, such as the use of types and careful naming.\nFigure 8.9 Rendering MyCar —> 4* Wheel"",2521",2624,"**Plain-Text Diagrams**\n- Updating diagrams is easy when using plain text\n- Renaming can be done through **Find and Replace**\n- IDEs may have refactoring automation to update plain-text files\n\n**Diagrams as Code**\n- Use code in a programming language to declare nodes and relationships\n- Benefits:\n  - **Autocompletion**\n  - Compiler/interpreter checks for invalid syntax\n  - Synchronization with automated refactoring\n  - Generation of dynamic diagrams from data sources\n- Drawbacks:\n  - Less readable by non-developers than plain text\n  - Identifier names cannot contain whitespace\n  - Not a ""living"" diagram, but generated from code\n\n**Example:** Using the **DotDiagram** library to generate a diagram from code\n1. Create a new **DotGraph** object with a name\n2. Add nodes and set labels and comments\n3. Add associations between nodes with labels and comments\n4. Render the graph as a string\n5. Example output: ""MyCar ---> 4* Wheel""\n\n**Data-Driven Diagram Generation**\n- Biggest benefit of diagrams as code is generating diagrams from data sources",1050,32.19435954093933
False,3,"118-Requirements Are More Stable Than Design Decisions.pdf,118-Requirements Are More Stable Than Design Decisions,""245Chapter 9\nStable Documentation\","118-Requirements Are More Stable Than Design Decisions.pdf,118-Requirements Are More Stable Than Design Decisions,""245Chapter 9\nStable Documentation\nStable knowledge is easy to document because it doesn’t change often. A great  benefit \nof stable knowledge is that you can use any form of documentation for it. Because there will be no need for updating the documents, even traditional forms that I would otherwise avoid, like Microsoft Word documents or wikis, are absolutely fine in this case. However, it does take some care to do it well; you need to properly design each detail to make sure everything is really  stable.\nEvergreen Content\nEvergreen content is a kind of content that remains of interest for a long period of time, without change, for a particular audience. Evergreen content does not change, and yet it remains useful, relevant, and accurate. Obviously , not every kind of docu-ment contains evergreen content.\nEvergreen content documents have the following characteristics:\n •They tend to be short, without much detail.\n •They focus on high-level knowledge—“the big picture.”\n •They focus on goals and intentions rather than implementation decisions.\n •They focus more on business concepts than on technical ones.\nThese characteristics are key for the stability of a document."",1194",1315,"**Chapter 9: Stable Documentation**\n\n**Requirements vs Design Decisions**:\n- Requirements are more stable than design decisions\n- Stable knowledge is easy to document\n- No need for updates as it doesn't change often\n- Can use traditional forms of documentation (e.g., Microsoft Word, wikis)\n- Careful design necessary to ensure stability\n\n**Evergreen Content**:\n- Remains interesting, relevant, and accurate over time for a specific audience\n- Does not change but remains useful\n- Characteristics:\n  - Short with limited detail\n  - Focuses on high-level knowledge, ""the big picture""\n  - Addresses goals and intentions rather than implementation decisions\n  - Focuses more on business concepts than technical ones",712,17.717427015304565
False,3,"119-A Lot of Knowledge Is Less Stable Than It Looks.pdf,119-A Lot of Knowledge Is Less Stable Than It Looks,""Chapter 9 Stable Documentation 246\nThere","119-A Lot of Knowledge Is Less Stable Than It Looks.pdf,119-A Lot of Knowledge Is Less Stable Than It Looks,""Chapter 9 Stable Documentation 246\nTherefore: Traditional means of documentation are appropriate for knowledge \nthat seldom changes. When this is the case and that the knowledge is useful, don’t bother with living documentation techniques. Just write the knowledge in any kind of document, even in a proprietary format or a PDF document or using a content management system, slides, or a spreadsheet. But make sure to leave out of that documentation anything that is at risk of changing.\nYou don’t have to spend a lot of time on creating evergreen content, but if you \ndo, it will benefit the readers for a long time.\nNote that just because knowledge is stable doesn’t mean it’s useful and worth \ndocumenting.\nRequirements Are More Stable Than Design Decisions\nIf you can’t change a decision, it’s a requirement to you. If you can, it’s your design.\n—Alistair Cockburn, https://twitter .com/T otherAlistair/status/606892091432701952\nIf you can’t change a decision, then from your perspective, that decision is already more stable than your design decisions. Hence, requirements tend to be more stable than design decisions. And in particular, high-level requirements may be stable enough to be expressed in evergreen documents.\nOf course, it happens that some requirements change frequently , but then \nit’s  usually the details of the expected behaviors that change. For these low-level  requirements that may change frequently , practices like BDD are more  appropriate to deal with the changes efficiently; conversations are efficient for fast-changing knowledge, together with some automation when it fits.\nHigh-Level Goals Tend to Be Stable\nA company may have a vision to change the world. Such a high-level vision is stable and is part of the company identity . A startup at an early stage may pivot regularly , but its vision often remains the same.\nIn large corporations, change happens all the time everywhere, but the traditional \napproach to management is to consider most decisions and knowledge as certain, predictable, and stable. Within a department, team, or project, everything around can often be considered stable.\nA project vision, expressed in a few sentences, like an elevator pitch, can be \nquite stable. And if it ever changes, the project will probably be stopped or totally reconsidered."",2331",2446,"**Documenting Stable Knowledge**\n- **Traditional documentation methods**: appropriate for stable, seldom-changing knowledge\n- No need to invest time in creating evergreen content if it remains unchanged\n- Examples of stable knowledge: requirements and high-level goals\n  - Requirements tend to be more stable than design decisions\n    * If unable to change a decision, consider it a requirement\n    * Designs are subject to change\n  - High-level goals remain the company identity and seldom change\n- **Efficient practices for frequently changing low-level requirements**: conversations and BDD (Behavior Driven Development)\n- **Vision or elevator pitch**: a stable project representation expressed in a few sentences.",717,18.092562913894653
False,3,"120-Case Study A README File.pdf,120-Case Study A README File,""Evergreen Content 247\nWhy does your project exist? Who’s sponsoring it? What are the b","120-Case Study A README File.pdf,120-Case Study A README File,""Evergreen Content 247\nWhy does your project exist? Who’s sponsoring it? What are the business drivers? \nWhat are the expected benefits, and what are the success criteria?\nY ou need to take extra care to keep the vision high level enough to avoid \n prematurely constraining the execution of the project.\nFor example, the project vision “Create a library to report sales to the  regulator” \nalready presumes the solution. When the vision is stated this way , the team has \nalready lost better opportunities, such as extending two existing services so that they together can deliver the report. This example of a vision is also fragile to changes. If a new CTO decides that everything now must be services—no library allowed—the team will have to update the vision of the project. A better vision for the project would simply be “Report the sales to the regulator” or, even better, “Extend report-ing to meet MIFID II regulation.” With visions like these, it does not matter what you do to achieve the goal; all options remain open. \nA Lot of Knowledge Is Less Stable Than It Looks\nThere’s a limit with evergreen documents: Even if the knowledge itself does not change much, evergreen documents involve a graphical style, with a company logo and company-specific footers, and these elements of style sometimes change.\nAnother limit is that it’s common for all documents, including evergreen docu-\nments, to reside on the same source control system as the source code. This encour-ages lightweight formats of documents, like text and HTML rather than Microsoft Office documents or other binary proprietary formats. Keeping knowledge in plain text is also the preferred way to go for stable knowledge.\nCase Study: A README File\nAs an example, let’s consider the following README file from a fleet management system:\n1  # Project Phenix\n2  (Fuel Card Integration)34  Project Manager: Andrea Willeave56  ## Syncs daily7  Transaction data from the pump is automatically sent to8  Fleetio. 9  No more manual entry of fuel receipts or downloading and\nChapter 9 Stable Documentation 248\n10 importing fuel transactions across systems.\n1112 ## Fuel Card Transaction Monitoring 1314 Transaction data from the pump are verified automatically 15 against various rules to detect potential frauds: gas 16 leakage, transactions too far from the vehicle etc.171819 *The class responsible for that is called 20 FuelCardMonitoring.*21 22 Anomalies are detected if the vehicle is further than 300m23 away from the gas station, of if the transaction quantity 24 exceeds the vehicle tank size by more than 5%25 26 When drivers enter mileage at the pump, Fleetio uses that27 information to trigger service reminders. This time-saving 28 approach helps you stay on top of maintenance and keeps 29 your vehicles performing their best.30 31 *This module is to be launched in February 2015. Please32 contact us for more details.*333435 ## Smart fuel management36 ...\nThere are many issues in this file that will require the file to be updated regularly:\n •The project name Phenix will change many times for political or marketing \nreasons.\n •The name of the project manager will also likely change, likely every two years.\n •The class name will be renamed, split, or merged with another at some point if the team is doing refactoring, which is expected to be the case. This document will need to be updated each time.\n •Close to the class name, there are concrete parameters that may change any time (for example, \n300m  may become 500m , and 5% may become 3%).\n •The launch date is likely to change as it’s already in the past. How do you fix that?\nEvergreen Content 249\nY ou can start by changing the title to be a stable name, by reference to the core \nbusiness of the module. It may not be stable forever either, but at least it is more sta-\nble than a name that is driven by internal company politics. T o do this, you change the following:\n1  # Project Phenix\n2  (Fuel Card Integration)34  Project Manager: Andrea Willeave\nto the following title, along with a short introduction line:\n1  # Fuel Card Integration23  Here are the main features of this module:\nY ou can also get rid of the project manager name in this file, as it is not the right \nplace for that information. Instead, it could be in a Team section of the wiki, or in \nthe Team section of your project manifest (a Maven POM file, for example). Y ou could also replace the project manager name with a link to the page that contains this information.\nY ou should also remove the launch date from this file. Instead of including it here, \nyou could link to the corporate calendar, news portal, dedicated forum or internal social network, or to the T witter or Facebook page where the launch will be announced.\nThe class name has nothing to do here. If you really want to bridge from this file \nto the code, you might instead link to a search on the source control, something like “link to the classes tagged as \n@EntryPoint .”\nFinally , the detailed parameters values are not necessary here. If you really need \nthem, you can either look at the code or configuration, or you can check the scenar-ios that describe the expected behavior and that are used by Cucumber or SpecFlow .\nT o sum it up, here’s what the code now looks like:\n1  # Project Phenix\n2  # Fuel Card Integration \n34  Project Manager: Andrea Willeave \n56  Find who's in the team here // link to the wiki78  910 Here are the main features of this module:\n11\nChapter 9 Stable Documentation 250\n12 ## Syncs daily\n13 Transaction data from the pump is automatically sent toFleetio. 14 No more manual entry of fuel receipts or downloading andimporting fuel transactions across systems.15 16 ## Fuel Card Transaction Monitoring17 18 Transaction data from the pump are verified automaticallyagainst various rules to detect potential frauds:19 gas leakage, transactions too far from the vehicle etc.20 212223 *The class responsible for that is called FuelCardMonitoring.*\n2425 The corresponding code is on the company Github // link to the source code repository, but not to a concrete class name26 2728 *Anomalies are detected if the vehicle is further than 300m\naway from the gas station,\n29 or if the transaction quantity exceeds the vehicle tank size \nby more than 5%*\n303132 For more details on the business rules of the fraud detection, please check the business scenarios here // link to the living documentation33343536 ## Odometer readings37 When drivers enter mileage at the pump, Fleetio uses thatinformation to trigger service reminders.38 This time-saving approach helps you stay on top of maintenance and keeps your vehicles performing their best.394042 *This module is to be launched in February 2015. Please\ncontact us for more details.*\n43"",6810",6879,"**Case Study: A README File**\n\n**Project Phenix (Fuel Card Integration)**\n- Project Manager: Andrea Willeave\n- **Features**:\n    - Syncs daily transaction data from pump to Fleetio for automatic entry and importing\n    - Fuel card transaction monitoring to detect potential frauds (gas leakage, transactions too far from vehicle)\n    - Odometer readings used to trigger service reminders for fleet maintenance\n\n**Changes Needed**:\n- Project name will change over time due to marketing or political reasons\n- Project manager's name should be removed and linked to team wiki or project manifest\n- Launch date is outdated, remove it and link to corporate calendar, news portal, or social media for announcements\n- Class name and specific parameters are not necessary in this document\n\n**Additional Information**:\n- Find team members responsible for the module on company Github repository (not concrete class name)\n- Business rules of fraud detection can be found in separate living documentation",993,34.15010952949524
False,3,"121-Tips for Evergreen Documentation.pdf,121-Tips for Evergreen Documentation,,0","121-Tips for Evergreen Documentation.pdf,121-Tips for Evergreen Documentation,,0",80,"**Tips for Evergreen Documentation**\n\n**Focus on clarity and concision**: Write comprehensive bulleted notes summarizing the provided text, with headings and terms in bold.\n\n**Tips for Effective Evergreen Documentation:**\n- **Clarity**: Emphasize key points by using headings and bold text.\n- **Concision**: Keep explanations succinct, focusing on essential information only.\n\n**Evergreen Documentation**: Write comprehensive bulleted notes summarizing provided tips.\n\n**Tips for Evergreen Documentation:**\n- Focus on clarity and concision\n- Use headings and bold text to emphasize key points\n- Keep explanations succinct, covering essential information only.",659,18.213948726654053
False,3,"122-Organizing Artifacts Along Stable Axes.pdf,122-Organizing Artifacts Along Stable Axes,""Tips for Evergreen Documentation 251\n44\n45 For news and a","122-Organizing Artifacts Along Stable Axes.pdf,122-Organizing Artifacts Along Stable Axes,""Tips for Evergreen Documentation 251\n44\n45 For news and announcements on this product, please check ourFacebook page link to the FB page464748 ## Smart fuel management49 ...\nNow you have an evergreen README.\nTips for Evergreen Documentation\nThe following sections present tips on how to keep your documentation current.\nAvoiding Mixing Strategy Documentation with the \nDocumentation of Its Implementation\nStrategy and its implementation don’t evolve at the same pace. In their book Agile \nT esting: A Practical Guide for T esters and Agile T eams, Lisa Crispin and Janet \n Gregory recommend not mixing the documentation of a strategy with the documen-tation of its implementation, using the example of the test strategy:\nIf your organization wants documentation about your overall test approach to projects, \nconsider taking this information and putting it in a static document that doesn’t change much over time. There is a lot of information that is not project specific and can be extracted into a Test Strategy or Test Approach document.\nThis document can then be used as a reference and needs to be updated only if pro-\ncesses change. A test strategy document can be used to give new employees a high-level understanding of how your test processes work.\n1\nI have had success with this approach at several organizations. Processes that \nwere common to all projects were captured into one document. Using this format \nanswered most compliance requirements. These are some of the topics that have been covered:\n •Testing practices\n •Story testing\n1. Crispin, Lisa, and Janet Gregory . Agile T esting: A Practical Guide for T esters and Agile T eams. \n Hoboken: Addison-Wesley , 2009.\nChapter 9 Stable Documentation 252\n •Solution verification testing\n •User acceptance testing\n •Exploratory testing\n •Load and performance testing\n •Test automation\n •Test results\n •Defect tracking process\n •Test tools\n •Test environments\nTherefore: Don’t mix documentation of a strategy and documentation of its \nimplementation. Make the strategy documentation a pure evergreen document. Use another living documentation approach for the implementation, consider-ing that the implementation will change more frequently.\nThe strategy should be documented as an evergreen document, stable and even \nshared between multiple projects. Omit from the strategy document every detail that could change or that would be project specific. All these details that change more frequently and that differ from project to project must be kept separate, using the techniques proposed in this book that are more suited for knowledge that changes often, such as declarative automation and BDD .\nEnsuring Stability\nNames describing business benefits tend to be stable, often over decades. Business is changing, but from a high-level perspective, it’s still about selling, purchasing, pre-venting losses, and reporting. If you open an old book about doing business in your domain, you’ll recognize that although the typical way of doing business has evolved since then, most words in the book are still valid and still mean the same thing.  Business domain vocabulary is on the stable end of the spectrum.\nOn the other end of the spectrum, everything about the organization, legal stuff, \nand marketing is volatile: Company names, subsidiaries, brands, and trademarks change all the time. Avoid using them in more than one place. Prefer stable names instead.\nLook at your company org chart now and compare it with one from two or three \nyears ago. How are they different? New executives often change the org structure. In some companies the top management switches every three years. Departments \nTips for Evergreen Documentation 253\nare split and merged and renamed. A game of perpetual business and politics-driven \nrefactoring may change the org structure without changing the underlying business operations much.\nDo you want to spend time changing words everywhere in your code and in your \ndocuments to accommodate those changes? I certainly don’t want that, so I choose to go for stable names whenever I can, with a preference for business domain names.\nArbitrary Versus Descriptive Names in Code\nI noticed that arbitrary code names, such as \nSuperOne , that don’t describe \nanything are more volatile than common names that describe what they do. Even if you just work with a company for two or three years, you will see some of these names changing. But arbitrary names are attractive because we change them often to match the current fashion. On the other hand, common words that describe the things they are, like \nAccountingValuation , are dull, but \nthey are less likely to be renamed and hence are more stable. More importantly , in the latter case, the name itself is an element of documentation. Without any-thing else, you may know what an \nAccountingValuation  component does. \nUsing Perennial Naming\nNaming is one of the most powerful means available to transfer knowledge. Unfortu-nately , many kinds of names change frequently , such as marketing brands and prod-uct names, project code names, and team names. When this happens, it costs maintenance work: Somebody has to find every place where the old name is used and update each instance.\nSome names last longer than others, and some names change more frequently \nthan others. For example, it’s common for marketing names, legal names, and company organization names to change every one to three years. These names are volatile.\nChoosing names judiciously so that they don’t change often is important for \nreducing the amount of maintenance work in all kinds of artifacts. This is important in the code and also in other documents.\nTherefore: Use stable names over volatile names in all documentation that you \nmaintain. Name classes, interfaces, methods, code comments, and every docu-ment using stable names. Avoid references to volatile names in all documents."",5964",6061,"**Tips for Evergreen Documentation**\n\n**Avoiding Mixing Strategy Documentation with Implementation**\n- Strategy and implementation evolve at different paces\n- Don't mix documentation of strategy with documentation of its implementation\n- Create separate documents: Evergreen (strategy) and living (implementation)\n- Example: Test Strategy document for overall test approach\n  - Topics covered: Testing practices, testing types, test tools, processes\n\n**Ensuring Stability in Documentation**\n- Use stable names over volatile ones in all documentation\n- Business domain vocabulary is stable, prefer it over company specific names\n- Arbitrary code names change frequently and are less descriptive\n- Perennial naming reduces maintenance work by using stable names in code and documents.",782,27.15114402770996
False,3,"123-Volatile-to-Stable Dependencies.pdf,123-Volatile-to-Stable Dependencies,""Chapter 9 Stable Documentation 254\nOrganizing Artifacts Along Stable Axe","123-Volatile-to-Stable Dependencies.pdf,123-Volatile-to-Stable Dependencies,""Chapter 9 Stable Documentation 254\nOrganizing Artifacts Along Stable Axes\nAt the macro level, how do you organize your documentation? There are many \n different ways to organize documents:\n •By application name: For example, CarPremiumPro , BestSocksOnline\n •By business process: For example, sell car in retail, sell socks online\n •By target client: For example, individual car buyers, urban middle-class men, B2B or B2C\n •By team name: For example, team B2B, Team Ninja\n •By team purpose: For example, Software Delivery Paris, R&D London\n •By project name: For example, MarketShareConquest , GoFastWeb\nFor each other these organization modes, how does it evolve over time? If you \nthink back on your past work experiences, which ones remained unchanged, and which ones were changing from time to time or even several times a year?\nProjects start and end. They are canceled and sometimes resuscitated under a new \nname. Applications last longer than projects, but they end up being decommissioned and replaced by other projects that provide similar business benefits.\nLinked Knowledge\nKnowledge is more valuable when it is connected, provided that the connections are stable. Knowledge becomes more valuable when it is connected as a graph of rela-tionships that conveys additional information and also brings structure.\nOn a particular topic or on a project, all knowledge is related to other knowledge \nin some way . On the Internet, links between resources add a lot of value: Who’s the author? Where can you find more? What does this definition mean? Who’s quoted here? In a book or paper, the bibliography tells you the context. Was the author aware of this publication? If it’s cited in the bibliography , then you can guess that was the case. The same concept applies to your documentation.\nTherefore: In your documentation, link knowledge to other related knowledge. \nQualify the relationship. Define a resource identification scheme, such as a URL or a citation scheme. Decide on a mechanism to ensure that the links remain stable in the long run.\nIt’s important to qualify a link with some metadata, such as the source, reference \non the topic, review , criticism, author, and so on."",2215",2298,"**Organizing Artifacts Along Stable Axes:**\n- **At a macro level**, how to organize documentation:\n  - By application name (e.g., CarPremiumPro, BestSocksOnline)\n  - By business process (e.g., sell car in retail, sell socks online)\n  - By target client (e.g., individual car buyers, urban middle-class men, B2B or B2C)\n  - By team name (e.g., team B2B, Team Ninja)\n  - By team purpose (e.g., Software Delivery Paris, R&D London)\n  - By project name (e.g., MarketShareConquest, GoFastWeb)\n- **Changes over time**:\n  - Some organization modes remain unchanged\n  - Others change from time to time or even several times a year\n- **Projects vs. Applications**:\n  - Projects start and end, canceled and resuscitated under a new name\n  - Applications last longer but are decommissioned and replaced by other projects\n\n**Linked Knowledge:**\n- **Knowledge is more valuable when connected**:\n  - Provided that the connections are stable\n- **Connections bring structure**:\n  - Define a resource identification scheme (e.g., URL, citation)\n  - Decide on a mechanism to ensure stability in the long run\n- **Qualify the relationship**:\n  - In documentation, link knowledge to other related knowledge\n  - Provide metadata about the source, reference, review, criticism, author, etc.",1267,35.03850865364075
False,3,"124-Categories of Stable Knowledge.pdf,124-Categories of Stable Knowledge,""Linked Knowledge 255\n Caution\nBe aware of the directions of the links. Ju","124-Categories of Stable Knowledge.pdf,124-Categories of Stable Knowledge,""Linked Knowledge 255\n Caution\nBe aware of the directions of the links. Just as in design, links should go from the \nless stable to the more stable.  \nA great way to link to some piece of knowledge is to make it accessible through a \nURL. Y ou can expose knowledge as web resource accessible through a link. When-ever necessary , you can refer to that knowledge by using the link. Use a link registry to ensure the permanence of the links.\nMany tools expose their knowledge through links: issue trackers, static analysis \ntools, planning tools, blogging platforms, and social code repositories like GitHub. If you want to link to a particular version of something, use permalinks. If, on the other hand, you prefer to link to the most recent version of something, link to the front page, or index, or folder, which will usually show the latest version first.\nVolatile-to-Stable Dependencies\nWhen you refer to something, make sure the direction of the reference is from the more volatile to the more stable elements. It’s much more convenient to couple the volatile to the stable than it is to couple the stable to the volatile. A reference to some-thing stable is not very expensive as there won’t be many impacts from the depend-ency because it does not change often. On the other hand, with a reference to a volatile dependency , you’ll have to make changes all the time, whenever the depend-ency changes. This applies to both code and documentation.\nFor an example in code, most programming language propose to couple the \nimplementation to the contract or interface they implement, and not the other way around. Generic stuff is usually more stable than more specific stuff.\nIn the universe of representing knowledge that we call documentation, prefer ref-\nerences the following ways, not the other way around:\n •From the artifacts (code, tests, configuration, resources) to the project goals, constraints, and requirements\n •From the goals to the project vision\nBroken Link Checkers\nIf you have a direct link to a resource, then you need a way to detect when the link is broken. Links to code at GitHub get broken when the code changes, and links to external websites get broken when the websites reorganize their content or when they disappear. \nChapter 9 Stable Documentation 256\nTherefore: Use a mechanism to detect broken links before your colleagues do.\nY ou can use a broken link checker on your overall documentation for detecting \nbroken links. (Y ou can find many such checkers by searching online for “broken \nlink checker.”) Y ou may also use low-tech contract tests that will fail when there is a change that breaks a link. This way , you know when you have to fix the link or the code to get them back in sync. This is another example of a reconciliation mechanism.\nY ou can create a unit test to compare the code, which could change at any time, \nagainst hardcoded laterals that represent the external contract, such as the link. When the test fails, you know you have to update the doc or perhaps revert the change.\nFor example, if the qualified class name is used directly in a link, the contract test \nmight look like this:\n1  @Test\n2  public void checkLinks() {3  assertEquals(4  """"flottio.fuelcardmonitoring.domain.FuelCardMonitoring"""",5  FuelCardMonitoring.class.getName());6  }\nWhenever you refactor and accidentally break the contract, this check against the \nhardcoded literal would fail to tell you that you need to make a fix.\nLink Registry\nAll links need maintenance because the web is a living thing, and so is your internal \ncompany web. When a link is broken, you don’t really want to have to go through every document that contains the broken link and replace it with another link. \nTherefore: Don’t directly include direct links in multiple places in your artifacts. \nInstead, use a link registry that is under your control.\nA link registry is an indirection that you can change to fix broken links in one sin-\ngle place. A link registry gives you intermediate URLs as aliases on the actual links. \nWhen a link is broken, you just need to update the link registry in one single place to redirect to another link.\nAn internal URL shortener works perfectly as a link registry . Some shorteners \nallow to choose your own pretty short link; not only do the links become more man-ageable, they also get shorter and prettier.\nI’ve seen companies install their own on-premise link registries. This is necessary \nfor companies that care a lot about the confidentiality of their knowledge. Y ou can find many URL shorteners that you can install on-premise, some open source and some with commercial licenses.\nLinked Knowledge 257\nBookmarked Searches\nAnother way to link in a way that is more robust to change is to link to a book-\nmarked search instead of linking to a direct resource. Imagine that you want to link to the class \nScenarioOutline  in a repository . Y ou could link through a direct link. \nFor example, in GitHub you would use a link like this:\nhttps://github.com/Arnauld/tzatziki/blob/\n4d99eeb094bc1d0900d763010b0fea495a5788d\d/tzatziki-core/src/main/java/tzatziki/analysis/step/ScenarioOutline.java\nThe problem is that this class could move into another package, or its package \nmight be renamed. The class itself could be renamed, too, even though that is not very \nlikely . But any of these changes would turn a link into a broken link, and that’s bad.\nTherefore: Replace direct links with searches on more stable criteria. There may \nbe more than one result, but it will help users locate the target of a link in a more robust way.\nY ou can make a link more robust by using a bookmarked search instead of a direct \nlink. For example, you could search for a Java class in this particular repository with \nScenarioOutline  in its name. Using the GitHub advanced search,2 you would cre-\nate the following search:\nScenarioOutline in:path extension:Java repo:Arnauld/tzatziki\nwhere each option can help create a more relevant search:\n •ScenarioOutline : Search for this term.\n •in:path : The search term must appear in the path name.\n •extension:Java : The file extension must be Java.\n •repo:Arnauld/tzatziki : Search only in this one repo.\nThe result page of this search will show more than one result, but the one you’re \nlooking for is easy to grab from the list (here it is the second result in the list):\n1  .../analysis/exec/model/ScenarioOutlineExec.java\n2  .../analysis/step/ScenarioOutline.java3  .../pdf/emitter/ScenarioOutlineEmitter.java4  .../analysis/exec/gson/ScenarioOutlineExecSerializer.java5  .../pdf/model/ScenarioOutlineWithResolved.java\n2. GitHub, https://help.github.com/articles/searching-code/"",6728",6811,"**Categories of Stable Knowledge**\n\n**Linking Knowledge**:\n- Links should go from less stable to more stable\n- Expose knowledge as web resources accessible through URLs\n- Refer to knowledge using links, and use link registries to ensure permanence\n- Tools like issue trackers, static analysis tools, planning tools, blogging platforms, and social code repositories expose their knowledge through links\n\n**Volatile-to-Stable Dependencies**:\n- Reference stable elements from volatile elements (e.g., artifacts to project goals)\n- Broken link checkers can detect broken links in documentation\n- Use a mechanism to detect broken links before colleagues do\n\n**Link Registry**:\n- Don't directly include links in multiple places, use a link registry instead\n- Link registries provide intermediate URLs as aliases and allow updating broken links in one place\n\n**Bookmarked Searches**:\n- Replace direct links with bookmarked searches on more stable criteria (e.g., search for Java class ""ScenarioOutline"" in a specific repository)",1021,31.886404752731323
False,3,"125-Domain Vision Statements.pdf,125-Domain Vision Statements,""Chapter 9 Stable Documentation 258\nA bookmarked advanced search is not just useful for","125-Domain Vision Statements.pdf,125-Domain Vision Statements,""Chapter 9 Stable Documentation 258\nA bookmarked advanced search is not just useful for more robust links. It is also \nan important tool for living documentation in general. It offers the power of an IDE \nfor everyone who has a browser. By creating curated bookmarked searches, you cre-ate guided tours for navigating code and for quickly discovering everything related to a concept, as shown here around the concept of \nScenarioOutline .\nCategories of Stable Knowledge\nAs discussed in the following sections, different pieces of knowledge have different lifespans, from volatile to long term. The following typical categories of stable knowledge are good candidates for evergreen documents.\nEvergreen README\nWe have projects with short, badly written, or entirely missing documentation.…There \nmust be some middle ground between reams of technical specifications and no specifi-cations at all. And in fact there is. That middle ground is the humble Readme.\n—T om Preston-Lerner , “Readme Driven Development”\nFor a given project Blabla, the README file can be safely evergreen if it focuses on \nanswering the following key questions:\n •What is Blabla?\n •How does Blabla work?\n •Who uses Blabla?\n •What is Blabla’s goal?\n •How can the organization benefit from using Blabla?\n •How do you get started with Blabla? (But beware: Keep it so simple that it should not change often. In particular, don’t embed the version number but instead refer to the place where you can find the most recent version number.)\n •What is the licensing information for Blabla? (This could also be detailed in a LICENSE.txt sidecar file.)\nThis level of key information is at the same time essential and quite stable over \ntime.\nBeware of including instructions on how to develop, use, test, or help, as well as \ncontact information, except with permanent mailing lists.\nVision Statement 259\nAlso, when using an online source code repository like GitHub, avoid linking \nfrom the README to pages on the wiki: The README is versioned, whereas the \nwiki is not, so links will break, in particular when cloning or forking.\nVision Statement\nA vision is a picture of the world as it will be when you’re done working on it.\n—The McCarthy Show (@mccarthyshow) on Twitter\nWhen a manager comes to me, I don’t ask him, “What’s the problem?” I say , “Tell me \nthe story .” That way I find out what the problem really is.\n—Grocery store chain owner Avram Goldberg, quoted in The Clock of  the Long Now\nOne of the single most important pieces of knowledge everybody in a project should \nabsolutely know is the vision of the project or of the product.\nWith a clear vision, the efforts of each team member can really converge to make \nthe vision come true. A vision is a dream, but it is a dream that is also a call to action for a team that decides to make it real.\nA vision often originates in a particular person, who tries to share it with other \npeople using various means:\n •A talk, lecture-style, perhaps with great visuals, like a TED talk\n •Repeating the pitch of the vision often and to everyone\n •Telling stories that illustrate or exemplify the vision\n •Writing down a vision statement \nAll this is documentation. A brilliant talk recorded on video may be the best doc-\numentation of the vision.\nA vision has to be simple so that it can be pitched in a few sentences. Startups love \nvision statements, but these statements sometimes lack depth because they just steal from existing successful startups. \nThe perfect companion to a vision statement is a couple of stories that illustrate \nit and make it more real.\nA vision statement is usually on the stable end of the spectrum, at least compared \nto other project artifacts, such as source code and configuration data. But it is possi-ble that a company could change its vision from time to time, such as when pivoting.\nOnce the vision is set, it can be split into high-level goals."",3954",4023,"**Domain Vision Statements**\n\n**Advantages of Bookmarked Advanced Searches:**\n- Useful for robust links\n- Important tool for living documentation\n- Offers power of an IDE for everyone with a browser\n- Creates guided tours for navigating code and discovering related concepts\n\n**Categories of Stable Knowledge:**\n- Evergreen README:\n  - Answers key questions about a project or system\n  - Focuses on what, how, who, goal, benefits, and getting started\n  - Essential and stable over time\n  - Avoid including development, testing, or contact information\n- Vision Statement:\n  - Picture of the world as it will be when completed\n  - Converges efforts of team members to make vision come true\n  - Simple and easy to pitch in a few sentences\n  - Often originates in a person and needs documentation\n  - Illustrated with stories that bring vision to life\n  - Usually on the stable end, but may change over time\n\n**Creating Evergreen Documents:**\n- For a project or system, answer key questions: what, how, who, goal, benefits, start\n- Focus on essential and stable information\n- Avoid including instructions for development, testing, or help\n- Use README file as a starting point\n\n**Documenting Vision Statements:**\n- Originates in a person and needs documentation\n- Simple enough to pitch in a few sentences\n- Illustrated with stories that make it more real\n- Usually on the stable end of the spectrum.",1396,36.7996723651886
False,3,"126-Investigation Wall.pdf,126-Investigation Wall,""Chapter 9 Stable Documentation 260\nDomain Vision Statements \nA particular kind of vision statemen","126-Investigation Wall.pdf,126-Investigation Wall,""Chapter 9 Stable Documentation 260\nDomain Vision Statements \nA particular kind of vision statement focuses on the business domain the product is \nabout. The purpose of this statement is to describe the value of the future system to be built before it actually exists. This description may span several subdomains since at the beginning no one knows yet how the domain should be split into subparts. The point of the domain vision statement is to focus on the critical aspects of the domain.\nIn the words of Eric Evans:\nWrite a short (1 page) description of the core domain and the value it will bring, the \n“value proposition.” Ignore those aspects that do not distinguish this domain from  others. Show how the domain model serves and balances diverse interests. Keep it  narrow . Write this statement early and revise it as you gain new insights.\n3\nMost technical aspects and infrastructure or UI details are not part of the domain \nvision statement.\nHere is an example of a domain vision statement for fuel card monitoring in the \nfleet management business:\nFuel card monitoring of every incoming fuel card transaction helps detect potential abnormal behavior by drivers.\nBy looking for abuse patterns and by cross-checking facts from various sources, the \nsystem reports anomalies that are therefore investigated by the fleet management team.\nFor example, a client using fuel card monitoring with GPS fleet-tracking features is \nable to catch employees who are padding hours, falsifying timesheets, stealing fuel, or buying non-fuel goods with the fuel card.\nEach fuel card transaction is verified against vehicle characteristics and its location \nhistory , considering which driver was assigned to the vehicle at the time and the address of the merchant of the transaction. Fuel economy can also be calculated to detect engines in need of a repair.\nA domain vision statement is useful as a summary of the main concepts of the \ndomain and how they are related to deliver value to the users. It can be seen as a \nproxy for the actual software that is not yet built.\nGoals\nThe vision is the single most important piece of knowledge everybody should know and keep in mind at all times. From that vision, many decisions will be made to  converge to a solution and its implementation.\n3. Evans, Eric. Domain-Driven Design: T ackling Complexity in the Heart of  Software. Boston: Pearson \nEducation, Inc. 2004.\nVision Statement 261\nA vision alone is often not enough for people to start working, and you may have \nto form precise intermediate goals, such as to share work between different teams or \nto explore early what could be done and the alternatives.\nGoals can be described as a tree of goals and subgoals, with the vision at the \nroot. Goals are lower level than the vision, but they are high level compared to all the details that describe how a system is built. They are therefore on the stable side, and the higher the level, the more stable.\nGoals are also long term must be known by most people, and they are criti-\ncal because they drive many further decisions. As a consequence, they must be documented in a persistent fashion. Since they are also on the stable end of the frequency-of-change spectrum, traditional forms of documentation fit for docu-menting goals:\n •Microsoft Word documents\n •Slide decks\n •Paper document\nThis does not mean that it’s easy to make good documentation for the goals. It’s \nstill all too easy to waste a lot of time on a document that will not be read because it’s too long or too boring.\n Caution\nRemember that there is danger in deciding goals prematurely: Y ou may over-constrain the project too early , at a time you know very little about it. This may impede the project execution. This is why Woody Zuill advises on his blog to “keep your requirements at a very high & general level until just before use,”\n4 as if \nthey were perishable goods. Y ou do not want to reject opportunities early because of premature subgoals.\n \nImpact Mapping\nA great technique for exploring goals and organizing high-level knowledge about a project or a business initiative is impact mapping,\n5 proposed by Gojko Adzic. It \nadvocates working on goals through interactive workshops and keeping the alterna-tive goals together on a map to keep options open during the execution of the \n4. Woody Zuill, “Life, Liberty , and the Pursuit of Agility” blog, http://zuill.us/WoodyZuill/2011/09/30/\nrequirements-hunting-and-gathering/\n5. Impact mapping, http://www .impactmapping.org/\nChapter 9 Stable Documentation 262\nproject. This collaborative technique is simple and lightweight, and it involves visu-\nalizing assumptions and targets.\nAn impact map shows options and alternate paths to reach a goal. It therefore \ndoes not constraint the execution as much as other traditional linear roadmaps.\nAn impact map itself is stable, but it’s recommended to reconsider it at low fre-\nquency , typically twice a year. On the other hand, tracking the project execution on the map obviously changes often if you release often, and this should not be done by modifying the map each time.\nLet’s take as an example the result of an impact mapping session for a company in \nthe music industry , presented as a tree-like mind map:\n1  Reduce processing cost of song royalties\n2   IT Department3     100x volumes4     50% Cheaper processing5   Sales Department6     Hourly stats feeds7   Billing Department8     Online real-time reporting (2s or less)\nImpact mapping suggests classifying the goals by main stakeholders, which would \nbe IT department, sales department, and billing department in this example. It also \nrequires the goals to be quantified in the impact maps, with quantitative figures of success, called “performance targets.”\nThere are other similar techniques, such as the EVO method Gilb,\n6 for exploring \nrequirements in various ways.\nWith or without impact mapping, a tree of goals is ideally created with sticky \nnotes on a wall. If you want to keep a clean representation for later, you can use any mind-mapping application, such as MindMup, MindNode, Mindjet MindManager, Zengobi Curio, or MindMeister, to record and show a cleaner layout of the map.\nThese applications can read and write mind maps in various forms, including as \nindented text, at least as an “import” option. As a fan of plain-text artifacts, I like indented text best!\nInvesting in Stable Knowledge\nStable knowledge is an investment that pays back over a long period of time. Learn-ing a subject is a costly investment. I have a hard time learning technologies that have a half-life of a few years. \n6. Gilb, http://gilb.com\nInvesting in Stable Knowledge 263\nBusiness domain knowledge (finance, insurance, accounting, e-commerce, manu-\nfacturing, and so on) is some of the most stable knowledge there is. But because \nyou may not always work within the same domain, you might wonder if it’s reason-able to learn the knowledge of a particular domain. But it also happens that a lot of domain-specific knowledge is reusable in some form in other business domains. As an example, the book Analysis Patterns by Martin Fowler describes a number of patterns taken from accounting or medical observations but that work almost directly in finance, insurance, and commerce. \nIn addition, the fundamentals of computing and of software architecture and \ndesign also belong to the stable knowledge category . Don’t hesitate to read the old papers and the patterns that map this territory .\nTherefore: Don’t hesitate to invest time and effort into learning stable knowl-\nedge. In particular, business domain knowledge and fundamentals of software architecture are evergreen content that are particularly worth learning.\nDomain Immersion\nDomain knowledge is typically on the stable side of the spectrum, even if your under-standing of it changes over time (and it should). But the core use cases, purposes, concrete examples, and conversations with business people are mostly evergreen. \nT raditionally , a software project itself is the main way to learn its domain. Task \nafter task, each work part brings new vocabulary and new concepts that are learned on the job because they are necessary to do the job. This leads to a number of weaknesses:\n •There is not enough time to deliver a task and to study seriously a part of the business domain in more depth. Learning remains superficial.\n •Many tasks can be done with only superficial understanding of the underlying business. Something might appear to work by coincidence while really being a time bomb for next business requirements.\n •Even if you decide to dedicate two hours out of the task to learn, the domain experts may not be available at that time.\nWhenever the lack of domain knowledge is a bottleneck, it’s an attractive propo-\nsition to invest time early on to learn the domain. One of the best ways to do this is through immersion. Invest time early to immerse the team into the domain. Visit the place where the business takes place. Take pictures. Get copies of the documents being used. Listen carefully to the conversations of the business people. When pos-sible, ask questions. Make sketches of what you see and take plenty of notes."",9284",9341,"**Chapter 9 Stable Documentation**\n\n**Domain Vision Statements**:\n- Focus on the business domain the product is about\n- Describe the value of the future system before it exists\n- Spans several subdomains, as no one knows how the domain should be split yet\n- Narrow in focus and ignore non-distinctive aspects\n- Help balance diverse interests and serve as a proxy for the actual software\n\n**Goals**:\n- Lower level than the vision, but higher than details of implementation\n- Long term and critical, driving many decisions\n- Documented in a persistent fashion (e.g., Microsoft Word, Slide Decks, Paper Documents)\n- Careful not to over-constrain the project too early\n\n**Impact Mapping**:\n- Technique for exploring goals and organizing high-level knowledge about a project or business initiative\n- Shows options and alternate paths to reach a goal\n- Recommends classifying goals by main stakeholders and quantifying success with ""performance targets""\n- Collaborative and lightweight, using mind maps to record and show a cleaner layout\n\n**Investing in Stable Knowledge**:\n- Business domain knowledge and fundamentals of software architecture are evergreen content worth learning\n- Domain-specific knowledge is often reusable in other business domains\n- Traditional projects are the main way to learn a domain, but lacks time for deep understanding\n- Immersion (e.g., visiting the business location, listening to conversations) can help overcome this limitation",1457,43.5936975479126
False,3,"127-Domain Training.pdf,127-Domain Training,,0","127-Domain Training.pdf,127-Domain Training,,0",46,"**Training on Domain 127**\n\n**127-Domain Training**:\n\n**127-Domain Training**, **127-Domain**\n\n**0**",100,5.821747779846191
False,3,"128-Shadow Users.pdf,128-Shadow Users,""Chapter 9 Stable Documentation 264\nDomain immersion is also an effective practice for new joiners to quickly d","128-Shadow Users.pdf,128-Shadow Users,""Chapter 9 Stable Documentation 264\nDomain immersion is also an effective practice for new joiners to quickly discover \nwhat the domain is about. It is an alternative form of knowledge transfer, directly \nfrom the field, which also means it is a genuine form of documentation.\nSometimes it is not possible to go to the field, or it is prohibitively expensive to do \nso, in which case you need cheaper alternatives for this precious knowledge, such as an investigation wall or simple trainings.\nInvestigation Wall\nY ou might want to create a wall of findings, much like an investigation wall in a criminal investigation movie, where the detectives cover the walls with lots of pic-tures, notes, and maps with pins to fully immerse themselves in the crime.\nSimilarly , you can create a space on a wall with pictures, notes, sketches, and \nsample business documents to keep a feel for the actual business domain while you work on it.\nDomain Training\nA team or part of a team might benefit from specialized trainings about the business domain.\nIn one of my past projects, we decided to invest in domain knowledge early , when \nthe pressure was not very strong, so twice a week we dedicated 30 minutes after lunch to a mini-training session. A business analyst or a product manager who was identified with an area of expertise joined the team as the domain expert to explain all we needed to know on one concept at a time (a session on bond coupons, another about standard financial options, another on a new regulation, and so on). The team considered this training useful, and the developers enjoyed it.\nLive-My-Life Sessions\nWith “live-my-life” sessions, for a period of time from half a day to two days, one or two developers stay close to someone doing business operations to see what it’s really like to work in the business, using the software tools those people have. The develop-ers may be in the back, trying not to interfere and just watching passively . However, it’s best for them to be able to ask questions at any time or during some predefined pauses.\nSuch an experiment may be more involved. For example, a developer might try \nbeing an assistant to a business person. Some companies go further and have employ-ees completely switch roles for a day . As a developer, doing the job of an accountant"",2325",2370,"**Documentation Practices:**\n- **Domain immersion**: effective practice for new joiners to discover domain knowledge through direct experience from the field\n- Alternative form of documentation\n- Useful when not possible or expensive to go to the field\n- **Investigation wall**: create a space with pictures, notes, sketches, and business documents to keep a feel for the business domain while working\n- **Domain training**: team can benefit from specialized trainings about the business domain led by a domain expert\n\n**Live-My-Life Sessions:**\n- Developers stay close to someone doing business operations for a period of time\n- Observe and learn without interfering\n- Best if developers can ask questions during pauses\n- More involved experiment: developer may try being an assistant in a different role.",806,21.892913818359375
False,3,"129-A Long-Term Investment.pdf,129-A Long-Term Investment,,0","129-A Long-Term Investment.pdf,129-A Long-Term Investment,,0",60,"**Investing Long-Term: A Strategic Approach**\n\n**Overview**: Focus on long-term investment strategies and benefits.\n\n**Long-Term Investment**: Emphasize the importance of strategic planning for long-term financial gains.\n\n---\n\n**Investment Advice**: Offer guidance on how to make smart, long-term investments.\n\n**Benefits**: Discuss advantages of long-term investment strategies such as compound interest and potential tax benefits.\n\n---\n\n**Embrace the Long Game**: Encourage readers to adopt a long-term perspective when managing their finances.\n\n**Patience and Perspective**: Emphasize that successful long-term investments require both patience and the right perspective.\n\n**Strategic Planning**: Stress the importance of creating a well thought-out plan for long-term financial growth.\n\n---\n\n**Compound Interest**: Explain how compound interest works and how it can benefit long-term investors.\n\n**Tax Benefits**: Discuss potential tax advantages of long-term investments, such as retirement accounts or real estate purchases.\n\n**Diversification**: Advise readers on the importance of diversifying their investment portfolio for added security.\n\n---\n\n**Risk Management**: Offer tips on managing risk in a long-term investment strategy.\n\n**Market Volatility**: Address concerns about market volatility and how it affects long-term investments.\n\n---\n\n**Education and Research**: Encourage continuous learning about various investment opportunities and strategies.\n\n**Informed Decisions**: Emphasize the importance of making informed decisions when investing for the long term.\n\n**Financial Literacy**: Highlight the value of becoming financially literate to make intelligent, long-term investments.\n\n---\n\n**Patience and Discipline**: Encourage readers to stay patient and disciplined in their long-term investment approach.\n\n**Consistency**: Advise on the importance of maintaining a consistent investment strategy over time.\n\n**Review and Adjust**: Recommend regularly reviewing and, if necessary, adjusting your long-term investment plan.\n\n---\n\n**Seeking Professional Guidance**: Offer resources for those seeking professional advice on their long-term investment strategies.",2179,45.8052282333374
False,3,"130-Summary.pdf,130-Summary,""Summary 265\nfor a day could be one of the best ways to appreciate their stakes, and therefore to \nimprove their softwar","130-Summary.pdf,130-Summary,""Summary 265\nfor a day could be one of the best ways to appreciate their stakes, and therefore to \nimprove their software. It can also do wonders for the user experience.\nShadow Users\nA variant of the live-my-life idea is to watch the behavior of users as a shadow user. A developer logs in as another real user, in a read-only fashion, and sees that person’s screen in real time. It can be very valuable to watch how they really use the software to achieve their business objectives.\nHaving shadow users is obviously not feasible in many cases—because of privacy \nreasons or because the installed software is not accessible. \nA Long-Term Investment\nAll these ways of investing in stable knowledge can be seen as an investment because \nthe business domain is usually quite stable. The details of doing the business change all the time, but the business still uses the same old concepts. I realized this in 2007 when I opened a book on finance written in 1992. The book was still relevant in all its content except that the examples were no longer realistic: Interest rates in 1992 were often around 12% to 15% in some currencies, whereas 15 years later they were closer to 2%. (T oday they are around 0.2%!)\nEven books written well before the advent of computers may remain interesting.All the contextual knowledge gained through these ways of investing in stable \nknowledge will inform and improve many decisions every day . And all the domain-specific words learned will make discussions during meetings more efficient. Y ou won’t have to spend the first part of each meeting clarifying vocabulary anymore.\nSummary\nEven in the most fast-changing projects there is still some room for traditional docu-ments, but only for stable knowledge, which can be documented once as evergreen content. The examples presented in this chapter are just examples, not rules. \nPaying attention to how often pieces of knowledge change is a good strategy for \nreducing your workload over time because it means you can create documents that need to be manually updated only for knowledge that almost never changes. In other cases, you need to use the more dynamic forms of documentation described else-where in this book, and you must rely more on conversations, working collectively , and living documents instead.\nThis page intentionally left blank"",2347",2382,"**Investing in Stable Knowledge for Improved Software Development**\n\n**Summary:**\n- Observing users' behavior as ""shadow users"" can enhance software development by providing valuable insights into how they use the product to achieve their business objectives.\n- This approach is not feasible for all projects due to privacy concerns or unaccessibility of installed software.\n- Investing in stable knowledge gained through traditional documentation, books, and contextual understanding is an investment as the business domain remains largely unchanged.\n- Traditional documents should focus on stable knowledge, which can be documented once as evergreen content.\n- Attention to changing pieces of knowledge helps reduce workload over time by creating documents that need minimal updates.\n- For fast-changing projects, dynamic forms of documentation and collective efforts are necessary instead.\n\n**Investing in Stable Knowledge:**\n- **Observing users as shadow users**: Developers log in as another real user to observe their behavior and screen activities.\n- **Long-term investment**: The business domain is usually quite stable, allowing for traditional documentation of evergreen content.\n- **Contextual understanding**: Books written before the advent of computers may still be relevant due to contextual knowledge gained.\n- **Improved decision making**: Stable knowledge and learned vocabulary make discussions during meetings more efficient.\n\n**Traditional Documentation:**\n- Focus on stable knowledge that can be documented once as evergreen content.\n- Reduce workload over time by creating documents that need minimal updates for changing pieces of knowledge.\n- Utilize dynamic forms of documentation, conversations, and living documents for fast-changing projects instead.",1779,36.538673400878906
False,3,"131-Conversations About Formal Documentation.pdf,131-Conversations About Formal Documentation,""267Chapter 10\nAvoiding Traditional \nDocumentation\nW","131-Conversations About Formal Documentation.pdf,131-Conversations About Formal Documentation,""267Chapter 10\nAvoiding Traditional \nDocumentation\nW e embrace documentation, but not hundreds of  pages of  never-maintained \nand rarely-used tomes. #agilemanifesto\n—@sgranese on T witter\nI don’t know many developers who love traditional forms of documentation. I’ve been collecting alternatives for years, some of which look like documentation and some of which don’t. As a continuation of Chapter 1, “Rethinking Documentation,” this chapter rebels against the documentation establishment (see Figure 10.1) and explores a number of techniques that contribute to the preservation and sharing of knowledge but that are usually not thought of as documentation techniques.\n \nFigure 10.1 NODocumentation is a manifesto for exploring alternatives to traditional forms \nof  documentation, where the “NO” actually means “Not Only.” W e acknowledge the purpose of  documentation, but we disagree with the way it’s usually done. NODocumentation is about exploring better alternatives for transferring knowledge between people and across time.\nChapter 10 Avoiding Traditional Documentation 268\nNote\nDocumentation is only a means, not an end, and it is not a product.  \nLet’s start this exploration with the recognition that healthy teams working \ntogether and having conversations already exchange knowledge efficiently .\nConversations About Formal Documentation\nHaving conversations, is more important than documenting conversations, is more \nimportant than automating conversations.\n—@lunivore (Liz Kheogh)\nA phone call can save twenty emails. A face to face chat can save twenty phone calls.\n—@geoffcwatts on Twitter \nWritten documentation is so often the default choice when it comes to \n documentation that the word documentation has often come to be used to mean “written document.” However, when we say we need documentation, we mean that there’s a need for knowledge transfer from some people to some other  people. The bad news is that not all media are equal when it comes to the efficiency of transferring knowledge.\nAlistair Cockburn analyzed three dozen projects over the course of two \n decades. He reported on his findings in books and articles, and he produced a famous  diagram illustrating the effectiveness of different modes of communica-tion (see Figure 10.2).\n1 \nAlthough the diagram in Figure 10.2 is a bit dated, it recaps Alistair’s observa-\ntion that people working and talking together at the whiteboard is the most effec-tive mode of communication, whereas paper is the least effective.\nMost of the time, effective sharing of knowledge is best done by simply talking \nand asking and answering questions rather than through written documents.\nTherefore: Favor conversations between everybody involved over written docu-\nments. Unlike all written artifacts, conversations are interactive and fast, they convey feelings, and they have a high bandwidth.\n1. Alistair Cockburn. Agile Software Development. Boston: Addison-Wesley Longman Publishing Co., \nInc., 2002.\nConversations About Formal Documentation 269\nConversations have several important characteristics:\n •High bandwidth: Conversations offer high bandwidth compared to writing \nplus reading, as more knowledge can be efficiently communicated over a given period of time.\n •Interactive: Both sides of the conversation have opportunities to ask for clarifi-cation and train the topic on what’s most useful for them, whenever they want to.\n •Just in time: Both sides of the conversation only talk about what’s of interest for them.\nThese key properties of conversations make them the most effective form of com-\nmunication for sharing knowledge.\nIn contrast, written documentation is wasteful because it takes time to write \nand also because it takes time to locate where the relevant parts are—and then it’s unlikely that the content will fit the expectations. Even worse, it’s likely that the con-tent will be misunderstood.\nCommunication\nEffectivenessTwo People\non EmailTwo People\non PhoneTwo People\nat Whiteboard\nAudiotape\nPaper(No Question-Answer)(Question-and-Answer)Videotape\nRichness (""""Temperature"""") of Communication Channel(Cold) (Hot)\nFigure 10.2 The effectiveness of  communication improves with the richness of  the \ncommunication channel"",4273",4376,"**Chapter 10: Avoiding Traditional Documentation**\n\n**Embracing Alternatives to Traditional Forms of Documentation**\n- **Nodocumentation Manifesto**: explores alternatives to traditional forms of documentation (Figure 10.1)\n- Agrees with the purpose of documentation but disagrees with its conventional methods\n- Nodocumentation is about exploring better ways to transfer knowledge between people and across time\n\n**Effectiveness of Communication Channels for Knowledge Transfer**\n- Conversations are more effective than written documentation:\n  - High bandwidth: conveys more knowledge efficiently\n  - Interactive: allows for questions and clarification\n  - Just in time: addresses immediate needs\n- Compared to written documentation, conversations offer a faster and more comprehensive exchange of information.\n\n**Conversations as Effective Means of Communication**\n- Conversations have several key characteristics that make them effective for knowledge transfer:\n  - High bandwidth: offers a high volume of information exchange\n  - Interactive: allows both parties to ask questions and clarify misunderstandstands\n  - Just in time: addresses the needs of all parties involved.\n\n**Limitations of Written Documentation**\n- Writing takes significant time, both for creation and understanding\n- Locating relevant information is time-consuming\n- Misunderstandings are common when reading written content\n\n**The Importance of Conversations in Knowledge Transfer**\n- Effective sharing of knowledge often happens through conversations rather than written documents\n- Conversations offer a more efficient and effective means of transferring knowledge compared to written documentation.",1679,52.074111461639404
False,3,"132-The Rule of Three Interpretations.pdf,132-The Rule of Three Interpretations,""Chapter 10 Avoiding Traditional Documentation 270\nWiio’s Laws\nProfe","132-The Rule of Three Interpretations.pdf,132-The Rule of Three Interpretations,""Chapter 10 Avoiding Traditional Documentation 270\nWiio’s Laws\nProfessor Osmo Antero Wiio created Wiio’s laws, which are serious observations for-\nmulated in a humorous way about how human communication usually fails except by accident:\n2\n •Communication usually fails except by accident. \n •If communication can fail, it will.\n •If communication cannot fail, it still most usually fails.\n •If communication seems to succeed in the intended way , there’s a misunderstanding.\n •If you are content with your message, communication certainly fails.\n •If a message can be interpreted in several ways, it will be interpreted in a  manner that maximizes the damage.\nHuman communication works best through interactive dialogue, with the oppor-\ntunity for the receiver of information to react, disagree, rephrase, or ask for more explanation. This feedback mechanism is essential to fix the curse of one-way human communication highlighted by Professor Wiio.\nAlistair Cockburn expresses similar findings in his book Agile Software \nDevelopment:\nT o make communications as effective as possible, it is essential to improve the likeli-\nhood that the receiver can jump the communication gaps that are always present. The sender needs to touch into the highest level of shared experience with the receiver. The two people should provide constant feedback to each other in this process so that they can detect the extent to which they miss their intention.\n3\nA face-to-face, interactive, and spontaneous form of documentation is the \nbest way to improve on the fate of miscommunication highlighted by Pro-\nfessor Wiio. If all your stakeholders are happy with talking with the team for all questions and feedback, then change nothing. Y ou don’t need written documentation.\n2. http://jkorpela.fi/wiio.html\n3. Alistair Cockburn. Agile Software Development. Boston: Addison-Wesley Longman Publishing Co., \nInc., 2002."",1923",2010,"**Wiio's Laws**:\n- **Communication usually fails except by accident.**\n- **If communication can fail, it will.**\n- **If communication cannot fail, it still most usually fails.**\n- **If communication seems to succeed in the intended way, there's a misunderstanding.**\n- **If you are content with your message, communication certainly fails.**\n- **If a message can be interpreted in several ways, it will be interpreted in a manner that maximizes the damage.**\n\n**Improving Communication**:\n- Human communication works best through interactive dialogue, with the opportunity for the receiver to:\n  - React\n  - Disagree\n  - Rewrite\n  - Ask for more explanation\n- This feedback mechanism is essential to ""fix the curse of one-way human communication"" highlighted by Wiio.\n\n**Effective Communication in Software Development**:\n- **It's essential to improve the likelihood that the receiver can ""jump the communication gaps"" that are always present.**\n- The sender needs to touch into the highest level of shared experience with the receiver, and the two people should provide constant feedback to each other.\n- A face-to-face, interactive, and spontaneous form of documentation is the best way to improve on the ""fate of miscommunication"" highlighted by Wiio.\n- If all stakeholders are happy with talking with the team for all questions and feedback, then written documentation may not be needed.",1391,44.6415159702301
False,3,"133-Cross Programming.pdf,133-Cross Programming,""Conversations About Formal Documentation 271\n Note\nThe goal of agile documentation is to “help peop","133-Cross Programming.pdf,133-Cross Programming,""Conversations About Formal Documentation 271\n Note\nThe goal of agile documentation is to “help people interact” in several ways:\n •Knowing who to contact\n •Knowing how to work on the project, guidelines, style, and inspirations\n •Sharing the same vocabulary\n •Sharing the same mental model and metaphors\n •Sharing the same goal  \nThe Rule of Three Interpretations\nJerry Weinberg also wrote about the problem of making an interpretation of a \nreceived message, and he’s proposing how to check your understanding thanks to what he calls the Rule of Three Interpretations:\nIf I can’t think of at least three different interpretations of what I received, I haven’t \nthought enough about what it might mean.4\nThis rule doesn’t prove that one of your interpretations is the one intended, but it \nhelps avoid the illusion that the random first interpretation is the right one.\nObstacles to Conversations\nThere would be no need to emphasize the importance of conversations if people \nwere easily having conversations in the workplace. Unfortunately , this is not the case often enough.\nY ears of working together by handing documents over the wall have trained many \npeople not to have conversations except in meetings, where conversations are often negotiations. Corporate environments with politics and information retention have also trained colleagues not to share too much knowledge too early in order to remain in the game and to keep power, including blocking power.\nPeople from different teams or departments or people assigned on different pro-\njects or in different locations tend to have far fewer conversations than do close neighbors in the same team and project. They tend to use colder (not interactive) and less effective modes of communication, such as email or phone calls instead of \n4. Weinberg, Gerald M. Quality Software Management V olume 2: First-Order Measurement. New Y ork: \nDorset House, 1993.\nChapter 10 Avoiding Traditional Documentation 272\nface-to-face communication. It’s important to note that hierarchical distance—that \nis, not having the same management—is at least as great an impediment to having conversations as geographic distance.\nWorking Collectively for Continuous \nKnowledge Sharing\nThe idea of ownership of activities is another conversation-killer:\nProduct “Manager”\nProduct “Owner”Scrum “Master”I have no idea why people aren’t collaborating!\n—@lissijean on Twitter\nDefect tracking systems certainly don’t promote communication between program-mers and testers. They can make it easy to avoid talking directly to each other.\n—Lisa Crispin and Janet Gregory, Agile T esting\n5\nSeparation of people by functions in separate teams, like the Dev , QA, and BA teams, \nis a great way to make conversations less likely . Old clichés also reduce the likelihood that people will even imagine meeting and talking together: \n“I’m a tester, I must wait for the development to be finished to start testing.”\n“I’m a BA, so I must solve the problem by myself before handing it to the develop-\ners to implement.”\n“I’m a developer, my job is to execute what’s been specified beforehand, and my \njob is not to test it once it’s done.”\nI’ve heard that some business analysts have a hard time imagining not producing \ndocuments of a large enough size, for fear that their work will not be visible. They \nseem to believe that simply talking to help the project may not be enough to justify their role. Here we see how perverse this system has become, producing waste (large early documents) not for their value per se but to make the work visible to manag-ers. Fear of losing a job or individual incentives feed this kind of  counterproductive behavior.\n5. Crispin, Lisa, and Janet Gregory . Agile T esting: A Practical Guide for T esters and Agile T eams. \n Hoboken: Addison-Wesley , 2009.\nWorking Collectively for Continuous Knowledge Sharing 273\nHowever, working collectively is an opportunity for continuous knowledge \n sharing. Make sure that everybody knows that the only goal is to deliver value. Make \nthe work environment safe for everyone. Even with far fewer documents, there’s still \na role for traditional BA and QA team members, but it needs to be transformed into a continuous contribution to a collective adventure that we call a project or a product.\nTherefore: Reassure everyone that it’s perfectly okay to have conversations often \nand spend less time writing, and nobody should feel guilty about that. Promote collective work over separate job posts. Embrace the idea that close collabora-tion enables continuous knowledge sharing. However, ensure that the few most critical pieces of knowledge are recorded in a persistent fashion somewhere.\nHave everyone, even from different teams, sit close to each other most of the time, \naround the same table if possible, so that spontaneous communication can happen without obstacle.\nConversations are good. When creating software, we need to have conversations, \nand we need to program code. It’s often a great idea to do all that at once, continu-ously , together with one or more colleagues.\nThere are many good reasons for working collectively , including improving the \nquality of the software for its users and for its maintainers, thanks to continuous review and the continuous discussions on the design.\nBut working collectively , with frequent conversations, is a particularly effective \nform of documentation, too. Pair programming, cross programming, mob program-ming and the three amigos totally change the game with respect to documentation, as knowledge transfer between people is done continuously and at the same time the knowledge is created or applied on a task.\nPair Programming\nOH: “Mob programming. It’s like ‘pair programming meets RAID6.’”\n—@pcalcado on Twitter\nPair programming: the best way to do less email, attend fewer meetings, AND write less \ndocumentation! \n—@sarahmei on Twitter\nPair programming is a key technique from Extreme Programming. If code reviews \nare good, why not do them all the time?\nIn pair programming, the person writing code, called the driver, narrates for the \nobserver what’s happening, and the observer in turn replies with acknowledgements, remarks, corrections, and any other kind of feedback. The observer, also known as"",6330",6385,"**Conversations and Documentation in Agile Development:**\n* Agile documentation helps people interact through various means:\n	+ Knowing who to contact\n	+ Understanding project guidelines, style, and inspiration\n	+ Sharing a common vocabulary and mental model\n	+ Collaborative goal setting\n* The Rule of Three Interpretations:\n	+ If you can't think of at least three different interpretations, reconsider your understanding\n	+ Helps avoid the illusion that the first interpretation is always correct\n* Obstacles to Conversations:\n	+ Lack of effective communication in workplaces due to politics or fear of information loss\n	+ Distance between teams or departments\n	+ Hierarchical distance as an impediment\n* Working Collectively for Continuous Knowledge Sharing:\n	+ Encouraging collaboration and continuous knowledge transfer\n	+ Reassuring employees that conversations are valuable and encouraged\n	+ Embracing the idea of close collaboration to improve software quality and maintainability\n* Pair Programming:\n	+ A technique from Extreme Programming where team members work together on coding tasks\n	+ One person (driver) writes code while the other (observer) provides feedback, corrections, and suggestions.",1208,39.24305558204651
False,3,"134-The Three Amigos or More.pdf,134-The Three Amigos or More,""Chapter 10 Avoiding Traditional Documentation 274\nthe navigator, talks to the driver t","134-The Three Amigos or More.pdf,134-The Three Amigos or More,""Chapter 10 Avoiding Traditional Documentation 274\nthe navigator, talks to the driver to guide the work in progress, suggesting possible \nnext steps and expressing strategies for solving the task.\nWorking in pairs may not be something you are comfortable with and good at \nimmediately , but it’s something you can learn through practice, on the job or in cod-\ning dojos or on code retreats. There are various styles of pair programming, such as ping-pong pairing, where one person in the pair writes a failing test and then passes the keyboard for the other to make it pass and refactor.\nFor sharing knowledge as much as possible in order to have true collective owner-\nship, in pair programming it’s common to regularly change the partners in the pairs on a given task. Depending on the teams, this rotation of pairs can happen as fre-quently as every hour, or every day , or it may happen just once a week. Some teams don’t have a fixed frequency but require that any task cannot be finished by the pair who started it.\nCross Programming\nCross programming is a variant of pair programming where the observer is not a developer but a business expert. Whenever the programming task requires a deep understanding of the business domain, it’s a form of collaboration that is highly efficient but also very effective as all decisions taken by the pair in front of the com-puter are more relevant to the business. The name was coined by my colleague Hous-sam Fakih, who talked about this approach in conferences.\n6\nMob Programming\nMob programming is a software development approach where the whole team works \non the same thing, at the same time, in the same space, and at the same computer. This is similar to pair programming where two people sit at the same computer and collab-orate on the same code at the same time. With Mob Programming the collaboration is extended to everyone on the team, while still using a single computer for writing the code and inputting it into the code base.\n—mobprogramming.org\nAll the brilliant people working at the same time, in the same space, at the same com-puter, on the same thing.\n—W oody Zuill\n7\n6. @Houssam, https://speakerdeck.com/fakih/cross-programming-forging-the-future-of-programming\n7. Zuill, Woody . “Mob Programming–A Whole Team Approach,” https://www .agilealliance.org/ \nwpcontent/uploads/2015/12/ExperienceReport.2014.Zuill_.pdf"",2400",2469,"**Pair Programming:**\n- Involves two developers working together on the same task\n- One person writes failing test and passes keyboard to partner for making it pass and refactoring\n- Commonly practiced in coding dojos or on-the-job\n- Partners change regularly to ensure collective ownership\n\n**Styles of Pair Programming:**\n- **Ping-pong pairing**: where one person writes a failing test, then passes keyboard for partner to make it pass and refactor\n\n**Cross Programming:**\n- Variant of pair programming with a business expert as observer\n- Efficient collaboration when deep understanding of business domain is required\n- Coined by Houssam Fakih\n\n**Mob Programming:**\n- Whole team works together on the same task, in the same space, at the same computer\n- Similar to pair programming but with the entire team collaboating on a single codebase\n- Encourages active engagement and effective communication\n\n**Resources:**\n- Fakih, Houssam. ""Cross Programming: Forging the Future of Programming."" Speaker Deck, [https://speakerdeck.com/fakih/cross-programming-forging-the-future-of-programming](https://speakerdeck.com/fakih/cross-programming-forging-the-future-of-programming)\n- Zuill, Woody. ""Mob Programming–A Whole Team Approach."" Agile Alliance, [https://www.agilealliance.org/resources/articles/experience-reports/2014-zuill-mob-programming](https://www.agilealliance.org/resources/articles/experience-reports/2014-zuill-mob-programming)",1439,44.867934226989746
False,3,"135-Knowledge Transfer Sessions.pdf,135-Knowledge Transfer Sessions,""Working Collectively for Continuous Knowledge Sharing 275\nMob programming is a r","135-Knowledge Transfer Sessions.pdf,135-Knowledge Transfer Sessions,""Working Collectively for Continuous Knowledge Sharing 275\nMob programming is a recent addition to the stable of collective forms of program-\nming and has quickly gained popularity . If Extreme Programming turned the code review knob to 10, mob programming goes even further, turning it to 11.\nIn mob programming, there is no question of pair rotation as everybody’s always \npresent on any task, so everybody knows about every task. That’s literally collective ownership—in the same place at the same time.\nIn a team of five people doing mob programming full time, knowledge sharing \nis not an issue, as it’s done continuously , every second. Whenever someone has to attend a meeting outside, the rest of the team keeps on working, almost unaffected.\nThe Three Amigos (or More)\nA Product Owner, a Developer, and a Tester sit down to talk about something that the \nsystem under development should do. The Product Owner describes the user story . The Developer and Tester ask questions (and make suggestions) until they think they can answer the basic question, “How will I know that this story has been accomplished?”\nNo matter how or when it’s done, these three amigos (to borrow a term from my \nfriends at Nationwide) must agree on this basic criteria or things will go wrong. \n—George Dinwiddie, http://blog.gdinwiddie.com/2009/06/17/\nif-you-dont-automate-acceptance-tests/\nThe concept of the three amigos working together during specification workshops  is \ncentral to the BDD approach. In contrast with pair programming, cross program-\nming, and mob programming, the three amigos are not working on code but on con-crete scenarios describing the expected business behavior of the software to build. Still, everyone involved owns the scenarios, and it does not matter who writes them down on paper or in a test automation tool (such as Cucumber). Although the com-mon term is “three amigos,” in practice there may be more than three whenever another perspective (for example, UX, Ops) is key for the success of the work. \nEvent Storming as an Onboarding Process\nAlberto Brandolini invented Event Storming,8 a collaborative modeling activity using \nsticker notes on a large wall. He says that some teams find it valuable to run a new event storming session whenever a new member joins the team, as a fast onboarding mechanism. I can testify that event storming works very well for that. As a consult-ant spending just a few days always in new teams with new domains, I need to learn as much as possible of the new domain in a short period of time. Recently I’ve used \n8. https://www .eventstorming.com/"",2623",2698,"**Collective Programming Practices:**\n- **Mob programming**: a collaborative coding practice gaining popularity\n- No pair rotation in mob programming, everyone is present for every task\n- Continuous knowledge sharing with no ownership issues\n- A team of five can do it full time without concerns for knowledge transfer\n\n**Three Amigos (or More)**:\n- Consists of Product Owner, Developer, and Tester\n- Discuss user stories and criteria for software development\n- Agree on basic testing requirements to prevent issues\n- Central concept in Behavior Driven Development (BDD) approach\n- Collaborative activity focusing on business behavior scenarios\n\n**Event Storming:**\n- A collaborative modeling activity using sticker notes on a large wall\n- Invented by Alberto Brandolini as a fast onboarding mechanism\n- Effective way for new team members to learn new domains quickly.",868,32.93799829483032
False,3,"136-Truck Factor.pdf,136-Truck Factor,""Chapter 10 Avoiding Traditional Documentation 276\nshort event storming sessions for that, even if the team has","136-Truck Factor.pdf,136-Truck Factor,""Chapter 10 Avoiding Traditional Documentation 276\nshort event storming sessions for that, even if the team has done it several times \nbefore. It is really impressive how much you can learn in just two hours with this kind of workshop.\nIt happened recently that a business domain expert in an event storming session \nsaid that he had already created well-crafted diagrams on the domain. When we were mostly done with posting the events on the wall and organizing them, he drew the diagram on the whiteboard. It was interesting that his diagram was in many ways more complete than our wall of events. Still, the interactive workshop form meant that we were all much more engaged with our wall of stickers than is typically the case when just looking at a static diagram. The session became a game of comparing the diagram and the events wall to better understand both, and a lot of new insights appeared in this process.\nKnowledge Transfer Sessions\nKnowledge transfer (KT) sessions are common in companies that don’t tend to do pair programming or mob programming. The teams plan KT as part of their sched-uled work, often in addition to creating brief documents, to make sure the knowl-edge is actually shared and well understood. According to Wiio’s laws, that is a good idea. A typical example of KT would be to exchange the knowledge on the deploy-ment before a release, when the Ops are in another silo of the organization. One way to share knowledge in this case is to perform a dry run of the deployment, based on the deployment document and all the automated deployment manifests. This way , any issue, question, or mistake can be spotted quickly during the session—and all this happens during regular working hours.\nOf course, an alternative is to directly work collectively between the developers \nand the Ops people to prepare, configure, and document all the deployment process. KT can be a step in this direction for traditional companies, just as code reviews are a step toward pair programming.\nContinuous Documentation\nCollective forms of work are optimal for continuous documentation. Face-to-face interactive conversations are the most efficient form of communication, and pair programming, cross programming, the three amigos, and mob programming organize the work precisely to maximize the opportunities for effective  conversations. Documentation happens at the very time the knowledge is necessary . Everyone who must know about it is present and can immediately ask questions to clarify points."",2525",2570,"**Event Storming Sessions:**\n- Team learns quickly during two-hour workshops\n- Business domain expert contributes diagrams to session\n- Interactive format leads to engagement and new insights\n\n**Knowledge Transfer (KT) Sessions:**\n- Planned sessions for sharing knowledge in companies\n- Common practice when pair programming or mob programming is not common\n- Ops teams exchange knowledge during deployment process\n  - Dry run of deployment with document and manifests\n  - Identify issues, questions, or mistakes during session\n- Alternative: Collective work between developers and Ops people\n\n**Continuous Documentation:**\n- Face-to-face interactive conversations essential for continuous documentation\n- Pair programming, cross programming, three amigos, mob programming facilitate effective conversations\n- Documentation happens at the time knowledge is necessary\n- Everyone present can ask questions to clarify points.",922,25.582444429397583
False,3,"137-Coffee Machine Communication.pdf,137-Coffee Machine Communication,""Coffee Machine Communication 277\nWhen the task is done, those involved remembe","137-Coffee Machine Communication.pdf,137-Coffee Machine Communication,""Coffee Machine Communication 277\nWhen the task is done, those involved remember some of the key parts of knowl-\nedge and can forget the rest. If someone goes on vacation, the knowledge is safe in his \nor her colleagues’ mind, so someone’s absence does not impede the work in progress.\nTruck Factor\nWorking collectively is very good for improving the truck factor of a project—that is, the number of people on a team who have to be hit with a truck before the project is in serious trouble. The truck factor measures the concentration of information in individual team members. A truck factor of one means that only one person knows critical parts of the system, and if that person is not available, it would be hard to recover the knowledge.\nWhen several team members collaborate on every part of a project, knowledge is \nnaturally replicated in more people. When they leave or go on vacations or just leave for a meeting, the work can carry on without them.\nA small truck factor usually means someone is a hero on the project, with a lot \nof knowledge not shared with other teammates. This is definitely a problem for the resilience of the project, and management should be aware of it. Introducing collec-tive forms of programming is a nice way to mitigate such risk. Moving the hero to another team nearby is another way to deal with it.\nCoffee Machine Communication\nNot all exchange of knowledge has to be planned and managed. Spontaneous  discussions in a relaxed environment often work better and must be encouraged.\nRandom discussions at the coffee machine or at the water fountain are  invaluable. \nSometimes the best exchange of knowledge is spontaneous. Y ou meet a colleague or two and start talking. Then you have something like a content negotiation to find topics each of you is interested in. Y ou may land on a nonprofessional topic. In this case, you are creating a bond, which is invaluable. When you choose a profes-sional topic, you’ve landed on the best type of communication: Y ou’ve chosen this topic because all of you have an interest in it. Y ou have questions about your current tasks, and the other people are happy to help with answers or stories from their own experience.\nI believe that this kind of communication is the very best way to exchange \n knowledge. The topic is chosen freely based on shared interests. It’s interactive, with questions and answers and a lot of spontaneous storytelling. It takes as long as required. I’ve sometimes missed meetings because the discussion at the coffee machine was way more essential to a project than the meeting I was supposed to attend.\nChapter 10 Avoiding Traditional Documentation 278\nOpen space technology used for meet-ups and un-conferences replicates just this \ntype of idea setting for larger groups. The Law of T wo Feet states that everyone is \nfree to move where the topic is most interesting. Other important principles are that “The people who are there are the right persons” and that “Whenever it starts it’s the right time.”\nFor this type of communication to work, there must be no hierarchy pressure \naround the coffee machine. Everybody must be free to chat with the CEO without being formal or shy .\nTherefore: Don’t discount the value of random discussions at the coffee machine \nor water fountain or in the relaxation area. Create opportunities for everyone to meet and talk at random in a relaxed setting. Decree that the rank in the hierar-chy must be ignored within all relaxed conversations.\nGoogle and other web startups propose fantastic facilities to encourage people \nto meet and talk. Just ask Jeff Dean, the famed Googler who often is referred to as the Chuck Norris of the Internet. As the 20th Googler, Dean has a laundry list of impressive achievements, including spearheading the design and implementation of the advertising serving system. Dean pushed limits by achieving great heights in the unfamiliar domain of deep learning, but he couldn’t have done it without proactively getting a collective total of 20,000 cappuccinos with his colleagues. “I didn’t know much about neural networks, but I did know a lot about distributed systems, and I just went up to people in the kitchen or wherever and talked to them,” Dean told Slate. “Y ou find you can learn really quickly and solve a lot of big problems just by talking to other experts and working together.”\n9\nLa Gaité Lyrique, a cultural venue in Paris devoted to digital arts, has offices and \nmeeting rooms, but the staff working there often prefer to host meetings in the foyers that are open to the public (see Figure 10.3). They even serve beer there, but I haven’t seen people from the staff drink beer during the day .\n \nI’ve spent countless hours in their foyers writing this book. I’ve experienced ben-\nefits that would be missed in a traditional work environments with closed meeting rooms:\n •The atmosphere: Because there is a mix of people from the outside, many working and other having fun around tea or beer, the atmosphere is quite relaxed. This is pleasant, and it encourages thinking more creatively . Y ou have the choice of low sofas and lounge chairs or dining tables with kitchen chairs. On a tense topic, I’d go for the lounge setting each time! T o work on a  diagram, I’d choose the dining table.\n9. Tech Crunch, http://techcrunch.com/2015/09/11/legendary-productivity-and-the-fear-of-modern- \nprogramming/\nCoffee Machine Communication 279\n •Impromptu discussions: For example, the general director of La Gaité  Lyrique \nhad a meeting with two people from the staff. They didn’t book a space. When that conversation was done, the general director looked around to see who was there and went on to have very brief side discussions with colleagues who were attending another meeting in the foyer.\nThinking back on all the frustrations of planning meetings with busy clients in \nthe corporate world, in boring meeting rooms, I was jealous that the staff working at La Gaite Lyrique had such a better collaboration experience.\nBeing there with the staff also means I had the opportunity to ask questions to the \ndirector of the venue himself in an impromptu fashion—no appointment, no secre-tary to filter access. Wow .\nThe director told me he definitely encourages informal meetings. Spending leisure \ntime in the foyer instead of working is not a problem because everyone owns their responsibilities, regardless of how , when, where, or how long they work. Impromptu meetings can be totally improvised or planned in an informal space, like in the coffee machine area.\nCoffee machine communication is not suited for every case, of course. There is no \nguarantee that you’ll find the people you want to talk to around the coffee machine \nFigure 10.3 One of  the informal foyers where most meetings take place"",6841",6918,"**Coffee Machine Communication**\n\n**Importance of Spontaneous Discussions**:\n- Random discussions at the coffee machine or water fountain are valuable\n- Often lead to best exchange of knowledge\n- Topic is chosen freely based on shared interests\n- Interactive with questions, answers, and storytelling\n- Can last as long as required\n- Encourages creativity and relaxed atmosphere\n\n**Creating Opportunities for Communication**:\n- Decrease hierarchy pressure around coffee machine\n- Everyone must be free to chat with anyone\n- Google and other web startups provide facilities to encourage meetings\n- La Gaité Lyrique in Paris has open foyers as meeting spaces\n- Mix of people from outside encourages thinking more creatively\n\n**Benefits of Informal Meetings**:\n- Relaxed atmosphere encourages impromptu discussions\n- People own their responsibilities, regardless of working hours\n- No need for appointment or secretary to filter access\n- Director encourages informal meetings\n\n**Impact on Productivity and Creativity**:\n- Impromptu meetings can be both improvised and planned\n- Informal settings lead to better collaboration experience\n- Opportunity to ask questions and get answers from colleagues.",1196,48.82778191566467
False,3,"138-Idea Sedimentation.pdf,138-Idea Sedimentation,""Chapter 10 Avoiding Traditional Documentation 280\nunless you plan the meeting. There’s also no fli","138-Idea Sedimentation.pdf,138-Idea Sedimentation,""Chapter 10 Avoiding Traditional Documentation 280\nunless you plan the meeting. There’s also no flip chart, no whiteboard, and unfortu-\nnately no teleconferencing system. And there is no privacy .\n Note\nConversations, working collectively and places for spontaneous knowledge shar-ing represent the ideal form of documentation for most knowledge. However, this approach doesn’t scale to large groups of people and is not enough for knowledge that is essential in the long term, when all team members are gone or have for-gotten knowledge from the remote past. It’s not enough for knowledge that is of interest to a large number of people, and it’s not enough for knowledge that’s too critical to be left as spoken words. Sometimes you need something more, and a way to gradually evolve from the informal to the more formal.\n \nIdea Sedimentation\n“Memory is the residue of thought.” - simple but profound realization that is so import-\nant to my work. I intend to honor it more fully .\n—@tottinge on Twitter\nIt can take time to find out whether a piece of knowledge was important or not. A lot \nof knowledge is important only at the moment it’s created. Y ou might debate design options, try one, find out it’s not right, and try another. After some time, it may be obvious that it was the right choice, and the choice is visible in the code. It’s already there. There is no need to do anything more.\nY ou discuss options around the coffee machine. Y ou mentally simulate how they \nwill perform. Everybody agrees on the best option. Then a pair goes back to their computer to implement it. The knowledge exchanged and created during the dis-cussion was important at that particular time. But the next day , it’s already nothing more than a mere detail.\nOnce in a while, some of this knowledge remains important even after a while. It \ngets reinforced until it’s worth recording to be shared with a larger audience and for the future.\nTherefore: Within small groups of people, favor quick, fast, cheap interactive \nmeans of knowledge exchange such as conversations, sketching, and sticky notes. Only promote to more heavyweight forms of documentation the fraction of knowledge that has repeatedly proven to be useful, that is critical, or that  everybody should know, especially at a larger scale.\nIdea Sedimentation 281\nStart with impromptu conversations and later turn the key bits into something \npermanent, whether it be augmented code, evergreen content, or anything durable.\nThe knowledge exchanged live can be captured as traces, through a photograph \ntaken with your smartphone, handwritten notes, and so on (see Figure 10.4). But \nthese forms of documentation will often be ignored later on. \nConversations\nDebates\nDecisionsPhoto\nand\nNotes\nFigure 10.4 Conversations to traces\nFigure 10.5 Particles settling to the bottom of  a wine decanterThe sedimentation metaphor relates ideas that are flowing to sand flowing in a \nstream. The sand particles move along quickly , but some of them become sediment at the bottom of the stream, where they accumulate slowly . A similar process is at work in a wine decanter (see Figure 10.5)."",3163",3220,"**Idea Sedimentation**\n- Importance of documenting knowledge:\n  - Necessary for long-term retention\n  - Essential when team members are gone or forgotten knowledge\n  - Crucial for wide dissemination to large groups\n  - Vital for critical knowledge not left as spoken words\n- Forms of documentation:\n  - Conversations, working collaboratively, spontaneous knowledge sharing (informal)\n  - Flip chart, whiteboard, teleconferencing systems, traditional documents (formal)\n- Idea Sedimentation:\n  - Memory is the residue of thought\n  - Importance of knowledge may not be apparent until later\n  - Discussions, sketching, sticky notes (informal)\n  - Turn key bits into something permanent (formal)\n  - Capture conversations as traces through photos, handwritten notes\n- Sedimentation metaphor:\n  - Ideas are like sand particles that settle at the bottom of a stream or wine decanter\n  - Accumulation of important knowledge over time.",927,27.157694101333618
False,3,"139-Throw-Away Documentation.pdf,139-Throw-Away Documentation,,0","139-Throw-Away Documentation.pdf,139-Throw-Away Documentation,,0",64,"**Limitations of ""Throw Away"" Documentation**:\n* The term ""Throw Away"" documentation is not widely recognized or standardized in industry terminology.\n* It can lead to misunderstandings and confusion when communicating with team members or stakeholders.\n* Instead, consider using terms like ""Discardable Documentation"" or ""Ephemeral Documentation"" to clearly convey the intended meaning.",387,9.909518241882324
False,3,"140-Astonishment Report.pdf,140-Astonishment Report,""Chapter 10 Avoiding Traditional Documentation 282\nMake a napkin sketch to document a design aspe","140-Astonishment Report.pdf,140-Astonishment Report,""Chapter 10 Avoiding Traditional Documentation 282\nMake a napkin sketch to document a design aspect, and later if it proves essential, \nturn it into something maintainable, like a plain-text diagram or a living diagram or \na visible test.\nUse bullet points to document the quality attributes, and later if it hasn’t changed \nmuch, turn the bullets into executable scenarios.\nThrow-Away Documentation\nSome documentation is useful for only a limited period of time and can then be deleted. For example, you need a specific diagram while you’re designing around a problem. When you’re done with the problem, the diagram immediately loses most of its value because nobody cares anymore about the focus of that diagram. And for the next problem, you need another completely different diagram with another focus.\nTherefore: Don’t hesitate to throw away documentation that is specific to a par-\nticular problem.\nWhen it’s worthwhile to archive a diagram, turn it into a blog post, telling the \nstory with the diagram as an illustration.\nOne important set of transient documentation is everything about planning, like \nthe user stories and everything about estimation, tracking, and so on. A user story is useful only just before development. A burndown chart is useful only during an iteration. (Y ou may want to keep the stats to check later how hard it is to plan and estimate, but that is something different.) Y ou can throw away user story sticky notes after the iteration.\nOn-Demand Documentation\nThe best documentation is documentation that you really need and that suits actual purposes. The best way to achieve it is to create the documentation on demand, in response to actual needs.\nA need you have right now is a proven need from a real person. It’s not a specula-\ntion of something that someone could find useful in some future. The need you have \nright now is precise and has a purpose, and it can be expressed as a question. The documentation to be created will just have to answer the question. This is a simple algorithm to decide when to create documentation about what topic.\nTherefore: Avoid speculating on what should be documented. Instead, pay attention to all questions asked or questions that were not asked but that should have been asked, as signals that some knowledge needs to be documented.\nOn-Demand Documentation 283\nJust-in-Time Documentation\nDocumentation is best introduced just in time. The need for documentation is a \nprecious feedback, a “knowledge gap” signal that should trigger some documenta-tion action in response. The most important bit of documentation may be the docu-mentation that is missing. Listen to knowledge frustrations to decide when to fill the gap.\n Note\nThe idea of just-in-time documentation is inspired by the pull system Lean. A  pull \nsystem is a production or service process that is designed to deliver goods or ser-vices as they are required by the customer or, in the production process, when required by the next step.\n \nY ou might not invest time in some documentation action on each question. \nThere’s a need for some threshold:\n •Some follow the “Rule of T wo”: When you must answer the same question twice, start documentation about it.\n •Open-source projects sometimes rely on community votes to decide what to spend time on, including for the documentation.\n •Commercial products sometimes rely on website analytics to decide what to spend time on, including for the documentation.\n •Peter Hilton on Documentation Avoidance has his own take on this process, which is similar to the Rule of T wo:\n  1. Don’t write the docs.\n  2. Bluff, “it’s on the wiki.”  3. Still don’t write the docs.  4. If they come back, feign ignorance.  5. Then write it.  6. Later, casually mention that you “found it.”\n10\n10.  Peter Hilton, https://www .slideshare.net/pirhilton/documentation-avoidance\nChapter 10 Avoiding Traditional Documentation 284\nIn practice, you can keep it low tech: Every time you’re asked for information for \nwhich you don’t have any documentation already available, log the request as a sticky \nnote on a wall.\nWhenever you have repeated requests for a similar kind of information, you can \ndecide as a team to invest some minimal work to create it, using a rustic voting mech-anism on the wall.\nStart manual and informal. Observe and discuss the sticky notes during team \nceremonies; throw them away or promote them to clean automated documentation tasks if that is what the team decides.\nThen start by explaining interactively , using any existing and improvised support: \nbrowsing the source code, searching and visualizing in the IDE, sketching on paper or a whiteboard, or even using PowerPoint or Keynote as a quick drawing pad. (It’s sometimes easier to use a tool when you need a lot of “copy-paste-change a little” kinds of sketches.) Then immediately refactor the key parts of the explanation into a little section of documentation. Y ou know what parts of the explanations are essen-tial from the interactions with your colleagues. If something was difficult to under-stand or surprising or came as an “ Aha!” moment, then it’s probably worth keeping for other people later.\nPeter Hilton has another fantastic trick for write documentation, which he calls \n“reverse just-in-time doc”:\nInstead of writing documentation in advance, you can trick other people into writing \nJIT documentation by asking questions in a chat room (and then pasting their answers into the docs).\n11\nProvoking Just-in-Time Learning Early\nFixing bugs or making small evolutions, from the code to production, is a great way \nto quickly learn about an application and its complete development process. That’s why many companies include bug fixing and minor evolution tasks as part of the immediate onboarding process for newcomers. This creates needs for knowledge, which in itself triggers the need to find sources of knowledge: people, artifacts, whatever.\nSome startups have a policy that you must deliver something into production by \nyourself, with some guidance, within your first two days on the job. This forces you to quickly discover the full process and all colleagues involved, if any . It’s also a mark of trust: Y ou are trusted enough to be allowed to deliver something immediately , for real. It’s also a mark of confidence in the process, the tests, and the deployment \n11.  Peter Hilton, https://www .slideshare.net/pirhilton/documentation-avoidance"",6480",6539,"**Avoiding Traditional Documentation**\n\n**Use of Sketches and Bullet Points**:\n- Use napkin sketches to document design aspects, then turn into maintainable diagrams later\n- Use bullet points to document quality attributes, then turn into executable scenarios\n\n**Throw-Away Documentation**:\n- Some documentation is useful for only a limited period of time and can be deleted\n- Diagrams specific to a problem are useful while working on it, but lose value once done\n- Transient documentation like planning materials (user stories, estimation, tracking) can be thrown away after use\n\n**On-Demand Documentation**:\n- Best documentation is created on demand, in response to actual needs\n- Create documentation to answer precise, purposeful questions\n- Avoid speculating on what should be documented, instead pay attention to unasked but needed knowledge\n\n**Just-In-Time Documentation**:\n- Documentation is best introduced just when it's needed\n- Listen for ""knowledge frustrations"" and fill the resulting ""gap"" in documentation\n\n**Keep It Low-Tech**:\n- Log requests for missing information as sticky notes, then decide as a team to create it\n- Use existing tools (code browsing, IDE, sketching) to explain interactions before writing formal documentation\n- Consider using reverse just-in-time doc by asking questions in a chat room and pasting answers into the docs\n\n**Learning Through Bug Fixing**:\n- Fixing bugs or making small evolutions is a great way to quickly learn about an application and its development process\n- Companies may require newcomers to deliver something into production within their first two days",1615,55.991552114486694
False,3,"141-Including Some Upfront Documentation.pdf,141-Including Some Upfront Documentation,""On-Demand Documentation 285\nautomation strategy . Y ou not onl","141-Including Some Upfront Documentation.pdf,141-Including Some Upfront Documentation,""On-Demand Documentation 285\nautomation strategy . Y ou not only learn the code, you also learn that you can trust \nthe delivery approach and that the typical timeframe of a change is very short. It’s also a great way to get fresh feedback on the process. If the installation and the pre-requisite workstation setup takes two days or more, there’s no way you can deliver something in two days. If someone has to help often during the local developer setup, then you need better documentation at a minimum or, preferably , better automation of the process. The same goes for the full delivery pipeline and any other matter.\nIf you have a weird in-house or proprietary stuff that new joiners have to learn, \nnewcomers will tell you that there is a standard alternative that you could switch to.\nAstonishment Report\nNewcomers’ superpower is to be bring a fresh perspective. An astonishment report is a simple yet effective tool for learning both about what should be documented and what could be improved.\nAsk all newcomers to report all their surprises during their very first days on the \njob. Even if they come from the same company or from a similar background, they might bring fresh perspectives. Suggest that each newcomer keep a notebook and take notes immediately after noticing an astonishment before they forget it. It’s para-mount to preserve the candor, so keep the observation period short, such as two days or a week. Even two days might be long enough to get so acclimated that weird stuff is no longer so weird. Improve based on the remarks.\nIncluding Some Upfront Documentation\nBe the adult you wish you had around when you were a child. Write the documentation \nyou wish you had when you started on this project.\n—@willowbl00 on Twitter\nSometimes an on-demand documentation approach can be supplemented by some \nupfront documentation. The danger is that you might create speculative documenta-tion that may never be useful. The benefit is that obviously essential knowledge becomes available to help people without waiting for the Rule of T wo to trigger.\nImagine yourself as a beginner on the project, knowing nothing. If you remember \nwhat it was like when you joined, it’s simpler. Then create the ideal documentation that you would have loved to find.\nHowever, the curse of knowledge can make this approach mostly ineffective. Y ou \nsimply might not be able to imagine anymore what it’s like not knowing something that you know now .\nChapter 10 Avoiding Traditional Documentation 286\nIt’s extremely hard to guess in advance what information will be useful for other \npeople you don’t know yet, trying to do tasks you can’t predict. Still, there some \nheuristics can help you decide when a piece of knowledge should be documented right now:\n •Everybody agrees it should be documented.\n •It’s a hot topic (controversial).\n •It’s been discussed for a long time, such as during the sprint planning ceremony .\n •There has been a profound or shocking misunderstanding by some people involved.\n •It’s important, and there’s no way it can be guessed or inferred from the code.\n •It should be refactored to avoid the need for documentation, but it’s not practi-cal to do that now . Andy Schneider\n12 has really nice words on improving the \ndocumentation every day , with a focus on empathy: “Make the value you’ve added permanent.”\nThe maxim “Comment code that you are working on so the next person doesn’t \nhave to go through the same pain” does not tell you precisely when or when not to do something documentation related. It’s still up to your judgment. But it brings home the point that it’s all about protecting value for other people.\nTechniques to stimulate on-demand documentation are to define the content of \nthe documentation with the help of a skills matrix or through a knowledge backlog.\nKnowledge Backlog\nFor a knowledge backlog, let each team member write on sticky notes the pieces of knowledge they’d like to have. Then have everyone put their notes on a wall and have them decide by consensus or by voting with dots what should be documented first. This could become your knowledge backlog. Every few weeks or every iteration, you can take one or two items and decide how to address them, whether through pair programming, augmenting the code to make the structure visible in the code itself, or documenting specific knowledge of the area as an evergreen document on the wiki.\nThe knowledge backlog creation session can be done within your retrospective.However, beware of backlogs growing and avoid using an electronic tracker; stick-\ners at the bottom of your whiteboard are enough, and the lack of room will remind you to keep the backlog small.\n12.  Andy Schneider was an attendee of the OOPSLA 2001 Workshop on Software Archeology who wrote \nthe position paper “Software Archaeology: Understanding Large Systems,” https://web.archive.org/\nweb/20081121110405/http://www .visibleworkings.com/archeology/schneider.html"",4991",5084,"**On-Demand Documentation Strategy:**\n* Learn both code and delivery approach for trust and short timeframe\n* Fresh feedback from newcomers improves process\n* Document installation, setup, and pre-requisites for efficient delivery\n* Address ""weird in-house or proprietary stuff"" with alternative solutions\n* Create an ""astonishment report"" of newcomers' observations during first days on the job\n* Write upfront documentation based on beginner's perspective\n* Use heuristics to decide when to document: agreement, controversy, long discussion, profound misunderstanding, importance, and refactoring feasibility\n* Document code as it is being worked on to protect value for other team members.\n\n**Techniques for On-Demand Documentation:**\n* Define content using a skills matrix or knowledge backlog\n* Create a knowledge backlog by having team members write down pieces of knowledge they'd like to have, then decide through consensus or voting which items should be documented first.\n* Conduct regular retrospectives to discuss and address items in the knowledge backlog.\n* Avoid growing backlogs and using electronic trackers; keep them small and on physical sticky notes for reminders.\n\n**Important Concepts:**\n* The curse of knowledge can hinder effective upfront documentation\n* Documentation should be created based on empathy: ""Make the value you've added permanent."" (Andy Schneider)\n* Comment code that you are working on to protect next person from pain.",1461,39.5129497051239
False,3,"142-Interactive Documentation.pdf,142-Interactive Documentation,""Interactive Documentation 287\nSkills Matrix\nAn alternative to creating a knowledge","142-Interactive Documentation.pdf,142-Interactive Documentation,""Interactive Documentation 287\nSkills Matrix\nAn alternative to creating a knowledge backlog is to create a skills matrix with pre-\ndefined areas and ask each team member to declare his or her level of proficiency for each area. One limitation here is that the matrix will reflect the views of the person creating it and will ignore the skills areas ignored or neglected by this person.\nY ou could use a skills matrix as a chart with many quadrants, as described by \nJim Heidema in a blog post.\n13 This is a chart that can be posted in the room to iden-\ntify the skills needed and the people on the team. In the left column you list all the team members. Along the top you list all the various skills you need on the team. Then each person reviews his or her row , looking at each skill, and then identifies how many quadrants of each circle he or she can fill in, based on the range below the chart. The range is from no skills through teaching all skills in each column:\n0: no skill \n1: basic knowledge 2: perform basic tasks 3: perform all tasks (expert) 4: teach all tasks\nWhenever the skills matrix reveals a lack of skills, you need to plan a training or \nimprove the documentation in some way .\nInteractive Documentation\nWritten documents don’t have the opportunity for interaction. As Jukka Korpela \ncomments on Wiio’s laws, whenever a written document “such as a book or a Web page or a newspaper article, miraculously works, it’s because the author participated in dialogues elsewhere.”\n14\nIt takes more work than just typing text for a written document to be useful. \nGeorge Dinwiddie advises in his blog to “document questions the reader may have” and to “get it reviewed by multiple people.”\n15 Written documentation should be like \na record of an interactive conversation that worked, which makes it more likely to work again.\n13.  Jim Heidema, Agile Advice blog, http://www .agileadvice.com/2013/06/18/agilemanagement/leaving-\nyour-title-at-the-scrum-team-room-door-and-pick-up-new-skills/\n14.  Jukka Korpela, http://jkorpela.fi/wiio.html\n15. George Dinwiddie, http://blog.gdinwiddie.com/2010/08/06/the-use-of-documentation/\nChapter 10 Avoiding Traditional Documentation 288\nBut you can also push the limits of written words on paper, thanks to the available \ntechnologies all around us. Y ou can create documentation that is interactive to some \nextent.\nFor example, Gojko Adzic turned a checklist of test heuristics into an additional \nmenu in the browser, as a small assistant called BugMagnet (see Figure 10.6).16 \nClicking on Names and then NULL in the menu directly fills the edit field in \nthe browser with the string """"NULL"""" . This could have remained a plain checklist to \ninput manually into forms, but Gojko took the extra step of making it a little more  interactive. Note the suggestive effect of navigating the menu: It calls for being used, at least more than a printed checklist.\nTherefore: Whenever possible, prefer documentation that is interactive over static \nwritten words. Use hypermedia to navigate to the content through links. Turn the documentation into tools like checkers, tasks assistants, or search engines.\nY ou already know several examples of interactive documentation as it is all \naround: \n •Hypermedia documentation with navigable links, as generated by Javadoc and equivalent systems in other languages.\n •T ools like Pickles that turn Cucumber or SpecFlow reports into interactive websites, or Fitnesse, which has been interactive from the start.\n •T ools like Swagger that document a web API into an interactive website, with built-in capability to directly send requests and show the responses.\n •Y our IDE, which offers a lot of documentation features with a keystroke or a mouse click: call stack, search for type or reference, type hierarchy , find occur-rences, find in the programming language abstract syntax tree, and so on.\n16.  BugMagnet, https://github.com/gojko/bugmagnet\nFigure 10.6 BugMagnet"",3998",4071,"**Interactive Documentation**\n\n**Skills Matrix**:\n- Alternative to creating a knowledge backlog\n- Create a skills matrix with pre-defined areas\n- Ask each team member to declare their level of proficiency for each area\n- Limitations: Reflects the views of the person creating it and ignores neglected skills areas\n\n**Interactive Documentation**:\n- Written documents don't have opportunity for interaction\n- Jukka Korpela: ""Written document works because the author participated in dialogues elsewhere""\n- George Dinwiddie: ""Document questions the reader may have and get it reviewed by multiple people""\n- Written documentation should be like a record of an interactive conversation\n\n**Prefer Interactive Over Static Documentation**:\n- Use hypermedia to navigate content through links\n- Turn documentation into tools like checkers, task assistants, or search engines\n- Examples:\n  - Hypermedia documentation with navigable links (Javadoc)\n  - Tools like Pickles, Fitnesse, Swagger\n  - IDE documentation features (call stack, search for type/reference, etc.)",1055,36.35521340370178
False,3,"143-Declarative Dependency Management.pdf,143-Declarative Dependency Management,""Declarative Automation 289\nAs described in the next section, putting","143-Declarative Dependency Management.pdf,143-Declarative Dependency Management,""Declarative Automation 289\nAs described in the next section, putting documentation into an automated form \nthat is also readable allows for interactive discovery: Y ou can execute and tinker with \nthe automation code (scripts and tests) to understand the topic in more depth as you change it and see the effects.\nDeclarative Automation\nEvery time you automate a software task, you should take the opportunity to make it a form of documentation as well. Software development is increasingly making use of automation in all its aspects. Over the past decades, popular tools have changed the way we work, replacing repetitive manual tasks with automated processes. \n Continuous integration tools automate the building of software from its source, and they automate tests executions, even on remote target machines.\nT ools like Maven, NuGet, and Gradle automate the burden of retrieving all the \nrequired dependencies. T ools like Ansible, Chef, and Puppet declare and automate the configuration of all IT infrastructures.\nThere’s something interesting in this trend: Y ou must describe what you want in \norder to automate it. Y ou declare the process, and then the tool interprets it to do it so that you don’t have to. The good news is that when you declare the process, you are documenting it—not just for the machine but also for humans as you have to maintain it, too.\nTherefore: Whenever you automate a process, take the opportunity to make it \nthe primary form of documentation for this process. Favor tools with a declara-tive style of configuration over tools that rely on a prescriptive style of scripts. Make sure the declarative configuration is meant primarily for a human audi-ence, not only for the tool.\nThe goal is for the declarative configuration to be the single source of truth for the \nprocess. This is a great example of documentation that is both documentation for humans and documentation for machines.\nWhat did we do before all the new automation tools? In the worst case, the pro-\ncess was done manually by someone with tacit knowledge of how to do it. When he or she was away , there was no way to do it at all. When we were a little luckier, there was a Microsoft Word document describing the process in a mix of text and command lines. However, the few times you tried to use it, you could hardly suc-ceed without asking questions of the author: Some parts were missing and other were obsolete, with wrong indications. It was a manual process with deceiving docu-mentation. When we were really lucky , there was a script to automate the process. \nChapter 10 Avoiding Traditional Documentation 290\nHowever, when it was throwing errors, we again had to ask the author for help to fix \nit, as the script code was quite obscure. And there was a separate Microsoft Word document, incomplete and obsolete, pretending to describe the process to please the management. It was an automated process, but still with no useful documentation.\nNow we know better, and the key words to fix all the earlier problems are declara-\ntive and automation.\nDeclarative Style\nFor an artifact to be considered documentation, it must be expressive and easy to understand by people. It should also explain the intentions and the high-level deci-sions, not just the details of how to make it happen.\nImperative scripts that prescribe, step by step, what to do fail for any nontrivial \nautomation. They only focus on the how, whereas all the interesting decisions and reflections that led to the how can only be expressed through comments, if at all.\nOn the other hand, declarative tools are more successful at supporting nice docu-\nmentation, thanks to two factors:\n •They already know how to do a lot of typical low-level things, which have been codified well by dedicated developers into reusable ready-made modules. This is an abstraction layer.\n •They offer a declarative domain-specific language on top, which is at the same time more concise and expressive. This DSL is standard and is itself well  documented, which makes it more accessible than your in-house  scripting lan-guage. This DSL usually describes the desired state in a stateless and  idempotent  fashion; by moving the current state out of the picture, the  explanations become much simpler.\nAutomation\nAutomation is essential to force declared knowledge to be honest. With modern approaches to automation, you tend to run the process very often, even continuously or dozens of times per hour. There is good pressure to keep it reliable and always up-to-date. Y ou have to be smart to reduce the maintenance. Automation you rely on therefore acts as a reconciliation mechanism that makes it obvious when the declared process becomes wrong.\nThere has been a revolution, or perhaps an evolution. At last you can have knowl-\nedge that is up-to-date and that really explains what you want, the way you would talk about it. T ools are getting closer to serving the way we think, and that’s chang-ing the game in many aspects, in particular with respect to documentation.\nDeclarative Automation 291\nDeclarative Dependency Management\nIn the build automation landscape, dependency managers, also known as package \nmanagers, are tools that play a key role in the build process. They reliably download libraries, including their transitive dependencies, resolve many of the conflicts, and support your dependency management strategy , even across many modules.\nBefore that automation, dependency management was a chore done manually . \nY ou would manually download the libraries in some version into a /lib folder, later stored in the source control system. If the dependency had dependencies, you had to look at a website and download all of those, too. And you had to redo all of this whenever you had to switch to a new version of a dependency . It was not fun.\nPopular dependency managers are available for most programming languages: \nApache Maven and Apache Ivy (Java), Gradle (Groovy and JVM), NuGet (.Net), RubyGems (Ruby), sbt (Scala), npm (Node.js), Leiningen (Clojure), Bower (web), and many others.\nT o do their job of automating, these tools need you to declare all the direct \ndependencies you expect. Y ou usually do this in a simple text file, often called a  manifest. This manifest is the bill of materials that dictates what to retrieve in order \nto build your application.\nWhen using Maven, the declaration is done in an XML manifest called pom.xml:\n1  <dependency>\n2  <groupId>com.google.guava</groupId>3  <artifactId>guava</artifactId>4  <version>18.0</version>5  </dependency>\nIn Leiningen, the declaration is done in Clojure:\n1  [com.google.guava/guava """"19.0-rc1""""]\nWhatever the syntax, the declaration of the expected dependencies always hap-\npens in a tuple of the three values: group ID, artifact ID, and requested version.\nIn some of the tools, the requested version can be not only a version number, \nlike 18.0 , but a range such as [15.0,18.0)  (meaning from version 15.0 to ver-\nsion  18.0 exclusive), or a special keyword such as LATEST, RELEASE, SNAPHOT, \nALPHA,  or BETA . Y ou can see from these concepts of range and keywords that \nthe tools have learned to work at the same level of abstraction we think at as \n developers. The syntax to express the necessary dependencies is declarative, and this is a good thing.\nChapter 10 Avoiding Traditional Documentation 292\nWith declarative automation, the declaration of the requested dependencies \nis also the single source of truth for the documentation of the dependencies. The \nknowledge is already there, in the dependency manifest. As a consequence, there is no need to list these dependencies again in another document or in a wiki. If you made such a list, you would just risk forgetting to update it.\nBut, as usual, there’s one thing missing so far in the declaration of the depend-\nencies: Y ou’d like to declare not just what you request from the tool but also the corresponding rationale. Y ou need to record the rationale so that future newcomers can quickly grasp the reason behind each dependency included. Adding one more dependency should never be done too easily , so it’s good to always be able to justify a dependency with a convincing reason. One way to do this is with comments next to the dependency entries in the file:\n1  <dependencies>\n2    <!-- Rationale: A very lightweight alternative to JDBC, with no magic -->3    <dependency>4        <groupId>org.jdbi</groupId>5        <artifactId>jdbi</artifactId>6        <version>2.63</version>7    </dependency>8  <dependencies/>\nY ou might be tempted to add a description, but you don’t have to because it’s \nalready included in the POM of the dependency itself. In an IDE such as Eclipse, it’s \nvery easy to navigate to the POM of the dependency by pressing Ctrl (or Cmd on a Mac). As your mouse hovers over the dependency element in your POM, it turns into a link that allows you to directly jump to the POM of the dependency , as shown in Figure 10.7. That’s integrated documentation mixed with declarative automation. Pure awesomeness!\n \nIs that knowledge on the dependencies and their version accessible? It depends on \nthe audience. For developers, the most accessible way is to look at the manifest and use the IDE, so there’s no need to do anything more. One issue may be that when \nFigure 10.7 Navigating the Maven dependencies in the Eclipse POM Editor"",9441",9530,"**Declarative Dependency Management**\n\n**Automation**:\n- Automated processes are more reliable and efficient than manual methods\n- Automated processes can be documented, providing single source of truth for the process\n- Declarative automation is more successful at supporting documentation than imperative scripts\n- Automation acts as a reconciliation mechanism that makes it obvious when the declared process becomes wrong\n\n**Dependency Management**:\n- Dependency managers (package managers) are tools that play a key role in the build process\n- They reliably download libraries, including transitive dependencies, resolve conflicts, and support dependency management strategies\n- Declarative automation is essential to force declared knowledge to be honest\n\n**Declarative Dependency Manifest**:\n- Dependencies are declared in a simple text file using a tuple of group ID, artifact ID, and requested version\n- This manifest is the bill of materials that dictates what to retrieve to build the application\n- Declarative automation allows the declaration of dependencies to be the single source of truth for documentation\n- Adding comments next to dependency entries in the file can provide rationale behind each dependency\n\n**Integrated Documentation**:\n- IDEs allow developers to navigate to the POM (Project Object Model) of dependencies, providing integrated documentation",1376,46.63526892662048
False,3,"144-Declarative Configuration Management.pdf,144-Declarative Configuration Management,""Declarative Automation 293\nusing ranges or keywords for the ve","144-Declarative Configuration Management.pdf,144-Declarative Configuration Management,""Declarative Automation 293\nusing ranges or keywords for the versions, you don’t know the exact version being \nretrieved at a given point in time just by looking at the manifest. However, developers know how to query the dependency manager to get this information on demand. For example, in Maven they would run the following:\nmvn dependency:tree -Dverbose\nFor nondevelopers, you would want to extract and publish the interesting content \ninto an Excel document or to the wiki. But are nondevelopers really very interested in that kind of knowledge?\nDeclarative Configuration Management\nSorry this is taking so long, I lost my bash history and therefore have no idea how we \nfixed this last time. \n—@honest status page on Twitter \nConfiguration management is much more complex than dependency management. \nIt involves resources such as applications, daemons, and files, each with many attrib-utes and with all their dependencies. However, some tools have taken a declarative approach similar to that of the dependency managers and their manifests. When using these tools, you are not supposed to use the command line much, as opposed to the situation in Figure 10.7.\nThe most popular tools for managing configuration are Ansible, Puppet, \n CfEngine, Chef, and Salt. However, some of them are imperative (Chef), while others are declarative (Puppet and Ansible). \nFor example, Ansible states that it “uses a very simple language […] that allow \nyou to describe your automation jobs in a way that approaches plain English,”\n17 \nwhich is typical of a declarative approach, as explained on the Big Panda blog:\nAnsible’s philosophy is that playbooks (whether for server provisioning, server orches-\ntration or application deployment) should be declarative. This means that writing a playbook does not require any knowledge of the current state of the server, only its desirable state.\n18\n17.  https://www .ansible.com/overview/how-ansible-works\n18.  https://www .bigpanda.io/blog/5-reasons-we-love-using-ansible-for-continuous-delivery/\nChapter 10 Avoiding Traditional Documentation 294\nPuppet has a similar philosophy . Here’s an excerpt of a Puppet manifest for man-\naging NTP:\n1  # Some comment if necessary...\n2  service { 'ntp':3    name      => $service_name,4    ensure     => running,5    enable    => true,6    subscribe => File['ntp.conf'], 7  }89  file { 'ntp.conf':10   path    => '/etc/ntp.conf',11   ensure  => file,12   require => Package['ntp'],13   source  =>  """"puppet:///modules/ntp/ntp.conf"""", 14 }\nPuppet emphasizes that its manifests are self-documented and provide proof of \ncompliance even for many regulatory bodies:\nSelf-documentation\nPuppet manifests are so simple, anyone can read and understand them, including  people \noutside your IT and engineering departments.\nAuditability\nWhether it’s an external or internal audit, it’s great to have proof that you pass. And you can easily validate to your own executives that compliance requirements have been met.\n19\nA declarative language like the ones used in these tools allows you to communi-\ncate the expected desired state not only to the too, but to the other humans on your \nteam or to external auditors.\nAgain, what’s often missing to make these manifests complete and useful docu-\nmentation for humans is the rationale for each decision. If you consider that a Pup-pet manifest as-is is accessible to all the interested audience, then it would make sense to document the rationales and other high-level information into the manifest—for example, with comments.\n19.  Puppet blog, https://puppetlabs.com/blog/puppets-declarative-language-modeling-instead-of-scripting\nDeclarative Automation 295\nBecause the knowledge about the configuration is declared in a formal way for \nthe tools, it also becomes possible to generate living diagrams when it can help rea-\nsoning. For example, Puppet includes a graph option that generates a .dot file of a \ndiagram showing all the dependencies. This is useful when you experience an issue in the dependencies or when you want to have a more visual view of what’s in the manifests. \nFigure 10.8 shows an example of a diagram generated from Puppet.\n20 \n20.  John Arundel, http://bitfieldconsulting.com/puppet-dependency-graphsRelationships\nFile[/root/nginx-upstream-fair.tar.gz] File[/root/nginx-0.6.39.tar.gz]\nExec[unpack-nginx-upstream-fair] Exec[unpack-nginx]\nFile[/tmp/enter.txt]\nFile[/opt/nginx/conf.d]\nFile[/etc/init.d/nginx]\nExec[nginx reload] Service[ngix]File[/etc/logrotate.d/nginx]\nFile[/opt/nginx/nginx.conf]File[/opt/nginx/conf.ssl]Package[passenger]\nExec[passenger-install-nginx-module]\nFile[/opt/nginx/conf/ssl/example.com.key]\nFile[/opt/nginx/conf/ssl/example.com.crt]\nFile[/opt/nginx/conf.d/example_production.conf]\nFigure 10.8 A diagram of  resources dependencies, generated by Puppet"",4871",4966,"**Declarative Configuration Management**\n\n**Declarative Approach**:\n- Developers can query dependency manager to get information on demand\n- For non-developers, interesting content can be extracted and published\n- Declarative approach used in tools like Ansible and Puppet\n\n**Ansible**:\n- Uses simple language for writing playbooks\n- Aims at describing automation jobs in plain English\n- Playbooks describe desirable state of server without requiring knowledge of current state\n\n**Puppet**:\n- Manifests are self-documented and provide proof of compliance\n- Provides a diagram of dependencies for visual understanding\n\n**Declarative Languages**:\n- Allow communication of expected desired state to tools and humans\n- Missing rationale for decisions need to be documented in manifests\n\n**Puppet Manifests**:\n- Examples show dependencies and files involved in configuration management\n\n**Relationships and Dependencies**:\n- Puppet generates a diagram showing relationships and dependencies\n- Useful for understanding issues or visualizing what's in the manifests.",1059,60.12060213088989
False,3,"145-Declarative Automated Deployment.pdf,145-Declarative Automated Deployment,""Chapter 10 Avoiding Traditional Documentation 296\nThis kind of diagram","145-Declarative Automated Deployment.pdf,145-Declarative Automated Deployment,""Chapter 10 Avoiding Traditional Documentation 296\nThis kind of diagram can be handy for refactoring the manifests to make them \ncleaner, simpler, and more modular. As John Arundel writes in his blog describing \nthis feature of Puppet:\nAs you develop Puppet manifests, from time to time you need to refactor them to make \nthem cleaner, simpler, smaller and more modular, and looking at a diagram can be very helpful with this process. For one thing, it can help make it clear that some refactoring is needed.\n21\nDeclarative Automated Deployment\nMuch as with configuration management, a number of tools can automate your \ndeployment, including the necessary company workflows and rollback procedures, and that can deploy only what needs to be changed. Some of these tools include  Jenkins with custom or standard plugins and Octopus Deploy (.Net).\nHere’s an example of a deployment workflow from the Octopus website:\n22\n •Redirect load balancer to a “down for maintenance” site\n •Remove web servers from load balancer\n •Stop application servers\n •Back up and upgrade the database\n •Start application servers\n •Add web servers back to load balancer\nIn a tool like this, the deployment and release workflow is typically set up by \nclicking on the UI, and it is persisted in a database. Still, the workflow is described in a declarative manner that everyone can understand when looking at the tool screens. Whenever you want to know how it’s done, you just have to look it up in the tool.\nBecause it’s declarative and because the tool knows about the basics of deploy-\nment, it is possible to describe complex workflows in a concise way , closer to the way we think about it. For example, it is possible to apply standard continuous delivery patterns such as canary releases and blue-green deployment. Octopus Deploy man-ages that with a concept call Lifecycle, an abstraction that is useful for easily taking care of this kind of strategy .\n21.  John Arundel, http://bitfieldconsulting.com/puppet-dependency-graphs\n22.  https://octopus.com/blog/octopus-vs-puppet-chef\nDeclarative Automation 297\nT ools can not only automate the work itself and reduce the likelihood of errors \nbut also provide ready-made documentation for the standard patterns you could, or \nshould, be using. This is therefore more documentation you don’t have to write by yourself!\nSay that you decide to adopt a blue-green deployment for your application. Y ou \ncan configure the tool to take care of it, and this is all you have to do now:\n •Declare in a stable document such as a README file that you have decided to do blue-green deployments.\n •Link to authoritative literature on the topic, such as the pattern on Martin Fowler’s website.\n23\n •Configure the tool and the lifecycle to support the pattern.\n •Link to the page on the tool website that describes how the pattern is taken care of specifically in the tool.\nThe following is a description of the pattern in the context of the tool:\nStaging: when blue is active, green becomes the staging environment for the next \ndeployment.\nRollback: we deploy to blue and make it active. Then a problem is discovered. Since \ngreen still runs the old code, we can roll back easily .\nDisaster recovery: after deploying to blue and we’re satisfied that it is stable, we can \ndeploy the new release to green too. This gives us a standby environment ready in case \nof disaster.24\nFor an automation to be a case of declarative automation that provides documen-\ntation, the configuration of the tool has to be genuinely declarative, whether in text \nor on a screen and in a database. It also has to be at an abstraction level close to what matters for everyone involved. It cannot be obscure imperative steps with a lot of conditionals based on low-level details such as the absence of a file or the state of an operating system process.\nScaffolding a Step-by-Step Guide\nWhenever you join a new team or a new project, you need to set up your work envi-ronment, and you need some documentation for that—at least this is how it still goes in many companies. There may be a Newcomers page on the wiki with a long list of \n23.  Martin Fowler, ThoughtWorks, http://martinfowler.com/bliki/BlueGreenDeployment.html\n24.  Octopus Deploy , http://docs.octopusdeploy .com/display/OD/Blue-green+deployments\nChapter 10 Avoiding Traditional Documentation 298\nsteps to go through in order to start working on an application. Such a list is often \nnot totally up-to-date. Links may be broken. Essential information may be missing because it was obvious in the mind of the author. Such issues exist even when new-comers join regularly .\nSome teams have taken a further step, providing an installer for newcomers. Y ou \nrun the installer, it prompts for some specific questions, and you’re done! These installers don’t always work very well when they’re custom in-house scripts, but the idea is there: Why document in text what could be automated, and documented, as a tool?\nThis approach, often called scaffolding, is not just for newcomers, but also allows \nusers to start an application quickly . Ruby on Rails is probably the most popular tool for this approach.\nMany tools can be used to do scaffolding. Y ou can do scaffolding with custom \nscripts, Maven archetypes, Spring Roo, JHipster, and many others. Configuration management tools can sometimes also be used to create a working setup for new team members or to set up templates of applications that can be modified later. \nIf the resulting automation is rock solid, documentation of what it does is less of \nan issue, but in general I favor standard tools over in-house scripts, and I definitely would choose tools that are themselves well documented and maintained and that have a declarative configuration that can be considered itself as the documentation.\nThe scaffolding has to be really easy to use without a user guide. It should ask \nsimple questions, guide the user step by step, provide sensible default values, and have very good examples of answers.\nThere is an open-source tool for scaffolding called JHipster.\n25 It works with a \ncommand-line wizard, and here are some of the questions prompted when creating a new application from scratch:\n •What is the base name of your application?\n •Do you want to use Java 8?\n •Which type of authentication would you like to use?\n •Which type of database would you like to use?\n •Which production database would you like to use?\n •Do you want to use Hibernate second-level cache?\n •Do you want to use clustered HTTP sessions?\n •Do you want to use WebSockets?\n25.  JHipster, http://drissamri.be/blog/technology/starting-modern-java-project-with-jhipster/"",6734",6819,"**Declarative Automated Deployment**\n\n**Using Diagrams for Refactoring Manifests**:\n- Diagrams can help make manifests cleaner, simpler, and more modular\n- They can help identify when refactoring is needed\n\n**Automated Deployment Tools**:\n- A number of tools can automate deployment workflows and rollback procedures\n- Examples: **Jenkins** with custom or standard plugins, **Octopus Deploy (.Net)**\n- Deployment workflows are typically described in a declarative manner\n\n**Declarative Automation**:\n- Tools can automate the work itself and reduce errors\n- They provide ready-made documentation for standard patterns like **Blue-Green Deployment**\n- Configuration of the tool should be genuinely declarative, at an abstraction level close to what matters for everyone involved\n\n**Scaffolding**:\n- Tools can provide a ""plug-and-play"" setup for newcomers, such as installers or Maven archetypes\n- This approach is not just for newcomers, but also allows users to start an application quickly\n- Examples: **Ruby on Rails**, **Spring Roo**, **JHipster**",1049,52.366589069366455
False,3,"146-Machine Documentation.pdf,146-Machine Documentation,,0","146-Machine Documentation.pdf,146-Machine Documentation,,0",58,"**146-Machine Documentation**\n\n**Title:** Machine Documentation\n\n**Content:**\n- Discusses machine documentation\n- Provides examples: ""146-Machine Documentation""\n- Emphasizes the importance of clear and comprehensive documentation for machines, including details about their operation, maintenance, and troubleshooting.\n\n**Keywords:** machine documentation, 146-Machine Documentation, clear documentation, comprehensive documentation, operation, maintenance, troubleshooting.",474,11.808579921722412
False,3,"147-Enforcement or Encouragement.pdf,147-Enforcement or Encouragement,""Declarative Automation 299\n •Would you like to use Maven or Gradle?\n •Would y","147-Enforcement or Encouragement.pdf,147-Enforcement or Encouragement,""Declarative Automation 299\n •Would you like to use Maven or Gradle?\n •Would you like to use Grunt or Gulp.js for building the front end?\n •Would you like to use the Compass CSS Authoring Framework?\n •Would you like to enable translation support with Angular T ranslate?\nFor each question, there is a clear narrative explaining the possible answers and \nthe consequences to help make the decision. This is also inline, tailored help. The \nresulting code is the result of all the decisions. If you’ve chosen MySQL as the data-base, then you have a MySQL database setup.\nIt would be interesting to record the responses to all the questions of the wizard \ninto a file (they’re only kept as logs or in the console) to provide a high-level technical overview of the application. It might be included in the README file, for example.\nA wizard should design helpful exceptions that precisely tell you what, how , and \nwhere to fix a problem that is thrown.\nMachine Documentation\nBefore the cloud, we had to know our machines one by one, and there was often an Excel spreadsheet somewhere with a list of machines and their main attributes. This list was often obsolete.\nNow that machines are moving somewhere in the cloud, we can no longer afford \nto keep a spreadsheet, as the information changes much too frequently , sometimes many times a day . But because the cloud itself is automated, very accurate documen-tation now comes for free, through the cloud API.\nThe cloud API is similar to declarative automation. Y ou declare what you want, \nsuch as “I want a Linux server with Apache,” and then you can query your current inventory of machines available and all their attributes. Many of these attributes are tags and metadata that add a higher level of information to the picture: Perhaps it’s not a “2.6GHz Intel Xeon E5,” but it’s a “High-CPU machine,” for example.\nRemarks on Automation in General\nDon’t do the same thing twice. If it’s déjà -vu, then it’s time to automate. \n—W oody Zuill, in a conversation\nPeople are good for novel stuff. Machines are good for repeated stuff. Automation provides benefits—but at a cost. It is not an end in itself but a mean to save time and to improve reliability on repeated tasks. But there is always a point where the cost exceeds the benefits. Y ou should invest in automation as long as the cost is low com-pared to the recurring benefits. \nChapter 10 Avoiding Traditional Documentation 300\nOn the other hand, if a task is new or different each time, you should wait until \nyou see enough repetition somewhere in the task before thinking about automation. \nDecide what to keep manual.\nEnforced Guidelines\nThe best documentation does not even have to be read, if it can alert you at the right time with the right piece of knowledge.\nMaking information available is not enough. Nobody can read and remember all \nthe possible knowledge ahead of time. And there is a lot of knowledge that you’d need without having any way to figure out that you need it.\n Note\nY ou don’t even know that you don’t know something that you should know .  \nConsider code guidelines. Many companies and teams spend time writing guide-\nlines, but the resulting documents are seldom read and often ignored.\nHow do you document all the decisions that have been made and that everybody \nshould conform to when doing their work? Examples of these decisions include the main architectural decisions, coding guidelines, and other decisions about style and team preferences.\nA common approach is to spend time writing these decisions into a guideline or \nstyle book. The problem is that these decisions quickly add up to more pages than you expected, and a 12-page-long document full of “you shall do this” and “you shall not do that” is far from an exciting read. As a consequence, most of these documents are like legal documents: They are so boring that most team members never read them—even once. They pretend to have read them on arrival, but in fact they hardly went further than the second or third page.\nEven when they’ve actually read them, the format as a list of rules is not memora-\nble, and unless you like all the rules, you won’t remember most of them. In practice, these guides are useful as a reference in case of doubt, and not much more.\nHowever, without guidelines, code is at the mercy of everybody’s own style, pref-\nerences or lack of skills. A consistent set of shared guidelines is essential to really work in a collective ownership fashion.\nSo how do people learn about all the decisions and style they have to conform to \nfor real? They learn all that by reading other people’s code, through code reviews, and through feedback from static analysis tools that catch rule violations.\nReading code works well when the code is exemplary , which is not always the case. \nOf course, code review and static analysis help improve that. Code review works \nEnforced Guidelines 301\nwell as long as the reviewers have all the decisions and style preferences in mind and \nall agree with them. Static analysis works well for every rule or decision that doesn’t need nuance or contextual interpretation. And because static analysis tools must be configured to be useful, once configured, they are themselves naturally the reference documentation about all the guidelines.\nTherefore: Use a mechanism to enforce the decisions that have been made into \nguidelines. Use tools to detect violations and provide instant feedback on viola-tions as visible alerts. Don’t waste time writing guideline documents that nobody reads. Instead, make the enforcement mechanism self-descriptive enough so that it can be used as the reference documentation of the guidelines.\nCode analysis tools help maintain a high level of quality anywhere in the code, \nwhich in turn helps the code to be exemplary . And it also helps as a reference when the programmers are hesitant about a rule during a code review or while pair programming.\nThe point of enforced guidelines is to accept that documentation does not even \nhave to be read to be useful. The best documentation brings you the right piece of knowledge at the right time—when you need it. Enforcing rules, properties, and decisions through tools (or code reviews) is a way to teach the team members the knowledge they need precisely at the moment they ignore it.\n Note\nEnforced guidelines provide persistent knowledge made interactive again.  \nSome Examples of Rules\nCosmetic rules help with code consistency and when merging code. The following are examples:\n •Curly brackets must not be omitted.\n •Field names must not use Hungarian notation.\nRules on metrics help discourage overly complicated code. The following are \nsome examples:\n •Avoid deep inheritance trees (max = 5).\n •Avoid complex methods (max = 13).\n •Avoid overly long lines (max = 120 chars).\nChapter 10 Avoiding Traditional Documentation 302\nRules provide a way to encourage or enforce better code. The following are some \nexamples:\n •Do not destroy the stack trace.\n •Exceptions should be public.\nSome rules can directly avoid bugs:\n •ImplementEqualsAndGetHashCodeInPairRule\n •Test for NaN correctly .\nEven some architectural decisions can be made as rules. Consider these examples:\n •DomainModelElementsMustNotDependOnInfrastructure\n •ValueObjectMustNotDependOnServices\nThen you can add some gamification on top of the rules, as illustrated in \nFigure 10.9. \nEvolving the Guidelines\nGuidelines have a purpose, such as helping a team work together, reducing issues like \nbugs or errors when merging code, and preserving quality attributes such as perfor-mance and maintainability . There’s no such a thing as an ideal and definitive set of guidelines. Instead, you should start with some guidelines, use them, and evolve them to make them as relevant as possible.You declared a\nHexagonal Architecture.\nBut I‛ve found a rogue\ndependency from your\ndomain to MongoDB!\nIt‛s $25. Or a\ncoffee for everyone\non the team!\nEnforced Rule\nFigure 10.9 The guidelines enforcement force"",8085",8162,"**Declarative Automation**\n- Provides clear narrative explaining possible answers for questions related to project setup\n  - Examples: Maven vs Gradle, Grunt or Gulp.js, Compass CSS Authoring Framework, Angular Translate\n- Helps make decisions on software tools and configurations\n- Consequences outlined to aid in decision making\n- Results in automated code setup based on chosen options\n\n**Machine Documentation**\n- Cloud automation provides accurate documentation through API\n  - Replaces Excel spreadsheets of machine information\n- Cloud API is similar to declarative automation, allowing users to declare desired machine specifications\n- Provides high-level technical overview of application through recorded responses to wizard questions\n\n**Automation in General**\n- Automation provides benefits but also cost\n  - Not an end goal, but a means to save time and improve reliability on repeated tasks\n- People are good at novel tasks; machines excel at repetition\n\n**Enforced Guidelines**\n- Best documentation is not always read\n- Enforced guidelines bring right piece of knowledge at right time\n  - No need to remember all decisions or style preferences\n- Code reviews and static analysis help maintain high level of quality, serving as reference for rules\n\n**Examples of Rules**\n- Cosmetic rules: curly brackets, field names\n- Metrics rules: deep inheritance trees, complex methods, overly long lines\n- Encouraging or enforcing better code: do not destroy stack trace, exceptions public\n\n**Architectural Decisions as Rules**\n- DomainModelElementsMustNotDependOnInfrastructure\n- ValueObjectMustNotDependOnServices\n\n**Evolving Guidelines**\n- No ideal and definitive set of guidelines\n- Start with some, use, and evolve to make them relevant",1743,59.09041666984558
False,3,"148-Trust-First Culture.pdf,148-Trust-First Culture,""Enforced Guidelines 303\nThe best guidelines don’t come from above. The best guidelines grow from","148-Trust-First Culture.pdf,148-Trust-First Culture,""Enforced Guidelines 303\nThe best guidelines don’t come from above. The best guidelines grow from the \nteam or teams doing work and talking to each other to agree on shared guidelines \nthat are useful. Don’t hesitate to change the guidelines when necessary . Of course, you may not want to change the length of lines of code every day .\nThe following is a list of sample guidelines for a greenfield project:\n •Unit test coverage >80%\n •Complexity by method <5\n •LOC by method < 25\n •Inheritance depth < 5\n •Number of args < 5\n •Number of member data fields < 5\n •All base Checkstyle rules\nEnforcement or Encouragement\nOn a greenfield project, you typically start with a lot of enforced guidelines in a strict fashion, and every new line of code that violates them has its commit rejected. Other the other hand, on a legacy project, you usually can’t do this because the existing code likely already contains thousands of violations, even for a small module. Instead, you might choose to enforce only the few most important guidelines and make every other guideline a warning. Another approach is to have stricter rules only for new lines of code.\nSome teams start with some guidelines, and when they are comfortable with them, \nthey add more rules and make the existing guidelines stricter in order to progress.\nWhen your company requires every application to follow a minimum set of guide-\nlines, each team or application can decide to make it stricter but not weaker. T ools like Sonar provide inheritance between sets of guidelines, called quality profiles, to help do that. Y ou can define a profile that extends the company profile and add more rules or make the existing rules stricter to suit your own taste.\nDeclarative Guidelines\nBecause sets of guidelines, or quality profiles, can be named, their names are also part of the documentation on guidelines. Y ou can simply refer new joiners to the build configuration, where they will find the name of the set of guidelines. From there, they can look it up on the tool and find out that it extends the company sets of guidelines. They can browse the rules by category or severity and check parameters as they wish, in an interactive fashion. There’s even a search engine.\nChapter 10 Avoiding Traditional Documentation 304\nEach given rule has a key , a title, and a brief description of what it is and why . \nWith the key or the title, you can look up more complete documentation on the tool \nor directly on the web.\nFor example, if you look up ImplementEqualsAndGetHashCodeInPairRule  \non the web, you immediately find its reference documentation, from the Gendarme plugin for .Net:\nThis rule checks for types that either override the Equals(object) method without \noverriding GetHashCode() or override GetHashCode without overriding Equals. \nIn order to work correctly types should always override these together.26\nSuch reference documentation usually includes several code samples, a bad exam-\nple, and a good example to illustrate the point of the rule. This is great, because the \ndocumentation is already there. Why write it again when it has been already done well by someone else?\nA Matter of Tools\nCompilers, code coverage, static code analysis tools, bug detectors, duplication detectors, and dependency checkers are common examples of tools for setting up enforced guidelines in practice.\nSonar is a popular tool that itself relies on many plugins to actually do its job. \nWhen the configuration of tools is not meant to be documentation with verbose XML and rules identifiers, tools like Sonar can make the configurations of cod-ing rules more accessible in a convenient UI, to the point of becoming the reference about guidelines.\nEven when plugins are actually configured via an XML file, Sonar displays the list \nof coding rules nicely onscreen, and you can modify them there, along with the ref-erence description in prose. This information can also be exported in a spreadsheet format. If you really want to spend time documenting coding guidelines manually , just give the overall intentions, priorities, and preferences and let the tools provide the details!\nOther guidelines may be enforced by access control. Say that you have decided \nthat a legacy component is frozen from now on, and nobody has the right to commit on it. Y ou can simply revoke the write grants to everyone. But this in itself does not \n26.   https://www .mono-project.com/docs/tools+libraries/tools/gendarme/rules/design/#implementequal\nsandgethashcodeinpairrule\nEnforced Guidelines 305\nexplain why you’re making the decision. Y ou should therefore expect questions, and \nthe knowledge transfer will happen as a conversation.\nMost automated means are not 100% relevant at any time, so sometimes enforce-\nment will be violated. This is not necessarily a disaster, as long as the enforcement maintains enough continuous awareness about the guidelines.\nIf an element of the guideline is not enforceable, then perhaps it is not really an \nelement of a guideline. Y ou might want to add it to a short checklist for manual code review or review during pair programming. But it is not an enforced guideline any longer.\nHowever, if you have new rules, you may consider extending the existing tools \nwith a new rule or new plugin. Compilers often have extension points where you can hook your own additional rules. T ools like Sonar are extensible with custom plugins, and checkers are extensible with new rules, sometimes with XML and sometime only with code.\nGuidelines or Design Documentation?\nImagine that your set of guidelines for the domain model is as follows:\n •Functional first (immutable and side effect free by default)\n •Null free\n •No framework pollution\n •No SQL\n •No direct use of a logging framework\n •No import of any infrastructure technology\nAt the time of this writing, existing static analysis tools and plugins don’t support \nall this out of the box, so you can’t do enforced guidelines unless you create your own tool. However, these guidelines are design decisions that can be documented in the code itself, perhaps by using annotations, as discussed in Chapter 4, “Knowledge Augmentation.”\nIn fact, such design declarations expressed as annotations make it possible to \nenforce your coding standards and other guidelines with analysis tools. Once you declare that your code should be all immutable in a given package, it becomes pos-sible to check the main violations using a parser. \nImmutability and null-free expectations can be enforced programmatically . This \nis far from perfect, but it is enough for any new joiner to learn the style after a few commits.\nChapter 10 Avoiding Traditional Documentation 306\nWarranty Sticker Void if Tampered With\nHamcrest27 is a popular open-source project that provides matchers to write beauti-\nful unit tests. It provides a lot of matchers out of the box, and you can also extend it \nwith your own custom matchers. Usually when you do that you should read the developer’s guide, but not everyone does that. Therefore, Hamcrest uses naming in a creative way to make it very unlikely to break a design decision by ignorance:\n1  /**\n2  * This method simply acts a friendly reminder not to implement\3  * Matcher directly and instead extend BaseMatcher. It's easy4  * to ignore JavaDoc, but a bit harder to ignore compile errors 5  *6  * 7  *8  * @see Matcher for reasons why.9  * @see BaseMatcher10 * @deprecated to make11 */12   @Deprecated 13 _ void _dont_implement_Matcher___instead_extend BaseMatcher_();\nThe Hamcrest Matcher  method doesn’t implement a matcher but extends Base \nMatcher , which is an impossible-to-miss and otherwise useless documentation \nmethod. Y ou can still break it deliberately , but the point is that you’re aware of doing \nthat. It’s a kind of “warranty void if tampered” sticker. This is an original way to do unavoidable documentation.\nThe funny things is that in this example of enforced guidelines, the enforcement is \ndone by the potential violator himself or herself.\nThe following are some more similar examples:\n •Documentation by exception: Say that you decide to turn a legacy compo-nent from read-write to read-only . Y ou can document this with text or annota-tions, but how can you make sure nobody will add write behavior? One way is to keep the write methods on all the data access objects but have them throw exceptions is to use \nIllegalAccessException(""""The component is now \nREAD-ONLY"""") .\n27.  Hamcrest, http://hamcrest.org"",8558",8619,"**Best Practices for Setting Up Enforced Guidelines:**\n* **Agree on shared guidelines**: teams agree on useful guidelines through discussion\n* **Change guidelines when necessary**: adapt to project needs\n* **Start with enforced guidelines (greenfield projects)**: enforce strict adherence for new code\n* **Gradual addition of guidelines (legacy projects)**: begin with important guidelines and add more over time\n* **Use tools for setting up enforced guidelines**: use compilers, static code analysis tools, bug detectors, duplication detectors, dependency checkers, etc.\n* **Documentation through comments or annotations**: document intentions, priorities, preferences in code\n* **Enforce access control**: revoke write grants to prevent modifications on frozen components\n* **Extend existing tools with new rules**: create custom plugins for specific guidelines\n* **Guidelines as design documentation**: express design decisions through annotations and parser checks\n* **Hamcrest's creative approach**: use naming conventions and deprecated methods as reminders.\n\n**Examples:**\n- Documentation by exception: turning a legacy component from read-write to read-only by throwing exceptions.\n- Warranty sticker effect: enforcing guidelines through potential violators' awareness, e.g., documenting unused or deprecated methods with clear messages.",1345,48.2682421207428
False,3,"149-Constrained Behavior.pdf,149-Constrained Behavior,,0","149-Constrained Behavior.pdf,149-Constrained Behavior,,0",56,"**Constrained Behavior**:\n\n**Keywords:** Constrained, Behavior\n\n**Notes**:\n\n* Definition: Actions or patterns of behavior limited by external factors.\n* Example: When a person's actions are restricted due to societal norms or expectations.\n* Opposite: Unconstrained/unrestricted behavior.",288,28.53076434135437
False,3,"150-Replaceability First.pdf,150-Replaceability First,""Constrained Behavior 307\n •License mechanism: Y ou can create a module that nobody should impo","150-Replaceability First.pdf,150-Replaceability First,""Constrained Behavior 307\n •License mechanism: Y ou can create a module that nobody should import \nexcept into one particular project, and you have no way to do that within the package manager itself. Y ou can implement a very simple license mechanism: When you import the module, it throws exceptions, complaining that it’s miss-ing a license text file or license \nENV variable. The license can be text such as \n“I should not import this module” acting as a disclaimer. Y ou can hack it, but if you do, you accept the disclaimer!\nTrust-First Culture\nEnforcing guidelines as automated rules or through access restrictions may express a lack of trust to the teams, but it depends a lot on your company culture. If your cul-ture really is a culture of trust, autonomy , and responsibility between everyone, then introducing enforced guidelines should be decided by consensus after discussions between everyone involved. In the worst case, introducing enforced guidelines could send the wrong signal and undermine trust, which would be a greater loss than the benefits you’re after.\nConstrained Behavior\nRather than document, you can influence or constrain the behavior instead. Enforc-ing guidelines is not the only approach to bring the right piece of knowledge at the right time to the developers; an interesting alternative is to influence or constrain them to do the right thing in the first place, without them being necessarily aware of it. \nMaking It Easy to Do the Right Thing\nFor example, you could decide that “from now on, developers MUST create more modular code, as new small services that MUST be deployed individually .” Y ou could even print this on the guidelines document and hope everyone will read it and follow this decision.\nOr you could invest in changing the environment:\n •Provide good self-service CI/CD tools: By making it easy to set up a new build and deployment pipeline, you make it more likely that developers will create new separate modules rather than put all new code into the same big ball of mud that they know how to build and deploy .\nChapter 10 Avoiding Traditional Documentation 308\n •Provide a good microservices chassis (see Chris Richardson’s website, \nhttps://microservices.io): Y ou can encourage modularity by making it easy to bootstrap a new microservice without spending time wiring together all the necessary libraries and frameworks.\nIn his book Building Microservices, Sam Newman writes on making it easy to do \nthe right thing, with what he calls tailored service templates:\nWouldn’t it be great if you could make it easy for all developers to follow most of the \nguidelines you have with very little work? What if, out of the box, the developers had most of the code in place to implement the core attributes that each service needs?\n…\nFor example, you might want to mandate the use of circuit breakers. In that case, you \nmight integrate a circuit breaker library like Hystrix. Or you might have a practice that \nall your metrics need to be sent to a central an open source library like  Dropwizard’s Metrics and configure it so that, out of the box, response times and error rates are pushed automatically to a known location.\n28\nThe most famous tech companies embrace this approach with open-source \n libraries that you, too, can use. In the words of Sam Newman:\nNetflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. T o handle this, a large amount of work has been done to ensure that there are client libraries on the JVM to provide teams with the tools they need to keep their services well behaved.\nThe environment is also passing information. It’s implicit and passive, and we \ndon’t often pay attention to that. Y ou can make it deliberate and decide what mes-\nsage to pass by designing the path of least resistance in the environment to be the one that you favor.\nMore generally , you want to make behavior not just easier but also more \n rewarding. By showing the commit history as a nice pixel art diagram, GitHub \nmakes it a rewarding thing to commit often. Developers’ pride is powerful!\nA major point of living documentation in general as advocated in this book is to \noffer simple ways to document to encourage doing it more.\nMaking Mistakes Impossible: Error-Proof API\nDesign your API in a way that makes it impossible to misuse. This reduces the need for documentation, since there’s nothing to warn the user about.\n28.  Newman, Sam. Building Microservices. Sebastopol, CA: O’Reilly Media, Inc., 2015.\nDesign Principles for Documentation Avoidance 309\nMichael L. Perry lists many common API traps in a blog post:\n •Y ou must set a property before calling this method.\n •Y ou must check a condition before calling this method.\n •Y ou must call this method after setting properties.\n •Y ou cannot change this property after calling a method.\n •This step must occur before that step.29\nThese traps should not be documented; instead, they should be refactored to be \nremoved! Otherwise, the documentation will be a great case of  shameful comment.\nThere are endless ways to make an API impossible to misuse, including the \nfollowing:\n •Use types only to expose methods you can actually call, in any order.\n •Use enums to enumerate every valid choice.\n •Detect invalid properties as early as possible (for example, catch invalid inputs \ndirectly in the constructor), well before they are actually used, and then repair whenever possible, such as replacing nulls with null objects in the constructors or setters.\n •It’s not just about errors but also about any harmful naive usage. For example, if a class is likely to be used as the key in a hashmap, it should not make the hashmap slow or inconsistent. Y ou could use internal caches to memorize the results of any slow computations of \nhashcode()  and toString() .\nA common objection is that experienced developers don’t make these easy mis-\ntakes, so there is no need to be so defensive. However, even good developers have more important things to focus on than avoiding the traps of your API.\nDon Norman calls advice on how to guide the use of something affordances.\n30 \nDesign Principles for Documentation Avoidance\nDuring QCon 2015, Dan North talked about a model in which code is either so old and well established that everybody knows how to deal with it, or it’s so young that \n29.  Michael L. Perry , QED Code blog, http://qedcode.com/practice/provable-apis.html\n30.  Norman, Donald A. The Design of  Everyday Things. New Y ork: Basic Books, Inc., 2002."",6650",6711,"**Constrained Behavior for Developers:**\n* Create modules with a license mechanism: restrict imports to specific projects (150-Replaceability First)\n	+ Throw exceptions during import if no license text file or variable is present\n	+ Disclaimer in the license text\n* Enforce guidelines through environment and tools instead of automated rules or access restrictions\n* Make it easy for developers to do the right thing:\n	+ Provide good self-service CI/CD tools\n	+ Offer microservices chassis for easier setup (Chris Richardson's website)\n	+ Use tailored service templates with predefined code structures and libraries\n* Design API in a way that makes it impossible to misuse, reducing the need for documentation\n* Refactor common API traps instead of documenting them:\n	+ Ensure proper ordering of method calls\n	+ Detect and repair invalid properties early on\n	+ Use types, enums, and internal caches to eliminate harmful naive usage.\n\n**Design Principles for Documentation Avoidance:**\n* Make it easy for experienced developers to follow best practices\n* Use clear naming conventions and well-organized code structure\n* Provide good documentation when necessary:\n	+ Write comprehensive bulleted notes summarizing key information\n	+ Use clear headings, terms in bold, and a simple style\n	+ Incorporate examples, diagrams, or code snippets for better understanding\n* Encourage communication between team members to avoid misunderstandings about the codebase.",1455,50.01190376281738
False,3,"151-Consistency First.pdf,151-Consistency First,""Chapter 10 Avoiding Traditional Documentation 310\nthe people doing it are still there, so they know","151-Consistency First.pdf,151-Consistency First,""Chapter 10 Avoiding Traditional Documentation 310\nthe people doing it are still there, so they know all about it. Problems happen when \nyou’re in a gray zone between these two extremes.\nDan emphasizes the central role of knowledge sharing and knowledge preserva-\ntion as a key ingredient of successful teams. He goes further, suggesting alternative ways to deal with this issue.\nReplaceability First\nDesigning for replaceability reduces the need to know how things work. Y ou don’t need much documentation for components you can replace easily . Sure, you need to know what the components were doing, but you don’t have to know how they were doing it.\nIn this mindset, you could give up maintenance. If you must change something, \nyou could just rebuild it all. For this approach to work, every part has to be reason-ably small and as independent as possible from every other component. This shifts the attention to the contracts that are between components.\nTherefore: Favor a design that makes it easy to replace a part within the whole. \nMake sure that everybody knows exactly what the part does. Otherwise, you need documentation for the behavior—for example, the working software you can easily play with, self-documented contracts of the inputs and outputs, or automated and readable tests.\nWhen the team does not care enough about design, the components grow and \nget hairy . They quickly get coupled to everything. As a result, you can never really replace them totally . Making the code easy to replace is still an act of design; it does not happen out of pure luck or without skills and care. It takes discipline. One obvi-ous way is to limit the size of the component—for example, up to one page on the screen. Another way is to create strict restrictions on what components can call each other and preventing them from sharing data storage.\nEven with an approach that favors replaceability , design skills remain necessary . \nFor example, the open/closed principle is indeed a case of making the implementa-tion replaceable easily; another is its good friend the Liskov substitution principle. Other solid principles also help. They are usually discussed at the class and interface levels. But they also apply at the level of components or services. But to be really replaceable at low cost, they have to be small—hence the idea of microservices.\nConsistency First\nConsistency in the code base is when code that you’ve never seen looks familiar so that \nyou can deal with it easily .\n—Dan North at QCon London 201531\n31.  https://qconlondon.com/london-2015/speakers/dan-north.html\nDesign Principles for Documentation Avoidance 311\nBeing consistent reduces the need for documentation.  In practice, consistency is \nhard to maintain beyond bounded areas; consistency is more natural within one \ncomponent, within one programming language, and even within one layer. Y ou often don’t follow the same programming style for GUI logic as for server-side domain logic.\nFor a given area of the code base with a consistent style of code, once you know \nthe style, there’s nothing more to say for all elements in the area. Consistency makes everything standard. Once you know the standard, there is nothing else to tell.\nThe level of consistency depends on the surrounding culture. For example, in a \nJEE-heavy company , there would be no need to tell why you decided to use EJB, but you’d need to explain when you decide not to use it. In another company with better taste, it would be the opposite.\nIf you decide as a team that no method is allowed to return null within your \ndomain model, then this decision only has to be documented in one place, such as in the root folder of the domain model source control system. Then there’s no need to talk about it anymore on each method.\nTherefore: Agree as a team on concrete guidelines to apply within chosen \nbounded areas. Document them briefly in one place.\nThere have to be exceptions to the rule. Not every class will be consistent. How-\never, as long as the number of exceptions is low , it’s still cheaper to document the exceptions explicitly than to document everything on every class.\nHere’s an example of the guidelines that a team created for a domain model:\n •No abbreviations in naming of public signatures\n •Business-readable names in all public interfaces and their methods\n •Null-free: no null allowed as a return type or as a method parameter\n •All classes immutable by default\n •All methods side effect free by default\n •No SQL\n •No import of frameworks at all, including javax\n •No import of infrastructure (such as middleware)\nEnforced guidelines provide a way to document the guidelines in a way that is \neffective even if nobody reads them."",4757",4812,"**Chapter 10: Avoiding Traditional Documentation**\n\n**Consistency First**\n- Dan North emphasizes the importance of knowledge sharing and preservation for successful teams\n- Replaceability first reduces the need for extensive documentation\n- Favor a design that makes it easy to replace components\n- Make sure everyone knows what each part does\n\n**Replaceability First**\n- Reduces the need for documentation on how things work\n- Components can be replaced easily without needing in-depth knowledge\n- Every part should be reasonably small and independent\n- Limit size of components and create strict restrictions on calls between them\n- Discipline is required to maintain replaceability\n\n**Consistency First (in Code)**\n- Consistency reduces the need for documentation\n- Consistent coding styles within a component, programming language, or layer\n- Agree on concrete guidelines within chosen bounded areas\n- Document exceptions to rules explicitly rather than documenting everything\n\n**Design Principles for Documentation Avoidance**\n- Consistency in code base makes it easy to understand\n- Consistency reduces the need for extensive documentation\n- Guidelines should be agreed upon and documented briefly\n- Exceptions to rules should be documented explicitly\n\n**Example of Guidelines for a Domain Model:**\n- No abbreviations in naming of public signatures\n- Business-readable names for all public interfaces and their methods\n- Null-free: no null allowed as return type or method parameter\n- All classes immutable by default\n- All methods side effect free by default\n- No SQL\n- No import of frameworks, including javax\n- No import of infrastructure (such as middleware)",1668,49.73932218551636
False,3,"152-Example The Zero Documentation Game.pdf,152-Example The Zero Documentation Game,,0","152-Example The Zero Documentation Game.pdf,152-Example The Zero Documentation Game,,0",86,"**The ""Zero Documentation Game"" Example**\n\n**The Documentation Game**: A term used to describe an activity or exercise in which participants aim to create documentation without any prior knowledge of the subject matter (O'Hara, 2019). In this example, the document is titled **""152-Example The Zero Documentation Game""**.\n\n**The Activity**: The ""Zero Documentation Game"" involves creating documentation related to a topic without any background knowledge or understanding of it. This exercise aims to highlight potential issues that may arise when documenting complex subjects without proper context and expertise (O'Hara, 2019).\n\n**Further Discussion**: For more information about the ""Zero Documentation Game"", please refer to **O'Hara's article**. It provides valuable insights into this activity, which can be useful for understanding the importance of proper documentation and effective communication.",906,32.09409189224243
False,3,"153-Summary.pdf,153-Summary,""Chapter 10 Avoiding Traditional Documentation 312\nExample: The Zero Documentation Game\nI’ve heard of a team that decide","153-Summary.pdf,153-Summary,""Chapter 10 Avoiding Traditional Documentation 312\nExample: The Zero Documentation Game\nI’ve heard of a team that decided to forbid documentation. They’re proudly doing \nzero documentation. It’s not as insane as it might at first seem: Zero documentation is an approach that forces better naming and better practices in general to share knowledge without additional prose.\nWhen you understand that most of the time written documentation in the form \nof prose or diagrams is a poor substitute for expressing the knowledge better within the work product itself in the first place, it makes sense to minimize it. And because striving for zero documentation sounds radical and a bit insane, it’s stimulating and becomes a game. This makes it more likely to stick in team members’ minds, driving their behavior for the better, hopefully .\nI haven’t tried it myself, but my colleague told me that zero documentation usu-\nally drives virtuous behavior in practice.\nBecause we don’t all share the same definition of the word documentation, a \ngame of zero documentation must clarify its rules. The previously mentioned team refuses comments in the code and on methods, all forms of written prose, external documents, and traditional Office documents. It happily embraces tests and Gherkin scenarios (Cucumber/SpecFlow), favors simple code, and enjoys working collectively as a primary means of sharing knowledge. The team is happy with all this.\nI think augmenting the code with annotations, keeping a simple README file, \nand generating living documents would still fit within the rules of the game, but you decide where to put the cursor!\nContinuous Training\nAs general knowledge becomes more widespread, the less you need to document. Investing in continuous training is therefore a way to reduce the need for documentation.\nLearning standard skills also makes it easier to use more ready-made knowledge \ninstead of original solutions. This is good for the quality of the solution, and it alle-viates the need for specific documentation.\nMore consistency of skills and shared culture also helps speed up decision mak-\ning. It’s not about removing all diversity in the team, since diversity is an essential ingredient. Still, we don’t need all diversity in every detail, and there’s a lot that we can make more consistent without losing much.\nInvesting in continuous training may involve the following:\n •Coding dojos on CodeKata (for example, at lunchtime every Friday)\n •Short training sessions during the day\nSummary 313\n •Interactive mini-trainings (for example, half an hour twice a week right after \nlunch)\n •Time for deliberate practice (for example, a 20%-time policy devoted to side projects)\n32\nSummary\nThe best documentation is doesn’t look like documentation. Interactive conversa-tions and working collectively rank very high when it comes to sharing knowledge. In addition, the serendipity and spontaneity of meeting at the coffee machine is an essential complement. \nOn a different note, making process automation more declarative, by discipline or \nthrough better tools, also makes it the proper authority of knowledge for a process. Having tools that scream when you do something wrong is yet another form of docu-mentation, and it is one of the most efficient, since it brings the right knowledge at the right time even to people who weren’t aware of it.\nIn all the cases listed in this chapter, the key point is that we need to turn up the \ndocumentation value of all the activities already carried out by the development teams in order to reduce the work that is specifically done on documentation tasks. Just because something doesn’t look like typical documentation doesn’t mean it’s not a valid form of knowledge sharing and preservation. The more developers and their managers understand this, the more efficient they will collectively be.\n32.   https://www .inc. com/adam-robinson/google-employees-dedicate-20-percent-of-their-time-to-side-\nprojects-heres-how-it-works.html\nThis page intentionally left blank"",4060",4095,"**Avoiding Traditional Documentation: The Zero Documentation Game**\n* **Zero documentation approach**: focuses on better naming and practices to share knowledge without additional prose\n* Radical yet stimulating for team members, driving better behavior\n* Clarifies rules: no comments in code or methods, external documents, or traditional Office documents\n* Embraces tests, Gherkin scenarios (Cucumber/SpecFlow), simple code, and collective work\n* Continuous training as a way to reduce need for documentation\n* Investment in skills development and shared culture\n* Coding dojos, short training sessions, interactive mini-trainings, time for deliberate practice\n\n**Best Documentation is Non-Traditional**\n* Interactive conversations and working collectively are effective forms of sharing knowledge\n* Serendipity and spontaneity from informal meetings complements formal documentation\n* Process automation can be more declarative, acting as an efficient form of documentation\n* Development teams need to turn up the ""documentation value"" of their activities to reduce specific documentation tasks.\n\n**Additional Information**\n* Google employees dedicating 20% of their time to side projects: https://www.inc.com/adam-robinson/google-employees-dedicate-20-percent-of-their-time-to-side-projects-heres-how-it-works.html",1318,51.22997498512268
False,3,"154-Chapter 11 Beyond Documentation Living Design.pdf,154-Chapter 11 Beyond Documentation Living Design,,0","154-Chapter 11 Beyond Documentation Living Design.pdf,154-Chapter 11 Beyond Documentation Living Design,,0",106,"**Chapter 11: Going Beyond Documentation - Embracing Living Design**\n\n**Living Design**: A Transcendent Approach to Documentation\n- Discover the significance of going beyond conventional documentation methods with Living Design.\n- Understand how this innovative approach transcends traditional limitations for enhanced impact and engagement.\n\n**Embrace Change**: The Evolution of Documentation\n- Witness the transformation of documentation practices through the lens of Living Design.\n- Learn about the evolving role of design in shaping future developments within the realm of documentation.\n\n**Experience, Not Just Read**: The Power of Immersive Learning Experiences\n- Delve into how Living Design fosters immersive learning experiences that go beyond mere reading.\n- Uncover the benefits of engaging with content in a more interactive and dynamic way.\n\n**Incorporating Best Practices from UX Design**\n- Learn from successful UX design principles to enhance the user experience within documentation.\n- Discover how incorporating these best practices can lead to better outcomes for both users and organizations.\n\n**Applying Living Design Principles**\n- Explore specific living design principles that can be applied to documentation projects.\n- Understand how these principles contribute to creating effective, engaging, and impactful technical writing.\n\n**Case Studies: Success Stories in Living Documentation**\n- Analyze real-life examples of successful living documentation implementations.\n- Gain insights into best practices and learn from the success stories of companies that have embraced this innovative approach.\n\n**Future Directions: The Impact of Technology on Living Design**\n- Discover how advancements in technology are shaping the future of living design within documentation.\n- Stay ahead of the curve by exploring potential developments and trends that will redefine the landscape of technical writing.",1921,38.315850019454956
False,3,"155-Programming by Coincidence Design.pdf,155-Programming by Coincidence Design,""315Chapter 11\nBeyond Documentation: \nLiving Design\nThis book has s","155-Programming by Coincidence Design.pdf,155-Programming by Coincidence Design,""315Chapter 11\nBeyond Documentation: \nLiving Design\nThis book has so far focused on how to record and transfer knowledge about what \nhas been done in a software project. However, when you start paying attention to this knowledge explicitly , an additional benefit kicks in: Y ou start to see improvements in your design. As you’re working to create living documentation, you often also see design improvement, a benefit that can quickly be far more important than just the documentation aspect. Y our initial goal of living documentation that can follow design changes morphs into living documentation that begins to suggest even more changes to the design! This chapter explores a number of patterns that can help you exploit this bonus effect to the max.\nAs one more bonus effect, making the inside of the software system more visible \nto all stakeholders will make it increasingly likely that it will be well designed.\nListening to the Documentation\nSo you’ve learned a bit about living documentation, and you want to try it. If you try to create a living diagram but find it hard to generate one from the current source code, this is a signal. If you try generate a living glossary but find this almost impos-sible to achieve, this is also a signal. As illustrated in Figure 11.1, you should listen to the signals.\n \nChapter 11 Beyond Documentation: Living Design 316\nNat Pryce and Steve Freeman wrote, “when code is difficult to test, the most likely \ncause is that our design needs improving.”1 Similarly , if you find it hard to generate \nliving documents from your code, it’s a signal that your code has design issues.\nWhat Happened to the Language of the Domain?\nIf you’re into DDD and you find it hard to generate a living glossary of the business \ndomain language, then it’s probably because this language is not expressed clearly enough in the code. Any of the following might be happening:\n •The language might be expressed in other words, such as technical words, syn-onyms, or (worst) legacy database names.\n •The language might be mixed with technical concerns in a way that is impossi-ble to recover; for example, business logic may be mixed with data persistence logic or presentation concerns.\n •The language might be completely lost, and code may be doing business stuff without any reference to the corresponding business language.\nWhatever the issue, if you’re finding it difficult to do living documentation, \nyou should take it as a signal that you’re doing DDD—and domain modeling in general— incorrectly . The design should be aligned as much as possible with the busi-ness domain and its language, word by word.\n1. Freeman, Steve, and Nat Pryce. Growing Object-Oriented Software, Guided by T ests. Boston: Pearson \nEducation, Inc. 2010.Figure 11.1 Listen to your documentation!\nListening to the Documentation 317\nSo instead of trying to make a complicated tool to generate a living glossary , \nyou should take the opportunity to redesign the code so that it better expresses the \ndomain language. Of course, it’s up to you to decide whether it’s reasonable to do this and when and how to do it. \nProgramming by Coincidence Design\nWe don’t know what we’re doing, and we don’t know what we’ve done.\n—Fred Brooks2\nIf there is no choice to be made, you’re not doing design.\n—Carlo Pescio3\nT o generate a design diagram, first you have to know what particular design decision you expect the diagram to explain. But can you tell what your design is like? The most common difficulty when trying to generate living diagrams is simply that you often don’t know clearly enough what your design is like or why it’s that way . This suggests that you may be programming by coincidence.\n4 Y ou might know how to \nmake your design work, but you don’t really know why , and you haven’t really con-sidered alternatives. Such a design is arbitrary rather than deliberate.\n Note\nI love Carlo Pescio’s essays. I don’t actually like his writing style much, but do I like the way he writes about his mind musing on hard and deep matters of soft-ware development. He’s got some crazy ideas and some stretched metaphors but a lot of insights to spark my imagination about future breakthroughs in our field. T o see what I mean, visit http://www .carlopescio.com.\n \nBuilding software involves continuous decision making. Big decisions usually get \na lot of attention, including dedicated meetings and written documents, and deci-sions deemed less important tend to be neglected. The problem is that many of the neglected decisions end up being arbitrary rather than well-thought-out, and the \n2. This quote from Fred Brooks (1999 Turing Award) was the summary of his working group on the role \nof science in design, as reported on page 15 in “Software Language Engineering: 6th International \nConference,” SLE 2013, Indianapolis, IN, USA, October 26-28, 2013. \n3. Carlo Pescio, “Design, Structure, and Decisions,” http://www .carlopescio.com/2010/11/design- \nstructure-and-decisions.html\n4. Hunt, Andrew and David Thomas. The Pragmatic Programmer: From Journeyman to Master . Boston: \nAddison-Wesley , 2000.\nChapter 11 Beyond Documentation: Living Design 318\naccumulated effect (even a compounding effect) may be to make the source code hos-\ntile to work with.\n“Why does this function return null instead of an empty list?” “Why do some \nfunctions return null and others return an empty list inconsistently?” “Why are most of the DAO, but not all, in this package?” “Why do we have the same method signa-ture in five different classes but without a common interface to unify them?” Such neglected decisions sometimes get close to better solutions but miss them due to fail-ure to properly thinking about the matters at hand. All these examples represent lost opportunities for better design.\n Tip\nWhenever you find out something unexpected in code or in code design, consider thinking about the question, “What would it take to come back to the standard situations in the literature?”\n \nI encourage deliberate thinking. Documenting decisions as they are made is one \nway to encourage deeper thinking because trying to explain a decision often reveals its weaknesses.\n Note\nIf you can’t explain something simply , you don’t understand it well enough.  \nSometimes it’s frustrating when working with a team at a customer site to observe \ndecisions being made without anyone being clear on the reasoning. “Just make it work right now” seems to be the motto. In one instance I took notes about one such situation:\nWe’ve been discussing for one hour the semantics of the messages between a legacy app \nand a new event-sourcing-based app. Is it event or command? As usual, the discussion doesn’t lead to a clear conclusion, and yet the unclear choice works. Had we decided to document the semantics of all integration interactions clearly , we would have had to decide, and to turn it into a tag or something written and visible. Then we would have to conform to it, or to question it explicitly when it’s no longer relevant.\nInstead, we’re going to live with the continuous confusion. Each contributor will \ninterpret as he or she wishes. And it will bite us.\nA year later, I can see that the team has matured, and now such a discussion would \nconverge to sound reasoning."",7334",7421,"**Living Documentation: Benefits and Improvements**\n* Living documentation improves design as we work on it\n* Signals for potential design issues: difficulty generating living documents from code or business domain language\n* Coincidence programming: making decisions without clear understanding or deliberate thought\n	+ Neglected decisions can lead to arbitrary design choices that may cause issues down the line\n* Importance of documenting decisions and encouraging deeper thinking\n	+ Explanation of decisions reveals weaknesses and fosters better understanding\n	+ Frustrating for teams when decisions are not documented or made without clear reasoning.\n\n**Living Documentation: Difficulties and Solutions**\n* Difficulty generating living documentation from code or business language signals potential design issues\n* Misaligned design with business domain language or loss of domain language in the code\n	+ Expressing the language clearly is essential for effective living documentation\n* Neglected decisions lead to arbitrary design choices, affecting the overall system negatively.\n\n**Living Documentation: Encouraging Deliberate Thinking and Decision Making**\n* Encourage team members to document decisions as they are made for deeper thinking\n* Clear understanding of concepts is essential before making decisions or creating living documentation\n* Standardize integration interactions by turning them into tags or written guidelines.",1441,52.09473156929016
False,3,"156-Deliberate Decision Making.pdf,156-Deliberate Decision Making,""Deliberate Decision Making 319\nDeliberate Decision Making\nThe path to better desi","156-Deliberate Decision Making.pdf,156-Deliberate Decision Making,""Deliberate Decision Making 319\nDeliberate Decision Making\nThe path to better design and better documentation starts by making more decisions \ndeliberately . It is very difficult to document random decisions. It is like attempting to describe noise: There are at the same time too many low-level details and almost nothing to tell at a higher level. In contrast, when decisions are deliberate, they are clearly and consciously made, and documentation is a simple matter of putting them into words.\nIf a decision is pretty standard, it’s ready-made knowledge that has been already \ndiscussed in a book under a standard name, such as a pattern. Documenting in such a case involves just making a mark in the code that refers to the standard name, along with some brief reasons, motivation, context, and main forces that led to the decision.\n Tip\nIf a decision is deliberate, it’s already half documented.  \nBeing deliberate in the way we do our work is a big recurring theme in agile \n circles. Software craftsmanship encourages deliberate practice to improve the craft. We  dedicate time to practice katas and coding dojos to achieve that goal of getting better at our craft. In the BDD community , Dan North explains that projects should be seen as learning initiatives, a mindset he calls deliberate discovery.\n5 He claims that \nwe should do whatever it takes to learn as quickly as possible as early as possible. Being deliberate is about expending extra effort to do better work in a conscious way .\nDeliberate design involves thinking clearly about each design decision. What is \nthe goal? What are the options? What do we know for sure, and what do we suspect? What does the literature say on this kind of situation?\nIn addition, the better the design, the less there is to document. Better design is \nsimpler, and “simpler” actually means fewer but more powerful decisions that solve more of the problem:\n •Symmetries: The same code or interface takes care of all symmetric cases.\n •Higher-level concepts: The same code deals with many special cases at once.\n •Consistency: Some decision is repeated everywhere without exception.\n5. https://dannorth.net/2010/08/30/introducing-deliberate-discovery/\nChapter 11 Beyond Documentation: Living Design 320\n •Replaceability and encapsulation: Local decisions within a boundary do not \nmatter, as they can be reconsidered or redone later, even if knowledge about them is lost.\nThe quantity of specific knowledge needed to document a piece of software \nis  an  indicator of the maturity of the design. Software that can be described in  10 sentences has a better design than software that needs 100 sentences of description.\nEngineering Is a Deliberate Practice\nIn French engineering schools and other grandes ecoles, from mechanical engineering to electronic engineering or even industrial design, it’s primarily important for students to demonstrate that all decisions they have made are substantiated. Arbitrary decisions are just not acceptable.\nDuring final exams, the most important aspects of the evaluation involve \nframing the work precisely , and then at each step of designing the solution, each decision has to be justified against enough alternatives and chosen according to explicit criteria: budget, weight, feasibility , or other constraints.\nIn software development, we are seldom so deliberate in every detail, but \nwe should be. Whether every decision is recorded in writing or not, making more conscious decisions often improves the decisions.\nIf you know what you’re doing, what it’s called in the literature, and why you’ve \nmade a particular decision, all it takes for complete documentation is to add that information in the code in one line: a link to the literature and some text to explain the rationale. Once you’ve got the thinking right, the writing takes care of itself.\nY ou have to realize, of course, that thinking takes time. It looks slow and may \nbe confused with slack, and in many companies people often think, “We don’t have time for that!” However, alternatives to thinking only give the illusion of speed, at the expense of accuracy . As Wyatt Earp said, “Fast is fine, but accuracy is everything.” Accuracy requires rigorous thinking. Thinking with more than one brain, as in pair programming or mob programming, also improves accuracy and helps you create more deliberate designs. With more brains, it’s more likely that someone knows the standard solutions from the literature for any situation.\nY ou have probably heard this saying: “Y ou don’t really understand something \nuntil you can explain it to someone else.” Having to clarify your thoughts for doc-umentation purposes is virtuous because, well, you have to clarify your thoughts. Having to justify decisions in a persistent form is another incentive to think with more rigor."",4861",4934,"**Deliberate Decision Making**\n\n**Importance of Documenting Deliberate Decisions**:\n- Documenting random decisions is difficult as they lack clarity and consciousness\n- Deliberate decisions are consciously made, making documentation simple\n- Standard decisions can be documented using ""ready-made knowledge"" such as patterns\n\n**Benefits of Being Deliberate**:\n- Encouraged in agile circles and software craftsmanship\n- Allows for learning through deliberate discovery\n- Results in better design that requires less documentation\n    - Symmetries, higher-level concepts, consistency, replaceability, and encapsulation\n\n**Thinking and Documentation**:\n- Thinking takes time but leads to accuracy\n- Alternatives to thinking give the illusion of speed at the expense of accuracy\n- Clarifying thoughts for documentation purposes is a virtuous process\n\n**Engineering as Deliberate Practice**:\n- French engineering schools emphasize substantiated decisions\n- Software development should adopt this deliberate approach to decision making",1028,32.765929222106934
False,3,"157-Deliberate Decision Does Not Mean Upfront Decision.pdf,157-Deliberate Decision Does Not Mean Upfront Decision,,0","157-Deliberate Decision Does Not Mean Upfront Decision.pdf,157-Deliberate Decision Does Not Mean Upfront Decision,,0",116,**Deliberate Decision**\n* **Does not** mean upfront decision.,61,4.523261308670044
False,3,"158-Documenting Errors or Avoiding Errors.pdf,158-Documenting Errors or Avoiding Errors,""Deliberate Decision Making 321\n Note\nDeliberate design work","158-Documenting Errors or Avoiding Errors.pdf,158-Documenting Errors or Avoiding Errors,""Deliberate Decision Making 321\n Note\nDeliberate design works particularly well when doing TDD . TDD is a very delib-\nerate practice with rules. Starting with naive code that just works, the design emerges from successive refactorings, but it’s the developers who are driving the refactorings, and they have to think before applying each refactoring. “Do we really need to make that more complex?” “Is it worth adding an interface now?” “Shall we introduce a pattern to replace these two IF statements?” It’s all about trade-offs, which requires clear thinking.\n \nLiving documentation encourages attention to virtuous practices—design in \nparticular. Living documentation makes bad design clearly visible. One of the great rewards is that you can improve the design, and your design documentation comes almost for free as a result.\n A Confession from the Author\nDeliberate design is my ulterior motive in writing this book. People don’t pay enough attention to design, and I’m very sorry about that. Living documentation is a T rojan horse, or a gateway to get more people addicted to better design.\n \n“Deliberate Decision” Does Not Mean “Upfront Decision”\nWith emerging design, the natural decision emerges by listening to the working code and its flaws. For example, noticing duplication may trigger a refactoring to some-thing better. When you reach such a point, you have to make a conscious, deliberate decision: What is the “better” you want to refactor to? Deliberate means that you \nunderstand the troubles, can imagine the benefits you are looking for, and have found more than one way to improve. Deciding means choosing one way out of all the pos-sible others. This is a deliberate decision.\nDocumentation Is a Form of Code Review\nDocumentation makes a product and the development process more transparent. As such, documentation is also a useful feedback tool that helps you adjust and correct over the complete lifecycle of an application. Decisions with no rationale have nowhere to hide. With living diagrams and the other ideas of living documentation, the neglected design areas become quite visible, making it harder to ignore them. This increases the pressure to put more care into every aspect of code quality .\nChapter 11 Beyond Documentation: Living Design 322\nLiving documentation generated from source code, especially diagrams, also \nworks great as a debugging tool to detect mistakes such as unexpected cycles in the \ndependencies or excessive coupling, shown as too many arrows on the diagram. Y ou might have been expecting some design structure, but when trying to render it as a diagram, you may have to admit that the code does not exhibit much structure. Y ou might have been expecting the code to tell the business domain, but when trying to make it into a glossary , it may appear that the business is mangled in the middle of the processing, and there is no easy way to get it out.\nIt is interesting to compare the top-down documentation you might have done \nbefore building the code with the actual bottom-up documentation generated from the sources. The differences can help you spot inconsistencies or, even better, to real-ize once again that it is difficult to speculate about what the code will be like before it is actually developed.\nIndeed, even before making living diagrams, just trying to document by hand \non paper can reveal design issues. Maxime Sanglan, a lead developer from one of my clients, reacted when reading an early version of this book: “That’s totally what happened when I started to have the team do sketching workshops around Simon Brown’s C4 model on the legacy system.”\nShameful Documentation\nJust because it is documented, it doesn’t make it less stupid.\n—@dalijap on Twitter\nWhen documentation is up-to-date and accurate, it is often considered a good thing. However, there are a number of cases where quite the opposite occurs: The existence of the documentation itself demonstrates the presence of a problem. The infamous troubleshooting guide is the best example in this category . Someone decided to take the time to document the known troubles, usage traps, and other anomalies of behavior, and this effort demonstrates that the issues are important enough to be worth documenting. However, it also means that these issues are not fixed, and it is possible that no one even plans to fix them.\nSuch documentation is what I call shameful documentation; it’s documentation \nyou should be ashamed of. This documentation, by its sole existence, should be seen as a confession of something to be fixed. The time spent creating this documentation should have been allocated to fixing the troubles instead.\nShameful Documentation 323\nTherefore: Recognize the situations when documentation is a poor substitute for \nactually fixing a problem. Whenever possible, decide against adding more docu-mentation and instead allocate time to fixing the problem.\nOf course, there are many reasons teams might add documentation instead of \nfixing the issues:\n •Budget: There might be money allocated for documentation but no more money for working on the code.\n •Laziness: It may seem easier to add some quick documentation on trouble-shooting rather than actually tackle the root issue.\n •Lack of time: Documenting the issue might be faster than fixing it.\n •Cost: It may be genuinely difficult to address some issues. For example, some issues would require releasing a new version of the application to dozens of clients, which would be prohibitively expensive.\n •Missing knowledge: Sometimes the team knows about issues but is missing knowledge and skills required to fix the issues.\nIf there is no time available to fix an issue now , then the right place to document \nthe issue is the defect tracker. However, in the mindset of shameful documentation, a defect tracker is also in itself a demonstration of a deeper issue: Defects should not accumulate but should be prevented earlier or fixed immediately as much as possible. And are defects that can remain for a long time without being fixed really defects?\nIf a feature is implemented so badly that it requires a manual with many pages of \nwarnings and workaround instructions or a lot of assistance from the support team, you might consider removing it until it is implemented correctly; chances are that almost nobody manages to use it anyway or that using it is so expensive that it’s not worth it.\nExample: Shameful Documentation\nIn a past mission at a customer site, I discovered a 16-page document on how to run and test an application. This guide was for the all users, including end users. I’ll call this application Icare to protect the innocents. This was not a new project; it was used several times every day by dozens of people in the company . The document was full of screenshots highlighted in red color bubbles to show how to proceed because the steps were not unintuitive. However, most of the 16 pages described where to \nChapter 11 Beyond Documentation: Living Design 324\n“pay attention”: “Pay attention…[this may not work properly]. Please note that…\n[there is a bug here].” “Pay attention, Icare is launched from another directory!” “Take really good care to not launch these tasks any time because it will kill every-thing on the corresponding environment!” It might as well have said “Pay attention; we’re not professional.”\nHalf of everything written was about traps waiting to bite users. “Pay attention \nto the name of the trigger; sometimes, it’s not correctly named, so check in the trig-ger.” Remember that this was a document for end users. And it got even better: “ After an export in XML, you should do a test of re-import to be sure that it works well.” Y ou can see that a developer had the time to write this document instead of fixing the code.\nThe document also said, “Pay attention: Partitions Icare_env1 and Icare_env2 are \ninversed between UAT and PROD!!!” Ah, so everyone knew about this, and it had been like that for years, but it wasn’t anyone’s plan to fix it? Or was the process so heavyweight that you’d first have had to find a sponsor to pay for the fix?\nThe Troubleshooting Guide\nFinally , at the end of the documentation was the infamous “known problems” sec-tion, shown here:\n1  1 Known problems\n23  1.1 Icare Job does not start45  It often happens. First, try to launch it directly from Icare (so6 launch the application manually from the correct directory7 [c:/icare/uat1/bin for UAT, c:/icare/prod/bin for PROD]).8910 If you are not able to launch it manually, it's becauseconfiguration of the job is not correct11  (missing or incorrect parameter date or calculation date, etc.).12 If it runs well, there is a problem when launching Icare in command line, so you need to check the log (to find where 13 it logs, check the icarius_mngt.exe.log4net).14 1516 In the past, there was also a problem for the firstexecution.\nShameful Documentation 325\n17 It requires to have made a manual connection to the \nenvironment with the good login (IcariusId).18 19 When a first connection was established, the batch mode wascorrectly working.20 \nNotice the inconsistent naming of the application as Icarius and Icare.\nShameful documentation does not always mean bugs; it may instead suggest \nopportunities for more Ops-friendliness, as shown in this example:\n1 """"you have to check the caches are up otherwise they will hit\n2 the DB and degrade performance results""""34 [...]5 6 """"Very important : 7 As we are not able to guarantee the synchronization of the8 two environments for the duration of jobs, we cannot launch 9 different type of jobs"""".\nWhen you listen carefully to the documentation, you see that it is a source of sug-\ngestions. What about having a way to automatically monitor the caches or, even bet-\nter, a mechanism to ensure that they are always preloaded before operations? What about adding a safety mechanism so that if you make the error, you’re warned, and you can avoid the issue?\nShameful Code Documentation\nY ou don’t have to tolerate documented pain. Say no. Writing such documentation is wasting time, and reading it is wasting time, too, and it will not even prevent anyone from falling into the trap completely , which will waste yet more time—again and again.\nThe Icare troubleshooting guide is just one example of shameful documentation. \nAny document that is getting too big becomes a case of a shameful documentation. A developer guide with 100 pages reveals issues of code quality , and a thick user manual is not user-friendly . Y ou might need a big user guide for an application that is not intuitive to use, but addressing the real issue instead would be a better investment if you care about the users."",10759",10859,"**Deliberate Decision Making**\n- **Practice of TDD (Test-Driven Development)**\n  * Developer drives design through refactorings\n  * Refactoring decisions require clear thinking\n- **Living Documentation**\n  + Encourages attention to virtuous practices, including design\n  + Makes bad design clearly visible for improvement\n- **Author's Perspective**\n  - Deliberate design is the author's ulterior motive in writing this book\n  - Believes living documentation can help more people adopt better design\n\n**Deliberate Design vs. Upfront Decision**\n- **Deliberate decision making**\n  * Conscious choice of refactoring improvements\n  * Understand troubles, benefits, and multiple ways to improve\n- **Documentation as a form of code review**\n  * Increases transparency for adjusting and correcting issues throughout application lifecycle\n  * Detects mistakes such as unexpected cycles or excessive coupling in dependencies\n\n**Living Documentation: Beyond Documentation**\n- Reveals design inconsistencies and can help spot issues before they become problems\n- Can be used for debugging, especially with diagrams\n- Comparing top-down documentation to actual bottom-up documentation from sources can reveal differences and inconsistencies\n\n**Shameful Documentation**\n- Ineffective documentation that demonstrates a problem instead of fixing it\n- Examples: Infamous troubleshooting guides, long user manuals, or poorly implemented features\n  - In some cases, such documentation may suggest opportunities for more Ops-friendliness\n\n**Conclusion**\n- Documentation is essential but should not be a substitute for fixing issues; allocate time and resources accordingly.",1653,64.64450025558472
False,3,"159-Diagnostic Tools.pdf,159-Diagnostic Tools,""Chapter 11 Beyond Documentation: Living Design 326\nSimilarly , with software design, if it takes a lot","159-Diagnostic Tools.pdf,159-Diagnostic Tools,""Chapter 11 Beyond Documentation: Living Design 326\nSimilarly , with software design, if it takes a lot of pages and many diagrams to \nexplain the architecture of an application, then it is most likely poor. (See Chapter 12, \n“Living Architecture Documentation,” for more on documenting the architecture.)\nFinally , shameful documentation also applies to code. Every time a developer feels \nthe urge to add a comment like this, it should trigger a reaction to remove the com-ment and immediately fix the questionable code instead:\n1  // beware this tricky case\n2  ...3  // should never happen4  ...5  // FIXME: remove this hack!\nDocumenting Errors or Avoiding Errors?\nComments in code are not the only signals which suggest that someone needs to \nimprove the code. Code that specifically deals with handling errors and that tradi-tionally deserves its specific documentation can become redundant if you learn to avoid the error cases altogether by using better design and coding practices.\nConsider the example of a function that calculates an inverse. If the divisor is \nzero, then there is no result. This is often a case of error management, but an alter-native is to make the function a total function, a function that works for the val-ues of all parameters. In this case, to make a function total, you need to extend the number type with the special value \nNotANumber . Then the function can just return \nNotANumber  when there’s a division by zero instead of taking the error management \nroad.\nDocumentation-Driven Development\nHere’s a secret about documentation. It’s not just useful to read. It’s the act of writing \nthat pushes for quality in the same way as tests.\n—@giorgiosironi on Twitter\nIn any project, it is typically a good idea to start with a focus on the end result you \nare aiming for. By focusing on the end, you first focus on where the value is, to make sure it’s really there. Then you can derive what’s really necessary to achieve the goal—no less and no more—and avoid unnecessary work. Start by explaining your goal or end result, such as how your system will be used, in order to drive the con-struction and to help notice early potential inconsistencies.\nDocumentation-Driven Development 327\nChris Matts, in his talk “Driving Requirements from Business Value” at the BDD \neXchange conference 2011 in London, gave a great example on the most typical Brit-\nish goal of having a cup of  tea. Starting with this goal, you can derive the need for hot water, a clean cup, a tea bag, and so on.\nSome developers find that starting with a piece of documentation helps start from \nthe goal. Dave Balmer said this in a blog post:\nI can start by documenting only that which is important. That satisfies the “write this \ndown before I forget” part of documentation and frees me up to improve it in later drafts.\n6\nTest-driven development and its close cousin BDD exploit this effect by focusing \non the desired behavior first, as a test or a scenario or an example written before \nstarting the coding. If you’re practicing TDD or BDD, you’re already doing a form of documentation-driven development, too.\nWhen uncertainty is very high, at the very inception of an idea, writing the \nREADME file as if  the project were already done helps clarify the purpose and flesh \nout expectations. Once materialized in writing, ideas become objects of deeper scru-tiny; they can be criticized, reviewed, and shared with other people early .\nIf you are alone, just let a few days pass before going back to these notes: When \nyou see them again with a fresh set of eyes, you can review your own work in a more objective fashion, thanks to the documentation from your past self to your future self.\nDocumentation to Keep You Honest\nContinuous improvement starts with honest retrospectives on how well we have per-formed. At the end of a project, it is easy to forget about our past assumptions and either blame the environment in the case of failure or congratulate ourselves for the success. Opportunities for improvement occur in looking back at our assumptions to learn from them. Y ou might think, “Next time I will not assume that” or “I will first check the assumption before investing more time.”\nTherefore: Document early what you assume and the experiments you try in \norder to have reliable and honest data when it is time for retrospection.\nThis is a way to be a little more data driven. And there are tools for it! For exam-\nple, growth.founders.as offers their Founders Growth T oolbox with a template to declare your assumptions and to describe your experiments.\n6. Dave Balmer, Webkit Developments blog, https://davebalmer.wordpress.com/2011/03/29/source-code-\ndocumentation-javadoc-vs-markdown/\nChapter 11 Beyond Documentation: Living Design 328\nThe Apparent Contradiction Between Documentation Driven \nand “Avoiding Documentation”\nAt this stage, you might be confused at the apparent contradiction between being \ndocumentation driven and trying to avoid documentation, as advocated in Chapter 10. The contradiction is a matter of ambiguity of words indeed. When talk-ing about documentation-driven development, even though we use the word docu-mentation, we don’t mean it as a way to share knowledge among people. Instead, it’s just a cheap way to explore the requirements at the very inception of a project, before we move on to more expensive material like tests and source code.\nThe fundamental idea is that it can be desirable to use different material at differ-\nent levels of uncertainty: At the very start of a project, conversations are usually the best material. During the early stages, conversations, notes and sketches on paper, low-fi mockups, a README file with intentions and scenarios, code exploration in a REPL, writing code in a spike without tests, and using a scripting or dynamic lan-guage might be the idea materials to learn and explore. A bit later, when things start to stabilize, another programming language with tests and even with TDD might become the material of choice. In this light, documentation early is basically a mate-rial to get started.\nApart from this case, however, documentation must not drive the development \nbut must capture and help present ideas and what has been developed that the sys-tem and its code cannot explain themselves (see Figure 11.2). Making the code as self-documenting as possible is the goal. Whenever we fail at making the code self-documenting, we have to resort to some documentation effort, but we keep it to the minimum.\n \nThere’s no contradiction between documentation driven and avoiding documen-\ntation. They just involve different meanings of the word documentation.\nExplore Present Capture\nFigure 11.2 Explore versus capture and present\nAbusing Living Documentation (Anti-pattern) 329\nAbusing Living Documentation (Anti-pattern)\nSo you’re now a fan of living documentation, and you’re generating diagrams during \neach build. Y ou like the idea so much that you spend your time figuring out what new kind of diagram could be generated. Y ou want to generate everything!\nY ou pretend to apply DDD, but you actually spend your time on exciting tools \nthat generate diagrams, if not code or bytecode. We all know that DDD is primarily about tools, right? Oh, yes, you remember some folks used to do that seriously , and they called it MDA. Ouch!\nY ou prefer working on the diagram generator rather than fixing bugs in the pro-\nduction code. Of course, it’s way more fun than boring production issues! Is all that really a good thing?\nIt’s easy to abuse living documentation, and doing so can backfire. If you spend \ntoo much time using tools to generate glossaries, reports, and diagrams instead of doing the work to be done, it’s not professional, and management may decide to stop and forbid any documentation-improving initiative. Y ou don’t want that.\nTherefore: Keep your efforts in automating living documentation reasonable \ncompared to the actual delivery work. Remember that living documentation is just a mean to an end and not an end in itself. The goal of living documentation is to help deliver more and with better quality, not just produce documentation or have fun. Ideally, every effort in improving your living documentation should yield short-term demonstrable benefits in delivery, quality, or user satisfaction.\nAs the author of this book on living documentation, I don’t want this topic to get \nbad press because people abuse it. Please don’t say that this book is asking you to put in place every example described into your own project, because that’s not true. All the examples are, well, examples, not requirements.\nIt is true that the point of this book is to excite your inner geek to try the ideas of \nliving documentation. But I would never advise you to do it all without a good reason for each of the ideas in the book.\nLiving documentation is not a free license to rehash old ideas from the 1990s. \nIn particular, beware of the following failure modes, which are not living \ndocumentation:\n •Doing living documentation for end-user documentation: Keep in mind that this book is not about end-user documentation at all. Some patterns may apply , but you still need skilled technical writing in order to produce  high-quality documentation for end users.\nChapter 11 Beyond Documentation: Living Design 330\n •MDA and everything code generation: No, code is not a dirty detail to \nreplace or generate; it is the reference and the preferred media whenever pos-sible. Y ou should extend your language or choose a better programming lan-guage rather than generate code from diagrams.\n •Documenting everything, even automatically: Documenting has a cost, which must be weighed against the benefits. The ideal case is code being so self- descriptive that it needs nothing else, but even that is not an absolute. Perfection and the quest for purity often amount to procrastination—and should be avoided.\n •UML obsession: Some basic UML is fine, but it is not an end in itself. Chose the simplest notation that the intended audience will really understand with as few explanations as possible. Don’t obsess over generic notations; problem-specific or domain-specific notations are often more expressive.\n •Design patterns everywhere: Knowing patterns can be helpful, and you can use them to help document a design thanks to the vocabulary they bring. But don’t abuse patterns. Simplicity should be your first priority . T wo \nIF state-\nments might be better than a strategy pattern at times.\n •Analysis paralysis: Having a whole team spend 15 minutes together on the whiteboard before each important design decision is time well spent. Spending many hours or even days is a waste. I encourage you to start new features as a whole team, on the whiteboard for a short while but then moving on quickly to the IDE. And you can just invite the whole team again next time you face a prominent decision, unless you’re into mob programming, which makes the whole team together the permanent state.\n •Living documentation masterpiece: Aiming for perfection is really a form of procrastination. Keep in mind that living documentation is a means to help deliver production code, not the other way around.\n •Documentation before building: Documentation should reflect what’s actu-ally built rather than prescribe what will be built. If a project is interesting, then nothing can beat starting the code. Detailed design specs are a waste. Beyond a short statement or a documentation-driven README as described earlier in this chapter, your team should code and reflect along the way , collec-tively , in a just-enough, just-in-time fashion.\nProcrastination by Living Documentation\nAs developers, we are often tempted to make things more complicated than they need to be. This is true for production code, and it also true with living documenta-tion tools.\nBiodegradable Documentation 331\nWhen the everyday work looks boring, making it technically more complicated \nis a great way to have fun. However, it’s not professional. If you consider yourself a \nsoftware craftsperson, you know you should not be doing that. However, we all fall for it from time to time, usually without being aware of it.\nTherefore: If you really need a space where you can have fun and make things \nneedlessly overcomplicated, then by all means do it in the code of your living documentation tools, not in your production code. Your life and the life of your colleagues will be better as a result.\nI’m not saying that you should gold-plate your living documentation tools. I’m \njust saying that if you’re lucky enough to have some slack time and want to play , do it with your documentation, not with your code!\nBiodegradable Documentation\nY ou should understand by now that living documentation is not an end in itself but a means to an end. T rying to set up living documentation can reveal issues about the design or other aspects of your code. This provides an opportunity to improve the root cause, which is above all good for the project and the product; it also helps improve your living documentation. Making such improvements repeatedly leads to a stream of simplifications and standardizations. Eventually , everything becomes so simple and so standard that you don’t need documentation any longer—and that would be perfect.\nTherefore: Consider what it would take for documentation to become unneces-\nsary. This is the direction you should move.\nIt does not matter whether you actually reach this point or not, but it has to be \nthe goal: The goal of a living documentation initiative is to achieve the level of qual-ity where documentation is mostly unnecessary . The process starts with setting up a documentation effort (see Figure 11.3), which will reveal some issues, which you fix, reducing the need for more documentation—and you repeat as needed. The goal of living documentation is not to end up with a lot of beautiful generated diagrams and documents. Instead, those documents and diagrams should be considered workaround solutions or intermediate steps toward better solutions that need less documentation.\n \nOne former Arolla colleague once told me of an experience at a bank:\nIn that bank, I joined a team that took pride in conforming to every standard. I mean \nmarket standards, not in-house standards. The result was that I was able to be produc-tive as soon as the first day! Since I knew the technologies and their standard usage, I was immediately familiar on all the project perimeter. No need for documentation, no surprise, no need for any specific customization.\nChapter 11 Beyond Documentation: Living Design 332\nMake no mistake, this was taking a real and continuous effort indeed. Find out the \nstandards, find out the way to solve specific issues while still conforming to standards. \nThis was a deliberate approach, and the benefits were real, for everyone but especially for new joiners!\nIn the book Apprenticeship Patterns: Guidance for the Aspiring Software Crafts-\nman, Dave Hoover and Adewale Oshineye advocate creating feedback loops.7  Living \ndocumentation with generated diagrams, a glossary , a word cloud, or any other \nmedia is a feedback loop that can help you evaluate what you’re doing and check against your own mental model. This feedback loop becomes particularly useful when your mental model and the content of the generated documents don’t match.\nHygienic Transparency\nInternal quality refers to the quality of code, the design, and, more generally , the whole process from the nebulous needs to working software that delights people. Internal quality is not meant to satisfy ego or to be a source of pride; by definition, it is meant to be economical beyond the short term. It is desirable for saving money and time sustainably , week after week, year after year.\nThe problem with internal quality is that it’s internal, which means you can’t \nsee it from the outside. That’s why , to a developer’s eyes, so many software systems are awful in the inside. Nondevelopers like managers and customers can hardly \n7. Hoover, Dave, and Adewale Oshineye. Apprenticeship Patterns: Guidance for the Aspiring Software \nCraftsman. Sebastopol, CA: O’Reilly Media, Inc. 2009.No Documentation\nBiodegradable\nDocumentation\nFix\nIssuesReveal\nIssuesSet Up\nLiving\nDocumentLess\nNeed for\nDocument\nFigure 11.3 The long-term goal of  living documentation is for documentation to become \nunnecessary\nHygienic Transparency 333\nappreciate how bad the code is inside. The only hints for them are the frequency of \ndefects and the feeling that new features are delivered more and more slowly .\nEverything that improves the transparency of how software is made helps improve \nits internal quality . When people can see the ugliness inside, there’s pressure to fix it.\nTherefore: Make the internal quality of software as visible as possible to devel-\nopers and nondevelopers alike. Use living documents, living diagrams, code metrics, and other means to expose the internal quality in a way that everyone can appreciate, even without any particular skills.\nUse all this material to trigger conversations and as a support to explain how \nthings are and why they are this way and to suggest improvements. Make sure the living documents and other techniques look better when the code gets better.\nKeep in mind that the techniques that help make software more transparent can’t \nprove that the internal quality is good, but they can highlight when it is bad, and that’s useful.\n \nLe Corbusier and The Law of Ripolin\nLe Corbusier, in his book The Decorative Art of  T oday, explains in 1925 his fascination for Ripolin, a brand famous for its white paint. In the chapter “ A Coat of Whitewash: The Law of Ripolin,” he imagines every citizen being required to replace everything with a plain coat of Ripolin white paint (see Figure 11.4): “His home is made clean. There are no more dirty , dark corners. Everything is shown as it is. Then comes inner cleanness…once you have put \nRipolin on your walls you will be the master of your own house.”\n8\nGood documentation should have a similar effect on the inner cleanness of \nthe code—its design and any other aspect that becomes visible so that people can see its dirty facets.\nDiagnostic Tools\nThe line is very thin between typical documentation media like diagrams and glos-saries and diagnostic tools like metrics and word clouds.\n8. Le Corbusier. The Decorative Art of T oday . MIT Press, 1987.\nChapter 11 Beyond Documentation: Living Design 334\nWord Clouds of the Language in the Code\nA word cloud is a very simple diagram in which words that occur frequently appear \nin a bigger font than less frequent words. One way to quickly assert what an applica-tion is really talking about is to generate a word cloud out of the source code.\nWhat does a word cloud really tell you about your code? If technical words domi-\nnate, then you know the code does not really talk about the business domain (see Figure 11.5). On the other hand, if the domain language is dominant (see  Figure 11.6), you must be doing a better job.\n Figure 11.4 In a house with everything painted white, dirt is immediately visible\nFigure 11.5 With this word cloud, either your business domain is on string manipulation, or \nit’s not visible in the source code\nHygienic Transparency 335\nCreating a word cloud out of source code is not difficult; you don’t even have to \nparse the source code but can simply consider it as plain text and filter the program-\nming language keywords and punctuation, like this:\n1 // From the root folder of the source code, walk recursively\n2 through all *.java files (respectively *.cs files in C#)34 // For each file read as a string, split by the language5 separators (you may consider to split by CamelCase too):678 SEPARATORS = """";:.,?!<><=+-^&|*/\"""" \r\n {}[]()""""910 // Ignore numbers and tokens starting with '@', or that are11 keywords and stopwords for the programming language:12 KEYWORDS = { """"abstract"""", """"continue"""", """"for"""", """"new"""", 13 """"switch"""", """"assert"""", """"default"""", """"if"""", """"package"""", """"boolean"""", 14 """"do"""", """"goto"""", """"private"""", """"this"""", """" break"""", """"double"""", 15 """"implements"""", """"protected"""", """"throw"""", """"byte"""", """"else"""", 16 """"import"""", """"public"""", """"throws"""", """"case"""", """"enum"""",17 """"instanceof"""", """"return"""", """"transient"""", """"catch"""", """"extends"""", 18 """"int"""", """""""",  """"short"""", """"try"""", """"char"""", """"final"""", """"interface"""", 19 """"static"""", """"void"""", """"class"""", """"finally"""", """"long"""", """"strictfp"""",20 """"volatile"""", """"const"""", """"float"""", """"native"""", """"super"""", """"while"""" }21 22\nFigure 11.6 In this word cloud, you can clearly see the language of  Flottio fuel cards and \nfleet management\nChapter 11 Beyond Documentation: Living Design 336\n23 STOPWORDS = { """"the"""", """"it"""",""""is"""", """"to"""", """"with"""", """"what's"""", \n24 """"by"""", """"or"""", """"and"""", """"both"""", """"be"""", """"of"""", """"in"""", """"obj"""", 25 """"string"""", """"hashcode"""", """"equals"""", """"other"""", """"tostring"""", 26 """"false"""", """"true"""", """"object"""", """"annotations"""" }24 \nAt this point, you could just print every token that was not filtered, and copy/\npaste the console into an online word cloud generator such as Wordle.com.\nY ou can also count the occurrences of tokens yourself by using a bag (that is, a \nmultiset from Guava):\n1  bag.add(token)\nY ou could render the word cloud within an HTML page with the d3.layout.cloud.js \nlibrary by dumping the word data into the page.\nSignature Survey of the Shape of Code\nAnother low-tech idea for visualizing the design of code out of the source code is the \nsignature survey proposed by Ward Cunningham.9 The idea is to filter out every-\nthing except the language punctuation (commas, quotation marks, and brackets) as pure string manipulation on the source code files.\nFor example, consider this signature survey , which has three big classes:\nBillMain.java ;;;;;;;;;;;;;{;;{"""""""";;"""""""";;{"""""""";"""""""";}\n{;;{;;}};;;;{{;;;{;;}{;;};;;}}{;}"""""""";}{;}{;;"""""""";"""""""";;;"""""""";"""""""";;;"""""""";"""""""";"""";;"""";;;"""""""";"""""""";;;"""""""";"""""""";;;};;{;{""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;;;;;;}""""""""{;}""""""""{;}""""""""{;};""""""""{;;;;;}""""""""{;;;;;}};}{;;;;;""""""""{"""""""";"""""""";;}""""""""{"""""""";"""""""";;""""""""{;}}""""""""{"""""""";"""""""";;""""""""{;}};{;}""""""""{;}{;};;;;;}{;;;;;;}{;;;;;;}{;""""""""{;{;}{;};;}{;{;}{;};;};}{{"""""""";}{"""""""""""""""";}{"""""""";};;{;}{"""""""";};}{{;};"""";"""";;;{""""""""{{"""""""";};}}{{;;;}}{;};}{;{;}"""";"""";;;{""""""""{{"""""""";};}}{;;{""""""""{{"""""""";}""""""""{;}{;}}}};{;;;}{"""""""";;;;;;;;}}{;{;}{;};}{;""""""""{;}{;};}{;{{"""""""";};}{{"""""""";};};}{;;;;;;;;;{{"""""""";};;}{{"""""""";};;;};}{;;;;;;;;;{;;;{"""""""";}{{"""""""";};}{{"""""""";};;};}\;}{;;""""""""{;}{;};}{;;{""""""""{"""""""";}{"""""""";};}{;}{{{;}{;}}};}}\n9. Ward Cunningham, “Signature Survey: A Method for Browsing Unfamiliar Code,” http://c2.com/doc/\nSignatureSurvey/"",22578",22913,"**Living Documentation vs. Avoiding Documentation**\n* **Documentation-driven development**: starting with the end goal, focusing on what's necessary to achieve it\n	+ Conversations, diagrams, notes, sketches, README files, code exploration in REPLs, or using scripting languages\n	+ Becomes more formalized as project stabilizes (e.g., tests, TDD, BDD)\n* **Avoiding documentation**: not an end in itself but a means to an end\n	+ Documentation reveals issues that can be fixed to improve the root cause\n	+ Continuous effort required for improving internal quality and hygienic transparency\n\n**Documentation Media vs. Diagnostic Tools**\n* Diagnostic tools (e.g., metrics, word clouds) have a thin line with typical documentation media\n* Word cloud: generates words from source code to understand the application's focus\n	+ Filters tokens and ignores numbers, stopwords, and keywords for Java or C#\n	+ Count occurrences using bag (Guava library) or render within an HTML page using d3.layout.cloud.js library\n* Signature survey: filters out everything except language punctuation to analyze the shape of code.\n\n**Documenting Errors or Avoiding Them**\n* Comments in code can indicate areas for improvement, leading to more effective documentation\n* Code that handles errors can become redundant when better design practices are employed (e.g., total functions, error management)\n* Focus on designing self-documenting code instead of relying on extensive documentation.",1462,85.80490136146545
False,3,"160-Reporter Porter Interviewing Mr. Living Doc Doc.pdf,160-Reporter Porter Interviewing Mr. Living Doc Doc,""Hygienic Transparency 337\nCallsImporter.","160-Reporter Porter Interviewing Mr. Living Doc Doc.pdf,160-Reporter Porter Interviewing Mr. Living Doc Doc,""Hygienic Transparency 337\nCallsImporter.java ;;;;;;;{;;{{"""""""";};{;;"""""""";;;{;}\n{;;{;};};{;"""""""";{;;};;{;;{;};}{;}{;};}}{;}{{{;}{;}}}}{""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;}""""""""{;};}}UserContract.java ;;{;;;;;{;}{;}{;}{;}{;}{;}{;}{;}{;}{;}}}{{""""""""\nNow compare the signature survey above with this one, which does exactly the \nsame thing but with more smaller classes:\nAllContracts.java ;;;;;{;{;}{{;}}{"""""""""""""""";}}BillingService.java ;;;;;;;{;{"""""""";}{;;;;;}{"""""""";;}{;;"""""""";}{;}{;}{"""""""";;;;;}{"""""""";;}{;;{{;;}};}{;}}BillPlusMain.java ;;;;;;{{;"""""""";"""""""";"""""""";"""""""";"""""""";"""""""";}}Config.java ;;;;;;;{;{;{"""""""";;}{;}{{{;}{;}}}}{;}{;;}{"""""""";}{"""""""";}{"""""""";}{"""""""";}{"""""""";;}{;"""";"""";{;};}}Contract.java ;;;;{;;;;{;;;;}{;}{;}{;}{;}{;}{"""""""""""""""""""""""""""""""""""""""";}}ContractCode.java ;{"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""";;{;}{;}}ImportUserConsumption.java ;;;;;{;;{;;}{{;}{;}}{;{;;}}{;"""""""";;;;{;};}{{;}{;}}{"""""""";;;{;{;}};}}OptionCode.java ;{"""""""""""""""""""""""""""""""";;{;}{;}}Payment.java ;;;{;;;{;;;{"""""""";}}{;}{;}{;}{{;}"""""""";}{;}{;;;;;}{;}{"""""""""""""""""""""""";}}PaymentScheduling.java ;;;;{{{;;;}}{{;;;}}{{;;;}};{;;;;}{;;{;};;}{;;;;;;;;}{;}}PaymentSequence.java ;;;;;;{;;{;}{;;}{;}{;}{;;;}{"""""""";}}UserConsumption.java ;{;;{;;}{;}{;}}UserConsumptionTracking.java ;{{;}{;}}\nWhich one do you prefer to work on?\nIt’s possible to imagine similarly low-tech yet useful plain-text visualization \napproaches. Let me know if you have any ideas.\nPositive Pressure to Clean the Inside\nOne huge issue in the field of software development is that internal quality is not vis-ible at all for the people who manage budgets and make the biggest decisions, such as saying yes or no to developers, contracting to another company , or offshoring. This lack of insight impedes these people from making good, informed decisions. Instead, it promotes decisions from people who are more convincing and seductive in their arguments.\nChapter 11 Beyond Documentation: Living Design 338\nDevelopers can become more convincing when they can show the internal  quality \nof code in a way that nontechnical people can apprehend emotionally . A  word \ncloud or a dependency diagram that is a total mess is easy to interpret even by  nondevelopers. Once they understand by themselves the problem shown visually , it becomes easier to talk about remediation.\nDevelopers’ opinions are often suspect to managers. In contrast, managers appre-\nciate the output of tools because tools are neutral and objective (or at least they believe this is the case). T ools are definitely not objective, but they do present actual facts, even if the presentation always carries some bias.\nThe ideas behind living documentation are not just to document for the team but \nto be part of the toolbox used to convince others. When everyone can see the disturb-ing reality—the mess, the cyclical dependencies, the unbearable coupling, the obscu-\nrity of the code—it becomes harder to tolerate it all.\n \nLiving documentation involves making the internal problems of code visible to \neveryone, creating positive pressure that encourages cleanup of the internal quality .\nDesign Skills Everywhere\nEven if you start a living documentation journey with the goal of solving the docu-\nmentation problem, you’ll quickly discover that your actual problem is that the design is poor or arbitrary , perhaps a result of “design by coincidence.” T o solve the documentation problems, you have to solve the design problems. This is all good news indeed!\nThrough the focus on documentation, you end up with concrete visible criteria \nfor everyone to see the big mess that is the current state of the design. There is then positive pressure to improve the design, which has benefits that well exceed the obvi-ous benefits of documentation. But as mentioned before, there’s even more good news: Good design skills also make good living documentation skills. Focus on liv-\ning documentation and focus on software design skills. Practice both together, and everything will get better!\nSoftware design involves deciding carefully among all the possible ways to code \nthe same behavior. Or, in Jeremie Chassaing’s words, “It’s picking one in gazil-lions of possibilities with good reasons.”\n10 “But it’s the same thing at the end!” is a LOL\nT o respect the acyclic dependencies principle: Have only one package! \nReporter Porter Interviewing Mr. Living Doc Doc 339\nfrequent objection during design discussions. Y es, it is. If you just care about making \nthe code work, design is irrelevant. Design means caring about concerns beyond just making it work.\nDesign skills include thinking about coupling and cohesion, hiding implementa-\ntion details, considering contracts and the governance of data, keeping options open for later, minimizing dependencies, and dealing with their relative stability , among other things.\nReporter Porter Interviewing Mr. Living Doc Doc\nThe following is an interview Reporter Porter (on the right in Figure 11.7) con-ducted with Living Doc Doc, an expert on living documentation (on the left in Figure 11.7).\n \nWhat is good documentation?\nThe best documentation is code that makes everything so obvious that you under-\nstand it immediately , with naming that is so good it’s instantly clear. Good docu-mentation is so integrated in the workflow and into the daily tools that you don’t even think about it as being documentation. One striking example is when a tool reminds you of something you forgot or didn’t know right when you need it. We Figure 11.7 Living Doc Doc interviewed by Reporter Porter\n10. On T witter @thinkb4coding https://twitter.com/thinkb4coding/status/837250039688933376\nChapter 11 Beyond Documentation: Living Design 340\nusually don’t call that documentation, but the end purpose of bringing the right \npiece of knowledge at the right time really is a form of documentation.\nWhy is living documentation not popular?\nI think many of the practices are popular, but nobody noticed. Remember all the \nfocus on UML in the early 2000s? Now projects are bigger today , and we don’t use UML much. Instead, every IDE offers instant, integrated, and highly contextual type hierarchy trees, outlines, smooth hypermedia navigation between classes,…and all this is more useful than hundreds of static UML diagrams. Still, we take it for granted and still feel bad about the “lack of proper documentation.” And there are new technologies as well.\nHow have new technologies changed the picture?\nMost people still haven’t realized all the potential of newer tools and practices \nwhen it comes to transferring knowledge.\nConsul and Zipkin offer live recaps of what’s actually there, even as living dia-\ngrams. They offer a tag mechanism to customize and convey intents.\nMonitoring of key SLA metrics with thresholds gets us close to documenting \nthe SLA.\nPuppet, Ansible, and Docker files allow for a declarative style for describing what \nyou expect. Imagine all the Word documents they advantageously replace!\nSo you need not do anything special now?\nAlmost. But not totally . All the new technologies and practices are fantastic \nfor documenting the what and the how, but the weak point mostly everywhere remains the rationale, the why , which is often forgotten. That’s why you still need \nto find a way to record the rationale for each of the main decisions. An immutable append-only log, code augmentation with tags, and a few evergreen content in tra-ditional documents for the overall vision can be invaluable to complete the picture.\nAnd what about the code?\nCode should be self-documenting as much as possible. Tests and business- \nreadable scenarios are an important part of this recorded knowledge. But some-times you have to add extra code just to record your design decision and intention right inside the corresponding code; custom annotations for documentation and naming conventions are your tools of choice here."",7933",8218,"**Living Documentation: The Importance and Benefits**\n* Living documentation makes internal code quality visible to everyone\n* Encourages positive pressure for cleanup of design problems\n* Developer opinions become more convincing with visualization approaches\n* Internal quality not easily understood by non-technical decision makers\n* Good design skills essential for effective living documentation\n* Documentation and software design go hand in hand\n* Living documentation makes problems visible, creating positive pressure for improvement\n\n**Challenges and Solutions:**\n* Lack of insight into internal code quality impedes informed decisions\n* Visualization approaches can help non-technical people understand the problem\n* Design skills are crucial for effective living documentation\n\n**Interview with Living Doc Doc:**\n* Good documentation is integrated, intuitive, and instantly understood\n* UML focus in early 2000s gave way to more contextual IDE tools\n* New technologies offer potential for transferring knowledge but not complete documentation\n* Immutable logs, code annotations, and traditional documents essential for recording rationale.",1151,45.15637397766113
False,3,"161-Summary.pdf,161-Summary,""Summary 341\nOkay, but these days systems are made of dozens of services. How do I work \nwith such fragmented systems?\n","161-Summary.pdf,161-Summary,""Summary 341\nOkay, but these days systems are made of dozens of services. How do I work \nwith such fragmented systems?\nY ou just apply the same techniques but at a different level. For example, annota-\ntions become tags in your service registry and in your distributed tracing system. Naming conventions of packages and modules become naming conventions of services and their endpoints. It involves similar thinking and similar design skills but different implementation.\nDo we really need documentation? We’ve been living with little or no docu-\nmentation for years, and we’re still alive!\nOf course, we can live without express documentation. Anyone can take an \nunknown system and make it work—at least under some definition of work . But \njust “making it work” is a very low bar, and “making it work” may take a lot of time. Documentation accelerates delivery because it shortens the time to rebuild your mental model of the system to work on. But the other effect of documenta-tion is that trying to record the knowledge about the system is a great way to learn what’s not right about the system. Paying attention to documentation is an invest-ment for later, obviously , but less obviously there’s also a return for right now!\nThank you very much!\nNo, thank you  very much!\nSummary\nThe main thesis of this book as a whole is that if you start with documentation, you \nend up with better design.\nMost teams initially embrace BDD for non-regression testing purposes and end \nup realizing that the bigger benefits are somewhere else—in the early conversations using concrete examples and in the resulting living documentation. Similarly , by reconsidering documentation; adopting practices that encourage speed, deliberate-ness, hygienic transparency , and interactions between people; and by listening to the signals throughout this process, good things happen.\nThis page intentionally left blank"",1916",1951,"**Documentation and Systems Design:**\n* Fragmented systems: apply same techniques at service level (tags, naming conventions)\n* No need for extensive documentation?\n	+ Can still function without it\n	+ Lowers development time\n	+ Reveals issues that may not be apparent otherwise\n* Benefits of documentation:\n	+ Accelerates delivery process\n	+ Encourages learning and understanding of the system.\n\n**Documentation and Book's Thesis:**\n* Start with documentation, end up with better design (main thesis)\n* BDD: non-regression testing not main benefit; early conversations and living documentation are more significant\n* Adopt practices for efficient documentation: speed, deliberateness, transparency, interactions between people.",727,22.643282175064087
False,3,"162-Stake-Driven Architecture Documentation.pdf,162-Stake-Driven Architecture Documentation,""343Chapter 12\nLiving Architecture \nDocumentation\nArchi","162-Stake-Driven Architecture Documentation.pdf,162-Stake-Driven Architecture Documentation,""343Chapter 12\nLiving Architecture \nDocumentation\nArchitecture can be defined in many ways: “ Architecture is what everybody in the \nproject should know” or “ Architecture is what is important, whatever that is” or “ Architecture is about the decisions that are hard to change later.” What is implied in all these definitions is that architecture involves exchanging knowledge about some decisions between multiple people over time. These decisions are not isolated events but are decisions in the context at the time.\nSo documentation is a significant part of architecture. A number of documenta-\ntion approaches have been proposed, and there are many books on this topic. This chapter focuses on how living documentation can help with architecture, especially in the context of teams practicing evolutionary architecture, where the architecture can change all the time.\nIn this view , architecture is not a phase but a continuous activity . Moreover, it is \nnot necessarily performed only by architects; rather, it is the domain of any software developer who has the skills to do it. This creates a need for even more people to share architectural decisions.\nA software architecture usually ends up being materialized as code in multiple \nplaces. This code is the consequence of the past architectural decisions. Y ou can rec-ognize many of these past decisions just by looking at the code base. With the right skills, you can recognize, or even reverse engineer, many of the past decisions just by \nnoticing the happy coincidences in the code base: Y ou might realize, “It cannot be that well-structured by chance, so it must have been designed for that.” The decisions are there, even if they are implicit.\nWith the right skills, usually grown from experience, you can read from the design \nwhat it is and how it is expected to be extended. This is similar to the power strip in Figure 12.1 that is visibly ready for extension if you just plug additional cords into it.\n \nChapter 12 Living Architecture Documentation 344\nHowever, because the decisions in code are implicit, by just looking at the code, \nyou may miss much of the architectural intent, depending on how familiar you are \nwith the styles being used. Helping you and others discover more of the architec-tural intent is one major goal of architectural documentation: Y ou want to make the implicit explicit!\nLiving documentation for architecture is precisely about finding practices that \nhelp explain more decisions accurately and explicitly , without slowing down the con-tinuous flow of changes that are expected and encouraged.\nThe chapters so far in this book have mentioned a number of examples of archi-\ntectural aspects, such as living diagrams and hexagonal architecture, context dia-grams, guided tours, code as documentation, and some enforced guideline examples. This chapter expands on all that and is dedicated to applying living documentation to software architecture.\nDocumenting the Problem\nArchitecture always starts with really understanding all the objectives and con-straints of the problem that needs to be solved. Y ou won’t build the same point-of-sale system for a brand with 50 hot dog stands in the street as you would for 1,500 high-end sandwich and salad shops around the world, even if they have the same high-level basic features.\nThe high-level goals and the main constraints are “things that everybody should \nknow” (see Figure 12.2), and as such they are always part of the architecture.\n \nTherefore: Whatever your definition of architecture, make sure it is considered as much a documentation challenge as a technical one. Focus discussions and written records on the problems to solve, not just on the solutions. Make sure the essential knowledge about the problem is well described in a persistent form, and ensure that it is in everybody’s mind.Visibly\nReady for\nExtension\nFigure 12.1 Visibly ready for extension\nDocumenting the Problem 345\nY ou might ask random questions from time to time to check whether everyone \ninvolved knows about the essential business knowledge. I regularly like to do this to \nmake sure we don’t waste a lot of time in every discussion.\nKeep in mind that a written form of documentation is never enough; not everyone \nwill read it. Y ou need to complement written documentation with random discus-sions and roadshows to present it to every team during official work time.\nAn Example of a Problem Brief\nHere is an example inspired by a real-world project on a legacy system for one of my customers. The brief is not in the wiki but in a single Markdown text file at the root of the source code repository of the new component. It may even be in the README file.Figure 12.2 Architecture is about things that everybody should know\nVision Statement\nDate: 01/06/2015\nDelight the users with great UIs and new features delivered frequently\nDescription\nThe INSURANCE 2020 program aims at revamping the legacy software  supporting the insurance claim management processes, with two main goals in target:\n 1. User experience (UX) and user-friendliness UIs\n 2. Continuous delivery: reduced time to market and reduced cost of change\nChapter 12 Living Architecture Documentation 346\nExplicit Quality Attributes\nIn software, quality attributes shape the solution. The technical solution to a given \nbusiness problem would be radically different for millions of concurrent users than it Stakeholders\nThe primary stakeholders are the insurance adjusters. Other stakeholders are:\n •Actuaries\n •Management\nIT-related stakeholders are:\n •Development team\n •Central architecture group\n •Support and Ops teams\nBusiness Domain\nThe business domain focuses on the claim management part, and in particular the Claim Adjustment phase. This starts at the early mention of a claim to start every investigation necessary to plan, witness the damages, contact the police officers, lawyer in order to propose a monetary amount to give to the policy holder.\nThe main business capabilities include, for example, the following:\n •Take note of a claim without much information about it\n •Enrich the claim whenever more information is available: parties involved, checks, evidences, photographs…\n •Prepare the claim with one or more settlement offer(s) (each made of one or more monetary amount(s))\n •Manage the claim team and the related workflows\n •Report the current state of one or all pending claims\n •Help users see their tasks to do at any time\nExplicit Quality Attributes 347\nwould be for 100 concurrent users, and the solution for real time would be quite dif-\nferent from the solution for daily use or if each minute of downtime cost the com-pany $500,000.\nBecause of the challenges involved, everybody on a team should be aware of the \nmost challenging quality attributes. They should also understand that other qual-ity attributes that are not as challenging present opportunities to keep the architec-ture simple. Pretending that your design should support millions of concurrent users when you really have only thousands is a dangerous misuse of the sponsor’s money and time.\nTherefore: At the start of a project and after each change of context, clarify the \nmain quality attributes in writing. This can be as simple as a list of bullet points. Make it clear how to interpret the quality attributes, such as by using maxims as guidance.\nThe following is an example of describing the main quality attributes: “The sys-\ntem shall respond to user requests with a response time within 1 second for 98% of the transactions. The system shall support 3,000 concurrent users.”\nThe book Site Reliability Engineering, and especially the “Service-Level Objec-\ntives” chapter, offers an in-depth discussion of quality attributes, introducing the concepts of service-level indicators, service-level objectives, and service-level agreements.\n1\nQuality attributes could come with some internal guidance on how to interpret \nthe them, as in this example:\nOver-quality is NOT quality\nDesign for ~10X growth, but plan to rewrite before ~100X2\nThese quality attributes can then be turned into executable scenarios against the \nsystem, and you can express the quality attributes in plain English sentences (see the \nsection “Test-Driven Architecture,” later in this chapter).\nStake-Driven Architecture Documentation\nThere are many perspectives on architecture. Some developers consider architecture as being all about the large-scale system, with its infrastructure, expensive middle-ware, distributed components, and database replication. It is normal for different \n1. Beyer, Betsy , Chris Jones, Jennifer Petoff, and Niall Murphy . Site Reliability Engineering. Boston: \nO’Reilly , 2016.\n2. Jeff Dean, “Challenges in Building Large-Scale Information Retrieval Systems,” Google, http://\nstatic.googleusercontent.com/media/research.google.com/en//people/jeff/WSDM09-keynote.pdf\nChapter 12 Living Architecture Documentation 348\npeople working on different systems to focus on different aspects of software and \ncall it architecture: They may use the term architecture for the aspects of the software that are most at stake in their context.\nThis diversity of perspectives is made obvious when doing an architectural kata. \nIn this workshop format, proposed by Ted Neward\n3, groups of people are tasked \nwith creating an architecture for a given business problem. Each group has 30 min-utes and a big piece of paper with markers to prepare and present a proposal. The rules clearly emphasize that the group members should be able to justify any deci-sion made. The workshop ends with each group presenting its architecture to every-one else, as if defending the proposal in front of a client. Other attendees are invited to ask questions to challenge the proposal, as a skeptical client would do.\nSuch a workshop provides a very interesting way to think about architecture. It is \nin itself a communication exercise. It is not just about the decisions made but also about expressing them in a convincing way . Invariably , a kata reveals how very differ-ently people think about the same problem.\n Caution\nY ou might be tempted to use the kata method on real business cases, as a form of competitive engineering, with different groups proposing different views that are later compared. However, the risk is that on a real case, you would have “winner” and “loser” groups at the end. Y ou should practice the kata idea several times as pure katas first, without real stakes. Y ou will get a lot of value and thinking out of it, and you will also learn how to avoid the “winner versus loser” effect.\n \nWhat I have learned from katas is that different business problems call for focus-\ning on different areas. The main aspects of a point-of-sale system for a hot dog ven-dor in the street are to be lightweight, low cost (in case it is stolen), and easy to use while making hot dogs in a hurry in the middle of a little crowd. In contrast, a mobile app meant to sell itself on an app store has to be primarily visually attractive. As yet another example, an enterprise system meant to serve millions of transactions per second should above all focus on performance as its main stake. In addition, with some systems the main stake is deeper understanding of the business domain.\nThe key stakes of the system are the primary information to record for everyone \nto know . Y ou wouldn’t, for example, want to spend too much of your time docu-menting the server technology stack when the main stake of the whole project is on the UX.\n3. https://archkatas.herokuapp.com"",11658",11758,"**Living Architecture Documentation**\n\n**Understanding Architecture**:\n- Architecture is defined in various ways: ""Architecture is what everybody in the project should know"", ""Architecture is about the decisions that are hard to change later""\n- Architecture involves exchanging knowledge about decisions between multiple people over time\n- Documentation is a significant part of architecture\n\n**Living Documentation for Architecture**:\n- Helps explain more decisions accurately and explicitly without slowing down continuous flow of changes\n- Focuses on practices that help document architecture, such as living diagrams, hexagonal architecture, context diagrams, guided tours, code as documentation, and enforced guideline examples\n\n**Documenting the Problem**:\n- Architecture starts with understanding all objectives and constraints of the problem to be solved\n- Written form of documentation is not enough, need to complement it with random discussions and roadshows\n\n**Quality Attributes**:\n- Shape the solution in software development\n- Should be clarified at the start of a project and after each change of context\n\n**Stakeholder-Driven Architecture Documentation**:\n- Different perspectives on architecture: infrastructure, expensive middleware, distributed components, database replication\n- Diversity of perspectives revealed through architectural kata workshops\n- Caution against using kata method on real business cases as ""winner versus loser"" effect\n\n**Key Stakes of the System**:\n- Primary information to record for everyone to know\n- Depending on the system, main stakes can be different: low cost, easy use, visually attractive, performance, deeper understanding of business domain.",1698,58.963603258132935
False,3,"163-Explicit Assumptions.pdf,163-Explicit Assumptions,,0","163-Explicit Assumptions.pdf,163-Explicit Assumptions,,0",56,"**Explicit Assumptions (PDF)**\n\n**Topic:** Explicit Assumptions\n\n**Notes**:\n- PDF titled ""163-Explicit Assumptions""\n- Contains information on explicit assumptions\n\n**Keywords**:\n- Explicit assumptions\n- PDF\n- Information\n\n**Word List**:\n- Contains\n- Information\n- Keywords: explicit assumptions, PDF\n- Topic: Explicit Assumptions\n- Notes: 163-Explicit Assumptions (PDF)\n\n**Continued Discussion**\n- Provide more details or examples of explicit assumptions\n- Discuss the importance of recognizing and understanding explicit assumptions in various contexts (e.g., business, research, personal relationships)\n\n**Possible Questions**:\n1. What are some common explicit assumptions that we often make when reading a document or interpreting data?\n2. How can acknowledging and addressing explicit assumptions help improve communication and collaboration in team settings?\n3. Can you think of any negative consequences of ignoring explicit assumptions in professional or academic work?\n4. Are there instances where explicit assumptions are better left unstated? If so, what are some examples and why?\n5. How can we ensure that we are accurately identifying and addressing explicit assumptions in order to avoid misunderstandings?",1220,27.02340793609619
False,3,"164-Evolving Continuously Change-Friendly Documentation.pdf,164-Evolving Continuously Change-Friendly Documentation,""Explicit Quality Attributes 349\n","164-Evolving Continuously Change-Friendly Documentation.pdf,164-Evolving Continuously Change-Friendly Documentation,""Explicit Quality Attributes 349\nTherefore: Identify early the main stake of the project, such as business domain \nchallenge, technical concern, quality of the user experience, or integration with other systems. You may answer the question, “What would most easily make the project a failure?” Make sure your documentation efforts primarily cover the main stake.\nExplicit Assumptions\nWhen knowledge is incomplete, as it usually is at the beginning of any interesting project, we make assumptions. Assumptions make it possible to move on, but at the expense of potentially being shown to be incorrect later on. Documentation makes it cheaper to rewind the tape when you reconsider an assumption. A simple way to cre-ate such documentation is to explicitly mark decisions with the assumptions they depend upon. This way , when an assumption is reconsidered, it is possible to find all its consequences so you can reconsider them in turn. For this to work efficiently , it should all be done as internal documentation, in place within the decisions (usually in the source code itself).\nBrevity Suggests Quality\nA good architecture is simple and looks obvious. It is also easy to describe in just a few sentences. A good architecture is a few key decisions, sharp and opinionated, that guide every other decision.\nIf architecture is “what everyone should know ,” then this puts an upper bound on \nits complexity . Anything that is complex to explain will not be understood by most.\n Tip\nI saw a good example of a good architecture in Fred George’s talk at Øredev 2013 on microservices architecture. Fred manages to explain the key ideas of this architecture in minutes. It sounds as if it was simplified, and it probably is—deliberately . There is a lot of value in a simplified architecture that can be quickly understood by everyone. Optimizing every detail is harmful if it makes the whole impossible to explain quickly .\n \nTherefore: Try to express an architecture out loud in less than two minutes as a test of its quality. If you succeed, write it down immediately. If it takes much longer and too many sentences to explain the architecture, then it can be improved a lot. Of course, an architecture may be too complicated to be explained in detail in two minutes. But this test challenges the presence of a high-level structure. An architec-ture should be more than just an inventory of details."",2409",2532,"**Documentation Best Practices**\n* Identify early the main stake of the project: business domain challenge, technical concern, user experience, or system integration\n* Explicitly document assumptions: mark decisions with their underlying assumptions for efficient reconsideration\n* Good architecture is simple and easy to describe in a few sentences: focuses on key decisions that guide others\n* Architecture's complexity should be limited to ensure understanding by all stakeholders\n* Expressing architecture out loud in less than two minutes can test its quality\n* Documentation should challenge the presence of a high-level structure, rather than an inventory of details.",674,19.43513011932373
False,3,"165-An Example of a Structured Decision Log.pdf,165-An Example of a Structured Decision Log,""Chapter 12 Living Architecture Documentation 350\nEvolvin","165-An Example of a Structured Decision Log.pdf,165-An Example of a Structured Decision Log,""Chapter 12 Living Architecture Documentation 350\nEvolving Continuously: Change-Friendly Documentation\nThe best architecture is an evolving creature, since it is hard to get it right on the first \ntry , and then it has to adapt to the changing context.\nA good architecture is easy to explain succinctly and minimizes the number of \ndecisions that are hard to change. Anything that is hard to change or that everybody should know has to be documented. It has to be persistent over time and made acces-sible to everyone, by definition.\nThis means that anything that makes an architecture or its documentation hard \nor expensive to change must be avoided. Y our team should learn how to make revers-ible decisions or to defer irreversible decisions. And if you fear changing the archi-tecture because you have a lot of static documentation about it that would have to be redone, your documentation is harming you, and you should reconsider how you do it.\nPay attention to how many words and diagrams are needed to explain the archi-\ntecture; the fewer being the better. Keep it all evolving, and remove any process or artifact that would impede continuous change.\nDecision Logs\nWhy does the project use this particular heavyweight technology? Hopefully it was chosen because of some requirements, following some evaluation. Who remembers that? Now that the work has changed, could you switch to something simpler?\nWhat do you talk about during meetings with the stakeholders? From incep-\ntion meetings to sprint planning meetings and other impromptu meetings, a lot of concepts, thinking, and decisions are covered. What happens to all this knowledge? Sometimes it only survives in the minds of the attendees. Sometimes it is quickly written as minutes of the meeting and sent by email. Sometimes a snapshot of the whiteboard is taken and shared. Some put everything into the tracker tool or in their wiki. One common problem is that this knowledge often lacks structure in the way it is organized.\nTherefore: Maintain a decision log of the most important architectural deci-\nsions. It can be as simple as structured text files in a folder at the root of the code repository. Keep the decision log versioned with the code base. For each impor-tant decision, record the decision, its rationale (why it was made), the main alternatives considered, and the main consequences, if any. Never update the entries in the decision log; instead, add a new entry that supersedes the previous one and provide a reference to it.\nDecision Logs 351\nMichael Nygard calls such a decision log an architectural decision record, or ADR \nfor short.4 Nat Pryce created adr-tools to support ADRs from the command line.5\nThe structuring assumptions that shape a solution are part of the decision log, \nas part of the rationale for an important decision. For example, if you assume that \narticles published in the past 24 hours represent over 80% of the visits on your web-site, then it will show in the rationale for the decision to partition recent news and archived news as two distinct subsystems, each with a different local architecture.\nIn practice, it’s not always easy to record the rationale for major architecture deci-\nsions, especially when the decisions are made for the wrong reasons (see Figure 12.3). For example, management might have insisted on including this technology or the developers might have insisted on trying this new library for resume-driven develop-ment reasons. It’s hard to make such rationales explicit in writing for everyone to see!\n \nY ou can find good examples of ADRs online in the Arachne-framework reposi-\ntory of ADRs.6\nAn Example of a Structured Decision Log\nIn the example shown here, the decision log is maintained as a single Markdown file at the root of the new Claim Management repository , after the vision statement and the descriptions of the business domain and the main stakeholders.\n4. Michael Nygard, Think Relevance blog, http://thinkrelevance.com/blog/2011/11/15/documenting-\narchitecture-decisions\n5. Nat Pryce, https://github.com/npryce/adr-tools\n6. Arachne-Framework/architecture, https://github.com/arachne-framework/architectureWhy this state\nmachine\neverywhere? The\nmanagement\nasked for it…\nFigure 12.3 Decision log recording a not-so-solid rationale\nChapter 12 Living Architecture Documentation 352\nThe Main Decisions\nT o improve the overall user experience, it has been decided:\nA UX approach, with a focus on beautiful and user-friendly screens, \nresponsive across mobile devices, consistent between them regardless of the \nactual application behind, and with fast perceived response times. The focus is also on making sure that common tasks can be fulfilled efficiently , with few clicks and few pages navigation.\nThe context of the existing legacy software makes it hard to achieve the \nvision stated above. This is why a large part of the program is to revamp the legacy , by decommissioning it as much as possible. T o mitigate the risks of this decommissioning, the following decisions have been made:\n •A progressive approach, with frequent delivery: no Big Bang. New modules and legacy modules will co-exist, with a progressive migration to new code.\n •A domain-driven design approach to help partition the legacy in a way which makes sense at the business domain level, to better understand the domain mechanisms, and to be easier to evolve when the business rules change.\nAnother challenge is that many business rules are tacit in the mind of \nsenior adjusters and need to be formalized. On top of that, with claims tak-\ning months to complete, these rules may change during the life of a pending claim. As a consequence, the following decision has been made:\n •A business process modeling approach to formalize tacit domain business rules in one place which can be easily audited and changed.\nConsequences\nRisks\nOne risk is the lack of expertise in the selected approaches. T o mitigate this \nrisk, external experts have been involved:\n •UX experts (from the internal UX center)\n •DDD expert (from Arolla)\nAnother risk comes from the legacy context, in particular:\n •Cost of testing: The lack of automated tests of all kinds makes each \nrelease expensive (manual testing) and/or dangerous (not enough testing)\nDecision Logs 353\n •User-perceived performance: The legacy system is slow , which makes it \nnot suited for the expected response time perceived by end users.\nT o reduce the cost of testing, and to not impede the users during all the \nchanges in the legacy , test automation will be key (unit tests, integration tests, \nnon-regression tests) in order to protect the system against regressions or defects.\nOn the issue of user-perceived performance, the design will have to find \nworkarounds to improve the perceived performance even though the legacy code behind may remain slow .\nTechnical Decisions\nNew Claim Management as Single Source of Truth Until the Claim Is Accepted by the Customer\nAccepted on 01/12/2015\nContextWe want avoid confusion arising from unclear authority of data, which con-sumes developer time to fix failing reconciliations. This requires that only source of truth (aka Golden Source) can exist at any point in time for a given piece of domain data.\nDecision\nWe decide that Claim Management is the only source of truth (aka golden source) for claim on claim inception and until the claim is accepted by the customer, at which time it is pushed to the legacy claim mainframe. From \nthe moment it is pushed, the only source of truth is the legacy claim main-frame (LCM).\nConsequences\nGiven the legacy background, it is unfortunately necessary for some time to have a different golden source across the life of a claim. Still, at any point in the life of the claim, the authoritative data are clearly in one single source. This should be re- considered to move to one constant single source whenever possible.\nBecause of that discrepancy , before the push: commands to create or \nupdate a claim are sent to Claim Management, with events sent around and in particular to LCM to sync the LCM data ( Legacy claim mainframe as a \nRead Model). After the push: remote calls to LCM are used to update the \nChapter 12 Living Architecture Documentation 354\nclaim in LCM, with events sent back to Claim Management to sync it ( Claim \nManagement as a Read Model).\nSee “CQRS, Read Models and Persistence” on InfoQ (https://\nwww .infoq.com/news/2015/10/cqrs-read-models-persistence).\nCQRS and Event Sourcing\nAccepted on 01/06/2015\nContext\nIn the claim adjustment domain, audit is paramount: We need to be able to tell what happened in an accurate fashion.\nWe want to exploit the asymmetry between write and read actions to the \nClaim Management models, in particular to speed up read accesses.\nWe also want to keep track of the user intents by being more task oriented.\nDecisionWe follow the CQRS approach combined with EVENT SOURCING.\nConsequences\nWe chose AxonFramework to structure the developments with its ready-made interfaces, annotations, and boilerplate code already written.\nValue-First\nAccepted on 01/06/2015\nContext\nWe want to avoid bugs that arise from mutability .\nWe also want to reduce the amount of boilerplate code necessary in Java to \ncreate value objects.\nDecision\nWe favor value objects whenever possible. They are immutable, with a valued constructor. They may come with a builder when needed.\nConsequences\nWe chose Lombok framework to help generate the boilerplate code for value objects and their builders in Java."",9610",9709,"**Structured Decision Logs**\n\n**Benefits of Using Structured Decision Logs**:\n- Evolving architecture documentation\n- Easy to explain and minimize hard-to-change decisions\n- Documentation is persistent over time and accessible to everyone\n- Avoids impediments to continuous change\n\n**Decision Logs**:\n- Maintain a decision log of important architectural decisions\n- Record:\n  - Decision\n  - Rationale (why it was made)\n  - Main alternatives considered\n  - Main consequences, if any\n- Keep the decision log versioned with the code base\n- Add new entries that supersede previous ones and provide a reference\n\n**Architectural Decision Records (ADRs)**:\n- Called ""architectural decision records"" or ""ADRs""\n- Supported by tools like adr-tools\n- Include structuring assumptions that shape the solution as part of the decision log\n\n**Documenting Controversial Decisions**:\n- Not always easy to record rationale for major architecture decisions, especially when made for the wrong reasons\n- External experts can be involved to mitigate risks\n\n**Technical Decisions**:\n- New Claim Management as Single Source of Truth Until Accepted by Customer\n  - Claim Management is the only source of truth until claim is accepted\n  - Before push: commands sent to Claim Management, events sent to LCM to sync\n  - After push: remote calls to LCM to update claim, events back to Claim Management\n- CQRS (Command-Query Responsibility Segregation) and Event Sourcing\n  - Keep track of user intents and exploit the asymmetry between write and read actions\n  - Choose AxonFramework for implementation\n- Value-First Approach\n  - Avoid bugs from mutability and reduce boilerplate code\n  - Use value objects whenever possible, they are immutable with a valued constructor.",1742,59.32905316352844
False,3,"166-Journals or Blogs as Brain Dumps.pdf,166-Journals or Blogs as Brain Dumps,,0","166-Journals or Blogs as Brain Dumps.pdf,166-Journals or Blogs as Brain Dumps,,0",80,**Journaling or Blogging as Brain Dumps**:\n- Provide a place to write down thoughts and ideas\n- Can be beneficial for personal growth and reflection\n- Helps process emotions and clarify goals\n- Digital platforms (blogs) offer wider audience and potential impact\n- Important: Do not replace face-to-face communication or professional help when needed.,350,9.057068347930908
False,3,"167-The Architecture Landscape.pdf,167-The Architecture Landscape,""Fractal Architecture Documentation 355\nJournals or Blogs as Brain Dumps\nAn altern","167-The Architecture Landscape.pdf,167-The Architecture Landscape,""Fractal Architecture Documentation 355\nJournals or Blogs as Brain Dumps\nAn alternative to using a formal decision log is to dump your brain by telling the full \nstory of what happened, what you learned, and how the team came up with a deci-sion, a trade-off, or a particular implementation detail.\nIn the book Apprenticeship Patterns: Guidance for the Aspiring Software Crafts-\nman, Dave Hoover and Adewale Oshineye advocate recording what you learn and sharing what you learn.\n7 A blog written by team members is a nice complement to \nany other kind of documentation. It is more personal, and it tells stories that are more compelling than most documentation. It tells important bits of the adventure, and the feelings of the people who were part of it.\nDan North (@tastapod) seems to agree. He said the following on T witter, when \ntalking to Liz Kheogh (@lunivore) and Jeff Sussna (@jeffsussna):\nI like having a product and/or team blog. Journaling decisions and conversations as you \nhave them documents history . It also shows how decisions got made, and lets you see changing tastes or learnings over time.\nFractal Architecture Documentation\nWhen dealing with a large system, you should give up on the idea of having one sin-\ngle uniform architecture everywhere. A system is made of several subsystems, and each should have its own architecture, plus the overall architecture of how they’re interrelated.\nTherefore: Consider your system as several smaller subsystems, or “modules.” \nThey may be physical units, such as components or services, or just logical mod-ules at compile time. Document the architecture independently for each module and describe the overarching architecture between the modules as one system-level architecture.\nTypically , you document the architecture of each module with internal documen-\ntation, using a combination of package naming conventions, annotations in the source code, and a little plain text. Y ou would document the overall architecture with more evergreen documents in plain text and perhaps some specific DSL if you have one that fits. However, the documentation of the overall architecture could also use some generated documents built by consolidation of the knowledge extracted from each module.\n7. Hoover, David H., and Adewale Oshineye. Apprenticeship Patterns: Guidance for the Aspiring \nSoftware Craftsman. Sebastopol, CA: O’Reilly Media, Inc. 2009.\nChapter 12 Living Architecture Documentation 356\nThe Architecture Landscape\nY our architecture deserves more than just a random bunch of diagrams and other \ndocumentation mechanisms; all these efforts can be organized into a whole that we can call an architecture landscape, drawing inspiration from what Andreas Rüping calls a document landscape in his book Agile Documentation: A Pattern Guide to Producing Lightweight Documents for Software Projects. In this book, Andreas sug-gests organizing the documents into a “landscape that team members use as a mental map when they retrieve or add information.”\n8 The idea is that the structure of the \ndocuments helps users navigate them and may also add knowledge in itself. In the case of living documentation, the problem is to imagine an overarching structure linking documents and diagrams, generated or not.\nTherefore: As your documentation grows to include a number of documenta-\ntion mechanisms, organize it into a consistent whole that people can learn to navigate efficiently. Document your documentation or conform to standards for that.\nReady-made architecture document templates may provide inspiration, if you \nhappen to like them:\n •Arc42\n •IBM/Rational RUP\n •Company-specific templates\nSome templates try to plan for every possible bit of architectural documentation \nneed. I loathe having to laboriously fill in large templates.  \nLOL\nI’ve spent one week working on a Software Architecture Document, friendly called SAD . No acronym would be more appropriate. \n—@weppos on Twitter  \n8. Rüping, Andreas. Agile Documentation: A Pattern Guide to Producing Lightweight Documents for \nSoftware Projects. Chichester, England: John Wiley & Sons, Ltd. 2003.\nThe Architecture Landscape 357\nTemplates are most useful as checklists. For example, the ARC 42 “Concepts”9 \nsection is a nice checklist that can help you find out what you may have forgotten to \nconsider. The following is an abbreviated list from the original template:\n •Ergonomics\n •T ransaction Processing\n •Session Handling\n •Security\n •Safety\n •Communication and Integration\n •Plausibility and Validity Checks\n •Exception/Error Handling\n •System Management and Administration\n •Logging, T racing\n •Configurability\n •Parallelization and Threading\n •Internationalization\n •Migration\n •Scaling, Clustering\n •High Availability\nHow many of these aspects do you neglect in your current project? How many of \nthem do you neglect to document?\nY ou can draw inspiration from all these established formalisms to derive your own \ndocumentation landscape, on a module-by-module basis. As seen previously in the \nsection, “Stake-Driven Architecture Documentation,” focus each documentation landscape on what matters most for the stakes of this subsystem.\nOn a module with a rich business domain, you would focus primarily on the \ndomain model and its behaviors as key scenarios. On a more CRUD-ish module, there might be very little to say , as everything is standard and obvious. On a legacy system, the testability and migration may be the most challenging aspects and would deserve the documentation.\n9. arc42, http://www .arc42.org"",5615",5688,"**Fractal Architecture Documentation: Living Architecture Landscape**\n\n**Alternative to Formal Decision Log:**\n- Dumping brain by telling the full story\n- Documenting decisions, trade-offs, implementation details\n- Personal blog as complement\n  * Compelling stories\n  * Shows changing learnings over time\n  * Documents history\n\n**Fractal Architecture:**\n- Large systems made up of several subsystems or modules\n- Each module has its own architecture and documentation\n- Document independently and describe overarching architecture between modules\n\n**Documentation Mechanisms:**\n- Internal documentation: package naming conventions, annotations in source code, plain text\n- Evergreen documents in plain text and specific DSL\n- Consolidated knowledge from each module\n\n**Organizing Documentation:**\n- Create a consistent whole for documentation\n- Navigate efficiently\n- Use ready-made templates as checklists\n\n**Formalisms Inspiration:**\n- ARC 42 ""Concepts"" section: checklist of aspects to consider\n  - Ergonomics, Transaction Processing, Session Handling, Security, etc.\n\n**Deriving Own Documentation Landscape:**\n- Focus on what matters most for each subsystem or module\n- Rich business domain: focus on domain model and behaviors\n- CRUD-ish module: little to document as it's standard and obvious\n- Legacy system: testability, migration are challenging aspects to document.",1375,41.194896936416626
False,3,"168-An Architecture Codex.pdf,168-An Architecture Codex,""Chapter 12 Living Architecture Documentation 358\nY our documentation landscape might be a pl","168-An Architecture Codex.pdf,168-An Architecture Codex,""Chapter 12 Living Architecture Documentation 358\nY our documentation landscape might be a plain-text file with predefined bullets \nand tables, or it can take the form of a small library of annotations, directly mark-\ning the source code elements with their architectural contributions and rationales. It could be a specific DSL. In practice, you would mix all these ideas according to what works best. Y ou might even use a wiki or proprietary tools that might instantly solve all your problems.\nA typical documentation landscape for a system would have to at least describe \nthe following points:\n •The overall purpose of the system, its context, users, and stakeholders\n •The overall required quality attributes\n •The key business behaviors and business rules and business vocabulary\n •The overall principles, architecture, technical style, and any opinionated decision \nThis does not mean at all that you need to create documents with all this informa-\ntion. Living documentation is all about reducing the need for manually written docu-ments and using alternatives that are cheaper and remain up-to-date.\nFor example, you could use plain-text evergreen documents for the first point, \nsystem-level acceptance tests for the second point, a BDD approach with automation for the third point, and a mix of a README, a codex, and custom annotations in the source code for the final point.\nArchitecture Diagrams and Notations\nMany authors have for a long time proposed formalisms to describe software archi-tecture. A number of standards are available, such as IEEE 1471, “Recommended Practice for Architecture Description of Software-Intensive Systems,” and ISO/IEC/IEEE 42010, “Systems and Software Engineering Architecture Description.” Kruchten’s “4+1 model” has gained recognition in the enterprise world. However, all these approaches are not precisely lightweight, and they require some learning curve \nto be understood. Each provides a set of views to describe different aspects of the software system, with a logical view , a physical view , and so on. Overall, these approaches are not particularly popular among developers.\nSimon Brown acknowledged the need for a lightweight alternative and conse-\nquently proposed the C4 Model,\n10 a lightweight approach to architecture diagrams \nthat is becoming increasingly popular among developers. This approach draws in \n10.  Simon Brown, Coding the Architecture blog, http://www .codingthearchitecture.com/2014/08/24/\nc4_model_poster.html\nAn Architecture Codex 359\nparticular from the work of Nick Rozanski and Eoin Woods, in their book Software \nSystems Architecture,11 and has the benefit of being usable without prior training. \nIt suggests four simple types of diagrams to describe a software architecture:\n •System context diagram: A starting point for diagramming and documenting \na software system, which allows you to step back and look at the big picture\n •Container diagram: T o illustrate the high-level technology choices, show-ing web applications, desktop applications, mobile apps, databases, and file systems\n •Components diagram: A way to zoom into a container, by decomposing in a way that makes sense to you (services, subsystems, layers, workflows, and so on)\n •Classes diagrams: (Optional) T o illustrate any specific implementation detail with one or more UML class diagrams\nMy favorite is the system context diagram, which is both simple and obvious but \nis often neglected.\nI think that these generic notations will never be enough. Strong architectural \nstyles should be expressed with their own specific visual notation. So while it’s obvi-ously a good thing to learn the standard notations, you shouldn’t limit yourself to them but should feel free to explore your own or more specific alternatives if they are more expressive.\nAn Architecture Codex\nWhen describing a solution to people, the most critical part is to share the thinking and reasoning that led to the solution.\nRebecca Wirfs-Brock was at the ITAKE un-conference in Bucharest in 2012, and \nduring her talk and the later conversations we had about it, she gave the example of EcmaScript, where the thinking process is clearly documented. She mentioned the following as some of the rationales for decisions in ECMAScript:\n •Invoke similarities with other existing folklore\n •Usually we want to learn and understand as little as possible to do the job\n •Recipe for making change: Figure out how similar change has been done before\n11.  Rozanski, Nick, and Eoin Woods, Software Systems Architecture. Boston: Pearson Education, Inc., \n2012.\nChapter 12 Living Architecture Documentation 360\nLater I was working on a departmentwide architecture in a bank, and I intro-\nduced this idea of a codex of principles guiding all the architecture-sensitive deci-\nsions (see Figure 12.4). The codex was built from the accumulation of concrete cases of decision making by trying to elucidate formally the reasoning behind the decision. Often, the principle was already in the heads of other senior architects, but it was tacit, and nobody else knew about it.\n \nFigure 12.4 The all-mighty codex\nSome of these principles were as follows:\n •Know your golden source (that is, single source of truth).\n •Don’t feed the monster; improving the legacy only helps it last longer.\n •Increase the straight-through processing automation ratio.\n •Customer convenience should come first.\n •The API should come first!\n •A manual process is just a special case of an electronic process.\nThis codex proved useful for everybody involved in the architecture. The goal was \nto publish the codex for everyone, even if it was incomplete and not always easy to understand. At least it was useful for provoking questions and reactions. It was never formally published, but the content of the codex leaked on many occasions and has been used several times for more consistent decision making.\nOn a recent consulting gig I found it helpful to express the value referential of the \nteam as a list of preferences, including the following:\n •Code over XML.\n •The templating engine is okay , but keep the logic out.\nAn Architecture Codex 361\nOf course, it is a good idea to adopt standard principles already that are doc-\numented in the literature, too, as they provide ready-made documentation. For \nexample:\n •“Keep your middleware dumb and keep the smarts in the endpoints.”12 \nThis codex addresses the need to spread the knowledge about architectural rea-\nsoning across more people than just the architecture team, which is a concern you should have.\nTherefore: Start paying attention to how decisions are made and make the tacit \nprinciples, rules, and heuristics underneath explicit into a codex. It can be as simple as a bulleted list of single sentences. Keep it short and easy to grasp for most people, such as by using a short concrete example next to each item. Share this codex with people whenever the opportunity arises. You don’t have to get it formally approved to be useful. Change it continuously to keep it short and relevant.\nIt is very important to keep a codex a working document that is never finished. \nWhenever you hit a contradiction in its principles, it’s time to fix it or evolve it. This should not be seen as a failure but as an opportunity for collective decision making to be even more relevant. Architecture involves consensus, doesn’t it?\nAn architecture codex can be a text file in the source control, a set of slides, or \neven expressed in code. The following is an example of using a simple enum to mate-rialize the principles of a codex:\n1  /**\n2  * The list of  all principles  the  team  agrees on.3  */4  public enum Codex {56     /** We have no clue to explain this decision */7     NO_CLUE(""""Nobody""""),89     /** There must be only one authoritative place      * for each piece of data. */10    SINGLE_GOLDEN_SOURCE(""""Team""""),1112    /** Keep your middleware dumb and keep the smarts      * in the endpoints. */13    DUMP_MIDDLEWARE(""""Sam  Newman"""");\n12.  Newman, Sam. 2015. Building Microservices. Sebastopol, CA: O’Reilly Media, Inc., 2015."",8154",8223,"**Living Architecture Documentation**\n\n**Documentation Landscape**:\n- Can be a plain-text file with predefined bullets and tables\n- Can take the form of a small library of annotations, marking source code elements\n- Can be a specific DSL, wiki, or proprietary tools\n\n**Typical Documentation Landscape for a System**:\n- Describes:\n  - Overall purpose, context, users, and stakeholders\n  - Overall required quality attributes\n  - Key business behaviors, business rules, and business vocabulary\n  - Overall principles, architecture, technical style, and opinionated decisions\n\n**Architecture Diagrams and Notations**:\n- Formalisms like IEEE 1471 and ISO/IEC/IEEE 42010 are not lightweight\n- C4 Model: Lightweight approach to architecture diagrams, inspired by Kruchten's ""4+1 model""\n- Suggests four simple types of diagrams: System context, container, components, and classes\n\n**Architecture Codex**:\n- Documenting the thinking and reasoning behind solutions\n- Example: EcmaScript's documentation of rationales for decisions\n- Developing a codex of principles to guide decision making\n  - Examples: Keep your golden source, don't feed the legacy, increase straight-through processing automation\n- Sharing this codex with the team, keeping it as a working document that is never finished",1283,45.64367723464966
False,3,"169-Architectural Annotations.pdf,169-Architectural Annotations,""Chapter 12 Living Architecture Documentation 362\n14\n15    private  final  String au","169-Architectural Annotations.pdf,169-Architectural Annotations,""Chapter 12 Living Architecture Documentation 362\n14\n15    private  final  String author;1617    private Codex(String author) {18        this.author  = author;19    }20 }\nIn his book Building Microservices, Sam Newman says that his colleague Evan \nBottcher created a big poster on the wall, displaying the key principles visibly , organ-\nized into three columns from left to right:\n •Strategic Goals (for example, enable scalable business, support entry into new markets)\n •Architectural Principles (for example, consistent interfaces and data flow , no silver bullet)\n •Design and Delivery Practices (for example, standard REST/HTTP , encapsu-late legacy , minimal customization of COTS/SAAS)\nThis is a nice way to sum up the system vision, principles, and practices in one \nplace!\nTransparent Architecture\nWhen architecture documentation becomes embedded within software artifacts in each source code repository , with living diagrams and living documents generated out of them automatically , every individual has access to all the architecture knowl-edge. In contrast, in some companies the architecture knowledge remains in tools and slide decks only known by the official architects and not kept up-to-date.\nOne consequence of embedding architecture documentation within software arti-\nfacts is that it enables decentralization of the architecture and the decision-making dependent on architecture knowledge. I call this transparent architecture: If every-one can see the quality of the architecture by himself or herself, then everyone can make decisions accordingly , by themselves, without necessarily asking the people in an architect role (see Figure 12.5).\n \nFor example, in a microservices architecture, a transparent architecture can make \nuse of living system diagrams generated out of the working system at runtime. \nTransparent Architecture 363\nThis knowledge is already there in the distributed tracing infrastructure (for exam-\nple, Zipkin). Y ou may have to augment it a bit with custom annotations and binary annotations added in your instrumentation.\nY ou might as well rely on your service registry (for example, Consul, Eureka) \nand its tags to produce living documents. Dependencies between services can also be derived from consumer-driven contracts if you apply this practice. And if you care about the physical infrastructure, it can be made visible through custom living diagrams generated with Graphviz from data you get from your cloud through its programmatic API.\n13 Note that more “virtuous” practices also make living docu-\nmentation easier!\nY ou can achieve transparent architecture through augmented code, annotation \nin architectural documentation, decision logs and architectural enforced guidelines, which together can unlock the benefits of an architectural reality check, as described later in this chapter.\nArchitectural Annotations\nAny design information that can make the code more explicit is worth adding. If you follow the layers pattern, you can document code by using the custom annotation \n@Layer  on the package com.example.infrastructure/package-info.java  \nat the root of each layer:\n13.  James Lewis has been showing in some of his talks an example of a living diagram of a cloud infra-\nstructure generated from cron, Python, Boto, pydot, and Graphviz.Team A Team B?\nTeam C\nWithout access to the up-to-date\nwhole picture, each team can\nmake only local decisions.Team A Team B Team C\nWith access to the up-to-date\nwhole picture, each team can make\nglobal decisions by itself.Whole System\nFigure 12.5 With access to the whole picture, each team can directly make decisions that are \nconsistent with respect to the whole system\nChapter 12 Living Architecture Documentation 364\n1  @Layer(LayerType.INFRASTRUCTURE)\n2  package com.example.infrastructure;\nStereotype-like patterns represent intrinsic roles or properties that qualify a lan-\nguage element like a method. Consider this example:\n1  @Idempotent2  void  store(Customer customer);\nAlso consider this example:\n1  @SideEffect(SideEffect.WRITE,  """"Database”)2  void  save(Event  e){...}\nSpecific risks or concerns can also be denoted directly on the corresponding class, \nmethod, or field, as shown here:\n1  @Sensitive(""""Risk of Fraud"""")2  public final class CreditCard {...\nDesign patterns in general are good candidates for design annotations. Y ou place \nan annotation on an element that participates actively in the pattern. Y ou can check \nit by considering whether you should keep the element if you removed the pattern. If not, you can safely declare the pattern on it; the class or method is only there to realize the pattern. It is often the element in the role that has the name of the pattern itself, such as adapter or command.\nSometimes you need values in your annotations. For example, if you wanted to \ndeclare an occurrence of the DDD repository pattern that is manipulating a particu-lar aggregate, you could do it like this:\n1  @Repository(aggregateRoot  =  Customer.class)\n2  public interface AllCustomers {...\nY ou can create your own patterns catalog with the patterns you use most \n commonly . It might include patterns from the Gang of Four, DDD, Martin Fowler \n(analysis patterns and PoEAA), EIP , some PLoP and POSA patterns, and several well-known and/or trivial basic patterns and idioms, plus all your custom in-house patterns.\nIn addition, you might create custom annotations to classify some important \nsources of knowledge, such as business rules, policies, and so on, as shown here:\n1  @BusinessRule\n2  Public Date shiftDate(Date date, BusinessCalendar calendar){...}"",5667",5741,"**Architectural Annotations**\n\n**Key Principles**:\n- Strategic Goals: enable scalable business, support entry into new markets\n- Architectural Principles: consistent interfaces and data flow, no silver bullet\n- Design and Delivery Practices: standard REST/HTTP, encapsulate legacy, minimal customization of COTS/SAAS\n\n**Transparent Architecture**:\n- Architecture documentation embedded within software artifacts in each source code repository\n- Accessible to everyone, enabling decentralization of architecture and decision-making\n- Use of living system diagrams, distributed tracing infrastructure, service registry, and custom annotations\n\n**Architectural Annotations**:\n- Documenting code using custom annotations (e.g., @Layer)\n- Stereotype-like patterns to qualify language elements\n- Specific risks or concerns denoted directly on the corresponding class, method, or field\n- Design patterns documented using annotations\n\n**Creating a Patterns Catalog**:\n- Including patterns from various sources: Gang of Four, DDD, Martin Fowler, EIP, PLoP and POSA, and custom in-house patterns\n- Creating custom annotations to classify important sources of knowledge (e.g., business rules, policies)",1191,58.54708385467529
False,3,"170-Other Quality Attributes.pdf,170-Other Quality Attributes,""Transparent Architecture 365\nHere are some more examples:\n •@Policy  to highlight the","170-Other Quality Attributes.pdf,170-Other Quality Attributes,""Transparent Architecture 365\nHere are some more examples:\n •@Policy  to highlight the major company policy expressed in the software\n •@BusinessConvention  to denote the lower-level policies that are just con-\nventions in the business domain\n •@KeyConcept  or @Core  to emphasize what’s important\n •@Adapter  or @Composite  to denote the use of a pattern\n •@Command  or @Query  to clarify the semantics of write or read on a module or\n •@CorrelationID  or AggregateID  on a field\nEnforced Design Decisions\nThanks to the augmentation of code with design knowledge (using annotations, \nnaming conventions, tags in your service registry , or any other mechanism), you can delegate conformity checks to tools. Y ou can check dependencies according to the declared patterns and stereotypes knowledge. I like to raise an anomaly if a class annotated as a value object has field-level dependencies to classes annotated as entity or service. That’s my taste, and I often ask tools to check these things for me:\n1  if  (type.isInvolvedIn(VALUE_OBJECT))  {\n2    if  (dependency.isInvolvedIn(ENTITY)  ||3      dependency.isInvolvedIn(SERVICE))  {4        ... raise an anomaly5  }\nY ou can also create custom rules in your static analysis tool. For example, using \nthe SonarQube built-in architectural constraint template or a specialized architec-\ntural assertion library such as ArchUnit,14 you could create these rules:\n •“Persistence layer cannot depend on web code”: Forbid access to .web.  \nfrom classes in **.dao . \n •“Hexagonal architecture”: Forbid access from .domain.  to .infra .\n •“Value objects should not have services injected as member fields”:  Classes \nannotated by ValueObjects  should not have fields of a type annotated by \nDomainService .\n14.  ArchUnit, https://github.com/TNG/ArchUnit\nChapter 12 Living Architecture Documentation 366\nFor example, the following enforces a naming guideline mentioned in Chapter 8, \n“Refactorable Documentation”:\n1 @Test\n2 public void domain_classes_must_not_be_named_with_prefix() {3   noClasses().that().resideInAPackage(""""..domain.."""")4     .should().haveSimpleNameEndingWith(""""Service"""")5     .check(importedClasses);6   noClasses().that().resideInAPackage(""""..domain.."""")7     .should(new DomainClassNamingCondition())8     .check(importedClasses);}\nIn this case, DomainClassNamingCondition  is custom code to check that \nthe name doesn’t end with a prefix from this list: Service , Repository , Value-\nObject , VO, Entity , Interface , Manager , Helper , DAO, DTO, Intf , Controler , \nor Controller .\nThe following rule enforces the hexagonal architecture constraint “forbid access \nfrom domain code to infrastructure code”:\n1 @Test2 public void domain_must_not_depend_on_anything() {3   noClasses().that().resideInAPackage(""""..domain.."""")4     .should().accessClassesThat()5            .resideOutsideOfPackage(""""..domain.."""")6     .check(importedClasses);7 }\nThe name of the rules and its declarative description clearly document and pro-\ntect the design decisions—as plain source code.\nArchitectural Reality Check\nArchitecture should not be defined but discovered, refined, evolved, and explained. \n#theFirstMisconceptionAboutArchitecture\n—@mittie on Twitter\nThe old-fashioned idea of architecture as something to perform before doing the \nimplementation doesn’t fit well with modern projects. Change is expected every-\nwhere and at any time, in the code and in the architecture—whatever you call architecture.\nTest-Driven Architecture 367\nY ou want to ensure that the major quality attributes of the overall system are met \n(for example, conceptual integrity , performance, maintainability , security , fault toler-\nance) and that the most important decisions are being communicated to everyone involved. But you don’t want old-fashioned architecture practices to slow down the project. Y ou want fast documentation that can help communicate knowledge to eve-ryone and that can also help reason and make sure the quality attributes are satisfied.\nBut there is another problem: The concrete implementation of the architecture \nmay not match its intent. Coding decisions may drift day after day , one small mistake at a time, until the system bears no resemblance to the architecture it was meant to implement. This problem is called architecture erosion.\n15\nNote that the quality attributes requirements usually don’t change very frequently , \nbut the decisions in the code do.\nTherefore: Regularly visualize the architecture as the software changes. Com-\npare the architecture as implemented to your architecture as intended. If they differ, you may want to adjust one or the other. With automated support of liv-ing diagrams or other living documents, this comparison can be done as often as during each build.\nAll this assumes that you have some vision of what your intended architecture \nshould be. But if you don’t have one, you can gradually reverse engineer it from your architecture as implemented.\nThere are tools available that can help with architecture visualization and check-\ning, and you can also create your own living diagram generator totally dedicated to your own specific context.\nTest-Driven Architecture\nTest-driven development has a mindset that is not just for writing code “in the small.” It’s a discipline that involves first describing what you want, before you implement it, and then you make it clean to enhance your work in the longer term.\nY ou can try to follow this same process at the architecture scale. The challenges \nyou face are the larger scale of everything and the longer feedback loops, which means you may forget what you were after when you eventually get the feedback.\nIdeally , you would start by defining the desired quality attributes as tests. They \nwill not pass for weeks or months; when they eventually pass, they become the only \n15.  Ricardo Terra, Marco Tulio Valente, Krzysztof Czarnecki, and Roberto S. Bigonha, “Recommend-\ning Refactorings to Reverse Software Architecture Erosion,” 16th European Conference on Software \nMaintenance and Reengineering, 2012: gsd.uwaterloo.ca/sites/default/files/Full%20Text.pdf\nChapter 12 Living Architecture Documentation 368\nreally sincere documentation of the current quality attributes. For example, consider \nthis performance quality attribute:\n10k requests over 5mn with less than 0.1% error and response time within 100ms at 99.5 percentile\nFirst write it down in a bulleted list of quality attributes, such as in a Markdown \nfile. Then implement this criterion as literally as possible as a Gatlin or JMeter test \non a realistic environment (even on production). It’s not very likely that it will pass right away . Now the team can work on it, along with other things, depending on the priorities. It may take a few sprints to make it pass.\nY ou might already do something similar if you create test scripts for proofs of \nconcepts. Instead of throwing away these scripts afterward, it doesn’t take much more effort to turn experiments you’re already doing on a one-off basis into main-tainable assets that can assert that you still meet the requirements and that can docu-ment them at the same time.\nQuality Attributes as Scenarios\nA test should describe the quality attribute as declaratively as possible. One way to do this is to dress the criterion as a special Cucumber scenario:\n@QualityAttribute @SlowTest @ProductionLikeEnv @Pending\nScenario: Number of requests at peak time\nGiven the system is deployed on a production-like environmentWhen it receives a load of 10k requests over 5mn, the error rate is less than 0.1%, \nand the response time is below 100ms at 99.5 percentile\nNote the custom tags here:\n •@QualityAttribute : T o classify something as a quality attribute requirement\n •@SlowTest: T o launch something only as part of the nightly slow tests run\n •@ProductionLikeEnv : T o flag that this test is relevant only in a production-\nlike environment for the metrics to be meaningful\n •@Pending : T o signal that this scenario is not passing yet\nWith this approach, as soon as the scenario is written, it can become the single \nsource of truth for the corresponding quality attribute. Moreover, the scenario tests \nreports becomes the table of contents for these “non-functional requirements,” too.\nTest-Driven Architecture 369\nNote that the quality attributes scenarios are useful even if they are never actually \nimplemented as true tests.\nY ou might describe the quality attributes this way:\n •Persistence: “Given that a purchase has been written, when we shut down and \nthen restart the service and then the purchase, we can read all the purchase data.” Is it going too far to document the obvious like this?\n •Security: “When we run standard penetration testing suites, zero flaw is \ndetected.” Note that here the trick is the word standard, which refers to a more complete description somewhere outside the scenario. This external link is part of your documentation, too, even if you didn’t write it yourself.\nWhen the quality attribute can be checked at compile time, it will be part of your \nquality dashboard (for example, in Sonar). In this case, you can turn this tool into your table of contents for these quality attributes. And you might use something like the Build Breaker plugin to fail the build in case of too many violations. This is \nanother way of implementing enforced guidelines.\nQuality Attributes at Runtime in Production\nSome quality attributes are too difficult to test outside their natural habitat. Such situations call for a more monitoring-oriented approach. Netflix introduced the Chaos Monkey to assert fault tolerance at the service level. Later it introduced the Chaos Gorilla at the data center level:\nChaos Gorilla is similar to Chaos Monkey but simulates an outage of an entire Amazon \navailability zone. We want to verify that our services automatically re-balance to the functional availability zones without user-visible impact or manual intervention.\n16\nThe mere description of these two Chaos engines, along with their configuration \nparameters in terms of outage frequencies, is in itself a documentation of the fault-\ntolerance requirements.\nSome cloud providers or container orchestration tools support automatic roll-\nback if some metrics are degraded following a deployment. This configuration de facto documents what’s considered “normal” metrics (for example, CPU/memory usage, conversion rate).\n16.  The Netflix tech blog, http://techblog.netflix.com/2011/07/netflix-simian-army .html"",10607",10687,"**Quality Attributes and Architecture Documentation**\n\n**Transparent Architecture**:\n- Highlight major company policy expressed in software\n- Denote lower-level policies as ""BusinessConvention""\n- Emphasize what's important with ""@KeyConcept"" or ""@Core""\n- Use ""Adapter"" or ""Composite"" to denote pattern usage\n- Clarify semantics of write/read on modules with ""@Command""/""@Query""\n- Use ""@CorrelationID""/""AggregateID"" on fields\n\n**Enforced Design Decisions**:\n- Use naming conventions, tags in service registry, or other mechanisms to delegate conformity checks to tools\n- Check dependencies according to declared patterns and stereotypes knowledge\n- Raise anomalies if certain design decisions are violated (e.g., value objects with field-level dependencies to entity/service classes)\n\n**Architectural Reality Check**:\n- Architecture should not be defined but discovered, refined, evolved, and explained\n- Old-fashioned architecture practices can slow down the project\n- Test-driven development mindset can help ensure quality attributes are met\n- Regularly visualize and compare implemented architecture to intended architecture\n\n**Test-Driven Architecture**:\n- Define desired quality attributes as tests\n- Implement tests to check these requirements, even if it takes time\n- Use custom tags to classify, prioritize, and flag test scenarios\n- Turn one-off experiments into maintainable assets that document the quality attributes\n\n**Quality Attributes as Scenarios**:\n- Write quality attribute requirements as bulleted lists in a Markdown file\n- Implement tests using tools like Gatlin or JMeter to check these requirements\n- Use Cucumber scenarios to describe the quality attributes declaratively\n\n**Quality Attributes at Runtime in Production**:\n- Monitor fault tolerance with tools like Chaos Monkey and Chaos Gorilla\n- Document fault tolerance requirements through monitoring configurations\n- Use automated rollback mechanisms to enforce ""normal"" metrics (e.g., CPU/memory usage, conversion rate)",1999,112.80701065063477
False,3,"171-From Fragmented Knowledge to Usable Documentation.pdf,171-From Fragmented Knowledge to Usable Documentation,,0","171-From Fragmented Knowledge to Usable Documentation.pdf,171-From Fragmented Knowledge to Usable Documentation,,0",114,**Usable Documentation from Fragmented Knowledge**,50,4.755712032318115
False,3,"172-Small-Scale Simulation as Living Architecture Documentation.pdf,172-Small-Scale Simulation as Living Architecture Documentation,""Chapter 12 Living","172-Small-Scale Simulation as Living Architecture Documentation.pdf,172-Small-Scale Simulation as Living Architecture Documentation,""Chapter 12 Living Architecture Documentation 370\nOther Quality Attributes\nT o keep track of your expectations before doing experiments on the product, its suc-\ncesses and failures: http://growth.founders.as #startup #hypotheses \n—@fchabanois on Twitter\nSome quality attributes cannot be tested automatically (for example, financials \nexpectations, user satisfaction). These attributes often reside within spreadsheets on shared drives. Alternatives exist online to encourage sincere declarations of the objectives before comparing them against actual achievements. These kinds of tools encourage working in a TDD-ish fashion for startup objectives.\nFrom Fragmented Knowledge to Usable Documentation\nThe approach described in this section can end up producing many fragmented and heterogenous sources of truths about all the quality attributes. They need to be curated and consolidated into one or two living tables of content.\nTherefore: Dress your quality attribute tests as Cucumber scenarios and put them \nin a separate “Quality Attributes” folder (and hence into a separate chapter in the corresponding living documentation). Use tags to classify them more precisely. Decide on one existing tool to host the main table of content as the single point of entry for all the quality attributes documentation, with references to any other tool.\nFor example, you might decide that Cucumber is the main table of contents. Y ou \ncan then add pseudo-scenarios to link to the Sonar configuration and to the perma-links to the configuration of each static analysis tools. Y ou might also mention the \nChaos Monkey as a scenario and a link to its configuration on some Git repository .\nAlternatively , you might decide on your build tool as the main table of contents. \nBy adding custom steps in the build pipeline (for example, Jenkins, Visual Studio), you can pinpoint to Cucumber reports, Sonar reports, and the Chaos Monkey configuration.\nThese tools can at the same time be a table of contents and fail the build in case one \nof the quality attribute is not met anymore. This helps keep the documentation sincere. If you just use a wiki as the main table of contents, you no longer have that enforcement.\nSmall-Scale Simulation as Living Architecture \nDocumentation\nLarge and complex software applications or systems of applications are challenging \nin terms of documentation. Judging by the size of the source code and configuration they’re made of, the amount of knowledge that is necessary to describe them is so \nSmall-Scale Simulation as Living Architecture Documentation 371\nhuge it is useless as is. And at the same time, the critical higher-level design decisions \nand all the thinking that went into building the systems are often implicit.\nIf a system were smaller, it would be easier to understand. Just by reading the \nhandful of classes, running the few tests, exploring what happens when playing with the code in a REPL, and watching the dynamic behavior at runtime, you could quickly get a sound understanding of its purpose and of how it works. Even if the thinking that led the design were lost, you would be able to recover it from observing the small-scale system in action. This would be tacit knowledge, but it would still be much better than nothing.\nTherefore: Create a small-scale replica of your software system, such as a \nstripped-down reimplementation of just the key bits of code with some tests, for the sole purpose of documentation. Through aggressive curation, select a small subset of features and code that focus only on the one or two aspects that matter and that fits on your brain as a whole. Simplify every other distracting aspect, even if doing so makes it slower and gives it a limited set of features. Make sure this small-scale replica works realistically, producing accurate, if not perfect, results, though it does not necessarily need to do so in all cases.\nThe advantage of small-scale simulation is that it becomes human-scale; it fits \nin your head. Note that here when I say small-scale, I’m primarily talking about reduced complexity , not just reduced size.\nI’ve tried small-scale simulation several times:\n •When developing an exchange system for financial products, the core of the matching engine was growing larger and more complex because of various optimizations, and other concerns, such as timing, scheduling, and permis-sions management, were blurring the overall picture. We created a smaller ver-sion of the core of the matching engine, with just a minimal set of basic and naive classes that could make matching work in its most interesting aspects. In this case, the smaller system was not a replica but was built mostly from the same elements as the actual system because the design was flexible enough to accommodate that.\n •In a very large legacy system with a few applications and many batches run-ning in the background several times a day , the overall behavior of the system was quite nebulous. We created a small-scale, simplified Java emulation of the most important batch so that we could better understand it and explore its interactions with our new code.\n •In two startups with a rich domain, we pair-programmed for a few days to create a small-scale model that worked for just one very simplified case."",5313",5452,"**Living Architecture Documentation: Best Practices**\n\n**Quality Attributes:**\n- Importance of documenting non-automated quality attributes\n- Examples: financial expectations, user satisfaction\n- Online tools encourage declarations of objectives before testing\n- Tools promote TDD-ish approach for startups' objectives\n\n**Fragmented Knowledge to Usable Documentation:**\n- Multiple sources of truths about quality attributes\n- Curate and consolidate information into one or two living tables of content\n- Use tags to classify quality attribute tests in Cucumber scenarios\n- Decide on a main tool as the single point of entry for documentation\n\n**Small-Scale Simulation as Living Architecture Documentation:**\n- Complex software applications challenging in terms of documentation\n- Create small-scale replicas for documentation purposes\n- Focus on key features and code that matters most\n- Make sure the small-scale replica works realistically\n- Advantage: fits in one's head, human-scale.\n\n**Quality Attributes:**\n- Document non-automated quality attributes (financials, user satisfaction)\n  - Keep track of expectations before testing\n  - Use online tools for declarations and testing\n\n**Fragmented Knowledge to Usable Documentation:**\n- Multiple sources of truths about all quality attributes\n- Curate and consolidate information into one or two living tables of content\n- Use tags to classify tests based on their nature\n- Decide on a main tool as the single point of entry for documentation.\n\n**Small-Scale Simulation:**\n- Create small-scale replicas of software systems for documentation purposes\n- Focus on key features and code that matters most\n- Make sure the small-scale replica works realistically\n- Advantage: fits in one's head, human-scale.",1753,57.3284375667572
False,3,"173-Building a Small-Scale Simulation Is Half the Fun.pdf,173-Building a Small-Scale Simulation Is Half the Fun,""Chapter 12 Living Architecture Docume","173-Building a Small-Scale Simulation Is Half the Fun.pdf,173-Building a Small-Scale Simulation Is Half the Fun,""Chapter 12 Living Architecture Documentation 372\nThis gave us an opportunity to quickly explore and map the domain, discover \nthe main issues and stakes, grow a vocabulary , and agree on a shared vision of the whole system. From that small-scale system, later discussions had a con-crete reference code to ground the discussions. We found that this was really a communication tool we could point at during conversations.\nAt a big company where everything takes ages, creating a small-scale model under \nthe name “proof of concept” is a great alternative to doing never-ending studies delivering nothing but slides and illusions. The focus on working code helps con-verge and makes it harder to elude the tough questions. Y ou probably already build proofs of concepts at the beginning. But do you keep them for their explanatory power later?\nThe Desirable Properties of a Small-Scale Simulation\nA small-scale simulation must have the following characteristics:\n •It must be small enough to fit in the brain of a normal person or of a developer: This is the most important property , and it implies that the simula-tion will not account for everything of the original system.\n •You must be able to tinker with it, and it must be inviting for interac-tive exploration: The code should be easy to run partially , by just being able to do something with a class or function without having to rebuild the full simulation.\n •It must be executable to exhibit the dynamic behavior at runtime: The sim-ulation must predict results through its execution, and you must be able to observe it easily , even during the computation, if possible, in debug mode, with traces or just by running its phases independently .\nA small-scale software project that is executable and works in a realistic fashion \nis valuable for reasoning on the system. Y ou can reason on its static structure just by observing it in the code. Y ou can also tinker with it by creating one more test cases or by interacting with it in a REPL.\nThis approach is also useful as an inexpensive proxy to impractical legacy or \nexternal systems; instead of running a complex batch that depends on the state of the database and that has numerous side effects all over the place, you can run its emulation to get a grasp of its effect in relationship to what you’re doing.\nSmall-Scale Simulation as Living Architecture Documentation 373\nTechniques to Simplify a System\nT o achieve a small-scale simulation, you want to simplify aggressively the full system, \nwith an exclusive focus on the one or two aspects that matter. Just like any other documentation, documentation of a system should explain 1 thing well rather than explain 10 things badly . (There’s already the real system for that.) Note that you can still decide to build more than one small-scale simulation, such as one for each important point to explain.\nA simplified small-scale system will lose a lot of details and will not show a lot \nof otherwise valuable knowledge. This simplification is harder to do than it seems because when you’re attached to a system you’ve built, you’d like to tell about all its interesting facets, but you have to refrain from doing so and learn to focus.\nInterestingly , the techniques used to build a small-scale simulation are the tech-\nniques you already use to create convenient tests.\nConcretely , you can simplify a system in many ways, always by deciding to ignore \none or many distracting concerns:\n •Curation: Give up the idea that it has to be feature complete. Get rid of all the member data that is not central to the current focus. Ignore side stories and secondary stuff like special cases that don’t intersect the current focus.\n •Mocks, stubs, and spies: Give up performing all the computations. Instead, use the usual test companions to totally get rid of all the non-centrally relevant subparts. Use in-memory collections instead of middleware and simulate third parties.\n •Approximation: Give up on strict accuracy and settle only on realistic accu-racy that looks good enough, such as the right value without the digits or 1% correct.\n •More convenient units: Give up the ability to really put in production the sim-ulation with the actual data. For example, if the dates are only used to decide if something happens before or after a given data, you might replace the dates that are cumbersome to manipulate by hand with plain integers.\n •Brute-force calculation: Give up the optimizations that are not central to your current focus. Instead, make the simulation work using the algorithm that’s the simplest to grasp or the one with the most explanatory power.\n •Batch versus event driven: Turn the original event-driven approach into a batch mode, or the other way around, if it’s simpler to code and understand, assuming that this approach is not central to the current focus."",4876",4995,"**Building a Small-Scale Simulation: Characteristics and Techniques**\n\n**Small-Scale Simulation:**\n- Quick exploration and mapping of system domain\n- Discover main issues and stakes\n- Grow vocabulary and shared vision\n- Concrete reference for discussions\n- Executable code, easy to tinker with\n- Predicts dynamic behavior at runtime\n- Inviting for interactive exploration\n- Simplifies complex systems\n- Reasoning through static structure or REPL\n- Useful as inexpensive proxy to external systems\n\n**Characteristics of a Small-Scale Simulation:**\n- Fits in the brain of a normal person or developer\n- Tinkerable and inviting for interactive exploration\n- Executable to exhibit dynamic behavior\n- Predicts results through execution, observable during computation (if possible)\n- Small enough to focus on one or two aspects\n\n**Techniques to Simplify a System:**\n- **Curation**: Give up feature completeness, ignore non-central data and secondary concerns\n- **Mocks, stubs, and spies**: Eliminate computation of non-central subparts, use in-memory collections instead of middleware or third parties\n- **Approximation**: Settle for realistic accuracy that looks good enough\n- **Convenient units**: Replace cumbersome data with simpler units (e.g., integers instead of dates)\n- **Brute-force calculation**: Use the simplest algorithm or one with explanatory power\n- **Batch versus event driven**: Turn event-driven approach into batch mode, or vice versa, if simpler for current focus\n\n**Small-Scale Simulation as Living Architecture Documentation:**\n- Small-scale simulations serve as documentation for system design and understanding\n- Useful in big companies where everything takes ages, focusing on working code and conversations\n- Helps converge and make it harder to elude tough questions during discussions.",1808,61.002551317214966
False,3,"174-Explaining a System by Talking About Another System.pdf,174-Explaining a System by Talking About Another System,""Chapter 12 Living Architecture Do","174-Explaining a System by Talking About Another System.pdf,174-Explaining a System by Talking About Another System,""Chapter 12 Living Architecture Documentation 374\nBuilding a Small-Scale Simulation Is Half the Fun\nY ou learn a lot by creating a small-scale simulation. Y ou have to clarify your thoughts, \nand nothing’s more demanding than simple, working code to force that.\nFrom a design perspective, cutting through the details to focus on the essen-\ntials gives you a lot of insights that can help you improve the design of the original system. For example, if you can replace dates with integers in the simulation, the original functions don’t really need to operate on dates but can operate on anything comparable.\nIf the simulation can work without all these distracting aspects, this also means \nthe original design should follow the single responsibility principle and, therefore, separate all the concerns. Y ou know when you’ve reached that state when you can create your small-scale simulation by reusing the same code from the original system just by assembling a naive subset of its elements.\nThis idea used in the context of starting a project is known under various names \nin the literature: Alistair Cockburn talks about a walking skeleton.\n17\nThis idea is also similar in many aspects to the pattern breakable toys described \nin the book Apprenticeship Patterns: Guidance for the Aspiring Software Craftsman by Dave Hoover and Adewale Oshineye.\n18 A small-scale simulation can be used to \ntry things much faster than the actual system. This comes in handy for trying two or three competing approaches quickly to decide on the best one, based on actual facts rather than on opinions.\nSuch a tinkerable system is very valuable because new joiners can build their own \nmental models about it. If, as Peter Naur\n19 claims, it’s very hard to express a theory \nusing codified rules such as text, having the ability to form your own theories about a system by just playing with it without risk can help. This is how kids learn about all the laws of physics.\nSystem Metaphor\nIf you run trainings, you know how hard it is to explain something to an audience you don’t know . Y ou need to determine what they know already so you can build on that.\n17.  Cockburn, Alistair. Crystal Clear: A Human-Powered Methodology for Small T eams. Boston: Pearson \nEducation, Inc., 2004.\n18.  Hoover, David H., and Adewale Oshineye. Apprenticeship Patterns: Guidance for the Aspiring Soft-\nware Craftsman. Sebastopol, CA: O’Reilly Media, Inc. 2009.\n19.  Naur, Peter. “Programming as theory building.” Microprocessing and Microprogramming 15, no. 5 \n(1985): 253-261."",2570",2693,**Explaining a System by Talking About Another System**\n\n**Creating Small-Scale Simulations**:\n- Helps clarify thoughts and improve design of original system\n- Reveals essential aspects of the system that can be used to simplify design\n- Allows for faster experimentation with competing approaches\n- Enables new joiners to build their own mental models of the system\n\n**Simulation as a Learning Tool**:\n- Allows for quicker understanding and exploration compared to text or code alone\n- Helps develop theories about a system through hands-on experience\n- Remarks by Peter Naur: Programming is a form of theory building\n\n**System Metaphor**:\n- Explaining a complex system to an unfamiliar audience can be challenging\n- Building on existing knowledge is key for effective communication,783,20.57210683822632
False,3,"175-A Metaphor in Another Metaphor.pdf,175-A Metaphor in Another Metaphor,""System Metaphor 375\nMetaphors get their power by leveraging on things most","175-A Metaphor in Another Metaphor.pdf,175-A Metaphor in Another Metaphor,""System Metaphor 375\nMetaphors get their power by leveraging on things most people are already famil-\niar with, so it’s possible to explain new stuff more efficiently .\nExplaining a System by Talking About Another System\nA simple shared story of how the system works, a metaphor.\n—C2 Wiki, http://c2.com/cgi/wiki?SystemMetaphor\nWhen I explain monoids and how they compose, I usually use the metaphor to the \ntangible world, such as real glasses of beers that I can stack or chairs that can be stacked or anything else that is stackable. This helps elucidate the idea of monoid-esque composability , and it’s fun, which is also very good for learning.\nSuggestive metaphors we’re all familiar with include an assembly line, a water \npipeline, Lego building blocks, a train on its rails, and a bill of materials.\nThe system metaphor was used in Extreme Programming (XP) to unify an archi-\ntecture and provide naming conventions.\nThe famous Extreme Programming project C3 “was built as a production line,” \nand the other famous XP project VCAPS “was structured like a bill of materials.” Each chosen metaphor acts as a system of names, relations, and roles working together toward a shared purpose. When using a metaphor, you invoke all the prior knowledge of the audience to be reused in the context of the system being considered. Y ou know that an assembly line is typically linear, with multiple machines in line alongside a conveyor belt that is moving parts from one machine to the next. Y ou also know that any defect upstream will result in defects downstream.\nUseful Even Without Prior Knowledge\nThe last time I was on a team that built a rich cash flow calculation engine that was able to re-create cash from any complicated financial instrument, the team used the metaphor of a modular synthesizer. Now I have to admit that not everyone is famil-iar with modular synths, but in that team, several people knew about them. The interesting thing is that the metaphor helped even the people who didn’t know about them.\nIt’s intriguing that a metaphor remains somehow useful even for people who don’t \nknow it, just as a redundancy mechanism. Imagine that you’re trying to mentally pic-ture the cash flow engine as an interpreter pattern, and you’re not fully sure you got it right. Now , if I explain what a modular synth is (“a set of electronic modules full of buttons and knobs, wired together in an arbitrary fashion via patch cords plugged into them”), it should help. The patching combinations between every connector are nearly infinite for a large variety of sounds to be produced, just as is the case with the financial engine."",2655",2736,"**Metaphors in Explanation of Systems**\n\n**Metaphor Use for System Explanation:**\n- Metaphors used to explain new systems efficiently (C2 Wiki)\n- Help clarify ideas and are fun for learning (metaphor examples: assembly line, water pipeline, Lego blocks, train on rails, bill of materials)\n- Unify architecture and provide naming conventions in Extreme Programming (XP)\n- Useful even without prior knowledge (modular synthesizer metaphor)\n\n**Benefits of Using Metaphors:**\n- Linear assembly line: multiple machines, conveyor belt, defects upstream affect downstream\n- Prior knowledge invoked for reuse in understanding new system\n\n**Metaphor Usefulness Despite Lack of Familiarity:**\n- Modular synth metaphor helped team members unfamiliar with it (cash flow calculation engine as interpreter pattern)\n- Remains useful as a redundancy mechanism.",844,24.555819988250732
False,3,"176-Chapter 13 Introducing Living Documentation to a New Environment.pdf,176-Chapter 13 Introducing Living Documentation to a New Environment,""Chapter","176-Chapter 13 Introducing Living Documentation to a New Environment.pdf,176-Chapter 13 Introducing Living Documentation to a New Environment,""Chapter 12 Living Architecture Documentation 376\nA Metaphor in Another Metaphor\nA good metaphor is a model with some generative power: If I know that stopping a \nproduction line is very expensive, I can wonder if and how that would translate into a software system. It does, and just as on a production line, we should perform strict validation of the raw materials in inputs to protect the line. But the metaphor may not stand on this aspect, and that’s it.\nThe more common culture that exists, the more ideas available to use as meta-\nphors. When you know what a sales funnel is (see Figure 12.6), you can talk about it to explain key aspects of an e-commerce system, with its successive business stages from visitor to inquiries, to proposals, to new customer. It’s called a funnel because the volume at each stage decreases significantly .\n \n1000 Leads\n100 Prospects\n10 Customers\nFigure 12.6 A sales funnel\nThis knowledge comes in handy when you’re doing architecture, as it informs \nscalability reasoning: The upstream stage, like the catalog, needs more scalability \nthan the downstream stage, like the payment.\nSummary\nDocumentation of software architecture doesn’t have to be slideware or models in proprietary modeling tools. It is best done in a living documentation fashion, in situ in the system itself, and organized into an architecture landscape that people become familiar with.\nThe ultimate form of this approach is a transparent and living architecture that is \nalso test driven. In this setting, the important architectural knowledge is accessible, enabling anyone to make informed changes quickly and any time, and it’s machine accessible for continuous reality checks and feedbacks, thanks to  automated assertions."",1753",1902,"**Living Documentation in Software Architecture**\n\n**Background:**\n- Good metaphors have generative power and can inform software architecture\n- Knowledge of production lines and sales funnels useful for understanding e-commerce system's scalability\n\n**Documentation**:\n- Doesn't have to be slideware or models in proprietary tools\n- Best done in living documentation, in situ within the system\n- Organized into an architecture landscape\n\n**Transparent and Living Architecture:**\n- Important architectural knowledge accessible to all\n- Enables informed changes quickly and anytime\n- Machine accessible for continuous reality checks and feedbacks\n- Thanks to automated assertions.\n\n**Metaphors in Software Architecture**:\n- Metaphors have generative power\n- Knowledge of production lines can inform software system's protection mechanisms\n- Sales funnel metaphor helps explain e-commerce system's business stages and scalability reasoning.\n\n**Scalability Reasoning:**\n- Upstream stage (catalog) needs more scalability than downstream stage (payment).\n\n**Living Documentation**:\n- Transparent and living architecture is the ultimate form\n- Accessible information for informed changes\n- Machine accessible for continuous feedback\n- Best done in situ within the system.",1265,43.1033353805542
False,3,"177-Official Ambition.pdf,177-Official Ambition,""377Chapter 13\nIntroducing Living \nDocumentation to a New Environment\nLiving documentation starts w","177-Official Ambition.pdf,177-Official Ambition,""377Chapter 13\nIntroducing Living \nDocumentation to a New Environment\nLiving documentation starts with someone willing to improve the current state of \naffairs in either the documentation or the way software is done. Since you are reading this book, you are that person. Y ou may want to start living documentation because you’re afraid to lose knowledge, or because you’d like to go faster with the relevant knowledge more readily available. Y ou may also want to start it as a pretext to show flaws in the way the team is making software, such as in the lack of deliberate design, and you expect the documentation to make it visible and obvious to everyone.\nA difficult step is to find a compelling case of missing knowledge. Once you have \na demonstrated case and provided that you can solve it with one of the living docu-mentation approaches, you’re on the right track.\nUndercover Experiments\nIf you feel alone in your interest in living documentation, you may want to start gen-tly and slowly , without making a lot of noise about it and, most importantly , without asking for authorization. The idea is that documenting, whatever way it is done, is part of the natural work of a professional developer.\nTherefore: Introduce nuggets of living documentation naturally as part of the \ndaily work. Start annotating your design decisions, intents, and rationales at the time you make them. When there is some slack time or a genuine need for docu-mentation, use the allotted time to create simple documentation automation such as a simple living diagram or a basic glossary. Keep it simple enough to have it work in a few hours or less. Don’t talk about it as a revolution but just as"",1695",1750,"**Living Documentation: Getting Started**\n* **Start with a compelling case**: Improve current state of affairs or address flaws in software development process through living documentation\n* **Find missing knowledge**: Demonstrate the value of living documentation by solving identified issues\n* **Introduce nuggets of living documentation**: Annotate design decisions, intents, and rationales as part of daily work\n* **Keep it simple**: Create automation such as living diagrams or glossaries in a short amount of time\n* **Don't make a lot of noise**: Introduce living documentation naturally without asking for authorization\n* **Documentation is part of professional development**: Justify the practice as an essential aspect of software development.",752,27.68556022644043
False,3,"178-Case Study A Tale of Introducing Living Documentation to a Team Member.pdf,178-Case Study A Tale of Introducing Living Documentation to a Team Mem","178-Case Study A Tale of Introducing Living Documentation to a Team Member.pdf,178-Case Study A Tale of Introducing Living Documentation to a Team Member,""Chapter 13 Introducing Living Documentation to a New Environment 378\na natural way to do things efficiently. Emphasize the benefits, not the theory \nfrom this book.\nOf course, when people become more interested in the approach, you can talk \nabout living documentation as a topic and direct them to this book.\nOfficial Ambition\nAnother way to introduce living documentation is through an official ambition, though I don’t recommend this one as a starting point.\nGoing the official route usually begins with management, or at least requires \nmanagement sponsorship. Documentation is often a source of frustration and anxi-ety for managers, and therefore this topic is often promoted even more by managers than by the development team.\nHaving a sponsor is good news: Y ou have dedicated time and perhaps even a team \nto implement. The counterpart is that as an official ambition, it will be highly visible and closely monitored, and there will be pressure to deliver something visible quickly . This pressure may endanger the initiative by forcing success. But living documenta-tion is a discovery journey , there’s an experimental side to it, and there is no clear path to success in your own context. Y ou’ll have to try things, decide that some are not applicable, and adjust some things to your own cases. This is best done without excessive scrutiny by higher-ups.\nI therefore recommend starting with undercover experiments and promoting the \ntopic as an official ambition only after you’ve found the sweet spots of living docu-mentation in your environment.\nNew Things Have to Work and Have to Be Accepted\nA typical path I advise is to start with creating appetite, then try for quick  opportunities to show benefits, and then to grow from there:\n 1. Start by creating awareness in the larger audience. A great way to do this is \nthrough an all-audience talk that is informative and entertaining. The point is \nnot to explain how to do things but to show how life could be better compared to the current state of affairs. Nancy Duarte’s book Resonate\n1 is full of advice \non how to do this well. Listen to the feedback at the end of the session and a few days later to decide whether there is appetite to go further. Otherwise, you may want to try again some weeks or months later, or you might decide to go undercover first.\n1. Duarte, Nancy . Resonate. Hoboken: John Wiley and Sons, Inc. 2010. \nNew Things Have to Work and Have to Be Accepted 379\n 2. Spend some time with the team or an influencer team member to identify what \nknowledge most deserves to be documented. From there, propose quick wins \nto try as short items in the backlog or as part of time dedicated to making improvements. Retrospectives are also a good time to consider living documen-tation issues and propose actions. It is important to focus on real needs that many people find important.\n 3. Build something useful in a short period of time and demo it as you would any \nother task. Collect feedback, improve, and collectively decide on whether to expand now or later, if needed.\nStarting Gently\nAs a consultant, I regularly sit with teams in companies of all sizes. When they ask to create more documentation, I tend to suggest several steps.\nFirst of all, I remind them that interactive and face-to-face knowledge transfer \nmust be the primary means of documentation, before anything else.\nNext, I tell them, we can then consider techniques to record the key bits of knowl-\nedge that have to be known by everyone, that every newcomer has to learn, and that matter in the long run.\nAt this point someone might say , “Let’s write that stuff in our wiki.” That is fine, \nas long as we understand that a wiki is a nice place for evergreen content, for knowl-edge that does not change often. For everything else, we can do better.\nWhere to start? I like to mention various ideas very quickly to scan the interest of \nthe team members. For example, I would mention briefly each of the following:\n •We could add a simple README describing what the project is about.\n •We could add a simple decision log as a Markdown file in the base directory of the project, with a recap of the three to five main architectural and design deci-sions since its inception.\n •We could tag the code with a custom annotation or attribute to highlight the key landmarks or the core concepts. Combined with the search by reference in the IDE, this becomes a simple yet effective way to provide a sightseeing map of the code base.\n •On a similar note, we could tag the code with guided tour annotation or attributes to provide a simple way to follow a request or processing end-to-end across the various fragments of code, across the various layers or modules, in a linear fashion. Again, this relies on search by reference in the IDE.\n •We could turn the most important napkin sketch into an ASCII diagram in the decision log file.\nChapter 13 Introducing Living Documentation to a New Environment 380\nThis list deliberately contains only stuff that can be done and committed within a \nshort period of time. For example, a team I’ve worked with has been able to add and \ncommit a decision log with five past key decisions, marking three key landmarks plus a guided tour with five steps within two hours. This included the creation of the two custom attributes for the key landmarks and the guided tour, respectively , ensuring that the search in the IDE worked well, and ensuring that the Markdown rendering was fine in TFS.\nThe goal at this stage is to create awareness and interest by reaching attractive \nresults quickly . The goal is to hear, “Wow , I really like that approach. I’m hooked now!”\nAnother goal is for the team members to already experience the “beyond docu-\nmentation effect” after just going through these simple steps: “Ouch, I now real-ize how sloppy and half-finished our structure is.” That’s a lot of goodness for two hours!\nGiven genuine interest from the team and some available time, you can go further \nand try word clouds, a living glossary , or living diagrams.\nGoing Big and Visible\nAfter a gentle start, you might want have bigger ambitions. I’m not saying that is always a bad idea, but it can be dangerous for several reasons:\n •Visible ambitions usually need to exhibit symbolic progress shown by a quan-tity of outcomes or even KPIs. But does it mean anything “to be 40% living documentation”? Doing living documentation just to do it will eventually dis-credit the approach.\n •The benefits can be deferred by months, and it can be difficult to show the return on investment if measured over three months.\n •As mentioned earlier, it may take various adjustments when applying the tech-niques from this book in your particular context; these adjustments may be perceived as failures in the meantime.\n •What’s useful for the team may not be what management expected. If this is the case, put yourself in management’s shoes: What would make you happy with respect to documentation? If you can make previously hidden knowledge accessible to nondevelopers, it might be a good thing for everyone. Manag-ers will be able to judge something by themselves, based on objective facts extracted from the code base. And when you set up the living diagram or some other mechanism, you have an opportunity to do the curation and the presen-tation in a way that promotes your agenda (for example, to encourage good things or to warn against bad ones)."",7481",7642,"**Introducing Living Documentation to a Team Member**\n\n**Benefits of Living Documentation**:\n- Can be introduced by emphasizing the benefits, not just the theory\n- Become more interested in the approach before delving into the details from the book\n\n**Official Ambition Approach**:\n- Started through management sponsorship or promotion\n- Visible and closely monitored, which may lead to pressure to deliver quickly\n- Discovery journey with experimental side and no clear path to success\n- Recommended to start with undercover experiments first and promote as official ambition later\n\n**New Things Have to Work and Be Accepted**:\n1. Create awareness through an all-audience talk that is informative and entertaining\n2. Identify key knowledge that deserves documentation and try for quick wins\n3. Build something useful in a short period of time, collect feedback, and make decisions on expansion\n\n**Starting Gently**:\n- Interactive and face-to-face knowledge transfer as primary means of documentation\n- Consider techniques to record key bits of knowledge, such as:\n  - README describing the project\n  - Decision log with key architectural and design decisions\n  - Code annotation for landmarks and core concepts\n  - Guided tour annotation for end-to-end processing\n4. Goal is to create awareness and interest by reaching attractive results quickly, and to experience the ""beyond documentation effect""\n\n**Going Big and Visible**:\n- Can be dangerous due to:\n  - Quantity of outcomes or KPIs needed for visible ambitions\n  - Deferred benefits that may take months to show\n  - Adjustments required in your particular context that may be perceived as failures\n  - Misalignment between what's useful for the team and what management expected\n- Management may be satisfied if you make previously hidden knowledge accessible to non-developers, allowing them to judge it based on objective facts.",1887,69.85422468185425
False,3,"179-The First Debriefing.pdf,179-The First Debriefing,""Case Study: A T ale of Introducing Living Documentation to a Team Member 381\nIn any case, reme","179-The First Debriefing.pdf,179-The First Debriefing,""Case Study: A T ale of Introducing Living Documentation to a Team Member 381\nIn any case, remember that documentation, living or not, is not an end in itself \nbut a means to accelerating delivery . This acceleration of delivery can be direct, when \ndecisions are made faster thanks to the knowledge readily available through the liv-ing documentation. It can also be indirect, when creating the documentation raises awareness on everything sloppy in the system, in the thinking or in the communica-tion between the stakeholders. By fixing the root cause, you improve the whole sys-tem, which in turn accelerates delivery .\nCase Study: A Tale of Introducing Living \nDocumentation to a Team Member\nI met with a team member who was interested in learning more about living docu-\nmentation. He was just curious, not convinced, but his curiosity was a good start.\nConversations First\nI tend to start explaining living documentation with questions in a conversational style. Instead of explaining what living documentation is, I start by putting myself in the shoes of another team member willing to learn about the project. I ask the team member to tell me about the current project, and I tell her I’m going to take notes and sketch what we say on a flipchart. Then I begin to ask questions like these: \nWhat’s the name of the project? What is its purpose, and who is it for?\nWhat is the ecosystem, including the external systems and external actors? What \nare the overall input and outputs?\nWhat’s the execution style? Is it interactive, a nightly batch, a GitHub hook? \nWhat’s the main language: Ruby , Java, T omcat, or something else?\nThese are all standard questions so far. Answers come naturally . But then I ask: \nWhat is the core domain, in your opinion?\nThis comes as a surprise. The team member needs some time to think about it. \nShe is surprised that the answer was not obvious, after several months on the project.\n“Oh, now that you mention that,” she says, “I realize that our core domain is \nprobably the way we insert deep links that point to our system in the feed we provide \nto the external partners, so that they bring us qualified inbound web traffic. I didn’t think about it this way before, and I’m not sure everyone on the team is aware of it.”"",2286",2347,"**Living Documentation: Case Study**\n\n**Background:**\n- Team member expresses interest in learning about living documentation\n- Initial conversation between team members to understand current project details\n\n**Conversational Style:**\n- Put yourself in another team member's shoes\n- Ask questions to gather information\n\n**Standard Questions:**\n- What is the name of the project? Purpose, stakeholders?\n- What is the ecosystem: external systems, actors?\n- Inputs and outputs?\n- Execution style: interactive or batch? Main language?\n\n**Core Domain Discussion:**\n- Core domain identification not always obvious\n- Team member realizes core domain is way they insert deep links in feed for external partners to bring qualified web traffic.\n- Not all team members may be aware of this aspect.",786,20.573751211166382
False,3,"180-Common Objections to Living Documentation.pdf,180-Common Objections to Living Documentation,""Chapter 13 Introducing Living Documentation to a New","180-Common Objections to Living Documentation.pdf,180-Common Objections to Living Documentation,""Chapter 13 Introducing Living Documentation to a New Environment 382\n“Is this deep link thing the raison d’être for the whole project?” I ask.\n“Y es,” she says, “absolutely .” I press further: “Do you think everyone should know about that? “Obviously , yes,” she said. “So we should document that somewhere?” I ask.“Of course!” she replies.\nThe First Debriefing\nAfter I learn what I am interested in through having conversations, I can hold a debriefing to introduce the basic concepts of living documentation.\nLiving documentation is primarily about having conversations to share knowledge. \nMy goal in the conversations so far was to learn a lot of what matters to me, quickly and without wasting time on any other stuff. Interactive conversations and the high bandwidth of talking are hard to beat, especially with the support of a flipchart, which helps me ensure that I understand the other person’s message.\nThe second point I can introduce now is that some of the knowledge we talked \nabout so far needs to be recorded in a persistent form. And the good thing to recog-nize is that most of this knowledge so far is stable over time. This is lucky , so in this \ncase we can use evergreen documents in any form: wiki, text and so on. But we must make sure not to mix any volatile and short-lived knowledge, or we immediately loose the benefits of evergreen documents, which are documents that don’t need any maintenance yet remain true forever (or for a very long period of time).\nThere is a third point here already: The concept of “deep linking” we uncovered is \na standard concept already documented in the online literature. As such, it’s ready-\nmade knowledge. We can link to it on the web, so there is no need to explain what it is again. We’re lazy .\nOne last point we begin to see in this example is that by paying attention to the \ndocumentation, even the person with the knowledge also learns and gains additional awareness in the process. That illustrates the benefits “beyond documentation,” and it’s probably the biggest value of living documentation.\nTime to Talk About the Code\nAfter all my questions on the context and on the problem side, I’d like to know more about the solution side—in other words, the code. So I ask how the code is organized.\nThen we draw the folder hierarchy on the flipchart, and the structure is very \nclose to the hexagonal architecture. At this point I provoke the team member a bit: “Imagine that you’re all gone after the project is delivered, and there is no budget \nCase Study: A T ale of Introducing Living Documentation to a Team Member 383\nleft to keep you. Then a year later, the project has to resume, and additional features \nneed to be delivered, so a new team is formed. What risks do you see that the new team will degrade the current system?”\nIn this fictitious situation, it’s easier for the team member to answer: “I’d say jun-\nior developers new to the project might put business logic in the REST endpoints, and that would bad.”\n“Sure,” I say . “It would be bad. Still, I think there should be no need to say this \ntoday , as it’s supposed to be known by professional developers these days.”\nThe team member says that they were doing everything pretty much standard, \nwith no surprises. T o me this means there’s no need for documentation on all the standard stuff. Also, the code is rather clean, and it shows how it’s done. However, it doesn’t tell why  it’s done this way .\nI ask whether there is a risk that any other new team members could degrade the \ndesign of the system by accident.\n“In fact,” says the team member, “we designed an includelist and an excludelist \nmechanism to filter content that we export, depending on the external partner. But we did it in such a way that the code is totally agnostic with respect to the external partner. Only their configuration is specific.”\n“Y ou mean that effort to be totally agnostic does not necessarily show from the \ncode without giving any hint?” I ask.\nShe replies, “Y es, it’s likely that a newcomer would quickly add an \nIF statement \naround the next partner-specific behavior he or she needed to support and would break the design as a result.”\nDecision Logs and Guided Tours\nI tell the team member that we should record the design decision she just described. We can do that in a decision log as a plain Markdown file at the root of the project, \nin the source control system. It’s quite concise: date, decision, rationale, conse-quence. Three sentences would be enough.\nWhat else? The code of the project is not bad, but it is still not obvious enough to \nfollow a user request through all its stages in the system.\n“For that we could do a guided tour,” I say . And I explain and show how to cre-\nate the custom annotation \n@GuidedTour  to mark each step in the guided tour. The \nteam member quickly devises the best seven steps of the guided tour and adds the annotation on each of them. It takes 20 minutes to introduce the first tour.\nFurthermore, through the tour, I find out that a significant part of the overall \nbehavior is a cache on calculations on web services, in a read-through fashion: That’s \nready-made knowledge again, that is described online!"",5238",5341,"**Living Documentation Overview**\n* Conversations for sharing knowledge (quick, efficient)\n* Recording persistent information (evergreen documents)\n* Standard concepts documented online (deep linking)\n* Benefits: learning and awareness beyond documentation\n\n**Introducing Living Documentation to a Team Member**\n\n**Step 1:** Learn about the context and problem side\n- Discover deep linking concept, already documented online\n- Pay attention to documentation for added benefits\n\n**Step 2:** Understand code organization\n- Draw folder hierarchy using a flipchart\n- Identify risks and potential issues\n\n**Step 3:** Record design decisions in decision logs\n- Create plain Markdown files at the root of project, source control system\n- Include date, decision, rationale, consequence\n\n**Step 4:** Create guided tours for code understanding\n- Annotate steps using @GuidedTour\n- Identify significant parts of overall behavior as ready-made knowledge.\n\n**Living Documentation: Benefits**\n* Learning and gaining additional awareness during the process\n* Efficient way to share knowledge\n* Evergreen documents save time and effort\n* Standard concepts documented online for easy reference\n* Improved understanding of code through guided tours.",1231,41.928735971450806
False,3,"181-We Do It Already.pdf,181-We Do It Already,""Chapter 13 Introducing Living Documentation to a New Environment 384\nWe then create another custom ann","181-We Do It Already.pdf,181-We Do It Already,""Chapter 13 Introducing Living Documentation to a New Environment 384\nWe then create another custom annotation, @ReadThroughCache , to mark that \nknowledge, with a brief definition and a link to a standard explanation on the web.\nAfter 2.5 hours talking and creating annotations to support our very first living \ndocumentation, it’s time to get feedback from the team member, and what I hear \nsounds encouraging: “I like the idea of using annotations for documentation: It’s lightweight and easy to add without asking the right to do it. I can start solo and locally . In contrast, other techniques like a living diagram are more like team deci-sions, I think. And linking to ready-made knowledge saves time and is more accurate than if I tried to explain it myself in writing.”\nI concur, mentioning that it’s part of an embedded learning approach: “Simple \nannotations in the core also hint to your team members about interesting ideas in the literature they may not know otherwise.”\nBut she’s not totally convinced that this embedded learning works for everyone: \n“Y es, some of my colleagues will realize that they don’t know and will be curious to learn more. Some will read the links and learn by themselves, but some will probably not and will ask me instead.”\n“But I see that as a feature,” I say . “It invites discussion, and that’s another oppor-\ntunity for learning, probably for both of you.”\nCommon Objections to Living Documentation\nY our desire to start doing living documentation doesn’t mean everyone around agrees. Perhaps they don’t have the need or they don’t see the benefits.\nAnnotations Are Not Meant for Documentation\nOne of the most common objections to living documentation is that annotations are not meant for documentation. This is how a conversation about this objection might go:\nTeam member: “I don’t like to use annotations for documentation because I don’t like adding code that does not execute”\nMe: “Y ou know , you do it already when you mark code as \n[Obsolete]  or \n@Deprecated. ”\nTeam member: “Oh, yes. Fair point.”\nI suggest framing the “comments versus annotation” choice as “good versus bad”: \n“Comments are bad and should be avoided; but if the information to record is really \nimportant, then it’s worth its own custom annotation.”"",2297",2350,"**Living Documentation Introduction: Chapter 13**\n* Creating a new living documentation method using annotations\n* Introducing @ReadThroughCache custom annotation for team member's feedback\n* Encouraging team member about lightweight and easy addition of annotations\n* Comparison between living documentation techniques, like diagrams vs. annotations\n* Embedded learning approach: inviting discussion for further opportunities\n\n**Objections to Living Documentation:**\n\n**Annotation Misconceptions:**\n* Some team members believe annotations are not meant for documentation\n* Discussing the difference between comments and custom annotations\n* Framing choice as ""good versus bad"": avoiding comments vs. using important information's own custom annotation.",753,19.97459363937378
False,3,"182-Marginal Documentation.pdf,182-Marginal Documentation,""Migrating Legacy Documentation into Living Documentation 385\n “We Do It Already”\nIf you’r","182-Marginal Documentation.pdf,182-Marginal Documentation,""Migrating Legacy Documentation into Living Documentation 385\n “We Do It Already”\nIf you’re having lots of technical meetings, it MA Y indicate that your internal documen-\ntation could be better.\n—@ploeh on Twitter\n“We do it already” is a standard objection to almost anything. T o some extent, every-\nthing looks like everything.\nY es, you probably do apply a number of the practices in this book, but are you really \ntaking a living documentation approach? The key word here is deliberate. If you happen to do some of the things discussed in this book by chance, it’s fine, but it would be even better to do those things deliberately . It’s up to your team to decide where to put the cursor and to decide on a documentation strategy . Such a strategy has to be emergent and deliberate. It must fit your particular context and be accepted by everyone involved.\nY our documentation strategy will mix practices you already do, push some of \nthem further, and introduce new practices that sound promising. And you will adjust all this over time to get the most benefits with minimal effort.\nSomeone might say , “We have all the knowledge that we need.”Perhaps this team does have all the knowledge because he or she was there before \nthe rest of the team. But does everyone else feel so comfortable?\nPerhaps you just hate documentation, and I can totally understand that. But it’s \nimportant to acknowledge what you don’t know .\nMigrating Legacy Documentation into Living \nDocumentation\nIf you have legacy documentation, you may leverage on it. Doing so avoids the blank \npage syndrome and offers an opportunity to review past knowledge in a new light. Y ou have old PowerPoint documentation? Turn it into living documentation! Put the knowledge from the PPT back into the source code, wherever it fits best:\n •Vision and goals could go into the README file, as Markdown.\n •Pseudo-code or a sequence diagram could be made into a plain-text diagram or ASCII diagram, or you could just replace it with a reference to a test doing the same scenario.\n •Description of main modules and methods could be done within the source code itself, through some class- and module-level comments, annotations, and naming conventions.\n •Comments can be put in config items."",2275",2340,"**Migrating Legacy Documentation into Living Documentation**\n\n**Standard Objection**: ""We Do It Already""\n- Opposition to new approaches in documentation\n- Team should deliberate on strategy\n- Strategy must be emergent and deliberate, fitting team context\n\n**Documentation Practices**\n- Team may already employ some practices\n- Deliberately adopt new or improved practices\n- Adjust over time for maximum benefits with minimal effort\n\n**Addressing Past Knowledge**\n- Old documentation can be leveraged\n- Review past knowledge in a new light\n- Convert PowerPoint documentation into living documentation\n  - Vision/goals: README file (Markdown)\n  - Pseudo-code, sequence diagrams: plain-text or ASCII diagrams\n  - Main modules and methods: source code comments, annotations, naming conventions\n\n**Addressing Unknown Knowledge**\n- Acknowledging what you don't know is important\n- Legacy documentation can provide insights.",917,25.511558294296265
False,3,"183-Business Behavior.pdf,183-Business Behavior,""Chapter 13 Introducing Living Documentation to a New Environment 386\nNotice that all this knowledge","183-Business Behavior.pdf,183-Business Behavior,""Chapter 13 Introducing Living Documentation to a New Environment 386\nNotice that all this knowledge can be pulled from shared drives and wikis to find a \nnew home in the source control.\nIt’s also striking that the old content that was all concentrated within a few slide decks \nor Word documents becomes spread all over the code base when you move to living docu-\nmentation. This might sound like a bad thing. Sometimes you might prefer to have some overview slides kept together as one document. But for most of the practical knowledge, the best location to keep it is as close as possible to the place where you need it.\nY ou could perform documentation mining on all existing written documents: \nemails, Word documents, reports, meeting minutes, forum posts, entries into various company tools such as application catalogs, and so on. Every time a piece of knowl-edge “still sounds relevant after all this time,” it’s probably worth preserving.\nIn practice, you would deprecate or remove the old content, possibly with a redi-\nrection to the new location for the similar knowledge or an explanation on how to find it from now on. A former colleague, Gilles Philippart, calls such migration “strangling your documentation,” which is similar to Martin Fowler’s strangler application pattern for rewriting parts of legacy applications.\nMarginal Documentation\nY our documentation endeavor does not have to be complete on the first attempt. It should evolve over time. One approach that’s often a good idea when you’re willing to improve something is to focus on the marginal work. Y ou might say , for example, “From now on, every new piece of work will follow a much higher standard.”\nImprove your documentation marginally . By paying close attention to what you do \nfrom now on, even the parts of the legacy code that still matter will be taken care of over time. And don’t worry too much about the rest.\nSometimes you can segregate new additions to live in their own clean bubble con-\ntext; this makes it easier to clearly meet a higher standard of living documentation, which is nothing but a higher standard of everything: naming, code organization, top-level comments, clear and bold design decisions made visible in the code, and the more “typical” living documentation stuff like a living glossary and diagrams, enforced guidelines, and so on.\nCase Study: Introducing Living Documentation \nin a Batch System\nThis real-world example is about batches to export credit authorizations from one \napplication to external systems. Members of the team stay less than three years, on \nCase Study: Introducing Living Documentation in a Batch System 387\naverage, and therefore the need for some documentation is not controversial here. \nThe team and the managers heard about living documentation and are interested, so we eventually spent an hour discussing what could be done.\nWhen considering what to do, we tried to focus on everything that should be doc-\numented in order to improve the life of the development team. Then, by looking at the current state of the available documentation, we could propose actions to better manage the knowledge.\nCurrently , team members say , “there are some documents, but they are out of date \nand not reliable. We usually have to ask the most knowledgeable team member all the time to get the knowledge needed to perform any task.”\nThere’s a lot of potential for improvement here, including some quick wins. We \ncould introduce all the items discussed in the following sections to start a living doc-umentation journey .\nREADME and Ready-Made Documentation\nThe source code repository does not have a README file at its root. Therefore, the team could first add a README file at the root of the module.\nIn this README file, the team should mention clearly that this module follows \nthe data pump pattern, with a brief explanation of the pattern and a link to a ref-erence on the web. From a living documentation perspective, the team would be referencing ready-made documentation.\nT o be more useful, the team can elaborate a little bit on the data pump with a \ndescription of its main parameters in the README file:\n •Target system and format: A company-standard XML dialect is used.\n •Governance: This data pump belongs to the Spartacus Credit Approval com-ponent and is managed as part of it.\n •Rationale: The data pump pattern is chosen over more standard integration through services endpoints because the target system imposes a bulk integra-tion style, with lots of data to transfer between the two systems daily .\nAll this remains a bit abstract, so it’s desirable to include in the README file a \nlink to a folder containing some sample files describing the inputs and outputs of the component:\n1 Sample input and output files can be found in \n2 '/samples/' (with a link to 'target/doc/samples')"",4879",4934,"**Living Documentation in Business Environment**\n* Introduction:\n	+ Knowledge can be pulled from shared drives and wikis to new home in source control\n	+ Old content becomes spread all over code base when moving to living documentation\n	+ Best location for knowledge is as close as possible to where it's needed\n	+ Documentation mining of existing documents: emails, Word docs, reports, meeting minutes, forum posts, company tools\n	+ Deprecate or remove old content with redirection to new locations\n* Marginal Documentation:\n	+ Documentation endeavor does not have to be complete on first attempt\n	+ Evolution over time through focus on marginal work\n	+ Improve documentation as new pieces of work are added\n	+ Segregate new additions into clean bubbles for higher standard living documentation\n* Case Study: Introducing Living Documentation in a Batch System\n	+ Team stays less than three years, so need for some documentation is not controversial\n	+ Discussed improving documentation to improve life of development team\n	+ Current documentation is outdated and unreliable, requiring team members to ask most knowledgeable member for knowledge\n* README and Ready-Made Documentation:\n	+ Source code repository does not have a README file at root\n	+ Add a README file with clear mention that module follows data pump pattern and brief explanation\n	+ Team can elaborate on data pump in README with target system, governance, rationale, and link to sample files",1459,47.84012961387634
False,3,"184-Integrated Documentation for Developers and a Living Glossary for Other Stakeholders.pdf,184-Integrated Documentation for Developers and a Living","184-Integrated Documentation for Developers and a Living Glossary for Other Stakeholders.pdf,184-Integrated Documentation for Developers and a Living Glossary for Other Stakeholders,""Chapter 13 Introducing Living Documentation to a New Environment 388\nBusiness Behavior\nThe core complexity of the module is the determination of the domain concept of \neligibility. It is best described by business scenarios that are already partially auto-mated in Cucumber JVM, in a feature file named eligibility .feature.\nThe team can reuse some of these scenarios to generate the sample files men-\ntioned before. This way , the sample files will remain up-to-date.\nHaving business-readable scenarios is nice, but this team needs to make these \nscenarios accessible to nondevelopers. The basic Cucumber report can show the scenarios as a web page online. The team could consider the alternative tool Pickles for the living documentation to be available online to anyone in a better form and with a search engine.\nVisible Workings and a Single Source of Truth\nThe transcoding used to generate the XML report is defined in code and in an Excel file as well:\n1  | input field name | output field name | formatter                              |\n2  | trade date        | TrdDate                    | ukToUsDateFormatter |\nThe team realizes that there is duplication of knowledge for no particular benefit \nhere. Who’s the authority in case of disagreement? Usually it should be the spread-sheet file, but after a while, it will be the code.\nThe team could improve that situation by deciding that the spreadsheet file \nis the single source of truth (aka the golden source) of the transcoding. The code then parses this file and interprets it to drive its behavior. In this approach, the file is directly its own documentation. For example, the parser code could look like this, in pseudo-code:\n1  For each input field declared in a data dictionary (e.g. the \nXLS file)2    Fetch the value from the input field3    Apply the formatter to obtain the value4    Lookup the corresponding output field5    Assign the formatted value to the output field\nThe team might go the other way around and decide that the code is the single \nsource of truth and so generate a file directly out of the code. This won’t work if the \ncode is mostly made of a lot of IF statements. Being able to generate a readable file \nfrom the code imposes a generic structure to the design of the code. Basically , the"",2304",2493,"**Living Documentation for Developers and Stakeholders**\n\n**Determining Eligibility**:\n- Core complexity of the module is determining the domain concept\n- Best described by business scenarios in Cucumber JVM feature files\n- Scenarios can be reused to generate sample files\n\n**Making Scenarios Accessible to Nondevelopers**:\n- Basic Cucumber report can show the scenarios as a web page\n- Consider using Pickles tool for living documentation\n\n**Visible Workings and Single Source of Truth**:\n- Transcoding used to generate XML report is defined in code and Excel file\n- Duplication of knowledge, no particular benefit\n- Spreadsheet file should be the ""golden source"" of transcoding:\n  - Code parses this file to drive its behavior\n  - File serves as its own documentation\n- Alternatively, code can generate a readable file with a generic structure",845,26.105167865753174
False,3,"185-Contact Information and Guided Tours.pdf,185-Contact Information and Guided Tours,""Case Study: Introducing Living Documentation in a Batch System","185-Contact Information and Guided Tours.pdf,185-Contact Information and Guided Tours,""Case Study: Introducing Living Documentation in a Batch System 389\ncode would embed the equivalent of the former spreadsheet file, but hardcoded as a \ndictionary (for example, in a map in Java).\nThis data structure can then be exported as a file in various formats (.xls, .csv , \n.xml, .json, and so on) for nondeveloper audiences.\nIntegrated Documentation for Developers and a Living \nGlossary for Other Stakeholders\nDoes the team really need to produce Javadoc reports? It’s so easy to browse the code \nin the IDE that the team probably won’t use the Javadoc reports much. The Javadoc reports are now available directly at their fingertips in your IDE. The same is true for UML class diagrams of classes and their type hierarchies. All this is already inte-grated documentation that is built in to the team’s editors.\nIf the team really needs a reference to give access to the concepts to nondevel-\nopers, it might introduce a living glossary , which scans the code in the \n/domain  \npackage to generate a Markdown and HTML glossary of all the business domain concepts in the code, extracted from classes, interfaces, enum constants, and perhaps some method names and Javadoc comments. Of course, for the glossary to be good, the team will probably have to review and fix many of these Javadoc comments.\nA Living Diagram to Show the Design Intent\nIf the internal design follows a known structure, such as the Hexagonal Architecture, the team can make it visible with the naming conventions of the corresponding modules. This naming convention and the name of the structure must be docu-mented in the README file:\n1 The design of this module follow the Hexagonal Architecture\npattern (link to a reference on the web).2 34 By convention, the domain model code is in the 6 src/*/domain*/ package, and the rest is all infrastructure7 code\nThis is more ready-made documentation.\nThe team might include a link to the domain model package, but it has to survive \nrefactoring changes such as moving the domain folder into another folder. T o make the link more stable, the team can make it a bookmarked search directly based on the naming convention as a regular expression: \nsrc/*/domain*/ ."",2206",2299,**Living Documentation for Software Development:**\n* **Integrated Documentation for Developers**\n	+ Javadoc reports available directly in IDEs\n	+ UML class diagrams integrated into editors\n	+ No need for separate Javadoc reports if integrated well\n* **Living Glossary for Other Stakeholders**\n	+ Extract business domain concepts from code\n	+ Generate Markdown and HTML glossary\n	+ Review and fix Javadoc comments for accuracy\n* **Design Intent Visualization**\n	+ Hexagonal Architecture design follow-up\n	+ Document naming conventions and structure\n	+ Use regular expressions as bookmarks for stability.,602,19.733833074569702
False,3,"186-Microservices Big Picture.pdf,186-Microservices Big Picture,,0","186-Microservices Big Picture.pdf,186-Microservices Big Picture,,0",66,"**Microservices: The Big Picture**\n\n**Concepts:** Microservices and Big Picture\n\n**Key Points:** Understanding microservices through a comprehensive perspective.\n\n**Explanation:**\n- Discuss the idea of microservices from a broad standpoint, emphasizing its significance in modern software development.\n- Highlight the benefits, such as scalability, flexibility, independent deployment, and easier maintenance that make it an attractive choice for building applications.\n- Describe how microservices work by breaking down large applications into smaller components or services that can communicate with each other through APIs.\n- Discuss how these independent parts of the application can be developed, deployed, and updated separately, leading to improved efficiency and faster development cycles.\n- Explain how microservices help organizations meet the challenges of modern technology landscapes by delivering more value to their customers through increased agility and flexibility.",983,25.168114185333252
False,3,"187-A Living Documentation Initiative.pdf,187-A Living Documentation Initiative,""Chapter 13 Introducing Living Documentation to a New Environment 390\","187-A Living Documentation Initiative.pdf,187-A Living Documentation Initiative,""Chapter 13 Introducing Living Documentation to a New Environment 390\nContact Information and Guided Tours\nWho should someone contact for questions? The service registry , Consul  in this case, \nshould have this information, as required by the company architects.\nA guided tour just for the batch is not very difficult to create with a custom anno-\ntation, but it may not be very useful for developers. The batch is built with the very \nstandard and well-documented Spring Batch framework. This framework com-pletely controls the way the processing takes place. It is safe to assume that all the developers know about this framework and the way it works or that they can learn about it from the standard documentation and tutorials. There is no need to create an additional custom guided tour for that.\nMicroservices Big Picture\nHow does the data pump module fit within the bigger system made of many micros-ervices? Answering this question takes some effort. One approach would be to regu-larly run a journey test (an end-to-end scenario that goes through a large number of components of the system) on some environment with distributed tracing enabled. T ools like Selenium for running the test and Zipkin for distributed tracing may come \nto mind. The team could then visualize the distributed traces to produce a guided tour that reveals what happens between services during each journey test, to provide a big picture of the system. As is usual for living documents, the curation is key to filtering what matters (for example, what services are talking to what other services in this scenario) out of a huge number of details (all the calls between services and all the events on the messaging bus between them).\nSelling Living Documentation to Management\nA common question about any new approach is “How do I convince my manage-ment to try it?” There are different answers to this question, depending on the context.\nThe first, and my preferred, answer is that it is up to the team to select the way to \nmeet the expectations of other stakeholders. It is everyone’s business to require that knowledge be shared, but does the team really need approval to decide how to per-form its work efficiently? Keep in mind that everyone on the team—developers, test-ers, and business analysts—are also stakeholders of the project. T o better deliver to other stakeholders, they have to take care of themselves first. They also need enough autonomy to try practices, and then, as Woody Zuill says, “to amplify the good\n2” \nand perhaps stop what does not.\n2. https://www .infoq.com/news/2016/06/mob-programming-zuill\nSelling Living Documentation to Management 391\nIf your company and managers are proud to be “really agile” and to “empower \ntheir teams,” then they should trust the team, and you should not need any formal \napproval to try living documentation or any of the related recommended practices, even the most radical ones, like pair programming and mob programming. Of course, this autonomy comes with the full responsibility for the actual results.\nThat said, it may be the case that putting in place a living glossary or a living \ndiagram for the first time requires somewhere between half a day and two days of work. This might be too long to do the effort without having it in a formal backlog, in which case you need to convince someone.\nIf there is a documentation budget or if documentation tasks are planned already , \nyou might also want to reuse that time to invest in living documentation instead. Again, this might require approval.\nStarting with an Actual Problem\nWhen introducing new approaches, you shouldn’t preach. Instead, you should show the benefits, and the best way to do this is on a real problem that is ready to be tackled.\nT o find out a real knowledge problem, you might ask those around you, “Is there \nanything you would not feel comfortable working on alone?” or “Is there anything that is not clear to you?” Or you might not ask anything but just pay attention to the questions asked during the day , during the week, or during the iteration. Some of them will hint at candidates for documentation.\nOne efficient way to know what’s important is to carefully take notes on every-\nthing you mention or explain to each newcomer during the onboarding period. If you ask new joiners for an astonishment report, it will contain candidates for stuff that should be either fixed or documented.\nIf you have identified a knowledge-sharing issue, make sure everyone acknowl-\nedges that it is a genuine documentation problem worth tackling. Then propose a solution, inspired from this book. Y ou don’t have to use the term living documenta-\ntion but can just mention that you know of an approach that has already been done in other companies, in large-ish corporations, and in small early startups, too.\nY ou may also start with something small, done on your own time, that you can \nshow to the managers you want to convince. It could be a report, or a diagram, or a mix of a documentation plus some indicators that managers are particularly inter-ested in. Emphasize how you can save time and improve satisfaction by using to the approach.\nOnce your project is done, the benefits should be enough to convince people to \nkeep the approach. And if the benefits are not there, please tell me, so that I can improve this book. Still, even in the worst case, you will learn something valuable in \nChapter 13 Introducing Living Documentation to a New Environment 392\nthe process, and you will probably have one example of traditional documentation \nthat was just a bit more expensive than typical.\nA Living Documentation Initiative\nIf there are a lot of pressing documentation issues, you might want to start with a somewhat ambitious initiative on living documentation. This book can help you push the idea forward and make it a standard package that can be a reference. Show the book to the people you want to convince. Show a video of talks on the same topic (see, for example, https://vimeo.com/131660202). I’ve done many such videos that have been well received.\nShowing the benefits on a “pilot” case within a company is usually the best option \nto start with. Nobody will pay much attention at first, but with early successes, more people will try to replicate or even formalize the initiative for the benefit of their careers.\nAs soon as we’re talking about an identified initiative, we have to convince upper \nmanagement that it is worth investing time in the teams, as well as possibly some additional coaching and consulting. One way to sell living documentation is to con-sider that it is a prerequisite to achieve sustainable continuous delivery , and a bit like testing is a prerequisite, too: Just as you need an automated testing strategy to go fast, you also need a living documentation strategy .\nMany of the key reasons to adopt a living documentation approach are already in \nfront of your eyes and show up in your weekly time-tracking and in the current state of your knowledge management.\nOverall, my feeling is that documentation is a concern close to managers’ hearts. \nThe matter of skills and knowledge transmission between team members is already a common source of anxiety for management; it represents a cost in time and also, more importantly , in defects and mistakes:\n •Skills matrix creation and updates\n •Turnover rate\n •Time spent onboarding newcomers\n •Anxiety related to the truck factor (the risk of losing key knowledge if a team is hit by a truck when coming back from lunch)\n •The ratio of defects and incidents caused by “I didn’t know that”\nLack of documentation is a hidden cost, just like the lack of tests. Every change \nneeds a complete investigation and an assessment, sometimes even a pre-study . \nSelling Living Documentation to Management 393\nThe hidden knowledge has to be mined again each time. Alternatively , changes will \nbe made in a way that is not in line with the previous vision of the system, which makes the application increasingly bloated and makes matters worse over time. This may show up in the following as\n •Increased time to deliver a change.\n •Negative trends of any code quality metrics, the most telling of which being the mere size of the code base. (If it grows regularly , it is probably a sign that the design is too weak. There is not enough refactoring to start with, and each change is an addition.)\nAnd there are also arguments on the documentation, or lack thereof, in itself:\n •Documentation tasks that are not done, or documentation that is visibly not updated frequently enough\n •Compliance requirements with respect to documentation\n •Time spent writing documentation or updating existing documentation\n •Time lost searching the right documentation\n •Time lost reading documentation that is incorrect\nY ou might want to perform a review of the quality of the existing documents that \npretend to be the documentation, with a focus on various indicators, such as the following:\n •The number of different places where documentation can be found (including the source code, the wiki, each shared drive, and team members’ machines)\n •The time of last update\n •The proportion of authors of the last updates who left the team\n •The amount of rationale (explaining why  rather than just what) in the \ndocumentation\n •The number of pages or paragraphs or diagram that can still be trusted\n •The among of knowledge that is redundant between the source and another kind of documentation\n •A short survey such as “Do you know where I can find knowledge on that?” on a random set of concerns"",9660",9747,"**Living Documentation Initiative**\n\n**Contact Information and Guided Tours**:\n- Contact information can be obtained from the service registry (Consul) as required by company architects\n- A guided tour for a specific batch is not very useful for developers, as the processing is completely controlled by the Spring Batch framework\n\n**Microservices Big Picture**:\n- How the data pump module fits within the larger system of many microservices\n- Regularly running journey tests with distributed tracing can help visualize the system and provide a ""big picture"" guided tour\n\n**Selling Living Documentation to Management**:\n- Team members should be empowered to select practices that meet stakeholder expectations\n- If the company is ""proud to be 'really agile' and 'empower their teams,'"" there should be no need for formal approval to try new approaches like living documentation, pair programming, or mob programming\n- However, some time may be required to get started, which might require approval if it's part of a formal documentation budget\n\n**Starting with an Actual Problem**:\n- Identify a real knowledge-sharing issue and propose a solution inspired by the living documentation approach\n- Start small and demonstrate the benefits on a ""pilot"" case within the company\n\n**Living Documentation Initiative**:\n- Consider presenting the book or videos on the topic to management as part of an initiative to adopt a living documentation strategy\n- Emphasize that living documentation is a prerequisite for achieving sustainable continuous delivery, similar to testing\n- Documentation is a concern close to managers' hearts, as it represents a cost in time and defects/mistakes",1675,54.93036079406738
False,3,"188-Compliance in Spirit.pdf,188-Compliance in Spirit,""Chapter 13 Introducing Living Documentation to a New Environment 394\nY ou can come up with man","188-Compliance in Spirit.pdf,188-Compliance in Spirit,""Chapter 13 Introducing Living Documentation to a New Environment 394\nY ou can come up with many other ideas to help realize the actual state of docu-\nmentation. If everything is fine and under control, then the only thing that living \ndocumentation might improve is the long-term cost, thanks to team members work-ing together more, automation, and reduction of various types of waste.\nIf everything is not fine and under control, living documentation can make docu-\nmentation feasible again, at a reasonable cost and with identified value added.\nOn the value side, it is worth putting the emphasis on the biggest benefits, which \nare not just the sharing of knowledge but especially the side benefits of improving the software in the process (see Chapter 11, “Beyond Documentation: Living Design”).\nContrasting the Current Situation with the Promise of a Better \nWorld to Match People’s Aspirations\nIn her book Resonate, Nancy Duarte offers suggestions on how to stimulate excite-\nment and enthusiasm through presentations. It starts with knowing why you want to change things. If you’ve decided to introduce living documentation to your team or company , you could start by answering these questions: “Why do I want to share and promote that?” and “Why am I excited?”\nThen you can contrast the current situation with the new practice you’d like to \npromote. Here are examples of common frustrations that could be contrasted with the benefits of a living documentation approach:\n •Y ou don’t write documentation, and you feel guilty about that.\n •Explaining things to team members, new joiners, and stakeholders outside the team takes a lot of time on an ongoing basis.\n •Y ou write documentation, but you’d prefer to write code.\n •Y ou’re looking for documentation, and when you find some, you cannot trust it because it’s out of date.\n •When you create diagrams, you’re frustrated that it takes so much time.\n •Looking for the right document takes so much time for such a small benefit that you often give up and try to do the work without it.\n •When you collaborate the agile, way with lots of conversations, you feel uncomfortable because your organization expects to deliver more traceable and archived documents.\n •Y ou do a lot of tedious work manually , including deployment, explaining stuff to external people, and paperwork, and you have a feeling it could be avoided.\nCompliance in Spirit 395\nOf course, it’s up to you to customize and decide which items have the most \nimpact in your context and to decide what part of living documentation remedies \nthat frustration most.\nThe following generalizations tend to be true of developers:\n •They don’t like writing documentation.\n •They like to write code.\n •They love code and find that doing more with code is appealing.\n •They hate manual, repetitive tasks.\n •They love automation.\n •They are proud of beautiful code.\n •They love plain text and their favorite tools.\n •They love logical things (for example, text-first, DR Y).\n •They love to exhibit mastery and geek culture.\n •They want recognition of skills.\n •They empathize with real-life messy situations. \nOn the other hand, the following generalizations tend to be true of managers: \n •They love more transparency on the work of their teams.\n •They love to see things presented in ways they can feel so that they can under-stand whether it’s getting better or worse.\n •They like documentation they can themselves show someone else and be proud of. \n •They want documentation to be more turnover-proof.\nIt’s important to see both sides. It’s critical for a documentation strategy to \nexhibit a vision that everybody would genuinely like to happen.\nCompliance in Spirit\nA living documentation approach can work with even the most demanding compli-ance requirements by aiming for the spirit instead of aiming for the letter.\nIf your domain is regulated or if your company requires a lot of documentation \nfor compliance reasons (for example, ITIL), you probably spend a lot of time on \nChapter 13 Introducing Living Documentation to a New Environment 396\ndocumentation tasks. The ideas from living documentation can meet the compliance \ngoals, reducing the burden for the teams and saving time, while improving the quality of the produced documentation and of the product at the same time.\nRegulators often focus on requirements tracking and change management as a \nway to improve quality . For example, the U.S. Food and Drug Administration writes in its “General Principles of Software Validation; Final Guidance for Industry and FDA Staff”:\nSeemingly insignificant changes in software code can create unexpected and very signif-\nicant problems elsewhere in the software program. The software development process should be sufficiently well planned, controlled, and documented to detect and correct unexpected results from software changes.\nGiven the high demand for software professionals and the highly mobile workforce, \nthe software personnel who make maintenance changes to software may not have been involved in the original software development. Therefore, accurate and thorough docu-mentation is essential.\n3\nThe same FDA document also describes the importance of testing and of design \nand code reviews.\nIt might look at first glance as though agile practices are less documentation ori-\nented and therefore are not well suited for demanding compliance requirements. But \nquite the opposite is true. When agile practices, which are part of the living docu-mentation spectrum, are applied, what you actually have is a documentation process that is more rigorous than all the traditional documentation-heavy processes.\nSpecification by example (BDD)—with scenarios with automation, living dia-\ngrams, and a living glossary—provides extensive documentation on each build. If you commit five times in an hour, you get your documentation updated five times per hour, and it is always accurate. Paper-heavy processes cannot dream of this level of performance!\nWorking collectively with colleagues to ensure that at least three or four peo-\nple know of each change is also an important contribution to various compliance requirements, even though the knowledge is not necessarily written outside the source code.\nY ou see the idea here: A development teams with a good command of agile devel-\nopment practices and principles, including living documentation and other continu-ous delivery ideas, is already quite close to matching most compliance requirements, even the notoriously heavy ones like ITIL.\nKeep in mind that agile practices in general do not necessarily meet the imple-\nmentation details of your company compliance guidelines, which are often full of \n3. U.S. Food and Drug Administration, “General Principles of Software Validation; Final Guidance for \nIndustry and FDA Staff,” http://www .fda.gov/RegulatoryInformation/Guidances/ucm085281.htm"",6950",7011,"**Benefits of Living Documentation**\n- Improves long-term cost through team collaboration, automation, and reduction of waste\n- Makes documentation feasible for teams when current situation is not under control\n- Emphasizes value added beyond just knowledge sharing: improving software in the process\n\n**Contrasting Current Situation with Promise of Living Documentation**\n- Frustrations:\n  - Not writing documentation, feeling guilty about it\n  - Time consuming to explain things to team members and stakeholders\n  - Preference for writing code over documentation\n  - Outdated or unreliable documentation\n  - Difficulty creating diagrams\n  - Tedious manual tasks like deployment and paperwork\n- Generalizations about developers:\n  - Dislike writing documentation\n  - Love code, automation, and beautiful code\n  - Emphasis on text-first, logical approaches\n  - Pride in mastery and geek culture\n- Generalizations about managers:\n  - Desire for team transparency and well-presented documentation\n  - Need for turnover-proof documentation\n\n**Living Documentation and Compliance Requirements**\n- Can meet compliance goals while reducing burden and saving time\n- Regulators focus on requirements tracking, change management, testing, and design reviews\n- Agile practices, including BDD, provide extensive documentation through specification by example and collaboration\n- Collective knowledge sharing contributes to various compliance requirements.\n\n**Living Documentation vs Traditional Approaches**\n- Living documentation is more rigorous than traditional approaches in terms of compliance due to continuous delivery ideas and agile practices.",1641,48.460837841033936
False,3,"189-Summary.pdf,189-Summary,""Compliance in Spirit 397\nburdensome procedures and paperwork. Still, agile practices often meet or even \nexceed the hig","189-Summary.pdf,189-Summary,""Compliance in Spirit 397\nburdensome procedures and paperwork. Still, agile practices often meet or even \nexceed the higher-level goals aimed for by the compliance bodies, which revolve around risk mitigation and traceability . Agile or not, in the development team or in the compliance office, we all want risk mitigation, some reasonable amount of traceability , quality under control, and improvement in everything. Y ou don’t have to follow 2,000 pages of boring ITIL guidelines. Y ou can substitute alternative practices that are more efficient and still be able to check most checkboxes in the checklist of the high-level objectives.\nTherefore: Review the compliance documentation requirements, and for each \nitem, identify how it could be satisfied with a living documentation approach, typically by using lightweight declarations, knowledge augmentation, and auto-mation. Mandatory formal documents based on company templates can easily be generated from knowledge managed in a totally different fashion (for exam-ple, from the source control system, the code, and the tests). When the compli-ance expectations are too burdensome, go back to their higher-level goal and identify how this goal could be directly satisfied with your practices instead. Whenever there is a real gap, it’s an opportunity to improve your development process. Finally, make sure your lightweight process is reviewed from time to time by the compliance team, so that it can grant your team a permanent  preapproval stamp.\nY ou’ll be surprised how your living documentation can meet or exceed compli-\nance expectations.\nCase Study: Compliance with ITIL\nPaul Reeves says in his great blog post “ Agile vs. ITIL”:\nOften people believe that rapid deployment / continuous deployment / daily builds etc. \ncan’t work in an environment that is highly process oriented, where rules and process have to be followed. (Usually they just don’t like someone else’s rules.)\nWell, the process is there to ensure consistency , responsibility , accountability , com-\nmunication, traceability , etc. and of course it CAN be designed to be a hinderance.  It, alternatively , CAN be designed to allow quick passage of releases. People blaming pro-cess or ITIL are just being immature. They may as well blame the weather.\n4\nMy experience from applying the ideas of continuous delivery has shown indeed \nthat it is possible to map from a lightweight, agile, low-cycle-time process inside the \ndevelopment team to a more traditional, usually slower and paper-intensive process \n4. Paul Reeves, Reeves’s Results blog, http://reevesresults.blogspot.fr/2011/03/agile-vs-itil.html\nChapter 13 Introducing Living Documentation to a New Environment 398\noutside. In contrast to common beliefs, your agile process is probably more disci-\nplined than the other project managed in an ITIL-by-the-book fashion: It’s hard to beat a process where automation can produce extensive functional documentation, extensive test results and coverage, security and accessibility checks, design dia-grams, and release notes with links to the requested features in a tool and archived emails for the release decision, on each build, several times a day!\nWhen strict procedures are important, using automation and enforced guidelines \nis the best way to make sure they are respected while reducing the burden of manu-ally applying them. Procedures are great for machines, not for people. The right tools protect a development team and remove the manual chores at the same time. How-ever, and it may seem like a paradox, good tools still draw attention to the quality expectations by making very visible whenever they are not met. With this protective harness, every team member is learning the quality expectations on the job, while having the satisfaction of always doing productive work.\nThe ITIL Example\nLet’s focus on an example of managing requests for change under the ITIL concep-tual framework by looking at Table 13.1 and Table 13.2.\n \nNote that agile practices promote slicing the work as thinly as possible. It is incon-\nvenient to track every slice in a tracking tool when an iteration contains dozens of slices, each only a few hours long. But this level of granularity does not matter much \nTable 13.1 Request for Change Management\nChange ActivityExample of Agile\nForgotten PracticeExample of Documentation Media\nCollection of change requestsUser stories or bugs and enhancements with description, origin, requestor, date, business priority , and expected benefitsStickers on the wall and a tracking tool (such as Jira)\nStudy and impacts BDD, TDD, tests All living documentation \nartifacts\nDecisions Decision, names of decision makers, target version, dateCAB report (email as PDF)\nFollow up Not started, in progress, done), assigneeT racking tool (such as Jira)\nSummary 399\nfor the management of requests for change; as a consequence, you may only track \ncohesive aggregates of slices in the tool. \nThe point here is really to realize that your living documentation can meet or \nexceed the toughest compliance expectations while keeping the extra compliance-specific work to a minimum. This could be an incentive in itself to introduce living documentation if you’re in a compliance-intensive environment.\nSummary\nIntroducing living documentation is best done undercover first, to build the confi-dence to expand to bigger and more visible initiatives. As a start, you can decide to Table 13.2 Release Management\nRelease Activity Example of Agile PracticeExample of \nDocumentation Media\nContent Release notes with a link to related change(s), dates, downtime, test strategy , impacts (business, IT , infrastructure, security)Ticketing tool (may be automated as a mix of prewritten documents and generated release notes)\nImpacts Based on the change study plus the feedbacks from the iteration demoLiving documentation, archived as PDF\nRelease checks Automated test, including \ntests on SLA, deployments tests in pre-production environments, smoke testsCI tool, deploy tool results, tests reports\nApproval Decision, names of the \ndecision makers, actual delivery date, target version, rollout date, decision date, go/no-go conditionsEmail saved as PDF\nDeployment successfulDeployment and post-deployment testsDeployment tool and post-deployment test reports\nContinuous improvementRetrospectives notes, with names, action plan, issuesWiki, email, picture of the whiteboard\nChapter 13 Introducing Living Documentation to a New Environment 400\nmigrate painful traditional documentation into its more living equivalent, using \nsome of the patterns discussed in this book. \nIf you need budget or time to expand your efforts, remember that managers are \noften concerned about retaining knowledge. And when facing the objection that your situation is special because you’re regulated or you have to follow some strict compliance framework, remember that you can meet and even exceed requirements by sticking to the spirit instead of the letter."",7040",7075,"**Compliance with Agile Practices**\n\n**Agile vs. Burdensome Procedures**:\n- Agile practices often exceed higher-level compliance goals, such as:\n  - Risk mitigation\n  - Traceability\n  - Reasonable amount of traceability\n  - Quality under control\n  - Improvement in everything\n- Compliance teams and development teams share common goals\n\n**Living Documentation Approach**:\n- Review compliance documentation requirements\n- Identify how they can be satisfied using a living documentation approach:\n  - Lightweight declarations\n  - Knowledge augmentation\n  - Automation\n- Mandatory formal documents can be generated from knowledge managed differently\n- If compliance expectations are burdensome, identify how the higher-level goals can be directly satisfied with your practices\n\n**Case Study: Compliance with ITIL**:\n- **Agile vs. ITIL**: Agile process is more disciplined than traditional ITIL-by-the-book processes\n  - Automation and enforced guidelines protect the development team and reduce manual chores\n  - Tools draw attention to quality expectations and make them visible\n\n**Living Documentation Patterns**:\n- Migrate painful traditional documentation into a more living equivalent\n  - Use patterns discussed in this book\n  - Managers are concerned about retaining knowledge\n\n**Continuous Improvement**:\n- Retrospectives notes with action plans and issues\n  - Wiki, email, picture of the whiteboard",1403,44.85467267036438
False,3,"190-Chapter 14 Documenting Legacy Applications.pdf,190-Chapter 14 Documenting Legacy Applications,,0","190-Chapter 14 Documenting Legacy Applications.pdf,190-Chapter 14 Documenting Legacy Applications,,0",100,"**Documenting Legacy Applications**\n\n**Chapter 14**: Focus on the topic of documenting legacy applications in software development or IT field.\n\n**Legacy Applications**: These are older software systems that need to be documented for various reasons such as:\n- Maintaining the software\n- Updating or modernizing it\n- Ensuring compatibility with new technologies\n\n**Documentation**: Crucial for understanding and managing legacy applications, which may lack adequate documentation. It includes:\n1. Functional description: Explanation of what the application does and how to use it.\n2. Technical details: Hardware/software requirements, programming languages, databases, interfaces, APIs, etc.\n3. Design documentation: Data flow diagrams, architecture diagrams, class diagrams, entity relationship diagrams.\n4. User guides or manual: Instructions for users to install, operate and maintain the application.\n5. Test plans and results: Documentation of testing procedures, tests cases, and test results.\n6. Maintenance and troubleshooting guide: Tips on how to diagnose issues, debug, and resolve them.\n7. Change log or release notes: Records of modifications, updates, and enhancements made to the application over time.\n8. Security information: Details about encryption methods, access control, vulnerabilities, and mitigation strategies.\n\n**Importance**: Proper documentation keeps legacy applications functional and secure while ensuring a smooth transition during upgrades or transfers to new teams. It also serves as valuable historical data for future projects.",1564,44.43138766288757
False,3,"191-Bubble Context.pdf,191-Bubble Context,""401Chapter 14\nDocumenting Legacy \nApplications\nThe universe is made of  information, but it doesn’t have","191-Bubble Context.pdf,191-Bubble Context,""401Chapter 14\nDocumenting Legacy \nApplications\nThe universe is made of  information, but it doesn’t have meaning - meaning is \nour  creation. Searches for meaning are searches in a mirror .\n—@KevlinHenney\nThis quote illustrates the case with legacy systems: They are full of knowledge, but it is usually encrypted, and we have lost the keys. Without tests, we have no clear defini-tion of the expected behavior of a legacy system. Without consistent structure, we have to guess how it was designed and for what reasons and how it is supposed to be evolved. Without careful naming, we have to guess and infer the meaning of varia-bles, methods, and classes, as well as what code is responsible for what.\nIn short, we call systems “legacy” when their knowledge is not readily accessible. \nThey exemplify what we could call “documentation bankruptcy .”\nDocumentation Bankruptcy\nLegacy applications are quite valuable, and they cannot be simply unplugged. Most attempts to completely rewrite large legacy systems eventually fail. Legacy systems are a problem of rich organizations, which lived and became profitable long enough to grow a legacy . \nStill, legacy applications raise issues when they have to evolve due to changing \ncontext because changing them is usually expensive. The prohibitive cost of change is related to many issues, including duplication and lack of automated testing, and also to the lost knowledge. Any change requires a long and painful reverse engineer-ing of the knowledge from the code base, including a lot of guesswork, before one line of code is eventually touched at the end.\nChapter 14 Documenting Legacy Applications 402\nAll is not lost, though. In the following pages, you’ll see a few living documenta-\ntion techniques that particularly apply when working in or from legacy systems.\nLegacy Application as Fossilized Knowledge\nY ou’ve seen before that anything that can answer a question can be considered docu-\nmentation. If you can answer questions by using an application, then the application is part of the documentation. This is of great relevance in the case of a legacy system with lost specifications. Y ou have to use it to know how it behaves.\nIn the context of rewriting part of a legacy system, considering the legacy system \nas a source of knowledge can be handy since the new system will probably inherit a significant part of the behaviors of its predecessor. For each feature that will make it into the new system, the specifications can draw on the former system. In practice, while doing the specifications workshops, you can check how the legacy application behaved to get inspiration for the new one.\n \nThe “Rewriting with the Same Specs” Fallacy\nOne common failure mode of rewriting legacy systems is to rewrite them with \nexactly the same specifications. It makes little sense to rewrite a system with-out changing anything; doing so is just a lot of risk and a lot of waste. Chang-ing only the technology stack hardly does any good idea either, unless your hardware is no longer available commercially and there is no emulator.\nRewriting a piece of software is an expensive endeavor, even from a purely \ntechnical perspective, and the best way to improve the return is by taking the opportunity to reconsider the specifications at the same time. Many features are no longer useful. Many features should be adapted to new usages and contexts. The UI and its UX have to change drastically , and the changes will have impacts on the underlying services. Y ou’ll also want the new application to be cheaper to deliver more frequently , so you’ll want automated testing, too, which comes cheaper when you start from clear specifications as concrete examples, as advised by BDD .\nI strongly suggest not rewriting with the same specs. Rewrite a limited por-\ntion of the system and consider it as a project from scratch, with the legacy system, the working application, and its source code as a bonus to draw inspi-ration from.\n \nTherefore: In the context a rewriting a part of a legacy system, consider the leg-acy system as documentation to complement the discussions on the \nLegacy Application as Fossilized Knowledge 403\nspecifications, not as the given specifications. Make sure a business person such \nas a domain expert, business analyst, or product owner works closely with the team. Don’t fall for the fallacy that the legacy system is in itself a sufficient description of the new system to be rebuilt. Take the opportunity of the rewrit-ing to challenge every aspect from the legacy system: the functional scope, the business behaviors, the way it is structured into modules, and so on. Regain control of the knowledge from the start, with clear specifications expressed as concrete scenarios and a clear design.\nThe ideal configuration is a whole team, with all skills and roles inside the team, \nusing the idea of the three amigos: business perspective, development perspective, and quality perspective.\nHaving access to both a working legacy application and its source code is a nice \nbonus compared to projects starting purely from scratch. It’s like having another expert on the team, even if it is an old, sometimes irrelevant, expert. After all, the legacy system is the result of a patchwork of the decisions of many different people over a long period of time. It’s a fossil.\nA perfect example of when a legacy system can be instrumented is when it can \nprovide the answer to the question “How often is this feature used?”\nArcheology\nSoftware source code is one of the most densely packed forms of communication we \nhave. But, it is still a form of human communication. Refactoring gives us a very power-ful tool for improving our understanding of what someone else has written.\n—Chet Hendrickson, Software Archeology: Understanding Large Systems\nWhen you ask questions of a legacy code base, you need a piece of paper and a pen \nclose to your keyboard at all times to take notes and draw . Y ou need to create an on-demand map of the terrain for the task at hand. While exploring the code and play-ing with it at runtime or with the debugger, you need to write down the inputs, outputs, and all the affects you discover. Y ou need to take note of what’s read or writ-ten because the side effects are ultimately what matter. Knowing this information is also essential for mocking up or estimating the impacts of a change. Y ou should sketch how each responsibility depends on its neighbors, a technique Michael Feath-ers calls an effect map in his book W orking Effectively with Legacy Code.\n1\nIt’s important to keep the process low tech so that it does not distract from the \ntask at hand. This documentation work needs to be dedicated to a specific task, and there is therefore no need to make it clean and formal right now . However, when you’re done with the task, you may review the notes and sketches and select the one \n1. Feathers, Michael. W orking Effectively with Legacy Code. Boston: Pearson Education, Inc., 2004.\nChapter 14 Documenting Legacy Applications 404\nor two key bits of information that are general enough that they would help for many \ntasks. They can be promoted into a clean diagram, an additional section, or an addi-tion within an existing document. Y ou grow your documentation by a sedimentation mechanism (see Chapter 10, “ Avoiding T raditional Documentation”).\nOf course, you might have questions that the code does not answer. Perhaps the \ncode is obscure or surprising. In such a case, you need help, ideally from colleagues nearby , and human communication comes back into the picture. The legacy system is not just code; there are documents of all ages, slides, old blog posts, and pages on the wiki, and of course they are all wrong to some extent now .\nA legacy environment also includes people who were there at the beginning. The \nold developers may have moved to other positions now , but they might be able to answer questions, especially about the context that led to the decisions years ago.\nBubble Context \nEven in a legacy system, you want to work as much as possible in the ideal land where everything is nice and clean. If you have some number of features to build, then you might decide to build the new features in their own new clean bubble con-text. In practice, it can be a new specific module, namespace, or project, which means it is then easy to document by using annotations, naming conventions, and enforced guidelines. A bubble context brings the comfort and the efficiency of writing soft-ware from scratch in a brand-new project but integrated within a bigger legacy sur-rounding (see Figure 14.1).\n \nAs a bubble context is a project from scratch inside a legacy project, it is also a \nperfect place to practice TDD, BDD, and DDD on a limited functional area to deliver a bulk of related business value.Bubble\nContextLegacy\nApplication\nFigure 14.1 A clean bubble context integrated within a legacy mess\nBubble Context 405\nTherefore: If you need to make a lot of changes on a legacy system, consider \ncreating a bubble context. A bubble context defines boundaries within the rest of the system. Within these boundaries, you can rewrite in a different way, such as driven by tests. In the bubble context, you can invest in knowledge by follow-ing a living documentation approach. Conversely, if you really need full docu-mentation of a part of a legacy application, consider rewriting that part as a bubble context, using the state-of-the-art practices for the tests, the code, and the documentation.\nIt is a good idea to start with high expectations for the code inside the bubble \ncontext. Its architecture and guidelines should be enforced using automated tools, as a set of enforced guidelines. For example, you might want to forbid any new commit from having direct references (Java \nimport  or C# using ) on a deprecated compo-\nnent. Y ou might require and enforce test coverage higher than 90%, no major vio-lation, a maximum code complexity of 2, and a maximum of five parameters by method.\nGoing further in the coding style, if you use the bubble context approach, you \ncan declare demanding requirements for the full bubble as a whole, such as by using package-level annotations, as shown here:\n1  @BubbleContext(ProjectName = """"Invest3.0"""")\n2  @Immutable3  @Null-Free4  @Side-Effect-Free5  package acme.bigsystem.investmentallocation67  package acme.bigsystem.investmentallocation.domain8  package acme.bigsystem.investmentallocation.infra\nThe first annotation here just declares that this module (package in Java or \nnamespace in C#) is the root of a bubble context corresponding to a project named \nInvest3.0. The other annotations document that the expected coding style in this module favors immutability and avoids nulls and side effects. These coding styles can then be enforced by pair-programming or code review .\nThe bubble context was introduced by Eric Evans in 2013.\n2 A bubble context is a \nperfect technique for rewriting part of a legacy system, as in the strangler application \n2. Eric Evans, Getting Started with DDD when Surrounded by Legacy Systems, 2013, http://domain\nlanguage.com/wp-content/uploads/2016/04/GettingStartedWithDDDWhenSurroundedByLegacy\nSystemsV1.pdf"",11308",11360,"**Understanding Legacy Applications as Fossilized Knowledge**\n\n**Legacy Systems:**\n- Information encrypted, knowledge lost\n- No clear definition of expected behavior\n- Lack consistent structure\n- Guessing meaning from variable names and methods\n- Costly to change due to:\n  - Duplication and lack of automated testing\n  - Lost knowledge\n\n**Documentation Bankruptcy:**\n- Legacy applications are valuable, cannot be ignored\n- Rewriting large systems often fails\n- Changing context requires reverse engineering\n\n**Legacy System as Fossilized Knowledge:**\n- Use legacy system to inform new development\n- Consider it a source of knowledge, not just code\n- Business expert should work closely with team\n- Challenge every aspect from the legacy system\n\n**Rewriting Legacy Systems:**\n- Avoid rewriting using same specs\n- Opportunity to improve functional scope, business behaviors\n- Automated testing and clear specifications\n\n**Archeology of Legacy Code:**\n- Source code as dense form of human communication\n- Note important inputs, outputs, effects\n- Documentation work dedicated to specific tasks\n- Grow documentation through sedimentation mechanism\n\n**Bubble Context:**\n- Work within boundaries of new project within legacy system\n- Enforced guidelines and automated tools for coding style\n- Declare requirements using package-level annotations\n- Perfect technique for rewriting part of a legacy system.",1399,46.572795152664185
False,3,"192-Superimposed Structure.pdf,192-Superimposed Structure,""Chapter 14 Documenting Legacy Applications 406\npattern by Martin Fowler.3 The idea is to r","192-Superimposed Structure.pdf,192-Superimposed Structure,""Chapter 14 Documenting Legacy Applications 406\npattern by Martin Fowler.3 The idea is to rebuild a consistent functional area that \nwill progressively take over the old system.\nSuperimposed Structure\nEspecially when creating a bubble context integrated within a bigger legacy applica-\ntion, it is hard to define the boundaries between the old and new systems. It is even difficult to just discuss it very clearly because it is hard to talk about a legacy system. Y ou would expect to see a simple and clear structure, but what you actually discover is a big unstructured mess (see Figure 14.2).\n \nEven when there is a structure, it is often arbitrary and can mislead more than it \nhelps (see Figure 14.3). \n3. https://www .martinfowler.com/bliki/StranglerApplication.htmlActual?\nExpected\nFigure 14.2 Mental expectations versus actual situation\n…+ com.acme.daocom.acme\n+ com.acme.dto\n+ com.acme.service\n+ com.acme.controller\nFigure 14.3 Typical project structure\nSuperimposed Structure 407\nWith legacy code, you usually start with lots of effort to make it testable. Tests \nenable you to make changes but are not enough. T o make changes, you also need to \nreconstruct a mental model of the legacy application in your head. This can be local within a function, or it can be as big as the full business behavior plus the complete technical architecture.\nY ou need to read code, interview older developers, and fix bugs to better under-\nstand the behavior. At the same time, you need to use your brain to make sense of what you see. The result is a structure in your head that you project over the existing \napplication. Since the existing application does not show this structure, it is up to you to superimpose a new clear structure onto the existing application.\nTherefore: In the context of creating a bubble context and adding a feature or \nfixing a difficult bug in a legacy system, create your own mental model of the legacy system. This model does not have to be visible at all when reading the legacy code. Instead, this new structure of the old system is a projected vision, an invention. Document this vision using any form of documentation, so that it becomes part of your language for future discussions and decisions.\nThis new structure is a hallucination, a vision, that is not directly extracted from \nthe system as it is currently built. Y ou might see it as a description of the system as it should have been built as opposed to how it is built, in retrospect, now that everyone knows better.\nY ou can show the new model as a superimposed structure on top of the legacy \nsystem, as a plain sketch that you show to everyone involved. It is desirable to show how the new structure relates to the current state, but it can be too hard to achieve as soon as you want some details, given that the current system may have a totally different structure. Y ou can invest time in making it a proper slide deck to present to every stakeholder during a roadshow . Y ou might instead decide to make it vis-ible within the code itself to make it more obvious and to pave the way for further transformations.\nThe following are some examples of mental models superimposed on top of leg-\nacy systems:\n •Business pipeline: This perspective of the business is similar to the standard sales funnel of salespeople. It focuses on the system as a pipeline of stages in the order in which they happen in a typical user journey: A visitor navigates the catalog (catalog stage), adds items to the shopping cart (shopping cart stage), reviews the order (order preparation stage), pays (payment stage), receives a confirma-tion and the product, and gets after-sale service if things go wrong. This model assumes that the volume decreases by a large factor at each stage, which is a nice insight for designing each stage technically and from an operational point of view ."",3891",3956,"**Superimposed Structure**\n\n**Chapter 14: Documenting Legacy Applications**\n- Discussing boundaries between old and new systems is difficult due to unstructured mess of legacy applications (Figure 14.2)\n  - Hard to define, even when there's structure it can be arbitrary and misleading (Figure 14.3)\n\n**Superimposed Structure in Legacy Code:**\n- Start with making code testable\n- Need a mental model of the legacy application\n  - Can be local or full business behavior + complete technical architecture\n- Read code, interview older developers, fix bugs to understand behavior\n- Document this new structure for future discussions and decisions (Figure 14.4)\n  - May not be visible when reading legacy code\n  - A ""hallucination,"" a vision not directly extracted from the system\n\n**Superimposed Structure Examples:**\n- Business pipeline: focuses on system as a series of stages in user journey (catalog, shopping cart, order preparation, payment, after-sales service)\n  - Decreases volume at each stage for designing and operational purposes.",1039,34.31815433502197
False,3,"193-Highlighted Structure.pdf,193-Highlighted Structure,""Chapter 14 Documenting Legacy Applications 408\n •Main business assets, as in asset capture (","193-Highlighted Structure.pdf,193-Highlighted Structure,""Chapter 14 Documenting Legacy Applications 408\n •Main business assets, as in asset capture (Martin Fowler): This perspective \nfocuses on the two or three main assets of the business domain, such as the customer and the product in the case of an e-commerce system. Each asset can be seen as a dimension that can itself be split into segments, such as customer segments and product segments.\n •Domains and subdomains, or bounded contexts (Eric Evans): This perspec-tive requires some maturity in terms of both DDD and the overall business domain, but it also has the most benefits.\n •Levels of responsibility: There are operational, tactical, and strategic levels, from a business perspective. Eric Evans mentions this in his book Domain-Driven Design.\n •A mix of these views: For example, you might consider three dimensions—customer, product, and processing—each segmented into stages, customer seg-ments, and product segments. Y ou can also mix a business pipeline from left to right and the operational, tactical, and strategic levels from the bottom up.\nWhatever the superimposed structure, once you have it, it becomes simpler to talk \nabout the system. Y ou might, for example, propose to “rewrite everything about the payment stage, starting with products that can be downloaded as a first phase.” Or you might decide to “rewrite the catalog part only for B2B customers.” Communica-tion becomes more efficient when you have a superimposed structure.\nHowever, it is up to every member of the team to interpret these sentences the \nway they see them. It is therefore useful to make the superimposed structure more visible.\nHighlighted Structure\nThe superimposed structure can be linked to the existing code. If you’re lucky , the mapping between the superimposed structure and the existing structure of the code is just a large number of messy one-to-one relationships. If you’re not lucky , this can just be an impossible task.\nY ou can add the intrinsic information of the superimposed structure on each ele-\nment. For example, one DTO shown in Figure 14.4 is part of the billing domain, another is part of the catalog domain, and so on.\n \nT o make the new structure visible, you can use annotation on classes, interfaces, \nmethods, and even modules or project-level files. Some IDEs also offer ways to tag files in order to group them, but it depends on the IDE, and the tags are not usually \nHighlighted Structure 409\nstored within the files themselves. In the following example, you would tag the classes \nin each module with annotations to denote the subdomain they are about:\n1  module DTO\n2    OrderDTO @ShoppingCart3    BAddressDTO @Billing4    ProductDTO @Catalog5    ShippingCostDTO @Billing\nThis will help prepare for the next step: Move the classes that deal with billing into \nthe same billing module. But even before you do that, your code now has an explicit \nstructure showing the business domain if you search by the @Billing  annotation:\n1  module Billing\n2    BillingAddressDTO //renamed to fix the abbreviation3    ShippingCostDTO4    ShippingCostConfiguration5    ShippingCost @Service\nThe end purpose of a superimposed structure should be to become the primary \nstructure of the system so that it is no longer superimposed. Unfortunately , in many \ncases, this never happens because the effort will not reach the “end state.” How-ever, this approach is still valuable because it can help deliver precious business value in the meantime. Even if the legacy code is badly structured, as long as you reason about it using a better structure, you are getting the benefits of better decisions.dto\ndao\nservice\ndto\ndao\nservice servicebilling\nshippingdto\ndao\nFigure 14.4 Example of  mapping between a technical structure and a business-driven \nstructure"",3810",3873,"**Documenting Legacy Applications**\n\n**Superimposed Structure**:\n- Focuses on main business assets (customer, product)\n- Can be split into segments (customer segments, product segments)\n- Operational, tactical, and strategic levels from a business perspective\n- Mix of views (dimensions, business pipeline)\n- Helps in efficient communication within the team\n\n**Making Superimposed Structure Visible**:\n- Can be linked to existing code\n- Use annotations on classes, interfaces, methods, or modules\n- Tag files based on IDE capabilities (not stored within files)\n- Example: @Billing annotation for billing domain classes\n\n**Benefits of Superimposed Structure**:\n- Helps deliver business value even if legacy code is badly structured\n- Better decision making by reasoning about it using a better structure.",803,28.903488636016846
False,3,"194-External Annotations.pdf,194-External Annotations,,0","194-External Annotations.pdf,194-External Annotations,,0",56,"**External Annotations: Understanding the Document**\n* The provided text is a brief instruction on repeating information using concision.\n* It consists of two parts:\n  1. A title ""External Annotations"" with no indication of its relevance or context.\n  2. A series of instructions for working with the document, including:\n      * Repeat the passage provided\n      * With concision\n      * Using the same headings and terms as given\n* No specific information about the content of the document is shared.",502,20.311394214630127
False,3,"195-Example Strangler Application.pdf,195-Example Strangler Application,""Chapter 14 Documenting Legacy Applications 410\nExternal Annotations\nSometim","195-Example Strangler Application.pdf,195-Example Strangler Application,""Chapter 14 Documenting Legacy Applications 410\nExternal Annotations\nSometimes we don’t want to touch a fragile system just to add some knowledge to it. \nIt is sometimes hard to touch and commit in large code bases just to add extra anno-tations. Y ou don’t want to risk introducing random regressions. Y ou don’t want to touch commit history . It may be so hard to build that you don’t want to build it unless absolutely necessary . Or your boss may not allow you to change the code at all “just for documentation.”\nIn such a situation, it is still possible to apply most techniques of living documen-\ntation, except that the internal means of documentation (for example, annotations, naming conventions) have to be replaced with an external document. For example, you might need a text file that maps package names to tags:\n1  acme.phenix.core = DomainModel FleetManagement\n2  acme.phenix.allocation = DomainModel Dispatching3  acme.phenix.spring = Infrastructure Dispatching4  ...\nWith such a document, it is possible to build tools that parse the source code and \nexploit these external annotations just as they would exploit the regular internal \nones (see Figure 14.5). \nThe issue with this approach is that it is an external kind of documentation, and \nhence it is fragile to changes in the legacy system. If you ever rename a package in the legacy system, for example, you have to update the related external annotations.\nBiodegradable Transformation\nDocumentation of a temporary process should disappear with it when it is done. Many legacy ambitions involve transformations from one state to another. Such a transformation might take years and may never really reach the end state. However, you need to explain this transformation to all teams, and you should show it as part of your living documentation.<<legacy>>Annotation\nRegistry@dto\n@dao\n@service\nFigure 14.5 Using a registry so that the code is not touched"",1940",2019,"**Legacy Application Documentation: External Annotations**\n\n**External Documentation**:\n- Not always possible to touch codebase for annotations or other internal documentation\n- Risk of introducing regressions or altering commit history\n- Boss may not allow changes for documentation only\n\n**Solution:**\n- Use external document instead\n- Map package names to tags in text file: `acme.phenix.core = DomainModel FleetManagement`\n- Parse source code and exploit these external annotations\n\n**Advantages**:\n- No need to modify the legacy system\n- Build tools that parse source code\n\n**Disadvantages:**\n- External documentation is fragile to changes in legacy system\n- Need to update external annotations when renaming packages\n\n**Biodegradable Transformation**:\n- Document temporary processes, which will disappear later\n- Show transformations as part of living documentation\n\n**Annotations**:\n- `@dao`, `@service`, `@registry` can be used for annotation references in external document.",983,26.93544626235962
False,3,"196-Enforced Legacy Rules.pdf,196-Enforced Legacy Rules,""Biodegradable Transformation 411\nExample: Strangler Application\nSay that you’re building a","196-Enforced Legacy Rules.pdf,196-Enforced Legacy Rules,""Biodegradable Transformation 411\nExample: Strangler Application\nSay that you’re building a strangler application that is expected to replace an older \napplication over time. This strangler application will probably live in its own bubble context. Y ou could just annotate this bubble context as being a strangler application. However, that it is playing the role of a strangler is a temporary fact and is not neces-sarily intrinsic to the new application; when it has successfully strangled the old one, it will just become the nominal application, and its annotation will be meaningless. The strangler application strategy is therefore a biodegradable transformation.\nIn the meantime, every developer needs to know to use the new strangler \napplication instead of the one being strangled. Therefore you’d need to add a \nStrangledBy(""""new bubble context application"""")  annotation to the stran-\ngled application to explain that a strangling is pending (see Figure 14.6). When it can be safely deleted, the annotation will go away with it.\n \nOf course, you could still tag the new application StranglerApplication , but \nyou will have to clean up this tag eventually , when you are done. And if the strangling never completes, this will by a hint pointing to the unfinished initiative.\nExample: Bankruptcy\nSome legacy applications are so fragile that they break any time you try to change them, and it takes weeks of work to stabilize them again. When you recognize this, you might decide to declare such applications officially “bankrupt,” which means nobody should ever change them.\nIn large legacy systems with new applications strangling older ones, you don’t \nwant to perform maintenance on two applications at the same time, so you can mark the older one as “frozen” or “bankrupt,” too. Y ou can mark the application as bank-rupt by using a number of means:\n •Use annotation on the package or an attribute in the AssemblyConfig  file\n •Use a BANKRUPTCY .txt file to explain what you need to know and to do (or avoid doing)@Strangled by (""""Butterfly"""")\nor\n@Deprecated (""""Strangled by Butterfly"""")Butterfly\nFigure 14.6 An application annotated as being strangled by another\nChapter 14 Documenting Legacy Applications 412\n •Remove the commit privilege from everyone, and if anyone tries to commit and \nasks why it is not possible, take the opportunity to explain that it is bankrupt\n •As a weaker alternative, monitor the commits and raise an alert when changes are met in a bankrupt application\nAgree on Maxims \nBig changes to legacy systems are made by a number of people who share common objectives; you can use maxims to share the vision, as advised in the book Object-Oriented Reengineering Patterns.\n4\nWhen you have a legacy transformation strategy , you need to make sure everyone \nknows about it. Y ou may have created a superimposed structure. Y ou may have anno-tated your bubble context in the code of the project. However, of all the things you need to share with everyone, there are a few key decisions you really want everyone to keep in mind at all times. Maxims are a powerful answer for such situations, and they have been around for ages.\nTherefore: Invent maxims to spread the most critical pieces of knowledge to \neveryone. Repeat these maxims often to advertise them. Make them rhyme to boost their effect.\nWhen your project is to rewrite only a portion of a large legacy system, and you \ndon’t want to rewrite more than what’s absolutely useful now (that is, the billing engine) and nothing else, you can use a maxim like this: “One work site at a time (the billing engine).” This has been one of my favorite maxims in a big legacy project. It was meant to remind everyone not to get distracted when working on the project; they needed to focus on the main worksite only .\n“When in Rome, do as the Romans do.” This was the counterpart to the single-work-site maxim. In other words, when you happen to walk outside the main work site, don’t innovate or change much; just do the minimum in the local style, even if you don’t like it. Be conservative when working in the legacy code that will not be rewritten.\n4. Demeyer, Serge, Stéphane Ducasse, and Oscar Nierstrasz, Object-Oriented Reengineering Patterns. \nSan Francisco: Morgan Kaufmann Publishers, 2003.\nEnforced Legacy Rules 413\nAnother legacy maxim that was proposed by Gilles Philippart is an extremely \npowerful one: “Don’t feed the monster! (Don’t improve the legacy Big Ball of Mud; it \nwould only make it live longer).”\nI’ve found maxims to be a valuable form of documentation. The point is to repeat \nthem often, whenever it makes sense to do so, ideally at least once a day . The maxim format is made to stick, and that is why you might want to give it a try . Maxims can also help share the conclusions of your team retrospectives, as agreed upon by the team.\nEnforced Legacy Rules\nLegacy transformations can last longer than the people doing them; automate the enforcement of the big decisions to protect them.\nSay that in a legacy application, you’ve decided that some method should not be \ncalled anymore, except from one specific place. For example, you might have decided to turn a read-write legacy application into a legacy read model, which should not accept any request to update it except from the listener responsible for syncing this read model from the other authoritative model. The design decision can be stated in a decision log:\nThis model is a read model. It is therefore read-only . Don’t call this Save method, unless \nyou are the listener that syncs this read model from the events sent from the authorita-tive write model.\nY ou might include with it the following rationale:\nThe legacy system has proven to be unmaintainable, so we don’t want to develop in it anymore. This is why we’re building another system as a replacement. But because so many external systems are integrated with it, we can simply remove it in one go. This is why we have decided to keep this old system just as a legacy read model for integration purposes.\nY ou could also document this directly in the code:\n •Mark the design decision with the custom annotation @LegacyReadModel  \nand include the message and the rationale\n •Mark the method as @Deprecated\nChapter 14 Documenting Legacy Applications 414\nHowever, being in a legacy system also means there are probably legacy teams \naround, some of them remote or in other departments, and you can never be sure \nthey will read your documentation or emails or that they will pay attention when you mention the design decision in the daily standup. And you know that if some devel-opers don’t respect the design decision, bad things will happen. Y ou’ll get bugs and pay the cost of extra accidental complexity due to the inconsistent data management strategies.\nMy colleague Igor Lovich came up with a simple way to document such a decision \nas an enforced guideline. Say that you express a design decision as follows:\nNever call this deprecated method unless you’re in the white-list of the one or two \nclasses responsible for the sync.\nThis is a custom design rule that can then be enforced at runtime with some addi-\ntional code:\n •Capture the stack trace in the method to find out who’s calling it and ensure \nthat it’s the allowed piece of code (for example, throw an exception within a \ntry-catch  and extract its stack trace in Java).\n •Check that at least one caller in the stack trace belongs to the white-list of allowed callers’ methods.\n •Make the check into a Java assert , if you want to fail-fast in some environ-\nments but not all of them.\n •Log when the check fails in a way that will trigger specific follow-up. (If it gets fired, then it’s actually a defect.)\nIn addition, you could turn the maxim “Don’t feed the monster! (Don’t improve \nthe legacy)” into an enforced legacy rule, too, by forbidding commits into a particu-lar area of the code base. Or you might raise a warning when a commit is done there. Such enforcement is simple and more effective than long explanations that people tend to miss or ignore.\nIn practice, legacy systems make everything more complicated than expected. It \ntakes courage and some creativity to come up with relatively “not too bad” solutions!"",8298",8367,"**Enforced Legacy Rules: Documenting Strategies for Legacy Applications**\n\n**Strangler Application:**\n- Biodegradable transformation\n- Annotation to indicate ""strangled by new application""\n  - e.g., `@StrangledBy(""new bubble context application"")` or `@Deprecated (""Stranged by Butterfly"")`\n- Explanation in documentation or comment\n\n**Bankrupt Legacy Applications:**\n- Declare officially ""bankrupt"" if fragile and unchanging\n- Prevent changes to avoid further complications\n- Options for marking:\n  - Annotation on package or assembly config file\n  - `BANKRUPTCY .txt` file with instructions\n  - Remove commit privileges\n  - Monitor commits and raise alert\n\n**Maxims:**\n- Share key decisions through maxims\n- Repeat often for effectiveness\n- Example: ""One work site at a time,"" ""Don't feed the monster!""\n\n**Enforced Legacy Rules:**\n- Document decisions as enforced guidelines\n- Use custom annotations and assertions to enforce rules\n  - e.g., `@LegacyReadModel`, `@Deprecated` with white-list of allowed callers\n- Log violations for follow-up actions.",1052,44.63348197937012
False,3,"197-Summary.pdf,197-Summary,""Summary 415\nSummary\nLegacy systems raise extensive challenges with respect to living documentation. \nThey come with a","197-Summary.pdf,197-Summary,""Summary 415\nSummary\nLegacy systems raise extensive challenges with respect to living documentation. \nThey come with a pessimistic perspective of the code and its knowledge, which is mostly there but is obfuscated as a fossilized knowledge, so you need ad hoc tech-niques to make it accessible again, like superimposed structure and highlighted structure. When the code is too fragile or can’t reasonably be changed, you must resort to external annotations.\nBecause a focus on legacy is often in the context of a legacy migration, it implies \nmajor changes, adding and removing entire sections—all these changes performed by many people over large time spans: this calls for biodegradable documentation means that go away with the deleted code. Then beyond the knowledge materialized in the artifact, you also need a way for people to act with some consistency , for exam-ple through shared maxims. And obviously , you also need a lot of courage!\nThis page intentionally left blank"",986",1020,"**Challenges with Legacy Systems:**\n* Difficulty in maintaining living documentation due to obfuscated knowledge within code\n* Ad hoc techniques required for accessing this knowledge: superimposed structure, highlighted structure\n* Inflexible or fragile systems may necessitate external annotations\n* Major changes during migration, involving additions and removals of sections\n* Biodegradable documentation needed to keep track of deleted code\n* Consistency needed through shared maxims for people to act\n* Courage required in handling legacy systems.",552,14.05486249923706
False,3,"198-Chapter 15 Extra Conspicuous Documentation.pdf,198-Chapter 15 Extra Conspicuous Documentation,,0","198-Chapter 15 Extra Conspicuous Documentation.pdf,198-Chapter 15 Extra Conspicuous Documentation,,0",100,**Extra Conspicuous Documentation**: Chapter 15 emphasizes the importance of clear and prominent record keeping.,112,8.129051446914673
False,3,"199-How Is Your Lemon.pdf,199-How Is Your Lemon,""417Chapter 15\nExtra: Conspicuous \nDocumentation\nThe most common myth of  communication is that it","199-How Is Your Lemon.pdf,199-How Is Your Lemon,""417Chapter 15\nExtra: Conspicuous \nDocumentation\nThe most common myth of  communication is that it happened.\n—@ixhd from T witter\nJust because you have a documentation mechanism in place does not guarantee that \npeople will notice it, remember the knowledge, or contribute to it. There are many techniques to get the message through with fewer words, more quickly , more accu-rately , without wasting people’s time, and in a fun way . Using such techniques can boost the efficiency of your living documentation by helping to infuse the living doc-umentation into your culture. This extra chapter enumerates some of these techniques.\nFocusing on Differences\nWhen describing a specific thing, such as a dog, we focus on its differences from the generic thing, such as a mammal. If the generic thing is well known or well described before the specific thing is mentioned, we can describe the specific thing with just a few points—one for each significant difference.\nThe important concept here is salience, defined as “a striking point or \n feature.”\n1 We primarily want to describe the salient points out of the mass of \ninformation.\n1. By Permission. From Merriam-Webster.com © 2019 by Merriam-Webster, Inc. https://www .merriam-\nwebster.com/dictionary/salience."",1274",1329,"**Techniques for Effective Communication of Living Documentation:**\n* Focus on salient points: describe differences from generic things by highlighting striking features or significant aspects.\n* Efficient communication: use techniques to get the message across with fewer words, more quickly, and accurately.\n* Boost efficiency: infuse living documentation into your culture in a fun way.\n\n**Challenges in Effective Documentation:**\n* Documentation mechanism in place does not guarantee effective communication\n* People may not notice, remember, or contribute to knowledge sharing\n\n**Salience:**\n* Defined as a striking point or feature\n* Important concept for describing specific things by focusing on differences.",716,25.29551410675049
False,3,"200-Segmenting by Known Audience.pdf,200-Segmenting by Known Audience,""Chapter 15 Extra: Conspicuous Documentation 418\nHow Is Your Lemon?\nDuring a t","200-Segmenting by Known Audience.pdf,200-Segmenting by Known Audience,""Chapter 15 Extra: Conspicuous Documentation 418\nHow Is Your Lemon?\nDuring a training session I’ve attended during Øredev 2013, Joe Rainsberger told a \nstory about lemons while discussing BDD . I don’t remember the exact story , but I do remember the key insight, so here’s my own totally distorted account of the story:\nThe trainer asked everyone to describe what a lemon’s like. The group described the \ntypical lemon shape, yellow color, acid taste, and grained texture of a lemon. The trainer then gave each attendee a real lemon and asked them to carefully study their lemons for a few minutes.\nThe trainer also analyzed his own lemon. One end of the lemon was bent in a weird \nway . There was a variation of color somewhere in the middle. The lemon was kind of small compared to the average lemon.\nHe then asked everyone to put their lemons back together into the basket, and then \nasked each person to recognize his or her lemon among all the other lemons. This was surprisingly easy! Each attendee realized that he or she had learned a particular lemon very intimately . “It’s my lemon!” they all said! They even felt a bit of attachment to their individual lemons.\nBy looking carefully at a specific lemon in contrast to the generic concept of a \nlemon that everybody knows, you can describe it very effectively . Y our description is \nat the same time precise with lots of details and efficient because you don’t have to describe the generic thing.\nI’ve seen colleagues use this technique to describe concepts from a business \ndomain. For example, during a presentation to new joiners on financial asset classes, the trainer was mentioning only the five to seven bullet points that were distinctive to a particular asset class, such as commodities, in contrast to a generic well-known asset class such as equities.\nIn the electricity market, a specificity is that the prices are very seasonal during the \nday and during the year, in contrast to company stocks. In the oil market, geography matters, as oil is not shipped just anywhere.\nTell Only What’s Unknown\nThere is no point in explaining something to people who already know it. The key is to identify what the audience knows. During conversations, it is possible to assert what the people you’re talking to already know or don’t know , based on their ques-tions, by watching their body language, and by asking them directly . In written form, this is more difficult, but it is not impossible. There are several ways to do it."",2508",2585,"**Story of the Lemon:**\n- During a training session at Öredev 2013, Joe Rainsberger told a story about lemons during a discussion on Behavior Driven Development (BDD)\n- The trainer asked attendees to describe what a lemon is like: shape, color, taste, texture\n- Each attendee was given their own real lemon and asked to study it for a few minutes\n- Trainer analyzed his own lemon, noting its unique characteristics: bent end, variation in color, smaller than average size\n- Attendees realized they had learned about their individual lemons intimately, feeling attachment to them\n- By focusing on specific details of an item compared to the generic concept, effective description results\n\n**Extra Information:**\n- Colleagues have used this technique to describe concepts in business domain by highlighting distinctive aspects\n- In electricity market, prices are very seasonal and in oil market, geography plays a role\n\n**Identifying Known Audience:**\n- Identify what audience knows or doesn't know during conversations through their questions and body language\n- Difficult to do in written form but not impossible\n- In written form, identify the specificity of unknown information for effective communication.",1208,29.02999472618103
False,3,"201-Flexible Content.pdf,201-Flexible Content,,0","201-Flexible Content.pdf,201-Flexible Content,,0",48,"**201-Flexible Content**:\n- Flexible content refers to content that can be easily modified or customized to suit different users or situations.\n- It is important in user interface (UI) design, as it helps ensure that the user's experience with the system is positive and relevant.\n\n**Providing Flexible Content**:\n- To provide flexible content, the system must have a well-defined set of rules for how to modify or customize the content.\n- This can be accomplished through the use of placeholders in the content that indicate where different types of information should go.\n- For example, a date picker UI element might have a placeholder that specifies the format (e.g., ""MM/DD/YYYY"") for the user to enter their date of birth.\n\n**Advantages of Flexible Content**:\n- Providing flexible content allows the system to better adapt to the specific needs and preferences of individual users, improving usability.\n- It also allows the system to provide relevant information based on the user's current context, such as displaying local weather information when they log in.\n\n**Constraints on Flexible Content**:\n- While flexible content is beneficial, it must be used with caution to avoid confusion or overwhelming the user.\n- The rules for modifying the content should be simple and easy to understand, and the default content should clearly indicate what type of information is expected.\n- The system should also provide a way for the user to easily revert any customizations they have made, such as providing an ""Undo"" button or clear ""Reset to Default"" option.",1560,31.269012689590454
False,3,"202-Low-Fidelity Content.pdf,202-Low-Fidelity Content,,0","202-Low-Fidelity Content.pdf,202-Low-Fidelity Content,,0",56,"**Low-Fidelity Content** (202)\n\n* Refer to the file ""Low-Fidelity Content"" for more information on this topic.\n* This concept relates to a specific type of content that may not be as polished or refined, but can still provide value.",232,9.834815979003906
False,3,"203-Together Now.pdf,203-Together Now,""Tell Only What’s Unknown 419\nSegmenting by Known Audience\nFor each audience, you can get feedback on the most","203-Together Now.pdf,203-Together Now,""Tell Only What’s Unknown 419\nSegmenting by Known Audience\nFor each audience, you can get feedback on the most frequently asked questions, and \nif there’s a support team, you can talk to them to better learn what’s well known and what needs to be explained more. Then you can focus on what’s unknown for each audience.\nFlexible Content\nY ou should organize written content so that it can be skimmed, skipped, and read partially . Y ou should also clearly mark optional sections and make titles informative enough so that readers can decide whether a section is what they are after.\nFor example, Martin Fowler suggests writing duplex books.\n2 The idea is to split \nthe content into two big parts, where the first part is a narrative designed to be read cover to cover, and the second part is reference material that is not meant to be read cover to cover. Y ou read the first part to get an overall understanding of the topic, and you keep the rest for when you actually to need it.\nLow-Fidelity Content\nUse low fidelity representation for output as long as you want people feel invited to add \ntheir input.\n—@kearnsey on Twitter\nT oo often a diagram that was meant for brainstorming, exploring, or proposing \nideas is misunderstood as a piece of specification. This results in premature feedback on details such as “I’d prefer another color,” even though the whole thing will change a lot in the next hours or days. This situation is especially true for everything done on a computer, since it is quick and easy to create nice-looking documents, pictures, and diagrams using the proper piece of software.\nTherefore: When the knowledge is still being shaped, make it clear in the docu-\nments by using low-fidelity content such as wireframes and sketchy rendering.\nVisual Facilitation\n“I’m talking about that” when pointing a finger at a box on a diagram on the white-board or on a screen (see Figure 15.1) is much more concise and precise than “I’m talking about this thing that takes care of filtering the duplicate entries upstream of \n2. Martin Fowler, ThoughtWorks blog, http://www .martinfowler.com/bliki/DuplexBook.html\nChapter 15 Extra: Conspicuous Documentation 420\nthe real-time secondary calculation engine.” As Rinat Abdulin said on T witter \nduring a conversation we had about living diagrams, “Stuff ‘you can point on to’ during discussions helps communicate faster and with better accuracy .” Having con-versations supported by visual media is a powerful technique.\n \nFigure 15.1 Communication is improved when you can point your finger at what you’re \ntalking about on a shared visual support\nDuring meetings or an open-space session, the visual notes on the flipchart not \nonly report on what has been said: they also influence further discussions just by \nbeing in front of everyone’s eyes. This influence is even stronger if the scribe who has the marker on the whiteboard is skilled in visual facilitation. He or she rearranges the way the information is organized, sorting concepts, using a meaningful layout, noting links, making side remarks, and drawing little decorations about the connota-tions involved in the discussions.\nTherefore: Don’t discount the importance of visual supports during discussions. \nInvest in some visual facilitation skills and learn how visual supports can help shape the dynamics of the work.\nVisual notes are redundant with has been said and therefore help if you did not \ncatch a word or an idea immediately . They help as a way to catch up and help eve-ryone remain involved in the session. When done well, visual facilitation is also an opportunity to make people smile, thanks to some visual humor.\nSearch-Friendly Documentation\nMaking information available is not enough. Y ou have to know where to find what you need when you need it, and it needs to be easily searchable.\nConcrete Examples, T ogether, Now 421\nBeing easily searchable is first of all a matter of using distinctive words.\n Distinctive Words\n“Go” as a name of a programming language, from a company like Google that is \ninto search, is not search friendly . A more search-friendly name is golang.  \nThe piece of knowledge should mention clearly the user needs it addresses, since \nthis is the question that will be searched for. T o help with this, keywords should be added, including words that don’t really occur in the actual content but that are likely to be used when someone is searching for it. It is helpful to use words from actual users, found from the analytics of failed searches, for example.\nRemember to mention synonyms, antonyms, mistranslations, and common con-\nfusions for improved discoverability by search.\nAll this is usually considered only for written text in a traditional document, but it \napplies just as well in the code, which is considered text, too. And you may even use annotations to add keywords.\nConcrete Examples, Together, Now\nMake sure to have every attendee agree on concrete examples when discussing specifications.\nThis probably sounds familiar:\nNow that we’re in agreement on this change, we can stop this meeting. Y ou will work \non test cases and the detailed design and screen mockup, and we’ll discuss them next week. In the meantime, feel free to ask if you have any questions.\nThe lost opportunity here is that everyone involved will most likely waste time \nafter the meeting. The collective agreement during the meeting is often an illusion. \nAs the saying goes: “The devil is in the details.” It’s only when starting to create the mockup for the new screen that the issues will really start to jump out. It’s only when trying to code the abstract requirement that the misinterpretation will happen, and it \nmay only be detected days or weeks later.\nA better approach is to reply on this type of unorthodox proposition:\nWhy about creating a concrete example together, right now , during this meeting?\nChapter 15 Extra: Conspicuous Documentation 422\nI often use a similar strategy:\nI believe we’re all in agreement on what needs to be done. But to be 100% sure, just in \ncase, we should take a few minutes to draft a concrete example all together right now .\nIt might sound like a waste of time to do this sort of thing right away . “We don’t \nhave time for the low-level details here” is an objection I sometimes hear. And it’s true \nthat it can be painful to observe your colleague slowly working the collage of buttons and panels on a screenshot in Microsoft PowerPoint on the big video screen. How-ever, at the same time, you’re saving much more time in decision making, because everybody is there to confirm, adjust, or raise an alarm—instantly .\nTherefore: Whenever there’s a workshop on specifications, make sure to have \nevery attendee agree on concrete examples during the meetings—right away during sessions. Resist the temptation to save time by doing it offline. Acknowl-edge that decision making is often the main bottleneck, not drafting concrete examples. Some of the resulting examples will be important parts of your documentation.\nIt does not matter if the examples are scenarios expressed in text, data tables, \nflipchart sketches, visual screen mockups in a tool projected on a big screen, or something else. What matters is that everyone involved understands the examples so that they can immediately notice whether something is wrong in them. For this purpose, it is essential that examples be concrete. Don’t settle on abstract agreement. Everybody may agree that “the discount is 50%” but how do you handle it when the price is $1.55? How do you take care of the rounding? Y ou need concrete examples to notice issues such as this.\nIn Practice\nY ou are likely to hear many common objections the first time you suggest creating concrete examples during the meeting. Concrete seems verbose and slow , whereas abstract seems concise and fast. This is true in the very short term, but in the longer term, in the context of specifications, it’s rather the opposite: Concrete is faster.\nIn fact, you may be painfully aware of this and might suggest doing the examples \noffline: “I don’t want to waste your time, so tell me how to do it, and I’ll do it later on my own.” Instead, however, consider using the following sentence: “Sorry you’ll have to wait for three minutes while I fire the tool, but then we know for sure we’re in agreement on the solution. This way we can avoid a ping-pong of emails and further meetings in the coming days and weeks.”\nConcrete Examples, T ogether, Now 423\nWhen it comes to specifications, where communication is particularly fallible, \nkeep in mind the following:\n• DON’T: “We can stop there to save time. I’ll go on alone, and then we’ll have \nanother meeting to discuss the results.”\n• DO: “Let’s go as far and as quickly as we can together so that we know quickly what the troubles are and where we may disagree.”\nFast Media and Prior Preparation\nIt helps to choose fast media, such as the following, to create consensus on concrete examples: \n• Flipchart or whiteboard and markers, written on carefully so that everybody can read\n• Big pages of paper on the table, with pens\n• People talking and a spokesperson taking verbatim notes, regularly restating the notes to other attendees\n• A simple text editor shown on a big screen \n• A screen mockup tool that you know well so that you’re quick with it \n• Microsoft PowerPoint to do a collage of preselected screenshots if you’re com-fortable with the solution\nIt helps to do some prior preparation and have some ready-made materials. I once \nhad a colleague bring a file folder full of screenshots from all the important screens and printed diagrams of the main workflows of the application to every meeting to improve communication during discussions. I’ve used a similar approach but elec-tronically , keeping a default PowerPoint full of screenshots and other stuff, just in case it is needed during specification workshops to answer questions or to be reused as part of a screen collage.\nThe same ideas can be generalized to other aspects of specifications, such as qual-\nity attributes. When discussing performance, latency , and fault tolerance require-ments, it would be a good idea to not only define the expectations but to go the extra mile and collectively agree on the acceptance criteria. The acceptance criteria should then become a test that will not only document but also ensure that the qual-ity attributes are actually met."",10503",10549,"**Conspicuous Documentation: Creating Effective Written Content for Technical Audiences**\n\n**Segmenting by Known Audience**:\n- Gather feedback from frequently asked questions\n- Communicate with support teams to learn what's well known and unknown\n- Focus on what's unknown for each audience\n\n**Flexible Content**:\n- Organize content to be skimmed, skipped, and partially read\n- Clearly mark optional sections\n- Use low-fidelity content during the content shaping stage\n\n**Low-Fidelity Content**:\n- Use diagrams and sketchy renderings for unformed ideas\n- Avoid premature feedback on details\n- Use low-fidelity content to encourage input from team members\n\n**Visual Facilitation**:\n- Use visual media during discussions to improve communication\n- Hire skilled scribes to create visual notes that influence further discussions\n- Use visual supports to help everyone remain involved and understand the session\n\n**Search-Friendly Documentation**:\n- Make information easily searchable by using distinctive words\n- Add keywords, including synonyms, antonyms, and common confusions\n- Ensure that written content is easily discoverable, both in text and code\n\n**Concrete Examples**:\n- Agree on concrete examples during meetings to avoid misinterpretations later\n- Use low-fidelity tools for creating examples, such as flipcharts or PowerPoint\n- Encourage fast media and prior preparation to create consensus on concrete examples",1420,45.87163043022156
False,3,"204-Affordable and Attractive.pdf,204-Affordable and Attractive,""Chapter 15 Extra: Conspicuous Documentation 424\nTogether, Now\nThe power of “togethe","204-Affordable and Attractive.pdf,204-Affordable and Attractive,""Chapter 15 Extra: Conspicuous Documentation 424\nTogether, Now\nThe power of “together, now” suggests going the extra mile after an agreement is \nmade until all attendees together consider and agree to a solution proven by concrete examples (for example, UI mockup, interaction workflow , impact map, scenarios of expected business behavior as text or sketches with accurate numbers on it).\nProductive specification meetings that really produce concrete examples are valu-\nable. They rely on face-to-face conversations for effective communication, and they produce quality documentation as an outcome.\nThe canonical example is, of course, specification workshops where the three ami-\ngos define the key scenarios. There are many similar examples of interactive collabo-rative creation of concrete results in the literature on agile software development:\n• Mob programming: All the brilliant minds together, on the same task, on the same machine\n• CRC Cards: A technique for instant, interactive, and collaborative modeling with CRC cards on a table (from Ward Cunningham and Kent Beck)\n• Modeling with stickers on the wall: Examples are model storming (from Scott Ambler)\n3 and event storming (from Alberto Brandolini)4 \n• Code analysis: Modeling directly in code in a programming language during a meeting with the domain expert (from Greg Y oung)\nStack Overflow Documentation\nSeveral times I have heard colleagues or even candidates say that Stack Overflow (SO) is by far the best place to go for documentation, and my experience tends to corrobo-rate this. Official documentation pages are often boring and seldom task oriented. The funny thing is that people answering on SO have often had to use the official documentation pages to build their own knowledge, together with trial and errors or even having to read the source code sometimes.\nPeople answer questions very quickly on SO. It’s another form of living docu-\nmentation: Contribute a question, and people all over the world quickly contribute answers, making the documentation a really living thing.\nTherefore: When the topic is popular enough, let SO provide good task-oriented \ndocumentation on top of the reference documentation you provided. Let your teams post questions on SO and let them answer other people’s questions as well.\n3. http://www .drdobbs.com/the-best-kept-secret/184415204\n4. https://www .eventstorming.com/"",2409",2480,"**Affordable and Attractive Documentation**\n* ""Together, now"": Go the extra mile after an agreement is made until all attendees consider and agree to a solution proven by concrete examples (UI mockups, interaction workflows, impact maps, scenarios of expected business behavior)\n* Productive specification meetings that produce concrete examples are valuable: rely on face-to-face conversations for effective communication and create quality documentation as an outcome\n* Examples of interactive collaborative creation of concrete results in agile software development:\n  * **Mob programming**: All brilliant minds together, on the same task, on the same machine\n  * CRC Cards: Instant, interactive, and collaborative modeling with CRC cards on a table\n  * Modeling with stickers on the wall: Examples are model storming and event storming\n  * Code analysis: Modeling directly in code during a meeting with the domain expert\n* Stack Overflow Documentation:\n  * Officially documented pages can be boring and seldom task-oriented\n  * People answering questions quickly on SO provide good, task-oriented documentation\n  * Encourage teams to post questions and answer others' questions as well",1189,29.00883984565735
False,3,"205-Easter Eggs and Fun Anecdotes.pdf,205-Easter Eggs and Fun Anecdotes,""Affordable and Attractive 425\nPosting on SO requires your project to be publ","205-Easter Eggs and Fun Anecdotes.pdf,205-Easter Eggs and Fun Anecdotes,""Affordable and Attractive 425\nPosting on SO requires your project to be published online, usually with its source \ncode. It especially requires the project to be successful, with enough demand to \nattract contributors.\nOr you can keep your project internal and closed source and use equivalent on-\npremises Stack Overflow clones.5 However, a domestic Stack Overflow clone will \nprobably not have the scale to work as efficiently as the true worldwide site.\nOne downside with Stack Overflow is that a product that is awful will be seen as \nawful. However, you can’t prevent that from happening on the web unless you make the product better, of course. Y ou may also dedicate many employees to answer questions in a positive way to improve the user experience.\nAffordable and Attractive\nWe can make information available, but we cannot make people care for it. Journalism \nas a solution?\n—Romeu Moura\nT o paraphrase my Arolla colleague Romeu Moura, documentation should be attrac-\ntive for the same reason flowers are attractive: for self-preservation. \nSpecs Digest\nI once saw a project where the team decided to curate all the accumulation of design and specifications documents into a much shorter (about 10 pages long) “specs digest” document. This was mostly done by copying and pasting the best parts out of various existing documents, and it was updated, fixed, and supplemented with the obviously missing bits in the process. This digest was a highly valued document for the team.\nA specs digest is strongly organized into sections, each typically a half page long, \nwith clear titles recapped in a table of contents. The structure allows for skipping any section safely to jump directly to the part of interest.\nThe content mostly focuses on everything that is not obvious: business calcula-\ntions (dates, eligibility , financial and risk calculations), principles, and rules. But it may also describe key semi-technical aspects such as the versioning scheme between multiple related concepts.\nNote that if you already have living documentation based on scenarios in a tool \nsuch as Cucumber, you should move all this content into the feature files themselves or into sidecar “preamble” Markdown files in the same folders.\n5. StackExchange, http://meta.stackexchange.com/questions/2267/stack-exchange-clones"",2337",2416,"**Stack Overflow and Documentation**\n\n**Requirements:**\n- Project publication with source code online\n- Success and demand for contributions\n- Internal or closed source alternatives\n- Domestic Stack Overflow clones may not be as efficient\n- Downside: poor projects will be criticized, but cannot prevent it\n- Dedicate employees to answer questions positively to improve user experience\n\n**Documentation:**\n- Romeu Moura's perspective on documentation\n- Information should be attractive for self-preservation\n- ""Specs Digest"" approach\n  - Curation of design and specifications documents\n  - Organized into sections with clear titles\n  - Focuses on non-obvious information: business calculations, principles, rules, versioning schemes\n  - Content can be moved to feature files or preamble Markdown in living documentation tools like Cucumber.\n\n**Additional Notes:**\n- StackExchange for Stack Overflow clones\n- Specs Digest document is a solution to presenting essential information in an organized and easily accessible format, making it valuable for the team.",1058,24.975385904312134
False,3,"206-Promoting News.pdf,206-Promoting News,,0","206-Promoting News.pdf,206-Promoting News,,0",44,**206-Promoting News:**\n- Promote news effectively\n- Repeat passage for concise understanding:\n  - Effectively promote news\n  - Understand passage for concise results\n\n**Important Keywords:**\n- Promote news\n- Concision\n- Understand passage\n- Repeat\n\n**Related Topics:**\n- Communication strategies\n- Public relations\n- Information dissemination\n- Effective messaging.,366,10.543254375457764
False,3,"207-Maxims.pdf,207-Maxims,""Chapter 15 Extra: Conspicuous Documentation 426\nEaster Eggs and Fun Anecdotes\nHaving fun is the best way to learn. Y ou c","207-Maxims.pdf,207-Maxims,""Chapter 15 Extra: Conspicuous Documentation 426\nEaster Eggs and Fun Anecdotes\nHaving fun is the best way to learn. Y ou can make any kind of documentation more \nengaging by hiding anecdotes from the project, its sponsors, and team members in the document as a way to encourage reading. Add simple illustrations.\nAs Peter Hilton mentions in his talk on documentation avoidance:\nUse humor. There’s no rule that says that jokes aren’t allowed. Insufficiently serious \ndocumentation is probably not your biggest problem. Staying awake might be.6\nPromoting News\nAdding knowledge somewhere is not enough for its audience to notice and use it. \nProvide ways to promote the documentation, especially when it changes:\n• A “Recent Changes” page should provide the bare minimum.\n• A changes feed (using Swagger, for example) can be pushed to Slack.\n• Slackbot custom replies can remind you where a document is in response to keywords.\n• Release notes might not be necessary . Determine whether you really read them.\n• When you’re really serious about knowledge sharing, hire a real professional journalist to work on-site.\nUnorthodox Media\nThe corporate world tends to be unoriginal. When it comes to documentation, the traditional media remain mighty email, Microsoft Office with the boring mandatory templates, SharePoint, and all the various enterprise tools that are notorious for their outstanding user experience. But life does not have to be so dull. Shake up your team or department by using unexpected, unorthodox media for communication and documentation.\nThe following sections provide various ideas to use as inspiration to spice up your \ncommunication in general and to share knowledge and objectives.\n6. Peter Hilton, “Documentation Avoidance for Developers,” https://www .slideshare.net/pirhilton/\ndocumentation-avoidance"",1845",1878,"**Documentation Best Practices: Unconventional Approaches**\n\n**Engaging Documentation:**\n- Hide anecdotes in documents for engagement\n- Add illustrations to make it more interesting\n- Use humor: no rules against jokes\n- Stay awake and keep documentation fun\n\n**Promoting Documentation:**\n- Provide recent changes page\n- Use a changes feed (e.g., Swagger)\n- Utilize Slackbot custom replies\n- Release notes may not be necessary, assess their usefulness\n- Hire a professional journalist for knowledge sharing\n\n**Unorthodox Media:**\n- Corporate world often lacks creativity in documentation methods\n- Use unexpected media to communicate and document\n- Examples: Microsoft Office templates, SharePoint, enterprise tools are common but dull\n- Shake up team/department with unconventional approaches. (Peter Hilton)\n\n**Documentation Avoidance:**\n- Humor can make documentation more engaging\n- Staying awake is essential to effective documentation\n- Traditional media in corporate world may be uninspiring.",998,25.57303547859192
False,3,"208-Comics.pdf,208-Comics,""Unorthodox Media 427\nMaxims\nWhen your current initiative is to improve the code quality come up with a catchy \nmaxim, su","208-Comics.pdf,208-Comics,""Unorthodox Media 427\nMaxims\nWhen your current initiative is to improve the code quality come up with a catchy \nmaxim, such as “Fix a bug? Add a test.” or “Fight legacy code. Write unit tests.”\nDon’t necessarily copy and paste maxims that others have used. Create your own \nthat will stick in your culture. The only way to know if a maxim will stick is to say it out loud on different occasions to see if it resonates and if anyone mentions it later.\n Note\nY ou might want to read the book Made to Stick: Why Some Ideas Survive and Others Die by Chip Heath and Dan Heath to learn more on this topic.\n \nGood maxims are useful and amusing at the same time. This one is from \n@BeRewt on T witter: “If in doubt, do it the way Erlang does it.”\nOnce you have a maxim, your job is to repeat it as often as possible (without \nbecoming a spammer, of course).\n Tip\nRepetition also works inside a maxim. For example, “Mutable state kills. Kill mutable state!” has internal repetition that can help make it more memorable.\n \nA maxim has to remain trivially simple because complicated stuff does not scale \nover larger audiences. Y ou need to broadcast your maxims, so be ready to trade nuance for stickiness. Y ou can only give one or two key messages. Make sure they are the most important messages. Take care of the other less important messages in a different way .\n Note\nStatements that rhyme are more believable than those that don’t. This is referred to as the rhyme-as-reason effect or the Eaton-Rosen phenomenon.\n \nPosters and Domestic Ads\nThink of your communication as a marketing campaign. Y ou can use the same tools but internally . \nChapter 15 Extra: Conspicuous Documentation 428\nWhen you have maxims, you can turn them into illustrated posters. The first thing \nyou can do is start with an image search. For example, consider the maxim “The \nonly way to go fast is to go well!” This maxim exhibits internal repetition and sym-metry around the word go , which makes it sticky .\nA Google image search turns up a ready-made meme with this exact maxim text \nover a picture of Uncle Bob, which is not surprising considering that he likes to repeat this maxim (Figure 15.2 gives a taste of this meme reinterpreted as cute monsters).\n \nTHE ONLY WAY TO GO \nFAST IS\nTO GO WELL!\nFigure 15.2 Robert C. Martin: The only way to go fast is to go well!\nMeme-Based Posters\nNow consider that you don’t have a maxim yet, but you’d like everyone to remember to close the door of the bathroom after use. Y ou can easily create a motivational poster for that, thanks to all the available free online meme generators. From a given idea, you can browse the most common memes until you find one that fits your mes-sage best. Here we found a “Mr. T” meme (see Figure 15.3, again re-interpreted as a cute monster, because hey , it’s really cute). (This example is a real one I’ve seen at a customer site. The poster was awesome on the bathroom door.)\n \nUnorthodox Media 429\nARE YOU AWESOME?\nCLOSE THE DOOR ONCE YOU'RE\nDONE.\nFigure 15.3 Are you awesome? Close the door once you’re done.\nOne drawback of memes is that they tend to become annoying when used too \nfrequently .\n Pro Tip\nDisplay a cute kitten along with or between your messages. Everybody loves cute \nkittens.  \nInformation Radiators\nPosters don’t necessarily have to be printed and pinned on walls or windows to be visible. Some companies have TV sets on the walls or in elevators and show on them a variety of slides for internal communication. This is a nice place for your posters.\nChapter 15 Extra: Conspicuous Documentation 430\nThe downside is that you have to go through an acceptance process, and you \nmight be rejected.\nStill, you can insert your posters as banners into your build walls, screen savers, or \npair-programming blocker screens.\nHumor and Cheap Media\nY ou might have seen already the very cheap, yet quite efficient, poster shown in \n Figure 15.4. \nFigure 15.4 An effective story\nStorytelling is very powerful, even when the story is this short. It takes training \nor pure luck to author this kind of gem. Fortunately , you can reuse (steal or hijack) many existing such gems for your own purposes. T witter is a great source of very short, and often funny , stories to plagiarize. But keep in mind that having everyone doing it does not mean it’s legal.\nMaxims can be short enough to fit within a hashtag. Our software industry \nalso loves using hashtags as a way to name new practices; consider #NoEstimates, #BeyondBudgeting, and #NoProject.\nNote that hashtags are not just for T witter and Facebook. Y ou can use them IRL \n(in real life), and even verbally , which sounds deliciously awkward.\nUnorthodox Media 431\n Tip\nName your Wi-Fi network after a maxim. For example, if you’d like to encour-\nage environmentally friendly behavior, you could rename your Wi-Fi network ReduceReuseRecycle.\n \nGoodies/Swag\nGoodies (for example, t-shirts, card decks, cheat sheets, large takeaway posters, mugs, pens, postcards, stickers, sweets, relaxation widgets) are not always green, but sometimes they are useful. Goodies have traditionally been used to repeat a message. Y ou might consider printing a maxim, rather than your brand, on goodies.\nThe conference DDD Europe recently did a great job of this with different t-shirt \ndesigns with different maxims such as\n• MAKE THE IMPLICIT EXPLICIT (see Figure 15.5)\n• THROW AW AY THE MODEL \nFigure 15.5 MAKE THE IMPLICIT EXPLICIT t-shirt"",5513",5546,"**Maxims and Unorthodox Media**\n\n**Creating Effective Maxims**:\n- Create your own maxims that resonate with your culture\n- Repeat them often to make them stick\n- Use internal repetition and symmetry for memorable maxim examples\n\n**Posters and Documentation**:\n- Illustrated posters can help reinforce effective maxims\n- Use Google image search or meme generators to create meme-based posters\n- Display posters in visible areas like walls, TVs, or screensavers\n\n**Humor and Cheap Media**:\n- Short stories and humor are powerful for internal communication\n- Use hashtags as a way to name new practices within the software industry\n- Name your Wi-Fi network after a maxim to encourage environmentally friendly behavior\n\n**Goodies/Swag**:\n- Goodies like t-shirts, posters, and stickers can be effective for reinforcing maxims\n- Consider using goodies as swag with your brand or a specific maxim on them.",899,29.457534551620483
False,3,"209-Infodecks.pdf,209-Infodecks,,0","209-Infodecks.pdf,209-Infodecks,,0",34,"**Creating Content: Infodecks**\n\n**Purpose**: N/A\n\n**Instructions**: Generate concise bulleted notes summarizing the provided text, with headings and terms in bold.\n\n**Infodecks:**\n- Creating content using infodecks: N/A\n- Instructions for generating concise bulleted notes:\n  * Repeat the provided passage\n  * Use headings and bolded terms\n\n**Provided Text**:\n```209-Infodecks.pdf,209-Infodecks,,0```\n\n**Concise Bulleted Notes:**\n\n**Creating Content: Infodecks**\n- Utilize infodecks to create content\n  - Repeat provided text\n    ***209-Infodecks.pdf, 209-Infodecks, 0***\n- Generate concise bulleted notes\n  * Use headings and bolded terms\n  * Follow instructions carefully for best results.",692,22.902667760849
False,3,"210-LEGO Bricks.pdf,210-LEGO Bricks,""Chapter 15 Extra: Conspicuous Documentation 432\nComics\nY ou can use comics as a compelling way to tell a story","210-LEGO Bricks.pdf,210-LEGO Bricks,""Chapter 15 Extra: Conspicuous Documentation 432\nComics\nY ou can use comics as a compelling way to tell a story , such as a story of frustrated \nusers who dream of better software. Y ou might use comics to document and explain the rationale for a new project.\nStories of users doing their job and sharing their most important stakes are also \ngreat for explaining—and documenting—in an accessible way the fundamental busi-ness stakes of a business activity .\nI once used childish comics in corporate environments to explain a process for the \ndevelopment team. Another time I used less childish comics to help explain a govern-ance process to senior management in a big, serious bank. In both cases, it worked and was appreciated.\nSeveral online comics generators can help you create basic comics from  libraries \nof characters, settings, and effects. They make it possible to anyone to create a comic, even without any drawing skill.\nInfodecks\nInfodecks are slides used as documents to be read onscreen rather than projected in front of an audience. Infodecks offer many advantages:\n• Y ou can use spatial layout to help with your explanation.\n• They are approachable and do not use long prose that people won’t read.\n• It’s easy to include diagrams as primary elements in the communication.\nThe important thing is not to confuse infodecks with slide decks that are meant \nto be projected to a large audience. An infodeck should contain very little text. The text it does include should be in a very big font size, and there should be many illustrations.\nMartin Fowler says, “Infodecks are an interesting form to me, if only because it \nseems nobody takes them seriously .…A colorful, diagram-heavy approach that uses lots of virtual pages is an effective form of document, especially as tablets become more prevalent.”\n7\nVisualizations and Animations\nAnimations and animated visualizations, which are a bit more difficult to produce than some of the other options listed here, are perfect for explaining temporal behaviors.\n7. Martin Fowler, ThoughtWorks blog, http://martinfowler.com/bliki/Infodeck.html"",2127",2170,"**LEGO Bricks as Documentation:**\n- **Comics**: can be used to tell a story or explain the rationale for a new project\n- Documenting user stories is great for illustrating business stakes of an activity\n- Use childish or less childish comics in corporate environments to explain processes\n- Online comic generators allow anyone to create comics without drawing skills\n\n**Infodecks**:\n- Slides used as documents, read onscreen instead of projected\n- Advantages: spatial layout for explanation, approachable with minimal text, easy to include diagrams\n- Not to be confused with slide decks meant for large audiences\n- Martin Fowler advocates for colorful infodecks with diagrams and tables as they are effective and taken seriously.\n\n**Visualizations and Animations:**\n- Perfect for explaining temporal behaviors\n- Difficult to produce but effective in illustrating concepts.",873,20.747410774230957
False,3,"211-Furniture.pdf,211-Furniture,,0","211-Furniture.pdf,211-Furniture,,0",34,"**Furniture:**\n* Discuss the topic in a concise manner, focusing on essential points only.\n* Provide information about furniture, its purpose and usage.\n\n**Information about Furniture:**\n* Furniture refers to movable items designed for use on or near a bed or table.\n* It serves various functions such as providing storage, supporting objects, enhancing decor, and creating a comfortable living environment.\n\n**Discuss Furniture in a Concise Manner:**\nFurniture is an essential component of interior design that serves multiple purposes like storing items, supporting objects, adding to the overall aesthetic appeal, and creating a comfortable living space.",657,14.44342565536499
False,3,"212-3D Printed Stuff.pdf,212-3D Printed Stuff,,0","212-3D Printed Stuff.pdf,212-3D Printed Stuff,,0",48,"**3D Printed Items**\n\n**Topic**: 3D Printed Items\n\n**Notes Summary**:\n- **No heading**\n- Discuss 3D printed items\n\n**Expanded Notes**:\n- **Concision**: Repeat the provided passage, with headings and terms in bold.\n\n**3D Printing**\n- Discuss various aspects of 3D printing\n\n**Items**\n- Focus on 3D printed items\n\n**212-3D Printed Stuff**\n- No heading\n- Mention ""3D Printed Stuff"" as a descriptive term for the topic\n\n**Notes with Headings and Terms in Bold**:\n**Topic**: **Discussing 3D Printed Items**\n\n**Subtopics**:\n- **3D Printing**: Discuss various aspects of 3D printing (optional)\n- **Items**: Focus on 3D printed items specifically\n\n**212-3D Printed Stuff**:\n- No heading, but can be used as a descriptive term for the topic.",732,24.14193081855774
False,3,"213-Summary.pdf,213-Summary,""Summary 433\nA great example is the beautiful visualization of distributed consensus in Raft, \nwhich shows how nodes ele","213-Summary.pdf,213-Summary,""Summary 433\nA great example is the beautiful visualization of distributed consensus in Raft, \nwhich shows how nodes elect their leader in the face of various events.8 Another per-\nsonal favorite is the apparently crazy idea of showing how sort algorithms work by \nusing sound along with a crude display .9 \nLEGO Bricks\nLEGO bricks have become popular in agile circles over the past years, and now we often use LEGO bricks during meetings, as a planning tool, or even to represent a software architecture physically in 3D . Other system of avatars or construction blocks can be used as mediation tools during conversations. The problem with these con-structions, however, is that people tend to forget what they meant after a few days.\nFurniture\nEven your furniture can tell stories. Fred Georges explained in one of his talks that tables express the internal organization of a startup: Each table represents one pro-ject team. No more room on the table means the team has reached its maximum size. If a table isn’t full, you’re welcome to join the team if you feel like it: It’s a direct proposition!\nFurthermore, you can tell from the huge iMac screens where the designers are, \nwhereas Linux machines more likely suggest developers’ workspaces.\n3D Printed Stuff\n3D printed models are now easy to produce. Y ou can, if you like, project a particular view of your application and print it in a solid material. This helps everyone use their visual and world-sensing strength to grasp visually and by touching the elements. 3D and removable layers are useful for representing several dimensions of a problem stacked on each other and well aligned.\nSummary\nHaving documentation doesn’t mean it’s useful. Keeping in mind some principles and ideas to make documentation more compelling—such as a focus on the audi-ence, discoverability , and the fun factor—can help you optimize the effectiveness of your living documentation initiatives.\n8. The Secret Lives of Data, http://thesecretlivesofdata.com/raft\n9. “15 Sorting Algorithms in 6 Minutes,” http://m.youtube.com/watch?v=kPRA0W1kECg\nThis page intentionally left blank"",2133",2168,"**Lego Bricks and Visualization Tools**\n- LEGO bricks: popular in agile circles for meetings, planning, and software architecture representation\n- Other system of avatars or construction blocks can be used as meditation tools during conversations\n- However, people may forget what they meant after a few days\n\n**Furniture**\n- Tables represent internal organization of a startup: each table = project team, no more room = max size\n- Furniture can tell stories about a company's structure and teams\n- Linux machines suggest developers' workspaces; huge iMac screens indicate where designers are\n\n**3D Printed Models**\n- 3D printed models easy to produce for application representation\n- Helps everyone use visual and world-sensing strength to grasp problems in several dimensions\n- Layers can be made removable and aligned for effective problem solving.\n\n**Documentation Principles**\n- Having documentation doesn't mean it's useful\n- Focus on audience, discoverability, and fun factor can optimize living documentation effectiveness.\n\n**Additional Information**\n- The Secret Lives of Data: Raft visualization of distributed consensus https://thesecretlivesofdata.com/raft\n- 15 Sorting Algorithms in 6 Minutes video: https://m.youtube.com/watch?v=kPRA0W1kECg",1255,31.721171617507935
False,3,"214-Index.pdf,214-Index,,0","214-Index.pdf,214-Index,,0",26,"**Index: A Note on Concision**\n\n**Passage:** 214-Index.pdf,214-Index,,0\n\n**Notes**:\n- The passage is a string of words without clear meaning or context.\n- No information can be extracted from this passage, making it non-functional.\n- To make the passage functional and meaningful, we need to add relevant context and organize the elements in a way that convey information efficiently.\n\n**Concise Version:** N/A - The original passage does not contain any significant details or meaning.",486,15.266422986984253
False,3,"215-C.pdf,215-C,""435Index\nNumbers\n3D printed models, 433\n4","1 model, 35897 Things Every Software Architect \nShould Know (High), 122\nA\nAbdullin, Rinat, 174, 419–420Abelson, Harold, 218abundance of knowledge, 24abuse of living documentation, 329–330accessibility of knowledge, 22, 24accounts of past events, 33accuracy\nensuring, 30–31\nhuman dedication, 32redundant sources with propagation \nmechanism, 32\nredundant sources with reconciliation \nmechanism, 32\nsingle source publishing, 31\npast events, accounts of, 33single-use knowledge and, 32–33\nad hoc documents, 124ADRs (architectural decision records), 350–354Adzic, Gojko, 51, 288affordances, 309aggregates, 207Agile Documentation (Rüping), 356Agile Manifesto, 11, 199Agile Software Development (Cockburn), 270Agile T esting (Crispin and Gregory), 251, 272Analysis Patterns (Fowler), 49, 263anecdotes, 426Angular.js commit guidelines\nchange logs, 132–133machine-accessible information, 132–133scope of change, specifying, 131–132type of change, specifying, 130–131\nanimation, 432–433AnnotationDoclet, 168annotations\nannotation by default, 115AOP (aspect-oriented programming) \nexample, 114–115\nin architecture documentation, 363–365custom\nadvantages of, 109package names, 112properties, 110standard annotations, extending, 113–114stereotypes and tactical patterns, 110–112\nembedded learning, 100–102examples of, 99–100Google Annotations Gallery , 103–104JPA (Java Persistence API), 113–114in legacy application documentation, 405, \n408–409, 410\nin living glossaries, 160living services diagram example, 200–202machine-accessible documentation, 119–120meta-annotations, 163objections to, 384overview of, 97–99rationales behind decisions, 100, 124Spring Framework, 113–114, 201strong coupling, 163\nAnsible, 289, 293AOP (aspect-oriented programming), 114–115Apache Ivy , 291Apache Maven, 289, 291APIs, error-proof, 308–309Apprenticeship Patterns (Hoover and Oshineye), \n332, 374\narbitrary names, 253archeology , software, 403–404architectural decision records (ADRs), 350–354architecture documentation, 370\narchitecture codex, 359–362architecture erosion, 367architecture landscape, 356–358change-friendly , 350decision logs, 350–354, 383–384definition of, 343–344\nIndex 436\narchitecture documentation (continued)\ndiagrams and notations in, 358–359\nhexagonal architecture living diagram, \n179–184\njournals/blogs, 355modules, 355problem briefs, 344–346quality attributes, 346–347\nexplicit, 346–347at runtime in production, 369as scenarios, 368–369tables of content, 370\nsmall-scale simulations\nadvantages of, 370–372building, 374characteristics of, 372techniques to achieve, 373\nstake-driven\nbrevity of, 349explicit assumptions, 349overview of, 347–349\nsummary of, 45–46system metaphors\nmetaphors in other metaphors, 376value of, 375\ntest-driven architecture\nother quality attributes, 370overview of, 367–368quality attributes as scenarios, 368–369quality attributes at runtime in \nproduction, 369\ntables of content, 370\ntransparent architecture\nannotations, 363–365architecture reality checks, 366–367enforced guidelines, 365–366overview of, 362–363\narchitecture erosion, 367architecture landscape, 356–358architecture reality checks, 366–367ArchUnit, 365Arrange-Act-Assert convention, 220–221AsciiDoc, 177@Aspect annotation, 114aspect-oriented programming (AOP), 114–115AssertJ, 234asset capture, 408associations, types and, 227Associative property , 110attributes, quality\nexplicit, 346–347other quality attributes, 370at runtime in production, 369tables of content, 370\naudiences, targeted, 66auditability , 294augmented code. See knowledge augmentationautomated deployment, declarative, 296–297automating documents\nchallenges with, 196–197declarative automation\ndeclarative automated deployment, \n296–297\ndeclarative configuration management, \n293–296\ndeclarative dependency management, \n291–293\ndeclarative style, 290machine documentation, 299overview of, 287–289scaffolding, 297–299\ninformation curation, 162–164living diagrams\nbusiness overview case study , 184–191context diagram example, 192–196conversations and, 171hexagonal architecture example, \n179–184\noverview of, 170–171rendering, 175–178scale of diagrams, 174–175storing code for, 173–174tools for, 177–178visualization guidelines, 178–179what to include, 171–174\nliving glossaries\nadvantages of, 158–159bounded contexts, 164–165case study , 165–170example of, 160–162how they work, 160\noverview of, 48, 158patterns in, 196–197presentation rules, 158as reconciliation mechanism, 290summary of, 43\navoidance of documentation. See documentation \navoidance\nB\nBalmer, Dave, 327batch systems, introducing living \ndocumentation in\nIndex 437\nbusiness behavior, 388\ncontact information, 390guided tours, 390journey tests, 390living diagrams, 389living glossary , 389overview of, 386–387README files, 387ready-made documentation, 387single source of truth, 388–389visible workings, 388–389\nBDD (behavior-driven development)\nas canonical case of living documentation, \n65–66\nconversations in, 55–56feature files\ndefinition of, 58example of, 63–65glossaries, 69intent of, 58links to nonfunctional knowledge, 70organization of, 61property-based testing, 66–68scenarios, 58–59, 61–63specification details, 59–60tags in, 60–61\nproperty-based testing, 66–68reconciliation in, 56–58redundancy in, 56–58\nBeck, Kent, 49, 218, 223behavior-driven development. See BDD \n(behavior-driven development)\nbibliographies, project, 126–127binary annotation, 201biodegradable documentation, 331–332biodegradable transformation, 410\nbankruptcy application example, 411–412strangler application example, 411\nblogs, 124, 351–354bookmarked searches, 257–258Bottcher, Evan, 362bounded contexts, 164–165, 408Bower, 291brain dumps, 8–9, 351–354Brandolini, Alberto, 275–276breakable toys, 374brevity , 349broken link checkers, 255–256Brooks, Fred, 317Brown, Simon, 177, 193, 358–359bubble context, 404–406BugMagnet, 288Building Microservices (Newman), 125, 308, 362business domains, 346business overview living diagram\nbackground, 184–185changes, accommodation of, 190code augmentation, 186–188existing source code, 186living diagram generation, 188–189living documentation patterns and, 191quality attributes, addition of, 190–191\nbusiness pipelines, 407\nC\nC#\nannotations, 160sightseeing map in, 146–148\nC4 model, 193, 358–359CamelCase, 221CfEngine, 293change logs, 132–133change management, 398–399change-friendly documentation, 350Chaos Gorilla, 369Chaos Monkey , 369Chassaing, Jeremie, 207cheap media, 430–431cheap upfront documentation, 36Chef, 289, 293Christopher, Martin, 29ClassPath class, 178clean code. See code as documentationClean Code (Martin), 218, 224Clojure, 121Cloud Sleuth, 201Cockburn, Alistair, 246, 268, 270, 374code as documentation, 218\ncode review , 321–322code searching, 238–239code-driven diagrams, 175coding against a framework, 224coding conventions, 221–223naming conventions\narbitrary versus descriptive names, 253contextual nature of, 224evergreen content, 252–253importance of, 223–224\nshameful documentation, 325–326text layout\nArrange-Act-Assert convention, 220–221tabular code layout, 219UML visual notation, 219\nIndex 438\nCodeKata, 312\ncodex, architecture, 359–362coffee machine communication, 277–280CoffeeScript, 121coincidence, programming by\ndeliberate decision making, 319–321documentation as code review , 321–322overview of, 317–318\ncollaboration\nadvantages of, 272–273best practices, 21–22continuous documentation, 276–277cross programming, 274Event Storming, 275–276mob programming, 274–275pair programming, 273–274“three amigos” concept, 275truck factor, 277\ncollective ownership, 22collective work\nadvantages of, 272–273continuous documentation, 276–277cross programming, 274Event Storming, 275–276mob programming, 274–275pair programming, 273–274“three amigos” concept, 275truck factor, 277\ncomics, 432comments\nadvantages of types over, 228shameful documentation, 325–326tags in, 102–103\ncommit messages\nadvantages of, 128–130commit guidelines, 129–133\nmachine-accessible information, 132–133scope of change, specifying, 131–132type of change, specifying, 130–131\ncompany services catalogs, 93compliance requirements\nITIL case study\noverview of, 397–398release management, 399request for change management, 398–399\nmeeting/exceeding, 395–397\ncomposed method, 230–232concrete examples, 421–423\nfast media, 423interactive collaborative creation, 424prior preparation, 423\nConcurrent V ersions System (CVS), 108configuration management, declarative, 293–296configuration servers, 93consistency\ndesigning for, 310–311tests of, 76–78\nconsole history , 93consolidation\ncontext diagram example, 194–195how it works, 81–82implementation considerations, 82–83need for, 80–81\nconspicuous documentation\nconcrete examples, 421–423\nfast media, 423interactive collaborative creation, 424prior preparation, 423\nEaster eggs and fun anecdotes, 426promotion of, 426specs digests, 425Stack Overflow documentation, 424–425unknowns, focus on\nflexible content, 419low-fidelity content, 419salient points, description of, 417–418search-friendly documentation, 420–421segmenting by known audience, 419visual facilitation, 419–420\nunorthodox media, 426\n3D printed models, 433comics, 432domestic ads, 427–429furniture, 433goodies/swag, 431humor and cheap media, 430–431infodecks, 432information radiators, 429–430Lego blocks, 433maxims, 427memes, 428–429posters, 427–429visualizations and animation, 432–433\nconstrained behavior, 307. See also enforced \nguidelines\nerror-proof APIs, 308–309tailored service templates, 307–308\ncontext diagrams\naugmented code, 194–195background, 192–193benefits and limitations of, 195–196hyperlinks to source code locations, 193–194knowledge consolidation, 194–195\ncontexts, bounded, 164–165, 408"",9845",9806,"**Software Architecture Documentation**\n\n**Background:**\n- Importance of software architecture documentation (343–359)\n- Architectural decision records (ADRs), 350–354\n- Agile Documentation, 355–358\n\n**Architecture Documentation:**\n- Diagrams and notations in, 358–360\n  * Hexagonal architecture living diagram, 179–184\n- Journals/blogs, 355\n- Modules, 355\n- Problem briefs, 344–346\n- Quality attributes, 346–347 (explicit and other)\n- Test-driven architecture, 367–369\n- Transparent architecture, 362–366\n\n**Living Documentation:**\n- Automated documents (declarative automation), 287–290\n  * Declarative automated deployment, 296–297\n  * Declarative configuration management, 293–296\n  * Declarative dependency management, 291–293\n- Living glossaries (advantages and examples), 158–165\n\n**Documentation Avoidance:**\n- Audiences, targeted, 66\n- BDD (Behavior-Driven Development) as a case of living documentation, 58–70\n- Documentation overhaul, 386–392\n\n**Miscellaneous Topics:**\n- Archeology: software, 403–404\n- AOP (Aspect-Oriented Programming) in architecture documentation, 363–365\n- Annotations in code, 97–102\n- Knowledge augmentation: automated documents and coding as documentation, 287–326\n  * Code reviews, 321–322\n  * Coding conventions, 221–224\n- Collaboration: best practices and continuous documentation, 21–22\n  * Continuous documentation, 276–277\n  * Cross programming, 274\n  * Event Storming, 275–280\n  * Mob programming, 274–275\n  * Pair programming, 273–274\n- Consolidation: benefits and limitations, 81–83\n  * Implementation considerations, 82–83\n  * Need for consolidation, 80–81\n- Conspicuous documentation: fast media, interactive collaborative creation, and unorthodox media, 417–429\n\n**Miscellaneous Terms:**\n- ADRs (Architectural Decision Records), 350\n- Agile Documentation (Rüping), 356\n- Agile Manifesto, 199\n- Agile T esting (Crispin and Gregory), 251, 272\n- Analysis Patterns (Fowler), 49\n- Angular.js commit guidelines: change logs, machine-accessible information, scope of change, type of change, 130–133\n- AnnotationDoclet, 168\n- AOP (Aspect-Oriented Programming) example, 114–115\n- Arrangement property, 110\n- Arrange-Act-Assert convention, 220–221\n- AsciiDoc, 177\n- @Aspect annotation, 114\n- AspectJ, 114\n- AssertJ, 234\n- Analysis Patterns (Fowler), 49\n- Animation, 432–433\n- Angular.js commit guidelines: change logs, machine-accessible information, scope of change, type of change, 130–133\n- AOP (Aspect-Oriented Programming) example, 114–115\n- Arrangement property, 110\n- Arrange-Act-Assert convention, 220–221\n- AsciiDoc, 177\n- @Aspect annotation, 114\n- AspectJ, 114\n- AssertJ, 234",2623,145.18487524986267
False,3,"216-F.pdf,216-F,""Index 439\ncontinuous delivery , 11–12\ncontinuous documentation, 276–277continuous training, 312–313contracts, published, 79–80conve","216-F.pdf,216-F,""Index 439\ncontinuous delivery , 11–12\ncontinuous documentation, 276–277continuous training, 312–313contracts, published, 79–80conventions\nadherence to, 106–107coding, 221–223documentation of, 105–106examples of, 104–105limitations of, 107living documentation in legacy code, 105\nconversations\ncharacteristics of, 268–269code discussions, 382–383decision logs, 383–384diagrams and, 171first debriefing, 382guided tours, 383–384introducing living documentation, 381–382knowledge from, 13obstacles to, 271–272ready-made knowledge in, 88–90Rule of Three Interpretations, 271Wiilo’s laws, 270\n@CoreConcept annotation, 164Crispin, Lisa, 251, 272cron, 202cross programming, 274–275Cucumber, 57Cunningham, Ward, 223, 230, 336curation. See knowledge curation\ncustom annotations\nadvantages of, 109package names, 112properties, 110standard annotations, extending, 113–114stereotypes and tactical patterns, 110–112\nCVS (Concurrent V ersions System), 108\nD\nD3.js, 177Dapper paper, 200–202databases, metadata, 108–109DDD (domain-driven design), 11–12\nliving documentation and, 50–53overview of, 49–50\nDean, Jeff, 278decision logs, 350–354, 383–384declarative automation\ndeclarative automated deployment, \n296–297declarative configuration management, \n293–296\ndeclarative dependency management, 291–293declarative guidelines, 303–304declarative style, 290machine documentation, 299overview of, 287–289as reconciliation mechanism, 290scaffolding, 297–299\nThe Decorative Art of  T oday (Le Corbusier), 333dedication, 32deliberate decision making, 319–321deliberate discovery , 319Dependency Injection (DI) containers, 212dependency management, declarative, 291–293dependency managers, 291design. See living design\ndesign documentation, guidelines versus, 305Design Patterns (Gamma et al), 102, 118development, documentation-driven, 326–328Dexy , 122, 177DI (Dependency Injection) containers, 212diagnostic tools\nsignature surveys, 336–337word clouds, 334–336\ndiagrams\nin architecture documentation, 358–359business overview case study\nbackground, 184–185changes, accommodation of, 190code augmentation, 186–188existing source code, 186living diagram generation, 188–189living documentation patterns and, 191quality attributes, addition of, 190–191\ncase study , 389context diagram example\naugmented code, 194–195background, 192–193benefits and limitations of, 195–196hyperlinks to source code locations, \n193–194\nknowledge consolidation, 194–195\nconversations and, 171domain-specific, 205–206event sourcing scenarios, 209–211hexagonal architecture example, 179–184overview of, 170–171plain-text\nadvantages of, 239–240diagrams as code, 243–244example of, 240–243\nIndex 440\ndiagrams (continued)\nrendering, 175–178\nscale of diagrams, 174–175storing code for, 173–174tools for, 177–178in traditional documentation, 9visualization guidelines, 178–179what to include, 171–174\ndifferences, focus on, 417–418digests, scenario, 139–140Dinwiddie, George, 275, 287dispersed facts, consolidating. See consolidationdistributed tracing, 200–202ditaa, 177Docco, 121Doclet, 168, 178document organization, 254documentation avoidance, 44–45, 48\nastonishment reports, 285constrained behavior, 307–309continuous training, 312–313conversations, 268–272declarative automation, 287–299on-demand documentation, 282design principles for, 309–311\nconsistency first, 310–311replaceability first, 310\nenforced guidelines, 300–307idea sedimentation, 280–282interactive documentation, 287–289just-in-time documentation, 282–285machine documentation, 299NODocumentation manifesto, 267spontaneous discussions, 277–280throw-away documentation, 282upfront documentation, 285–287working collectively , 272–276\ndocumentation bankruptcy , 401–402documentation by convention. See conventionsdocumentation by exception, 306documentation-driven development, 326–328domain immersion, 263–264domain training, 264domain vision statements, 260domain-driven design (DDD), 11–12Domain-Driven Design (Evans), 4, 408domains, business, 346domain-specific diagrams, 205–206domain-specific language (DSL), 96, 232–233Domain-Specific Languages (Fowler), 235domain-specific notation, 203–205domain-specific test language (DSTL), 234–235domains/subdomains, 408domestic ads, 427–429DotDiagram, 243drivers (pair programming), 273–274Dropwizard’s Metrics, 308DSL (domain-specific language), 96, 232–233DSTL (domain-specific test language), 234–235Duarte, Nancy , 378, 394duplex books, 419dynamic curation\nart museum curation metaphor, 153–155corpus of knowledge, 139editorial curation, 137–138examples of, 137guided tours\ncreating, 148–153literate programming compared to, 153overview of, 144–146\nhighlighted core, 141–144low-maintenance curation, 138overview of, 135–137scenario digests, 139–140sightseeing maps\ncreating, 146–148overview of, 144–146\nsummary of, 42–43\nE\nEaster eggs, 426Eaton-Rosen phenomenon, 427Eclipse, 229ECMAScript, 359editorial curation, 137–138effect maps, 403–404eligibility , 388email servers, 93embedded learning, 22, 100–102encouragement, enforcement versus, 303enforced guidelines. See also constrained \nbehavior\nadvantages of, 300–301in architecture documentation, 365–366declarative guidelines, 303–304enforcement versus encouragement, 303evolving guidelines, 302–303guidelines versus design documentation, 305Hamcrest example, 306–307legacy rules, 413–414rule examples, 301–302tools for, 304–305trust-first culture, 307"",5423",5446,"**Living Documentation:** An Overview\n* Continuous delivery: 11–12\n* Continuous documentation: 276–290\n	+ Conventions: adherence to (106–107), coding (221–223), documentation of (105–106)\n	+ Examples: 104–105, limitations (107)\n	+ Living documentation in legacy code: 105\n* Continuous training: 312–313\n* Contracts: published, 79–80\n* Conversations: characteristics of (268–269), code discussions (382–383), decision logs (383–384), diagrams and (171)\n	+ First debriefing: 382\n	+ Guided tours: creating (148–153), overview of (144–146)\n	+ Knowledge from: 13\n	+ Obstacles: 271–272, ready-made knowledge in (88–90), Rule of Three Interpretations (271), Wiilo’s laws (270)\n* @CoreConcept annotation: 164\n* Crispin, Lisa: 251, 272\n* Cron: 202\n* Cross programming: 274–275\n* Cucumber: 57\n* Cunningham, Ward: 223, 230, 336\n* Curiation. See knowledge curation\n* Custom annotations: advantages of (109), package names (112), properties (110), standard annotations, extending (113–114)\n* CVS (Concurrent Versions System): 108\n* DDD (domain-driven design): 11–12, living documentation and (50–53), overview of (49–50)\n* Dean, Jeff: 278\n* Decision logs: 350–354, 383–384\n* Declarative automation: 287–299\n	+ Declarative automated deployment: 296–297\n	+ Declarative configuration management: 293–296\n	+ Declarative dependency management: 291–293\n	+ Declarative guidelines: 303–304\n	+ Declarative style: 290\n	+ Machine documentation: 299\n* The Decorative Art of Today (Le Corbusier): 333\n* Dexy: 122, 177\n* DI (Dependency Injection) containers: 212\n* Dependency management: declarative (291–293), dependency managers (291)\n* Design. See living design\n* Design documentation: guidelines versus (305)\n* Design Patterns (Gamma et al): 102, 118\n* Development: documentation-driven (326–328)\n* Dexy: 122, 177\n* Diagnostic tools:\n	+ Signature surveys: 336–337\n	+ Word clouds: 334–336\n* Diagrams:\n	+ In architecture documentation: 358–359\n	+ Case study: background (184–185), changes, accommodation of (190), code augmentation (186–188), living diagram generation (188–189), quality attributes, addition of (190–191)\n	+ Case study : 389\n	+ Event sourcing scenarios: 209–211\n	+ Hexagonal architecture example: 179–184\n* Diagrams (continued):\n	+ Rendering: 175–178\n	+ Scale of diagrams: 174–175\n	+ Storing code for: 173–174\n	+ Tools for: 177–178\n* Traditional documentation: 9\n* Visualization guidelines: 178–179\n* What to include: 171–174\n* Differences: focus on (417–418)\n* Digests: scenario (139–140)\n* Dinwiddie, George: 275, 287\n* Distributed tracing: 200–202\n* Diagrams (Diaa): 177\n* Docco: 121\n* Doclet: 168, 178\n* Document organization: 254\n* Documentation avoidance: 44–45, 48\n	+ Asthonishment reports: 285\n	+ Constrained behavior: 307–309\n	+ Continuous training: 312–313\n	+ Conversations: 268–272\n	+ Declarative automation: 287–299\n	+ On-demand documentation: 282\n	+ Design principles for: 309–11\n	+ Consistency first: 310–11\n	+ Replaceability first: 310\n	+ Enforced guidelines: 300–307\n	+ Idea sedimentation: 280–282\n	+ Interactive documentation: 287–289\n	+ Just-in-time documentation: 282–285\n	+ Machine documentation: 299\n	+ NODocumentation manifesto: 267\n	+ Spontaneous discussions: 277–280\n	+ Throw-away documentation: 282\n	+ Upfront documentation: 285–287\n	+ Working collectively: 272–276\n* Documentation bankruptcy: 401–402\n* Documentation by convention. See conventions\n* Documentation by exception: 306\n* Domain immersion: 263–264\n* Domain training: 264\n* Domain vision statements: 260\n* Domain-driven design (DDD): 11–12, living documentation and (50–53), overview of (49–50)\n* Domain-specific diagrams: 205–206\n* Domain-specific language (DSL): 96, 232–233\n* Domain-specific notation: 203–205\n* Domain-specific test language (DSTL): 234–235\n* Duarte, Nancy: 378, 394\n* Duplex books: 419\n* Dynamic curation: 135–140\n	+ Art museum curation metaphor: 153–155\n	+ Corpus of knowledge: 139\n	+ Editorial curation: 137–138\n	+ Examples: 137\n	+ Guided tours: creating (148–153), overview of (144–146)\n	+ Highlighted core: 141–144\n	+ Low-maintenance curation: 138\n	+ Sightseeing maps: creating (146–148), overview of (144–146)\n* Easter eggs: 426\n* Eaton-Rosen phenomenon: 427\n* Email servers: 93\n* Embedded learning: 22, 100–102\n* Encouragement, enforcement versus: 303\n* Enforced guidelines. See also constrained behavior\n	+ Advantages of: 300–301\n	+ In architecture documentation: 365–366\n	+ Declarative guidelines: 303–304\n	+ Enforcement versus encouragement: 303\n	+ Evolutionary guidelines: 302–303\n	+ Guidelines versus design documentation: 305\n	+ Legacy rules: 413–414\n	+ Rule examples: 301–302\n	+ Tools for: 304–305\n	+ Trust-first culture: 307\n* Evolutionary guidelines: 302–303\n* Explicit invitations: 322\n* Extending standard annotations: 113–114\n* Eye-catching graphics: 362\n* F",4776,252.99905443191528
False,3,"217-H.pdf,217-H,""Index 441\nengineering\nas deliberate practice, 320\nreverse, 343\nerosion, architecture, 367error-proof APIs, 308–309Evans, Eric, 4,","217-H.pdf,217-H,""Index 441\nengineering\nas deliberate practice, 320\nreverse, 343\nerosion, architecture, 367error-proof APIs, 308–309Evans, Eric, 4, 126, 260, 405–406, 408event sourcing, visible tests with, 207, \n209–211\nEvent Storming, 275–276evergreen content\ndocument organization, 254ensuring stability of, 252–253high-level goals, 246–247limitations of, 247naming conventions, 252–253overview of, 21, 245–246README file case study , 247–251requirements, 246strategy documentation, 251–252\nevolving guidelines, 302–303exception, documentation by , 306expensive upfront documentation, 36exploitation of knowledge. See knowledge \nexploitation\nexternal actors, 194external annotations, 410external documentation\nexamples of, 27–28versus internal documentation, 25–26metadata databases, 108–109sidecar files, 108\nextra work, minimizing, 37–39Extreme Programming Explained (Beck), 49Extreme Programming (XP), 375\nF\nF#121Fakih, Houssam, 274fast media, 423Feathers, Michael, 23, 403feature files\ndefinition of, 58example of, 63–65glossaries, 69intent of, 58links to nonfunctional knowledge, 70organization of, 61property-based testing, 66–68scenarios, 58–59, 61–62specification details, 59–60tags in, 60–61files\ndecision logs, 350–354, 383–384feature\ndefinition of, 58example of, 63–65glossaries, 69intent of, 58links to nonfunctional knowledge, 70organization of, 61property-based testing, 66–68scenarios, 58–59, 61–62specification details, 59–60tags in, 60–61\nmanifests, 291sidecar, 107, 108\nflexible content, 419flottio.fuelcardmonitoring.legacy package, \n194–195\nfluent interfaces, 233–234Fluent NHibernate, 113fluent style\ndomain-specific test language (DSTL), \n234–235\nfluent interfaces, 233–234internal domain-specific language (DSL), \n232–233\noverview of, 232when to use, 235\nFluentValidation, 233formalized documentation, challenging need \nfor, 37\nfossilized knowledge, legacy applications as, \n402–403\nFowler, Martin, 49, 230, 235, 263, 296, 405–406, \n419, 432\nfractal architecture documentation, 355fragmentation of knowledge, 24frameworks, coding against, 224Freeman, Steve, 180, 316FSharp.Formatting, 121FuelCardMonitoring, 151FuelCardT ransaction, 151FuelCardT ransactionReport, 151FuelCardTxListener, 150–151fun activities, 39–40fun anecdotes, 426furniture, as media, 433\nG\ngeneral knowledge, 29–30George, Fred, 349"",2343",2366,"**Documentation Practices:**\n* Deliberate practice vs reverse engineering: 320\n* Documentation by exception (Beck): 306\n* Expensive upfront documentation vs evolving guidelines: 245–251, 36\n* Evergreen content: document organization, stability, high-level goals, limitations, naming conventions, README file case study, strategy documentation. 21, 245–251\n* External actors and annotations: 194\n* External versus internal documentation: 25–26, 27–28\n* Metadata databases and sidecar files: 108\n* Minimizing extra work: 37–39\n\n**Extreme Programming:**\n* XP Explained (Beck): 49\n* Feature files: definition, example, glossaries, intent, organization, property-based testing, scenarios, specification details, tags. 58–62\n* Files: decision logs, manifests, sidecar. 107, 108\n\n**Fluent Interfaces:**\n* Domain-specific test language (DSTL): 234–235\n* Fluent interfaces: overview, when to use. 233–234\n* Internal domain-specific language (DSL): 232–233\n\n**Fossilized Knowledge:**\n* Legacy applications as fossilized knowledge: 402–403\n\n**Miscellaneous:**\n* Fowler, Martin: architecture documentation, fractal architecture documentation. 263, 355\n* Fractal architecture documentation: 355\n* Fragmentation of knowledge. 24\n* Formality vs. need for formalized documentation: 37\n* Furniture as media: 433\n\n**People:**\n* Eric Evans: 4, 126, 260, 405–408\n* Houssam Fakih: 274\n* Michael Feathers: 23, 403\n\n**Processes and Tools:**\n* Fast media: 423\n* Feature files: definition, example, glossaries, intent, organization, property-based testing, scenarios, specification details, tags. 58–62\n* Fluent NHibernate: 113\n* Formatting tools (FSharp): 121\n* FuelCardMonitoring, FuelCardTransaction, FuelCardTransactionReport, FuelCardTxListener: 149–151\n* Fluent interfaces and style: domain-specific test language (DSTL), fluent interfaces, internal domain-specific language (DSL). 232–235\n* Formality vs. need for formalized documentation: 37\n* Furniture as media: 433\n\n**Technologies:**\n* Extreme Programming Explained (Beck): 49\n* Extreme Programming (XP): 375\n* Fluent NHibernate: 113\n* FluentValidation: 233\n* FSharp.Formatting: 121.",2119,77.96430039405823
False,3,"218-J.pdf,218-J,""Index 442\nGibson, William, 5\nGierke, Oliver, 128Gilb, 262Git, 92GitHub, 73, 177, 308glossaries, 69\nadvantages of, 158–159bounded c","218-J.pdf,218-J,""Index 442\nGibson, William, 5\nGierke, Oliver, 128Gilb, 262Git, 92GitHub, 73, 177, 308glossaries, 69\nadvantages of, 158–159bounded contexts, 164–165case study , 165–170, 389example of, 160–162how they work, 160\ngoals\ndeveloping, 260–261impact mapping, 261–262\nGoldberg, Avram, 259goodies/swag, 431Google\nAnnotations Gallery , 103–104Dapper paper, 200Guava ClassPath class, 178\nGradle, 289, 291Graphviz, 4, 177, 188, 210Grassé, Pierre-Paul, 23Gregory , Janet, 251, 272Growing Object-Oriented Software, Guided by \nT ests (Freeman), 180\nGuava ClassPath class, 178guided tours\ncase studies, 383–384, 390creating, 148–153literate programming compared to, 153overview of, 144–146\n@GuidedT our annotation, 149–153guideline enforcement. See also constrained \nbehavior\nadvantages of, 300–301declarative guidelines, 303–304enforcement versus encouragement, 303evolving guidelines, 302–303guidelines versus design documentation, 305Hamcrest example, 306–307legacy rules, 413–414rule examples, 301–302tools for, 304–305trust-first culture, 307\ngut feelings, 91\nH\nHamcrest, 306–307hashtags, 430Haskell, 121, 229Hawley , Steve, 91Hendrickson, Chet, 403Henney , Kevlin, 401hexagonal architecture living diagram, 179–184High, Timothy , 122high-level goals, 246–247highlighted core, 141–144highlighted structure, 408–409Hilton, Peter, 283–284, 426Holmes, James R.10Hoogle, 229Hoover, Dave, 332, 374Hull, John C.29humor and cheap media, 430–431Hungarian notation, 221–222Hunt, Dave, 73Hystrix, 308\nI\nIcare troubleshooting guide, 323–325IDE (integrated development environment) \nintegrated documentation, 238\ncode searching, 238–239semantics, 239type hierarchy , 238\nidea sedimentation, 280–282Idempotent property , 110Identity by value property , 110IEEE 1471, 358Immutable property , 110impact mapping, 261–262Implementation Patterns (Beck), 218Implementing DDD (V ernon), 180implicit knowledge, 24in situ documentation, 28indented text, 262infodecks, 432information graveyard, traditional \ndocumentation as, 10\ninformation radiators, 429–430@Inheritance annotation, 114insightful documentation, 22instrumentation, 200integrated development environment. See IDE \n(integrated development environment) integrated documentation\nintegrated documentation, 226–227, 238\ncode searching, 238–239semantics, 239type hierarchy , 238\ninteractive collaborative creation, 424"",2370",2393,"**Documentation**\n* **Types of Documentation**:\n    + Glossaries:\n        - Advantages: 158–159\n        - Bounded contexts: 164–165\n        - Case study: 165–170, 389\n            * Example of: 160–162\n            * How they work: 160\n        - Goals: developing 260–261\n        - Impact mapping: 261–262\n    + Guided tours:\n        - Case studies: 383–384, 390\n            * Creating: 148–153\n        - Overview of: 144–146\n        - @GuidedTour annotation: 149–153\n    + Guidelines enforcement:\n        - Advantages: 300–301\n        - Declarative guidelines: 303–304\n            * Enforcement versus encouragement: 303\n        - Evolving guidelines: 302–303\n        - Guidelines versus design documentation: 305\n        - Hamcrest example: 306–307\n        - Legacy rules: 413–414\n        - Rule examples: 301–302\n        - Tools for: 304–305\n        - Trust-first culture: 307\n* **Other Documentation Forms**:\n    + High-level goals: 246–247\n    + Highlighted core: 141–144\n    + Highlighted structure: 408–409\n    + IDE (integrated development environment):\n        - Integrated documentation: 238\n            * Code searching: 238–239\n            * Semantics: 239\n            * Type hierarchy: 238\n        - Idea sedimentation: 280–282\n    + Immutable property: 110\n    + Impat mapping: 261–262\n    + Implementing DDD (Vernon): 180\n    + Implementation Patterns (Beck): 218\n    + IDEM potent property: 110\n    + Identity by value property: 110\n    + IEEE 1471: 358\n    + Instrumentation: 200\n    + Integrated documentation: 226–227, 238\n        * Code searching: 238–239\n        * Semantics: 239\n        * Type hierarchy: 238\n    + Interactive collaborative creation: 424\n    + Indented text: 262\n* **Other Concepts**:\n    + Ideas: 280–282\n    + IDEM potent guide: 323–325\n    + Implicit knowledge: 24\n    + In situ documentation: 28\n    + Indented text: 262\n    + Infodecks: 432\n    + Information graveyard, traditional documentation as: 10\n    + Information radiators: 429–430\n    + @Inheritance annotation: 114\n    + Insightful documentation: 22\n    + Instrumentation: 200\n    + Integrated development environment. See also IDE (integrated development environment)\n    + Integrated documentation, integrated documentation, 238\n        * Code searching: 238–239\n        * Semantics: 239\n        * Type hierarchy: 238\n    + Interactive collaborative creation: 424\n    + Indented text: 262",2392,96.36688733100891
False,3,"219-K.pdf,219-K,""Index 443\ninteractive documentation, 287–289\ninterfaces\nfluent, 233–234Introspectable, 214–215\ninternal documentation\ndefinition","219-K.pdf,219-K,""Index 443\ninteractive documentation, 287–289\ninterfaces\nfluent, 233–234Introspectable, 214–215\ninternal documentation\ndefinition of, 21examples of, 27–28versus external documentation, 25–26overview of, 25–26preference for, 28in situ documentation, 28\ninternal domain-specific language (DSL), \n232–233\ninternal quality , 337–338intrinsic knowledge augmentation, 117–119introduction of living documentation\nbatch systems\nbusiness behavior, 388contact information, 390guided tours, 390journey tests, 390living diagrams, 389living glossary , 389overview of, 386–387README files, 387ready-made documentation, 387single source of truth, 388–389visible workings, 388–389\ncommon objections, 384–385compliance requirements\nITIL case study , 397–399meeting/exceeding, 395–397\nguided tours, 383–384legacy documentation, migrating, 385–386to management\nchallenges of, 390–391living documentation initiatives, 392–394problem briefs, 391–392vision, exhibiting, 394–395\nmarginal documentation, 386official ambition, 378suggested path\ngoing big and visible, 379–380starting gently , 379–380steps for, 378–379\nto team members\ncode discussions, 382–383conversations, 381–382decision logs, 383–384first debriefing, 382\nundercover experiments, 377–378\nIntrospectable interface, 214–215introspectable workings\nintrospecting with reflection, 213–214introspecting without reflection, 214–215overview of, 211–212\ninvestigation walls, 264investment in stable knowledge\ndomain immersion, 263–264domain training, 264investigation walls, 264“live-my-life” sessions, 264–265shadow users, 265value of, 262–263, 265\nISO/IEC/IEEE 42010, 358ITIL compliance case study\noverview of, 397–398release management, 399request for change management, 398–399\nIvy , 291\nJ\nJava\nannotations, 160JPA (Java Persistence API), 113–114naming conventions, 221sightseeing map in, 146–148\nJavadoc, 74JAX-RS annotations, 114Jeffries, Ron, 218Jenkins, 296JGiven, 234JHipster, 298–299JMock, 234journals, 351–354journey tests, 390JPA (Java Persistence API), 113–114just-in-time documentation, 35, 282–285\nK\nKheogh, Liz, 355knowledge augmentation\nannotations\nannotation by default, 115AOP (aspect-oriented programming) \nexample, 114–115\nin architecture documentation, 363–365custom, 109–114embedded learning, 100–102examples of, 99–100Google Annotations Gallery , 103–104JPA (Java Persistence API), 113–114\nIndex 444\nknowledge augmentation (continued)\nin legacy application documentation, 405, \n408–409, 410\nin living glossaries, 160\nliving services diagram example, 200–202machine-accessible documentation, \n119–120\nmeta-annotations, 163objections to, 384overview of, 97–99rationales behind decisions, 100, 124Spring Framework, 113–114, 201strong coupling, 163\ncommit messages\nadvantages of, 128–130commit guidelines, 129–133\ncontext diagram example, 194–195conventions\nadherence to, 106–107documentation of, 105–106examples of, 104–105limitations of, 107living documentation in legacy code, 105\nembedded learning, 22external documentation\nexamples of, 27–28versus internal documentation, 25–26metadata databases, 108–109sidecar files, 108\nliterate programming, 120–122, 153machine-accessible documentation, 119–120module-wide knowledge, 115–116\nintrinsic knowledge augmentation, 117–119module types, 116–117module-wide augmentation in practice, \n117\noverview of, 95programming languages, limitations of, 95–97rationales behind decisions, recording\nad hoc documents, 124annotations, 100, 124blog posts, 124as enabled for change, 126importance of, 122–123information to include, 123–124project bibliographies, 126–127purpose of, 124–125, 126skills as pre-documented rationales, 125speculation, avoidance of, 125style declarations, 127–128\nstability of knowledge, 38–39structured tags within comments, 102–103summary of, 42theory building and passing, 15–17transfer of knowledge, 17–18, 23what to document, 18–19\nknowledge backlog, 286knowledge curation\nart museum curation metaphor, 153–155corpus of knowledge, 139editorial curation, 137–138examples of, 137guided tours\ncreating, 148–153literate programming compared to, 153overview of, 144–146\nhighlighted core, 141–144for living documents, 162–164low-maintenance curation, 138overview of, 135–137scenario digests, 139–140sightseeing maps\ncreating, 146–148overview of, 144–146\nsummary of, 42–43\nknowledge exploitation\nauthoritative knowledge, identifying, 71–86consolidation\ncontext diagram example, 194–195how it works, 81–82implementation considerations, 82–83need for, 80–81\ndocumentation and, 12–13evolution of knowledge, 13–14gut feelings and, 91location of knowledge, 72necessity of, 14–15origination of knowledge, 13overview of, 71past events, accounts of, 33ready-made documentation, 23–24, 83–85, \n105, 387\nreconciliation mechanisms\nconsistency tests, 76–78overview of, 75–76published contracts, 79–80reconciliation on test assumptions, 78–79redundant sources with, 32\nsingle-source publishing\nexamples of, 73–74overview of, 72–73remarks, 75snapshots with version numbers, 75\nsingle-use knowledge, 32–33specific versus general knowledge, 29–30standard vocabulary , 85–87summary of, 41tools history , 92–93"",5179",5202,"**Documentation Types:**\n- **Interactive documentation**: fluent interfaces, introspectable interfaces and workings (233–240)\n  - Fluent interfaces: 233–234\n  - Introspectable interfaces and workings: 214–215\n- **Living documentation**: introduction of living documentation, objections (384–396)\n  - Batch systems: business behavior, contact information, guided tours, journey tests, living diagrams, living glossary, README files, single source of truth, visible workings (386–390)\n  - Legacy documentation: migrating, objections (385–386)\n  - Management challenges: living documentation initiatives, problem briefs, vision (390–395)\n- **Marginal documentation**: going big and visible, starting gently, steps for (379–380)\n\n**Knowledge Augmentation:**\n- **Annotations**: annotation by default, AOP example, in architecture documentation, custom, embedded learning, Google Annotations Gallery, JPA, meta-annotations, objections, Spring Framework (95–114)\n  - Commit messages: advantages, commit guidelines, conventions (124–133)\n- **Conventions**: adherence to, documentation of, examples, limitations, living documentation in legacy code, sidecar files (104–110)\n- **Literate programming**: 153\n- **Machine-accessible documentation**: machine-accessible documentation, metadata databases, sidecar files (119–120)\n- **Module-wide knowledge augmentation**: module types, embedded learning, limitations of programming languages (115–117)\n- **Stability of knowledge**: 38–39\n\n**Knowledge Curation:**\n- **Guided tours**: creating, overview (144–153)\n- **Sightseeing maps**: creating, overview (146–150)\n\n**Knowledge Exploitation:**\n- **Consolidation**: context diagram example, how it works, implementation considerations, necessity, documentation and reconciliation mechanisms (71–89)\n- **Single source publishing**: examples, overview (72–74)\n- **Ready-made documentation**: objections, examples (387)\n- **Redundant sources with reconciliation mechanisms**: consistency tests, published contracts, reconciliation on test assumptions (75–80)\n\n**Other:**\n- **ISO/IEC/IEEE 42010**: 358\n- **Ivy**: 291\n- **Java**: annotations, JPA, naming conventions, sightseeing map in Java (146–148)\n- **Javadoc**: 74\n- **JAX-RS annotations**: 114\n- **JUnit and Selenium**: JGiven, JMock (234)\n- **JHipster**: 298–299",2298,87.48336672782898
False,3,"220-M.pdf,220-M,""Index 445\nKnuth, Donald, 120\nKorpela, Jukka, 287\nL\nLa Gaité Lyrique, 278–279“ A Laboratory for Teaching Object-Oriented \nThinkin","220-M.pdf,220-M,""Index 445\nKnuth, Donald, 120\nKorpela, Jukka, 287\nL\nLa Gaité Lyrique, 278–279“ A Laboratory for Teaching Object-Oriented \nThinking” (Beck and Cunningham), 223\nLaw of T wo Feet, 278layout, text\nArrange-Act-Assert convention, 220–221tabular code layout, 219UML visual notation, 219\nLe Corbusier, 333Leanpub, 74legacy application documentation, 401\nbiodegradable transformation, 410\nbankruptcy application example, 411–412strangler application example, 411\nbubble context, 404–406conventions, 105documentation bankruptcy , 401–402effect maps, 403–404enforced legacy rules, 413–414external annotations, 410highlighted structure, 408–409legacy applications as fossilized knowledge, \n402–403\nmaxims, 412–413migrating into living documentation, 385–386“rewriting the same specs” fallacy , 402summary of, 46–47superimposed structure, 406–408\nlegacy read models, 413Lego blocks, 433Leiningen, 291libraries\nAssertJ, 234DotDiagram, 243JGiven, 234JMock, 234NFluent, 234XDoclet, 102\nlicense mechanisms, 307link registries, 256linked knowledge\nbookmarked searches, 257–258broken link checkers, 255–256link registries, 256nonfunctional knowledge, 70overview of, 254–255source code locations, 193–194standard knowledge, 86–87volatile-to-stable dependencies, 255LINQ syntax, 233listening to documentation, 315–316literate programming, 120–122, 153“live-my-life” sessions, 264–265living architecture documentation. See \narchitecture documentation\nliving curation. See knowledge curationliving design, 315–316\nabuse of, 329–330biodegradable documentation, 331–332design principles for documentation \navoidance, 309–311consistency first, 310–311replaceability first, 310\ndiagnostic tools\nsignature surveys, 336–337word clouds, 334–336\ndocumentation-driven development, \n326–328\nimportance of, 338–339internal quality of, 337–338listening to documentation, 315–316overview of, 339–341procrastination by , 329–330programming by coincidence design\ndeliberate decision making, 319–321documentation as code review , 321–322overview of, 317–318\nshameful documentation\ncode documentation, 325–326Icare troubleshooting guide example, \n323–325\noverview of, 322–323\ntransparency , 332–334\nliving diagrams. See diagramsliving documentation principles\ncollaboration, 21–22insight, 22low effort, 21overview of, 19–20reliability , 20–21\nliving documents\nchallenges with automated generation of, \n196–197\ncreating, 158definition of, 157information curation for, 162–164living diagrams\nbusiness overview case study , 184–191context diagram example, 192–196conversations and, 171hexagonal architecture example, 179–184overview of, 170–171rendering, 175–178"",2657",2680,"**Legacy Application Documentation**\n- **Arrange-Act-Assert convention**: described in ""A Laboratory for Teaching Object-Oriented Thinking"" by Beck and Cunningham (p. 220–221)\n- **Tabular code layout**: used in UML visual notation (p. 219)\n- **Biodegradable transformation**: applying to legacy applications, such as bankruptcy and strangle applications (pp. 401–412)\n  * Documentation: biodegradable documentation, legacy read models, superimposed structure, summarized on p. 46\n- **Legacy applications as fossilized knowledge**: legacy applications are discussed as sources of valuable but ""fossilized"" knowledge (p. 402–403)\n- **Conventions**: arranged into categories like Arrange-Act-Assert, documented by Beck and Cunningham (p. 105)\n\n**Linking Knowledge**\n- **Linked knowledge**: includes bookmarked searches, broken link checkers, and link registries (pp. 254–258)\n- **Legacy read models**: documentation for legacy systems, as mentioned on p. 413\n\n**Lego Blocks and Documentation Tools**\n- **Lego blocks**: used as an analogy to build understanding in ""living design"" (p. 315–316)\n- **Leiningen**: a tool referenced in relation to libraries (p. 291)\n- **Libraries**: mentioned as part of the documentation process, such as AssertJ, DotDiagram, JGiven, JMock, NFluent, and XDoclet (pp. 233–243)\n- **Link registries**: used for managing information within documentation, discussed on p. 256\n\n**Listening to Documentation**\n- ""Literate programming"": a concept related to documentation, discussed on pp. 120–122 and 153\n- **""Live-my-life"" sessions**: an approach for understanding design through real-world context (p. 264–265)\n\n**Living Architecture Documentation**\n- **Living architecture documentation**: discussed as a way to represent knowledge, including diagrams and design principles (pp. 315–316)\n\n**Documentation-Driven Development**\n- **""Listening to documentation""**: the importance of documenting code and design decisions (p. 315–316)\n- **Shameful documentation**: a term for poor quality or inconsistent documentation, with examples of both code documentation and Icare troubleshooting guide (pp. 323–326)\n- **""Living documentation principles""**: collaboration, insight, low effort, reliability, and transparency (p. 317)\n\n**Living Documents**\n- **Creating living documents**: a process for documenting information in an accessible format (p. 158)\n- **Living diagrams**: business overview case study, context diagram example, hexagonal architecture example, rendering, and overview (pp. 170–191)",2515,79.00367450714111
False,3,"221-O.pdf,221-O,""Index 446\nliving documents (continued)\nscale of diagrams, 174–175\nstoring code for, 173–174tools for, 177–178visualization guideli","221-O.pdf,221-O,""Index 446\nliving documents (continued)\nscale of diagrams, 174–175\nstoring code for, 173–174tools for, 177–178visualization guidelines, 178–179what to include, 171–174\nliving glossaries\nadvantages of, 158–159bounded contexts, 164–165case study , 165–170example of, 160–162how they work, 160\nneed for, 1–4patterns in, 196–197presentation rules, 158\nliving services diagram example, 200–202location of knowledge, 72Logistics and Supply Chain Management \n(Christopher), 29\nlogs, decision, 350–354, 383–384Lovich, Igor, 414low-fidelity content, 419low-maintenance dynamic curation, 138Loyer, Arnauld, 62\nM\nmachine documentation, 299machine-accessible documentation, 29, 119–120, \n132–133\nmanagement, selling living documentation to\nchallenges of, 390–391living documentation initiatives, 392–394problem briefs, 391–392vision, exhibiting, 394–395\nmanifests, 291manual transcription, 7, 8maps\nimpact mapping, 261–262sightseeing maps\nconcept of, 144–146creating, 146–148\nmarginal documentation, 386Marginalia, 121Marick, Brian, 202–203Markdown descriptions, 68Martin, Robert Cecil, 218, 224Matcher method, 306Matts, Chris, 327Maven, 74, 289, 291maxims\nfor conspicuous documentation, 427in legacy application documentation, 412–413\nmedia\n3D printed models, 433comics, 432domestic ads, 427–429furniture, 433goodies/swag, 431humor and cheap media, 430–431infodecks, 432information radiators, 429–430Lego blocks, 433maxims, 427memes, 428–429posters, 427–429visualizations and animation, 432–433\nmeme-based posters, 428–429messages, commit\nadvantages of, 128–130commit guidelines, 129–133\nmeta-annotations, 163metadata databases, 108–109metaphors, system\nmetaphors in other metaphors, 376value of, 375\nmethods\nMatcher, 306monitor(), 151printMethod(), 170process(), 168–169toString(), 208, 212\nMetrics (Dropwizard), 308migrating legacy application documentation, \n385–386\nMindjet MindManager, 262MindMeister, 262MindMup, 262MindNode, 262minimizing extra work, 37–39misleading help, traditional documentation as, \n10–11\nmodule-wide knowledge, 115–116\nintrinsic knowledge augmentation, 117–119module types, 116–117module-wide augmentation in practice, 117\nmonitor() method, 151Moura, Romeu, 425music theory living glossary , 165–170\nN\nNagy , Zoltán, 241naming conventions\nannotation packages, 112arbitrary versus descriptive names, 253contextual nature of, 224evergreen content, 252–253importance of, 223–224\nnapkin sketches, 174"",2451",2474,"**Living Documents and Visualization Guidelines**\n* Diagrams: scale of diagrams, 174–175; storing code for, 173–174; tools for creating, 177–178\n* Visualization guidelines, 178–179\n* Include in living documents: what to include, 171–174\n\n**Living Glossaries**\n* Advantages: 158–159\n* Bounded contexts, 164–165\n* Case study, 165–170\n* How they work, 160\n* Need for, 1–4\n* Patterns in living glossaries, 196–197\n* Presentation rules, 158\n\n**Living Services Diagram Example**\n* Concept of, 200–202\n\n**Location of Knowledge**\n* 72\n\n**Logistics and Supply Chain Management (Christopher)**\n* Logs: decision logs, 350–354; documentation for logistics and supply chain management, 29\n\n**Documentation Formats**\n* Low-fidelity content, 419\n* Low-maintenance dynamic curation, 138\n* Machine documentation, 299\n* Machine-accessible documentation, 119–120, 132–133\n* Manual transcription, 7, 8\n\n**Maps**\n* Impact mapping, 261–262\n* Sightseeing maps: creating, 146–148; concept of, 144–146\n\n**Documentation Best Practices**\n* Marginal documentation, 386\n* Marick method, 202–203\n* Maven, 74, 289, 291\n* Maxims: for conspicuous documentation, 427; in legacy application documentation, 412–413\n\n**Media**\n* 3D printed models, 433\n* Comics, 432\n* Domestic ads, 427–429\n* Furniture, 433\n* Goodies/swag, 431\n* Humor and cheap media, 430–431\n* Infodecks, 432\n* Information radiators, 429–430\n* Lego blocks, 433\n* Meme-based posters, 428–429\n* Memes, 428–429\n* Posters, 427–429\n* Visualizations and animation, 432–433\n\n**Commit Messages**\n* Advantages: 128–130\n* Commit guidelines, 129–133\n\n**Metadata Databases**\n* Metadata databases, 108–109\n\n**Methods**\n* Matcher method, 306\n* monitor(), 151\n* printMethod(), 170\n* process(), 168–169\n* toString(), 208, 212\n\n**Metrics (Dropwizard)**\n* Migrating legacy application documentation, 385–386\n\n**Mindjet MindManager, MindMeister, MindMup, and MindNode**\n* Tools for creating diagrams and visualizations, 177–178\n\n**Minimizing Extra Work**\n* 37–39\n\n**Misleading Help: Traditional Documentation as Misleading Help**\n* 10–11\n\n**Module-Wide Knowledge**\n* Intrinsic knowledge augmentation, 117–119\n* Module types, 116–117\n* Module-wide augmentation in practice, 117\n\n**Music Theory Living Glossary**\n* Example of, 165–170",2244,98.28291773796082
False,3,"222-Q.pdf,222-Q,""Index 447\nNaur, Peter, 15, 374\nnavigators (pair programming), 273–274Neo4j, 177Netflix, 308, 369new environments, introduction of l","222-Q.pdf,222-Q,""Index 447\nNaur, Peter, 15, 374\nnavigators (pair programming), 273–274Neo4j, 177Netflix, 308, 369new environments, introduction of living \ndocumentation to\nbatch systems\nbusiness behavior, 388contact information, 390guided tours, 390journey tests, 390living diagrams, 389living glossary , 389overview of, 386–387README files, 387ready-made documentation, 387single source of truth, 388–389visible workings, 388–389\ncommon objections, 384–385compliance requirements\nITIL case study , 397–399meeting/exceeding, 395–397\nguided tours, 383–384legacy documentation, migrating, 385–386marginal documentation, 386official ambition, 378selling to management\nchallenges of, 390–391living documentation initiatives, 392–394problem briefs, 391–392vision, exhibiting, 394–395\nsuggested path\ngoing big and visible, 380–381starting gently , 379–380steps for, 378–379\nto team members\ncode discussions, 382–383conversations, 381–382decision logs, 383–384first debriefing, 382\nundercover experiments, 377–378\nNewman, Sam, 125, 308, 362NFluent, 234NODocumentation manifesto, 267nonfunctional knowledge, linking to, 70Norman, Don, 309North, Dan, 309–310, 319notation\nin architecture documentation, 358–359domain-specific, 203–205Hungarian, 221–222in traditional documentation, 9\nNotNull property , 110npm, 291NuGet, 289, 291Nygard, Michael, 351\nO\nObject Design (Wirfs-Brock), 49object trees\nintrospecting with reflection, 213–214introspecting without reflection, 214–215nature of, 211–212\nobjections to living documentation, 384–385Object-Oriented Reengineering Patterns \n(Demeyer), 412\nOctopus Deploy , 296official ambition, introducing living \ndocumentation through, 378\nonboarding, Event Storming and, 275–276on-demand documentation, 282Options, Futures, and Other Derivatives (Hull), \n29\norganization\nevergreen content, 254of feature files, 61\nORM engines, 91Oshineye, Adewale, 332, 374Ottinger, Tim, 10, 224\nP\npackage managers, 291package names, 112package-info.java, 164–165package-level annotations, 405pair programming, 273–274Pandoc, 177paper documentation. See traditional \ndocumentation\nParent ID, 200past events, accounts of, 33Patterns of  Enterprise Application Architecture \n(Fowler), 112\nPauly , Mathieu, 239PaymentJourneySteps enumeration, 152Perry , Michael L.309Pescio, Carlo, 317Petricek, T omas, 121Philippart, Gilles, 413Pickles, 61–62, 66, 67, 68Pierrain, Thomas, 234plain-text diagrams, 175\nadvantages of, 239–240diagrams as code, 243–244example of, 240–243\nplain-text documents, 66"",2522",2545,"**Living Documentation:** Introduction of Living Documentation in New Environments\n- **Navigators (pair programming):** 273–274\n- **Neo4j:** 177\n- **Netflix:** 308, 369\n\n**New Environments:** Introduction of Living Documentation to Batch Systems\n- Common objections: 384–385\n  * Compliance requirements\n    + ITIL case study: 397–399\n      - Meeting/exceeding compliance requirements: 395–397\n      - Guided tours: 383–384\n  * Legacy documentation, migrating: 385–386\n  * Marginal documentation: 386\n  * Officially ambitioned living documentation initiatives: 392–394\n  * Problem briefs: 391–392\n  + Vision, exhibiting: 394–395\n- **Suggested path:**\n  * Goin big and visible: 380–381\n  * Starting gently: 379–380\n  * Steps for introducing living documentation: 378–379\n\n**Living Documentation to Team Members:**\n- Code discussions: 382–383\n- Conversations: 381–382\n- Decision logs: 383–384\n- First debriefing: 382\n- Undercover experiments: 377–378\n\n**Notation in Architecture Documentation:**\n- In architecture documentation: 358–359\n- Domain-specific notation: 203–205\n  * Hungarian notation: 221–222\n- Notation in traditional documentation: 9\n\n**Object Design (Wirfs-Brock):** 49\n\n**Object Trees:**\n- Introspecting with reflection: 213–214\n- Introspecting without reflection: 214–215\n- Nature of object trees: 211–212\n- Objections to living documentation: 384–385\n\n**Object-Oriented Reengineering Patterns (Demeyer):** 412\n\n**Octopus Deploy:** 296\n\n**Official Ambition:** Introducing Living Documentation Through, 378\n\n**Onboarding: Event Storming and, 275–276**\n\n**On-demand documentation:** 282\n\n**Options, Futures, and Other Derivatives (Hull):** 29\n\n**Organization:**\n- Nevergreen content: 254\n- Organization of feature files: 61\n\n**Oshineye, Adewale:** 332, 374\n\n**Ottinger, Tim:** 10, 224\n\n**Patterns of Enterprise Application Architecture (Fowler):** 112\n\n**Pauly, Mathieu:** 239\n\n**Percival, Michael L.:** 309\n\n**Pescio, Carlo:** 317\n\n**Petriček, Tomas:** 121\n\n**Philippart, Gilles:** 413\n\n**Pickles:** 61–62, 66, 68\n\n**Pierrain, Thomas:** 234\n\n**Plain-text diagrams:**\n- Advantages of: 239–240\n- Diagrams as code: 243–244\n- Example of plain-text diagrams: 240–243\n\n**Plain-text documents:** 66",2204,97.29113602638245
False,3,"223-S.pdf,223-S,""Index 448\nPlantUML, 177\nPositive property , 110posters, 427–429The Pragmatic Programmer \n(Hunt and Thomas), 73\npresentation rules","223-S.pdf,223-S,""Index 448\nPlantUML, 177\nPositive property , 110posters, 427–429The Pragmatic Programmer \n(Hunt and Thomas), 73\npresentation rules, 158Preston-Lerner, T om, 258primitives, 226principles of living documentation\ncollaboration, 21–22insight, 22low effort, 21overview of, 19–20reliability , 20–21\nprintMethod() method, 170problem briefs, 344–346, 391–392process() method, 168–169procrastination, 329–330“Programming as Theory Building” \n(Naur), 15\nprogramming by coincidence design\ndeliberate decision making, 319–321documentation as code review , 321–322overview of, 317–318\nprogramming languages, limitations of, 95–97project bibliographies, 126–127project structure\nhighlighted, 408–409superimposed, 406–408\nproject tracking tools, 93promoting documentation, 426“proof of concept”, 372propagation mechanism, redundant sources \nwith, 32\nPropel, 91property-based testing, 66–68proprietary diagrams, 174Pryce, Nat, 180, 316published contracts, 79–80published documents\nremarks, 75single-source publishing, 72–73snapshots with version numbers, 75\npull systems, 283Puppet, 289, 293–296Pure property , 110\nQ\nQDox, 178quality attributes, 370explicit, 346–347at runtime in production, 369as scenarios, 368–369tables of content, 370\nquestions of documentation, 33–36\nR\nRaft, 433Rainsberger, Joe, 418rationales behind decisions, recording\nad hoc documents, 124annotations, 100, 124blog posts, 124as enabled for change, 126importance of, 122–123information to include, 123–124project bibliographies, 126–127purpose of, 124–125, 126skills as pre-documented rationales, 125speculation, avoidance of, 125style declarations, 127–128\nREADME files\ncase study , 387evergreen content in, 247–251\nready-made documentation, 23–24, 83–85, 105, \n387\nreality checks, architecture, 366–367reconciliation mechanisms\nautomation as, 290in BDD (behavior-driven development), 56–58consistency tests, 76–78overview of, 75–76published contracts, 79–80reconciliation on test assumptions, 78–79redundant sources with, 32\nrecoverability of knowledge, 24redundancy\nin BDD (behavior-driven development), \n56–58\nredundant sources\nwith propagation mechanism, 32with reconciliation mechanism, 32\nReeves, Paul, 397refactorable documentation\ncase study , 236–237code as documentation, 218\ncoding against a framework, 224coding conventions, 221–223naming, 223–224text layout, 219–221\nIndex 449\ncomposed method, 230–232\nfluent style\ndomain-specific test language (DSTL), \n234–235\nfluent interfaces, 233–234internal domain-specific language (DSL), \n232–233\noverview of, 232when to use, 235\nintegrated documentation, 238\ncode searching, 238–239semantics, 239type hierarchy , 238\noverview of, 217–218plain-text diagrams\nadvantages of, 239–240diagrams as code, 243–244example of, 240–243\nsummary of, 43type-driven documentation\nadvantages of, 225–226advantages over comments, 228associations, 227example of, 228–230integrated documentation, 226–227weak typing, 226\nrefactoring-proof knowledge, 21reflection\nintrospecting with, 213–214introspecting without, 214–215\nregistries, link, 256release management, 399reliability , 20–21Relish, 68remarks, 75rendering living diagrams, 175–178replaceability , designing for, 310ReportDAO, 151–152requirements, stability of, 246Resonate (Duarte), 378, 394RESTful web service, 114reverse engineering, 343reverse just-in-time doc, 284“rewriting the same specs” fallacy , 402rhyme-as-reason effect, 427Richardson, Chris, 308Rozanski, Nick, 358–359Ruby on Rails, 298RubyGems, 291Rule of Three Interpretations, 271Rule of T wo, 283ruleslegacy , 413–414presentation, 158\nruntime documentation\nintrospectable workings\nintrospecting with reflection, 213–214introspecting without reflection, 214–215overview of, 211–212\nliving services diagram example, 200–202overview of, 199quality attributes at runtime, 369software as documentation, 202–203summary of, 43visible tests\ndomain-specific diagrams, 205–206domain-specific notation, 203–205event sourcing scenarios, 207, 209–211example of, 208–209overview of, 203\nvisible workings, 202–203\nRüping, Andreas, 356Russell, Matt, 135\nS\nsalient points, description of, 417–418Salt, 293sbt, 291scaffolding, 297–299scale of diagrams, 174–175scenarios\nkey , 66as living documentation, 61–62overview of, 58–59quality attributes as, 368–369scenario digests, 139–140\nSchwarzenegger, Arnold, 213scope of change, specifying, 131–132searches\nbookmarked, 257–258code, 238–239search-friendly documentation, 420–421\nsedimentation of ideas, 280–282segmenting by known audience, 419Selenium, 390self-documentation, 294separate activities, 6–7services registry , 93shadow users, 265shameful documentation\ncode documentation, 325–326Icare troubleshooting guide example, 323–325overview of, 322–323\nIndex 450\nsidecar files, 108\nsightseeing maps\nconcept of, 144–146creating, 146–148\nsignature surveys, 336–337SimpleTest class, 209Simple.Testing framework, 207simplicity , 21simulations, small-scale\nadvantages of, 370–372building, 374characteristics of, 372techniques to achieve, 373\nsingle-source publishing\ncase study , 388–389examples of, 73–74overview of, 31, 72–73remarks, 75snapshots with version numbers, 75\nsingle-use knowledge, 32–33Site Reliability Engineering (Beyer et al), 347skills\nas pre-documented rationales, 125skills matrix, 287\nSlack, 92small-scale simulations\nadvantages of, 370–372building, 374characteristics of, 372techniques to achieve, 373\nsnapshots, 66, 75SO (Stack Overflow) documentation, 424–425software archeology , 403–404Software Archeology (Hendrickson), 403software architecture documentation. See \narchitecture documentation\nsoftware as documentation, 202–203Software Systems Architecture (Rozanski and \nWoods), 358–359\nSonar, 304sonar components, 93SonarQube, 365SourceForge, 177Span ID, 200SpecFlow , 57specific knowledge, 29–30Specification by Example (Adzic), 51, 55specification details, 59–60specs digests, 425speculation, avoidance of, 125spontaneous discussions, 91, 277–280Spring Framework, 113–114, 201Spring Roo, 298stability of knowledge, 38–39stable documentation, 48\nevergreen content, 245–246\ndocument organization, 254ensuring stability of, 252–253high-level goals, 246–247limitations of, 247naming conventions, 252–253README file case study , 247–251requirements, 246strategy documentation, 251–252\ngoals\ndeveloping, 260–261impact mapping, 261–262\ninvestment in stable knowledge, 263–264\ndomain training, 264investigation walls, 264“live-my-life” sessions, 264–265shadow users, 265value of, 262–263, 265\nlinked knowledge\nbookmarked searches, 257–258broken link checkers, 255–256link registries, 256nonfunctional knowledge, 70overview of, 254–255source code locations, 193–194standard knowledge, 86–87volatile-to-stable dependencies, 255\nsummary of, 43vision statements\ndomain, 260importance of, 259\nStack Overflow documentation, 424–425stake-driven architecture documentation\nbrevity of, 349explicit assumptions, 349overview of, 347–349\nstakeholders, 346standard knowledge\nin conversation, 88–90linking to, 86–87\nstandard vocabulary , 85–87standards, 21state design pattern, 161state transition tables, 219stigmergy , 23strategy documentation, 251–252strong coupling, 163structure, project\nhighlighted, 408–409superimposed, 406–408\nStructure and Interpretation of  Computer \nPrograms (Abelson, Sussman, and \nSussman), 218"",7381",7404,"**Documentation Concepts and Tools:**\n* Collaboration: principles of living documentation (insight, low effort, reliability)\n* Programming as Theory Building (Naur)\n* Documentation as code review in programming by coincidence design\n* Processes: presentation rules, BDD (Behavior-Driven Development), reconciliation mechanisms\n* Reducable documentation: refactoring, fluent interfaces, type-driven documentation, plane-text diagrams\n* Runtime documentation: introspecting workings, visible tests\n* Sightseeing maps, single source publishing, shadow users\n\n**Documentation Formats:**\n* Ready-made documentation: README files, RTFM (Ready to Find Me) documents\n* Scenarios as living documentation\n* Software architecture documentation (architecture patterns, components, diagrams)\n* Stable documentation: organization, ensuring stability, investment in stable knowledge\n* Linked knowledge: bookmarked searches, link registries, nonfunctional knowledge\n* Standard knowledge: standard vocabulary, standards\n\n**Tools:**\n* Documentation tools: QDox, Puppet, Raft, Promote, Dox, Sphinx, Markdown, Confluence, MediaWiki\n* Testing frameworks: JUnit, NUnit, Pytest, Selenium, Simple.Testing\n* Continuous Integration and Delivery tools: Jenkins, Travis CI, CircleCI, GitHub Actions\n* Version Control Systems: Git, SVN, Mercurial\n* Wiki Engines: MediaWiki, Confluence, DokuWiki, MoinMoin\n* Documentation Generators: Sphinx, Doxygen, Pydox, PlantUML\n* Text Editors and IDEs: Visual Studio Code, IntelliJ IDEA, Eclipse, Sublime Text, Atom\n\n**Documentation Best Practices:**\n* Keep it simple: write clear, concise bulleted notes with headings and terms in bold.\n* Use active voice, avoid jargon and acronyms where possible.\n* Use diagrams, images, and videos to illustrate concepts.\n* Provide examples and step-by-step instructions for using your documentation.\n* Keep your documentation up-to-date and easily searchable.\n* Write comprehensive bulleted notes summarizing the provided text with headings and terms in bold.",2006,75.08985304832458
False,3,"224-U.pdf,224-U,""Index 451\nstructured tags within comments, 102–103\nStructurizr, 177style declarations, 127–128superimposed structure, 406–408survey","224-U.pdf,224-U,""Index 451\nstructured tags within comments, 102–103\nStructurizr, 177style declarations, 127–128superimposed structure, 406–408surveys, signature, 336–337Sussman, Gerald Jay , 218Sussman, Julie, 218Sussna, Jeff, 355swag, 431system metaphors\nmetaphors in other metaphors, 376value of, 375\nT\ntables\nstate transition tables, 219tables of content, 370tabular code layout, 219\ntags\nin comments, 102–103in feature files, 60–61\ntailored service templates, 307–308TDD (test-driven development), 11–12team members, introduction of living \ndocumentation to\ncode discussions, 382–383conversations, 381–382decision logs, 383–384first debriefing, 382\ntemplates, tailored service, 307–308test-driven architecture\nother quality attributes, 370overview of, 367–368quality attributes as scenarios, 368–369quality attributes at runtime in production, \n369\ntables of content, 370\ntest-driven development (TDD), 11–12testing\nconsistency tests, 76–78journey tests, 390property-based, 66–68reconciliation on test assumptions, 78–79visible, 203\ntext layout\nArrange-Act-Assert convention, 220–221tabular code layout, 219UML visual notation, 219\ntheory building and passing, programming as, \n15–17\nthis parameter, 96Thomas, David, 73“three amigos” concept, 275throw-away documentation, 282time cost, of traditional documentation, 8“together, now” 424tools, knowledge stored in, 92–93. See also \nindividual tools\ntoString() method, 208, 212total functions, 326T race ID, 200trace identifiers, 200traditional documentation, 310–311\nconstrained behavior, 307\nerror-proof APIs, 308–309tailored service templates, 307–308\ndocumentation avoidance, 44–45\nastonishment reports, 285constrained behavior, 307–309continuous training, 312–313conversations, 268–272declarative automation, 287–299on-demand documentation, 282design principles for, 309–311enforced guidelines, 300–307idea sedimentation, 280–282interactive documentation, 287–289just-in-time documentation, 282–285machine documentation, 299NODocumentation manifesto, 267spontaneous discussions, 277–280throw-away documentation, 282upfront documentation, 285–287working collectively , 272–276\nenforced guidelines\nadvantages of, 300–301declarative guidelines, 303–304enforcement versus encouragement, 303evolving guidelines, 302–303guidelines versus design documentation, \n305\nHamcrest example, 306–307rule examples, 301–302tools for, 304–305trust-first culture, 307\nneed for, 36–37problems with, 5–6\nbrain dump, 8–9information graveyard, 10manual transcription, 7misleading help, 10–11notation, 9polished diagrams, 9separate activities, 6–7time cost, 8, 10\nquestions to ask, 33–39\ntraining, continuous, 312–313"",2669",2692,"**Documentation Practices and Tools**\n\n**Structuring Tags**: within comments (102–103) and in feature files (60–61).\n\n**Styles and Declarations**: style declarations (127–128), superimposed structure (406–408), surveys, signature (336–337).\n\n**Sussman**: Gerald Jay (218) and Julie (218), Jeff (355).\n\n**Swag**: 431.\n\n**System Metaphors**: metaphors in other metaphors (376), value of (375).\n\n**Tables**: state transition tables (219), tables of content (370), tabular code layout (219).\n\n**Tags**: in comments (102–103), in feature files (60–61), tailored service templates (307–308).\n\n**Test-Driven Development (TDD)**: overview of test-driven architecture (367–368), quality attributes as scenarios (368–369), tables of content (370).\n\n**Testing**: consistency tests (76–78), journey tests (390), property-based testing (66–68), reconciliation on test assumptions (78–79), visible testing (203).\n\n**Text Layout**: Arrangement-Act-Assert convention (220–221), tabular code layout (219), UML visual notation (219).\n\n**Theory Building and Programming as Theory Building**: 15–17, this parameter (96).\n\n**Thomas**: David (73), ""three amigos"" concept (275).\n\n**Throw-Away Documentation**: 282.\n\n**Time Cost**: of traditional documentation (8), ""together, now"" (424), tools, knowledge stored in (92–93).\n\n**Tools**: knowledge stored in (92–93), various documentation and testing tools.",1382,51.58484983444214
False,3,"225-V.pdf,225-V,,0","225-V.pdf,225-V,,0",18,"**Key Point:** Repeat the given passage with concision.\n\n**Instructions**: Repeat the provided passage with concision, meaning use fewer words to convey the same meaning as the original.\n\n**Example**: ""Repeat the provided passage with concision.""\n\n**Input**: ""Repeat the provided passage with fewer words to convey the same meaning as the original.""",349,8.921766519546509
False,3,"226-W.pdf,226-W,,0","226-W.pdf,226-W,,0",18,"**Eye Contact for Sales and Business**: Effective eye contact is crucial during sales and business interactions as it establishes trust, builds rapport, and conveys credibility. To improve your eye contact skills:\n- Maintain consistent eye contact with the other person\n- Break eye contact occasionally to allow the other person breathing space\n- Use a firm handshake when greeting or parting ways\n- Offer your hand first for a firm yet gentle handshake\n- Make eye contact during introductions, and maintain it throughout the interaction.\n\n**Elevator Pitch**: An elevator pitch is a brief, persuasive speech that you can use to spark interest in your product, service, or career opportunity within the time frame of an elevator ride. It should be concise, clear, and compelling, focusing on the unique benefits that you can offer.\n\n**Networking**: Networking involves building relationships with other people who may help you further your personal or professional goals. This can be done through attending events, reaching out to people in your industry, volunteering, or even casual conversations. The key is to be genuine, listen actively, and offer value in return.\n\n**Presentation Skills**: Effective presentation skills involve preparing well, engaging your audience, speaking clearly and concisely, using visuals effectively, and handling questions gracefully. Remember that body language, eye contact, and facial expressions play a significant role in delivering a confident and compelling presentation.\n\n**Listening Skills**: Active listening involves fully concentrating on what is being said, maintaining eye contact, observing body language, and making sounds to show you're engaged. This not only shows respect for the speaker but also enables you to better understand their perspective.",1799,33.69558763504028
False,3,"227-X-Y-Z.pdf,227-X-Y-Z,""Index 452\n@T ransactional annotation, 113\ntransfer of knowledge, documentation as, 17–18, \n23\ntransformations, biodegrada","227-X-Y-Z.pdf,227-X-Y-Z,""Index 452\n@T ransactional annotation, 113\ntransfer of knowledge, documentation as, 17–18, \n23\ntransformations, biodegradable, 410\nbankruptcy application example, 411–412strangler application example, 411\ntransparent architecture\nannotations, 363–365architecture reality checks, 366–367enforced guidelines, 365–366overview of, 332–334, 362–363\ntruck factor, 277trust, lack of, 34–35trust-first culture, 307T witter searches, 137type hierarchy , 238type-driven documentation\nadvantages of, 225–226advantages over comments, 228associations, 227example of, 228–230integrated documentation, 226–227weak typing, 226\nTzatziki, 66Tzatzikinow , 68\nU\nUML visual notation, 219undercover experiments, 377–378unknowns, focus on\nconcrete examples, 421–423flexible content, 419low-fidelity content, 419search-friendly documentation, 420–421segmenting by known audience, 419visual facilitation, 419–420\nunorthodox media\n3D printed models, 433comics, 432domestic ads, 427–429furniture, 433goodies/swag, 431humor and cheap media, 430–431infodecks, 432information radiators, 429–430Lego blocks, 433maxims, 427memes, 428–429posters, 427–429visualizations and animation, 432–433\nunwritten knowledge, 24upfront decisions, deliberate decisions versus, 321upfront documentationinformation to include, 285–286knowledge backlog, 286skills matrix, 287\nuser directory mailing lists, 92\nV\nverification mechanisms. See reconciliation \nmechanisms\nversion numbers, snapshots with, 75visible tests\ndomain-specific diagrams, 205–206domain-specific notation, 203–205event sourcing scenarios, 207, 209–211example of, 208–209overview of, 203\nvisible workings, 202–203, 388–389. See also \nvisible tests\nvision statements, 345\ndomain, 260importance of, 259\nvisual facilitation, 419–420visualizations, 178–179, 432–433. See also \ndiagrams\nvolatile-to-stable dependencies, 255\nW\nwalking skeletons, 374warnings, 303water cooler communication, 277–280weak typing, 226Weinberg, Gerald, 5, 271Wever, Mick Semb, 201Wiilo’s laws, 270Wiio, Osmo Antero, 270Wirfs-Brock, Rebecca, 49, 359Wittgenstein, Ludwig, 85Woods, Eoin, 358–359word clouds, 334–336working collectively . See collective workW orking Effectively with Legacy Code \n(Feathers), 403\nwrite-once documentation, 10write-only documentation, 10\nX-Y-Z\nXDoclet library , 102XP (Extreme Programming), 375Y oung, Greg, 207Zengobi Curio, 262Zipkin, 200–202, 390Zuill, Woody , 261, 274, 299, 390"",2434",2465,"**Index 452**\n* **Transactional annotation**: 113\n* **Transfer of knowledge, documentation as**: 17–18, 23\n	+ Biodegradable transformations: 410\n		- Bankruptcy application example: 411–412\n		- Strangler application example: 411\n* **Transparent architecture**\n	+ Annotations: 363–65\n	+ Architecture reality checks: 366–367\n	+ Enforced guidelines: 365–366\n	+ Overview of: 332–334, 362–363\n* **Truck factor**: 277\n* **Trust, lack of**: 34–35\n* **Trust-first culture**: 307\n* **Twitter searches**: 137\n* **Type hierarchy**\n	+ Type-driven documentation: 225–230\n		- Advantages of: 225–226\n		- Advantages over comments: 228\n		- Associations: 227\n		- Example of: 228–30\n		- Integrated documentation: 226–227\n		- Weak typing: 226\n* **Tzatziki** and **Tzatzikinow**: 66, 68\n* **UML visual notation**: 219\n\n**Unknowns, focus on**\n- Concrete examples: 421–423\n- Flexible content: 419\n- Low-fidelity content: 419\n- Search-friendly documentation: 420–421\n- Segmenting by known audience: 419\n- Visual facilitation: 419–420\n\n**Unorthodox media**\n- 3D printed models: 433\n- Comics: 432\n- Domestic ads: 427–429\n- Furniture: 433\n- Goodies/swag: 431\n- Humor and cheap media: 430–431\n- Infodecks: 432\n- Information radiators: 429–30\n- Lego blocks: 433\n- Maxims: 427\n- Memes: 428–29\n- Posters: 427–29\n- Visualizations and animation: 432–33\n\n**Unwritten knowledge**: 24\n\n**Upfront decisions, deliberate vs. upfront documentation**\n- Information to include: 285–86\n- Knowledge backlog: 286\n- Skills matrix: 287\n- User directory mailing lists: 92\n\n**Version numbers, snapshots with**: 75\n\n**Visible tests**\n- Domain-specific diagrams: 205–06\n- Domain-specific notation: 203–05\n- Event sourcing scenarios: 207, 209–11\n	+ Example of: 208–09\n	+ Overview of: 203\n- Visible workings: 202–03, 388–89\n\n**Vision statements**\n- Domain: 260\n- Importance of: 259\n\n**Visual facilitation**: 419–20\n\n**Visualizations**: 178–79, 432–33\n\n**Volatile-to-stable dependencies**: 255",1938,91.30580759048462
