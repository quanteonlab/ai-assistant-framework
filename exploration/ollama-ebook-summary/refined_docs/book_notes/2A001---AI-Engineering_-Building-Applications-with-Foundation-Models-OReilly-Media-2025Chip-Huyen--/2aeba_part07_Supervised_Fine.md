# High-Quality Flashcards: 2A001---AI-Engineering_-Building-Applications-with-Foundation-Models-OReilly-Media-2025Chip-Huyen--_processed (Part 7)

**Rating threshold:** >= 8/10

**Starting Chapter:** Supervised Finetuning

---

**Rating: 8/10**

#### Types of Requests and Responses

Background context: Different types of requests require different responses. The text mentions that demonstration data should cover a range of tasks such as question answering, summarization, and translation to ensure the model can handle various types of user inputs appropriately.

:p What is the importance of demonstrating different types of requests for fine-tuning a model?
??x
Demonstrating different types of requests helps the model understand how to respond to varying input scenarios. By providing examples that cover multiple task types, you enable the model to learn the appropriate responses for each context, ensuring it can handle diverse user interactions effectively.

```java
// Example of preparing demonstration data
public class PrepareDataExample {
    public static void main(String[] args) {
        List<Pair<String, String>> demoData = new ArrayList<>();
        // Question answering task
        demoData.add(new Pair<>("What is the capital of France?", "Paris"));
        
        // Summarization task
        demoData.add(new Pair("Generate a summary for this article.", "The article discusses ..."));
        
        // Translation task
        demoData.add(new Pair("Translate 'Hello, how are you?' to Spanish.", "Hola, ¿cómo estás?"));
    }
}
```
x??

---

**Rating: 8/10**

#### RLHF (Reward Model-based Fine-tuning)
Background context: The text discusses the use of Reward Model-based Fine-tuning (RLHF) as a method for training language models. It involves training a reward model to score responses generated by a foundation model, followed by optimizing the foundation model based on these scores.
:p What is RLHF and how does it work?
??x
RLHF stands for Reward Model-based Fine-tuning. It consists of two main steps: first, a reward model is trained to score the outputs of a foundation model; second, the foundation model is optimized to generate responses that maximize the rewards from the reward model.

Here's a simplified pseudocode example:
```pseudocode
// Step 1: Train Reward Model
TrainRewardModel(prompt, response) -> Score

// Step 2: Optimize Foundation Model
OptimizeFoundationModel(prompt, response, reward_model) {
    while (not converged) {
        GenerateResponses(prompt, foundation_model)
        for each response in responses {
            Score = reward_model(prompt, response)
            AdjustWeights(foundation_model, response, Score)
        }
    }
}
```
x??

---

**Rating: 8/10**

#### Role of the Reward Model in Fine-Tuning
Background context: The trained reward model is used to further train the SFT model. Prompts are randomly selected and input into the model, whose responses are scored by the reward model.

:p How does InstructGPT use the trained reward model for fine-tuning?
??x
InstructGPT uses the trained reward model to score the output responses generated by the SFT model during the fine-tuning process. Random prompts are selected and input into the model, whose responses are then scored by the reward model.

This process often employs PPO (Proximal Policy Optimization) as a reinforcement learning algorithm.
x??

---

---

**Rating: 8/10**

#### Sampling Fundamentals
Background context explaining the concept. A neural network produces an output by computing the probabilities of possible outcomes for a given input. These probabilities are used to make decisions or generate outputs probabilistically.

For classification models, this means calculating the probability of each class and making decisions based on these probabilities. For language models, it involves generating tokens based on their probability distribution in the vocabulary.
:p What is greedy sampling?
??x
Greedy sampling always picks the outcome with the highest probability. This approach works well for classification tasks where choosing the most likely outcome makes logical sense, such as marking an email as spam if its probability of being spam is higher than not spam.

For a language model, however, this can lead to uninteresting and repetitive outputs because the model would always select the most common words or phrases.
??x

---

**Rating: 8/10**

#### Language Model Sampling
Explanation of how language models generate tokens based on the probability distribution over all possible tokens in their vocabulary. The model computes logits for each token and uses these to sample the next token.

The process involves transforming logit values into a probability distribution, often through a softmax function, which converts raw scores (logits) into probabilities.
:p How does a language model decide the next token?
??x
A language model decides the next token by sampling from the probability distribution over all possible tokens in its vocabulary. This is done after computing logits for each token, which are then transformed into probabilities using a softmax function.

The logic can be represented as follows:
```java
public class TokenSampler {
    private final List<Double> logits;
    
    public TokenSampler(List<Double> logits) {
        this.logits = logits;
    }
    
    public int sampleNextToken() {
        // Apply softmax to get probability distribution over tokens
        List<Double> probabilities = applySoftmax(logits);
        
        // Sample the next token based on these probabilities
        return sampleFromDistribution(probabilities);
    }
    
    private List<Double> applySoftmax(List<Double> logits) {
        double sumExp = 0;
        for (double logit : logits) {
            sumExp += Math.exp(logit);
        }
        
        List<Double> probabilities = new ArrayList<>();
        for (double logit : logits) {
            probabilities.add(Math.exp(logit) / sumExp);
        }
        return probabilities;
    }
    
    private int sampleFromDistribution(List<Double> probabilities) {
        Random random = new Random();
        double cumulativeProbability = 0.0;
        int sampledTokenIndex = -1;
        
        for (int i = 0; i < probabilities.size(); i++) {
            cumulativeProbability += probabilities.get(i);
            if (cumulativeProbability > random.nextDouble()) {
                sampledTokenIndex = i;
                break;
            }
        }
        return sampledTokenIndex;
    }
}
```
x??

---

