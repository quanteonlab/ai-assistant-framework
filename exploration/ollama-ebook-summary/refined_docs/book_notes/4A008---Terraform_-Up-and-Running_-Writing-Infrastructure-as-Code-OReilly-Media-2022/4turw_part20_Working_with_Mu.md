# High-Quality Flashcards: 4A008---Terraform_-Up-and-Running_-Writing-Infrastructure-as-Code-OReilly-Media-2022_processed (Part 20)

**Rating threshold:** >= 8/10

**Starting Chapter:** Working with Multiple Copies of the Same Provider

---

**Rating: 8/10**

#### Version Constraint Explanation
Background context: In Terraform, version constraints are used to specify which versions of providers you want to use. This ensures that your infrastructure changes based on the specific provider versions you define.

:p What is a version constraint in Terraform?
??x
A version constraint in Terraform allows you to specify the exact version or range of versions for a particular provider, ensuring consistency and compatibility with your infrastructure code.
x??

---

**Rating: 8/10**

#### Required Providers Block Usage
Background context: The `required_providers` block in your Terraform configuration file is essential for specifying which providers your code needs and their desired versions. If you don't include this block, Terraform will automatically try to install the latest version of the provider from the public registry.

:p What does the `required_providers` block do?
??x
The `required_providers` block in Terraform is used to specify which providers are required by your configuration and what versions of these providers you want to use. This helps in maintaining consistency and ensuring that specific versions of providers are used during the execution of Terraform commands.
x??

---

**Rating: 8/10**

#### Custom Provider Installation
Background context: If you want to use a provider not in the `hashicorp` namespace or control the specific version, you need to include a `required_providers` block.

:p How do you handle custom providers with Terraform?
??x
To handle custom providers or specify versions of non-hashicorp providers, you must include a `required_providers` block in your configuration. This block specifies the source and version (if needed) for the provider, allowing you to control which specific version is installed.
x??

---

**Rating: 8/10**

#### Always Include required_providers Block
Background context: Including a `required_providers` block ensures that you always specify the correct versions of providers used in your Terraform configurations.

:p Why is it important to always include a `required_providers` block?
??x
It is important to always include a `required_providers` block because it allows you to explicitly control which version of each provider will be used. This ensures consistency, reproducibility, and compatibility across different executions of your Terraform configurations.
x??

---

---

**Rating: 8/10**

#### Configuring AWS Provider in Terraform
Background context: When working with cloud providers like AWS in Terraform, you need to configure a provider block. This configuration typically includes essential settings such as region and credentials. The documentation for these configurations is usually available on the same registry from which the provider is downloaded.

:p What does configuring an AWS provider include?
??x
Configuring an AWS provider involves setting up necessary parameters like `region`, `version`, and sometimes `credentials`. You can check detailed configuration options in the Terraform Registry associated with the provider. For example, you might set the region as follows:

```hcl
provider "aws" {
  region = "us-east-2"
}
```

x??

---

**Rating: 8/10**

#### Multiple Provider Configurations for Different Regions
Background context: Sometimes, a single provider block is not sufficient to handle resources that need to be deployed in different regions. This requires configuring multiple copies of the same provider with unique aliases.

:p How do you configure AWS providers for multiple regions?
??x
To configure AWS providers for multiple regions, use alias names to differentiate them:

```hcl
provider "aws" {
  region = "us-east-2"
  alias  = "region_1"
}

provider "aws" {
  region = "us-west-1"
  alias  = "region_2"
}
```

Then, you can use these provider aliases to specify which configuration each resource or data source should use:

```hcl
data "aws_region" "region_1" {
  provider = aws.region_1
}

data "aws_region" "region_2" {
  provider = aws.region_2
}
```

x??

---

**Rating: 8/10**

#### Using Different Providers for Resources Across Regions
Background context: This concept explains how to deploy resources (in this case, AWS EC2 instances) into different regions using Terraform. The main idea is to use the `provider` parameter to specify the region and ensure that each resource's specific requirements, such as AMI IDs, are correctly managed.
:p How can you deploy two EC2 instances in different regions while ensuring they use the correct AMI ID?
??x
To deploy two EC2 instances in different regions using Terraform, you need to define a provider for each region and ensure that the `ami` parameter is set appropriately. You should also use the `aws_ami` data source to automatically find the correct AMI ID based on the desired operating system.

Here’s how to achieve this:

1. Define the `provider` block for each region.
2. Use the `aws_ami` data source to look up the correct AMI ID for each region.
3. Set the `ami` parameter of the `aws_instance` resource using the output from the `aws_ami` data source.

Example Terraform code:

```hcl
provider "aws" {
  region = "us-east-2"
}

resource "aws_instance" "region_1" {
  provider      = aws.region_1
  ami           = data.aws_ami.ubuntu_region_1.id
  instance_type = "t2.micro"
}

data "aws_ami" "ubuntu_region_1" {
  provider  = aws.region_1
  most_recent  = true
  owners       = ["099720109477"] # Canonical
  filter {
    name    = "name"
    values  = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

provider "aws" {
  region = "us-west-1"
}

resource "aws_instance" "region_2" {
  provider      = aws.region_2
  ami           = data.aws_ami.ubuntu_region_2.id
  instance_type = "t2.micro"
}

data "aws_ami" "ubuntu_region_2" {
  provider  = aws.region_2
  most_recent  = true
  owners       = ["099720109477"] # Canonical
  filter {
    name    = "name"
    values  = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}
```

By using the `provider` parameter and the `aws_ami` data source, you ensure that each instance is deployed into the correct region with the appropriate AMI ID.

???
x??

---

**Rating: 8/10**

#### Deploying Resources with Correct AMI ID Across Regions
Background context: When deploying resources like EC2 instances across different AWS regions, it's crucial to use the correct AMI ID for that specific region. Using a hardcoded value can lead to errors if the region-specific information is not correctly managed.
:p How do you ensure that each AWS instance uses the correct AMI ID in different regions?
??x
To ensure that each AWS instance uses the correct AMI ID in different regions, you should use the `aws_ami` data source within Terraform. This data source allows you to look up the most recent AMI ID for a specific operating system by specifying filters.

Here’s how you can do it:

1. Define a provider block for each region.
2. Use the `aws_ami` data source with appropriate filters to find the correct AMI ID.
3. Set the `ami` parameter of the `aws_instance` resource using the output from the `aws_ami` data source.

Example Terraform code:

```hcl
provider "aws" {
  region = "us-east-2"
}

data "aws_ami" "ubuntu_region_1" {
  provider  = aws.region_1
  most_recent  = true
  owners       = ["099720109477"] # Canonical
  filter {
    name    = "name"
    values  = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

resource "aws_instance" "region_1" {
  provider      = aws.region_1
  ami           = data.aws_ami.ubuntu_region_1.id
  instance_type = "t2.micro"
}

provider "aws" {
  region = "us-west-1"
}

data "aws_ami" "ubuntu_region_2" {
  provider  = aws.region_2
  most_recent  = true
  owners       = ["099720109477"] # Canonical
  filter {
    name    = "name"
    values  = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

resource "aws_instance" "region_2" {
  provider      = aws.region_2
  ami           = data.aws_ami.ubuntu_region_2.id
  instance_type = "t2.micro"
}
```

By using the `aws_ami` data source, you ensure that each instance is deployed with the correct AMI ID specific to its region.

???
x??

---

**Rating: 8/10**

#### Managing Multiple Copies of the Same Provider in Terraform
Background context: When working with multiple regions, it's common to need to use the same provider type (e.g., AWS) but with different regions. This requires defining multiple `provider` blocks and ensuring that each resource or data source is associated with the correct region.
:p How can you manage multiple copies of the same provider in Terraform for deploying resources into different regions?
??x
To manage multiple copies of the same provider (e.g., AWS) in Terraform for deploying resources into different regions, you need to define a separate `provider` block for each region. Each `provider` block specifies the region and can be used by associated resources or data sources.

Here’s an example:

1. Define a `provider` block for each region.
2. Use the correct `provider` in each resource or data source that needs to operate in that specific region.

Example Terraform code:

```hcl
provider "aws" {
  region = "us-east-2"
}

resource "aws_instance" "region_1" {
  provider      = aws.region_1
  ami           = "ami-0fb653ca2d3203ac1"
  instance_type = "t2.micro"
}

provider "aws" {
  region = "us-west-1"
}

resource "aws_instance" "region_2" {
  provider      = aws.region_2
  ami           = "ami-01f87c43e618bf8f0"
  instance_type = "t2.micro"
}
```

By defining multiple `provider` blocks, you can deploy resources into different regions while ensuring that each resource uses the correct configuration.

???
x??

---

---

**Rating: 8/10**

#### Module Creation for MySQL Replication
Background context: The provided text explains how to create a reusable module for deploying a MySQL database with replication capabilities using Terraform. This is crucial for ensuring high availability and scalability, as a single point of failure can be mitigated by having a secondary read-only replica.
:p What are the key steps involved in creating a reusable MySQL module that supports replication?
??x
The key steps involve organizing the existing code into a new module folder, exposing necessary variables to control the primary and replica configurations, updating resource definitions to handle conditional logic based on whether the instance is a replica or not. This modular approach allows for easier management of database instances in both staging and production environments.

For example:
```hcl
# Step 1: Organize code into module folder
- Copy stage/data-stores/mysql content to modules/data-stores/mysql

# Step 2: Expose variables in variables.tf
variable "backup_retention_period" {
   description = "Days to retain backups. Must be > 0 to enable replication."
   type        = number
   default      = null
}
variable "replicate_source_db" {
   description = "If specified, replicate the RDS database at the given ARN."
   type        = string
   default      = null
}

# Step 3: Update main.tf to conditionally set parameters
resource "aws_db_instance" "example" {
   identifier_prefix    = "terraform-up-and-running"
   allocated_storage    = 10
   instance_class       = "db.t2.micro"
   skip_final_snapshot  = true

   # Enable backups
   backup_retention_period  = var.backup_retention_period

   # If specified, this DB will be a replica
   replicate_source_db  = var.replicate_source_db

   # Only set these params if replicate_source_db is not set
   engine    = var.replicate_source_db == null ? "mysql" : null
   db_name   = var.replicate_source_db == null ? var.db_name : null
   username  = var.replicate_source_db == null ? var.db_username : null
   password  = var.replicate_source_db == null ? var.db_password : null
}
```
x??

---

**Rating: 8/10**

#### Conditional Logic in Module Definition
Background context: The text provides an example of using conditional logic within a Terraform resource to handle different configurations based on whether the database instance is meant to be a primary or a replica.
:p How does the conditional logic work for setting parameters in the AWS RDS `aws_db_instance` resource?
??x
The conditional logic checks if the `replicate_source_db` variable is set. If it is not (`null`), then the normal settings (like `engine`, `db_name`, `username`, and `password`) are applied. Otherwise, these parameters are omitted because AWS does not allow setting them for replicas.
For example:
```hcl
resource "aws_db_instance" "example" {
   identifier_prefix    = "terraform-up-and-running"
   allocated_storage    = 10
   instance_class       = "db.t2.micro"
   skip_final_snapshot  = true

   # Enable backups
   backup_retention_period  = var.backup_retention_period

   # If specified, this DB will be a replica
   replicate_source_db  = var.replicate_source_db

   # Only set these params if replicate_source_db is not set
   engine    = var.replicate_source_db == null ? "mysql" : null
   db_name   = var.replicate_source_db == null ? var.db_name : null
   username  = var.replicate_source_db == null ? var.db_username : null
   password  = var.replicate_source_db == null ? var.db_password : null
}
```
In this code, if `replicate_source_db` is not set (null), the resource will have the normal settings for a primary database. If it is set, then these parameters are omitted.

```hcl
# Example of conditional logic in action
if replicate_source_db == null {
   # Primary: Set all required fields
   engine = "mysql"
   db_name = var.db_name
   username = var.db_username
   password = var.db_password
} else {
   # Replica: Do not set these fields
   // Fields are left as null, meaning the replica will inherit from the source DB.
}
```
x??

---

**Rating: 8/10**

#### Variables for Database Module
Background context: The text highlights how to define and use variables in a Terraform module to control database configurations such as backup retention period and replication settings. These variables make the module reusable across different environments (staging, production) with varying needs.
:p What changes are made to the `variables.tf` file to support replication?
??x
The `variables.tf` file is updated to expose two new variables: `backup_retention_period` for enabling backups and specifying how long to retain them, and `replicate_source_db` for configuring a replica. The default values are set to null, indicating that these settings are optional.

Example:
```hcl
# Exposing backup retention period variable
variable "backup_retention_period" {
   description = "Days to retain backups. Must be > 0 to enable replication."
   type        = number
   default      = null
}

# Exposing replicate source database ARN for replica configuration
variable "replicate_source_db" {
   description = "If specified, replicate the RDS database at the given ARN."
   type        = string
   default      = null
}
```
By making these variables optional, the module can be used flexibly in both primary and secondary configurations.

```hcl
# Example of defining variables
variable "backup_retention_period" {}
variable "replicate_source_db" {}
```
x??

---

**Rating: 8/10**

#### Outputs for Module
Background context: The text emphasizes the importance of outputs to provide useful information about deployed resources. In this case, an output is added to provide the ARN (Amazon Resource Name) of the database, which can be used in other configurations or for monitoring purposes.
:p How does the `outputs.tf` file support replication configuration?
??x
The `outputs.tf` file includes an output block that provides the ARN of the database. This ARN is useful for referencing the deployed database instance from other Terraform configurations.

Example:
```hcl
# Adding output to provide the ARN of the database
output "arn" {
   value       = aws_db_instance.example.arn
   description = "The ARN of the database"
}
```
With this addition, you can easily reference the deployed database's ARN in other modules or configurations.

```hcl
# Example output definition
output "arn" {}
```
x??

---

**Rating: 8/10**

#### Required Providers Block
Background context: The text explains that adding a `required_providers` block is necessary to specify which provider and version should be used with this module. This ensures consistency across different environments where the module might be applied.
:p What does the `required_providers` block in the module do?
??x
The `required_providers` block specifies that the module requires a specific AWS Provider and its version. This is important to ensure compatibility and correctness when applying the module in different Terraform configurations.

Example:
```hcl
# Adding required_providers block for AWS Provider
required_providers {
   aws = {
      source  = "hashicorp/aws"
      version = "~> 3.0" # Ensure the correct version is used
   }
}
```
This block tells Terraform which provider to use and ensures that it uses a compatible version.

```hcl
# Example of required_providers block
required_providers {
   aws = {
      source  = "hashicorp/aws"
      version = "~> 3.0"
   }
}
```
x??

---

---

**Rating: 8/10**

#### Introduction to Multiple Provider Usage in Terraform

Terraform is a powerful tool for infrastructure as code, allowing developers and engineers to manage resources across different cloud providers. However, managing resources within the same provider but across multiple regions can be challenging.

Background context: In this scenario, we are working with a MySQL primary database in one region (us-east-2) and its replica in another region (us-west-1). We need to ensure that both databases are managed using Terraform while being deployed in different regions.

:p What is the purpose of creating multiple provider blocks in Terraform?
??x
The purpose of creating multiple provider blocks in Terraform is to manage resources across different regions. Each provider block can have its own set of configuration parameters, such as the region and other settings, allowing us to deploy resources in separate regions using the same Terraform code.

Example:
```terraform
provider "aws" {
  region = "us-east-2"
  alias  = "primary"
}

provider "aws" {
  region = "us-west-1"
  alias  = "replica"
}
```

x??

---

**Rating: 8/10**

#### Configuring Modules for Different Providers

In Terraform, modules are reusable pieces of code that encapsulate a set of resources and their configurations. We can use modules to manage complex infrastructure structures, such as deploying multiple databases with replication.

Background context: In this example, we have created two modules, `mysql_primary` and `mysql_replica`, each responsible for setting up a MySQL database in different regions. To ensure that these databases are deployed correctly, we need to specify which provider each module should use.

:p How do you configure a module to use a specific provider in Terraform?
??x
To configure a module to use a specific provider in Terraform, you set the `providers` parameter within the module block. This parameter is a map that maps the local name of the provider (from the `required_providers` section) to the actual provider configuration.

Example:
```terraform
module "mysql_primary" {
  source      = "../../../../modules/data-stores/mysql"
  providers   = { aws = aws.primary }
  db_name     = "prod_db"
  db_username = var.db_username
  db_password = var.db_password
  backup_retention_period = 1
}
```

x??

---

**Rating: 8/10**

#### Differentiating Multiple Provider Usage

Multiple provider usage allows deploying resources across different regions or environments using a single Terraform configuration file. This is achieved by defining multiple provider blocks with unique aliases and specifying which provider to use when creating resources.

Background context: In the given example, we are deploying a MySQL primary database in `us-east-2` and its replica in `us-west-1`. Each module uses a different provider block to ensure the correct region is targeted for deployment.

:p How do you differentiate between multiple provider blocks in Terraform?
??x
You differentiate between multiple provider blocks by giving each an alias and using that alias when specifying which provider should be used within a module or resource. This allows you to manage resources across different regions while keeping your Terraform configuration clean and reusable.

Example:
```terraform
provider "aws" {
  region = "us-east-2"
  alias  = "primary"
}

provider "aws" {
  region = "us-west-1"
  alias  = "replica"
}
```

x??

---

---

**Rating: 8/10**

---
#### Creating Outputs for MySQL Module
This section explains how to define outputs in Terraform that will provide information about your deployed MySQL primary and replica instances. Outputs are useful for capturing deployment details such as connection endpoints, ports, and ARNs which can be used for further automation or manual verification.

:p How do you create the necessary output variables in `outputs.tf` for a MySQL module with both primary and replica configurations?
??x
To create the required output variables, you need to define them within the `outputs.tf` file of your Terraform configuration. Here's an example of how it should be structured:

```hcl
output "primary_address" {
  value       = module.mysql_primary.address
  description = "Connect to the primary database at this endpoint"
}

output "primary_port" {
  value       = module.mysql_primary.port
  description = "The port the primary database is listening on"
}

output "primary_arn" {
  value       = module.mysql_primary.arn
  description = "The ARN of the primary database"
}

output "replica_address" {
  value       = module.mysql_replica.address
  description = "Connect to the replica database at this endpoint"
}

output "replica_port" {
  value       = module.mysql_replica.port
  description = "The port the replica database is listening on"
}

output "replica_arn" {
  value       = module.mysql_replica.arn
  description = "The ARN of the replica database"
}
```

This configuration provides clear and descriptive outputs that can be used to verify and utilize the deployed resources.
x??

---

**Rating: 8/10**

#### Multiregion Deployments - Challenges
Deploying infrastructure across multiple regions can be challenging due to various technical and regulatory issues. These challenges include dealing with latency between regions, deciding on writer policies (which affect availability), generating unique IDs, and complying with local data regulations.

:p What are some of the key challenges in multiregion deployments according to the provided text?
??x
Some key challenges in multiregion deployments include:

1. **Latency**: Managing data latency between regions can impact performance.
2. **Writer Policies**: Deciding whether to have one writer (reducing availability but improving consistency) or multiple writers (eventual consistency or sharding).
3. **Unique IDs**: Standard auto-increment ID generation might not work as expected across regions, requiring custom solutions for generating unique IDs.
4. **Local Data Regulations**: Ensuring compliance with local data protection and privacy laws.

These challenges require careful planning and often involve complex trade-offs between performance, availability, consistency, and regulatory compliance.
x??

---

**Rating: 8/10**

#### Prudent Use of Aliases
While Terraform's provider aliases make it easy to use multiple regions in a deployment, overusing them can complicate your infrastructure definitions. It’s important to use aliases judiciously.

:p Why should you be cautious about using too many provider aliases?
??x
Using too many provider aliases can lead to overly complex and harder-to-maintain Terraform configurations. While it is convenient to use aliases for different regions, overuse can result in:

1. **Complexity**: Increased complexity in your configuration files.
2. **Maintenance Issues**: Difficulties in maintaining and updating multiple regions.
3. **Redundant Code**: Repetitive code across similar resource definitions.

Therefore, it’s advisable to use provider aliases sparingly and only where necessary for the specific needs of your infrastructure setup.
x??

---

---

**Rating: 8/10**

#### Multiregion Infrastructure Resilience
Background context: Setting up multiregion infrastructure is crucial for ensuring resilience against outages. Typically, you deploy your application and its associated resources across multiple AWS regions to ensure that if one region goes down, others can still function. However, managing this setup with a single Terraform module using aliases poses risks during outages.
:p Why should environments be kept isolated when managing multiregion infrastructure?
??x
When environments are kept isolated in separate modules for each region, it minimizes the blast radius of errors and outages. This means that if something goes wrong in one region (e.g., a mistake or an outage), it is less likely to impact another region.
x??

---

**Rating: 8/10**

#### Managing Resources Across Multiple Regions
Background context: Some resources in AWS need to be deployed in every region you use, like GuardDuty for automated threat detection. Using separate modules for each region allows for better isolation and management of these resources.
:p How does managing resources across multiple regions work with Terraform?
??x
Managing resources across multiple regions works by creating a separate module for each region. This ensures that resources are deployed in every required region, maintaining consistency and adhering to AWS recommendations such as deploying GuardDuty in all regions.
x??

---

**Rating: 8/10**

#### Isolation (Compartmentalization)
Background context explaining the concept. Isolation helps separate different environments to limit the impact of failures or unauthorized access. For example, staging and production environments should be isolated to prevent accidental changes from affecting live systems.

:p How does isolation between AWS accounts help in managing risks?
??x
Isolation between AWS accounts is crucial for limiting the blast radius when something goes wrong. By keeping your staging environment separate from the production environment, any security breaches or misconfigurations in the staging account do not affect the production environment directly. This ensures that if an attacker gains access to the staging account, they have no direct access to the production environment.

For example:
- Staging: Used for testing and development.
- Production: Hosts live services and should be kept isolated from staging to prevent accidental changes or data leaks.

??x

---

**Rating: 8/10**

#### Cross-Account Authentication Mechanisms
Background context explaining how cross-account authentication works in AWS, specifically using IAM roles. It allows different accounts to interact securely without sharing credentials directly.

:p How can you authenticate across AWS accounts?
??x
In AWS, cross-account authentication is facilitated through the use of IAM roles. An IAM role in one account can be assumed by a user or an entity from another account, allowing secure interaction between them. This mechanism helps manage permissions and avoid credential management complexities.

Example: A developer in the `stage-account` needs to access resources in the `prod-account`.

```java
// Example code snippet for assuming an IAM role
import com.amazonaws.servicesSTS.AWSSecurityTokenServiceClient;
import com.amazonaws.servicesSTS.model.AssumeRoleRequest;
import com.amazonaws.servicesSTS.model.Credentials;

public class CrossAccountAuthExample {
    public static void main(String[] args) {
        AWSSecurityTokenServiceClient sts = new AWSSecurityTokenServiceClient();
        
        AssumeRoleRequest assumeRoleRequest = new AssumeRoleRequest()
                .withRoleArn("arn:aws:iam::123456789012:role/ExampleRole")
                .withRoleSessionName("ExampleSession");

        Credentials credentials = sts.assumeRole(assumeRoleRequest).getCredentials();
        
        // Use the assumed role's credentials to access other services
    }
}
```

This code demonstrates assuming a role from one AWS account and using those temporary credentials in another.

??x

---

**Rating: 8/10**

#### AWS Organizations for Multi-Account Management
AWS Organizations allows you to create and manage multiple AWS accounts from a single console. This is useful for organizing and managing resources across different environments or teams, while keeping costs transparent through consolidated billing.

:p What are the primary benefits of using AWS Organizations?
??x
The primary benefits include centralized management, cost transparency via consolidated billing, and ease of creating and managing multiple AWS accounts.
x??

---

**Rating: 8/10**

#### Terraform Configuration for Multiple AWS Accounts
Background context: This concept covers how to configure multiple AWS accounts using Terraform by adding provider blocks with different aliases and assume_role blocks. The goal is to authenticate to the child account via an IAM role.

:p How do you set up providers in Terraform to work with multiple AWS accounts?
??x
To set up providers in Terraform for working with multiple AWS accounts, follow these steps:

1. Define a provider block for the parent AWS account:
```hcl
provider "aws" {
  region = "us-east-2"
  alias  = "parent"
}
```

2. Define another provider block for the child AWS account and add an assume_role block:
```hcl
provider "aws" {
  region    = "us-east-2"
  alias     = "child"
  assume_role {
    role_arn = "arn:aws:iam::123456789012:role/OrganizationAccountAccessRole"
  }
}
```

Example of Terraform configuration:
```hcl
provider "aws" {
  region      = "us-east-2"
  alias       = "parent"
}

provider "aws" {
  region      = "us-east-2"
  alias       = "child"
  assume_role { role_arn = "arn:aws:iam::123456789012:role/OrganizationAccountAccessRole" }
}
```

x??

---

