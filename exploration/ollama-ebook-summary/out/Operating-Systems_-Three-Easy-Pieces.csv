filename,title,text,len
01-Introduction.pdf,01-Introduction,,0
02-Preface.pdf,02-Preface,"Preface\nTo Everyone\nWelcome to this book! We hope you’ll enjoy reading it as much as we e njoyed\nwriting it. The book is called Operating Systems: Three Easy Pieces (available\nathttp://www.ostep.org ), and the title is obviously an homage to one of the\ngreatest sets of lecture notes ever created, by one Richard Fe ynman on the topic of\nPhysics [F96]. While this book will undoubtedly fall short of t he high standard set\nby that famous physicist, perhaps it will be good enough for you i n your quest to\nunderstand what operating systems (and more generally, systems ) are all about.\nThe three easy pieces refer to the three major thematic elements the book is\norganized around: virtualization ,concurrency ,a n d persistence . In discussing\nthese concepts, we’ll end up discussing most of the important thi ngs an operating\nsystem does; hopefully, you’ll also have some fun along the way. L earning new\nthings is fun, right? At least, it should be.\nEach major concept is divided into a set of chapters, most of whi ch present a\nparticular problem and then show how to solve it. The chapters are short, and try\n(as best as possible) to reference the source material where th e ideas really came\nfrom. One of our goals in writing this book is to make the paths of h istory as clear\nas possible, as we think that helps a student understand what is ,w h a tw a s ,a n d\nwhat will be more clearly. In this case, seeing how the sausage w as made is nearly\nas important as understanding what the sausage is good for1.\nThere are a couple devices we use throughout the book which are pro bably\nworth introducing here. The ﬁrst is the crux of the problem. Anytime we are\ntrying to solve a problem, we ﬁrst try to state what the most impor tant issue is;\nsuch a crux of the problem is explicitly called out in the text, and hopefully solved\nvia the techniques, algorithms, and ideas presented in the res t of the text.\nIn many places, we’ll explain how a system works by showing its b ehavior\nover time. These timelines are at the essence of understanding; if you know what\nhappens, for example, when a process page faults, you are on your w ay to truly\nunderstanding how virtual memory operates. If you comprehend wha tt a k e sp l a c e\nwhen a journaling ﬁle system writes a block to disk, you have ta ken the ﬁrst steps\ntowards mastery of storage systems.\nThere are also numerous asides and tipsthroughout the text, adding a little\ncolor to the mainline presentation. Asides tend to discuss somet hing relevant (but\nperhaps not essential) to the main text; tips tend to be general lessons that can be\n1Hint: eating! Or if you’re a vegetarian, running away from.\niii\niv\napplied to systems you build. An index at the end of the book lists all of these tips\nand asides (as well as cruces, the odd plural of crux) for your conve nience.\nWe use one of the oldest didactic methods, the dialogue , throughout the book,\nas a way of presenting some of the material in a different light. These are used to\nintroduce the major thematic concepts (in a peachy way, as we wil ls e e ) ,a sw e l la s\nto review material every now and then. They are also a chance to write in a more\nhumorous style. Whether you ﬁnd them useful, or humorous, well, that’ sa n o t h e r\nmatter entirely.\nAt the beginning of each major section, we’ll ﬁrst present an abstraction that an\noperating system provides, and then work in subsequent chapte rs on the mecha-\nnisms, policies, and other support needed to provide the abstr action. Abstractions\nare fundamental to all aspects of Computer Science, so it is perha ps no surprise\nthat they are also essential in operating systems.\nThroughout the chapters, we try to use real code (not pseudocode )w h e r ep o s -\nsible, so for virtually all examples, you should be able to type th em up yourself and\nrun them. Running real code on real systems is the best way to learn about operat-\ning systems, so we encourage you to do so when you can. We are also making code\navailable at https://github.com/remzi-arpacidusseau/ostep-code for\nyour viewing pleasure.\nIn various parts of the text, we have sprinkled in a few homeworks to ensure\nthat you are understanding what is going on. Many of these homew orks are little\nsimulations of pieces of the operating system; you should download the home-\nworks, and run them to quiz yourself. The homework simulators have t he follow-\ning feature: by giving them a different random seed, you can ge nerate a virtually\ninﬁnite set of problems; the simulators can also be told to solve the problems for\nyou. Thus, you can test and re-test yourself until you have achiev ed a good level\nof understanding.\nThe most important addendum to this book is a set of projects in which you\nlearn about how real systems work by designing, implementing, an d testing your\nown code. All projects (as well as the code examples, mentioned ab ove) are in\ntheCp r o g r a m m i n gl a n g u a g e [KR88]; C is a simple and powerful language that\nunderlies most operating systems, and thus worth adding to your to ol-chest of\nlanguages. Two types of projects are available (see the onlin ea p p e n d i xf o ri d e a s ) .\nThe ﬁrst are systems programming projects; these projects are great for those who\nare new to C and U NIXand want to learn how to do low-level C programming.\nThe second type are based on a real operating system kernel de veloped at MIT\ncalled xv6 [CK+08]; these projects are great for students that already have some C\nand want to get their hands dirty inside the OS. At Wisconsin, w e’ve run the course\nin three different ways: either all systems programming, all xv6 programming, or\na mix of both.\nWe are slowly making project descriptions, and a testing frame work, avail-\nable. See https://github.com/remzi-arpacidusseau/ostep-projec ts\nfor more information. If not part of a class, this will give you a chance to do these\nprojects on your own, to better learn the material. Unfortunatel y, you don’t have\na TA to bug when you get stuck, but not everything in life can be fre e (but books\ncan be!).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nv\nTo Educators\nIf you are an instructor or professor who wishes to use this book ,p l e a s ef e e l\nfree to do so. As you may have noticed, they are free and availabl eo n - l i n ef r o m\nthe following web page:\nhttp://www.ostep.org\nYou can also purchase a printed copy from lulu.com .L o o kf o ri to nt h ew e b\npage above.\nThe (current) proper citation for the book is as follows:\nOperating Systems: Three Easy Pieces\nRemzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau\nArpaci-Dusseau Books\nAugust, 2018 (Version 1.00)\nhttp://www.ostep.org\nThe course divides fairly well across a 15-week semester, in wh ich you can\ncover most of the topics within at a reasonable level of depth. Cramming the\ncourse into a 10-week quarter probably requires dropping some de tail from each\nof the pieces. There are also a few chapters on virtual machine mo nitors, which we\nusually squeeze in sometime during the semester, either right at end of the large\nsection on virtualization, or near the end as an aside.\nOne slightly unusual aspect of the book is that concurrency, a top ic at the front\nof many OS books, is pushed off herein until the student has built an understand-\ning of virtualization of the CPU and of memory. In our experience in teaching\nthis course for nearly 20 years, students have a hard time underst anding how the\nconcurrency problem arises, or why they are trying to solve it , if they don’t yet un-\nderstand what an address space is, what a process is, or why co ntext switches can\noccur at arbitrary points in time. Once they do understand these concepts, how-\never, introducing the notion of threads and the problems that a rise due to them\nbecomes rather easy, or at least, easier.\nAs much as is possible, we use a chalkboard (or whiteboard) to deli ver a lec-\nture. On these more conceptual days, we come to class with a few majo ri d e a s\nand examples in mind and use the board to present them. Handouts are us eful\nto give the students concrete problems to solve based on the mate rial. On more\npractical days, we simply plug a laptop into the projector and s how real code; this\nstyle works particularly well for concurrency lectures as well as for any discus-\nsion sections where you show students code that is relevant fo rt h e i rp r o j e c t s .W e\ndon’t generally use slides to present material, but have now made as e ta v a i l a b l e\nfor those who prefer that style of presentation.\nIf you’d like a copy of any of these materials, please drop us an ema il. We\nhave already shared them with many others around the world, and others have\ncontributed their materials as well.\nOne last request: if you use the free online chapters, please jus tlink to them,\ninstead of making a local copy. This helps us track usage (over 1 mi llion chapters\ndownloaded in the past few years!) and also ensures students ge tt h el a t e s t( a n d\ngreatest?) version.\nc⃝2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\nvi\nTo Students\nIf you are a student reading this book, thank you! It is an honor f or us to\nprovide some material to help you in your pursuit of knowledge about operating\nsystems. We both think back fondly towards some textbooks of our un dergraduate\ndays (e.g., Hennessy and Patterson [HP90], the classic book on computer architec-\nture) and hope this book will become one of those positive memorie s for you.\nYou may have noticed this book is free and available online2. There is one major\nreason for this: textbooks are generally too expensive. This b ook, we hope, is the\nﬁrst of a new wave of free materials to help those in pursuit of the ir education,\nregardless of which part of the world they come from or how much th ey are willing\nto spend for a book. Failing that, it is one free book, which is better than none.\nWe also hope, where possible, to point you to the original sour ces of much\nof the material in the book: the great papers and persons who ha ve shaped the\nﬁeld of operating systems over the years. Ideas are not pulled o ut of the air; they\ncome from smart and hard-working people (including numerous Turing -award\nwinners3), and thus we should strive to celebrate those ideas and people where\npossible. In doing so, we hopefully can better understand the r evolutions that\nhave taken place, instead of writing texts as if those thoughts have always been\npresent [K62]. Further, perhaps such references will encourag ey o ut od i gd e e p e r\non your own; reading the famous papers of our ﬁeld is certainly one of the best\nways to learn.\n2A digression here: “free” in the way we use it here does not mean open source, and it\ndoes not mean the book is not copyrighted with the usual protections – i t is! What it means is\nthat you can download the chapters and use them to learn about operating systems. Why not\nan open-source book, just like Linux is an open-source kernel? Well, w e believe it is important\nfor a book to have a single voice throughout, and have worked hard to prov ide such a voice.\nWhen you’re reading it, the book should kind of feel like a dialogue w ith the person explaining\nsomething to you. Hence, our approach.\n3The Turing Award is the highest award in Computer Science; it is like the N obel Prize,\nexcept that you have never heard of it.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nvii\nAcknowledgments\nThis section will contain thanks to those who helped us put the b ook together.\nThe important thing for now: your name could go here! But, you have to help. So\nsend us some feedback and help debug this book. And you could be famous !O r ,\nat least, have your name in some book.\nThe people who have helped so far include: Aaron Gember (Colgate), Aashrith\nH Govindraj (USF), Abhinav Mehra, Abhirami Senthilkumaran*, Adam Dresche r* (WUSTL),\nAdam Eggum, Aditya Venkataraman, Adriana Iamnitchi and class (USF), Ahm ad Jarara, Ahmed\nFikri*, Ajaykrishna Raghavan, Akiel Khan, Alex Curtis, Alex Wyler , Alex Zhao (U. Colorado at\nColorado Springs), Ali Razeen (Duke), Alistair Martin, AmirBehzad Eslami, Anand Mundada,\nAndrew Mahler, Andrew Valencik (Saint Mary’s), Angela Demke Brown (Toront o), Antonella\nBernobich (UoPeople)*, Arek Bulski, B. Brahmananda Reddy (Minnesota), Ba la Subrahmanyam\nKambala, Bart Miller, Ben Kushigian (U. Mass), Benita Bose, Biswaj it Mazumder (Clemson),\nBobby Jack, Bj ¨orn Lindberg, Brandon Harshe (U. Minn), Brennan Payne, Brian Gorman, Brian\nKroth, Caleb Sumner (Southern Adventist), Cara Lauritzen, Charlott e Kissinger, Cheng Su,\nChien-Chung Shen (Delaware)*, Christian Stober, Christoph Jaeger, C.J. Stanbridge (Memorial\nU. of Newfoundland), Cody Hanson, Constantinos Georgiades, Dakota Cr ane (U. Washington-\nTacoma), Dan Soendergaard (U. Aarhus), Dan Tsafrir (Technion), Danilo Brus chi (Universita\nDegli Studi Di Milano), Darby Asher Noam Haller, David Hanle (Grinnel l), David Hartman,\nDeepika Muthukumar, Demir Delic, Dennis Zhou, Dheeraj Shetty (North C arolina State), Do-\nrian Arnold (New Mexico), Dustin Metzler, Dustin Passofaro, Eduardo Stelmaszczyk, Emad\nSadeghi, Emil Hessman, Emily Jacobson, Emmett Witchel (Texas), Eric F reudenthal (UTEP),\nEric Johansson, Erik Turk, Ernst Biersack (France), Fangjun Kuang (U. Stut tgart), Feng Zhang\n(IBM), Finn Kuusisto*, Giovanni Lagorio (DIBRIS), Glenn Bruns (CSU Monter ey Bay), Glen\nGranzow (College of Idaho), Guilherme Baptista, Hamid Reza Ghasem i, Hao Chen, Henry\nAbbey, Hilmar G ´ustafsson (Aalborg University), Hrishikesh Amur, Huanchen Zhang*, Hu seyin\nSular, Hugo Diaz, Ilya Oblomkov, Itai Hass (Toronto), Jackson “Jake ” Haenchen (Texas), Ja-\ngannathan Eachambadi, Jake Gillberg, Jakob Olandt, James Earley, James Perry (U. Michigan-\nDearborn)*, Jan Reineke (Universit ¨at des Saarlandes), Jason MacLafferty (Southern Adven-\ntist), Jason Waterman (Vassar), Jay Lim, Jerod Weinman (Grinnell), Jhih-Cheng Luo, Jiao Dong\n(Rutgers), Jia-Shen Boon, Jiawen Bao, Jingxin Li, Joe Jean (NYU), Joel Kuntz (S aint Mary’s),\nJoel Sommers (Colgate), John Brady (Grinnell), John Komenda, Jonathan Perry (MI T), Joshua\nCarpenter (NCSU), Jun He, Karl Wallinger, Kartik Singhal, Katherine D udenas, Katie Coyle\n(Georgia Tech), Kaushik Kannan, Kemal Bıc ¸akcı, Kevin Liu*, Lanyue Lu, Laura Xu, Lei Tian\n(U. Nebraska-Lincoln), Leonardo Medici (U. Milan), Leslie Schultz, Liang Yin, Lihao Wang,\nLooserof, Manav Batra (IIIT-Delhi), Manu Awasthi (Samsung), Marcel van der H olst, Marco\nGuazzone (U. Piemonte Orientale), Mart Oskamp, Martha Ferris, Masashi Kis hikawa (Sony),\nMatt Reichoff, Mattia Monga (U. Milan), Matty Williams, Meng Huang, Michael Machtel\n(Hochschule Konstanz), Michael Walﬁsh (NYU), Michael Wu (UCLA), Mike Griepentrog , Ming\nChen (Stonybrook), Mohammed Alali (Delaware), Mohamed Omran (GUST), Muruga n Kan-\ndaswamy, Nadeem Shaikh, Natasha Eilbert, Natasha Stopa, Nathan D ipiazza, Nathan Sulli-\nvan, Neeraj Badlani (N.C. State), Neil Perry, Nelson Gomez, Nghia H uynh (Texas), Nicholas\nMandal, Nick Weinandt, Patel Pratyush Ashesh (BITS-Pilani), Patricio Jara, Pav le Kostovic,\nPerry Kivolowitz, Peter Peterson (Minnesota), Pieter Kockx, Radford Sm ith, Riccardo Mut-\nschlechner, Ripudaman Singh, Robert Ord ´o˜nez and class (Southern Adventist), Roger Watten-\nhofer (ETH), Rohan Das (Toronto)*, Rohan Pasalkar (Minnesota), Rohan Puri, Ross Aiken, Rus-\nlan Kiselev, Ryland Herrick, Sam Kelly, Sam Noh (UNIST), Samer Al-K iswany, Sandeep Um-\nmadi (Minnesota), Sankaralingam Panneerselvam, Satish Chebrolu (NetAp p), Satyanarayana\nc⃝2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\nviii\nShanmugam*, Scott Catlin, Scott Lee (UCLA), Seth Pollen, Sharad Punuganti , Shreevatsa R.,\nSimon Pratt (Waterloo), Sivaraman Sivaraman*, Song Jiang (Wayne Stat e), Spencer Harston\n(Weber State), Srinivasan Thirunarayanan*, Stefan Dekanski, Stephen B ye, Suriyhaprakhas\nBalaram Sankari, Sy Jin Cheah, Teri Zhao (EMC), Thanumalayan S. Pillai, Thom as Griebel,\nThomas Scrace, Tianxia Bai, Tong He, Tongxin Zheng, Tony Adkins, Torin Ru deen (Princeton),\nTuo Wang, Tyler Couto, Varun Vats, Vikas Goel, Waciuma Wanjohi, W illiam Royle (Grinnell),\nXiang Peng, Xu Di, Yifan Hao, Yuanyuan Chen, Yubin Ruan, Yudong Sun, Yue Z huo (Texas\nA&M), Yufui Ren, Zef RosnBrick, Zeyuan Hu (Texas), ZiHan Zheng (USTC), Zuyu Zhang.\nSpecial thanks to those marked with an asterisk above, who hav eg o n ea b o v ea n d\nbeyond in their suggestions for improvement.\nIn addition, a hearty thanks to Professor Joe Meehean (Lynch burg) for his de-\ntailed notes on each chapter, to Professor Jerod Weinman (Gri nnell) and his entire\nclass for their incredible booklets, to Professor Chien-Ch ung Shen (Delaware) for\nhis invaluable and detailed reading and comments, to Adam Dresch er (WUSTL)\nfor his careful reading and suggestions, to Glen Granzow (Coll ege of Idaho) for his\nincredibly detailed comments and tips, Michael Walﬁsh (NYU) fo r his enthusiasm\nand detailed suggestions for improvement, Peter Peterson (UMD) for his many\nbits of useful feedback and commentary, Mark Kampe (Pomona) for det ailed crit-\nicism (we only wish we could ﬁx all suggestions!), and Youjip Won (Hanyang) for\nhis translation work into Korean(!) and numerous insightful sugg estions. All have\nhelped these authors immeasurably in the reﬁnement of the material sh e r e i n .\nAlso, many thanks to the hundreds of students who have taken 537 ov er the\nyears. In particular, the Fall ’08 class who encouraged the ﬁrs tw r i t t e nf o r mo f\nthese notes (they were sick of not having any kind of textbook t o read — pushy\nstudents!), and then praised them enough for us to keep going (in cluding one hi-\nlarious “ZOMG! You should totally write a textbook!” comment in our course\nevaluations that year).\nAg r e a td e b to ft h a n k si sa l s oo w e dt ot h eb r a v ef e ww h ot o o kt h e xv6 project\nlab course, much of which is now incorporated into the main 537 cour se. From\nSpring ’09: Justin Cherniak, Patrick Deline, Matt Czech, Ton yG r e g e r s o n ,M i c h a e l\nGriepentrog, Tyler Harter, Ryan Kroiss, Eric Radzikowski, Wesley Reardan, Rajiv\nVaidyanathan, and Christopher Waclawik. From Fall ’09: Nic k Bearson, Aaron\nBrown, Alex Bird, David Capel, Keith Gould, Tom Grim, Jeffrey Hug o, Brandon\nJohnson, John Kjell, Boyan Li, James Loethen, Will McCardell ,R y a nS z a r o l e t t a ,S i -\nmon Tso, and Ben Yule. From Spring ’10: Patrick Blesi, Aidan Denn is-Oehling,\nParas Doshi, Jake Friedman, Benjamin Frisch, Evan Hanson, Pik kili Hemanth,\nMichael Jeung, Alex Langenfeld, Scott Rick, Mike Treffert, Ga rret Staus, Brennan\nWall, Hans Werner, Soo-Young Yang, and Carlos Grifﬁn (almost) .\nAlthough they do not directly help with the book, our graduate stud ents have\ntaught us much of what we know about systems. We talk with them regular ly\nwhile they are at Wisconsin, but they do all the real work — and b y telling us about\nwhat they are doing, we learn new things every week. This list includes the follow-\ning collection of current and former students and post-docs wit hw h o mw eh a v e\npublished papers; an asterisk marks those who received a Ph.D. under our guid-\nance: Abhishek Rajimwale, Aishwarya Ganesan, Andrew Krioukov, Ao Ma, Brian\nForney, Chris Dragga, Deepak Ramamurthi, Dennis Zhou, Edward Oa kes, Flo-\nrentina Popovici*, Hariharan Gopalakrishnan, Haryadi S. G unawi*, James Nugent,\nJoe Meehean*, John Bent*, Jun He, Kevin Houck, Lanyue Lu*, Lakshmi Bairava-\nsundaram*, Laxman Visampalli, Leo Arulraj*, Leon Yang, Meenali Rung ta, Muthian\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nix\nSivathanu*, Nathan Burnett*, Nitin Agrawal*, Ram Alagappan, Sa mer Al-Kiswany,\nScott Hendrickson, Sriram Subramanian*, Stephen Todd Jones* , Stephen Sturde-\nvant, Sudarsun Kannan, Suli Yang*, Swaminathan Sundararaman*, Sw etha Krish-\nnan, Thanh Do*, Thanumalayan S. Pillai*, Timothy Denehy*, Tyle rH a r t e r * ,V e n k a t\nVenkataramani, Vijay Chidambaram*, Vijayan Prabhakaran*, Yi ying Zhang*, Yupu\nZhang*, Yuvraj Patel, Zev Weiss*.\nOur graduate students have largely been funded by the National Sc ience Foun-\ndation (NSF), the Department of Energy Ofﬁce of Science (DOE), and by industry\ngrants. We are especially grateful to the NSF for their support over many years, as\nour research has shaped the content of many chapters herein.\nWe thank Thomas Griebel, who demanded a better cover for the boo k. Al-\nthough we didn’t take his speciﬁc suggestion (a dinosaur, can yo ub e l i e v ei t ? ) ,t h e\nbeautiful picture of Halley’s comet would not be found on the cover w ithout him.\nA ﬁnal debt of gratitude is also owed to Aaron Brown, who ﬁrst too k this course\nmany years ago (Spring ’09), then took the xv6 lab course (Fall ’0 9), and ﬁnally was\na graduate teaching assistant for the course for two years or so (Fall ’10 through\nSpring ’12). His tireless work has vastly improved the state o ft h ep r o j e c t s( p a r -\nticularly those in xv6 land) and thus has helped better the learn ing experience for\ncountless undergraduates and graduates here at Wisconsin. As Aaro n would say\n(in his usual succinct manner): “Thx.”\nc⃝2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\nx\nFinal Words\nYeats famously said “Education is not the ﬁlling of a pail but the l ighting of a\nﬁre.” He was right but wrong at the same time4.Y o ud oh a v et o“ ﬁ l lt h ep a i l ”ab i t ,\nand these notes are certainly here to help with that part of yo ur education; after all,\nwhen you go to interview at Google, and they ask you a trick ques tion about how\nto use semaphores, it might be good to actually know what a semaphor ei s ,r i g h t ?\nBut Yeats’s larger point is obviously on the mark: the real point of education\nis to get you interested in something, to learn something more ab out the subject\nmatter on your own and not just what you have to digest to get a good grade in\nsome class. As one of our fathers (Remzi’s dad, Vedat Arpaci) used to say, “Learn\nbeyond the classroom”.\nWe created these notes to spark your interest in operating sys tems, to read more\nabout the topic on your own, to talk to your professor about all the exciting re-\nsearch that is going on in the ﬁeld, and even to get involved wi th that research. It\nis a great ﬁeld(!), full of exciting and wonderful ideas that hav e shaped computing\nhistory in profound and important ways. And while we understand t his ﬁre won’t\nlight for all of you, we hope it does for many, or even a few. Becaus eo n c et h a tﬁ r e\nis lit, well, that is when you truly become capable of doing somet hing great. And\nthus the real point of the educational process: to go forth, to s tudy many new and\nfascinating topics, to learn, to mature, and most importantly, t o ﬁnd something\nthat lights a ﬁre for you.\nAndrea and Remzi\nMarried couple\nProfessors of Computer Science at the University of Wisconsin\nChief Lighters of Fires, hopefully5\n4If he actually said this; as with many famous quotes, the history of thi sg e mi sm u r k y .\n5If this sounds like we are admitting some past history as arsonists ,y o ua r ep r o b a b l y\nmissing the point. Probably. If this sounds cheesy, well, that’s becaus e it is, but you’ll just have\nto forgive us for that.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nxi\nReferences\n[CK+08] “The xv6 Operating System” by Russ Cox, Frans Kaashoek, Robe rt Morris, Nickolai\nZeldovich. From: http://pdos.csail.mit.edu/6.828/2008/index.html .xv6 was\ndeveloped as a port of the original UNIXversion 6 and represents a beautiful, clean, and simple way to\nunderstand a modern operating system.\n[F96] “Six Easy Pieces: Essentials Of Physics Explained By Its Most Bril liant Teacher” by\nRichard P. Feynman. Basic Books, 1996. This book reprints the six easiest chapters of Feynman’s\nLectures on Physics, from 1963. If you like Physics, it is a fantastic read .\n[HP90] “Computer Architecture a Quantitative Approach” (1st ed.) by Da vid A. Patterson and\nJohn L. Hennessy . Morgan-Kaufman, 1990. A book that encouraged each of us at our undergraduate\ninstitutions to pursue graduate studies; we later both had the pleasure of working with Patterson, who\ngreatly shaped the foundations of our research careers.\n[KR88] “The C Programming Language” by Brian Kernighan and Dennis Ritchie. Prentice-\nHall, April 1988. The C programming reference that everyone should have, by the people wh o invented\nthe language.\n[K62] “The Structure of Scientiﬁc Revolutions” by Thomas S. Kuhn. Univers ity of Chicago\nPress, 1962. A great and famous read about the fundamentals of the scientiﬁc process. Mop-up w ork,\nanomaly, crisis, and revolution. We are mostly destined to do mop-up work, alas.\nc⃝2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",24976
03-Table of Contents.pdf,03-Table of Contents,"Contents\nTo Everyone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii\nTo Educators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v\nTo Students . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\nFinal Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . x\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi\n1A D i a l o g u e o n t h e B o o k 1\n2I n t r o d u c t i o n t o O p e r a t i n g S y s t e m s 3\n2.1 Virtualizing The CPU . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Virtualizing Memory . . . . . . . . . . . . . . . . . . . . . . 7\n2.3 Concurrency . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.4 Persistence . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.5 Design Goals . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.6 Some History . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nIV i r t u a l i z a t i o n 21\n3A D i a l o g u e o n V i r t u a l i z a t i o n 2 3\n4T h e A b s t r a c t i o n : T h e P r o c e s s 2 5\n4.1 The Abstraction: A Process . . . . . . . . . . . . . . . . . . 26\n4.2 Process API . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n4.3 Process Creation: A Little More Detail . . . . . . . . . . . . 28\n4.4 Process States . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4.5 Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\nxiii\nxiv CONTENTS\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 35\n5I n t e r l u d e : P r o c e s s A P I 37\n5.1 The fork() System Call . . . . . . . . . . . . . . . . . . . 37\n5.2 The wait() System Call . . . . . . . . . . . . . . . . . . . 39\n5.3 Finally, The exec() System Call . . . . . . . . . . . . . . . 40\n5.4 Why? Motivating The API . . . . . . . . . . . . . . . . . . . 41\n5.5 Process Control And Users . . . . . . . . . . . . . . . . . . 44\n5.6 Useful Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n6M e c h a n i s m : L i m i t e d D i r e c t E x e c u t i o n 4 9\n6.1 Basic Technique: Limited Direct Execution . . . . . . . . . 49\n6.2 Problem #1: Restricted Operations . . . . . . . . . . . . . . 50\n6.3 Problem #2: Switching Between Processes . . . . . . . . . . 55\n6.4 Worried About Concurrency? . . . . . . . . . . . . . . . . . 59\n6.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\nHomework (Measurement) . . . . . . . . . . . . . . . . . . . . . . 63\n7S c h e d u l i n g : I n t r o d u c t i o n 65\n7.1 Workload Assumptions . . . . . . . . . . . . . . . . . . . . 65\n7.2 Scheduling Metrics . . . . . . . . . . . . . . . . . . . . . . . 66\n7.3 First In, First Out (FIFO) . . . . . . . . . . . . . . . . . . . . 66\n7.4 Shortest Job First (SJF) . . . . . . . . . . . . . . . . . . . . . 68\n7.5 Shortest Time-to-Completion First (STCF) . . . . . . . . . . 69\n7.6 A New Metric: Response Time . . . . . . . . . . . . . . . . 70\n7.7 Round Robin . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n7.8 Incorporating I/O . . . . . . . . . . . . . . . . . . . . . . . 73\n7.9 No More Oracle . . . . . . . . . . . . . . . . . . . . . . . . . 74\n7.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 76\n8S c h e d u l i n g :\nThe Multi-Level Feedback Queue 77\n8.1 MLFQ: Basic Rules . . . . . . . . . . . . . . . . . . . . . . . 78\n8.2 Attempt #1: How To Change Priority . . . . . . . . . . . . 79\n8.3 Attempt #2: The Priority Boost . . . . . . . . . . . . . . . . 83\n8.4 Attempt #3: Better Accounting . . . . . . . . . . . . . . . . 84\n8.5 Tuning MLFQ And Other Issues . . . . . . . . . . . . . . . 84\n8.6 MLFQ: Summary . . . . . . . . . . . . . . . . . . . . . . . . 86\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 88\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONTENTS xv\n9S c h e d u l i n g : P r o p o r t i o n a l S h a r e 8 9\n9.1 Basic Concept: Tickets Represent Your Share . . . . . . . . 89\n9.2 Ticket Mechanisms . . . . . . . . . . . . . . . . . . . . . . . 91\n9.3 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . 92\n9.4 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n9.5 How To Assign Tickets? . . . . . . . . . . . . . . . . . . . . 94\n9.6 Why Not Deterministic? . . . . . . . . . . . . . . . . . . . . 94\n9.7 The Linux Completely Fair Scheduler (CFS) . . . . . . . . . 95\n9.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 102\n10 Multiprocessor Scheduling (Advanced) 103\n10.1 Background: Multiprocessor Architecture . . . . . . . . . . 104\n10.2 Don’t Forget Synchronization . . . . . . . . . . . . . . . . . 106\n10.3 One Final Issue: Cache Afﬁnity . . . . . . . . . . . . . . . . 107\n10.4 Single-Queue Scheduling . . . . . . . . . . . . . . . . . . . 107\n10.5 Multi-Queue Scheduling . . . . . . . . . . . . . . . . . . . . 109\n10.6 Linux Multiprocessor Schedulers . . . . . . . . . . . . . . . 112\n10.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 114\n11 Summary Dialogue on CPU Virtualization 117\n12 A Dialogue on Memory Virtualization 119\n13 The Abstraction: Address Spaces 121\n13.1 Early Systems . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n13.2 Multiprogramming and Time Sharing . . . . . . . . . . . . 122\n13.3 The Address Space . . . . . . . . . . . . . . . . . . . . . . . 123\n13.4 Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n13.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n14 Interlude: Memory API 131\n14.1 Types of Memory . . . . . . . . . . . . . . . . . . . . . . . . 131\n14.2 The malloc() Call . . . . . . . . . . . . . . . . . . . . . . 132\n14.3 The free() Call . . . . . . . . . . . . . . . . . . . . . . . . 134\n14.4 Common Errors . . . . . . . . . . . . . . . . . . . . . . . . 134\n14.5 Underlying OS Support . . . . . . . . . . . . . . . . . . . . 137\n14.6 Other Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n14.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\nc⃝2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\nxvi CONTENTS\n15 Mechanism: Address Translation 141\n15.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n15.2 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n15.3 Dynamic (Hardware-based) Relocation . . . . . . . . . . . 145\n15.4 Hardware Support: A Summary . . . . . . . . . . . . . . . 148\n15.5 Operating System Issues . . . . . . . . . . . . . . . . . . . . 149\n15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 154\n16 Segmentation 155\n16.1 Segmentation: Generalized Base/Bounds . . . . . . . . . . 155\n16.2 Which Segment Are We Referring To? . . . . . . . . . . . . 158\n16.3 What About The Stack? . . . . . . . . . . . . . . . . . . . . 159\n16.4 Support for Sharing . . . . . . . . . . . . . . . . . . . . . . 160\n16.5 Fine-grained vs. Coarse-grained Segmentation . . . . . . . 16 1\n16.6 OS Support . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n16.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 165\n17 Free-Space Management 167\n17.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n17.2 Low-level Mechanisms . . . . . . . . . . . . . . . . . . . . 169\n17.3 Basic Strategies . . . . . . . . . . . . . . . . . . . . . . . . . 177\n17.4 Other Approaches . . . . . . . . . . . . . . . . . . . . . . . 179\n17.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 183\n18 Paging: Introduction 185\n18.1 A Simple Example And Overview . . . . . . . . . . . . . . 185\n18.2 Where Are Page Tables Stored? . . . . . . . . . . . . . . . . 189\n18.3 What’s Actually In The Page Table? . . . . . . . . . . . . . 190\n18.4 Paging: Also Too Slow . . . . . . . . . . . . . . . . . . . . . 191\n18.5 A Memory Trace . . . . . . . . . . . . . . . . . . . . . . . . 192\n18.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 197\n19 Paging: Faster Translations (TLBs) 199\n19.1 TLB Basic Algorithm . . . . . . . . . . . . . . . . . . . . . . 199\n19.2 Example: Accessing An Array . . . . . . . . . . . . . . . . 201\n19.3 Who Handles The TLB Miss? . . . . . . . . . . . . . . . . . 203\n19.4 TLB Contents: What’s In There? . . . . . . . . . . . . . . . 205\n19.5 TLB Issue: Context Switches . . . . . . . . . . . . . . . . . 206\n19.6 Issue: Replacement Policy . . . . . . . . . . . . . . . . . . . 208\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONTENTS xvii\n19.7 A Real TLB Entry . . . . . . . . . . . . . . . . . . . . . . . . 209\n19.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\nHomework (Measurement) . . . . . . . . . . . . . . . . . . . . . . 212\n20 Paging: Smaller Tables 215\n20.1 Simple Solution: Bigger Pages . . . . . . . . . . . . . . . . 215\n20.2 Hybrid Approach: Paging and Segments . . . . . . . . . . 216\n20.3 Multi-level Page Tables . . . . . . . . . . . . . . . . . . . . 219\n20.4 Inverted Page Tables . . . . . . . . . . . . . . . . . . . . . . 226\n20.5 Swapping the Page Tables to Disk . . . . . . . . . . . . . . 227\n20.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 229\n21 Beyond Physical Memory: Mechanisms 231\n21.1 Swap Space . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n21.2 The Present Bit . . . . . . . . . . . . . . . . . . . . . . . . . 233\n21.3 The Page Fault . . . . . . . . . . . . . . . . . . . . . . . . . 234\n21.4 What If Memory Is Full? . . . . . . . . . . . . . . . . . . . . 235\n21.5 Page Fault Control Flow . . . . . . . . . . . . . . . . . . . . 236\n21.6 When Replacements Really Occur . . . . . . . . . . . . . . 237\n21.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\nHomework (Measurement) . . . . . . . . . . . . . . . . . . . . . . 240\n22 Beyond Physical Memory: Policies 243\n22.1 Cache Management . . . . . . . . . . . . . . . . . . . . . . 243\n22.2 The Optimal Replacement Policy . . . . . . . . . . . . . . . 244\n22.3 A Simple Policy: FIFO . . . . . . . . . . . . . . . . . . . . . 246\n22.4 Another Simple Policy: Random . . . . . . . . . . . . . . . 248\n22.5 Using History: LRU . . . . . . . . . . . . . . . . . . . . . . 249\n22.6 Workload Examples . . . . . . . . . . . . . . . . . . . . . . 250\n22.7 Implementing Historical Algorithms . . . . . . . . . . . . . 253\n22.8 Approximating LRU . . . . . . . . . . . . . . . . . . . . . . 254\n22.9 Considering Dirty Pages . . . . . . . . . . . . . . . . . . . . 255\n22.10 Other VM Policies . . . . . . . . . . . . . . . . . . . . . . . 256\n22.11 Thrashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\n22.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 259\n23 Complete Virtual Memory Systems 261\n23.1 VAX/VMS Virtual Memory . . . . . . . . . . . . . . . . . . 262\n23.2 The Linux Virtual Memory System . . . . . . . . . . . . . . 268\n23.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\nc⃝2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\nxviii CONTENTS\n24 Summary Dialogue on Memory Virtualization 279\nII Concurrency 283\n25 A Dialogue on Concurrency 285\n26 Concurrency: An Introduction 287\n26.1 Why Use Threads? . . . . . . . . . . . . . . . . . . . . . . . 288\n26.2 An Example: Thread Creation . . . . . . . . . . . . . . . . 289\n26.3 Why It Gets Worse: Shared Data . . . . . . . . . . . . . . . 292\n26.4 The Heart Of The Problem: Uncontrolled Scheduling . . . 294\n26.5 The Wish For Atomicity . . . . . . . . . . . . . . . . . . . . 296\n26.6 One More Problem: Waiting For Another . . . . . . . . . . 298\n26.7 Summary: Why in OS Class? . . . . . . . . . . . . . . . . . 298\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 301\n27 Interlude: Thread API 303\n27.1 Thread Creation . . . . . . . . . . . . . . . . . . . . . . . . 303\n27.2 Thread Completion . . . . . . . . . . . . . . . . . . . . . . . 304\n27.3 Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\n27.4 Condition Variables . . . . . . . . . . . . . . . . . . . . . . 309\n27.5 Compiling and Running . . . . . . . . . . . . . . . . . . . . 311\n27.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 314\n28 Locks 315\n28.1 Locks: The Basic Idea . . . . . . . . . . . . . . . . . . . . . 315\n28.2 Pthread Locks . . . . . . . . . . . . . . . . . . . . . . . . . . 316\n28.3 Building A Lock . . . . . . . . . . . . . . . . . . . . . . . . 317\n28.4 Evaluating Locks . . . . . . . . . . . . . . . . . . . . . . . . 317\n28.5 Controlling Interrupts . . . . . . . . . . . . . . . . . . . . . 318\n28.6 A Failed Attempt: Just Using Loads/Stores . . . . . . . . . 319\n28.7 Building Working Spin Locks with Test-And-Set . . . . . . 320\n28.8 Evaluating Spin Locks . . . . . . . . . . . . . . . . . . . . . 322\n28.9 Compare-And-Swap . . . . . . . . . . . . . . . . . . . . . . 323\n28.10 Load-Linked and Store-Conditional . . . . . . . . . . . . . 324\n28.11 Fetch-And-Add . . . . . . . . . . . . . . . . . . . . . . . . . 326\n28.12 Too Much Spinning: What Now? . . . . . . . . . . . . . . . 327\n28.13 A Simple Approach: Just Yield, Baby . . . . . . . . . . . . . 328\n28.14 Using Queues: Sleeping Instead Of Spinning . . . . . . . . 329\n28.15 Different OS, Different Support . . . . . . . . . . . . . . . . 332\n28.16 Two-Phase Locks . . . . . . . . . . . . . . . . . . . . . . . . 332\n28.17 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONTENTS xix\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 336\n29 Lock-based Concurrent Data Structures 337\n29.1 Concurrent Counters . . . . . . . . . . . . . . . . . . . . . . 337\n29.2 Concurrent Linked Lists . . . . . . . . . . . . . . . . . . . . 342\n29.3 Concurrent Queues . . . . . . . . . . . . . . . . . . . . . . . 345\n29.4 Concurrent Hash Table . . . . . . . . . . . . . . . . . . . . 346\n29.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\n30 Condition Variables 351\n30.1 Deﬁnition and Routines . . . . . . . . . . . . . . . . . . . . 352\n30.2 The Producer/Consumer (Bounded Buffer) Problem . . . . 355\n30.3 Covering Conditions . . . . . . . . . . . . . . . . . . . . . . 363\n30.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 366\n31 Semaphores 367\n31.1 Semaphores: A Deﬁnition . . . . . . . . . . . . . . . . . . . 367\n31.2 Binary Semaphores (Locks) . . . . . . . . . . . . . . . . . . 369\n31.3 Semaphores For Ordering . . . . . . . . . . . . . . . . . . . 370\n31.4 The Producer/Consumer (Bounded Buffer) Problem . . . . 372\n31.5 Reader-Writer Locks . . . . . . . . . . . . . . . . . . . . . . 376\n31.6 The Dining Philosophers . . . . . . . . . . . . . . . . . . . 378\n31.7 How To Implement Semaphores . . . . . . . . . . . . . . . 381\n31.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n32 Common Concurrency Problems 385\n32.1 What Types Of Bugs Exist? . . . . . . . . . . . . . . . . . . 385\n32.2 Non-Deadlock Bugs . . . . . . . . . . . . . . . . . . . . . . 386\n32.3 Deadlock Bugs . . . . . . . . . . . . . . . . . . . . . . . . . 389\n32.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 400\n33 Event-based Concurrency (Advanced) 401\n33.1 The Basic Idea: An Event Loop . . . . . . . . . . . . . . . . 401\n33.2 An Important API: select() (orpoll() )......... 4 0 2\n33.3 Using select() ........................ 4 0 3\n33.4 Why Simpler? No Locks Needed . . . . . . . . . . . . . . . 404\n33.5 A Problem: Blocking System Calls . . . . . . . . . . . . . . 405\n33.6 A Solution: Asynchronous I/O . . . . . . . . . . . . . . . . 405\n33.7 Another Problem: State Management . . . . . . . . . . . . 408\nc⃝2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\nxx CONTENTS\n33.8 What Is Still Difﬁcult With Events . . . . . . . . . . . . . . 409\n33.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\n34 Summary Dialogue on Concurrency 413\nIII Persistence 415\n35 A Dialogue on Persistence 417\n36 I/O Devices 419\n36.1 System Architecture . . . . . . . . . . . . . . . . . . . . . . 419\n36.2 A Canonical Device . . . . . . . . . . . . . . . . . . . . . . 421\n36.3 The Canonical Protocol . . . . . . . . . . . . . . . . . . . . 422\n36.4 Lowering CPU Overhead With Interrupts . . . . . . . . . . 423\n36.5 More Efﬁcient Data Movement With DMA . . . . . . . . . 424\n36.6 Methods Of Device Interaction . . . . . . . . . . . . . . . . 425\n36.7 Fitting Into The OS: The Device Driver . . . . . . . . . . . . 426\n36.8 Case Study: A Simple IDE Disk Driver . . . . . . . . . . . . 427\n36.9 Historical Notes . . . . . . . . . . . . . . . . . . . . . . . . 430\n36.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431\n37 Hard Disk Drives 433\n37.1 The Interface . . . . . . . . . . . . . . . . . . . . . . . . . . 433\n37.2 Basic Geometry . . . . . . . . . . . . . . . . . . . . . . . . . 434\n37.3 A Simple Disk Drive . . . . . . . . . . . . . . . . . . . . . . 435\n37.4 I/O Time: Doing The Math . . . . . . . . . . . . . . . . . . 438\n37.5 Disk Scheduling . . . . . . . . . . . . . . . . . . . . . . . . 442\n37.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 448\n38 Redundant Arrays of Inexpensive Disks (RAIDs) 449\n38.1 Interface And RAID Internals . . . . . . . . . . . . . . . . . 450\n38.2 Fault Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 451\n38.3 How To Evaluate A RAID . . . . . . . . . . . . . . . . . . . 451\n38.4 RAID Level 0: Striping . . . . . . . . . . . . . . . . . . . . . 452\n38.5 RAID Level 1: Mirroring . . . . . . . . . . . . . . . . . . . . 455\n38.6 RAID Level 4: Saving Space With Parity . . . . . . . . . . . 458\n38.7 RAID Level 5: Rotating Parity . . . . . . . . . . . . . . . . 462\n38.8 RAID Comparison: A Summary . . . . . . . . . . . . . . . 463\n38.9 Other Interesting RAID Issues . . . . . . . . . . . . . . . . 464\n38.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONTENTS xxi\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 466\n39 Interlude: Files and Directories 467\n39.1 Files And Directories . . . . . . . . . . . . . . . . . . . . . . 467\n39.2 The File System Interface . . . . . . . . . . . . . . . . . . . 469\n39.3 Creating Files . . . . . . . . . . . . . . . . . . . . . . . . . . 469\n39.4 Reading And Writing Files . . . . . . . . . . . . . . . . . . 470\n39.5 Reading And Writing, But Not Sequentially . . . . . . . . . 472\n39.6 Shared File Table Entries: fork() And dup() ....... 4 7 5\n39.7 Writing Immediately With fsync() ............. 4 7 7\n39.8 Renaming Files . . . . . . . . . . . . . . . . . . . . . . . . . 478\n39.9 Getting Information About Files . . . . . . . . . . . . . . . 479\n39.10 Removing Files . . . . . . . . . . . . . . . . . . . . . . . . . 480\n39.11 Making Directories . . . . . . . . . . . . . . . . . . . . . . . 480\n39.12 Reading Directories . . . . . . . . . . . . . . . . . . . . . . 481\n39.13 Deleting Directories . . . . . . . . . . . . . . . . . . . . . . 482\n39.14 Hard Links . . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n39.15 Symbolic Links . . . . . . . . . . . . . . . . . . . . . . . . . 484\n39.16 Permission Bits And Access Control Lists . . . . . . . . . . 485\n39.17 Making And Mounting A File System . . . . . . . . . . . . 488\n39.18 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 492\n40 File System Implementation 493\n40.1 The Way To Think . . . . . . . . . . . . . . . . . . . . . . . 493\n40.2 Overall Organization . . . . . . . . . . . . . . . . . . . . . . 494\n40.3 File Organization: The Inode . . . . . . . . . . . . . . . . . 496\n40.4 Directory Organization . . . . . . . . . . . . . . . . . . . . 501\n40.5 Free Space Management . . . . . . . . . . . . . . . . . . . . 501\n40.6 Access Paths: Reading and Writing . . . . . . . . . . . . . . 502\n40.7 Caching and Buffering . . . . . . . . . . . . . . . . . . . . . 506\n40.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 510\n41 Locality and The Fast File System 511\n41.1 The Problem: Poor Performance . . . . . . . . . . . . . . . 511\n41.2 FFS: Disk Awareness Is The Solution . . . . . . . . . . . . . 513\n41.3 Organizing Structure: The Cylinder Group . . . . . . . . . 513\n41.4 Policies: How To Allocate Files and Directories . . . . . . . 515\n41.5 Measuring File Locality . . . . . . . . . . . . . . . . . . . . 517\n41.6 The Large-File Exception . . . . . . . . . . . . . . . . . . . 518\n41.7 A Few Other Things About FFS . . . . . . . . . . . . . . . . 520\n41.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 524\nc⃝2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\nxxii CONTENTS\n42 Crash Consistency: FSCK and Journaling 525\n42.1 A Detailed Example . . . . . . . . . . . . . . . . . . . . . . 526\n42.2 Solution #1: The File System Checker . . . . . . . . . . . . 529\n42.3 Solution #2: Journaling (or Write-Ahead Logging) . . . . . 531\n42.4 Solution #3: Other Approaches . . . . . . . . . . . . . . . . 541\n42.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 545\n43 Log-structured File Systems 547\n43.1 Writing To Disk Sequentially . . . . . . . . . . . . . . . . . 548\n43.2 Writing Sequentially And Effectively . . . . . . . . . . . . . 54 9\n43.3 How Much To Buffer? . . . . . . . . . . . . . . . . . . . . . 550\n43.4 Problem: Finding Inodes . . . . . . . . . . . . . . . . . . . 551\n43.5 Solution Through Indirection: The Inode Map . . . . . . . 551\n43.6 Completing The Solution: The Checkpoint Region . . . . . 553\n43.7 Reading A File From Disk: A Recap . . . . . . . . . . . . . 553\n43.8 What About Directories? . . . . . . . . . . . . . . . . . . . 554\n43.9 A New Problem: Garbage Collection . . . . . . . . . . . . . 555\n43.10 Determining Block Liveness . . . . . . . . . . . . . . . . . . 556\n43.11 A Policy Question: Which Blocks To Clean, And When? . . 557\n43.12 Crash Recovery And The Log . . . . . . . . . . . . . . . . . 558\n43.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 561\n44 Flash-based SSDs 563\n44.1 Storing a Single Bit . . . . . . . . . . . . . . . . . . . . . . . 563\n44.2 From Bits to Banks/Planes . . . . . . . . . . . . . . . . . . 564\n44.3 Basic Flash Operations . . . . . . . . . . . . . . . . . . . . . 565\n44.4 Flash Performance And Reliability . . . . . . . . . . . . . . 567\n44.5 From Raw Flash to Flash-Based SSDs . . . . . . . . . . . . 568\n44.6 FTL Organization: A Bad Approach . . . . . . . . . . . . . 569\n44.7 A Log-Structured FTL . . . . . . . . . . . . . . . . . . . . . 570\n44.8 Garbage Collection . . . . . . . . . . . . . . . . . . . . . . . 572\n44.9 Mapping Table Size . . . . . . . . . . . . . . . . . . . . . . 574\n44.10 Wear Leveling . . . . . . . . . . . . . . . . . . . . . . . . . 579\n44.11 SSD Performance And Cost . . . . . . . . . . . . . . . . . . 579\n44.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 585\n45 Data Integrity and Protection 587\n45.1 Disk Failure Modes . . . . . . . . . . . . . . . . . . . . . . . 587\n45.2 Handling Latent Sector Errors . . . . . . . . . . . . . . . . 589\n45.3 Detecting Corruption: The Checksum . . . . . . . . . . . . 590\n45.4 Using Checksums . . . . . . . . . . . . . . . . . . . . . . . 593\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONTENTS xxiii\n45.5 A New Problem: Misdirected Writes . . . . . . . . . . . . . 594\n45.6 One Last Problem: Lost Writes . . . . . . . . . . . . . . . . 595\n45.7 Scrubbing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595\n45.8 Overheads Of Checksumming . . . . . . . . . . . . . . . . 596\n45.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 598\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 599\n46 Summary Dialogue on Persistence 601\n47 A Dialogue on Distribution 603\n48 Distributed Systems 605\n48.1 Communication Basics . . . . . . . . . . . . . . . . . . . . . 606\n48.2 Unreliable Communication Layers . . . . . . . . . . . . . . 607\n48.3 Reliable Communication Layers . . . . . . . . . . . . . . . 609\n48.4 Communication Abstractions . . . . . . . . . . . . . . . . . 611\n48.5 Remote Procedure Call (RPC) . . . . . . . . . . . . . . . . . 613\n48.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619\nHomework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 620\n49 Sun’s Network File System (NFS) 621\n49.1 A Basic Distributed File System . . . . . . . . . . . . . . . . 622\n49.2 On To NFS . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623\n49.3 Focus: Simple And Fast Server Crash Recovery . . . . . . . 623\n49.4 Key To Fast Crash Recovery: Statelessness . . . . . . . . . 624\n49.5 The NFSv2 Protocol . . . . . . . . . . . . . . . . . . . . . . 625\n49.6 From Protocol To Distributed File System . . . . . . . . . . 627\n49.7 Handling Server Failure With Idempotent Operations . . . 62 9\n49.8 Improving Performance: Client-side Caching . . . . . . . . 63 1\n49.9 The Cache Consistency Problem . . . . . . . . . . . . . . . 631\n49.10 Assessing NFS Cache Consistency . . . . . . . . . . . . . . 633\n49.11 Implications On Server-Side Write Buffering . . . . . . . . 63 3\n49.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637\nHomework (Measurement) . . . . . . . . . . . . . . . . . . . . . . 638\n50 The Andrew File System (AFS) 639\n50.1 AFS Version 1 . . . . . . . . . . . . . . . . . . . . . . . . . . 639\n50.2 Problems with Version 1 . . . . . . . . . . . . . . . . . . . . 641\n50.3 Improving the Protocol . . . . . . . . . . . . . . . . . . . . 642\n50.4 AFS Version 2 . . . . . . . . . . . . . . . . . . . . . . . . . . 642\n50.5 Cache Consistency . . . . . . . . . . . . . . . . . . . . . . . 644\n50.6 Crash Recovery . . . . . . . . . . . . . . . . . . . . . . . . . 646\n50.7 Scale And Performance Of AFSv2 . . . . . . . . . . . . . . 646\nc⃝2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\nxxiv CONTENTS\n50.8 AFS: Other Improvements . . . . . . . . . . . . . . . . . . . 649\n50.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 651\nHomework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 652\n51 Summary Dialogue on Distribution 653\nGeneral Index 655\nAsides 667\nTips 671\nCruces 675\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",31965
04-1. Dialogue.pdf,04-1. Dialogue,"1\nA Dialogue on the Book\nProfessor: Welcome to this book! It’s called Operating Systems in Three Easy\nPieces , and I am here to teach you the things you need to know about oper ating\nsystems. I am called “Professor”; who are you?\nStudent: Hi Professor! I am called “Student”, as you might have guessed. An d\nI am here and ready to learn!\nProfessor: Sounds good. Any questions?\nStudent: Sure! Why is it called “Three Easy Pieces”?\nProfessor: That’s an easy one. Well, you see, there are these great lectures on\nPhysics by Richard Feynman...\nStudent: Oh! The guy who wrote “Surely You’re Joking, Mr. Feynman”, right?\nGreat book! Is this going to be hilarious like that book was?\nProfessor: Um... well, no. That book was great, and I’m glad you’ve read it.\nHopefully this book is more like his notes on Physics. Some of the basics w ere\nsummed up in a book called “Six Easy Pieces”. He was talking about Phys ics;\nwe’re going to do Three Easy Pieces on the ﬁne topic of Operating Syst ems. This\nis appropriate, as Operating Systems are about half as hard as Phy sics.\nStudent: Well, I liked physics, so that is probably good. What are those pieces?\nProfessor: They are the three key ideas we’re going to learn about: virtualiza-\ntion,concurrency , and persistence . In learning about these ideas, we’ll learn\nall about how an operating system works, including how it decides wha t program\nto run next on a CPU, how it handles memory overload in a virtual memo ry sys-\ntem, how virtual machine monitors work, how to manage information o n disks,\nand even a little about how to build a distributed system that works wh en parts\nhave failed. That sort of stuff.\nStudent: I have no idea what you’re talking about, really.\nProfessor: Good! That means you are in the right class.\nStudent: I have another question: what’s the best way to learn this stuff?\nProfessor: Excellent query! Well, each person needs to ﬁgure this out on their\n1\n2 A D IALOGUE ON THE BOOK\nown, of course, but here is what I would do: go to class, to hear the professor\nintroduce the material. Then, at the end of every week, read thes e notes, to help\nthe ideas sink into your head a bit better. Of course, some time later (hint: before\nthe exam!), read the notes again to ﬁrm up your knowledge. Of cour se, your pro-\nfessor will no doubt assign some homeworks and projects, so you sho uld do those;\nin particular, doing projects where you write real code to solve real problems is\nthe best way to put the ideas within these notes into action. As Confu cius said...\nStudent: Oh, I know! ’I hear and I forget. I see and I remember. I do and I\nunderstand.’ Or something like that.\nProfessor: (surprised) How did you know what I was going to say?!\nStudent: It seemed to follow. Also, I am a big fan of Confucius, and an even\nbigger fan of Xunzi, who actually is a better source for this quote1.\nProfessor: (stunned) Well, I think we are going to get along just ﬁne! Just ﬁne\nindeed.\nStudent: Professor – just one more question, if I may. What are these dialogue s\nfor? I mean, isn’t this just supposed to be a book? Why not present t he material\ndirectly?\nProfessor: Ah, good question, good question! Well, I think it is sometimes\nuseful to pull yourself outside of a narrative and think a bit; these d ialogues are\nthose times. So you and I are going to work together to make sense of all of these\npretty complex ideas. Are you up for it?\nStudent: So we have to think? Well, I’m up for that. I mean, what else do I have\nto do anyhow? It’s not like I have much of a life outside of this book.\nProfessor: Me neither, sadly. So let’s get to work!\n1According to this website (http://www.barrypopik.com/index.php/ne wyork city/\nentry/tell meand iforget teach meand imay remember involve meand iwill lear/),\nConfucian philosopher Xunzi said “Not having heard something is not as goo d as having\nheard it; having heard it is not as good as having seen it; having seen it i s not as good as\nknowing it; knowing it is not as good as putting it into practice.” Later on, the wisdom got\nattached to Confucius for some reason. Thanks to Jiao Dong (Rutgers) for tell ing us!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",4236
05-2. Introduction.pdf,05-2. Introduction,"2\nIntroduction to Operating Systems\nIf you are taking an undergraduate operating systems course, you should\nalready have some idea of what a computer program does when it runs.\nIf not, this book (and the corresponding course) is going to be difﬁcu lt\n— so you should probably stop reading this book, or run to the near-\nest bookstore and quickly consume the necessary background mater ial\nbefore continuing (both Patt & Patel [PP03] and Bryant & O’Hallar on\n[BOH10] are pretty great books).\nSo what happens when a program runs?\nWell, a running program does one very simple thing: it executes i n-\nstructions. Many millions (and these days, even billions) of tim es ev-\nery second, the processor fetches an instruction from memory, decodes\nit (i.e., ﬁgures out which instruction this is), and executes it (i.e., it does\nthe thing that it is supposed to do, like add two numbers together , access\nmemory, check a condition, jump to a function, and so forth). After it is\ndone with this instruction, the processor moves on to the next instr uction,\nand so on, and so on, until the program ﬁnally completes1.\nThus, we have just described the basics of the Von Neumann model of\ncomputing2. Sounds simple, right? But in this class, we will be learning\nthat while a program runs, a lot of other wild things are going on with\nthe primary goal of making the system easy to use .\nThere is a body of software, in fact, that is responsible for making it\neasy to run programs (even allowing you to seemingly run many at t he\nsame time), allowing programs to share memory, enabling program s to\ninteract with devices, and other fun stuff like that. That body of software\n1Of course, modern processors do many bizarre and frightening things under neath the\nhood to make programs run faster, e.g., executing multiple instru ctions at once, and even issu-\ning and completing them out of order! But that is not our concern here; we ar e just concerned\nwith the simple model most programs assume: that instructions see mingly execute one at a\ntime, in an orderly and sequential fashion.\n2Von Neumann was one of the early pioneers of computing systems. He al so did pioneer-\ning work on game theory and atomic bombs, and played in the NBA for six years. OK, one of\nthose things isn’t true.\n1\n2 INTRODUCTION TO OPERATING SYSTEMS\nTHECRUX OF THE PROBLEM :\nHOWTOVIRTUALIZE RESOURCES\nOne central question we will answer in this book is quite simple: how\ndoes the operating system virtualize resources? This is the cru x of our\nproblem. Why the OS does this is not the main question, as the answer\nshould be obvious: it makes the system easier to use. Thus, we focu s on\nthehow: what mechanisms and policies are implemented by the OS to\nattain virtualization? How does the OS do so efﬁciently? What ha rdware\nsupport is needed?\nWe will use the “crux of the problem”, in shaded boxes such as this one,\nas a way to call out speciﬁc problems we are trying to solve in buil ding\nan operating system. Thus, within a note on a particular topic, you may\nﬁnd one or more cruces (yes, this is the proper plural) which highlight the\nproblem. The details within the chapter, of course, present the solution,\nor at least the basic parameters of a solution.\nis called the operating system (OS)3, as it is in charge of making sure the\nsystem operates correctly and efﬁciently in an easy-to-use man ner.\nThe primary way the OS does this is through a general technique t hat\nwe call virtualization . That is, the OS takes a physical resource (such as\nthe processor, or memory, or a disk) and transforms it into a more gen-\neral, powerful, and easy-to-use virtual form of itself. Thus, we sometimes\nrefer to the operating system as a virtual machine .\nOf course, in order to allow users to tell the OS what to do and thus\nmake use of the features of the virtual machine (such as running a pro-\ngram, or allocating memory, or accessing a ﬁle), the OS also provid es\nsome interfaces (APIs) that you can call. A typical OS, in fact, e xports\na few hundred system calls that are available to applications. Because\nthe OS provides these calls to run programs, access memory and de vices,\nand other related actions, we also sometimes say that the OS provi des a\nstandard library to applications.\nFinally, because virtualization allows many programs to run (t hus shar-\ning the CPU), and many programs to concurrently access their own in-\nstructions and data (thus sharing memory), and many programs to access\ndevices (thus sharing disks and so forth), the OS is sometimes k nown as\naresource manager . Each of the CPU, memory, and disk is a resource\nof the system; it is thus the operating system’s role to manage those re-\nsources, doing so efﬁciently or fairly or indeed with many other pos sible\ngoals in mind. To understand the role of the OS a little bit better , let’s take\na look at some examples.\n3Another early name for the OS was the supervisor or even the master control program .\nApparently, the latter sounded a little overzealous (see the movi e Tron for details) and thus,\nthankfully, “operating system” caught on instead.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTRODUCTION TO OPERATING SYSTEMS 3\n1#include <stdio.h>\n2#include <stdlib.h>\n3#include <sys/time.h>\n4#include <assert.h>\n5#include ""common.h""\n6\n7int\n8main(int argc, char *argv[])\n9{\n10 if (argc != 2) {\n11 fprintf(stderr, ""usage: cpu <string>\n"");\n12 exit(1);\n13 }\n14 char*str = argv[1];\n15 while (1) {\n16 Spin(1);\n17 printf(""%s\n"", str);\n18 }\n19 return 0;\n20}\nFigure 2.1: Simple Example: Code That Loops And Prints ( cpu.c )\n2.1 Virtualizing The CPU\nFigure 2.1 depicts our ﬁrst program. It doesn’t do much. In fact, a ll\nit does is call Spin() , a function that repeatedly checks the time and\nreturns once it has run for a second. Then, it prints out the string that the\nuser passed in on the command line, and repeats, forever.\nLet’s say we save this ﬁle as cpu.c and decide to compile and run it\non a system with a single processor (or CPU as we will sometimes call it).\nHere is what we will see:\nprompt> gcc -o cpu cpu.c -Wall\nprompt> ./cpu ""A""\nA\nA\nA\nA\nˆC\nprompt>\nNot too interesting of a run — the system begins running the progra m,\nwhich repeatedly checks the time until a second has elapsed. O nce a sec-\nond has passed, the code prints the input string passed in by the user\n(in this example, the letter “A”), and continues. Note the progr am will\nrun forever; only by pressing “Control-c” (which on U NIX-based systems\nwill terminate the program running in the foreground) can we hal t the\nprogram.\nNow, let’s do the same thing, but this time, let’s run many differ ent in-\nstances of this same program. Figure 2.2 shows the results of this slightly\nmore complicated example.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 INTRODUCTION TO OPERATING SYSTEMS\nprompt> ./cpu A & ; ./cpu B & ; ./cpu C & ; ./cpu D &\n[1] 7353\n[2] 7354\n[3] 7355\n[4] 7356\nA\nB\nD\nC\nA\nB\nD\nC\nA\nC\nB\nD\n...\nFigure 2.2: Running Many Programs At Once\nWell, now things are getting a little more interesting. Even th ough we\nhave only one processor, somehow all four of these programs seem to be\nrunning at the same time! How does this magic happen?4\nIt turns out that the operating system, with some help from the har d-\nware, is in charge of this illusion , i.e., the illusion that the system has a\nvery large number of virtual CPUs. Turning a single CPU (or smal l set of\nthem) into a seemingly inﬁnite number of CPUs and thus allowing many\nprograms to seemingly run at once is what we call virtualizing the CPU ,\nthe focus of the ﬁrst major part of this book.\nOf course, to run programs, and stop them, and otherwise tell the O S\nwhich programs to run, there need to be some interfaces (APIs) t hat you\ncan use to communicate your desires to the OS. We’ll talk about thes e\nAPIs throughout this book; indeed, they are the major way in which m ost\nusers interact with operating systems.\nYou might also notice that the ability to run multiple programs a t once\nraises all sorts of new questions. For example, if two programs wan t to\nrun at a particular time, which should run? This question is answered by\napolicy of the OS; policies are used in many different places within an\nOS to answer these types of questions, and thus we will study the m as\nwe learn about the basic mechanisms that operating systems implement\n(such as the ability to run multiple programs at once). Hence th e role of\nthe OS as a resource manager .\n4Note how we ran four processes at the same time, by using the &symbol. Doing so runs a\njob in the background in the tcsh shell, which means that the user is able to immediately issue\ntheir next command, which in this case is another program to run. The semi-colo n between\ncommands allows us to run multiple programs at the same time in tcsh . If you’re using a\ndifferent shell (e.g., bash ), it works slightly differently; read documentation online for detai ls.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTRODUCTION TO OPERATING SYSTEMS 5\n1#include <unistd.h>\n2#include <stdio.h>\n3#include <stdlib.h>\n4#include ""common.h""\n5\n6int\n7main(int argc, char *argv[])\n8{\n9int*p = malloc(sizeof(int)); // a1\n10 assert(p != NULL);\n11 printf(""(%d) address pointed to by p: %p\n"",\n12 getpid(), p); // a2\n13 *p = 0; // a3\n14 while (1) {\n15 Spin(1);\n16 *p =*p + 1;\n17 printf(""(%d) p: %d\n"", getpid(), *p); // a4\n18 }\n19 return 0;\n20}\nFigure 2.3: A Program That Accesses Memory ( mem.c )\n2.2 Virtualizing Memory\nNow let’s consider memory. The model of physical memory pre-\nsented by modern machines is very simple. Memory is just an arra y of\nbytes; to read memory, one must specify an address to be able to access\nthe data stored there; to write (orupdate ) memory, one must also specify\nthe data to be written to the given address.\nMemory is accessed all the time when a program is running. A pro-\ngram keeps all of its data structures in memory, and accesses th em through\nvarious instructions, like loads and stores or other explicit inst ructions\nthat access memory in doing their work. Don’t forget that each instr uc-\ntion of the program is in memory too; thus memory is accessed on each\ninstruction fetch.\nLet’s take a look at a program (in Figure 2.3) that allocates some mem -\nory by calling malloc() . The output of this program can be found here:\nprompt> ./mem\n(2134) address pointed to by p: 0x200000\n(2134) p: 1\n(2134) p: 2\n(2134) p: 3\n(2134) p: 4\n(2134) p: 5\nˆC\nThe program does a couple of things. First, it allocates some memory\n(line a1). Then, it prints out the address of the memory (a2), and then\nputs the number zero into the ﬁrst slot of the newly allocated mem ory\n(a3). Finally, it loops, delaying for a second and incrementing t he value\nstored at the address held in p.With every print statement, it also prints\nout what is called the process identiﬁer (the PID) of the running program.\nThis PID is unique per running process.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 INTRODUCTION TO OPERATING SYSTEMS\nprompt> ./mem &; ./mem &\n[1] 24113\n[2] 24114\n(24113) address pointed to by p: 0x200000\n(24114) address pointed to by p: 0x200000\n(24113) p: 1\n(24114) p: 1\n(24114) p: 2\n(24113) p: 2\n(24113) p: 3\n(24114) p: 3\n(24113) p: 4\n(24114) p: 4\n...\nFigure 2.4: Running The Memory Program Multiple Times\nAgain, this ﬁrst result is not too interesting. The newly alloca ted mem-\nory is at address 0x200000 . As the program runs, it slowly updates the\nvalue and prints out the result.\nNow, we again run multiple instances of this same program to see\nwhat happens (Figure 2.4). We see from the example that each ru nning\nprogram has allocated memory at the same address ( 0x200000 ), and yet\neach seems to be updating the value at 0x200000 independently! It is as\nif each running program has its own private memory, instead of sha ring\nthe same physical memory with other running programs5.\nIndeed, that is exactly what is happening here as the OS is virtualiz-\ning memory . Each process accesses its own private virtual address space\n(sometimes just called its address space ), which the OS somehow maps\nonto the physical memory of the machine. A memory reference withi n\none running program does not affect the address space of other proces ses\n(or the OS itself); as far as the running program is concerned, it has phys-\nical memory all to itself. The reality, however, is that physic al memory is\na shared resource, managed by the operating system. Exactly how all of\nthis is accomplished is also the subject of the ﬁrst part of this b ook, on the\ntopic of virtualization .\n2.3 Concurrency\nAnother main theme of this book is concurrency . We use this concep-\ntual term to refer to a host of problems that arise, and must be add ressed,\nwhen working on many things at once (i.e., concurrently) in the sa me\nprogram. The problems of concurrency arose ﬁrst within the operati ng\nsystem itself; as you can see in the examples above on virtualiza tion, the\nOS is juggling many things at once, ﬁrst running one process, the n an-\nother, and so forth. As it turns out, doing so leads to some deep and\ninteresting problems.\n5For this example to work, you need to make sure address-space rando mization is dis-\nabled; randomization, as it turns out, can be a good defense against ce rtain kinds of security\nﬂaws. Read more about it on your own, especially if you want to lea rn how to break into\ncomputer systems via stack-smashing attacks. Not that we would recom mend such a thing...\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTRODUCTION TO OPERATING SYSTEMS 7\n1#include <stdio.h>\n2#include <stdlib.h>\n3#include ""common.h""\n4\n5volatile int counter = 0;\n6int loops;\n7\n8void*worker(void *arg) {\n9int i;\n10 for (i = 0; i < loops; i++) {\n11 counter++;\n12 }\n13 return NULL;\n14}\n15\n16int\n17main(int argc, char *argv[])\n18{\n19 if (argc != 2) {\n20 fprintf(stderr, ""usage: threads <value>\n"");\n21 exit(1);\n22 }\n23 loops = atoi(argv[1]);\n24 pthread_t p1, p2;\n25 printf(""Initial value : %d\n"", counter);\n26\n27 Pthread_create(&p1, NULL, worker, NULL);\n28 Pthread_create(&p2, NULL, worker, NULL);\n29 Pthread_join(p1, NULL);\n30 Pthread_join(p2, NULL);\n31 printf(""Final value : %d\n"", counter);\n32 return 0;\n33}\nFigure 2.5: A Multi-threaded Program ( threads.c )\nUnfortunately, the problems of concurrency are no longer limited just\nto the OS itself. Indeed, modern multi-threaded programs exhibit the\nsame problems. Let us demonstrate with an example of a multi-threaded\nprogram (Figure 2.5).\nAlthough you might not understand this example fully at the momen t\n(and we’ll learn a lot more about it in later chapters, in the secti on of the\nbook on concurrency), the basic idea is simple. The main program cr eates\ntwo threads usingPthread create()6. You can think of a thread as a\nfunction running within the same memory space as other functions , with\nmore than one of them active at a time. In this example, each threa d starts\nrunning in a routine called worker() , in which it simply increments a\ncounter in a loop for loops number of times.\nBelow is a transcript of what happens when we run this program wit h\nthe input value for the variable loops set to 1000. The value of loops\n6The actual call should be to lower-case pthread create() ; the upper-case version is\nour own wrapper that calls pthread create() and makes sure that the return code indicates\nthat the call succeeded. See the code for details.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 INTRODUCTION TO OPERATING SYSTEMS\nTHECRUX OF THE PROBLEM :\nHOWTOBUILD CORRECT CONCURRENT PROGRAMS\nWhen there are many concurrently executing threads within th e same\nmemory space, how can we build a correctly working program? What\nprimitives are needed from the OS? What mechanisms should be pro-\nvided by the hardware? How can we use them to solve the problems of\nconcurrency?\ndetermines how many times each of the two workers will increment the\nshared counter in a loop. When the program is run with the value of\nloops set to 1000, what do you expect the ﬁnal value of counter to be?\nprompt> gcc -o thread thread.c -Wall -pthread\nprompt> ./thread 1000\nInitial value : 0\nFinal value : 2000\nAs you probably guessed, when the two threads are ﬁnished, the ﬁ nal\nvalue of the counter is 2000, as each thread incremented the coun ter 1000\ntimes. Indeed, when the input value of loops is set to N, we would\nexpect the ﬁnal output of the program to be 2N. But life is not so simple,\nas it turns out. Let’s run the same program, but with higher value s for\nloops , and see what happens:\nprompt> ./thread 100000\nInitial value : 0\nFinal value : 143012 // huh??\nprompt> ./thread 100000\nInitial value : 0\nFinal value : 137298 // what the??\nIn this run, when we gave an input value of 100,000, instead of ge tting\na ﬁnal value of 200,000, we instead ﬁrst get 143,012. Then, whe n we run\nthe program a second time, we not only again get the wrong value, but\nalso a different value than the last time. In fact, if you run the program\nover and over with high values of loops , you may ﬁnd that sometimes\nyou even get the right answer! So why is this happening?\nAs it turns out, the reason for these odd and unusual outcomes relate\nto how instructions are executed, which is one at a time. Unfortun ately, a\nkey part of the program above, where the shared counter is increme nted,\ntakes three instructions: one to load the value of the counter from m em-\nory into a register, one to increment it, and one to store it back in to mem-\nory. Because these three instructions do not execute atomically (all at\nonce), strange things can happen. It is this problem of concurrency that\nwe will address in great detail in the second part of this book.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTRODUCTION TO OPERATING SYSTEMS 9\n1#include <stdio.h>\n2#include <unistd.h>\n3#include <assert.h>\n4#include <fcntl.h>\n5#include <sys/types.h>\n6\n7int\n8main(int argc, char *argv[])\n9{\n10 int fd = open(""/tmp/file"", O_WRONLY | O_CREAT | O_TRUNC, S_I RWXU);\n11 assert(fd > -1);\n12 int rc = write(fd, ""hello world\n"", 13);\n13 assert(rc == 13);\n14 close(fd);\n15 return 0;\n16}\nFigure 2.6: A Program That Does I/O ( io.c )\n2.4 Persistence\nThe third major theme of the course is persistence . In system memory,\ndata can be easily lost, as devices such as DRAM store values in a volatile\nmanner; when power goes away or the system crashes, any data in me m-\nory is lost. Thus, we need hardware and software to be able to store data\npersistently ; such storage is thus critical to any system as users care a\ngreat deal about their data.\nThe hardware comes in the form of some kind of input/output orI/O\ndevice; in modern systems, a hard drive is a common repository for long-\nlived information, although solid-state drives (SSDs ) are making head-\nway in this arena as well.\nThe software in the operating system that usually manages the d isk is\ncalled the ﬁle system ; it is thus responsible for storing any ﬁles the user\ncreates in a reliable and efﬁcient manner on the disks of the sys tem.\nUnlike the abstractions provided by the OS for the CPU and memory,\nthe OS does not create a private, virtualized disk for each appli cation.\nRather, it is assumed that often times, users will want to share informa-\ntion that is in ﬁles. For example, when writing a C program, you mig ht\nﬁrst use an editor (e.g., Emacs7) to create and edit the C ﬁle ( emacs -nw\nmain.c ). Once done, you might use the compiler to turn the source code\ninto an executable (e.g., gcc -o main main.c ). When you’re ﬁnished,\nyou might run the new executable (e.g., ./main ). Thus, you can see how\nﬁles are shared across different processes. First, Emacs crea tes a ﬁle that\nserves as input to the compiler; the compiler uses that input ﬁl e to create\na new executable ﬁle (in many steps — take a compiler course for de tails);\nﬁnally, the new executable is then run. And thus a new program i s born!\nTo understand this better, let’s look at some code. Figure 2.6 pres ents\ncode to create a ﬁle ( /tmp/file ) that contains the string “hello world”.\n7You should be using Emacs. If you are using vi, there is probably som ething wrong with\nyou. If you are using something that is not a real code editor, that is e ven worse.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 INTRODUCTION TO OPERATING SYSTEMS\nTHECRUX OF THE PROBLEM :\nHOWTOSTORE DATA PERSISTENTLY\nThe ﬁle system is the part of the OS in charge of managing persist ent data.\nWhat techniques are needed to do so correctly? What mechanism s and\npolicies are required to do so with high performance? How is reli ability\nachieved, in the face of failures in hardware and software?\nTo accomplish this task, the program makes three calls into the oper-\nating system. The ﬁrst, a call to open() , opens the ﬁle and creates it; the\nsecond,write() , writes some data to the ﬁle; the third, close() , sim-\nply closes the ﬁle thus indicating the program won’t be writing an y more\ndata to it. These system calls are routed to the part of the operating sys-\ntem called the ﬁle system , which then handles the requests and returns\nsome kind of error code to the user.\nYou might be wondering what the OS does in order to actually write\nto disk. We would show you but you’d have to promise to close your\neyes ﬁrst; it is that unpleasant. The ﬁle system has to do a fai r bit of work:\nﬁrst ﬁguring out where on disk this new data will reside, and the n keep-\ning track of it in various structures the ﬁle system maintains. Doing so\nrequires issuing I/O requests to the underlying storage devi ce, to either\nread existing structures or update (write) them. As anyone who has writ-\nten a device driver8knows, getting a device to do something on your\nbehalf is an intricate and detailed process. It requires a dee p knowledge\nof the low-level device interface and its exact semantics. Fort unately, the\nOS provides a standard and simple way to access devices through its sys-\ntem calls. Thus, the OS is sometimes seen as a standard library .\nOf course, there are many more details in how devices are accesse d,\nand how ﬁle systems manage data persistently atop said devices . For\nperformance reasons, most ﬁle systems ﬁrst delay such writes for a while,\nhoping to batch them into larger groups. To handle the problems of sys-\ntem crashes during writes, most ﬁle systems incorporate some kin d of\nintricate write protocol, such as journaling orcopy-on-write , carefully\nordering writes to disk to ensure that if a failure occurs durin g the write\nsequence, the system can recover to reasonable state afterwar ds. To make\ndifferent common operations efﬁcient, ﬁle systems employ many di ffer-\nent data structures and access methods, from simple lists to com plex b-\ntrees. If all of this doesn’t make sense yet, good! We’ll be talking a bout\nall of this quite a bit more in the third part of this book on persistence ,\nwhere we’ll discuss devices and I/O in general, and then disks , RAIDs,\nand ﬁle systems in great detail.\n8A device driver is some code in the operating system that knows how to d eal with a\nspeciﬁc device. We will talk more about devices and device drivers later.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTRODUCTION TO OPERATING SYSTEMS 11\n2.5 Design Goals\nSo now you have some idea of what an OS actually does: it takes phys-\nicalresources , such as a CPU, memory, or disk, and virtualizes them. It\nhandles tough and tricky issues related to concurrency . And it stores ﬁles\npersistently , thus making them safe over the long-term. Given that we\nwant to build such a system, we want to have some goals in mind to h elp\nfocus our design and implementation and make trade-offs as neces sary;\nﬁnding the right set of trade-offs is a key to building systems.\nOne of the most basic goals is to build up some abstractions in order\nto make the system convenient and easy to use. Abstractions are fun-\ndamental to everything we do in computer science. Abstraction makes\nit possible to write a large program by dividing it into small an d under-\nstandable pieces, to write such a program in a high-level lang uage like\nC9without thinking about assembly, to write code in assembly with out\nthinking about logic gates, and to build a processor out of gates wit hout\nthinking too much about transistors. Abstraction is so fundamen tal that\nsometimes we forget its importance, but we won’t here; thus, in eac h sec-\ntion, we’ll discuss some of the major abstractions that have develop ed\nover time, giving you a way to think about pieces of the OS.\nOne goal in designing and implementing an operating system is t o\nprovide high performance ; another way to say this is our goal is to mini-\nmize the overheads of the OS. Virtualization and making the system easy\nto use are well worth it, but not at any cost; thus, we must strive t o pro-\nvide virtualization and other OS features without excessive ove rheads.\nThese overheads arise in a number of forms: extra time (more instr uc-\ntions) and extra space (in memory or on disk). We’ll seek solutions th at\nminimize one or the other or both, if possible. Perfection, however, i s not\nalways attainable, something we will learn to notice and (wher e appro-\npriate) tolerate.\nAnother goal will be to provide protection between applications, as\nwell as between the OS and applications. Because we wish to all ow\nmany programs to run at the same time, we want to make sure that t he\nmalicious or accidental bad behavior of one does not harm others; we\ncertainly don’t want an application to be able to harm the OS itse lf (as\nthat would affect allprograms running on the system). Protection is at\nthe heart of one of the main principles underlying an operating sy stem,\nwhich is that of isolation ; isolating processes from one another is the key\nto protection and thus underlies much of what an OS must do.\nThe operating system must also run non-stop; when it fails, allappli-\ncations running on the system fail as well. Because of this depen dence,\noperating systems often strive to provide a high degree of reliability . As\noperating systems grow evermore complex (sometimes containing mi l-\nlions of lines of code), building a reliable operating system is qu ite a chal-\n9Some of you might object to calling C a high-level language. Remember t his is an OS\ncourse, though, where we’re simply happy not to have to code in assembl y all the time!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 INTRODUCTION TO OPERATING SYSTEMS\nlenge — and indeed, much of the on-going research in the ﬁeld (inc luding\nsome of our own work [BS+09, SS+10]) focuses on this exact problem.\nOther goals make sense: energy-efﬁciency is important in our increas-\ningly green world; security (an extension of protection, really) against\nmalicious applications is critical, especially in these high ly-networked\ntimes; mobility is increasingly important as OSes are run on smaller and\nsmaller devices. Depending on how the system is used, the OS wil l have\ndifferent goals and thus likely be implemented in at least sli ghtly differ-\nent ways. However, as we will see, many of the principles we will present\non how to build an OS are useful on a range of different devices.\n2.6 Some History\nBefore closing this introduction, let us present a brief history of how\noperating systems developed. Like any system built by humans, good\nideas accumulated in operating systems over time, as engineer s learned\nwhat was important in their design. Here, we discuss a few major devel-\nopments. For a richer treatment, see Brinch Hansen’s excellent history of\noperating systems [BH00].\nEarly Operating Systems: Just Libraries\nIn the beginning, the operating system didn’t do too much. Basic ally,\nit was just a set of libraries of commonly-used functions; for examp le,\ninstead of having each programmer of the system write low-level I /O\nhandling code, the “OS” would provide such APIs, and thus make lif e\neasier for the developer.\nUsually, on these old mainframe systems, one program ran at a time ,\nas controlled by a human operator. Much of what you think a modern\nOS would do (e.g., deciding what order to run jobs in) was performe d by\nthis operator. If you were a smart developer, you would be nice to thi s\noperator, so that they might move your job to the front of the queue.\nThis mode of computing was known as batch processing, as a number\nof jobs were set up and then run in a “batch” by the operator. Compute rs,\nas of that point, were not used in an interactive manner, because of cost:\nit was simply too expensive to let a user sit in front of the compute r and\nuse it, as most of the time it would just sit idle then, costing the f acility\nhundreds of thousands of dollars per hour [BH00].\nBeyond Libraries: Protection\nIn moving beyond being a simple library of commonly-used services , op-\nerating systems took on a more central role in managing machines. O ne\nimportant aspect of this was the realization that code run on behal f of the\nOS was special; it had control of devices and thus should be treate d dif-\nferently than normal application code. Why is this? Well, imagi ne if you\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTRODUCTION TO OPERATING SYSTEMS 13\nallowed any application to read from anywhere on the disk; the noti on of\nprivacy goes out the window, as any program could read any ﬁle. Thus ,\nimplementing a ﬁle system (to manage your ﬁles) as a library makes little\nsense. Instead, something else was needed.\nThus, the idea of a system call was invented, pioneered by the Atlas\ncomputing system [K+61,L78]. Instead of providing OS routines a s a li-\nbrary (where you just make a procedure call to access them), the idea here\nwas to add a special pair of hardware instructions and hardware state to\nmake the transition into the OS a more formal, controlled process.\nThe key difference between a system call and a procedure call i s that\na system call transfers control (i.e., jumps) into the OS while simultane-\nously raising the hardware privilege level . User applications run in what\nis referred to as user mode which means the hardware restricts what ap-\nplications can do; for example, an application running in user mod e can’t\ntypically initiate an I/O request to the disk, access any phy sical memory\npage, or send a packet on the network. When a system call is initia ted\n(usually through a special hardware instruction called a trap), the hard-\nware transfers control to a pre-speciﬁed trap handler (that the OS set up\npreviously) and simultaneously raises the privilege level to kernel mode .\nIn kernel mode, the OS has full access to the hardware of the syst em and\nthus can do things like initiate an I/O request or make more memor y\navailable to a program. When the OS is done servicing the reques t, it\npasses control back to the user via a special return-from-trap instruction,\nwhich reverts to user mode while simultaneously passing control back to\nwhere the application left off.\nThe Era of Multiprogramming\nWhere operating systems really took off was in the era of computing b e-\nyond the mainframe, that of the minicomputer . Classic machines like\nthe PDP family from Digital Equipment made computers hugely mor e\naffordable; thus, instead of having one mainframe per large orga nization,\nnow a smaller collection of people within an organization could likel y\nhave their own computer. Not surprisingly, one of the major impacts of\nthis drop in cost was an increase in developer activity; more smar t people\ngot their hands on computers and thus made computer systems do more\ninteresting and beautiful things.\nIn particular, multiprogramming became commonplace due to the de-\nsire to make better use of machine resources. Instead of just run ning one\njob at a time, the OS would load a number of jobs into memory and switch\nrapidly between them, thus improving CPU utilization. This sw itching\nwas particularly important because I/O devices were slow; hav ing a pro-\ngram wait on the CPU while its I/O was being serviced was a waste of\nCPU time. Instead, why not switch to another job and run it for a whi le?\nThe desire to support multiprogramming and overlap in the prese nce\nof I/O and interrupts forced innovation in the conceptual developm ent of\noperating systems along a number of directions. Issues such as memory\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 INTRODUCTION TO OPERATING SYSTEMS\nprotection became important; we wouldn’t want one program to be able\nto access the memory of another program. Understanding how to deal\nwith the concurrency issues introduced by multiprogramming was also\ncritical; making sure the OS was behaving correctly despite t he presence\nof interrupts is a great challenge. We will study these issues and related\ntopics later in the book.\nOne of the major practical advances of the time was the introducti on\nof the U NIX operating system, primarily thanks to Ken Thompson (and\nDennis Ritchie) at Bell Labs (yes, the phone company). U NIXtook many\ngood ideas from different operating systems (particularly from M ultics\n[O72], and some from systems like TENEX [B+72] and the Berkeley Time-\nSharing System [S+68]), but made them simpler and easier to use. Soon\nthis team was shipping tapes containing U NIX source code to people\naround the world, many of whom then got involved and added to the\nsystem themselves; see the Aside (next page) for more detail10.\nThe Modern Era\nBeyond the minicomputer came a new type of machine, cheaper, fas ter,\nand for the masses: the personal computer , orPCas we call it today. Led\nby Apple’s early machines (e.g., the Apple II) and the IBM PC, t his new\nbreed of machine would soon become the dominant force in computing,\nas their low-cost enabled one machine per desktop instead of a shar ed\nminicomputer per workgroup.\nUnfortunately, for operating systems, the PC at ﬁrst represent ed a\ngreat leap backwards, as early systems forgot (or never knew of) t he\nlessons learned in the era of minicomputers. For example, early op erat-\ning systems such as DOS (the Disk Operating System , from Microsoft )\ndidn’t think memory protection was important; thus, a malicious (or per-\nhaps just a poorly-programmed) application could scribble all ove r mem-\nory. The ﬁrst generations of the Mac OS (v9 and earlier) took a coopera-\ntive approach to job scheduling; thus, a thread that accidenta lly got stuck\nin an inﬁnite loop could take over the entire system, forcing a reboot . The\npainful list of OS features missing in this generation of system s is long,\ntoo long for a full discussion here.\nFortunately, after some years of suffering, the old features of mi ni-\ncomputer operating systems started to ﬁnd their way onto the des ktop.\nFor example, Mac OS X/macOS has U NIXat its core, including all of the\nfeatures one would expect from such a mature system. Windows has s im-\nilarly adopted many of the great ideas in computing history, star ting in\nparticular with Windows NT, a great leap forward in Microsoft OS t ech-\nnology. Even today’s cell phones run operating systems (such as Lin ux)\nthat are much more like what a minicomputer ran in the 1970s than what\n10We’ll use asides and other related text boxes to call attention to various items that don’t\nquite ﬁt the main ﬂow of the text. Sometimes, we’ll even use them j ust to make a joke, because\nwhy not have a little fun along the way? Yes, many of the jokes are bad.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTRODUCTION TO OPERATING SYSTEMS 15\nASIDE : THEIMPORTANCE OF UNIX\nIt is difﬁcult to overstate the importance of U NIX in the history of oper-\nating systems. Inﬂuenced by earlier systems (in particular , the famous\nMultics system from MIT), U NIXbrought together many great ideas and\nmade a system that was both simple and powerful.\nUnderlying the original “Bell Labs” U NIX was the unifying principle of\nbuilding small powerful programs that could be connected togethe r to\nform larger workﬂows. The shell , where you type commands, provided\nprimitives such as pipes to enable such meta-level programming, and\nthus it became easy to string together programs to accomplish a b ig-\nger task. For example, to ﬁnd lines of a text ﬁle that have the word\n“foo” in them, and then to count how many such lines exist, you would\ntype:grep foo file.txt|wc -l , thus using the grep andwc(word\ncount) programs to achieve your task.\nThe U NIX environment was friendly for programmers and developers\nalike, also providing a compiler for the new C programming language .\nMaking it easy for programmers to write their own programs, as wel l as\nshare them, made U NIX enormously popular. And it probably helped a\nlot that the authors gave out copies for free to anyone who asked, an e arly\nform of open-source software .\nAlso of critical importance was the accessibility and readabi lity of the\ncode. Having a beautiful, small kernel written in C invited oth ers to play\nwith the kernel, adding new and cool features. For example, an en ter-\nprising group at Berkeley, led by Bill Joy , made a wonderful distribution\n(the Berkeley Systems Distribution , orBSD ) which had some advanced\nvirtual memory, ﬁle system, and networking subsystems. Joy lat er co-\nfounded Sun Microsystems .\nUnfortunately, the spread of U NIXwas slowed a bit as companies tried to\nassert ownership and proﬁt from it, an unfortunate (but common) res ult\nof lawyers getting involved. Many companies had their own varian ts:\nSunOS from Sun Microsystems, AIX from IBM, HPUX (a.k.a. “H-Pucks”)\nfrom HP , and IRIX from SGI. The legal wrangling among AT&T/Bell\nLabs and these other players cast a dark cloud over U NIX, and many\nwondered if it would survive, especially as Windows was introduc ed and\ntook over much of the PC market...\na PC ran in the 1980s (thank goodness); it is good to see that the good\nideas developed in the heyday of OS development have found their w ay\ninto the modern world. Even better is that these ideas continue t o de-\nvelop, providing more features and making modern systems even be tter\nfor users and applications.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 INTRODUCTION TO OPERATING SYSTEMS\nASIDE : ANDTHEN CAME LINUX\nFortunately for U NIX, a young Finnish hacker named Linus Torvalds de-\ncided to write his own version of U NIX which borrowed heavily on the\nprinciples and ideas behind the original system, but not from th e code\nbase, thus avoiding issues of legality. He enlisted help from ma ny others\naround the world, took advantage of the sophisticated GNU tools that\nalready existed [G85], and soon Linux was born (as well as the modern\nopen-source software movement).\nAs the internet era came into place, most companies (such as Googl e,\nAmazon, Facebook, and others) chose to run Linux, as it was free and\ncould be readily modiﬁed to suit their needs; indeed, it is hard to imag-\nine the success of these new companies had such a system not exist ed.\nAs smart phones became a dominant user-facing platform, Linux f ound\na stronghold there too (via Android), for many of the same reasons. An d\nSteve Jobs took his U NIX-based NeXTStep operating environment with\nhim to Apple, thus making U NIX popular on desktops (though many\nusers of Apple technology are probably not even aware of this fact). Thus\nUNIX lives on, more important today than ever before. The computing\ngods, if you believe in them, should be thanked for this wonderful ou t-\ncome.\n2.7 Summary\nThus, we have an introduction to the OS. Today’s operating systems\nmake systems relatively easy to use, and virtually all operat ing systems\nyou use today have been inﬂuenced by the developments we will dis cuss\nthroughout the book.\nUnfortunately, due to time constraints, there are a number of pa rts of\nthe OS we won’t cover in the book. For example, there is a lot of net-\nworking code in the operating system; we leave it to you to take the net-\nworking class to learn more about that. Similarly, graphics devices are\nparticularly important; take the graphics course to expand you r knowl-\nedge in that direction. Finally, some operating system books talk a great\ndeal about security ; we will do so in the sense that the OS must provide\nprotection between running programs and give users the ability to pro-\ntect their ﬁles, but we won’t delve into deeper security issues that one\nmight ﬁnd in a security course.\nHowever, there are many important topics that we will cover, incl ud-\ning the basics of virtualization of the CPU and memory, concurrenc y, and\npersistence via devices and ﬁle systems. Don’t worry! While the re is a\nlot of ground to cover, most of it is quite cool, and at the end of the road,\nyou’ll have a new appreciation for how computer systems really work.\nNow get to work!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTRODUCTION TO OPERATING SYSTEMS 17\nReferences\n[BS+09] “Tolerating File-System Mistakes with EnvyFS” by L. Bair avasundaram, S. Sundarara-\nman, A. Arpaci-Dusseau, R. Arpaci-Dusseau. USENIX ’09, San Diego , CA, June 2009. A fun\npaper about using multiple ﬁle systems at once to tolerate a mistake in any one of the m.\n[BH00] “The Evolution of Operating Systems” by P . Brinch Hansen. In ’Cl assic Operating\nSystems: From Batch Processing to Distributed Systems.’ Springe r-Verlag, New York, 2000.\nThis essay provides an intro to a wonderful collection of papers about histori cally signiﬁcant systems.\n[B+72] “TENEX, A Paged Time Sharing System for the PDP-10” by D. Bobrow, J. Burchﬁel, D.\nMurphy, R. Tomlinson. CACM, Volume 15, Number 3, March 1972. TENEX has much of the\nmachinery found in modern operating systems; read more about it to see how mu ch innovation was\nalready in place in the early 1970’s.\n[B75] “The Mythical Man-Month” by F. Brooks. Addison-Wesley, 1975. A classic text on software\nengineering; well worth the read.\n[BOH10] “Computer Systems: A Programmer’s Perspective” by R. Bry ant and D. O’Hallaron.\nAddison-Wesley, 2010. Another great intro to how computer systems work. Has a little bit of overlap\nwith this book — so if you’d like, you can skip the last few chapters of that book, or sim ply read them to\nget a different perspective on some of the same material. After all, one good w ay to build up your own\nknowledge is to hear as many other perspectives as possible, and then develop your own opinion and\nthoughts on the matter. You know, by thinking!\n[G85] “The GNU Manifesto” by R. Stallman. 1985. www.gnu.org/gnu/manifesto.html .\nA huge part of Linux’s success was no doubt the presence of an excellen t compiler, gcc, and other\nrelevant pieces of open software, thanks to the GNU effort headed by Stallman. S tallman is a visionary\nwhen it comes to open source, and this manifesto lays out his thoughts as to why.\n[K+61] “One-Level Storage System” by T. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner.\nIRE Transactions on Electronic Computers, April 1962. The Atlas pioneered much of what you see\nin modern systems. However, this paper is not the best read. If you were to on ly read one, you might\ntry the historical perspective below [L78].\n[L78] “The Manchester Mark I and Atlas: A Historical Perspective” by S. H. L avington. Com-\nmunications of the ACM, Volume 21:1, January 1978. A nice piece of history on the early devel-\nopment of computer systems and the pioneering efforts of the Atlas. Of course, one could go back and\nread the Atlas papers themselves, but this paper provides a great overvie w and adds some historical\nperspective.\n[O72] “The Multics System: An Examination of its Structure” by Elliot t Organick. MIT Press,\n1972. A great overview of Multics. So many good ideas, and yet it was an over-design ed system,\nshooting for too much, and thus never really worked. A classic example of what F red Brooks would call\nthe “second-system effect” [B75].\n[PP03] “Introduction to Computing Systems: From Bits and Gates to C a nd Beyond” by Yale\nN. Patt, Sanjay J. Patel. McGraw-Hill, 2003. One of our favorite intro to computing systems books.\nStarts at transistors and gets you all the way up to C; the early material is particularly great.\n[RT74] “The U NIXTime-Sharing System” by Dennis M. Ritchie, Ken Thompson. CACM, Vol-\nume 17: 7, July 1974. A great summary of UNIXwritten as it was taking over the world of computing,\nby the people who wrote it.\n[S68] “SDS 940 Time-Sharing System” by Scientiﬁc Data Systems. TECH NICAL MANUAL,\nSDS 90 11168, August 1968. Yes, a technical manual was the best we could ﬁnd. But it is fascinating\nto read these old system documents, and see how much was already in place in the late 1960’s. One of\nthe minds behind the Berkeley Time-Sharing System (which eventually b ecame the SDS system) was\nButler Lampson, who later won a Turing award for his contributions in systems .\n[SS+10] “Membrane: Operating System Support for Restartable File Systems” by S. Sundarara-\nman, S. Subramanian, A. Rajimwale, A. Arpaci-Dusseau, R. Arpaci-Du sseau, M. Swift. FAST\n’10, San Jose, CA, February 2010. The great thing about writing your own class notes: you can ad-\nvertise your own research. But this paper is actually pretty neat — when a ﬁl e system hits a bug and\ncrashes, Membrane auto-magically restarts it, all without applications or the rest of the system being\naffected.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n18 INTRODUCTION TO OPERATING SYSTEMS\nHomework\nMost (and eventually, all) chapters of this book have homework sec-\ntions at the end. Doing these homeworks is important, as each lets y ou,\nthe reader, gain more experience with the concepts presented w ithin the\nchapter.\nThere are two types of homeworks. The ﬁrst is based on simulation . A\nsimulation of a computer system is just a simple program that pret ends to\ndo some of the interesting parts of what a real system does, and the n re-\nport some output metrics to show how the system behaves. For example ,\na hard drive simulator might take a series of requests, simulat e how long\nthey would take to get serviced by a hard drive with certain per formance\ncharacteristics, and then report the average latency of the re quests.\nThe cool thing about simulations is they let you easily explore how\nsystems behave without the difﬁculty of running a real system. Indeed,\nthey even let you create systems that cannot exist in the real wor ld (for\nexample, a hard drive with unimaginably fast performance), a nd thus see\nthe potential impact of future technologies.\nOf course, simulations are not without their downsides. By their v ery\nnature, simulations are just approximations of how a real system b ehaves.\nIf an important aspect of real-world behavior is omitted, the simu lation\nwill report bad results. Thus, results from a simulation should a lways be\ntreated with some suspicion. In the end, how a system behaves in t he real\nworld is what matters.\nThe second type of homework requires interaction with real-world\ncode . Some of these homeworks are measurement focused, whereas oth-\ners just require some small-scale development and experiment ation. Both\nare just small forays into the larger world you should be getting i nto,\nwhich is how to write systems code in C on U NIX-based systems. Indeed,\nlarger-scale projects, which go beyond these homeworks, are nee ded to\npush you in this direction; thus, beyond just doing homeworks, we st rongly\nrecommend you do projects to solidify your systems skills. See this page\n(https://github.com/remzi-arpacidusseau/ostep-projec ts)\nfor some projects.\nTo do these homeworks, you likely have to be on a U NIX-based ma-\nchine, running either Linux, macOS, or some similar system. It s hould\nalso have a C compiler installed (e.g., gcc) as well as Python. You should\nalso know how to edit code in a real code editor of some kind.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",48560
06-Part I Virtualization.pdf,06-Part I Virtualization,Part I\nVirtualization\n1,25
07-3. Dialogue.pdf,07-3. Dialogue,"3\nA Dialogue on Virtualization\nProfessor: And thus we reach the ﬁrst of our three pieces on operating system s:\nvirtualization .\nStudent: But what is virtualization, oh noble professor?\nProfessor: Imagine we have a peach.\nStudent: A peach? (incredulous)\nProfessor: Yes, a peach. Let us call that the physical peach. But we have many\neaters who would like to eat this peach. What we would like to present t o each\neater is their own peach, so that they can be happy. We call the pea ch we give\neaters virtual peaches; we somehow create many of these virtual peaches out o f\nthe one physical peach. And the important thing: in this illusion, it look s to each\neater like they have a physical peach, but in reality they don’t.\nStudent: So you are sharing the peach, but you don’t even know it?\nProfessor: Right! Exactly.\nStudent: But there’s only one peach.\nProfessor: Yes. And...?\nStudent: Well, if I was sharing a peach with somebody else, I think I would\nnotice.\nProfessor: Ah yes! Good point. But that is the thing with many eaters; most\nof the time they are napping or doing something else, and thus, you c an snatch\nthat peach away and give it to someone else for a while. And thus we cre ate the\nillusion of many virtual peaches, one peach for each person!\nStudent: Sounds like a bad campaign slogan. You are talking about computers,\nright Professor?\nProfessor: Ah, young grasshopper, you wish to have a more concrete example .\nGood idea! Let us take the most basic of resources, the CPU. Assu me there is one\nphysical CPU in a system (though now there are often two or four or m ore). What\nvirtualization does is take that single CPU and make it look like many virtu al\nCPUs to the applications running on the system. Thus, while each app lication\n3\n4 A D IALOGUE ON VIRTUALIZATION\nthinks it has its own CPU to use, there is really only one. And thus the O S has\ncreated a beautiful illusion: it has virtualized the CPU.\nStudent: Wow! That sounds like magic. Tell me more! How does that work?\nProfessor: In time, young student, in good time. Sounds like you are ready to\nbegin.\nStudent: I am! Well, sort of. I must admit, I’m a little worried you are going to\nstart talking about peaches again.\nProfessor: Don’t worry too much; I don’t even like peaches. And thus we be-\ngin...\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",2374
08-4. Processes.pdf,08-4. Processes,"4\nThe Abstraction: The Process\nIn this chapter, we discuss one of the most fundamental abstract ions that\nthe OS provides to users: the process . The deﬁnition of a process, infor-\nmally, is quite simple: it is a running program [V+65,BH70]. The program\nitself is a lifeless thing: it just sits there on the disk, a bun ch of instructions\n(and maybe some static data), waiting to spring into action. It is the oper-\nating system that takes these bytes and gets them running, tr ansforming\nthe program into something useful.\nIt turns out that one often wants to run more than one program at\nonce; for example, consider your desktop or laptop where you might lik e\nto run a web browser, mail program, a game, a music player, and so forth.\nIn fact, a typical system may be seemingly running tens or even hundreds\nof processes at the same time. Doing so makes the system easy to us e, as\none never need be concerned with whether a CPU is available; one s imply\nruns programs. Hence our challenge:\nTHECRUX OF THE PROBLEM :\nHOWTOPROVIDE THEILLUSION OFMANY CPU S?\nAlthough there are only a few physical CPUs available, how can th e\nOS provide the illusion of a nearly-endless supply of said CPUs?\nThe OS creates this illusion by virtualizing the CPU. By running one\nprocess, then stopping it and running another, and so forth, the O S can\npromote the illusion that many virtual CPUs exist when in fact th ere is\nonly one physical CPU (or a few). This basic technique, known as time\nsharing of the CPU, allows users to run as many concurrent processes as\nthey would like; the potential cost is performance, as each will r un more\nslowly if the CPU(s) must be shared.\nTo implement virtualization of the CPU, and to implement it wel l, the\nOS will need both some low-level machinery and some high-level in -\ntelligence. We call the low-level machinery mechanisms ; mechanisms\nare low-level methods or protocols that implement a needed piece of\nfunctionality. For example, we’ll learn later how to implement a context\n1\n2 THEABSTRACTION : THEPROCESS\nTIP: USETIME SHARING (AND SPACE SHARING )\nTime sharing is a basic technique used by an OS to share a resource. By\nallowing the resource to be used for a little while by one entity, a nd then\na little while by another, and so forth, the resource in question ( e.g., the\nCPU, or a network link) can be shared by many. The counterpart of ti me\nsharing is space sharing , where a resource is divided (in space) among\nthose who wish to use it. For example, disk space is naturally a s pace-\nshared resource; once a block is assigned to a ﬁle, it is normally n ot as-\nsigned to another ﬁle until the user deletes the original ﬁle.\nswitch , which gives the OS the ability to stop running one program and\nstart running another on a given CPU; this time-sharing mechanism is\nemployed by all modern OSes.\nOn top of these mechanisms resides some of the intelligence in the\nOS, in the form of policies . Policies are algorithms for making some\nkind of decision within the OS. For example, given a number of possi-\nble programs to run on a CPU, which program should the OS run? A\nscheduling policy in the OS will make this decision, likely using histori-\ncal information (e.g., which program has run more over the last min ute?),\nworkload knowledge (e.g., what types of programs are run), and per for-\nmance metrics (e.g., is the system optimizing for interactive performance,\nor throughput?) to make its decision.\n4.1 The Abstraction: A Process\nThe abstraction provided by the OS of a running program is somethin g\nwe will call a process . As we said above, a process is simply a running\nprogram; at any instant in time, we can summarize a process by ta king an\ninventory of the different pieces of the system it accesses or aff ects during\nthe course of its execution.\nTo understand what constitutes a process, we thus have to under stand\nitsmachine state : what a program can read or update when it is running.\nAt any given time, what parts of the machine are important to the execu-\ntion of this program?\nOne obvious component of machine state that comprises a process is\nitsmemory . Instructions lie in memory; the data that the running pro-\ngram reads and writes sits in memory as well. Thus the memory tha t the\nprocess can address (called its address space ) is part of the process.\nAlso part of the process’s machine state are registers ; many instructions\nexplicitly read or update registers and thus clearly they are important to\nthe execution of the process.\nNote that there are some particularly special registers that f orm part\nof this machine state. For example, the program counter (PC) (sometimes\ncalled the instruction pointer orIP) tells us which instruction of the pro-\ngram is currently being executed; similarly a stack pointer and associated\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEABSTRACTION : THEPROCESS 3\nTIP: SEPARATE POLICY ANDMECHANISM\nIn many operating systems, a common design paradigm is to separa te\nhigh-level policies from their low-level mechanisms [L+75]. Y ou can\nthink of the mechanism as providing the answer to a how question about\na system; for example, how does an operating system perform a context\nswitch? The policy provides the answer to a which question; for example,\nwhich process should the operating system run right now? Separating the\ntwo allows one easily to change policies without having to rethin k the\nmechanism and is thus a form of modularity , a general software design\nprinciple.\nframe pointer are used to manage the stack for function parameters, local\nvariables, and return addresses.\nFinally, programs often access persistent storage devices too. Su chI/O\ninformation might include a list of the ﬁles the process currently has open.\n4.2 Process API\nThough we defer discussion of a real process API until a subsequen t\nchapter, here we ﬁrst give some idea of what must be included in a ny\ninterface of an operating system. These APIs, in some form, are av ailable\non any modern operating system.\n•Create: An operating system must include some method to cre-\nate new processes. When you type a command into the shell, or\ndouble-click on an application icon, the OS is invoked to create a\nnew process to run the program you have indicated.\n•Destroy: As there is an interface for process creation, systems also\nprovide an interface to destroy processes forcefully. Of course, many\nprocesses will run and just exit by themselves when complete; w hen\nthey don’t, however, the user may wish to kill them, and thus an in -\nterface to halt a runaway process is quite useful.\n•Wait: Sometimes it is useful to wait for a process to stop running;\nthus some kind of waiting interface is often provided.\n•Miscellaneous Control: Other than killing or waiting for a process,\nthere are sometimes other controls that are possible. For example,\nmost operating systems provide some kind of method to suspend a\nprocess (stop it from running for a while) and then resume it (con-\ntinue it running).\n•Status: There are usually interfaces to get some status information\nabout a process as well, such as how long it has run for, or what\nstate it is in.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 THEABSTRACTION : THEPROCESS\nMemory CPU\nDiskcode\nstatic data\nheap\nstack\nProcess\ncode\nstatic data\nProgramLoading:\nTakes on-disk program\nand reads it into the\naddress space of process\nFigure 4.1: Loading: From Program To Process\n4.3 Process Creation: A Little More Detail\nOne mystery that we should unmask a bit is how programs are trans-\nformed into processes. Speciﬁcally, how does the OS get a program up\nand running? How does process creation actually work?\nThe ﬁrst thing that the OS must do to run a program is to load its code\nand any static data (e.g., initialized variables) into memor y, into the ad-\ndress space of the process. Programs initially reside on disk (or, in some\nmodern systems, ﬂash-based SSDs ) in some kind of executable format ;\nthus, the process of loading a program and static data into memory r e-\nquires the OS to read those bytes from disk and place them in memor y\nsomewhere (as shown in Figure 4.1).\nIn early (or simple) operating systems, the loading process is don eea-\ngerly , i.e., all at once before running the program; modern OSes perform\nthe process lazily , i.e., by loading pieces of code or data only as they are\nneeded during program execution. To truly understand how lazy l oading\nof pieces of code and data works, you’ll have to understand more about\nthe machinery of paging and swapping , topics we’ll cover in the future\nwhen we discuss the virtualization of memory. For now, just rememb er\nthat before running anything, the OS clearly must do some work to get\nthe important program bits from disk into memory.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEABSTRACTION : THEPROCESS 5\nOnce the code and static data are loaded into memory, there are a f ew\nother things the OS needs to do before running the process. Some mem -\nory must be allocated for the program’s run-time stack (or just stack ).\nAs you should likely already know, C programs use the stack for local\nvariables, function parameters, and return addresses; the O S allocates\nthis memory and gives it to the process. The OS will also likely i nitial-\nize the stack with arguments; speciﬁcally, it will ﬁll in the parameters to\nthemain() function, i.e., argc and theargv array.\nThe OS may also allocate some memory for the program’s heap . In C\nprograms, the heap is used for explicitly requested dynamical ly-allocated\ndata; programs request such space by calling malloc() and free it ex-\nplicitly by calling free() . The heap is needed for data structures such as\nlinked lists, hash tables, trees, and other interesting data structures. The\nheap will be small at ﬁrst; as the program runs, and requests mor e mem-\nory via the malloc() library API, the OS may get involved and allocate\nmore memory to the process to help satisfy such calls.\nThe OS will also do some other initialization tasks, particular ly as re-\nlated to input/output (I/O). For example, in U NIXsystems, each process\nby default has three open ﬁle descriptors , for standard input, output, and\nerror; these descriptors let programs easily read input from the terminal\nand print output to the screen. We’ll learn more about I/O, ﬁle des crip-\ntors, and the like in the third part of the book on persistence .\nBy loading the code and static data into memory, by creating and i ni-\ntializing a stack, and by doing other work as related to I/O setup , the OS\nhas now (ﬁnally) set the stage for program execution. It thus has on e last\ntask: to start the program running at the entry point, namely main() . By\njumping to the main() routine (through a specialized mechanism that\nwe will discuss next chapter), the OS transfers control of the CP U to the\nnewly-created process, and thus the program begins its execut ion.\n4.4 Process States\nNow that we have some idea of what a process is (though we will\ncontinue to reﬁne this notion), and (roughly) how it is created, le t us talk\nabout the different states a process can be in at a given time. The notion\nthat a process can be in one of these states arose in early computer s ystems\n[DV66,V+65]. In a simpliﬁed view, a process can be in one of three states:\n•Running : In the running state, a process is running on a processor.\nThis means it is executing instructions.\n•Ready : In the ready state, a process is ready to run but for some\nreason the OS has chosen not to run it at this given moment.\n•Blocked : In the blocked state, a process has performed some kind\nof operation that makes it not ready to run until some other event\ntakes place. A common example: when a process initiates an I/O\nrequest to a disk, it becomes blocked and thus some other process\ncan use the processor.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 THEABSTRACTION : THEPROCESS\nRunning Ready\nBlockedDescheduled\nScheduled\nI/O: initiate I/O: done\nFigure 4.2: Process: State Transitions\nIf we were to map these states to a graph, we would arrive at the d i-\nagram in Figure 4.2. As you can see in the diagram, a process can b e\nmoved between the ready and running states at the discretion of t he OS.\nBeing moved from ready to running means the process has been sched-\nuled ; being moved from running to ready means the process has been\ndescheduled . Once a process has become blocked (e.g., by initiating an\nI/O operation), the OS will keep it as such until some event occurs (e.g.,\nI/O completion); at that point, the process moves to the ready stat e again\n(and potentially immediately to running again, if the OS so de cides).\nLet’s look at an example of how two processes might transition through\nsome of these states. First, imagine two processes running, eac h of which\nonly use the CPU (they do no I/O). In this case, a trace of the stat e of each\nprocess might look like this (Figure 4.3).\nTime Process 0 Process 1 Notes\n1 Running Ready\n2 Running Ready\n3 Running Ready\n4 Running Ready Process 0now done\n5 – Running\n6 – Running\n7 – Running\n8 – Running Process 1now done\nFigure 4.3: Tracing Process State: CPU Only\nIn this next example, the ﬁrst process issues an I/O after runn ing for\nsome time. At that point, the process is blocked, giving the other p rocess\na chance to run. Figure 4.4 shows a trace of this scenario.\nMore speciﬁcally, Process 0initiates an I/O and becomes blocked wait-\ning for it to complete; processes become blocked, for example, when read-\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEABSTRACTION : THEPROCESS 7\nTime Process 0 Process 1 Notes\n1 Running Ready\n2 Running Ready\n3 Running Ready Process 0initiates I/O\n4 Blocked Running Process 0is blocked,\n5 Blocked Running so Process 1runs\n6 Blocked Running\n7 Ready Running I/O done\n8 Ready Running Process 1now done\n9 Running –\n10 Running – Process 0now done\nFigure 4.4: Tracing Process State: CPU and I/O\ning from a disk or waiting for a packet from a network. The OS recog-\nnizes Process 0is not using the CPU and starts running Process 1. While\nProcess 1is running, the I/O completes, moving Process 0back to ready.\nFinally, Process 1ﬁnishes, and Process 0runs and then is done.\nNote that there are many decisions the OS must make, even in this\nsimple example. First, the system had to decide to run Process 1while\nProcess 0issued an I/O; doing so improves resource utilization by keep-\ning the CPU busy. Second, the system decided not to switch back to\nProcess 0when its I/O completed; it is not clear if this is a good deci-\nsion or not. What do you think? These types of decisions are made by th e\nOSscheduler , a topic we will discuss a few chapters in the future.\n4.5 Data Structures\nThe OS is a program, and like any program, it has some key data stru c-\ntures that track various relevant pieces of information. To trac k the state\nof each process, for example, the OS likely will keep some kind of pro-\ncess list for all processes that are ready and some additional informa-\ntion to track which process is currently running. The OS must al so track,\nin some way, blocked processes; when an I/O event completes, the O S\nshould make sure to wake the correct process and ready it to run ag ain.\nFigure 4.5 shows what type of information an OS needs to track about\neach process in the xv6 kernel [CK+08]. Similar process structu res exist\nin “real” operating systems such as Linux, Mac OS X, or Windows; l ook\nthem up and see how much more complex they are.\nFrom the ﬁgure, you can see a couple of important pieces of informa-\ntion the OS tracks about a process. The register context will hold, for a\nstopped process, the contents of its registers. When a process is s topped,\nits registers will be saved to this memory location; by restoring these reg-\nisters (i.e., placing their values back into the actual phys ical registers), the\nOS can resume running the process. We’ll learn more about this tec hnique\nknown as a context switch in future chapters.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 THEABSTRACTION : THEPROCESS\n// the registers xv6 will save and restore\n// to stop and subsequently restart a process\nstruct context {\nint eip;\nint esp;\nint ebx;\nint ecx;\nint edx;\nint esi;\nint edi;\nint ebp;\n};\n// the different states a process can be in\nenum proc_state { UNUSED, EMBRYO, SLEEPING,\nRUNNABLE, RUNNING, ZOMBIE };\n// the information xv6 tracks about each process\n// including its register context and state\nstruct proc {\nchar*mem; // Start of process memory\nuint sz; // Size of process memory\nchar*kstack; // Bottom of kernel stack\n// for this process\nenum proc_state state; // Process state\nint pid; // Process ID\nstruct proc *parent; // Parent process\nvoid*chan; // If non-zero, sleeping on chan\nint killed; // If non-zero, have been killed\nstruct file *ofile[NOFILE]; // Open files\nstruct inode *cwd; // Current directory\nstruct context context; // Switch here to run process\nstruct trapframe *tf; // Trap frame for the\n// current interrupt\n};\nFigure 4.5: The xv6 Proc Structure\nYou can also see from the ﬁgure that there are some other states a pr o-\ncess can be in, beyond running, ready, and blocked. Sometimes a sy stem\nwill have an initial state that the process is in when it is being created.\nAlso, a process could be placed in a ﬁnal state where it has exited but\nhas not yet been cleaned up (in UNIX-based systems, this is cal led the\nzombie state1). This ﬁnal state can be useful as it allows other processes\n(usually the parent that created the process) to examine the return code\nof the process and see if the just-ﬁnished process executed succ essfully\n(usually, programs return zero in U NIX-based systems when they have\naccomplished a task successfully, and non-zero otherwise). Wh en ﬁn-\nished, the parent will make one ﬁnal call (e.g., wait() ) to wait for the\ncompletion of the child, and to also indicate to the OS that it can clean up\nany relevant data structures that referred to the now-extinc t process.\n1Yes, the zombie state. Just like real zombies, these zombies are relatively easy to kill.\nHowever, different techniques are usually recommended.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEABSTRACTION : THEPROCESS 9\nASIDE : DATA STRUCTURE — T HEPROCESS LIST\nOperating systems are replete with various important data structures\nthat we will discuss in these notes. The process list (also called the task\nlist) is the ﬁrst such structure. It is one of the simpler ones, but cer tainly\nany OS that has the ability to run multiple programs at once will have\nsomething akin to this structure in order to keep track of all the running\nprograms in the system. Sometimes people refer to the individual struc-\nture that stores information about a process as a Process Control Block\n(PCB ), a fancy way of talking about a C structure that contains informa -\ntion about each process (also sometimes called a process descriptor ).\nASIDE : KEYPROCESS TERMS\n•The process is the major OS abstraction of a running program. At\nany point in time, the process can be described by its state: the con-\ntents of memory in its address space , the contents of CPU registers\n(including the program counter and stack pointer , among others),\nand information about I/O (such as open ﬁles which can be read or\nwritten).\n•The process API consists of calls programs can make related to pro-\ncesses. Typically, this includes creation, destruction, and other use-\nful calls.\n•Processes exist in one of many different process states , including\nrunning, ready to run, and blocked. Different events (e.g., g etting\nscheduled or descheduled, or waiting for an I/O to complete) tran -\nsition a process from one of these states to the other.\n•Aprocess list contains information about all processes in the sys-\ntem. Each entry is found in what is sometimes called a process\ncontrol block (PCB ), which is really just a structure that contains\ninformation about a speciﬁc process.\n4.6 Summary\nWe have introduced the most basic abstraction of the OS: the process .\nIt is quite simply viewed as a running program. With this concep tual\nview in mind, we will now move on to the nitty-gritty: the low-leve l\nmechanisms needed to implement processes, and the higher-le vel poli-\ncies required to schedule them in an intelligent way. By combi ning mech-\nanisms and policies, we will build up our understanding of how an op er-\nating system virtualizes the CPU.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 THEABSTRACTION : THEPROCESS\nReferences\n[BH70] “The Nucleus of a Multiprogramming System” by Per Brinch Hansen. C ommunica-\ntions of the ACM, Volume 13:4, April 1970. This paper introduces one of the ﬁrst microkernels in\noperating systems history, called Nucleus. The idea of smaller, more mi nimal systems is a theme that\nrears its head repeatedly in OS history; it all began with Brinch Hansen’s work described herein.\n[CK+08] “The xv6 Operating System” by Russ Cox, Frans Kaashoek, Robe rt Morris, Nickolai\nZeldovich. From: https://github.com/mit-pdos/xv6-public. The coolest real and little OS in the\nworld. Download and play with it to learn more about the details of how operating syste ms actually\nwork. We have been using an older version (2012-01-30-1-g1c41342) and hen ce some examples in the\nbook may not match the latest in the source.\n[DV66] “Programming Semantics for Multiprogrammed Computations” b y Jack B. Dennis,\nEarl C. Van Horn. Communications of the ACM, Volume 9, Number 3, March 1 966 . This paper\ndeﬁned many of the early terms and concepts around building multiprogramme d systems.\n[L+75] “Policy/mechanism separation in Hydra” by R. Levin, E. Cohen, W . Corwin, F. Pollack,\nW. Wulf. SOSP ’75, Austin, Texas, November 1975. An early paper about how to structure operat-\ning systems in a research OS known as Hydra. While Hydra never became a mainstream OS, some of\nits ideas inﬂuenced OS designers.\n[V+65] “Structure of the Multics Supervisor” by V .A. Vyssotsky, F . J. Corbato, R. M. Graham.\nFall Joint Computer Conference, 1965. An early paper on Multics, which described many of the basic\nideas and terms that we ﬁnd in modern systems. Some of the vision behind comp uting as a utility are\nﬁnally being realized in modern cloud systems.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEABSTRACTION : THEPROCESS 11\nHomework (Simulation)\nThis program, process-run.py , allows you to see how process states\nchange as programs run and either use the CPU (e.g., perform an a dd\ninstruction) or do I/O (e.g., send a request to a disk and wait for it to\ncomplete). See the README for details.\nQuestions\n1. Runprocess-run.py with the following ﬂags: -l 5:100,5:100 . What\nshould the CPU utilization be (e.g., the percent of time the CPU i s in use?)\nWhy do you know this? Use the -cand-pﬂags to see if you were right.\n2. Now run with these ﬂags: ./process-run.py -l 4:100,1:0 . These\nﬂags specify one process with 4 instructions (all to use the CPU) , and one\nthat simply issues an I/O and waits for it to be done. How long doe s it take\nto complete both processes? Use -cand-pto ﬁnd out if you were right.\n3. Switch the order of the processes: ./process-run.py -l 1:0,4:100 .\nWhat happens now? Does switching the order matter? Why? (As alw ays,\nuse-cand-pto see if you were right)\n4. We’ll now explore some of the other ﬂags. One important ﬂag is -S, which\ndetermines how the system reacts when a process issues an I/O. Wi th the\nﬂag set to SWITCH ON END, the system will NOT switch to another pro-\ncess while one is doing I/O, instead waiting until the process is completely\nﬁnished. What happens when you run the following two processe s (-l\n1:0,4:100 -c -S SWITCH ONEND), one doing I/O and the other doing\nCPU work?\n5. Now, run the same processes, but with the switching behavior s et to switch\nto another process whenever one is WAITING for I/O ( -l 1:0,4:100 -c\n-S SWITCH ONIO). What happens now? Use -cand-pto conﬁrm that\nyou are right.\n6. One other important behavior is what to do when an I/O complet es. With\n-I IORUNLATER , when an I/O completes, the process that issued it is not\nnecessarily run right away; rather, whatever was running at th e time keeps\nrunning. What happens when you run this combination of processe s? (Run\n./process-run.py -l 3:0,5:100,5:100,5:100 -S SWITCH ONIO\n-I IORUNLATER -c -p ) Are system resources being effectively utilized?\n7. Now run the same processes, but with -I IORUNIMMEDIATE set, which\nimmediately runs the process that issued the I/O. How does this beh avior\ndiffer? Why might running a process that just completed an I/O aga in be a\ngood idea?\n8. Now run with some randomly generated processes: -s 1 -l 3:50,3:50\nor-s 2 -l 3:50,3:50 or-s 3 -l 3:50,3:50 . See if you can pre-\ndict how the trace will turn out. What happens when you use the ﬂag -I\nIORUNIMMEDIATE vs.-I IORUNLATER ? What happens when you use\n-S SWITCH ONIOvs.-S SWITCH ONEND?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",25379
09-5. Process API.pdf,09-5. Process API,"5\nInterlude: Process API\nASIDE : INTERLUDES\nInterludes will cover more practical aspects of systems, inclu ding a par-\nticular focus on operating system APIs and how to use them. If you don ’t\nlike practical things, you could skip these interludes. But you should like\npractical things, because, well, they are generally useful in real life; com-\npanies, for example, don’t usually hire you for your non-practical s kills.\nIn this interlude, we discuss process creation in U NIX systems. U NIX\npresents one of the most intriguing ways to create a new process wi th\na pair of system calls: fork() andexec() . A third routine, wait() ,\ncan be used by a process wishing to wait for a process it has create d to\ncomplete. We now present these interfaces in more detail, with a few\nsimple examples to motivate us. And thus, our problem:\nCRUX: HOWTOCREATE ANDCONTROL PROCESSES\nWhat interfaces should the OS present for process creation and con -\ntrol? How should these interfaces be designed to enable powerful func-\ntionality, ease of use, and high performance?\n5.1 Thefork() System Call\nThefork() system call is used to create a new process [C63]. How-\never, be forewarned: it is certainly the strangest routine you w ill ever\ncall1. More speciﬁcally, you have a running program whose code looks\nlike what you see in Figure 5.1; examine the code, or better yet, t ype it in\nand run it yourself!\n1Well, OK, we admit that we don’t know that for sure; who knows what routine s you\ncall when no one is looking? But fork() is pretty odd, no matter how unusual your routine-\ncalling patterns are.\n1\n2 INTERLUDE : PROCESS API\n1#include <stdio.h>\n2#include <stdlib.h>\n3#include <unistd.h>\n4\n5int main(int argc, char *argv[]) {\n6printf(""hello world (pid:%d)\n"", (int) getpid());\n7int rc = fork();\n8if (rc < 0) { // fork failed; exit\n9 fprintf(stderr, ""fork failed\n"");\n10 exit(1);\n11 } else if (rc == 0) { // child (new process)\n12 printf(""hello, I am child (pid:%d)\n"", (int) getpid());\n13 } else { // parent goes down this path (main)\n14 printf(""hello, I am parent of %d (pid:%d)\n"",\n15 rc, (int) getpid());\n16 }\n17 return 0;\n18}\n19\nFigure 5.1: Callingfork() (p1.c )\nWhen you run this program (called p1.c ), you’ll see the following:\nprompt> ./p1\nhello world (pid:29146)\nhello, I am parent of 29147 (pid:29146)\nhello, I am child (pid:29147)\nprompt>\nLet us understand what happened in more detail in p1.c . When it\nﬁrst started running, the process prints out a hello world messa ge; in-\ncluded in that message is its process identiﬁer , also known as a PID. The\nprocess has a PID of 29146; in U NIX systems, the PID is used to name\nthe process if one wants to do something with the process, such as ( for\nexample) stop it from running. So far, so good.\nNow the interesting part begins. The process calls the fork() system\ncall, which the OS provides as a way to create a new process. The od d\npart: the process that is created is an (almost) exact copy of the calling pro-\ncess. That means that to the OS, it now looks like there are two copies of\nthe program p1running, and both are about to return from the fork()\nsystem call. The newly-created process (called the child , in contrast to the\ncreating parent ) doesn’t start running at main() , like you might expect\n(note, the “hello, world” message only got printed out once); rather , it\njust comes into life as if it had called fork() itself.\nYou might have noticed: the child isn’t an exact copy. Speciﬁcally, al-\nthough it now has its own copy of the address space (i.e., its own priv ate\nmemory), its own registers, its own PC, and so forth, the value it r eturns\nto the caller of fork() is different. Speciﬁcally, while the parent receives\nthe PID of the newly-created child, the child receives a retur n code of\nzero. This differentiation is useful, because it is simple the n to write the\ncode that handles the two different cases (as above).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : PROCESS API 3\n1#include <stdio.h>\n2#include <stdlib.h>\n3#include <unistd.h>\n4#include <sys/wait.h>\n5\n6int main(int argc, char *argv[]) {\n7printf(""hello world (pid:%d)\n"", (int) getpid());\n8int rc = fork();\n9if (rc < 0) { // fork failed; exit\n10 fprintf(stderr, ""fork failed\n"");\n11 exit(1);\n12 } else if (rc == 0) { // child (new process)\n13 printf(""hello, I am child (pid:%d)\n"", (int) getpid());\n14 } else { // parent goes down this path (main)\n15 int rc_wait = wait(NULL);\n16 printf(""hello, I am parent of %d (rc_wait:%d) (pid:%d)\n"",\n17 rc, rc_wait, (int) getpid());\n18 }\n19 return 0;\n20}\n21\nFigure 5.2: Callingfork() Andwait() (p2.c )\nYou might also have noticed: the output (of p1.c ) is not deterministic .\nWhen the child process is created, there are now two active proce sses in\nthe system that we care about: the parent and the child. Assumi ng we\nare running on a system with a single CPU (for simplicity), then either\nthe child or the parent might run at that point. In our example (ab ove),\nthe parent did and thus printed out its message ﬁrst. In other ca ses, the\nopposite might happen, as we show in this output trace:\nprompt> ./p1\nhello world (pid:29146)\nhello, I am child (pid:29147)\nhello, I am parent of 29147 (pid:29146)\nprompt>\nThe CPU scheduler , a topic we’ll discuss in great detail soon, deter-\nmines which process runs at a given moment in time; because the s ched-\nuler is complex, we cannot usually make strong assumptions about w hat\nit will choose to do, and hence which process will run ﬁrst. This non-\ndeterminism , as it turns out, leads to some interesting problems, par-\nticularly in multi-threaded programs ; hence, we’ll see a lot more non-\ndeterminism when we study concurrency in the second part of the book.\n5.2 Thewait() System Call\nSo far, we haven’t done much: just created a child that prints out a\nmessage and exits. Sometimes, as it turns out, it is quite useful for a\nparent to wait for a child process to ﬁnish what it has been doing. This\ntask is accomplished with the wait() system call (or its more complete\nsiblingwaitpid() ); see Figure 5.2 for details.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 INTERLUDE : PROCESS API\nIn this example ( p2.c ), the parent process calls wait() to delay its\nexecution until the child ﬁnishes executing. When the child i s done,\nwait() returns to the parent.\nAdding await() call to the code above makes the output determin-\nistic. Can you see why? Go ahead, think about it.\n(waiting for you to think .... and done)\nNow that you have thought a bit, here is the output:\nprompt> ./p2\nhello world (pid:29266)\nhello, I am child (pid:29267)\nhello, I am parent of 29267 (rc_wait:29267) (pid:29266)\nprompt>\nWith this code, we now know that the child will always print ﬁrst.\nWhy do we know that? Well, it might simply run ﬁrst, as before, an d\nthus print before the parent. However, if the parent does happen to run\nﬁrst, it will immediately call wait() ; this system call won’t return until\nthe child has run and exited2. Thus, even when the parent runs ﬁrst, it\npolitely waits for the child to ﬁnish running, then wait() returns, and\nthen the parent prints its message.\n5.3 Finally, The exec() System Call\nA ﬁnal and important piece of the process creation API is the exec()\nsystem call3. This system call is useful when you want to run a program\nthat is different from the calling program. For example, callin gfork()\ninp2.c is only useful if you want to keep running copies of the same\nprogram. However, often you want to run a different program;exec()\ndoes just that (Figure 5.3, page 5).\nIn this example, the child process calls execvp() in order to run the\nprogramwc, which is the word counting program. In fact, it runs wcon\nthe source ﬁle p3.c , thus telling us how many lines, words, and bytes are\nfound in the ﬁle:\nprompt> ./p3\nhello world (pid:29383)\nhello, I am child (pid:29384)\n29 107 1030 p3.c\nhello, I am parent of 29384 (rc_wait:29384) (pid:29383)\nprompt>\n2There are a few cases where wait() returns before the child exits; read the man page\nfor more details, as always. And beware of any absolute and unquali ﬁed statements this book\nmakes, such as “the child will always print ﬁrst” or “U NIXis the best thing in the world, even\nbetter than ice cream.”\n3On Linux, there are six variants of exec() :execl, execlp(), execle(),\nexecv(), execvp() , andexecvpe() . Read the man pages to learn more.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : PROCESS API 5\n1#include <stdio.h>\n2#include <stdlib.h>\n3#include <unistd.h>\n4#include <string.h>\n5#include <sys/wait.h>\n6\n7int main(int argc, char *argv[]) {\n8printf(""hello world (pid:%d)\n"", (int) getpid());\n9int rc = fork();\n10 if (rc < 0) { // fork failed; exit\n11 fprintf(stderr, ""fork failed\n"");\n12 exit(1);\n13 } else if (rc == 0) { // child (new process)\n14 printf(""hello, I am child (pid:%d)\n"", (int) getpid());\n15 char*myargs[3];\n16 myargs[0] = strdup(""wc""); // program: ""wc"" (word count)\n17 myargs[1] = strdup(""p3.c""); // argument: file to count\n18 myargs[2] = NULL; // marks end of array\n19 execvp(myargs[0], myargs); // runs word count\n20 printf(""this shouldn’t print out"");\n21 } else { // parent goes down this path (main)\n22 int rc_wait = wait(NULL);\n23 printf(""hello, I am parent of %d (rc_wait:%d) (pid:%d)\n"",\n24 rc, rc_wait, (int) getpid());\n25 }\n26 return 0;\n27}\n28\nFigure 5.3: Callingfork() ,wait() , Andexec() (p3.c )\nThefork() system call is strange; its partner in crime, exec() , is not\nso normal either. What it does: given the name of an executable (e .g.,wc),\nand some arguments (e.g., p3.c ), itloads code (and static data) from that\nexecutable and overwrites its current code segment (and curre nt static\ndata) with it; the heap and stack and other parts of the memory spa ce of\nthe program are re-initialized. Then the OS simply runs that p rogram,\npassing in any arguments as the argv of that process. Thus, it does not\ncreate a new process; rather, it transforms the currently runn ing program\n(formerly p3) into a different running program ( wc). After the exec()\nin the child, it is almost as if p3.c never ran; a successful call to exec()\nnever returns.\n5.4 Why? Motivating The API\nOf course, one big question you might have: why would we build\nsuch an odd interface to what should be the simple act of creating a new\nprocess? Well, as it turns out, the separation of fork() andexec() is\nessential in building a U NIX shell, because it lets the shell run code after\nthe call to fork() butbefore the call to exec() ; this code can alter the\nenvironment of the about-to-be-run program, and thus enables a va riety\nof interesting features to be readily built.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 INTERLUDE : PROCESS API\nTIP: GETTING ITRIGHT (LAMPSON ’SLAW)\nAs Lampson states in his well-regarded “Hints for Computer Syste ms\nDesign” [L83], “ Get it right . Neither abstraction nor simplicity is a sub-\nstitute for getting it right.” Sometimes, you just have to do the r ight thing,\nand when you do, it is way better than the alternatives. There ar e lots\nof ways to design APIs for process creation; however, the combinati on\noffork() andexec() are simple and immensely powerful. Here, the\nUNIXdesigners simply got it right. And because Lampson so often “got\nit right”, we name the law in his honor.\nThe shell is just a user program4. It shows you a prompt and then\nwaits for you to type something into it. You then type a command (i.e .,\nthe name of an executable program, plus any arguments) into it; in most\ncases, the shell then ﬁgures out where in the ﬁle system the exe cutable\nresides, calls fork() to create a new child process to run the command,\ncalls some variant of exec() to run the command, and then waits for the\ncommand to complete by calling wait() . When the child completes, the\nshell returns from wait() and prints out a prompt again, ready for your\nnext command.\nThe separation of fork() andexec() allows the shell to do a whole\nbunch of useful things rather easily. For example:\nprompt> wc p3.c > newfile.txt\nIn the example above, the output of the program wcisredirected into\nthe output ﬁle newfile.txt (the greater-than sign is how said redirec-\ntion is indicated). The way the shell accomplishes this task is quite sim-\nple: when the child is created, before calling exec() , the shell closes\nstandard output and opens the ﬁle newfile.txt . By doing so, any out-\nput from the soon-to-be-running program wcare sent to the ﬁle instead\nof the screen.\nFigure 5.4 (page 7) shows a program that does exactly this. The re ason\nthis redirection works is due to an assumption about how the operati ng\nsystem manages ﬁle descriptors. Speciﬁcally, U NIXsystems start looking\nfor free ﬁle descriptors at zero. In this case, STDOUT FILENO will be the\nﬁrst available one and thus get assigned when open() is called. Subse-\nquent writes by the child process to the standard output ﬁle des criptor,\nfor example by routines such as printf() , will then be routed transpar-\nently to the newly-opened ﬁle instead of the screen.\nHere is the output of running the p4.c program:\nprompt> ./p4\nprompt> cat p4.output\n32 109 846 p4.c\nprompt>\n4And there are lots of shells; tcsh ,bash , andzsh to name a few. You should pick one,\nread its man pages, and learn more about it; all U NIXexperts do.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : PROCESS API 7\n1#include <stdio.h>\n2#include <stdlib.h>\n3#include <unistd.h>\n4#include <string.h>\n5#include <fcntl.h>\n6#include <sys/wait.h>\n7\n8int main(int argc, char *argv[]) {\n9int rc = fork();\n10 if (rc < 0) { // fork failed; exit\n11 fprintf(stderr, ""fork failed\n"");\n12 exit(1);\n13 } else if (rc == 0) { // child: redirect standard output to a fil e\n14 close(STDOUT_FILENO);\n15 open(""./p4.output"", O_CREAT|O_WRONLY|O_TRUNC, S_IRWXU );\n16\n17 // now exec ""wc""...\n18 char*myargs[3];\n19 myargs[0] = strdup(""wc""); // program: ""wc"" (word count)\n20 myargs[1] = strdup(""p4.c""); // argument: file to count\n21 myargs[2] = NULL; // marks end of array\n22 execvp(myargs[0], myargs); // runs word count\n23 } else { // parent goes down this path (main)\n24 int rc_wait = wait(NULL);\n25 }\n26 return 0;\n27}\nFigure 5.4: All Of The Above With Redirection ( p4.c )\nYou’ll notice (at least) two interesting tidbits about this outpu t. First,\nwhenp4is run, it looks as if nothing has happened; the shell just prints\nthe command prompt and is immediately ready for your next command.\nHowever, that is not the case; the program p4did indeed call fork() to\ncreate a new child, and then run the wcprogram via a call to execvp() .\nYou don’t see any output printed to the screen because it has been r edi-\nrected to the ﬁle p4.output . Second, you can see that when we cat the\noutput ﬁle, all the expected output from running wcis found. Cool, right?\nUNIX pipes are implemented in a similar way, but with the pipe()\nsystem call. In this case, the output of one process is connected to an in-\nkernel pipe (i.e., queue), and the input of another process is connected\nto that same pipe; thus, the output of one process seamlessly is us ed as\ninput to the next, and long and useful chains of commands can be st rung\ntogether. As a simple example, consider looking for a word in a ﬁle, a nd\nthen counting how many times said word occurs; with pipes and the u til-\nitiesgrep andwc, it is easy — just type grep -o foo file | wc -l\ninto the command prompt and marvel at the result.\nFinally, while we just have sketched out the process API at a hig h level,\nthere is a lot more detail about these calls out there to be learned and\ndigested; we’ll learn more, for example, about ﬁle descriptors wh en we\ntalk about ﬁle systems in the third part of the book. For now, sufﬁce i t\nto say that the fork()/exec() combination is a powerful way to create\nand manipulate processes.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 INTERLUDE : PROCESS API\nASIDE : RTFM — R EAD THEMANPAGES\nMany times in this book, when referring to a particular system c all or\nlibrary call, we’ll tell you to read the manual pages , or man pages for\nshort. Man pages are the original form of documentation that exist on\nUNIX systems; realize that they were created before the thing call edthe\nweb existed.\nSpending some time reading man pages is a key step in the growth of\na systems programmer; there are tons of useful tidbits hidden in those\npages. Some particularly useful pages to read are the man pages for\nwhichever shell you are using (e.g., tcsh, orbash ), and certainly for any\nsystem calls your program makes (in order to see what return valu es and\nerror conditions exist).\nFinally, reading the man pages can save you some embarrassment . When\nyou ask colleagues about some intricacy of fork() , they may simply\nreply: “RTFM.” This is your colleagues’ way of gently urging you t o Read\nThe Man pages. The F in RTFM just adds a little color to the phrase ...\n5.5 Process Control And Users\nBeyondfork() ,exec() , andwait() , there are a lot of other inter-\nfaces for interacting with processes in U NIX systems. For example, the\nkill() system call is used to send signals to a process, including di-\nrectives to pause, die, and other useful imperatives. For conve nience,\nin most U NIX shells, certain keystroke combinations are conﬁgured to\ndeliver a speciﬁc signal to the currently running process; for example,\ncontrol-c sends a SIGINT (interrupt) to the process (normally terminating\nit) and control-z sends a SIGTSTP (stop) signal thus pausing the process\nin mid-execution (you can resume it later with a command, e.g., t hefg\nbuilt-in command found in many shells).\nThe entire signals subsystem provides a rich infrastructure to deliver\nexternal events to processes, including ways to receive and p rocess those\nsignals within individual processes, and ways to send signal s to individ-\nual processes as well as entire process groups . To use this form of com-\nmunication, a process should use the signal() system call to “catch”\nvarious signals; doing so ensures that when a particular signa l is deliv-\nered to a process, it will suspend its normal execution and run a p articu-\nlar piece of code in response to the signal. Read elsewhere [SR05] to learn\nmore about signals and their many intricacies.\nThis naturally raises the question: who can send a signal to a p rocess,\nand who cannot? Generally, the systems we use can have multipl e people\nusing them at the same time; if one of these people can arbitraril y send\nsignals such as SIGINT (to interrupt a process, likely terminating it), the\nusability and security of the system will be compromised. As a re sult,\nmodern systems include a strong conception of the notion of a user . The\nuser, after entering a password to establish credentials, log s in to gain\naccess to system resources. The user may then launch one or many p ro-\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : PROCESS API 9\nASIDE : THESUPERUSER (ROOT )\nA system generally needs a user who can administer the system, and is\nnot limited in the way most users are. Such a user should be able to k ill\nan arbitrary process (e.g., if it is abusing the system in some w ay), even\nthough that process was not started by this user. Such a user should also\nbe able to run powerful commands such as shutdown (which, unsurpris-\ningly, shuts down the system). In U NIX-based systems, these special abil-\nities are given to the superuser (sometimes called root). While most users\ncan’t kill other users processes, the superuser can. Being root is much like\nbeing Spider-Man: with great power comes great responsibility [ QI15].\nThus, to increase security (and avoid costly mistakes), it’s usually better\nto be a regular user; if you do need to be root, tread carefully, as all of the\ndestructive powers of the computing world are now at your ﬁngertips .\ncesses, and exercise full control over them (pause them, kill th em, etc.).\nUsers generally can only control their own processes; it is the job of the\noperating system to parcel out resources (such as CPU, memory, an d disk)\nto each user (and their processes) to meet overall system goals.\n5.6 Useful Tools\nThere are many command-line tools that are useful as well. For exa m-\nple, using the pscommand allows you to see which processes are run-\nning; read the man pages for some useful ﬂags to pass to ps. The tooltop\nis also quite helpful, as it displays the processes of the syste m and how\nmuch CPU and other resources they are eating up. Humorously, many\ntimes when you run it, top claims it is the top resource hog; perhaps it is\na bit of an egomaniac. The command kill can be used to send arbitrary\nsignals to processes, as can the slightly more user friendly killall . Be\nsure to use these carefully; if you accidentally kill your wind ow manager,\nthe computer you are sitting in front of may become quite difﬁcult t o use.\nFinally, there are many different kinds of CPU meters you can us e to\nget a quick glance understanding of the load on your system; for exa mple,\nwe always keep MenuMeters (from Raging Menace software) running on\nour Macintosh toolbars, so we can see how much CPU is being utilized\nat any moment in time. In general, the more information about what i s\ngoing on, the better.\n5.7 Summary\nWe have introduced some of the APIs dealing with U NIXprocess cre-\nation:fork() ,exec() , andwait() . However, we have just skimmed\nthe surface. For more detail, read Stevens and Rago [SR05], of cours e,\nparticularly the chapters on Process Control, Process Relationsh ips, and\nSignals. There is much to extract from the wisdom therein.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 INTERLUDE : PROCESS API\nASIDE : KEYPROCESS API T ERMS\n•Each process has a name; in most systems, that name is a number\nknown as a process ID (PID).\n•The fork() system call is used in U NIXsystems to create a new pro-\ncess. The creator is called the parent ; the newly created process is\ncalled the child . As sometimes occurs in real life [J16], the child\nprocess is a nearly identical copy of the parent.\n•The wait() system call allows a parent to wait for its child to com-\nplete execution.\n•The exec() family of system calls allows a child to break free from\nits similarity to its parent and execute an entirely new progr am.\n•A U NIX shell commonly uses fork() ,wait() , andexec() to\nlaunch user commands; the separation of fork and exec enables fea -\ntures like input/output redirection ,pipes , and other cool features,\nall without changing anything about the programs being run.\n•Process control is available in the form of signals , which can cause\njobs to stop, continue, or even terminate.\n•Which processes can be controlled by a particular person is encap -\nsulated in the notion of a user ; the operating system allows multiple\nusers onto the system, and ensures users can only control their own\nprocesses.\n•Asuperuser can control all processes (and indeed do many other\nthings); this role should be assumed infrequently and with cau tion\nfor security reasons.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : PROCESS API 11\nReferences\n[C63] “A Multiprocessor System Design” by Melvin E. Conway. AFIPS ’6 3 Fall Joint Computer\nConference. New York, USA 1963 An early paper on how to design multiprocessing systems; may\nbe the ﬁrst place the term fork() was used in the discussion of spawning new processes.\n[DV66] “Programming Semantics for Multiprogrammed Computations” b y Jack B. Dennis and\nEarl C. Van Horn. Communications of the ACM, Volume 9, Number 3, March 1 966. A classic\npaper that outlines the basics of multiprogrammed computer systems. Undoubte dly had great inﬂuence\non Project MAC, Multics, and eventually UNIX.\n[J16] “They could be twins!” by Phoebe Jackson-Edwards. The Daily Mail. March 1, 2016.\nAvailable: www.dailymail.co.uk/femail/article-3469189/Photos-c hildren-\nlook-IDENTICAL-parents-age-sweep-web.html .This hard-hitting piece of journalism shows\na bunch of weirdly similar child/parent photos and is frankly kind of mesm erizing. Go ahead, waste two\nminutes of your life and check it out. But don’t forget to come back here! Th is, in a microcosm, is the\ndanger of surﬁng the web.\n[L83] “Hints for Computer Systems Design” by Butler Lampson. ACM Op erating Systems\nReview, Volume 15:5, October 1983. Lampson’s famous hints on how to design computer systems.\nYou should read it at some point in your life, and probably at many points in your life.\n[QI15] “With Great Power Comes Great Responsibility” by The Quote Inv estigator. Available:\nhttps://quoteinvestigator.com/2015/07/23/great-powe r.The quote investigator\nconcludes that the earliest mention of this concept is 1793, in a collection of decrees made at the French\nNational Convention. The speciﬁc quote: “Ils doivent envisager qu’une gr ande responsabilit est la\nsuite insparable d’un grand pouvoir”, which roughly translates to “They m ust consider that great\nresponsibility follows inseparably from great power.” Only in 1962 di d the following words appear in\nSpider-Man: “...with great power there must also come–great responsib ility!” So it looks like the French\nRevolution gets credit for this one, not Stan Lee. Sorry, Stan.\n[SR05] “Advanced Programming in the U NIX Environment” by W. Richard Stevens, Stephen\nA. Rago. Addison-Wesley, 2005. All nuances and subtleties of using UNIX APIs are found herein.\nBuy this book! Read it! And most importantly, live it.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 INTERLUDE : PROCESS API\nASIDE : CODING HOMEWORKS\nCoding homeworks are small exercises where you write code to run\non a real machine to get some experience with some basic operating sys-\ntem APIs. After all, you are (probably) a computer scientist, an d there-\nfore should like to code, right? (if you don’t, there is always CS the ory,\nbut that’s pretty hard) Of course, to truly become an expert, you h ave to\nspend more than a little time hacking away at the machine; inde ed, ﬁnd\nevery excuse you can to write some code and see how it works. Spend\nthe time, and become the wise master you know you can be.\nHomework (Code)\nIn this homework, you are to gain some familiarity with the process\nmanagement APIs about which you just read. Don’t worry – it’s even\nmore fun than it sounds! You’ll in general be much better off if you ﬁn d\nas much time as you can to write some code, so why not start now?\nQuestions\n1. Write a program that calls fork() . Before calling fork() , have the main\nprocess access a variable (e.g., x) and set its value to something (e.g., 100).\nWhat value is the variable in the child process? What happens t o the vari-\nable when both the child and parent change the value of x?\n2. Write a program that opens a ﬁle (with the open() system call) and then\ncallsfork() to create a new process. Can both the child and parent ac-\ncess the ﬁle descriptor returned by open() ? What happens when they are\nwriting to the ﬁle concurrently, i.e., at the same time?\n3. Write another program using fork() . The child process should print “hello”;\nthe parent process should print “goodbye”. You should try to en sure that\nthe child process always prints ﬁrst; can you do this without calling wait() in\nthe parent?\n4. Write a program that calls fork() and then calls some form of exec() to\nrun the program /bin/ls. See if you can try all of the variants of exec() ,\nincluding (on Linux) execl(), execle(), execlp(), execv(), execvp(),\nandexecvpe() . Why do you think there are so many variants of the same\nbasic call?\n5. Now write a program that uses wait() to wait for the child process to ﬁnish\nin the parent. What does wait() return? What happens if you use wait()\nin the child?\n6. Write a slight modiﬁcation of the previous program, this time us ingwaitpid()\ninstead of wait() . When would waitpid() be useful?\n7. Write a program that creates a child process, and then in th e child closes\nstandard output ( STDOUTFILENO ). What happens if the child calls printf()\nto print some output after closing the descriptor?\n8. Write a program that creates two children, and connects th e standard output\nof one to the standard input of the other, using the pipe() system call.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",28694
10-6. Direct Execution.pdf,10-6. Direct Execution,"6\nMechanism: Limited Direct Execution\nIn order to virtualize the CPU, the operating system needs to som ehow\nshare the physical CPU among many jobs running seemingly at the same\ntime. The basic idea is simple: run one process for a little while , then\nrun another one, and so forth. By time sharing the CPU in this manner,\nvirtualization is achieved.\nThere are a few challenges, however, in building such virtual ization\nmachinery. The ﬁrst is performance : how can we implement virtualiza-\ntion without adding excessive overhead to the system? The second is\ncontrol : how can we run processes efﬁciently while retaining control over\nthe CPU? Control is particularly important to the OS, as it is in ch arge of\nresources; without control, a process could simply run forever and t ake\nover the machine, or access information that it should not be allowed to\naccess. Obtaining high performance while maintaining control is thus\none of the central challenges in building an operating system.\nTHECRUX:\nHOWTOEFFICIENTLY VIRTUALIZE THECPU W ITHCONTROL\nThe OS must virtualize the CPU in an efﬁcient manner while ret aining\ncontrol over the system. To do so, both hardware and operating-syst em\nsupport will be required. The OS will often use a judicious bit of h ard-\nware support in order to accomplish its work effectively.\n6.1 Basic Technique: Limited Direct Execution\nTo make a program run as fast as one might expect, not surprisingl y\nOS developers came up with a technique, which we call limited direct\nexecution . The “direct execution” part of the idea is simple: just run the\nprogram directly on the CPU. Thus, when the OS wishes to start a p ro-\ngram running, it creates a process entry for it in a process list, allocates\nsome memory for it, loads the program code into memory (from disk), lo-\ncates its entry point (i.e., the main() routine or something similar), jumps\n1\n2 MECHANISM : LIMITED DIRECT EXECUTION\nOS Program\nCreate entry for process list\nAllocate memory for program\nLoad program into memory\nSet up stack with argc/argv\nClear registers\nExecute callmain()\nRun main()\nExecute return from main\nFree memory of process\nRemove from process list\nFigure 6.1: Direct Execution Protocol (Without Limits)\nto it, and starts running the user’s code. Figure 6.1 shows this b asic di-\nrect execution protocol (without any limits, yet), using a normal c all and\nreturn to jump to the program’s main() and later to get back into the\nkernel.\nSounds simple, no? But this approach gives rise to a few problems\nin our quest to virtualize the CPU. The ﬁrst is simple: if we jus t run a\nprogram, how can the OS make sure the program doesn’t do anything\nthat we don’t want it to do, while still running it efﬁciently? Th e second:\nwhen we are running a process, how does the operating system stop it\nfrom running and switch to another process, thus implementing t hetime\nsharing we require to virtualize the CPU?\nIn answering these questions below, we’ll get a much better sens e of\nwhat is needed to virtualize the CPU. In developing these tech niques,\nwe’ll also see where the “limited” part of the name arises from; w ithout\nlimits on running programs, the OS wouldn’t be in control of anything\nand thus would be “just a library” — a very sad state of affairs for an\naspiring operating system!\n6.2 Problem #1: Restricted Operations\nDirect execution has the obvious advantage of being fast; the prog ram\nruns natively on the hardware CPU and thus executes as quickly as one\nwould expect. But running on the CPU introduces a problem: what if\nthe process wishes to perform some kind of restricted operation, su ch\nas issuing an I/O request to a disk, or gaining access to more sys tem\nresources such as CPU or memory?\nTHECRUX: HOWTOPERFORM RESTRICTED OPERATIONS\nA process must be able to perform I/O and some other restricted oper -\nations, but without giving the process complete control over the sys tem.\nHow can the OS and hardware work together to do so?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : LIMITED DIRECT EXECUTION 3\nASIDE : W HYSYSTEM CALLS LOOK LIKEPROCEDURE CALLS\nYou may wonder why a call to a system call, such as open() orread() ,\nlooks exactly like a typical procedure call in C; that is, if it look s just like\na procedure call, how does the system know it’s a system call, and do all\nthe right stuff? The simple reason: it isa procedure call, but hidden in-\nside that procedure call is the famous trap instruction. More spe ciﬁcally,\nwhen you call open() (for example), you are executing a procedure call\ninto the C library. Therein, whether for open() or any of the other sys-\ntem calls provided, the library uses an agreed-upon calling con vention\nwith the kernel to put the arguments to open in well-known locati ons\n(e.g., on the stack, or in speciﬁc registers), puts the system- call number\ninto a well-known location as well (again, onto the stack or a regis ter),\nand then executes the aforementioned trap instruction. The code in the\nlibrary after the trap unpacks return values and returns cont rol to the\nprogram that issued the system call. Thus, the parts of the C lib rary that\nmake system calls are hand-coded in assembly, as they need to c arefully\nfollow convention in order to process arguments and return values c or-\nrectly, as well as execute the hardware-speciﬁc trap instru ction. And now\nyou know why you personally don’t have to write assembly code to trap\ninto an OS; somebody has already written that assembly for you.\nOne approach would simply be to let any process do whatever it wan ts\nin terms of I/O and other related operations. However, doing so would\nprevent the construction of many kinds of systems that are desira ble. For\nexample, if we wish to build a ﬁle system that checks permissi ons before\ngranting access to a ﬁle, we can’t simply let any user process is sue I/Os\nto the disk; if we did, a process could simply read or write the ent ire disk\nand thus all protections would be lost.\nThus, the approach we take is to introduce a new processor mode,\nknown as user mode ; code that runs in user mode is restricted in what it\ncan do. For example, when running in user mode, a process can’t issu e\nI/O requests; doing so would result in the processor raising an ex ception;\nthe OS would then likely kill the process.\nIn contrast to user mode is kernel mode , which the operating system\n(or kernel) runs in. In this mode, code that runs can do what it lik es, in-\ncluding privileged operations such as issuing I/O requests an d executing\nall types of restricted instructions.\nWe are still left with a challenge, however: what should a user p ro-\ncess do when it wishes to perform some kind of privileged operation ,\nsuch as reading from disk? To enable this, virtually all modern hard-\nware provides the ability for user programs to perform a system call .\nPioneered on ancient machines such as the Atlas [K+61,L78], sy stem calls\nallow the kernel to carefully expose certain key pieces of funct ionality to\nuser programs, such as accessing the ﬁle system, creating and destroy-\ning processes, communicating with other processes, and allocati ng more\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 MECHANISM : LIMITED DIRECT EXECUTION\nTIP: USEPROTECTED CONTROL TRANSFER\nThe hardware assists the OS by providing different modes of exec ution.\nInuser mode , applications do not have full access to hardware resources.\nInkernel mode , the OS has access to the full resources of the machine.\nSpecial instructions to trap into the kernel and return-from-trap back to\nuser-mode programs are also provided, as well as instructions th at allow\nthe OS to tell the hardware where the trap table resides in memory.\nmemory. Most operating systems provide a few hundred calls (see t he\nPOSIX standard for details [P10]); early Unix systems exposed a more\nconcise subset of around twenty calls.\nTo execute a system call, a program must execute a special trap instruc-\ntion. This instruction simultaneously jumps into the kernel an d raises the\nprivilege level to kernel mode; once in the kernel, the system c an now per-\nform whatever privileged operations are needed (if allowed), an d thus do\nthe required work for the calling process. When ﬁnished, the OS c alls a\nspecial return-from-trap instruction, which, as you might expect, returns\ninto the calling user program while simultaneously reducing t he privi-\nlege level back to user mode.\nThe hardware needs to be a bit careful when executing a trap, i n that it\nmust make sure to save enough of the caller’s registers in order to be able\nto return correctly when the OS issues the return-from-trap in struction.\nOn x86, for example, the processor will push the program counter, ﬂ ags,\nand a few other registers onto a per-process kernel stack ; the return-from-\ntrap will pop these values off the stack and resume execution of th e user-\nmode program (see the Intel systems manuals [I11] for details). Other\nhardware systems use different conventions, but the basic conc epts are\nsimilar across platforms.\nThere is one important detail left out of this discussion: how does th e\ntrap know which code to run inside the OS? Clearly, the calling pr ocess\ncan’t specify an address to jump to (as you would when making a pro-\ncedure call); doing so would allow programs to jump anywhere into the\nkernel which clearly is a Very Bad Idea1. Thus the kernel must carefully\ncontrol what code executes upon a trap.\nThe kernel does so by setting up a trap table at boot time. When the\nmachine boots up, it does so in privileged (kernel) mode, and thus is free\nto conﬁgure machine hardware as need be. One of the ﬁrst things t he OS\nthus does is to tell the hardware what code to run when certain ex cep-\ntional events occur. For example, what code should run when a hard-\ndisk interrupt takes place, when a keyboard interrupt occurs, or when\na program makes a system call? The OS informs the hardware of the\nlocations of these trap handlers , usually with some kind of special in-\n1Imagine jumping into code to access a ﬁle, but just after a permission check; in fact, it is\nlikely such an ability would enable a wily programmer to get the k ernel to run arbitrary code\nsequences [S07]. In general, try to avoid Very Bad Ideas like this one .\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : LIMITED DIRECT EXECUTION 5\nOS @ boot Hardware\n(kernel mode)\ninitialize trap table\nremember address of...\nsyscall handler\nOS @ run Hardware Program\n(kernel mode) (user mode)\nCreate entry for process list\nAllocate memory for program\nLoad program into memory\nSetup user stack with argv\nFill kernel stack with reg/PC\nreturn-from-trap\nrestore regs from kernel stack\nmove to user mode\njump to main\nRun main()\n...\nCall system call\ntrap into OS\nsave regs to kernel stack\nmove to kernel mode\njump to trap handler\nHandle trap\nDo work of syscall\nreturn-from-trap\nrestore regs from kernel stack\nmove to user mode\njump to PC after trap\n...\nreturn from main\ntrap (viaexit() )\nFree memory of process\nRemove from process list\nFigure 6.2: Limited Direct Execution Protocol\nstruction. Once the hardware is informed, it remembers the loca tion of\nthese handlers until the machine is next rebooted, and thus the hardware\nknows what to do (i.e., what code to jump to) when system calls and other\nexceptional events take place.\nTo specify the exact system call, a system-call number is usually as-\nsigned to each system call. The user code is thus responsible for placing\nthe desired system-call number in a register or at a speciﬁed l ocation on\nthe stack; the OS, when handling the system call inside the tra p handler,\nexamines this number, ensures it is valid, and, if it is, exec utes the corre-\nsponding code. This level of indirection serves as a form of protection ;\nuser code cannot specify an exact address to jump to, but rather m ust\nrequest a particular service via number.\nOne last aside: being able to execute the instruction to tell t he hard-\nware where the trap tables are is a very powerful capability. T hus, as you\nmight have guessed, it is also a privileged operation. If you try to exe-\ncute this instruction in user mode, the hardware won’t let you, and you\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 MECHANISM : LIMITED DIRECT EXECUTION\nTIP: BEWARY OFUSERINPUTS INSECURE SYSTEMS\nEven though we have taken great pains to protect the OS during sy stem\ncalls (by adding a hardware trapping mechanism, and ensurin g all calls to\nthe OS are routed through it), there are still many other aspects to imple-\nmenting a secure operating system that we must consider. One of these\nis the handling of arguments at the system call boundary; the OS must\ncheck what the user passes in and ensure that arguments are pr operly\nspeciﬁed, or otherwise reject the call.\nFor example, with a write() system call, the user speciﬁes an address\nof a buffer as a source of the write call. If the user (either accid entally\nor maliciously) passes in a “bad” address (e.g., one inside the k ernel’s\nportion of the address space), the OS must detect this and reject the call.\nOtherwise, it would be possible for a user to read all of kernel mem ory;\ngiven that kernel (virtual) memory also usually includes all of the physi-\ncal memory of the system, this small slip would enable a program to read\nthe memory of any other process in the system.\nIn general, a secure system must treat user inputs with great suspicion.\nNot doing so will undoubtedly lead to easily hacked software, a de spair-\ning sense that the world is an unsafe and scary place, and the los s of job\nsecurity for the all-too-trusting OS developer.\ncan probably guess what will happen (hint: adios, offending prog ram).\nPoint to ponder: what horrible things could you do to a system if you\ncould install your own trap table? Could you take over the machine?\nThe timeline (with time increasing downward, in Figure 6.2) s umma-\nrizes the protocol. We assume each process has a kernel stack wher e reg-\nisters (including general purpose registers and the program c ounter) are\nsaved to and restored from (by the hardware) when transitioning into and\nout of the kernel.\nThere are two phases in the limited direct execution ( LDE ) protocol.\nIn the ﬁrst (at boot time), the kernel initializes the trap tabl e, and the\nCPU remembers its location for subsequent use. The kernel does so via a\nprivileged instruction (all privileged instructions are hig hlighted in bold).\nIn the second (when running a process), the kernel sets up a few t hings\n(e.g., allocating a node on the process list, allocating memory) be fore us-\ning a return-from-trap instruction to start the execution of the process;\nthis switches the CPU to user mode and begins running the proces s.\nWhen the process wishes to issue a system call, it traps back in to the OS,\nwhich handles it and once again returns control via a return-from -trap\nto the process. The process then completes its work, and returns f rom\nmain() ; this usually will return into some stub code which will properl y\nexit the program (say, by calling the exit() system call, which traps into\nthe OS). At this point, the OS cleans up and we are done.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : LIMITED DIRECT EXECUTION 7\n6.3 Problem #2: Switching Between Processes\nThe next problem with direct execution is achieving a switch be tween\nprocesses. Switching between processes should be simple, right ? The\nOS should just decide to stop one process and start another. What’s t he\nbig deal? But it actually is a little bit tricky: speciﬁcally , if a process is\nrunning on the CPU, this by deﬁnition means the OS is notrunning. If\nthe OS is not running, how can it do anything at all? (hint: it can ’t) While\nthis sounds almost philosophical, it is a real problem: there is cl early no\nway for the OS to take an action if it is not running on the CPU. Thus w e\narrive at the crux of the problem.\nTHECRUX: HOWTOREGAIN CONTROL OFTHECPU\nHow can the operating system regain control of the CPU so that it can\nswitch between processes?\nA Cooperative Approach: Wait For System Calls\nOne approach that some systems have taken in the past (for exampl e,\nearly versions of the Macintosh operating system [M11], or the old X erox\nAlto system [A79]) is known as the cooperative approach. In this style,\nthe OS trusts the processes of the system to behave reasonably. Processes\nthat run for too long are assumed to periodically give up the CPU so that\nthe OS can decide to run some other task.\nThus, you might ask, how does a friendly process give up the CPU in\nthis utopian world? Most processes, as it turns out, transfer contr ol of\nthe CPU to the OS quite frequently by making system calls , for example,\nto open a ﬁle and subsequently read it, or to send a message to anot her\nmachine, or to create a new process. Systems like this often inclu de an\nexplicit yield system call, which does nothing except to transfer control\nto the OS so it can run other processes.\nApplications also transfer control to the OS when they do somethi ng\nillegal. For example, if an application divides by zero, or tries to access\nmemory that it shouldn’t be able to access, it will generate a trap to the\nOS. The OS will then have control of the CPU again (and likely termi nate\nthe offending process).\nTIP: DEALING WITHAPPLICATION MISBEHAVIOR\nOperating systems often have to deal with misbehaving process es, those\nthat either through design (maliciousness) or accident (bugs) attempt to\ndo something that they shouldn’t. In modern systems, the way the O S\ntries to handle such malfeasance is to simply terminate the of fender. One\nstrike and you’re out! Perhaps brutal, but what else should the OS do\nwhen you try to access memory illegally or execute an illegal ins truction?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 MECHANISM : LIMITED DIRECT EXECUTION\nThus, in a cooperative scheduling system, the OS regains control of\nthe CPU by waiting for a system call or an illegal operation of some ki nd\nto take place. You might also be thinking: isn’t this passive ap proach less\nthan ideal? What happens, for example, if a process (whether ma licious,\nor just full of bugs) ends up in an inﬁnite loop, and never makes a sy stem\ncall? What can the OS do then?\nA Non-Cooperative Approach: The OS Takes Control\nWithout some additional help from the hardware, it turns out the OS can’t\ndo much at all when a process refuses to make system calls (or mis takes)\nand thus return control to the OS. In fact, in the cooperative approa ch,\nyour only recourse when a process gets stuck in an inﬁnite loop is to\nresort to the age-old solution to all problems in computer systems: reboot\nthe machine . Thus, we again arrive at a subproblem of our general quest\nto gain control of the CPU.\nTHECRUX: HOWTOGAIN CONTROL WITHOUT COOPERATION\nHow can the OS gain control of the CPU even if processes are not being\ncooperative? What can the OS do to ensure a rogue process does not tak e\nover the machine?\nThe answer turns out to be simple and was discovered by a number\nof people building computer systems many years ago: a timer interrupt\n[M+63]. A timer device can be programmed to raise an interrupt every\nso many milliseconds; when the interrupt is raised, the curre ntly running\nprocess is halted, and a pre-conﬁgured interrupt handler in the OS runs.\nAt this point, the OS has regained control of the CPU, and thus can d o\nwhat it pleases: stop the current process, and start a differen t one.\nAs we discussed before with system calls, the OS must inform the\nhardware of which code to run when the timer interrupt occurs; th us,\nat boot time, the OS does exactly that. Second, also during the boot\nsequence, the OS must start the timer, which is of course a privi leged\noperation. Once the timer has begun, the OS can thus feel safe in that\ncontrol will eventually be returned to it, and thus the OS is fre e to run\nuser programs. The timer can also be turned off (also a privileg ed opera-\ntion), something we will discuss later when we understand concu rrency\nin more detail.\nTIP: USETHETIMER INTERRUPT TOREGAIN CONTROL\nThe addition of a timer interrupt gives the OS the ability to run again\non a CPU even if processes act in a non-cooperative fashion. Thus, th is\nhardware feature is essential in helping the OS maintain cont rol of the\nmachine.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : LIMITED DIRECT EXECUTION 9\nTIP: REBOOT ISUSEFUL\nEarlier on, we noted that the only solution to inﬁnite loops (and simi lar\nbehaviors) under cooperative preemption is to reboot the machine. While\nyou may scoff at this hack, researchers have shown that reboot (or in gen-\neral, starting over some piece of software) can be a hugely useful tool in\nbuilding robust systems [C+04].\nSpeciﬁcally, reboot is useful because it moves software back to a k nown\nand likely more tested state. Reboots also reclaim stale or leake d re-\nsources (e.g., memory) which may otherwise be hard to handle. Fi nally,\nreboots are easy to automate. For all of these reasons, it is not uncomm on\nin large-scale cluster Internet services for system managem ent software\nto periodically reboot sets of machines in order to reset them and t hus\nobtain the advantages listed above.\nThus, next time you reboot, you are not just enacting some ugly hack.\nRather, you are using a time-tested approach to improving the be havior\nof a computer system. Well done!\nNote that the hardware has some responsibility when an interrup t oc-\ncurs, in particular to save enough of the state of the program that was\nrunning when the interrupt occurred such that a subsequent re turn-from-\ntrap instruction will be able to resume the running program corr ectly.\nThis set of actions is quite similar to the behavior of the hardwar e during\nan explicit system-call trap into the kernel, with various re gisters thus\ngetting saved (e.g., onto a kernel stack) and thus easily rest ored by the\nreturn-from-trap instruction.\nSaving and Restoring Context\nNow that the OS has regained control, whether cooperatively via a s ys-\ntem call, or more forcefully via a timer interrupt, a decision has to be\nmade: whether to continue running the currently-running proc ess, or\nswitch to a different one. This decision is made by a part of the ope rating\nsystem known as the scheduler ; we will discuss scheduling policies in\ngreat detail in the next few chapters.\nIf the decision is made to switch, the OS then executes a low-lev el\npiece of code which we refer to as a context switch . A context switch is\nconceptually simple: all the OS has to do is save a few register values\nfor the currently-executing process (onto its kernel stack, for example)\nand restore a few for the soon-to-be-executing process (from its ker nel\nstack). By doing so, the OS thus ensures that when the return-fr om-trap\ninstruction is ﬁnally executed, instead of returning to the pr ocess that was\nrunning, the system resumes execution of another process.\nTo save the context of the currently-running process, the OS wil l ex-\necute some low-level assembly code to save the general purpose re gis-\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 MECHANISM : LIMITED DIRECT EXECUTION\nOS @ boot Hardware\n(kernel mode)\ninitialize trap table\nremember addresses of...\nsyscall handler\ntimer handler\nstart interrupt timer\nstart timer\ninterrupt CPU in X ms\nOS @ run Hardware Program\n(kernel mode) (user mode)\nProcess A\n...\ntimer interrupt\nsave regs(A) to k-stack(A)\nmove to kernel mode\njump to trap handler\nHandle the trap\nCallswitch() routine\nsave regs(A) to proc-struct(A)\nrestore regs(B) from proc-struct(B)\nswitch to k-stack(B)\nreturn-from-trap (into B)\nrestore regs(B) from k-stack(B)\nmove to user mode\njump to B’s PC\nProcess B\n...\nFigure 6.3: Limited Direct Execution Protocol (Timer Interrupt)\nters, PC, and the kernel stack pointer of the currently-runnin g process,\nand then restore said registers, PC, and switch to the kernel s tack for the\nsoon-to-be-executing process. By switching stacks, the kernel enters the\ncall to the switch code in the context of one process (the one that was in-\nterrupted) and returns in the context of another (the soon-to-be-e xecuting\none). When the OS then ﬁnally executes a return-from-trap inst ruction,\nthe soon-to-be-executing process becomes the currently-runnin g process.\nAnd thus the context switch is complete.\nA timeline of the entire process is shown in Figure 6.3. In this ex ample,\nProcess A is running and then is interrupted by the timer inter rupt. The\nhardware saves its registers (onto its kernel stack) and ente rs the kernel\n(switching to kernel mode). In the timer interrupt handler, t he OS decides\nto switch from running Process A to Process B. At that point, it cal ls the\nswitch() routine, which carefully saves current register values (int o the\nprocess structure of A), restores the registers of Process B (from i ts process\nstructure entry), and then switches contexts , speciﬁcally by changing the\nstack pointer to use B’s kernel stack (and not A’s). Finally, the O S returns-\nfrom-trap, which restores B’s registers and starts running it.\nNote that there are two types of register saves/restores that ha ppen\nduring this protocol. The ﬁrst is when the timer interrupt occurs ; in this\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : LIMITED DIRECT EXECUTION 11\n1# void swtch(struct context **old, struct context *new);\n2#\n3# Save current register context in old\n4# and then load register context from new.\n5.globl swtch\n6swtch:\n7# Save old registers\n8movl 4(%esp), %eax # put old ptr into eax\n9popl 0(%eax) # save the old IP\n10movl %esp, 4(%eax) # and stack\n11movl %ebx, 8(%eax) # and other registers\n12movl %ecx, 12(%eax)\n13movl %edx, 16(%eax)\n14movl %esi, 20(%eax)\n15movl %edi, 24(%eax)\n16movl %ebp, 28(%eax)\n17\n18# Load new registers\n19movl 4(%esp), %eax # put new ptr into eax\n20movl 28(%eax), %ebp # restore other registers\n21movl 24(%eax), %edi\n22movl 20(%eax), %esi\n23movl 16(%eax), %edx\n24movl 12(%eax), %ecx\n25movl 8(%eax), %ebx\n26movl 4(%eax), %esp # stack is switched here\n27pushl 0(%eax) # return addr put in place\n28ret # finally return into new ctxt\nFigure 6.4: The xv6 Context Switch Code\ncase, the user registers of the running process are implicitly saved by the\nhardware , using the kernel stack of that process. The second is when the\nOS decides to switch from A to B; in this case, the kernel registers are ex-\nplicitly saved by the software (i.e., the OS), but this time into memory in\nthe process structure of the process. The latter action moves the s ystem\nfrom running as if it just trapped into the kernel from A to as if i t just\ntrapped into the kernel from B.\nTo give you a better sense of how such a switch is enacted, Figure 6 .4\nshows the context switch code for xv6. See if you can make sense of it\n(you’ll have to know a bit of x86, as well as some xv6, to do so). The\ncontext structures old andnew are found in the old and new process’s\nprocess structures, respectively.\n6.4 Worried About Concurrency?\nSome of you, as attentive and thoughtful readers, may be now think-\ning: “Hmm... what happens when, during a system call, a timer interrupt\noccurs?” or “What happens when you’re handling one interrupt and a n-\nother one happens? Doesn’t that get hard to handle in the kernel?” Good\nquestions — we really have some hope for you yet!\nThe answer is yes, the OS does indeed need to be concerned as to wh at\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 MECHANISM : LIMITED DIRECT EXECUTION\nASIDE : HOWLONG CONTEXT SWITCHES TAKE\nA natural question you might have is: how long does something like a\ncontext switch take? Or even a system call? For those of you that are cu-\nrious, there is a tool called lmbench [MS96] that measures exactly those\nthings, as well as a few other performance measures that might b e rele-\nvant.\nResults have improved quite a bit over time, roughly tracking pr ocessor\nperformance. For example, in 1996 running Linux 1.3.37 on a 200- MHz\nP6 CPU, system calls took roughly 4 microseconds, and a context swit ch\nroughly 6 microseconds [MS96]. Modern systems perform almost an or-\nder of magnitude better, with sub-microsecond results on system s with\n2- or 3-GHz processors.\nIt should be noted that not all operating-system actions track CPU per-\nformance. As Ousterhout observed, many OS operations are memory\nintensive, and memory bandwidth has not improved as dramatical ly as\nprocessor speed over time [O90]. Thus, depending on your workload,\nbuying the latest and greatest processor may not speed up your OS a s\nmuch as you might hope.\nhappens if, during interrupt or trap handling, another interr upt occurs.\nThis, in fact, is the exact topic of the entire second piece of this book, on\nconcurrency ; we’ll defer a detailed discussion until then.\nTo whet your appetite, we’ll just sketch some basics of how the OS\nhandles these tricky situations. One simple thing an OS might do is dis-\nable interrupts during interrupt processing; doing so ensures that when\none interrupt is being handled, no other one will be delivered to the CPU.\nOf course, the OS has to be careful in doing so; disabling interru pts for\ntoo long could lead to lost interrupts, which is (in technical ter ms) bad.\nOperating systems also have developed a number of sophisticate d\nlocking schemes to protect concurrent access to internal data structu res.\nThis enables multiple activities to be on-going within the ker nel at the\nsame time, particularly useful on multiprocessors. As we’ll see in the\nnext piece of this book on concurrency, though, such locking can be com -\nplicated and lead to a variety of interesting and hard-to-ﬁnd b ugs.\n6.5 Summary\nWe have described some key low-level mechanisms to implement C PU\nvirtualization, a set of techniques which we collectively refe r to as limited\ndirect execution . The basic idea is straightforward: just run the program\nyou want to run on the CPU, but ﬁrst make sure to set up the hardwar e\nso as to limit what the process can do without OS assistance.\nThis general approach is taken in real life as well. For example , those\nof you who have children, or, at least, have heard of children, may be\nfamiliar with the concept of baby prooﬁng a room: locking cabinets con-\ntaining dangerous stuff and covering electrical sockets. When the room is\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : LIMITED DIRECT EXECUTION 13\nASIDE : KEYCPU V IRTUALIZATION TERMS (MECHANISMS )\n•The CPU should support at least two modes of execution: a re-\nstricted user mode and a privileged (non-restricted) kernel mode .\n•Typical user applications run in user mode, and use a system call\ntotrap into the kernel to request operating system services.\n•The trap instruction saves register state carefully, change s the hard-\nware status to kernel mode, and jumps into the OS to a pre-speci ﬁed\ndestination: the trap table .\n•When the OS ﬁnishes servicing a system call, it returns to the user\nprogram via another special return-from-trap instruction, which re-\nduces privilege and returns control to the instruction after th e trap\nthat jumped into the OS.\n•The trap tables must be set up by the OS at boot time, and make\nsure that they cannot be readily modiﬁed by user programs. All\nof this is part of the limited direct execution protocol which runs\nprograms efﬁciently but without loss of OS control.\n•Once a program is running, the OS must use hardware mechanisms\nto ensure the user program does not run forever, namely the timer\ninterrupt . This approach is a non-cooperative approach to CPU\nscheduling.\n•Sometimes the OS, during a timer interrupt or system call, might\nwish to switch from running the current process to a different on e,\na low-level technique known as a context switch .\nthus readied, you can let your baby roam freely, secure in the know ledge\nthat the most dangerous aspects of the room have been restricted.\nIn an analogous manner, the OS “baby proofs” the CPU, by ﬁrst (dur-\ning boot time) setting up the trap handlers and starting an inte rrupt timer,\nand then by only running processes in a restricted mode. By doing s o, the\nOS can feel quite assured that processes can run efﬁciently, on ly requir-\ning OS intervention to perform privileged operations or when they have\nmonopolized the CPU for too long and thus need to be switched out.\nWe thus have the basic mechanisms for virtualizing the CPU in p lace.\nBut a major question is left unanswered: which process should we r un at\na given time? It is this question that the scheduler must answe r, and thus\nthe next topic of our study.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 MECHANISM : LIMITED DIRECT EXECUTION\nReferences\n[A79] “Alto User’s Handbook” by Xerox. Xerox Palo Alto Research C enter, September 1979.\nAvailable: http://history-computer.com/Library/AltoUsersHandbo ok.pdf .An\namazing system, way ahead of its time. Became famous because Steve Jobs visited, took notes, and built\nLisa and eventually Mac.\n[C+04] “Microreboot — A Technique for Cheap Recovery” by G. Candea, S. Ka wamoto, Y.\nFujiki, G. Friedman, A. Fox. OSDI ’04, San Francisco, CA, December 2 004. An excellent paper\npointing out how far one can go with reboot in building more robust systems.\n[I11] “Intel 64 and IA-32 Architectures Software Developer’s Manual” b y Volume 3A and 3B:\nSystem Programming Guide. Intel Corporation, January 2011. This is just a boring manual, but\nsometimes those are useful.\n[K+61] “One-Level Storage System” by T. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner.\nIRE Transactions on Electronic Computers, April 1962. The Atlas pioneered much of what you see\nin modern systems. However, this paper is not the best one to read. If you wer e to only read one, you\nmight try the historical perspective below [L78].\n[L78] “The Manchester Mark I and Atlas: A Historical Perspective” by S. H. L avington. Com-\nmunications of the ACM, 21:1, January 1978. A history of the early development of computers and\nthe pioneering efforts of Atlas.\n[M+63] “A Time-Sharing Debugging System for a Small Computer” by J. McCa rthy, S. Boilen,\nE. Fredkin, J. C. R. Licklider. AFIPS ’63 (Spring), May, 1963, New York , USA. An early paper\nabout time-sharing that refers to using a timer interrupt; the quote that discu sses it: “The basic task of\nthe channel 17 clock routine is to decide whether to remove the current us er from core and if so to decide\nwhich user program to swap in as he goes out.”\n[MS96] “lmbench: Portable tools for performance analysis” by Larry McVoy a nd Carl Staelin.\nUSENIX Annual Technical Conference, January 1996. A fun paper about how to measure a number\nof different things about your OS and its performance. Download lmbench and give it a try.\n[M11] “Mac OS 9” by Apple Computer, Inc.. January 2011. http://en.wikipedia.org/wiki/\nMacOS9.You can probably even ﬁnd an OS 9 emulator out there if you want to; check it out, it’ s a\nfun little Mac!\n[O90] “Why Aren’t Operating Systems Getting Faster as Fast as Hardwa re?” by J. Ouster-\nhout. USENIX Summer Conference, June 1990. A classic paper on the nature of operating system\nperformance.\n[P10] “The Single UNIX Speciﬁcation, Version 3” by The Open Group, May 2010 . Available:\nhttp://www.unix.org/version3/ .This is hard and painful to read, so probably avoid it if you\ncan. Like, unless someone is paying you to read it. Or, you’re just so curi ous you can’t help it!\n[S07] “The Geometry of Innocent Flesh on the Bone: Return-into-libc without Function Calls\n(on the x86)” by Hovav Shacham. CCS ’07, October 2007. One of those awesome, mind-blowing\nideas that you’ll see in research from time to time. The author shows that if you c an jump into code\narbitrarily, you can essentially stitch together any code sequence you like (gi ven a large code base); read\nthe paper for the details. The technique makes it even harder to defend again st malicious attacks, alas.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : LIMITED DIRECT EXECUTION 15\nHomework (Measurement)\nASIDE : M EASUREMENT HOMEWORKS\nMeasurement homeworks are small exercises where you write code t o\nrun on a real machine, in order to measure some aspect of OS or hardwa re\nperformance. The idea behind such homeworks is to give you a littl e bit\nof hands-on experience with a real operating system.\nIn this homework, you’ll measure the costs of a system call and contex t\nswitch. Measuring the cost of a system call is relatively easy. For example,\nyou could repeatedly call a simple system call (e.g., performin g a 0-byte\nread), and time how long it takes; dividing the time by the numbe r of\niterations gives you an estimate of the cost of a system call.\nOne thing you’ll have to take into account is the precision and acc u-\nracy of your timer. A typical timer that you can use is gettimeofday() ;\nread the man page for details. What you’ll see there is that gettimeofday()\nreturns the time in microseconds since 1970; however, this does n ot mean\nthat the timer is precise to the microsecond. Measure back-to-b ack calls\ntogettimeofday() to learn something about how precise the timer re-\nally is; this will tell you how many iterations of your null system- call\ntest you’ll have to run in order to get a good measurement result. If\ngettimeofday() is not precise enough for you, you might look into\nusing therdtsc instruction available on x86 machines.\nMeasuring the cost of a context switch is a little trickier. The l mbench\nbenchmark does so by running two processes on a single CPU, and se t-\nting up two U NIX pipes between them; a pipe is just one of many ways\nprocesses in a U NIXsystem can communicate with one another. The ﬁrst\nprocess then issues a write to the ﬁrst pipe, and waits for a read on the\nsecond; upon seeing the ﬁrst process waiting for something to read from\nthe second pipe, the OS puts the ﬁrst process in the blocked state , and\nswitches to the other process, which reads from the ﬁrst pipe and then\nwrites to the second. When the second process tries to read from th e ﬁrst\npipe again, it blocks, and thus the back-and-forth cycle of commu nication\ncontinues. By measuring the cost of communicating like this repe atedly,\nlmbench can make a good estimate of the cost of a context switch. You\ncan try to re-create something similar here, using pipes, or pe rhaps some\nother communication mechanism such as U NIXsockets.\nOne difﬁculty in measuring context-switch cost arises in syst ems with\nmore than one CPU; what you need to do on such a system is ensure that\nyour context-switching processes are located on the same processor . For-\ntunately, most operating systems have calls to bind a process to a partic-\nular processor; on Linux, for example, the schedsetaffinity() call\nis what you’re looking for. By ensuring both processes are on the same\nprocessor, you are making sure to measure the cost of the OS stopping\none process and restoring another on the same CPU.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",39694
11-8. Multi-level Feedback.pdf,11-8. Multi-level Feedback,"8\nScheduling:\nThe Multi-Level Feedback Queue\nIn this chapter, we’ll tackle the problem of developing one of the mos t\nwell-known approaches to scheduling, known as the Multi-level Feed-\nback Queue (MLFQ) . The Multi-level Feedback Queue (MLFQ) sched-\nuler was ﬁrst described by Corbato et al. in 1962 [C+62] in a sys tem\nknown as the Compatible Time-Sharing System (CTSS), and this work,\nalong with later work on Multics, led the ACM to award Corbato its\nhighest honor, the T uring Award . The scheduler has subsequently been\nreﬁned throughout the years to the implementations you will encou nter\nin some modern systems.\nThe fundamental problem MLFQ tries to address is two-fold. Firs t, it\nwould like to optimize turnaround time , which, as we saw in the previous\nnote, is done by running shorter jobs ﬁrst; unfortunately, the OS d oesn’t\ngenerally know how long a job will run for, exactly the knowledge tha t\nalgorithms like SJF (or STCF) require. Second, MLFQ would like to mak e\na system feel responsive to interactive users (i.e., users si tting and staring\nat the screen, waiting for a process to ﬁnish), and thus minimiz eresponse\ntime; unfortunately, algorithms like Round Robin reduce response tim e\nbut are terrible for turnaround time. Thus, our problem: given th at we\nin general do not know anything about a process, how can we build a\nscheduler to achieve these goals? How can the scheduler learn, as the\nsystem runs, the characteristics of the jobs it is running, and thus make\nbetter scheduling decisions?\nTHECRUX:\nHOWTOSCHEDULE WITHOUT PERFECT KNOWLEDGE ?\nHow can we design a scheduler that both minimizes response time f or\ninteractive jobs while also minimizing turnaround time withou ta priori\nknowledge of job length?\n1\n2SCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE\nTIP: LEARN FROM HISTORY\nThe multi-level feedback queue is an excellent example of a sy stem that\nlearns from the past to predict the future. Such approaches are c om-\nmon in operating systems (and many other places in Computer Scienc e,\nincluding hardware branch predictors and caching algorithms ). Such\napproaches work when jobs have phases of behavior and are thus pre-\ndictable; of course, one must be careful with such techniques, a s they can\neasily be wrong and drive a system to make worse decisions than th ey\nwould have with no knowledge at all.\n8.1 MLFQ: Basic Rules\nTo build such a scheduler, in this chapter we will describe th e basic\nalgorithms behind a multi-level feedback queue; although the speciﬁcs of\nmany implemented MLFQs differ [E95], most approaches are simi lar.\nIn our treatment, the MLFQ has a number of distinct queues , each\nassigned a different priority level . At any given time, a job that is ready\nto run is on a single queue. MLFQ uses priorities to decide which job\nshould run at a given time: a job with higher priority (i.e., a job on a\nhigher queue) is chosen to run.\nOf course, more than one job may be on a given queue, and thus have\nthesame priority. In this case, we will just use round-robin scheduling\namong those jobs.\nThus, we arrive at the ﬁrst two basic rules for MLFQ:\n•Rule 1: If Priority(A) >Priority(B), A runs (B doesn’t).\n•Rule 2: If Priority(A) =Priority(B), A & B run in RR.\nThe key to MLFQ scheduling therefore lies in how the scheduler s ets\npriorities. Rather than giving a ﬁxed priority to each job, MLFQ varies\nthe priority of a job based on its observed behavior . If, for example, a job\nrepeatedly relinquishes the CPU while waiting for input from t he key-\nboard, MLFQ will keep its priority high, as this is how an interac tive\nprocess might behave. If, instead, a job uses the CPU intensive ly for long\nperiods of time, MLFQ will reduce its priority. In this way, MLFQ will try\ntolearn about processes as they run, and thus use the history of the job to\npredict its future behavior.\nIf we were to put forth a picture of what the queues might look like a t\na given instant, we might see something like the following (Figu re 8.1).\nIn the ﬁgure, two jobs (A and B) are at the highest priority level , while job\nC is in the middle and Job D is at the lowest priority. Given our curr ent\nknowledge of how MLFQ works, the scheduler would just alternate ti me\nslices between A and B because they are the highest priority job s in the\nsystem; poor jobs C and D would never even get to run — an outrage!\nOf course, just showing a static snapshot of some queues does not re-\nally give you an idea of how MLFQ works. What we need is to under-\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE 3\nQ1Q2Q3Q4Q5Q6Q7Q8\n[Low Priority][High Priority]\nDCA B\nFigure 8.1: MLFQ Example\nstand how job priority changes over time. And that, in a surprise only\nto those who are reading a chapter from this book for the ﬁrst time, i s\nexactly what we will do next.\n8.2 Attempt #1: How To Change Priority\nWe now must decide how MLFQ is going to change the priority level\nof a job (and thus which queue it is on) over the lifetime of a job. To do\nthis, we must keep in mind our workload: a mix of interactive jobs th at\nare short-running (and may frequently relinquish the CPU), a nd some\nlonger-running “CPU-bound” jobs that need a lot of CPU time but whe re\nresponse time isn’t important. Here is our ﬁrst attempt at a priori ty-\nadjustment algorithm:\n•Rule 3: When a job enters the system, it is placed at the highest\npriority (the topmost queue).\n•Rule 4a: If a job uses up an entire time slice while running, its pri-\nority is reduced (i.e., it moves down one queue).\n•Rule 4b: If a job gives up the CPU before the time slice is up, it stays\nat the same priority level.\nExample 1: A Single Long-Running Job\nLet’s look at some examples. First, we’ll look at what happens when th ere\nhas been a long running job in the system. Figure 8.2 shows what ha ppens\nto this job over time in a three-queue scheduler.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4SCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE\nQ2\nQ1\nQ0\n0 50 100 150 200\nFigure 8.2: Long-running Job Over Time\nAs you can see in the example, the job enters at the highest priori ty\n(Q2). After a single time-slice of 10 ms, the scheduler reduce s the job’s\npriority by one, and thus the job is on Q1. After running at Q1 for a ti me\nslice, the job is ﬁnally lowered to the lowest priority in the syst em (Q0),\nwhere it remains. Pretty simple, no?\nExample 2: Along Came A Short Job\nNow let’s look at a more complicated example, and hopefully see how\nMLFQ tries to approximate SJF. In this example, there are two job s: A,\nwhich is a long-running CPU-intensive job, and B, which is a shor t-running\ninteractive job. Assume A has been running for some time, and the n B ar-\nrives. What will happen? Will MLFQ approximate SJF for B?\nFigure 8.3 plots the results of this scenario. A (shown in black) i s run-\nning along in the lowest-priority queue (as would any long-runnin g CPU-\nintensive jobs); B (shown in gray) arrives at time T= 100 , and thus is\nQ0Q1Q2\n0 50 100 150 200\nFigure 8.3: Along Came An Interactive Job\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE 5\nQ0Q1Q2\n0 50 100 150 200\nFigure 8.4: A Mixed I/O-intensive and CPU-intensive Workload\ninserted into the highest queue; as its run-time is short (only 20 ms), B\ncompletes before reaching the bottom queue, in two time slices; t hen A\nresumes running (at low priority).\nFrom this example, you can hopefully understand one of the major\ngoals of the algorithm: because it doesn’t know whether a job will be a\nshort job or a long-running job, it ﬁrst assumes it might be a short job, thus\ngiving the job high priority. If it actually is a short job, it will run quickly\nand complete; if it is not a short job, it will slowly move down the queu es,\nand thus soon prove itself to be a long-running more batch-like proc ess.\nIn this manner, MLFQ approximates SJF.\nExample 3: What About I/O?\nLet’s now look at an example with some I/O. As Rule 4b states above, if a\nprocess gives up the processor before using up its time slice, we k eep it at\nthe same priority level. The intent of this rule is simple: if an interactive\njob, for example, is doing a lot of I/O (say by waiting for user input f rom\nthe keyboard or mouse), it will relinquish the CPU before its time slice is\ncomplete; in such case, we don’t wish to penalize the job and thus s imply\nkeep it at the same level.\nFigure 8.4 shows an example of how this works, with an interactive job\nB (shown in gray) that needs the CPU only for 1 ms before performing a n\nI/O competing for the CPU with a long-running batch job A (shown in\nblack). The MLFQ approach keeps B at the highest priority becau se B\nkeeps releasing the CPU; if B is an interactive job, MLFQ furth er achieves\nits goal of running interactive jobs quickly.\nProblems With Our Current MLFQ\nWe thus have a basic MLFQ. It seems to do a fairly good job, sharing the\nCPU fairly between long-running jobs, and letting short or I/O-i ntensive\ninteractive jobs run quickly. Unfortunately, the approach we h ave devel-\noped thus far contains serious ﬂaws. Can you think of any?\n(This is where you pause and think as deviously as you can)\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6SCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE\nQ0Q1Q2\n0 50 100 150 200Q0Q1Q2\n0 50 100 150 200\nBoost\nBoost\nBoost\nBoost\nFigure 8.5: Without (Left) and With (Right) Priority Boost\nFirst, there is the problem of starvation : if there are “too many” in-\nteractive jobs in the system, they will combine to consume allCPU time,\nand thus long-running jobs will never receive any CPU time (they starve ).\nWe’d like to make some progress on these jobs even in this scenario.\nSecond, a smart user could rewrite their program to game the sched-\nuler. Gaming the scheduler generally refers to the idea of doing some -\nthing sneaky to trick the scheduler into giving you more than you r fair\nshare of the resource. The algorithm we have described is suscep tible to\nthe following attack: before the time slice is over, issue an I/O op eration\n(to some ﬁle you don’t care about) and thus relinquish the CPU; doing so\nallows you to remain in the same queue, and thus gain a higher per cent-\nage of CPU time. When done right (e.g., by running for 99% of a time s lice\nbefore relinquishing the CPU), a job could nearly monopolize the CP U.\nFinally, a program may change its behavior over time; what was CPU-\nbound may transition to a phase of interactivity. With our curren t ap-\nproach, such a job would be out of luck and not be treated like the other\ninteractive jobs in the system.\nTIP: SCHEDULING MUST BESECURE FROM ATTACK\nYou might think that a scheduling policy, whether inside the OS itself\n(as discussed herein), or in a broader context (e.g., in a distri buted stor-\nage system’s I/O request handling [Y+18]), is not a security concern, but\nin increasingly many cases, it is exactly that. Consider the m odern dat-\nacenter, in which users from around the world share CPUs, memorie s,\nnetworks, and storage systems; without care in policy design and en-\nforcement, a single user may be able to adversely harm others an d gain\nadvantage for itself. Thus, scheduling policy forms an importan t part of\nthe security of a system, and should be carefully constructed.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE 7\nQ0Q1Q2\n0 50 100 150 200 250 300Q0Q1Q2\n0 50 100 150 200 250 300\nFigure 8.6: Without (Left) and With (Right) Gaming Tolerance\n8.3 Attempt #2: The Priority Boost\nLet’s try to change the rules and see if we can avoid the problem of\nstarvation. What could we do in order to guarantee that CPU-bound jobs\nwill make some progress (even if it is not much?).\nThe simple idea here is to periodically boost the priority of all the jobs\nin system. There are many ways to achieve this, but let’s just d o some-\nthing simple: throw them all in the topmost queue; hence, a new ru le:\n•Rule 5: After some time period S, move all the jobs in the system\nto the topmost queue.\nOur new rule solves two problems at once. First, processes are gua r-\nanteed not to starve: by sitting in the top queue, a job will share the CPU\nwith other high-priority jobs in a round-robin fashion, and thus ev entu-\nally receive service. Second, if a CPU-bound job has become intera ctive,\nthe scheduler treats it properly once it has received the priori ty boost.\nLet’s see an example. In this scenario, we just show the behavior of\na long-running job when competing for the CPU with two short-runni ng\ninteractive jobs. Two graphs are shown in Figure 8.5 (page 6). O n the left,\nthere is no priority boost, and thus the long-running job gets star ved once\nthe two short jobs arrive; on the right, there is a priority boost eve ry 50\nms (which is likely too small of a value, but used here for the exam ple),\nand thus we at least guarantee that the long-running job will ma ke some\nprogress, getting boosted to the highest priority every 50 ms and thus\ngetting to run periodically.\nOf course, the addition of the time period Sleads to the obvious ques-\ntion: what should Sbe set to? John Ousterhout, a well-regarded systems\nresearcher [O11], used to call such values in systems voo-doo constants ,\nbecause they seemed to require some form of black magic to set the m cor-\nrectly. Unfortunately, Shas that ﬂavor. If it is set too high, long-running\njobs could starve; too low, and interactive jobs may not get a proper s hare\nof the CPU.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8SCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE\nQ2\nQ1\nQ0\n0 50 100 150 200 250 300\nFigure 8.7: Lower Priority, Longer Quanta\n8.4 Attempt #3: Better Accounting\nWe now have one more problem to solve: how to prevent gaming of\nour scheduler? The real culprit here, as you might have guessed , are\nRules 4a and 4b, which let a job retain its priority by relinquis hing the\nCPU before the time slice expires. So what should we do?\nThe solution here is to perform better accounting of CPU time at each\nlevel of the MLFQ. Instead of forgetting how much of a time slice a pr o-\ncess used at a given level, the scheduler should keep track; onc e a process\nhas used its allotment, it is demoted to the next priority queue. Whether\nit uses the time slice in one long burst or many small ones does not mat ter.\nWe thus rewrite Rules 4a and 4b to the following single rule:\n•Rule 4: Once a job uses up its time allotment at a given level (re-\ngardless of how many times it has given up the CPU), its priority i s\nreduced (i.e., it moves down one queue).\nLet’s look at an example. Figure 8.6 (page 7) shows what happens\nwhen a workload tries to game the scheduler with the old Rules 4a a nd 4b\n(on the left) as well the new anti-gaming Rule 4. Without any prot ection\nfrom gaming, a process can issue an I/O just before a time slice en ds and\nthus dominate CPU time. With such protections in place, regardl ess of\nthe I/O behavior of the process, it slowly moves down the queues, and\nthus cannot gain an unfair share of the CPU.\n8.5 Tuning MLFQ And Other Issues\nA few other issues arise with MLFQ scheduling. One big question is\nhow to parameterize such a scheduler. For example, how many queues\nshould there be? How big should the time slice be per queue? How ofte n\nshould priority be boosted in order to avoid starvation and account for\nchanges in behavior? There are no easy answers to these questi ons, and\nthus only some experience with workloads and subsequent tuning of the\nscheduler will lead to a satisfactory balance.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE 9\nTIP: AVOID VOO-DOO CONSTANTS (OUSTERHOUT ’SLAW)\nAvoiding voo-doo constants is a good idea whenever possible. Unfor-\ntunately, as in the example above, it is often difﬁcult. One coul d try to\nmake the system learn a good value, but that too is not straightforw ard.\nThe frequent result: a conﬁguration ﬁle ﬁlled with default par ameter val-\nues that a seasoned administrator can tweak when something isn’t quite\nworking correctly. As you can imagine, these are often left unmodi ﬁed,\nand thus we are left to hope that the defaults work well in the ﬁel d. This\ntip brought to you by our old OS professor, John Ousterhout, and hence\nwe call it Ousterhout’s Law .\nFor example, most MLFQ variants allow for varying time-slice len gth\nacross different queues. The high-priority queues are usuall y given short\ntime slices; they are comprised of interactive jobs, after all, and thus\nquickly alternating between them makes sense (e.g., 10 or few er millisec-\nonds). The low-priority queues, in contrast, contain long-runnin g jobs\nthat are CPU-bound; hence, longer time slices work well (e.g., 1 00s of\nms). Figure 8.7 (page 8) shows an example in which two jobs run for 20\nms at the highest queue (with a 10-ms time slice), 40 ms in the m iddle\n(20-ms time slice), and with a 40-ms time slice at the lowest.\nThe Solaris MLFQ implementation — the Time-Sharing scheduling\nclass, or TS — is particularly easy to conﬁgure; it provides a set of tables\nthat determine exactly how the priority of a process is altered th rough-\nout its lifetime, how long each time slice is, and how often to boost th e\npriority of a job [AD00]; an administrator can muck with this tabl e in or-\nder to make the scheduler behave in different ways. Default v alues for\nthe table are 60 queues, with slowly increasing time-slice le ngths from\n20 milliseconds (highest priority) to a few hundred millisecon ds (lowest),\nand priorities boosted around every 1 second or so.\nOther MLFQ schedulers don’t use a table or the exact rules descri bed\nin this chapter; rather they adjust priorities using mathema tical formu-\nlae. For example, the FreeBSD scheduler (version 4.3) uses a form ula to\ncalculate the current priority level of a job, basing it on how much CPU\nthe process has used [LM+89]; in addition, usage is decayed over time,\nproviding the desired priority boost in a different manner than d escribed\nherein. See Epema’s paper for an excellent overview of such decay-usage\nalgorithms and their properties [E95].\nFinally, many schedulers have a few other features that you mig ht en-\ncounter. For example, some schedulers reserve the highest prior ity levels\nfor operating system work; thus typical user jobs can never obtain the\nhighest levels of priority in the system. Some systems also allow s ome\nuser advice to help set priorities; for example, by using the command-line\nutilitynice you can increase or decrease the priority of a job (somewhat)\nand thus increase or decrease its chances of running at any give n time.\nSee the man page for more.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10SCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE\nTIP: USEADVICE WHERE POSSIBLE\nAs the operating system rarely knows what is best for each and eve ry\nprocess of the system, it is often useful to provide interfaces to allow users\nor administrators to provide some hints to the OS. We often call such\nhints advice , as the OS need not necessarily pay attention to it, but rather\nmight take the advice into account in order to make a better deci sion.\nSuch hints are useful in many parts of the OS, including the sched uler\n(e.g., with nice ), memory manager (e.g., madvise ), and ﬁle system (e.g.,\ninformed prefetching and caching [P+95]).\n8.6 MLFQ: Summary\nWe have described a scheduling approach known as the Multi-Lev el\nFeedback Queue (MLFQ). Hopefully you can now see why it is called\nthat: it has multiple levels of queues, and uses feedback to determine the\npriority of a given job. History is its guide: pay attention to how job s\nbehave over time and treat them accordingly.\nThe reﬁned set of MLFQ rules, spread throughout the chapter, are re-\nproduced here for your viewing pleasure:\n•Rule 1: If Priority(A) >Priority(B), A runs (B doesn’t).\n•Rule 2: If Priority(A) =Priority(B), A & B run in round-robin fash-\nion using the time slice (quantum length) of the given queue.\n•Rule 3: When a job enters the system, it is placed at the highest\npriority (the topmost queue).\n•Rule 4: Once a job uses up its time allotment at a given level (re-\ngardless of how many times it has given up the CPU), its priority i s\nreduced (i.e., it moves down one queue).\n•Rule 5: After some time period S, move all the jobs in the system\nto the topmost queue.\nMLFQ is interesting for the following reason: instead of demandin g\na priori knowledge of the nature of a job, it observes the execution of a\njob and prioritizes it accordingly. In this way, it manages to ac hieve the\nbest of both worlds: it can deliver excellent overall performance (similar\nto SJF/STCF) for short-running interactive jobs, and is fair and m akes\nprogress for long-running CPU-intensive workloads. For this reas on,\nmany systems, including BSD U NIX derivatives [LM+89, B86], Solaris\n[M06], and Windows NT and subsequent Windows operating systems\n[CS97] use a form of MLFQ as their base scheduler.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE 11\nReferences\n[AD00] “Multilevel Feedback Queue Scheduling in Solaris” by Andrea A rpaci-Dusseau. Avail-\nable: http://www.ostep.org/Citations/notes-solaris.pdf. A great short set of notes by one of the\nauthors on the details of the Solaris scheduler. OK, we are probably biased in th is description, but the\nnotes are pretty darn good.\n[B86] “The Design of the U NIXOperating System” by M.J. Bach. Prentice-Hall, 1986. One of the\nclassic old books on how a real UNIXoperating system is built; a deﬁnite must-read for kernel hackers.\n[C+62] “An Experimental Time-Sharing System” by F. J. Corbato, M. M. D aggett, R. C. Daley.\nIFIPS 1962. A bit hard to read, but the source of many of the ﬁrst ideas in multi-level fee dback schedul-\ning. Much of this later went into Multics, which one could argue was the most inﬂuential operating\nsystem of all time.\n[CS97] “Inside Windows NT” by Helen Custer and David A. Solomon. Micro soft Press, 1997.\nThe NT book, if you want to learn about something other than UNIX. Of course, why would you? OK,\nwe’re kidding; you might actually work for Microsoft some day you know.\n[E95] “An Analysis of Decay-Usage Scheduling in Multiprocessors” by D .H.J. Epema. SIG-\nMETRICS ’95. A nice paper on the state of the art of scheduling back in the mid 1990s, inclu ding a\ngood overview of the basic approach behind decay-usage schedulers.\n[LM+89] “The Design and Implementation of the 4.3BSD U NIXOperating System” by S.J. Lef-\nﬂer, M.K. McKusick, M.J. Karels, J.S. Quarterman. Addison-Wesley, 1989 .Another OS classic,\nwritten by four of the main people behind BSD. The later versions of this book, w hile more up to date,\ndon’t quite match the beauty of this one.\n[M06] “Solaris Internals: Solaris 10 and OpenSolaris Kernel Architectu re” by Richard Mc-\nDougall. Prentice-Hall, 2006. A good book about Solaris and how it works.\n[O11] “John Ousterhout’s Home Page” by John Ousterhout. www.stanford.edu/˜ouster/ .\nThe home page of the famous Professor Ousterhout. The two co-authors of this book had the pleasure of\ntaking graduate operating systems from Ousterhout while in graduate school; ind eed, this is where the\ntwo co-authors got to know each other, eventually leading to marriage, kids, and even this book. Thus,\nyou really can blame Ousterhout for this entire mess you’re in.\n[P+95] “Informed Prefetching and Caching” by R.H. Patterson, G.A. Gibson, E. G inting, D.\nStodolsky, J. Zelenka. SOSP ’95, Copper Mountain, Colorado, October 1995. A fun paper about\nsome very cool ideas in ﬁle systems, including how applications can give th e OS advice about what ﬁles\nit is accessing and how it plans to access them.\n[Y+18] “Principled Schedulability Analysis for Distributed Storag e Systems using Thread Ar-\nchitecture Models” by Suli Yang, Jing Liu, Andrea C. Arpaci-Dusseau, Re mzi H. Arpaci-\nDusseau. OSDI ’18, San Diego, California. A recent work of our group that demonstrates the\ndifﬁculty of scheduling I/O requests within modern distributed storage systems such as Hive/HDFS,\nCassandra, MongoDB, and Riak. Without care, a single user might be able to mon opolize system re-\nsources.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12SCHEDULING :\nTHEMULTI -LEVEL FEEDBACK QUEUE\nHomework (Simulation)\nThis program, mlfq.py , allows you to see how the MLFQ scheduler\npresented in this chapter behaves. See the README for details.\nQuestions\n1. Run a few randomly-generated problems with just two jobs and tw o queues;\ncompute the MLFQ execution trace for each. Make your life easier by limit-\ning the length of each job and turning off I/Os.\n2. How would you run the scheduler to reproduce each of the examples i n the\nchapter?\n3. How would you conﬁgure the scheduler parameters to behave just l ike a\nround-robin scheduler?\n4. Craft a workload with two jobs and scheduler parameters so th at one job\ntakes advantage of the older Rules 4a and 4b (turned on with the -Sﬂag)\nto game the scheduler and obtain 99% of the CPU over a particular t ime\ninterval.\n5. Given a system with a quantum length of 10 ms in its highest queue, how of-\nten would you have to boost jobs back to the highest priority le vel (with the\n-Bﬂag) in order to guarantee that a single long-running (and pote ntially-\nstarving) job gets at least 5% of the CPU?\n6. One question that arises in scheduling is which end of a queue to add a job\nthat just ﬁnished I/O; the -Iﬂag changes this behavior for this scheduling\nsimulator. Play around with some workloads and see if you can see t he\neffect of this ﬂag.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",26102
12-9. Lottery Scheduling.pdf,12-9. Lottery Scheduling,"9\nScheduling: Proportional Share\nIn this chapter, we’ll examine a different type of scheduler kn own as a\nproportional-share scheduler, also sometimes referred to as a fair-share\nscheduler. Proportional-share is based around a simple concept: instead\nof optimizing for turnaround or response time, a scheduler might in stead\ntry to guarantee that each job obtain a certain percentage of CPU time.\nAn excellent early example of proportional-share scheduling is found\nin research by Waldspurger and Weihl [WW94], and is known as lottery\nscheduling ; however, the idea is certainly older [KL88]. The basic idea\nis quite simple: every so often, hold a lottery to determine whic h process\nshould get to run next; processes that should run more often should b e\ngiven more chances to win the lottery. Easy, no? Now, onto the detai ls!\nBut not before our crux:\nCRUX: HOWTOSHARE THECPU P ROPORTIONALLY\nHow can we design a scheduler to share the CPU in a proportional\nmanner? What are the key mechanisms for doing so? How effective ar e\nthey?\n9.1 Basic Concept: Tickets Represent Your Share\nUnderlying lottery scheduling is one very basic concept: tickets , which\nare used to represent the share of a resource that a process (or use r or\nwhatever) should receive. The percent of tickets that a process has repre-\nsents its share of the system resource in question.\nLet’s look at an example. Imagine two processes, A and B, and furth er\nthat A has 75 tickets while B has only 25. Thus, what we would like is for\nA to receive 75% of the CPU and B the remaining 25%.\nLottery scheduling achieves this probabilistically (but not d eterminis-\ntically) by holding a lottery every so often (say, every time sli ce). Holding\na lottery is straightforward: the scheduler must know how many tot al\ntickets there are (in our example, there are 100). The schedul er then picks\n1\n2 S CHEDULING : PROPORTIONAL SHARE\nTIP: USERANDOMNESS\nOne of the most beautiful aspects of lottery scheduling is its use ofran-\ndomness . When you have to make a decision, using such a randomized\napproach is often a robust and simple way of doing so.\nRandom approaches has at least three advantages over more tradit ional\ndecisions. First, random often avoids strange corner-case behav iors that\na more traditional algorithm may have trouble handling. For examp le,\nconsider the LRU replacement policy (studied in more detail in a future\nchapter on virtual memory); while often a good replacement algorit hm,\nLRU attains worst-case performance for some cyclic-sequential work-\nloads. Random, on the other hand, has no such worst case.\nSecond, random also is lightweight, requiring little state to t rack alter-\nnatives. In a traditional fair-share scheduling algorithm, t racking how\nmuch CPU each process has received requires per-process accoun ting,\nwhich must be updated after running each process. Doing so rand omly\nnecessitates only the most minimal of per-process state (e.g., t he number\nof tickets each has).\nFinally, random can be quite fast. As long as generating a random num-\nber is quick, making the decision is also, and thus random can be u sed\nin a number of places where speed is required. Of course, the fas ter the\nneed, the more random tends towards pseudo-random.\na winning ticket, which is a number from 0 to 991. Assuming A holds\ntickets 0 through 74 and B 75 through 99, the winning ticket simp ly de-\ntermines whether A or B runs. The scheduler then loads the state of that\nwinning process and runs it.\nHere is an example output of a lottery scheduler’s winning ticket s:\n63 85 70 39 76 17 29 41 36 39 10 99 68 83 63 62 43 0 49\nHere is the resulting schedule:\nA A A A A A A A A A A A A A A\nB B B B\nAs you can see from the example, the use of randomness in lottery\nscheduling leads to a probabilistic correctness in meeting th e desired pro-\nportion, but no guarantee. In our example above, B only gets to run 4 out\nof 20 time slices (20%), instead of the desired 25% allocation. How ever,\nthe longer these two jobs compete, the more likely they are to achi eve the\ndesired percentages.\n1Computer Scientists always start counting at 0. It is so odd to non-comp uter-types that\nfamous people have felt obliged to write about why we do it this way [D 82].\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nSCHEDULING : PROPORTIONAL SHARE 3\nTIP: USETICKETS TOREPRESENT SHARES\nOne of the most powerful (and basic) mechanisms in the design of lot tery\n(and stride) scheduling is that of the ticket . The ticket is used to represent\na process’s share of the CPU in these examples, but can be applied much\nmore broadly. For example, in more recent work on virtual memory man-\nagement for hypervisors, Waldspurger shows how tickets can be us ed to\nrepresent a guest operating system’s share of memory [W02]. Thus , if you\nare ever in need of a mechanism to represent a proportion of ownershi p,\nthis concept just might be ... (wait for it) ... the ticket.\n9.2 Ticket Mechanisms\nLottery scheduling also provides a number of mechanisms to mani p-\nulate tickets in different and sometimes useful ways. One way is with\nthe concept of ticket currency . Currency allows a user with a set of tick-\nets to allocate tickets among their own jobs in whatever currency they\nwould like; the system then automatically converts said curren cy into the\ncorrect global value.\nFor example, assume users A and B have each been given 100 ticke ts.\nUser A is running two jobs, A1 and A2, and gives them each 500 tic kets\n(out of 1000 total) in A’s currency. User B is running only 1 job and gi ves\nit 10 tickets (out of 10 total). The system converts A1’s and A2’s all ocation\nfrom 500 each in A’s currency to 50 each in the global currency; si milarly,\nB1’s 10 tickets is converted to 100 tickets. The lottery is then h eld over the\nglobal ticket currency (200 total) to determine which job runs.\nUser A -> 500 (A’s currency) to A1 -> 50 (global currency)\n-> 500 (A’s currency) to A2 -> 50 (global currency)\nUser B -> 10 (B’s currency) to B1 -> 100 (global currency)\nAnother useful mechanism is ticket transfer . With transfers, a process\ncan temporarily hand off its tickets to another process. This abi lity is\nespecially useful in a client/server setting, where a clien t process sends\na message to a server asking it to do some work on the client’s behal f.\nTo speed up the work, the client can pass the tickets to the serv er and\nthus try to maximize the performance of the server while the ser ver is\nhandling the client’s request. When ﬁnished, the server then transfers the\ntickets back to the client and all is as before.\nFinally, ticket inﬂation can sometimes be a useful technique. With\ninﬂation, a process can temporarily raise or lower the number of tic kets\nit owns. Of course, in a competitive scenario with processes that do not\ntrust one another, this makes little sense; one greedy process cou ld give\nitself a vast number of tickets and take over the machine. Rathe r, inﬂation\ncan be applied in an environment where a group of processes trust on e\nanother; in such a case, if any one process knows it needs more CPU ti me,\nit can boost its ticket value as a way to reﬂect that need to the sy stem, all\nwithout communicating with any other processes.\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 S CHEDULING : PROPORTIONAL SHARE\n1// counter: used to track if we’ve found the winner yet\n2int counter = 0;\n3\n4// winner: use some call to a random number generator to\n5// get a value, between 0 and the total # of tickets\n6int winner = getrandom(0, totaltickets);\n7\n8// current: use this to walk through the list of jobs\n9node_t*current = head;\n10while (current) {\n11counter = counter + current->tickets;\n12if (counter > winner)\n13 break; // found the winner\n14current = current->next;\n15}\n16// ’current’ is the winner: schedule it...\nFigure 9.1: Lottery Scheduling Decision Code\n9.3 Implementation\nProbably the most amazing thing about lottery scheduling is the s im-\nplicity of its implementation. All you need is a good random number\ngenerator to pick the winning ticket, a data structure to trac k the pro-\ncesses of the system (e.g., a list), and the total number of ticke ts.\nLet’s assume we keep the processes in a list. Here is an example c om-\nprised of three processes, A, B, and C, each with some number of tic kets.\nheadJob:A\nTix:100Job:B\nTix:50Job:C\nTix:250NULL\nTo make a scheduling decision, we ﬁrst have to pick a random numb er\n(the winner) from the total number of tickets (400)2Let’s say we pick the\nnumber 300. Then, we simply traverse the list, with a simple c ounter\nused to help us ﬁnd the winner (Figure 9.1).\nThe code walks the list of processes, adding each ticket value to counter\nuntil the value exceeds winner . Once that is the case, the current list el-\nement is the winner. With our example of the winning ticket bein g 300,\nthe following takes place. First, counter is incremented to 100 to ac-\ncount for A’s tickets; because 100 is less than 300, the loop continu es.\nThencounter would be updated to 150 (B’s tickets), still less than 300\nand thus again we continue. Finally, counter is updated to 400 (clearly\ngreater than 300), and thus we break out of the loop with current point-\ning at C (the winner).\n2Surprisingly, as pointed out by Bj ¨orn Lindberg, this can be challenging to do\ncorrectly; for more details, see http://stackoverflow.com/questions/2509679/\nhow-to-generate-a-random-number-from-within-a-range .\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nSCHEDULING : PROPORTIONAL SHARE 5\n1 10 100 10000.00.20.40.60.81.0\nJob LengthUnfairness (Average)\nFigure 9.2: Lottery Fairness Study\nTo make this process most efﬁcient, it might generally be best t o or-\nganize the list in sorted order, from the highest number of ticket s to the\nlowest. The ordering does not affect the correctness of the algorith m;\nhowever, it does ensure in general that the fewest number of list itera-\ntions are taken, especially if there are a few processes that pos sess most\nof the tickets.\n9.4 An Example\nTo make the dynamics of lottery scheduling more understandable , we\nnow perform a brief study of the completion time of two jobs competing\nagainst one another, each with the same number of tickets (100) a nd same\nrun time ( R, which we will vary).\nIn this scenario, we’d like for each job to ﬁnish at roughly the same\ntime, but due to the randomness of lottery scheduling, sometimes one\njob ﬁnishes before the other. To quantify this difference, we de ﬁne a\nsimple unfairness metric ,Uwhich is simply the time the ﬁrst job com-\npletes divided by the time that the second job completes. For exam ple,\nifR= 10 , and the ﬁrst job ﬁnishes at time 10 (and the second job at 20),\nU=10\n20= 0.5. When both jobs ﬁnish at nearly the same time, Uwill be\nquite close to 1. In this scenario, that is our goal: a perfectly fa ir scheduler\nwould achieve U= 1.\nFigure 9.2 plots the average unfairness as the length of the two jobs\n(R) is varied from 1 to 1000 over thirty trials (results are genera ted via the\nsimulator provided at the end of the chapter). As you can see from th e\ngraph, when the job length is not very long, average unfairness c an be\nquite severe. Only as the jobs run for a signiﬁcant number of time slices\ndoes the lottery scheduler approach the desired outcome.\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 S CHEDULING : PROPORTIONAL SHARE\n9.5 How To Assign Tickets?\nOne problem we have not addressed with lottery scheduling is: how\nto assign tickets to jobs? This problem is a tough one, because of cou rse\nhow the system behaves is strongly dependent on how tickets are al lo-\ncated. One approach is to assume that the users know best; in suc h a\ncase, each user is handed some number of tickets, and a user can a llocate\ntickets to any jobs they run as desired. However, this solution is a non-\nsolution: it really doesn’t tell you what to do. Thus, given a set of job s,\nthe “ticket-assignment problem” remains open.\n9.6 Why Not Deterministic?\nYou might also be wondering: why use randomness at all? As we saw\nabove, while randomness gets us a simple (and approximately corr ect)\nscheduler, it occasionally will not deliver the exact right prop ortions, es-\npecially over short time scales. For this reason, Waldspurger in vented\nstride scheduling , a deterministic fair-share scheduler [W95].\nStride scheduling is also straightforward. Each job in the syst em has\na stride, which is inverse in proportion to the number of tickets i t has. In\nour example above, with jobs A, B, and C, with 100, 50, and 250 tick ets,\nrespectively, we can compute the stride of each by dividing some large\nnumber by the number of tickets each process has been assigned. For\nexample, if we divide 10,000 by each of those ticket values, we ob tain\nthe following stride values for A, B, and C: 100, 200, and 40. We ca ll\nthis value the stride of each process; every time a process runs, we will\nincrement a counter for it (called its pass value) by its stride to track its\nglobal progress.\nThe scheduler then uses the stride and pass to determine whic h pro-\ncess should run next. The basic idea is simple: at any given tim e, pick\nthe process to run that has the lowest pass value so far; when you r un\na process, increment its pass counter by its stride. A pseudocode imple-\nmentation is provided by Waldspurger [W95]:\ncurr = remove_min(queue); // pick client with min pass\nschedule(curr); // run for quantum\ncurr->pass += curr->stride; // update pass using stride\ninsert(queue, curr); // return curr to queue\nIn our example, we start with three processes (A, B, and C), with stride\nvalues of 100, 200, and 40, and all with pass values initially a t 0. Thus, at\nﬁrst, any of the processes might run, as their pass values are eq ually low.\nAssume we pick A (arbitrarily; any of the processes with equal l ow pass\nvalues can be chosen). A runs; when ﬁnished with the time slice , we\nupdate its pass value to 100. Then we run B, whose pass value is t hen\nset to 200. Finally, we run C, whose pass value is incremented t o 40. At\nthis point, the algorithm will pick the lowest pass value, which is C’s, and\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nSCHEDULING : PROPORTIONAL SHARE 7\nPass(A) Pass(B) Pass(C) Who Runs?\n(stride=100) (stride=200) (stride=40)\n0 0 0 A\n100 0 0 B\n100 200 0 C\n100 200 40 C\n100 200 80 C\n100 200 120 A\n200 200 120 C\n200 200 160 C\n200 200 200 ...\nFigure 9.3: Stride Scheduling: A Trace\nrun it, updating its pass to 80 (C’s stride is 40, as you recall). Then C will\nrun again (still the lowest pass value), raising its pass to 12 0. A will run\nnow, updating its pass to 200 (now equal to B’s). Then C will run tw ice\nmore, updating its pass to 160 then 200. At this point, all pass v alues are\nequal again, and the process will repeat, ad inﬁnitum. Figure 9.3 traces\nthe behavior of the scheduler over time.\nAs we can see from the ﬁgure, C ran ﬁve times, A twice, and B just\nonce, exactly in proportion to their ticket values of 250, 100, and 50. Lot-\ntery scheduling achieves the proportions probabilistically ove r time; stride\nscheduling gets them exactly right at the end of each scheduli ng cycle.\nSo you might be wondering: given the precision of stride schedulin g,\nwhy use lottery scheduling at all? Well, lottery scheduling ha s one nice\nproperty that stride scheduling does not: no global state. Imagi ne a new\njob enters in the middle of our stride scheduling example above; w hat\nshould its pass value be? Should it be set to 0? If so, it will monopoliz e\nthe CPU. With lottery scheduling, there is no global state per p rocess;\nwe simply add a new process with whatever tickets it has, updat e the\nsingle global variable to track how many total tickets we have, a nd go\nfrom there. In this way, lottery makes it much easier to incorpora te new\nprocesses in a sensible manner.\n9.7 The Linux Completely Fair Scheduler (CFS)\nDespite these earlier works in fair-share scheduling, the cu rrent Linux\napproach achieves similar goals in an alternate manner. The sc heduler,\nentitled the Completely Fair Scheduler (orCFS ) [J09], implements fair-\nshare scheduling, but does so in a highly efﬁcient and scalabl e manner.\nTo achieve its efﬁciency goals, CFS aims to spend very little t ime mak-\ning scheduling decisions, through both its inherent design and its clever\nuse of data structures well-suited to the task. Recent studie s have shown\nthat scheduler efﬁciency is surprisingly important; speciﬁ cally, in a study\nof Google datacenters, Kanev et al. show that even after aggressi ve opti-\nmization, scheduling uses about 5% of overall datacenter CPU tim e. Re-\nducing that overhead as much as possible is thus a key goal in moder n\nscheduler architecture.\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 S CHEDULING : PROPORTIONAL SHARE\n0 50 100 150 200 250\nTimeABCDABCDA B A B A B\nFigure 9.4: CFS Simple Example\nBasic Operation\nWhereas most schedulers are based around the concept of a ﬁxed tim e\nslice, CFS operates a bit differently. Its goal is simple: to fa irly divide a\nCPU evenly among all competing processes. It does so through a simp le\ncounting-based technique known as virtual runtime (vruntime ).\nAs each process runs, it accumulates vruntime . In the most basic\ncase, each process’s vruntime increases at the same rate, in proportion\nwith physical (real) time. When a scheduling decision occurs, CFS will\npick the process with the lowestvruntime to run next.\nThis raises a question: how does the scheduler know when to stop\nthe currently running process, and run the next one? The tension here is\nclear: if CFS switches too often, fairness is increased, as CFS will ensure\nthat each process receives its share of CPU even over miniscule t ime win-\ndows, but at the cost of performance (too much context switching); i f CFS\nswitches less often, performance is increased (reduced contex t switching),\nbut at the cost of near-term fairness.\nCFS manages this tension through various control parameters. The\nﬁrst isschedlatency . CFS uses this value to determine how long one\nprocess should run before considering a switch (effectively det ermining\nits time slice but in a dynamic fashion). A typical schedlatency value\nis 48 (milliseconds); CFS divides this value by the number ( n) of processes\nrunning on the CPU to determine the time slice for a process, and t hus\nensures that over this period of time, CFS will be completely fair .\nFor example, if there are n= 4 processes running, CFS divides the\nvalue ofschedlatency bynto arrive at a per-process time slice of 12\nms. CFS then schedules the ﬁrst job and runs it until it has used 12 ms\nof (virtual) runtime, and then checks to see if there is a job wit h lower\nvruntime to run instead. In this case, there is, and CFS would switch\nto one of the three other jobs, and so forth. Figure 9.4 shows an examp le\nwhere the four jobs (A, B, C, D) each run for two time slices in this fashion;\ntwo of them (C, D) then complete, leaving just two remaining, wh ich then\neach run for 24 ms in round-robin fashion.\nBut what if there are “too many” processes running? Wouldn’t that\nlead to too small of a time slice, and thus too many context switche s?\nGood question! And the answer is yes.\nTo address this issue, CFS adds another parameter, mingranularity ,\nwhich is usually set to a value like 6 ms. CFS will never set the time slice\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nSCHEDULING : PROPORTIONAL SHARE 9\nof a process to less than this value, ensuring that not too much tim e is\nspent in scheduling overhead.\nFor example, if there are ten processes running, our original cal cula-\ntion would divide schedlatency by ten to determine the time slice\n(result: 4.8 ms). However, because of mingranularity , CFS will set\nthe time slice of each process to 6 ms instead. Although CFS won’t (q uite)\nbe perfectly fair over the target scheduling latency ( schedlatency ) of\n48 ms, it will be close, while still achieving high CPU efﬁcien cy.\nNote that CFS utilizes a periodic timer interrupt, which means it can\nonly make decisions at ﬁxed time intervals. This interrupt goes off fre-\nquently (e.g., every 1 ms), giving CFS a chance to wake up and d etermine\nif the current job has reached the end of its run. If a job has a time slice\nthat is not a perfect multiple of the timer interrupt interval, that is OK;\nCFS tracks vruntime precisely, which means that over the long haul, it\nwill eventually approximate ideal sharing of the CPU.\nWeighting (Niceness)\nCFS also enables controls over process priority, enabling users or admin-\nistrators to give some processes a higher share of the CPU. It does t his\nnot with tickets, but through a classic U NIX mechanism known as the\nnice level of a process. The nice parameter can be set anywhere from -2 0\nto +19 for a process, with a default of 0. Positive nice values impl ylower\npriority and negative values imply higher priority; when you’re too nice,\nyou just don’t get as much (scheduling) attention, alas.\nCFS maps the nice value of each process to a weight , as shown here:\nstatic const int prio_to_weight[40] = {\n/*-20*/ 88761, 71755, 56483, 46273, 36291,\n/*-15*/ 29154, 23254, 18705, 14949, 11916,\n/*-10*/ 9548, 7620, 6100, 4904, 3906,\n/*-5*/ 3121, 2501, 1991, 1586, 1277,\n/*0*/ 1024, 820, 655, 526, 423,\n/*5*/ 335, 272, 215, 172, 137,\n/*10*/ 110, 87, 70, 56, 45,\n/*15*/ 36, 29, 23, 18, 15,\n};\nThese weights allow us to compute the effective time slice of eac h pro-\ncess (as we did before), but now accounting for their priority diff erences.\nThe formula used to do so is as follows:\ntimeslice k=weightk/summationtextn−1\ni=0weighti·schedlatency (9.1)\nLet’s do an example to see how this works. Assume there are two jobs ,\nA and B. A, because its our most precious job, is given a higher prior ity by\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 S CHEDULING : PROPORTIONAL SHARE\nassigning it a nice value of -5; B, because we hates it3, just has the default\npriority (nice value equal to 0). This means weight A(from the table)\nis 3121, whereas weight Bis 1024. If you then compute the time slice\nof each job, you’ll ﬁnd that A’s time slice is about3\n4ofschedlatency\n(hence, 36 ms), and B’s about1\n4(hence, 12 ms).\nIn addition to generalizing the time slice calculation, the wa y CFS cal-\nculatesvruntime must also be adapted. Here is the new formula, which\ntakes the actual run time that process ihas accrued ( runtime i) and scales\nit inversely by the weight of the process. In our running example , A’s\nvruntime will accumulate at one-third the rate of B’s.\nvruntime i=vruntime i+weight0\nweighti·runtime i (9.2)\nOne smart aspect of the construction of the table of weights above is\nthat the table preserves CPU proportionality ratios when the dif ference in\nnice values is constant. For example, if process A instead had a n ice value\nof 5 (not -5), and process B had a nice value of 10 (not 0), CFS would\nschedule them in exactly the same manner as before. Run through the\nmath yourself to see why.\nUsing Red-Black Trees\nOne major focus of CFS is efﬁciency, as stated above. For a schedule r,\nthere are many facets of efﬁciency, but one of them is as simple as this:\nwhen the scheduler has to ﬁnd the next job to run, it should do so a s\nquickly as possible. Simple data structures like lists don’t sca le: modern\nsystems sometimes are comprised of 1000s of processes, and thus se arch-\ning through a long-list every so many milliseconds is wasteful.\nCFS addresses this by keeping processes in a red-black tree [B72]. A\nred-black tree is one of many types of balanced trees; in contrast to a\nsimple binary tree (which can degenerate to list-like perfor mance un-\nder worst-case insertion patterns), balanced trees do a littl e extra work\nto maintain low depths, and thus ensure that operations are logar ithmic\n(and not linear) in time.\nCFS does not keep allprocess in this structure; rather, only running\n(or runnable) processes are kept therein. If a process goes to sle ep (say,\nwaiting on an I/O to complete, or for a network packet to arrive), it is\nremoved from the tree and kept track of elsewhere.\nLet’s look at an example to make this more clear. Assume there are t en\njobs, and that they have the following values of vruntime : 1, 5, 9, 10, 14,\n18, 17, 21, 22, and 24. If we kept these jobs in an ordered list, ﬁn ding the\nnext job to run would be simple: just remove the ﬁrst element. Howe ver,\nwhen placing that job back into the list (in order), we would have to scan\n3Yes, yes, we are using bad grammar here on purpose, please don’t s end in a bug ﬁx.\nWhy? Well, just a most mild of references to the Lord of the Rings, and ou r favorite anti-hero\nGollum, nothing to get too excited about.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nSCHEDULING : PROPORTIONAL SHARE 11\n1\n59\n1014\n18\n17 22\n21 24\nFigure 9.5: CFS Red-Black Tree\nthe list, looking for the right spot to insert it, an O(n)operation. Any\nsearch is also quite inefﬁcient, also taking linear time on av erage.\nKeeping the same values in a red-black tree makes most operation s\nmore efﬁcient, as depicted in Figure 9.5. Processes are ordered in the tree\nbyvruntime , and most operations (such as insertion and deletion) are\nlogarithmic in time, i.e., O(logn). Whennis in the thousands, logarith-\nmic is noticeably more efﬁcient than linear.\nDealing With I/O And Sleeping Processes\nOne problem with picking the lowest vruntime to run next arises with\njobs that have gone to sleep for a long period of time. Imagine two pro-\ncesses, A and B, one of which (A) runs continuously, and the other (B )\nwhich has gone to sleep for a long period of time (say, 10 seconds). Wh en\nB wakes up, its vruntime will be 10 seconds behind A’s, and thus (if\nwe’re not careful), B will now monopolize the CPU for the next 10 sec-\nonds while it catches up, effectively starving A.\nCFS handles this case by altering the vruntime of a job when it wakes\nup. Speciﬁcally, CFS sets the vruntime of that job to the minimum value\nfound in the tree (remember, the tree only contains running jobs) [B+18].\nIn this way, CFS avoids starvation, but not without a cost: jobs that sleep\nfor short periods of time frequently do not ever get their fair shar e of the\nCPU [AC97].\nOther CFS Fun\nCFS has many other features, too many to discuss at this point in t he\nbook. It includes numerous heuristics to improve cache performan ce, has\nstrategies for handling multiple CPUs effectively (as discu ssed later in the\nbook), can schedule across large groups of processes (instead of tre ating\neach process as an independent entity), and many other interes ting fea-\ntures. Read recent research, starting with Bouron [B+18], to l earn more.\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 S CHEDULING : PROPORTIONAL SHARE\nTIP: USEEFFICIENT DATA STRUCTURES WHEN APPROPRIATE\nIn many cases, a list will do. In many cases, it will not. Knowing w hich\ndata structure to use when is a hallmark of good engineering. In t he case\ndiscussed herein, simple lists found in earlier schedulers s imply do not\nwork well on modern systems, particular in the heavily loaded ser vers\nfound in datacenters. Such systems contain thousands of active pr o-\ncesses; searching through a long list to ﬁnd the next job to run on e ach\ncore every few milliseconds would waste precious CPU cycles. A be tter\nstructure was needed, and CFS provided one by adding an excelle nt im-\nplementation of a red-black tree. More generally, when picking a data\nstructure for a system you are building, carefully consider its access pat-\nterns and its frequency of usage; by understanding these, you w ill be able\nto implement the right structure for the task at hand.\n9.8 Summary\nWe have introduced the concept of proportional-share scheduling a nd\nbrieﬂy discussed three approaches: lottery scheduling, stri de scheduling,\nand the Completely Fair Scheduler (CFS) of Linux. Lottery uses ran dom-\nness in a clever way to achieve proportional share; stride does so deter-\nministically. CFS, the only “real” scheduler discussed in thi s chapter, is a\nbit like weighted round-robin with dynamic time slices, but bu ilt to scale\nand perform well under load; to our knowledge, it is the most widely\nused fair-share scheduler in existence today.\nNo scheduler is a panacea, and fair-share schedulers have th eir fair\nshare of problems. One issue is that such approaches do not partic ularly\nmesh well with I/O [AC97]; as mentioned above, jobs that perform I /O\noccasionally may not to get their fair share of CPU. Another issue i s that\nthey leave open the hard problem of ticket or priority assignment, i.e.,\nhow do you know how many tickets your browser should be allocated, or\nto what nice value to set your text editor? Other general-purpos e sched-\nulers (such as the MLFQ we discussed previously, and other simi lar Linux\nschedulers) handle these issues automatically and thus may b e more eas-\nily deployed.\nThe good news is that there are many domains in which these prob-\nlems are not the dominant concern, and proportional-share schedul ers\nare used to great effect. For example, in a virtualized data center (or\ncloud ), where you might like to assign one-quarter of your CPU cycles\nto the Windows VM and the rest to your base Linux installation, pr opor-\ntional sharing can be simple and effective. The idea can also b e extended\nto other resources; see Waldspurger [W02] for further details on how to\nproportionally share memory in VMWare’s ESX Server.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nSCHEDULING : PROPORTIONAL SHARE 13\nReferences\n[AC97] “Extending Proportional-Share Scheduling to a Network of Works tations” by Andrea\nC. Arpaci-Dusseau and David E. Culler. PDPTA’97, June 1997. A paper by one of the authors on\nhow to extend proportional-share scheduling to work better in a clustered env ironment.\n[B+18] “The Battle of the Schedulers: FreeBSD ULE vs. Linux CFS” by J. B ouron, S. Chevalley,\nB. Lepers, W. Zwaenepoel, R. Gouicem, J. Lawall, G. Muller, J. Sope na. USENIX ATC ’18,\nJuly 2018, Boston, Massachusetts. A recent, detailed work comparing Linux CFS and the FreeBSD\nschedulers. An excellent overview of each scheduler is also provi ded. The result of the comparison:\ninconclusive (in some cases CFS was better, and in others, ULE (the BSD sche duler), was. Sometimes\nin life there are no easy answers.\n[B72] “Symmetric binary B-Trees: Data Structure And Maintenance Algor ithms” by Rudolf\nBayer. Acta Informatica, Volume 1, Number 4, December 1972. A cool balanced tree introduced\nbefore you were born (most likely). One of many balanced trees out there; study your algorithms book\nfor more alternatives!\n[D82] “Why Numbering Should Start At Zero” by Edsger Dijkstra, Au gust 1982. Available:\nhttp://www.cs.utexas.edu/users/EWD/ewd08xx/EWD831.P DF.A short note from E.\nDijkstra, one of the pioneers of computer science. We’ll be hearing much m ore on this guy in the\nsection on Concurrency. In the meanwhile, enjoy this note, which include s this motivating quote: “One\nof my colleagues — not a computing scientist — accused a number of younger computing scientists of\n’pedantry’ because they started numbering at zero.” The note explains why d oing so is logical.\n[K+15] “Proﬁling A Warehouse-scale Computer” by S. Kanev, P . Ranganat han, J. P . Darago,\nK. Hazelwood, T. Moseley, G. Wei, D. Brooks. ISCA ’15, June, 2015 , Portland, Oregon. A\nfascinating study of where the cycles go in modern data centers, which are increasingly where most of\ncomputing happens. Almost 20% of CPU time is spent in the operating system , 5% in the scheduler\nalone!\n[J09] “Inside The Linux 2.6 Completely Fair Scheduler” by M. Tim Jones. De cember 15, 2009.\nhttp://ostep.org/Citations/inside-cfs.pdf .A simple overview of CFS from its ear-\nlier days. CFS was created by Ingo Molnar in a short burst of creativity whic h led to a 100K kernel\npatch developed in 62 hours.\n[KL88] “A Fair Share Scheduler” by J. Kay and P . Lauder. CACM, Volume 3 1 Issue 1, January\n1988. An early reference to a fair-share scheduler.\n[WW94] “Lottery Scheduling: Flexible Proportional-Share Resource Ma nagement” by Carl A.\nWaldspurger and William E. Weihl. OSDI ’94, November 1994. The landmark paper on lottery\nscheduling that got the systems community re-energized about schedulin g, fair sharing, and the power\nof simple randomized algorithms.\n[W95] “Lottery and Stride Scheduling: Flexible Proportional-Share R esource Management” by\nCarl A. Waldspurger. Ph.D. Thesis, MIT, 1995. The award-winning thesis of Waldspurger’s that\noutlines lottery and stride scheduling. If you’re thinking of writing a Ph. D. dissertation at some point,\nyou should always have a good example around, to give you something to strive for: this is such a good\none.\n[W02] “Memory Resource Management in VMware ESX Server” by Carl A. Wa ldspurger.\nOSDI ’02, Boston, Massachusetts. The paper to read about memory management in VMMs (a.k.a.,\nhypervisors). In addition to being relatively easy to read, the paper contains n umerous cool ideas about\nthis new type of VMM-level memory management.\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 S CHEDULING : PROPORTIONAL SHARE\nHomework (Simulation)\nThis program, lottery.py , allows you to see how a lottery scheduler\nworks. See the README for details.\nQuestions\n1. Compute the solutions for simulations with 3 jobs and random see ds of 1,\n2, and 3.\n2. Now run with two speciﬁc jobs: each of length 10, but one (job 0 ) with\njust 1 ticket and the other (job 1) with 100 (e.g., -l 10:1,10:100 ). What\nhappens when the number of tickets is so imbalanced? Will job 0 ev er run\nbefore job 1 completes? How often? In general, what does such a t icket\nimbalance do to the behavior of lottery scheduling?\n3. When running with two jobs of length 100 and equal ticket allo cations of 100\n(-l 100:100,100:100 ), how unfair is the scheduler? Run with some dif-\nferent random seeds to determine the (probabilistic) answer ; let unfairness\nbe determined by how much earlier one job ﬁnishes than the other.\n4. How does your answer to the previous question change as the quan tum size\n(-q) gets larger?\n5. Can you make a version of the graph that is found in the chapter ? What\nelse would be worth exploring? How would the graph look with a str ide\nscheduler?\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG",34914
13-10. Multi-CPU Scheduling.pdf,13-10. Multi-CPU Scheduling,"10\nMultiprocessor Scheduling (Advanced)\nThis chapter will introduce the basics of multiprocessor scheduling . As\nthis topic is relatively advanced, it may be best to cover it after you have\nstudied the topic of concurrency in some detail (i.e., the second m ajor\n“easy piece” of the book).\nAfter years of existence only in the high-end of the computing spe c-\ntrum, multiprocessor systems are increasingly commonplace, and have\nfound their way into desktop machines, laptops, and even mobile d e-\nvices. The rise of the multicore processor, in which multiple CPU cores\nare packed onto a single chip, is the source of this proliferation; these\nchips have become popular as computer architects have had a difﬁ cult\ntime making a single CPU much faster without using (way) too muc h\npower. And thus we all now have a few CPUs available to us, which i s a\ngood thing, right?\nOf course, there are many difﬁculties that arise with the arri val of more\nthan a single CPU. A primary one is that a typical application (i .e., some C\nprogram you wrote) only uses a single CPU; adding more CPUs does not\nmake that single application run faster. To remedy this proble m, you’ll\nhave to rewrite your application to run in parallel , perhaps using threads\n(as discussed in great detail in the second piece of this book). Mu lti-\nthreaded applications can spread work across multiple CPUs and thus\nrun faster when given more CPU resources.\nASIDE : ADVANCED CHAPTERS\nAdvanced chapters require material from a broad swath of the book t o\ntruly understand, while logically ﬁtting into a section that i s earlier than\nsaid set of prerequisite materials. For example, this chapter on multipro-\ncessor scheduling makes much more sense if you’ve ﬁrst read the mi ddle\npiece on concurrency; however, it logically ﬁts into the part of th e book\non virtualization (generally) and CPU scheduling (speciﬁcal ly). Thus, it\nis recommended such chapters be covered out of order; in this case, after\nthe second piece of the book.\n1\n2 MULTIPROCESSOR SCHEDULING (ADVANCED )\nMemoryCPU\nCache\nFigure 10.1: Single CPU With Cache\nBeyond applications, a new problem that arises for the operating s ys-\ntem is (not surprisingly!) that of multiprocessor scheduling . Thus far\nwe’ve discussed a number of principles behind single-processor schedul-\ning; how can we extend those ideas to work on multiple CPUs? What\nnew problems must we overcome? And thus, our problem:\nCRUX: HOWTOSCHEDULE JOBS ONMULTIPLE CPU S\nHow should the OS schedule jobs on multiple CPUs? What new prob-\nlems arise? Do the same old techniques work, or are new ideas requ ired?\n10.1 Background: Multiprocessor Architecture\nTo understand the new issues surrounding multiprocessor sched ul-\ning, we have to understand a new and fundamental difference b etween\nsingle-CPU hardware and multi-CPU hardware. This differen ce centers\naround the use of hardware caches (e.g., Figure 10.1), and exactly how\ndata is shared across multiple processors. We now discuss this is sue fur-\nther, at a high level. Details are available elsewhere [CSG99 ], in particular\nin an upper-level or perhaps graduate computer architecture c ourse.\nIn a system with a single CPU, there are a hierarchy of hardware\ncaches that in general help the processor run programs faster. Caches\nare small, fast memories that (in general) hold copies of popular data that\nis found in the main memory of the system. Main memory, in contrast,\nholds allof the data, but access to this larger memory is slower. By keep-\ning frequently accessed data in a cache, the system can make t he large,\nslow memory appear to be a fast one.\nAs an example, consider a program that issues an explicit load in struc-\ntion to fetch a value from memory, and a simple system with only a si ngle\nCPU; the CPU has a small cache (say 64 KB) and a large main memory .\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMULTIPROCESSOR SCHEDULING (ADVANCED ) 3\nMemoryCPU CPU\nCache Cache\nBus\nFigure 10.2: Two CPUs With Caches Sharing Memory\nThe ﬁrst time a program issues this load, the data resides in mai n mem-\nory, and thus takes a long time to fetch (perhaps in the tens of nan osec-\nonds, or even hundreds). The processor, anticipating that the da ta may be\nreused, puts a copy of the loaded data into the CPU cache. If the pr ogram\nlater fetches this same data item again, the CPU ﬁrst checks f or it in the\ncache; if it ﬁnds it there, the data is fetched much more quickl y (say, just\na few nanoseconds), and thus the program runs faster.\nCaches are thus based on the notion of locality , of which there are\ntwo kinds: temporal locality and spatial locality . The idea behind tem-\nporal locality is that when a piece of data is accessed, it is like ly to be\naccessed again in the near future; imagine variables or even i nstructions\nthemselves being accessed over and over again in a loop. The idea b e-\nhind spatial locality is that if a program accesses a data item a t address\nx, it is likely to access data items near xas well; here, think of a program\nstreaming through an array, or instructions being executed one a fter the\nother. Because locality of these types exist in many programs, ha rdware\nsystems can make good guesses about which data to put in a cache an d\nthus work well.\nNow for the tricky part: what happens when you have multiple pro-\ncessors in a single system, with a single shared main memory, as we see\nin Figure 10.2?\nAs it turns out, caching with multiple CPUs is much more compli-\ncated. Imagine, for example, that a program running on CPU 1 read s\na data item (with value D) at address A; because the data is not in the\ncache on CPU 1, the system fetches it from main memory, and gets th e\nvalueD. The program then modiﬁes the value at address A, just updat-\ning its cache with the new value D′; writing the data through all the way\nto main memory is slow, so the system will (usually) do that late r. Then\nassume the OS decides to stop running the program and move it to CP U\n2. The program then re-reads the value at address A; there is no such data\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 MULTIPROCESSOR SCHEDULING (ADVANCED )\nCPU 2’s cache, and thus the system fetches the value from main me mory,\nand gets the old value Dinstead of the correct value D′. Oops!\nThis general problem is called the problem of cache coherence , and\nthere is a vast research literature that describes many diff erent subtleties\ninvolved with solving the problem [SHW11]. Here, we will skip all of the\nnuance and make some major points; take a computer architecture c lass\n(or three) to learn more.\nThe basic solution is provided by the hardware: by monitoring mem-\nory accesses, hardware can ensure that basically the “right t hing” hap-\npens and that the view of a single shared memory is preserved. On e way\nto do this on a bus-based system (as described above) is to use an old\ntechnique known as bus snooping [G83]; each cache pays attention to\nmemory updates by observing the bus that connects them to main me m-\nory. When a CPU then sees an update for a data item it holds in its ca che,\nit will notice the change and either invalidate its copy (i.e., remove it\nfrom its own cache) or update it (i.e., put the new value into its cache\ntoo). Write-back caches, as hinted at above, make this more compli cated\n(because the write to main memory isn’t visible until later), b ut you can\nimagine how the basic scheme might work.\n10.2 Don’t Forget Synchronization\nGiven that the caches do all of this work to provide coherence, do p ro-\ngrams (or the OS itself) have to worry about anything when they ac cess\nshared data? The answer, unfortunately, is yes, and is documen ted in\ngreat detail in the second piece of this book on the topic of concurrenc y.\nWhile we won’t get into the details here, we’ll sketch/review som e of the\nbasic ideas here (assuming you’re familiar with concurrency).\nWhen accessing (and in particular, updating) shared data it ems or\nstructures across CPUs, mutual exclusion primitives (such as locks) should\nlikely be used to guarantee correctness (other approaches, suc h as build-\ninglock-free data structures, are complex and only used on occasion;\nsee the chapter on deadlock in the piece on concurrency for details ). For\nexample, assume we have a shared queue being accessed on multi ple\nCPUs concurrently. Without locks, adding or removing elements fr om\nthe queue concurrently will not work as expected, even with the u nder-\nlying coherence protocols; one needs locks to atomically update the data\nstructure to its new state.\nTo make this more concrete, imagine this code sequence, which is used\nto remove an element from a shared linked list, as we see in Figur e 10.3.\nImagine if threads on two CPUs enter this routine at the same tim e. If\nThread 1 executes the ﬁrst line, it will have the current valu e ofhead\nstored in its tmp variable; if Thread 2 then executes the ﬁrst line as well,\nit also will have the same value of head stored in its own private tmp\nvariable (tmp is allocated on the stack, and thus each thread will have\nits own private storage for it). Thus, instead of each thread remov ing\nan element from the head of the list, each thread will try to remov e the\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMULTIPROCESSOR SCHEDULING (ADVANCED ) 5\n1typedef struct __Node_t {\n2int value;\n3struct __Node_t *next;\n4} Node_t;\n5\n6int List_Pop() {\n7Node_t*tmp = head; // remember old head ...\n8int value = head->value; // ... and its value\n9head = head->next; // advance head to next pointer\n10 free(tmp); // free old head\n11 return value; // return value at head\n12}\nFigure 10.3: Simple List Delete Code\nsame head element, leading to all sorts of problems (such as an at tempted\ndouble free of the head element at line 4, as well as potentially r eturning\nthe same data value twice).\nThe solution, of course, is to make such routines correct via lock-\ning. In this case, allocating a simple mutex (e.g., pthread mutext\nm;) and then adding a lock(&m) at the beginning of the routine and\nanunlock(&m) at the end will solve the problem, ensuring that the code\nwill execute as desired. Unfortunately, as we will see, such a n approach is\nnot without problems, in particular with regards to performance . Speciﬁ-\ncally, as the number of CPUs grows, access to a synchronized shar ed data\nstructure becomes quite slow.\n10.3 One Final Issue: Cache Afﬁnity\nOne ﬁnal issue arises in building a multiprocessor cache sched uler,\nknown as cache afﬁnity [TTG95]. This notion is simple: a process, when\nrun on a particular CPU, builds up a fair bit of state in the cache s (and\nTLBs) of the CPU. The next time the process runs, it is often advan ta-\ngeous to run it on the same CPU, as it will run faster if some of its st ate\nis already present in the caches on that CPU. If, instead, one ru ns a pro-\ncess on a different CPU each time, the performance of the process w ill be\nworse, as it will have to reload the state each time it runs (note i t will run\ncorrectly on a different CPU thanks to the cache coherence protocol s of\nthe hardware). Thus, a multiprocessor scheduler should conside r cache\nafﬁnity when making its scheduling decisions, perhaps prefe rring to keep\na process on the same CPU if at all possible.\n10.4 Single-Queue Scheduling\nWith this background in place, we now discuss how to build a sched -\nuler for a multiprocessor system. The most basic approach is to sim ply\nreuse the basic framework for single processor scheduling, by pu tting all\njobs that need to be scheduled into a single queue; we call this single-\nqueue multiprocessor scheduling orSQMS for short. This approach\nhas the advantage of simplicity; it does not require much work to t ake an\nexisting policy that picks the best job to run next and adapt it t o work on\nmore than one CPU (where it might pick the best two jobs to run, if t here\nare two CPUs, for example).\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 MULTIPROCESSOR SCHEDULING (ADVANCED )\nHowever, SQMS has obvious shortcomings. The ﬁrst problem is a lack\nofscalability . To ensure the scheduler works correctly on multiple CPUs,\nthe developers will have inserted some form of locking into the code, as\ndescribed above. Locks ensure that when SQMS code accesses the si ngle\nqueue (say, to ﬁnd the next job to run), the proper outcome arises.\nLocks, unfortunately, can greatly reduce performance, partic ularly as\nthe number of CPUs in the systems grows [A91]. As contention for suc h\na single lock increases, the system spends more and more time in l ock\noverhead and less time doing the work the system should be doing (not e:\nit would be great to include a real measurement of this in here som eday).\nThe second main problem with SQMS is cache afﬁnity. For example,\nlet us assume we have ﬁve jobs to run ( A,B,C,D,E) and four processors.\nOur scheduling queue thus looks like this:\nQueue A B C D E NULL\nOver time, assuming each job runs for a time slice and then anothe r\njob is chosen, here is a possible job schedule across CPUs:\nCPU 3CPU 2CPU 1CPU 0\nDCBAECBAEDBAEDCAEDCB\n ... (repeat) ... ... (repeat) ... ... (repeat) ... ... (repeat) ...\nBecause each CPU simply picks the next job to run from the globall y-\nshared queue, each job ends up bouncing around from CPU to CPU, thu s\ndoing exactly the opposite of what would make sense from the stand-\npoint of cache afﬁnity.\nTo handle this problem, most SQMS schedulers include some kind of\nafﬁnity mechanism to try to make it more likely that process wil l continue\nto run on the same CPU if possible. Speciﬁcally, one might provide a fﬁn-\nity for some jobs, but move others around to balance load. For example,\nimagine the same ﬁve jobs scheduled as follows:\nCPU 3CPU 2CPU 1CPU 0\nDDDDECCCECBBEBBAEAAA\n ... (repeat) ... ... (repeat) ... ... (repeat) ... ... (repeat) ...\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMULTIPROCESSOR SCHEDULING (ADVANCED ) 7\nIn this arrangement, jobs Athrough Dare not moved across proces-\nsors, with only job Emigrating from CPU to CPU, thus preserving afﬁn-\nity for most. You could then decide to migrate a different job the ne xt\ntime through, thus achieving some kind of afﬁnity fairness as we ll. Im-\nplementing such a scheme, however, can be complex.\nThus, we can see the SQMS approach has its strengths and weak-\nnesses. It is straightforward to implement given an existing single-CPU\nscheduler, which by deﬁnition has only a single queue. However, it does\nnot scale well (due to synchronization overheads), and it does not r eadily\npreserve cache afﬁnity.\n10.5 Multi-Queue Scheduling\nBecause of the problems caused in single-queue schedulers, som e sys-\ntems opt for multiple queues, e.g., one per CPU. We call this appr oach\nmulti-queue multiprocessor scheduling (orMQMS ).\nIn MQMS, our basic scheduling framework consists of multiple sche dul-\ning queues. Each queue will likely follow a particular schedul ing disci-\npline, such as round robin, though of course any algorithm can be use d.\nWhen a job enters the system, it is placed on exactly one scheduli ng\nqueue, according to some heuristic (e.g., random, or picking one w ith\nfewer jobs than others). Then it is scheduled essentially inde pendently,\nthus avoiding the problems of information sharing and synchroniza tion\nfound in the single-queue approach.\nFor example, assume we have a system where there are just two CP Us\n(labeled CPU 0 and CPU 1), and some number of jobs enter the system :\nA,B,C, andDfor example. Given that each CPU has a scheduling queue\nnow, the OS has to decide into which queue to place each job. It mi ght do\nsomething like this:\nQ0 A C Q1 B D\nDepending on the queue scheduling policy, each CPU now has two\njobs to choose from when deciding what should run. For example, with\nround robin , the system might produce a schedule that looks like this:\nCPU 1CPU 0 AACCAACCAACC\nBBDDBBDDBBDD  ...  ... \nMQMS has a distinct advantage of SQMS in that it should be inher-\nently more scalable. As the number of CPUs grows, so too does the num -\nber of queues, and thus lock and cache contention should not become a\ncentral problem. In addition, MQMS intrinsically provides cac he afﬁnity;\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 MULTIPROCESSOR SCHEDULING (ADVANCED )\njobs stay on the same CPU and thus reap the advantage of reusing ca ched\ncontents therein.\nBut, if you’ve been paying attention, you might see that we have a n ew\nproblem, which is fundamental in the multi-queue based approa ch:load\nimbalance . Let’s assume we have the same set up as above (four jobs,\ntwo CPUs), but then one of the jobs (say C) ﬁnishes. We now have the\nfollowing scheduling queues:\nQ0 A Q1 B D\nIf we then run our round-robin policy on each queue of the system, we\nwill see this resulting schedule:\nCPU 1CPU 0 AAAAAAAAAAAA\nBBDDBBDDBBDD  ...  ... \nAs you can see from this diagram, Agets twice as much CPU as Band\nD, which is not the desired outcome. Even worse, let’s imagine that b oth\nAandCﬁnish, leaving just jobs BandDin the system. The scheduling\nqueues will look like this:\nQ0 Q1 B D\nAs a result, CPU 0 will be left idle! (insert dramatic and sinister music here)\nAnd hence our CPU usage timeline looks sad:\nCPU 0\nCPU 1 BBDDBBDDBBDD  ... \nSo what should a poor multi-queue multiprocessor scheduler do? How\ncan we overcome the insidious problem of load imbalance and defeat t he\nevil forces of ... the Decepticons1? How do we stop asking questions that\nare hardly relevant to this otherwise wonderful book?\n1Little known fact is that the home planet of Cybertron was destroyed b y bad CPU\nscheduling decisions. And now let that be the ﬁrst and last reference to Trans formers in this\nbook, for which we sincerely apologize.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMULTIPROCESSOR SCHEDULING (ADVANCED ) 9\nCRUX: HOWTODEAL WITHLOAD IMBALANCE\nHow should a multi-queue multiprocessor scheduler handle load im -\nbalance, so as to better achieve its desired scheduling goals ?\nThe obvious answer to this query is to move jobs around, a technique\nwhich we (once again) refer to as migration . By migrating a job from one\nCPU to another, true load balance can be achieved.\nLet’s look at a couple of examples to add some clarity. Once again, we\nhave a situation where one CPU is idle and the other has some jobs.\nQ0 Q1 B D\nIn this case, the desired migration is easy to understand: the OS should\nsimply move one of BorDto CPU 0. The result of this single job migra-\ntion is evenly balanced load and everyone is happy.\nA more tricky case arises in our earlier example, where Awas left\nalone on CPU 0 and BandDwere alternating on CPU 1:\nQ0 A Q1 B D\nIn this case, a single migration does not solve the problem. What\nwould you do in this case? The answer, alas, is continuous migrati on\nof one or more jobs. One possible solution is to keep switching jobs, as\nwe see in the following timeline. In the ﬁgure, ﬁrst Ais alone on CPU 0,\nandBandDalternate on CPU 1. After a few time slices, Bis moved to\ncompete with Aon CPU 0, while Denjoys a few time slices alone on CPU\n1. And thus load is balanced:\nCPU 0\nCPU 1AAAABABABBBB\nBDBDDDDDADAD  ...  ... \nOf course, many other possible migration patterns exist. But now f or\nthe tricky part: how should the system decide to enact such a mig ration?\nOne basic approach is to use a technique known as work stealing\n[FLR98]. With a work-stealing approach, a (source) queue that i s low\non jobs will occasionally peek at another (target) queue, to see how full\nit is. If the target queue is (notably) more full than the source q ueue, the\nsource will “steal” one or more jobs from the target to help balance l oad.\nOf course, there is a natural tension in such an approach. If you look\naround at other queues too often, you will suffer from high overhead\nand have trouble scaling, which was the entire purpose of implem enting\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 MULTIPROCESSOR SCHEDULING (ADVANCED )\nthe multiple queue scheduling in the ﬁrst place! If, on the othe r hand,\nyou don’t look at other queues very often, you are in danger of suffering\nfrom severe load imbalances. Finding the right threshold remai ns, as is\ncommon in system policy design, a black art.\n10.6 Linux Multiprocessor Schedulers\nInterestingly, in the Linux community, no common solution has ap-\nproached to building a multiprocessor scheduler. Over time, th ree dif-\nferent schedulers arose: the O(1) scheduler, the Completely F air Sched-\nuler (CFS), and the BF Scheduler (BFS)2. See Meehean’s dissertation for\nan excellent overview of the strengths and weaknesses of said sc hedulers\n[M11]; here we just summarize a few of the basics.\nBoth O(1) and CFS use multiple queues, whereas BFS uses a singl e\nqueue, showing that both approaches can be successful. Of course , there\nare many other details which separate these schedulers. For ex ample, the\nO(1) scheduler is a priority-based scheduler (similar to the MLFQ dis-\ncussed before), changing a process’s priority over time and then s chedul-\ning those with highest priority in order to meet various scheduli ng objec-\ntives; interactivity is a particular focus. CFS, in contrast, i s a deterministic\nproportional-share approach (more like Stride scheduling, as dis cussed\nearlier). BFS, the only single-queue approach among the three, i s also\nproportional-share, but based on a more complicated scheme known as\nEarliest Eligible Virtual Deadline First (EEVDF) [SA96]. Re ad more about\nthese modern algorithms on your own; you should be able to understand\nhow they work now!\n10.7 Summary\nWe have seen various approaches to multiprocessor scheduling. T he\nsingle-queue approach (SQMS) is rather straightforward to buil d and bal-\nances load well but inherently has difﬁculty with scaling to m any pro-\ncessors and cache afﬁnity. The multiple-queue approach (MQMS) scales\nbetter and handles cache afﬁnity well, but has trouble with loa d imbal-\nance and is more complicated. Whichever approach you take, there is no\nsimple answer: building a general purpose scheduler remains a daunting\ntask, as small code changes can lead to large behavioral differ ences. Only\nundertake such an exercise if you know exactly what you are doing, or,\nat least, are getting paid a large amount of money to do so.\n2Look up what BF stands for on your own; be forewarned, it is not for the faint of heart.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMULTIPROCESSOR SCHEDULING (ADVANCED ) 11\nReferences\n[A90] “The Performance of Spin Lock Alternatives for Shared-Memory Multipr ocessors” by\nThomas E. Anderson. IEEE TPDS Volume 1:1, January 1990. A classic paper on how different\nlocking alternatives do and don’t scale. By Tom Anderson, very well known re searcher in both systems\nand networking. And author of a very ﬁne OS textbook, we must say.\n[B+10] “An Analysis of Linux Scalability to Many Cores Abstract” by S ilas Boyd-Wickizer,\nAustin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek , Robert Morris, Nick-\nolai Zeldovich. OSDI ’10, Vancouver, Canada, October 2010. A terriﬁc modern paper on the\ndifﬁculties of scaling Linux to many cores.\n[CSG99] “Parallel Computer Architecture: A Hardware/Software Ap proach” by David E.\nCuller, Jaswinder Pal Singh, and Anoop Gupta. Morgan Kaufmann, 1999. A treasure ﬁlled\nwith details about parallel machines and algorithms. As Mark Hill humorously observes on the jacket,\nthe book contains more information than most research papers.\n[FLR98] “The Implementation of the Cilk-5 Multithreaded Language” by Matteo Frigo, Charles\nE. Leiserson, Keith Randall. PLDI ’98, Montreal, Canada, June 1998. Cilk is a lightweight\nlanguage and runtime for writing parallel programs, and an excellent examp le of the work-stealing\nparadigm.\n[G83] “Using Cache Memory To Reduce Processor-Memory Trafﬁc” by James R. G oodman.\nISCA ’83, Stockholm, Sweden, June 1983. The pioneering paper on how to use bus snooping, i.e.,\npaying attention to requests you see on the bus, to build a cache coherence protoc ol. Goodman’s research\nover many years at Wisconsin is full of cleverness, this being but one e xample.\n[M11] “Towards Transparent CPU Scheduling” by Joseph T. Meehean. Doctoral Dissertation\nat University of Wisconsin—Madison, 2011. A dissertation that covers a lot of the details of how\nmodern Linux multiprocessor scheduling works. Pretty awesome! But, as co-ad visors of Joe’s, we may\nbe a bit biased here.\n[SHW11] “A Primer on Memory Consistency and Cache Coherence” by Daniel J. Sor in, Mark\nD. Hill, and David A. Wood. Synthesis Lectures in Computer Architect ure. Morgan and Clay-\npool Publishers, May 2011. A deﬁnitive overview of memory consistency and multiprocessor caching.\nRequired reading for anyone who likes to know way too much about a given topic.\n[SA96] “Earliest Eligible Virtual Deadline First: A Flexible and Accurate Mechanism for Pro-\nportional Share Resource Allocation” by Ion Stoica and Hussein Abde l-Wahab. Technical Re-\nport TR-95-22, Old Dominion University, 1996. A tech report on this cool scheduling idea, from\nIon Stoica, now a professor at U.C. Berkeley and world expert in networking, di stributed systems, and\nmany other things.\n[TTG95] “Evaluating the Performance of Cache-Afﬁnity Scheduling in Shared-Memo ry Mul-\ntiprocessors” by Josep Torrellas, Andrew Tucker, Anoop Gupta. Jou rnal of Parallel and Dis-\ntributed Computing, Volume 24:2, February 1995. This is not the ﬁrst paper on the topic, but it has\ncitations to earlier work, and is a more readable and practical paper than some of the ear lier queuing-\nbased analysis papers.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 MULTIPROCESSOR SCHEDULING (ADVANCED )\nHomework (Simulation)\nIn this homework, we’ll use multi.py to simulate a multi-processor\nCPU scheduler, and learn about some of its details. Read the rela ted\nREADME for more information about the simulator and its options.\nQuestions\n1. To start things off, let’s learn how to use the simulator to stud y how to build\nan effective multi-processor scheduler. The ﬁrst simulation wil l run just one\njob, which has a run-time of 30, and a working-set size of 200. Run this job\n(called job ’a’ here) on one simulated CPU as follows: ./multi.py -n 1\n-L a:30:200 . How long will it take to complete? Turn on the -cﬂag to\nsee a ﬁnal answer, and the -tﬂag to see a tick-by-tick trace of the job and\nhow it is scheduled.\n2. Now increase the cache size so as to make the job’s working se t (size=200) ﬁt\ninto the cache (which, by default, is size=100); for example, run ./multi.py\n-n 1 -L a:30:200 -M 300 . Can you predict how fast the job will run\nonce it ﬁts in cache? (hint: remember the key parameter of the war mrate,\nwhich is set by the -r ﬂag) Check your answer by running with the s olve\nﬂag (-c) enabled.\n3. One cool thing about multi.py is that you can see more detail about what\nis going on with different tracing ﬂags. Run the same simulation a s above,\nbut this time with time left tracing enabled ( -T). This ﬂag shows both the\njob that was scheduled on a CPU at each time step, as well as how much\nrun-time that job has left after each tick has run. What do you not ice about\nhow that second column decreases?\n4. Now add one more bit of tracing, to show the status of each CPU c ache for\neach job, with the -Cﬂag. For each job, each cache will either show a blank\nspace (if the cache is cold for that job) or a ’w’ (if the cache i s warm for that\njob). At what point does the cache become warm for job ’a’ in this simple\nexample? What happens as you change the warmup time parameter ( -w)\nto lower or higher values than the default?\n5. At this point, you should have a good idea of how the simulator wo rks for\na single job running on a single CPU. But hey, isn’t this a multi-pro cessor\nCPU scheduling chapter? Oh yeah! So let’s start working with mul tiple jobs.\nSpeciﬁcally, let’s run the following three jobs on a two-CPU s ystem (i.e., type\n./multi.py -n 2 -L a:100:100,b:100:50,c:100:50 ) Can you pre-\ndict how long this will take, given a round-robin centralized scheduler? Use\n-cto see if you were right, and then dive down into details with -tto see a\nstep-by-step and then -Cto see whether caches got warmed effectively for\nthese jobs. What do you notice?\n6. Now we’ll apply some explicit controls to study cache afﬁnity , as described\nin the chapter. To do this, you’ll need the -Aﬂag. This ﬂag can be used\nto limit which CPUs the scheduler can place a particular job upon. I n this\ncase, let’s use it to place jobs ’b’ and ’c’ on CPU 1, while restr icting ’a’ to\nCPU 0. This magic is accomplished by typing this ./multi.py -n 2 -L\na:100:100,b:100:50, c:100:50 -A a:0,b:1,c:1 ; don’t forget to\nturn on various tracing options to see what is really happening ! Can you\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMULTIPROCESSOR SCHEDULING (ADVANCED ) 13\npredict how fast this version will run? Why does it do better? W ill other\ncombinations of ’a’, ’b’, and ’c’ onto the two processors run fa ster or slower?\n7. One interesting aspect of caching multiprocessors is the op portunity for\nbetter-than-expected speed up of jobs when using multiple CPUs (an d their\ncaches) as compared to running jobs on a single processor. Spec iﬁcally,\nwhen you run on NCPUs, sometimes you can speed up by more than a fac-\ntor ofN, a situation entitled super-linear speedup . To experiment with this,\nuse the job description here ( -L a:100:100,b:100:100,c:100:100 ) with\na small cache ( -M 50 ) to create three jobs. Run this on systems with 1, 2, and\n3 CPUs (-n 1 ,-n 2 ,-n 3 ). Now, do the same, but with a larger per-CPU\ncache of size 100. What do you notice about performance as the numb er\nof CPUs scales? Use -cto conﬁrm your guesses, and other tracing ﬂags to\ndive even deeper.\n8. One other aspect of the simulator worth studying is the per-CP U scheduling\noption, the -pﬂag. Run with two CPUs again, and this three job conﬁgu-\nration (-L a:100:100,b:100:50,c:100:50 ). How does this option do,\nas opposed to the hand-controlled afﬁnity limits you put in pla ce above?\nHow does performance change as you alter the ’peek interval’ ( -P) to lower\nor higher values? How does this per-CPU approach work as the numb er of\nCPUs scales?\n9. Finally, feel free to just generate random workloads and se e if you can pre-\ndict their performance on different numbers of processors, cac he sizes, and\nscheduling options. If you do this, you’ll soon be a multi-processor schedul-\ning master , which is a pretty awesome thing to be. Good luck!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",31072
14-11. Summary Dialogue on CPU Virtualization.pdf,14-11. Summary Dialogue on CPU Virtualization,"11\nSummary Dialogue on CPU Virtualization\nProfessor: So, Student, did you learn anything?\nStudent: Well, Professor, that seems like a loaded question. I think you only\nwant me to say “yes.”\nProfessor: That’s true. But it’s also still an honest question. Come on, give a\nprofessor a break, will you?\nStudent: OK, OK. I think I did learn a few things. First, I learned a little about\nhow the OS virtualizes the CPU. There are a bunch of important mechanisms\nthat I had to understand to make sense of this: traps and trap ha ndlers, timer\ninterrupts, and how the OS and the hardware have to carefully save and restore\nstate when switching between processes.\nProfessor: Good, good!\nStudent: All those interactions do seem a little complicated though; how can I\nlearn more?\nProfessor: Well, that’s a good question. I think there is no substitute for doing;\njust reading about these things doesn’t quite give you the proper s ense. Do the\nclass projects and I bet by the end it will all kind of make sense.\nStudent: Sounds good. What else can I tell you?\nProfessor: Well, did you get some sense of the philosophy of the OS in your\nquest to understand its basic machinery?\nStudent: Hmm... I think so. It seems like the OS is fairly paranoid. It wants\nto make sure it stays in charge of the machine. While it wants a progra m to run\nas efﬁciently as possible (and hence the whole reasoning behind limited direct\nexecution ), the OS also wants to be able to say “Ah! Not so fast my friend”\nin case of an errant or malicious process. Paranoia rules the day, an d certainly\nkeeps the OS in charge of the machine. Perhaps that is why we think o f the OS\nas a resource manager.\nProfessor: Yes indeed — sounds like you are starting to put it together! Nice.\nStudent: Thanks.\n1\n2 S UMMARY DIALOGUE ON CPU V IRTUALIZATION\nProfessor: And what about the policies on top of those mechanisms — any\ninteresting lessons there?\nStudent: Some lessons to be learned there for sure. Perhaps a little obvious, b ut\nobvious can be good. Like the notion of bumping short jobs to the fron t of the\nqueue — I knew that was a good idea ever since the one time I was buyin g some\ngum at the store, and the guy in front of me had a credit card that wo uldn’t work.\nHe was no short job, let me tell you.\nProfessor: That sounds oddly rude to that poor fellow. What else?\nStudent: Well, that you can build a smart scheduler that tries to be like SJF\nand RR all at once — that MLFQ was pretty neat. Building up a real sch eduler\nseems difﬁcult.\nProfessor: Indeed it is. That’s why there is still controversy to this day over\nwhich scheduler to use; see the Linux battles between CFS, BFS, an d the O(1)\nscheduler, for example. And no, I will not spell out the full name of BFS .\nStudent: And I won’t ask you to! These policy battles seem like they could rage\nforever; is there really a right answer?\nProfessor: Probably not. After all, even our own metrics are at odds: if your\nscheduler is good at turnaround time, it’s bad at response time, an d vice versa.\nAs Lampson said, perhaps the goal isn’t to ﬁnd the best solution, bu t rather to\navoid disaster.\nStudent: That’s a little depressing.\nProfessor: Good engineering can be that way. And it can also be uplifting!\nIt’s just your perspective on it, really. I personally think being prag matic is a\ngood thing, and pragmatists realize that not all problems have clean and easy\nsolutions. Anything else that caught your fancy?\nStudent: I really liked the notion of gaming the scheduler; it seems like that\nmight be something to look into when I’m next running a job on Amazon’s EC2\nservice. Maybe I can steal some cycles from some other unsuspect ing (and more\nimportantly, OS-ignorant) customer!\nProfessor: It looks like I might have created a monster! Professor Frankenste in\nis not what I’d like to be called, you know.\nStudent: But isn’t that the idea? To get us excited about something, so much so\nthat we look into it on our own? Lighting ﬁres and all that?\nProfessor: I guess so. But I didn’t think it would work!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",4151
15-7. CPU Scheduling.pdf,15-7. CPU Scheduling,"7\nScheduling: Introduction\nBy now low-level mechanisms of running processes (e.g., context switch-\ning) should be clear; if they are not, go back a chapter or two, and r ead the\ndescription of how that stuff works again. However, we have yet to u n-\nderstand the high-level policies that an OS scheduler employs. We will\nnow do just that, presenting a series of scheduling policies (sometimes\ncalled disciplines ) that various smart and hard-working people have de-\nveloped over the years.\nThe origins of scheduling, in fact, predate computer systems; e arly\napproaches were taken from the ﬁeld of operations management and a p-\nplied to computers. This reality should be no surprise: assemb ly lines\nand many other human endeavors also require scheduling, and ma ny of\nthe same concerns exist therein, including a laser-like desi re for efﬁciency.\nAnd thus, our problem:\nTHECRUX: HOWTODEVELOP SCHEDULING POLICY\nHow should we develop a basic framework for thinking about\nscheduling policies? What are the key assumptions? What metri cs are\nimportant? What basic approaches have been used in the earlies t of com-\nputer systems?\n7.1 Workload Assumptions\nBefore getting into the range of possible policies, let us ﬁrst ma ke a\nnumber of simplifying assumptions about the processes running i n the\nsystem, sometimes collectively called the workload . Determining the\nworkload is a critical part of building policies, and the more you kn ow\nabout workload, the more ﬁne-tuned your policy can be.\nThe workload assumptions we make here are mostly unrealistic, bu t\nthat is alright (for now), because we will relax them as we go, and even-\ntually develop what we will refer to as ... (dramatic pause) ...\n1\n2 SCHEDULING : INTRODUCTION\nafully-operational scheduling discipline1.\nWe will make the following assumptions about the processes, some-\ntimes called jobs , that are running in the system:\n1. Each job runs for the same amount of time.\n2. All jobs arrive at the same time.\n3. Once started, each job runs to completion.\n4. All jobs only use the CPU (i.e., they perform no I/O)\n5. The run-time of each job is known.\nWe said many of these assumptions were unrealistic, but just as some\nanimals are more equal than others in Orwell’s Animal Farm [O45], some\nassumptions are more unrealistic than others in this chapter. I n particu-\nlar, it might bother you that the run-time of each job is known: this would\nmake the scheduler omniscient, which, although it would be grea t (prob-\nably), is not likely to happen anytime soon.\n7.2 Scheduling Metrics\nBeyond making workload assumptions, we also need one more thing\nto enable us to compare different scheduling policies: a scheduling met-\nric. A metric is just something that we use to measure something, and\nthere are a number of different metrics that make sense in sche duling.\nFor now, however, let us also simplify our life by simply having a s in-\ngle metric: turnaround time . The turnaround time of a job is deﬁned\nas the time at which the job completes minus the time at which the job\narrived in the system. More formally, the turnaround time Tturnaround is:\nTturnaround =Tcompletion −Tarrival (7.1)\nBecause we have assumed that all jobs arrive at the same time, f or now\nTarrival= 0 and hence Tturnaround =Tcompletion . This fact will change\nas we relax the aforementioned assumptions.\nYou should note that turnaround time is a performance metric, which\nwill be our primary focus this chapter. Another metric of interes t isfair-\nness , as measured (for example) by Jain’s Fairness Index [J91]. Perfor-\nmance and fairness are often at odds in scheduling; a scheduler , for ex-\nample, may optimize performance but at the cost of preventing a fe w jobs\nfrom running, thus decreasing fairness. This conundrum shows u s that\nlife isn’t always perfect.\n7.3 First In, First Out (FIFO)\nThe most basic algorithm we can implement is known as First In, First\nOut (FIFO ) scheduling or sometimes First Come, First Served (FCFS ).\n1Said in the same way you would say “A fully-operational Death Star .”\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSCHEDULING : INTRODUCTION 3\nFIFO has a number of positive properties: it is clearly simple an d thus\neasy to implement. And, given our assumptions, it works pretty w ell.\nLet’s do a quick example together. Imagine three jobs arrive in t he\nsystem, A, B, and C, at roughly the same time ( Tarrival= 0). Because\nFIFO has to put some job ﬁrst, let’s assume that while they all arr ived\nsimultaneously, A arrived just a hair before B which arrived ju st a hair\nbefore C. Assume also that each job runs for 10 seconds. What will t he\naverage turnaround time be for these jobs?\n0 20 40 60 80 100 120\nTimeABC\nFigure 7.1: FIFO Simple Example\nFrom Figure 7.1, you can see that A ﬁnished at 10, B at 20, and C at 3 0.\nThus, the average turnaround time for the three jobs is simply10+20+30\n3=\n20. Computing turnaround time is as easy as that.\nNow let’s relax one of our assumptions. In particular, let’s relax as -\nsumption 1, and thus no longer assume that each job runs for the sam e\namount of time. How does FIFO perform now? What kind of workload\ncould you construct to make FIFO perform poorly?\n(think about this before reading on ... keep thinking ... got it?!)\nPresumably you’ve ﬁgured this out by now, but just in case, let’s do\nan example to show how jobs of different lengths can lead to trouble for\nFIFO scheduling. In particular, let’s again assume three jobs (A, B, and\nC), but this time A runs for 100 seconds while B and C run for 10 each .\n0 20 40 60 80 100 120\nTimeA BC\nFigure 7.2: Why FIFO Is Not That Great\nAs you can see in Figure 7.2, Job A runs ﬁrst for the full 100 seconds\nbefore B or C even get a chance to run. Thus, the average turnaroun d\ntime for the system is high: a painful 110 seconds (100+110+120\n3= 110 ).\nThis problem is generally referred to as the convoy effect [B+79], where\na number of relatively-short potential consumers of a resource get queued\nbehind a heavyweight resource consumer. This scheduling scen ario might\nremind you of a single line at a grocery store and what you feel like w hen\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 SCHEDULING : INTRODUCTION\nTIP: THEPRINCIPLE OF SJF\nShortest Job First represents a general scheduling principle t hat can be\napplied to any system where the perceived turnaround time per customer\n(or, in our case, a job) matters. Think of any line you have waited in : if\nthe establishment in question cares about customer satisfacti on, it is likely\nthey have taken SJF into account. For example, grocery stores common ly\nhave a “ten-items-or-less” line to ensure that shoppers with on ly a few\nthings to purchase don’t get stuck behind the family preparing for some\nupcoming nuclear winter.\nyou see the person in front of you with three carts full of provisions an d\na checkbook out; it’s going to be a while2.\nSo what should we do? How can we develop a better algorithm to\ndeal with our new reality of jobs that run for different amounts of ti me?\nThink about it ﬁrst; then read on.\n7.4 Shortest Job First (SJF)\nIt turns out that a very simple approach solves this problem; in fa ct\nit is an idea stolen from operations research [C54,PV56] and appl ied to\nscheduling of jobs in computer systems. This new scheduling dis cipline\nis known as Shortest Job First (SJF) , and the name should be easy to\nremember because it describes the policy quite completely: it runs the\nshortest job ﬁrst, then the next shortest, and so on.\n0 20 40 60 80 100 120\nTimeBC A\nFigure 7.3: SJF Simple Example\nLet’s take our example above but with SJF as our scheduling policy.\nFigure 7.3 shows the results of running A, B, and C. Hopefully the dia-\ngram makes it clear why SJF performs much better with regards to aver-\nage turnaround time. Simply by running B and C before A, SJF reduce s\naverage turnaround from 110 seconds to 50 (10+20+120\n3= 50 ), more than\na factor of two improvement.\nIn fact, given our assumptions about jobs all arriving at the same time,\nwe could prove that SJF is indeed an optimal scheduling algorithm. How-\never, you are in a systems class, not theory or operations research; no\nproofs are allowed.\n2Recommended action in this case: either quickly switch to a different li ne, or take a long,\ndeep, and relaxing breath. That’s right, breathe in, breathe out. It wi ll be OK, don’t worry.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSCHEDULING : INTRODUCTION 5\nASIDE : PREEMPTIVE SCHEDULERS\nIn the old days of batch computing, a number of non-preemptive sched-\nulers were developed; such systems would run each job to completi on\nbefore considering whether to run a new job. Virtually all modern sched-\nulers are preemptive , and quite willing to stop one process from run-\nning in order to run another. This implies that the scheduler em ploys the\nmechanisms we learned about previously; in particular, the sc heduler can\nperform a context switch , stopping one running process temporarily and\nresuming (or starting) another.\nThus we arrive upon a good approach to scheduling with SJF, but our\nassumptions are still fairly unrealistic. Let’s relax another . In particular,\nwe can target assumption 2, and now assume that jobs can arrive at any\ntime instead of all at once. What problems does this lead to?\n(Another pause to think ... are you thinking? Come on, you can do it )\nHere we can illustrate the problem again with an example. This time,\nassume A arrives at t= 0 and needs to run for 100 seconds, whereas B\nand C arrive at t= 10 and each need to run for 10 seconds. With pure\nSJF, we’d get the schedule seen in Figure 7.4.\n0 20 40 60 80 100 120\nTimeA BC[B,C arrive]\nFigure 7.4: SJF With Late Arrivals From B and C\nAs you can see from the ﬁgure, even though B and C arrived shortly\nafter A, they still are forced to wait until A has completed, and thus suffer\nthe same convoy problem. Average turnaround time for these three j obs\nis 103.33 seconds (100+(110 −10)+(120 −10)\n3). What can a scheduler do?\n7.5 Shortest Time-to-Completion First (STCF)\nTo address this concern, we need to relax assumption 3 (that jobs must\nrun to completion), so let’s do that. We also need some machinery w ithin\nthe scheduler itself. As you might have guessed, given our prev ious dis-\ncussion about timer interrupts and context switching, the sche duler can\ncertainly do something else when B and C arrive: it can preempt job A\nand decide to run another job, perhaps continuing A later. SJF by ou r deﬁ-\nnition is a non-preemptive scheduler, and thus suffers from the problems\ndescribed above.\nFortunately, there is a scheduler which does exactly that: add preemp-\ntion to SJF, known as the Shortest Time-to-Completion First (STCF ) or\nPreemptive Shortest Job First (PSJF ) scheduler [CK68]. Any time a new\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 SCHEDULING : INTRODUCTION\n0 20 40 60 80 100 120\nTimeABC A[B,C arrive]\nFigure 7.5: STCF Simple Example\njob enters the system, the STCF scheduler determines which of th e re-\nmaining jobs (including the new job) has the least time left, an d schedules\nthat one. Thus, in our example, STCF would preempt A and run B and C\nto completion; only when they are ﬁnished would A’s remaining time be\nscheduled. Figure 7.5 shows an example.\nThe result is a much-improved average turnaround time: 50 secon ds\n((120−0)+(20−10)+(30 −10)\n3). And as before, given our new assumptions,\nSTCF is provably optimal; given that SJF is optimal if all jobs arriv e at\nthe same time, you should probably be able to see the intuition beh ind\nthe optimality of STCF.\n7.6 A New Metric: Response Time\nThus, if we knew job lengths, and that jobs only used the CPU, and ou r\nonly metric was turnaround time, STCF would be a great policy. In fa ct,\nfor a number of early batch computing systems, these types of sche duling\nalgorithms made some sense. However, the introduction of time-sha red\nmachines changed all that. Now users would sit at a terminal and de-\nmand interactive performance from the system as well. And thus , a new\nmetric was born: response time .\nWe deﬁne response time as the time from when the job arrives in a\nsystem to the ﬁrst time it is scheduled3. More formally:\nTresponse=Tfirstrun−Tarrival (7.2)\nFor example, if we had the schedule above (with A arriving at tim e 0,\nand B and C at time 10), the response time of each job is as follows: 0 f or\njob A, 0 for B, and 10 for C (average: 3.33).\nAs you might be thinking, STCF and related disciplines are not pa r-\nticularly good for response time. If three jobs arrive at the same t ime,\nfor example, the third job has to wait for the previous two jobs to ru nin\ntheir entirety before being scheduled just once. While great for turnaround\ntime, this approach is quite bad for response time and interacti vity. In-\ndeed, imagine sitting at a terminal, typing, and having to wa it 10 seconds\n3Some deﬁne it slightly differently, e.g., to also include the time unt il the job produces\nsome kind of “response”; our deﬁnition is the best-case version of t his, essentially assuming\nthat the job produces a response instantaneously.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSCHEDULING : INTRODUCTION 7\n0 5 10 15 20 25 30\nTimeA B C\nFigure 7.6: SJF Again (Bad for Response Time)\n0 5 10 15 20 25 30\nTimeABCABCABCABCABC\nFigure 7.7: Round Robin (Good For Response Time)\nto see a response from the system just because some other job got sche d-\nuled in front of yours: not too pleasant.\nThus, we are left with another problem: how can we build a schedul er\nthat is sensitive to response time?\n7.7 Round Robin\nTo solve this problem, we will introduce a new scheduling algorit hm,\nclassically referred to as Round-Robin (RR) scheduling [K64]. The basic\nidea is simple: instead of running jobs to completion, RR runs a job for a\ntime slice (sometimes called a scheduling quantum ) and then switches\nto the next job in the run queue. It repeatedly does so until the j obs are\nﬁnished. For this reason, RR is sometimes called time-slicing . Note that\nthe length of a time slice must be a multiple of the timer-interr upt period;\nthus if the timer interrupts every 10 milliseconds, the time s lice could be\n10, 20, or any other multiple of 10 ms.\nTo understand RR in more detail, let’s look at an example. Assume\nthree jobs A, B, and C arrive at the same time in the system, and t hat\nthey each wish to run for 5 seconds. An SJF scheduler runs each job t o\ncompletion before running another (Figure 7.6). In contrast, RR w ith a\ntime-slice of 1 second would cycle through the jobs quickly (Figur e 7.7).\nThe average response time of RR is:0+1+2\n3= 1; for SJF, average re-\nsponse time is:0+5+10\n3= 5.\nAs you can see, the length of the time slice is critical for RR. The shorter\nit is, the better the performance of RR under the response-time m etric.\nHowever, making the time slice too short is problematic: suddenl y the\ncost of context switching will dominate overall performance. Thus , de-\nciding on the length of the time slice presents a trade-off to a sy stem de-\nsigner, making it long enough to amortize the cost of switching without\nmaking it so long that the system is no longer responsive.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 SCHEDULING : INTRODUCTION\nTIP: AMORTIZATION CANREDUCE COSTS\nThe general technique of amortization is commonly used in systems\nwhen there is a ﬁxed cost to some operation. By incurring that cost l ess\noften (i.e., by performing the operation fewer times), the total c ost to the\nsystem is reduced. For example, if the time slice is set to 10 ms , and the\ncontext-switch cost is 1 ms, roughly 10% of time is spent context sw itch-\ning and is thus wasted. If we want to amortize this cost, we can increase\nthe time slice, e.g., to 100 ms. In this case, less than 1% of tim e is spent\ncontext switching, and thus the cost of time-slicing has been am ortized.\nNote that the cost of context switching does not arise solely from the\nOS actions of saving and restoring a few registers. When programs run,\nthey build up a great deal of state in CPU caches, TLBs, branch p redictors,\nand other on-chip hardware. Switching to another job causes this s tate\nto be ﬂushed and new state relevant to the currently-running job to be\nbrought in, which may exact a noticeable performance cost [MB91] .\nRR, with a reasonable time slice, is thus an excellent schedul er if re-\nsponse time is our only metric. But what about our old friend turnarou nd\ntime? Let’s look at our example above again. A, B, and C, each with ru n-\nning times of 5 seconds, arrive at the same time, and RR is the sch eduler\nwith a (long) 1-second time slice. We can see from the picture abov e that\nA ﬁnishes at 13, B at 14, and C at 15, for an average of 14. Pretty aw ful!\nIt is not surprising, then, that RR is indeed one of the worst policies if\nturnaround time is our metric. Intuitively, this should make se nse: what\nRR is doing is stretching out each job as long as it can, by only runni ng\neach job for a short bit before moving to the next. Because turnaroun d\ntime only cares about when jobs ﬁnish, RR is nearly pessimal, eve n worse\nthan simple FIFO in many cases.\nMore generally, any policy (such as RR) that is fair, i.e., that evenly di-\nvides the CPU among active processes on a small time scale, will p erform\npoorly on metrics such as turnaround time. Indeed, this is an inhe rent\ntrade-off: if you are willing to be unfair, you can run shorter jobs to com-\npletion, but at the cost of response time; if you instead value fair ness,\nresponse time is lowered, but at the cost of turnaround time. This t ype of\ntrade-off is common in systems; you can’t have your cake and eat it too4.\nWe have developed two types of schedulers. The ﬁrst type (SJF, STC F)\noptimizes turnaround time, but is bad for response time. The secon d type\n(RR) optimizes response time but is bad for turnaround. And we sti ll\nhave two assumptions which need to be relaxed: assumption 4 (th at jobs\ndo no I/O), and assumption 5 (that the run-time of each job is known ).\nLet’s tackle those assumptions next.\n4A saying that confuses people, because it should be “You can’t keep your cake and eat it\ntoo” (which is kind of obvious, no?). Amazingly, there is a wikipedia pa ge about this saying;\neven more amazingly, it is kind of fun to read [W15]. As they say in Ita lian, you can’t Avere la\nbotte piena e la moglie ubriaca.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSCHEDULING : INTRODUCTION 9\nTIP: OVERLAP ENABLES HIGHER UTILIZATION\nWhen possible, overlap operations to maximize the utilization of sys-\ntems. Overlap is useful in many different domains, including when per-\nforming disk I/O or sending messages to remote machines; in eith er case,\nstarting the operation and then switching to other work is a good ide a,\nand improves the overall utilization and efﬁciency of the system .\n7.8 Incorporating I/O\nFirst we will relax assumption 4 — of course all programs perform\nI/O. Imagine a program that didn’t take any input: it would produc e the\nsame output each time. Imagine one without output: it is the prover bial\ntree falling in the forest, with no one to see it; it doesn’t matter that it ran.\nA scheduler clearly has a decision to make when a job initiates a n I/O\nrequest, because the currently-running job won’t be using the C PU dur-\ning the I/O; it is blocked waiting for I/O completion. If the I/O is sent to\na hard disk drive, the process might be blocked for a few millisec onds or\nlonger, depending on the current I/O load of the drive. Thus, the s ched-\nuler should probably schedule another job on the CPU at that time.\nThe scheduler also has to make a decision when the I/O completes .\nWhen that occurs, an interrupt is raised, and the OS runs and mov es\nthe process that issued the I/O from blocked back to the ready sta te. Of\ncourse, it could even decide to run the job at that point. How should t he\nOS treat each job?\nTo understand this issue better, let us assume we have two jobs , A and\nB, which each need 50 ms of CPU time. However, there is one obvious\ndifference: A runs for 10 ms and then issues an I/O request (ass ume here\nthat I/Os each take 10 ms), whereas B simply uses the CPU for 50 m s and\nperforms no I/O. The scheduler runs A ﬁrst, then B after (Figur e 7.8).\n0 20 40 60 80 100 120 140\nTimeA A A A A B BBBB\nCPU\nDisk\nFigure 7.8: Poor Use Of Resources\nAssume we are trying to build a STCF scheduler. How should such a\nscheduler account for the fact that A is broken up into 5 10-ms sub -jobs,\nwhereas B is just a single 50-ms CPU demand? Clearly, just run ning one\njob and then the other without considering how to take I/O into accou nt\nmakes little sense.\nA common approach is to treat each 10-ms sub-job of A as an indepen-\ndent job. Thus, when the system starts, its choice is whether to schedule\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 SCHEDULING : INTRODUCTION\n0 20 40 60 80 100 120 140\nTimeA A A A A B B B B B\nCPU\nDisk\nFigure 7.9: Overlap Allows Better Use Of Resources\na 10-ms A or a 50-ms B. With STCF, the choice is clear: choose the short er\none, in this case A. Then, when the ﬁrst sub-job of A has completed, only\nB is left, and it begins running. Then a new sub-job of A is submit ted,\nand it preempts B and runs for 10 ms. Doing so allows for overlap , with\nthe CPU being used by one process while waiting for the I/O of anothe r\nprocess to complete; the system is thus better utilized (see Fi gure 7.9).\nAnd thus we see how a scheduler might incorporate I/O. By treatin g\neach CPU burst as a job, the scheduler makes sure processes that are “in-\nteractive” get run frequently. While those interactive jobs a re performing\nI/O, other CPU-intensive jobs run, thus better utilizing the p rocessor.\n7.9 No More Oracle\nWith a basic approach to I/O in place, we come to our ﬁnal assump-\ntion: that the scheduler knows the length of each job. As we said be fore,\nthis is likely the worst assumption we could make. In fact, in a ge neral-\npurpose OS (like the ones we care about), the OS usually knows very little\nabout the length of each job. Thus, how can we build an approach that be-\nhaves like SJF/STCF without such a priori knowledge? Further, how can\nwe incorporate some of the ideas we have seen with the RR scheduler so\nthat response time is also quite good?\n7.10 Summary\nWe have introduced the basic ideas behind scheduling and deve loped\ntwo families of approaches. The ﬁrst runs the shortest job remain ing and\nthus optimizes turnaround time; the second alternates between all jobs\nand thus optimizes response time. Both are bad where the other is g ood,\nalas, an inherent trade-off common in systems. We have also seen how we\nmight incorporate I/O into the picture, but have still not solved the prob-\nlem of the fundamental inability of the OS to see into the future . Shortly,\nwe will see how to overcome this problem, by building a scheduler t hat\nuses the recent past to predict the future. This scheduler is known as the\nmulti-level feedback queue , and it is the topic of the next chapter.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSCHEDULING : INTRODUCTION 11\nReferences\n[B+79] “The Convoy Phenomenon” by M. Blasgen, J. Gray, M. Mitoma, T. Price. ACM Op -\nerating Systems Review, 13:2, April 1979. Perhaps the ﬁrst reference to convoys, which occurs in\ndatabases as well as the OS.\n[C54] “Priority Assignment in Waiting Line Problems” by A. Cobham. Jou rnal of Operations\nResearch, 2:70, pages 70–76, 1954. The pioneering paper on using an SJF approach in scheduling the\nrepair of machines.\n[K64] “Analysis of a Time-Shared Processor” by Leonard Kleinrock. Nav al Research Logistics\nQuarterly, 11:1, pages 59–73, March 1964. May be the ﬁrst reference to the round-robin scheduling\nalgorithm; certainly one of the ﬁrst analyses of said approach to scheduling a ti me-shared system.\n[CK68] “Computer Scheduling Methods and their Countermeasures” by Ed ward G. Coffman\nand Leonard Kleinrock. AFIPS ’68 (Spring), April 1968. An excellent early introduction to and\nanalysis of a number of basic scheduling disciplines.\n[J91] “The Art of Computer Systems Performance Analysis: Techniques for Ex perimental De-\nsign, Measurement, Simulation, and Modeling” by R. Jain. Interscience, Ne w York, April 1991.\nThe standard text on computer systems measurement. A great reference for y our library, for sure.\n[O45] “Animal Farm” by George Orwell. Secker and Warburg (London), 1945 .A great but\ndepressing allegorical book about power and its corruptions. Some say it is a c ritique of Stalin and the\npre-WWII Stalin era in the U.S.S.R; we say it’s a critique of pigs.\n[PV56] “Machine Repair as a Priority Waiting-Line Problem” by Thomas E. Phipp s Jr., W. R.\nVan Voorhis. Operations Research, 4:1, pages 76–86, February 195 6.Follow-on work that gen-\neralizes the SJF approach to machine repair from Cobham’s original work; also postu lates the utility of\nan STCF approach in such an environment. Speciﬁcally, “There are certain types of repair work, ...\ninvolving much dismantling and covering the ﬂoor with nuts and bolts, which certainly should not be\ninterrupted once undertaken; in other cases it would be inadvisable to contin ue work on a long job if one\nor more short ones became available (p.81).”\n[MB91] “The effect of context switches on cache performance” by Jeffrey C. Mogul, A nita Borg.\nASPLOS, 1991. A nice study on how cache performance can be affected by context switching; less of an\nissue in today’s systems where processors issue billions of instruction s per second but context-switches\nstill happen in the millisecond time range.\n[W15] “You can’t have your cake and eat it” by Authors: Unknown.. Wikiped ia (as of Decem-\nber 2015).http://en.wikipedia.org/wiki/You can’thaveyourcakeandeatit.\nThe best part of this page is reading all the similar idioms from other languag es. In Tamil, you can’t\n“have both the moustache and drink the soup.”\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 SCHEDULING : INTRODUCTION\nHomework (Simulation)\nThis program, scheduler.py , allows you to see how different sched-\nulers perform under scheduling metrics such as response time, turnaround\ntime, and total wait time. See the README for details.\nQuestions\n1. Compute the response time and turnaround time when running three j obs\nof length 200 with the SJF and FIFO schedulers.\n2. Now do the same but with jobs of different lengths: 100, 200, a nd 300.\n3. Now do the same, but also with the RR scheduler and a time-slice o f 1.\n4. For what types of workloads does SJF deliver the same turnaro und times as\nFIFO?\n5. For what types of workloads and quantum lengths does SJF deli ver the\nsame response times as RR?\n6. What happens to response time with SJF as job lengths increa se? Can you\nuse the simulator to demonstrate the trend?\n7. What happens to response time with RR as quantum lengths incre ase? Can\nyou write an equation that gives the worst-case response time, givenNjobs?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",27485
16-12. A Dialogue on Memory Virtualization.pdf,16-12. A Dialogue on Memory Virtualization,"12\nA Dialogue on Memory Virtualization\nStudent: So, are we done with virtualization?\nProfessor: No!\nStudent: Hey, no reason to get so excited; I was just asking a question. Stud ents\nare supposed to do that, right?\nProfessor: Well, professors do always say that, but really they mean this: ask\nquestions, ifthey are good questions, and you have actually put a little thought\ninto them.\nStudent: Well, that sure takes the wind out of my sails.\nProfessor: Mission accomplished. In any case, we are not nearly done with\nvirtualization! Rather, you have just seen how to virtualize the CPU, but really\nthere is a big monster waiting in the closet: memory. Virtualizing memor y is\ncomplicated and requires us to understand many more intricate det ails about\nhow the hardware and OS interact.\nStudent: That sounds cool. Why is it so hard?\nProfessor: Well, there are a lot of details, and you have to keep them straight\nin your head to really develop a mental model of what is going on. We’ll s tart\nsimple, with very basic techniques like base/bounds, and slowly add co mplexity\nto tackle new challenges, including fun topics like TLBs and multi-level pa ge\ntables. Eventually, we’ll be able to describe the workings of a fully-func tional\nmodern virtual memory manager.\nStudent: Neat! Any tips for the poor student, inundated with all of this infor-\nmation and generally sleep-deprived?\nProfessor: For the sleep deprivation, that’s easy: sleep more (and party less ).\nFor understanding virtual memory, start with this: every address generated\nby a user program is a virtual address . The OS is just providing an illusion\nto each process, speciﬁcally that it has its own large and private memo ry; with\nsome hardware help, the OS will turn these pretend virtual addres ses into real\nphysical addresses, and thus be able to locate the desired informat ion.\n1\n2 A D IALOGUE ON MEMORY VIRTUALIZATION\nStudent: OK, I think I can remember that... (to self) every address from a us er\nprogram is virtual, every address from a user program is virtual, eve ry ...\nProfessor: What are you mumbling about?\nStudent: Oh nothing.... (awkward pause) ... Anyway, why does the OS wan t\nto provide this illusion again?\nProfessor: Mostly ease of use : the OS will give each program the view that it\nhas a large contiguous address space to put its code and data into; thus, as a\nprogrammer, you never have to worry about things like “where sho uld I store this\nvariable?” because the virtual address space of the program is lar ge and has lots\nof room for that sort of thing. Life, for a programmer, becomes much more tricky\nif you have to worry about ﬁtting all of your code data into a small, cro wded\nmemory.\nStudent: Why else?\nProfessor: Well, isolation andprotection are big deals, too. We don’t want\none errant program to be able to read, or worse, overwrite, some other program’s\nmemory, do we?\nStudent: Probably not. Unless it’s a program written by someone you don’t\nlike.\nProfessor: Hmmm.... I think we might need to add a class on morals and ethics\nto your schedule for next semester. Perhaps OS class isn’t getting the right mes-\nsage across.\nStudent: Maybe we should. But remember, it’s not me who taught us that the\nproper OS response to errant process behavior is to kill the offendin g process!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",3390
17-13. Address Spaces.pdf,17-13. Address Spaces,"13\nThe Abstraction: Address Spaces\nIn the early days, building computer systems was easy. Why, you ask?\nBecause users didn’t expect much. It is those darned users with their\nexpectations of “ease of use”, “high performance”, “reliabilit y”, etc., that\nreally have led to all these headaches. Next time you meet one of t hose\ncomputer users, thank them for all the problems they have caused .\n13.1 Early Systems\nFrom the perspective of memory, early machines didn’t provide muc h\nof an abstraction to users. Basically, the physical memory of the machine\nlooked something like what you see in Figure 13.1.\nThe OS was a set of routines (a library, really) that sat in memory (start-\ning at physical address 0 in this example), and there would be on e run-\nning program (a process) that currently sat in physical memory ( starting\nat physical address 64k in this example) and used the rest of me mory.\nThere were few illusions here, and the user didn’t expect much f rom the\nOS. Life was sure easy for OS developers in those days, wasn’t it?\nmax64KB0KB\nCurrent Program\n(code, data, etc.)Operating System\n(code, data, etc.)\nFigure 13.1: Operating Systems: The Early Days\n1\n2 THEABSTRACTION : ADDRESS SPACES\n512KB448KB384KB320KB256KB192KB128KB64KB0KB\n(free)(free)(free)(free)Operating System\n(code, data, etc.)\nProcess A\n(code, data, etc.)Process B\n(code, data, etc.)Process C\n(code, data, etc.)\nFigure 13.2: Three Processes: Sharing Memory\n13.2 Multiprogramming and Time Sharing\nAfter a time, because machines were expensive, people began t o share\nmachines more effectively. Thus the era of multiprogramming was born\n[DV66], in which multiple processes were ready to run at a give n time,\nand the OS would switch between them, for example when one decide d\nto perform an I/O. Doing so increased the effective utilization of the\nCPU. Such increases in efﬁciency were particularly important in those\ndays where each machine cost hundreds of thousands or even million s of\ndollars (and you thought your Mac was expensive!).\nSoon enough, however, people began demanding more of machines,\nand the era of time sharing was born [S59, L60, M62, M83]. Speciﬁcally,\nmany realized the limitations of batch computing, particularl y on pro-\ngrammers themselves [CV65], who were tired of long (and hence i neffec-\ntive) program-debug cycles. The notion of interactivity became impor-\ntant, as many users might be concurrently using a machine, eac h waiting\nfor (or hoping for) a timely response from their currently-executi ng tasks.\nOne way to implement time sharing would be to run one process for a\nshort while, giving it full access to all memory (Figure 13.1, p age 1), then\nstop it, save all of its state to some kind of disk (including all of p hysical\nmemory), load some other process’s state, run it for a while, and thus\nimplement some kind of crude sharing of the machine [M+63].\nUnfortunately, this approach has a big problem: it is way too slow,\nparticularly as memory grows. While saving and restoring regis ter-level\nstate (the PC, general-purpose registers, etc.) is relative ly fast, saving the\nentire contents of memory to disk is brutally non-performant. Thu s, what\nwe’d rather do is leave processes in memory while switching betw een\nthem, allowing the OS to implement time sharing efﬁciently (F igure 13.2).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEABSTRACTION : ADDRESS SPACES 3\n16KB15KB2KB1KB0KB\nStack(free)HeapProgram Codethe code segment:\nwhere instructions live\nthe heap segment:\ncontains malloc’d data\ndynamic data structures\n(it grows downward)\n(it grows upward)\nthe stack segment:\ncontains local variables\narguments to routines, \nreturn values, etc.\nFigure 13.3: An Example Address Space\nIn the diagram, there are three processes (A, B, and C) and each of\nthem have a small part of the 512KB physical memory carved out for\nthem. Assuming a single CPU, the OS chooses to run one of the process es\n(say A), while the others (B and C) sit in the ready queue waitin g to run.\nAs time sharing became more popular, you can probably guess that\nnew demands were placed on the operating system. In particular , allow-\ning multiple programs to reside concurrently in memory makes protec-\ntion an important issue; you don’t want a process to be able to read, or\nworse, write some other process’s memory.\n13.3 The Address Space\nHowever, we have to keep those pesky users in mind, and doing so\nrequires the OS to create an easy to use abstraction of physical memory.\nWe call this abstraction the address space , and it is the running program’s\nview of memory in the system. Understanding this fundamental O S ab-\nstraction of memory is key to understanding how memory is virtuali zed.\nThe address space of a process contains all of the memory state of the\nrunning program. For example, the code of the program (the instruc-\ntions) have to live in memory somewhere, and thus they are in the a d-\ndress space. The program, while it is running, uses a stack to keep track\nof where it is in the function call chain as well as to allocate loca l variables\nand pass parameters and return values to and from routines. Fin ally, the\nheap is used for dynamically-allocated, user-managed memory, such as\nthat you might receive from a call to malloc() in C ornew in an object-\noriented language such as C++ or Java. Of course, there are other t hings\nin there too (e.g., statically-initialized variables), but for now let us just\nassume those three components: code, stack, and heap.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 THEABSTRACTION : ADDRESS SPACES\nIn the example in Figure 13.3 (page 3), we have a tiny address s pace\n(only 16KB)1. The program code lives at the top of the address space\n(starting at 0 in this example, and is packed into the ﬁrst 1K of the ad-\ndress space). Code is static (and thus easy to place in memory), so we can\nplace it at the top of the address space and know that it won’t need an y\nmore space as the program runs.\nNext, we have the two regions of the address space that may grow\n(and shrink) while the program runs. Those are the heap (at the t op) and\nthe stack (at the bottom). We place them like this because each w ishes to\nbe able to grow, and by putting them at opposite ends of the address\nspace, we can allow such growth: they just have to grow in opposite\ndirections. The heap thus starts just after the code (at 1KB) an d grows\ndownward (say when a user requests more memory via malloc() ); the\nstack starts at 16KB and grows upward (say when a user makes a pr oce-\ndure call). However, this placement of stack and heap is just a c onvention;\nyou could arrange the address space in a different way if you’d lik e (as\nwe’ll see later, when multiple threads co-exist in an address space, no\nnice way to divide the address space like this works anymore, al as).\nOf course, when we describe the address space, what we are desc rib-\ning is the abstraction that the OS is providing to the running program.\nThe program really isn’t in memory at physical addresses 0 throug h 16KB;\nrather it is loaded at some arbitrary physical address(es). Ex amine pro-\ncesses A, B, and C in Figure 13.2; there you can see how each proces s is\nloaded into memory at a different address. And hence the problem :\nTHECRUX: HOWTOVIRTUALIZE MEMORY\nHow can the OS build this abstraction of a private, potentially la rge\naddress space for multiple running processes (all sharing mem ory) on\ntop of a single, physical memory?\nWhen the OS does this, we say the OS is virtualizing memory , because\nthe running program thinks it is loaded into memory at a particul ar ad-\ndress (say 0) and has a potentially very large address space (s ay 32-bits or\n64-bits); the reality is quite different.\nWhen, for example, process A in Figure 13.2 tries to perform a load\nat address 0 (which we will call a virtual address ), somehow the OS, in\ntandem with some hardware support, will have to make sure the loa d\ndoesn’t actually go to physical address 0 but rather to physica l address\n320KB (where A is loaded into memory). This is the key to virtual ization\nof memory, which underlies every modern computer system in the wor ld.\n1We will often use small examples like this because (a) it is a pain t o represent a 32-bit\naddress space and (b) the math is harder. We like simple math.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEABSTRACTION : ADDRESS SPACES 5\nTIP: THEPRINCIPLE OFISOLATION\nIsolation is a key principle in building reliable systems. If t wo entities are\nproperly isolated from one another, this implies that one can fail w ith-\nout affecting the other. Operating systems strive to isolate pr ocesses from\neach other and in this way prevent one from harming the other. By us ing\nmemory isolation, the OS further ensures that running programs c annot\naffect the operation of the underlying OS. Some modern OS’s take iso-\nlation even further, by walling off pieces of the OS from other piec es of\nthe OS. Such microkernels [BH70, R+89, S+03] thus may provide greater\nreliability than typical monolithic kernel designs.\n13.4 Goals\nThus we arrive at the job of the OS in this set of notes: to virtualiz e\nmemory. The OS will not only virtualize memory, though; it will do s o\nwith style. To make sure the OS does so, we need some goals to guide u s.\nWe have seen these goals before (think of the Introduction), and we ’ll see\nthem again, but they are certainly worth repeating.\nOne major goal of a virtual memory (VM) system is transparency2.\nThe OS should implement virtual memory in a way that is invisibl e to\nthe running program. Thus, the program shouldn’t be aware of the fa ct\nthat memory is virtualized; rather, the program behaves as if i t has its\nown private physical memory. Behind the scenes, the OS (and har dware)\ndoes all the work to multiplex memory among many different jobs, an d\nhence implements the illusion.\nAnother goal of VM is efﬁciency . The OS should strive to make the\nvirtualization as efﬁcient as possible, both in terms of time (i.e., not mak-\ning programs run much more slowly) and space (i.e., not using too mu ch\nmemory for structures needed to support virtualization). In imp lement-\ning time-efﬁcient virtualization, the OS will have to rely on h ardware\nsupport, including hardware features such as TLBs (which we w ill learn\nabout in due course).\nFinally, a third VM goal is protection . The OS should make sure to\nprotect processes from one another as well as the OS itself from pro-\ncesses. When one process performs a load, a store, or an instruction f etch,\nit should not be able to access or affect in any way the memory conten ts\nof any other process or the OS itself (that is, anything outside its address\nspace). Protection thus enables us to deliver the property of isolation\namong processes; each process should be running in its own isolated co-\ncoon, safe from the ravages of other faulty or even malicious processe s.\n2This usage of transparency is sometimes confusing; some students think tha t “being\ntransparent” means keeping everything out in the open, i.e., what gove rnment should be like.\nHere, it means the opposite: that the illusion provided by the OS shou ld not be visible to ap-\nplications. Thus, in common usage, a transparent system is one that is ha rd to notice, not one\nthat responds to requests as stipulated by the Freedom of Informat ion Act.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 THEABSTRACTION : ADDRESS SPACES\nASIDE : EVERY ADDRESS YOUSEEISVIRTUAL\nEver write a C program that prints out a pointer? The value you see\n(some large number, often printed in hexadecimal), is a virtual address .\nEver wonder where the code of your program is found? You can print\nthat out too, and yes, if you can print it, it also is a virtual addre ss. In\nfact, any address you can see as a programmer of a user-level progr am\nis a virtual address. It’s only the OS, through its tricky techniq ues of\nvirtualizing memory, that knows where in the physical memory of t he\nmachine these instructions and data values lie. So never forget : if you\nprint out an address in a program, it’s a virtual one, an illusion of h ow\nthings are laid out in memory; only the OS (and the hardware) knows the\nreal truth.\nHere’s a little program ( va.c ) that prints out the locations of the main()\nroutine (where code lives), the value of a heap-allocated value r eturned\nfrommalloc() , and the location of an integer on the stack:\n1#include <stdio.h>\n2#include <stdlib.h>\n3int main(int argc, char *argv[]) {\n4printf(""location of code : %p\n"", (void *) main);\n5printf(""location of heap : %p\n"", (void *) malloc(1));\n6int x = 3;\n7printf(""location of stack : %p\n"", (void *) &x);\n8return x;\n9}\nWhen run on a 64-bit Mac, we get the following output:\nlocation of code : 0x1095afe50\nlocation of heap : 0x1096008c0\nlocation of stack : 0x7fff691aea64\nFrom this, you can see that code comes ﬁrst in the address space, th en\nthe heap, and the stack is all the way at the other end of this larg e virtual\nspace. All of these addresses are virtual, and will be transla ted by the OS\nand hardware in order to fetch values from their true physical l ocations.\nIn the next chapters, we’ll focus our exploration on the basic mecha-\nnisms needed to virtualize memory, including hardware and operatin g\nsystems support. We’ll also investigate some of the more relevant poli-\ncies that you’ll encounter in operating systems, including how to mana ge\nfree space and which pages to kick out of memory when you run low on\nspace. In doing so, we’ll build up your understanding of how a modern\nvirtual memory system really works3.\n3Or, we’ll convince you to drop the course. But hold on; if you make it throu gh VM, you’ll\nlikely make it all the way!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEABSTRACTION : ADDRESS SPACES 7\n13.5 Summary\nWe have seen the introduction of a major OS subsystem: virtual mem -\nory. The VM system is responsible for providing the illusion of a lar ge,\nsparse, private address space to programs, which hold all of the ir instruc-\ntions and data therein. The OS, with some serious hardware help, w ill\ntake each of these virtual memory references, and turn them int o physi-\ncal addresses, which can be presented to the physical memory i n order to\nfetch the desired information. The OS will do this for many proces ses at\nonce, making sure to protect programs from one another, as well as pr o-\ntect the OS. The entire approach requires a great deal of mechani sm (lots\nof low-level machinery) as well as some critical policies to work; we’ll\nstart from the bottom up, describing the critical mechanisms ﬁr st. And\nthus we proceed!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 THEABSTRACTION : ADDRESS SPACES\nReferences\n[BH70] “The Nucleus of a Multiprogramming System” by Per Brinch Hansen. C ommunica-\ntions of the ACM, 13:4, April 1970. The ﬁrst paper to suggest that the OS, or kernel, should be\na minimal and ﬂexible substrate for building customized operating systems ; this theme is revisited\nthroughout OS research history.\n[CV65] “Introduction and Overview of the Multics System” by F. J. Corba to, V . A. Vyssotsky.\nFall Joint Computer Conference, 1965. A great early Multics paper. Here is the great quote about\ntime sharing: “The impetus for time-sharing ﬁrst arose from professional p rogrammers because of their\nconstant frustration in debugging programs at batch processing installations. Thus, the original goal\nwas to time-share computers to allow simultaneous access by several persons wh ile giving to each of\nthem the illusion of having the whole machine at his disposal.”\n[DV66] “Programming Semantics for Multiprogrammed Computations” b y Jack B. Dennis,\nEarl C. Van Horn. Communications of the ACM, Volume 9, Number 3, March 1 966. An early\npaper (but not the ﬁrst) on multiprogramming.\n[L60] “Man-Computer Symbiosis” by J. C. R. Licklider. IRE Transactio ns on Human Factors in\nElectronics, HFE-1:1, March 1960. A funky paper about how computers and people are going to enter\ninto a symbiotic age; clearly well ahead of its time but a fascinating read none theless.\n[M62] “Time-Sharing Computer Systems” by J. McCarthy. Management and the Co mputer\nof the Future, MIT Press, Cambridge, MA, 1962. Probably McCarthy’s earliest recorded paper\non time sharing. In another paper [M83], he claims to have been thinking of the ide a since 1957.\nMcCarthy left the systems area and went on to become a giant in Artiﬁcial Intel ligence at Stanford,\nincluding the creation of the LISP programming language. See McCarthy’s hom e page for more info:\nhttp://www-formal.stanford.edu/jmc/\n[M+63] “A Time-Sharing Debugging System for a Small Computer” by J. McCa rthy, S. Boilen,\nE. Fredkin, J. C. R. Licklider. AFIPS ’63 (Spring), New York, NY, May 19 63.A great early example\nof a system that swapped program memory to the “drum” when the program wasn’t r unning, and then\nback into “core” memory when it was about to be run.\n[M83] “Reminiscences on the History of Time Sharing” by John McCarthy. 1983. Available:\nhttp://www-formal.stanford.edu/jmc/history/timesharing/times haring.html. A terriﬁc his-\ntorical note on where the idea of time-sharing might have come fzshortm, includ ing some doubts towards\nthose who cite Strachey’s work [S59] as the by pioneering work in this area..\n[NS07] “Valgrind: A Framework for Heavyweight Dynamic Binary Instr umentation” by N.\nNethercote, J. Seward. PLDI 2007, San Diego, California, June 2007. Valgrind is a lifesaver of a\nprogram for those who use unsafe languages like C. Read this paper to learn about its very cool binary\ninstrumentation techniques – it’s really quite impressive.\n[R+89] “Mach: A System Software kernel” by R. Rashid, D. Julin, D. Orr, R. Sanzi, R. Baron,\nA. Forin, D. Golub, M. Jones. COMPCON ’89, February 1989. Although not the ﬁrst project on\nmicrokernels per se, the Mach project at CMU was well-known and inﬂuential; it still lives today deep\nin the bowels of Mac OS X.\n[S59] “Time Sharing in Large Fast Computers” by C. Strachey. Proceed ings of the International\nConference on Information Processing, UNESCO, June 1959. One of the earliest references on time\nsharing.\n[S+03] “Improving the Reliability of Commodity Operating System s” by M. M. Swift, B. N.\nBershad, H. M. Levy. SOSP ’03. The ﬁrst paper to show how microkernel-like thinking can improve\noperating system reliability.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEABSTRACTION : ADDRESS SPACES 9\nHomework (Code)\nIn this homework, we’ll just learn about a few useful tools to examin e\nvirtual memory usage on Linux-based systems. This will only be a brief\nhint at what is possible; you’ll have to dive deeper on your own to tru ly\nbecome an expert (as always!).\nQuestions\n1. The ﬁrst Linux tool you should check out is the very simple tool free . First,\ntypeman free and read its entire manual page; it’s short, don’t worry!\n2. Now, run free , perhaps using some of the arguments that might be useful\n(e.g.,-m, to display memory totals in megabytes). How much memory is in\nyour system? How much is free? Do these numbers match your intuition?\n3. Next, create a little program that uses a certain amount of memory , called\nmemory-user.c . This program should take one command-line argument:\nthe number of megabytes of memory it will use. When run, it should allo-\ncate an array, and constantly stream through the array, touchi ng each entry.\nThe program should do this indeﬁnitely, or, perhaps, for a cer tain amount\nof time also speciﬁed at the command line.\n4. Now, while running your memory-user program, also (in a different ter-\nminal window, but on the same machine) run the free tool. How do the\nmemory usage totals change when your program is running? How about\nwhen you kill the memory-user program? Do the numbers match your ex-\npectations? Try this for different amounts of memory usage. What h appens\nwhen you use really large amounts of memory?\n5. Let’s try one more tool, known as pmap . Spend some time, and read the\npmap manual page in detail.\n6. To usepmap , you have to know the process ID of the process you’re inter-\nested in. Thus, ﬁrst run ps auxw to see a list of all processes; then, pick\nan interesting one, such as a browser. You can also use your memory-user\nprogram in this case (indeed, you can even have that program c allgetpid()\nand print out its PID for your convenience).\n7. Now run pmap on some of these processes, using various ﬂags (like -X)\nto reveal many details about the process. What do you see? How man y\ndifferent entities make up a modern address space, as opposed to our simple\nconception of code/stack/heap?\n8. Finally, let’s run pmap on your your memory-user program, with different\namounts of used memory. What do you see here? Does the output from\npmap match your expectations?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",21206
18-14. Memory API.pdf,18-14. Memory API,"14\nInterlude: Memory API\nIn this interlude, we discuss the memory allocation interfaces in U NIX\nsystems. The interfaces provided are quite simple, and hence the chapter\nis short and to the point1. The main problem we address is this:\nCRUX: HOWTOALLOCATE ANDMANAGE MEMORY\nIn U NIX/C programs, understanding how to allocate and manage\nmemory is critical in building robust and reliable software. Wh at inter-\nfaces are commonly used? What mistakes should be avoided?\n14.1 Types of Memory\nIn running a C program, there are two types of memory that are allo-\ncated. The ﬁrst is called stack memory, and allocations and deallocations\nof it are managed implicitly by the compiler for you, the programmer; for\nthis reason it is sometimes called automatic memory.\nDeclaring memory on the stack in C is easy. For example, let’s say y ou\nneed some space in a function func() for an integer, called x. To declare\nsuch a piece of memory, you just do something like this:\nvoid func() {\nint x; // declares an integer on the stack\n...\n}\nThe compiler does the rest, making sure to make space on the stack\nwhen you call into func() . When you return from the function, the com-\npiler deallocates the memory for you; thus, if you want some informat ion\nto live beyond the call invocation, you had better not leave that in forma-\ntion on the stack.\nIt is this need for long-lived memory that gets us to the second typ e\nof memory, called heap memory, where all allocations and deallocations\n1Indeed, we hope all chapters are! But this one is shorter and pointier, w e think.\n1\n2 INTERLUDE : M EMORY API\nareexplicitly handled by you, the programmer. A heavy responsibility,\nno doubt! And certainly the cause of many bugs. But if you are care ful\nand pay attention, you will use such interfaces correctly and wi thout too\nmuch trouble. Here is an example of how one might allocate an intege r\non the heap:\nvoid func() {\nint*x = (int *) malloc(sizeof(int));\n...\n}\nA couple of notes about this small code snippet. First, you might no-\ntice that both stack and heap allocation occur on this line: ﬁrst th e com-\npiler knows to make room for a pointer to an integer when it sees your\ndeclaration of said pointer ( int*x); subsequently, when the program\ncallsmalloc() , it requests space for an integer on the heap; the routine\nreturns the address of such an integer (upon success, or NULL on failure),\nwhich is then stored on the stack for use by the program.\nBecause of its explicit nature, and because of its more varied us age,\nheap memory presents more challenges to both users and systems. Thus,\nit is the focus of the remainder of our discussion.\n14.2 The malloc() Call\nThemalloc() call is quite simple: you pass it a size asking for some\nroom on the heap, and it either succeeds and gives you back a pointer to\nthe newly-allocated space, or fails and returns NULL2.\nThe manual page shows what you need to do to use malloc; type man\nmalloc at the command line and you will see:\n#include <stdlib.h>\n...\nvoid*malloc(size_t size);\nFrom this information, you can see that all you need to do is include\nthe header ﬁle stdlib.h to use malloc. In fact, you don’t really need to\neven do this, as the C library, which all C programs link with by default,\nhas the code for malloc() inside of it; adding the header just lets the\ncompiler check whether you are calling malloc() correctly (e.g., passing\nthe right number of arguments to it, of the right type).\nThe single parameter malloc() takes is of type sizetwhich sim-\nply describes how many bytes you need. However, most programmers\ndo not type in a number here directly (such as 10); indeed, it wou ld be\nconsidered poor form to do so. Instead, various routines and macros ar e\nutilized. For example, to allocate space for a double-precision ﬂ oating\npoint value, you simply do this:\ndouble*d = (double *) malloc(sizeof(double));\n2Note thatNULL in C isn’t really anything special at all, just a macro for the value zer o.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : M EMORY API 3\nTIP: W HEN INDOUBT , TRYITOUT\nIf you aren’t sure how some routine or operator you are using behaves,\nthere is no substitute for simply trying it out and making sure i t behaves\nas you expect. While reading the manual pages or other documentat ion\nis useful, how it works in practice is what matters. Write some cod e and\ntest it! That is no doubt the best way to make sure your code behave s as\nyou desire. Indeed, that is what we did to double-check the thin gs we\nwere saying about sizeof() were actually true!\nWow, that’s lot of double -ing! This invocation of malloc() uses the\nsizeof() operator to request the right amount of space; in C, this is\ngenerally thought of as a compile-time operator, meaning that the actual\nsize is known at compile time and thus a number (in this case, 8, for a\ndouble) is substituted as the argument to malloc() . For this reason,\nsizeof() is correctly thought of as an operator and not a function call\n(a function call would take place at run time).\nYou can also pass in the name of a variable (and not just a type) to\nsizeof() , but in some cases you may not get the desired results, so be\ncareful. For example, let’s look at the following code snippet:\nint*x = malloc(10 *sizeof(int));\nprintf(""%d\n"", sizeof(x));\nIn the ﬁrst line, we’ve declared space for an array of 10 integers , which\nis ﬁne and dandy. However, when we use sizeof() in the next line,\nit returns a small value, such as 4 (on 32-bit machines) or 8 (on 64 -bit\nmachines). The reason is that in this case, sizeof() thinks we are sim-\nply asking how big a pointer to an integer is, not how much memory we\nhave dynamically allocated. However, sometimes sizeof() does work\nas you might expect:\nint x[10];\nprintf(""%d\n"", sizeof(x));\nIn this case, there is enough static information for the compiler t o\nknow that 40 bytes have been allocated.\nAnother place to be careful is with strings. When declaring sp ace for a\nstring, use the following idiom: malloc(strlen(s) + 1) , which gets\nthe length of the string using the function strlen() , and adds 1 to it\nin order to make room for the end-of-string character. Using sizeof()\nmay lead to trouble here.\nYou might also notice that malloc() returns a pointer to type void .\nDoing so is just the way in C to pass back an address and let the pr o-\ngrammer decide what to do with it. The programmer further help s out\nby using what is called a cast; in our example above, the programmer\ncasts the return type of malloc() to a pointer to a double . Casting\ndoesn’t really accomplish anything, other than tell the compiler and other\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 INTERLUDE : M EMORY API\nprogrammers who might be reading your code: “yeah, I know what I’m\ndoing.” By casting the result of malloc() , the programmer is just giving\nsome reassurance; the cast is not needed for the correctness.\n14.3 The free() Call\nAs it turns out, allocating memory is the easy part of the equation;\nknowing when, how, and even if to free memory is the hard part. To f ree\nheap memory that is no longer in use, programmers simply call free() :\nint*x = malloc(10 *sizeof(int));\n...\nfree(x);\nThe routine takes one argument, a pointer returned by malloc() .\nThus, you might notice, the size of the allocated region is not passe d in\nby the user, and must be tracked by the memory-allocation librar y itself.\n14.4 Common Errors\nThere are a number of common errors that arise in the use of malloc()\nandfree() . Here are some we’ve seen over and over again in teaching\nthe undergraduate operating systems course. All of these examp les com-\npile and run with nary a peep from the compiler; while compiling a C\nprogram is necessary to build a correct C program, it is far from su fﬁ-\ncient, as you will learn (often in the hard way).\nCorrect memory management has been such a problem, in fact, that\nmany newer languages have support for automatic memory manage-\nment . In such languages, while you call something akin to malloc()\nto allocate memory (usually new or something similar to allocate a new\nobject), you never have to call something to free space; rather, agarbage\ncollector runs and ﬁgures out what memory you no longer have refer-\nences to and frees it for you.\nForgetting To Allocate Memory\nMany routines expect memory to be allocated before you call them. F or\nexample, the routine strcpy(dst, src) copies a string from a source\npointer to a destination pointer. However, if you are not careful, y ou\nmight do this:\nchar*src = ""hello"";\nchar*dst; // oops! unallocated\nstrcpy(dst, src); // segfault and die\nWhen you run this code, it will likely lead to a segmentation fault3,\nwhich is a fancy term for YOU DID SOMETHING WRONG WITH\nMEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY.\n3Although it sounds arcane, you will soon learn why such an illegal me mory access is\ncalled a segmentation fault; if that isn’t incentive to read on, what is?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : M EMORY API 5\nTIP: ITCOMPILED ORITRAN/negationslash=ITISCORRECT\nJust because a program compiled(!) or even ran once or many times cor-\nrectly does not mean the program is correct. Many events may have c on-\nspired to get you to a point where you believe it works, but then some -\nthing changes and it stops. A common student reaction is to say (or y ell)\n“But it worked before!” and then blame the compiler, operating sys tem,\nhardware, or even (dare we say it) the professor. But the problem i s usu-\nally right where you think it would be, in your code. Get to work and\ndebug it before you blame those other components.\nIn this case, the proper code might instead look like this:\nchar*src = ""hello"";\nchar*dst = (char *) malloc(strlen(src) + 1);\nstrcpy(dst, src); // work properly\nAlternately, you could use strdup() and make your life even easier.\nRead thestrdup man page for more information.\nNot Allocating Enough Memory\nA related error is not allocating enough memory, sometimes called a buffer\noverﬂow . In the example above, a common error is to make almost enough\nroom for the destination buffer.\nchar*src = ""hello"";\nchar*dst = (char *) malloc(strlen(src)); // too small!\nstrcpy(dst, src); // work properly\nOddly enough, depending on how malloc is implemented and many\nother details, this program will often run seemingly correctly. In some\ncases, when the string copy executes, it writes one byte too far p ast the\nend of the allocated space, but in some cases this is harmless, pe rhaps\noverwriting a variable that isn’t used anymore. In some cases, th ese over-\nﬂows can be incredibly harmful, and in fact are the source of many secu-\nrity vulnerabilities in systems [W06]. In other cases, the ma lloc library\nallocated a little extra space anyhow, and thus your program actu ally\ndoesn’t scribble on some other variable’s value and works quite ﬁne. In\neven other cases, the program will indeed fault and crash. And t hus we\nlearn another valuable lesson: even though it ran correctly once, doesn’t\nmean it’s correct.\nForgetting to Initialize Allocated Memory\nWith this error, you call malloc() properly, but forget to ﬁll in some val-\nues into your newly-allocated data type. Don’t do this! If you do for get,\nyour program will eventually encounter an uninitialized read , where it\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 INTERLUDE : M EMORY API\nreads from the heap some data of unknown value. Who knows what\nmight be in there? If you’re lucky, some value such that the progra m still\nworks (e.g., zero). If you’re not lucky, something random and harmfu l.\nForgetting To Free Memory\nAnother common error is known as a memory leak , and it occurs when\nyou forget to free memory. In long-running applications or systems (such\nas the OS itself), this is a huge problem, as slowly leaking memor y even-\ntually leads one to run out of memory, at which point a restart is req uired.\nThus, in general, when you are done with a chunk of memory, you should\nmake sure to free it. Note that using a garbage-collected langu age doesn’t\nhelp here: if you still have a reference to some chunk of memory, no\ngarbage collector will ever free it, and thus memory leaks remai n a prob-\nlem even in more modern languages.\nIn some cases, it may seem like not calling free() is reasonable. For\nexample, your program is short-lived, and will soon exit; in this c ase,\nwhen the process dies, the OS will clean up all of its allocated pa ges and\nthus no memory leak will take place per se. While this certainl y “works”\n(see the aside on page 7), it is probably a bad habit to develop, so be wary\nof choosing such a strategy. In the long run, one of your goals as a pro-\ngrammer is to develop good habits; one of those habits is understand ing\nhow you are managing memory, and (in languages like C), freeing t he\nblocks you have allocated. Even if you can get away with not doing so,\nit is probably good to get in the habit of freeing each and every byt e you\nexplicitly allocate.\nFreeing Memory Before You Are Done With It\nSometimes a program will free memory before it is ﬁnished using it; such\na mistake is called a dangling pointer , and it, as you can guess, is also a\nbad thing. The subsequent use can crash the program, or overwrit e valid\nmemory (e.g., you called free() , but then called malloc() again to\nallocate something else, which then recycles the errantly-fr eed memory).\nFreeing Memory Repeatedly\nPrograms also sometimes free memory more than once; this is known as\nthedouble free . The result of doing so is undeﬁned. As you can imag-\nine, the memory-allocation library might get confused and do all sorts of\nweird things; crashes are a common outcome.\nCallingfree() Incorrectly\nOne last problem we discuss is the call of free() incorrectly. After all,\nfree() expects you only to pass to it one of the pointers you received\nfrommalloc() earlier. When you pass in some other value, bad things\ncan (and do) happen. Thus, such invalid frees are dangerous and of\ncourse should also be avoided.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : M EMORY API 7\nASIDE : W HYNOMEMORY ISLEAKED ONCE YOUR PROCESS EXITS\nWhen you write a short-lived program, you might allocate some space\nusingmalloc() . The program runs and is about to complete: is there\nneed to call free() a bunch of times just before exiting? While it seems\nwrong not to, no memory will be “lost” in any real sense. The reason is\nsimple: there are really two levels of memory management in the system.\nThe ﬁrst level of memory management is performed by the OS, which\nhands out memory to processes when they run, and takes it back whe n\nprocesses exit (or otherwise die). The second level of management\niswithin each process, for example within the heap when you call\nmalloc() andfree() . Even if you fail to call free() (and thus leak\nmemory in the heap), the operating system will reclaim allthe memory of\nthe process (including those pages for code, stack, and, as relev ant here,\nheap) when the program is ﬁnished running. No matter what the s tate\nof your heap in your address space, the OS takes back all of those pag es\nwhen the process dies, thus ensuring that no memory is lost despi te the\nfact that you didn’t free it.\nThus, for short-lived programs, leaking memory often does not cause any\noperational problems (though it may be considered poor form). When\nyou write a long-running server (such as a web server or database man-\nagement system, which never exit), leaked memory is a much big ger is-\nsue, and will eventually lead to a crash when the application r uns out of\nmemory. And of course, leaking memory is an even larger issue insi de\none particular program: the operating system itself. Showing us on ce\nagain: those who write the kernel code have the toughest job of all. ..\nSummary\nAs you can see, there are lots of ways to abuse memory. Because of fre -\nquent errors with memory, a whole ecosphere of tools have developed to\nhelp ﬁnd such problems in your code. Check out both purify [HJ92] and\nvalgrind [SN05]; both are excellent at helping you locate the source of\nyour memory-related problems. Once you become accustomed to using\nthese powerful tools, you will wonder how you survived without them.\n14.5 Underlying OS Support\nYou might have noticed that we haven’t been talking about system\ncalls when discussing malloc() andfree() . The reason for this is sim-\nple: they are not system calls, but rather library calls. Thus the malloc li-\nbrary manages space within your virtual address space, but it self is built\non top of some system calls which call into the OS to ask for more mem-\nory or release some back to the system.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 INTERLUDE : M EMORY API\nOne such system call is called brk, which is used to change the loca-\ntion of the program’s break : the location of the end of the heap. It takes\none argument (the address of the new break), and thus either inc reases or\ndecreases the size of the heap based on whether the new break is l arger\nor smaller than the current break. An additional call sbrk is passed an\nincrement but otherwise serves a similar purpose.\nNote that you should never directly call either brk orsbrk . They\nare used by the memory-allocation library; if you try to use them, you\nwill likely make something go (horribly) wrong. Stick to malloc() and\nfree() instead.\nFinally, you can also obtain memory from the operating system via t he\nmmap() call. By passing in the correct arguments, mmap() can create an\nanonymous memory region within your program — a region which is not\nassociated with any particular ﬁle but rather with swap space , something\nwe’ll discuss in detail later on in virtual memory. This memory ca n then\nalso be treated like a heap and managed as such. Read the manua l page\nofmmap() for more details.\n14.6 Other Calls\nThere are a few other calls that the memory-allocation library su p-\nports. For example, calloc() allocates memory and also zeroes it be-\nfore returning; this prevents some errors where you assume that m emory\nis zeroed and forget to initialize it yourself (see the paragrap h on “unini-\ntialized reads” above). The routine realloc() can also be useful, when\nyou’ve allocated space for something (say, an array), and then nee d to\nadd something to it: realloc() makes a new larger region of memory,\ncopies the old region into it, and returns the pointer to the new re gion.\n14.7 Summary\nWe have introduced some of the APIs dealing with memory allocation.\nAs always, we have just covered the basics; more details are ava ilable\nelsewhere. Read the C book [KR88] and Stevens [SR05] (Chapter 7) f or\nmore information. For a cool modern paper on how to detect and correct\nmany of these problems automatically, see Novark et al. [N+07]; t his\npaper also contains a nice summary of common problems and some neat\nideas on how to ﬁnd and ﬁx them.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : M EMORY API 9\nReferences\n[HJ92] “Purify: Fast Detection of Memory Leaks and Access Errors” by R. Hastings, B. Joyce.\nUSENIX Winter ’92. The paper behind the cool Purify tool, now a commercial product.\n[KR88] “The C Programming Language” by Brian Kernighan, Dennis Ritchie. Pre ntice-Hall\n1988. The C book, by the developers of C. Read it once, do some programming, then re ad it again, and\nthen keep it near your desk or wherever you program.\n[N+07] “Exterminator: Automatically Correcting Memory Errors wi th High Probability” by\nG. Novark, E. D. Berger, B. G. Zorn. PLDI 2007, San Diego, Califor nia. A cool paper on ﬁnding\nand correcting memory errors automatically, and a great overview of many common e rrors in C and\nC++ programs. An extended version of this paper is available CACM (Volume 5 1, Issue 12, December\n2008).\n[SN05] “Using Valgrind to Detect Undeﬁned Value Errors with Bit-pre cision” by J. Seward, N.\nNethercote. USENIX ’05. How to use valgrind to ﬁnd certain types of errors.\n[SR05] “Advanced Programming in the U NIX Environment” by W. Richard Stevens, Stephen\nA. Rago. Addison-Wesley, 2005. We’ve said it before, we’ll say it again: read this book many times\nand use it as a reference whenever you are in doubt. The authors are always su rprised at how each time\nthey read something in this book, they learn something new, even after many year s of C programming.\n[W06] “Survey on Buffer Overﬂow Attacks and Countermeasures” by T. Werthman. Avail-\nable: www.nds.rub.de/lehre/seminar/SS06/Werthmann BufferOverﬂow.pdf. A nice survey of\nbuffer overﬂows and some of the security problems they cause. Refers to man y of the famous exploits.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 INTERLUDE : M EMORY API\nHomework (Code)\nIn this homework, you will gain some familiarity with memory allo-\ncation. First, you’ll write some buggy programs (fun!). Then, you’ll use\nsome tools to help you ﬁnd the bugs you inserted. Then, you will reali ze\nhow awesome these tools are and use them in the future, thus making\nyourself more happy and productive. The tools are the debugger (e. g.,\ngdb), and a memory-bug detector called valgrind [SN05].\nQuestions\n1. First, write a simple program called null.c that creates a pointer to an\ninteger, sets it to NULL , and then tries to dereference it. Compile this into an\nexecutable called null . What happens when you run this program?\n2. Next, compile this program with symbol information included (w ith the-g\nﬂag). Doing so let’s put more information into the executable, ena bling the\ndebugger to access more useful information about variable names and the\nlike. Run the program under the debugger by typing gdb null and then,\noncegdb is running, typing run. What does gdb show you?\n3. Finally, use the valgrind tool on this program. We’ll use the memcheck\ntool that is a part of valgrind to analyze what happens. Run this by typing\nin the following: valgrind --leak-check=yes null . What happens\nwhen you run this? Can you interpret the output from the tool?\n4. Write a simple program that allocates memory using malloc() but forgets\nto free it before exiting. What happens when this program runs? Can you\nusegdb to ﬁnd any problems with it? How about valgrind (again with\nthe--leak-check=yes ﬂag)?\n5. Write a program that creates an array of integers called data of size 100\nusingmalloc ; then, set data[100] to zero. What happens when you run\nthis program? What happens when you run this program using valgrind ?\nIs the program correct?\n6. Create a program that allocates an array of integers (as ab ove), frees them,\nand then tries to print the value of one of the elements of the arr ay. Does the\nprogram run? What happens when you use valgrind on it?\n7. Now pass a funny value to free (e.g., a pointer in the middle of t he array\nyou allocated above). What happens? Do you need tools to ﬁnd t his type of\nproblem?\n8. Try out some of the other interfaces to memory allocation. For e xample, cre-\nate a simple vector-like data structure and related routines tha t userealloc()\nto manage the vector. Use an array to store the vectors elements; when a\nuser adds an entry to the vector, use realloc() to allocate more space for\nit. How well does such a vector perform? How does it compare to a li nked\nlist? Usevalgrind to help you ﬁnd bugs.\n9. Spend more time and read about using gdb andvalgrind . Knowing your\ntools is critical; spend the time and learn how to become an exper t debugger\nin the U NIXand C environment.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",23870
19-15. Address Translation.pdf,19-15. Address Translation,"15\nMechanism: Address Translation\nIn developing the virtualization of the CPU, we focused on a genera l\nmechanism known as limited direct execution (orLDE ). The idea be-\nhind LDE is simple: for the most part, let the program run directl y on the\nhardware; however, at certain key points in time (such as when a process\nissues a system call, or a timer interrupt occurs), arrange so t hat the OS\ngets involved and makes sure the “right” thing happens. Thus, the OS,\nwith a little hardware support, tries its best to get out of the wa y of the\nrunning program, to deliver an efﬁcient virtualization; however, by inter-\nposing at those critical points in time, the OS ensures that it maintai ns\ncontrol over the hardware. Efﬁciency and control together are two of the\nmain goals of any modern operating system.\nIn virtualizing memory, we will pursue a similar strategy, at taining\nboth efﬁciency and control while providing the desired virtuali zation. Ef-\nﬁciency dictates that we make use of hardware support, which at ﬁrst\nwill be quite rudimentary (e.g., just a few registers) but wi ll grow to be\nfairly complex (e.g., TLBs, page-table support, and so forth, a s you will\nsee). Control implies that the OS ensures that no application is allowed\nto access any memory but its own; thus, to protect applications fr om one\nanother, and the OS from applications, we will need help from the h ard-\nware here too. Finally, we will need a little more from the VM syste m, in\nterms of ﬂexibility ; speciﬁcally, we’d like for programs to be able to use\ntheir address spaces in whatever way they would like, thus mak ing the\nsystem easier to program. And thus we arrive at the reﬁned crux :\nTHECRUX:\nHOWTOEFFICIENTLY ANDFLEXIBLY VIRTUALIZE MEMORY\nHow can we build an efﬁcient virtualization of memory? How do\nwe provide the ﬂexibility needed by applications? How do we main tain\ncontrol over which memory locations an application can access, and t hus\nensure that application memory accesses are properly restrict ed? How\ndo we do all of this efﬁciently?\n1\n2 MECHANISM : ADDRESS TRANSLATION\nThe generic technique we will use, which you can consider an add ition\nto our general approach of limited direct execution, is something that is\nreferred to as hardware-based address translation , or just address trans-\nlation for short. With address translation, the hardware transforms ea ch\nmemory access (e.g., an instruction fetch, load, or store), chang ing the vir-\ntual address provided by the instruction to a physical address where the\ndesired information is actually located. Thus, on each and every memory\nreference, an address translation is performed by the hardwar e to redirect\napplication memory references to their actual locations in memor y.\nOf course, the hardware alone cannot virtualize memory, as it jus t pro-\nvides the low-level mechanism for doing so efﬁciently. The OS mu st get\ninvolved at key points to set up the hardware so that the correct t rans-\nlations take place; it must thus manage memory , keeping track of which\nlocations are free and which are in use, and judiciously interve ning to\nmaintain control over how memory is used.\nOnce again the goal of all of this work is to create a beautiful illu-\nsion : that the program has its own private memory, where its own code\nand data reside. Behind that virtual reality lies the ugly ph ysical truth:\nthat many programs are actually sharing memory at the same time , as\nthe CPU (or CPUs) switches between running one program and the ne xt.\nThrough virtualization, the OS (with the hardware’s help) turn s the ugly\nmachine reality into something that is a useful, powerful, and easy to use\nabstraction.\n15.1 Assumptions\nOur ﬁrst attempts at virtualizing memory will be very simple, almost\nlaughably so. Go ahead, laugh all you want; pretty soon it will be t he OS\nlaughing at you, when you try to understand the ins and outs of TLBs ,\nmulti-level page tables, and other technical wonders. Don’t lik e the idea\nof the OS laughing at you? Well, you may be out of luck then; that’s jus t\nhow the OS rolls.\nSpeciﬁcally, we will assume for now that the user’s address space must\nbe placed contiguously in physical memory. We will also assume, for sim-\nplicity, that the size of the address space is not too big; speciﬁ cally, that\nit is less than the size of physical memory . Finally, we will also assume that\neach address space is exactly the same size . Don’t worry if these assump-\ntions sound unrealistic; we will relax them as we go, thus achiev ing a\nrealistic virtualization of memory.\n15.2 An Example\nTo understand better what we need to do to implement address t rans-\nlation, and why we need such a mechanism, let’s look at a simple exa m-\nple. Imagine there is a process whose address space is as indica ted in\nFigure 15.1. What we are going to examine here is a short code sequ ence\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : ADDRESS TRANSLATION 3\nTIP: INTERPOSITION ISPOWERFUL\nInterposition is a generic and powerful technique that is often u sed to\ngreat effect in computer systems. In virtualizing memory, the hardware\nwill interpose on each memory access, and translate each virtua l address\nissued by the process to a physical address where the desired i nforma-\ntion is actually stored. However, the general technique of inter position is\nmuch more broadly applicable; indeed, almost any well-deﬁned i nterface\ncan be interposed upon, to add new functionality or improve some othe r\naspect of the system. One of the usual beneﬁts of such an approach i s\ntransparency ; the interposition often is done without changing the client\nof the interface, thus requiring no changes to said client.\nthat loads a value from memory, increments it by three, and then s tores\nthe value back into memory. You can imagine the C-language repr esen-\ntation of this code might look like this:\nvoid func() {\nint x = 3000; // thanks, Perry.\nx = x + 3; // this is the line of code we are interested in\n...\nThe compiler turns this line of code into assembly, which might l ook\nsomething like this (in x86 assembly). Use objdump on Linux or otool\non a Mac to disassemble it:\n128: movl 0x0(%ebx), %eax ;load 0+ebx into eax\n132: addl $0x03, %eax ;add 3 to eax register\n135: movl %eax, 0x0(%ebx) ;store eax back to mem\nThis code snippet is relatively straightforward; it presumes that the\naddress of xhas been placed in the register ebx, and then loads the value\nat that address into the general-purpose register eax using themovl in-\nstruction (for “longword” move). The next instruction adds 3 to eax,\nand the ﬁnal instruction stores the value in eax back into memory at that\nsame location.\nIn Figure 15.1 (page 4), observe how both the code and data are laid\nout in the process’s address space; the three-instruction code se quence is\nlocated at address 128 (in the code section near the top), and the v alue\nof the variable xat address 15 KB (in the stack near the bottom). In the\nﬁgure, the initial value of xis 3000, as shown in its location on the stack.\nWhen these instructions run, from the perspective of the process , the\nfollowing memory accesses take place.\n•Fetch instruction at address 128\n•Execute this instruction (load from address 15 KB)\n•Fetch instruction at address 132\n•Execute this instruction (no memory reference)\n•Fetch the instruction at address 135\n•Execute this instruction (store to address 15 KB)\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 MECHANISM : ADDRESS TRANSLATION\n16KB15KB14KB4KB3KB2KB1KB0KB\nStack(free)HeapProgram Code128\n132\n135movl 0x0(%ebx),%eax\naddl 0x03, %eax\nmovl %eax,0x0(%ebx)\n3000\nFigure 15.1: A Process And Its Address Space\nFrom the program’s perspective, its address space starts at address 0\nand grows to a maximum of 16 KB; all memory references it generate s\nshould be within these bounds. However, to virtualize memory, th e OS\nwants to place the process somewhere else in physical memory, not nec-\nessarily at address 0. Thus, we have the problem: how can we relocate\nthis process in memory in a way that is transparent to the process? How\ncan we provide the illusion of a virtual address space starting a t 0, when\nin reality the address space is located at some other physical ad dress?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : ADDRESS TRANSLATION 5\n64KB48KB32KB16KB0KB\n(not in use)(not in use)Operating System\nStackCode\nHeap\n(allocated but not in use)\nRelocated Process\nFigure 15.2: Physical Memory with a Single Relocated Process\nAn example of what physical memory might look like once this pro-\ncess’s address space has been placed in memory is found in Figure 15.2.\nIn the ﬁgure, you can see the OS using the ﬁrst slot of physical mem ory\nfor itself, and that it has relocated the process from the example above\ninto the slot starting at physical memory address 32 KB. The othe r two\nslots are free (16 KB-32 KB and 48 KB-64 KB).\n15.3 Dynamic (Hardware-based) Relocation\nTo gain some understanding of hardware-based address transla tion,\nwe’ll ﬁrst discuss its ﬁrst incarnation. Introduced in the ﬁrst time-sharing\nmachines of the late 1950’s is a simple idea referred to as base and bounds ;\nthe technique is also referred to as dynamic relocation ; we’ll use both\nterms interchangeably [SS74].\nSpeciﬁcally, we’ll need two hardware registers within each CP U: one\nis called the base register, and the other the bounds (sometimes called a\nlimit register). This base-and-bounds pair is going to allow us to pla ce the\naddress space anywhere we’d like in physical memory, and do so w hile\nensuring that the process can only access its own address space.\nIn this setup, each program is written and compiled as if it is loa ded at\naddress zero. However, when a program starts running, the OS dec ides\nwhere in physical memory it should be loaded and sets the base reg ister\nto that value. In the example above, the OS decides to load the pr ocess at\nphysical address 32 KB and thus sets the base register to this value.\nInteresting things start to happen when the process is runnin g. Now,\nwhen any memory reference is generated by the process, it is translated\nby the processor in the following manner:\nphysical address = virtual address + base\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 MECHANISM : ADDRESS TRANSLATION\nASIDE : SOFTWARE -BASED RELOCATION\nIn the early days, before hardware support arose, some systems pe r-\nformed a crude form of relocation purely via software methods. The\nbasic technique is referred to as static relocation , in which a piece of soft-\nware known as the loader takes an executable that is about to be run and\nrewrites its addresses to the desired offset in physical memor y.\nFor example, if an instruction was a load from address 1000 into a r eg-\nister (e.g., movl 1000, %eax ), and the address space of the program\nwas loaded starting at address 3000 (and not 0, as the program thi nks),\nthe loader would rewrite the instruction to offset each address b y 3000\n(e.g.,movl 4000, %eax ). In this way, a simple static relocation of the\nprocess’s address space is achieved.\nHowever, static relocation has numerous problems. First and most i m-\nportantly, it does not provide protection, as processes can generat e bad\naddresses and thus illegally access other process’s or even OS me mory; in\ngeneral, hardware support is likely needed for true protection [ WL+93].\nAnother negative is that once placed, it is difﬁcult to later re locate an ad-\ndress space to another location [M65].\nEach memory reference generated by the process is a virtual address ;\nthe hardware in turn adds the contents of the base register to th is address\nand the result is a physical address that can be issued to the memory\nsystem.\nTo understand this better, let’s trace through what happens wh en a\nsingle instruction is executed. Speciﬁcally, let’s look at one ins truction\nfrom our earlier sequence:\n128: movl 0x0(%ebx), %eax\nThe program counter (PC) is set to 128; when the hardware needs t o\nfetch this instruction, it ﬁrst adds the value to the base regi ster value\nof 32 KB (32768) to get a physical address of 32896; the hardware then\nfetches the instruction from that physical address. Next, the processor\nbegins executing the instruction. At some point, the process the n issues\nthe load from virtual address 15 KB, which the processor takes and again\nadds to the base register (32 KB), getting the ﬁnal physical a ddress of\n47 KB and thus the desired contents.\nTransforming a virtual address into a physical address is exa ctly the\ntechnique we refer to as address translation ; that is, the hardware takes a\nvirtual address the process thinks it is referencing and tran sforms it into\na physical address which is where the data actually resides. Because this\nrelocation of the address happens at runtime, and because we can move\naddress spaces even after the process has started running, th e technique\nis often referred to as dynamic relocation [M65].\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : ADDRESS TRANSLATION 7\nTIP: HARDWARE -BASED DYNAMIC RELOCATION\nWith dynamic relocation, a little hardware goes a long way. Namel y, a\nbase register is used to transform virtual addresses (generated b y the pro-\ngram) into physical addresses. A bounds (orlimit ) register ensures that\nsuch addresses are within the conﬁnes of the address space. Toge ther\nthey provide a simple and efﬁcient virtualization of memory.\nNow you might be asking: what happened to that bounds (limit) reg -\nister? After all, isn’t this the base andbounds approach? Indeed, it is. As\nyou might have guessed, the bounds register is there to help wit h protec-\ntion. Speciﬁcally, the processor will ﬁrst check that the memory r eference\niswithin bounds to make sure it is legal; in the simple example above, the\nbounds register would always be set to 16 KB. If a process generat es a vir-\ntual address that is greater than the bounds, or one that is negat ive, the\nCPU will raise an exception, and the process will likely be term inated.\nThe point of the bounds is thus to make sure that all addresses gen erated\nby the process are legal and within the “bounds” of the process.\nWe should note that the base and bounds registers are hardware st ruc-\ntures kept on the chip (one pair per CPU). Sometimes people call the\npart of the processor that helps with address translation the memory\nmanagement unit (MMU) ; as we develop more sophisticated memory-\nmanagement techniques, we will be adding more circuitry to th e MMU.\nA small aside about bound registers, which can be deﬁned in one of\ntwo ways. In one way (as above), it holds the sizeof the address space,\nand thus the hardware checks the virtual address against it ﬁ rst before\nadding the base. In the second way, it holds the physical address of the\nend of the address space, and thus the hardware ﬁrst adds the ba se and\nthen makes sure the address is within bounds. Both methods are log ically\nequivalent; for simplicity, we’ll usually assume the former me thod.\nExample Translations\nTo understand address translation via base-and-bounds in more detail,\nlet’s take a look at an example. Imagine a process with an address s pace of\nsize 4 KB (yes, unrealistically small) has been loaded at phys ical address\n16 KB. Here are the results of a number of address translations:\nVirtual Address Physical Address\n0→ 16 KB\n1 KB→ 17 KB\n3000→ 19384\n4400→ Fault (out of bounds)\nAs you can see from the example, it is easy for you to simply add the\nbase address to the virtual address (which can rightly be vie wed as an\noffset into the address space) to get the resulting physical addres s. Only\nif the virtual address is “too big” or negative will the result b e a fault,\ncausing an exception to be raised.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 MECHANISM : ADDRESS TRANSLATION\nASIDE : DATA STRUCTURE — T HEFREE LIST\nThe OS must track which parts of free memory are not in use, so as to\nbe able to allocate memory to processes. Many different data str uctures\ncan of course be used for such a task; the simplest (which we will a ssume\nhere) is a free list , which simply is a list of the ranges of the physical\nmemory which are not currently in use.\n15.4 Hardware Support: A Summary\nLet us now summarize the support we need from the hardware (also\nsee Figure 15.3, page 9). First, as discussed in the chapter on CPU virtual-\nization, we require two different CPU modes. The OS runs in privileged\nmode (orkernel mode ), where it has access to the entire machine; appli-\ncations run in user mode , where they are limited in what they can do. A\nsingle bit, perhaps stored in some kind of processor status word , indi-\ncates which mode the CPU is currently running in; upon certain s pecial\noccasions (e.g., a system call or some other kind of exception or inter rupt),\nthe CPU switches modes.\nThe hardware must also provide the base and bounds registers them-\nselves; each CPU thus has an additional pair of registers, part of the mem-\nory management unit (MMU ) of the CPU. When a user program is run-\nning, the hardware will translate each address, by adding th e base value\nto the virtual address generated by the user program. The hard ware must\nalso be able to check whether the address is valid, which is ac complished\nby using the bounds register and some circuitry within the CPU.\nThe hardware should provide special instructions to modify the b ase\nand bounds registers, allowing the OS to change them when diffe rent\nprocesses run. These instructions are privileged ; only in kernel (or priv-\nileged) mode can the registers be modiﬁed. Imagine the havoc a us er\nprocess could wreak1if it could arbitrarily change the base register while\nrunning. Imagine it! And then quickly ﬂush such dark thoughts from\nyour mind, as they are the ghastly stuff of which nightmares are made.\nFinally, the CPU must be able to generate exceptions in situations\nwhere a user program tries to access memory illegally (with an a ddress\nthat is “out of bounds”); in this case, the CPU should stop executin g the\nuser program and arrange for the OS “out-of-bounds” exception handler\nto run. The OS handler can then ﬁgure out how to react, in this cas e likely\nterminating the process. Similarly, if a user program tries to c hange the\nvalues of the (privileged) base and bounds registers, the CPU s hould raise\nan exception and run the “tried to execute a privileged operati on while\nin user mode” handler. The CPU also must provide a method to inform\nit of the location of these handlers; a few more privileged instruc tions are\nthus needed.\n1Is there anything other than “havoc” that can be “wreaked”? [W17]\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : ADDRESS TRANSLATION 9\nHardware Requirements Notes\nPrivileged mode Needed to prevent user-mode processes\nfrom executing privileged operations\nBase/bounds registers Need pair of registers per CPU to support\naddress translation and bounds checks\nAbility to translate virtual addresses Circuitry to do translations and check\nand check if within bounds limits; in this case, quite simple\nPrivileged instruction(s) to OS must be able to set these values\nupdate base/bounds before letting a user program run\nPrivileged instruction(s) to register OS must be able to tell hardware what\nexception handlers code to run if exception occurs\nAbility to raise exceptions When processes try to access privileged\ninstructions or out-of-bounds memory\nFigure 15.3: Dynamic Relocation: Hardware Requirements\n15.5 Operating System Issues\nJust as the hardware provides new features to support dynamic r elo-\ncation, the OS now has new issues it must handle; the combination of\nhardware support and OS management leads to the implementati on of\na simple virtual memory. Speciﬁcally, there are a few critical junctures\nwhere the OS must get involved to implement our base-and-bounds ver-\nsion of virtual memory.\nFirst, the OS must take action when a process is created, ﬁnding space\nfor its address space in memory. Fortunately, given our assumpti ons that\neach address space is (a) smaller than the size of physical mem ory and\n(b) the same size, this is quite easy for the OS; it can simply vie w physical\nmemory as an array of slots, and track whether each one is free or in\nuse. When a new process is created, the OS will have to search a d ata\nstructure (often called a free list ) to ﬁnd room for the new address space\nand then mark it used. With variable-sized address spaces, l ife is more\ncomplicated, but we will leave that concern for future chapters .\nLet’s look at an example. In Figure 15.2 (page 5), you can see the OS\nusing the ﬁrst slot of physical memory for itself, and that it has r elocated\nthe process from the example above into the slot starting at physi cal mem-\nory address 32 KB. The other two slots are free (16 KB-32 KB and 48 K B-\n64 KB); thus, the free list should consist of these two entries.\nSecond, the OS must do some work when a process is terminated (i.e.,\nwhen it exits gracefully, or is forcefully killed because it mi sbehaved),\nreclaiming all of its memory for use in other processes or the OS. Upon\ntermination of a process, the OS thus puts its memory back on the fre e\nlist, and cleans up any associated data structures as need be.\nThird, the OS must also perform a few additional steps when a cont ext\nswitch occurs. There is only one base and bounds register pair on ea ch\nCPU, after all, and their values differ for each running progra m, as each\nprogram is loaded at a different physical address in memory. Thu s, the\nOS must save and restore the base-and-bounds pair when it switches be-\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 MECHANISM : ADDRESS TRANSLATION\nOS Requirements Notes\nMemory management Need to allocate memory for new processes;\nReclaim memory from terminated processes;\nGenerally manage memory via free list\nBase/bounds management Must set base/bounds properly upon context switch\nException handling Code to run when exceptions arise;\nlikely action is to terminate offending process\nFigure 15.4: Dynamic Relocation: Operating System Responsibilities\ntween processes. Speciﬁcally, when the OS decides to stop runni ng a pro-\ncess, it must save the values of the base and bounds registers to memory,\nin some per-process structure such as the process structure orprocess\ncontrol block (PCB). Similarly, when the OS resumes a running process\n(or runs it the ﬁrst time), it must set the values of the base and b ounds on\nthe CPU to the correct values for this process.\nWe should note that when a process is stopped (i.e., not running), i t is\npossible for the OS to move an address space from one location in mem-\nory to another rather easily. To move a process’s address space, th e OS\nﬁrst deschedules the process; then, the OS copies the address s pace from\nthe current location to the new location; ﬁnally, the OS updates t he saved\nbase register (in the process structure) to point to the new loca tion. When\nthe process is resumed, its (new) base register is restored, an d it begins\nrunning again, oblivious that its instructions and data are now i n a com-\npletely new spot in memory.\nFourth, the OS must provide exception handlers , or functions to be\ncalled, as discussed above; the OS installs these handlers at boot time (via\nprivileged instructions). For example, if a process tries to ac cess mem-\nory outside its bounds, the CPU will raise an exception; the OS mus t be\nprepared to take action when such an exception arises. The common reac-\ntion of the OS will be one of hostility: it will likely terminate the offending\nprocess. The OS should be highly protective of the machine it is ru nning,\nand thus it does not take kindly to a process trying to access memor y or\nexecute instructions that it shouldn’t. Bye bye, misbehaving p rocess; it’s\nbeen nice knowing you.\nFigure 15.5 (page 11) illustrates much of the hardware/OS int eraction\nin a timeline. The ﬁgure shows what the OS does at boot time to ready\nthe machine for use, and then what happens when a process (Proces s\nA) starts running; note how its memory translations are handled b y the\nhardware with no OS intervention. At some point, a timer interru pt oc-\ncurs, and the OS switches to Process B, which executes a “bad loa d” (to an\nillegal memory address); at that point, the OS must get involved , termi-\nnating the process and cleaning up by freeing B’s memory and remov ing\nits entry from the process table. As you can see from the diagram, w e\nare still following the basic approach of limited direct execution . In most\ncases, the OS just sets up the hardware appropriately and lets the process\nrun directly on the CPU; only when the process misbehaves does the OS\nhave to become involved.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : ADDRESS TRANSLATION 11\nOS @ boot Hardware\n(kernel mode)\ninitialize trap table\nremember addresses of...\nsystem call handler\ntimer handler\nillegal mem-access handler\nillegal instruction handler\nstart interrupt timer\nstart timer; interrupt after X ms\ninitialize process table\ninitialize free list\nOS @ run Hardware Program\n(kernel mode) (user mode)\nTo start process A:\nallocate entry in process table\nallocate memory for process\nset base/bounds registers\nreturn-from-trap (into A)\nrestore registers of A\nmove to user mode\njump to A’s (initial) PC\nProcess A runs\nFetch instruction\nTranslate virtual address\nand perform fetch\nExecute instruction\nIf explicit load/store:\nEnsure address is in-bounds;\nTranslate virtual address\nand perform load/store\n...\nTimer interrupt\nmove to kernel mode\nJump to interrupt handler\nHandle the trap\nCallswitch() routine\nsave regs(A) to proc-struct(A)\n(including base/bounds)\nrestore regs(B) from proc-struct(B)\n(including base/bounds)\nreturn-from-trap (into B)\nrestore registers of B\nmove to user mode\njump to B’s PC\nProcess B runs\nExecute bad load\nLoad is out-of-bounds;\nmove to kernel mode\njump to trap handler\nHandle the trap\nDecide to terminate process B\nde-allocate B’s memory\nfree B’s entry in process table\nFigure 15.5: Limited Direct Execution Protocol (Dynamic Relocation)\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 MECHANISM : ADDRESS TRANSLATION\n15.6 Summary\nIn this chapter, we have extended the concept of limited direct exe-\ncution with a speciﬁc mechanism used in virtual memory, known as ad-\ndress translation . With address translation, the OS can control each and\nevery memory access from a process, ensuring the accesses stay w ithin\nthe bounds of the address space. Key to the efﬁciency of this tech nique\nis hardware support, which performs the translation quickly for each ac-\ncess, turning virtual addresses (the process’s view of memory) i nto phys-\nical ones (the actual view). All of this is performed in a way that istrans-\nparent to the process that has been relocated; the process has no idea it s\nmemory references are being translated, making for a wonderful illusion.\nWe have also seen one particular form of virtualization, known as b ase\nand bounds or dynamic relocation. Base-and-bounds virtualizati on is\nquite efﬁcient , as only a little more hardware logic is required to add a\nbase register to the virtual address and check that the addre ss generated\nby the process is in bounds. Base-and-bounds also offers protection ; the\nOS and hardware combine to ensure no process can generate memory\nreferences outside its own address space. Protection is certain ly one of\nthe most important goals of the OS; without it, the OS could not control\nthe machine (if processes were free to overwrite memory, they cou ld eas-\nily do nasty things like overwrite the trap table and take over t he system).\nUnfortunately, this simple technique of dynamic relocation does have\nits inefﬁciencies. For example, as you can see in Figure 15.2 (p age 5), the\nrelocated process is using physical memory from 32 KB to 48 KB; how-\never, because the process stack and heap are not too big, all of the space\nbetween the two is simply wasted . This type of waste is usually called in-\nternal fragmentation , as the space inside the allocated unit is not all used\n(i.e., is fragmented) and thus wasted. In our current approach , although\nthere might be enough physical memory for more processes, we are cu r-\nrently restricted to placing an address space in a ﬁxed-size d slot and thus\ninternal fragmentation can arise2. Thus, we are going to need more so-\nphisticated machinery, to try to better utilize physical me mory and avoid\ninternal fragmentation. Our ﬁrst attempt will be a slight gen eralization\nof base and bounds known as segmentation , which we will discuss next.\n2A different solution might instead place a ﬁxed-sized stack within the address space,\njust below the code region, and a growing heap below that. However, thi s limits ﬂexibility\nby making recursion and deeply-nested function calls challenging, and thus i s something we\nhope to avoid.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMECHANISM : ADDRESS TRANSLATION 13\nReferences\n[M65] “On Dynamic Program Relocation” by W.C. McGee. IBM Systems Journal , Volume 4:3,\n1965, pages 184–199. This paper is a nice summary of early work on dynamic relocation, as well as\nsome basics on static relocation.\n[P90] “Relocating loader for MS-DOS .EXE executable ﬁles” by Kenneth D. A . Pillay. Micro-\nprocessors & Microsystems archive, Volume 14:7 (September 1990). An example of a relocating\nloader for MS-DOS. Not the ﬁrst one, but just a relatively modern example of how such a system works.\n[SS74] “The Protection of Information in Computer Systems” by J. Salt zer and M. Schroeder.\nCACM, July 1974. From this paper: “The concepts of base-and-bound register and hardware-interp reted\ndescriptors appeared, apparently independently, between 1957 and 195 9 on three projects with diverse\ngoals. At M.I.T., McCarthy suggested the base-and-bound idea as part of the mem ory protection system\nnecessary to make time-sharing feasible. IBM independently developed th e base-and-bound register as a\nmechanism to permit reliable multiprogramming of the Stretch (7030) comp uter system. At Burroughs,\nR. Barton suggested that hardware-interpreted descriptors would provide di rect support for the naming\nscope rules of higher level languages in the B5000 computer system.” W e found this quote on Mark\nSmotherman’s cool history pages [S04]; see them for more information.\n[S04] “System Call Support” by Mark Smotherman. May 2004. people.cs.clemson.edu/\n˜mark/syscall.html .A neat history of system call support. Smotherman has also collected some\nearly history on items like interrupts and other fun aspects of computing history . See his web pages for\nmore details.\n[WL+93] “Efﬁcient Software-based Fault Isolation” by Robert Wahbe , Steven Lucco, Thomas\nE. Anderson, Susan L. Graham. SOSP ’93. A terriﬁc paper about how you can use compiler support\nto bound memory references from a program, without hardware support. The p aper sparked renewed\ninterest in software techniques for isolation of memory references.\n[W17] Answer to footnote: “Is there anything other than havoc that can be wre aked?” by\nWaciuma Wanjohi. October 2017. Amazingly, this enterprising reader found the answer via google’s\nNgram viewing tool (available at the following URL: http://books.google.com/ngrams ).\nThe answer, thanks to Mr. Wanjohi: “It’s only since about 1970 that ’wreak havoc’ has been more\npopular than ’wreak vengeance’. In the 1800s, the word wreak was almost always f ollowed by ’his/their\nvengeance’.” Apparently, when you wreak, you are up to no good, but at least w reakers have some\noptions now.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 MECHANISM : ADDRESS TRANSLATION\nHomework (Simulation)\nThe program relocation.py allows you to see how address trans-\nlations are performed in a system with base and bounds registers . See the\nREADME for details.\nQuestions\n1. Run with seeds 1, 2, and 3, and compute whether each virtual addr ess gen-\nerated by the process is in or out of bounds. If in bounds, compute th e\ntranslation.\n2. Run with these ﬂags: -s 0 -n 10 . What value do you have set -l(the\nbounds register) to in order to ensure that all the generated vi rtual addresses\nare within bounds?\n3. Run with these ﬂags: -s 1 -n 10 -l 100 . What is the maximum value\nthat base can be set to, such that the address space still ﬁts in to physical\nmemory in its entirety?\n4. Run some of the same problems above, but with larger address spac es (-a)\nand physical memories ( -p).\n5. What fraction of randomly-generated virtual addresses are valid, as a func-\ntion of the value of the bounds register? Make a graph from runnin g with\ndifferent random seeds, with limit values ranging from 0 up to th e maxi-\nmum size of the address space.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",33395
20-16. Segmentation.pdf,20-16. Segmentation,"16\nSegmentation\nSo far we have been putting the entire address space of each proce ss in\nmemory. With the base and bounds registers, the OS can easily re locate\nprocesses to different parts of physical memory. However, you mig ht\nhave noticed something interesting about these address spaces of ours:\nthere is a big chunk of “free” space right in the middle, betwee n the stack\nand the heap.\nAs you can imagine from Figure 16.1, although the space between t he\nstack and heap is not being used by the process, it is still takin g up phys-\nical memory when we relocate the entire address space somewhere in\nphysical memory; thus, the simple approach of using a base and bou nds\nregister pair to virtualize memory is wasteful. It also makes it quite hard\nto run a program when the entire address space doesn’t ﬁt into mem ory;\nthus, base and bounds is not as ﬂexible as we would like. And thus:\nTHECRUX: HOWTOSUPPORT A L ARGE ADDRESS SPACE\nHow do we support a large address space with (potentially) a lot of\nfree space between the stack and the heap? Note that in our examp les,\nwith tiny (pretend) address spaces, the waste doesn’t seem too b ad. Imag-\nine, however, a 32-bit address space (4 GB in size); a typical p rogram will\nonly use megabytes of memory, but still would demand that the enti re\naddress space be resident in memory.\n16.1 Segmentation: Generalized Base/Bounds\nTo solve this problem, an idea was born, and it is called segmenta-\ntion. It is quite an old idea, going at least as far back as the very ear ly\n1960’s [H61, G62]. The idea is simple: instead of having just one base\nand bounds pair in our MMU, why not have a base and bounds pair per\nlogical segment of the address space? A segment is just a contiguous\nportion of the address space of a particular length, and in our canon ical\n1\n2 SEGMENTATION\n16KB15KB14KB6KB5KB4KB3KB2KB1KB0KB\nProgram Code\nHeap\n(free)\nStack\nFigure 16.1: An Address Space (Again)\naddress space, we have three logically-different segments: code, stack,\nand heap. What segmentation allows the OS to do is to place each on e\nof those segments in different parts of physical memory, and thus avoid\nﬁlling physical memory with unused virtual address space.\nLet’s look at an example. Assume we want to place the address spac e\nfrom Figure 16.1 into physical memory. With a base and bounds pai r\nper segment, we can place each segment independently in physical mem-\nory. For example, see Figure 16.2 (page 3); there you see a 64KB ph ysical\nmemory with those three segments in it (and 16KB reserved for the OS).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEGMENTATION 3\n64KB48KB32KB16KB0KB\n(not in use)(not in use)\n(not in use)Operating System\nStack\nCode\nHeap\nFigure 16.2: Placing Segments In Physical Memory\nAs you can see in the diagram, only used memory is allocated space\nin physical memory, and thus large address spaces with large a mounts of\nunused address space (which we sometimes call sparse address spaces )\ncan be accommodated.\nThe hardware structure in our MMU required to support segmenta -\ntion is just what you’d expect: in this case, a set of three base and bounds\nregister pairs. Figure 16.3 below shows the register values for the exam-\nple above; each bounds register holds the size of a segment.\nSegment Base Size\nCode 32K 2K\nHeap 34K 2K\nStack 28K 2K\nFigure 16.3: Segment Register Values\nYou can see from the ﬁgure that the code segment is placed at physi cal\naddress 32KB and has a size of 2KB and the heap segment is placed at\n34KB and also has a size of 2KB.\nLet’s do an example translation, using the address space in Fig ure 16.1.\nAssume a reference is made to virtual address 100 (which is in the code\nsegment). When the reference takes place (say, on an instruct ion fetch),\nthe hardware will add the base value to the offset into this segment (100 in\nthis case) to arrive at the desired physical address: 100 + 32 KB, or 32868.\nIt will then check that the address is within bounds (100 is les s than 2KB),\nﬁnd that it is, and issue the reference to physical memory addr ess 32868.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 SEGMENTATION\nASIDE : THESEGMENTATION FAULT\nThe term segmentation fault or violation arises from a memory acces s\non a segmented machine to an illegal address. Humorously, the te rm\npersists, even on machines with no support for segmentation at al l. Or\nnot so humorously, if you can’t ﬁgure out why your code keeps faulting.\nNow let’s look at an address in the heap, virtual address 4200 (aga in\nrefer to Figure 16.1). If we just add the virtual address 4200 to the base\nof the heap (34KB), we get a physical address of 39016, which is notthe\ncorrect physical address. What we need to ﬁrst do is extract th eoffset into\nthe heap, i.e., which byte(s) in this segment the address refers to. Because\nthe heap starts at virtual address 4KB (4096), the offset of 420 0 is actually\n4200 minus 4096, or 104. We then take this offset (104) and add it to the\nbase register physical address (34K) to get the desired resu lt: 34920.\nWhat if we tried to refer to an illegal address, such as 7KB whi ch is be-\nyond the end of the heap? You can imagine what will happen: the har d-\nware detects that the address is out of bounds, traps into the OS, l ikely\nleading to the termination of the offending process. And now you know\nthe origin of the famous term that all C programmers learn to dread : the\nsegmentation violation orsegmentation fault .\n16.2 Which Segment Are We Referring To?\nThe hardware uses segment registers during translation. How d oes it\nknow the offset into a segment, and to which segment an address r efers?\nOne common approach, sometimes referred to as an explicit approach,\nis to chop up the address space into segments based on the top few b its\nof the virtual address; this technique was used in the VAX/VMS system\n[LL82]. In our example above, we have three segments; thus we ne ed two\nbits to accomplish our task. If we use the top two bits of our 14-bit v irtual\naddress to select the segment, our virtual address looks like th is:\n13 12 11 10 9 8 7 6 5 4 3 2 1 0\nSegment Offset\nIn our example, then, if the top two bits are 00, the hardware know s\nthe virtual address is in the code segment, and thus uses the cod e base\nand bounds pair to relocate the address to the correct physical l ocation.\nIf the top two bits are 01, the hardware knows the address is in th e heap,\nand thus uses the heap base and bounds. Let’s take our example hea p\nvirtual address from above (4200) and translate it, just to mak e sure this\nis clear. The virtual address 4200, in binary form, can be seen here:\n13\n012\n111\n010\n09\n08\n07\n06\n15\n14\n03\n12\n01\n00\n0\nSegment Offset\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEGMENTATION 5\nAs you can see from the picture, the top two bits (01) tell the hard ware\nwhich segment we are referring to. The bottom 12 bits are the offset into\nthe segment: 0000 0110 1000, or hex 0x068, or 104 in decimal. Thu s, the\nhardware simply takes the ﬁrst two bits to determine which se gment reg-\nister to use, and then takes the next 12 bits as the offset into t he segment.\nBy adding the base register to the offset, the hardware arrive s at the ﬁ-\nnal physical address. Note the offset eases the bounds check too: w e can\nsimply check if the offset is less than the bounds; if not, the add ress is ille-\ngal. Thus, if base and bounds were arrays (with one entry per seg ment),\nthe hardware would be doing something like this to obtain the desi red\nphysical address:\n1// get top 2 bits of 14-bit VA\n2Segment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT\n3// now get offset\n4Offset = VirtualAddress & OFFSET_MASK\n5if (Offset >= Bounds[Segment])\n6RaiseException(PROTECTION_FAULT)\n7else\n8PhysAddr = Base[Segment] + Offset\n9Register = AccessMemory(PhysAddr)\nIn our running example, we can ﬁll in values for the constants abov e.\nSpeciﬁcally, SEGMASK would be set to 0x3000 ,SEGSHIFT to12, and\nOFFSETMASK to0xFFF .\nYou may also have noticed that when we use the top two bits, and we\nonly have three segments (code, heap, stack), one segment of the a ddress\nspace goes unused. Thus, some systems put code in the same segmen t as\nthe heap and thus use only one bit to select which segment to use [ LL82].\nThere are other ways for the hardware to determine which segmen t\na particular address is in. In the implicit approach, the hardware deter-\nmines the segment by noticing how the address was formed. If, for e x-\nample, the address was generated from the program counter (i.e. , it was\nan instruction fetch), then the address is within the code segm ent; if the\naddress is based off of the stack or base pointer, it must be in the s tack\nsegment; any other address must be in the heap.\n16.3 What About The Stack?\nThus far, we’ve left out one important component of the address space :\nthe stack. The stack has been relocated to physical address 28 KB in the di-\nagram above, but with one critical difference: it grows backwards . In phys-\nical memory, it starts at 28KB and grows back to 26KB, correspondi ng to\nvirtual addresses 16KB to 14KB; translation must proceed diff erently.\nThe ﬁrst thing we need is a little extra hardware support. Inst ead of\njust base and bounds values, the hardware also needs to know whi ch way\nthe segment grows (a bit, for example, that is set to 1 when the se gment\ngrows in the positive direction, and 0 for negative). Our updated view of\nwhat the hardware tracks is seen in Figure 16.4.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 SEGMENTATION\nSegment Base Size Grows Positive?\nCode 32K 2K 1\nHeap 34K 2K 1\nStack 28K 2K 0\nFigure 16.4: Segment Registers (With Negative-Growth Support)\nWith the hardware understanding that segments can grow in the neg-\native direction, the hardware must now translate such virtual addresses\nslightly differently. Let’s take an example stack virtual ad dress and trans-\nlate it to understand the process.\nIn this example, assume we wish to access virtual address 15K B, which\nshould map to physical address 27KB. Our virtual address, in b inary\nform, thus looks like this: 11 1100 0000 0000 (hex 0x3C00). The ha rd-\nware uses the top two bits (11) to designate the segment, but th en we are\nleft with an offset of 3KB. To obtain the correct negative offset, w e must\nsubtract the maximum segment size from 3KB: in this example, a seg-\nment can be 4KB, and thus the correct negative offset is 3KB minu s 4KB\nwhich equals -1KB. We simply add the negative offset (-1KB) to the base\n(28KB) to arrive at the correct physical address: 27KB. The bou nds check\ncan be calculated by ensuring the absolute value of the negativ e offset is\nless than the segment’s size.\n16.4 Support for Sharing\nAs support for segmentation grew, system designers soon realized that\nthey could realize new types of efﬁciencies with a little more ha rdware\nsupport. Speciﬁcally, to save memory, sometimes it is useful to share\ncertain memory segments between address spaces. In particul ar,code\nsharing is common and still in use in systems today.\nTo support sharing, we need a little extra support from the hardw are,\nin the form of protection bits . Basic support adds a few bits per segment,\nindicating whether or not a program can read or write a segment, or p er-\nhaps execute code that lies within the segment. By setting a cod e segment\nto read-only, the same code can be shared across multiple process es, with-\nout worry of harming isolation; while each process still thinks tha t it is ac-\ncessing its own private memory, the OS is secretly sharing memor y which\ncannot be modiﬁed by the process, and thus the illusion is preserv ed.\nAn example of the additional information tracked by the hardware\n(and OS) is shown in Figure 16.5. As you can see, the code segment is\nset to read and execute, and thus the same physical segment in memory\ncould be mapped into multiple virtual address spaces.\nSegment Base Size Grows Positive? Protection\nCode 32K 2K 1 Read-Execute\nHeap 34K 2K 1 Read-Write\nStack 28K 2K 0 Read-Write\nFigure 16.5: Segment Register Values (with Protection)\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEGMENTATION 7\nWith protection bits, the hardware algorithm described earlie r would\nalso have to change. In addition to checking whether a virtual address is\nwithin bounds, the hardware also has to check whether a partic ular access\nis permissible. If a user process tries to write to a read-only s egment, or\nexecute from a non-executable segment, the hardware should rai se an\nexception, and thus let the OS deal with the offending process.\n16.5 Fine-grained vs. Coarse-grained Segmentation\nMost of our examples thus far have focused on systems with just a\nfew segments (i.e., code, stack, heap); we can think of this seg mentation\nascoarse-grained , as it chops up the address space into relatively large,\ncoarse chunks. However, some early systems (e.g., Multics [CV6 5,DD68])\nwere more ﬂexible and allowed for address spaces to consist of a lar ge\nnumber of smaller segments, referred to as ﬁne-grained segmentation.\nSupporting many segments requires even further hardware supp ort,\nwith a segment table of some kind stored in memory. Such segment ta-\nbles usually support the creation of a very large number of segmen ts, and\nthus enable a system to use segments in more ﬂexible ways than w e have\nthus far discussed. For example, early machines like the Burr oughs B5000\nhad support for thousands of segments, and expected a compiler to c hop\ncode and data into separate segments which the OS and hardware would\nthen support [RK68]. The thinking at the time was that by havin g ﬁne-\ngrained segments, the OS could better learn about which segmen ts are in\nuse and which are not and thus utilize main memory more effective ly.\n16.6 OS Support\nYou now should have a basic idea as to how segmentation works.\nPieces of the address space are relocated into physical memory a s the\nsystem runs, and thus a huge savings of physical memory is achie ved\nrelative to our simpler approach with just a single base/bounds pair for\nthe entire address space. Speciﬁcally, all the unused space b etween the\nstack and the heap need not be allocated in physical memory, allow ing\nus to ﬁt more address spaces into physical memory.\nHowever, segmentation raises a number of new issues. We’ll ﬁrst d e-\nscribe the new OS issues that must be addressed. The ﬁrst is an old one:\nwhat should the OS do on a context switch? You should have a good\nguess by now: the segment registers must be saved and restored. Clearly,\neach process has its own virtual address space, and the OS must m ake\nsure to set up these registers correctly before letting the proc ess run again.\nThe second, and more important, issue is managing free space in p hys-\nical memory. When a new address space is created, the OS has to b e\nable to ﬁnd space in physical memory for its segments. Previousl y, we\nassumed that each address space was the same size, and thus ph ysical\nmemory could be thought of as a bunch of slots where processes would\nﬁt in. Now, we have a number of segments per process, and each segm ent\nmight be a different size.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 SEGMENTATION\n64KB56KB48KB40KB32KB24KB16KB8KB0KB\nOperating SystemNot Compacted\n(not in use)\n(not in use)\n(not in use)Allocated\nAllocated\nAllocated\n64KB56KB48KB40KB32KB24KB16KB8KB0KB\n(not in use)AllocatedOperating SystemCompacted\nFigure 16.6: Non-compacted and Compacted Memory\nThe general problem that arises is that physical memory quickl y be-\ncomes full of little holes of free space, making it difﬁcult to all ocate new\nsegments, or to grow existing ones. We call this problem external frag-\nmentation [R69]; see Figure 16.6 (left).\nIn the example, a process comes along and wishes to allocate a 20KB\nsegment. In that example, there is 24KB free, but not in one conti guous\nsegment (rather, in three non-contiguous chunks). Thus, the OS cannot\nsatisfy the 20KB request.\nOne solution to this problem would be to compact physical memory\nby rearranging the existing segments. For example, the OS coul d stop\nwhichever processes are running, copy their data to one contiguou s re-\ngion of memory, change their segment register values to point to t he\nnew physical locations, and thus have a large free extent of memor y with\nwhich to work. By doing so, the OS enables the new allocation reques t\nto succeed. However, compaction is expensive, as copying segmen ts is\nmemory-intensive and generally uses a fair amount of processor ti me.\nSee Figure 16.6 (right) for a diagram of compacted physical memory .\nA simpler approach is to use a free-list management algorithm t hat\ntries to keep large extents of memory available for allocation. Th ere are\nliterally hundreds of approaches that people have taken, inclu ding clas-\nsic algorithms like best-ﬁt (which keeps a list of free spaces and returns\nthe one closest in size that satisﬁes the desired allocation to th e requester),\nworst-ﬁt ,ﬁrst-ﬁt , and more complex schemes like buddy algorithm [K68].\nAn excellent survey by Wilson et al. is a good place to start if you w ant to\nlearn more about such algorithms [W+95], or you can wait until we cov er\nsome of the basics ourselves in a later chapter. Unfortunately, t hough, no\nmatter how smart the algorithm, external fragmentation will st ill exist;\nthus, a good algorithm simply attempts to minimize it.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEGMENTATION 9\nTIP: IF1000 S OLUTIONS EXIST, NOGREAT ONEDOES\nThe fact that so many different algorithms exist to try to mini mize exter-\nnal fragmentation is indicative of a stronger underlying truth : there is no\none “best” way to solve the problem. Thus, we settle for something r ea-\nsonable and hope it is good enough. The only real solution (as we will\nsee in forthcoming chapters) is to avoid the problem altogether, b y never\nallocating memory in variable-sized chunks.\n16.7 Summary\nSegmentation solves a number of problems, and helps us build a more\neffective virtualization of memory. Beyond just dynamic relocat ion, seg-\nmentation can better support sparse address spaces, by avoidin g the huge\npotential waste of memory between logical segments of the address space.\nIt is also fast, as doing the arithmetic segmentation requires is easy and\nwell-suited to hardware; the overheads of translation are mini mal. A\nfringe beneﬁt arises too: code sharing. If code is placed within a sepa-\nrate segment, such a segment could potentially be shared across multiple\nrunning programs.\nHowever, as we learned, allocating variable-sized segments i n mem-\nory leads to some problems that we’d like to overcome. The ﬁrst, as di s-\ncussed above, is external fragmentation. Because segments ar e variable-\nsized, free memory gets chopped up into odd-sized pieces, and th us sat-\nisfying a memory-allocation request can be difﬁcult. One can tr y to use\nsmart algorithms [W+95] or periodically compact memory, but the p rob-\nlem is fundamental and hard to avoid.\nThe second and perhaps more important problem is that segmentati on\nstill isn’t ﬂexible enough to support our fully generalized, spa rse address\nspace. For example, if we have a large but sparsely-used heap a ll in one\nlogical segment, the entire heap must still reside in memory in order to be\naccessed. In other words, if our model of how the address space is bei ng\nused doesn’t exactly match how the underlying segmentation has b een\ndesigned to support it, segmentation doesn’t work very well. We th us\nneed to ﬁnd some new solutions. Ready to ﬁnd them?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 SEGMENTATION\nReferences\n[CV65] “Introduction and Overview of the Multics System” by F. J. Corba to, V . A. Vyssotsky.\nFall Joint Computer Conference, 1965. One of ﬁve papers presented on Multics at the Fall Joint\nComputer Conference; oh to be a ﬂy on the wall in that room that day!\n[DD68] “Virtual Memory, Processes, and Sharing in Multics” by Robert C . Daley and Jack B.\nDennis. Communications of the ACM, Volume 11:5, May 1968. An early paper on how to perform\ndynamic linking in Multics, which was way ahead of its time. Dynamic linkin g ﬁnally found its way\nback into systems about 20 years later, as the large X-windows libraries deman ded it. Some say that\nthese large X11 libraries were MIT’s revenge for removing support for dynamic linking in early versions\nofUNIX!\n[G62] “Fact Segmentation” by M. N. Greenﬁeld. Proceedings of the SJCC, Vo lume 21, May\n1962. Another early paper on segmentation; so early that it has no references to other work.\n[H61] “Program Organization and Record Keeping for Dynamic Storage” b y A. W. Holt. Com-\nmunications of the ACM, Volume 4:10, October 1961. An incredibly early and difﬁcult to read paper\nabout segmentation and some of its uses.\n[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals” by Intel. 2009. Avail-\nable: http://www.intel.com/products/processor/manuals. Try reading about segmentation in\nhere (Chapter 3 in Volume 3a); it’ll hurt your head, at least a little bit.\n[K68] “The Art of Computer Programming: Volume I” by Donald Knuth. Add ison-Wesley,\n1968. Knuth is famous not only for his early books on the Art of Computer Programming but for his\ntypesetting system TeX which is still a powerhouse typesetting tool used by professionals today, and\nindeed to typeset this very book. His tomes on algorithms are a great early refe rence to many of the\nalgorithms that underly computing systems today.\n[L83] “Hints for Computer Systems Design” by Butler Lampson. ACM Op erating Systems\nReview, 15:5, October 1983. A treasure-trove of sage advice on how to build systems. Hard to read in\none sitting; take it in a little at a time, like a ﬁne wine, or a reference manual .\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System” by Henry M. Levy,\nPeter H. Lipman. IEEE Computer, Volume 15:3, March 1982. A classic memory management\nsystem, with lots of common sense in its design. We’ll study it in more detai l in a later chapter.\n[RK68] “Dynamic Storage Allocation Systems” by B. Randell and C.J. Kuehner. Communica-\ntions of the ACM, Volume 11:5, May 1968. A nice overview of the differences between paging and\nsegmentation, with some historical discussion of various machines.\n[R69] “A note on storage fragmentation and program segmentation” by Brian Randell. Com-\nmunications of the ACM, Volume 12:7, July 1969. One of the earliest papers to discuss fragmenta-\ntion.\n[W+95] “Dynamic Storage Allocation: A Survey and Critical Review” by Paul R. Wilson, Mark\nS. Johnstone, Michael Neely, David Boles. International Workshop on Memo ry Management,\nScotland, UK, September 1995. A great survey paper on memory allocators.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEGMENTATION 11\nHomework (Simulation)\nThis program allows you to see how address translations are perform ed\nin a system with segmentation. See the README for details.\nQuestions\n1. First let’s use a tiny address space to translate some addres ses. Here’s a sim-\nple set of parameters with a few different random seeds; can yo u translate\nthe addresses?\nsegmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 0\nsegmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 1\nsegmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 2\n2. Now, let’s see if we understand this tiny address space we’v e constructed\n(using the parameters from the question above). What is the high est legal\nvirtual address in segment 0? What about the lowest legal virtual address in\nsegment 1? What are the lowest and highest illegal addresses in this entire\naddress space? Finally, how would you run segmentation.py with the\n-Aﬂag to test if you are right?\n3. Let’s say we have a tiny 16-byte address space in a 128-byte physical mem-\nory. What base and bounds would you set up so as to get the simulator t o\ngenerate the following translation results for the speciﬁed address stream:\nvalid, valid, violation, ..., violation, valid, valid? Assume the following pa-\nrameters:\nsegmentation.py -a 16 -p 128\n-A 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\n--b0 ? --l0 ? --b1 ? --l1 ?\n4. Assume we want to generate a problem where roughly 90% of the ran domly-\ngenerated virtual addresses are valid (not segmentation viol ations). How\nshould you conﬁgure the simulator to do so? Which parameters are imp or-\ntant to getting this outcome?\n5. Can you run the simulator such that no virtual addresses are vali d? How?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",24937
21-17. Free Space Management.pdf,21-17. Free Space Management,"17\nFree-Space Management\nIn this chapter, we take a small detour from our discussion of virtu al-\nizing memory to discuss a fundamental aspect of any memory manag e-\nment system, whether it be a malloc library (managing pages of a pro-\ncess’s heap) or the OS itself (managing portions of the address spa ce of a\nprocess). Speciﬁcally, we will discuss the issues surrounding free-space\nmanagement .\nLet us make the problem more speciﬁc. Managing free space can ce r-\ntainly be easy, as we will see when we discuss the concept of paging . It is\neasy when the space you are managing is divided into ﬁxed-size d units;\nin such a case, you just keep a list of these ﬁxed-sized units; wh en a client\nrequests one of them, return the ﬁrst entry.\nWhere free-space management becomes more difﬁcult (and inter est-\ning) is when the free space you are managing consists of variable -sized\nunits; this arises in a user-level memory-allocation library ( as inmalloc()\nandfree() ) and in an OS managing physical memory when using seg-\nmentation to implement virtual memory. In either case, the problem that\nexists is known as external fragmentation : the free space gets chopped\ninto little pieces of different sizes and is thus fragmented; subsequent re-\nquests may fail because there is no single contiguous space tha t can sat-\nisfy the request, even though the total amount of free space excee ds the\nsize of the request.\nfree used free\n0 10 20 30\nThe ﬁgure shows an example of this problem. In this case, the total\nfree space available is 20 bytes; unfortunately, it is fragme nted into two\nchunks of size 10 each. As a result, a request for 15 bytes will fa il even\nthough there are 20 bytes free. And thus we arrive at the problem ad-\ndressed in this chapter.\n1\n2 FREE-SPACE MANAGEMENT\nCRUX: HOWTOMANAGE FREE SPACE\nHow should free space be managed, when satisfying variable-si zed re-\nquests? What strategies can be used to minimize fragmentati on? What\nare the time and space overheads of alternate approaches?\n17.1 Assumptions\nMost of this discussion will focus on the great history of allocators\nfound in user-level memory-allocation libraries. We draw on Wils on’s\nexcellent survey [W+95] but encourage interested readers to go to the\nsource document itself for more details1.\nWe assume a basic interface such as that provided by malloc() and\nfree() . Speciﬁcally, void*malloc(size t size) takes a single pa-\nrameter,size , which is the number of bytes requested by the applica-\ntion; it hands back a pointer (of no particular type, or a void pointer in\nC lingo) to a region of that size (or greater). The complementary rou tine\nvoid free(void *ptr) takes a pointer and frees the corresponding\nchunk. Note the implication of the interface: the user, when fre eing the\nspace, does not inform the library of its size; thus, the library m ust be able\nto ﬁgure out how big a chunk of memory is when handed just a pointer\nto it. We’ll discuss how to do this a bit later on in the chapter.\nThe space that this library manages is known historically as th eheap ,\nand the generic data structure used to manage free space in th e heap is\nsome kind of free list . This structure contains references to all of the free\nchunks of space in the managed region of memory. Of course, this dat a\nstructure need not be a list per se , but just some kind of data structure to\ntrack free space.\nWe further assume that primarily we are concerned with external frag-\nmentation , as described above. Allocators could of course also have the\nproblem of internal fragmentation ; if an allocator hands out chunks of\nmemory bigger than that requested, any unasked for (and thus un used)\nspace in such a chunk is considered internal fragmentation (because the\nwaste occurs inside the allocated unit) and is another example of space\nwaste. However, for the sake of simplicity, and because it is the more in-\nteresting of the two types of fragmentation, we’ll mostly focus on ex ternal\nfragmentation.\nWe’ll also assume that once memory is handed out to a client, it can not\nbe relocated to another location in memory. For example, if a program\ncallsmalloc() and is given a pointer to some space within the heap,\nthat memory region is essentially “owned” by the program (and can not\nbe moved by the library) until the program returns it via a corres pond-\ning call to free() . Thus, no compaction of free space is possible, which\n1It is nearly 80 pages long; thus, you really have to be interested!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nFREE-SPACE MANAGEMENT 3\nwould be useful to combat fragmentation2. Compaction could, however,\nbe used in the OS to deal with fragmentation when implementing seg-\nmentation (as discussed in said chapter on segmentation).\nFinally, we’ll assume that the allocator manages a contiguous reg ion\nof bytes. In some cases, an allocator could ask for that region to grow;\nfor example, a user-level memory-allocation library might call into the\nkernel to grow the heap (via a system call such as sbrk ) when it runs out\nof space. However, for simplicity, we’ll just assume that the reg ion is a\nsingle ﬁxed size throughout its life.\n17.2 Low-level Mechanisms\nBefore delving into some policy details, we’ll ﬁrst cover some com-\nmon mechanisms used in most allocators. First, we’ll discuss the b asics of\nsplitting and coalescing, common techniques in most any allocator . Sec-\nond, we’ll show how one can track the size of allocated regions quickly\nand with relative ease. Finally, we’ll discuss how to build a si mple list\ninside the free space to keep track of what is free and what isn’t .\nSplitting and Coalescing\nA free list contains a set of elements that describe the free spa ce still re-\nmaining in the heap. Thus, assume the following 30-byte heap:\nfree used free\n0 10 20 30\nThe free list for this heap would have two elements on it. One entr y de-\nscribes the ﬁrst 10-byte free segment (bytes 0-9), and one ent ry describes\nthe other free segment (bytes 20-29):\nheadaddr:0\nlen:10addr:20\nlen:10NULL\nAs described above, a request for anything greater than 10 byte s will\nfail (returning NULL); there just isn’t a single contiguous chu nk of mem-\nory of that size available. A request for exactly that size (10 by tes) could\nbe satisﬁed easily by either of the free chunks. But what happe ns if the\nrequest is for something smaller than 10 bytes?\nAssume we have a request for just a single byte of memory. In this\ncase, the allocator will perform an action known as splitting : it will ﬁnd\n2Once you hand a pointer to a chunk of memory to a C program, it is generally d ifﬁcult\nto determine all references (pointers) to that region, which may be stor ed in other variables\nor even in registers at a given point in execution. This may not be the ca se in more strongly-\ntyped, garbage-collected languages, which would thus enable compacti on as a technique to\ncombat fragmentation.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 FREE-SPACE MANAGEMENT\na free chunk of memory that can satisfy the request and split it i nto two.\nThe ﬁrst chunk it will return to the caller; the second chunk wi ll remain\non the list. Thus, in our example above, if a request for 1 byte were made,\nand the allocator decided to use the second of the two elements on th e list\nto satisfy the request, the call to malloc() would return 20 (th e address of\nthe 1-byte allocated region) and the list would end up looking like this:\nheadaddr:0\nlen:10addr:21\nlen:9NULL\nIn the picture, you can see the list basically stays intact; th e only change\nis that the free region now starts at 21 instead of 20, and the leng th of that\nfree region is now just 93. Thus, the split is commonly used in allocators\nwhen requests are smaller than the size of any particular free chunk.\nA corollary mechanism found in many allocators is known as coalesc-\ningof free space. Take our example from above once more (free 10 bytes,\nused 10 bytes, and another free 10 bytes).\nGiven this (tiny) heap, what happens when an application call s free(10),\nthus returning the space in the middle of the heap? If we simply add this\nfree space back into our list without too much thinking, we might end up\nwith a list that looks like this:\nheadaddr:10\nlen:10addr:0\nlen:10addr:20\nlen:10NULL\nNote the problem: while the entire heap is now free, it is seeming ly\ndivided into three chunks of 10 bytes each. Thus, if a user requ ests 20\nbytes, a simple list traversal will not ﬁnd such a free chunk, a nd return\nfailure.\nWhat allocators do in order to avoid this problem is coalesce free sp ace\nwhen a chunk of memory is freed. The idea is simple: when returni ng a\nfree chunk in memory, look carefully at the addresses of the chunk you\nare returning as well as the nearby chunks of free space; if the newly-\nfreed space sits right next to one (or two, as in this example) exi sting free\nchunks, merge them into a single larger free chunk. Thus, wit h coalesc-\ning, our ﬁnal list should look like this:\nheadaddr:0\nlen:30NULL\nIndeed, this is what the heap list looked like at ﬁrst, before any allo-\ncations were made. With coalescing, an allocator can better ensu re that\nlarge free extents are available for the application.\n3This discussion assumes that there are no headers, an unrealistic but simplifying assump-\ntion we make for now.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nFREE-SPACE MANAGEMENT 5\nptrThe header used by malloc library\nThe 20 bytes returned to caller\nFigure 17.1: An Allocated Region Plus Header\nsize: 20\nmagic: 1234567hptr\nptr\nThe 20 bytes returned to caller\nFigure 17.2: Speciﬁc Contents Of The Header\nTracking The Size Of Allocated Regions\nYou might have noticed that the interface to free(void *ptr) does\nnot take a size parameter; thus it is assumed that given a pointe r, the\nmalloc library can quickly determine the size of the region of mem ory\nbeing freed and thus incorporate the space back into the free li st.\nTo accomplish this task, most allocators store a little bit of extra infor-\nmation in a header block which is kept in memory, usually just before\nthe handed-out chunk of memory. Let’s look at an example again (Fig-\nure 17.1). In this example, we are examining an allocated block of size 20\nbytes, pointed to by ptr; imagine the user called malloc() and stored\nthe results in ptr, e.g.,ptr = malloc(20); .\nThe header minimally contains the size of the allocated region (i n this\ncase, 20); it may also contain additional pointers to speed up de alloca-\ntion, a magic number to provide additional integrity checking, and other\ninformation. Let’s assume a simple header which contains the siz e of the\nregion and a magic number, like this:\ntypedef struct __header_t {\nint size;\nint magic;\n} header_t;\nThe example above would look like what you see in Figure 17.2. When\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 FREE-SPACE MANAGEMENT\nthe user calls free(ptr) , the library then uses simple pointer arithmetic\nto ﬁgure out where the header begins:\nvoid free(void *ptr) {\nheader_t *hptr = (void *)ptr - sizeof(header_t);\n...\nAfter obtaining such a pointer to the header, the library can ea sily de-\ntermine whether the magic number matches the expected value as a san-\nity check ( assert(hptr->magic == 1234567) ) and calculate the to-\ntal size of the newly-freed region via simple math (i.e., addin g the size of\nthe header to size of the region). Note the small but critical det ail in the\nlast sentence: the size of the free region is the size of the heade r plus the\nsize of the space allocated to the user. Thus, when a user reques tsNbytes\nof memory, the library does not search for a free chunk of size N; rather,\nit searches for a free chunk of size Nplus the size of the header.\nEmbedding A Free List\nThus far we have treated our simple free list as a conceptual ent ity; it is\njust a list describing the free chunks of memory in the heap. But how do\nwe build such a list inside the free space itself?\nIn a more typical list, when allocating a new node, you would just ca ll\nmalloc() when you need space for the node. Unfortunately, within the\nmemory-allocation library, you can’t do this! Instead, you need to build\nthe list inside the free space itself. Don’t worry if this sounds a little weird;\nit is, but not so weird that you can’t do it!\nAssume we have a 4096-byte chunk of memory to manage (i.e., the\nheap is 4KB). To manage this as a free list, we ﬁrst have to init ialize said\nlist; initially, the list should have one entry, of size 4096 (mi nus the header\nsize). Here is the description of a node of the list:\ntypedef struct __node_t {\nint size;\nstruct __node_t *next;\n} node_t;\nNow let’s look at some code that initializes the heap and puts the ﬁrs t\nelement of the free list inside that space. We are assuming tha t the heap is\nbuilt within some free space acquired via a call to the system c allmmap() ;\nthis is not the only way to build such a heap but serves us well in t his\nexample. Here is the code:\n// mmap() returns a pointer to a chunk of free space\nnode_t*head = mmap(NULL, 4096, PROT_READ|PROT_WRITE,\nMAP_ANON|MAP_PRIVATE, -1, 0);\nhead->size = 4096 - sizeof(node_t);\nhead->next = NULL;\nAfter running this code, the status of the list is that it has a si ngle entry,\nof size 4088. Yes, this is a tiny heap, but it serves as a ﬁne exam ple for us\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nFREE-SPACE MANAGEMENT 7\nsize: 4088\nnext: 0\n...head [virtual address: 16KB]\nheader: size field\nheader: next field (NULL is 0)\nthe rest of the 4KB chunk\nFigure 17.3: A Heap With One Free Chunk\nsize: 100\nmagic: 1234567\n. . .\nsize: 3980\nnext: 0\n. . .ptr[virtual address: 16KB]\nheadThe 100 bytes now allocated\nThe free 3980 byte chunk\nFigure 17.4: A Heap: After One Allocation\nhere. Thehead pointer contains the beginning address of this range; let’s\nassume it is 16KB (though any virtual address would be ﬁne). Vis ually,\nthe heap thus looks like what you see in Figure 17.3.\nNow, let’s imagine that a chunk of memory is requested, say of size\n100 bytes. To service this request, the library will ﬁrst ﬁnd a chunk that is\nlarge enough to accommodate the request; because there is only one free\nchunk (size: 4088), this chunk will be chosen. Then, the chunk will be\nsplit into two: one chunk big enough to service the request (and header ,\nas described above), and the remaining free chunk. Assuming a n 8-byte\nheader (an integer size and an integer magic number), the spa ce in the\nheap now looks like what you see in Figure 17.4.\nThus, upon the request for 100 bytes, the library allocated 108 b ytes\nout of the existing one free chunk, returns a pointer (marked ptr in the\nﬁgure above) to it, stashes the header information immediately before the\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 FREE-SPACE MANAGEMENT\nsize: 100\nmagic: 1234567\n. . .\nsize: 100\nmagic: 1234567\n. . .\nsize: 100\nmagic: 1234567\n. . .\nsize: 3764\nnext: 0\n. . .sptr[virtual address: 16KB]\nhead100 bytes still allocated\n100 bytes still allocated\n (but about to be freed)\n100-bytes still allocated\nThe free 3764-byte chunk\nFigure 17.5: Free Space With Three Chunks Allocated\nallocated space for later use upon free() , and shrinks the one free node\nin the list to 3980 bytes (4088 minus 108).\nNow let’s look at the heap when there are three allocated regions, ea ch\nof 100 bytes (or 108 including the header). A visualization of thi s heap is\nshown in Figure 17.5.\nAs you can see therein, the ﬁrst 324 bytes of the heap are now allo-\ncated, and thus we see three headers in that space as well as th ree 100-\nbyte regions being used by the calling program. The free list re mains\nuninteresting: just a single node (pointed to by head ), but now only 3764\nbytes in size after the three splits. But what happens when th e calling\nprogram returns some memory via free() ?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nFREE-SPACE MANAGEMENT 9\nsize: 100\nmagic: 1234567\n. . .\nsize: 100\nnext: 16708\n. . .\nsize: 100\nmagic: 1234567\n. . .\nsize: 3764\nnext: 0\n. . .[virtual address: 16KB]\nhead\nsptr100 bytes still allocated\n(now a free chunk of memory)\n100-bytes still allocated\nThe free 3764-byte chunk\nFigure 17.6: Free Space With Two Chunks Allocated\nIn this example, the application returns the middle chunk of al located\nmemory, by calling free(16500) (the value 16500 is arrived upon by\nadding the start of the memory region, 16384, to the 108 of the prev ious\nchunk and the 8 bytes of the header for this chunk). This value is shown\nin the previous diagram by the pointer sptr .\nThe library immediately ﬁgures out the size of the free region, a nd\nthen adds the free chunk back onto the free list. Assuming we in sert at\nthe head of the free list, the space now looks like this (Figure 17. 6).\nAnd now we have a list that starts with a small free chunk (100 by tes,\npointed to by the head of the list) and a large free chunk (3764 by tes).\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 FREE-SPACE MANAGEMENT\nsize: 100\nnext: 16492\n. . .\nsize: 100\nnext: 16708\n. . .\nsize: 100\nnext: 16384\n. . .\nsize: 3764\nnext: 0\n. . .[virtual address: 16KB]\nhead(now free)\n(now free)\n(now free)\nThe free 3764-byte chunk\nFigure 17.7: A Non-Coalesced Free List\nOur list ﬁnally has more than one element on it! And yes, the free s pace\nis fragmented, an unfortunate but common occurrence.\nOne last example: let’s assume now that the last two in-use chun ks are\nfreed. Without coalescing, you might end up with a free list that is highly\nfragmented (see Figure 17.7).\nAs you can see from the ﬁgure, we now have a big mess! Why? Simple,\nwe forgot to coalesce the list. Although all of the memory is free, it is\nchopped up into pieces, thus appearing as a fragmented memory d espite\nnot being one. The solution is simple: go through the list and merge\nneighboring chunks; when ﬁnished, the heap will be whole again .\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nFREE-SPACE MANAGEMENT 11\nGrowing The Heap\nWe should discuss one last mechanism found within many allocation li-\nbraries. Speciﬁcally, what should you do if the heap runs out of spa ce?\nThe simplest approach is just to fail. In some cases this is the on ly option,\nand thus returning NULL is an honorable approach. Don’t feel bad! Y ou\ntried, and though you failed, you fought the good ﬁght.\nMost traditional allocators start with a small-sized heap and th en re-\nquest more memory from the OS when they run out. Typically, this me ans\nthey make some kind of system call (e.g., sbrk in most U NIXsystems) to\ngrow the heap, and then allocate the new chunks from there. To ser vice\nthesbrk request, the OS ﬁnds free physical pages, maps them into the\naddress space of the requesting process, and then returns the v alue of\nthe end of the new heap; at that point, a larger heap is available , and the\nrequest can be successfully serviced.\n17.3 Basic Strategies\nNow that we have some machinery under our belt, let’s go over some\nbasic strategies for managing free space. These approaches ar e mostly\nbased on pretty simple policies that you could think up yourself; t ry it\nbefore reading and see if you come up with all of the alternatives ( or\nmaybe some new ones!).\nThe ideal allocator is both fast and minimizes fragmentation. Un fortu-\nnately, because the stream of allocation and free requests can b e arbitrary\n(after all, they are determined by the programmer), any parti cular strat-\negy can do quite badly given the wrong set of inputs. Thus, we wil l not\ndescribe a “best” approach, but rather talk about some basics an d discuss\ntheir pros and cons.\nBest Fit\nThe best ﬁt strategy is quite simple: ﬁrst, search through the free list a nd\nﬁnd chunks of free memory that are as big or bigger than the reques ted\nsize. Then, return the one that is the smallest in that group of ca ndidates;\nthis is the so called best-ﬁt chunk (it could be called smalles t ﬁt too). One\npass through the free list is enough to ﬁnd the correct block to ret urn.\nThe intuition behind best ﬁt is simple: by returning a block tha t is close\nto what the user asks, best ﬁt tries to reduce wasted space. How ever, there\nis a cost; naive implementations pay a heavy performance penalt y when\nperforming an exhaustive search for the correct free block.\nWorst Fit\nThe worst ﬁt approach is the opposite of best ﬁt; ﬁnd the largest chunk\nand return the requested amount; keep the remaining (large) c hunk on\nthe free list. Worst ﬁt tries to thus leave big chunks free inst ead of lots of\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 FREE-SPACE MANAGEMENT\nsmall chunks that can arise from a best-ﬁt approach. Once again , how-\never, a full search of free space is required, and thus this app roach can be\ncostly. Worse, most studies show that it performs badly, leading t o excess\nfragmentation while still having high overheads.\nFirst Fit\nThe ﬁrst ﬁt method simply ﬁnds the ﬁrst block that is big enough and\nreturns the requested amount to the user. As before, the remain ing free\nspace is kept free for subsequent requests.\nFirst ﬁt has the advantage of speed — no exhaustive search of all the\nfree spaces are necessary — but sometimes pollutes the beginni ng of the\nfree list with small objects. Thus, how the allocator manages the free list’s\norder becomes an issue. One approach is to use address-based ordering ;\nby keeping the list ordered by the address of the free space, coal escing\nbecomes easier, and fragmentation tends to be reduced.\nNext Fit\nInstead of always beginning the ﬁrst-ﬁt search at the beginni ng of the list,\nthenext ﬁt algorithm keeps an extra pointer to the location within the\nlist where one was looking last. The idea is to spread the searche s for\nfree space throughout the list more uniformly, thus avoiding spli ntering\nof the beginning of the list. The performance of such an approach is quite\nsimilar to ﬁrst ﬁt, as an exhaustive search is once again avoide d.\nExamples\nHere are a few examples of the above strategies. Envision a free l ist with\nthree elements on it, of sizes 10, 30, and 20 (we’ll ignore headers and other\ndetails here, instead just focusing on how strategies operate):\nhead 10 30 20 NULL\nAssume an allocation request of size 15. A best-ﬁt approach would\nsearch the entire list and ﬁnd that 20 was the best ﬁt, as it is t he smallest\nfree space that can accommodate the request. The resulting fre e list:\nhead 10 30 5 NULL\nAs happens in this example, and often happens with a best-ﬁt ap -\nproach, a small free chunk is now left over. A worst-ﬁt approach is s imilar\nbut instead ﬁnds the largest chunk, in this example 30. The re sulting list:\nhead 10 15 20 NULL\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nFREE-SPACE MANAGEMENT 13\nThe ﬁrst-ﬁt strategy, in this example, does the same thing as w orst-ﬁt,\nalso ﬁnding the ﬁrst free block that can satisfy the request. T he difference\nis in the search cost; both best-ﬁt and worst-ﬁt look through the ent ire list;\nﬁrst-ﬁt only examines free chunks until it ﬁnds one that ﬁts, th us reducing\nsearch cost.\nThese examples just scratch the surface of allocation policies. More\ndetailed analysis with real workloads and more complex allocator b ehav-\niors (e.g., coalescing) are required for a deeper understandin g. Perhaps\nsomething for a homework section, you say?\n17.4 Other Approaches\nBeyond the basic approaches described above, there have been a h ost\nof suggested techniques and algorithms to improve memory allocat ion in\nsome way. We list a few of them here for your consideration (i.e., to m ake\nyou think about a little more than just best-ﬁt allocation).\nSegregated Lists\nOne interesting approach that has been around for some time is the use\nofsegregated lists . The basic idea is simple: if a particular application\nhas one (or a few) popular-sized request that it makes, keep a sep arate\nlist just to manage objects of that size; all other requests are f orwarded to\na more general memory allocator.\nThe beneﬁts of such an approach are obvious. By having a chunk of\nmemory dedicated for one particular size of requests, fragmenta tion is\nmuch less of a concern; moreover, allocation and free requests can b e\nserved quite quickly when they are of the right size, as no compl icated\nsearch of a list is required.\nJust like any good idea, this approach introduces new complication s\ninto a system as well. For example, how much memory should one ded-\nicate to the pool of memory that serves specialized requests of a gi ven\nsize, as opposed to the general pool? One particular allocator, the slab\nallocator by uber-engineer Jeff Bonwick (which was designed for use in\nthe Solaris kernel), handles this issue in a rather nice way [B9 4].\nSpeciﬁcally, when the kernel boots up, it allocates a number of object\ncaches for kernel objects that are likely to be requested frequently ( such as\nlocks, ﬁle-system inodes, etc.); the object caches thus are eac h segregated\nfree lists of a given size and serve memory allocation and free req uests\nquickly. When a given cache is running low on free space, it requ ests\nsome slabs of memory from a more general memory allocator (the to-\ntal amount requested being a multiple of the page size and the obj ect in\nquestion). Conversely, when the reference counts of the objects w ithin\na given slab all go to zero, the general allocator can reclaim the m from\nthe specialized allocator, which is often done when the VM system needs\nmore memory.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 FREE-SPACE MANAGEMENT\nASIDE : GREAT ENGINEERS AREREALLY GREAT\nEngineers like Jeff Bonwick (who not only wrote the slab allocator m en-\ntioned herein but also was the lead of an amazing ﬁle system, ZFS) are\nthe heart of Silicon Valley. Behind almost any great product or tech nol-\nogy is a human (or small group of humans) who are way above average\nin their talents, abilities, and dedication. As Mark Zuckerb erg (of Face-\nbook) says: “Someone who is exceptional in their role is not just a litt le\nbetter than someone who is pretty good. They are 100 times better. ” This\nis why, still today, one or two people can start a company that chang es\nthe face of the world forever (think Google, Apple, or Facebook). Work\nhard and you might become such a “100x” person as well. Failing th at,\nwork with such a person; you’ll learn more in a day than most learn in a\nmonth. Failing that, feel sad.\nThe slab allocator also goes beyond most segregated list approache s\nby keeping free objects on the lists in a pre-initialized state . Bonwick\nshows that initialization and destruction of data structures is costly [B94];\nby keeping freed objects in a particular list in their initial ized state, the\nslab allocator thus avoids frequent initialization and destruc tion cycles\nper object and thus lowers overheads noticeably.\nBuddy Allocation\nBecause coalescing is critical for an allocator, some approaches h ave been\ndesigned around making coalescing simple. One good example is fou nd\nin the binary buddy allocator [K65].\nIn such a system, free memory is ﬁrst conceptually thought of as one\nbig space of size 2N. When a request for memory is made, the search for\nfree space recursively divides free space by two until a block that is big\nenough to accommodate the request is found (and a further split in to two\nwould result in a space that is too small). At this point, the requ ested\nblock is returned to the user. Here is an example of a 64KB free sp ace\ngetting divided in the search for a 7KB block:\n64 KB\n32 KB 32 KB\n16 KB 16 KB\n8 KB 8 KB\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nFREE-SPACE MANAGEMENT 15\nIn the example, the leftmost 8KB block is allocated (as indicate d by the\ndarker shade of gray) and returned to the user; note that this sc heme can\nsuffer from internal fragmentation , as you are only allowed to give out\npower-of-two-sized blocks.\nThe beauty of buddy allocation is found in what happens when that\nblock is freed. When returning the 8KB block to the free list, th e allocator\nchecks whether the “buddy” 8KB is free; if so, it coalesces the t wo blocks\ninto a 16KB block. The allocator then checks if the buddy of the 16K B\nblock is still free; if so, it coalesces those two blocks. This recu rsive coa-\nlescing process continues up the tree, either restoring the ent ire free space\nor stopping when a buddy is found to be in use.\nThe reason buddy allocation works so well is that it is simple to de -\ntermine the buddy of a particular block. How, you ask? Think about t he\naddresses of the blocks in the free space above. If you think caref ully\nenough, you’ll see that the address of each buddy pair only differs by\na single bit; which bit is determined by the level in the buddy tree. And\nthus you have a basic idea of how binary buddy allocation schemes wor k.\nFor more detail, as always, see the Wilson survey [W+95].\nOther Ideas\nOne major problem with many of the approaches described above is th eir\nlack of scaling . Speciﬁcally, searching lists can be quite slow. Thus,\nadvanced allocators use more complex data structures to address these\ncosts, trading simplicity for performance. Examples include b alanced bi-\nnary trees, splay trees, or partially-ordered trees [W+95].\nGiven that modern systems often have multiple processors and run\nmulti-threaded workloads (something you’ll learn about in great d etail\nin the section of the book on Concurrency), it is not surprising that a lot\nof effort has been spent making allocators work well on multiprocess or-\nbased systems. Two wonderful examples are found in Berger et al . [B+00]\nand Evans [E06]; check them out for the details.\nThese are but two of the thousands of ideas people have had over time\nabout memory allocators; read on your own if you are curious. Failing\nthat, read about how the glibc allocator works [S15], to give you a sen se\nof what the real world is like.\n17.5 Summary\nIn this chapter, we’ve discussed the most rudimentary forms of me m-\nory allocators. Such allocators exist everywhere, linked into eve ry C pro-\ngram you write, as well as in the underlying OS which is managin g mem-\nory for its own data structures. As with many systems, there are m any\ntrade-offs to be made in building such a system, and the more you k now\nabout the exact workload presented to an allocator, the more you could do\nto tune it to work better for that workload. Making a fast, space-e fﬁcient,\nscalable allocator that works well for a broad range of workloads rema ins\nan on-going challenge in modern computer systems.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 FREE-SPACE MANAGEMENT\nReferences\n[B+00] “Hoard: A Scalable Memory Allocator for Multithreaded Appli cations” by Emery D.\nBerger, Kathryn S. McKinley, Robert D. Blumofe, Paul R. Wilson. ASPLOS -IX, November 2000.\nBerger and company’s excellent allocator for multiprocessor systems. Bey ond just being a fun paper, also\nused in practice!\n[B94] “The Slab Allocator: An Object-Caching Kernel Memory Allocator” by Je ff Bonwick.\nUSENIX ’94. A cool paper about how to build an allocator for an operating system kernel, and a great\nexample of how to specialize for particular common object sizes.\n[E06] “A Scalable Concurrent malloc(3) Implementation for FreeBSD” b y Jason Evans. April,\n2006. http://people.freebsd.org/˜jasone/jemalloc/bsdcan2006 /jemalloc.pdf. A detailed look at\nhow to build a real modern allocator for use in multiprocessors. The “jemalloc ” allocator is in widespread\nuse today, within FreeBSD, NetBSD, Mozilla Firefox, and within Facebook.\n[K65] “A Fast Storage Allocator” by Kenneth C. Knowlton. Communicati ons of the ACM,\nVolume 8:10, October 1965. The common reference for buddy allocation. Random strange fact: Knuth\ngives credit for the idea not to Knowlton but to Harry Markowitz, a Nobel-prize wi nning economist.\nAnother strange fact: Knuth communicates all of his emails via a secretary; he doesn’t send email\nhimself, rather he tells his secretary what email to send and then the secr etary does the work of emailing.\nLast Knuth fact: he created TeX, the tool used to typeset this book. It is an amazing pie ce of software4.\n[S15] “Understanding glibc malloc” by Sploitfun. February, 2015. s ploitfun.wordpress.com/\n2015/02/10/understanding-glibc-malloc/. A deep dive into how glibc malloc works. Amazingly\ndetailed and a very cool read.\n[W+95] “Dynamic Storage Allocation: A Survey and Critical Review” by Paul R. Wilson, Mark\nS. Johnstone, Michael Neely, David Boles. International Workshop on Memo ry Management,\nScotland, UK, September 1995. An excellent and far-reaching survey of many facets of memory\nallocation. Far too much detail to go into in this tiny chapter!\n4Actually we use LaTeX, which is based on Lamport’s additions to Te X, but close enough.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nFREE-SPACE MANAGEMENT 17\nHomework (Simulation)\nThe program, malloc.py , lets you explore the behavior of a simple\nfree-space allocator as described in the chapter. See the READM E for\ndetails of its basic operation.\nQuestions\n1. First run with the ﬂags -n 10 -H 0 -p BEST -s 0 to generate a few\nrandom allocations and frees. Can you predict what alloc()/ free() will re-\nturn? Can you guess the state of the free list after each request? What do\nyou notice about the free list over time?\n2. How are the results different when using a WORST ﬁt policy to s earch the\nfree list (-p WORST )? What changes?\n3. What about when using FIRST ﬁt ( -p FIRST )? What speeds up when you\nuse ﬁrst ﬁt?\n4. For the above questions, how the list is kept ordered can aff ect the time\nit takes to ﬁnd a free location for some of the policies. Use the d ifferent\nfree list orderings ( -l ADDRSORT ,-l SIZESORT+ ,-l SIZESORT- ) to see\nhow the policies and the list orderings interact.\n5. Coalescing of a free list can be quite important. Increase th e number of\nrandom allocations (say to -n 1000 ). What happens to larger allocation\nrequests over time? Run with and without coalescing (i.e., witho ut and with\nthe-Cﬂag). What differences in outcome do you see? How big is the free\nlist over time in each case? Does the ordering of the list matter in this case?\n6. What happens when you change the percent allocated fracti on-Pto higher\nthan 50? What happens to allocations as it nears 100? What abo ut as the\npercent nears 0?\n7. What kind of speciﬁc requests can you make to generate a highl y-fragmented\nfree space? Use the -Aﬂag to create fragmented free lists, and see how dif-\nferent policies and options change the organization of the f ree list.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",35178
22-18. Introduction to Paging.pdf,22-18. Introduction to Paging,"18\nPaging: Introduction\nIt is sometimes said that the operating system takes one of two app roaches\nwhen solving most any space-management problem. The ﬁrst approa ch\nis to chop things up into variable-sized pieces, as we saw with segmenta-\ntion in virtual memory. Unfortunately, this solution has inherent di fﬁcul-\nties. In particular, when dividing a space into different-s ize chunks, the\nspace itself can become fragmented , and thus allocation becomes more\nchallenging over time.\nThus, it may be worth considering the second approach: to chop up\nspace into ﬁxed-sized pieces. In virtual memory, we call this idea paging ,\nand it goes back to an early and important system, the Atlas [KE+ 62, L78].\nInstead of splitting up a process’s address space into some numbe r of\nvariable-sized logical segments (e.g., code, heap, stack), w e divide it into\nﬁxed-sized units, each of which we call a page . Correspondingly, we view\nphysical memory as an array of ﬁxed-sized slots called page frames ; each\nof these frames can contain a single virtual-memory page. Our ch allenge:\nTHECRUX:\nHOWTOVIRTUALIZE MEMORY WITHPAGES\nHow can we virtualize memory with pages, so as to avoid the prob-\nlems of segmentation? What are the basic techniques? How do we ma ke\nthose techniques work well, with minimal space and time overhea ds?\n18.1 A Simple Example And Overview\nTo help make this approach more clear, let’s illustrate it with a simple\nexample. Figure 18.1 (page 2) presents an example of a tiny add ress space,\nonly 64 bytes total in size, with four 16-byte pages (virtual pag es 0, 1, 2,\nand 3). Real address spaces are much bigger, of course, commonly 3 2 bits\nand thus 4-GB of address space, or even 64 bits1; in the book, we’ll often\nuse tiny examples to make them easier to digest.\n1A 64-bit address space is hard to imagine, it is so amazingly large. An analogy might\nhelp: if you think of a 32-bit address space as the size of a tennis court, a 64-bit address space\nis about the size of Europe(!).\n1\n2 PAGING : INTRODUCTION\n644832160\n(page 3)(page 2)(page 1)(page 0 of the address space)\nFigure 18.1: A Simple 64-byte Address Space\nPhysical memory, as shown in Figure 18.2, also consists of a numbe r\nof ﬁxed-sized slots, in this case eight page frames (making for a 128-byte\nphysical memory, also ridiculously small). As you can see in the diagram,\nthe pages of the virtual address space have been placed at diff erent loca-\ntions throughout physical memory; the diagram also shows the OS us ing\nsome of physical memory for itself.\nPaging, as we will see, has a number of advantages over our previou s\napproaches. Probably the most important improvement will be ﬂexibil-\nity: with a fully-developed paging approach, the system will be ab le to\nsupport the abstraction of an address space effectively, regar dless of how\na process uses the address space; we won’t, for example, make assu mp-\ntions about the direction the heap and stack grow and how they are us ed.\nAnother advantage is the simplicity of free-space management that pag-\ning affords. For example, when the OS wishes to place our tiny 64- byte\naddress space into our eight-page physical memory, it simply ﬁ nds four\nfree pages; perhaps the OS keeps a free list of all free pages for this, and\njust grabs the ﬁrst four free pages off of this list. In the exampl e, the OS\n1281129680644832160\npage frame 7page frame 6page frame 5page frame 4page frame 3page frame 2page frame 1page frame 0 of physical memory reserved for OS\n(unused)\npage 3 of AS\npage 0 of AS\n(unused)\npage 2 of AS\n(unused)\npage 1 of AS\nFigure 18.2: A 64-Byte Address Space In A 128-Byte Physical Memory\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : INTRODUCTION 3\nhas placed virtual page 0 of the address space (AS) in physical f rame 3,\nvirtual page 1 of the AS in physical frame 7, page 2 in frame 5, an d page\n3 in frame 2. Page frames 1, 4, and 6 are currently free.\nTo record where each virtual page of the address space is placed in\nphysical memory, the operating system usually keeps a per-process data\nstructure known as a page table . The major role of the page table is to\nstore address translations for each of the virtual pages of the address\nspace, thus letting us know where in physical memory each page r esides.\nFor our simple example (Figure 18.2, page 2), the page table woul d thus\nhave the following four entries: (Virtual Page 0 →Physical Frame 3),\n(VP 1→PF 7), (VP 2 →PF 5), and (VP 3 →PF 2).\nIt is important to remember that this page table is a per-process data\nstructure (most page table structures we discuss are per-proc ess struc-\ntures; an exception we’ll touch on is the inverted page table ). If another\nprocess were to run in our example above, the OS would have to manag e\na different page table for it, as its virtual pages obviously map todifferent\nphysical pages (modulo any sharing going on).\nNow, we know enough to perform an address-translation example.\nLet’s imagine the process with that tiny address space (64 byte s) is per-\nforming a memory access:\nmovl <virtual address>, %eax\nSpeciﬁcally, let’s pay attention to the explicit load of the data f rom\naddress<virtual address> into the register eax (and thus ignore the\ninstruction fetch that must have happened prior).\nTotranslate this virtual address that the process generated, we have\nto ﬁrst split it into two components: the virtual page number (VPN) , and\ntheoffset within the page. For this example, because the virtual addres s\nspace of the process is 64 bytes, we need 6 bits total for our virtual address\n(26= 64 ). Thus, our virtual address can be conceptualized as follows:\nVa5 Va4 Va3 Va2 Va1 Va0\nIn this diagram, Va5 is the highest-order bit of the virtual add ress, and\nVa0 the lowest-order bit. Because we know the page size (16 bytes ), we\ncan further divide the virtual address as follows:\nVa5 Va4 Va3 Va2 Va1 Va0VPN offset\nThe page size is 16 bytes in a 64-byte address space; thus we ne ed to\nbe able to select 4 pages, and the top 2 bits of the address do just that.\nThus, we have a 2-bit virtual page number (VPN). The remainin g bits tell\nus which byte of the page we are interested in, 4 bits in this cas e; we call\nthis the offset.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 PAGING : INTRODUCTION\nWhen a process generates a virtual address, the OS and hardwar e\nmust combine to translate it into a meaningful physical addre ss. For ex-\nample, let us assume the load above was to virtual address 21:\nmovl 21, %eax\nTurning “21” into binary form, we get “010101”, and thus we can ex-\namine this virtual address and see how it breaks down into a virt ual page\nnumber (VPN) and offset:\n0 1 0 1 0 1VPN offset\nThus, the virtual address “21” is on the 5th (“0101”th) byte of v irtual\npage “01” (or 1). With our virtual page number, we can now index our\npage table and ﬁnd which physical frame virtual page 1 reside s within. In\nthe page table above the physical frame number (PFN ) (also sometimes\ncalled the physical page number orPPN ) is 7 (binary 111). Thus, we can\ntranslate this virtual address by replacing the VPN with the PFN and then\nissue the load to physical memory (Figure 18.3).\nNote the offset stays the same (i.e., it is not translated), beca use the\noffset just tells us which byte within the page we want. Our ﬁnal physical\naddress is 1110101 (117 in decimal), and is exactly where we w ant our\nload to fetch data from (Figure 18.2, page 2).\nWith this basic overview in mind, we can now ask (and hopefully,\nanswer) a few basic questions you may have about paging. For examp le,\nwhere are these page tables stored? What are the typical conten ts of the\npage table, and how big are the tables? Does paging make the syst em\n(too) slow? These and other beguiling questions are answered, at l east in\npart, in the text below. Read on!\n0 1 0 1 0 1VPN offset\n1 1 1 0 1 0 1Address\nTranslation\nPFN offsetVirtual\nAddress\nPhysical\nAddress\nFigure 18.3: The Address Translation Process\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : INTRODUCTION 5\n1281129680644832160\npage frame 7page frame 6page frame 5page frame 4page frame 3page frame 2page frame 1page frame 0 of physical memory\n(unused)\npage 3 of AS\npage 0 of AS\n(unused)\npage 2 of AS\n(unused)\npage 1 of ASpage table:\n3 7 5 2\nFigure 18.4: Example: Page Table in Kernel Physical Memory\n18.2 Where Are Page Tables Stored?\nPage tables can get terribly large, much bigger than the smal l segment\ntable or base/bounds pair we have discussed previously. For exam ple,\nimagine a typical 32-bit address space, with 4KB pages. This virtual ad-\ndress splits into a 20-bit VPN and 12-bit offset (recall that 1 0 bits would\nbe needed for a 1KB page size, and just add two more to get to 4KB).\nA 20-bit VPN implies that there are 220translations that the OS would\nhave to manage for each process (that’s roughly a million); assumi ng we\nneed 4 bytes per page table entry (PTE) to hold the physical translation\nplus any other useful stuff, we get an immense 4MB of memory neede d\nfor each page table! That is pretty large. Now imagine there are 100\nprocesses running: this means the OS would need 400MB of memory\njust for all those address translations! Even in the modern era, w here\nmachines have gigabytes of memory, it seems a little crazy to us e a large\nchunk of it just for translations, no? And we won’t even think about how\nbig such a page table would be for a 64-bit address space; that wou ld be\ntoo gruesome and perhaps scare you off entirely.\nBecause page tables are so big, we don’t keep any special on-chip hard-\nware in the MMU to store the page table of the currently-running process.\nInstead, we store the page table for each process in memory somewhere.\nLet’s assume for now that the page tables live in physical memory t hat\nthe OS manages; later we’ll see that much of OS memory itself can b e vir-\ntualized, and thus page tables can be stored in OS virtual memor y (and\neven swapped to disk), but that is too confusing right now, so we’l l ig-\nnore it. In Figure 18.4 is a picture of a page table in OS memory; se e the\ntiny set of translations in there?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 PAGING : INTRODUCTION\n31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0\nPFN\nG\nPAT\nD\nA\nPCD\nPWT\nU/S\nR/W\nP\nFigure 18.5: An x86 Page Table Entry (PTE)\n18.3 What’s Actually In The Page Table?\nLet’s talk a little about page table organization. The page table is just\na data structure that is used to map virtual addresses (or real ly, virtual\npage numbers) to physical addresses (physical frame number s). Thus,\nany data structure could work. The simplest form is called a linear page\ntable , which is just an array. The OS indexes the array by the virtual page\nnumber (VPN), and looks up the page-table entry (PTE) at that in dex in\norder to ﬁnd the desired physical frame number (PFN). For now, we will\nassume this simple linear structure; in later chapters, we w ill make use of\nmore advanced data structures to help solve some problems with pa ging.\nAs for the contents of each PTE, we have a number of different bits\nin there worth understanding at some level. A valid bit is common to\nindicate whether the particular translation is valid; for exa mple, when\na program starts running, it will have code and heap at one end of it s\naddress space, and the stack at the other. All the unused space in-between\nwill be marked invalid , and if the process tries to access such memory, it\nwill generate a trap to the OS which will likely terminate the process.\nThus, the valid bit is crucial for supporting a sparse address s pace; by\nsimply marking all the unused pages in the address space inva lid, we\nremove the need to allocate physical frames for those pages and th us save\na great deal of memory.\nWe also might have protection bits , indicating whether the page could\nbe read from, written to, or executed from. Again, accessing a pag e in a\nway not allowed by these bits will generate a trap to the OS.\nThere are a couple of other bits that are important but we won’t talk\nabout much for now. A present bit indicates whether this page is in phys-\nical memory or on disk (i.e., it has been swapped out ). We will under-\nstand this machinery further when we study how to swap parts of the\naddress space to disk to support address spaces that are large r than phys-\nical memory; swapping allows the OS to free up physical memory by\nmoving rarely-used pages to disk. A dirty bit is also common, indicating\nwhether the page has been modiﬁed since it was brought into memor y.\nAreference bit (a.k.a. accessed bit ) is sometimes used to track whether\na page has been accessed, and is useful in determining which p ages are\npopular and thus should be kept in memory; such knowledge is criti cal\nduring page replacement , a topic we will study in great detail in subse-\nquent chapters.\nFigure 18.5 shows an example page table entry from the x86 archi tec-\nture [I09]. It contains a present bit (P); a read/write bit (R/ W) which\ndetermines if writes are allowed to this page; a user/supervi sor bit (U/S)\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : INTRODUCTION 7\nwhich determines if user-mode processes can access the page; a few bits\n(PWT, PCD, PAT, and G) that determine how hardware caching work s for\nthese pages; an accessed bit (A) and a dirty bit (D); and ﬁnall y, the page\nframe number (PFN) itself.\nRead the Intel Architecture Manuals [I09] for more details on x8 6 pag-\ning support. Be forewarned, however; reading manuals such as th ese,\nwhile quite informative (and certainly necessary for those who write code\nto use such page tables in the OS), can be challenging at ﬁrst. A little pa-\ntience, and a lot of desire, is required.\n18.4 Paging: Also Too Slow\nWith page tables in memory, we already know that they might be too\nbig. As it turns out, they can slow things down too. For example, take\nour simple instruction:\nmovl 21, %eax\nAgain, let’s just examine the explicit reference to address 2 1 and not\nworry about the instruction fetch. In this example, we’ll assume the hard-\nware performs the translation for us. To fetch the desired data, the system\nmust ﬁrst translate the virtual address (21) into the correct physical ad-\ndress (117). Thus, before fetching the data from address 117, t he system\nmust ﬁrst fetch the proper page table entry from the process’s pag e table,\nperform the translation, and then load the data from physical mem ory.\nTo do so, the hardware must know where the page table is for the\ncurrently-running process. Let’s assume for now that a single page-table\nbase register contains the physical address of the starting location of the\npage table. To ﬁnd the location of the desired PTE, the hardware w ill thus\nperform the following functions:\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\nPTEAddr = PageTableBaseRegister + (VPN *sizeof(PTE))\nIn our example, VPNMASK would be set to 0x30 (hex 30, or binary\n110000) which picks out the VPN bits from the full virtual addre ss;SHIFT\nis set to 4 (the number of bits in the offset), such that we move the VPN\nbits down to form the correct integer virtual page number. For exa m-\nple, with virtual address 21 (010101), and masking turns thi s value into\n010000; the shift turns it into 01, or virtual page 1, as desire d. We then use\nthis value as an index into the array of PTEs pointed to by the pag e table\nbase register.\nOnce this physical address is known, the hardware can fetch th e PTE\nfrom memory, extract the PFN, and concatenate it with the offset f rom the\nvirtual address to form the desired physical address. Speciﬁc ally, you can\nthink of the PFN being left-shifted by SHIFT , and then bitwise OR’d with\nthe offset to form the ﬁnal address as follows:\noffset = VirtualAddress & OFFSET_MASK\nPhysAddr = (PFN << SHIFT) | offset\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 PAGING : INTRODUCTION\n1// Extract the VPN from the virtual address\n2VPN = (VirtualAddress & VPN_MASK) >> SHIFT\n3\n4// Form the address of the page-table entry (PTE)\n5PTEAddr = PTBR + (VPN *sizeof(PTE))\n6\n7// Fetch the PTE\n8PTE = AccessMemory(PTEAddr)\n9\n10// Check if process can access the page\n11if (PTE.Valid == False)\n12 RaiseException(SEGMENTATION_FAULT)\n13else if (CanAccess(PTE.ProtectBits) == False)\n14 RaiseException(PROTECTION_FAULT)\n15else\n16 // Access is OK: form physical address and fetch it\n17 offset = VirtualAddress & OFFSET_MASK\n18 PhysAddr = (PTE.PFN << PFN_SHIFT) | offset\n19 Register = AccessMemory(PhysAddr)\nFigure 18.6: Accessing Memory With Paging\nFinally, the hardware can fetch the desired data from memory an d put\nit into register eax. The program has now succeeded at loading a value\nfrom memory!\nTo summarize, we now describe the initial protocol for what happen s\non each memory reference. Figure 18.6 shows the basic approach. F or\nevery memory reference (whether an instruction fetch or an expl icit load\nor store), paging requires us to perform one extra memory referenc e in\norder to ﬁrst fetch the translation from the page table. That is a lot of\nwork! Extra memory references are costly, and in this case will l ikely\nslow down the process by a factor of two or more.\nAnd now you can hopefully see that there are tworeal problems that\nwe must solve. Without careful design of both hardware and softwar e,\npage tables will cause the system to run too slowly, as well as ta ke up\ntoo much memory. While seemingly a great solution for our memory\nvirtualization needs, these two crucial problems must ﬁrst be overcome.\n18.5 A Memory Trace\nBefore closing, we now trace through a simple memory access exam-\nple to demonstrate all of the resulting memory accesses that occu r when\nusing paging. The code snippet (in C, in a ﬁle called array.c ) that we\nare interested in is as follows:\nint array[1000];\n...\nfor (i = 0; i < 1000; i++)\narray[i] = 0;\nWe compile array.c and run it with the following commands:\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : INTRODUCTION 9\nASIDE : DATA STRUCTURE — T HEPAGE TABLE\nOne of the most important data structures in the memory managemen t\nsubsystem of a modern OS is the page table . In general, a page table\nstores virtual-to-physical address translations , thus letting the system\nknow where each page of an address space actually resides in phy sical\nmemory. Because each address space requires such translation s, in gen-\neral there is one page table per process in the system. The exact structure\nof the page table is either determined by the hardware (older sy stems) or\ncan be more ﬂexibly managed by the OS (modern systems).\nprompt> gcc -o array array.c -Wall -O\nprompt> ./array\nOf course, to truly understand what memory accesses this code sn ip-\npet (which simply initializes an array) will make, we’ll have to know (or\nassume) a few more things. First, we’ll have to disassemble the result-\ning binary (using objdump on Linux, or otool on a Mac) to see what\nassembly instructions are used to initialize the array in a loop . Here is the\nresulting assembly code:\n1024 movl $0x0,(%edi,%eax,4)\n1028 incl %eax\n1032 cmpl $0x03e8,%eax\n1036 jne 0x1024\nThe code, if you know a little x86, is actually quite easy to understand2.\nThe ﬁrst instruction moves the value zero (shown as $0x0 ) into the vir-\ntual memory address of the location of the array; this address is com puted\nby taking the contents of %edi and adding %eax multiplied by four to it.\nThus,%edi holds the base address of the array, whereas %eax holds the\narray index ( i); we multiply by four because the array is an array of inte-\ngers, each of size four bytes.\nThe second instruction increments the array index held in %eax , and\nthe third instruction compares the contents of that register to t he hex\nvalue0x03e8 , or decimal 1000. If the comparison shows that two val-\nues are not yet equal (which is what the jne instruction tests), the fourth\ninstruction jumps back to the top of the loop.\nTo understand which memory accesses this instruction sequenc e makes\n(at both the virtual and physical levels), we’ll have to assume something\nabout where in virtual memory the code snippet and array are found , as\nwell as the contents and location of the page table.\nFor this example, we assume a virtual address space of size 64KB (un-\nrealistically small). We also assume a page size of 1KB.\n2We are cheating a little bit here, assuming each instruction is four byt es in size for sim-\nplicity; in actuality, x86 instructions are variable-sized.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 PAGING : INTRODUCTION\n0 10 20 30 40 50102410741124\nMemory AccessCode (VA)400004005040100Array (VA)10241074112411741224\nPage Table (PA)\n409641464196\nCode (PA)723272827332\nArray (PA)mov\ninc\ncmp\njnemovPageTable[1]PageTable[39]\nFigure 18.7: A Virtual (And Physical) Memory Trace\nAll we need to know now are the contents of the page table, and its\nlocation in physical memory. Let’s assume we have a linear (array -based)\npage table and that it is located at physical address 1KB (1024 ).\nAs for its contents, there are just a few virtual pages we need to worry\nabout having mapped for this example. First, there is the virtu al page the\ncode lives on. Because the page size is 1KB, virtual address 102 4 resides\non the second page of the virtual address space (VPN=1, as VPN=0 i s\nthe ﬁrst page). Let’s assume this virtual page maps to physica l frame 4\n(VPN 1→PFN 4).\nNext, there is the array itself. Its size is 4000 bytes (1000 i ntegers),\nand we assume that it resides at virtual addresses 40000 throu gh 44000\n(not including the last byte). The virtual pages for this decim al range are\nVPN=39 ... VPN=42. Thus, we need mappings for these pages. Let ’s as-\nsume these virtual-to-physical mappings for the example: (VPN 39 →PFN 7),\n(VPN 40 →PFN 8), (VPN 41 →PFN 9), (VPN 42 →PFN 10).\nWe are now ready to trace the memory references of the program.\nWhen it runs, each instruction fetch will generate two memory r eferences:\none to the page table to ﬁnd the physical frame that the instruc tion resides\nwithin, and one to the instruction itself to fetch it to the CPU f or process-\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : INTRODUCTION 11\ning. In addition, there is one explicit memory reference in the f orm of\nthemov instruction; this adds another page table access ﬁrst (to tran slate\nthe array virtual address to the correct physical one) and then the array\naccess itself.\nThe entire process, for the ﬁrst ﬁve loop iterations, is depicted i n Fig-\nure 18.7 (page 10). The bottom most graph shows the instruction mem ory\nreferences on the y-axis in black (with virtual addresses on th e left, and\nthe actual physical addresses on the right); the middle graph shows array\naccesses in dark gray (again with virtual on left and physical on right); ﬁ-\nnally, the topmost graph shows page table memory accesses in ligh t gray\n(just physical, as the page table in this example resides in p hysical mem-\nory). The x-axis, for the entire trace, shows memory accesses acr oss the\nﬁrst ﬁve iterations of the loop; there are 10 memory accesses per loop ,\nwhich includes four instruction fetches, one explicit update of memory,\nand ﬁve page table accesses to translate those four fetches and one explicit\nupdate.\nSee if you can make sense of the patterns that show up in this visu-\nalization. In particular, what will change as the loop continues to run\nbeyond these ﬁrst ﬁve iterations? Which new memory locations will be\naccessed? Can you ﬁgure it out?\nThis has just been the simplest of examples (only a few lines of C c ode),\nand yet you might already be able to sense the complexity of under stand-\ning the actual memory behavior of real applications. Don’t worry: it deﬁ-\nnitely gets worse, because the mechanisms we are about to introd uce only\ncomplicate this already complex machinery. Sorry3!\n18.6 Summary\nWe have introduced the concept of paging as a solution to our chal-\nlenge of virtualizing memory. Paging has many advantages over p revi-\nous approaches (such as segmentation). First, it does not lead to e xternal\nfragmentation, as paging (by design) divides memory into ﬁxed -sized\nunits. Second, it is quite ﬂexible, enabling the sparse use of vi rtual ad-\ndress spaces.\nHowever, implementing paging support without care will lead to a\nslower machine (with many extra memory accesses to access the p age\ntable) as well as memory waste (with memory ﬁlled with page tabl es in-\nstead of useful application data). We’ll thus have to think a lit tle harder\nto come up with a paging system that not only works, but works well.\nThe next two chapters, fortunately, will show us how to do so.\n3We’re not really sorry. But, we are sorry about not being sorry, i f that makes sense.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 PAGING : INTRODUCTION\nReferences\n[KE+62] “One-level Storage System” by T. Kilburn, D.B.G. Edwards , M.J. Lanigan, F.H. Sum-\nner. IRE Trans. EC-11, 2, 1962. Reprinted in Bell and Newell, “Comp uter Structures: Readings\nand Examples”. McGraw-Hill, New York, 1971. The Atlas pioneered the idea of dividing memory\ninto ﬁxed-sized pages and in many senses was an early form of the memory-managem ent ideas we see\nin modern computer systems.\n[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals” Intel, 2009. Available:\nhttp://www.intel.com/products/processor/manuals. In particular, pay attention to “Volume\n3A: System Programming Guide Part 1” and “Volume 3B: System Programmin g Guide Part 2”.\n[L78] “The Manchester Mark I and Atlas: A Historical Perspective” by S. H. L avington. Com-\nmunications of the ACM, Volume 21:1, January 1978. This paper is a great retrospective of some of\nthe history of the development of some important computer systems. As we someti mes forget in the US,\nmany of these new ideas came from overseas.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : INTRODUCTION 13\nHomework (Simulation)\nIn this homework, you will use a simple program, which is known as\npaging-linear-translate.py , to see if you understand how simple\nvirtual-to-physical address translation works with linear pa ge tables. See\nthe README for details.\nQuestions\n1. Before doing any translations, let’s use the simulator to stud y how linear\npage tables change size given different parameters. Compute th e size of\nlinear page tables as different parameters change. Some sugges ted inputs\nare below; by using the -v flag , you can see how many page-table entries\nare ﬁlled. First, to understand how linear page table size cha nges as the\naddress space grows:\npaging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 1k -a 2m -p 512m -v -n 0\npaging-linear-translate.py -P 1k -a 4m -p 512m -v -n 0\nThen, to understand how linear page table size changes as page size grows:\npaging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 2k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 4k -a 1m -p 512m -v -n 0\nBefore running any of these, try to think about the expected tren ds. How\nshould page-table size change as the address space grows? As th e page size\ngrows? Why shouldn’t we just use really big pages in general?\n2. Now let’s do some translations. Start with some small examples, and change\nthe number of pages that are allocated to the address space with the-u\nflag . For example:\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 0\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 25\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 50\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 75\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 100\nWhat happens as you increase the percentage of pages that are allocated in\neach address space?\n3. Now let’s try some different random seeds, and some differen t (and some-\ntimes quite crazy) address-space parameters, for variety:\npaging-linear-translate.py -P 8 -a 32 -p 1024 -v -s 1\npaging-linear-translate.py -P 8k -a 32k -p 1m -v -s 2\npaging-linear-translate.py -P 1m -a 256m -p 512m -v -s 3\nWhich of these parameter combinations are unrealistic? Why?\n4. Use the program to try out some other problems. Can you ﬁnd the li mits of\nwhere the program doesn’t work anymore? For example, what happe ns if\nthe address-space size is bigger than physical memory?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",28877
23-19. Translation Lookaside Buffers.pdf,23-19. Translation Lookaside Buffers,"19\nPaging: Faster Translations (TLBs)\nUsing paging as the core mechanism to support virtual memory can lead\nto high performance overheads. By chopping the address space in to small,\nﬁxed-sized units (i.e., pages), paging requires a large amou nt of mapping\ninformation. Because that mapping information is generally stor ed in\nphysical memory, paging logically requires an extra memory looku p for\neach virtual address generated by the program. Going to memory f or\ntranslation information before every instruction fetch or explic it load or\nstore is prohibitively slow. And thus our problem:\nTHECRUX:\nHOWTOSPEED UPADDRESS TRANSLATION\nHow can we speed up address translation, and generally avoid the\nextra memory reference that paging seems to require? What har dware\nsupport is required? What OS involvement is needed?\nWhen we want to make things fast, the OS usually needs some help .\nAnd help often comes from the OS’s old friend: the hardware. To speed\naddress translation, we are going to add what is called (for hist orical rea-\nsons [CP78]) a translation-lookaside buffer , orTLB [CG68, C95]. A TLB\nis part of the chip’s memory-management unit (MMU ), and is simply a\nhardware cache of popular virtual-to-physical address translations; thus,\na better name would be an address-translation cache . Upon each virtual\nmemory reference, the hardware ﬁrst checks the TLB to see if th e desired\ntranslation is held therein; if so, the translation is performed (quickly)\nwithout having to consult the page table (which has all translations). Be-\ncause of their tremendous performance impact, TLBs in a real sen se make\nvirtual memory possible [C95].\n1\n2 P AGING : FASTER TRANSLATIONS (TLB S)\n1VPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2(Success, TlbEntry) = TLB_Lookup(VPN)\n3if (Success == True) // TLB Hit\n4if (CanAccess(TlbEntry.ProtectBits) == True)\n5 Offset = VirtualAddress & OFFSET_MASK\n6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7 Register = AccessMemory(PhysAddr)\n8else\n9 RaiseException(PROTECTION_FAULT)\n10else // TLB Miss\n11PTEAddr = PTBR + (VPN *sizeof(PTE))\n12PTE = AccessMemory(PTEAddr)\n13if (PTE.Valid == False)\n14 RaiseException(SEGMENTATION_FAULT)\n15else if (CanAccess(PTE.ProtectBits) == False)\n16 RaiseException(PROTECTION_FAULT)\n17else\n18 TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n19 RetryInstruction()\nFigure 19.1: TLB Control Flow Algorithm\n19.1 TLB Basic Algorithm\nFigure 19.1 shows a rough sketch of how hardware might handle a\nvirtual address translation, assuming a simple linear page table (i.e., the\npage table is an array) and a hardware-managed TLB (i.e., the hardware\nhandles much of the responsibility of page table accesses; we’ll explain\nmore about this below).\nThe algorithm the hardware follows works like this: ﬁrst, extrac t the\nvirtual page number (VPN) from the virtual address (Line 1 in F igure 19.1),\nand check if the TLB holds the translation for this VPN (Line 2). I f it does,\nwe have a TLB hit , which means the TLB holds the translation. Success!\nWe can now extract the page frame number (PFN) from the relevant TLB\nentry, concatenate that onto the offset from the original virtual address,\nand form the desired physical address (PA), and access memory ( Lines\n5–7), assuming protection checks do not fail (Line 4).\nIf the CPU does not ﬁnd the translation in the TLB (a TLB miss ), we\nhave some more work to do. In this example, the hardware accesses t he\npage table to ﬁnd the translation (Lines 11–12), and, assumin g that the\nvirtual memory reference generated by the process is valid and accessi-\nble (Lines 13, 15), updates the TLB with the translation (Line 18). These\nset of actions are costly, primarily because of the extra memory re ference\nneeded to access the page table (Line 12). Finally, once the TL B is up-\ndated, the hardware retries the instruction; this time, the t ranslation is\nfound in the TLB, and the memory reference is processed quickly.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nPAGING : FASTER TRANSLATIONS (TLB S) 3\nThe TLB, like all caches, is built on the premise that in the comm on\ncase, translations are found in the cache (i.e., are hits). If s o, little over-\nhead is added, as the TLB is found near the processing core and is d e-\nsigned to be quite fast. When a miss occurs, the high cost of pagin g is\nincurred; the page table must be accessed to ﬁnd the translat ion, and an\nextra memory reference (or more, with more complex page tables) re sults.\nIf this happens often, the program will likely run noticeably mor e slowly;\nmemory accesses, relative to most CPU instructions, are quite c ostly, and\nTLB misses lead to more memory accesses. Thus, it is our hope to avoi d\nTLB misses as much as we can.\n19.2 Example: Accessing An Array\nTo make clear the operation of a TLB, let’s examine a simple virtua l\naddress trace and see how a TLB can improve its performance. In th is\nexample, let’s assume we have an array of 10 4-byte integers in m emory,\nstarting at virtual address 100. Assume further that we have a small 8-bit\nvirtual address space, with 16-byte pages; thus, a virtual a ddress breaks\ndown into a 4-bit VPN (there are 16 virtual pages) and a 4-bit off set (there\nare 16 bytes on each of those pages).\nFigure 19.2 (page 4) shows the array laid out on the 16 16-byte pag es\nof the system. As you can see, the array’s ﬁrst entry ( a[0] ) begins on\n(VPN=06, offset=04); only three 4-byte integers ﬁt onto that pa ge. The\narray continues onto the next page (VPN=07), where the next four entries\n(a[3] ...a[6] ) are found. Finally, the last three entries of the 10-entry\narray (a[7] ...a[9] ) are located on the next page of the address space\n(VPN=08).\nNow let’s consider a simple loop that accesses each array element,\nsomething that would look like this in C:\nint sum = 0;\nfor (i = 0; i < 10; i++) {\nsum += a[i];\n}\nFor the sake of simplicity, we will pretend that the only memory ac -\ncesses the loop generates are to the array (ignoring the variabl esiand\nsum, as well as the instructions themselves). When the ﬁrst array element\n(a[0] ) is accessed, the CPU will see a load to virtual address 100. Th e\nhardware extracts the VPN from this (VPN=06), and uses that to check\nthe TLB for a valid translation. Assuming this is the ﬁrst time t he pro-\ngram accesses the array, the result will be a TLB miss.\nThe next access is to a[1] , and there is some good news here: a TLB\nhit! Because the second element of the array is packed next to th e ﬁrst, it\nlives on the same page; because we’ve already accessed this pag e when\naccessing the ﬁrst element of the array, the translation is alr eady loaded\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 P AGING : FASTER TRANSLATIONS (TLB S)\nVPN = 15VPN = 14VPN = 13VPN = 12VPN = 11VPN = 10VPN = 09VPN = 08VPN = 07VPN = 06VPN = 05VPN = 04VPN = 03VPN = 02VPN = 01VPN = 0000 04 08 12 16Offset\na[0] a[1] a[2]\na[3] a[4] a[5] a[6]\na[7] a[8] a[9]\nFigure 19.2: Example: An Array In A Tiny Address Space\ninto the TLB. And hence the reason for our success. Access to a[2] en-\ncounters similar success (another hit), because it too lives on t he same\npage asa[0] anda[1] .\nUnfortunately, when the program accesses a[3] , we encounter an-\nother TLB miss. However, once again, the next entries ( a[4] ...a[6] )\nwill hit in the TLB, as they all reside on the same page in memory.\nFinally, access to a[7] causes one last TLB miss. The hardware once\nagain consults the page table to ﬁgure out the location of this virt ual page\nin physical memory, and updates the TLB accordingly. The ﬁnal t wo ac-\ncesses (a[8] anda[9] ) receive the beneﬁts of this TLB update; when the\nhardware looks in the TLB for their translations, two more hits res ult.\nLet us summarize TLB activity during our ten accesses to the ar ray:\nmiss , hit, hit, miss , hit, hit, hit, miss , hit, hit. Thus, our TLB hit rate ,\nwhich is the number of hits divided by the total number of accesse s, is\n70%. Although this is not too high (indeed, we desire hit rates th at ap-\nproach 100%), it is non-zero, which may be a surprise. Even though this\nis the ﬁrst time the program accesses the array, the TLB improve s per-\nformance due to spatial locality . The elements of the array are packed\ntightly into pages (i.e., they are close to one another in space ), and thus\nonly the ﬁrst access to an element on a page yields a TLB miss.\nAlso note the role that page size plays in this example. If the pa ge size\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nPAGING : FASTER TRANSLATIONS (TLB S) 5\nTIP: USECACHING WHEN POSSIBLE\nCaching is one of the most fundamental performance techniques in com-\nputer systems, one that is used again and again to make the “comm on-\ncase fast” [HP06]. The idea behind hardware caches is to take advantage\noflocality in instruction and data references. There are usually two typ es\nof locality: temporal locality and spatial locality . With temporal locality,\nthe idea is that an instruction or data item that has been recent ly accessed\nwill likely be re-accessed soon in the future. Think of loop variab les or in-\nstructions in a loop; they are accessed repeatedly over time. Wit h spatial\nlocality, the idea is that if a program accesses memory at addres sx, it will\nlikely soon access memory near x. Imagine here streaming through an\narray of some kind, accessing one element and then the next. Of cou rse,\nthese properties depend on the exact nature of the program, and th us are\nnot hard-and-fast laws but more like rules of thumb.\nHardware caches, whether for instructions, data, or address tr anslations\n(as in our TLB) take advantage of locality by keeping copies of memor y in\nsmall, fast on-chip memory. Instead of having to go to a (slow) mem ory\nto satisfy a request, the processor can ﬁrst check if a nearby cop y exists\nin a cache; if it does, the processor can access it quickly (i.e., in a few\nCPU cycles) and avoid spending the costly time it takes to acces s memory\n(many nanoseconds).\nYou might be wondering: if caches (like the TLB) are so great, wh y don’t\nwe just make bigger caches and keep all of our data in them? Unfor-\ntunately, this is where we run into more fundamental laws like those of\nphysics. If you want a fast cache, it has to be small, as issues l ike the\nspeed-of-light and other physical constraints become relevant . Any large\ncache by deﬁnition is slow, and thus defeats the purpose. Thus, w e are\nstuck with small, fast caches; the question that remains is how to best use\nthem to improve performance.\nhad simply been twice as big (32 bytes, not 16), the array acces s would\nsuffer even fewer misses. As typical page sizes are more like 4 KB, these\ntypes of dense, array-based accesses achieve excellent TLB p erformance,\nencountering only a single miss per page of accesses.\nOne last point about TLB performance: if the program, soon after thi s\nloop completes, accesses the array again, we’d likely see an even bet-\nter result, assuming that we have a big enough TLB to cache the n eeded\ntranslations: hit, hit, hit, hit, hit, hit, hit, hit, hit, hit . In this case, the\nTLB hit rate would be high because of temporal locality , i.e., the quick\nre-referencing of memory items in time . Like any cache, TLBs rely upon\nboth spatial and temporal locality for success, which are program proper-\nties. If the program of interest exhibits such locality (and man y programs\ndo), the TLB hit rate will likely be high.\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 P AGING : FASTER TRANSLATIONS (TLB S)\n1VPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2(Success, TlbEntry) = TLB_Lookup(VPN)\n3if (Success == True) // TLB Hit\n4if (CanAccess(TlbEntry.ProtectBits) == True)\n5 Offset = VirtualAddress & OFFSET_MASK\n6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7 Register = AccessMemory(PhysAddr)\n8else\n9 RaiseException(PROTECTION_FAULT)\n10else // TLB Miss\n11RaiseException(TLB_MISS)\nFigure 19.3: TLB Control Flow Algorithm (OS Handled)\n19.3 Who Handles The TLB Miss?\nOne question that we must answer: who handles a TLB miss? Two an -\nswers are possible: the hardware, or the software (OS). In the olde n days,\nthe hardware had complex instruction sets (sometimes called CISC , for\ncomplex-instruction set computers) and the people who built the hard-\nware didn’t much trust those sneaky OS people. Thus, the hardwar e\nwould handle the TLB miss entirely. To do this, the hardware ha s to\nknow exactly where the page tables are located in memory (via a page-\ntable base register , used in Line 11 in Figure 19.1), as well as their exact\nformat ; on a miss, the hardware would “walk” the page table, ﬁnd the cor-\nrect page-table entry and extract the desired translation, u pdate the TLB\nwith the translation, and retry the instruction. An example of a n “older”\narchitecture that has hardware-managed TLBs is the Intel x86 architec-\nture, which uses a ﬁxed multi-level page table (see the next chapter for\ndetails); the current page table is pointed to by the CR3 regis ter [I09].\nMore modern architectures (e.g., MIPS R10k [H93] or Sun’s SPARC v9\n[WG00], both RISC or reduced-instruction set computers) have what is\nknown as a software-managed TLB . On a TLB miss, the hardware sim-\nply raises an exception (line 11 in Figure 19.3), which pauses the current\ninstruction stream, raises the privilege level to kernel mode , and jumps\nto a trap handler . As you might guess, this trap handler is code within\nthe OS that is written with the express purpose of handling TLB m isses.\nWhen run, the code will lookup the translation in the page table, u se spe-\ncial “privileged” instructions to update the TLB, and return from the trap;\nat this point, the hardware retries the instruction (resultin g in a TLB hit).\nLet’s discuss a couple of important details. First, the return-f rom-trap\ninstruction needs to be a little different than the return-fr om-trap we saw\nbefore when servicing a system call. In the latter case, the re turn-from-\ntrap should resume execution at the instruction after the trap into the OS,\njust as a return from a procedure call returns to the instruction imme-\ndiately following the call into the procedure. In the former case , when\nreturning from a TLB miss-handling trap, the hardware must re sume ex-\necution at the instruction that caused the trap; this retry thus lets the in-\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nPAGING : FASTER TRANSLATIONS (TLB S) 7\nASIDE : RISC VS. CISC\nIn the 1980’s, a great battle took place in the computer architect ure com-\nmunity. On one side was the CISC camp, which stood for Complex\nInstruction Set Computing ; on the other side was RISC , for Reduced\nInstruction Set Computing [PS81]. The RISC side was spear-headed by\nDavid Patterson at Berkeley and John Hennessy at Stanford (who ar e also\nco-authors of some famous books [HP06]), although later John Cocke was\nrecognized with a Turing award for his earliest work on RISC [CM00] .\nCISC instruction sets tend to have a lot of instructions in them, an d each\ninstruction is relatively powerful. For example, you might see a string\ncopy, which takes two pointers and a length and copies bytes from s ource\nto destination. The idea behind CISC was that instructions shoul d be\nhigh-level primitives, to make the assembly language itsel f easier to use,\nand to make code more compact.\nRISC instruction sets are exactly the opposite. A key observation b ehind\nRISC is that instruction sets are really compiler targets, and a ll compil-\ners really want are a few simple primitives that they can use t o gener-\nate high-performance code. Thus, RISC proponents argued, let’s ri p out\nas much from the hardware as possible (especially the microcode) , and\nmake what’s left simple, uniform, and fast.\nIn the early days, RISC chips made a huge impact, as they were not iceably\nfaster [BC91]; many papers were written; a few companies were formed\n(e.g., MIPS and Sun). However, as time progressed, CISC manufact urers\nsuch as Intel incorporated many RISC techniques into the core of th eir\nprocessors, for example by adding early pipeline stages that tr ansformed\ncomplex instructions into micro-instructions which could then b e pro-\ncessed in a RISC-like manner. These innovations, plus a growing n umber\nof transistors on each chip, allowed CISC to remain competitive. Th e end\nresult is that the debate died down, and today both types of process ors\ncan be made to run fast.\nstruction run again, this time resulting in a TLB hit. Thus, de pending on\nhow a trap or exception was caused, the hardware must save a diffe rent\nPC when trapping into the OS, in order to resume properly when the time\nto do so arrives.\nSecond, when running the TLB miss-handling code, the OS needs to be\nextra careful not to cause an inﬁnite chain of TLB misses to occur . Many\nsolutions exist; for example, you could keep TLB miss handlers in p hysi-\ncal memory (where they are unmapped and not subject to address trans-\nlation), or reserve some entries in the TLB for permanently-vali d transla-\ntions and use some of those permanent translation slots for the handl er\ncode itself; these wired translations always hit in the TLB.\nThe primary advantage of the software-managed approach is ﬂexibil-\nity: the OS can use any data structure it wants to implement the pa ge\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 P AGING : FASTER TRANSLATIONS (TLB S)\nASIDE : TLB V ALID BIT/negationslash=PAGE TABLE VALID BIT\nA common mistake is to confuse the valid bits found in a TLB with thos e\nfound in a page table. In a page table, when a page-table entry ( PTE)\nis marked invalid, it means that the page has not been allocated by the\nprocess, and should not be accessed by a correctly-working program . The\nusual response when an invalid page is accessed is to trap to th e OS,\nwhich will respond by killing the process.\nA TLB valid bit, in contrast, simply refers to whether a TLB ent ry has a\nvalid translation within it. When a system boots, for example, a c ommon\ninitial state for each TLB entry is to be set to invalid, becaus e no address\ntranslations are yet cached there. Once virtual memory is enab led, and\nonce programs start running and accessing their virtual addre ss spaces,\nthe TLB is slowly populated, and thus valid entries soon ﬁll the TL B.\nThe TLB valid bit is quite useful when performing a context swit ch too,\nas we’ll discuss further below. By setting all TLB entries to in valid, the\nsystem can ensure that the about-to-be-run process does not accid entally\nuse a virtual-to-physical translation from a previous process.\ntable, without necessitating hardware change. Another advan tage is sim-\nplicity , as seen in the TLB control ﬂow (line 11 in Figure 19.3, in contrast\nto lines 11–19 in Figure 19.1). The hardware doesn’t do much on a m iss:\njust raise an exception and let the OS TLB miss handler do the re st.\n19.4 TLB Contents: What’s In There?\nLet’s look at the contents of the hardware TLB in more detail. A typic al\nTLB might have 32, 64, or 128 entries and be what is called fully associa-\ntive. Basically, this just means that any given translation can be anywhere\nin the TLB, and that the hardware will search the entire TLB in parallel to\nﬁnd the desired translation. A TLB entry might look like this:\nVPN PFN other bits\nNote that both the VPN and PFN are present in each entry, as a tran s-\nlation could end up in any of these locations (in hardware terms, th e TLB\nis known as a fully-associative cache). The hardware searches the entries\nin parallel to see if there is a match.\nMore interesting are the “other bits”. For example, the TLB common ly\nhas a valid bit, which says whether the entry has a valid translation or\nnot. Also common are protection bits, which determine how a page can\nbe accessed (as in the page table). For example, code pages migh t be\nmarked read and execute , whereas heap pages might be marked read and\nwrite . There may also be a few other ﬁelds, including an address-space\nidentiﬁer , adirty bit , and so forth; see below for more information.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nPAGING : FASTER TRANSLATIONS (TLB S) 9\n19.5 TLB Issue: Context Switches\nWith TLBs, some new issues arise when switching between proces ses\n(and hence address spaces). Speciﬁcally, the TLB contains vir tual-to-physical\ntranslations that are only valid for the currently running proce ss; these\ntranslations are not meaningful for other processes. As a result, when\nswitching from one process to another, the hardware or OS (or both) mu st\nbe careful to ensure that the about-to-be-run process does not acc identally\nuse translations from some previously run process.\nTo understand this situation better, let’s look at an example. Wh en one\nprocess (P1) is running, it assumes the TLB might be caching tr anslations\nthat are valid for it, i.e., that come from P1’s page table. Assume , for this\nexample, that the 10th virtual page of P1 is mapped to physical frame 100.\nIn this example, assume another process (P2) exists, and the OS soon\nmight decide to perform a context switch and run it. Assume here that\nthe 10th virtual page of P2 is mapped to physical frame 170. If e ntries for\nboth processes were in the TLB, the contents of the TLB would be:\nVPN PFN valid prot\n10 100 1 rwx\n— — 0 —\n10 170 1 rwx\n— — 0 —\nIn the TLB above, we clearly have a problem: VPN 10 translates to\neither PFN 100 (P1) or PFN 170 (P2), but the hardware can’t disti nguish\nwhich entry is meant for which process. Thus, we need to do some mor e\nwork in order for the TLB to correctly and efﬁciently support virtu aliza-\ntion across multiple processes. And thus, a crux:\nTHECRUX:\nHOWTOMANAGE TLB C ONTENTS ONA C ONTEXT SWITCH\nWhen context-switching between processes, the translations i n the TLB\nfor the last process are not meaningful to the about-to-be-run proc ess.\nWhat should the hardware or OS do in order to solve this problem?\nThere are a number of possible solutions to this problem. One ap-\nproach is to simply ﬂush the TLB on context switches, thus emptying\nit before running the next process. On a software-based system, this\ncan be accomplished with an explicit (and privileged) hardwa re instruc-\ntion; with a hardware-managed TLB, the ﬂush could be enacted wh en the\npage-table base register is changed (note the OS must change t he PTBR\non a context switch anyhow). In either case, the ﬂush operation sim ply\nsets all valid bits to 0, essentially clearing the contents of t he TLB.\nBy ﬂushing the TLB on each context switch, we now have a working\nsolution, as a process will never accidentally encounter the wron g trans-\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 P AGING : FASTER TRANSLATIONS (TLB S)\nlations in the TLB. However, there is a cost: each time a process ru ns, it\nmust incur TLB misses as it touches its data and code pages. If th e OS\nswitches between processes frequently, this cost may be high.\nTo reduce this overhead, some systems add hardware support to en -\nable sharing of the TLB across context switches. In particular, some hard-\nware systems provide an address space identiﬁer (ASID ) ﬁeld in the\nTLB. You can think of the ASID as a process identiﬁer (PID), but usu-\nally it has fewer bits (e.g., 8 bits for the ASID versus 32 bits for a PID).\nIf we take our example TLB from above and add ASIDs, it is clear\nprocesses can readily share the TLB: only the ASID ﬁeld is needed to dif-\nferentiate otherwise identical translations. Here is a depic tion of a TLB\nwith the added ASID ﬁeld:\nVPN PFN valid prot ASID\n10 100 1 rwx 1\n— — 0 — —\n10 170 1 rwx 2\n— — 0 — —\nThus, with address-space identiﬁers, the TLB can hold transl ations\nfrom different processes at the same time without any confusion. O f\ncourse, the hardware also needs to know which process is current ly run-\nning in order to perform translations, and thus the OS must, on a con text\nswitch, set some privileged register to the ASID of the current p rocess.\nAs an aside, you may also have thought of another case where two\nentries of the TLB are remarkably similar. In this example, th ere are two\nentries for two different processes with two different VPNs th at point to\nthesame physical page:\nVPN PFN valid prot ASID\n10 101 1 r-x 1\n— — 0 — —\n50 101 1 r-x 2\n— — 0 — —\nThis situation might arise, for example, when two processes share a\npage (a code page, for example). In the example above, Process 1 is shar-\ning physical page 101 with Process 2; P1 maps this page into the 10th\npage of its address space, whereas P2 maps it to the 50th page of i ts ad-\ndress space. Sharing of code pages (in binaries, or shared librar ies) is\nuseful as it reduces the number of physical pages in use, thus r educing\nmemory overheads.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nPAGING : FASTER TRANSLATIONS (TLB S) 11\n19.6 Issue: Replacement Policy\nAs with any cache, and thus also with the TLB, one more issue that we\nmust consider is cache replacement . Speciﬁcally, when we are installing\na new entry in the TLB, we have to replace an old one, and thus the\nquestion: which one to replace?\nTHECRUX: HOWTODESIGN TLB R EPLACEMENT POLICY\nWhich TLB entry should be replaced when we add a new TLB entry?\nThe goal, of course, being to minimize the miss rate (or increase hit rate )\nand thus improve performance.\nWe will study such policies in some detail when we tackle the prob lem\nof swapping pages to disk; here we’ll just highlight a few typic al policies.\nOne common approach is to evict the least-recently-used orLRU entry.\nLRU tries to take advantage of locality in the memory-reference stream,\nassuming it is likely that an entry that has not recently been u sed is a good\ncandidate for eviction. Another typical approach is to use a random pol-\nicy, which evicts a TLB mapping at random. Such a policy is useful due\nto its simplicity and ability to avoid corner-case behaviors; f or example,\na “reasonable” policy such as LRU behaves quite unreasonably wh en a\nprogram loops over n+ 1 pages with a TLB of size n; in this case, LRU\nmisses upon every access, whereas random does much better.\n19.7 A Real TLB Entry\nFinally, let’s brieﬂy look at a real TLB. This example is from the M IPS\nR4000 [H93], a modern system that uses software-managed TLBs; a slightly\nsimpliﬁed MIPS TLB entry can be seen in Figure 19.4.\nThe MIPS R4000 supports a 32-bit address space with 4KB pages. Thus,\nwe would expect a 20-bit VPN and 12-bit offset in our typical virt ual ad-\ndress. However, as you can see in the TLB, there are only 19 bits for the\nVPN; as it turns out, user addresses will only come from half the ad dress\nspace (the rest reserved for the kernel) and hence only 19 bits of VPN\nare needed. The VPN translates to up to a 24-bit physical fram e number\n(PFN), and hence can support systems with up to 64GB of (physica l) main\nmemory ( 2244KB pages).\nThere are a few other interesting bits in the MIPS TLB. We see a global\nbit (G), which is used for pages that are globally-shared among p rocesses.\nThus, if the global bit is set, the ASID is ignored. We also see the 8-bit\nASID , which the OS can use to distinguish between address spaces ( as\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\nVPN G ASID\nPFN C D V\nFigure 19.4: A MIPS TLB Entry\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 P AGING : FASTER TRANSLATIONS (TLB S)\nTIP: RAM I SN’TALWAYS RAM (C ULLER ’SLAW)\nThe term random-access memory , orRAM , implies that you can access\nany part of RAM just as quickly as another. While it is generally good to\nthink of RAM in this way, because of hardware/OS features such a s the\nTLB, accessing a particular page of memory may be costly, partic ularly\nif that page isn’t currently mapped by your TLB. Thus, it is alwa ys good\nto remember the implementation tip: RAM isn’t always RAM . Some-\ntimes randomly accessing your address space, particularly if the number\nof pages accessed exceeds the TLB coverage, can lead to severe p erfor-\nmance penalties. Because one of our advisors, David Culler, used to al-\nways point to the TLB as the source of many performance problems, we\nname this law in his honor: Culler’s Law .\ndescribed above). One question for you: what should the OS do if the re\nare more than 256 ( 28) processes running at a time? Finally, we see 3\nCoherence (C) bits, which determine how a page is cached by the hardware\n(a bit beyond the scope of these notes); a dirty bit which is marked when\nthe page has been written to (we’ll see the use of this later); a valid bit\nwhich tells the hardware if there is a valid translation prese nt in the entry.\nThere is also a page mask ﬁeld (not shown), which supports multiple page\nsizes; we’ll see later why having larger pages might be useful . Finally,\nsome of the 64 bits are unused (shaded gray in the diagram).\nMIPS TLBs usually have 32 or 64 of these entries, most of which are\nused by user processes as they run. However, a few are reserved f or the\nOS. A wired register can be set by the OS to tell the hardware how many\nslots of the TLB to reserve for the OS; the OS uses these reserved ma p-\npings for code and data that it wants to access during critical t imes, where\na TLB miss would be problematic (e.g., in the TLB miss handler).\nBecause the MIPS TLB is software managed, there needs to be ins truc-\ntions to update the TLB. The MIPS provides four such instructions :TLBP ,\nwhich probes the TLB to see if a particular translation is in the re;TLBR ,\nwhich reads the contents of a TLB entry into registers; TLBWI , which re-\nplaces a speciﬁc TLB entry; and TLBWR , which replaces a random TLB\nentry. The OS uses these instructions to manage the TLB’s conten ts. It is\nof course critical that these instructions are privileged ; imagine what a\nuser process could do if it could modify the contents of the TLB (hint : just\nabout anything, including take over the machine, run its own mal icious\n“OS”, or even make the Sun disappear).\n19.8 Summary\nWe have seen how hardware can help us make address translation\nfaster. By providing a small, dedicated on-chip TLB as an addre ss-translation\ncache, most memory references will hopefully be handled without having\nto access the page table in main memory. Thus, in the common case,\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nPAGING : FASTER TRANSLATIONS (TLB S) 13\nthe performance of the program will be almost as if memory isn’t bein g\nvirtualized at all, an excellent achievement for an operating system, and\ncertainly essential to the use of paging in modern systems.\nHowever, TLBs do not make the world rosy for every program that\nexists. In particular, if the number of pages a program accesse s in a short\nperiod of time exceeds the number of pages that ﬁt into the TLB, th e pro-\ngram will generate a large number of TLB misses, and thus run qu ite a\nbit more slowly. We refer to this phenomenon as exceeding the TLB cov-\nerage , and it can be quite a problem for certain programs. One solution,\nas we’ll discuss in the next chapter, is to include support for la rger page\nsizes; by mapping key data structures into regions of the progra m’s ad-\ndress space that are mapped by larger pages, the effective cov erage of the\nTLB can be increased. Support for large pages is often exploited by pro-\ngrams such as a database management system (aDBMS ), which have\ncertain data structures that are both large and randomly-acce ssed.\nOne other TLB issue worth mentioning: TLB access can easily be-\ncome a bottleneck in the CPU pipeline, in particular with what i s called a\nphysically-indexed cache . With such a cache, address translation has to\ntake place before the cache is accessed, which can slow things down quite\na bit. Because of this potential problem, people have looked into al l sorts\nof clever ways to access caches with virtual addresses, thus avoiding the\nexpensive step of translation in the case of a cache hit. Such a virtually-\nindexed cache solves some performance problems, but introduces new\nissues into hardware design as well. See Wiggins’s ﬁne survey f or more\ndetails [W03].\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 P AGING : FASTER TRANSLATIONS (TLB S)\nReferences\n[BC91] “Performance from Architecture: Comparing a RISC and a CISC with S imilar Hard-\nware Organization” by D. Bhandarkar and Douglas W. Clark. Communicat ions of the ACM,\nSeptember 1991. A great and fair comparison between RISC and CISC. The bottom line: on similar\nhardware, RISC was about a factor of three better in performance.\n[CM00] “The evolution of RISC technology at IBM” by John Cocke, V . Markstei n. IBM Journal\nof Research and Development, 44:1/2. A summary of the ideas and work behind the IBM 801, which\nmany consider the ﬁrst true RISC microprocessor.\n[C95] “The Core of the Black Canyon Computer Corporation” by John Coule ur. IEEE Annals\nof History of Computing, 17:4, 1995. In this fascinating historical note, Couleur talks about how he\ninvented the TLB in 1964 while working for GE, and the fortuitous collaboration th at thus ensued with\nthe Project MAC folks at MIT.\n[CG68] “Shared-access Data Processing System” by John F. Couleur, Edwa rd L. Glaser. Patent\n3412382, November 1968. The patent that contains the idea for an associative memory to store address\ntranslations. The idea, according to Couleur, came in 1964.\n[CP78] “The architecture of the IBM System/370” by R.P . Case, A. Padegs . Communications of\nthe ACM. 21:1, 73-96, January 1978. Perhaps the ﬁrst paper to use the term translation lookaside\nbuffer . The name arises from the historical name for a cache, which was a lookaside buffer as called by\nthose developing the Atlas system at the University of Manchester; a cache of ad dress translations thus\nbecame a translation lookaside buffer . Even though the term lookaside buffer fell out of favor, TLB\nseems to have stuck, for whatever reason.\n[H93] “MIPS R4000 Microprocessor User’s Manual”. by Joe Heinrich. Prentice-H all, June 1993.\nAvailable: http://cag.csail.mit.edu/raw/ . documents/R4400 Uman book Ed2.pdf A manual,\none that is surprisingly readable. Or is it?\n[HP06] “Computer Architecture: A Quantitative Approach” by John Hennessy and David Pat-\nterson. Morgan-Kaufmann, 2006. A great book about computer architecture. We have a particular\nattachment to the classic ﬁrst edition.\n[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals” by Intel, 2009. Avail-\nable: http://www.intel.com/products/processor/manuals. In particular, pay attention to “Vol-\nume 3A: System Programming Guide” Part 1 and “Volume 3B: System Programming Guide Part\n2”.\n[PS81] “RISC-I: A Reduced Instruction Set VLSI Computer” by D.A. Pat terson and C.H. Se-\nquin. ISCA ’81, Minneapolis, May 1981. The paper that introduced the term RISC, and started the\navalanche of research into simplifying computer chips for performance.\n[SB92] “CPU Performance Evaluation and Execution Time Prediction Using Narrow Spectrum\nBenchmarking” by Rafael H. Saavedra-Barrera. EECS Department, Univ ersity of California,\nBerkeley. Technical Report No. UCB/CSD-92-684, February 1992.. A great dissertation about how\nto predict execution time of applications by breaking them down into constituen t pieces and knowing the\ncost of each piece. Probably the most interesting part that comes out of this work is the tool to measure\ndetails of the cache hierarchy (described in Chapter 5). Make sure to check ou t the wonderful diagrams\ntherein.\n[W03] “A Survey on the Interaction Between Caching, Translation and Pro tection” by Adam\nWiggins. University of New South Wales TR UNSW-CSE-TR-0321, August , 2003. An excellent\nsurvey of how TLBs interact with other parts of the CPU pipeline, namely hard ware caches.\n[WG00] “The SPARC Architecture Manual: Version 9” by David L. Weaver a nd Tom Germond.\nSPARC International, San Jose, California, September 2000. Avail able:www.sparc.org/\nstandards/SPARCV9.pdf .Another manual. I bet you were hoping for a more fun citation to\nend this chapter.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nPAGING : FASTER TRANSLATIONS (TLB S) 15\nHomework (Measurement)\nIn this homework, you are to measure the size and cost of accessing\na TLB. The idea is based on work by Saavedra-Barrera [SB92], who de -\nveloped a simple but beautiful method to measure numerous aspec ts of\ncache hierarchies, all with a very simple user-level program . Read his\nwork for more details.\nThe basic idea is to access some number of pages within a large da ta\nstructure (e.g., an array) and to time those accesses. For exam ple, let’s say\nthe TLB size of a machine happens to be 4 (which would be very smal l,\nbut useful for the purposes of this discussion). If you write a progr am\nthat touches 4 or fewer pages, each access should be a TLB hit, and thus\nrelatively fast. However, once you touch 5 pages or more, repeatedl y in a\nloop, each access will suddenly jump in cost, to that of a TLB miss.\nThe basic code to loop through an array once should look like this:\nint jump = PAGESIZE / sizeof(int);\nfor (i = 0; i < NUMPAGES *jump; i += jump) {\na[i] += 1;\n}\nIn this loop, one integer per page of the array ais updated, up to the\nnumber of pages speciﬁed by NUMPAGES . By timing such a loop repeat-\nedly (say, a few hundred million times in another loop around this on e, or\nhowever many loops are needed to run for a few seconds), you can time\nhow long each access takes (on average). By looking for jumps in cost a s\nNUMPAGES increases, you can roughly determine how big the ﬁrst-level\nTLB is, determine whether a second-level TLB exists (and how bi g it is if\nit does), and in general get a good sense of how TLB hits and misses ca n\naffect performance.\nFigure 19.5 (page 16) shows the average time per access as the n umber\nof pages accessed in the loop is increased. As you can see in the gra ph,\nwhen just a few pages are accessed (8 or fewer), the average acc ess time\nis roughly 5 nanoseconds. When 16 or more pages are accessed, there is\na sudden jump to about 20 nanoseconds per access. A ﬁnal jump in cos t\noccurs at around 1024 pages, at which point each access takes arou nd 70\nnanoseconds. From this data, we can conclude that there is a two-le vel\nTLB hierarchy; the ﬁrst is quite small (probably holding betwe en 8 and\n16 entries); the second is larger but slower (holding roughly 512 entries).\nThe overall difference between hits in the ﬁrst-level TLB and misses is\nquite large, roughly a factor of fourteen. TLB performance matter s!\nQuestions\n1. For timing, you’ll need to use a timer (e.g., gettimeofday() ). How pre-\ncise is such a timer? How long does an operation have to take in or der for\nyou to time it precisely? (this will help determine how many times , in a\nloop, you’ll have to repeat a page access in order to time it succe ssfully)\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 P AGING : FASTER TRANSLATIONS (TLB S)\n1 4 16 64 256 1024020406080TLB Size Measurement\nNumber Of PagesTime Per Access (ns)\nFigure 19.5: Discovering TLB Sizes and Miss Costs\n2. Write the program, called tlb.c , that can roughly measure the cost of ac-\ncessing each page. Inputs to the program should be: the number of p ages to\ntouch and the number of trials.\n3. Now write a script in your favorite scripting language (csh, python, etc.) to\nrun this program, while varying the number of pages accessed from 1 up to\na few thousand, perhaps incrementing by a factor of two per iter ation. Run\nthe script on different machines and gather some data. How many t rials are\nneeded to get reliable measurements?\n4. Next, graph the results, making a graph that looks similar to th e one above.\nUse a good tool like ploticus or evenzplot . Visualization usually makes\nthe data much easier to digest; why do you think that is?\n5. One thing to watch out for is compiler optimization. Compilers do all sorts\nof clever things, including removing loops which increment val ues that no\nother part of the program subsequently uses. How can you ensure th e com-\npiler does not remove the main loop above from your TLB size estima tor?\n6. Another thing to watch out for is the fact that most systems toda y ship with\nmultiple CPUs, and each CPU, of course, has its own TLB hierarchy. T o\nreally get good measurements, you have to run your code on just one CP U,\ninstead of letting the scheduler bounce it from one CPU to the ne xt. How\ncan you do that? (hint: look up “pinning a thread” on Google for some\nclues) What will happen if you don’t do this, and the code moves f rom one\nCPU to the other?\n7. Another issue that might arise relates to initialization. If you don’t initialize\nthe array aabove before accessing it, the ﬁrst time you access it will be\nvery expensive, due to initial access costs such as demand zeroin g. Will this\naffect your code and its timing? What can you do to counterbalanc e these\npotential costs?\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG",41496
24-20. Advanced Page Tables.pdf,24-20. Advanced Page Tables,"20\nPaging: Smaller Tables\nWe now tackle the second problem that paging introduces: page tab les\nare too big and thus consume too much memory. Let’s start out with\na linear page table. As you might recall1, linear page tables get pretty\nbig. Assume again a 32-bit address space ( 232bytes), with 4KB ( 212byte)\npages and a 4-byte page-table entry. An address space thus ha s roughly\none million virtual pages in it (232\n212); multiply by the page-table entry size\nand you see that our page table is 4MB in size. Recall also: we usua lly\nhave one page table for every process in the system! With a hundred active\nprocesses (not uncommon on a modern system), we will be allocating\nhundreds of megabytes of memory just for page tables! As a result, we\nare in search of some techniques to reduce this heavy burden. Th ere are\na lot of them, so let’s get going. But not before our crux:\nCRUX: HOWTOMAKE PAGE TABLES SMALLER ?\nSimple array-based page tables (usually called linear page t ables) are\ntoo big, taking up far too much memory on typical systems. How can we\nmake page tables smaller? What are the key ideas? What inefﬁc iencies\narise as a result of these new data structures?\n20.1 Simple Solution: Bigger Pages\nWe could reduce the size of the page table in one simple way: use\nbigger pages. Take our 32-bit address space again, but this ti me assume\n16KB pages. We would thus have an 18-bit VPN plus a 14-bit offset . As-\nsuming the same size for each PTE (4 bytes), we now have 218entries in\nour linear page table and thus a total size of 1MB per page table, a factor\n1Or indeed, you might not; this paging thing is getting out of control, no? That said,\nalways make sure you understand the problem you are solving before moving onto the solution;\nindeed, if you understand the problem, you can often derive the solut ion yourself. Here, the\nproblem should be clear: simple linear (array-based) page tables are too big.\n1\n2 PAGING : SMALLER TABLES\nASIDE : M ULTIPLE PAGE SIZES\nAs an aside, do note that many architectures (e.g., MIPS, SPARC, x86-64)\nnow support multiple page sizes. Usually, a small (4KB or 8KB) pa ge\nsize is used. However, if a “smart” application requests it, a s ingle large\npage (e.g., of size 4MB) can be used for a speciﬁc portion of the addr ess\nspace, enabling such applications to place a frequently-use d (and large)\ndata structure in such a space while consuming only a single TLB en-\ntry. This type of large page usage is common in database manageme nt\nsystems and other high-end commercial applications. The main r eason\nfor multiple page sizes is not to save page table space, however; it is to\nreduce pressure on the TLB, enabling a program to access more of it s ad-\ndress space without suffering from too many TLB misses. However, as\nresearchers have shown [N+02], using multiple page sizes mak es the OS\nvirtual memory manager notably more complex, and thus large page s\nare sometimes most easily used simply by exporting a new interfa ce to\napplications to request large pages directly.\nof four reduction in size of the page table (not surprisingly, the r eduction\nexactly mirrors the factor of four increase in page size).\nThe major problem with this approach, however, is that big pages l ead\nto waste within each page, a problem known as internal fragmentation\n(as the waste is internal to the unit of allocation). Applications thus end\nup allocating pages but only using little bits and pieces of each , and mem-\nory quickly ﬁlls up with these overly-large pages. Thus, most sy stems use\nrelatively small page sizes in the common case: 4KB (as in x86) or 8KB (as\nin SPARCv9). Our problem will not be solved so simply, alas.\n20.2 Hybrid Approach: Paging and Segments\nWhenever you have two reasonable but different approaches to som e-\nthing in life, you should always examine the combination of the two to\nsee if you can obtain the best of both worlds. We call such a combinati on a\nhybrid . For example, why eat just chocolate or plain peanut butter when\nyou can instead combine the two in a lovely hybrid known as the Rees e’s\nPeanut Butter Cup [M28]?\nYears ago, the creators of Multics (in particular Jack Dennis) c hanced\nupon such an idea in the construction of the Multics virtual memory sys-\ntem [M07]. Speciﬁcally, Dennis had the idea of combining paging and\nsegmentation in order to reduce the memory overhead of page tables .\nWe can see why this might work by examining a typical linear pag e ta-\nble in more detail. Assume we have an address space in which the used\nportions of the heap and stack are small. For the example, we use a t iny\n16KB address space with 1KB pages (Figure 20.1); the page tab le for this\naddress space is in Figure 20.2.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : SMALLER TABLES 3\ncode\nheap\nstackVirtual Address Space Physical Memory\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n150\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nFigure 20.1: A 16KB Address Space With 1KB Pages\nThis example assumes the single code page (VPN 0) is mapped to\nphysical page 10, the single heap page (VPN 4) to physical pag e 23, and\nthe two stack pages at the other end of the address space (VPNs 14 and\n15) are mapped to physical pages 28 and 4, respectively. As you can see\nfrom the picture, most of the page table is unused, full of invalid entries.\nWhat a waste! And this is for a tiny 16KB address space. Imagine the\npage table of a 32-bit address space and all the potential waste d space in\nthere! Actually, don’t imagine such a thing; it’s far too gruesome .\nPFN valid prot present dirty\n10 1 r-x 1 0\n- 0 — - -\n- 0 — - -\n- 0 — - -\n23 1 rw- 1 1\n- 0 — - -\n- 0 — - -\n- 0 — - -\n- 0 — - -\n- 0 — - -\n- 0 — - -\n- 0 — - -\n- 0 — - -\n- 0 — - -\n28 1 rw- 1 1\n4 1 rw- 1 1\nFigure 20.2: A Page Table For 16KB Address Space\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 PAGING : SMALLER TABLES\nThus, our hybrid approach: instead of having a single page table for\nthe entire address space of the process, why not have one per logica l seg-\nment? In this example, we might thus have three page tables, on e for the\ncode, heap, and stack parts of the address space.\nNow, remember with segmentation, we had a base register that told\nus where each segment lived in physical memory, and a bound orlimit\nregister that told us the size of said segment. In our hybrid, we s till have\nthose structures in the MMU; here, we use the base not to point to t he\nsegment itself but rather to hold the physical address of the page table of that\nsegment. The bounds register is used to indicate the end of the p age table\n(i.e., how many valid pages it has).\nLet’s do a simple example to clarify. Assume a 32-bit virtual a ddress\nspace with 4KB pages, and an address space split into four segm ents.\nWe’ll only use three segments for this example: one for code, one for\nheap, and one for stack.\nTo determine which segment an address refers to, we’ll use the t op\ntwo bits of the address space. Let’s assume 00 is the unused segm ent,\nwith 01 for code, 10 for the heap, and 11 for the stack. Thus, a virtu al\naddress looks like this:\n31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0\nSeg VPN Offset\nIn the hardware, assume that there are thus three base/bounds pairs,\none each for code, heap, and stack. When a process is running, the b ase\nregister for each of these segments contains the physical addre ss of a lin-\near page table for that segment; thus, each process in the syste m now has\nthree page tables associated with it. On a context switch, these regi sters\nmust be changed to reﬂect the location of the page tables of the new ly-\nrunning process.\nOn a TLB miss (assuming a hardware-managed TLB, i.e., where t he\nhardware is responsible for handling TLB misses), the hardwar e uses the\nsegment bits ( SN) to determine which base and bounds pair to use. The\nhardware then takes the physical address therein and combine s it with\nthe VPN as follows to form the address of the page table entry (PTE) :\nSN = (VirtualAddress & SEG_MASK) >> SN_SHIFT\nVPN = (VirtualAddress & VPN_MASK) >> VPN_SHIFT\nAddressOfPTE = Base[SN] + (VPN *sizeof(PTE))\nThis sequence should look familiar; it is virtually identical t o what we\nsaw before with linear page tables. The only difference, of cours e, is the\nuse of one of three segment base registers instead of the single pa ge table\nbase register.\nThe critical difference in our hybrid scheme is the presence of a bounds\nregister per segment; each bounds register holds the value of th e maxi-\nmum valid page in the segment. For example, if the code segment i s\nusing its ﬁrst three pages (0, 1, and 2), the code segment page t able will\nonly have three entries allocated to it and the bounds register w ill be set\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : SMALLER TABLES 5\nTIP: USEHYBRIDS\nWhen you have two good and seemingly opposing ideas, you should\nalways see if you can combine them into a hybrid that manages to achieve\nthe best of both worlds. Hybrid corn species, for example, are known to\nbe more robust than any naturally-occurring species. Of course, not all\nhybrids are a good idea; see the Zeedonk (or Zonkey), which is a cross of\na Zebra and a Donkey. If you don’t believe such a creature exists, l ook it\nup, and prepare to be amazed.\nto 3; memory accesses beyond the end of the segment will generate an ex-\nception and likely lead to the termination of the process. In this manner,\nour hybrid approach realizes a signiﬁcant memory savings compar ed to\nthe linear page table; unallocated pages between the stack an d the heap\nno longer take up space in a page table (just to mark them as not va lid).\nHowever, as you might notice, this approach is not without problems.\nFirst, it still requires us to use segmentation; as we discuss ed before, seg-\nmentation is not quite as ﬂexible as we would like, as it assumes a certain\nusage pattern of the address space; if we have a large but spars ely-used\nheap, for example, we can still end up with a lot of page table wast e.\nSecond, this hybrid causes external fragmentation to arise aga in. While\nmost of memory is managed in page-sized units, page tables now can be\nof arbitrary size (in multiples of PTEs). Thus, ﬁnding free spa ce for them\nin memory is more complicated. For these reasons, people continued t o\nlook for better ways to implement smaller page tables.\n20.3 Multi-level Page Tables\nA different approach doesn’t rely on segmentation but attacks the same\nproblem: how to get rid of all those invalid regions in the page tabl e in-\nstead of keeping them all in memory? We call this approach a multi-level\npage table , as it turns the linear page table into something like a tree. T his\napproach is so effective that many modern systems employ it (e.g ., x86\n[BOH10]). We now describe this approach in detail.\nThe basic idea behind a multi-level page table is simple. Fir st, chop up\nthe page table into page-sized units; then, if an entire page of page-table\nentries (PTEs) is invalid, don’t allocate that page of the page ta ble at all.\nTo track whether a page of the page table is valid (and if valid, where it\nis in memory), use a new structure, called the page directory . The page\ndirectory thus either can be used to tell you where a page of the pa ge\ntable is, or that the entire page of the page table contains no val id pages.\nFigure 20.3 shows an example. On the left of the ﬁgure is the clas sic\nlinear page table; even though most of the middle regions of the add ress\nspace are not valid, we still require page-table space allocat ed for those\nregions (i.e., the middle two pages of the page table). On the ri ght is a\nmulti-level page table. The page directory marks just two pag es of the\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 PAGING : SMALLER TABLESvalidprotPFN\n1 rx 12\n1 rx 13\n0 - -\n1 rw 100\n0 - -\n0 - -\n0 - -\n0 - -\n0 - -\n0 - -\n0 - -\n0 - -\n0 - -\n0 - -\n1 rw 86\n1 rw 15Linear Page Table\nPTBR 201\nPFN 201 PFN 202 PFN 203 PFN 204\nvalidprotPFN\n1 rx 12\n1 rx 13\n0 - -\n1 rw 100\n0 - -\n0 - -\n1 rw 86\n1 rw 15[Page 1 of PT: Not Allocated]\n[Page 2 of PT: Not Allocated]\nPFN 201 PFN 204Multi-level Page Table\nPDBR 200\nvalid PFN\n1 201\n0 -\n0 -\n1 204PFN 200\nThe Page Directory\nFigure 20.3: Linear (Left) And Multi-Level (Right) Page Tables\npage table as valid (the ﬁrst and last); thus, just those two pa ges of the\npage table reside in memory. And thus you can see one way to visual ize\nwhat a multi-level table is doing: it just makes parts of the lin ear page\ntable disappear (freeing those frames for other uses), and trac ks which\npages of the page table are allocated with the page directory.\nThe page directory, in a simple two-level table, contains one ent ry per\npage of the page table. It consists of a number of page directory entries\n(PDE ). A PDE (minimally) has a valid bit and a page frame number\n(PFN), similar to a PTE. However, as hinted at above, the meanin g of\nthis valid bit is slightly different: if the PDE is valid, it m eans that at least\none of the pages of the page table that the entry points to (via the P FN)\nis valid, i.e., in at least one PTE on that page pointed to by this P DE, the\nvalid bit in that PTE is set to one. If the PDE is not valid (i.e., e qual to\nzero), the rest of the PDE is not deﬁned.\nMulti-level page tables have some obvious advantages over approa ches\nwe’ve seen thus far. First, and perhaps most obviously, the multi -level ta-\nble only allocates page-table space in proportion to the amount of ad dress\nspace you are using; thus it is generally compact and supports sp arse ad-\ndress spaces.\nSecond, if carefully constructed, each portion of the page table ﬁt s\nneatly within a page, making it easier to manage memory; the OS can\nsimply grab the next free page when it needs to allocate or grow a p age\ntable. Contrast this to a simple (non-paged) linear page table2, which\nis just an array of PTEs indexed by VPN; with such a structure, t he en-\ntire linear page table must reside contiguously in physical me mory. For\na large page table (say 4MB), ﬁnding such a large chunk of unuse d con-\ntiguous free physical memory can be quite a challenge. With a mu lti-level\n2We are making some assumptions here, i.e., that all page tables re side in their entirety in\nphysical memory (i.e., they are not swapped to disk); we’ll soon rel ax this assumption.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : SMALLER TABLES 7\nTIP: UNDERSTAND TIME-SPACE TRADE -OFFS\nWhen building a data structure, one should always consider time-space\ntrade-offs in its construction. Usually, if you wish to make access to a par-\nticular data structure faster, you will have to pay a space-us age penalty\nfor the structure.\nstructure, we add a level of indirection through use of the page directory,\nwhich points to pieces of the page table; that indirection allows us to place\npage-table pages wherever we would like in physical memory.\nIt should be noted that there is a cost to multi-level tables; on a T LB\nmiss, two loads from memory will be required to get the right tran slation\ninformation from the page table (one for the page directory, and one f or\nthe PTE itself), in contrast to just one load with a linear page ta ble. Thus,\nthe multi-level table is a small example of a time-space trade-off . We\nwanted smaller tables (and got them), but not for free; although i n the\ncommon case (TLB hit), performance is obviously identical, a TLB m iss\nsuffers from a higher cost with this smaller table.\nAnother obvious negative is complexity . Whether it is the hardware or\nOS handling the page-table lookup (on a TLB miss), doing so is undou bt-\nedly more involved than a simple linear page-table lookup. Often we are\nwilling to increase complexity in order to improve performance or reduce\noverheads; in the case of a multi-level table, we make page-tab le lookups\nmore complicated in order to save valuable memory.\nA Detailed Multi-Level Example\nTo understand the idea behind multi-level page tables bette r, let’s do an\nexample. Imagine a small address space of size 16KB, with 64-b yte pages.\nThus, we have a 14-bit virtual address space, with 8 bits for th e VPN and\n6 bits for the offset. A linear page table would have 28(256) entries, even\nif only a small portion of the address space is in use. Figure 20.4 p resents\none example of such an address space.\nstackstack(free)(free)... all free ...(free)(free)heapheap(free)(free)codecode\n1111 11111111 11101111 11011111 11000000 01110000 01100000 01010000 01000000 00110000 00100000 00010000 0000\n................\nFigure 20.4: A 16KB Address Space With 64-byte Pages\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 PAGING : SMALLER TABLES\nTIP: BEWARY OFCOMPLEXITY\nSystem designers should be wary of adding complexity into their s ys-\ntem. What a good systems builder does is implement the least compl ex\nsystem that achieves the task at hand. For example, if disk spa ce is abun-\ndant, you shouldn’t design a ﬁle system that works hard to use as fe w\nbytes as possible; similarly, if processors are fast, it is bett er to write a\nclean and understandable module within the OS than perhaps th e most\nCPU-optimized, hand-assembled code for the task at hand. Be war y of\nneedless complexity, in prematurely-optimized code or other form s; such\napproaches make systems harder to understand, maintain, and debug.\nAs Antoine de Saint-Exupery famously wrote: “Perfection is ﬁnall y at-\ntained not when there is no longer anything to add, but when ther e is no\nlonger anything to take away.” What he didn’t write: “It’s a lot ea sier to\nsay something about perfection than to actually achieve it.”\nIn this example, virtual pages 0 and 1 are for code, virtual page s 4 and\n5 for the heap, and virtual pages 254 and 255 for the stack; the re st of the\npages of the address space are unused.\nTo build a two-level page table for this address space, we start with\nour full linear page table and break it up into page-sized unit s. Recall our\nfull table (in this example) has 256 entries; assume each PTE is 4 bytes\nin size. Thus, our page table is 1KB (256 ×4 bytes) in size. Given that\nwe have 64-byte pages, the 1KB page table can be divided into 1 6 64-byte\npages; each page can hold 16 PTEs.\nWhat we need to understand now is how to take a VPN and use it to\nindex ﬁrst into the page directory and then into the page of the p age table.\nRemember that each is an array of entries; thus, all we need to ﬁ gure out\nis how to construct the index for each from pieces of the VPN.\nLet’s ﬁrst index into the page directory. Our page table in this example\nis small: 256 entries, spread across 16 pages. The page direct ory needs one\nentry per page of the page table; thus, it has 16 entries. As a re sult, we\nneed four bits of the VPN to index into the directory; we use the top four\nbits of the VPN, as follows:\n13 12 11 10 9 8 7 6 5 4 3 2 1 0VPN offset\nPage Directory Index\nOnce we extract the page-directory index (PDIndex for short) from\nthe VPN, we can use it to ﬁnd the address of the page-directory en try\n(PDE) with a simple calculation: PDEAddr = PageDirBase + (PDIndex\n*sizeof(PDE)) . This results in our page directory, which we now ex-\namine to make further progress in our translation.\nIf the page-directory entry is marked invalid, we know that the access\nis invalid, and thus raise an exception. If, however, the PDE is valid,\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : SMALLER TABLES 9\nwe have more work to do. Speciﬁcally, we now have to fetch the page-\ntable entry (PTE) from the page of the page table pointed to by thi s page-\ndirectory entry. To ﬁnd this PTE, we have to index into the porti on of the\npage table using the remaining bits of the VPN:\n13 12 11 10 9 8 7 6 5 4 3 2 1 0VPN offset\nPage Directory Index Page Table Index\nThis page-table index (PTIndex for short) can then be used to index\ninto the page table itself, giving us the address of our PTE:\nPTEAddr = (PDE.PFN << SHIFT) + (PTIndex *sizeof(PTE))\nNote that the page-frame number (PFN) obtained from the page-di rectory\nentry must be left-shifted into place before combining it with the page-\ntable index to form the address of the PTE.\nTo see if this all makes sense, we’ll now ﬁll in a multi-level pag e ta-\nble with some actual values, and translate a single virtual ad dress. Let’s\nbegin with the page directory for this example (left side of Figure 20.5).\nIn the ﬁgure, you can see that each page directory entry (PDE) de -\nscribes something about a page of the page table for the address sp ace.\nIn this example, we have two valid regions in the address space (at the\nbeginning and end), and a number of invalid mappings in-betwe en.\nIn physical page 100 (the physical frame number of the 0th page of the\npage table), we have the ﬁrst page of 16 page table entries for th e ﬁrst 16\nVPNs in the address space. See Figure 20.5 (middle part) for the contents\nof this portion of the page table.\nPage Directory Page of PT (@PFN:100) Page of PT (@PFN:101)\nPFN valid? PFN valid prot PFN valid prot\n100 1 10 1 r-x — 0 —\n— 0 23 1 r-x — 0 —\n— 0 — 0 — — 0 —\n— 0 — 0 — — 0 —\n— 0 80 1 rw- — 0 —\n— 0 59 1 rw- — 0 —\n— 0 — 0 — — 0 —\n— 0 — 0 — — 0 —\n— 0 — 0 — — 0 —\n— 0 — 0 — — 0 —\n— 0 — 0 — — 0 —\n— 0 — 0 — — 0 —\n— 0 — 0 — — 0 —\n— 0 — 0 — — 0 —\n— 0 — 0 — 55 1 rw-\n101 1 — 0 — 45 1 rw-\nFigure 20.5: A Page Directory, And Pieces Of Page Table\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 PAGING : SMALLER TABLES\nThis page of the page table contains the mappings for the ﬁrst 16\nVPNs; in our example, VPNs 0 and 1 are valid (the code segment), a s\nare 4 and 5 (the heap). Thus, the table has mapping information f or each\nof those pages. The rest of the entries are marked invalid.\nThe other valid page of the page table is found inside PFN 101. Thi s\npage contains mappings for the last 16 VPNs of the address space; see\nFigure 20.5 (right) for details.\nIn the example, VPNs 254 and 255 (the stack) have valid mappin gs.\nHopefully, what we can see from this example is how much space sav ings\nare possible with a multi-level indexed structure. In this ex ample, instead\nof allocating the full sixteen pages for a linear page table, we allocate only\nthree : one for the page directory, and two for the chunks of the page table\nthat have valid mappings. The savings for large (32-bit or 64-b it) address\nspaces could obviously be much greater.\nFinally, let’s use this information in order to perform a translat ion.\nHere is an address that refers to the 0th byte of VPN 254: 0x3F80 , or\n11 1111 1000 0000 in binary.\nRecall that we will use the top 4 bits of the VPN to index into the\npage directory. Thus, 1111 will choose the last (15th, if you start at the\n0th) entry of the page directory above. This points us to a valid pa ge\nof the page table located at address 101. We then use the next 4 bits\nof the VPN ( 1110 ) to index into that page of the page table and ﬁnd\nthe desired PTE. 1110 is the next-to-last (14th) entry on the pa ge, and\ntells us that page 254 of our virtual address space is mapped at p hysi-\ncal page 55. By concatenating PFN=55 (or hex 0x37 ) with offset=000000,\nwe can thus form our desired physical address and issue the requ est to\nthe memory system: PhysAddr = (PTE.PFN << SHIFT) + offset\n= 00 1101 1100 0000 = 0x0DC0 .\nYou should now have some idea of how to construct a two-level page\ntable, using a page directory which points to pages of the page ta ble. Un-\nfortunately, however, our work is not done. As we’ll now discuss, some-\ntimes two levels of page table is not enough!\nMore Than Two Levels\nIn our example thus far, we’ve assumed that multi-level page ta bles only\nhave two levels: a page directory and then pieces of the page tab le. In\nsome cases, a deeper tree is possible (and indeed, needed).\nLet’s take a simple example and use it to show why a deeper multi-\nlevel table can be useful. In this example, assume we have a 30 -bit virtual\naddress space, and a small (512 byte) page. Thus our virtual ad dress has\na 21-bit virtual page number component and a 9-bit offset.\nRemember our goal in constructing a multi-level page table: to m ake\neach piece of the page table ﬁt within a single page. Thus far, w e’ve only\nconsidered the page table itself; however, what if the page dir ectory gets\ntoo big?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : SMALLER TABLES 11\nTo determine how many levels are needed in a multi-level table to\nmake all pieces of the page table ﬁt within a page, we start by de termining\nhow many page-table entries ﬁt within a page. Given our page siz e of 512\nbytes, and assuming a PTE size of 4 bytes, you should see that you ca n ﬁt\n128 PTEs on a single page. When we index into a page of the page tab le,\nwe can thus conclude we’ll need the least signiﬁcant 7 bits ( log2128) of\nthe VPN as an index:\n29282726252423222120191817161514131211109876543210VPN offset\nPage Directory Index Page Table Index\nWhat you also might notice from the diagram above is how many bits\nare left into the (large) page directory: 14. If our page direct ory has214\nentries, it spans not one page but 128, and thus our goal of making ev ery\npiece of the multi-level page table ﬁt into a page vanishes.\nTo remedy this problem, we build a further level of the tree, by s plit-\nting the page directory itself into multiple pages, and then a dding another\npage directory on top of that, to point to the pages of the page direct ory.\nWe can thus split up our virtual address as follows:\n29282726252423222120191817161514131211109876543210VPN offset\nPD Index 0 PD Index 1 Page Table Index\nNow, when indexing the upper-level page directory, we use the v ery\ntop bits of the virtual address ( PD Index 0 in the diagram); this index\ncan be used to fetch the page-directory entry from the top-level page di-\nrectory. If valid, the second level of the page directory is consul ted by\ncombining the physical frame number from the top-level PDE and t he\nnext part of the VPN ( PD Index 1 ). Finally, if valid, the PTE address\ncan be formed by using the page-table index combined with the ad dress\nfrom the second-level PDE. Whew! That’s a lot of work. And all just to\nlook something up in a multi-level table.\nThe Translation Process: Remember the TLB\nTo summarize the entire process of address translation using a t wo-level\npage table, we once again present the control ﬂow in algorithmic for m\n(Figure 20.6). The ﬁgure shows what happens in hardware (assu ming a\nhardware-managed TLB) upon every memory reference.\nAs you can see from the ﬁgure, before any of the complicated multi-\nlevel page table access occurs, the hardware ﬁrst checks the T LB; upon\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 PAGING : SMALLER TABLES\n1VPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2(Success, TlbEntry) = TLB_Lookup(VPN)\n3if (Success == True) // TLB Hit\n4if (CanAccess(TlbEntry.ProtectBits) == True)\n5 Offset = VirtualAddress & OFFSET_MASK\n6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7 Register = AccessMemory(PhysAddr)\n8else\n9 RaiseException(PROTECTION_FAULT)\n10else // TLB Miss\n11 // first, get page directory entry\n12 PDIndex = (VPN & PD_MASK) >> PD_SHIFT\n13 PDEAddr = PDBR + (PDIndex *sizeof(PDE))\n14 PDE = AccessMemory(PDEAddr)\n15 if (PDE.Valid == False)\n16 RaiseException(SEGMENTATION_FAULT)\n17 else\n18 // PDE is valid: now fetch PTE from page table\n19 PTIndex = (VPN & PT_MASK) >> PT_SHIFT\n20 PTEAddr = (PDE.PFN << SHIFT) + (PTIndex *sizeof(PTE))\n21 PTE = AccessMemory(PTEAddr)\n22 if (PTE.Valid == False)\n23 RaiseException(SEGMENTATION_FAULT)\n24 else if (CanAccess(PTE.ProtectBits) == False)\n25 RaiseException(PROTECTION_FAULT)\n26 else\n27 TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n28 RetryInstruction()\nFigure 20.6: Multi-level Page Table Control Flow\na hit, the physical address is formed directly without accessing the page\ntable at all, as before. Only upon a TLB miss does the hardware nee d to\nperform the full multi-level lookup. On this path, you can see the cost of\nour traditional two-level page table: two additional memory acce sses to\nlook up a valid translation.\n20.4 Inverted Page Tables\nAn even more extreme space savings in the world of page tables is\nfound with inverted page tables . Here, instead of having many page\ntables (one per process of the system), we keep a single page tabl e that\nhas an entry for each physical page of the system. The entry tells us which\nprocess is using this page, and which virtual page of that proces s maps to\nthis physical page.\nFinding the correct entry is now a matter of searching through thi s\ndata structure. A linear scan would be expensive, and thus a ha sh table\nis often built over the base structure to speed up lookups. The Powe rPC\nis one example of such an architecture [JM98].\nMore generally, inverted page tables illustrate what we’ve sa id from\nthe beginning: page tables are just data structures. You can d o lots of\ncrazy things with data structures, making them smaller or big ger, making\nthem slower or faster. Multi-level and inverted page tables ar e just two\nexamples of the many things one could do.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : SMALLER TABLES 13\n20.5 Swapping the Page Tables to Disk\nFinally, we discuss the relaxation of one ﬁnal assumption. Thus f ar,\nwe have assumed that page tables reside in kernel-owned physi cal mem-\nory. Even with our many tricks to reduce the size of page tables, i t is still\npossible, however, that they may be too big to ﬁt into memory all at once.\nThus, some systems place such page tables in kernel virtual memory ,\nthereby allowing the system to swap some of these page tables to disk\nwhen memory pressure gets a little tight. We’ll talk more about th is in\na future chapter (namely, the case study on VAX/VMS), once we und er-\nstand how to move pages in and out of memory in more detail.\n20.6 Summary\nWe have now seen how real page tables are built; not necessarily j ust\nas linear arrays but as more complex data structures. The trade -offs such\ntables present are in time and space — the bigger the table, th e faster a\nTLB miss can be serviced, as well as the converse — and thus the r ight\nchoice of structure depends strongly on the constraints of the give n envi-\nronment.\nIn a memory-constrained system (like many older systems), smal l struc-\ntures make sense; in a system with a reasonable amount of memory an d\nwith workloads that actively use a large number of pages, a bigge r ta-\nble that speeds up TLB misses might be the right choice. With sof tware-\nmanaged TLBs, the entire space of data structures opens up to th e delight\nof the operating system innovator (hint: that’s you). What new stru c-\ntures can you come up with? What problems do they solve? Think of\nthese questions as you fall asleep, and dream the big dreams tha t only\noperating-system developers can dream.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 PAGING : SMALLER TABLES\nReferences\n[BOH10] “Computer Systems: A Programmer’s Perspective” by Randal E. Bryant and David\nR. O’Hallaron. Addison-Wesley, 2010. We have yet to ﬁnd a good ﬁrst reference to the multi-level\npage table. However, this great textbook by Bryant and O’Hallaron dives into the details of x86, which\nat least is an early system that used such structures. It’s also just a great b ook to have.\n[JM98] “Virtual Memory: Issues of Implementation” by Bruce Jacob, Tre vor Mudge. IEEE\nComputer, June 1998. An excellent survey of a number of different systems and their approac h to\nvirtualizing memory. Plenty of details on x86, PowerPC, MIPS, and other archi tectures.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System” by Hank Levy, P .\nLipman. IEEE Computer, Vol. 15, No. 3, March 1982. A terriﬁc paper about a real virtual memory\nmanager in a classic operating system, VMS. So terriﬁc, in fact, that we’ll use it to review everything\nwe’ve learned about virtual memory thus far a few chapters from now.\n[M28] “Reese’s Peanut Butter Cups” by Mars Candy Corporation. Publis hed at stores near\nyou. Apparently these ﬁne confections were invented in 1928 by Harry Burne tt Reese, a former dairy\nfarmer and shipping foreman for one Milton S. Hershey. At least, that is what it says on Wikipedia. If\ntrue, Hershey and Reese probably hate each other’s guts, as any two chocolate baron s should.\n[N+02] “Practical, Transparent Operating System Support for Superpa ges” by Juan Navarro,\nSitaram Iyer, Peter Druschel, Alan Cox. OSDI ’02, Boston, Massachuse tts, October 2002. A nice\npaper showing all the details you have to get right to incorporate large pages, or superpages , into a\nmodern OS. Not as easy as you might think, alas.\n[M07] “Multics: History” Available: http://www.multicians.org/history.html .This\namazing web site provides a huge amount of history on the Multics system, certai nly one of the most\ninﬂuential systems in OS history. The quote from therein: “Jack Dennis of M IT contributed inﬂuential\narchitectural ideas to the beginning of Multics, especially the idea of com bining paging and segmenta-\ntion.” (from Section 1.2.1)\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nPAGING : SMALLER TABLES 15\nHomework (Simulation)\nThis fun little homework tests if you understand how a multi-leve l\npage table works. And yes, there is some debate over the use of the t erm\n“fun” in the previous sentence. The program is called, perhaps unsur-\nprisingly: paging-multilevel-translate.py ; see the README for\ndetails.\nQuestions\n1. With a linear page table, you need a single register to loca te the page table,\nassuming that hardware does the lookup upon a TLB miss. How many\nregisters do you need to locate a two-level page table? A thre e-level table?\n2. Use the simulator to perform translations given random seeds 0, 1, and 2,\nand check your answers using the -cﬂag. How many memory references\nare needed to perform each lookup?\n3. Given your understanding of how cache memory works, how do you t hink\nmemory references to the page table will behave in the cache? Wi ll they lead\nto lots of cache hits (and thus fast accesses?) Or lots of misses (and thus slow\naccesses)?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",34864
25-21. Swapping Mechanisms.pdf,25-21. Swapping Mechanisms,"21\nBeyond Physical Memory: Mechanisms\nThus far, we’ve assumed that an address space is unrealistica lly small\nand ﬁts into physical memory. In fact, we’ve been assuming that every\naddress space of every running process ﬁts into memory. We will n ow\nrelax these big assumptions, and assume that we wish to support many\nconcurrently-running large address spaces.\nTo do so, we require an additional level in the memory hierarchy .\nThus far, we have assumed that all pages reside in physical me mory.\nHowever, to support large address spaces, the OS will need a pla ce to\nstash away portions of address spaces that currently aren’t in gr eat de-\nmand. In general, the characteristics of such a location are tha t it should\nhave more capacity than memory; as a result, it is generally slow er (if it\nwere faster, we would just use it as memory, no?). In modern system s,\nthis role is usually served by a hard disk drive . Thus, in our memory\nhierarchy, big and slow hard drives sit at the bottom, with memory just\nabove. And thus we arrive at the crux of the problem:\nTHECRUX: HOWTOGOBEYOND PHYSICAL MEMORY\nHow can the OS make use of a larger, slower device to transparentl y pro-\nvide the illusion of a large virtual address space?\nOne question you might have: why do we want to support a single\nlarge address space for a process? Once again, the answer is conv enience\nand ease of use. With a large address space, you don’t have to worry\nabout if there is room enough in memory for your program’s data struc-\ntures; rather, you just write the program naturally, allocatin g memory as\nneeded. It is a powerful illusion that the OS provides, and makes your\nlife vastly simpler. You’re welcome! A contrast is found in older sy stems\nthat used memory overlays , which required programmers to manually\nmove pieces of code or data in and out of memory as they were needed\n[D97]. Try imagining what this would be like: before calling a f unction or\naccessing some data, you need to ﬁrst arrange for the code or data to be\nin memory; yuck!\n1\n2 BEYOND PHYSICAL MEMORY : M ECHANISMS\nASIDE : STORAGE TECHNOLOGIES\nWe’ll delve much more deeply into how I/O devices actually work la ter\n(see the chapter on I/O devices). So be patient! And of course the s lower\ndevice need not be a hard disk, but could be something more modern\nsuch as a Flash-based SSD. We’ll talk about those things too. For now,\njust assume we have a big and relatively-slow device which we c an use\nto help us build the illusion of a very large virtual memory, even bigger\nthan physical memory itself.\nBeyond just a single process, the addition of swap space allows the OS\nto support the illusion of a large virtual memory for multiple concu rrently-\nrunning processes. The invention of multiprogramming (running multi-\nple programs “at once”, to better utilize the machine) almost de manded\nthe ability to swap out some pages, as early machines clearly cou ld not\nhold all the pages needed by all processes at once. Thus, the combi na-\ntion of multiprogramming and ease-of-use leads us to want to supp ort\nusing more memory than is physically available. It is something that all\nmodern VM systems do; it is now something we will learn more about.\n21.1 Swap Space\nThe ﬁrst thing we will need to do is to reserve some space on the di sk\nfor moving pages back and forth. In operating systems, we general ly refer\nto such space as swap space , because we swap pages out of memory to it\nand swap pages into memory from it. Thus, we will simply assume that\nthe OS can read from and write to the swap space, in page-sized u nits. To\ndo so, the OS will need to remember the disk address of a given page.\nThe size of the swap space is important, as ultimately it determ ines\nthe maximum number of memory pages that can be in use by a system a t\na given time. Let us assume for simplicity that it is very large for now.\nIn the tiny example (Figure 21.1), you can see a little example of a 4-\npage physical memory and an 8-page swap space. In the example, three\nprocesses (Proc 0, Proc 1, and Proc 2) are actively sharing physic al mem-\nory; each of the three, however, only have some of their valid pages i n\nmemory, with the rest located in swap space on disk. A fourth proces s\n(Proc 3) has all of its pages swapped out to disk, and thus clearly isn’t\ncurrently running. One block of swap remains free. Even from thi s tiny\nexample, hopefully you can see how using swap space allows the sys tem\nto pretend that memory is larger than it actually is.\nWe should note that swap space is not the only on-disk location for\nswapping trafﬁc. For example, assume you are running a program b inary\n(e.g.,ls, or your own compiled main program). The code pages from this\nbinary are initially found on disk, and when the program runs, th ey are\nloaded into memory (either all at once when the program starts exe cution,\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : M ECHANISMS 3\nPhysical\nMemoryPFN 0\nProc 0\n[VPN 0]PFN 1\nProc 1\n[VPN 2]PFN 2\nProc 1\n[VPN 3]PFN 3\nProc 2\n[VPN 0]\nSwap\nSpaceProc 0\n[VPN 1]Block 0\nProc 0\n[VPN 2]Block 1\n[Free]Block 2\nProc 1\n[VPN 0]Block 3\nProc 1\n[VPN 1]Block 4\nProc 3\n[VPN 0]Block 5\nProc 2\n[VPN 1]Block 6\nProc 3\n[VPN 1]Block 7\nFigure 21.1: Physical Memory and Swap Space\nor, as in modern systems, one page at a time when needed). However, if\nthe system needs to make room in physical memory for other needs, it\ncan safely re-use the memory space for these code pages, knowing t hat it\ncan later swap them in again from the on-disk binary in the ﬁle sy stem.\n21.2 The Present Bit\nNow that we have some space on the disk, we need to add some ma-\nchinery higher up in the system in order to support swapping pag es to\nand from the disk. Let us assume, for simplicity, that we have a s ystem\nwith a hardware-managed TLB.\nRecall ﬁrst what happens on a memory reference. The running pro-\ncess generates virtual memory references (for instruction fet ches, or data\naccesses), and, in this case, the hardware translates them i nto physical\naddresses before fetching the desired data from memory.\nRemember that the hardware ﬁrst extracts the VPN from the virt ual\naddress, checks the TLB for a match (a TLB hit ), and if a hit, produces the\nresulting physical address and fetches it from memory. This is hopefully\nthe common case, as it is fast (requiring no additional memory acc esses).\nIf the VPN is not found in the TLB (i.e., a TLB miss ), the hardware\nlocates the page table in memory (using the page table base register )\nand looks up the page table entry (PTE) for this page using the VPN\nas an index. If the page is valid and present in physical memory , the\nhardware extracts the PFN from the PTE, installs it in the TLB, and retries\nthe instruction, this time generating a TLB hit; so far, so good.\nIf we wish to allow pages to be swapped to disk, however, we must\nadd even more machinery. Speciﬁcally, when the hardware looks in the\nPTE, it may ﬁnd that the page is not present in physical memory. The way\nthe hardware (or the OS, in a software-managed TLB approach) dete r-\nmines this is through a new piece of information in each page-tabl e entry,\nknown as the present bit . If the present bit is set to one, it means the\npage is present in physical memory and everything proceeds as a bove; if\nit is set to zero, the page is notin memory but rather on disk somewhere.\nThe act of accessing a page that is not in physical memory is commonl y\nreferred to as a page fault .\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 BEYOND PHYSICAL MEMORY : M ECHANISMS\nASIDE : SWAPPING TERMINOLOGY ANDOTHER THINGS\nTerminology in virtual memory systems can be a little confusing a nd vari-\nable across machines and operating systems. For example, a page fault\nmore generally could refer to any reference to a page table that generates\na fault of some kind: this could include the type of fault we are dis cussing\nhere, i.e., a page-not-present fault, but sometimes can refer to illegal mem-\nory accesses. Indeed, it is odd that we call what is deﬁnitely a l egal access\n(to a page mapped into the virtual address space of a process, bu t simply\nnot in physical memory at the time) a “fault” at all; really, it s hould be\ncalled a page miss . But often, when people say a program is “page fault-\ning”, they mean that it is accessing parts of its virtual addre ss space that\nthe OS has swapped out to disk.\nWe suspect the reason that this behavior became known as a “fault ” re-\nlates to the machinery in the operating system to handle it. Wh en some-\nthing unusual happens, i.e., when something the hardware does n’t know\nhow to handle occurs, the hardware simply transfers control to th e OS,\nhoping it can make things better. In this case, a page that a proc ess wants\nto access is missing from memory; the hardware does the only thing it\ncan, which is raise an exception, and the OS takes over from there . As\nthis is identical to what happens when a process does something i llegal,\nit is perhaps not surprising that we term the activity a “fault .”\nUpon a page fault, the OS is invoked to service the page fault. A p artic-\nular piece of code, known as a page-fault handler , runs, and must service\nthe page fault, as we now describe.\n21.3 The Page Fault\nRecall that with TLB misses, we have two types of systems: hard ware-\nmanaged TLBs (where the hardware looks in the page table to ﬁnd t he\ndesired translation) and software-managed TLBs (where the OS does). In\neither type of system, if a page is not present, the OS is put in ch arge to\nhandle the page fault. The appropriately-named OS page-fault handler\nruns to determine what to do. Virtually all systems handle pag e faults in\nsoftware; even with a hardware-managed TLB, the hardware tru sts the\nOS to manage this important duty.\nIf a page is not present and has been swapped to disk, the OS will need\nto swap the page into memory in order to service the page fault. T hus, a\nquestion arises: how will the OS know where to ﬁnd the desired pag e? In\nmany systems, the page table is a natural place to store such in formation.\nThus, the OS could use the bits in the PTE normally used for data su ch as\nthe PFN of the page for a disk address. When the OS receives a page fault\nfor a page, it looks in the PTE to ﬁnd the address, and issues the re quest\nto disk to fetch the page into memory.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : M ECHANISMS 5\nASIDE : W HYHARDWARE DOESN ’THANDLE PAGE FAULTS\nWe know from our experience with the TLB that hardware designers are\nloathe to trust the OS to do much of anything. So why do they trust t he\nOS to handle a page fault? There are a few main reasons. First, p age\nfaults to disk are slow; even if the OS takes a long time to handle a fault,\nexecuting tons of instructions, the disk operation itself is trad itionally so\nslow that the extra overheads of running software are minimal. Sec ond,\nto be able to handle a page fault, the hardware would have to und erstand\nswap space, how to issue I/Os to the disk, and a lot of other details which\nit currently doesn’t know much about. Thus, for both reasons of perfor-\nmance and simplicity, the OS handles page faults, and even ha rdware\ntypes can be happy.\nWhen the disk I/O completes, the OS will then update the page ta ble\nto mark the page as present, update the PFN ﬁeld of the page-tab le entry\n(PTE) to record the in-memory location of the newly-fetched page, and\nretry the instruction. This next attempt may generate a TLB mi ss, which\nwould then be serviced and update the TLB with the translation ( one\ncould alternately update the TLB when servicing the page faul t to avoid\nthis step). Finally, a last restart would ﬁnd the translation i n the TLB and\nthus proceed to fetch the desired data or instruction from memory a t the\ntranslated physical address.\nNote that while the I/O is in ﬂight, the process will be in the blocked\nstate. Thus, the OS will be free to run other ready processes whi le the\npage fault is being serviced. Because I/O is expensive, this overlap of\nthe I/O (page fault) of one process and the execution of another is ye t\nanother way a multiprogrammed system can make the most effectiv e use\nof its hardware.\n21.4 What If Memory Is Full?\nIn the process described above, you may notice that we assumed the re\nis plenty of free memory in which to page in a page from swap space.\nOf course, this may not be the case; memory may be full (or close to it ).\nThus, the OS might like to ﬁrst page out one or more pages to make room\nfor the new page(s) the OS is about to bring in. The process of picki ng a\npage to kick out, or replace is known as the page-replacement policy .\nAs it turns out, a lot of thought has been put into creating a good page -\nreplacement policy, as kicking out the wrong page can exact a gre at cost\non program performance. Making the wrong decision can cause a pro-\ngram to run at disk-like speeds instead of memory-like speeds; in cur-\nrent technology that means a program could run 10,000 or 100,000 ti mes\nslower. Thus, such a policy is something we should study in some det ail;\nindeed, that is exactly what we will do in the next chapter. For now, it is\ngood enough to understand that such a policy exists, built on top of th e\nmechanisms described here.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 BEYOND PHYSICAL MEMORY : M ECHANISMS\n1VPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2(Success, TlbEntry) = TLB_Lookup(VPN)\n3if (Success == True) // TLB Hit\n4if (CanAccess(TlbEntry.ProtectBits) == True)\n5 Offset = VirtualAddress & OFFSET_MASK\n6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7 Register = AccessMemory(PhysAddr)\n8else\n9 RaiseException(PROTECTION_FAULT)\n10else // TLB Miss\n11 PTEAddr = PTBR + (VPN *sizeof(PTE))\n12 PTE =AccessMemory(PTEAddr)\n13 if (PTE.Valid == False)\n14 RaiseException(SEGMENTATION_FAULT)\n15 else\n16 if (CanAccess(PTE.ProtectBits) == False)\n17 RaiseException(PROTECTION_FAULT)\n18 else if (PTE.Present == True)\n19 // assuming hardware-managed TLB\n20 TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n21 RetryInstruction()\n22 else if (PTE.Present == False)\n23 RaiseException(PAGE_FAULT)\nFigure 21.2: Page-Fault Control Flow Algorithm (Hardware)\n21.5 Page Fault Control Flow\nWith all of this knowledge in place, we can now roughly sketch the\ncomplete control ﬂow of memory access. In other words, when some-\nbody asks you “what happens when a program fetches some data from\nmemory?”, you should have a pretty good idea of all the different pos-\nsibilities. See the control ﬂow in Figures 21.2 and 21.3 for more det ails;\nthe ﬁrst ﬁgure shows what the hardware does during translation, and the\nsecond what the OS does upon a page fault.\nFrom the hardware control ﬂow diagram in Figure 21.2, notice that\nthere are now three important cases to understand when a TLB mis s oc-\ncurs. First, that the page was both present and valid (Lines 18–21); in\nthis case, the TLB miss handler can simply grab the PFN from the PTE,\nretry the instruction (this time resulting in a TLB hit), and t hus continue\nas described (many times) before. In the second case (Lines 22– 23), the\npage fault handler must be run; although this was a legitimate page for\nthe process to access (it is valid, after all), it is not present in physical\nmemory. Third (and ﬁnally), the access could be to an invalid pa ge, due\nfor example to a bug in the program (Lines 13–14). In this case, n o other\nbits in the PTE really matter; the hardware traps this invali d access, and\nthe OS trap handler runs, likely terminating the offending pr ocess.\nFrom the software control ﬂow in Figure 21.3, we can see what the OS\nroughly must do in order to service the page fault. First, the OS must ﬁnd\na physical frame for the soon-to-be-faulted-in page to reside wi thin; if\nthere is no such page, we’ll have to wait for the replacement alg orithm to\nrun and kick some pages out of memory, thus freeing them for use here .\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : M ECHANISMS 7\n1PFN = FindFreePhysicalPage()\n2if (PFN == -1) // no free page found\n3PFN = EvictPage() // run replacement algorithm\n4DiskRead(PTE.DiskAddr, PFN) // sleep (waiting for I/O)\n5PTE.present = True // update page table with present\n6PTE.PFN = PFN // bit and translation (PFN)\n7RetryInstruction() // retry instruction\nFigure 21.3: Page-Fault Control Flow Algorithm (Software)\nWith a physical frame in hand, the handler then issues the I/O request\nto read in the page from swap space. Finally, when that slow opera tion\ncompletes, the OS updates the page table and retries the instr uction. The\nretry will result in a TLB miss, and then, upon another retry, a T LB hit, at\nwhich point the hardware will be able to access the desired ite m.\n21.6 When Replacements Really Occur\nThus far, the way we’ve described how replacements occur assume s\nthat the OS waits until memory is entirely full, and only then re places\n(evicts) a page to make room for some other page. As you can imagine,\nthis is a little bit unrealistic, and there are many reasons for the OS to keep\na small portion of memory free more proactively.\nTo keep a small amount of memory free, most operating systems thus\nhave some kind of high watermark (HW ) and low watermark (LW) to\nhelp decide when to start evicting pages from memory. How this wor ks is\nas follows: when the OS notices that there are fewer than LW pages avail-\nable, a background thread that is responsible for freeing memory runs.\nThe thread evicts pages until there are HW pages available. The back-\nground thread, sometimes called the swap daemon orpage daemon1,\nthen goes to sleep, happy that it has freed some memory for running pro-\ncesses and the OS to use.\nBy performing a number of replacements at once, new performance\noptimizations become possible. For example, many systems will cluster\norgroup a number of pages and write them out at once to the swap parti-\ntion, thus increasing the efﬁciency of the disk [LL82]; as we wi ll see later\nwhen we discuss disks in more detail, such clustering reduces seek and\nrotational overheads of a disk and thus increases performance noti ceably.\nTo work with the background paging thread, the control ﬂow in Figur e\n21.3 should be modiﬁed slightly; instead of performing a replace ment\ndirectly, the algorithm would instead simply check if there ar e any free\npages available. If not, it would inform the background paging th read\nthat free pages are needed; when the thread frees up some pages , it would\nre-awaken the original thread, which could then page in the des ired page\nand go about its work.\n1The word “daemon”, usually pronounced “demon”, is an old term for a back ground\nthread or process that does something useful. Turns out (once again!) that the source of the\nterm is Multics [CS94].\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 BEYOND PHYSICAL MEMORY : M ECHANISMS\nTIP: DOWORK INTHEBACKGROUND\nWhen you have some work to do, it is often a good idea to do it in the\nbackground to increase efﬁciency and to allow for grouping of opera-\ntions. Operating systems often do work in the background; for exam ple,\nmany systems buffer ﬁle writes in memory before actually writi ng the\ndata to disk. Doing so has many possible beneﬁts: increased dis k efﬁ-\nciency, as the disk may now receive many writes at once and thus b etter\nbe able to schedule them; improved latency of writes, as the app lication\nthinks the writes completed quite quickly; the possibility of w ork reduc-\ntion, as the writes may need never to go to disk (i.e., if the ﬁle is deleted);\nand better use of idle time , as the background work may possibly be\ndone when the system is otherwise idle, thus better utilizing t he hard-\nware [G+95].\n21.7 Summary\nIn this brief chapter, we have introduced the notion of accessing more\nmemory than is physically present within a system. To do so req uires\nmore complexity in page-table structures, as a present bit (of some kind)\nmust be included to tell us whether the page is present in memor y or not.\nWhen not, the operating system page-fault handler runs to service the\npage fault , and thus arranges for the transfer of the desired page from\ndisk to memory, perhaps ﬁrst replacing some pages in memory to ma ke\nroom for those soon to be swapped in.\nRecall, importantly (and amazingly!), that these actions all t ake place\ntransparently to the process. As far as the process is concerned, it is just\naccessing its own private, contiguous virtual memory. Behind th e scenes,\npages are placed in arbitrary (non-contiguous) locations in phys ical mem-\nory, and sometimes they are not even present in memory, requiring a fetch\nfrom disk. While we hope that in the common case a memory access is\nfast, in some cases it will take multiple disk operations to serv ice it; some-\nthing as simple as performing a single instruction can, in the w orst case,\ntake many milliseconds to complete.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : M ECHANISMS 9\nReferences\n[CS94] “Take Our Word For It” by F. Corbato, R. Steinberg. www.takeourword.com/TOW146\n(Page 4). Richard Steinberg writes: “Someone has asked me the origin of the word daemon as it applies\nto computing. Best I can tell based on my research, the word was ﬁrst used by people on your team at\nProject MAC using the IBM 7094 in 1963.” Professor Corbato replies: “O ur use of the word daemon\nwas inspired by the Maxwell’s daemon of physics and thermodynamics (my bac kground is in physics).\nMaxwell’s daemon was an imaginary agent which helped sort molecules of di fferent speeds and worked\ntirelessly in the background. We fancifully began to use the word daemon to d escribe background pro-\ncesses which worked tirelessly to perform system chores.”\n[D97] “Before Memory Was Virtual” by Peter Denning. In the Beginning: Reco llections of\nSoftware Pioneers, Wiley, November 1997. An excellent historical piece by one of the pioneers of\nvirtual memory and working sets.\n[G+95] “Idleness is not sloth” by Richard Golding, Peter Bosch, Carl Sta elin, Tim Sullivan, John\nWilkes. USENIX ATC ’95, New Orleans, Louisiana. A fun and easy-to-read discussion of how idle\ntime can be better used in systems, with lots of good examples.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System” by Hank Levy, P .\nLipman. IEEE Computer, Vol. 15, No. 3, March 1982. Not the ﬁrst place where page clustering was\nused, but a clear and simple explanation of how such a mechanism works. We s ure cite this paper a lot!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 BEYOND PHYSICAL MEMORY : M ECHANISMS\nHomework (Measurement)\nThis homework introduces you to a new tool, vmstat , and how it can\nbe used to understand memory, CPU, and I/O usage. Read the assoc i-\nated README and examine the code in mem.c before proceeding to the\nexercises and questions below.\nQuestions\n1. First, open two separate terminal connections to the same machine, so that\nyou can easily run something in one window and the other.\nNow, in one window, run vmstat 1 , which shows statistics about machine\nusage every second. Read the man page, the associated README, an d any\nother information you need so that you can understand its output. Leave\nthis window running vmstat for the rest of the exercises below.\nNow, we will run the program mem.c but with very little memory usage.\nThis can be accomplished by typing ./mem 1 (which uses only 1 MB of\nmemory). How do the CPU usage statistics change when running mem? Do\nthe numbers in the user time column make sense? How does this change\nwhen running more than one instance of mem at once?\n2. Let’s now start looking at some of the memory statistics while runningmem.\nWe’ll focus on two columns: swpd (the amount of virtual memory used) and\nfree (the amount of idle memory). Run ./mem 1024 (which allocates 1024\nMB) and watch how these values change. Then kill the running pro gram\n(by typing control-c) and watch again how the values change. W hat do you\nnotice about the values? In particular, how does the free column change\nwhen the program exits? Does the amount of free memory increase by t he\nexpected amount when mem exits?\n3. We’ll next look at the swap columns (siandso), which indicate how much\nswapping is taking place to and from the disk. Of course, to act ivate these,\nyou’ll need to run mem with large amounts of memory. First, examine how\nmuch free memory is on your Linux system (for example, by typing cat\n/proc/meminfo ; typeman proc for details on the /proc ﬁle system and\nthe types of information you can ﬁnd there). One of the ﬁrst ent ries in\n/proc/meminfo is the total amount of memory in your system. Let’s as-\nsume it’s something like 8 GB of memory; if so, start by running mem 4000\n(about 4 GB) and watching the swap in/out columns. Do they ever giv e\nnon-zero values? Then, try with 5000 ,6000 , etc. What happens to these\nvalues as the program enters the second loop (and beyond), as c ompared to\nthe ﬁrst loop? How much data (total) are swapped in and out during t he\nsecond, third, and subsequent loops? (do the numbers make sense?)\n4. Do the same experiments as above, but now watch the other statis tics (such\nas CPU utilization, and block I/O statistics). How do they cha nge when\nmem is running?\n5. Now let’s examine performance. Pick an input for mem that comfortably\nﬁts in memory (say 4000 if the amount of memory on the system is 8 GB).\nHow long does loop 0 take (and subsequent loops 1, 2, etc.)? Now p ick a size\ncomfortably beyond the size of memory (say 12000 again assuming 8 GB of\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : M ECHANISMS 11\nmemory). How long do the loops take here? How do the bandwidth num-\nbers compare? How different is performance when constantly sw apping\nversus ﬁtting everything comfortably in memory? Can you make a gra ph,\nwith the size of memory used by mem on the x-axis, and the bandwidth of\naccessing said memory on the y-axis? Finally, how does the perfo rmance of\nthe ﬁrst loop compare to that of subsequent loops, for both the ca se where\neverything ﬁts in memory and where it doesn’t?\n6. Swap space isn’t inﬁnite. You can use the tool swapon with the-sﬂag to\nsee how much swap space is available. What happens if you try to r unmem\nwith increasingly large values, beyond what seems to be availa ble in swap?\nAt what point does the memory allocation fail?\n7. Finally, if you’re advanced, you can conﬁgure your system to us e different\nswap devices using swapon andswapoff . Read the man pages for details.\nIf you have access to different hardware, see how the performa nce of swap-\nping changes when swapping to a classic hard drive, a ﬂash-ba sed SSD, and\neven a RAID array. How much can swapping performance be improved vi a\nnewer devices? How close can you get to in-memory performance?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",27355
26-22. Swapping Policies.pdf,26-22. Swapping Policies,"22\nBeyond Physical Memory: Policies\nIn a virtual memory manager, life is easy when you have a lot of free\nmemory. A page fault occurs, you ﬁnd a free page on the free-page li st,\nand assign it to the faulting page. Hey, Operating System, cong ratula-\ntions! You did it again.\nUnfortunately, things get a little more interesting when litt le memory\nis free. In such a case, this memory pressure forces the OS to start paging\noutpages to make room for actively-used pages. Deciding which page\n(or pages) to evict is encapsulated within the replacement policy of the\nOS; historically, it was one of the most important decisions the earl y vir-\ntual memory systems made, as older systems had little physical memory.\nMinimally, it is an interesting set of policies worth knowing a li ttle more\nabout. And thus our problem:\nTHECRUX: HOWTODECIDE WHICH PAGE TOEVICT\nHow can the OS decide which page (or pages) to evict from memory?\nThis decision is made by the replacement policy of the system, wh ich usu-\nally follows some general principles (discussed below) but also includes\ncertain tweaks to avoid corner-case behaviors.\n22.1 Cache Management\nBefore diving into policies, we ﬁrst describe the problem we are trying\nto solve in more detail. Given that main memory holds some subset of\nall the pages in the system, it can rightly be viewed as a cache for virtual\nmemory pages in the system. Thus, our goal in picking a replaceme nt\npolicy for this cache is to minimize the number of cache misses , i.e., to\nminimize the number of times that we have to fetch a page from dis k.\nAlternately, one can view our goal as maximizing the number of cache\nhits, i.e., the number of times a page that is accessed is found in mem ory.\nKnowing the number of cache hits and misses let us calculate the av-\nerage memory access time (AMAT ) for a program (a metric computer\narchitects compute for hardware caches [HP06]). Speciﬁcally, given these\nvalues, we can compute the AMAT of a program as follows:\nAMAT =TM+(PMiss·TD) (22.1)\n1\n2 BEYOND PHYSICAL MEMORY : POLICIES\nwhereTMrepresents the cost of accessing memory, TDthe cost of ac-\ncessing disk, and PMiss the probability of not ﬁnding the data in the\ncache (a miss); PMiss varies from 0.0 to 1.0, and sometimes we refer to\na percent miss rate instead of a probability (e.g., a 10% miss ra te means\nPMiss= 0.10). Note you always pay the cost of accessing the data in\nmemory; when you miss, however, you must additionally pay the cost of\nfetching the data from disk.\nFor example, let us imagine a machine with a (tiny) address spa ce:\n4KB, with 256-byte pages. Thus, a virtual address has two comp onents: a\n4-bit VPN (the most-signiﬁcant bits) and an 8-bit offset (the l east-signiﬁcant\nbits). Thus, a process in this example can access 24or 16 total virtual\npages. In this example, the process generates the following mem ory ref-\nerences (i.e., virtual addresses): 0x000, 0x100, 0x200, 0x 300, 0x400, 0x500,\n0x600, 0x700, 0x800, 0x900. These virtual addresses refer t o the ﬁrst byte\nof each of the ﬁrst ten pages of the address space (the page number being\nthe ﬁrst hex digit of each virtual address).\nLet us further assume that every page except virtual page 3 is already\nin memory. Thus, our sequence of memory references will encounter the\nfollowing behavior: hit, hit, hit, miss, hit, hit, hit, hit, hit , hit. We can\ncompute the hit rate (the percent of references found in memory): 90%, as\n9 out of 10 references are in memory. The miss rate is thus 10% ( PMiss=\n0.1). In general, PHit+PMiss= 1.0; hit rate plus miss rate sum to 100%.\nTo calculate AMAT, we need to know the cost of accessing memory\nand the cost of accessing disk. Assuming the cost of accessing mem ory\n(TM) is around 100 nanoseconds, and the cost of accessing disk ( TD) is\nabout 10 milliseconds, we have the following AMAT: 100ns+0.1·10ms,\nwhich is 100ns+ 1ms, or 1.0001 ms, or about 1 millisecond. If our hit\nrate had instead been 99.9% ( Pmiss= 0.001), the result is quite different:\nAMAT is 10.1 microseconds, or roughly 100 times faster. As the hit rate\napproaches 100%, AMAT approaches 100 nanoseconds.\nUnfortunately, as you can see in this example, the cost of disk acc ess\nis so high in modern systems that even a tiny miss rate will quic kly dom-\ninate the overall AMAT of running programs. Clearly, we need to a void\nas many misses as possible or run slowly, at the rate of the disk. On e way\nto help with this is to carefully develop a smart policy, as we now do.\n22.2 The Optimal Replacement Policy\nTo better understand how a particular replacement policy works , it\nwould be nice to compare it to the best possible replacement polic y. As it\nturns out, such an optimal policy was developed by Belady many years\nago [B66] (he originally called it MIN). The optimal replaceme nt policy\nleads to the fewest number of misses overall. Belady showed that a sim-\nple (but, unfortunately, difﬁcult to implement!) approach tha t replaces\nthe page that will be accessed furthest in the future is the optimal policy,\nresulting in the fewest-possible cache misses.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : POLICIES 3\nTIP: COMPARING AGAINST OPTIMAL ISUSEFUL\nAlthough optimal is not very practical as a real policy, it is incr edibly\nuseful as a comparison point in simulation or other studies. Saying t hat\nyour fancy new algorithm has a 80% hit rate isn’t meaningful in is olation;\nsaying that optimal achieves an 82% hit rate (and thus your new a pproach\nis quite close to optimal) makes the result more meaningful and g ives it\ncontext. Thus, in any study you perform, knowing what the optimal i s\nlets you perform a better comparison, showing how much improvement\nis still possible, and also when you can stop making your policy better,\nbecause it is close enough to the ideal [AD03].\nHopefully, the intuition behind the optimal policy makes sense. Think\nabout it like this: if you have to throw out some page, why not throw\nout the one that is needed the furthest from now? By doing so, you are\nessentially saying that all the other pages in the cache are mor e important\nthan the one furthest out. The reason this is true is simple: you wi ll refer\nto the other pages before you refer to the one furthest out.\nLet’s trace through a simple example to understand the decision s the\noptimal policy makes. Assume a program accesses the following str eam\nof virtual pages: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1. Figure 22.1 shows t he behavior\nof optimal, assuming a cache that ﬁts three pages.\nIn the ﬁgure, you can see the following actions. Not surprisingly, the\nﬁrst three accesses are misses, as the cache begins in an empt y state; such\na miss is sometimes referred to as a cold-start miss (orcompulsory miss ).\nThen we refer again to pages 0 and 1, which both hit in the cache. Finally,\nwe reach another miss (to page 3), but this time the cache is ful l; a re-\nplacement must take place! Which begs the question: which pag e should\nwe replace? With the optimal policy, we examine the future for ea ch page\ncurrently in the cache (0, 1, and 2), and see that 0 is accessed almost imme-\ndiately, 1 is accessed a little later, and 2 is accessed furth est in the future.\nThus the optimal policy has an easy choice: evict page 2, resulti ng in\npages 0, 1, and 3 in the cache. The next three references are hi ts, but then\nResulting\nAccess Hit/Miss? Evict Cache State\n0 Miss 0\n1 Miss 0, 1\n2 Miss 0, 1, 2\n0 Hit 0, 1, 2\n1 Hit 0, 1, 2\n3 Miss 2 0, 1, 3\n0 Hit 0, 1, 3\n3 Hit 0, 1, 3\n1 Hit 0, 1, 3\n2 Miss 3 0, 1, 2\n1 Hit 0, 1, 2\nFigure 22.1: Tracing The Optimal Policy\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 BEYOND PHYSICAL MEMORY : POLICIES\nASIDE : TYPES OF CACHE MISSES\nIn the computer architecture world, architects sometimes ﬁnd i t useful\nto characterize misses by type, into one of three categories: com pulsory,\ncapacity, and conﬂict misses, sometimes called the Three C’s [H87]. A\ncompulsory miss (orcold-start miss [EF78]) occurs because the cache is\nempty to begin with and this is the ﬁrst reference to the item; in con-\ntrast, a capacity miss occurs because the cache ran out of space and had\nto evict an item to bring a new item into the cache. The third ty pe of\nmiss (a conﬂict miss ) arises in hardware because of limits on where an\nitem can be placed in a hardware cache, due to something known as set-\nassociativity ; it does not arise in the OS page cache because such caches\nare always fully-associative , i.e., there are no restrictions on where in\nmemory a page can be placed. See H&P for details [HP06].\nwe get to page 2, which we evicted long ago, and suffer another mis s.\nHere the optimal policy again examines the future for each page i n the\ncache (0, 1, and 3), and sees that as long as it doesn’t evict page 1 (which\nis about to be accessed), we’ll be OK. The example shows page 3 get ting\nevicted, although 0 would have been a ﬁne choice too. Finally, we hi t on\npage 1 and the trace completes.\nWe can also calculate the hit rate for the cache: with 6 hits and 5 misses,\nthe hit rate isHits\nHits+Misseswhich is6\n6+5or 54.5%. You can also compute\nthe hit rate modulo compulsory misses (i.e., ignore the ﬁrstmiss to a given\npage), resulting in a 85.7% hit rate.\nUnfortunately, as we saw before in the development of scheduling\npolicies, the future is not generally known; you can’t build the opt imal\npolicy for a general-purpose operating system1. Thus, in developing a\nreal, deployable policy, we will focus on approaches that ﬁnd some ot her\nway to decide which page to evict. The optimal policy will thus s erve\nonly as a comparison point, to know how close we are to “perfect”.\n22.3 A Simple Policy: FIFO\nMany early systems avoided the complexity of trying to approach\noptimal and employed very simple replacement policies. For exam ple,\nsome systems used FIFO (ﬁrst-in, ﬁrst-out) replacement, where pages\nwere simply placed in a queue when they enter the system; when a re-\nplacement occurs, the page on the tail of the queue (the “ﬁrst-in ” page) is\nevicted. FIFO has one great strength: it is quite simple to imp lement.\nLet’s examine how FIFO does on our example reference stream (Figur e\n22.2, page 5). We again begin our trace with three compulsory mis ses to\npages 0, 1, and 2, and then hit on both 0 and 1. Next, page 3 is refer enced,\ncausing a miss; the replacement decision is easy with FIFO: pi ck the page\n1If you can, let us know! We can become rich together. Or, like the scientist s who “discov-\nered” cold fusion, widely scorned and mocked [FP89].\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : POLICIES 5\nResulting\nAccess Hit/Miss? Evict Cache State\n0 Miss First-in → 0\n1 Miss First-in → 0, 1\n2 Miss First-in → 0, 1, 2\n0 Hit First-in → 0, 1, 2\n1 Hit First-in → 0, 1, 2\n3 Miss 0 First-in → 1, 2, 3\n0 Miss 1 First-in → 2, 3, 0\n3 Hit First-in → 2, 3, 0\n1 Miss 2 First-in → 3, 0, 1\n2 Miss 3 First-in → 0, 1, 2\n1 Hit First-in → 0, 1, 2\nFigure 22.2: Tracing The FIFO Policy\nthat was the “ﬁrst one” in (the cache state in the ﬁgure is kept i n FIFO\norder, with the ﬁrst-in page on the left), which is page 0. Unfort unately,\nour next access is to page 0, causing another miss and replaceme nt (of\npage 1). We then hit on page 3, but miss on 1 and 2, and ﬁnally hit on 3 .\nComparing FIFO to optimal, FIFO does notably worse: a 36.4% hit\nrate (or 57.1% excluding compulsory misses). FIFO simply can’t d eter-\nmine the importance of blocks: even though page 0 had been accesse d\na number of times, FIFO still kicks it out, simply because it was the ﬁrst\none brought into memory.\nASIDE : BELADY ’SANOMALY\nBelady (of the optimal policy) and colleagues found an interestin g refer-\nence stream that behaved a little unexpectedly [BNS69]. The m emory-\nreference stream: 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5. The replacem ent policy\nthey were studying was FIFO. The interesting part: how the cac he hit\nrate changed when moving from a cache size of 3 to 4 pages.\nIn general, you would expect the cache hit rate to increase (get better)\nwhen the cache gets larger. But in this case, with FIFO, it get s worse! Cal-\nculate the hits and misses yourself and see. This odd behavior is generally\nreferred to as Belady’s Anomaly (to the chagrin of his co-authors).\nSome other policies, such as LRU, don’t suffer from this problem. Can\nyou guess why? As it turns out, LRU has what is known as a stack prop-\nerty [M+70]. For algorithms with this property, a cache of size N+ 1\nnaturally includes the contents of a cache of size N. Thus, when increas-\ning the cache size, hit rate will either stay the same or improve . FIFO and\nRandom (among others) clearly do not obey the stack property, and th us\nare susceptible to anomalous behavior.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 BEYOND PHYSICAL MEMORY : POLICIES\nResulting\nAccess Hit/Miss? Evict Cache State\n0 Miss 0\n1 Miss 0, 1\n2 Miss 0, 1, 2\n0 Hit 0, 1, 2\n1 Hit 0, 1, 2\n3 Miss 0 1, 2, 3\n0 Miss 1 2, 3, 0\n3 Hit 2, 3, 0\n1 Miss 3 2, 0, 1\n2 Hit 2, 0, 1\n1 Hit 2, 0, 1\nFigure 22.3: Tracing The Random Policy\n22.4 Another Simple Policy: Random\nAnother similar replacement policy is Random, which simply pic ks a\nrandom page to replace under memory pressure. Random has propert ies\nsimilar to FIFO; it is simple to implement, but it doesn’t reall y try to be\ntoo intelligent in picking which blocks to evict. Let’s look at how R andom\ndoes on our famous example reference stream (see Figure 22.3).\nOf course, how Random does depends entirely upon how lucky (or\nunlucky) Random gets in its choices. In the example above, Random does\na little better than FIFO, and a little worse than optimal. In fa ct, we can\nrun the Random experiment thousands of times and determine how it\ndoes in general. Figure 22.4 shows how many hits Random achieves ov er\n10,000 trials, each with a different random seed. As you can see , some-\ntimes (just over 40% of the time), Random is as good as optimal, achie ving\n6 hits on the example trace; sometimes it does much worse, achievi ng 2\nhits or fewer. How Random does depends on the luck of the draw.\n0 1 2 3 4 5 6 701020304050\nNumber of HitsFrequency\nFigure 22.4: Random Performance Over 10,000 Trials\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : POLICIES 7\nResulting\nAccess Hit/Miss? Evict Cache State\n0 Miss LRU → 0\n1 Miss LRU → 0, 1\n2 Miss LRU → 0, 1, 2\n0 Hit LRU → 1, 2, 0\n1 Hit LRU → 2, 0, 1\n3 Miss 2 LRU → 0, 1, 3\n0 Hit LRU → 1, 3, 0\n3 Hit LRU → 1, 0, 3\n1 Hit LRU → 0, 3, 1\n2 Miss 0 LRU → 3, 1, 2\n1 Hit LRU → 3, 2, 1\nFigure 22.5: Tracing The LRU Policy\n22.5 Using History: LRU\nUnfortunately, any policy as simple as FIFO or Random is likely to\nhave a common problem: it might kick out an important page, one that\nis about to be referenced again. FIFO kicks out the page that was ﬁrst\nbrought in; if this happens to be a page with important code or data\nstructures upon it, it gets thrown out anyhow, even though it will s oon be\npaged back in. Thus, FIFO, Random, and similar policies are not l ikely to\napproach optimal; something smarter is needed.\nAs we did with scheduling policy, to improve our guess at the futu re,\nwe once again lean on the past and use history as our guide. For example,\nif a program has accessed a page in the near past, it is likely to access it\nagain in the near future.\nOne type of historical information a page-replacement policy coul d\nuse is frequency ; if a page has been accessed many times, perhaps it\nshould not be replaced as it clearly has some value. A more commonly-\nused property of a page is its recency of access; the more recently a page\nhas been accessed, perhaps the more likely it will be accessed again.\nThis family of policies is based on what people refer to as the prin-\nciple of locality [D70], which basically is just an observation about pro-\ngrams and their behavior. What this principle says, quite sim ply, is that\nprograms tend to access certain code sequences (e.g., in a loop) a nd data\nstructures (e.g., an array accessed by the loop) quite frequen tly; we should\nthus try to use history to ﬁgure out which pages are important, an d keep\nthose pages in memory when it comes to eviction time.\nAnd thus, a family of simple historically-based algorithms are born.\nThe Least-Frequently-Used (LFU ) policy replaces the least-frequently-\nused page when an eviction must take place. Similarly, the Least-Recently-\nUsed (LRU ) policy replaces the least-recently-used page. These algo-\nrithms are easy to remember: once you know the name, you know exactl y\nwhat it does, which is an excellent property for a name.\nTo better understand LRU, let’s examine how LRU does on our exam-\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 BEYOND PHYSICAL MEMORY : POLICIES\nASIDE : TYPES OF LOCALITY\nThere are two types of locality that programs tend to exhibit. Th e ﬁrst\nis known as spatial locality , which states that if a page Pis accessed,\nit is likely the pages around it (say P−1orP+ 1) will also likely be\naccessed. The second is temporal locality , which states that pages that\nhave been accessed in the near past are likely to be accessed a gain in the\nnear future. The assumption of the presence of these types of local ity\nplays a large role in the caching hierarchies of hardware syste ms, which\ndeploy many levels of instruction, data, and address-translat ion caching\nto help programs run fast when such locality exists.\nOf course, the principle of locality , as it is often called, is no hard-and-\nfast rule that all programs must obey. Indeed, some programs acce ss\nmemory (or disk) in rather random fashion and don’t exhibit much or\nany locality in their access streams. Thus, while locality is a good thing to\nkeep in mind while designing caches of any kind (hardware or soft ware),\nit does not guarantee success. Rather, it is a heuristic that often proves\nuseful in the design of computer systems.\nple reference stream. Figure 22.5 (page 7) shows the results. From the\nﬁgure, you can see how LRU can use history to do better than statel ess\npolicies such as Random or FIFO. In the example, LRU evicts page 2 when\nit ﬁrst has to replace a page, because 0 and 1 have been accesse d more re-\ncently. It then replaces page 0 because 1 and 3 have been acces sed more\nrecently. In both cases, LRU’s decision, based on history, turns ou t to be\ncorrect, and the next references are thus hits. Thus, in our exa mple, LRU\ndoes as well as possible, matching optimal in its performance2.\nWe should also note that the opposites of these algorithms exist: Most-\nFrequently-Used (MFU ) and Most-Recently-Used (MRU ). In most cases\n(not all!), these policies do not work well, as they ignore the locali ty most\nprograms exhibit instead of embracing it.\n22.6 Workload Examples\nLet’s look at a few more examples in order to better understand how\nsome of these policies behave. Here, we’ll examine more complex work-\nloads instead of small traces. However, even these workloads are great ly\nsimpliﬁed; a better study would include application traces.\nOur ﬁrst workload has no locality, which means that each referen ce\nis to a random page within the set of accessed pages. In this simp le ex-\nample, the workload accesses 100 unique pages over time, choosing the\nnext page to refer to at random; overall, 10,000 pages are acces sed. In the\nexperiment, we vary the cache size from very small (1 page) to e nough\nto hold all the unique pages (100 page), in order to see how each pol icy\nbehaves over the range of cache sizes.\n2OK, we cooked the results. But sometimes cooking is necessary to prov e a point.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : POLICIES 9\n0 20 40 60 80 1000%20%40%60%80%100%The No-Locality Workload\nCache Size (Blocks)Hit Rate\nOPT\nLRU\nFIFO\nRAND\nFigure 22.6: The No-Locality Workload\nFigure 22.6 plots the results of the experiment for optimal, LRU, Ran-\ndom, and FIFO. The y-axis of the ﬁgure shows the hit rate that each policy\nachieves; the x-axis varies the cache size as described above .\nWe can draw a number of conclusions from the graph. First, when\nthere is no locality in the workload, it doesn’t matter much which r ealistic\npolicy you are using; LRU, FIFO, and Random all perform the same, w ith\nthe hit rate exactly determined by the size of the cache. Second, when\nthe cache is large enough to ﬁt the entire workload, it also doesn’t matter\nwhich policy you use; all policies (even Random) converge to a 100% hit\nrate when all the referenced blocks ﬁt in cache. Finally, you ca n see that\noptimal performs noticeably better than the realistic policies ; peeking into\nthe future, if it were possible, does a much better job of replacem ent.\nThe next workload we examine is called the “80-20” workload, whic h\nexhibits locality: 80% of the references are made to 20% of the pa ges (the\n“hot” pages); the remaining 20% of the references are made to th e re-\nmaining 80% of the pages (the “cold” pages). In our workload, there are\na total 100 unique pages again; thus, “hot” pages are referred t o most of\nthe time, and “cold” pages the remainder. Figure 22.7 (page 10 ) shows\nhow the policies perform with this workload.\nAs you can see from the ﬁgure, while both random and FIFO do rea-\nsonably well, LRU does better, as it is more likely to hold onto the h ot\npages; as those pages have been referred to frequently in the p ast, they\nare likely to be referred to again in the near future. Optimal once again\ndoes better, showing that LRU’s historical information is not perfe ct.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 BEYOND PHYSICAL MEMORY : POLICIES\n0 20 40 60 80 1000%20%40%60%80%100%The 80-20 Workload\nCache Size (Blocks)Hit Rate\nOPT\nLRU\nFIFO\nRAND\nFigure 22.7: The 80-20 Workload\nYou might now be wondering: is LRU’s improvement over Random\nand FIFO really that big of a deal? The answer, as usual, is “it d epends.” If\neach miss is very costly (not uncommon), then even a small increas e in hit\nrate (reduction in miss rate) can make a huge difference on perf ormance.\nIf misses are not so costly, then of course the beneﬁts possible wit h LRU\nare not nearly as important.\nLet’s look at one ﬁnal workload. We call this one the “looping sequen-\ntial” workload, as in it, we refer to 50 pages in sequence, start ing at 0,\nthen 1, ..., up to page 49, and then we loop, repeating those acces ses, for a\ntotal of 10,000 accesses to 50 unique pages. The last graph in Fi gure 22.8\nshows the behavior of the policies under this workload.\nThis workload, common in many applications (including important\ncommercial applications such as databases [CD85]), represen ts a worst-\ncase for both LRU and FIFO. These algorithms, under a looping-sequ ential\nworkload, kick out older pages; unfortunately, due to the looping na ture\nof the workload, these older pages are going to be accessed sooner tha n\nthe pages that the policies prefer to keep in cache. Indeed, ev en with\na cache of size 49, a looping-sequential workload of 50 pages result s in\na 0% hit rate. Interestingly, Random fares notably better, not q uite ap-\nproaching optimal, but at least achieving a non-zero hit rate. T urns out\nthat random has some nice properties; one such property is not havin g\nweird corner-case behaviors.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : POLICIES 11\n0 20 40 60 80 1000%20%40%60%80%100%The Looping-Sequential Workload\nCache Size (Blocks)Hit Rate\nOPT\nLRU\nFIFO\nRAND\nFigure 22.8: The Looping Workload\n22.7 Implementing Historical Algorithms\nAs you can see, an algorithm such as LRU can generally do a better\njob than simpler policies like FIFO or Random, which may throw out\nimportant pages. Unfortunately, historical policies present u s with a new\nchallenge: how do we implement them?\nLet’s take, for example, LRU. To implement it perfectly, we nee d to\ndo a lot of work. Speciﬁcally, upon each page access (i.e., each memory\naccess, whether an instruction fetch or a load or store), we must up date\nsome data structure to move this page to the front of the list (i.e. , the\nMRU side). Contrast this to FIFO, where the FIFO list of pages is only\naccessed when a page is evicted (by removing the ﬁrst-in page) or when a\nnew page is added to the list (to the last-in side). To keep tra ck of which\npages have been least- and most-recently used, the system has to do some\naccounting work on every memory reference. Clearly, without great care,\nsuch accounting could greatly reduce performance.\nOne method that could help speed this up is to add a little bit of ha rd-\nware support. For example, a machine could update, on each page ac cess,\na time ﬁeld in memory (for example, this could be in the per-proces s page\ntable, or just in some separate array in memory, with one entry per phys-\nical page of the system). Thus, when a page is accessed, the tim e ﬁeld\nwould be set, by hardware, to the current time. Then, when repl acing a\npage, the OS could simply scan all the time ﬁelds in the system t o ﬁnd the\nleast-recently-used page.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 BEYOND PHYSICAL MEMORY : POLICIES\nUnfortunately, as the number of pages in a system grows, scannin g a\nhuge array of times just to ﬁnd the absolute least-recently-us ed page is\nprohibitively expensive. Imagine a modern machine with 4GB of m em-\nory, chopped into 4KB pages. This machine has 1 million pages, an d thus\nﬁnding the LRU page will take a long time, even at modern CPU spee ds.\nWhich begs the question: do we really need to ﬁnd the absolute old est\npage to replace? Can we instead survive with an approximation?\nCRUX: HOWTOIMPLEMENT ANLRU R EPLACEMENT POLICY\nGiven that it will be expensive to implement perfect LRU, can we ap-\nproximate it in some way, and still obtain the desired behavior?\n22.8 Approximating LRU\nAs it turns out, the answer is yes: approximating LRU is more fea-\nsible from a computational-overhead standpoint, and indeed it is what\nmany modern systems do. The idea requires some hardware support,\nin the form of a use bit (sometimes called the reference bit ), the ﬁrst of\nwhich was implemented in the ﬁrst system with paging, the Atl as one-\nlevel store [KE+62]. There is one use bit per page of the system, a nd the\nuse bits live in memory somewhere (they could be in the per-proces s page\ntables, for example, or just in an array somewhere). Whenever a p age is\nreferenced (i.e., read or written), the use bit is set by hardw are to 1. The\nhardware never clears the bit, though (i.e., sets it to 0); tha t is the respon-\nsibility of the OS.\nHow does the OS employ the use bit to approximate LRU? Well, there\ncould be a lot of ways, but with the clock algorithm [C69], one simple\napproach was suggested. Imagine all the pages of the system arr anged in\na circular list. A clock hand points to some particular page to begin with\n(it doesn’t really matter which). When a replacement must occur , the OS\nchecks if the currently-pointed to page Phas a use bit of 1 or 0. If 1, this\nimplies that page Pwas recently used and thus is nota good candidate\nfor replacement. Thus, the use bit for Pset to 0 (cleared), and the clock\nhand is incremented to the next page ( P+ 1). The algorithm continues\nuntil it ﬁnds a use bit that is set to 0, implying this page has n ot been\nrecently used (or, in the worst case, that all pages have been an d that we\nhave now searched through the entire set of pages, clearing all t he bits).\nNote that this approach is not the only way to employ a use bit to\napproximate LRU. Indeed, any approach which periodically clea rs the\nuse bits and then differentiates between which pages have us e bits of 1\nversus 0 to decide which to replace would be ﬁne. The clock algori thm of\nCorbato’s was just one early approach which met with some success, a nd\nhad the nice property of not repeatedly scanning through all of mem ory\nlooking for an unused page.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : POLICIES 13\n0 20 40 60 80 1000%20%40%60%80%100%The 80-20 Workload\nCache Size (Blocks)Hit Rate\nOPT\nLRU\nFIFO\nRAND\nClock\nFigure 22.9: The 80-20 Workload With Clock\nThe behavior of a clock algorithm variant is shown in Figure 22.9. T his\nvariant randomly scans pages when doing a replacement; when it en-\ncounters a page with a reference bit set to 1, it clears the bit ( i.e., sets it\nto 0); when it ﬁnds a page with the reference bit set to 0, it choos es it as\nits victim. As you can see, although it doesn’t do quite as well as p erfect\nLRU, it does better than approaches that don’t consider history at a ll.\n22.9 Considering Dirty Pages\nOne small modiﬁcation to the clock algorithm (also originally sug -\ngested by Corbato [C69]) that is commonly made is the additional c on-\nsideration of whether a page has been modiﬁed or not while in memory.\nThe reason for this: if a page has been modiﬁed and is thus dirty , it must\nbe written back to disk to evict it, which is expensive. If it h as not been\nmodiﬁed (and is thus clean ), the eviction is free; the physical frame can\nsimply be reused for other purposes without additional I/O. Thus, some\nVM systems prefer to evict clean pages over dirty pages.\nTo support this behavior, the hardware should include a modiﬁed bit\n(a.k.a. dirty bit ). This bit is set any time a page is written, and thus can be\nincorporated into the page-replacement algorithm. The clock al gorithm,\nfor example, could be changed to scan for pages that are both unuse d\nand clean to evict ﬁrst; failing to ﬁnd those, then for unused pa ges that\nare dirty, and so forth.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 BEYOND PHYSICAL MEMORY : POLICIES\n22.10 Other VM Policies\nPage replacement is not the only policy the VM subsystem employs\n(though it may be the most important). For example, the OS also has to\ndecide when to bring a page into memory. This policy, sometimes called\nthepage selection policy (as it was called by Denning [D70]), presents\nthe OS with some different options.\nFor most pages, the OS simply uses demand paging , which means the\nOS brings the page into memory when it is accessed, “on demand” a s\nit were. Of course, the OS could guess that a page is about to be use d,\nand thus bring it in ahead of time; this behavior is known as prefetching\nand should only be done when there is reasonable chance of success. For\nexample, some systems will assume that if a code page Pis brought into\nmemory, that code page P+1will likely soon be accessed and thus should\nbe brought into memory too.\nAnother policy determines how the OS writes pages out to disk. Of\ncourse, they could simply be written out one at a time; however, man y\nsystems instead collect a number of pending writes together in m emory\nand write them to disk in one (more efﬁcient) write. This behavi or is\nusually called clustering or simply grouping of writes, and is effective\nbecause of the nature of disk drives, which perform a single larg e write\nmore efﬁciently than many small ones.\n22.11 Thrashing\nBefore closing, we address one ﬁnal question: what should the OS do\nwhen memory is simply oversubscribed, and the memory demands of t he\nset of running processes simply exceeds the available physica l memory?\nIn this case, the system will constantly be paging, a condition s ometimes\nreferred to as thrashing [D70].\nSome earlier operating systems had a fairly sophisticated set of m ech-\nanisms to both detect and cope with thrashing when it took place. F or\nexample, given a set of processes, a system could decide not to run a sub-\nset of processes, with the hope that the reduced set of processes’ working\nsets (the pages that they are using actively) ﬁt in memory and thus c an\nmake progress. This approach, generally known as admission control ,\nstates that it is sometimes better to do less work well than to tr y to do\neverything at once poorly, a situation we often encounter in real li fe as\nwell as in modern computer systems (sadly).\nSome current systems take more a draconian approach to memory\noverload. For example, some versions of Linux run an out-of-memory\nkiller when memory is oversubscribed; this daemon chooses a memory-\nintensive process and kills it, thus reducing memory in a none-t oo-subtle\nmanner. While successful at reducing memory pressure, this a pproach\ncan have problems, if, for example, it kills the X server and thu s renders\nany applications requiring the display unusable.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : POLICIES 15\n22.12 Summary\nWe have seen the introduction of a number of page-replacement (an d\nother) policies, which are part of the VM subsystem of all modern ope rat-\ning systems. Modern systems add some tweaks to straightforward LRU\napproximations like clock; for example, scan resistance is an important\npart of many modern algorithms, such as ARC [MM03]. Scan-resista nt al-\ngorithms are usually LRU-like but also try to avoid the worst-ca se behav-\nior of LRU, which we saw with the looping-sequential workload. Thus ,\nthe evolution of page-replacement algorithms continues.\nHowever, in many cases the importance of said algorithms has de-\ncreased, as the discrepancy between memory-access and disk- access times\nhas increased. Because paging to disk is so expensive, the cos t of frequent\npaging is prohibitive. Thus, the best solution to excessive pag ing is often\na simple (if intellectually unsatisfying) one: buy more memory .\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 BEYOND PHYSICAL MEMORY : POLICIES\nReferences\n[AD03] “Run-Time Adaptation in River” by Remzi H. Arpaci-Dusseau . ACM TOCS, 21:1,\nFebruary 2003. A summary of one of the authors’ dissertation work on a system named River, where\nhe learned that comparison against the ideal is an important technique for syste m designers.\n[B66] “A Study of Replacement Algorithms for Virtual-Storage Comp uter” by Laszlo A. Be-\nlady. IBM Systems Journal 5(2): 78-101, 1966. The paper that introduces the simple way to compute\nthe optimal behavior of a policy (the MIN algorithm).\n[BNS69] “An Anomaly in Space-time Characteristics of Certain Progra ms Running in a Paging\nMachine” by L. A. Belady, R. A. Nelson, G. S. Shedler. Communications of the ACM, 12:6, June\n1969. Introduction of the little sequence of memory references known as Belady’s An omaly. How do\nNelson and Shedler feel about this name, we wonder?\n[CD85] “An Evaluation of Buffer Management Strategies for Relatio nal Database Systems”\nby Hong-Tai Chou, David J. DeWitt. VLDB ’85, Stockholm, Sweden, Augu st 1985. A famous\ndatabase paper on the different buffering strategies you should use under a number of common database\naccess patterns. The more general lesson: if you know something about a workload, you can tailor\npolicies to do better than the general-purpose ones usually found in the OS.\n[C69] “A Paging Experiment with the Multics System” by F.J. Corbato. I ncluded in a Festschrift\npublished in honor of Prof. P .M. Morse. MIT Press, Cambridge, MA, 1969. The original (and\nhard to ﬁnd!) reference to the clock algorithm, though not the ﬁrst usage of a us e bit. Thanks to H.\nBalakrishnan of MIT for digging up this paper for us.\n[D70] “Virtual Memory” by Peter J. Denning. Computing Surveys, Vol. 2 , No. 3, September\n1970. Denning’s early and famous survey on virtual memory systems.\n[EF78] “Cold-start vs. Warm-start Miss Ratios” by Malcolm C. Easto n, Ronald Fagin. Commu-\nnications of the ACM, 21:10, October 1978. A good discussion of cold- vs. warm-start misses.\n[FP89] “Electrochemically Induced Nuclear Fusion of Deuterium” by Mar tin Fleischmann,\nStanley Pons. Journal of Electroanalytical Chemistry, Volume 26, Num ber 2, Part 1, April,\n1989. The famous paper that would have revolutionized the world in providing an easy w ay to generate\nnearly-inﬁnite power from jars of water with a little metal in them. Unfortunately , the results pub-\nlished (and widely publicized) by Pons and Fleischmann were impossi ble to reproduce, and thus these\ntwo well-meaning scientists were discredited (and certainly, mocked). Th e only guy really happy about\nthis result was Marvin Hawkins, whose name was left off this paper even thoug h he participated in the\nwork, thus avoiding association with one of the biggest scientiﬁc goofs of the 20th c entury.\n[HP06] “Computer Architecture: A Quantitative Approach” by John Hennessy and David\nPatterson. Morgan-Kaufmann, 2006. A marvelous book about computer architecture. Read it!\n[H87] “Aspects of Cache Memory and Instruction Buffer Performance” by Mark D . Hill. Ph.D.\nDissertation, U.C. Berkeley, 1987. Mark Hill, in his dissertation work, introduced the Three C’s,\nwhich later gained wide popularity with its inclusion in H&P [HP06]. The q uote from therein: “I have\nfound it useful to partition misses ... into three components intuitively b ased on the cause of the misses\n(page 49).”\n[KE+62] “One-level Storage System” by T. Kilburn, D.B.G. Edwards , M.J. Lanigan, F.H. Sum-\nner. IRE Trans. EC-11:2, 1962. Although Atlas had a use bit, it only had a very small number of pages,\nand thus the scanning of the use bits in large memories was not a problem the auth ors solved.\n[M+70] “Evaluation Techniques for Storage Hierarchies” by R. L. Matts on, J. Gecsei, D. R.\nSlutz, I. L. Traiger. IBM Systems Journal, Volume 9:2, 1970. A paper that is mostly about how to\nsimulate cache hierarchies efﬁciently; certainly a classic in that reg ard, as well for its excellent discussion\nof some of the properties of various replacement algorithms. Can you ﬁgure out w hy the stack property\nmight be useful for simulating a lot of different-sized caches at once?\n[MM03] “ARC: A Self-Tuning, Low Overhead Replacement Cache” by Nimrod Megid do and\nDharmendra S. Modha. FAST 2003, February 2003, San Jose, California .An excellent modern\npaper about replacement algorithms, which includes a new policy, ARC , that is now used in some\nsystems. Recognized in 2014 as a “Test of Time” award winner by the storage systems community at\nthe FAST ’14 conference.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nBEYOND PHYSICAL MEMORY : POLICIES 17\nHomework (Simulation)\nThis simulator, paging-policy.py , allows you to play around with\ndifferent page-replacement policies. See the README for detai ls.\nQuestions\n1. Generate random addresses with the following arguments: -s 0 -n 10 ,\n-s 1 -n 10 , and-s 2 -n 10 . Change the policy from FIFO, to LRU, to\nOPT. Compute whether each access in said address traces are hit s or misses.\n2. For a cache of size 5, generate worst-case address referen ce streams for\neach of the following policies: FIFO, LRU, and MRU (worst-cas e reference\nstreams cause the most misses possible. For the worst case refere nce streams,\nhow much bigger of a cache is needed to improve performance dramati cally\nand approach OPT?\n3. Generate a random trace (use python or perl). How would you exp ect the\ndifferent policies to perform on such a trace?\n4. Now generate a trace with some locality. How can you generat e such a\ntrace? How does LRU perform on it? How much better than RAND is\nLRU? How does CLOCK do? How about CLOCK with different numbers\nof clock bits?\n5. Use a program like valgrind to instrument a real application and gen-\nerate a virtual page reference stream. For example, running valgrind\n--tool=lackey --trace-mem=yes ls will output a nearly-complete\nreference trace of every instruction and data reference made b y the program\nls. To make this useful for the simulator above, you’ll have to ﬁrst tra ns-\nform each virtual memory reference into a virtual page-number refe rence\n(done by masking off the offset and shifting the resulting bits downward).\nHow big of a cache is needed for your application trace in order to satisfy a\nlarge fraction of requests? Plot a graph of its working set as t he size of the\ncache increases.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",40583
27-23. Complete VM Systems.pdf,27-23. Complete VM Systems,"23\nComplete Virtual Memory Systems\nBefore we end our study of virtualizing memory, let us take a closer look\nat how entire virtual memory systems are put together. We’ve seen key\nelements of such systems, including numerous page-table desi gns, inter-\nactions with the TLB (sometimes, even handled by the OS itself) , and\nstrategies for deciding which pages to keep in memory and which to kick\nout. However, there are many other features that comprise a comple te\nvirtual memory system, including numerous features for perform ance,\nfunctionality, and security. And thus, our crux:\nTHECRUX: HOWTOBUILD A C OMPLETE VM S YSTEM\nWhat features are needed to realize a complete virtual memory s ys-\ntem? How do they improve performance, increase security, or other wise\nimprove the system?\nWe’ll do this by covering two systems. The ﬁrst is one of the earli-\nest examples of a “modern” virtual memory manager, that found in t he\nV AX/VMS operating system [LL82], as developed in the 1970’s and early\n1980’s; a surprising number of techniques and approaches from th is sys-\ntem survive to this day, and thus it it is well worth studying. Som e ideas,\neven those that are 50 years old, are still worth knowing, a thought that\nis well known to those in most other ﬁelds (e.g., Physics), but has to be\nstated in technology-driven disciplines (e.g., Computer Scien ce).\nThe second is that of Linux , for reasons that should be obvious. Linux\nis a widely used system, and runs effectively on systems as sma ll and\nunderpowered as phones to the most scalable multicore systems fou nd\nin modern datacenters. Thus, its VM system must be ﬂexible enou gh to\nrun successfully in all of those scenarios. We will discuss each system to\nillustrate how concepts brought forth in earlier chapters come tog ether in\na complete memory manager.\n1\n2 C OMPLETE VIRTUAL MEMORY SYSTEMS\n23.1 VAX/VMS Virtual Memory\nThe VAX-11 minicomputer architecture was introduced in the la te 1970’s\nbyDigital Equipment Corporation (DEC ). DEC was a massive player\nin the computer industry during the era of the mini-computer; un fortu-\nnately, a series of bad decisions and the advent of the PC slowly (b ut\nsurely) led to their demise [C03]. The architecture was real ized in a num-\nber of implementations, including the VAX-11/780 and the less powerful\nVAX-11/750.\nThe OS for the system was known as VAX/VMS (or just plain VMS),\none of whose primary architects was Dave Cutler, who later led th e effort\nto develop Microsoft’s Windows NT [C93]. VMS had the general prob-\nlem that it would be run on a broad range of machines, including ver y\ninexpensive VAXen (yes, that is the proper plural) to extreme ly high-end\nand powerful machines in the same architecture family. Thus, the OS had\nto have mechanisms and policies that worked (and worked well) ac ross\nthis huge range of systems.\nAs an additional issue, VMS is an excellent example of software i nno-\nvations used to hide some of the inherent ﬂaws of the architecture . Al-\nthough the OS often relies on the hardware to build efﬁcient abst ractions\nand illusions, sometimes the hardware designers don’t quite get every-\nthing right; in the VAX hardware, we’ll see a few examples of thi s, and\nwhat the VMS operating system does to build an effective, workin g sys-\ntem despite these hardware ﬂaws.\nMemory Management Hardware\nThe VAX-11 provided a 32-bit virtual address space per process , divided\ninto 512-byte pages. Thus, a virtual address consisted of a 23- bit VPN\nand a 9-bit offset. Further, the upper two bits of the VPN were us ed to\ndifferentiate which segment the page resided within; thus, the system\nwas a hybrid of paging and segmentation, as we saw previously.\nThe lower-half of the address space was known as “process space” a nd\nis unique to each process. In the ﬁrst half of process space (known asP0),\nthe user program is found, as well as a heap which grows downward.\nIn the second half of process space ( P1), we ﬁnd the stack, which grows\nupwards. The upper-half of the address space is known as system space\n(S), although only half of it is used. Protected OS code and data resid e\nhere, and the OS is in this way shared across processes.\nOne major concern of the VMS designers was the incredibly small s ize\nof pages in the VAX hardware (512 bytes). This size, chosen for hi storical\nreasons, has the fundamental problem of making simple linear pa ge ta-\nbles excessively large. Thus, one of the ﬁrst goals of the VMS desi gners\nwas to ensure that VMS would not overwhelm memory with page tables .\nThe system reduced the pressure page tables place on memory in t wo\nways. First, by segmenting the user address space into two, th e VAX-11\nprovides a page table for each of these regions ( P0andP1) per process;\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCOMPLETE VIRTUAL MEMORY SYSTEMS 3\nASIDE : THECURSE OFGENERALITY\nOperating systems often have a problem known as the curse of gener-\nality , where they are tasked with general support for a broad class of\napplications and systems. The fundamental result of the curse is that the\nOS is not likely to support any one installation very well. In the c ase of\nVMS, the curse was very real, as the VAX-11 architecture was re alized\nin a number of different implementations. It is no less real toda y, where\nLinux is expected to run well on your phone, a TV set-top box, a laptop\ncomputer, desktop computer, and a high-end server running thous ands\nof processes in a cloud-based datacenter.\nthus, no page-table space is needed for the unused portion of the a ddress\nspace between the stack and the heap. The base and bounds regis ters\nare used as you would expect; a base register holds the address of t he\npage table for that segment, and the bounds holds its size (i.e., number of\npage-table entries).\nSecond, the OS reduces memory pressure even further by placing u ser\npage tables (for P0andP1, thus two per process) in kernel virtual mem-\nory. Thus, when allocating or growing a page table, the kernel all ocates\nspace out of its own virtual memory, in segment S. If memory comes un-\nder severe pressure, the kernel can swap pages of these page ta bles out to\ndisk, thus making physical memory available for other uses.\nPutting page tables in kernel virtual memory means that addre ss trans-\nlation is even further complicated. For example, to translate a virtual ad-\ndress inP0orP1, the hardware has to ﬁrst try to look up the page-table\nentry for that page in its page table (the P0orP1page table for that pro-\ncess); in doing so, however, the hardware may ﬁrst have to consult the\nsystem page table (which lives in physical memory); with that transla-\ntion complete, the hardware can learn the address of the page of th e page\ntable, and then ﬁnally learn the address of the desired memory a ccess.\nAll of this, fortunately, is made faster by the VAX’s hardware-m anaged\nTLBs, which usually (hopefully) circumvent this laborious looku p.\nA Real Address Space\nOne neat aspect of studying VMS is that we can see how a real addre ss\nspace is constructed (Figure 23.1. Thus far, we have assumed a simple\naddress space of just user code, user data, and user heap, but as we can\nsee above, a real address space is notably more complex.\nFor example, the code segment never begins at page 0. This page,\ninstead, is marked inaccessible, in order to provide some suppor t for de-\ntecting null-pointer accesses. Thus, one concern when designing an ad-\ndress space is support for debugging, which the inaccessible z ero page\nprovides here in some form.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 C OMPLETE VIRTUAL MEMORY SYSTEMS\nPage 0: Invalid\nUser Code\nUser Heap\nUser Stack\nTrap Tables\nKernel Data\nKernel Code\nKernel Heap\nUnusedSystem (S)User (P1)User (P0)0\n230\n231\n232\nFigure 23.1: The V AX/VMS Address Space\nPerhaps more importantly, the kernel virtual address space (i .e., its\ndata structures and code) is a part of each user address space. O n a con-\ntext switch, the OS changes the P0andP1registers to point to the ap-\npropriate page tables of the soon-to-be-run process; however, it doe s not\nchange the Sbase and bound registers, and as a result the “same” kernel\nstructures are mapped into each user address space.\nThe kernel is mapped into each address space for a number of reas ons.\nThis construction makes life easier for the kernel; when, for exa mple, the\nOS is handed a pointer from a user program (e.g., on a write() system\ncall), it is easy to copy data from that pointer to its own structur es. The\nOS is naturally written and compiled, without worry of where the d ata\nit is accessing comes from. If in contrast the kernel were located entirely\nin physical memory, it would be quite hard to do things like swap pages\nof the page table to disk; if the kernel were given its own addres s space,\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCOMPLETE VIRTUAL MEMORY SYSTEMS 5\nASIDE : W HYNULL POINTER ACCESSES CAUSE SEGFAULTS\nYou should now have a good understanding of exactly what happens on\na null-pointer dereference. A process generates a virtual add ress of 0, by\ndoing something like this:\nint*p = NULL; // set p = 0\n*p = 10; // try to store 10 to virtual addr 0\nThe hardware tries to look up the VPN (also 0 here) in the TLB, and suf-\nfers a TLB miss. The page table is consulted, and the entry for VP N 0\nis found to be marked invalid. Thus, we have an invalid access, which\ntransfers control to the OS, which likely terminates the process (on U NIX\nsystems, processes are sent a signal which allows them to react to such a\nfault; if uncaught, however, the process is killed).\nmoving data between user applications and the kernel would agai n be\ncomplicated and painful. With this construction (now used widel y), the\nkernel appears almost as a library to applications, albeit a pr otected one.\nOne last point about this address space relates to protection. Cl early,\nthe OS does not want user applications reading or writing OS data or\ncode. Thus, the hardware must support different protection leve ls for\npages to enable this. The VAX did so by specifying, in protecti on bits\nin the page table, what privilege level the CPU must be at in ord er to\naccess a particular page. Thus, system data and code are set to a higher\nlevel of protection than user data and code; an attempted access t o such\ninformation from user code will generate a trap into the OS, and (you\nguessed it) the likely termination of the offending process.\nPage Replacement\nThe page table entry (PTE) in VAX contains the following bits: a v alid\nbit, a protection ﬁeld (4 bits), a modify (or dirty) bit, a ﬁeld res erved for\nOS use (5 bits), and ﬁnally a physical frame number (PFN) to st ore the\nlocation of the page in physical memory. The astute reader might n ote:\nnoreference bit ! Thus, the VMS replacement algorithm must make do\nwithout hardware support for determining which pages are activ e.\nThe developers were also concerned about memory hogs , programs\nthat use a lot of memory and make it hard for other programs to run.\nMost of the policies we have looked at thus far are susceptible to su ch\nhogging; for example, LRU is a global policy that doesn’t share memory\nfairly among processes.\nTo address these two problems, the developers came up with the seg-\nmented FIFO replacement policy [RL81]. The idea is simple: each process\nhas a maximum number of pages it can keep in memory, known as its res-\nident set size (RSS ). Each of these pages is kept on a FIFO list; when a\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 C OMPLETE VIRTUAL MEMORY SYSTEMS\nASIDE : EMULATING REFERENCE BITS\nAs it turns out, you don’t need a hardware reference bit in order to g et\nsome notion of which pages are in use in a system. In fact, in the ear ly\n1980’s, Babaoglu and Joy showed that protection bits on the VAX can be\nused to emulate reference bits [BJ81]. The basic idea: if you w ant to gain\nsome understanding of which pages are actively being used in a s ystem,\nmark all of the pages in the page table as inaccessible (but kee p around\nthe information as to which pages are really accessible by the p rocess,\nperhaps in the “reserved OS ﬁeld” portion of the page table entry ). When\na process accesses a page, it will generate a trap into the OS; th e OS will\nthen check if the page really should be accessible, and if so, re vert the\npage to its normal protections (e.g., read-only, or read-write). At the time\nof a replacement, the OS can check which pages remain marked in acces-\nsible, and thus get an idea of which pages have not been recently used.\nThe key to this “emulation” of reference bits is reducing overhe ad while\nstill obtaining a good idea of page usage. The OS must not be too aggre s-\nsive in marking pages inaccessible, or overhead would be too high . The\nOS also must not be too passive in such marking, or all pages will e nd up\nreferenced; the OS will again have no good idea which page to evi ct.\nprocess exceeds its RSS, the “ﬁrst-in” page is evicted. FIFO cle arly does\nnot need any support from the hardware, and is thus easy to implem ent.\nOf course, pure FIFO does not perform particularly well, as we saw\nearlier. To improve FIFO’s performance, VMS introduced two second-\nchance lists where pages are placed before getting evicted from memory,\nspeciﬁcally a global clean-page free list and dirty-page list . When a process\nPexceeds its RSS, a page is removed from its per-process FIFO; if cle an\n(not modiﬁed), it is placed on the end of the clean-page list; if di rty (mod-\niﬁed), it is placed on the end of the dirty-page list.\nIf another process Qneeds a free page, it takes the ﬁrst free page off\nof the global clean list. However, if the original process Pfaults on that\npage before it is reclaimed, Preclaims it from the free (or dirty) list, thus\navoiding a costly disk access. The bigger these global second-ch ance lists\nare, the closer the segmented FIFO algorithm performs to LRU [RL 81].\nAnother optimization used in VMS also helps overcome the small pag e\nsize in VMS. Speciﬁcally, with such small pages, disk I/O durin g swap-\nping could be highly inefﬁcient, as disks do better with large transfers.\nTo make swapping I/O more efﬁcient, VMS adds a number of optimiza -\ntions, but most important is clustering . With clustering, VMS groups\nlarge batches of pages together from the global dirty list, and wr ites them\nto disk in one fell swoop (thus making them clean). Clustering is used\nin most modern systems, as the freedom to place pages anywhere wi thin\nswap space lets the OS group pages, perform fewer and bigger wri tes,\nand thus improve performance.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCOMPLETE VIRTUAL MEMORY SYSTEMS 7\nOther Neat Tricks\nVMS had two other now-standard tricks: demand zeroing and copy-on -\nwrite. We now describe these lazy optimizations. One form of laziness\nin VMS (and most modern systems) is demand zeroing of pages. To un-\nderstand this better, let’s consider the example of adding a pag e to your\naddress space, say in your heap. In a naive implementation, the OS re-\nsponds to a request to add a page to your heap by ﬁnding a page in ph ys-\nical memory, zeroing it (required for security; otherwise you’d be able to\nsee what was on the page from when some other process used it!), and\nthen mapping it into your address space (i.e., setting up the p age table to\nrefer to that physical page as desired). But the naive implem entation can\nbe costly, particularly if the page does not get used by the proces s.\nWith demand zeroing, the OS instead does very little work when th e\npage is added to your address space; it puts an entry in the page table that\nmarks the page inaccessible. If the process then reads or write s the page,\na trap into the OS takes place. When handling the trap, the OS n otices\n(usually through some bits marked in the “reserved for OS” portion of the\npage table entry) that this is actually a demand-zero page; a t this point,\nthe OS does the needed work of ﬁnding a physical page, zeroing it, a nd\nmapping it into the process’s address space. If the process neve r accesses\nthe page, all such work is avoided, and thus the virtue of demand z eroing.\nAnother cool optimization found in VMS (and again, in virtually eve ry\nmodern OS) is copy-on-write (COW for short). The idea, which goes at\nleast back to the TENEX operating system [BB+72], is simple: w hen the\nOS needs to copy a page from one address space to another, instead of\ncopying it, it can map it into the target address space and mark it read-\nonly in both address spaces. If both address spaces only read the p age, no\nfurther action is taken, and thus the OS has realized a fast copy without\nactually moving any data.\nIf, however, one of the address spaces does indeed try to write to t he\npage, it will trap into the OS. The OS will then notice that the pa ge is a\nCOW page, and thus (lazily) allocate a new page, ﬁll it with the data, and\nmap this new page into the address space of the faulting process . The\nprocess then continues and now has its own private copy of the page.\nCOW is useful for a number of reasons. Certainly any sort of shared\nlibrary can be mapped copy-on-write into the address spaces of m any\nprocesses, saving valuable memory space. In U NIX systems, COW is\neven more critical, due to the semantics of fork() andexec() . As\nyou might recall, fork() creates an exact copy of the address space of\nthe caller; with a large address space, making such a copy is sl ow and\ndata intensive. Even worse, most of the address space is immedia tely\nover-written by a subsequent call to exec() , which overlays the calling\nprocess’s address space with that of the soon-to-be-exec’d program. By\ninstead performing a copy-on-write fork() , the OS avoids much of the\nneedless copying and thus retains the correct semantics while improving\nperformance.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 C OMPLETE VIRTUAL MEMORY SYSTEMS\nTIP: BELAZY\nBeing lazy can be a virtue in both life as well as in operating sys tems.\nLaziness can put off work until later, which is beneﬁcial withi n an OS for\na number of reasons. First, putting off work might reduce the late ncy of\nthe current operation, thus improving responsiveness; for examp le, op-\nerating systems often report that writes to a ﬁle succeeded imm ediately,\nand only write them to disk later in the background. Second, and mor e\nimportantly, laziness sometimes obviates the need to do the work at all;\nfor example, delaying a write until the ﬁle is deleted removes t he need to\ndo the write at all. Laziness is also good in life: for example, by putting\noff your OS project, you may ﬁnd that the project speciﬁcation bugs a re\nworked out by your fellow classmates; however, the class project is un-\nlikely to get canceled, so being too lazy may be problematic, le ading to a\nlate project, bad grade, and a sad professor. Don’t make professors s ad!\n23.2 The Linux Virtual Memory System\nWe’ll now discuss some of the more interesting aspects of the Linux\nVM system. Linux development has been driven forward by real en gi-\nneers solving real problems encountered in production, and thus a large\nnumber of features have slowly been incorporated into what is now a\nfully functional, feature-ﬁlled virtual memory system.\nWhile we won’t be able to discuss every aspect of Linux VM, we’ll\ntouch on the most important ones, especially where it has gone beyond\nwhat is found in classic VM systems such as VAX/VMS. We’ll also tr y to\nhighlight commonalities between Linux and older systems.\nFor this discussion, we’ll focus on Linux for Intel x86. While Linux can\nand does run on many different processor architectures, Linux on x 86 is\nits most dominant and important deployment, and thus the focus of our\nattention.\nThe Linux Address Space\nMuch like other modern operating systems, and also like VAX/VMS,\na Linux virtual address space1consists of a user portion (where user\nprogram code, stack, heap, and other parts reside) and a kernel p ortion\n(where kernel code, stacks, heap, and other parts reside). Lik e those other\nsystems, upon a context switch, the user portion of the currently- running\naddress space changes; the kernel portion is the same across proc esses.\nLike those other systems, a program running in user mode cannot acc ess\nkernel virtual pages; only by trapping into the kernel and tra nsitioning to\nprivileged mode can such memory be accessed.\n1Until recent changes, due to security threats, that is. Read the subsecti ons below about\nLinux security for details on this modiﬁcation.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCOMPLETE VIRTUAL MEMORY SYSTEMS 9\nPage 0: Invalid\nUser Code\nUser Heap\nUser Stack\nKernel (Logical)\nKernel (Virtual)User\nKernel0x00000000\n0xC0000000\nFigure 23.2: The Linux Address Space\nIn classic 32-bit Linux (i.e., Linux with a 32-bit virtual ad dress space),\nthe split between user and kernel portions of the address space t akes\nplace at address 0xC0000000 , or three-quarters of the way through the\naddress space. Thus, virtual addresses 0through0xBFFFFFFF are user\nvirtual addresses; the remaining virtual addresses ( 0xC0000000 through\n0xFFFFFFFF ) are in the kernel’s virtual address space. 64-bit Linux has a\nsimilar split but at slightly different points. Figure 23.2 s hows a depiction\nof a typical (simpliﬁed) address space.\nOne slightly interesting aspect of Linux is that it contains tw o types of\nkernel virtual addresses. The ﬁrst are known as kernel logical addresses\n[O16]. This is what you would consider the normal virtual address space\nof the kernel; to get more memory of this type, kernel code merely ne eds\nto callkmalloc . Most kernel data structures live here, such as page ta-\nbles, per-process kernel stacks, and so forth. Unlike most other memory\nin the system, kernel logical memory cannot be swapped to disk.\nThe most interesting aspect of kernel logical addresses is thei r con-\nnection to physical memory. Speciﬁcally, there is a direct mapp ing be-\ntween kernel logical addresses and the ﬁrst portion of physical m emory.\nThus, kernel logical address 0xC0000000 translates to physical address\n0x00000000 ,0xC0000FFF to0x00000FFF , and so forth. This direct\nmapping has two implications. The ﬁrst is that it is simple to t ranslate\nback and forth between kernel logical addresses and physical a ddresses;\nas a result, these addresses are often treated as if they are in deed physi-\ncal. The second is that if a chunk of memory is contiguous in kernel l og-\nical address space, it is also contiguous in physical memory. Th is makes\nmemory allocated in this part of the kernel’s address space suita ble for\noperations which need contiguous physical memory to work correctly ,\nsuch as I/O transfers to and from devices via directory memory access\n(DMA ) (something we’ll learn about in the third part of this book).\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 C OMPLETE VIRTUAL MEMORY SYSTEMS\nThe other type of kernel address is a kernel virtual address . To get\nmemory of this type, kernel code calls a different allocator, vmalloc ,\nwhich returns a pointer to a virtually contiguous region of the des ired\nsize. Unlike kernel logical memory, kernel virtual memory is us ually not\ncontiguous; each kernel virtual page may map to non-contiguous ph ysi-\ncal pages (and is thus not suitable for DMA). However, such memory is\neasier to allocate as a result, and thus used for large buffers w here ﬁnding\na contiguous large chunk of physical memory would be challenging.\nIn 32-bit Linux, one other reason for the existence of kernel virtu al\naddresses is that they enable the kernel to address more than ( roughly) 1\nGB of memory. Years ago, machines had much less memory than this, a nd\nenabling access to more than 1 GB was not an issue. However, techn ology\nprogressed, and soon there was a need to enable the kernel to use l arger\namounts of memory. Kernel virtual addresses, and their disconne ction\nfrom a strict one-to-one mapping to physical memory, make this poss ible.\nHowever, with the move to 64-bit Linux, the need is less urgent, because\nthe kernel is not conﬁned to only the last 1 GB of the virtual addres s space.\nPage Table Structure\nBecause we are focused on Linux for x86, our discussion will center on\nthe type of page-table structure provided by x86, as it determi nes what\nLinux can and cannot do. As mentioned before, x86 provides a hardwa re-\nmanaged, multi-level page table structure, with one page tab le per pro-\ncess; the OS simply sets up mappings in its memory, points a priv ileged\nregister at the start of the page directory, and the hardware ha ndles the\nrest. The OS gets involved, as expected, at process creation, de letion, and\nupon context switches, making sure in each case that the correct page\ntable is being used by the hardware MMU to perform translations .\nProbably the biggest change in recent years is the move from 32-b it\nx86 to 64-bit x86, as brieﬂy mentioned above. As seen in the VAX/ VMS\nsystem, 32-bit address spaces have been around for a long time, a nd as\ntechnology changed, they were ﬁnally starting to become a real l imit for\nprograms. Virtual memory makes it easy to program systems, but w ith\nmodern systems containing many GB of memory, 32 bits were no longer\nenough to refer to each of them. Thus, the next leap became neces sary.\nMoving to a 64-bit address affects page table structure in x86 in the\nexpected manner. Because x86 uses a multi-level page table, current 64-\nbit systems use a four-level table. The full 64-bit nature of th e virtual\naddress space is not yet in use, however, rather only the bottom 48 b its.\nThus, a virtual address can be viewed as follows:\n63 47 31 15 0\nUnused P1 P2 P3 P4 Offset\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCOMPLETE VIRTUAL MEMORY SYSTEMS 11\nAs you can see in the picture, the top 16 bits of a virtual address a re\nunused (and thus play no role in translation), the bottom 12 bits ( due to\nthe 4-KB page size) are used as the offset (and hence just used d irectly,\nand not translated), leaving the middle 36 bits of virtual addr ess to take\npart in the translation. The P1 portion of the address is used to in dex into\nthe topmost page directory, and the translation proceeds from ther e, one\nlevel at a time, until the actual page of the page table is index ed by P4,\nyielding the desired page table entry.\nAs system memories grow even larger, more parts of this voluminous\naddress space will become enabled, leading to ﬁve-level and e ventually\nsix-level page-table tree structures. Imagine that: a simp le page table\nlookup requiring six levels of translation, just to ﬁgure out wher e in mem-\nory a certain piece of data resides.\nLarge Page Support\nIntel x86 allows for the use of multiple page sizes, not just the st andard 4-\nKB page. Speciﬁcally, recent designs support 2-MB and even 1-G B pages\nin hardware. Thus, over time, Linux has evolved to allow applica tions to\nutilize these huge pages (as they are called in the world of Linux).\nUsing huge pages, as hinted at earlier, leads to numerous bene ﬁts. As\nseen in VAX/VMS, doing so reduces the number of mappings that are\nneeded in the page table; the larger the pages, the fewer the m appings.\nHowever, fewer page-table entries is not the driving force behi nd huge\npages; rather, it’s better TLB behavior and related performanc e gains.\nWhen a process actively uses a large amount of memory, it quickly\nﬁlls up the TLB with translations. If those translations are for 4 -KB pages,\nonly a small amount of total memory can be accessed without inducing\nTLB misses. The result, for modern “big memory” workloads running on\nmachines with many GBs of memory, is a noticeable performance cost ;\nrecent research shows that some applications spend 10% of their c ycles\nservicing TLB misses [B+13].\nHuge pages allow a process to access a large tract of memory with-\nout TLB misses, by using fewer slots in the TLB, and thus is the ma in\nadvantage. However, there are other beneﬁts to huge pages: the re is a\nshorter TLB-miss path, meaning that when a TLB miss does occur, i t is\nserviced more quickly. In addition, allocation can be quite fast (in certain\nscenarios), a small but sometimes important beneﬁt.\nOne interesting aspect of Linux support for huge pages is how it wa s\ndone incrementally. At ﬁrst, Linux developers knew such suppor t was\nonly important for a few applications, such as large databases wi th strin-\ngent performance demands. Thus, the decision was made to allow a ppli-\ncations to explicitly request memory allocations with large pag es (either\nthrough the mmap() orshmget() calls). In this way, most applications\nwould be unaffected (and continue to use only 4-KB pages; a few de -\nmanding applications would have to be changed to use these inte rfaces,\nbut for them it would be worth the pain.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 C OMPLETE VIRTUAL MEMORY SYSTEMS\nTIP: CONSIDER INCREMENTALISM\nMany times in life, you are encouraged to be a revolutionary. “Thi nk\nbig!”, they say. “Change the world!”, they scream. And you can see why\nit is appealing; in some cases, big changes are needed, and thu s pushing\nhard for them makes a lot of sense. And, if you try it this way, at lea st\nthey might stop yelling at you.\nHowever, in many cases, a slower, more incremental approach migh t be\nthe right thing to do. The Linux huge page example in this chapt er is\nan example of engineering incrementalism; instead of taking t he stance\nof a fundamentalist and insisting large pages were the way of th e future,\ndevelopers took the measured approach of ﬁrst introducing special ized\nsupport for it, learning more about its upsides and downsides, and , only\nwhen there was real reason for it, adding more generic support for a ll\napplications.\nIncrementalism, while sometimes scorned, often leads to slow, t hought-\nful, and sensible progress. When building systems, such an ap proach\nmight just be the thing you need. Indeed, this may be true in lif e as well.\nMore recently, as the need for better TLB behavior is more common\namong many applications, Linux developers have added transparent huge\npage support. When this feature is enabled, the operating syst em auto-\nmatically looks for opportunities to allocate huge pages (usually 2 MB,\nbut on some systems, 1 GB) without requiring application modiﬁcat ion.\nHuge pages are not without their costs. The biggest potential cost is\ninternal fragmentation , i.e., a page that is large but sparsely used. This\nform of waste can ﬁll memory with large but little used pages. Swap ping,\nif enabled, also does not work well with huge pages, sometimes gre atly\namplifying the amount of I/O a system does. Overhead of allocation\ncan also be bad (in some other cases). Overall, one thing is clear : the 4-\nKB page size which served systems so well for so many years is not the\nuniversal solution it once was; growing memory sizes demand that w e\nconsider large pages and other solutions as part of a necessary evol ution\nof VM systems. Linux’s slow adoption of this hardware-based technol ogy\nis evidence of the coming change.\nThe Page Cache\nTo reduce costs of accessing persistent storage (the focus of the t hird part\nof this book), most systems use aggressive caching subsystems to keep\npopular data items in memory. Linux, in this regard, is no diffe rent than\ntraditional operating systems.\nThe Linux page cache is uniﬁed, keeping pages in memory from three\nprimary sources: memory-mapped ﬁles , ﬁle data and metadata from de-\nvices (usually accessed by directing read() andwrite() calls to the ﬁle\nsystem), and heap and stack pages that comprise each process (s ometimes\ncalled anonymous memory , because there is no named ﬁle underneath of\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCOMPLETE VIRTUAL MEMORY SYSTEMS 13\nASIDE : THEUBIQUITY OFMEMORY -MAPPING\nMemory mapping predates Linux by some years, and is used in many\nplaces within Linux and other modern systems. The idea is simpl e: by\ncallingmmap() on an already opened ﬁle descriptor, a process is returned\na pointer to the beginning of a region of virtual memory where the con -\ntents of the ﬁle seem to be located. By then using that pointer, a p rocess\ncan access any part of the ﬁle with a simple pointer dereference .\nAccesses to parts of a memory-mapped ﬁle that have not yet been br ought\ninto memory trigger page faults , at which point the OS will page in the\nrelevant data and make it accessible by updating the page tab le of the\nprocess accordingly (i.e., demand paging ).\nEvery regular Linux process uses memory-mapped ﬁles, even the code\ninmain() does not call mmap() directly, because of how Linux loads\ncode from the executable and shared library code into memory. Bel ow\nis the (highly abbreviated) output of the pmap command line tool, which\nshows what different mapping comprise the virtual address spa ce of a\nrunning program (the shell, in this example, tcsh ). The output shows\nfour columns: the virtual address of the mapping, its size, the p rotection\nbits of the region, and the source of the mapping:\n0000000000400000 372K r-x-- tcsh\n00000000019d5000 1780K rw--- [anon ]\n00007f4e7cf06000 1792K r-x-- libc-2.23.so\n00007f4e7d2d0000 36K r-x-- libcrypt-2.23.so\n00007f4e7d508000 148K r-x-- libtinfo.so.5.9\n00007f4e7d731000 152K r-x-- ld-2.23.so\n00007f4e7d932000 16K rw--- [stack ]\nAs you can see from this output, the code from the tcsh binary, as well\nas code from libc ,libcrypt ,libtinfo , and code from the dynamic\nlinker itself ( ld.so ) are all mapped into the address space. Also present\nare two anonymous regions, the heap (the second entry, labeled anon )\nand the stack (labeled stack ). Memory-mapped ﬁles provide a straight-\nforward and efﬁcient way for the OS to construct a modern address s pace.\nit, but rather swap space). These entities are kept in a page cache hash\ntable , allowing for quick lookup when said data is needed.\nThe page cache tracks if entries are clean (read but not updated) or\ndirty (a.k.a., modiﬁed ). Dirty data is periodically written to the back-\ning store (i.e., to a speciﬁc ﬁle for ﬁle data, or to swap space for a nony-\nmous regions) by background threads (called pdflush ), thus ensuring\nthat modiﬁed data eventually is written back to persistent st orage. This\nbackground activity either takes place after a certain time p eriod or if too\nmany pages are considered dirty (both conﬁgurable parameters) .\nIn some cases, a system runs low on memory, and Linux has to decide\nwhich pages to kick out of memory to free up space. To do so, Linux us es\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 C OMPLETE VIRTUAL MEMORY SYSTEMS\na modiﬁed form of 2Qreplacement [JS94], which we describe here.\nThe basic idea is simple: standard LRU replacement is effect ive, but\ncan be subverted by certain common access patterns. For example , if a\nprocess repeatedly accesses a large ﬁle (especially one that i s nearly the\nsize of memory, or larger), LRU will kick every other ﬁle out of memory .\nEven worse: retaining portions of this ﬁle in memory isn’t useful, a s they\nare never re-referenced before getting kicked out of memory.\nThe Linux version of the 2Q replacement algorithm solves this prob -\nlem by keeping two lists, and dividing memory between them. Wh en\naccessed for the ﬁrst time, a page is placed on one queue (called A1in the\noriginal paper, but the inactive list in Linux); when it is re-referenced, the\npage is promoted to the other queue (called Aqin the original, but the ac-\ntive list in Linux). When replacement needs to take place, the candida te\nfor replacement is taken from the inactive list. Linux also per iodically\nmoves pages from the bottom of the active list to the inactive list, keeping\nthe active list to about two-thirds of the total page cache size [G 04].\nLinux would ideally manage these lists in perfect LRU order, bu t, as\ndiscussed in earlier chapters, doing so is costly. Thus, as wit h many OSes,\nan approximation of LRU (similar to clock replacement) is used.\nThis 2Q approach generally behaves quite a bit like LRU, but not ably\nhandles the case where a cyclic large-ﬁle access occurs by conﬁ ning the\npages of that cyclic access to the inactive list. Because said pages are never\nre-referenced before getting kicked out of memory, they do not ﬂus h out\nother useful pages found in the active list.\nSecurity And Buffer Overﬂows\nProbably the biggest difference between modern VM systems (Li nux, So-\nlaris, or one of the BSD variants) and ancient ones (VAX/VMS) is the\nemphasis on security in the modern era. Protection has always bee n\na serious concern for operating systems, but with machines more in ter-\nconnected than ever, it is no surprise that developers have imp lemented\na variety of defensive countermeasures to halt those wily hacke rs from\ngaining control of systems.\nOne major threat is found in buffer overﬂow attacks [W18], which can\nbe used against normal user programs and even the kernel itself . The idea\nof these attacks is to ﬁnd a bug in the target system which lets t he attacker\ninject arbitrary data into the target’s address space. Such vu lnerabilities\nsometime arise because the developer assumes (erroneously) tha t an in-\nput will not be overly long, and thus (trustingly) copies the inpu t into a\nbuffer; because the input is in fact too long, it overﬂows the buff er, thus\noverwriting memory of the target. Code as innocent as the below can b e\nthe source of the problem:\nint some_function(char *input) {\nchar dest_buffer[100];\nstrcpy(dest_buffer, input); // oops, unbounded copy!\n}\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCOMPLETE VIRTUAL MEMORY SYSTEMS 15\nIn many cases, such an overﬂow is not catastrophic, e.g., bad inpu t\ninnocently given to a user program or even the OS will probably cau se it\nto crash, but no worse. However, malicious programmers can caref ully\ncraft the input that overﬂows the buffer so as to inject their own code\ninto the targeted system, essentially allowing them to take i t over and\ndo their own bidding. If successful upon a network-connected use r pro-\ngram, attackers can run arbitrary computations or even rent out c ycles on\nthe compromised system; if successful upon the operating system itself,\nthe attack can access even more resources, and is a form of what is c alled\nprivilege escalation (i.e., user code gaining kernel access rights). If you\ncan’t guess, these are all Bad Things.\nThe ﬁrst and most simple defense against buffer overﬂow is to pre vent\nexecution of any code found within certain regions of an address spa ce\n(e.g., within the stack). The NX bit (for No-eXecute), introduced by AMD\ninto their version of x86 (a similar XD bit is now available on Inte l’s), is\none such defense; it just prevents execution from any page which has this\nbit set in its corresponding page table entry. The approach prev ents code,\ninjected by an attacker into the target’s stack, from being exe cuted, and\nthus mitigates the problem.\nHowever, clever attackers are ... clever, and even when injec ted code\ncannot be added explicitly by the attacker, arbitrary code seq uences can\nbe executed by malicious code. The idea is known, in its most gener al\nform, as a return-oriented programming (ROP ) [S07], and really it is\nquite brilliant. The observation behind ROP is that there are l ots of bits of\ncode ( gadgets , in ROP terminology) within any program’s address space,\nespecially C programs that link with the voluminous C library. T hus,\nan attacker can overwrite the stack such that the return addre ss in the\ncurrently executing function points to a desired malicious ins truction (or\nseries of instructions), followed by a return instruction. By str inging to-\ngether a large number of gadgets (i.e., ensuring each return j umps to the\nnext gadget), the attacker can execute arbitrary code. Amazi ng!\nTo defend against ROP (including its earlier form, the return-to-libc\nattack [S+04]), Linux (and other systems) add another defense, known\nasaddress space layout randomization (ASLR ). Instead of placing code,\nstack, and the heap at ﬁxed locations within the virtual addres s space, the\nOS randomizes their placement, thus making it quite challeng ing to craft\nthe intricate code sequence required to implement this class of attacks.\nMost attacks on vulnerable user programs will thus cause crashe s, but\nnot be able to gain control of the running program.\nInterestingly, you can observe this randomness in practice rat her eas-\nily. Here’s a piece of code that demonstrates it on a modern Linux sys tem:\nint main(int argc, char *argv[]) {\nint stack = 0;\nprintf(""%p\n"", &stack);\nreturn 0;\n}\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 C OMPLETE VIRTUAL MEMORY SYSTEMS\nThis code just prints out the (virtual) address of a variable on th e stack.\nIn older non-ASLR systems, this value would be the same each time. But,\nas you can see below, the value changes with each run:\nprompt> ./random\n0x7ffd3e55d2b4\nprompt> ./random\n0x7ffe1033b8f4\nprompt> ./random\n0x7ffe45522e94\nASLR is such a useful defense for user-level programs that it has also\nbeen incorporated into the kernel, in a feature unimaginative ly called ker-\nnel address space layout randomization (KASLR ). However, it turns out\nthe kernel may have even bigger problems to handle, as we discu ss next.\nOther Security Problems: Meltdown And Spectre\nAs we write these words (August, 2018), the world of systems secu rity\nhas been turned upside down by two new and related attacks. The ﬁrst\nis called Meltdown , and the second Spectre . They were discovered at\nabout the same time by four different groups of researchers/engi neers,\nand have led to deep questioning of the fundamental protections of fered\nby computer hardware and the OS above. See meltdownattack.com\nandspectreattack.com for papers describing each attack in detail.\nSpectre is considered the more problematic of the two.\nThe general weakness exploited in each of these attacks is that the\nCPUs found in modern systems perform all sorts of crazy behind-the -\nscenes tricks to improve performance. One class of technique th at lies\nat the core of the problem is called speculative execution , in which the\nCPU guesses which instructions will soon be executed in the futu re, and\nstarts executing them ahead of time. If the guesses are correct , the pro-\ngram runs faster; if not, the CPU undoes their effects on archite ctural state\n(e.g., registers) tries again, this time going down the right p ath.\nThe problem with speculation is that it tends to leave traces of i ts ex-\necution in various parts of the system, such as processor caches, b ranch\npredictors, etc. And thus the problem: as the authors of the attac ks show,\nsuch state can make vulnerable the contents of memory, even memor y\nthat we thought was protected by the MMU.\nOne avenue to increasing kernel protection was thus to remove as\nmuch of the kernel address space from each user process and inste ad have\na separate kernel page table for most kernel data (called kernel page-\ntable isolation , orKPTI ) [G+17]. Thus, instead of mapping the kernel’s\ncode and data structures into each process, only the barest mini mum is\nkept therein; when switching into the kernel, then, a switch to the kernel\npage table is now needed. Doing so improves security and avoids som e\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCOMPLETE VIRTUAL MEMORY SYSTEMS 17\nattack vectors, but at a cost: performance. Switching page table s is costly.\nAh, the costs of security: convenience andperformance.\nUnfortunately, KPTI doesn’t solve all of the security problems lai d out\nabove, just some of them. And simple solutions, such as turning off s pec-\nulation, would make little sense, because systems would run thou sands\nof times slower. Thus, it is an interesting time to be alive, if s ystems secu-\nrity is your thing.\nTo truly understand these attacks, you’ll (likely) have to lea rn a lot\nmore ﬁrst. Begin by understanding modern computer architectur e, as\nfound in advanced books on the topic, focusing on speculation and all t he\nmechanisms needed to implement it. Deﬁnitely read about the M eltdown\nand Spectre attacks, at the websites mentioned above; they actu ally also\ninclude a useful primer on speculation, so perhaps are not a bad p lace to\nstart. And study the operating system for further vulnerabili ties. Who\nknows what problems remain?\n23.3 Summary\nYou have now seen a top-to-bottom review of two virtual memory sys-\ntems. Hopefully, most of the details were easy to follow, as you shoul d\nhave already had a good understanding of the basic mechanisms an d\npolicies. More detail on VAX/VMS is available in the excellent ( and short)\npaper by Levy and Lipman [LL82]. We encourage you to read it, as i t is a\ngreat way to see what the source material behind these chapter s is like.\nYou have also learned a bit about Linux. While a large and complex\nsystem, it inherits many good ideas from the past, many of which we\nhave not had room to discuss in detail. For example, Linux performs lazy\ncopy-on-write copying of pages upon fork() , thus lowering overheads\nby avoiding unnecessary copying. Linux also demand zeroes page s (us-\ning memory-mapping of the /dev/zero device), and has a background\nswap daemon ( swapd ) that swaps pages to disk to reduce memory pres-\nsure. Indeed, the VM is ﬁlled with good ideas taken from the past, and\nalso includes many of its own innovations.\nTo learn more, check out these reasonable (but, alas, outdated) b ooks\n[BC05,G04]. We encourage you to read them on your own, as we can\nonly provide the merest drop from what is an ocean of complexity. But,\nyou’ve got to start somewhere. What is any ocean, but a multitude of\ndrops? [M04]\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n18 C OMPLETE VIRTUAL MEMORY SYSTEMS\nReferences\n[B+13] “Efﬁcient Virtual Memory for Big Memory Servers” by A. Basu, J. Gandhi, J. Chang,\nM. D. Hill, M. M. Swift. ISCA ’13, June 2013, Tel-Aviv, Israel. A recent work showing that TLBs\nmatter, consuming 10% of cycles for large-memory workloads. The solution: one m assive segment to\nhold large data sets. We go backward, so that we can go forward!\n[BB+72] “TENEX, A Paged Time Sharing System for the PDP-10” by D. G. Bobro w, J. D. Burch-\nﬁel, D. L. Murphy, R. S. Tomlinson. CACM, Volume 15, March 1972. An early time-sharing OS\nwhere a number of good ideas came from. Copy-on-write was just one of those; also an inspiration for\nother aspects of modern systems, including process management, virtual me mory, and ﬁle systems.\n[BJ81] “Converting a Swap-Based System to do Paging in an Architecture L acking Page-Reference\nBits” by O. Babaoglu, W. N. Joy. SOSP ’81, Paciﬁc Grove, California, D ecember 1981. How to\nexploit existing protection machinery to emulate reference bits, from a g roup at Berkeley working on\ntheir own version of UNIX: the Berkeley Systems Distribution (BSD ). The group was inﬂuential in\nthe development of virtual memory, ﬁle systems, and networking.\n[BC05] “Understanding the Linux Kernel” by D. P . Bovet, M. Cesati. O’Re illy Media, Novem-\nber 2005. One of the many books you can ﬁnd on Linux, which are out of date, but still worthwhi le.\n[C03] “The Innovator’s Dilemma” by Clayton M. Christenson. Harper Pape rbacks, January\n2003. A fantastic book about the disk-drive industry and how new innovations disrupt ex isting ones.\nA good read for business majors and computer scientists alike. Provides insi ght on how large and\nsuccessful companies completely fail.\n[C93] “Inside Windows NT” by H. Custer, D. Solomon. Microsoft Press, 19 93.The book about\nWindows NT that explains the system top to bottom, in more detail than you might like. But seriously,\na pretty good book.\n[G04] “Understanding the Linux Virtual Memory Manager” by M. Gorman. Prent ice Hall,\n2004. An in-depth look at Linux VM, but alas a little out of date.\n[G+17] “KASLR is Dead: Long Live KASLR” by D. Gruss, M. Lipp, M. Schwa rz, R. Fell-\nner, C. Maurice, S. Mangard. Engineering Secure Software and Systems, 20 17. Available:\nhttps://gruss.cc/files/kaiser.pdf Excellent info on KASLR, KPTI, and beyond.\n[JS94] “2Q: A Low Overhead High Performance Buffer Management Replacement Al gorithm”\nby T. Johnson, D. Shasha. VLDB ’94, Santiago, Chile. A simple but effective approach to building\npage replacement.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System” by H. Levy, P .\nLipman. IEEE Computer, Volume 15:3, March 1982. Read the original source of most of this\nmaterial. Particularly important if you wish to go to graduate school, where all y ou do is read papers,\nwork, read some more papers, work more, eventually write a paper, and then work some more.\n[M04] “Cloud Atlas” by D. Mitchell. Random House, 2004. It’s hard to pick a favorite book. There\nare too many! Each is great in its own unique way. But it’d be hard for these author s not to pick “Cloud\nAtlas”, a fantastic, sprawling epic about the human condition, from where the th e last quote of this\nchapter is lifted. If you are smart – and we think you are – you should stop reading obscure commentary\nin the references and instead read “Cloud Atlas”; you’ll thank us later.\n[O16] “Virtual Memory and Linux” by A. Ott. Embedded Linux Conference, Ap ril 2016.\nhttps://events.static.linuxfound.org/sites/events/ﬁles/sli des/elc 2016 mem.pdf . A useful\nset of slides which gives an overview of the Linux VM.\n[RL81] “Segmented FIFO Page Replacement” by R. Turner, H. Levy. SIG METRICS ’81, Las\nVegas, Nevada, September 1981. A short paper that shows for some workloads, segmented FIFO can\napproach the performance of LRU.\n[S07] “The Geometry of Innocent Flesh on the Bone: Return-into-libc without Function Calls\n(on the x86)” by H. Shacham. CCS ’07, October 2007. A generalization of return-to-libc. Dr. Beth\nGarner said in Basic Instinct, “She’s crazy! She’s brilliant!” We might s ay the same about ROP .\n[S+04] “On the Effectiveness of Address-space Randomization” by H. Shacha m, M. Page, B.\nPfaff, E. J. Goh, N. Modadugu, D. Boneh. CCS ’04, October 2004. A description of the return-to-\nlibc attack and its limits. Start reading, but be wary: the rabbit hole of system s security is deep...\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG",51089
28-24. Summary Dialogue on Memory Virtualization.pdf,28-24. Summary Dialogue on Memory Virtualization,"24\nSummary Dialogue on Memory Virtualization\nStudent: (Gulps) Wow, that was a lot of material.\nProfessor: Yes, and?\nStudent: Well, how am I supposed to remember it all? You know, for the exam?\nProfessor: Goodness, I hope that’s not why you are trying to remember it.\nStudent: Why should I then?\nProfessor: Come on, I thought you knew better. You’re trying to learn some-\nthing here, so that when you go off into the world, you’ll understand how systems\nactually work.\nStudent: Hmm... can you give an example?\nProfessor: Sure! One time back in graduate school, my friends and I were\nmeasuring how long memory accesses took, and once in a while the num bers\nwere way higher than we expected; we thought all the data was ﬁttin g nicely into\nthe second-level hardware cache, you see, and thus should have been really fast\nto access.\nStudent: (nods)\nProfessor: We couldn’t ﬁgure out what was going on. So what do you do in such\na case? Easy, ask a professor! So we went and asked one of our pro fessors, who\nlooked at the graph we had produced, and simply said “TLB”. Aha! Of course,\nTLB misses! Why didn’t we think of that? Having a good model of how v irtual\nmemory works helps diagnose all sorts of interesting performance p roblems.\nStudent: I think I see. I’m trying to build these mental models of how things\nwork, so that when I’m out there working on my own, I won’t be surp rised when\na system doesn’t quite behave as expected. I should even be able t o anticipate how\nthe system will work just by thinking about it.\nProfessor: Exactly. So what have you learned? What’s in your mental model of\nhow virtual memory works?\nStudent: Well, I think I now have a pretty good idea of what happens when\nmemory is referenced by a process, which, as you’ve said many times, happens\n1\n2 S UMMARY DIALOGUE ON MEMORY VIRTUALIZATION\non each instruction fetch as well as explicit loads and stores.\nProfessor: Sounds good — tell me more.\nStudent: Well, one thing I’ll always remember is that the addresses we see in a\nuser program, written in C for example...\nProfessor: What other language is there?\nStudent: (continuing) ... Yes, I know you like C. So do I! Anyhow, as I was\nsaying, I now really know that all addresses that we can observe wit hin a program\nare virtual addresses; that I, as a programmer, am just given this illusion of where\ndata and code are in memory. I used to think it was cool that I could p rint the\naddress of a pointer, but now I ﬁnd it frustrating — it’s just a virtual a ddress! I\ncan’t see the real physical address where the data lives.\nProfessor: Nope, the OS deﬁnitely hides that from you. What else?\nStudent: Well, I think the TLB is a really key piece, providing the system with\na small hardware cache of address translations. Page tables are u sually quite\nlarge and hence live in big and slow memories. Without that TLB, progra ms\nwould certainly run a great deal more slowly. Seems like the TLB truly m akes\nvirtualizing memory possible. I couldn’t imagine building a system withou t one!\nAnd I shudder at the thought of a program with a working set that e xceeds the\ncoverage of the TLB: with all those TLB misses, it would be hard to wa tch.\nProfessor: Yes, cover the eyes of the children! Beyond the TLB, what did you\nlearn?\nStudent: I also now understand that the page table is one of those data stru ctures\nyou need to know about; it’s just a data structure, though, and t hat means almost\nany structure could be used. We started with simple structures, lik e arrays (a.k.a.\nlinear page tables), and advanced all the way up to multi-level tables (which look\nlike trees), and even crazier things like pageable page tables in kerne l virtual\nmemory. All to save a little space in memory!\nProfessor: Indeed.\nStudent: And here’s one more important thing: I learned that the address t rans-\nlation structures need to be ﬂexible enough to support what progra mmers want\nto do with their address spaces. Structures like the multi-level tab le are perfect\nin this sense; they only create table space when the user needs a po rtion of the\naddress space, and thus there is little waste. Earlier attempts, like the simple base\nand bounds register, just weren’t ﬂexible enough; the structures need to match\nwhat users expect and want out of their virtual memory system.\nProfessor: That’s a nice perspective. What about all of the stuff we learned\nabout swapping to disk?\nStudent: Well, it’s certainly fun to study, and good to know how page replace-\nment works. Some of the basic policies are kind of obvious (like LRU, for ex-\nample), but building a real virtual memory system seems more intere sting, like\nwe saw in the VMS case study. But somehow, I found the mechanisms m ore\ninteresting, and the policies less so.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSUMMARY DIALOGUE ON MEMORY VIRTUALIZATION 3\nProfessor: Oh, why is that?\nStudent: Well, as you said, in the end the best solution to policy problems is\nsimple: buy more memory. But the mechanisms you need to understa nd to know\nhow stuff really works. Speaking of which...\nProfessor: Yes?\nStudent: Well, my machine is running a little slowly these days... and memory\ncertainly doesn’t cost that much...\nProfessor: Oh ﬁne, ﬁne! Here’s a few bucks. Go and get yourself some DRAM,\ncheapskate.\nStudent: Thanks professor! I’ll never swap to disk again — or, if I do, at least\nI’ll know what’s actually going on!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",5540
29-Part II Concurrency.pdf,29-Part II Concurrency,Part II\nConcurrency\n1,23
30-25. A Dialogue on Concurrency.pdf,30-25. A Dialogue on Concurrency,"25\nA Dialogue on Concurrency\nProfessor: And thus we reach the second of our three pillars of operating sys-\ntems: concurrency .\nStudent: I thought there were four pillars...?\nProfessor: Nope, that was in an older version of the book.\nStudent: Umm... OK. So what is concurrency, oh wonderful professor?\nProfessor: Well, imagine we have a peach —\nStudent: (interrupting) Peaches again! What is it with you and peaches?\nProfessor: Ever read T.S. Eliot? The Love Song of J. Alfred Prufrock, “Do I dare\nto eat a peach”, and all that fun stuff?\nStudent: Oh yes! In English class in high school. Great stuff! I really liked the\npart where —\nProfessor: (interrupting) This has nothing to do with that — I just like peaches.\nAnyhow, imagine there are a lot of peaches on a table, and a lot of peo ple who\nwish to eat them. Let’s say we did it this way: each eater ﬁrst identiﬁes a peach\nvisually, and then tries to grab it and eat it. What is wrong with this app roach?\nStudent: Hmmm... seems like you might see a peach that somebody else also\nsees. If they get there ﬁrst, when you reach out, no peach for you !\nProfessor: Exactly! So what should we do about it?\nStudent: Well, probably develop a better way of going about this. Maybe form a\nline, and when you get to the front, grab a peach and get on with it.\nProfessor: Good! But what’s wrong with your approach?\nStudent: Sheesh, do I have to do all the work?\nProfessor: Yes.\nStudent: OK, let me think. Well, we used to have many people grabbing for\npeaches all at once, which is faster. But in my way, we just go one at a t ime,\nwhich is correct, but quite a bit slower. The best kind of approach wo uld be fast\nand correct, probably.\n3\n4 A D IALOGUE ON CONCURRENCY\nProfessor: You are really starting to impress. In fact, you just told us everythin g\nwe need to know about concurrency! Well done.\nStudent: I did? I thought we were just talking about peaches. Remember, this\nis usually the part where you make it about computers again.\nProfessor: Indeed. My apologies! One must never forget the concrete. Well,\nas it turns out, there are certain types of programs that we call multi-threaded\napplications; each thread is kind of like an independent agent running around\nin this program, doing things on the program’s behalf. But these thr eads access\nmemory, and for them, each spot of memory is kind of like one of those peaches. If\nwe don’t coordinate access to memory between threads, the pro gram won’t work\nas expected. Make sense?\nStudent: Kind of. But why do we talk about this in an OS class? Isn’t that just\napplication programming?\nProfessor: Good question! A few reasons, actually. First, the OS must support\nmulti-threaded applications with primitives such as locks andcondition vari-\nables , which we’ll talk about soon. Second, the OS itself was the ﬁrst concu rrent\nprogram — it must access its own memory very carefully or many stran ge and\nterrible things will happen. Really, it can get quite grisly.\nStudent: I see. Sounds interesting. There are more details, I imagine?\nProfessor: Indeed there are...\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",3167
31-26. Concurrency and Threads.pdf,31-26. Concurrency and Threads,"26\nConcurrency: An Introduction\nThus far, we have seen the development of the basic abstractions that the\nOS performs. We have seen how to take a single physical CPU and tu rn\nit into multiple virtual CPUs , thus enabling the illusion of multiple pro-\ngrams running at the same time. We have also seen how to create t he\nillusion of a large, private virtual memory for each process; this abstrac-\ntion of the address space enables each program to behave as if it has its\nown memory when indeed the OS is secretly multiplexing address spaces\nacross physical memory (and sometimes, disk).\nIn this note, we introduce a new abstraction for a single running p ro-\ncess: that of a thread . Instead of our classic view of a single point of\nexecution within a program (i.e., a single PC where instruction s are be-\ning fetched from and executed), a multi-threaded program has more than\none point of execution (i.e., multiple PCs, each of which is being f etched\nand executed from). Perhaps another way to think of this is that e ach\nthread is very much like a separate process, except for one diffe rence:\nthey share the same address space and thus can access the same data.\nThe state of a single thread is thus very similar to that of a proce ss.\nIt has a program counter (PC) that tracks where the program is fet ch-\ning instructions from. Each thread has its own private set of regi sters it\nuses for computation; thus, if there are two threads that are run ning on\na single processor, when switching from running one (T1) to runni ng the\nother (T2), a context switch must take place. The context switch between\nthreads is quite similar to the context switch between process es, as the\nregister state of T1 must be saved and the register state of T2 re stored\nbefore running T2. With processes, we saved state to a process control\nblock (PCB) ; now, we’ll need one or more thread control blocks (TCBs)\nto store the state of each thread of a process. There is one major diff erence,\nthough, in the context switch we perform between threads as compa red\nto processes: the address space remains the same (i.e., there is no need to\nswitch which page table we are using).\nOne other major difference between threads and processes concer ns\nthe stack. In our simple model of the address space of a classic proc ess\n(which we can now call a single-threaded process), there is a single stack,\nusually residing at the bottom of the address space (Figure 26.1 , left).\n1\n2 C ONCURRENCY : ANINTRODUCTION\n16KB15KB2KB1KB0KB\nStack(free)HeapProgram Codethe code segment:\nwhere instructions live\nthe heap segment:\ncontains malloc’d data\ndynamic data structures\n(it grows downward)\n(it grows upward)\nthe stack segment:\ncontains local variables\narguments to routines, \nreturn values, etc.\n16KB15KB2KB1KB0KB\nStack (1)Stack (2)(free)\n(free)HeapProgram Code\nFigure 26.1: Single-Threaded And Multi-Threaded Address Spaces\nHowever, in a multi-threaded process, each thread runs indepe ndently\nand of course may call into various routines to do whatever work it i s do-\ning. Instead of a single stack in the address space, there will be one per\nthread. Let’s say we have a multi-threaded process that has two threads\nin it; the resulting address space looks different (Figure 26. 1, right).\nIn this ﬁgure, you can see two stacks spread throughout the addre ss\nspace of the process. Thus, any stack-allocated variables, par ameters, re-\nturn values, and other things that we put on the stack will be pla ced in\nwhat is sometimes called thread-local storage, i.e., the stack of the rele-\nvant thread.\nYou might also notice how this ruins our beautiful address space l ay-\nout. Before, the stack and heap could grow independently and troub le\nonly arose when you ran out of room in the address space. Here, we\nno longer have such a nice situation. Fortunately, this is usual ly OK, as\nstacks do not generally have to be very large (the exception bei ng in pro-\ngrams that make heavy use of recursion).\n26.1 Why Use Threads?\nBefore getting into the details of threads and some of the problems you\nmight have in writing multi-threaded programs, let’s ﬁrst ans wer a more\nsimple question. Why should you use threads at all?\nAs it turns out, there are at least two major reasons you should use\nthreads. The ﬁrst is simple: parallelism . Imagine you are writing a pro-\ngram that performs operations on very large arrays, for example, a dding\ntwo large arrays together, or incrementing the value of each ele ment in\nthe array by some amount. If you are running on just a single proces-\nsor, the task is straightforward: just perform each operation and be done.\nHowever, if you are executing the program on a system with multipl e\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCONCURRENCY : ANINTRODUCTION 3\nprocessors, you have the potential of speeding up this process consi der-\nably by using the processors to each perform a portion of the work. The\ntask of transforming your standard single-threaded program into a pro-\ngram that does this sort of work on multiple CPUs is called paralleliza-\ntion, and using a thread per CPU to do this work is a natural and typic al\nway to make programs run faster on modern hardware.\nThe second reason is a bit more subtle: to avoid blocking program\nprogress due to slow I/O. Imagine that you are writing a program th at\nperforms different types of I/O: either waiting to send or recei ve a mes-\nsage, for an explicit disk I/O to complete, or even (implicitly) for a page\nfault to ﬁnish. Instead of waiting, your program may wish to do som e-\nthing else, including utilizing the CPU to perform computation , or even\nissuing further I/O requests. Using threads is a natural way to avoid\ngetting stuck; while one thread in your program waits (i.e., is b locked\nwaiting for I/O), the CPU scheduler can switch to other threads , which\nare ready to run and do something useful. Threading enables overlap of\nI/O with other activities within a single program, much like multipro-\ngramming did for processes across programs; as a result, many modern\nserver-based applications (web servers, database manageme nt systems,\nand the like) make use of threads in their implementations.\nOf course, in either of the cases mentioned above, you could use mult i-\npleprocesses instead of threads. However, threads share an address space\nand thus make it easy to share data, and hence are a natural choi ce when\nconstructing these types of programs. Processes are a more sound ch oice\nfor logically separate tasks where little sharing of data struc tures in mem-\nory is needed.\n26.2 An Example: Thread Creation\nLet’s get into some of the details. Say we wanted to run a program\nthat creates two threads, each of which does some independent wor k, in\nthis case printing “A” or “B”. The code is shown in Figure 26.2 (pa ge 4).\nThe main program creates two threads, each of which will run the\nfunctionmythread() , though with different arguments (the string Aor\nB). Once a thread is created, it may start running right away (d epending\non the whims of the scheduler); alternately, it may be put in a “r eady” but\nnot “running” state and thus not run yet. Of course, on a multiproce ssor,\nthe threads could even be running at the same time, but let’s not w orry\nabout this possibility quite yet.\nAfter creating the two threads (let’s call them T1 and T2), the main\nthread calls pthread join() , which waits for a particular thread to\ncomplete. It does so twice, thus ensuring T1 and T2 will run and c om-\nplete before ﬁnally allowing the main thread to run again; when it does,\nit will print “main: end” and exit. Overall, three threads we re employed\nduring this run: the main thread, T1, and T2.\nLet us examine the possible execution ordering of this little prog ram.\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 C ONCURRENCY : ANINTRODUCTION\n1#include <stdio.h>\n2#include <assert.h>\n3#include <pthread.h>\n4#include ""common.h""\n5#include ""common_threads.h""\n6\n7void*mythread(void *arg) {\n8printf(""%s\n"", (char *) arg);\n9return NULL;\n10}\n11\n12int\n13main(int argc, char *argv[]) {\n14pthread_t p1, p2;\n15int rc;\n16printf(""main: begin\n"");\n17Pthread_create(&p1, NULL, mythread, ""A"");\n18Pthread_create(&p2, NULL, mythread, ""B"");\n19// join waits for the threads to finish\n20Pthread_join(p1, NULL);\n21Pthread_join(p2, NULL);\n22printf(""main: end\n"");\n23return 0;\n24}\nFigure 26.2: Simple Thread Creation Code (t0.c)\nIn the execution diagram (Figure 26.3, page 5), time increase s in the down-\nwards direction, and each column shows when a different thread ( the\nmain one, or Thread 1, or Thread 2) is running.\nNote, however, that this ordering is not the only possible ordering. In\nfact, given a sequence of instructions, there are quite a few, d epending on\nwhich thread the scheduler decides to run at a given point. For e xample,\nonce a thread is created, it may run immediately, which would le ad to the\nexecution shown in Figure 26.4 (page 5).\nWe also could even see “B” printed before “A”, if, say, the sched uler\ndecided to run Thread 2 ﬁrst even though Thread 1 was created ea rlier;\nthere is no reason to assume that a thread that is created ﬁrst w ill run ﬁrst.\nFigure 26.5 (page 5) shows this ﬁnal execution ordering, with Th read 2\ngetting to strut its stuff before Thread 1.\nAs you might be able to see, one way to think about thread creation\nis that it is a bit like making a function call; however, instead of ﬁrst ex-\necuting the function and then returning to the caller, the sys tem instead\ncreates a new thread of execution for the routine that is being cal led, and\nit runs independently of the caller, perhaps before returning from the cre-\nate, but perhaps much later. What runs next is determined by t he OS\nscheduler , and although the scheduler likely implements some sensible\nalgorithm, it is hard to know what will run at any given moment in t ime.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCONCURRENCY : ANINTRODUCTION 5\nmain Thread 1 Thread2\nstarts running\nprints “main: begin”\ncreates Thread 1\ncreates Thread 2\nwaits for T1\nruns\nprints “A”\nreturns\nwaits for T2\nruns\nprints “B”\nreturns\nprints “main: end”\nFigure 26.3: Thread Trace (1)\nmain Thread 1 Thread2\nstarts running\nprints “main: begin”\ncreates Thread 1\nruns\nprints “A”\nreturns\ncreates Thread 2\nruns\nprints “B”\nreturns\nwaits for T1\nreturns immediately; T1 is done\nwaits for T2\nreturns immediately; T2 is done\nprints “main: end”\nFigure 26.4: Thread Trace (2)\nmain Thread 1 Thread2\nstarts running\nprints “main: begin”\ncreates Thread 1\ncreates Thread 2\nruns\nprints “B”\nreturns\nwaits for T1\nruns\nprints “A”\nreturns\nwaits for T2\nreturns immediately; T2 is done\nprints “main: end”\nFigure 26.5: Thread Trace (3)\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 C ONCURRENCY : ANINTRODUCTION\nAs you also might be able to tell from this example, threads make life\ncomplicated: it is already hard to tell what will run when! Comp uters are\nhard enough to understand without concurrency. Unfortunately, with\nconcurrency, it simply gets worse. Much worse.\n26.3 Why It Gets Worse: Shared Data\nThe simple thread example we showed above was useful in showing\nhow threads are created and how they can run in different orders d epend-\ning on how the scheduler decides to run them. What it doesn’t show you ,\nthough, is how threads interact when they access shared data.\nLet us imagine a simple example where two threads wish to upda te a\nglobal shared variable. The code we’ll study is in Figure 26.6 (p age 7).\nHere are a few notes about the code. First, as Stevens suggests [SR0 5],\nwe wrap the thread creation and join routines to simply exit on fai lure;\nfor a program as simple as this one, we want to at least notice an err or\noccurred (if it did), but not do anything very smart about it (e.g ., just\nexit). Thus, Pthread create() simply calls pthread create() and\nmakes sure the return code is 0; if it isn’t, Pthread create() just prints\na message and exits.\nSecond, instead of using two separate function bodies for the worker\nthreads, we just use a single piece of code, and pass the thread a n argu-\nment (in this case, a string) so we can have each thread print a different\nletter before its messages.\nFinally, and most importantly, we can now look at what each worker is\ntrying to do: add a number to the shared variable counter , and do so 10\nmillion times (1e7) in a loop. Thus, the desired ﬁnal result is: 2 0,000,000.\nWe now compile and run the program, to see how it behaves. Some-\ntimes, everything works how we might expect:\nprompt> gcc -o main main.c -Wall -pthread\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 20000000)\nUnfortunately, when we run this code, even on a single processor, w e\ndon’t necessarily get the desired result. Sometimes, we get:\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 19345221)\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCONCURRENCY : ANINTRODUCTION 7\n1#include <stdio.h>\n2#include <pthread.h>\n3#include ""common.h""\n4#include ""common_threads.h""\n5\n6static volatile int counter = 0;\n7\n8//\n9// mythread()\n10//\n11// Simply adds 1 to counter repeatedly, in a loop\n12// No, this is not how you would add 10,000,000 to\n13// a counter, but it shows the problem nicely.\n14//\n15void*mythread(void *arg) {\n16printf(""%s: begin\n"", (char *) arg);\n17int i;\n18for (i = 0; i < 1e7; i++) {\n19 counter = counter + 1;\n20}\n21printf(""%s: done\n"", (char *) arg);\n22return NULL;\n23}\n24\n25//\n26// main()\n27//\n28// Just launches two threads (pthread_create)\n29// and then waits for them (pthread_join)\n30//\n31int main(int argc, char *argv[]) {\n32pthread_t p1, p2;\n33printf(""main: begin (counter = %d)\n"", counter);\n34Pthread_create(&p1, NULL, mythread, ""A"");\n35Pthread_create(&p2, NULL, mythread, ""B"");\n36\n37// join waits for the threads to finish\n38Pthread_join(p1, NULL);\n39Pthread_join(p2, NULL);\n40printf(""main: done with both (counter = %d)\n"", counter);\n41return 0;\n42}\nFigure 26.6: Sharing Data: Uh Oh (t1.c)\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 C ONCURRENCY : ANINTRODUCTION\nTIP: KNOW ANDUSEYOUR TOOLS\nYou should always learn new tools that help you write, debug, and un -\nderstand computer systems. Here, we use a neat tool called a disassem-\nbler. When you run a disassembler on an executable, it shows you what\nassembly instructions make up the program. For example, if we wi sh to\nunderstand the low-level code to update a counter (as in our examp le),\nwe runobjdump (Linux) to see the assembly code:\nprompt> objdump -d main\nDoing so produces a long listing of all the instructions in the progr am,\nneatly labeled (particularly if you compiled with the -gﬂag), which in-\ncludes symbol information in the program. The objdump program is just\none of many tools you should learn how to use; a debugger like gdb,\nmemory proﬁlers like valgrind orpurify , and of course the compiler\nitself are others that you should spend time to learn more about; th e better\nyou are at using your tools, the better systems you’ll be able to buil d.\nLet’s try it one more time, just to see if we’ve gone crazy. After all ,\naren’t computers supposed to produce deterministic results, as you have\nbeen taught?! Perhaps your professors have been lying to you? (gasp)\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 19221041)\nNot only is each run wrong, but also yields a different result! A big\nquestion remains: why does this happen?\n26.4 The Heart Of The Problem: Uncontrolled Scheduling\nTo understand why this happens, we must understand the code se -\nquence that the compiler generates for the update to counter . In this\ncase, we wish to simply add a number (1) to counter . Thus, the code\nsequence for doing so might look something like this (in x86);\nmov 0x8049a1c, %eax\nadd $0x1, %eax\nmov %eax, 0x8049a1c\nThis example assumes that the variable counter is located at address\n0x8049a1c. In this three-instruction sequence, the x86 mov instruction is\nused ﬁrst to get the memory value at the address and put it into r egister\neax. Then, the add is performed, adding 1 (0x1) to the contents of the\neax register, and ﬁnally, the contents of eax are stored back into memory\nat the same address.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCONCURRENCY : ANINTRODUCTION 9\nLet us imagine one of our two threads (Thread 1) enters this region of\ncode, and is thus about to increment counter by one. It loads the value\nofcounter (let’s say it’s 50 to begin with) into its register eax. Thus,\neax=50 for Thread 1. Then it adds one to the register; thus eax=51 .\nNow, something unfortunate happens: a timer interrupt goes off; t hus,\nthe OS saves the state of the currently running thread (its PC, its registers\nincluding eax, etc.) to the thread’s TCB.\nNow something worse happens: Thread 2 is chosen to run, and it en-\nters this same piece of code. It also executes the ﬁrst instruct ion, getting\nthe value of counter and putting it into its eax (remember: each thread\nwhen running has its own private registers; the registers are virtualized\nby the context-switch code that saves and restores them). The va lue of\ncounter is still 50 at this point, and thus Thread 2 has eax=50 . Let’s\nthen assume that Thread 2 executes the next two instructions, increment-\ningeax by 1 (thus eax=51 ), and then saving the contents of eax into\ncounter (address 0x8049a1c). Thus, the global variable counter now\nhas the value 51.\nFinally, another context switch occurs, and Thread 1 resumes ru nning.\nRecall that it had just executed the mov andadd, and is now about to\nperform the ﬁnal mov instruction. Recall also that eax=51 . Thus, the ﬁnal\nmov instruction executes, and saves the value to memory; the counte r is\nset to 51 again.\nPut simply, what has happened is this: the code to increment counter\nhas been run twice, but counter , which started at 50, is now only equal\nto 51. A “correct” version of this program should have resulted in t he\nvariablecounter equal to 52.\nLet’s look at a detailed execution trace to understand the problem bet-\nter. Assume, for this example, that the above code is loaded at add ress\n100 in memory, like the following sequence (note for those of you used t o\nnice, RISC-like instruction sets: x86 has variable-length in structions; this\nmov instruction takes up 5 bytes of memory, and the add only 3):\n100 mov 0x8049a1c, %eax\n105 add $0x1, %eax\n108 mov %eax, 0x8049a1c\nWith these assumptions, what happens is shown in Figure 26.7 (p age\n10). Assume the counter starts at value 50, and trace through th is example\nto make sure you understand what is going on.\nWhat we have demonstrated here is called a race condition (or, more\nspeciﬁcally, a data race ): the results depend on the timing execution of\nthe code. With some bad luck (i.e., context switches that occur at un-\ntimely points in the execution), we get the wrong result. In fact , we may\nget a different result each time; thus, instead of a nice deterministic com-\nputation (which we are used to from computers), we call this resu ltinde-\nterminate , where it is not known what the output will be and it is indeed\nlikely to be different across runs.\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 C ONCURRENCY : ANINTRODUCTION\n(after instruction)\nOS Thread 1 Thread 2 PC %eax counter\nbefore critical section 100 0 50\nmov 0x8049a1c, %eax 105 50 50\nadd $0x1, %eax 108 51 50\ninterrupt\nsave T1’s state\nrestore T2’s state 100 0 50\nmov 0x8049a1c, %eax 105 50 50\nadd $0x1, %eax 108 51 50\nmov %eax, 0x8049a1c 113 51 51\ninterrupt\nsave T2’s state\nrestore T1’s state 108 51 51\nmov %eax, 0x8049a1c 113 51 51\nFigure 26.7: The Problem: Up Close and Personal\nBecause multiple threads executing this code can result in a r ace con-\ndition, we call this code a critical section . A critical section is a piece of\ncode that accesses a shared variable (or more generally, a share d resource)\nand must not be concurrently executed by more than one thread.\nWhat we really want for this code is what we call mutual exclusion .\nThis property guarantees that if one thread is executing withi n the critical\nsection, the others will be prevented from doing so.\nVirtually all of these terms, by the way, were coined by Edsger D ijk-\nstra, who was a pioneer in the ﬁeld and indeed won the Turing Awar d\nbecause of this and other work; see his 1968 paper on “Cooperating Se-\nquential Processes” [D68] for an amazingly clear description of the prob-\nlem. We’ll be hearing more about Dijkstra in this section of the book.\n26.5 The Wish For Atomicity\nOne way to solve this problem would be to have more powerful in-\nstructions that, in a single step, did exactly whatever we nee ded done\nand thus removed the possibility of an untimely interrupt. For ex ample,\nwhat if we had a super instruction that looked like this:\nmemory-add 0x8049a1c, $0x1\nAssume this instruction adds a value to a memory location, and the\nhardware guarantees that it executes atomically ; when the instruction\nexecuted, it would perform the update as desired. It could not be i nter-\nrupted mid-instruction, because that is precisely the guara ntee we receive\nfrom the hardware: when an interrupt occurs, either the instru ction has\nnot run at all, or it has run to completion; there is no in-between s tate.\nHardware can be a beautiful thing, no?\nAtomically, in this context, means “as a unit”, which sometimes we\ntake as “all or none.” What we’d like is to execute the three instr uction\nsequence atomically:\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCONCURRENCY : ANINTRODUCTION 11\nTIP: USEATOMIC OPERATIONS\nAtomic operations are one of the most powerful underlying technique s\nin building computer systems, from the computer architecture, to concur-\nrent code (what we are studying here), to ﬁle systems (which we ’ll study\nsoon enough), database management systems, and even distribut ed sys-\ntems [L+93].\nThe idea behind making a series of actions atomic is simply expressed\nwith the phrase “all or nothing”; it should either appear as if al l of the ac-\ntions you wish to group together occurred, or that none of them occurred ,\nwith no in-between state visible. Sometimes, the grouping of man y ac-\ntions into a single atomic action is called a transaction , an idea devel-\noped in great detail in the world of databases and transaction proc essing\n[GR92].\nIn our theme of exploring concurrency, we’ll be using synchronizat ion\nprimitives to turn short sequences of instructions into atomic b locks of\nexecution, but the idea of atomicity is much bigger than that, as we will\nsee. For example, ﬁle systems use techniques such as journalin g or copy-\non-write in order to atomically transition their on-disk state, c ritical for\noperating correctly in the face of system failures. If that doesn ’t make\nsense, don’t worry — it will, in some future chapter.\nmov 0x8049a1c, %eax\nadd $0x1, %eax\nmov %eax, 0x8049a1c\nAs we said, if we had a single instruction to do this, we could jus t\nissue that instruction and be done. But in the general case, we w on’t have\nsuch an instruction. Imagine we were building a concurrent B-t ree, and\nwished to update it; would we really want the hardware to suppor t an\n“atomic update of B-tree” instruction? Probably not, at least in a sane\ninstruction set.\nThus, what we will instead do is ask the hardware for a few usefu l\ninstructions upon which we can build a general set of what we call syn-\nchronization primitives . By using this hardware support, in combina-\ntion with some help from the operating system, we will be able to bu ild\nmulti-threaded code that accesses critical sections in a sync hronized and\ncontrolled manner, and thus reliably produces the correct resul t despite\nthe challenging nature of concurrent execution. Pretty awesome , right?\nThis is the problem we will study in this section of the book. It is a\nwonderful and hard problem, and should make your mind hurt (a bit) .\nIf it doesn’t, then you don’t understand! Keep working until your hea d\nhurts; you then know you’re headed in the right direction. At that p oint,\ntake a break; we don’t want your head hurting too much.\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 C ONCURRENCY : ANINTRODUCTION\nTHECRUX: HOWTOSUPPORT SYNCHRONIZATION\nWhat support do we need from the hardware in order to build use-\nful synchronization primitives? What support do we need from the OS?\nHow can we build these primitives correctly and efﬁciently? How can\nprograms use them to get the desired results?\n26.6 One More Problem: Waiting For Another\nThis chapter has set up the problem of concurrency as if only one typ e\nof interaction occurs between threads, that of accessing shared variables\nand the need to support atomicity for critical sections. As it tur ns out,\nthere is another common interaction that arises, where one thread must\nwait for another to complete some action before it continues. This in ter-\naction arises, for example, when a process performs a disk I/O and is put\nto sleep; when the I/O completes, the process needs to be roused f rom its\nslumber so it can continue.\nThus, in the coming chapters, we’ll be not only studying how to buil d\nsupport for synchronization primitives to support atomicity but a lso for\nmechanisms to support this type of sleeping/waking interacti on that is\ncommon in multi-threaded programs. If this doesn’t make sense rig ht\nnow, that is OK! It will soon enough, when you read the chapter on con-\ndition variables . If it doesn’t by then, well, then it is less OK, and you\nshould read that chapter again (and again) until it does make se nse.\n26.7 Summary: Why in OS Class?\nBefore wrapping up, one question that you might have is: why are we\nstudying this in OS class? “History” is the one-word answer; the OS was\nthe ﬁrst concurrent program, and many techniques were created for use\nwithin the OS. Later, with multi-threaded processes, application prog ram-\nmers also had to consider such things.\nFor example, imagine the case where there are two processes run ning.\nAssume they both call write() to write to the ﬁle, and both wish to\nappend the data to the ﬁle (i.e., add the data to the end of the ﬁl e, thus\nincreasing its length). To do so, both must allocate a new block, r ecord\nin the inode of the ﬁle where this block lives, and change the size of the\nﬁle to reﬂect the new larger size (among other things; we’ll lear n more\nabout ﬁles in the third part of the book). Because an interrupt may occur\nat any time, the code that updates these shared structures (e. g., a bitmap\nfor allocation, or the ﬁle’s inode) are critical sections; thus, OS d esign-\ners, from the very beginning of the introduction of the interrupt, had to\nworry about how the OS updates internal structures. An untimely inter-\nrupt causes all of the problems described above. Not surprisingl y, page\ntables, process lists, ﬁle system structures, and virtually every kernel data\nstructure has to be carefully accessed, with the proper synch ronization\nprimitives, to work correctly.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCONCURRENCY : ANINTRODUCTION 13\nASIDE : KEYCONCURRENCY TERMS\nCRITICAL SECTION , RACE CONDITION ,\nINDETERMINATE , M UTUAL EXCLUSION\nThese four terms are so central to concurrent code that we thought it\nworth while to call them out explicitly. See some of Dijkstra’s earl y work\n[D65,D68] for more details.\n•Acritical section is a piece of code that accesses a shared resource,\nusually a variable or data structure.\n•Arace condition (ordata race [NM92]) arises if multiple threads of\nexecution enter the critical section at roughly the same time; b oth\nattempt to update the shared data structure, leading to a sur prising\n(and perhaps undesirable) outcome.\n•Anindeterminate program consists of one or more race conditions;\nthe output of the program varies from run to run, depending on\nwhich threads ran when. The outcome is thus not deterministic ,\nsomething we usually expect from computer systems.\n•To avoid these problems, threads should use some kind of mutual\nexclusion primitives; doing so guarantees that only a single thread\never enters a critical section, thus avoiding races, and resul ting in\ndeterministic program outputs.\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 C ONCURRENCY : ANINTRODUCTION\nReferences\n[D65] “Solution of a problem in concurrent programming control” by E. W. Dijkstra. Commu-\nnications of the ACM, 8(9):569, September 1965. Pointed to as the ﬁrst paper of Dijkstra’s where\nhe outlines the mutual exclusion problem and a solution. The solution, howeve r, is not widely used;\nadvanced hardware and OS support is needed, as we will see in the coming c hapters.\n[D68] “Cooperating sequential processes” by Edsger W. Dijkstra . 1968. Available at this site:\nhttp://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.P DF.Dijkstra has an amaz-\ning number of his old papers, notes, and thoughts recorded (for posterity) on thi s website at the last\nplace he worked, the University of Texas. Much of his foundational work, howeve r, was done years\nearlier while he was at the Technische Hochshule of Eindhoven (THE), in cluding this famous paper on\n“cooperating sequential processes”, which basically outlines all of the thi nking that has to go into writ-\ning multi-threaded programs. Dijkstra discovered much of this while worki ng on an operating system\nnamed after his school: the “THE” operating system (said “T”, “H”, “E”, and n ot like the word “the”).\n[GR92] “Transaction Processing: Concepts and Techniques” by Jim Gray and And reas Reuter.\nMorgan Kaufmann, September 1992. This book is the bible of transaction processing, written by one\nof the legends of the ﬁeld, Jim Gray. It is, for this reason, also considered Ji m Gray’s “brain dump”,\nin which he wrote down everything he knows about how database management system s work. Sadly,\nGray passed away tragically a few years back, and many of us lost a friend and gre at mentor, including\nthe co-authors of said book, who were lucky enough to interact with Gray during thei r graduate school\nyears.\n[L+93] “Atomic Transactions” by Nancy Lynch, Michael Merritt, William Wei hl, Alan Fekete.\nMorgan Kaufmann, August 1993. A nice text on some of the theory and practice of atomic transac-\ntions for distributed systems. Perhaps a bit formal for some, but lots of good mate rial is found herein.\n[NM92] “What Are Race Conditions? Some Issues and Formalizations” by Robert H. B. Netzer\nand Barton P . Miller. ACM Letters on Programming Languages and Syst ems, Volume 1:1,\nMarch 1992. An excellent discussion of the different types of races found in concu rrent programs. In\nthis chapter (and the next few), we focus on data races, but later we will broaden to discuss general\nraces as well.\n[SR05] “Advanced Programming in the U NIXEnvironment” by W. Richard Stevens and Stephen\nA. Rago. Addison-Wesley, 2005. As we’ve said many times, buy this book, and read it, in little\nchunks, preferably before going to bed. This way, you will actually fall as leep more quickly; more im-\nportantly, you learn a little more about how to become a serious UNIXprogrammer.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCONCURRENCY : ANINTRODUCTION 15\nHomework (Simulation)\nThis program, x86.py , allows you to see how different thread inter-\nleavings either cause or avoid race conditions. See the README for d e-\ntails on how the program works, then answer the questions below.\nQuestions\n1. Let’s examine a simple program, “loop.s”. First, just read and un derstand it.\nThen, run it with these arguments ( ./x86.py -p loop.s -t 1 -i 100\n-R dx ) This speciﬁes a single thread, an interrupt every 100 instruc tions,\nand tracing of register %dx. What will %dx be during the run? Use the -c\nﬂag to check your answers; the answers, on the left, show the va lue of the\nregister (or memory value) after the instruction on the right has run.\n2. Same code, different ﬂags: ( ./x86.py -p loop.s -t 2 -i 100 -a\ndx=3,dx=3 -R dx ) This speciﬁes two threads, and initializes each %dx to\n3. What values will %dx see? Run with -cto check. Does the presence of\nmultiple threads affect your calculations? Is there a race in thi s code?\n3. Run this: ./x86.py -p loop.s -t 2 -i 3 -r -a dx=3,dx=3 -R dx\nThis makes the interrupt interval small/random; use different se eds (-s) to\nsee different interleavings. Does the interrupt frequency ch ange anything?\n4. Now, a different program, looping-race-nolock.s , which accesses a\nshared variable located at address 2000; we’ll call this var iablevalue . Run\nit with a single thread to conﬁrm your understanding: ./x86.py -p\nlooping-race-nolock.s -t 1 -M 2000 What isvalue (i.e., at mem-\nory address 2000) throughout the run? Use -cto check.\n5. Run with multiple iterations/threads: ./x86.py -p looping-race-nolock.s\n-t 2 -a bx=3 -M 2000 Why does each thread loop three times? What\nis ﬁnal value of value ?\n6. Run with random interrupt intervals: ./x86.py -p looping-race-nolock.s\n-t 2 -M 2000 -i 4 -r -s 0 with different seeds ( -s 1 ,-s 2 , etc.)\nCan you tell by looking at the thread interleaving what the ﬁn al value of\nvalue will be? Does the timing of the interrupt matter? Where can it saf ely\noccur? Where not? In other words, where is the critical sectio n exactly?\n7. Now examine ﬁxed interrupt intervals: ./x86.py -p looping-race-nolock.s\n-a bx=1 -t 2 -M 2000 -i 1 What will the ﬁnal value of the shared\nvariablevalue be? What about when you change -i 2 ,-i 3 , etc.? For\nwhich interrupt intervals does the program give the “correct ” answer?\n8. Run the same for more loops (e.g., set -a bx=100 ). What interrupt inter-\nvals (-i) lead to a correct outcome? Which intervals are surprising?\n9. One last program: wait-for-me.s . Run:./x86.py -p wait-for-me.s\n-a ax=1,ax=0 -R ax -M 2000 This sets the %ax register to 1 for thread\n0, and 0 for thread 1, and watches %ax and memory location 2000. How\nshould the code behave? How is the value at location 2000 being us ed by\nthe threads? What will its ﬁnal value be?\n10. Now switch the inputs: ./x86.py -p wait-for-me.s -a ax=0,ax=1\n-R ax -M 2000 How do the threads behave? What is thread 0 doing?\nHow would changing the interrupt interval (e.g., -i 1000 , or perhaps to\nuse random intervals) change the trace outcome? Is the program e fﬁciently\nusing the CPU?\nc/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",35058
32-27. Thread API.pdf,32-27. Thread API,"27\nInterlude: Thread API\nThis chapter brieﬂy covers the main portions of the thread API. Ea ch part\nwill be explained further in the subsequent chapters, as we s how how\nto use the API. More details can be found in various books and online\nsources [B89, B97, B+96, K+96]. We should note that the subseque nt chap-\nters introduce the concepts of locks and condition variables more sl owly,\nwith many examples; this chapter is thus better used as a refe rence.\nCRUX: HOWTOCREATE ANDCONTROL THREADS\nWhat interfaces should the OS present for thread creation and con trol?\nHow should these interfaces be designed to enable ease of use as w ell as\nutility?\n27.1 Thread Creation\nThe ﬁrst thing you have to be able to do to write a multi-threade d\nprogram is to create new threads, and thus some kind of thread cre ation\ninterface must exist. In POSIX, it is easy:\n#include <pthread.h>\nint\npthread_create( pthread_t * thread,\nconst pthread_attr_t *attr,\nvoid* (*start_routine)(void *),\nvoid* arg);\nThis declaration might look a little complex (particularly if you haven’t\nused function pointers in C), but actually it’s not too bad. There a re\nfour arguments: thread ,attr ,startroutine , andarg. The ﬁrst,\nthread , is a pointer to a structure of type pthread t; we’ll use this\nstructure to interact with this thread, and thus we need to pa ss it to\npthread create() in order to initialize it.\n1\n2 INTERLUDE : THREAD API\nThe second argument, attr , is used to specify any attributes this thread\nmight have. Some examples include setting the stack size or perh aps in-\nformation about the scheduling priority of the thread. An attribu te is\ninitialized with a separate call to pthread attrinit() ; see the man-\nual page for details. However, in most cases, the defaults will b e ﬁne; in\nthis case, we will simply pass the value NULL in.\nThe third argument is the most complex, but is really just askin g: which\nfunction should this thread start running in? In C, we call this afunction\npointer , and this one tells us the following is expected: a function name\n(startroutine ), which is passed a single argument of type void*(as\nindicated in the parentheses after startroutine ), and which returns a\nvalue of type void*(i.e., a void pointer ).\nIf this routine instead required an integer argument, instea d of a void\npointer, the declaration would look like this:\nint pthread_create(..., // first two args are the same\nvoid*(*start_routine)(int),\nint arg);\nIf instead the routine took a void pointer as an argument, but retur ned\nan integer, it would look like this:\nint pthread_create(..., // first two args are the same\nint ( *start_routine)(void *),\nvoid*arg);\nFinally, the fourth argument, arg, is exactly the argument to be passed\nto the function where the thread begins execution. You might ask : why\ndo we need these void pointers? Well, the answer is quite simple : having\na void pointer as an argument to the function startroutine allows us\nto pass in anytype of argument; having it as a return value allows the\nthread to return anytype of result.\nLet’s look at an example in Figure 27.1. Here we just create a thre ad\nthat is passed two arguments, packaged into a single type we d eﬁne our-\nselves (myargt). The thread, once created, can simply cast its argument\nto the type it expects and thus unpack the arguments as desire d.\nAnd there it is! Once you create a thread, you really have another\nlive executing entity, complete with its own call stack, runni ng within the\nsame address space as all the currently existing threads in the pr ogram.\nThe fun thus begins!\n27.2 Thread Completion\nThe example above shows how to create a thread. However, what\nhappens if you want to wait for a thread to complete? You need to do\nsomething special in order to wait for completion; in particular, you must\ncall the routine pthread join() .\nint pthread_join(pthread_t thread, void **value_ptr);\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : THREAD API 3\n1#include <pthread.h>\n2\n3typedef struct __myarg_t {\n4int a;\n5int b;\n6} myarg_t;\n7\n8void*mythread(void *arg) {\n9myarg_t *m = (myarg_t *) arg;\n10 printf(""%d %d\n"", m->a, m->b);\n11 return NULL;\n12}\n13\n14int\n15main(int argc, char *argv[]) {\n16 pthread_t p;\n17 int rc;\n18\n19 myarg_t args;\n20 args.a = 10;\n21 args.b = 20;\n22 rc = pthread_create(&p, NULL, mythread, &args);\n23 ...\n24}\nFigure 27.1: Creating a Thread\nThis routine takes two arguments. The ﬁrst is of type pthread t, and\nis used to specify which thread to wait for. This variable is in itialized by\nthe thread creation routine (when you pass a pointer to it as an arg ument\ntopthread create() ); if you keep it around, you can use it to wait for\nthat thread to terminate.\nThe second argument is a pointer to the return value you expect to get\nback. Because the routine can return anything, it is deﬁned to return a\npointer to void; because the pthread join() routine changes the value\nof the passed in argument, you need to pass in a pointer to that val ue, not\njust the value itself.\nLet’s look at another example (Figure 27.2, page 4). In the code, a\nsingle thread is again created, and passed a couple of argument s via the\nmyargtstructure. To return values, the myretttype is used. Once\nthe thread is ﬁnished running, the main thread, which has bee n waiting\ninside of the pthread join() routine1, then returns, and we can access\nthe values returned from the thread, namely whatever is in myrett.\nA few things to note about this example. First, often times we don’t\nhave to do all of this painful packing and unpacking of argument s. For\nexample, if we just create a thread with no arguments, we can p assNULL\nin as an argument when the thread is created. Similarly, we can passNULL\nintopthread join() if we don’t care about the return value.\nSecond, if we are just passing in a single value (e.g., an int), w e don’t\n1Note we use wrapper functions here; speciﬁcally, we call Malloc(), Pthread join(), and\nPthread create(), which just call their similarly-named lower-case versions and m ake sure the\nroutines did not return anything unexpected.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 INTERLUDE : THREAD API\n1#include <stdio.h>\n2#include <pthread.h>\n3#include <assert.h>\n4#include <stdlib.h>\n5\n6typedef struct __myarg_t {\n7int a;\n8int b;\n9} myarg_t;\n10\n11typedef struct __myret_t {\n12 int x;\n13 int y;\n14} myret_t;\n15\n16void*mythread(void *arg) {\n17 myarg_t *m = (myarg_t *) arg;\n18 printf(""%d %d\n"", m->a, m->b);\n19 myret_t *r = Malloc(sizeof(myret_t));\n20 r->x = 1;\n21 r->y = 2;\n22 return (void *) r;\n23}\n24\n25int\n26main(int argc, char *argv[]) {\n27 pthread_t p;\n28 myret_t *m;\n29\n30 myarg_t args = {10, 20};\n31 Pthread_create(&p, NULL, mythread, &args);\n32 Pthread_join(p, (void **) &m);\n33 printf(""returned %d %d\n"", m->x, m->y);\n34 free(m);\n35 return 0;\n36}\nFigure 27.2: Waiting for Thread Completion\nhave to package it up as an argument. Figure 27.3 (page 5) shows an\nexample. In this case, life is a bit simpler, as we don’t have to p ackage\narguments and return values inside of structures.\nThird, we should note that one has to be extremely careful with how\nvalues are returned from a thread. In particular, never retur n a pointer\nwhich refers to something allocated on the thread’s call stack. I f you do,\nwhat do you think will happen? (think about it!) Here is an exampl e of a\ndangerous piece of code, modiﬁed from the example in Figure 27.3.\n1void*mythread(void *arg) {\n2myarg_t *m = (myarg_t *) arg;\n3printf(""%d %d\n"", m->a, m->b);\n4myret_t r; // ALLOCATED ON STACK: BAD!\n5r.x = 1;\n6r.y = 2;\n7return (void *) &r;\n8}\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : THREAD API 5\nvoid*mythread(void *arg) {\nint m = (int) arg;\nprintf(""%d\n"", m);\nreturn (void *) (arg + 1);\n}\nint main(int argc, char *argv[]) {\npthread_t p;\nint rc, m;\nPthread_create(&p, NULL, mythread, (void *) 100);\nPthread_join(p, (void **) &m);\nprintf(""returned %d\n"", m);\nreturn 0;\n}\nFigure 27.3: Simpler Argument Passing to a Thread\nIn this case, the variable ris allocated on the stack of mythread . How-\never, when it returns, the value is automatically deallocated (that’s why\nthe stack is so easy to use, after all!), and thus, passing back a pointer to\na now deallocated variable will lead to all sorts of bad results. C ertainly,\nwhen you print out the values you think you returned, you’ll probably\n(but not necessarily!) be surprised. Try it and ﬁnd out for yoursel f2!\nFinally, you might notice that the use of pthread create() to create\na thread, followed by an immediate call to pthread join() , is a pretty\nstrange way to create a thread. In fact, there is an easier way to accom-\nplish this exact task; it’s called a procedure call . Clearly, we’ll usually be\ncreating more than just one thread and waiting for it to complete, other-\nwise there is not much purpose to using threads at all.\nWe should note that not all code that is multi-threaded uses the joi n\nroutine. For example, a multi-threaded web server might creat e a number\nof worker threads, and then use the main thread to accept reques ts and\npass them to the workers, indeﬁnitely. Such long-lived programs thus\nmay not need to join. However, a parallel program that creates thr eads\nto execute a particular task (in parallel) will likely use joi n to make sure\nall such work completes before exiting or moving onto the next stage of\ncomputation.\n27.3 Locks\nBeyond thread creation and join, probably the next most useful set of\nfunctions provided by the POSIX threads library are those for provi ding\nmutual exclusion to a critical section via locks . The most basic pair of\nroutines to use for this purpose is provided by the following:\nint pthread_mutex_lock(pthread_mutex_t *mutex);\nint pthread_mutex_unlock(pthread_mutex_t *mutex);\n2Fortunately the compiler gcc will likely complain when you write code like this, which\nis yet another reason to pay attention to compiler warnings.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 INTERLUDE : THREAD API\nThe routines should be easy to understand and use. When you have a\nregion of code that is a critical section , and thus needs to be protected to\nensure correct operation, locks are quite useful. You can probably imag-\nine what the code looks like:\npthread_mutex_t lock;\npthread_mutex_lock(&lock);\nx = x + 1; // or whatever your critical section is\npthread_mutex_unlock(&lock);\nThe intent of the code is as follows: if no other thread holds the lock\nwhenpthread mutexlock() is called, the thread will acquire the lock\nand enter the critical section. If another thread does indeed hol d the lock,\nthe thread trying to grab the lock will not return from the call un til it has\nacquired the lock (implying that the thread holding the lock has released\nit via the unlock call). Of course, many threads may be stuck wai ting\ninside the lock acquisition function at a given time; only the thr ead with\nthe lock acquired, however, should call unlock.\nUnfortunately, this code is broken, in two important ways. The ﬁr st\nproblem is a lack of proper initialization . All locks must be properly\ninitialized in order to guarantee that they have the correct va lues to begin\nwith and thus work as desired when lock and unlock are called.\nWith POSIX threads, there are two ways to initialize locks. One way\nto do this is to use PTHREAD MUTEXINITIALIZER , as follows:\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\nDoing so sets the lock to the default values and thus makes the loc k\nusable. The dynamic way to do it (i.e., at run time) is to make a call to\npthread mutexinit() , as follows:\nint rc = pthread_mutex_init(&lock, NULL);\nassert(rc == 0); // always check success!\nThe ﬁrst argument to this routine is the address of the lock itsel f, whereas\nthe second is an optional set of attributes. Read more about the attr ibutes\nyourself; passing NULL in simply uses the defaults. Either way works, but\nwe usually use the dynamic (latter) method. Note that a correspon ding\ncall topthread mutexdestroy() should also be made, when you are\ndone with the lock; see the manual page for all of details.\nThe second problem with the code above is that it fails to check err or\ncodes when calling lock and unlock. Just like virtually any libr ary rou-\ntine you call in a U NIX system, these routines can also fail! If your code\ndoesn’t properly check error codes, the failure will happen silen tly, which\nin this case could allow multiple threads into a critical secti on. Minimally,\nuse wrappers, which assert that the routine succeeded (e.g., as in Fig-\nure 27.4); more sophisticated (non-toy) programs, which can’t sim ply exit\nwhen something goes wrong, should check for failure and do somethin g\nappropriate when the lock or unlock does not succeed.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : THREAD API 7\n// Use this to keep your code clean but check for failures\n// Only use if exiting program is OK upon failure\nvoid Pthread_mutex_lock(pthread_mutex_t *mutex) {\nint rc = pthread_mutex_lock(mutex);\nassert(rc == 0);\n}\nFigure 27.4: An Example Wrapper\nThe lock and unlock routines are not the only routines within the\npthreads library to interact with locks. In particular, here are two more\nroutines which may be of interest:\nint pthread_mutex_trylock(pthread_mutex_t *mutex);\nint pthread_mutex_timedlock(pthread_mutex_t *mutex,\nstruct timespec *abs_timeout);\nThese two calls are used in lock acquisition. The trylock version re-\nturns failure if the lock is already held; the timedlock version of acquir-\ning a lock returns after a timeout or after acquiring the lock, whi chever\nhappens ﬁrst. Thus, the timedlock with a timeout of zero degener ates\nto the trylock case. Both of these versions should generally be avoi ded;\nhowever, there are a few cases where avoiding getting stuck (pe rhaps in-\ndeﬁnitely) in a lock acquisition routine can be useful, as we’ll s ee in future\nchapters (e.g., when we study deadlock).\n27.4 Condition Variables\nThe other major component of any threads library, and certainly th e\ncase with POSIX threads, is the presence of a condition variable . Con-\ndition variables are useful when some kind of signaling must tak e place\nbetween threads, if one thread is waiting for another to do someth ing be-\nfore it can continue. Two primary routines are used by programs wi shing\nto interact in this way:\nint pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);\nint pthread_cond_signal(pthread_cond_t *cond);\nTo use a condition variable, one has to in addition have a lock that i s\nassociated with this condition. When calling either of the above r outines,\nthis lock should be held.\nThe ﬁrst routine, pthread condwait() , puts the calling thread to\nsleep, and thus waits for some other thread to signal it, usually when\nsomething in the program has changed that the now-sleeping thre ad might\ncare about. A typical usage looks like this:\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\npthread_cond_t cond = PTHREAD_COND_INITIALIZER;\nPthread_mutex_lock(&lock);\nwhile (ready == 0)\nPthread_cond_wait(&cond, &lock);\nPthread_mutex_unlock(&lock);\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 INTERLUDE : THREAD API\nIn this code, after initialization of the relevant lock and condit ion3, a\nthread checks to see if the variable ready has yet been set to something\nother than zero. If not, the thread simply calls the wait routine i n order to\nsleep until some other thread wakes it.\nThe code to wake a thread, which would run in some other thread,\nlooks like this:\nPthread_mutex_lock(&lock);\nready = 1;\nPthread_cond_signal(&cond);\nPthread_mutex_unlock(&lock);\nA few things to note about this code sequence. First, when signal ing\n(as well as when modifying the global variable ready ), we always make\nsure to have the lock held. This ensures that we don’t accidental ly intro-\nduce a race condition into our code.\nSecond, you might notice that the wait call takes a lock as its second\nparameter, whereas the signal call only takes a condition. The r eason\nfor this difference is that the wait call, in addition to puttin g the call-\ning thread to sleep, releases the lock when putting said caller to sleep.\nImagine if it did not: how could the other thread acquire the lock an d\nsignal it to wake up? However, before returning after being woken, the\npthread condwait() re-acquires the lock, thus ensuring that any time\nthe waiting thread is running between the lock acquire at the b eginning\nof the wait sequence, and the lock release at the end, it holds the lock.\nOne last oddity: the waiting thread re-checks the condition in a while\nloop, instead of a simple if statement. We’ll discuss this issue i n detail\nwhen we study condition variables in a future chapter, but in ge neral,\nusing a while loop is the simple and safe thing to do. Although it re checks\nthe condition (perhaps adding a little overhead), there are some pthread\nimplementations that could spuriously wake up a waiting thread ; in such\na case, without rechecking, the waiting thread will continue t hinking that\nthe condition has changed even though it has not. It is safer thus t o view\nwaking up as a hint that something might have changed, rather t han an\nabsolute fact.\nNote that sometimes it is tempting to use a simple ﬂag to signal b e-\ntween two threads, instead of a condition variable and associate d lock.\nFor example, we could rewrite the waiting code above to look more like\nthis in the waiting code:\nwhile (ready == 0)\n; // spin\nThe associated signaling code would look like this:\nready = 1;\n3Note that one could use pthread condinit() (and correspond-\ning the pthread conddestroy() call) instead of the static initializer\nPTHREAD CONDINITIALIZER . Sound like more work? It is.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : THREAD API 9\nDon’t ever do this, for the following reasons. First, it performs poorl y\nin many cases (spinning for a long time just wastes CPU cycles). Second,\nit is error prone. As recent research shows [X+10], it is surpris ingly easy\nto make mistakes when using ﬂags (as above) to synchronize betw een\nthreads; in that study, roughly half the uses of these ad hoc synchroniza-\ntions were buggy! Don’t be lazy; use condition variables even when you\nthink you can get away without doing so.\nIf condition variables sound confusing, don’t worry too much (yet) –\nwe’ll be covering them in great detail in a subsequent chapter. Until then,\nit should sufﬁce to know that they exist and to have some idea how an d\nwhy they are used.\n27.5 Compiling and Running\nAll of the code examples in this chapter are relatively easy to g et up\nand running. To compile them, you must include the header pthread.h\nin your code. On the link line, you must also explicitly link with the\npthreads library, by adding the -pthread ﬂag.\nFor example, to compile a simple multi-threaded program, all you\nhave to do is the following:\nprompt> gcc -o main main.c -Wall -pthread\nAs long as main.c includes the pthreads header, you have now suc-\ncessfully compiled a concurrent program. Whether it works or not, a s\nusual, is a different matter entirely.\n27.6 Summary\nWe have introduced the basics of the pthread library, includin g thread\ncreation, building mutual exclusion via locks, and signaling a nd waiting\nvia condition variables. You don’t need much else to write robust an d\nefﬁcient multi-threaded code, except patience and a great de al of care!\nWe now end the chapter with a set of tips that might be useful to you\nwhen you write multi-threaded code (see the aside on the following page\nfor details). There are other aspects of the API that are interes ting; if you\nwant more information, type man -k pthread on a Linux system to\nsee over one hundred APIs that make up the entire interface. Howe ver,\nthe basics discussed herein should enable you to build sophisti cated (and\nhopefully, correct and performant) multi-threaded programs. T he hard\npart with threads is not the APIs, but rather the tricky logic of h ow you\nbuild concurrent programs. Read on to learn more.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 INTERLUDE : THREAD API\nASIDE : THREAD API G UIDELINES\nThere are a number of small but important things to remember whe n\nyou use the POSIX thread library (or really, any thread library) to build a\nmulti-threaded program. They are:\n•Keep it simple. Above all else, any code to lock or signal between\nthreads should be as simple as possible. Tricky thread interac tions\nlead to bugs.\n•Minimize thread interactions. Try to keep the number of ways\nin which threads interact to a minimum. Each interaction shoul d\nbe carefully thought out and constructed with tried and true ap-\nproaches (many of which we will learn about in the coming chap-\nters).\n•Initialize locks and condition variables. Failure to do so will lead\nto code that sometimes works and sometimes fails in very strange\nways.\n•Check your return codes. Of course, in any C and U NIXprogram-\nming you do, you should be checking each and every return code,\nand it’s true here as well. Failure to do so will lead to bizarre and\nhard to understand behavior, making you likely to (a) scream, ( b)\npull some of your hair out, or (c) both.\n•Be careful with how you pass arguments to, and return values\nfrom, threads. In particular, any time you are passing a reference to\na variable allocated on the stack, you are probably doing something\nwrong.\n•Each thread has its own stack. As related to the point above, please\nremember that each thread has its own stack. Thus, if you have a\nlocally-allocated variable inside of some function a thread is ex e-\ncuting, it is essentially private to that thread; no other thread can\n(easily) access it. To share data between threads, the value s must be\nin the heap or otherwise some locale that is globally accessible.\n•Always use condition variables to signal between threads. While\nit is often tempting to use a simple ﬂag, don’t do it.\n•Use the manual pages. On Linux, in particular, the pthread man\npages are highly informative and discuss much of the nuances pr e-\nsented here, often in even more detail. Read them carefully!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : THREAD API 11\nReferences\n[B89] “An Introduction to Programming with Threads” by Andrew D. Birr ell. DEC Techni-\ncal Report, January, 1989. Available: https://birrell.org/and rew/papers/035-Threads.pdf A\nclassic but older introduction to threaded programming. Still a worthwhile read, and freely available.\n[B97] “Programming with POSIX Threads” by David R. Butenhof. Addison-Wes ley, May 1997.\nAnother one of these books on threads.\n[B+96] “PThreads Programming: by A POSIX Standard for Better Multiproces sing. ” Dick\nButtlar, Jacqueline Farrell, Bradford Nichols. O’Reilly, Septemb er 1996 A reasonable book from the\nexcellent, practical publishing house O’Reilly. Our bookshelves ce rtainly contain a great deal of books\nfrom this company, including some excellent offerings on Perl, Python , and Javascript (particularly\nCrockford’s “Javascript: The Good Parts”.)\n[K+96] “Programming With Threads” by Steve Kleiman, Devang Shah, Bart Smaalders. Pren-\ntice Hall, January 1996. Probably one of the better books in this space. Get it at your local library. Or\nsteal it from your mother. More seriously, just ask your mother for it – she’l l let you borrow it, don’t\nworry.\n[X+10] “Ad Hoc Synchronization Considered Harmful” by Weiwei Xiong, Soyeon Park, Jiaqi\nZhang, Yuanyuan Zhou, Zhiqiang Ma. OSDI 2010, Vancouver, Canada. This paper shows how\nseemingly simple synchronization code can lead to a surprising number of bugs. Use condition variables\nand do the signaling correctly!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 INTERLUDE : THREAD API\nHomework (Code)\nIn this section, we’ll write some simple multi-threaded program s and\nuse a speciﬁc tool, called helgrind , to ﬁnd problems in these programs.\nRead the README in the homework download for details on how to\nbuild the programs and run helgrind.\nQuestions\n1. First build main-race.c . Examine the code so you can see the (hopefully\nobvious) data race in the code. Now run helgrind (by typing valgrind\n--tool=helgrind main-race ) to see how it reports the race. Does it\npoint to the right lines of code? What other information does i t give to you?\n2. What happens when you remove one of the offending lines of co de? Now\nadd a lock around one of the updates to the shared variable, and t hen around\nboth. What does helgrind report in each of these cases?\n3. Now let’s look at main-deadlock.c . Examine the code. This code has a\nproblem known as deadlock (which we discuss in much more depth in a\nforthcoming chapter). Can you see what problem it might have?\n4. Now run helgrind on this code. What does helgrind report?\n5. Now run helgrind onmain-deadlock-global.c . Examine the code;\ndoes it have the same problem that main-deadlock.c has? Should helgrind\nbe reporting the same error? What does this tell you about tools likehelgrind ?\n6. Let’s next look at main-signal.c . This code uses a variable ( done ) to\nsignal that the child is done and that the parent can now conti nue. Why is\nthis code inefﬁcient? (what does the parent end up spending it s time doing,\nparticularly if the child thread takes a long time to complete?)\n7. Now run helgrind on this program. What does it report? Is the code\ncorrect?\n8. Now look at a slightly modiﬁed version of the code, which is f ound in\nmain-signal-cv.c . This version uses a condition variable to do the sig-\nnaling (and associated lock). Why is this code preferred to t he previous\nversion? Is it correctness, or performance, or both?\n9. Once again run helgrind onmain-signal-cv . Does it report any errors?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",26251
33-28. Locks.pdf,33-28. Locks,"28\nLocks\nFrom the introduction to concurrency, we saw one of the fundamental\nproblems in concurrent programming: we would like to execute a se ries\nof instructions atomically, but due to the presence of interrupt s on a single\nprocessor (or multiple threads executing on multiple processors c oncur-\nrently), we couldn’t. In this chapter, we thus attack this probl em directly,\nwith the introduction of something referred to as a lock . Programmers\nannotate source code with locks, putting them around critical sec tions,\nand thus ensure that any such critical section executes as if i t were a sin-\ngle atomic instruction.\n28.1 Locks: The Basic Idea\nAs an example, assume our critical section looks like this, the ca nonical\nupdate of a shared variable:\nbalance = balance + 1;\nOf course, other critical sections are possible, such as adding a n ele-\nment to a linked list or other more complex updates to shared struc tures,\nbut we’ll just keep to this simple example for now. To use a lock, we add\nsome code around the critical section like this:\n1lock_t mutex; // some globally-allocated lock ’mutex’\n2...\n3lock(&mutex);\n4balance = balance + 1;\n5unlock(&mutex);\nA lock is just a variable, and thus to use one, you must declare a lock\nvariable of some kind (such as mutex above). This lock variable (or just\n“lock” for short) holds the state of the lock at any instant in time. I t is ei-\nther available (orunlocked orfree) and thus no thread holds the lock, or\nacquired (orlocked orheld ), and thus exactly one thread holds the lock\nand presumably is in a critical section. We could store other infor mation\nin the data type as well, such as which thread holds the lock, or a q ueue\n1\n2 LOCKS\nfor ordering lock acquisition, but information like that is hidden from the\nuser of the lock.\nThe semantics of the lock() andunlock() routines are simple. Call-\ning the routine lock() tries to acquire the lock; if no other thread holds\nthe lock (i.e., it is free), the thread will acquire the lock and enter the crit-\nical section; this thread is sometimes said to be the owner of the lock. If\nanother thread then calls lock() on that same lock variable ( mutex in\nthis example), it will not return while the lock is held by anothe r thread;\nin this way, other threads are prevented from entering the crit ical section\nwhile the ﬁrst thread that holds the lock is in there.\nOnce the owner of the lock calls unlock() , the lock is now available\n(free) again. If no other threads are waiting for the lock (i.e., no other\nthread has called lock() and is stuck therein), the state of the lock is\nsimply changed to free. If there are waiting threads (stuck i nlock() ),\none of them will (eventually) notice (or be informed of) this change of the\nlock’s state, acquire the lock, and enter the critical section.\nLocks provide some minimal amount of control over scheduling to\nprogrammers. In general, we view threads as entities created by the pro-\ngrammer but scheduled by the OS, in any fashion that the OS chooses .\nLocks yield some of that control back to the programmer; by putting\na lock around a section of code, the programmer can guarantee that no\nmore than a single thread can ever be active within that code. Th us locks\nhelp transform the chaos that is traditional OS scheduling into a more\ncontrolled activity.\n28.2 Pthread Locks\nThe name that the POSIX library uses for a lock is a mutex , as it is used\nto provide mutual exclusion between threads, i.e., if one thread is in the\ncritical section, it excludes the others from entering until it has completed\nthe section. Thus, when you see the following POSIX threads code, you\nshould understand that it is doing the same thing as above (we aga in use\nour wrappers that check for errors upon lock and unlock):\n1pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\n2\n3Pthread_mutex_lock(&lock); // wrapper; exits on failure\n4balance = balance + 1;\n5Pthread_mutex_unlock(&lock);\nYou might also notice here that the POSIX version passes a variabl e\nto lock and unlock, as we may be using different locks to protect different\nvariables. Doing so can increase concurrency: instead of one big lock that\nis used any time any critical section is accessed (a coarse-grained locking\nstrategy), one will often protect different data and data struc tures with\ndifferent locks, thus allowing more threads to be in locked code at once\n(a more ﬁne-grained approach).\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOCKS 3\n28.3 Building A Lock\nBy now, you should have some understanding of how a lock works,\nfrom the perspective of a programmer. But how should we build a lock?\nWhat hardware support is needed? What OS support? It is this set of\nquestions we address in the rest of this chapter.\nTHECRUX: HOWTOBUILD A L OCK\nHow can we build an efﬁcient lock? Efﬁcient locks provide mutual\nexclusion at low cost, and also might attain a few other properties we\ndiscuss below. What hardware support is needed? What OS support ?\nTo build a working lock, we will need some help from our old friend,\nthe hardware, as well as our good pal, the OS. Over the years, a num-\nber of different hardware primitives have been added to the in struction\nsets of various computer architectures; while we won’t study how th ese\ninstructions are implemented (that, after all, is the topic of a computer\narchitecture class), we will study how to use them in order to bu ild a mu-\ntual exclusion primitive like a lock. We will also study how the O S gets\ninvolved to complete the picture and enable us to build a sophist icated\nlocking library.\n28.4 Evaluating Locks\nBefore building any locks, we should ﬁrst understand what our goal s\nare, and thus we ask how to evaluate the efﬁcacy of a particular l ock\nimplementation. To evaluate whether a lock works (and works well ), we\nshould ﬁrst establish some basic criteria. The ﬁrst is whether the lock does\nits basic task, which is to provide mutual exclusion . Basically, does the\nlock work, preventing multiple threads from entering a critica l section?\nThe second is fairness . Does each thread contending for the lock get\na fair shot at acquiring it once it is free? Another way to look at thi s is\nby examining the more extreme case: does any thread contending f or the\nlock starve while doing so, thus never obtaining it?\nThe ﬁnal criterion is performance , speciﬁcally the time overheads added\nby using the lock. There are a few different cases that are worth con-\nsidering here. One is the case of no contention; when a single thr ead\nis running and grabs and releases the lock, what is the overhead of do-\ning so? Another is the case where multiple threads are contendin g for\nthe lock on a single CPU; in this case, are there performance conce rns? Fi-\nnally, how does the lock perform when there are multiple CPUs invol ved,\nand threads on each contending for the lock? By comparing these dif fer-\nent scenarios, we can better understand the performance impac t of using\nvarious locking techniques, as described below.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 LOCKS\n28.5 Controlling Interrupts\nOne of the earliest solutions used to provide mutual exclusion was\nto disable interrupts for critical sections; this solution was i nvented for\nsingle-processor systems. The code would look like this:\n1void lock() {\n2DisableInterrupts();\n3}\n4void unlock() {\n5EnableInterrupts();\n6}\nAssume we are running on such a single-processor system. By turn -\ning off interrupts (using some kind of special hardware instruc tion) be-\nfore entering a critical section, we ensure that the code inside the critical\nsection will notbe interrupted, and thus will execute as if it were atomic.\nWhen we are ﬁnished, we re-enable interrupts (again, via a ha rdware in-\nstruction) and thus the program proceeds as usual.\nThe main positive of this approach is its simplicity. You certain ly don’t\nhave to scratch your head too hard to ﬁgure out why this works. With out\ninterruption, a thread can be sure that the code it executes wil l execute\nand that no other thread will interfere with it.\nThe negatives, unfortunately, are many. First, this approach requires\nus to allow any calling thread to perform a privileged operation (turning\ninterrupts on and off), and thus trust that this facility is not abused. As\nyou already know, any time we are required to trust an arbitrary pro-\ngram, we are probably in trouble. Here, the trouble manifests in numer-\nous ways: a greedy program could call lock() at the beginning of its\nexecution and thus monopolize the processor; worse, an errant or mali -\ncious program could call lock() and go into an endless loop. In this\nlatter case, the OS never regains control of the system, and ther e is only\none recourse: restart the system. Using interrupt disabling a s a general-\npurpose synchronization solution requires too much trust in appli cations.\nSecond, the approach does not work on multiprocessors. If multiple\nthreads are running on different CPUs, and each try to enter th e same\ncritical section, it does not matter whether interrupts are dis abled; threads\nwill be able to run on other processors, and thus could enter the cri tical\nsection. As multiprocessors are now commonplace, our general soluti on\nwill have to do better than this.\nThird, turning off interrupts for extended periods of time can le ad to\ninterrupts becoming lost, which can lead to serious systems prob lems.\nImagine, for example, if the CPU missed the fact that a disk dev ice has\nﬁnished a read request. How will the OS know to wake the process wa it-\ning for said read?\nFinally, and probably least important, this approach can be ine fﬁcient.\nCompared to normal instruction execution, code that masks or unmas ks\ninterrupts tends to be executed slowly by modern CPUs.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOCKS 5\n1typedef struct __lock_t { int flag; } lock_t;\n2\n3void init(lock_t *mutex) {\n4// 0 -> lock is available, 1 -> held\n5mutex->flag = 0;\n6}\n7\n8void lock(lock_t *mutex) {\n9while (mutex->flag == 1) // TEST the flag\n10 ; // spin-wait (do nothing)\n11mutex->flag = 1; // now SET it!\n12}\n13\n14void unlock(lock_t *mutex) {\n15mutex->flag = 0;\n16}\nFigure 28.1: First Attempt: A Simple Flag\nFor these reasons, turning off interrupts is only used in limited con-\ntexts as a mutual-exclusion primitive. For example, in some cas es an\noperating system itself will use interrupt masking to guaran tee atom-\nicity when accessing its own data structures, or at least to pre vent cer-\ntain messy interrupt handling situations from arising. This u sage makes\nsense, as the trust issue disappears inside the OS, which alwa ys trusts\nitself to perform privileged operations anyhow.\n28.6 A Failed Attempt: Just Using Loads/Stores\nTo move beyond interrupt-based techniques, we will have to rel y on\nCPU hardware and the instructions it provides us to build a prope r lock.\nLet’s ﬁrst try to build a simple lock by using a single ﬂag variab le. In this\nfailed attempt, we’ll see some of the basic ideas needed to build a lock,\nand (hopefully) see why just using a single variable and acces sing it via\nnormal loads and stores is insufﬁcient.\nIn this ﬁrst attempt (Figure 28.1), the idea is quite simple: use a simple\nvariable (flag ) to indicate whether some thread has possession of a lock.\nThe ﬁrst thread that enters the critical section will call lock() , which\ntests whether the ﬂag is equal to 1 (in this case, it is not), and then sets\nthe ﬂag to 1 to indicate that the thread now holds the lock. When ﬁnished\nwith the critical section, the thread calls unlock() and clears the ﬂag,\nthus indicating that the lock is no longer held.\nIf another thread happens to call lock() while that ﬁrst thread is in\nthe critical section, it will simply spin-wait in the while loop for that\nthread to call unlock() and clear the ﬂag. Once that ﬁrst thread does\nso, the waiting thread will fall out of the while loop, set the ﬂag to 1 for\nitself, and proceed into the critical section.\nUnfortunately, the code has two problems: one of correctness, and a n-\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 LOCKS\nThread 1 Thread 2\ncalllock()\nwhile (ﬂag == 1)\ninterrupt: switch to Thread 2\ncalllock()\nwhile (ﬂag == 1)\nﬂag = 1;\ninterrupt: switch to Thread 1\nﬂag = 1; // set ﬂag to 1 (too!)\nFigure 28.2: Trace: No Mutual Exclusion\nother of performance. The correctness problem is simple to see once you\nget used to thinking about concurrent programming. Imagine the code\ninterleaving in Figure 28.2; assume flag=0 to begin.\nAs you can see from this interleaving, with timely (untimely?) inter-\nrupts, we can easily produce a case where both threads set the ﬂag to 1\nand both threads are thus able to enter the critical section. Th is behavior\nis what professionals call “bad” – we have obviously failed to prov ide the\nmost basic requirement: providing mutual exclusion.\nThe performance problem, which we will address more later on, is t he\nfact that the way a thread waits to acquire a lock that is alread y held:\nit endlessly checks the value of ﬂag, a technique known as spin-waiting .\nSpin-waiting wastes time waiting for another thread to release a lock. The\nwaste is exceptionally high on a uniprocessor, where the thread t hat the\nwaiter is waiting for cannot even run (at least, until a context s witch oc-\ncurs)! Thus, as we move forward and develop more sophisticated solu -\ntions, we should also consider ways to avoid this kind of waste.\n28.7 Building Working Spin Locks with Test-And-Set\nBecause disabling interrupts does not work on multiple processors ,\nand because simple approaches using loads and stores (as shown ab ove)\ndon’t work, system designers started to invent hardware support for lock-\ning. The earliest multiprocessor systems, such as the Burrough s B5000 in\nthe early 1960’s [M82], had such support; today all systems provi de this\ntype of support, even for single CPU systems.\nThe simplest bit of hardware support to understand is known as a\ntest-and-set (oratomic exchange1) instruction. We deﬁne what the test-\nand-set instruction does via the following C code snippet:\n1int TestAndSet(int *old_ptr, int new) {\n2int old = *old_ptr; // fetch old value at old_ptr\n3*old_ptr = new; // store ’new’ into old_ptr\n4return old; // return the old value\n5}\n1Each architecture that supports test-and-set calls it by a different name . On SPARC it is\ncalled the load/store unsigned byte instruction ( ldstub ); on x86 it is the locked version of the\natomic exchange ( xchg ).\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOCKS 7\nASIDE : DEKKER ’SANDPETERSON ’SALGORITHMS\nIn the 1960’s, Dijkstra posed the concurrency problem to his frie nds, and\none of them, a mathematician named Theodorus Jozef Dekker, came up\nwith a solution [D68]. Unlike the solutions we discuss here, whic h use\nspecial hardware instructions and even OS support, Dekker’s algorithm\nuses just loads and stores (assuming they are atomic with respec t to each\nother, which was true on early hardware).\nDekker’s approach was later reﬁned by Peterson [P81]. Once agai n, just\nloads and stores are used, and the idea is to ensure that two thre ads never\nenter a critical section at the same time. Here is Peterson’s algorithm (for\ntwo threads); see if you can understand the code. What are the flag and\nturn variables used for?\nint flag[2];\nint turn;\nvoid init() {\n// indicate you intend to hold the lock w/ ’flag’\nflag[0] = flag[1] = 0;\n// whose turn is it? (thread 0 or 1)\nturn = 0;\n}\nvoid lock() {\n// ’self’ is the thread ID of caller\nflag[self] = 1;\n// make it other thread’s turn\nturn = 1 - self;\nwhile ((flag[1-self] == 1) && (turn == 1 - self))\n; // spin-wait while it’s not your turn\n}\nvoid unlock() {\n// simply undo your intent\nflag[self] = 0;\n}\nFor some reason, developing locks that work without special hardwar e\nsupport became all the rage for a while, giving theory-types a lot of prob-\nlems to work on. Of course, this line of work became quite useless wh en\npeople realized it is much easier to assume a little hardware s upport (and\nindeed that support had been around from the earliest days of mult ipro-\ncessing). Further, algorithms like the ones above don’t work on mod-\nern hardware (due to relaxed memory consistency models), thus m aking\nthem even less useful than they were before. Yet more research r elegated\nto the dustbin of history...\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 LOCKS\n1typedef struct __lock_t {\n2int flag;\n3} lock_t;\n4\n5void init(lock_t *lock) {\n6// 0: lock is available, 1: lock is held\n7lock->flag = 0;\n8}\n9\n10void lock(lock_t *lock) {\n11while (TestAndSet(&lock->flag, 1) == 1)\n12 ; // spin-wait (do nothing)\n13}\n14\n15void unlock(lock_t *lock) {\n16lock->flag = 0;\n17}\nFigure 28.3: A Simple Spin Lock Using Test-and-set\nWhat the test-and-set instruction does is as follows. It returns the old\nvalue pointed to by the ptr, and simultaneously updates said value to\nnew. The key, of course, is that this sequence of operations is performe d\natomically . The reason it is called “test and set” is that it enables you\nto “test” the old value (which is what is returned) while simul taneously\n“setting” the memory location to a new value; as it turns out, this slightly\nmore powerful instruction is enough to build a simple spin lock , as we\nnow examine in Figure 28.3. Or better yet: ﬁgure it out ﬁrst yours elf!\nLet’s make sure we understand why this lock works. Imagine ﬁrst t he\ncase where a thread calls lock() and no other thread currently holds the\nlock; thus, flag should be 0. When the thread calls TestAndSet(flag,\n1), the routine will return the old value of flag , which is 0; thus, the call-\ning thread, which is testing the value of ﬂag, will not get caught spinning\nin the while loop and will acquire the lock. The thread will also a tomi-\ncally setthe value to 1, thus indicating that the lock is now held. When\nthe thread is ﬁnished with its critical section, it calls unlock() to set the\nﬂag back to zero.\nThe second case we can imagine arises when one thread already ha s\nthe lock held (i.e., flag is 1). In this case, this thread will call lock() and\nthen callTestAndSet(flag, 1) as well. This time, TestAndSet()\nwill return the old value at ﬂag, which is 1 (because the lock is h eld),\nwhile simultaneously setting it to 1 again. As long as the lock is held by\nanother thread, TestAndSet() will repeatedly return 1, and thus this\nthread will spin and spin until the lock is ﬁnally released. Wh en the ﬂag is\nﬁnally set to 0 by some other thread, this thread will call TestAndSet()\nagain, which will now return 0 while atomically setting the val ue to 1 and\nthus acquire the lock and enter the critical section.\nBy making both the test (of the old lock value) and set(of the new\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOCKS 9\nTIP: THINK ABOUT CONCURRENCY ASA M ALICIOUS SCHEDULER\nFrom this example, you might get a sense of the approach you need to\ntake to understand concurrent execution. What you should try to d o is to\npretend you are a malicious scheduler , one that interrupts threads at the\nmost inopportune of times in order to foil their feeble attempts at b uilding\nsynchronization primitives. What a mean scheduler you are! Alt hough\nthe exact sequence of interrupts may be improbable , it is possible , and that\nis all we need to demonstrate that a particular approach does not w ork.\nIt can be useful to think maliciously! (at least, sometimes)\nvalue) a single atomic operation, we ensure that only one thread ac quires\nthe lock. And that’s how to build a working mutual exclusion primit ive!\nYou may also now understand why this type of lock is usually referr ed\nto as a spin lock . It is the simplest type of lock to build, and simply spins,\nusing CPU cycles, until the lock becomes available. To work corre ctly\non a single processor, it requires a preemptive scheduler (i.e., one that\nwill interrupt a thread via a timer, in order to run a different thread, from\ntime to time). Without preemption, spin locks don’t make much sens e on\na single CPU, as a thread spinning on a CPU will never relinquis h it.\n28.8 Evaluating Spin Locks\nGiven our basic spin lock, we can now evaluate how effective it is\nalong our previously described axes. The most important aspect of a lock\niscorrectness : does it provide mutual exclusion? The answer here is yes:\nthe spin lock only allows a single thread to enter the critical se ction at a\ntime. Thus, we have a correct lock.\nThe next axis is fairness . How fair is a spin lock to a waiting thread?\nCan you guarantee that a waiting thread will ever enter the cri tical sec-\ntion? The answer here, unfortunately, is bad news: spin locks don ’t pro-\nvide any fairness guarantees. Indeed, a thread spinning may spin forever,\nunder contention. Simple spin locks (as discussed thus far) are n ot fair\nand may lead to starvation.\nThe ﬁnal axis is performance . What are the costs of using a spin lock?\nTo analyze this more carefully, we suggest thinking about a few different\ncases. In the ﬁrst, imagine threads competing for the lock on a sin gle\nprocessor; in the second, consider threads spread out across many C PUs.\nFor spin locks, in the single CPU case, performance overheads can\nbe quite painful; imagine the case where the thread holding th e lock is\npreempted within a critical section. The scheduler might the n run every\nother thread (imagine there are N−1others), each of which tries to ac-\nquire the lock. In this case, each of those threads will spin for th e duration\nof a time slice before giving up the CPU, a waste of CPU cycles.\nHowever, on multiple CPUs, spin locks work reasonably well (if the\nnumber of threads roughly equals the number of CPUs). The thinki ng\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 LOCKS\n1int CompareAndSwap(int *ptr, int expected, int new) {\n2int actual = *ptr;\n3if (actual == expected)\n4 *ptr = new;\n5return actual;\n6}\nFigure 28.4: Compare-and-swap\ngoes as follows: imagine Thread A on CPU 1 and Thread B on CPU 2,\nboth contending for a lock. If Thread A (CPU 1) grabs the lock, and th en\nThread B tries to, B will spin (on CPU 2). However, presumably the crit-\nical section is short, and thus soon the lock becomes available, and is ac-\nquired by Thread B. Spinning to wait for a lock held on another proces sor\ndoesn’t waste many cycles in this case, and thus can be effectiv e.\n28.9 Compare-And-Swap\nAnother hardware primitive that some systems provide is known as\nthecompare-and-swap instruction (as it is called on SPARC, for exam-\nple), or compare-and-exchange (as it called on x86). The C pseudocode\nfor this single instruction is found in Figure 28.4.\nThe basic idea is for compare-and-swap to test whether the valu e at the\naddress speciﬁed by ptr is equal to expected ; if so, update the memory\nlocation pointed to by ptr with the new value. If not, do nothing. In\neither case, return the actual value at that memory location, th us allowing\nthe code calling compare-and-swap to know whether it succeeded or not.\nWith the compare-and-swap instruction, we can build a lock in a m an-\nner quite similar to that with test-and-set. For example, we c ould just\nreplace the lock() routine above with the following:\n1void lock(lock_t *lock) {\n2while (CompareAndSwap(&lock->flag, 0, 1) == 1)\n3 ; // spin\n4}\nThe rest of the code is the same as the test-and-set example above .\nThis code works quite similarly; it simply checks if the ﬂag is 0 and if\nso, atomically swaps in a 1 thus acquiring the lock. Threads that try to\nacquire the lock while it is held will get stuck spinning until the lock is\nﬁnally released.\nIf you want to see how to really make a C-callable x86-version of\ncompare-and-swap, the code sequence (from [S05]) might be usefu l2.\nFinally, as you may have sensed, compare-and-swap is a more power -\nful instruction than test-and-set. We will make some use of this power in\n2github.com/remzi-arpacidusseau/ostep-code/tree/mast er/threads-locks\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOCKS 11\nthe future when we brieﬂy delve into topics such as lock-free synchro-\nnization [H91]. However, if we just build a simple spin lock with it, its\nbehavior is identical to the spin lock we analyzed above.\n28.10 Load-Linked and Store-Conditional\nSome platforms provide a pair of instructions that work in concert to\nhelp build critical sections. On the MIPS architecture [H93] , for example,\ntheload-linked and store-conditional instructions can be used in tandem\nto build locks and other concurrent structures. The C pseudocode f or\nthese instructions is as found in Figure 28.5. Alpha, PowerPC, a nd ARM\nprovide similar instructions [W09].\nThe load-linked operates much like a typical load instruction, a nd sim-\nply fetches a value from memory and places it in a register. The k ey differ-\nence comes with the store-conditional, which only succeeds (and u pdates\nthe value stored at the address just load-linked from) if no inte rvening\nstore to the address has taken place. In the case of success, the store-\nconditional returns 1 and updates the value at ptr tovalue ; if it fails,\nthe value at ptr isnotupdated and 0 is returned.\nAs a challenge to yourself, try thinking about how to build a lock u sing\nload-linked and store-conditional. Then, when you are ﬁnished, l ook at\nthe code below which provides one simple solution. Do it! The solution\nis in Figure 28.6.\nThelock() code is the only interesting piece. First, a thread spins\nwaiting for the ﬂag to be set to 0 (and thus indicate the lock is not held).\nOnce so, the thread tries to acquire the lock via the store-condit ional; if it\nsucceeds, the thread has atomically changed the ﬂag’s value to 1 and thus\ncan proceed into the critical section.\nNote how failure of the store-conditional might arise. One thread c alls\nlock() and executes the load-linked, returning 0 as the lock is not held .\nBefore it can attempt the store-conditional, it is interrupted a nd another\nthread enters the lock code, also executing the load-linked ins truction,\n1int LoadLinked(int *ptr) {\n2return*ptr;\n3}\n4\n5int StoreConditional(int *ptr, int value) {\n6if (no update to *ptr since LoadLinked to this address) {\n7 *ptr = value;\n8 return 1; // success!\n9} else {\n10 return 0; // failed to update\n11}\n12}\nFigure 28.5: Load-linked And Store-conditional\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 LOCKS\n1void lock(lock_t *lock) {\n2while (1) {\n3 while (LoadLinked(&lock->flag) == 1)\n4 ; // spin until it’s zero\n5 if (StoreConditional(&lock->flag, 1) == 1)\n6 return; // if set-it-to-1 was a success: all done\n7 // otherwise: try it all over again\n8}\n9}\n10\n11void unlock(lock_t *lock) {\n12lock->flag = 0;\n13}\nFigure 28.6: Using LL/SC To Build A Lock\nand also getting a 0 and continuing. At this point, two threads h ave\neach executed the load-linked and each are about to attempt the store-\nconditional. The key feature of these instructions is that only one of these\nthreads will succeed in updating the ﬂag to 1 and thus acquire the lock;\nthe second thread to attempt the store-conditional will fail (be cause the\nother thread updated the value of ﬂag between its load-linked an d store-\nconditional) and thus have to try to acquire the lock again.\nIn class a few years ago, undergraduate student David Capel su g-\ngested a more concise form of the above, for those of you who enjoy\nshort-circuiting boolean conditionals. See if you can ﬁgure out why i t\nis equivalent. It certainly is shorter!\n1void lock(lock_t *lock) {\n2while (LoadLinked(&lock->flag) ||\n3 !StoreConditional(&lock->flag, 1))\n4 ; // spin\n5}\n28.11 Fetch-And-Add\nOne ﬁnal hardware primitive is the fetch-and-add instruction, which\natomically increments a value while returning the old value at a partic-\nular address. The C pseudocode for the fetch-and-add instructi on looks\nlike this:\n1int FetchAndAdd(int *ptr) {\n2int old = *ptr;\n3*ptr = old + 1;\n4return old;\n5}\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOCKS 13\nTIP: LESSCODE ISBETTER CODE (LAUER ’SLAW)\nProgrammers tend to brag about how much code they wrote to do some-\nthing. Doing so is fundamentally broken. What one should brag abou t,\nrather, is how little code one wrote to accomplish a given task. Short,\nconcise code is always preferred; it is likely easier to unders tand and has\nfewer bugs. As Hugh Lauer said, when discussing the construct ion of\nthe Pilot operating system: “If the same people had twice as much time,\nthey could produce as good of a system in half the code.” [L81] We’ll ca ll\nthis Lauer’s Law , and it is well worth remembering. So next time you’re\nbragging about how much code you wrote to ﬁnish the assignment, thi nk\nagain, or better yet, go back, rewrite, and make the code as clea r and con-\ncise as possible.\nIn this example, we’ll use fetch-and-add to build a more intere sting\nticket lock , as introduced by Mellor-Crummey and Scott [MS91]. The\nlock and unlock code is found in Figure 28.7 (page 14).\nInstead of a single value, this solution uses a ticket and turn va riable in\ncombination to build a lock. The basic operation is pretty simple: when\na thread wishes to acquire a lock, it ﬁrst does an atomic fetch-an d-add\non the ticket value; that value is now considered this thread’s “t urn”\n(myturn ). The globally shared lock->turn is then used to determine\nwhich thread’s turn it is; when (myturn == turn) for a given thread,\nit is that thread’s turn to enter the critical section. Unlock is accomplished\nsimply by incrementing the turn such that the next waiting th read (if\nthere is one) can now enter the critical section.\nNote one important difference with this solution versus our previou s\nattempts: it ensures progress for all threads. Once a thread is assigned its\nticket value, it will be scheduled at some point in the future (on ce those in\nfront of it have passed through the critical section and released the lock).\nIn our previous attempts, no such guarantee existed; a thread s pinning\non test-and-set (for example) could spin forever even as other thr eads\nacquire and release the lock.\n28.12 Too Much Spinning: What Now?\nOur simple hardware-based locks are simple (only a few lines of c ode)\nand they work (you could even prove that if you’d like to, by writing\nsome code), which are two excellent properties of any system or code .\nHowever, in some cases, these solutions can be quite inefﬁcient. Imagine\nyou are running two threads on a single processor. Now imagine that\none thread (thread 0) is in a critical section and thus has a lock h eld, and\nunfortunately gets interrupted. The second thread (thread 1) now tries to\nacquire the lock, but ﬁnds that it is held. Thus, it begins to sp in. And spin.\nThen it spins some more. And ﬁnally, a timer interrupt goes off, th read\n0 is run again, which releases the lock, and ﬁnally (the next ti me it runs,\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 LOCKS\n1typedef struct __lock_t {\n2int ticket;\n3int turn;\n4} lock_t;\n5\n6void lock_init(lock_t *lock) {\n7lock->ticket = 0;\n8lock->turn = 0;\n9}\n10\n11void lock(lock_t *lock) {\n12int myturn = FetchAndAdd(&lock->ticket);\n13while (lock->turn != myturn)\n14 ; // spin\n15}\n16\n17void unlock(lock_t *lock) {\n18lock->turn = lock->turn + 1;\n19}\nFigure 28.7: Ticket Locks\nsay), thread 1 won’t have to spin so much and will be able to acqui re the\nlock. Thus, any time a thread gets caught spinning in a situati on like this,\nit wastes an entire time slice doing nothing but checking a valu e that isn’t\ngoing to change! The problem gets worse with Nthreads contending\nfor a lock; N−1time slices may be wasted in a similar manner, simply\nspinning and waiting for a single thread to release the lock. An d thus,\nour next problem:\nTHECRUX: HOWTOAVOID SPINNING\nHow can we develop a lock that doesn’t needlessly waste time spin-\nning on the CPU?\nHardware support alone cannot solve the problem. We’ll need OS sup-\nport too! Let’s now ﬁgure out just how that might work.\n28.13 A Simple Approach: Just Yield, Baby\nHardware support got us pretty far: working locks, and even (as wi th\nthe case of the ticket lock) fairness in lock acquisition. However , we still\nhave a problem: what to do when a context switch occurs in a critic al\nsection, and threads start to spin endlessly, waiting for the i nterrupted\n(lock-holding) thread to be run again?\nOur ﬁrst try is a simple and friendly approach: when you are going to\nspin, instead give up the CPU to another thread. Or, as Al Davis might\nsay, “just yield, baby!” [D91]. Figure 28.8 (page 15) present s the approach.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOCKS 15\n1void init() {\n2flag = 0;\n3}\n4\n5void lock() {\n6while (TestAndSet(&flag, 1) == 1)\n7 yield(); // give up the CPU\n8}\n9\n10void unlock() {\n11flag = 0;\n12}\nFigure 28.8: Lock With Test-and-set And Yield\nIn this approach, we assume an operating system primitive yield()\nwhich a thread can call when it wants to give up the CPU and let a n-\nother thread run. A thread can be in one of three states (running, ready,\nor blocked); yield is simply a system call that moves the caller f rom the\nrunning state to the ready state, and thus promotes another thread to\nrunning. Thus, the yielding process essentially deschedules itself.\nThink about the example with two threads on one CPU; in this case,\nour yield-based approach works quite well. If a thread happens t o call\nlock() and ﬁnd a lock held, it will simply yield the CPU, and thus the\nother thread will run and ﬁnish its critical section. In this si mple case, the\nyielding approach works well.\nLet us now consider the case where there are many threads (say 10 0)\ncontending for a lock repeatedly. In this case, if one thread acqu ires\nthe lock and is preempted before releasing it, the other 99 will e ach call\nlock() , ﬁnd the lock held, and yield the CPU. Assuming some kind\nof round-robin scheduler, each of the 99 will execute this run-an d-yield\npattern before the thread holding the lock gets to run again. Whi le better\nthan our spinning approach (which would waste 99 time slices spi nning),\nthis approach is still costly; the cost of a context switch can be su bstantial,\nand there is thus plenty of waste.\nWorse, we have not tackled the starvation problem at all. A thread\nmay get caught in an endless yield loop while other threads repea tedly\nenter and exit the critical section. We clearly will need an ap proach that\naddresses this problem directly.\n28.14 Using Queues: Sleeping Instead Of Spinning\nThe real problem with our previous approaches is that they leave t oo\nmuch to chance. The scheduler determines which thread runs n ext; if\nthe scheduler makes a bad choice, a thread runs that must eithe r spin\nwaiting for the lock (our ﬁrst approach), or yield the CPU immediat ely\n(our second approach). Either way, there is potential for waste an d no\nprevention of starvation.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 LOCKS\n1typedef struct __lock_t {\n2int flag;\n3int guard;\n4queue_t *q;\n5} lock_t;\n6\n7void lock_init(lock_t *m) {\n8m->flag = 0;\n9m->guard = 0;\n10queue_init(m->q);\n11}\n12\n13void lock(lock_t *m) {\n14while (TestAndSet(&m->guard, 1) == 1)\n15 ; //acquire guard lock by spinning\n16if (m->flag == 0) {\n17 m->flag = 1; // lock is acquired\n18 m->guard = 0;\n19} else {\n20 queue_add(m->q, gettid());\n21 m->guard = 0;\n22 park();\n23}\n24}\n25\n26void unlock(lock_t *m) {\n27while (TestAndSet(&m->guard, 1) == 1)\n28 ; //acquire guard lock by spinning\n29if (queue_empty(m->q))\n30 m->flag = 0; // let go of lock; no one wants it\n31else\n32 unpark(queue_remove(m->q)); // hold lock\n33 // (for next thread!)\n34m->guard = 0;\n35}\nFigure 28.9: Lock With Queues, Test-and-set, Yield, And Wakeup\nThus, we must explicitly exert some control over which thread nex t\ngets to acquire the lock after the current holder releases it. T o do this, we\nwill need a little more OS support, as well as a queue to keep trac k of\nwhich threads are waiting to acquire the lock.\nFor simplicity, we will use the support provided by Solaris, in ter ms of\ntwo calls:park() to put a calling thread to sleep, and unpark(threadID)\nto wake a particular thread as designated by threadID . These two rou-\ntines can be used in tandem to build a lock that puts a caller to s leep if it\ntries to acquire a held lock and wakes it when the lock is free. Le t’s look at\nthe code in Figure 28.9 to understand one possible use of such prim itives.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOCKS 17\nASIDE : M ORE REASON TOAVOID SPINNING : PRIORITY INVERSION\nOne good reason to avoid spin locks is performance: as described in t he\nmain text, if a thread is interrupted while holding a lock, other threads\nthat use spin locks will spend a large amount of CPU time just wait ing for\nthe lock to become available. However, it turns out there is anothe r inter-\nesting reason to avoid spin locks on some systems: correctness. The prob-\nlem to be wary of is known as priority inversion , which unfortunately is\nan intergalactic scourge, occurring on Earth [M15] and Mars [R9 7]!\nLet’s assume there are two threads in a system. Thread 2 (T2) ha s a high\nscheduling priority, and Thread 1 (T1) has lower priority. In th is example,\nlet’s assume that the CPU scheduler will always run T2 over T1, i f indeed\nboth are runnable; T1 only runs when T2 is not able to do so (e.g., w hen\nT2 is blocked on I/O).\nNow, the problem. Assume T2 is blocked for some reason. So T1 runs,\ngrabs a spin lock, and enters a critical section. T2 now becomes un blocked\n(perhaps because an I/O completed), and the CPU scheduler imm edi-\nately schedules it (thus descheduling T1). T2 now tries to acq uire the lock,\nand because it can’t (T1 holds the lock), it just keeps spinning. Because\nthe lock is a spin lock, T2 spins forever, and the system is hung.\nJust avoiding the use of spin locks, unfortunately, does not avoid th e\nproblem of inversion (alas). Imagine three threads, T1, T2, and T3, with\nT3 at the highest priority, and T1 the lowest. Imagine now that T1 grabs\na lock. T3 then starts, and because it is higher priority than T1 , runs im-\nmediately (preempting T1). T3 tries to acquire the lock that T 1 holds, but\ngets stuck waiting, because T1 still holds it. If T2 starts to r un, it will have\nhigher priority than T1, and thus it will run. T3, which is high er priority\nthan T2, is stuck waiting for T1, which may never run now that T2 i s run-\nning. Isn’t it sad that the mighty T3 can’t run, while lowly T2 cont rols the\nCPU? Having high priority just ain’t what it used to be.\nYou can address the priority inversion problem in a number of ways. In\nthe speciﬁc case where spin locks cause the problem, you can avoid us-\ning spin locks (described more below). More generally, a higher- priority\nthread waiting for a lower-priority thread can temporarily boost t he\nlower thread’s priority, thus enabling it to run and overcoming th e in-\nversion, a technique known as priority inheritance . A last solution is\nsimplest: ensure all threads have the same priority.\nWe do a couple of interesting things in this example. First, we c ombine\nthe old test-and-set idea with an explicit queue of lock waiters to make a\nmore efﬁcient lock. Second, we use a queue to help control who gets th e\nlock next and thus avoid starvation.\nYou might notice how the guard is used (Figure 28.9, page 16), bas i-\ncally as a spin-lock around the ﬂag and queue manipulations the l ock is\nusing. This approach thus doesn’t avoid spin-waiting entirely; a thread\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n18 LOCKS\nmight be interrupted while acquiring or releasing the lock, an d thus cause\nother threads to spin-wait for this one to run again. However, the time\nspent spinning is quite limited (just a few instructions insi de the lock and\nunlock code, instead of the user-deﬁned critical section), and t hus this\napproach may be reasonable.\nSecond, you might notice that in lock() , when a thread can not ac-\nquire the lock (it is already held), we are careful to add oursel ves to a\nqueue (by calling the gettid() function to get the thread ID of the\ncurrent thread), set guard to 0, and yield the CPU. A question f or the\nreader: What would happen if the release of the guard lock came after the\npark() , and not before? Hint: something bad.\nYou might also notice the interesting fact that the ﬂag does not ge t set\nback to 0 when another thread gets woken up. Why is this? Well, it is not\nan error, but rather a necessity! When a thread is woken up, it wi ll be as\nif it is returning from park() ; however, it does not hold the guard at that\npoint in the code and thus cannot even try to set the ﬂag to 1. Thus, we\njust pass the lock directly from the thread releasing the lock to the next\nthread acquiring it; ﬂag is not set to 0 in-between.\nFinally, you might notice the perceived race condition in the solu tion,\njust before the call to park() . With just the wrong timing, a thread will\nbe about to park, assuming that it should sleep until the lock is n o longer\nheld. A switch at that time to another thread (say, a thread hold ing the\nlock) could lead to trouble, for example, if that thread then rele ased the\nlock. The subsequent park by the ﬁrst thread would then sleep for ever\n(potentially), a problem sometimes called the wakeup/waiting race .\nSolaris solves this problem by adding a third system call: setpark() .\nBy calling this routine, a thread can indicate it is about to park. If it then\nhappens to be interrupted and another thread calls unpark bef ore park is\nactually called, the subsequent park returns immediately i nstead of sleep-\ning. The code modiﬁcation, inside of lock() , is quite small:\n1queue_add(m->q, gettid());\n2setpark(); // new code\n3m->guard = 0;\nA different solution could pass the guard into the kernel. In tha t case,\nthe kernel could take precautions to atomically release the lock and de-\nqueue the running thread.\n28.15 Different OS, Different Support\nWe have thus far seen one type of support that an OS can provide in\norder to build a more efﬁcient lock in a thread library. Other OS’s p rovide\nsimilar support; the details vary.\nFor example, Linux provides a futex which is similar to the Solaris in-\nterface but provides more in-kernel functionality. Speciﬁcall y, each futex\nhas associated with it a speciﬁc physical memory location, as wel l as a\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOCKS 19\n1void mutex_lock (int *mutex) {\n2int v;\n3/*Bit 31 was clear, we got the mutex (the fastpath) */\n4if (atomic_bit_test_set (mutex, 31) == 0)\n5return;\n6atomic_increment (mutex);\n7while (1) {\n8 if (atomic_bit_test_set (mutex, 31) == 0) {\n9 atomic_decrement (mutex);\n10 return;\n11 }\n12 /*We have to waitFirst make sure the futex value\n13 we are monitoring is truly negative (locked). */\n14 v =*mutex;\n15 if (v >= 0)\n16 continue;\n17 futex_wait (mutex, v);\n18}\n19}\n20\n21void mutex_unlock (int *mutex) {\n22/*Adding 0x80000000 to counter results in 0 if and\n23 only if there are not other interested threads */\n24if (atomic_add_zero (mutex, 0x80000000))\n25return;\n26\n27/*There are other threads waiting for this mutex,\n28 wake one of them up. */\n29futex_wake (mutex);\n30}\nFigure 28.10: Linux-based Futex Locks\nper-futex in-kernel queue. Callers can use futex calls (des cribed below)\nto sleep and wake as need be.\nSpeciﬁcally, two calls are available. The call to futexwait(address,\nexpected) puts the calling thread to sleep, assuming the value at address\nis equal to expected . If it is notequal, the call returns immediately. The\ncall to the routine futexwake(address) wakes one thread that is wait-\ning on the queue. The usage of these calls in a Linux mutex is shown in\nFigure 28.10 (page 19).\nThis code snippet from lowlevellock.h in the nptl library (part of\nthe gnu libc library) [L09] is interesting for a few reasons. Fi rst, it uses a\nsingle integer to track both whether the lock is held or not (the hi gh bit\nof the integer) and the number of waiters on the lock (all the other b its).\nThus, if the lock is negative, it is held (because the high bit i s set and that\nbit determines the sign of the integer).\nSecond, the code snippet shows how to optimize for the common case,\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n20 LOCKS\nspeciﬁcally when there is no contention for the lock; with only one t hread\nacquiring and releasing a lock, very little work is done (the atom ic bit\ntest-and-set to lock and an atomic add to release the lock).\nSee if you can puzzle through the rest of this “real-world” lock to un -\nderstand how it works. Do it and become a master of Linux locking, or a t\nleast somebody who listens when a book tells you to do something3.\n28.16 Two-Phase Locks\nOne ﬁnal note: the Linux approach has the ﬂavor of an old approach\nthat has been used on and off for years, going at least as far back to Dahm\nLocks in the early 1960’s [M82], and is now referred to as a two-phase\nlock . A two-phase lock realizes that spinning can be useful, partic ularly\nif the lock is about to be released. So in the ﬁrst phase, the lock sp ins for\na while, hoping that it can acquire the lock.\nHowever, if the lock is not acquired during the ﬁrst spin phase, a sec-\nond phase is entered, where the caller is put to sleep, and only w oken up\nwhen the lock becomes free later. The Linux lock above is a form of suc h\na lock, but it only spins once; a generalization of this could spin in a loop\nfor a ﬁxed amount of time before using futex support to sleep.\nTwo-phase locks are yet another instance of a hybrid approach, where\ncombining two good ideas may indeed yield a better one. Of course,\nwhether it does depends strongly on many things, including the h ard-\nware environment, number of threads, and other workload details. As\nalways, making a single general-purpose lock, good for all possibl e use\ncases, is quite a challenge.\n28.17 Summary\nThe above approach shows how real locks are built these days: some\nhardware support (in the form of a more powerful instruction) plus s ome\noperating system support (e.g., in the form of park() andunpark()\nprimitives on Solaris, or futex on Linux). Of course, the details differ, and\nthe exact code to perform such locking is usually highly tuned. C heck out\nthe Solaris or Linux code bases if you want to see more details; they a re\na fascinating read [L09, S09]. Also see David et al.’s excellen t work for a\ncomparison of locking strategies on modern multiprocessors [D+13].\n3Like buy a print copy of OSTEP! Even though the book is available for free online,\nwouldn’t you just love a hard cover for your desk? Or, better yet, te n copies to share with\nfriends and family? And maybe one extra copy to throw at an enemy? (the book isheavy, and\nthus chucking it is surprisingly effective)\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOCKS 21\nReferences\n[D91] “Just Win, Baby: Al Davis and His Raiders” by Glenn Dickey. Har court, 1991. The book\nabout Al Davis and his famous quote. Or, we suppose, the book is more about Al Davi s and the Raiders,\nand not so much the quote. To be clear: we are not recommending this book, we ju st needed a citation.\n[D+13] “Everything You Always Wanted to Know about Synchronization bu t Were Afraid to\nAsk” by Tudor David, Rachid Guerraoui, Vasileios Trigonakis. S OSP ’13, Nemacolin Wood-\nlands Resort, Pennsylvania, November 2013. An excellent paper comparing many different ways to\nbuild locks using hardware primitives. Great to see how many ideas work on m odern hardware.\n[D68] “Cooperating sequential processes” by Edsger W. Dijkstra . 1968. Available online here:\nhttp://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.P DF.One of the early semi-\nnal papers. Discusses how Dijkstra posed the original concurrency probl em, and Dekker’s solution.\n[H93] “MIPS R4000 Microprocessor User’s Manual” by Joe Heinrich. Prentice-Ha ll, June 1993.\nAvailable: http://cag.csail.mit.edu/raw/documents/R4400 Uman book Ed2.pdf. The old MIPS\nuser’s manual. Download it while it still exists.\n[H91] “Wait-free Synchronization” by Maurice Herlihy. ACM TOPLAS, Volume 13: 1, January\n1991. A landmark paper introducing a different approach to building concurre nt data structures. Be-\ncause of the complexity involved, some of these ideas have been slow to gain acc eptance in deployment.\n[L81] “Observations on the Development of an Operating System” by Hu gh Lauer. SOSP ’81,\nPaciﬁc Grove, California, December 1981. A must-read retrospective about the development of the\nPilot OS, an early PC operating system. Fun and full of insights.\n[L09] “glibc 2.9 (include Linux pthreads implementation)” by Many author s.. Available here:\nhttp://ftp.gnu.org/gnu/glibc .In particular, take a look at the nptl subdirectory where you\nwill ﬁnd most of the pthread support in Linux today.\n[M82] “The Architecture of the Burroughs B5000: 20 Years Later and Still Ahead of the Times?”\nby A. Mayer. 1982. Available: www.ajwm.net/amayer/papers/B5000.html .“It (RDLK)\nis an indivisible operation which reads from and writes into a memory location .” RDLK is thus test-\nand-set! Dave Dahm created spin locks (“Buzz Locks”) and a two-phase lock called “Dah m Locks.”\n[M15] “OSSpinLock Is Unsafe” by J. McCall. mjtsai.com/blog/2015/12/16/osspinlock\n-is-unsafe .Calling OSSpinLock on a Mac is unsafe when using threads of different p riorities – you\nmight spin forever! So be careful, Mac fanatics, even your mighty syste m can be less than perfect...\n[MS91] “Algorithms for Scalable Synchronization on Shared-Memory Multip rocessors” by\nJohn M. Mellor-Crummey and M. L. Scott. ACM TOCS, Volume 9, Issue 1, Febr uary 1991.\nAn excellent and thorough survey on different locking algorithms. Howev er, no operating systems\nsupport is used, just fancy hardware instructions.\n[P81] “Myths About the Mutual Exclusion Problem” by G.L. Peterson. Inform ation Processing\nLetters, 12(3), pages 115–116, 1981. Peterson’s algorithm introduced here.\n[R97] “What Really Happened on Mars?” by Glenn E. Reeves. research.microsoft.com/\nen-us/um/people/mbj/Mars Pathfinder/Authoritative Account.html .A descrip-\ntion of priority inversion on Mars Pathﬁnder. Concurrent code correctness m atters, especially in space!\n[S05] “Guide to porting from Solaris to Linux on x86” by Ajay Sood, April 29, 2005. Available:\nhttp://www.ibm.com/developerworks/linux/library/l-s olar/ .\n[S09] “OpenSolaris Thread Library” by Sun.. Code: src.opensolaris.org/source/xref/\nonnv/onnv-gate/usr/src/lib/libc/port/threads/synch. c.Pretty interesting, al-\nthough who knows what will happen now that Oracle owns Sun. Thanks to Mike Swift for the pointer.\n[W09] “Load-Link, Store-Conditional” by Many authors.. en.wikipedia.org/wiki/Load-\nLink/Store-Conditional .Can you believe we referenced Wikipedia? But, we found the infor-\nmation there and it felt wrong not to. Further, it was useful, listing the ins tructions for the different ar-\nchitectures: ldll/stlcandldql/stqc(Alpha),lwarx/stwcx (PowerPC), ll/sc (MIPS),\nandldrex/strex (ARM). Actually Wikipedia is pretty amazing, so don’t be so harsh, OK?\n[WG00] “The SPARC Architecture Manual: Version 9” by D. Weaver, T. Ger mond. SPARC In-\nternational, 2000. http://www.sparc.org/standards/SPARCV9.pdf .Seedevelopers.\nsun.com/solaris/articles/atomic sparc/ for more on atomics.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n22 LOCKS\nHomework (Simulation)\nThis program, x86.py , allows you to see how different thread inter-\nleavings either cause or avoid race conditions. See the README for d e-\ntails on how the program works and answer the questions below.\nQuestions\n1. Examine flag.s . This code “implements” locking with a single memory\nﬂag. Can you understand the assembly?\n2. When you run with the defaults, does flag.s work? Use the -Mand-R\nﬂags to trace variables and registers (and turn on -cto see their values).\nCan you predict what value will end up in flag ?\n3. Change the value of the register %bx with the-aﬂag (e.g.,-a bx=2,bx=2\nif you are running just two threads). What does the code do? How d oes it\nchange your answer for the question above?\n4. Setbxto a high value for each thread, and then use the -iﬂag to generate\ndifferent interrupt frequencies; what values lead to a bad outco mes? Which\nlead to good outcomes?\n5. Now let’s look at the program test-and-set.s . First, try to understand\nthe code, which uses the xchg instruction to build a simple locking primi-\ntive. How is the lock acquire written? How about lock release?\n6. Now run the code, changing the value of the interrupt interval (-i) again,\nand making sure to loop for a number of times. Does the code always wo rk\nas expected? Does it sometimes lead to an inefﬁcient use of the CPU? How\ncould you quantify that?\n7. Use the -Pﬂag to generate speciﬁc tests of the locking code. For example,\nrun a schedule that grabs the lock in the ﬁrst thread, but then tri es to acquire\nit in the second. Does the right thing happen? What else should you test?\n8. Now let’s look at the code in peterson.s , which implements Peterson’s\nalgorithm (mentioned in a sidebar in the text). Study the code an d see if\nyou can make sense of it.\n9. Now run the code with different values of -i. What kinds of different be-\nhavior do you see? Make sure to set the thread IDs appropriatel y (using-a\nbx=0,bx=1 for example) as the code assumes it.\n10. Can you control the scheduling (with the -Pﬂag) to “prove” that the code\nworks? What are the different cases you should show hold? Thin k about\nmutual exclusion and deadlock avoidance.\n11. Now study the code for the ticket lock in ticket.s . Does it match the code\nin the chapter? Then run with the following ﬂags: -a bx=1000,bx=1000\n(causing each thread to loop through the critical section 1000 times). Watch\nwhat happens; do the threads spend much time spin-waiting for th e lock?\n12. How does the code behave as you add more threads?\n13. Now examine yield.s , in which a yield instruction enables one thread\nto yield control of the CPU (realistically, this would be an OS primitive, but\nfor the simplicity, we assume an instruction does the task). Find a scenario\nwheretest-and-set.s wastes cycles spinning, but yield.s does not.\nHow many instructions are saved? In what scenarios do these sav ings arise?\n14. Finally, examine test-and-test-and-set.s . What does this lock do?\nWhat kind of savings does it introduce as compared to test-and-set.s ?\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG",55344
34-29. Locked Data Structures.pdf,34-29. Locked Data Structures,"29\nLock-based Concurrent Data Structures\nBefore moving beyond locks, we’ll ﬁrst describe how to use locks in som e\ncommon data structures. Adding locks to a data structure to make it us-\nable by threads makes the structure thread safe . Of course, exactly how\nsuch locks are added determines both the correctness and perform ance of\nthe data structure. And thus, our challenge:\nCRUX: HOWTOADDLOCKS TODATA STRUCTURES\nWhen given a particular data structure, how should we add locks t o\nit, in order to make it work correctly? Further, how do we add locks s uch\nthat the data structure yields high performance, enabling ma ny threads\nto access the structure at once, i.e., concurrently ?\nOf course, we will be hard pressed to cover all data structures or all\nmethods for adding concurrency, as this is a topic that has been st udied\nfor years, with (literally) thousands of research papers publi shed about\nit. Thus, we hope to provide a sufﬁcient introduction to the type of think-\ning required, and refer you to some good sources of material for furth er\ninquiry on your own. We found Moir and Shavit’s survey to be a great\nsource of information [MS04].\n29.1 Concurrent Counters\nOne of the simplest data structures is a counter. It is a structu re that\nis commonly used and has a simple interface. We deﬁne a simple non -\nconcurrent counter in Figure 29.1.\nSimple But Not Scalable\nAs you can see, the non-synchronized counter is a trivial data str ucture,\nrequiring a tiny amount of code to implement. We now have our next\nchallenge: how can we make this code thread safe ? Figure 29.2 shows\nhow we do so.\n1\n2 LOCK -BASED CONCURRENT DATA STRUCTURES\n1typedef struct __counter_t {\n2int value;\n3} counter_t;\n4\n5void init(counter_t *c) {\n6c->value = 0;\n7}\n8\n9void increment(counter_t *c) {\n10 c->value++;\n11}\n12\n13void decrement(counter_t *c) {\n14 c->value--;\n15}\n16\n17int get(counter_t *c) {\n18 return c->value;\n19}\nFigure 29.1: A Counter Without Locks\n1typedef struct __counter_t {\n2int value;\n3pthread_mutex_t lock;\n4} counter_t;\n5\n6void init(counter_t *c) {\n7c->value = 0;\n8Pthread_mutex_init(&c->lock, NULL);\n9}\n10\n11void increment(counter_t *c) {\n12 Pthread_mutex_lock(&c->lock);\n13 c->value++;\n14 Pthread_mutex_unlock(&c->lock);\n15}\n16\n17void decrement(counter_t *c) {\n18 Pthread_mutex_lock(&c->lock);\n19 c->value--;\n20 Pthread_mutex_unlock(&c->lock);\n21}\n22\n23int get(counter_t *c) {\n24 Pthread_mutex_lock(&c->lock);\n25 int rc = c->value;\n26 Pthread_mutex_unlock(&c->lock);\n27 return rc;\n28}\nFigure 29.2: A Counter With Locks\nThis concurrent counter is simple and works correctly. In fact, i t fol-\nlows a design pattern common to the simplest and most basic concurr ent\ndata structures: it simply adds a single lock, which is acquir ed when call-\ning a routine that manipulates the data structure, and is rele ased when\nreturning from the call. In this manner, it is similar to a data structure\nbuilt with monitors [BH73], where locks are acquired and released auto-\nmatically as you call and return from object methods.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCK -BASED CONCURRENT DATA STRUCTURES 3\n1 2 3 4051015\nThreadsTime (seconds)Precise\nApproximate\nFigure 29.3: Performance of Traditional vs. Approximate Counters\nAt this point, you have a working concurrent data structure. The p rob-\nlem you might have is performance. If your data structure is too sl ow,\nyou’ll have to do more than just add a single lock; such optimization s, if\nneeded, are thus the topic of the rest of the chapter. Note that if t he data\nstructure is nottoo slow, you are done! No need to do something fancy if\nsomething simple will work.\nTo understand the performance costs of the simple approach, we ru n a\nbenchmark in which each thread updates a single shared counte r a ﬁxed\nnumber of times; we then vary the number of threads. Figure 29.3 shows\nthe total time taken, with one to four threads active; each threa d updates\nthe counter one million times. This experiment was run upon an iMa c\nwith four Intel 2.7 GHz i5 CPUs; with more CPUs active, we hope to g et\nmore total work done per unit time.\nFrom the top line in the ﬁgure (labeled ’Precise’), you can see that the\nperformance of the synchronized counter scales poorly. Whereas a s ingle\nthread can complete the million counter updates in a tiny amount of time\n(roughly 0.03 seconds), having two threads each update the coun ter one\nmillion times concurrently leads to a massive slowdown (taking ov er 5\nseconds!). It only gets worse with more threads.\nIdeally, you’d like to see the threads complete just as quickly on mul-\ntiple processors as the single thread does on one. Achieving this e nd is\ncalled perfect scaling ; even though more work is done, it is done in par-\nallel, and hence the time taken to complete the task is not incre ased.\nScalable Counting\nAmazingly, researchers have studied how to build more scalabl e coun-\nters for years [MS04]. Even more amazing is the fact that scalabl e coun-\nters matter, as recent work in operating system performance ana lysis has\nshown [B+10]; without scalable counting, some workloads running on\nLinux suffer from serious scalability problems on multicore mach ines.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 LOCK -BASED CONCURRENT DATA STRUCTURES\nTime L1L2L3L4G\n0 0 0 0 0 0\n1 0 0 1 1 0\n2 1 0 2 1 0\n3 2 0 3 1 0\n4 3 0 3 2 0\n5 4 1 3 3 0\n6 5→0 1 3 4 5 (from L1)\n7 0 2 4 5 →0 10 (from L4)\nFigure 29.4: Tracing the Approximate Counters\nMany techniques have been developed to attack this problem. We ’ll\ndescribe one approach known as an approximate counter [C06].\nThe approximate counter works by representing a single logical c ounter\nvia numerous local physical counters, one per CPU core, as well as a single\nglobal counter. Speciﬁcally, on a machine with four CPUs, there are four\nlocal counters and one global one. In addition to these counters, the re are\nalso locks: one for each local counter1, and one for the global counter.\nThe basic idea of approximate counting is as follows. When a thread\nrunning on a given core wishes to increment the counter, it incre ments its\nlocal counter; access to this local counter is synchronized via th e corre-\nsponding local lock. Because each CPU has its own local counter, thr eads\nacross CPUs can update local counters without contention, and thus up-\ndates to the counter are scalable.\nHowever, to keep the global counter up to date (in case a thread wi shes\nto read its value), the local values are periodically transfer red to the global\ncounter, by acquiring the global lock and incrementing it by the local\ncounter’s value; the local counter is then reset to zero.\nHow often this local-to-global transfer occurs is determined by a t hresh-\noldS. The smaller Sis, the more the counter behaves like the non-scalable\ncounter above; the bigger Sis, the more scalable the counter, but the fur-\nther off the global value might be from the actual count. One could s im-\nply acquire all the local locks and the global lock (in a speciﬁed or der, to\navoid deadlock) to get an exact value, but that is not scalable.\nTo make this clear, let’s look at an example (Figure 29.4). In thi s ex-\nample, the threshold Sis set to 5, and there are threads on each of four\nCPUs updating their local counters L1...L4. The global counter value\n(G) is also shown in the trace, with time increasing downward. At e ach\ntime step, a local counter may be incremented; if the local value reaches\nthe threshold S, the local value is transferred to the global counter and\nthe local counter is reset.\nThe lower line in Figure 29.3 (labeled ’Approximate’, on page 3) sh ows\nthe performance of approximate counters with a threshold Sof1024 . Per-\nformance is excellent; the time taken to update the counter four million\ntimes on four processors is hardly higher than the time taken to up date it\none million times on one processor.\n1We need the local locks because we assume there may be more than one threa d on each\ncore. If, instead, only one thread ran on each core, no local lock would be ne eded.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCK -BASED CONCURRENT DATA STRUCTURES 5\n1typedef struct __counter_t {\n2int global; // global count\n3pthread_mutex_t glock; // global lock\n4int local[NUMCPUS]; // local count (per cpu)\n5pthread_mutex_t llock[NUMCPUS]; // ... and locks\n6int threshold; // update frequency\n7} counter_t;\n8\n9// init: record threshold, init locks, init values\n10// of all local counts and global count\n11void init(counter_t *c, int threshold) {\n12 c->threshold = threshold;\n13 c->global = 0;\n14 pthread_mutex_init(&c->glock, NULL);\n15 int i;\n16 for (i = 0; i < NUMCPUS; i++) {\n17 c->local[i] = 0;\n18 pthread_mutex_init(&c->llock[i], NULL);\n19 }\n20}\n21\n22// update: usually, just grab local lock and update local amo unt\n23// once local count has risen by ’threshold’, grab global\n24// lock and transfer local values to it\n25void update(counter_t *c, int threadID, int amt) {\n26 int cpu = threadID % NUMCPUS;\n27 pthread_mutex_lock(&c->llock[cpu]);\n28 c->local[cpu] += amt; // assumes amt > 0\n29 if (c->local[cpu] >= c->threshold) { // transfer to global\n30 pthread_mutex_lock(&c->glock);\n31 c->global += c->local[cpu];\n32 pthread_mutex_unlock(&c->glock);\n33 c->local[cpu] = 0;\n34 }\n35 pthread_mutex_unlock(&c->llock[cpu]);\n36}\n37\n38// get: just return global amount (which may not be perfect)\n39int get(counter_t *c) {\n40 pthread_mutex_lock(&c->glock);\n41 int val = c->global;\n42 pthread_mutex_unlock(&c->glock);\n43 return val; // only approximate!\n44}\nFigure 29.5: Approximate Counter Implementation\nFigure 29.6 shows the importance of the threshold value S, with four\nthreads each incrementing the counter 1 million times on four CPU s. IfS\nis low, performance is poor (but the global count is always quite acc urate);\nifSis high, performance is excellent, but the global count lags (by at most\nthe number of CPUs multiplied by S). This accuracy/performance trade-\noff is what approximate counters enable.\nA rough version of an approximate counter is found in Figure 29.5.\nRead it, or better yet, run it yourself in some experiments to bet ter under-\nstand how it works.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 LOCK -BASED CONCURRENT DATA STRUCTURES\n1 2 4 8 16 32 64 128 256 1024 512051015\nApproximation Factor (S)Time (seconds)\nFigure 29.6: Scaling Approximate Counters\n29.2 Concurrent Linked Lists\nWe next examine a more complicated structure, the linked list. Let’s\nstart with a basic approach once again. For simplicity, we’ll omit some of\nthe obvious routines that such a list would have and just focus on conc ur-\nrent insert; we’ll leave it to the reader to think about lookup, de lete, and\nso forth. Figure 29.7 shows the code for this rudimentary data str ucture.\nAs you can see in the code, the code simply acquires a lock in the ins ert\nroutine upon entry, and releases it upon exit. One small tricky i ssue arises\nifmalloc() happens to fail (a rare case); in this case, the code must also\nrelease the lock before failing the insert.\nThis kind of exceptional control ﬂow has been shown to be quite error\nprone; a recent study of Linux kernel patches found that a huge fr action of\nbugs (nearly 40%) are found on such rarely-taken code paths (ind eed, this\nobservation sparked some of our own research, in which we removed all\nmemory-failing paths from a Linux ﬁle system, resulting in a mor e robust\nsystem [S+11]).\nThus, a challenge: can we rewrite the insert and lookup routines to re-\nmain correct under concurrent insert but avoid the case where th e failure\npath also requires us to add the call to unlock?\nThe answer, in this case, is yes. Speciﬁcally, we can rearrang e the code\na bit so that the lock and release only surround the actual critic al section\nin the insert code, and that a common exit path is used in the lookup c ode.\nThe former works because part of the lookup actually need not be locke d;\nassuming that malloc() itself is thread-safe, each thread can call into it\nwithout worry of race conditions or other concurrency bugs. Only when\nupdating the shared list does a lock need to be held. See Figure 29 .8 for\nthe details of these modiﬁcations.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCK -BASED CONCURRENT DATA STRUCTURES 7\n1// basic node structure\n2typedef struct __node_t {\n3int key;\n4struct __node_t *next;\n5} node_t;\n6\n7// basic list structure (one used per list)\n8typedef struct __list_t {\n9node_t *head;\n10 pthread_mutex_t lock;\n11} list_t;\n12\n13void List_Init(list_t *L) {\n14 L->head = NULL;\n15 pthread_mutex_init(&L->lock, NULL);\n16}\n17\n18int List_Insert(list_t *L, int key) {\n19 pthread_mutex_lock(&L->lock);\n20 node_t*new = malloc(sizeof(node_t));\n21 if (new == NULL) {\n22 perror(""malloc"");\n23 pthread_mutex_unlock(&L->lock);\n24 return -1; // fail\n25 }\n26 new->key = key;\n27 new->next = L->head;\n28 L->head = new;\n29 pthread_mutex_unlock(&L->lock);\n30 return 0; // success\n31}\n32\n33int List_Lookup(list_t *L, int key) {\n34 pthread_mutex_lock(&L->lock);\n35 node_t*curr = L->head;\n36 while (curr) {\n37 if (curr->key == key) {\n38 pthread_mutex_unlock(&L->lock);\n39 return 0; // success\n40 }\n41 curr = curr->next;\n42 }\n43 pthread_mutex_unlock(&L->lock);\n44 return -1; // failure\n45}\nFigure 29.7: Concurrent Linked List\nAs for the lookup routine, it is a simple code transformation to jump\nout of the main search loop to a single return path. Doing so again re -\nduces the number of lock acquire/release points in the code, and t hus\ndecreases the chances of accidentally introducing bugs (such as forget-\nting to unlock before returning) into the code.\nScaling Linked Lists\nThough we again have a basic concurrent linked list, once again w e\nare in a situation where it does not scale particularly well. One technique\nthat researchers have explored to enable more concurrency with in a list is\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 LOCK -BASED CONCURRENT DATA STRUCTURES\n1void List_Init(list_t *L) {\n2L->head = NULL;\n3pthread_mutex_init(&L->lock, NULL);\n4}\n5\n6void List_Insert(list_t *L, int key) {\n7// synchronization not needed\n8node_t*new = malloc(sizeof(node_t));\n9if (new == NULL) {\n10 perror(""malloc"");\n11 return;\n12 }\n13 new->key = key;\n14\n15 // just lock critical section\n16 pthread_mutex_lock(&L->lock);\n17 new->next = L->head;\n18 L->head = new;\n19 pthread_mutex_unlock(&L->lock);\n20}\n21\n22int List_Lookup(list_t *L, int key) {\n23 int rv = -1;\n24 pthread_mutex_lock(&L->lock);\n25 node_t*curr = L->head;\n26 while (curr) {\n27 if (curr->key == key) {\n28 rv = 0;\n29 break;\n30 }\n31 curr = curr->next;\n32 }\n33 pthread_mutex_unlock(&L->lock);\n34 return rv; // now both success and failure\n35}\nFigure 29.8: Concurrent Linked List: Rewritten\nsomething called hand-over-hand locking (a.k.a. lock coupling ) [MS04].\nThe idea is pretty simple. Instead of having a single lock for the entire\nlist, you instead add a lock per node of the list. When traversing t he\nlist, the code ﬁrst grabs the next node’s lock and then releases th e current\nnode’s lock (which inspires the name hand-over-hand).\nConceptually, a hand-over-hand linked list makes some sense; i t en-\nables a high degree of concurrency in list operations. However, in prac-\ntice, it is hard to make such a structure faster than the simpl e single lock\napproach, as the overheads of acquiring and releasing locks for ea ch node\nof a list traversal is prohibitive. Even with very large lists, and a large\nnumber of threads, the concurrency enabled by allowing multipl e on-\ngoing traversals is unlikely to be faster than simply grabbin g a single\nlock, performing an operation, and releasing it. Perhaps some kin d of hy-\nbrid (where you grab a new lock every so many nodes) would be worth\ninvestigating.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCK -BASED CONCURRENT DATA STRUCTURES 9\nTIP: M ORE CONCURRENCY ISN’TNECESSARILY FASTER\nIf the scheme you design adds a lot of overhead (for example, by acqu ir-\ning and releasing locks frequently, instead of once), the fact t hat it is more\nconcurrent may not be important. Simple schemes tend to work well,\nespecially if they use costly routines rarely. Adding more locks and com-\nplexity can be your downfall. All of that said, there is one way to r eally\nknow: build both alternatives (simple but less concurrent, and complex\nbut more concurrent) and measure how they do. In the end, you can’t\ncheat on performance; your idea is either faster, or it isn’t.\nTIP: BEWARY OFLOCKS ANDCONTROL FLOW\nA general design tip, which is useful in concurrent code as well as\nelsewhere, is to be wary of control ﬂow changes that lead to functi on re-\nturns, exits, or other similar error conditions that halt the exec ution of\na function. Because many functions will begin by acquiring a loc k, al-\nlocating some memory, or doing other similar stateful operations, wh en\nerrors arise, the code has to undo all of the state before returnin g, which\nis error-prone. Thus, it is best to structure code to minimize th is pattern.\n29.3 Concurrent Queues\nAs you know by now, there is always a standard method to make a\nconcurrent data structure: add a big lock. For a queue, we’ll skip that\napproach, assuming you can ﬁgure it out.\nInstead, we’ll take a look at a slightly more concurrent queue desi gned\nby Michael and Scott [MS98]. The data structures and code used for t his\nqueue are found in Figure 29.9 on the following page.\nIf you study this code carefully, you’ll notice that there are two l ocks,\none for the head of the queue, and one for the tail. The goal of these two\nlocks is to enable concurrency of enqueue and dequeue operations. In\nthe common case, the enqueue routine will only access the tail lock , and\ndequeue only the head lock.\nOne trick used by Michael and Scott is to add a dummy node (allo-\ncated in the queue initialization code); this dummy enables th e separa-\ntion of head and tail operations. Study the code, or better yet, type i t in,\nrun it, and measure it, to understand how it works deeply.\nQueues are commonly used in multi-threaded applications. Howev er,\nthe type of queue used here (with just locks) often does not complete ly\nmeet the needs of such programs. A more fully developed bounded\nqueue, that enables a thread to wait if the queue is either emp ty or overly\nfull, is the subject of our intense study in the next chapter on con dition\nvariables. Watch for it!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 L OCK -BASED CONCURRENT DATA STRUCTURES\n1typedef struct __node_t {\n2int value;\n3struct __node_t *next;\n4} node_t;\n5\n6typedef struct __queue_t {\n7node_t *head;\n8node_t *tail;\n9pthread_mutex_t headLock;\n10 pthread_mutex_t tailLock;\n11} queue_t;\n12\n13void Queue_Init(queue_t *q) {\n14 node_t*tmp = malloc(sizeof(node_t));\n15 tmp->next = NULL;\n16 q->head = q->tail = tmp;\n17 pthread_mutex_init(&q->headLock, NULL);\n18 pthread_mutex_init(&q->tailLock, NULL);\n19}\n20\n21void Queue_Enqueue(queue_t *q, int value) {\n22 node_t*tmp = malloc(sizeof(node_t));\n23 assert(tmp != NULL);\n24 tmp->value = value;\n25 tmp->next = NULL;\n26\n27 pthread_mutex_lock(&q->tailLock);\n28 q->tail->next = tmp;\n29 q->tail = tmp;\n30 pthread_mutex_unlock(&q->tailLock);\n31}\n32\n33int Queue_Dequeue(queue_t *q, int*value) {\n34 pthread_mutex_lock(&q->headLock);\n35 node_t*tmp = q->head;\n36 node_t*newHead = tmp->next;\n37 if (newHead == NULL) {\n38 pthread_mutex_unlock(&q->headLock);\n39 return -1; // queue was empty\n40 }\n41 *value = newHead->value;\n42 q->head = newHead;\n43 pthread_mutex_unlock(&q->headLock);\n44 free(tmp);\n45 return 0;\n46}\nFigure 29.9: Michael and Scott Concurrent Queue\n29.4 Concurrent Hash Table\nWe end our discussion with a simple and widely applicable concur rent\ndata structure, the hash table. We’ll focus on a simple hash tabl e that does\nnot resize; a little more work is required to handle resizing, wh ich we\nleave as an exercise for the reader (sorry!).\nThis concurrent hash table is straightforward, is built using the con-\ncurrent lists we developed earlier, and works incredibly well . The reason\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCK -BASED CONCURRENT DATA STRUCTURES 11\n1#define BUCKETS (101)\n2\n3typedef struct __hash_t {\n4list_t lists[BUCKETS];\n5} hash_t;\n6\n7void Hash_Init(hash_t *H) {\n8int i;\n9for (i = 0; i < BUCKETS; i++) {\n10 List_Init(&H->lists[i]);\n11 }\n12}\n13\n14int Hash_Insert(hash_t *H, int key) {\n15 int bucket = key % BUCKETS;\n16 return List_Insert(&H->lists[bucket], key);\n17}\n18\n19int Hash_Lookup(hash_t *H, int key) {\n20 int bucket = key % BUCKETS;\n21 return List_Lookup(&H->lists[bucket], key);\n22}\nFigure 29.10: A Concurrent Hash Table\nfor its good performance is that instead of having a single lock for th e en-\ntire structure, it uses a lock per hash bucket (each of which is r epresented\nby a list). Doing so enables many concurrent operations to take pl ace.\nFigure 29.11 shows the performance of the hash table under concur -\nrent updates (from 10,000 to 50,000 concurrent updates from eac h of four\nthreads, on the same iMac with four CPUs). Also shown, for the sake\nof comparison, is the performance of a linked list (with a single loc k).\nAs you can see from the graph, this simple concurrent hash table s cales\nmagniﬁcently; the linked list, in contrast, does not.\n0 10 20 30 40051015\nInserts (Thousands)Time (seconds)Simple Concurrent List\nConcurrent Hash Table\nFigure 29.11: Scaling Hash Tables\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 L OCK -BASED CONCURRENT DATA STRUCTURES\nTIP: AVOID PREMATURE OPTIMIZATION (KNUTH ’SLAW)\nWhen building a concurrent data structure, start with the most basic ap-\nproach, which is to add a single big lock to provide synchronized a ccess.\nBy doing so, you are likely to build a correct lock; if you then ﬁnd that it\nsuffers from performance problems, you can reﬁne it, thus only mak ing\nit fast if need be. As Knuth famously stated, “Premature optimization is\nthe root of all evil.”\nMany operating systems utilized a single lock when ﬁrst transi tioning\nto multiprocessors, including Sun OS and Linux. In the latter, t his lock\neven had a name, the big kernel lock (BKL ). For many years, this sim-\nple approach was a good one, but when multi-CPU systems became the\nnorm, only allowing a single active thread in the kernel at a time became\na performance bottleneck. Thus, it was ﬁnally time to add the opt imiza-\ntion of improved concurrency to these systems. Within Linux, the more\nstraightforward approach was taken: replace one lock with many. Within\nSun, a more radical decision was made: build a brand new operating sys-\ntem, known as Solaris, that incorporates concurrency more fundamen -\ntally from day one. Read the Linux and Solaris kernel books for more\ninformation about these fascinating systems [BC05, MM00].\n29.5 Summary\nWe have introduced a sampling of concurrent data structures, fr om\ncounters, to lists and queues, and ﬁnally to the ubiquitous and heavily-\nused hash table. We have learned a few important lessons along th e way:\nto be careful with acquisition and release of locks around control ﬂ ow\nchanges; that enabling more concurrency does not necessarily in crease\nperformance; that performance problems should only be remedied on ce\nthey exist. This last point, of avoiding premature optimization , is cen-\ntral to any performance-minded developer; there is no value in making\nsomething faster if doing so will not improve the overall performan ce of\nthe application.\nOf course, we have just scratched the surface of high performanc e\nstructures. See Moir and Shavit’s excellent survey for more informa tion,\nas well as links to other sources [MS04]. In particular, you might be inter-\nested in other structures (such as B-trees); for this knowledge , a database\nclass is your best bet. You also might be interested in techniqu es that don’t\nuse traditional locks at all; such non-blocking data structures are some-\nthing we’ll get a taste of in the chapter on common concurrency bugs,\nbut frankly this topic is an entire area of knowledge requiring m ore study\nthan is possible in this humble book. Find out more on your own if you\nare interested (as always!).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCK -BASED CONCURRENT DATA STRUCTURES 13\nReferences\n[B+10] “An Analysis of Linux Scalability to Many Cores” by Silas Boy d-Wickizer, Austin T.\nClements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek, Robert Mo rris, Nickolai Zel-\ndovich . OSDI ’10, Vancouver, Canada, October 2010. A great study of how Linux performs on\nmulticore machines, as well as some simple solutions. Includes a neat sloppy counter to solve one form\nof the scalable counting problem.\n[BH73] “Operating System Principles” by Per Brinch Hansen. Prentice-Hall, 1 973. Available:\nhttp://portal.acm.org/citation.cfm?id=540365 .One of the ﬁrst books on operating\nsystems; certainly ahead of its time. Introduced monitors as a concurrency pri mitive.\n[BC05] “Understanding the Linux Kernel (Third Edition)” by Daniel P . Bovet and Marco Cesati.\nO’Reilly Media, November 2005. The classic book on the Linux kernel. You should read it.\n[C06] “The Search For Fast, Scalable Counters” by Jonathan Corbet. Feb ruary 1, 2006. Avail-\nable:https://lwn.net/Articles/170003 .LWN has many wonderful articles about the latest\nin Linux This article is a short description of scalable approximate countin g; read it, and others, to learn\nmore about the latest in Linux.\n[L+13] “A Study of Linux File System Evolution” by Lanyue Lu, Andre a C. Arpaci-Dusseau,\nRemzi H. Arpaci-Dusseau, Shan Lu. FAST ’13, San Jose, CA, Februar y 2013. Our paper that\nstudies every patch to Linux ﬁle systems over nearly a decade. Lots of fun ﬁ ndings in there; read it to\nsee! The work was painful to do though; the poor graduate student, Lanyue Lu, had to look through\nevery single patch by hand in order to understand what they did.\n[MS98] “Nonblocking Algorithms and Preemption-safe Locking on by Multipro grammed Shared-\nmemory Multiprocessors. ” M. Michael, M. Scott. Journal of Parallel and Di stributed Com-\nputing, Vol. 51, No. 1, 1998 Professor Scott and his students have been at the forefront of concurrent\nalgorithms and data structures for many years; check out his web page, numer ous papers, or books to\nﬁnd out more.\n[MS04] “Concurrent Data Structures” by Mark Moir and Nir Shavit. In Handb ook of Data\nStructures and Applications (Editors D. Metha and S.Sahni). Chapman and Ha ll/CRC Press,\n2004. Available: www.cs.tau.ac.il/˜shanir/concurrent-data-structures .pdf .\nA short but relatively comprehensive reference on concurrent data str uctures. Though it is missing\nsome of the latest works in the area (due to its age), it remains an incredibly use ful reference.\n[MM00] “Solaris Internals: Core Kernel Architecture” by Jim Mauro and Richa rd McDougall.\nPrentice Hall, October 2000. The Solaris book. You should also read this, if you want to learn about\nsomething other than Linux.\n[S+11] “Making the Common Case the Only Case with Anticipatory Memory Al location” by\nSwaminathan Sundararaman, Yupu Zhang, Sriram Subramanian, Andrea C. Arpaci-Dusseau,\nRemzi H. Arpaci-Dusseau . FAST ’11, San Jose, CA, February 2011. Our work on removing\npossibly-failing allocation calls from kernel code paths. By allocating all potenti ally needed memory\nbefore doing any work, we avoid failure deep down in the storage stack.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 L OCK -BASED CONCURRENT DATA STRUCTURES\nHomework (Code)\nIn this homework, you’ll gain some experience with writing concur-\nrent code and measuring its performance. Learning to build code that\nperforms well is a critical skill and thus gaining a little exp erience here\nwith it is quite worthwhile.\nQuestions\n1. We’ll start by redoing the measurements within this chapter. Use the call\ngettimeofday() to measure time within your program. How accurate is\nthis timer? What is the smallest interval it can measure? Gain con ﬁdence\nin its workings, as we will need it in all subsequent questions. Y ou can also\nlook into other timers, such as the cycle counter available on x86 via the\nrdtsc instruction.\n2. Now, build a simple concurrent counter and measure how long it tak es to\nincrement the counter many times as the number of threads increases . How\nmany CPUs are available on the system you are using? Does this numbe r\nimpact your measurements at all?\n3. Next, build a version of the sloppy counter. Once again, measure its per-\nformance as the number of threads varies, as well as the threshol d. Do the\nnumbers match what you see in the chapter?\n4. Build a version of a linked list that uses hand-over-hand loc king [MS04], as\ncited in the chapter. You should read the paper ﬁrst to understa nd how it\nworks, and then implement it. Measure its performance. When does a hand-\nover-hand list work better than a standard list as shown in th e chapter?\n5. Pick your favorite interesting data structure, such as a B-tre e or other slightly\nmore interested structure. Implement it, and start with a simple lo cking\nstrategy such as a single lock. Measure its performance as the numb er of\nconcurrent threads increases.\n6. Finally, think of a more interesting locking strategy for t his favorite data\nstructure of yours. Implement it, and measure its performance. How do es\nit compare to the straightforward locking approach?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",30009
35-30. Condition Variables.pdf,35-30. Condition Variables,"30\nCondition Variables\nThus far we have developed the notion of a lock and seen how one can be\nproperly built with the right combination of hardware and OS supp ort.\nUnfortunately, locks are not the only primitives that are needed to build\nconcurrent programs.\nIn particular, there are many cases where a thread wishes to c heck\nwhether a condition is true before continuing its execution. For example,\na parent thread might wish to check whether a child thread has completed\nbefore continuing (this is often called a join() ); how should such a wait\nbe implemented? Let’s look at Figure 30.1.\n1void*child(void *arg) {\n2printf(""child\n"");\n3// XXX how to indicate we are done?\n4return NULL;\n5}\n6\n7int main(int argc, char *argv[]) {\n8printf(""parent: begin\n"");\n9pthread_t c;\n10 Pthread_create(&c, NULL, child, NULL); // create child\n11 // XXX how to wait for child?\n12 printf(""parent: end\n"");\n13 return 0;\n14}\nFigure 30.1: A Parent Waiting For Its Child\nWhat we would like to see here is the following output:\nparent: begin\nchild\nparent: end\nWe could try using a shared variable, as you see in Figure 30.2. T his\nsolution will generally work, but it is hugely inefﬁcient as the parent spins\nand wastes CPU time. What we would like here instead is some way t o\nput the parent to sleep until the condition we are waiting for (e. g., the\nchild is done executing) comes true.\n1\n2 CONDITION VARIABLES\n1volatile int done = 0;\n2\n3void*child(void *arg) {\n4printf(""child\n"");\n5done = 1;\n6return NULL;\n7}\n8\n9int main(int argc, char *argv[]) {\n10 printf(""parent: begin\n"");\n11 pthread_t c;\n12 Pthread_create(&c, NULL, child, NULL); // create child\n13 while (done == 0)\n14 ; // spin\n15 printf(""parent: end\n"");\n16 return 0;\n17}\nFigure 30.2: Parent Waiting For Child: Spin-based Approach\nTHECRUX: HOWTOWAITFORA C ONDITION\nIn multi-threaded programs, it is often useful for a thread to wa it for\nsome condition to become true before proceeding. The simple approac h,\nof just spinning until the condition becomes true, is grossly inef ﬁcient\nand wastes CPU cycles, and in some cases, can be incorrect. Thus , how\nshould a thread wait for a condition?\n30.1 Deﬁnition and Routines\nTo wait for a condition to become true, a thread can make use of what\nis known as a condition variable . A condition variable is an explicit\nqueue that threads can put themselves on when some state of execu tion\n(i.e., some condition ) is not as desired (by waiting on the condition);\nsome other thread, when it changes said state, can then wake one ( or\nmore) of those waiting threads and thus allow them to continue (by sig-\nnaling on the condition). The idea goes back to Dijkstra’s use of “private\nsemaphores” [D68]; a similar idea was later named a “condition v ariable”\nby Hoare in his work on monitors [H74].\nTo declare such a condition variable, one simply writes somethin g\nlike this:pthread condt c; , which declares cas a condition variable\n(note: proper initialization is also required). A condition vari able has two\noperations associated with it: wait() andsignal() . Thewait() call\nis executed when a thread wishes to put itself to sleep; the signal() call\nis executed when a thread has changed something in the program a nd\nthus wants to wake a sleeping thread waiting on this condition. Sp eciﬁ-\ncally, the POSIX calls look like this:\npthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);\npthread_cond_signal(pthread_cond_t *c);\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONDITION VARIABLES 3\n1int done = 0;\n2pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n3pthread_cond_t c = PTHREAD_COND_INITIALIZER;\n4\n5void thr_exit() {\n6Pthread_mutex_lock(&m);\n7done = 1;\n8Pthread_cond_signal(&c);\n9Pthread_mutex_unlock(&m);\n10}\n11\n12void*child(void *arg) {\n13 printf(""child\n"");\n14 thr_exit();\n15 return NULL;\n16}\n17\n18void thr_join() {\n19 Pthread_mutex_lock(&m);\n20 while (done == 0)\n21 Pthread_cond_wait(&c, &m);\n22 Pthread_mutex_unlock(&m);\n23}\n24\n25int main(int argc, char *argv[]) {\n26 printf(""parent: begin\n"");\n27 pthread_t p;\n28 Pthread_create(&p, NULL, child, NULL);\n29 thr_join();\n30 printf(""parent: end\n"");\n31 return 0;\n32}\nFigure 30.3: Parent Waiting For Child: Use A Condition Variable\nWe will often refer to these as wait() andsignal() for simplicity.\nOne thing you might notice about the wait() call is that it also takes a\nmutex as a parameter; it assumes that this mutex is locked when wait()\nis called. The responsibility of wait() is to release the lock and put the\ncalling thread to sleep (atomically); when the thread wakes u p (after some\nother thread has signaled it), it must re-acquire the lock befor e returning\nto the caller. This complexity stems from the desire to prevent certain\nrace conditions from occurring when a thread is trying to put itse lf to\nsleep. Let’s take a look at the solution to the join problem (Figure 30 .3) to\nunderstand this better.\nThere are two cases to consider. In the ﬁrst, the parent create s the child\nthread but continues running itself (assume we have only a sing le pro-\ncessor) and thus immediately calls into thrjoin() to wait for the child\nthread to complete. In this case, it will acquire the lock, chec k if the child\nis done (it is not), and put itself to sleep by calling wait() (hence releas-\ning the lock). The child will eventually run, print the messag e “child”,\nand callthrexit() to wake the parent thread; this code just grabs the\nlock, sets the state variable done , and signals the parent thus waking it.\nFinally, the parent will run (returning from wait() with the lock held),\nunlock the lock, and print the ﬁnal message “parent: end”.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 CONDITION VARIABLES\nIn the second case, the child runs immediately upon creation, se ts\ndone to 1, calls signal to wake a sleeping thread (but there is none, so\nit just returns), and is done. The parent then runs, calls thrjoin() , sees\nthatdone is 1, and thus does not wait and returns.\nOne last note: you might observe the parent uses a while loop instead\nof just anifstatement when deciding whether to wait on the condition.\nWhile this does not seem strictly necessary per the logic of the pr ogram,\nit is always a good idea, as we will see below.\nTo make sure you understand the importance of each piece of the\nthrexit() andthrjoin() code, let’s try a few alternate implemen-\ntations. First, you might be wondering if we need the state varia bledone .\nWhat if the code looked like the example below? Would this work?\n1void thr_exit() {\n2Pthread_mutex_lock(&m);\n3Pthread_cond_signal(&c);\n4Pthread_mutex_unlock(&m);\n5}\n6\n7void thr_join() {\n8Pthread_mutex_lock(&m);\n9Pthread_cond_wait(&c, &m);\n10 Pthread_mutex_unlock(&m);\n11}\nUnfortunately this approach is broken. Imagine the case where t he\nchild runs immediately and calls threxit() immediately; in this case,\nthe child will signal, but there is no thread asleep on the condi tion. When\nthe parent runs, it will simply call wait and be stuck; no thre ad will ever\nwake it. From this example, you should appreciate the importance of\nthe state variable done ; it records the value the threads are interested in\nknowing. The sleeping, waking, and locking all are built around it.\nHere is another poor implementation. In this example, we imagine\nthat one does not need to hold a lock in order to signal and wait. What\nproblem could occur here? Think about it!\n1void thr_exit() {\n2done = 1;\n3Pthread_cond_signal(&c);\n4}\n5\n6void thr_join() {\n7if (done == 0)\n8 Pthread_cond_wait(&c);\n9}\nThe issue here is a subtle race condition. Speciﬁcally, if the pa rent calls\nthrjoin() and then checks the value of done , it will see that it is 0 and\nthus try to go to sleep. But just before it calls wait to go to sle ep, the parent\nis interrupted, and the child runs. The child changes the sta te variable\ndone to 1 and signals, but no thread is waiting and thus no thread is\nwoken. When the parent runs again, it sleeps forever, which is s ad.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONDITION VARIABLES 5\nTIP: ALWAYS HOLD THELOCK WHILE SIGNALING\nAlthough it is strictly not necessary in all cases, it is likely simplest and\nbest to hold the lock while signaling when using condition variab les. The\nexample above shows a case where you must hold the lock for correct-\nness; however, there are some other cases where it is likely OK not to, but\nprobably is something you should avoid. Thus, for simplicity, hold the\nlock when calling signal .\nThe converse of this tip, i.e., hold the lock when calling wait, is not just\na tip, but rather mandated by the semantics of wait, because wa it always\n(a) assumes the lock is held when you call it, (b) releases said l ock when\nputting the caller to sleep, and (c) re-acquires the lock just before return-\ning. Thus, the generalization of this tip is correct: hold the lock when\ncalling signal or wait , and you will always be in good shape.\nHopefully, from this simple join example, you can see some of the ba-\nsic requirements of using condition variables properly. To make sure you\nunderstand, we now go through a more complicated example: the pro-\nducer/consumer orbounded-buffer problem.\n30.2 The Producer/Consumer (Bounded Buffer) Problem\nThe next synchronization problem we will confront in this chapter is\nknown as the producer/consumer problem, or sometimes as the bounded\nbuffer problem, which was ﬁrst posed by Dijkstra [D72]. Indeed, it was\nthis very producer/consumer problem that led Dijkstra and his c o-workers\nto invent the generalized semaphore (which can be used as eith er a lock\nor a condition variable) [D01]; we will learn more about semaphores later.\nImagine one or more producer threads and one or more consumer\nthreads. Producers generate data items and place them in a buf fer; con-\nsumers grab said items from the buffer and consume them in some wa y.\nThis arrangement occurs in many real systems. For example, in a\nmulti-threaded web server, a producer puts HTTP requests int o a work\nqueue (i.e., the bounded buffer); consumer threads take reque sts out of\nthis queue and process them.\nA bounded buffer is also used when you pipe the output of one pro-\ngram into another, e.g., grep foo file.txt | wc -l . This example\nruns two processes concurrently; grep writes lines from file.txt with\nthe string foo in them to what it thinks is standard output; the U NIX\nshell redirects the output to what is called a U NIX pipe (created by the\npipe system call). The other end of this pipe is connected to the stan-\ndard input of the process wc, which simply counts the number of lines in\nthe input stream and prints out the result. Thus, the grep process is the\nproducer; the wcprocess is the consumer; between them is an in-kernel\nbounded buffer; you, in this example, are just the happy user.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 CONDITION VARIABLES\n1int buffer;\n2int count = 0; // initially, empty\n3\n4void put(int value) {\n5assert(count == 0);\n6count = 1;\n7buffer = value;\n8}\n9\n10int get() {\n11 assert(count == 1);\n12 count = 0;\n13 return buffer;\n14}\nFigure 30.4: The Put And Get Routines (Version 1)\n1void*producer(void *arg) {\n2int i;\n3int loops = (int) arg;\n4for (i = 0; i < loops; i++) {\n5 put(i);\n6}\n7}\n8\n9void*consumer(void *arg) {\n10 int i;\n11 while (1) {\n12 int tmp = get();\n13 printf(""%d\n"", tmp);\n14 }\n15}\nFigure 30.5: Producer/Consumer Threads (Version 1)\nBecause the bounded buffer is a shared resource, we must of course\nrequire synchronized access to it, lest1a race condition arise. To begin to\nunderstand this problem better, let us examine some actual code .\nThe ﬁrst thing we need is a shared buffer, into which a producer puts\ndata, and out of which a consumer takes data. Let’s just use a singl e\ninteger for simplicity (you can certainly imagine placing a poi nter to a\ndata structure into this slot instead), and the two inner routi nes to put\na value into the shared buffer, and to get a value out of the buffe r. See\nFigure 30.4 for details.\nPretty simple, no? The put() routine assumes the buffer is empty\n(and checks this with an assertion), and then simply puts a val ue into the\nshared buffer and marks it full by setting count to 1. Theget() routine\ndoes the opposite, setting the buffer to empty (i.e., setting count to 0)\nand returning the value. Don’t worry that this shared buffer has just a\nsingle entry; later, we’ll generalize it to a queue that can hol d multiple\nentries, which will be even more fun than it sounds.\nNow we need to write some routines that know when it is OK to access\nthe buffer to either put data into it or get data out of it. The condi tions for\nthis should be obvious: only put data into the buffer when count is zero\n1This is where we drop some serious Old English on you, and the subjunctiv e form.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONDITION VARIABLES 7\n1int loops; // must initialize somewhere...\n2cond_t cond;\n3mutex_t mutex;\n4\n5void*producer(void *arg) {\n6int i;\n7for (i = 0; i < loops; i++) {\n8 Pthread_mutex_lock(&mutex); // p1\n9 if (count == 1) // p2\n10 Pthread_cond_wait(&cond, &mutex); // p3\n11 put(i); // p4\n12 Pthread_cond_signal(&cond); // p5\n13 Pthread_mutex_unlock(&mutex); // p6\n14 }\n15}\n16\n17void*consumer(void *arg) {\n18 int i;\n19 for (i = 0; i < loops; i++) {\n20 Pthread_mutex_lock(&mutex); // c1\n21 if (count == 0) // c2\n22 Pthread_cond_wait(&cond, &mutex); // c3\n23 int tmp = get(); // c4\n24 Pthread_cond_signal(&cond); // c5\n25 Pthread_mutex_unlock(&mutex); // c6\n26 printf(""%d\n"", tmp);\n27 }\n28}\nFigure 30.6: Producer/Consumer: Single CV And If Statement\n(i.e., when the buffer is empty), and only get data from the buff er when\ncount is one (i.e., when the buffer is full). If we write the synchroni zation\ncode such that a producer puts data into a full buffer, or a consume r gets\ndata from an empty one, we have done something wrong (and in this\ncode, an assertion will ﬁre).\nThis work is going to be done by two types of threads, one set of which\nwe’ll call the producer threads, and the other set which we’ll call con-\nsumer threads. Figure 30.5 shows the code for a producer that puts an\ninteger into the shared buffer loops number of times, and a consumer\nthat gets the data out of that shared buffer (forever), each time printing\nout the data item it pulled from the shared buffer.\nA Broken Solution\nNow imagine that we have just a single producer and a single consu mer.\nObviously the put() andget() routines have critical sections within\nthem, asput() updates the buffer, and get() reads from it. However,\nputting a lock around the code doesn’t work; we need something more.\nNot surprisingly, that something more is some condition variables . In this\n(broken) ﬁrst try (Figure 30.6), we have a single condition vari ablecond\nand associated lock mutex .\nLet’s examine the signaling logic between producers and consume rs.\nWhen a producer wants to ﬁll the buffer, it waits for it to be empt y (p1–\np3). The consumer has the exact same logic, but waits for a differ ent\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 CONDITION VARIABLES\nTc1 State T c2 State Tp State Count Comment\nc1 Running Ready Ready 0\nc2 Running Ready Ready 0\nc3 Sleep Ready Ready 0 Nothing to get\nSleep Ready p1 Running 0\nSleep Ready p2 Running 0\nSleep Ready p4 Running 1 Buffer now full\nReady Ready p5 Running 1 T c1awoken\nReady Ready p6 Running 1\nReady Ready p1 Running 1\nReady Ready p2 Running 1\nReady Ready p3 Sleep 1 Buffer full; sleep\nReady c1 Running Sleep 1 T c2sneaks in ...\nReady c2 Running Sleep 1\nReady c4 Running Sleep 0 ... and grabs data\nReady c5 Running Ready 0 T pawoken\nReady c6 Running Ready 0\nc4 Running Ready Ready 0 Oh oh! No data\nFigure 30.7: Thread Trace: Broken Solution (Version 1)\ncondition: fullness (c1–c3).\nWith just a single producer and a single consumer, the code in Fig ure\n30.6 works. However, if we have more than one of these threads (e.g. ,\ntwo consumers), the solution has two critical problems. What are they?\n... (pause here to think) ...\nLet’s understand the ﬁrst problem, which has to do with the ifstate-\nment before the wait. Assume there are two consumers ( Tc1andTc2) and\none producer ( Tp). First, a consumer ( Tc1) runs; it acquires the lock (c1),\nchecks if any buffers are ready for consumption (c2), and ﬁnding that\nnone are, waits (c3) (which releases the lock).\nThen the producer ( Tp) runs. It acquires the lock (p1), checks if all\nbuffers are full (p2), and ﬁnding that not to be the case, goes ah ead and\nﬁlls the buffer (p4). The producer then signals that a buffer h as been\nﬁlled (p5). Critically, this moves the ﬁrst consumer ( Tc1) from sleeping\non a condition variable to the ready queue; Tc1is now able to run (but\nnot yet running). The producer then continues until realizing t he buffer\nis full, at which point it sleeps (p6, p1–p3).\nHere is where the problem occurs: another consumer ( Tc2) sneaks in\nand consumes the one existing value in the buffer (c1, c2, c4, c5 , c6, skip-\nping the wait at c3 because the buffer is full). Now assume Tc1runs; just\nbefore returning from the wait, it re-acquires the lock and then returns. It\nthen callsget() (c4), but there are no buffers to consume! An assertion\ntriggers, and the code has not functioned as desired. Clearly, w e should\nhave somehow prevented Tc1from trying to consume because Tc2snuck\nin and consumed the one value in the buffer that had been produced . Fig-\nure 30.7 shows the action each thread takes, as well as its sched uler state\n(Ready, Running, or Sleeping) over time.\nThe problem arises for a simple reason: after the producer woke Tc1,\nbutbeforeTc1ever ran, the state of the bounded buffer changed (thanks to\nTc2). Signaling a thread only wakes them up; it is thus a hint that the state\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONDITION VARIABLES 9\n1int loops;\n2cond_t cond;\n3mutex_t mutex;\n4\n5void*producer(void *arg) {\n6int i;\n7for (i = 0; i < loops; i++) {\n8 Pthread_mutex_lock(&mutex); // p1\n9 while (count == 1) // p2\n10 Pthread_cond_wait(&cond, &mutex); // p3\n11 put(i); // p4\n12 Pthread_cond_signal(&cond); // p5\n13 Pthread_mutex_unlock(&mutex); // p6\n14 }\n15}\n16\n17void*consumer(void *arg) {\n18 int i;\n19 for (i = 0; i < loops; i++) {\n20 Pthread_mutex_lock(&mutex); // c1\n21 while (count == 0) // c2\n22 Pthread_cond_wait(&cond, &mutex); // c3\n23 int tmp = get(); // c4\n24 Pthread_cond_signal(&cond); // c5\n25 Pthread_mutex_unlock(&mutex); // c6\n26 printf(""%d\n"", tmp);\n27 }\n28}\nFigure 30.8: Producer/Consumer: Single CV And While\nof the world has changed (in this case, that a value has been plac ed in the\nbuffer), but there is no guarantee that when the woken thread r uns, the\nstate will still be as desired. This interpretation of what a signal means\nis often referred to as Mesa semantics , after the ﬁrst research that built\na condition variable in such a manner [LR80]; the contrast, refe rred to as\nHoare semantics , is harder to build but provides a stronger guarantee\nthat the woken thread will run immediately upon being woken [H74 ].\nVirtually every system ever built employs Mesa semantics.\nBetter, But Still Broken: While, Not If\nFortunately, this ﬁx is easy (Figure 30.8): change the ifto awhile . Think\nabout why this works; now consumer Tc1wakes up and (with the lock\nheld) immediately re-checks the state of the shared variable (c2). If the\nbuffer is empty at that point, the consumer simply goes back to sl eep\n(c3). The corollary ifis also changed to a while in the producer (p2).\nThanks to Mesa semantics, a simple rule to remember with condi tion\nvariables is to always use while loops . Sometimes you don’t have to re-\ncheck the condition, but it is always safe to do so; just do it and b e happy.\nHowever, this code still has a bug, the second of two problems men-\ntioned above. Can you see it? It has something to do with the fact th at\nthere is only one condition variable. Try to ﬁgure out what the probl em\nis, before reading ahead. DO IT!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 CONDITION VARIABLES\nTc1 State T c2 State Tp State Count Comment\nc1 Running Ready Ready 0\nc2 Running Ready Ready 0\nc3 Sleep Ready Ready 0 Nothing to get\nSleep c1 Running Ready 0\nSleep c2 Running Ready 0\nSleep c3 Sleep Ready 0 Nothing to get\nSleep Sleep p1 Running 0\nSleep Sleep p2 Running 0\nSleep Sleep p4 Running 1 Buffer now full\nReady Sleep p5 Running 1 T c1awoken\nReady Sleep p6 Running 1\nReady Sleep p1 Running 1\nReady Sleep p2 Running 1\nReady Sleep p3 Sleep 1 Must sleep (full)\nc2 Running Sleep Sleep 1 Recheck condition\nc4 Running Sleep Sleep 0 T c1grabs data\nc5 Running Ready Sleep 0 Oops! Woke T c2\nc6 Running Ready Sleep 0\nc1 Running Ready Sleep 0\nc2 Running Ready Sleep 0\nc3 Sleep Ready Sleep 0 Nothing to get\nSleep c2 Running Sleep 0\nSleep c3 Sleep Sleep 0 Everyone asleep...\nFigure 30.9: Thread Trace: Broken Solution (Version 2)\n... (another pause for you to think, or close your eyes for a bit) ...\nLet’s conﬁrm you ﬁgured it out correctly, or perhaps let’s conﬁrm that\nyou are now awake and reading this part of the book. The problem oc-\ncurs when two consumers run ﬁrst ( Tc1andTc2) and both go to sleep (c3).\nThen, the producer runs, puts a value in the buffer, and wakes on e of the\nconsumers (say Tc1). The producer then loops back (releasing and reac-\nquiring the lock along the way) and tries to put more data in the bu ffer;\nbecause the buffer is full, the producer instead waits on the con dition\n(thus sleeping). Now, one consumer is ready to run ( Tc1), and two threads\nare sleeping on a condition ( Tc2andTp). We are about to cause a problem:\nthings are getting exciting!\nThe consumer Tc1then wakes by returning from wait() (c3), re-checks\nthe condition (c2), and ﬁnding the buffer full, consumes the val ue (c4).\nThis consumer then, critically, signals on the condition (c5), w aking only\nonethread that is sleeping. However, which thread should it wake?\nBecause the consumer has emptied the buffer, it clearly should wake\nthe producer. However, if it wakes the consumer Tc2(which is deﬁnitely\npossible, depending on how the wait queue is managed), we have a p rob-\nlem. Speciﬁcally, the consumer Tc2will wake up and ﬁnd the buffer\nempty (c2), and go back to sleep (c3). The producer Tp, which has a value\nto put into the buffer, is left sleeping. The other consumer thr ead,Tc1,\nalso goes back to sleep. All three threads are left sleeping, a clear bug; see\nFigure 30.9 for the brutal step-by-step of this terrible calam ity.\nSignaling is clearly needed, but must be more directed. A consum er\nshould not wake other consumers, only producers, and vice-versa.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONDITION VARIABLES 11\n1cond_t empty, fill;\n2mutex_t mutex;\n3\n4void*producer(void *arg) {\n5int i;\n6for (i = 0; i < loops; i++) {\n7 Pthread_mutex_lock(&mutex);\n8 while (count == 1)\n9 Pthread_cond_wait(&empty, &mutex);\n10 put(i);\n11 Pthread_cond_signal(&fill);\n12 Pthread_mutex_unlock(&mutex);\n13 }\n14}\n15\n16void*consumer(void *arg) {\n17 int i;\n18 for (i = 0; i < loops; i++) {\n19 Pthread_mutex_lock(&mutex);\n20 while (count == 0)\n21 Pthread_cond_wait(&fill, &mutex);\n22 int tmp = get();\n23 Pthread_cond_signal(&empty);\n24 Pthread_mutex_unlock(&mutex);\n25 printf(""%d\n"", tmp);\n26 }\n27}\nFigure 30.10: Producer/Consumer: Two CVs And While\nThe Single Buffer Producer/Consumer Solution\nThe solution here is once again a small one: use twocondition variables,\ninstead of one, in order to properly signal which type of thread shou ld\nwake up when the state of the system changes. Figure 30.10 shows the\nresulting code.\nIn the code above, producer threads wait on the condition empty , and\nsignals ﬁll. Conversely, consumer threads wait on ﬁlland signal empty .\nBy doing so, the second problem above is avoided by design: a consumer\ncan never accidentally wake a consumer, and a producer can neve r acci-\ndentally wake a producer.\nThe Correct Producer/Consumer Solution\nWe now have a working producer/consumer solution, albeit not a fully\ngeneral one. The last change we make is to enable more concurrenc y and\nefﬁciency; speciﬁcally, we add more buffer slots, so that mult iple values\ncan be produced before sleeping, and similarly multiple value s can be\nconsumed before sleeping. With just a single producer and consum er, this\napproach is more efﬁcient as it reduces context switches; with m ultiple\nproducers or consumers (or both), it even allows concurrent producin g\nor consuming to take place, thus increasing concurrency. Fortun ately, it\nis a small change from our current solution.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 CONDITION VARIABLES\n1int buffer[MAX];\n2int fill_ptr = 0;\n3int use_ptr = 0;\n4int count = 0;\n5\n6void put(int value) {\n7buffer[fill_ptr] = value;\n8fill_ptr = (fill_ptr + 1) % MAX;\n9count++;\n10}\n11\n12int get() {\n13 int tmp = buffer[use_ptr];\n14 use_ptr = (use_ptr + 1) % MAX;\n15 count--;\n16 return tmp;\n17}\nFigure 30.11: The Correct Put And Get Routines\n1cond_t empty, fill;\n2mutex_t mutex;\n3\n4void*producer(void *arg) {\n5int i;\n6for (i = 0; i < loops; i++) {\n7 Pthread_mutex_lock(&mutex); // p1\n8 while (count == MAX) // p2\n9 Pthread_cond_wait(&empty, &mutex); // p3\n10 put(i); // p4\n11 Pthread_cond_signal(&fill); // p5\n12 Pthread_mutex_unlock(&mutex); // p6\n13 }\n14}\n15\n16void*consumer(void *arg) {\n17 int i;\n18 for (i = 0; i < loops; i++) {\n19 Pthread_mutex_lock(&mutex); // c1\n20 while (count == 0) // c2\n21 Pthread_cond_wait(&fill, &mutex); // c3\n22 int tmp = get(); // c4\n23 Pthread_cond_signal(&empty); // c5\n24 Pthread_mutex_unlock(&mutex); // c6\n25 printf(""%d\n"", tmp);\n26 }\n27}\nFigure 30.12: The Correct Producer/Consumer Synchronization\nThe ﬁrst change for this correct solution is within the buffer str ucture\nitself and the corresponding put() andget() (Figure 30.11). We also\nslightly change the conditions that producers and consumers che ck in or-\nder to determine whether to sleep or not. Figure 30.12 shows the c orrect\nwaiting and signaling logic. A producer only sleeps if all buffe rs are cur-\nrently ﬁlled (p2); similarly, a consumer only sleeps if all buf fers are cur-\nrently empty (c2). And thus we solve the producer/consumer probl em;\ntime to sit back and drink a cold one.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONDITION VARIABLES 13\nTIP: USEWHILE (NOTIF) FORCONDITIONS\nWhen checking for a condition in a multi-threaded program, using\nawhile loop is always correct; using an ifstatement only might be,\ndepending on the semantics of signaling. Thus, always use while and\nyour code will behave as expected.\nUsing while loops around conditional checks also handles the case\nwhere spurious wakeups occur. In some thread packages, due to de-\ntails of the implementation, it is possible that two threads get woken up\nthough just a single signal has taken place [L11]. Spurious wake ups are\nfurther reason to re-check the condition a thread is waiting on.\n30.3 Covering Conditions\nWe’ll now look at one more example of how condition variables can\nbe used. This code study is drawn from Lampson and Redell’s paper on\nPilot [LR80], the same group who ﬁrst implemented the Mesa semantics\ndescribed above (the language they used was Mesa, hence the na me).\nThe problem they ran into is best shown via simple example, in th is\ncase in a simple multi-threaded memory allocation library. Fig ure 30.13\nshows a code snippet which demonstrates the issue.\nAs you might see in the code, when a thread calls into the memory\nallocation code, it might have to wait in order for more memory to be-\ncome free. Conversely, when a thread frees memory, it signals th at more\nmemory is free. However, our code above has a problem: which waiting\nthread (there can be more than one) should be woken up?\nConsider the following scenario. Assume there are zero bytes fre e;\nthreadTacallsallocate(100) , followed by thread Tbwhich asks for\nless memory by calling allocate(10) . BothTaandTbthus wait on the\ncondition and go to sleep; there aren’t enough free bytes to satis fy either\nof these requests.\nAt that point, assume a third thread, Tc, callsfree(50) . Unfortu-\nnately, when it calls signal to wake a waiting thread, it migh t not wake\nthe correct waiting thread, Tb, which is waiting for only 10 bytes to be\nfreed;Tashould remain waiting, as not enough memory is yet free. Thus,\nthe code in the ﬁgure does not work, as the thread waking other threa ds\ndoes not know which thread (or threads) to wake up.\nThe solution suggested by Lampson and Redell is straightforward : re-\nplace thepthread condsignal() call in the code above with a call to\npthread condbroadcast() , which wakes up allwaiting threads. By\ndoing so, we guarantee that any threads that should be woken are. T he\ndownside, of course, can be a negative performance impact, as we m ight\nneedlessly wake up many other waiting threads that shouldn’t (y et) be\nawake. Those threads will simply wake up, re-check the conditi on, and\nthen go immediately back to sleep.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 CONDITION VARIABLES\n1// how many bytes of the heap are free?\n2int bytesLeft = MAX_HEAP_SIZE;\n3\n4// need lock and condition too\n5cond_t c;\n6mutex_t m;\n7\n8void*\n9allocate(int size) {\n10 Pthread_mutex_lock(&m);\n11 while (bytesLeft < size)\n12 Pthread_cond_wait(&c, &m);\n13 void*ptr = ...; // get mem from heap\n14 bytesLeft -= size;\n15 Pthread_mutex_unlock(&m);\n16 return ptr;\n17}\n18\n19void free(void *ptr, int size) {\n20 Pthread_mutex_lock(&m);\n21 bytesLeft += size;\n22 Pthread_cond_signal(&c); // whom to signal??\n23 Pthread_mutex_unlock(&m);\n24}\nFigure 30.13: Covering Conditions: An Example\nLampson and Redell call such a condition a covering condition , as it\ncovers all the cases where a thread needs to wake up (conservati vely);\nthe cost, as we’ve discussed, is that too many threads might be wok en.\nThe astute reader might also have noticed we could have used thi s ap-\nproach earlier (see the producer/consumer problem with only a sin gle\ncondition variable). However, in that case, a better solution was avail-\nable to us, and thus we used it. In general, if you ﬁnd that your pr ogram\nonly works when you change your signals to broadcasts (but you don’t\nthink it should need to), you probably have a bug; ﬁx it! But in case s like\nthe memory allocator above, broadcast may be the most straightforwa rd\nsolution available.\n30.4 Summary\nWe have seen the introduction of another important synchronization\nprimitive beyond locks: condition variables. By allowing thread s to sleep\nwhen some program state is not as desired, CVs enable us to neatly solve\na number of important synchronization problems, including the fa mous\n(and still important) producer/consumer problem, as well as cove ring\nconditions. A more dramatic concluding sentence would go here, su ch as\n“He loved Big Brother” [O49].\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCONDITION VARIABLES 15\nReferences\n[D68] “Cooperating sequential processes” by Edsger W. Dijkstra . 1968. Available online here:\nhttp://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.P DF.Another classic from Di-\njkstra; reading his early works on concurrency will teach you much of what you n eed to know.\n[D72] “Information Streams Sharing a Finite Buffer” by E.W. Dijkstr a. Information Processing\nLetters 1: 179180, 1972. Available: http://www.cs.utexas.e du/users/EWD/ewd03xx/EWD329.PDF\nThe famous paper that introduced the producer/consumer problem.\n[D01] “My recollections of operating system design” by E.W. Dijkstr a. April, 2001. Avail-\nable:http://www.cs.utexas.edu/users/EWD/ewd13xx/EWD1303. PDF.A fascinating\nread for those of you interested in how the pioneers of our ﬁeld came up with some v ery basic and\nfundamental concepts, including ideas like “interrupts” and even “a stac k”!\n[H74] “Monitors: An Operating System Structuring Concept” by C.A.R. H oare. Communica-\ntions of the ACM, 17:10, pages 549–557, October 1974. Hoare did a fair amount of theoretical work\nin concurrency. However, he is still probably most known for his work on Qu icksort, the coolest sorting\nalgorithm in the world, at least according to these authors.\n[L11] “Pthread cond signal Man Page” by Mysterious author. March, 2011. Available online:\nhttp://linux.die.net/man/3/pthread condsignal .The Linux man page shows a nice\nsimple example of why a thread might get a spurious wakeup, due to race con ditions within the sig-\nnal/wakeup code.\n[LR80] “Experience with Processes and Monitors in Mesa” by B.W. Lampso n, D.R. Redell.\nCommunications of the ACM. 23:2, pages 105-117, February 1980. A terriﬁc paper about how\nto actually implement signaling and condition variables in a real system, l eading to the term “Mesa”\nsemantics for what it mzshortns to be woken up; the older semantics, developed by Tony Hoare [H74],\nthen became known as “Hoare” semantics, which is hard to say out loud in class wi th a straight face.\n[O49] “1984” by George Orwell. Secker and Warburg, 1949. A little heavy-handed, but of course\na must read. That said, we kind of gave away the ending by quoting the last sente nce. Sorry! And if\nthe government is reading this, let us just say that we think that the govern ment is “double plus good”.\nHear that, our pals at the NSA?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 CONDITION VARIABLES\nHomework (Code)\nThis homework lets you explore some real code that uses locks and\ncondition variables to implement various forms of the producer/con sumer\nqueue discussed in the chapter. You’ll look at the real code, run it in\nvarious conﬁgurations, and use it to learn about what works and wha t\ndoesn’t, as well as other intricacies. Read the README for detail s.\nQuestions\n1. Our ﬁrst question focuses on main-two-cvs-while.c (the working so-\nlution). First, study the code. Do you think you have an understa nding of\nwhat should happen when you run the program?\n2. Run with one producer and one consumer, and have the producer pro duce\na few values. Start with a buffer (size 1), and then increase it. How does the\nbehavior of the code change with larger buffers? (or does it?) What would\nyou predict numfull to be with different buffer sizes (e.g., -m 10 ) and\ndifferent numbers of produced items (e.g., -l 100 ), when you change the\nconsumer sleep string from default (no sleep) to -C 0,0,0,0,0,0,1 ?\n3. If possible, run the code on different systems (e.g., a Mac an d Linux). Do\nyou see different behavior across these systems?\n4. Let’s look at some timings. How long do you think the followin g execution,\nwith one producer, three consumers, a single-entry shared buffe r, and each\nconsumer pausing at point c3for a second, will take? ./main-two-cvs-while\n-p 1 -c 3 -m 1 -C 0,0,0,1,0,0,0:0,0,0,1,0,0,0:0,0,0,1,0,0, 0\n-l 10 -v -t\n5. Now change the size of the shared buffer to 3 ( -m 3 ). Will this make any\ndifference in the total time?\n6. Now change the location of the sleep to c6(this models a consumer taking\nsomething off the queue and then doing something with it), again us ing a\nsingle-entry buffer. What time do you predict in this case? ./main-two-cvs-while\n-p 1 -c 3 -m 1 -C 0,0,0,0,0,0,1:0,0,0,0,0,0,1:0,0,0,0,0,0, 1\n-l 10 -v -t\n7. Finally, change the buffer size to 3 again ( -m 3 ). What time do you predict\nnow?\n8. Now let’s look at main-one-cv-while.c . Can you conﬁgure a sleep\nstring, assuming a single producer, one consumer, and a buffer of si ze 1,\nto cause a problem with this code?\n9. Now change the number of consumers to two. Can you construct slee p\nstrings for the producer and the consumers so as to cause a problem in the\ncode?\n10. Now examine main-two-cvs-if.c . Can you cause a problem to happen\nin this code? Again consider the case where there is only one co nsumer, and\nthen the case where there is more than one.\n11. Finally, examine main-two-cvs-while-extra-unlock.c . What prob-\nlem arises when you release the lock before doing a put or a get? Can you\nreliably cause such a problem to happen, given the sleep string s? What bad\nthing can happen?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",36742
36-31. Semaphores.pdf,36-31. Semaphores,"31\nSemaphores\nAs we know now, one needs both locks and condition variables to solve\na broad range of relevant and interesting concurrency problems. One of\nthe ﬁrst people to realize this years ago was Edsger Dijkstra (though it\nis hard to know the exact history [GR92]), known among other things for\nhis famous “shortest paths” algorithm in graph theory [D59], an e arly\npolemic on structured programming entitled “Goto Statements Cons id-\nered Harmful” [D68a] (what a great title!), and, in the case we will study\nhere, the introduction of a synchronization primitive called the semaphore\n[D68b,D72]. Indeed, Dijkstra and colleagues invented the se maphore as a\nsingle primitive for all things related to synchronization; as you will see,\none can use semaphores as both locks and condition variables.\nTHECRUX: HOWTOUSESEMAPHORES\nHow can we use semaphores instead of locks and condition variables?\nWhat is the deﬁnition of a semaphore? What is a binary semaphore? I s\nit straightforward to build a semaphore out of locks and condition va ri-\nables? To build locks and condition variables out of semaphores?\n31.1 Semaphores: A Deﬁnition\nA semaphore is an object with an integer value that we can manipu late\nwith two routines; in the POSIX standard, these routines are semwait()\nandsempost()1. Because the initial value of the semaphore deter-\nmines its behavior, before calling any other routine to interact with the\nsemaphore, we must ﬁrst initialize it to some value, as the code i n Figure\n31.1 does.\n1Historically, semwait() was called P() by Dijkstra and sempost() called V(). P()\ncomes from “prolaag”, a contraction of “probeer” (Dutch for “try”) and “verlaag” (“de-\ncrease”); V() comes from the Dutch word “verhoog” which means “increase” (tha nks to Mart\nOskamp for this information). Sometimes, people call them down and up. U se the Dutch\nversions to impress your friends, or confuse them, or both.\n1\n2 SEMAPHORES\n1#include <semaphore.h>\n2sem_t s;\n3sem_init(&s, 0, 1);\nFigure 31.1: Initializing A Semaphore\nIn the ﬁgure, we declare a semaphore s and initialize it to the v alue 1\nby passing 1 in as the third argument. The second argument to seminit()\nwill be set to 0 in all of the examples we’ll see; this indicates t hat the\nsemaphore is shared between threads in the same process. See the man\npage for details on other usages of semaphores (namely, how they can\nbe used to synchronize access across different processes), which require a\ndifferent value for that second argument.\nAfter a semaphore is initialized, we can call one of two functions to\ninteract with it, semwait() orsempost() . The behavior of these two\nfunctions is seen in Figure 31.2.\nFor now, we are not concerned with the implementation of these rou-\ntines, which clearly requires some care; with multiple threa ds calling into\nsemwait() andsempost() , there is the obvious need for managing\nthese critical sections. We will now focus on how to usethese primitives;\nlater we may discuss how they are built.\nWe should discuss a few salient aspects of the interfaces here. First, we\ncan see that semwait() will either return right away (because the value\nof the semaphore was one or higher when we called semwait() ), or it\nwill cause the caller to suspend execution waiting for a subseq uent post.\nOf course, multiple calling threads may call into semwait() , and thus\nall be queued waiting to be woken.\nSecond, we can see that sempost() does not wait for some particular\ncondition to hold like semwait() does. Rather, it simply increments the\nvalue of the semaphore and then, if there is a thread waiting to b e woken,\nwakes one of them up.\nThird, the value of the semaphore, when negative, is equal to th e num-\nber of waiting threads [D68b]. Though the value generally isn’t seen by\nusers of the semaphores, this invariant is worth knowing and perh aps\ncan help you remember how a semaphore functions.\nDon’t worry (yet) about the seeming race conditions possible within\nthe semaphore; assume that the actions they make are performed a tomi-\ncally. We will soon use locks and condition variables to do just thi s.\n1int sem_wait(sem_t *s) {\n2decrement the value of semaphore s by one\n3wait if value of semaphore s is negative\n4}\n5\n6int sem_post(sem_t *s) {\n7increment the value of semaphore s by one\n8if there are one or more threads waiting, wake one\n9}\nFigure 31.2: Semaphore: Deﬁnitions Of Wait And Post\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEMAPHORES 3\n1sem_t m;\n2sem_init(&m, 0, X); // initialize semaphore to X; what shoul d X be?\n3\n4sem_wait(&m);\n5// critical section here\n6sem_post(&m);\nFigure 31.3: A Binary Semaphore (That Is, A Lock)\n31.2 Binary Semaphores (Locks)\nWe are now ready to use a semaphore. Our ﬁrst use will be one with\nwhich we are already familiar: using a semaphore as a lock. See Fi gure\n31.3 for a code snippet; therein, you’ll see that we simply surroun d the\ncritical section of interest with a semwait() /sempost() pair. Criti-\ncal to making this work, though, is the initial value of the semap horem\n(initialized to Xin the ﬁgure). What should Xbe?\n... (Try thinking about it before going on) ...\nLooking back at deﬁnition of the semwait() andsempost() rou-\ntines above, we can see that the initial value should be 1.\nTo make this clear, let’s imagine a scenario with two threads. The ﬁrst\nthread (Thread 0) calls semwait() ; it will ﬁrst decrement the value of\nthe semaphore, changing it to 0. Then, it will wait only if the va lue is\nnotgreater than or equal to 0. Because the value is 0, semwait() will\nsimply return and the calling thread will continue; Thread 0 i s now free to\nenter the critical section. If no other thread tries to acquire the lock while\nThread 0 is inside the critical section, when it calls sempost() , it will\nsimply restore the value of the semaphore to 1 (and not wake a waiti ng\nthread, because there are none). Figure 31.4 shows a trace of thi s scenario.\nA more interesting case arises when Thread 0 “holds the lock” (i. e.,\nit has called semwait() but not yet called sempost() ), and another\nthread (Thread 1) tries to enter the critical section by calli ngsemwait() .\nIn this case, Thread 1 will decrement the value of the semaphore to -1, and\nthus wait (putting itself to sleep and relinquishing the proc essor). When\nThread 0 runs again, it will eventually call sempost() , incrementing the\nvalue of the semaphore back to zero, and then wake the waiting thr ead\n(Thread 1), which will then be able to acquire the lock for itsel f. When\nThread 1 ﬁnishes, it will again increment the value of the sema phore,\nrestoring it to 1 again.\nValue of Semaphore Thread 0 Thread 1\n1\n1 call semwait()\n0 semwait() returns\n0 (crit sect)\n0 call sempost()\n1 sempost() returns\nFigure 31.4: Thread Trace: Single Thread Using A Semaphore\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 SEMAPHORES\nValue Thread 0 State Thread 1 State\n1 Running Ready\n1 call semwait() Running Ready\n0semwait() returns Running Ready\n0(crit sect: begin) Running Ready\n0 Interrupt; Switch →T1 Ready Running\n0 Ready call semwait() Running\n-1 Ready decrement sem Running\n-1 Ready (sem<0)→sleep Sleeping\n-1 Running Switch→T0 Sleeping\n-1(crit sect: end) Running Sleeping\n-1 call sempost() Running Sleeping\n0increment sem Running Sleeping\n0wake(T1) Running Ready\n0sempost() returns Running Ready\n0 Interrupt; Switch →T1 Ready Running\n0 Ready semwait() returns Running\n0 Ready (crit sect) Running\n0 Ready call sempost() Running\n1 Ready sempost() returns Running\nFigure 31.5: Thread Trace: Two Threads Using A Semaphore\nFigure 31.5 shows a trace of this example. In addition to thread a ctions,\nthe ﬁgure shows the scheduler state of each thread: Running, Ready (i.e.,\nrunnable but not running), and Sleeping. Note in particular tha t Thread 1\ngoes into the sleeping state when it tries to acquire the alrea dy-held lock;\nonly when Thread 0 runs again can Thread 1 be awoken and potential ly\nrun again.\nIf you want to work through your own example, try a scenario where\nmultiple threads queue up waiting for a lock. What would the valu e of\nthe semaphore be during such a trace?\nThus we are able to use semaphores as locks. Because locks only hav e\ntwo states (held and not held), we sometimes call a semaphore use d as a\nlock a binary semaphore . Note that if you are using a semaphore only\nin this binary fashion, it could be implemented in a simpler man ner than\nthe generalized semaphores we present here.\n31.3 Semaphores For Ordering\nSemaphores are also useful to order events in a concurrent program .\nFor example, a thread may wish to wait for a list to become non-empt y,\nso it can delete an element from it. In this pattern of usage, we of ten ﬁnd\none thread waiting for something to happen, and another thread making\nthat something happen and then signaling that it has happened, thus wak-\ning the waiting thread. We are thus using the semaphore as an ordering\nprimitive (similar to our use of condition variables earlier).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEMAPHORES 5\n1sem_t s;\n2\n3void*\n4child(void *arg) {\n5printf(""child\n"");\n6sem_post(&s); // signal here: child is done\n7return NULL;\n8}\n9\n10int\n11main(int argc, char *argv[]) {\n12 sem_init(&s, 0, X); // what should X be?\n13 printf(""parent: begin\n"");\n14 pthread_t c;\n15 Pthread_create(&c, NULL, child, NULL);\n16 sem_wait(&s); // wait here for child\n17 printf(""parent: end\n"");\n18 return 0;\n19}\nFigure 31.6: A Parent Waiting For Its Child\nA simple example is as follows. Imagine a thread creates another\nthread and then wants to wait for it to complete its execution (Fi gure\n31.6). When this program runs, we would like to see the following:\nparent: begin\nchild\nparent: end\nThe question, then, is how to use a semaphore to achieve this effe ct; as\nit turns out, the answer is relatively easy to understand. As y ou can see in\nthe code, the parent simply calls semwait() and the child sempost()\nto wait for the condition of the child ﬁnishing its execution to bec ome\ntrue. However, this raises the question: what should the initia l value of\nthis semaphore be?\n(Again, think about it here, instead of reading ahead)\nThe answer, of course, is that the value of the semaphore should be s et\nto is 0. There are two cases to consider. First, let us assume th at the parent\ncreates the child but the child has not run yet (i.e., it is sitt ing in a ready\nqueue but not running). In this case (Figure 31.7, page 6), the parent will\ncallsemwait() before the child has called sempost() ; we’d like the\nparent to wait for the child to run. The only way this will happen is if the\nvalue of the semaphore is not greater than 0; hence, 0 is the initi al value.\nThe parent runs, decrements the semaphore (to -1), then waits (sleeping).\nWhen the child ﬁnally runs, it will call sempost() , increment the value\nof the semaphore to 0, and wake the parent, which will then retur n from\nsemwait() and ﬁnish the program.\nThe second case (Figure 31.8, page 6) occurs when the child runs to\ncompletion before the parent gets a chance to call semwait() . In this\ncase, the child will ﬁrst call sempost() , thus incrementing the value of\nthe semaphore from 0 to 1. When the parent then gets a chance to ru n,\nit will call semwait() and ﬁnd the value of the semaphore to be 1; the\nparent will thus decrement the value (to 0) and return from semwait()\nwithout waiting, also achieving the desired effect.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 SEMAPHORES\nValue Parent State Child State\n0create(Child) Running (Child exists; is runnable) Ready\n0 call semwait() Running Ready\n-1decrement sem Running Ready\n-1(sem<0)→sleep Sleeping Ready\n-1 Switch→Child Sleeping child runs Running\n-1 Sleeping call sempost() Running\n0 Sleeping increment sem Running\n0 Ready wake(Parent) Running\n0 Ready sempost() returns Running\n0 Ready Interrupt; Switch →Parent Ready\n0semwait() returns Running Ready\nFigure 31.7: Thread Trace: Parent Waiting For Child (Case 1)\nValue Parent State Child State\n0create(Child) Running (Child exists; is runnable) Ready\n0 Interrupt; Switch →Child Ready child runs Running\n0 Ready call sempost() Running\n1 Ready increment sem Running\n1 Ready wake(nobody) Running\n1 Ready sempost() returns Running\n1parent runs Running Interrupt; Switch →Parent Ready\n1 call semwait() Running Ready\n0decrement sem Running Ready\n0(sem≥0)→awake Running Ready\n0semwait() returns Running Ready\nFigure 31.8: Thread Trace: Parent Waiting For Child (Case 2)\n31.4 The Producer/Consumer (Bounded Buffer) Problem\nThe next problem we will confront in this chapter is known as the pro-\nducer/consumer problem, or sometimes as the bounded buffer problem\n[D72]. This problem is described in detail in the previous chap ter on con-\ndition variables; see there for details.\nFirst Attempt\nOur ﬁrst attempt at solving the problem introduces two semaphore s,empty\nandfull , which the threads will use to indicate when a buffer entry ha s\nbeen emptied or ﬁlled, respectively. The code for the put and get routines\nis in Figure 31.9, and our attempt at solving the producer and cons umer\nproblem is in Figure 31.10.\nIn this example, the producer ﬁrst waits for a buffer to become em pty\nin order to put data into it, and the consumer similarly waits for a buffer\nto become ﬁlled before using it. Let us ﬁrst imagine that MAX=1 (there is\nonly one buffer in the array), and see if this works.\nImagine again there are two threads, a producer and a consumer. Let\nus examine a speciﬁc scenario on a single CPU. Assume the consum er\ngets to run ﬁrst. Thus, the consumer will hit Line C1 in Figure 3 1.10,\ncallingsemwait(&full) . Because full was initialized to the value 0,\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEMAPHORES 7\n1int buffer[MAX];\n2int fill = 0;\n3int use = 0;\n4\n5void put(int value) {\n6buffer[fill] = value; // Line F1\n7fill = (fill + 1) % MAX; // Line F2\n8}\n9\n10int get() {\n11 int tmp = buffer[use]; // Line G1\n12 use = (use + 1) % MAX; // Line G2\n13 return tmp;\n14}\nFigure 31.9: The Put And Get Routines\n1sem_t empty;\n2sem_t full;\n3\n4void*producer(void *arg) {\n5int i;\n6for (i = 0; i < loops; i++) {\n7 sem_wait(&empty); // Line P1\n8 put(i); // Line P2\n9 sem_post(&full); // Line P3\n10 }\n11}\n12\n13void*consumer(void *arg) {\n14 int i, tmp = 0;\n15 while (tmp != -1) {\n16 sem_wait(&full); // Line C1\n17 tmp = get(); // Line C2\n18 sem_post(&empty); // Line C3\n19 printf(""%d\n"", tmp);\n20 }\n21}\n22\n23int main(int argc, char *argv[]) {\n24 // ...\n25 sem_init(&empty, 0, MAX); // MAX buffers are empty to begin w ith...\n26 sem_init(&full, 0, 0); // ... and 0 are full\n27 // ...\n28}\nFigure 31.10: Adding The Full And Empty Conditions\nthe call will decrement full (to -1), block the consumer, and wait for\nanother thread to call sempost() onfull , as desired.\nAssume the producer then runs. It will hit Line P1, thus callin g the\nsemwait(&empty) routine. Unlike the consumer, the producer will\ncontinue through this Line, because empty was initialized to t he value\nMAX (in this case, 1). Thus, empty will be decremented to 0 and the\nproducer will put a data value into the ﬁrst entry of buffer (Lin e P2). The\nproducer will then continue on to P3 and call sempost(&full) , chang-\ning the value of the full semaphore from -1 to 0 and waking the consu mer\n(e.g., move it from blocked to ready).\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 SEMAPHORES\nIn this case, one of two things could happen. If the producer contin -\nues to run, it will loop around and hit Line P1 again. This time, how -\never, it would block, as the empty semaphore’s value is 0. If the pr oducer\ninstead was interrupted and the consumer began to run, it would call\nsemwait(&full) (Line C1) and ﬁnd that the buffer was indeed full\nand thus consume it. In either case, we achieve the desired beh avior.\nYou can try this same example with more threads (e.g., multiple pro-\nducers, and multiple consumers). It should still work.\nLet us now imagine that MAX is greater than 1 (say MAX = 10). For this\nexample, let us assume that there are multiple producers and m ultiple\nconsumers. We now have a problem: a race condition. Do you see where\nit occurs? (take some time and look for it) If you can’t see it, here’s a h int:\nlook more closely at the put() and get() code.\nOK, let’s understand the issue. Imagine two producers (Pa and P b)\nboth calling into put() at roughly the same time. Assume produce r Pa gets\nto run ﬁrst, and just starts to ﬁll the ﬁrst buffer entry (ﬁll = 0 at Line F1).\nBefore Pa gets a chance to increment the ﬁll counter to 1, it is in terrupted.\nProducer Pb starts to run, and at Line F1 it also puts its data in to the\n0th element of buffer, which means that the old data there is over written!\nThis is a no-no; we don’t want any data from the producer to be lost.\nA Solution: Adding Mutual Exclusion\nAs you can see, what we’ve forgotten here is mutual exclusion . The\nﬁlling of a buffer and incrementing of the index into the buffer is a critical\nsection, and thus must be guarded carefully. So let’s use our frie nd the\nbinary semaphore and add some locks. Figure 31.11 shows our attemp t.\nNow we’ve added some locks around the entire put()/get() parts of\nthe code, as indicated by the NEW LINE comments. That seems like the\nright idea, but it also doesn’t work. Why? Deadlock. Why does deadl ock\noccur? Take a moment to consider it; try to ﬁnd a case where deadloc k\narises. What sequence of steps must happen for the program to dea dlock?\nAvoiding Deadlock\nOK, now that you ﬁgured it out, here is the answer. Imagine two thr eads,\none producer and one consumer. The consumer gets to run ﬁrst. It\nacquires the mutex (Line C0), and then calls semwait() on the full\nsemaphore (Line C1); because there is no data yet, this call ca uses the\nconsumer to block and thus yield the CPU; importantly, though, th e con-\nsumer still holds the lock.\nA producer then runs. It has data to produce and if it were able to run,\nit would be able to wake the consumer thread and all would be good. Un -\nfortunately, the ﬁrst thing it does is call semwait() on the binary mutex\nsemaphore (Line P0). The lock is already held. Hence, the produc er is\nnow stuck waiting too.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEMAPHORES 9\n1sem_t empty;\n2sem_t full;\n3sem_t mutex;\n4\n5void*producer(void *arg) {\n6int i;\n7for (i = 0; i < loops; i++) {\n8 sem_wait(&mutex); // Line P0 (NEW LINE)\n9 sem_wait(&empty); // Line P1\n10 put(i); // Line P2\n11 sem_post(&full); // Line P3\n12 sem_post(&mutex); // Line P4 (NEW LINE)\n13 }\n14}\n15\n16void*consumer(void *arg) {\n17 int i;\n18 for (i = 0; i < loops; i++) {\n19 sem_wait(&mutex); // Line C0 (NEW LINE)\n20 sem_wait(&full); // Line C1\n21 int tmp = get(); // Line C2\n22 sem_post(&empty); // Line C3\n23 sem_post(&mutex); // Line C4 (NEW LINE)\n24 printf(""%d\n"", tmp);\n25 }\n26}\n27\n28int main(int argc, char *argv[]) {\n29 // ...\n30 sem_init(&empty, 0, MAX); // MAX buffers are empty to begin w ith...\n31 sem_init(&full, 0, 0); // ... and 0 are full\n32 sem_init(&mutex, 0, 1); // mutex=1 because it is a lock (NEW L INE)\n33 // ...\n34}\nFigure 31.11: Adding Mutual Exclusion (Incorrectly)\nThere is a simple cycle here. The consumer holds the mutex and is\nwaiting for the someone to signal full. The producer could signal full but\niswaiting for the mutex. Thus, the producer and consumer are each stuck\nwaiting for each other: a classic deadlock.\nAt Last, A Working Solution\nTo solve this problem, we simply must reduce the scope of the lock. F ig-\nure 31.12 shows the correct solution. As you can see, we simply move t he\nmutex acquire and release to be just around the critical secti on; the full\nand empty wait and signal code is left outside. The result is a si mple and\nworking bounded buffer, a commonly-used pattern in multi-threa ded\nprograms. Understand it now; use it later. You will thank us for ye ars\nto come. Or at least, you will thank us when the same question is as ked\non the ﬁnal exam.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 SEMAPHORES\n1sem_t empty;\n2sem_t full;\n3sem_t mutex;\n4\n5void*producer(void *arg) {\n6int i;\n7for (i = 0; i < loops; i++) {\n8 sem_wait(&empty); // Line P1\n9 sem_wait(&mutex); // Line P1.5 (MOVED MUTEX HERE...)\n10 put(i); // Line P2\n11 sem_post(&mutex); // Line P2.5 (... AND HERE)\n12 sem_post(&full); // Line P3\n13 }\n14}\n15\n16void*consumer(void *arg) {\n17 int i;\n18 for (i = 0; i < loops; i++) {\n19 sem_wait(&full); // Line C1\n20 sem_wait(&mutex); // Line C1.5 (MOVED MUTEX HERE...)\n21 int tmp = get(); // Line C2\n22 sem_post(&mutex); // Line C2.5 (... AND HERE)\n23 sem_post(&empty); // Line C3\n24 printf(""%d\n"", tmp);\n25 }\n26}\n27\n28int main(int argc, char *argv[]) {\n29 // ...\n30 sem_init(&empty, 0, MAX); // MAX buffers are empty to begin w ith...\n31 sem_init(&full, 0, 0); // ... and 0 are full\n32 sem_init(&mutex, 0, 1); // mutex=1 because it is a lock\n33 // ...\n34}\nFigure 31.12: Adding Mutual Exclusion (Correctly)\n31.5 Reader-Writer Locks\nAnother classic problem stems from the desire for a more ﬂexible loc k-\ning primitive that admits that different data structure acc esses might re-\nquire different kinds of locking. For example, imagine a number of con-\ncurrent list operations, including inserts and simple lookups. While in-\nserts change the state of the list (and thus a traditional criti cal section\nmakes sense), lookups simply read the data structure; as long as we can\nguarantee that no insert is on-going, we can allow many lookups to p ro-\nceed concurrently. The special type of lock we will now develop to s up-\nport this type of operation is known as a reader-writer lock [CHP71]. The\ncode for such a lock is available in Figure 31.13.\nThe code is pretty simple. If some thread wants to update the dat a\nstructure in question, it should call the new pair of synchroniza tion op-\nerations:rwlockacquire writelock() , to acquire a write lock, and\nrwlockrelease writelock() , to release it. Internally, these simply\nuse thewritelock semaphore to ensure that only a single writer can ac-\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEMAPHORES 11\n1typedef struct _rwlock_t {\n2sem_t lock; // binary semaphore (basic lock)\n3sem_t writelock; // used to allow ONE writer or MANY readers\n4int readers; // count of readers reading in critical section\n5} rwlock_t;\n6\n7void rwlock_init(rwlock_t *rw) {\n8rw->readers = 0;\n9sem_init(&rw->lock, 0, 1);\n10sem_init(&rw->writelock, 0, 1);\n11}\n12\n13void rwlock_acquire_readlock(rwlock_t *rw) {\n14sem_wait(&rw->lock);\n15rw->readers++;\n16if (rw->readers == 1)\n17 sem_wait(&rw->writelock); // first reader acquires write lock\n18sem_post(&rw->lock);\n19}\n20\n21void rwlock_release_readlock(rwlock_t *rw) {\n22sem_wait(&rw->lock);\n23rw->readers--;\n24if (rw->readers == 0)\n25 sem_post(&rw->writelock); // last reader releases writel ock\n26sem_post(&rw->lock);\n27}\n28\n29void rwlock_acquire_writelock(rwlock_t *rw) {\n30sem_wait(&rw->writelock);\n31}\n32\n33void rwlock_release_writelock(rwlock_t *rw) {\n34sem_post(&rw->writelock);\n35}\nFigure 31.13: A Simple Reader-Writer Lock\nquire the lock and thus enter the critical section to update the data struc-\nture in question.\nMore interesting is the pair of routines to acquire and release r ead\nlocks. When acquiring a read lock, the reader ﬁrst acquires lock and\nthen increments the readers variable to track how many readers are\ncurrently inside the data structure. The important step then taken within\nrwlockacquire readlock() occurs when the ﬁrst reader acquires\nthe lock; in that case, the reader also acquires the write lock b y calling\nsemwait() on thewritelock semaphore, and then releasing the lock\nby calling sempost() .\nThus, once a reader has acquired a read lock, more readers will be\nallowed to acquire the read lock too; however, any thread that wish es to\nacquire the write lock will have to wait until allreaders are ﬁnished; the\nlast one to exit the critical section calls sempost() on “writelock” and\nthus enables a waiting writer to acquire the lock.\nThis approach works (as desired), but does have some negatives, e spe-\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 SEMAPHORES\nTIP: SIMPLE ANDDUMB CANBEBETTER (HILL’SLAW)\nYou should never underestimate the notion that the simple and dum b\napproach can be the best one. With locking, sometimes a simple spi n lock\nworks best, because it is easy to implement and fast. Although s omething\nlike reader/writer locks sounds cool, they are complex, and comple x can\nmean slow. Thus, always try the simple and dumb approach ﬁrst.\nThis idea, of appealing to simplicity, is found in many places. One early\nsource is Mark Hill’s dissertation [H87], which studied how to de sign\ncaches for CPUs. Hill found that simple direct-mapped caches w orked\nbetter than fancy set-associative designs (one reason is that i n caching,\nsimpler designs enable faster lookups). As Hill succinctly su mmarized\nhis work: “Big and dumb is better.” And thus we call this simila r advice\nHill’s Law .\ncially when it comes to fairness. In particular, it would be rel atively easy\nfor readers to starve writers. More sophisticated solutions to th is prob-\nlem exist; perhaps you can think of a better implementation? Hin t: think\nabout what you would need to do to prevent more readers from enterin g\nthe lock once a writer is waiting.\nFinally, it should be noted that reader-writer locks should be us ed\nwith some caution. They often add more overhead (especially with m ore\nsophisticated implementations), and thus do not end up speedin g up\nperformance as compared to just using simple and fast locking pr imi-\ntives [CB08]. Either way, they showcase once again how we can use\nsemaphores in an interesting and useful way.\n31.6 The Dining Philosophers\nOne of the most famous concurrency problems posed, and solved, by\nDijkstra, is known as the dining philosopher’s problem [D71]. The prob-\nlem is famous because it is fun and somewhat intellectually int eresting;\nhowever, its practical utility is low. However, its fame forces i ts inclu-\nsion here; indeed, you might be asked about it on some interview, an d\nyou’d really hate your OS professor if you miss that question and don’t\nget the job. Conversely, if you get the job, please feel free to sen d your OS\nprofessor a nice note, or some stock options.\nThe basic setup for the problem is this (as shown in Figure 31.14) : as-\nsume there are ﬁve “philosophers” sitting around a table. Betwe en each\npair of philosophers is a single fork (and thus, ﬁve total). The phi loso-\nphers each have times where they think, and don’t need any forks, and\ntimes where they eat. In order to eat, a philosopher needs two fork s, both\nthe one on their left and the one on their right. The contention for the se\nforks, and the synchronization problems that ensue, are what mak es this\na problem we study in concurrent programming.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEMAPHORES 13\nP0P1\nP2\nP3\nP4f0f1f2\nf3\nf4\nFigure 31.14: The Dining Philosophers\nHere is the basic loop of each philosopher:\nwhile (1) {\nthink();\ngetforks();\neat();\nputforks();\n}\nThe key challenge, then, is to write the routines getforks() and\nputforks() such that there is no deadlock, no philosopher starves and\nnever gets to eat, and concurrency is high (i.e., as many philos ophers can\neat at the same time as possible).\nFollowing Downey’s solutions [D08], we’ll use a few helper functions\nto get us towards a solution. They are:\nint left(int p) { return p; }\nint right(int p) { return (p + 1) % 5; }\nWhen philosopher pwishes to refer to the fork on their left, they sim-\nply callleft(p) . Similarly, the fork on the right of a philosopher pis\nreferred to by calling right(p) ; the modulo operator therein handles\nthe one case where the last philosopher ( p=4) tries to grab the fork on\ntheir right, which is fork 0.\nWe’ll also need some semaphores to solve this problem. Let us assum e\nwe have ﬁve, one for each fork: semt forks[5] .\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 SEMAPHORES\n1void getforks() {\n2sem_wait(forks[left(p)]);\n3sem_wait(forks[right(p)]);\n4}\n5\n6void putforks() {\n7sem_post(forks[left(p)]);\n8sem_post(forks[right(p)]);\n9}\nFigure 31.15: Thegetforks() Andputforks() Routines\nBroken Solution\nWe attempt our ﬁrst solution to the problem. Assume we initialize each\nsemaphore (in the forks array) to a value of 1. Assume also that each\nphilosopher knows its own number ( p). We can thus write the getforks()\nandputforks() routine as shown in Figure 31.15.\nThe intuition behind this (broken) solution is as follows. To acqui re\nthe forks, we simply grab a “lock” on each one: ﬁrst the one on the left ,\nand then the one on the right. When we are done eating, we release t hem.\nSimple, no? Unfortunately, in this case, simple means broken. Ca n you\nsee the problem that arises? Think about it.\nThe problem is deadlock . If each philosopher happens to grab the fork\non their left before any philosopher can grab the fork on their right , each\nwill be stuck holding one fork and waiting for another, forever. Spec iﬁ-\ncally, philosopher 0 grabs fork 0, philosopher 1 grabs fork 1, philos opher\n2 grabs fork 2, philosopher 3 grabs fork 3, and philosopher 4 grabs for k 4;\nall the forks are acquired, and all the philosophers are stuck wa iting for\na fork that another philosopher possesses. We’ll study deadlock in m ore\ndetail soon; for now, it is safe to say that this is not a working soluti on.\nA Solution: Breaking The Dependency\nThe simplest way to attack this problem is to change how forks are ac-\nquired by at least one of the philosophers; indeed, this is how Dijk stra\nhimself solved the problem. Speciﬁcally, let’s assume that phil osopher\n4 (the highest numbered one) acquires the forks in a different order. The\ncode to do so is as follows:\n1void getforks() {\n2if (p == 4) {\n3sem_wait(forks[right(p)]);\n4sem_wait(forks[left(p)]);\n5} else {\n6sem_wait(forks[left(p)]);\n7sem_wait(forks[right(p)]);\n8}\n9}\nBecause the last philosopher tries to grab right before left, th ere is no\nsituation where each philosopher grabs one fork and is stuck waiti ng for\nanother; the cycle of waiting is broken. Think through the ramiﬁc ations\nof this solution, and convince yourself that it works.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEMAPHORES 15\n1typedef struct __Zem_t {\n2int value;\n3pthread_cond_t cond;\n4pthread_mutex_t lock;\n5} Zem_t;\n6\n7// only one thread can call this\n8void Zem_init(Zem_t *s, int value) {\n9s->value = value;\n10 Cond_init(&s->cond);\n11 Mutex_init(&s->lock);\n12}\n13\n14void Zem_wait(Zem_t *s) {\n15 Mutex_lock(&s->lock);\n16 while (s->value <= 0)\n17 Cond_wait(&s->cond, &s->lock);\n18 s->value--;\n19 Mutex_unlock(&s->lock);\n20}\n21\n22void Zem_post(Zem_t *s) {\n23 Mutex_lock(&s->lock);\n24 s->value++;\n25 Cond_signal(&s->cond);\n26 Mutex_unlock(&s->lock);\n27}\nFigure 31.16: Implementing Zemaphores With Locks And CVs\nThere are other “famous” problems like this one, e.g., the cigarette\nsmoker’s problem or the sleeping barber problem . Most of them are\njust excuses to think about concurrency; some of them have fascin ating\nnames. Look them up if you are interested in learning more, or just g et-\nting more practice thinking in a concurrent manner [D08].\n31.7 How To Implement Semaphores\nFinally, let’s use our low-level synchronization primitives, loc ks and\ncondition variables, to build our own version of semaphores called . ..\n(drum roll here) ...Zemaphores . This task is fairly straightforward, as\nyou can see in Figure 31.16.\nAs you can see from the ﬁgure, we use just one lock and one condition\nvariable, plus a state variable to track the value of the semap hore. Study\nthe code for yourself until you really understand it. Do it!\nOne subtle difference between our Zemaphore and pure semaphore s\nas deﬁned by Dijkstra is that we don’t maintain the invariant th at the\nvalue of the semaphore, when negative, reﬂects the number of wai ting\nthreads; indeed, the value will never be lower than zero. This b ehavior is\neasier to implement and matches the current Linux implement ation.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 SEMAPHORES\nTIP: BECAREFUL WITHGENERALIZATION\nThe abstract technique of generalization can thus be quite use ful in sys-\ntems design, where one good idea can be made slightly broader and t hus\nsolve a larger class of problems. However, be careful when genera lizing;\nas Lampson warns us “Don’t generalize; generalizations are gene rally\nwrong” [L83].\nOne could view semaphores as a generalization of locks and condition\nvariables; however, is such a generalization needed? And, giv en the dif-\nﬁculty of realizing a condition variable on top of a semaphore, perha ps\nthis generalization is not as general as you might think.\nCuriously, building condition variables out of semaphores is a muc h\ntrickier proposition. Some highly experienced concurrent program mers\ntried to do this in the Windows environment, and many different bugs\nensued [B04]. Try it yourself, and see if you can ﬁgure out why bui lding\ncondition variables out of semaphores is more challenging than it m ight\nappear.\n31.8 Summary\nSemaphores are a powerful and ﬂexible primitive for writing concu r-\nrent programs. Some programmers use them exclusively, shunning locks\nand condition variables, due to their simplicity and utility.\nIn this chapter, we have presented just a few classic problems and solu-\ntions. If you are interested in ﬁnding out more, there are many othe r ma-\nterials you can reference. One great (and free reference) is A llen Downey’s\nbook on concurrency and programming with semaphores [D08]. This\nbook has lots of puzzles you can work on to improve your understand-\ning of both semaphores in speciﬁc and concurrency in general. Bec oming\na real concurrency expert takes years of effort; going beyond what you\nlearn in this class is undoubtedly the key to mastering such a t opic.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSEMAPHORES 17\nReferences\n[B04] “Implementing Condition Variables with Semaphores” by Andr ew Birrell. December\n2004. An interesting read on how difﬁcult implementing CVs on top of semaphores r eally is, and the\nmistakes the author and co-workers made along the way. Particularly relevant because the group had\ndone a ton of concurrent programming; Birrell, for example, is known for (amon g other things) writing\nvarious thread-programming guides.\n[CB08] “Real-world Concurrency” by Bryan Cantrill, Jeff Bonwick. ACM Qu eue. Volume 6,\nNo. 5. September 2008. A nice article by some kernel hackers from a company formerly known as Sun\non the real problems faced in concurrent code.\n[CHP71] “Concurrent Control with Readers and Writers” by P .J. Courto is, F. Heymans, D.L.\nParnas. Communications of the ACM, 14:10, October 1971. The introduction of the reader-writer\nproblem, and a simple solution. Later work introduced more complex solutions , skipped here because,\nwell, they are pretty complex.\n[D59] “A Note on Two Problems in Connexion with Graphs” by E. W. Dijk stra. Numerische\nMathematik 1, 269271, 1959. Available: http://www-m3.ma.tum.de/twiki/pub/MN0506/\nWebHome/dijkstra.pdf .Can you believe people worked on algorithms in 1959? We can’t. Even\nbefore computers were any fun to use, these people had a sense that they woul d transform the world...\n[D68a] “Go-to Statement Considered Harmful” by E.W. Dijkstra. CA CM, volume 11(3), March\n1968.http://www.cs.utexas.edu/users/EWD/ewd02xx/EWD215.P DF.Sometimes thought\nas the beginning of the ﬁeld of software engineering.\n[D68b] “The Structure of the THE Multiprogramming System” by E.W. Dij kstra. CACM, vol-\nume 11(5), 1968. One of the earliest papers to point out that systems work in computer science is an\nengaging intellectual endeavor. Also argues strongly for modularity in the form of layered systems.\n[D72] “Information Streams Sharing a Finite Buffer” by E.W. Dijkstr a. Information Processing\nLetters 1, 1972. http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD329.P DF.Did\nDijkstra invent everything? No, but maybe close. He certainly was the ﬁr st to clearly write down what\nthe problems were in concurrent code. However, it is true that practitione rs in operating system design\nknew of many of the problems described by Dijkstra, so perhaps giving him too much credit would be a\nmisrepresentation of history.\n[D08] “The Little Book of Semaphores” by A.B. Downey. Available at the following site:\nhttp://greenteapress.com/semaphores/ .A nice (and free!) book about semaphores. Lots\nof fun problems to solve, if you like that sort of thing.\n[D71] “Hierarchical ordering of sequential processes” by E.W. Dijk stra. Available online here:\nhttp://www.cs.utexas.edu/users/EWD/ewd03xx/EWD310.P DF.Presents numerous con-\ncurrency problems, including the Dining Philosophers. The wikip edia page about this problem is also\nquite informative.\n[GR92] “Transaction Processing: Concepts and Techniques” by Jim Gray, Andr eas Reuter.\nMorgan Kaufmann, September 1992. The exact quote that we ﬁnd particularly humorous is found\non page 485, at the top of Section 8.8: “The ﬁrst multiprocessors, circa 196 0, had test and set instruc-\ntions ... presumably the OS implementors worked out the appropriate algorithms , although Dijkstra is\ngenerally credited with inventing semaphores many years later.” Oh, sn ap!\n[H87] “Aspects of Cache Memory and Instruction Buffer Performance” by Mark D . Hill. Ph.D.\nDissertation, U.C. Berkeley, 1987. Hill’s dissertation work, for those obsessed with caching in early\nsystems. A great example of a quantitative dissertation.\n[L83] “Hints for Computer Systems Design” by Butler Lampson. ACM Op erating Systems\nReview, 15:5, October 1983. Lampson, a famous systems researcher, loved using hints in the design\nof computer systems. A hint is something that is often correct but can be wron g; in this use, a signal()\nis telling a waiting thread that it changed the condition that the waiter was waiting on, but not to trust\nthat the condition will be in the desired state when the waiting thread wakes up. In this paper about\nhints for designing systems, one of Lampson’s general hints is that you shou ld use hints. It is not as\nconfusing as it sounds.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n18 SEMAPHORES\nHomework (Code)\nIn this homework, we’ll use semaphores to solve some well-known\nconcurrency problems. Many of these are taken from Downey’s excell ent\n“Little Book of Semaphores”2, which does a good job of pulling together\na number of classic problems as well as introducing a few new vari ants;\ninterested readers should check out the Little Book for more fun.\nEach of the following questions provides a code skeleton; your job is\nto ﬁll in the code to make it work given semaphores. On Linux, you\nwill be using native semaphores; on a Mac (where there is no sema phore\nsupport), you’ll have to ﬁrst build an implementation (using lock s and\ncondition variables, as described in the chapter). Good luck!\nQuestions\n1. The ﬁrst problem is just to implement and test a solution to the fork/join\nproblem , as described in the text. Even though this solution is describe d in\nthe text, the act of typing it in on your own is worthwhile; even B ach would\nrewrite Vivaldi, allowing one soon-to-be master to learn fro m an existing\none. Seefork-join.c for details. Add the call sleep(1) to the child to\nensure it is working.\n2. Let’s now generalize this a bit by investigating the rendezvous problem .\nThe problem is as follows: you have two threads, each of which are about\nto enter the rendezvous point in the code. Neither should exit th is part of\nthe code before the other enters it. Consider using two semapho res for this\ntask, and see rendezvous.c for details.\n3. Now go one step further by implementing a general solution to barrier syn-\nchronization . Assume there are two points in a sequential piece of code,\ncalledP1andP2. Putting a barrier between P1andP2guarantees that all\nthreads will execute P1before any one thread executes P2. Your task: write\nthe code to implement a barrier() function that can be used in this man-\nner. It is safe to assume you know N(the total number of threads in the\nrunning program) and that all Nthreads will try to enter the barrier. Again,\nyou should likely use two semaphores to achieve the solution, and some\nother integers to count things. See barrier.c for details.\n4. Now let’s solve the reader-writer problem , also as described in the text. In\nthis ﬁrst take, don’t worry about starvation. See the code in reader-writer.c\nfor details. Add sleep() calls to your code to demonstrate it works as you\nexpect. Can you show the existence of the starvation problem?\n5. Let’s look at the reader-writer problem again, but this time , worry about\nstarvation. How can you ensure that all readers and writers ev entually\nmake progress? See reader-writer-nostarve.c for details.\n6. Use semaphores to build a no-starve mutex , in which any thread that tries to\nacquire the mutex will eventually obtain it. See the code in mutex-nostarve.c\nfor more information.\n7. Liked these problems? See Downey’s free text for more just like them. And\ndon’t forget, have fun! But, you always do when you write code, n o?\n2Available: http://greenteapress.com/semaphores/downey08semapho res.pdf .\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",42078
37-32. Concurrency Bugs.pdf,37-32. Concurrency Bugs,"32\nCommon Concurrency Problems\nResearchers have spent a great deal of time and effort looking int o con-\ncurrency bugs over many years. Much of the early work focused on\ndeadlock , a topic which we’ve touched on in the past chapters but will\nnow dive into deeply [C+71]. More recent work focuses on studying\nother types of common concurrency bugs (i.e., non-deadlock bugs). I n\nthis chapter, we take a brief look at some example concurrency prob lems\nfound in real code bases, to better understand what problems to l ook out\nfor. And thus our central issue for this chapter:\nCRUX: HOWTOHANDLE COMMON CONCURRENCY BUGS\nConcurrency bugs tend to come in a variety of common patterns.\nKnowing which ones to look out for is the ﬁrst step to writing more ro-\nbust, correct concurrent code.\n32.1 What Types Of Bugs Exist?\nThe ﬁrst, and most obvious, question is this: what types of concur-\nrency bugs manifest in complex, concurrent programs? This ques tion is\ndifﬁcult to answer in general, but fortunately, some others hav e done the\nwork for us. Speciﬁcally, we rely upon a study by Lu et al. [L+08], w hich\nanalyzes a number of popular concurrent applications in great de tail to\nunderstand what types of bugs arise in practice.\nThe study focuses on four major and important open-source applica-\ntions: MySQL (a popular database management system), Apache (a well-\nknown web server), Mozilla (the famous web browser), and OpenOfﬁ ce\n(a free version of the MS Ofﬁce suite, which some people actually u se).\nIn the study, the authors examine concurrency bugs that have be en found\nand ﬁxed in each of these code bases, turning the developers’ work into a\nquantitative bug analysis; understanding these results ca n help you un-\nderstand what types of problems actually occur in mature code bas es.\n1\n2 COMMON CONCURRENCY PROBLEMS\nApplication What it does Non-Deadlock Deadlock\nMySQL Database Server 14 9\nApache Web Server 13 4\nMozilla Web Browser 41 16\nOpenOfﬁce Ofﬁce Suite 6 2\nTotal 74 31\nFigure 32.1: Bugs In Modern Applications\nFigure 32.1 shows a summary of the bugs Lu and colleagues studied .\nFrom the ﬁgure, you can see that there were 105 total bugs, most of wh ich\nwere not deadlock (74); the remaining 31 were deadlock bugs. Fur ther,\nyou can see the number of bugs studied from each application; whil e\nOpenOfﬁce only had 8 total concurrency bugs, Mozilla had nearly 6 0.\nWe now dive into these different classes of bugs (non-deadlock, d ead-\nlock) a bit more deeply. For the ﬁrst class of non-deadlock bugs, we u se\nexamples from the study to drive our discussion. For the second cla ss of\ndeadlock bugs, we discuss the long line of work that has been done in\neither preventing, avoiding, or handling deadlock.\n32.2 Non-Deadlock Bugs\nNon-deadlock bugs make up a majority of concurrency bugs, accord-\ning to Lu’s study. But what types of bugs are these? How do they ari se?\nHow can we ﬁx them? We now discuss the two major types of non-\ndeadlock bugs found by Lu et al.: atomicity violation bugs and order\nviolation bugs.\nAtomicity-Violation Bugs\nThe ﬁrst type of problem encountered is referred to as an atomicity vi-\nolation . Here is a simple example, found in MySQL. Before reading the\nexplanation, try ﬁguring out what the bug is. Do it!\n1Thread 1::\n2if (thd->proc_info) {\n3...\n4fputs(thd->proc_info, ...);\n5...\n6}\n7\n8Thread 2::\n9thd->proc_info = NULL;\nIn the example, two different threads access the ﬁeld procinfo in\nthe structure thd. The ﬁrst thread checks if the value is non-NULL and\nthen prints its value; the second thread sets it to NULL. Clear ly, if the\nﬁrst thread performs the check but then is interrupted before t he call to\nfputs , the second thread could run in-between, thus setting the point er\nto NULL; when the ﬁrst thread resumes, it will crash, as a NULL pointer\nwill be dereferenced by fputs .\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCOMMON CONCURRENCY PROBLEMS 3\nThe more formal deﬁnition of an atomicity violation, according to Lu\net al, is this: “The desired serializability among multiple m emory accesses\nis violated (i.e. a code region is intended to be atomic, but the at omicity\nis not enforced during execution).” In our example above, the code h as\nanatomicity assumption (in Lu’s words) about the check for non-NULL of\nprocinfo and the usage of procinfo in thefputs() call; when the\nassumption is incorrect, the code will not work as desired.\nFinding a ﬁx for this type of problem is often (but not always) strai ght-\nforward. Can you think of how to ﬁx the code above?\nIn this solution, we simply add locks around the shared-variable ref-\nerences, ensuring that when either thread accesses the procinfo ﬁeld,\nit has a lock held ( procinfolock ). Of course, any other code that ac-\ncesses the structure should also acquire this lock before doing s o.\n1pthread_mutex_t proc_info_lock = PTHREAD_MUTEX_INITIAL IZER;\n2\n3Thread 1::\n4pthread_mutex_lock(&proc_info_lock);\n5if (thd->proc_info) {\n6...\n7fputs(thd->proc_info, ...);\n8...\n9}\n10pthread_mutex_unlock(&proc_info_lock);\n11\n12Thread 2::\n13pthread_mutex_lock(&proc_info_lock);\n14thd->proc_info = NULL;\n15pthread_mutex_unlock(&proc_info_lock);\nOrder-Violation Bugs\nAnother common type of non-deadlock bug found by Lu et al. is known\nas an order violation . Here is another simple example; once again, see if\nyou can ﬁgure out why the code below has a bug in it.\n1Thread 1::\n2void init() {\n3...\n4mThread = PR_CreateThread(mMain, ...);\n5...\n6}\n7\n8Thread 2::\n9void mMain(...) {\n10 ...\n11 mState = mThread->State;\n12 ...\n13}\nAs you probably ﬁgured out, the code in Thread 2 seems to assume\nthat the variable mThread has already been initialized (and is not NULL);\nhowever, if Thread 2 runs immediately once created, the value of mThread\nwill not be set when it is accessed within mMain() in Thread 2, and will\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 COMMON CONCURRENCY PROBLEMS\nlikely crash with a NULL-pointer dereference. Note that we ass ume the\nvalue ofmThread is initially NULL; if not, even stranger things could\nhappen as arbitrary memory locations are accessed through the de refer-\nence in Thread 2.\nThe more formal deﬁnition of an order violation is this: “The desired\norder between two (groups of) memory accesses is ﬂipped (i.e., Ashould\nalways be executed before B, but the order is not enforced during execu-\ntion)” [L+08].\nThe ﬁx to this type of bug is generally to enforce ordering. As we\ndiscussed in detail previously, using condition variables is an easy and\nrobust way to add this style of synchronization into modern code bas es.\nIn the example above, we could thus rewrite the code as follows:\n1pthread_mutex_t mtLock = PTHREAD_MUTEX_INITIALIZER;\n2pthread_cond_t mtCond = PTHREAD_COND_INITIALIZER;\n3int mtInit = 0;\n4\n5Thread 1::\n6void init() {\n7...\n8mThread = PR_CreateThread(mMain, ...);\n9\n10// signal that the thread has been created...\n11pthread_mutex_lock(&mtLock);\n12mtInit = 1;\n13pthread_cond_signal(&mtCond);\n14pthread_mutex_unlock(&mtLock);\n15...\n16}\n17\n18Thread 2::\n19void mMain(...) {\n20 ...\n21 // wait for the thread to be initialized...\n22 pthread_mutex_lock(&mtLock);\n23 while (mtInit == 0)\n24 pthread_cond_wait(&mtCond, &mtLock);\n25 pthread_mutex_unlock(&mtLock);\n26\n27 mState = mThread->State;\n28 ...\n29}\nIn this ﬁxed-up code sequence, we have added a lock ( mtLock ) and\ncorresponding condition variable ( mtCond ), as well as a state variable\n(mtInit ). When the initialization code runs, it sets the state of mtInit\nto 1 and signals that it has done so. If Thread 2 had run before this point,\nit will be waiting for this signal and corresponding state chang e; if it runs\nlater, it will check the state and see that the initialization has already oc-\ncurred (i.e., mtInit is set to 1), and thus continue as is proper. Note that\nwe could likely use mThread as the state variable itself, but do not do so\nfor the sake of simplicity here. When ordering matters between t hreads,\ncondition variables (or semaphores) can come to the rescue.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCOMMON CONCURRENCY PROBLEMS 5\nNon-Deadlock Bugs: Summary\nA large fraction (97%) of non-deadlock bugs studied by Lu et al. ar e either\natomicity or order violations. Thus, by carefully thinking about t hese\ntypes of bug patterns, programmers can likely do a better job of av oiding\nthem. Moreover, as more automated code-checking tools develop, they\nshould likely focus on these two types of bugs as they constitute su ch a\nlarge fraction of non-deadlock bugs found in deployment.\nUnfortunately, not all bugs are as easily ﬁxed as the examples w e\nlooked at above. Some require a deeper understanding of what the pro-\ngram is doing, or a larger amount of code or data structure reorganiza tion\nto ﬁx. Read Lu et al.’s excellent (and readable) paper for more de tails.\n32.3 Deadlock Bugs\nBeyond the concurrency bugs mentioned above, a classic problem th at\narises in many concurrent systems with complex locking protocols i s known\nasdeadlock . Deadlock occurs, for example, when a thread (say Thread\n1) is holding a lock ( L1) and waiting for another one ( L2); unfortunately,\nthe thread (Thread 2) that holds lock L2is waiting for L1to be released.\nHere is a code snippet that demonstrates such a potential deadloc k:\nThread 1: Thread 2:\npthread_mutex_lock(L1); pthread_mutex_lock(L2);\npthread_mutex_lock(L2); pthread_mutex_lock(L1);\nNote that if this code runs, deadlock does not necessarily occur; ra ther,\nit may occur, if, for example, Thread 1 grabs lock L1and then a context\nswitch occurs to Thread 2. At that point, Thread 2 grabs L2, and tries to\nacquireL1. Thus we have a deadlock, as each thread is waiting for the\nother and neither can run. See Figure 32.2 for a graphical depict ion; the\npresence of a cycle in the graph is indicative of the deadlock.\nThe ﬁgure should make the problem clear. How should programmers\nwrite code so as to handle deadlock in some way?\nCRUX: HOWTODEAL WITHDEADLOCK\nHow should we build systems to prevent, avoid, or at least detect a nd\nrecover from deadlock? Is this a real problem in systems today?\nWhy Do Deadlocks Occur?\nAs you may be thinking, simple deadlocks such as the one above seem\nreadily avoidable. For example, if Thread 1 and 2 both made sure t o grab\nlocks in the same order, the deadlock would never arise. So why do de ad-\nlocks happen?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 COMMON CONCURRENCY PROBLEMS\nThread 1\nThread 2Lock L1\nLock L2\nHoldsHoldsWanted byWanted by\nFigure 32.2: The Deadlock Dependency Graph\nOne reason is that in large code bases, complex dependencies ari se\nbetween components. Take the operating system, for example. The vir-\ntual memory system might need to access the ﬁle system in order t o page\nin a block from disk; the ﬁle system might subsequently require a page\nof memory to read the block into and thus contact the virtual memory\nsystem. Thus, the design of locking strategies in large system s must be\ncarefully done to avoid deadlock in the case of circular dependen cies that\nmay occur naturally in the code.\nAnother reason is due to the nature of encapsulation . As software de-\nvelopers, we are taught to hide details of implementations and t hus make\nsoftware easier to build in a modular way. Unfortunately, such m odular-\nity does not mesh well with locking. As Jula et al. point out [J+08], some\nseemingly innocuous interfaces almost invite you to deadlock. For exam-\nple, take the Java Vector class and the method AddAll() . This routine\nwould be called as follows:\nVector v1, v2;\nv1.AddAll(v2);\nInternally, because the method needs to be multi-thread safe , locks for\nboth the vector being added to (v1) and the parameter (v2) need t o be\nacquired. The routine acquires said locks in some arbitrary orde r (say v1\nthen v2) in order to add the contents of v2 to v1. If some other thread\ncallsv2.AddAll(v1) at nearly the same time, we have the potential for\ndeadlock, all in a way that is quite hidden from the calling appl ication.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCOMMON CONCURRENCY PROBLEMS 7\nConditions for Deadlock\nFour conditions need to hold for a deadlock to occur [C+71]:\n•Mutual exclusion: Threads claim exclusive control of resources that\nthey require (e.g., a thread grabs a lock).\n•Hold-and-wait: Threads hold resources allocated to them (e.g., locks\nthat they have already acquired) while waiting for additional re-\nsources (e.g., locks that they wish to acquire).\n•No preemption: Resources (e.g., locks) cannot be forcibly removed\nfrom threads that are holding them.\n•Circular wait: There exists a circular chain of threads such that each\nthread holds one or more resources (e.g., locks) that are being re-\nquested by the next thread in the chain.\nIf any of these four conditions are not met, deadlock cannot occur.\nThus, we ﬁrst explore techniques to prevent deadlock; each of these strate-\ngies seeks to prevent one of the above conditions from arising and th us is\none approach to handling the deadlock problem.\nPrevention\nCircular Wait\nProbably the most practical prevention technique (and certain ly one that\nis frequently employed) is to write your locking code such that you never\ninduce a circular wait. The most straightforward way to do that is to pro-\nvide a total ordering on lock acquisition. For example, if there are only\ntwo locks in the system ( L1andL2), you can prevent deadlock by always\nacquiringL1beforeL2. Such strict ordering ensures that no cyclical wait\narises; hence, no deadlock.\nOf course, in more complex systems, more than two locks will ex-\nist, and thus total lock ordering may be difﬁcult to achieve (and per-\nhaps is unnecessary anyhow). Thus, a partial ordering can be a useful\nway to structure lock acquisition so as to avoid deadlock. An exce llent\nreal example of partial lock ordering can be seen in the memory map -\nping code in Linux [T+94]; the comment at the top of the source code\nreveals ten different groups of lock acquisition orders, includi ng simple\nones such as “ imutex beforeimmapmutex ” and more complex orders\nsuch as “immapmutex beforeprivate lock beforeswaplock before\nmapping->tree lock ”.\nAs you can imagine, both total and partial ordering require caref ul\ndesign of locking strategies and must be constructed with great care. Fur-\nther, ordering is just a convention, and a sloppy programmer can ea sily\nignore the locking protocol and potentially cause deadlock. Finall y, lock\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 COMMON CONCURRENCY PROBLEMS\nTIP: ENFORCE LOCK ORDERING BYLOCK ADDRESS\nIn some cases, a function must grab two (or more) locks; thus, we know\nwe must be careful or deadlock could arise. Imagine a function tha t is\ncalled as follows: dosomething(mutex t*m1, mutex t*m2). If\nthe code always grabs m1beforem2(or always m2beforem1), it could\ndeadlock, because one thread could call dosomething(L1, L2) while\nanother thread could call dosomething(L2, L1) .\nTo avoid this particular issue, the clever programmer can use t headdress\nof each lock as a way of ordering lock acquisition. By acquiring locks in\neither high-to-low or low-to-high address order, dosomething() can\nguarantee that it always acquires locks in the same order, rega rdless of\nwhich order they are passed in. The code would look something like th is:\nif (m1 > m2) { // grab locks in high-to-low address order\npthread_mutex_lock(m1);\npthread_mutex_lock(m2);\n} else {\npthread_mutex_lock(m2);\npthread_mutex_lock(m1);\n}\n// Code assumes that m1 != m2 (it is not the same lock)\nBy using this simple technique, a programmer can ensure a simp le and\nefﬁcient deadlock-free implementation of multi-lock acquisit ion.\nordering requires a deep understanding of the code base, and how v ari-\nous routines are called; just one mistake could result in the “D” w ord1.\nHold-and-wait\nThe hold-and-wait requirement for deadlock can be avoided by acq uiring\nall locks at once, atomically. In practice, this could be achieve d as follows:\n1pthread_mutex_lock(prevention); // begin lock acquisiti on\n2pthread_mutex_lock(L1);\n3pthread_mutex_lock(L2);\n4...\n5pthread_mutex_unlock(prevention); // end\nBy ﬁrst grabbing the lock prevention , this code guarantees that no\nuntimely thread switch can occur in the midst of lock acquisition and thus\ndeadlock can once again be avoided. Of course, it requires that an y time\nany thread grabs a lock, it ﬁrst acquires the global prevention l ock. For\nexample, if another thread was trying to grab locks L1andL2in a dif-\nferent order, it would be OK, because it would be holding the preve ntion\nlock while doing so.\n1Hint: “D” stands for “Deadlock”.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCOMMON CONCURRENCY PROBLEMS 9\nNote that the solution is problematic for a number of reasons. As\nbefore, encapsulation works against us: when calling a routine, this ap-\nproach requires us to know exactly which locks must be held and to ac-\nquire them ahead of time. This technique also is likely to decr ease con-\ncurrency as all locks must be acquired early on (at once) instead of when\nthey are truly needed.\nNo Preemption\nBecause we generally view locks as held until unlock is called, multiple\nlock acquisition often gets us into trouble because when waiting for one\nlock we are holding another. Many thread libraries provide a more ﬂ ex-\nible set of interfaces to help avoid this situation. Speciﬁcally , the routine\npthread mutextrylock() either grabs the lock (if it is available) and\nreturns success or returns an error code indicating the lock is he ld; in the\nlatter case, you can try again later if you want to grab that lock.\nSuch an interface could be used as follows to build a deadlock-free ,\nordering-robust lock acquisition protocol:\n1top:\n2pthread_mutex_lock(L1);\n3if (pthread_mutex_trylock(L2) != 0) {\n4pthread_mutex_unlock(L1);\n5goto top;\n6}\nNote that another thread could follow the same protocol but grab the\nlocks in the other order ( L2thenL1) and the program would still be dead-\nlock free. One new problem does arise, however: livelock . It is possible\n(though perhaps unlikely) that two threads could both be repeat edly at-\ntempting this sequence and repeatedly failing to acquire bot h locks. In\nthis case, both systems are running through this code sequence ov er and\nover again (and thus it is not a deadlock), but progress is not being made,\nhence the name livelock. There are solutions to the livelock probl em, too:\nfor example, one could add a random delay before looping back and try-\ning the entire thing over again, thus decreasing the odds of repe ated in-\nterference among competing threads.\nOne point about this solution: it skirts around the hard parts of usi ng\na trylock approach. The ﬁrst problem that would likely exist agai n arises\ndue to encapsulation: if one of these locks is buried in some routine that\nis getting called, the jump back to the beginning becomes more c omplex\nto implement. If the code had acquired some resources (other than L1)\nalong the way, it must make sure to carefully release them as we ll; for\nexample, if after acquiring L1, the code had allocated some memory, it\nwould have to release that memory upon failure to acquire L2, before\njumping back to the top to try the entire sequence again. Howeve r, in\nlimited circumstances (e.g., the Java vector method mentioned earlier),\nthis type of approach could work well.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 COMMON CONCURRENCY PROBLEMS\nYou might also notice that this approach doesn’t really addpreemption\n(the forcible action of taking a lock away from a thread that owns it) ,\nbut rather uses the trylock approach to allow a developer to back ou t of\nlock ownership (i.e., preempt their own ownership) in a graceful way.\nHowever, it is a practical approach, and thus we include it here , despite\nits imperfection in this regard.\nMutual Exclusion\nThe ﬁnal prevention technique would be to avoid the need for mutua l\nexclusion at all. In general, we know this is difﬁcult, because the code we\nwish to run does indeed have critical sections. So what can we do?\nHerlihy had the idea that one could design various data structur es\nwithout locks at all [H91, H93]. The idea behind these lock-free (and\nrelated wait-free ) approaches here is simple: using powerful hardware\ninstructions, you can build data structures in a manner that doe s not re-\nquire explicit locking.\nAs a simple example, let us assume we have a compare-and-swap i n-\nstruction, which as you may recall is an atomic instruction provid ed by\nthe hardware that does the following:\n1int CompareAndSwap(int *address, int expected, int new) {\n2if (*address == expected) {\n3*address = new;\n4return 1; // success\n5}\n6return 0; // failure\n7}\nImagine we now wanted to atomically increment a value by a certa in\namount. We could do it as follows:\n1void AtomicIncrement(int *value, int amount) {\n2do {\n3int old = *value;\n4} while (CompareAndSwap(value, old, old + amount) == 0);\n5}\nInstead of acquiring a lock, doing the update, and then releasin g it, we\nhave instead built an approach that repeatedly tries to updat e the value to\nthe new amount and uses the compare-and-swap to do so. In this man ner,\nno lock is acquired, and no deadlock can arise (though livelock is still a\npossibility).\nLet us consider a slightly more complex example: list insertion. Here\nis code that inserts at the head of a list:\n1void insert(int value) {\n2node_t*n = malloc(sizeof(node_t));\n3assert(n != NULL);\n4n->value = value;\n5n->next = head;\n6head = n;\n7}\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCOMMON CONCURRENCY PROBLEMS 11\nThis code performs a simple insertion, but if called by multiple threads\nat the “same time”, has a race condition (see if you can ﬁgure out wh y). Of\ncourse, we could solve this by surrounding this code with a lock acqu ire\nand release:\n1void insert(int value) {\n2node_t*n = malloc(sizeof(node_t));\n3assert(n != NULL);\n4n->value = value;\n5pthread_mutex_lock(listlock); // begin critical section\n6n->next = head;\n7head = n;\n8pthread_mutex_unlock(listlock); // end critical section\n9}\nIn this solution, we are using locks in the traditional manner2. Instead,\nlet us try to perform this insertion in a lock-free manner simply using the\ncompare-and-swap instruction. Here is one possible approach:\n1void insert(int value) {\n2node_t*n = malloc(sizeof(node_t));\n3assert(n != NULL);\n4n->value = value;\n5do {\n6n->next = head;\n7} while (CompareAndSwap(&head, n->next, n) == 0);\n8}\nThe code here updates the next pointer to point to the current hea d,\nand then tries to swap the newly-created node into position as th e new\nhead of the list. However, this will fail if some other thread succ essfully\nswapped in a new head in the meanwhile, causing this thread to retry\nagain with the new head.\nOf course, building a useful list requires more than just a list insert,\nand not surprisingly building a list that you can insert into, de lete from,\nand perform lookups on in a lock-free manner is non-trivial. Read th e\nrich literature on lock-free and wait-free synchronization to l earn more\n[H01, H91, H93].\nDeadlock Avoidance via Scheduling\nInstead of deadlock prevention, in some scenarios deadlock avoidance\nis preferable. Avoidance requires some global knowledge of which locks\nvarious threads might grab during their execution, and subseq uently sched-\nules said threads in a way as to guarantee no deadlock can occur.\nFor example, assume we have two processors and four threads which\nmust be scheduled upon them. Assume further we know that Thread\n2The astute reader might be asking why we grabbed the lock so late, inste ad of right\nwhen entering insert() ; can you, astute reader, ﬁgure out why that is likely correct? What\nassumptions does the code make, for example, about the call to malloc() ?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 COMMON CONCURRENCY PROBLEMS\n1 (T1) grabs locks L1andL2(in some order, at some point during its\nexecution), T2 grabs L1andL2as well, T3 grabs just L2, and T4 grabs no\nlocks at all. We can show these lock acquisition demands of the thre ads\nin tabular form:\nT1 T2 T3 T4\nL1 yes yes no no\nL2 yes yes yes no\nA smart scheduler could thus compute that as long as T1 and T2 are\nnot run at the same time, no deadlock could ever arise. Here is one s uch\nschedule:\nCPU 1\nCPU 2 T1 T2T3 T4\nNote that it is OK for (T3 and T1) or (T3 and T2) to overlap. Even\nthough T3 grabs lock L2, it can never cause a deadlock by running con-\ncurrently with other threads because it only grabs one lock.\nLet’s look at one more example. In this one, there is more contention\nfor the same resources (again, locks L1andL2), as indicated by the fol-\nlowing contention table:\nT1 T2 T3 T4\nL1 yes yes yes no\nL2 yes yes yes no\nIn particular, threads T1, T2, and T3 all need to grab both locks L1and\nL2at some point during their execution. Here is a possible schedule that\nguarantees that no deadlock could ever occur:\nCPU 1\nCPU 2 T1 T2 T3T4\nAs you can see, static scheduling leads to a conservative approa ch\nwhere T1, T2, and T3 are all run on the same processor, and thus the\ntotal time to complete the jobs is lengthened considerably. Thoug h it may\nhave been possible to run these tasks concurrently, the fear of d eadlock\nprevents us from doing so, and the cost is performance.\nOne famous example of an approach like this is Dijkstra’s Banker’s Al-\ngorithm [D64], and many similar approaches have been describe d in the\nliterature. Unfortunately, they are only useful in very limit ed environ-\nments, for example, in an embedded system where one has full know l-\nedge of the entire set of tasks that must be run and the locks that t hey\nneed. Further, such approaches can limit concurrency, as we sa w in the\nsecond example above. Thus, avoidance of deadlock via scheduling is\nnot a widely-used general-purpose solution.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCOMMON CONCURRENCY PROBLEMS 13\nTIP: DON’TALWAYS DOITPERFECTLY (TOMWEST’SLAW)\nTom West, famous as the subject of the classic computer-industry book\nSoul of a New Machine [K81], says famously: “Not everything worth doing\nis worth doing well”, which is a terriﬁc engineering maxim. If a bad\nthing happens rarely, certainly one should not spend a great dea l of effort\nto prevent it, particularly if the cost of the bad thing occurrin g is small.\nIf, on the other hand, you are building a space shuttle, and the cos t of\nsomething going wrong is the space shuttle blowing up, well, perh aps\nyou should ignore this piece of advice.\nSome readers object: “This sounds like your are suggesting mediocr ity\nas a solution!” Perhaps they are right, that we should be careful w ith\nadvice such as this. However, our experience tells us that in th e world of\nengineering, with pressing deadlines and other real-world con cerns, one\nwill always have to decide which aspects of a system to build we ll and\nwhich to put aside for another day. The hard part is knowing which to\ndo when, a bit of insight only gained through experience and dedi cation\nto the task at hand.\nDetect and Recover\nOne ﬁnal general strategy is to allow deadlocks to occasionally oc cur, and\nthen take some action once such a deadlock has been detected. For ex am-\nple, if an OS froze once a year, you would just reboot it and get happil y (or\ngrumpily) on with your work. If deadlocks are rare, such a non-solut ion\nis indeed quite pragmatic.\nMany database systems employ deadlock detection and recovery te ch-\nniques. A deadlock detector runs periodically, building a resou rce graph\nand checking it for cycles. In the event of a cycle (deadlock), th e system\nneeds to be restarted. If more intricate repair of data structu res is ﬁrst\nrequired, a human being may be involved to ease the process.\nMore detail on database concurrency, deadlock, and related issu es can\nbe found elsewhere [B+87, K87]. Read these works, or better yet, take a\ncourse on databases to learn more about this rich and interesting topic.\n32.4 Summary\nIn this chapter, we have studied the types of bugs that occur in c on-\ncurrent programs. The ﬁrst type, non-deadlock bugs, are surpri singly\ncommon, but often are easier to ﬁx. They include atomicity violati ons,\nin which a sequence of instructions that should have been execut ed to-\ngether was not, and order violations, in which the needed order bet ween\ntwo threads was not enforced.\nWe have also brieﬂy discussed deadlock: why it occurs, and what can\nbe done about it. The problem is as old as concurrency itself, and ma ny\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 COMMON CONCURRENCY PROBLEMS\nhundreds of papers have been written about the topic. The best sol u-\ntion in practice is to be careful, develop a lock acquisition order , and\nthus prevent deadlock from occurring in the ﬁrst place. Wait-fr ee ap-\nproaches also have promise, as some wait-free data structures a re now\nﬁnding their way into commonly-used libraries and critical sy stems, in-\ncluding Linux. However, their lack of generality and the comple xity to\ndevelop a new wait-free data structure will likely limit the ov erall util-\nity of this approach. Perhaps the best solution is to develop new con cur-\nrent programming models: in systems such as MapReduce (from Googl e)\n[GD02], programmers can describe certain types of parallel com putations\nwithout any locks whatsoever. Locks are problematic by their very na-\nture; perhaps we should seek to avoid using them unless we truly must.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nCOMMON CONCURRENCY PROBLEMS 15\nReferences\n[B+87] “Concurrency Control and Recovery in Database Systems” by Phili p A. Bernstein, Vas-\nsos Hadzilacos, Nathan Goodman. Addison-Wesley, 1987. The classic text on concurrency in\ndatabase management systems. As you can tell, understanding concurrency, deadlock, and other topics\nin the world of databases is a world unto itself. Study it and ﬁnd out for yoursel f.\n[C+71] “System Deadlocks” by E.G. Coffman, M.J. Elphick, A. Shoshani. AC M Computing\nSurveys, 3:2, June 1971. The classic paper outlining the conditions for deadlock and how you might\ngo about dealing with it. There are certainly some earlier papers on this topic ; see the references within\nthis paper for details.\n[D64] “Een algorithme ter voorkoming van de dodelijke omarming” by Edsger Dijkstra. 1964.\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd01xx/ EWD108.PDF. Indeed, not only\ndid Dijkstra come up with a number of solutions to the deadlock problem, he was the ﬁrst to note its\nexistence, at least in written form. However, he called it the “deadly em brace”, which (thankfully) did\nnot catch on.\n[GD02] “MapReduce: Simpliﬁed Data Processing on Large Clusters” b y Sanjay Ghemawhat,\nJeff Dean. OSDI ’04, San Francisco, CA, October 2004. The MapReduce paper ushered in the era of\nlarge-scale data processing, and proposes a framework for performing such computations on clusters of\ngenerally unreliable machines.\n[H01] “A Pragmatic Implementation of Non-blocking Linked-lists” by T im Harris. Interna-\ntional Conference on Distributed Computing (DISC), 2001. A relatively modern example of the\ndifﬁculties of building something as simple as a concurrent linked li st without locks.\n[H91] “Wait-free Synchronization” by Maurice Herlihy . ACM TOPLAS, 13:1, Ja nuary 1991.\nHerlihy’s work pioneers the ideas behind wait-free approaches to writing concurrent programs. These\napproaches tend to be complex and hard, often more difﬁcult than using locks cor rectly, probably limiting\ntheir success in the real world.\n[H93] “A Methodology for Implementing Highly Concurrent Data Objects” by Maurice Her-\nlihy. ACM TOPLAS, 15:5, November 1993. A nice overview of lock-free and wait-free structures.\nBoth approaches eschew locks, but wait-free approaches are harder to realize, as th ey try to ensure than\nany operation on a concurrent structure will terminate in a ﬁnite number of ste ps (e.g., no unbounded\nlooping).\n[J+08] “Deadlock Immunity: Enabling Systems To Defend Against Deadlo cks” by Horatiu\nJula, Daniel Tralamazza, Cristian Zamﬁr, George Candea. OSDI ’0 8, San Diego, CA, December\n2008. An excellent recent paper on deadlocks and how to avoid getting caught in the s ame ones over\nand over again in a particular system.\n[K81] “Soul of a New Machine” by Tracy Kidder. Backbay Books, 2000 (rep rint of 1980 ver-\nsion). A must-read for any systems builder or engineer, detailing the early days of how a team inside\nData General (DG), led by Tom West, worked to produce a “new machine.” Kidde r’s other books are\nalso excellent, including “Mountains beyond Mountains.” Or maybe you don ’t agree with us, comma?\n[K87] “Deadlock Detection in Distributed Databases” by Edgar K napp. ACM Computing Sur-\nveys, 19:4, December 1987. An excellent overview of deadlock detection in distributed database sys-\ntems. Also points to a number of other related works, and thus is a good place to start your reading.\n[L+08] “Learning from Mistakes — A Comprehensive Study on Real World Concurrency Bug\nCharacteristics” by Shan Lu, Soyeon Park, Eunsoo Seo, Yuanyuan Zhou. ASPLOS ’08, March\n2008, Seattle, Washington. The ﬁrst in-depth study of concurrency bugs in real software, and the basis\nfor this chapter. Look at Y.Y. Zhou’s or Shan Lu’s web pages for many more intere sting papers on bugs.\n[T+94] “Linux File Memory Map Code” by Linus Torvalds and many others. A vailable online\nat:http://lxr.free-electrons.com/source/mm/filemap.c .Thanks to Michael Wal-\nﬁsh (NYU) for pointing out this precious example. The real world, as you can s ee in this ﬁle, can be a\nbit more complex than the simple clarity found in textbooks...\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 COMMON CONCURRENCY PROBLEMS\nHomework (Code)\nThis homework lets you explore some real code that deadlocks (or\navoids deadlock). The different versions of code correspond to diff erent\napproaches to avoiding deadlock in a simpliﬁed vectoradd() routine.\nSee the README for details on these programs and their common sub-\nstrate.\nQuestions\n1. First let’s make sure you understand how the programs generall y work, and\nsome of the key options. Study the code in vector-deadlock.c , as well\nas inmain-common.c and related ﬁles.\nNow, run ./vector-deadlock -n 2 -l 1 -v , which instantiates two\nthreads (-n 2 ), each of which does one vector add ( -l 1 ), and does so in\nverbose mode ( -v). Make sure you understand the output. How does the\noutput change from run to run?\n2. Now add the -dﬂag, and change the number of loops ( -l) from 1 to higher\nnumbers. What happens? Does the code (always) deadlock?\n3. How does changing the number of threads ( -n) change the outcome of the\nprogram? Are there any values of -nthat ensure no deadlock occurs?\n4. Now examine the code in vector-global-order.c . First, make sure you\nunderstand what the code is trying to do; do you understand why t he code\navoids deadlock? Also, why is there a special case in this vectoradd()\nroutine when the source and destination vectors are the same?\n5. Now run the code with the following ﬂags: -t -n 2 -l 100000 -d .\nHow long does the code take to complete? How does the total time c hange\nwhen you increase the number of loops, or the number of threads?\n6. What happens if you turn on the parallelism ﬂag ( -p)? How much would\nyou expect performance to change when each thread is working on adding\ndifferent vectors (which is what -penables) versus working on the same\nones?\n7. Now let’s study vector-try-wait.c . First make sure you understand\nthe code. Is the ﬁrst call to pthread mutextrylock() really needed?\nNow run the code. How fast does it run compared to the global order ap-\nproach? How does the number of retries, as counted by the code, ch ange as\nthe number of threads increases?\n8. Now let’s look at vector-avoid-hold-and-wait.c . What is the main\nproblem with this approach? How does its performance compare t o the\nother versions, when running both with -pand without it?\n9. Finally, let’s look at vector-nolock.c . This version doesn’t use locks at\nall; does it provide the exact same semantics as the other versio ns? Why or\nwhy not?\n10. Now compare its performance to the other versions, both whe n threads are\nworking on the same two vectors (no -p) and when each thread is working\non separate vectors ( -p). How does this no-lock version perform?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",37072
38-33. Event-based Concurrency.pdf,38-33. Event-based Concurrency,"33\nEvent-based Concurrency (Advanced)\nThus far, we’ve written about concurrency as if the only way to bui ld\nconcurrent applications is to use threads. Like many things in life, this\nis not completely true. Speciﬁcally, a different style of concurr ent pro-\ngramming is often used in both GUI-based applications [O96] as w ell as\nsome types of internet servers [PDZ99]. This style, known as event-based\nconcurrency , has become popular in some modern systems, including\nserver-side frameworks such as node.js [N13], but its roots are found in\nC/U NIXsystems that we’ll discuss below.\nThe problem that event-based concurrency addresses is two-fold . The\nﬁrst is that managing concurrency correctly in multi-threade d applica-\ntions can be challenging; as we’ve discussed, missing locks, de adlock,\nand other nasty problems can arise. The second is that in a multi- threaded\napplication, the developer has little or no control over what is sch eduled\nat a given moment in time; rather, the programmer simply create s threads\nand then hopes that the underlying OS schedules them in a reason able\nmanner across available CPUs. Given the difﬁculty of building a general-\npurpose scheduler that works well in all cases for all workloads, s ome-\ntimes the OS will schedule work in a manner that is less than opti mal.\nAnd thus, we have ...\nTHECRUX:\nHOWTOBUILD CONCURRENT SERVERS WITHOUT THREADS\nHow can we build a concurrent server without using threads, and t hus\nretain control over concurrency as well as avoid some of the problems\nthat seem to plague multi-threaded applications?\n33.1 The Basic Idea: An Event Loop\nThe basic approach we’ll use, as stated above, is called event-based\nconcurrency . The approach is quite simple: you simply wait for some-\nthing (i.e., an “event”) to occur; when it does, you check what ty pe of\n1\n2 EVENT -BASED CONCURRENCY (ADVANCED )\nevent it is and do the small amount of work it requires (which may i n-\nclude issuing I/O requests, or scheduling other events for futu re han-\ndling, etc.). That’s it!\nBefore getting into the details, let’s ﬁrst examine what a canon ical\nevent-based server looks like. Such applications are based aroun d a sim-\nple construct known as the event loop . Pseudocode for an event loop\nlooks like this:\nwhile (1) {\nevents = getEvents();\nfor (e in events)\nprocessEvent(e);\n}\nIt’s really that simple. The main loop simply waits for something t o do\n(by calling getEvents() in the code above) and then, for each event re-\nturned, processes them, one at a time; the code that processes eac h event\nis known as an event handler . Importantly, when a handler processes\nan event, it is the only activity taking place in the system; th us, deciding\nwhich event to handle next is equivalent to scheduling. This explicit con-\ntrol over scheduling is one of the fundamental advantages of the ev ent-\nbased approach.\nBut this discussion leaves us with a bigger question: how exactl y does\nan event-based server determine which events are taking pla ce, in par-\nticular with regards to network and disk I/O? Speciﬁcally, how c an an\nevent server tell if a message has arrived for it?\n33.2 An Important API: select() (orpoll() )\nWith that basic event loop in mind, we next must address the ques tion\nof how to receive events. In most systems, a basic API is availabl e, via\neither theselect() orpoll() system calls.\nWhat these interfaces enable a program to do is simple: check w hether\nthere is any incoming I/O that should be attended to. For example, imag-\nine that a network application (such as a web server) wishes to c heck\nwhether any network packets have arrived, in order to service t hem.\nThese system calls let you do exactly that.\nTakeselect() for example. The manual page (on a Mac) describes\nthe API in this manner:\nint select(int nfds,\nfd_set*restrict readfds,\nfd_set*restrict writefds,\nfd_set*restrict errorfds,\nstruct timeval *restrict timeout);\nThe actual description from the man page: select() examines the I/O de-\nscriptor sets whose addresses are passed in readfds, writefds, an d errorfds to see\nif some of their descriptors are ready for reading, are ready for writ ing, or have\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nEVENT -BASED CONCURRENCY (ADVANCED ) 3\nASIDE : BLOCKING VS . NON-BLOCKING INTERFACES\nBlocking (or synchronous ) interfaces do all of their work before returning\nto the caller; non-blocking (or asynchronous ) interfaces begin some work\nbut return immediately, thus letting whatever work that need s to be done\nget done in the background.\nThe usual culprit in blocking calls is I/O of some kind. For exampl e, if a\ncall must read from disk in order to complete, it might block, wait ing for\nthe I/O request that has been sent to the disk to return.\nNon-blocking interfaces can be used in any style of programming ( e.g.,\nwith threads), but are essential in the event-based approach , as a call that\nblocks will halt all progress.\nan exceptional condition pending, respectively. The ﬁrst nfds descr iptors are\nchecked in each set, i.e., the descriptors from 0 through nfds-1 in t he descriptor\nsets are examined. On return, select() replaces the given descrip tor sets with\nsubsets consisting of those descriptors that are ready for the re quested operation.\nselect() returns the total number of ready descriptors in all the sets.\nA couple of points about select() . First, note that it lets you check\nwhether descriptors can be read from as well as written to; the former\nlets a server determine that a new packet has arrived and is in need of\nprocessing, whereas the latter lets the service know when it is OK to reply\n(i.e., the outbound queue is not full).\nSecond, note the timeout argument. One common usage here is to\nset the timeout to NULL, which causes select() to block indeﬁnitely,\nuntil some descriptor is ready. However, more robust servers will usually\nspecify some kind of timeout; one common technique is to set the time out\nto zero, and thus use the call to select() to return immediately.\nThepoll() system call is quite similar. See its manual page, or Stevens\nand Rago [SR05], for details.\nEither way, these basic primitives give us a way to build a non- blocking\nevent loop, which simply checks for incoming packets, reads from s ockets\nwith messages upon them, and replies as needed.\n33.3 Using select()\nTo make this more concrete, let’s examine how to use select() to see\nwhich network descriptors have incoming messages upon them. Fig ure\n33.1 shows a simple example.\nThis code is actually fairly simple to understand. After some i nitial-\nization, the server enters an inﬁnite loop. Inside the loop, it use s the\nFDZERO() macro to ﬁrst clear the set of ﬁle descriptors, and then uses\nFDSET() to include all of the ﬁle descriptors from minFD tomaxFD in\nthe set. This set of descriptors might represent, for example, a ll of the net-\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 EVENT -BASED CONCURRENCY (ADVANCED )\n1#include <stdio.h>\n2#include <stdlib.h>\n3#include <sys/time.h>\n4#include <sys/types.h>\n5#include <unistd.h>\n6\n7int main(void) {\n8// open and set up a bunch of sockets (not shown)\n9// main loop\n10 while (1) {\n11 // initialize the fd_set to all zero\n12 fd_set readFDs;\n13 FD_ZERO(&readFDs);\n14\n15 // now set the bits for the descriptors\n16 // this server is interested in\n17 // (for simplicity, all of them from min to max)\n18 int fd;\n19 for (fd = minFD; fd < maxFD; fd++)\n20 FD_SET(fd, &readFDs);\n21\n22 // do the select\n23 int rc = select(maxFD+1, &readFDs, NULL, NULL, NULL);\n24\n25 // check which actually have data using FD_ISSET()\n26 int fd;\n27 for (fd = minFD; fd < maxFD; fd++)\n28 if (FD_ISSET(fd, &readFDs))\n29 processFD(fd);\n30 }\n31}\nFigure 33.1: Simple Code Using select()\nwork sockets to which the server is paying attention. Finally, t he server\ncallsselect() to see which of the connections have data available upon\nthem. By then using FDISSET() in a loop, the event server can see\nwhich of the descriptors have data ready and process the incoming data.\nOf course, a real server would be more complicated than this, and\nrequire logic to use when sending messages, issuing disk I/O, and many\nother details. For further information, see Stevens and Rago [SR05 ] for\nAPI information, or Pai et. al or Welsh et al. for a good overview of the\ngeneral ﬂow of event-based servers [PDZ99, WCB01].\n33.4 Why Simpler? No Locks Needed\nWith a single CPU and an event-based application, the problems found\nin concurrent programs are no longer present. Speciﬁcally, beca use only\none event is being handled at a time, there is no need to acquire or release\nlocks; the event-based server cannot be interrupted by another thread be-\ncause it is decidedly single threaded. Thus, concurrency bug s common in\nthreaded programs do not manifest in the basic event-based app roach.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nEVENT -BASED CONCURRENCY (ADVANCED ) 5\nTIP: DON’TBLOCK INEVENT -BASED SERVERS\nEvent-based servers enable ﬁne-grained control over scheduli ng of tasks.\nHowever, to maintain such control, no call that blocks the execut ion of\nthe caller can ever be made; failing to obey this design tip wil l result in\na blocked event-based server, frustrated clients, and seriou s questions as\nto whether you ever read this part of the book.\n33.5 A Problem: Blocking System Calls\nThus far, event-based programming sounds great, right? You prog ram\na simple loop, and handle events as they arise. You don’t even need t o\nthink about locking! But there is an issue: what if an event requ ires that\nyou issue a system call that might block?\nFor example, imagine a request comes from a client into a server t o\nread a ﬁle from disk and return its contents to the requesting cl ient (much\nlike a simple HTTP request). To service such a request, some ev ent han-\ndler will eventually have to issue an open() system call to open the ﬁle,\nfollowed by a series of read() calls to read the ﬁle. When the ﬁle is read\ninto memory, the server will likely start sending the results to the client.\nBoth theopen() andread() calls may issue I/O requests to the stor-\nage system (when the needed metadata or data is not in memory alre ady),\nand thus may take a long time to service. With a thread-based se rver, this\nis no issue: while the thread issuing the I/O request suspend s (waiting\nfor the I/O to complete), other threads can run, thus enabling th e server\nto make progress. Indeed, this natural overlap of I/O and other computa-\ntion is what makes thread-based programming quite natural and straight-\nforward.\nWith an event-based approach, however, there are no other threa ds to\nrun: just the main event loop. And this implies that if an event h andler\nissues a call that blocks, the entire server will do just that: block until the\ncall completes. When the event loop blocks, the system sits idle, and thus\nis a huge potential waste of resources. We thus have a rule that mu st be\nobeyed in event-based systems: no blocking calls are allowed.\n33.6 A Solution: Asynchronous I/O\nTo overcome this limit, many modern operating systems have intro-\nduced new ways to issue I/O requests to the disk system, refer red to\ngenerically as asynchronous I/O . These interfaces enable an application\nto issue an I/O request and return control immediately to the ca ller, be-\nfore the I/O has completed; additional interfaces enable an app lication to\ndetermine whether various I/Os have completed.\nFor example, let us examine the interface provided on a Mac (other\nsystems have similar APIs). The APIs revolve around a basic str ucture,\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 EVENT -BASED CONCURRENCY (ADVANCED )\nthestruct aiocb orAIO control block in common terminology. A\nsimpliﬁed version of the structure looks like this (see the manua l pages\nfor more information):\nstruct aiocb {\nint aio_fildes; / *File descriptor */\noff_t aio_offset; / *File offset */\nvolatile void *aio_buf; / *Location of buffer */\nsize_t aio_nbytes; / *Length of transfer */\n};\nTo issue an asynchronous read to a ﬁle, an application should ﬁrst\nﬁll in this structure with the relevant information: the ﬁle de scriptor of\nthe ﬁle to be read ( aiofildes ), the offset within the ﬁle ( aiooffset )\nas well as the length of the request ( aionbytes ), and ﬁnally the tar-\nget memory location into which the results of the read should be copi ed\n(aiobuf).\nAfter this structure is ﬁlled in, the application must issue t he asyn-\nchronous call to read the ﬁle; on a Mac, this API is simply the asyn-\nchronous read API:\nint aio_read(struct aiocb *aiocbp);\nThis call tries to issue the I/O; if successful, it simply ret urns right\naway and the application (i.e., the event-based server) can c ontinue with\nits work.\nThere is one last piece of the puzzle we must solve, however. How can\nwe tell when an I/O is complete, and thus that the buffer (pointe d to by\naiobuf) now has the requested data within it?\nOne last API is needed. On a Mac, it is referred to (somewhat conf us-\ningly) asaioerror() . The API looks like this:\nint aio_error(const struct aiocb *aiocbp);\nThis system call checks whether the request referred to by aiocbp has\ncompleted. If it has, the routine returns success (indicated b y a zero);\nif not, EINPROGRESS is returned. Thus, for every outstanding asy n-\nchronous I/O, an application can periodically poll the system via a call\ntoaioerror() to determine whether said I/O has yet completed.\nOne thing you might have noticed is that it is painful to check wh ether\nan I/O has completed; if a program has tens or hundreds of I/Os issu ed\nat a given point in time, should it simply keep checking each of th em\nrepeatedly, or wait a little while ﬁrst, or ... ?\nTo remedy this issue, some systems provide an approach based on th e\ninterrupt . This method uses U NIX signals to inform applications when\nan asynchronous I/O completes, thus removing the need to repeate dly\nask the system. This polling vs. interrupts issue is seen in de vices too, as\nyou will see (or already have seen) in the chapter on I/O devices.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nEVENT -BASED CONCURRENCY (ADVANCED ) 7\nASIDE : UNIXSIGNALS\nA huge and fascinating infrastructure known as signals is present in all mod-\nern U NIX variants. At its simplest, signals provide a way to communicate wi th a\nprocess. Speciﬁcally, a signal can be delivered to an applic ation; doing so stops the\napplication from whatever it is doing to run a signal handler , i.e., some code in\nthe application to handle that signal. When ﬁnished, the pro cess just resumes its\nprevious behavior.\nEach signal has a name, such as HUP (hang up), INT (interrupt), SEGV (seg-\nmentation violation), etc; see the manual page for details. Int erestingly, sometimes\nit is the kernel itself that does the signaling. For example, wh en your program en-\ncounters a segmentation violation, the OS sends it a SIGSEGV (prepending SIG\nto signal names is common); if your program is conﬁgured to catch th at signal,\nyou can actually run some code in response to this erroneous progr am behavior\n(which can be useful for debugging). When a signal is sent to a pro cess not conﬁg-\nured to handle that signal, some default behavior is enacted; fo r SEGV , the process\nis killed.\nHere is a simple program that goes into an inﬁnite loop, but has ﬁ rst set up a\nsignal handler to catch SIGHUP:\n#include <stdio.h>\n#include <signal.h>\nvoid handle(int arg) {\nprintf(""stop wakin’ me up...\n"");\n}\nint main(int argc, char *argv[]) {\nsignal(SIGHUP, handle);\nwhile (1)\n; // doin’ nothin’ except catchin’ some sigs\nreturn 0;\n}\nYou can send signals to it with the kill command line tool (yes, this is an odd\nand aggressive name). Doing so will interrupt the main while loo p in the program\nand run the handler code handle() :\nprompt> ./main &\n[3] 36705\nprompt> kill -HUP 36705\nstop wakin’ me up...\nprompt> kill -HUP 36705\nstop wakin’ me up...\nprompt> kill -HUP 36705\nstop wakin’ me up...\nThere is a lot more to learn about signals, so much that a single cha pter, much\nless a single page, does not nearly sufﬁce. As always, there is o ne great source:\nStevens and Rago [SR05]. Read more if interested.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 EVENT -BASED CONCURRENCY (ADVANCED )\nIn systems without asynchronous I/O, the pure event-based appr oach\ncannot be implemented. However, clever researchers have deri ved meth-\nods that work fairly well in their place. For example, Pai et al. [ PDZ99]\ndescribe a hybrid approach in which events are used to process n etwork\npackets, and a thread pool is used to manage outstanding I/Os. Re ad\ntheir paper for details.\n33.7 Another Problem: State Management\nAnother issue with the event-based approach is that such code is gen-\nerally more complicated to write than traditional thread-base d code. The\nreason is as follows: when an event handler issues an asynchronous I/O,\nit must package up some program state for the next event handler t o use\nwhen the I/O ﬁnally completes; this additional work is not needed in\nthread-based programs, as the state the program needs is on the s tack of\nthe thread. Adya et al. call this work manual stack management , and it\nis fundamental to event-based programming [A+02].\nTo make this point more concrete, let’s look at a simple example in\nwhich a thread-based server needs to read from a ﬁle descriptor (fd) and,\nonce complete, write the data that it read from the ﬁle to a network socket\ndescriptor ( sd). The code (ignoring error checking) looks like this:\nint rc = read(fd, buffer, size);\nrc = write(sd, buffer, size);\nAs you can see, in a multi-threaded program, doing this kind of work\nis trivial; when the read() ﬁnally returns, the code immediately knows\nwhich socket to write to because that information is on the stack of the\nthread (in the variable sd).\nIn an event-based system, life is not so easy. To perform the sam e task,\nwe’d ﬁrst issue the read asynchronously, using the AIO calls des cribed\nabove. Let’s say we then periodically check for completion of the rea d\nusing the aioerror() call; when that call informs us that the read is\ncomplete, how does the event-based server know what to do?\nThe solution, as described by Adya et al. [A+02], is to use an old p ro-\ngramming language construct known as a continuation [FHK84]. Though\nit sounds complicated, the idea is rather simple: basically, r ecord the\nneeded information to ﬁnish processing this event in some data st ruc-\nture; when the event happens (i.e., when the disk I/O complete s), look\nup the needed information and process the event.\nIn this speciﬁc case, the solution would be to record the socket de-\nscriptor (sd) in some kind of data structure (e.g., a hash table), indexed\nby the ﬁle descriptor ( fd). When the disk I/O completes, the event han-\ndler would use the ﬁle descriptor to look up the continuation, which will\nreturn the value of the socket descriptor to the caller. At this p oint (ﬁ-\nnally), the server can then do the last bit of work to write the da ta to the\nsocket.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nEVENT -BASED CONCURRENCY (ADVANCED ) 9\n33.8 What Is Still Difﬁcult With Events\nThere are a few other difﬁculties with the event-based approac h that\nwe should mention. For example, when systems moved from a single\nCPU to multiple CPUs, some of the simplicity of the event-based a p-\nproach disappeared. Speciﬁcally, in order to utilize more than on e CPU,\nthe event server has to run multiple event handlers in parall el; when do-\ning so, the usual synchronization problems (e.g., critical sect ions) arise,\nand the usual solutions (e.g., locks) must be employed. Thus, on mod -\nern multicore systems, simple event handling without locks is n o longer\npossible.\nAnother problem with the event-based approach is that it does not\nintegrate well with certain kinds of systems activity, such a spaging . For\nexample, if an event-handler page faults, it will block, and t hus the server\nwill not make progress until the page fault completes. Even thoug h the\nserver has been structured to avoid explicit blocking, this type of implicit\nblocking due to page faults is hard to avoid and thus can lead to l arge\nperformance problems when prevalent.\nA third issue is that event-based code can be hard to manage over time,\nas the exact semantics of various routines changes [A+02]. For ex ample,\nif a routine changes from non-blocking to blocking, the event hand ler\nthat calls that routine must also change to accommodate its new n ature,\nby ripping itself into two pieces. Because blocking is so disa strous for\nevent-based servers, a programmer must always be on the lookout for\nsuch changes in the semantics of the APIs each event uses.\nFinally, though asynchronous disk I/O is now possible on most plat-\nforms, it has taken a long time to get there [PDZ99], and it never quite\nintegrates with asynchronous network I/O in as simple and unifor m a\nmanner as you might think. For example, while one would simply lik e\nto use the select() interface to manage all outstanding I/Os, usually\nsome combination of select() for networking and the AIO calls for\ndisk I/O are required.\n33.9 Summary\nWe’ve presented a bare bones introduction to a different style of c on-\ncurrency based on events. Event-based servers give control of sc hedul-\ning to the application itself, but do so at some cost in complexity and\ndifﬁculty of integration with other aspects of modern systems (e. g., pag-\ning). Because of these challenges, no single approach has emer ged as\nbest; thus, both threads and events are likely to persist as tw o different\napproaches to the same concurrency problem for many years to come.\nRead some research papers (e.g., [A+02, PDZ99, vB+03, WCB01] ) or bet-\nter yet, write some event-based code, to learn more.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 EVENT -BASED CONCURRENCY (ADVANCED )\nReferences\n[A+02] “Cooperative Task Management Without Manual Stack Management” b y Atul Adya,\nJon Howell, Marvin Theimer, William J. Bolosky, John R. Douceur. USEN IX ATC ’02, Monterey,\nCA, June 2002. This gem of a paper is the ﬁrst to clearly articulate some of the difﬁculties of event-based\nconcurrency, and suggests some simple solutions, as well explores the e ven crazier idea of combining\nthe two types of concurrency management into a single application!\n[FHK84] “Programming With Continuations” by Daniel P . Friedman, Chri stopher T. Haynes,\nEugene E. Kohlbecker. In Program Transformation and Programming Envir onments, Springer\nVerlag, 1984. The classic reference to this old idea from the world of programming langu ages. Now\nincreasingly popular in some modern languages.\n[N13] “Node.js Documentation” by the folks who built node.js. Availa ble:nodejs.org/api .\nOne of the many cool new frameworks that help you readily build web services and applications. Every\nmodern systems hacker should be proﬁcient in frameworks such as this one (and likely, more than one).\nSpend the time and do some development in one of these worlds and become an ex pert.\n[O96] “Why Threads Are A Bad Idea (for most purposes)” by John Ousterhout. I nvited Talk\nat USENIX ’96, San Diego, CA, January 1996. A great talk about how threads aren’t a great match\nfor GUI-based applications (but the ideas are more general). Ousterhout formed m any of these opinions\nwhile he was developing Tcl/Tk, a cool scripting language and toolkit that made it 100x easier to develop\nGUI-based applications than the state of the art at the time. While the Tk GUI toolkit live s on (in Python\nfor example), Tcl seems to be slowly dying (unfortunately).\n[PDZ99] “Flash: An Efﬁcient and Portable Web Server” by Vivek S. Pai, Pe ter Druschel, Willy\nZwaenepoel. USENIX ’99, Monterey, CA, June 1999. A pioneering paper on how to structure\nweb servers in the then-burgeoning Internet era. Read it to understand the basics as well as to see the\nauthors’ ideas on how to build hybrids when support for asynchronous I/O is l acking.\n[SR05] “Advanced Programming in the U NIXEnvironment” by W. Richard Stevens and Stephen\nA. Rago. Addison-Wesley, 2005. Once again, we refer to the classic must-have-on-your-bookshelf\nbook of UNIXsystems programming. If there is some detail you need to know, it is in here.\n[vB+03] “Capriccio: Scalable Threads for Internet Services” by Rob von Behren, Jeremy Condit,\nFeng Zhou, George C. Necula, Eric Brewer. SOSP ’03, Lake George, N ew York, October 2003.\nA paper about how to make threads work at extreme scale; a counter to all the event-base d work ongoing\nat the time.\n[WCB01] “SEDA: An Architecture for Well-Conditioned, Scalable Interne t Services” by Matt\nWelsh, David Culler, and Eric Brewer. SOSP ’01, Banff, Canada, Octobe r 2001. A nice twist\non event-based serving that combines threads, queues, and event-based hand ling into one streamlined\nwhole. Some of these ideas have found their way into the infrastructures of comp anies such as Google,\nAmazon, and elsewhere.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nEVENT -BASED CONCURRENCY (ADVANCED ) 11\nHomework (Code)\nIn this (short) homework, you’ll gain some experience with event-\nbased code and some of its key concepts. Good luck!\nQuestions\n1. First, write a simple server that can accept and serve TCP co nnections. You’ll\nhave to poke around the Internet a bit if you don’t already know how to do\nthis. Build this to serve exactly one request at a time; have each r equest be\nvery simple, e.g., to get the current time of day.\n2. Now, add the select() interface. Build a main program that can accept\nmultiple connections, and an event loop that checks which ﬁle d escriptors\nhave data on them, and then read and process those requests. Mak e sure to\ncarefully test that you are using select() correctly.\n3. Next, let’s make the requests a little more interesting, to mimic a simple web\nor ﬁle server. Each request should be to read the contents of a ﬁl e (named in\nthe request), and the server should respond by reading the ﬁle i nto a buffer,\nand then returning the contents to the client. Use the standard open() ,\nread() ,close() system calls to implement this feature. Be a little careful\nhere: if you leave this running for a long time, someone may ﬁgure out how\nto use it to read all the ﬁles on your computer!\n4. Now, instead of using standard I/O system calls, use the asyn chronous I/O\ninterfaces as described in the chapter. How hard was it to inc orporate asyn-\nchronous interfaces into your program?\n5. For fun, add some signal handling to your code. One common use of si gnals\nis to poke a server to reload some kind of conﬁguration ﬁle, or ta ke some\nother kind of administrative action. Perhaps one natural way t o play around\nwith this is to add a user-level ﬁle cache to your server, which s tores recently\naccessed ﬁles. Implement a signal handler that clears the cach e when the\nsignal is sent to the server process.\n6. Finally, we have the hard part: how can you tell if the effor t to build an\nasynchronous, event-based approach are worth it? Can you cre ate an ex-\nperiment to show the beneﬁts? How much implementation complexity di d\nyour approach add?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",27674
39-34. Summary Dialogue on Concurrency.pdf,39-34. Summary Dialogue on Concurrency,"34\nSummary Dialogue on Concurrency\nProfessor: So, does your head hurt now?\nStudent: (taking two Motrin tablets) Well, some. It’s hard to think about all t he\nways threads can interleave.\nProfessor: Indeed it is. I am always amazed that when concurrent execution is\ninvolved, just a few lines of code can become nearly impossible to unders tand.\nStudent: Me too! It’s kind of embarrassing, as a Computer Scientist, not to be\nable to make sense of ﬁve lines of code.\nProfessor: Oh, don’t feel too badly. If you look through the ﬁrst papers on con -\ncurrent algorithms, they are sometimes wrong! And the authors o ften professors!\nStudent: (gasps) Professors can be ... umm... wrong?\nProfessor: Yes, it is true. Though don’t tell anybody — it’s one of our trade\nsecrets.\nStudent: I am sworn to secrecy. But if concurrent code is so hard to think ab out,\nand so hard to get right, how are we supposed to write correct con current code?\nProfessor: Well that is the real question, isn’t it? I think it starts with a few\nsimple things. First, keep it simple! Avoid complex interactions betwee n threads,\nand use well-known and tried-and-true ways to manage thread inte ractions.\nStudent: Like simple locking, and maybe a producer-consumer queue?\nProfessor: Exactly! Those are common paradigms, and you should be able to\nproduce the working solutions given what you’ve learned. Second, o nly use con-\ncurrency when absolutely needed; avoid it if at all possible. There is n othing\nworse than premature optimization of a program.\nStudent: I see — why add threads if you don’t need them?\nProfessor: Exactly. Third, if you really need parallelism, seek it in other sim-\npliﬁed forms. For example, the Map-Reduce method for writing parallel d ata\nanalysis code is an excellent example of achieving parallelism without hav ing to\nhandle any of the horriﬁc complexities of locks, condition variables, an d the other\nnasty things we’ve talked about.\n1\n2 SUMMARY DIALOGUE ON CONCURRENCY\nStudent: Map-Reduce, huh? Sounds interesting — I’ll have to read more abou t\nit on my own.\nProfessor: Good! You should. In the end, you’ll have to do a lot of that, as\nwhat we learn together can only serve as the barest introduction t o the wealth of\nknowledge that is out there. Read, read, and read some more! And then try things\nout, write some code, and then write some more too. As Gladwell talk s about in\nhis book “Outliers”, you need to put roughly 10,000 hours into some thing in\norder to become a real expert. You can’t do that all inside of class time!\nStudent: Wow, I’m not sure if that is depressing, or uplifting. But I’ll assume\nthe latter, and get to work! Time to write some more concurrent co de...\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",2780
40-Part III Persistence.pdf,40-Part III Persistence,Part III\nPersistence\n1,24
41-35. A Dialogue on Persistence.pdf,41-35. A Dialogue on Persistence,"35\nA Dialogue on Persistence\nProfessor: And thus we reach the third of our four ... err... three pillars of\noperating systems: persistence .\nStudent: Did you say there were three pillars, or four? What is the fourth?\nProfessor: No. Just three, young student, just three. Trying to keep it simple\nhere.\nStudent: OK, ﬁne. But what is persistence, oh ﬁne and noble professor?\nProfessor: Actually, you probably know what it means in the traditional sense,\nright? As the dictionary would say: “a ﬁrm or obstinate continuance in a course\nof action in spite of difﬁculty or opposition.”\nStudent: It’s kind of like taking your class: some obstinance required.\nProfessor: Ha! Yes. But persistence here means something else. Let me explain .\nImagine you are outside, in a ﬁeld, and you pick a —\nStudent: (interrupting) I know! A peach! From a peach tree!\nProfessor: I was going to say apple, from an apple tree. Oh well; we’ll do it your\nway, I guess.\nStudent: (stares blankly)\nProfessor: Anyhow, you pick a peach; in fact, you pick many many peaches,\nbut you want to make them last for a long time. Winter is hard and cruel in\nWisconsin, after all. What do you do?\nStudent: Well, I think there are some different things you can do. You can pickle\nit! Or bake a pie. Or make a jam of some kind. Lots of fun!\nProfessor: Fun? Well, maybe. Certainly, you have to do a lot more work to make\nthe peach persist . And so it is with information as well; making information\npersist, despite computer crashes, disk failures, or power outage s is a tough and\ninteresting challenge.\nStudent: Nice segue; you’re getting quite good at that.\nProfessor: Thanks! A professor can always use a few kind words, you know.\n3\n4 A D IALOGUE ON PERSISTENCE\nStudent: I’ll try to remember that. I guess it’s time to stop talking peaches, and\nstart talking computers?\nProfessor: Yes, it is that time...\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",1949
42-36.  IO Devices.pdf,42-36.  IO Devices,"36\nI/O Devices\nBefore delving into the main content of this part of the book (on persi s-\ntence), we ﬁrst introduce the concept of an input/output (I/O) device and\nshow how the operating system might interact with such an entity . I/O is\nquite critical to computer systems, of course; imagine a program without\nany input (it produces the same result each time); now imagine a pro-\ngram with no output (what was the purpose of it running?). Clearl y, for\ncomputer systems to be interesting, both input and output are re quired.\nAnd thus, our general problem:\nCRUX: HOWTOINTEGRATE I/O I NTO SYSTEMS\nHow should I/O be integrated into systems? What are the general\nmechanisms? How can we make them efﬁcient?\n36.1 System Architecture\nTo begin our discussion, let’s look at a “classical” diagram of a typ -\nical system (Figure 36.1). The picture shows a single CPU atta ched to\nthe main memory of the system via some kind of memory bus or inter-\nconnect. Some devices are connected to the system via a general I/O bus ,\nwhich in many modern systems would be PCI (or one of its many deriva-\ntives); graphics and some other higher-performance I/O device s might be\nfound here. Finally, even lower down are one or more of what we call a\nperipheral bus , such as SCSI ,SATA , orUSB . These connect slow devices\nto the system, including disks ,mice , and keyboards .\nOne question you might ask is: why do we need a hierarchical stru c-\nture like this? Put simply: physics, and cost. The faster a bus is, the\nshorter it must be; thus, a high-performance memory bus does not ha ve\nmuch room to plug devices and such into it. In addition, engineer ing\na bus for high performance is quite costly. Thus, system designe rs have\nadopted this hierarchical approach, where components that dema nd high\nperformance (such as the graphics card) are nearer the CPU. Low er per-\n1\n2 I/O D EVICES\nGraphicsMemory CPU\nMemory Bus\n(proprietary)\nGeneral I/O Bus\n(e.g., PCI)\nPeripheral I/O Bus\n(e.g., SCSI, SATA, USB)\nFigure 36.1: Prototypical System Architecture\nformance components are further away. The beneﬁts of placing dis ks and\nother slow devices on a peripheral bus are manifold; in particula r, you\ncan place a large number of devices on it.\nOf course, modern systems increasingly use specialized chips ets and\nfaster point-to-point interconnects to improve performance. Fig ure 36.2\n(page 3) shows an approximate diagram of Intel’s Z270 Chipset [H1 7].\nAlong the top, the CPU connects most closely to the memory system,\nbut also has a high-performance connection to the graphics card (and\nthus, the display) to enable gaming (oh, the horror!) and other gra phics-\nintensive applications.\nThe CPU connects to an I/O chip via Intel’s proprietary DMI (Direct\nMedia Interface ), and the rest of the devices connect to this chip via a\nnumber of different interconnects. On the right, one or more hard d rives\nconnect to the system via the eSATA interface; ATA (the AT Attachment ,\nin reference to providing connection to the IBM PC AT), then SATA (for\nSerial ATA ), and now eSATA (for external SATA ) represent an evolu-\ntion of storage interfaces over the past decades, with each step f orward\nincreasing performance to keep pace with modern storage device s.\nBelow the I/O chip are a number of USB (Universal Serial Bus ) con-\nnections, which in this depiction enable a keyboard and mouse to b e at-\ntached to the computer. On many modern systems, USB is used for low\nperformance devices such as these.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nI/O D EVICES 3\nPCIe\nGraphicsMemory\nInterconnect\nGraphics CPU MemoryDMI\nI/O ChipeSATA\nDiskDiskDiskDiskUSBKeyboard\nMousePCIe\nNetwork\nFigure 36.2: Modern System Architecture\nFinally, on the left, other higher performance devices can be con nected\nto the system via PCIe (Peripheral Component Interconnect Express ). In\nthis diagram, a network interface is attached to the system he re; higher\nperformance storage devices (such as NVMe persistent storage devices)\nare often connected here.\n36.2 A Canonical Device\nLet us now look at a canonical device (not a real one), and use this\ndevice to drive our understanding of some of the machinery requir ed to\nmake device interaction efﬁcient. From Figure 36.3 (page 4), w e can see\nthat a device has two important components. The ﬁrst is the hardw are\ninterface it presents to the rest of the system. Just like a piece of softwar e,\nhardware must also present some kind of interface that allows th e system\nsoftware to control its operation. Thus, all devices have some spec iﬁed\ninterface and protocol for typical interaction.\nThe second part of any device is its internal structure . This part of\nthe device is implementation speciﬁc and is responsible for imp lement-\ning the abstraction the device presents to the system. Very si mple devices\nwill have one or a few hardware chips to implement their function ality;\nmore complex devices will include a simple CPU, some general pur pose\nmemory, and other device-speciﬁc chips to get their job done. For e xam-\nple, modern RAID controllers might consist of hundreds of thousands of\nlines of ﬁrmware (i.e., software within a hardware device) to implement\nits functionality.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 I/O D EVICES\nOther Hardware-specific ChipsMemory (DRAM or SRAM or both)Micro-controller (CPU)Registers Status Command Data Interface\nInternals\nFigure 36.3: A Canonical Device\n36.3 The Canonical Protocol\nIn the picture above, the (simpliﬁed) device interface is comp rised of\nthree registers: a status register, which can be read to see the current sta-\ntus of the device; a command register, to tell the device to perform a cer-\ntain task; and a data register to pass data to the device, or get data from\nthe device. By reading and writing these registers, the opera ting system\ncan control device behavior.\nLet us now describe a typical interaction that the OS might have with\nthe device in order to get the device to do something on its behalf . The\nprotocol is as follows:\nWhile (STATUS == BUSY)\n; // wait until device is not busy\nWrite data to DATA register\nWrite command to COMMAND register\n(Doing so starts the device and executes the command)\nWhile (STATUS == BUSY)\n; // wait until device is done with your request\nThe protocol has four steps. In the ﬁrst, the OS waits until the dev ice is\nready to receive a command by repeatedly reading the status re gister; we\ncall this polling the device (basically, just asking it what is going on). Sec-\nond, the OS sends some data down to the data register; one can imagi ne\nthat if this were a disk, for example, that multiple writes woul d need to\ntake place to transfer a disk block (say 4KB) to the device. Whe n the main\nCPU is involved with the data movement (as in this example protocol ),\nwe refer to it as programmed I/O (PIO) . Third, the OS writes a command\nto the command register; doing so implicitly lets the device kn ow that\nboth the data is present and that it should begin working on the com-\nmand. Finally, the OS waits for the device to ﬁnish by again poll ing it\nin a loop, waiting to see if it is ﬁnished (it may then get an error c ode to\nindicate success or failure).\nThis basic protocol has the positive aspect of being simple and work -\ning. However, there are some inefﬁciencies and inconveniences involved.\nThe ﬁrst problem you might notice in the protocol is that polling seem s\ninefﬁcient; speciﬁcally, it wastes a great deal of CPU time ju st waiting for\nthe (potentially slow) device to complete its activity, instea d of switching\nto another ready process and thus better utilizing the CPU.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nI/O D EVICES 5\nTHECRUX: HOWTOAVOID THECOSTS OFPOLLING\nHow can the OS check device status without frequent polling, and\nthus lower the CPU overhead required to manage the device?\n36.4 Lowering CPU Overhead With Interrupts\nThe invention that many engineers came upon years ago to improve\nthis interaction is something we’ve seen already: the interrupt . Instead of\npolling the device repeatedly, the OS can issue a request, put the calling\nprocess to sleep, and context switch to another task. When the de vice\nis ﬁnally ﬁnished with the operation, it will raise a hardware i nterrupt,\ncausing the CPU to jump into the OS at a predetermined interrupt service\nroutine (ISR) or more simply an interrupt handler . The handler is just a\npiece of operating system code that will ﬁnish the request (for ex ample,\nby reading data and perhaps an error code from the device) and wak e the\nprocess waiting for the I/O, which can then proceed as desired.\nInterrupts thus allow for overlap of computation and I/O, which is\nkey for improved utilization. This timeline shows the problem:\nCPU\nDisk 1111111111ppppp11111\nIn the diagram, Process 1 runs on the CPU for some time (indicated b y\na repeated 1on the CPU line), and then issues an I/O request to the disk\nto read some data. Without interrupts, the system simply spins , polling\nthe status of the device repeatedly until the I/O is complete (i ndicated by\nap). The disk services the request and ﬁnally Process 1 can run ag ain.\nIf instead we utilize interrupts and allow for overlap, the OS ca n do\nsomething else while waiting for the disk:\nCPU\nDisk 11111111112222211111\nIn this example, the OS runs Process 2 on the CPU while the disk se r-\nvices Process 1’s request. When the disk request is ﬁnished, an interrupt\noccurs, and the OS wakes up Process 1 and runs it again. Thus, both the\nCPU and the disk are properly utilized during the middle stret ch of time.\nNote that using interrupts is not always the best solution. For example,\nimagine a device that performs its tasks very quickly: the ﬁrs t poll usually\nﬁnds the device to be done with task. Using an interrupt in this case will\nactually slow down the system: switching to another process, handling the\ninterrupt, and switching back to the issuing process is expen sive. Thus, if\na device is fast, it may be best to poll; if it is slow, interrupts , which allow\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 I/O D EVICES\nTIP: INTERRUPTS NOTALWAYS BETTER THAN PIO\nAlthough interrupts allow for overlap of computation and I/O, they on ly\nreally make sense for slow devices. Otherwise, the cost of interr upt han-\ndling and context switching may outweigh the beneﬁts interrup ts pro-\nvide. There are also cases where a ﬂood of interrupts may overload a sys-\ntem and lead it to livelock [MR96]; in such cases, polling provid es more\ncontrol to the OS in its scheduling and thus is again useful.\noverlap, are best. If the speed of the device is not known, or sometim es\nfast and sometimes slow, it may be best to use a hybrid that polls for a\nlittle while and then, if the device is not yet ﬁnished, uses in terrupts. This\ntwo-phased approach may achieve the best of both worlds.\nAnother reason not to use interrupts arises in networks [MR96]. W hen\na huge stream of incoming packets each generate an interrupt, i t is pos-\nsible for the OS to livelock , that is, ﬁnd itself only processing interrupts\nand never allowing a user-level process to run and actually ser vice the re-\nquests. For example, imagine a web server that experiences a l oad burst\nbecause it became the top-ranked entry on hacker news [H18]. In this\ncase, it is better to occasionally use polling to better control wh at is hap-\npening in the system and allow the web server to service some req uests\nbefore going back to the device to check for more packet arrivals.\nAnother interrupt-based optimization is coalescing . In such a setup, a\ndevice which needs to raise an interrupt ﬁrst waits for a bit be fore deliv-\nering the interrupt to the CPU. While waiting, other requests may soon\ncomplete, and thus multiple interrupts can be coalesced into a single in-\nterrupt delivery, thus lowering the overhead of interrupt proce ssing. Of\ncourse, waiting too long will increase the latency of a request, a common\ntrade-off in systems. See Ahmad et al. [A+11] for an excellent su mmary.\n36.5 More Efﬁcient Data Movement With DMA\nUnfortunately, there is one other aspect of our canonical protocol tha t\nrequires our attention. In particular, when using programmed I /O (PIO)\nto transfer a large chunk of data to a device, the CPU is once agai n over-\nburdened with a rather trivial task, and thus wastes a lot of tim e and\neffort that could better be spent running other processes. This t imeline\nillustrates the problem:\nCPU\nDisk 1111111111ccc2222211\nIn the timeline, Process 1 is running and then wishes to write s ome data to\nthe disk. It then initiates the I/O, which must copy the data fr om memory\nto the device explicitly, one word at a time (marked cin the diagram).\nWhen the copy is complete, the I/O begins on the disk and the CPU ca n\nﬁnally be used for something else.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nI/O D EVICES 7\nTHECRUX: HOWTOLOWER PIO O VERHEADS\nWith PIO, the CPU spends too much time moving data to and from\ndevices by hand. How can we ofﬂoad this work and thus allow the CPU\nto be more effectively utilized?\nThe solution to this problem is something we refer to as Direct Mem-\nory Access (DMA) . A DMA engine is essentially a very speciﬁc device\nwithin a system that can orchestrate transfers between devic es and main\nmemory without much CPU intervention.\nDMA works as follows. To transfer data to the device, for example, the\nOS would program the DMA engine by telling it where the data live s in\nmemory, how much data to copy, and which device to send it to. At tha t\npoint, the OS is done with the transfer and can proceed with other w ork.\nWhen the DMA is complete, the DMA controller raises an interrupt , and\nthe OS thus knows the transfer is complete. The revised timelin e:\nCPU\nDMA\nDisk 11111111112222222211\nccc\nFrom the timeline, you can see that the copying of data is now handle d\nby the DMA controller. Because the CPU is free during that time, the OS\ncan do something else, here choosing to run Process 2. Process 2 thu s gets\nto use more CPU before Process 1 runs again.\n36.6 Methods Of Device Interaction\nNow that we have some sense of the efﬁciency issues involved with\nperforming I/O, there are a few other problems we need to handle t o\nincorporate devices into modern systems. One problem you may have\nnoticed thus far: we have not really said anything about how the OS ac-\ntually communicates with the device! Thus, the problem:\nTHECRUX: HOWTOCOMMUNICATE WITHDEVICES\nHow should the hardware communicate with a device? Should there\nbe explicit instructions? Or are there other ways to do it?\nOver time, two primary methods of device communication have de-\nveloped. The ﬁrst, oldest method (used by IBM mainframes for many\nyears) is to have explicit I/O instructions . These instructions specify a\nway for the OS to send data to speciﬁc device registers and thus allow the\nconstruction of the protocols described above.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 I/O D EVICES\nFor example, on x86, the inandout instructions can be used to com-\nmunicate with devices. For example, to send data to a device, t he caller\nspeciﬁes a register with the data in it, and a speciﬁc port which names the\ndevice. Executing the instruction leads to the desired behav ior.\nSuch instructions are usually privileged . The OS controls devices, and\nthe OS thus is the only entity allowed to directly communicate wi th them.\nImagine if any program could read or write the disk, for example: t otal\nchaos (as always), as any user program could use such a loophole to ga in\ncomplete control over the machine.\nThe second method to interact with devices is known as memory-\nmapped I/O . With this approach, the hardware makes device registers\navailable as if they were memory locations. To access a particul ar register,\nthe OS issues a load (to read) or store (to write) the address; the hardware\nthen routes the load/store to the device instead of main memory.\nThere is not some great advantage to one approach or the other. The\nmemory-mapped approach is nice in that no new instructions are n eeded\nto support it, but both approaches are still in use today.\n36.7 Fitting Into The OS: The Device Driver\nOne ﬁnal problem we will discuss: how to ﬁt devices, each of which\nhave very speciﬁc interfaces, into the OS, which we would like t o keep\nas general as possible. For example, consider a ﬁle system. We’d l ike\nto build a ﬁle system that worked on top of SCSI disks, IDE disks, USB\nkeychain drives, and so forth, and we’d like the ﬁle system to be relatively\noblivious to all of the details of how to issue a read or write request to\nthese difference types of drives. Thus, our problem:\nTHECRUX: HOWTOBUILD A D EVICE -NEUTRAL OS\nHow can we keep most of the OS device-neutral, thus hiding the de-\ntails of device interactions from major OS subsystems?\nThe problem is solved through the age-old technique of abstraction .\nAt the lowest level, a piece of software in the OS must know in detai l\nhow a device works. We call this piece of software a device driver , and\nany speciﬁcs of device interaction are encapsulated within.\nLet us see how this abstraction might help OS design and impleme n-\ntation by examining the Linux ﬁle system software stack. Figur e 36.4 is\na rough and approximate depiction of the Linux software organizati on.\nAs you can see from the diagram, a ﬁle system (and certainly, an a ppli-\ncation above) is completely oblivious to the speciﬁcs of which disk class\nit is using; it simply issues block read and write requests to t he generic\nblock layer, which routes them to the appropriate device driver , which\nhandles the details of issuing the speciﬁc request. Although s impliﬁed,\nthe diagram shows how such detail can be hidden from most of the OS.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nI/O D EVICES 9\nApplication\nFile System Raw\nGeneric Block Layer\nDevice Driver [SCSI, ATA, etc.]POSIX API [open, read, write, close, etc.]\nGeneric Block Interface [block read/write]\nSpecific Block Interface [protocol-specific read/write]\nuser kernel mode\nFigure 36.4: The File System Stack\nThe diagram also shows a raw interface to devices, which enables spe-\ncial applications (such as a ﬁle-system checker , described later [AD14],\nor a disk defragmentation tool) to directly read and write blocks without\nusing the ﬁle abstraction. Most systems provide this type of inte rface to\nsupport these low-level storage management applications.\nNote that the encapsulation seen above can have its downside as we ll.\nFor example, if there is a device that has many special capabil ities, but\nhas to present a generic interface to the rest of the kernel, th ose special\ncapabilities will go unused. This situation arises, for examp le, in Linux\nwith SCSI devices, which have very rich error reporting; because other\nblock devices (e.g., ATA/IDE) have much simpler error handlin g, all that\nhigher levels of software ever receive is a generic EIO (generic IO error)\nerror code; any extra detail that SCSI may have provided is thus lost to\nthe ﬁle system [G08].\nInterestingly, because device drivers are needed for any dev ice you\nmight plug into your system, over time they have come to represen t a\nhuge percentage of kernel code. Studies of the Linux kernel revea l that\nover 70% of OS code is found in device drivers [C01]; for Windows-bas ed\nsystems, it is likely quite high as well. Thus, when people tel l you that the\nOS has millions of lines of code, what they are really saying is tha t the OS\nhas millions of lines of device-driver code. Of course, for any give n in-\nstallation, most of that code may not be active (i.e., only a few devi ces are\nconnected to the system at a time). Perhaps more depressingly, as drivers\nare often written by “amateurs” (instead of full-time kernel d evelopers),\nthey tend to have many more bugs and thus are a primary contribut or to\nkernel crashes [S03].\n36.8 Case Study: A Simple IDE Disk Driver\nTo dig a little deeper here, let’s take a quick look at an actual de vice: an\nIDE disk drive [L94]. We summarize the protocol as described in t his ref-\nerence [W10]; we’ll also peek at the xv6 source code for a simple ex ample\nof a working IDE driver [CK+08].\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 I/O D EVICES\nControl Register:\nAddress 0x3F6 = 0x08 (0000 1RE0): R=reset, E=0 means ""enable interrupt""\nCommand Block Registers:\nAddress 0x1F0 = Data Port\nAddress 0x1F1 = Error\nAddress 0x1F2 = Sector Count\nAddress 0x1F3 = LBA low byte\nAddress 0x1F4 = LBA mid byte\nAddress 0x1F5 = LBA hi byte\nAddress 0x1F6 = 1B1D TOP4LBA: B=LBA, D=drive\nAddress 0x1F7 = Command/status\nStatus Register (Address 0x1F7):\n7 6 5 4 3 2 1 0\nBUSY READY FAULT SEEK DRQ CORR IDDEX ERROR\nError Register (Address 0x1F1): (check when Status ERROR== 1)\n7 6 5 4 3 2 1 0\nBBK UNC MC IDNF MCR ABRT T0NF AMNF\nBBK = Bad Block\nUNC = Uncorrectable data error\nMC = Media Changed\nIDNF = ID mark Not Found\nMCR = Media Change Requested\nABRT = Command aborted\nT0NF = Track 0 Not Found\nAMNF = Address Mark Not Found\nFigure 36.5: The IDE Interface\nAn IDE disk presents a simple interface to the system, consist ing of\nfour types of register: control, command block, status, and error. T hese\nregisters are available by reading or writing to speciﬁc “I/O addresses”\n(such as0x3F6 below) using (on x86) the inandout I/O instructions.\nThe basic protocol to interact with the device is as follows, assum ing\nit has already been initialized.\n•Wait for drive to be ready. Read Status Register (0x1F7) until drive\nis READY and not BUSY.\n•Write parameters to command registers. Write the sector count,\nlogical block address (LBA) of the sectors to be accessed, and dri ve\nnumber (master=0x00 or slave=0x10, as IDE permits just two dr ives)\nto command registers (0x1F2-0x1F6).\n•Start the I/O. by issuing read/write to command register. Write\nREAD—WRITE command to command register (0x1F7).\n•Data transfer (for writes): Wait until drive status is READY and\nDRQ (drive request for data); write data to data port.\n•Handle interrupts. In the simplest case, handle an interrupt for\neach sector transferred; more complex approaches allow batching\nand thus one ﬁnal interrupt when the entire transfer is complet e.\n•Error handling. After each operation, read the status register. If the\nERROR bit is on, read the error register for details.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nI/O D EVICES 11\nstatic int ide_wait_ready() {\nwhile (((int r = inb(0x1f7)) & IDE_BSY) || !(r & IDE_DRDY))\n; // loop until drive isn’t busy\n}\nstatic void ide_start_request(struct buf *b) {\nide_wait_ready();\noutb(0x3f6, 0); // generate interrupt\noutb(0x1f2, 1); // how many sectors?\noutb(0x1f3, b->sector & 0xff); // LBA goes here ...\noutb(0x1f4, (b->sector >> 8) & 0xff); // ... and here\noutb(0x1f5, (b->sector >> 16) & 0xff); // ... and here!\noutb(0x1f6, 0xe0 | ((b->dev&1)<<4) | ((b->sector>>24)&0x 0f));\nif(b->flags & B_DIRTY){\noutb(0x1f7, IDE_CMD_WRITE); // this is a WRITE\noutsl(0x1f0, b->data, 512/4); // transfer data too!\n} else {\noutb(0x1f7, IDE_CMD_READ); // this is a READ (no data)\n}\n}\nvoid ide_rw(struct buf *b) {\nacquire(&ide_lock);\nfor (struct buf **pp = &ide_queue; *pp; pp=&( *pp)->qnext)\n; // walk queue\n*pp = b; // add request to end\nif (ide_queue == b) // if q is empty\nide_start_request(b); // send req to disk\nwhile ((b->flags & (B_VALID|B_DIRTY)) != B_VALID)\nsleep(b, &ide_lock); // wait for completion\nrelease(&ide_lock);\n}\nvoid ide_intr() {\nstruct buf *b;\nacquire(&ide_lock);\nif (!(b->flags & B_DIRTY) && ide_wait_ready() >= 0)\ninsl(0x1f0, b->data, 512/4); // if READ: get data\nb->flags |= B_VALID;\nb->flags &= ˜B_DIRTY;\nwakeup(b); // wake waiting process\nif ((ide_queue = b->qnext) != 0) // start next request\nide_start_request(ide_queue); // (if one exists)\nrelease(&ide_lock);\n}\nFigure 36.6: The xv6 IDE Disk Driver (Simpliﬁed)\nMost of this protocol is found in the xv6 IDE driver (Figure 36.6),\nwhich (after initialization) works through four primary functi ons. The\nﬁrst isiderw() , which queues a request (if there are others pending),\nor issues it directly to the disk (via idestartrequest() ); in either\ncase, the routine waits for the request to complete and the calli ng pro-\ncess is put to sleep. The second is idestartrequest() , which is\nused to send a request (and perhaps data, in the case of a write) to the\ndisk; theinandout x86 instructions are called to read and write device\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 I/O D EVICES\nregisters, respectively. The start request routine uses the third function,\nidewaitready() , to ensure the drive is ready before issuing a request\nto it. Finally, ideintr() is invoked when an interrupt takes place; it\nreads data from the device (if the request is a read, not a write) , wakes the\nprocess waiting for the I/O to complete, and (if there are more req uests\nin the I/O queue), launches the next I/O via idestartrequest() .\n36.9 Historical Notes\nBefore ending, we include a brief historical note on the origin of som e\nof these fundamental ideas. If you are interested in learning m ore, read\nSmotherman’s excellent summary [S08].\nInterrupts are an ancient idea, existing on the earliest of mac hines. For\nexample, the UNIVAC in the early 1950’s had some form of interrupt vec-\ntoring, although it is unclear in exactly which year this featu re was avail-\nable [S08]. Sadly, even in its infancy, we are beginning to lose t he origins\nof computing history.\nThere is also some debate as to which machine ﬁrst introduced th e idea\nof DMA. For example, Knuth and others point to the DYSEAC (a “mo-\nbile” machine, which at the time meant it could be hauled in a tr ailer),\nwhereas others think the IBM SAGE may have been the ﬁrst [S08]. Ei -\nther way, by the mid 50’s, systems with I/O devices that communi cated\ndirectly with memory and interrupted the CPU when ﬁnished exi sted.\nThe history here is difﬁcult to trace because the inventions ar e tied to\nreal, and sometimes obscure, machines. For example, some think t hat the\nLincoln Labs TX-2 machine was ﬁrst with vectored interrupts [S0 8], but\nthis is hardly clear.\nBecause the ideas are relatively obvious — no Einsteinian leap is re-\nquired to come up with the idea of letting the CPU do something els e\nwhile a slow I/O is pending — perhaps our focus on “who ﬁrst?” is mis -\nguided. What is certainly clear: as people built these early m achines, it\nbecame obvious that I/O support was needed. Interrupts, DMA, an d re-\nlated ideas are all direct outcomes of the nature of fast CPUs and s low\ndevices; if you were there at the time, you might have had simila r ideas.\n36.10 Summary\nYou should now have a very basic understanding of how an OS inter-\nacts with a device. Two techniques, the interrupt and DMA, ha ve been\nintroduced to help with device efﬁciency, and two approaches t o access-\ning device registers, explicit I/O instructions and memory-m apped I/O,\nhave been described. Finally, the notion of a device driver has b een pre-\nsented, showing how the OS itself can encapsulate low-level det ails and\nthus make it easier to build the rest of the OS in a device-neutr al fashion.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nI/O D EVICES 13\nReferences\n[A+11] “vIC: Interrupt Coalescing for Virtual Machine Storage Device I O” by Irfan Ahmad,\nAjay Gulati, Ali Mashtizadeh. USENIX ’11. A terriﬁc survey of interrupt coalescing in traditional\nand virtualized environments.\n[AD14] “Operating Systems: Three Easy Pieces” (Chapters: Crash Consis tency: FSCK and\nJournaling and Log-Structured File Systems) by Remzi Arpaci-Dussea u and Andrea Arpaci-\nDusseau. Arpaci-Dusseau Books, 2014. A description of a ﬁle-system checker and how it works,\nwhich requires low-level access to disk devices not normally provide d by the ﬁle system directly.\n[C01] “An Empirical Study of Operating System Errors” by Andy Chou, Junfeng Yang, Ben-\njamin Chelf, Seth Hallem, Dawson Engler. SOSP ’01. One of the ﬁrst papers to systematically\nexplore how many bugs are in modern operating systems. Among other neat ﬁndi ngs, the authors show\nthat device drivers have something like seven times more bugs than mainli ne kernel code.\n[CK+08] “The xv6 Operating System” by Russ Cox, Frans Kaashoek, Robe rt Morris, Nickolai\nZeldovich. From: http://pdos.csail.mit.edu/6.828/2008/inde x.html. Seeide.c for the IDE\ndevice driver, with a few more details therein.\n[D07] “What Every Programmer Should Know About Memory” by Ulrich Dre pper. Novem-\nber, 2007. Available: http://www.akkadia.org/drepper/cpumemory.pdf .A fantastic\nread about modern memory systems, starting at DRAM and going all the way up to vir tualization and\ncache-optimized algorithms.\n[G08] “EIO: Error-handling is Occasionally Correct” by Haryadi Gunawi, C indy Rubio-Gonzalez,\nAndrea Arpaci-Dusseau, Remzi Arpaci-Dusseau, Ben Liblit. FAST ’ 08, San Jose, CA, February\n2008. Our own work on building a tool to ﬁnd code in Linux ﬁle systems that does not han dle error\nreturn properly. We found hundreds and hundreds of bugs, many of whic h have now been ﬁxed.\n[H17] “Intel Core i7-7700K review: Kaby Lake Debuts for Desktop” by Joel Hruska. January 3,\n2017.www.extremetech.com/extreme/241950-intels-core-i7-7 700k-reviewed-kaby\n-lake-debuts-desktop .An in-depth review of a recent Intel chipset, including CPUs and the\nI/O subsystem.\n[H18] “Hacker News” by Many contributors. Available: https://news. ycombinator.com. One\nof the better aggregrators for tech-related stuff. Once back in 2014, this book be came a highly-ranked\nentry, leading to 1 million chapter downloads in just one day! Sadly, we have yet to re-experience such\na high.\n[L94] “AT Attachment Interface for Disk Drives” by Lawrence J. Lamers. Re ference number:\nANSI X3.221, 1994. Available: ftp://ftp.t10.org/t13/project/d0791r4c-ATA-1.pdf .\nA rather dry document about device interfaces. Read it at your own peril.\n[MR96] “Eliminating Receive Livelock in an Interrupt-driven Kernel” by Jeffrey Mogul, K. K.\nRamakrishnan. USENIX ’96, San Diego, CA, January 1996. Mogul and colleagues did a great deal\nof pioneering work on web server network performance. This paper is but one example.\n[S08] “Interrupts” by Mark Smotherman. July ’08. Available: http://people.cs.clemson.edu/\n˜mark/interrupts.html .A treasure trove of information on the history of interrupts, DMA, and\nrelated early ideas in computing.\n[S03] “Improving the Reliability of Commodity Operating Systems ” by Michael M. Swift, Brian\nN. Bershad, Henry M. Levy. SOSP ’03. Swift’s work revived interest in a more microkernel-like\napproach to operating systems; minimally, it ﬁnally gave some good reasons why address-space based\nprotection could be useful in a modern OS.\n[W10] “Hard Disk Driver” by Washington State Course Homepage. A vailable online at this\nsite:http://eecs.wsu.edu/˜cs460/cs560/HDdriver.html .A nice summary of a simple\nIDE disk drive’s interface and how to build a device driver for it.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",31562
43-37. Hard Disk Drives.pdf,43-37. Hard Disk Drives,"37\nHard Disk Drives\nThe last chapter introduced the general concept of an I/O device and\nshowed you how the OS might interact with such a beast. In this cha pter,\nwe dive into more detail about one device in particular: the hard disk\ndrive . These drives have been the main form of persistent data storage in\ncomputer systems for decades and much of the development of ﬁle sys -\ntem technology (coming soon) is predicated on their behavior. Thus, it\nis worth understanding the details of a disk’s operation before bui lding\nthe ﬁle system software that manages it. Many of these details a re avail-\nable in excellent papers by Ruemmler and Wilkes [RW92] and An derson,\nDykes, and Riedel [ADR03].\nCRUX: HOWTOSTORE ANDACCESS DATA ONDISK\nHow do modern hard-disk drives store data? What is the interface ?\nHow is the data actually laid out and accessed? How does disk sched ul-\ning improve performance?\n37.1 The Interface\nLet’s start by understanding the interface to a modern disk dri ve. The\nbasic interface for all modern drives is straightforward. The d rive consists\nof a large number of sectors (512-byte blocks), each of which can be read\nor written. The sectors are numbered from 0ton−1on a disk with n\nsectors. Thus, we can view the disk as an array of sectors; 0ton−1is\nthus the address space of the drive.\nMulti-sector operations are possible; indeed, many ﬁle systems will\nread or write 4KB at a time (or more). However, when updating the di sk,\nthe only guarantee drive manufacturers make is that a single 5 12-byte\nwrite is atomic (i.e., it will either complete in its entirety or it won’t com-\nplete at all); thus, if an untimely power loss occurs, only a portion of a\nlarger write may complete (sometimes called a torn write ).\n1\n2 HARD DISKDRIVES\n0111098\n7\n6\n5\n4321Spindle\nFigure 37.1: A Disk With Just A Single Track\nThere are some assumptions most clients of disk drives make, but\nthat are not speciﬁed directly in the interface; Schlosser and G anger have\ncalled this the “unwritten contract” of disk drives [SG04]. Spec iﬁcally,\none can usually assume that accessing two blocks1near one-another within\nthe drive’s address space will be faster than accessing two bl ocks that are\nfar apart. One can also usually assume that accessing blocks i n a contigu-\nous chunk (i.e., a sequential read or write) is the fastest acce ss mode, and\nusually much faster than any more random access pattern.\n37.2 Basic Geometry\nLet’s start to understand some of the components of a modern disk.\nWe start with a platter , a circular hard surface on which data is stored\npersistently by inducing magnetic changes to it. A disk may h ave one\nor more platters; each platter has 2 sides, each of which is calle d asur-\nface. These platters are usually made of some hard material (such as\naluminum), and then coated with a thin magnetic layer that ena bles the\ndrive to persistently store bits even when the drive is powered off.\nThe platters are all bound together around the spindle , which is con-\nnected to a motor that spins the platters around (while the drive is pow-\nered on) at a constant (ﬁxed) rate. The rate of rotation is often meas ured in\nrotations per minute (RPM) , and typical modern values are in the 7,200\nRPM to 15,000 RPM range. Note that we will often be interested in the\ntime of a single rotation, e.g., a drive that rotates at 10,000 RPM means\nthat a single rotation takes about 6 milliseconds (6 ms).\nData is encoded on each surface in concentric circles of sectors; w e call\none such concentric circle a track . A single surface contains many thou-\nsands and thousands of tracks, tightly packed together, with hu ndreds of\ntracks ﬁtting into the width of a human hair.\nTo read and write from the surface, we need a mechanism that all ows\nus to either sense (i.e., read) the magnetic patterns on the di sk or to in-\nduce a change in (i.e., write) them. This process of reading and writing is\naccomplished by the disk head ; there is one such head per surface of the\ndrive. The disk head is attached to a single disk arm , which moves across\nthe surface to position the head over the desired track.\n1We, and others, often use the terms block and sector interchangeably, assuming the\nreader will know exactly what is meant per context. Sorry about this!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nHARD DISKDRIVES 3\nHead\nArm0111098\n7\n6\n5\n4321SpindleRotates this way\nFigure 37.2: A Single Track Plus A Head\n37.3 A Simple Disk Drive\nLet’s understand how disks work by building up a model one track at\na time. Assume we have a simple disk with a single track (Figur e 37.1).\nThis track has just 12 sectors, each of which is 512 bytes in size (our\ntypical sector size, recall) and addressed therefore by the nu mbers 0 through\n11. The single platter we have here rotates around the spindle, to which\na motor is attached. Of course, the track by itself isn’t too intere sting; we\nwant to be able to read or write those sectors, and thus we need a di sk\nhead, attached to a disk arm, as we now see (Figure 37.2).\nIn the ﬁgure, the disk head, attached to the end of the arm, is pos i-\ntioned over sector 6, and the surface is rotating counter-clockwis e.\nSingle-track Latency: The Rotational Delay\nTo understand how a request would be processed on our simple, one-\ntrack disk, imagine we now receive a request to read block 0. How s hould\nthe disk service this request?\nIn our simple disk, the disk doesn’t have to do much. In particula r, it\nmust just wait for the desired sector to rotate under the disk hea d. This\nwait happens often enough in modern drives, and is an important en ough\ncomponent of I/O service time, that it has a special name: rotational de-\nlay(sometimes rotation delay , though that sounds weird). In the exam-\nple, if the full rotational delay is R, the disk has to incur a rotational delay\nof aboutR\n2to wait for 0 to come under the read/write head (if we start at\n6). A worst-case request on this single track would be to sector 5, causing\nnearly a full rotational delay in order to service such a request .\nMultiple Tracks: Seek Time\nSo far our disk just has a single track, which is not too realistic; modern\ndisks of course have many millions. Let’s thus look at ever-so-sligh tly\nmore realistic disk surface, this one with three tracks (Figur e 37.3, left).\nIn the ﬁgure, the head is currently positioned over the innermost track\n(which contains sectors 24 through 35); the next track over contai ns the\nnext set of sectors (12 through 23), and the outermost track contain s the\nﬁrst sectors (0 through 11).\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 HARD DISKDRIVES\n011109\n8\n7\n6\n5\n4\n32112232221\n20\n19\n18\n17\n16\n1514132435343332\n31\n30\n29\n28272625SpindleRotates this way\nSeekRemaining rotation\n3210\n11\n10\n9\n8\n7\n65415141312\n23\n22\n21\n20\n19\n1817162726252435\n34\n33\n32\n31302928SpindleRotates this way\nFigure 37.3: Three Tracks Plus A Head (Right: With Seek)\nTo understand how the drive might access a given sector, we now tr ace\nwhat would happen on a request to a distant sector, e.g., a read to sector\n11. To service this read, the drive has to ﬁrst move the disk arm to the cor-\nrect track (in this case, the outermost one), in a process known as a seek .\nSeeks, along with rotations, are one of the most costly disk operations.\nThe seek, it should be noted, has many phases: ﬁrst an acceleration\nphase as the disk arm gets moving; then coasting as the arm is moving\nat full speed, then deceleration as the arm slows down; ﬁnally settling as\nthe head is carefully positioned over the correct track. The settling time\nis often quite signiﬁcant, e.g., 0.5 to 2 ms, as the drive must b e certain to\nﬁnd the right track (imagine if it just got close instead!).\nAfter the seek, the disk arm has positioned the head over the righ t\ntrack. A depiction of the seek is found in Figure 37.3 (right).\nAs we can see, during the seek, the arm has been moved to the desi red\ntrack, and the platter of course has rotated, in this case about 3 s ectors.\nThus, sector 9 is just about to pass under the disk head, and we mu st\nonly endure a short rotational delay to complete the transfer.\nWhen sector 11 passes under the disk head, the ﬁnal phase of I/O\nwill take place, known as the transfer , where data is either read from or\nwritten to the surface. And thus, we have a complete picture of I /O time:\nﬁrst a seek, then waiting for the rotational delay, and ﬁnally th e transfer.\nSome Other Details\nThough we won’t spend too much time on it, there are some other inter-\nesting details about how hard drives operate. Many drives employ some\nkind of track skew to make sure that sequential reads can be properly\nserviced even when crossing track boundaries. In our simple exa mple\ndisk, this might appear as seen in Figure 37.4.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nHARD DISKDRIVES 5\nTrack skew: 2 blocks011109\n8\n7\n6\n5\n4\n32122212019\n18\n17\n16\n15\n14\n1312233231302928\n27\n26\n25\n24353433SpindleRotates this way\nFigure 37.4: Three Tracks: Track Skew Of 2\nSectors are often skewed like this because when switching from one\ntrack to another, the disk needs time to reposition the head (eve n to neigh-\nboring tracks). Without such skew, the head would be moved to the n ext\ntrack but the desired next block would have already rotated unde r the\nhead, and thus the drive would have to wait almost the entire rota tional\ndelay to access the next block.\nAnother reality is that outer tracks tend to have more sectors tha n\ninner tracks, which is a result of geometry; there is simply more room\nout there. These tracks are often referred to as multi-zoned disk drives,\nwhere the disk is organized into multiple zones, and where a zone is con-\nsecutive set of tracks on a surface. Each zone has the same number of\nsectors per track, and outer zones have more sectors than inner zone s.\nFinally, an important part of any modern disk drive is its cache , for\nhistorical reasons sometimes called a track buffer . This cache is just some\nsmall amount of memory (usually around 8 or 16 MB) which the drive\ncan use to hold data read from or written to the disk. For example, w hen\nreading a sector from the disk, the drive might decide to read in all of the\nsectors on that track and cache them in its memory; doing so allows t he\ndrive to quickly respond to any subsequent requests to the sam e track.\nOn writes, the drive has a choice: should it acknowledge the writ e has\ncompleted when it has put the data in its memory, or after the writ e has\nactually been written to disk? The former is called write back caching\n(or sometimes immediate reporting ), and the latter write through . Write\nback caching sometimes makes the drive appear “faster”, but c an be dan-\ngerous; if the ﬁle system or applications require that data be wr itten to\ndisk in a certain order for correctness, write-back caching can lead to\nproblems (read the chapter on ﬁle-system journaling for details ).\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 HARD DISKDRIVES\nASIDE : DIMENSIONAL ANALYSIS\nRemember in Chemistry class, how you solved virtually every prob -\nlem by simply setting up the units such that they canceled out, and some-\nhow the answers popped out as a result? That chemical magic is know n\nby the highfalutin name of dimensional analysis and it turns out it is\nuseful in computer systems analysis too.\nLet’s do an example to see how dimensional analysis works and why\nit is useful. In this case, assume you have to ﬁgure out how long, in mil-\nliseconds, a single rotation of a disk takes. Unfortunately, you ar e given\nonly the RPM of the disk, or rotations per minute . Let’s assume we’re\ntalking about a 10K RPM disk (i.e., it rotates 10,000 times per m inute).\nHow do we set up the dimensional analysis so that we get time per r ota-\ntion in milliseconds?\nTo do so, we start by putting the desired units on the left; in thi s case,\nwe wish to obtain the time (in milliseconds) per rotation, so that is ex-\nactly what we write down:Time(ms)\n1Rotation. We then write down everything\nwe know, making sure to cancel units where possible. First, we ob tain\n1minute\n10,000Rotations(keeping rotation on the bottom, as that’s where it is on\nthe left), then transform minutes into seconds with60seconds\n1minute, and then\nﬁnally transform seconds in milliseconds with1000ms\n1second. The ﬁnal result is\nthe following (with units nicely canceled):\nTime(ms)\n1Rot.=1✘✘✘minute\n10,000Rot.·60✘✘✘seconds\n1✘✘✘minute·1000ms\n1✘✘✘second=60,000ms\n10,000Rot.=6ms\nRotation\nAs you can see from this example, dimensional analysis makes wha t\nseems intuitive into a simple and repeatable process. Beyond t he\nRPM calculation above, it comes in handy with I/O analysis regul arly.\nFor example, you will often be given the transfer rate of a disk, e. g.,\n100 MB/second, and then asked: how long does it take to transfer a\n512 KB block (in milliseconds)? With dimensional analysis, it’s easy:\nTime(ms)\n1Request=512✟✟KB\n1Request·1✟✟MB\n1024✟✟KB·1✘✘✘second\n100✟✟MB·1000ms\n1✘✘✘second=5ms\nRequest\n37.4 I/O Time: Doing The Math\nNow that we have an abstract model of the disk, we can use a little\nanalysis to better understand disk performance. In particul ar, we can\nnow represent I/O time as the sum of three major components:\nTI/O=Tseek+Trotation +Ttransfer (37.1)\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nHARD DISKDRIVES 7\nCheetah 15K.5 Barracuda\nCapacity 300 GB 1 TB\nRPM 15,000 7,200\nAverage Seek 4 ms 9 ms\nMax Transfer 125 MB/s 105 MB/s\nPlatters 4 4\nCache 16 MB 16/32 MB\nConnects via SCSI SATA\nFigure 37.5: Disk Drive Specs: SCSI Versus SATA\nNote that the rate of I/O ( RI/O), which is often more easily used for\ncomparison between drives (as we will do below), is easily comput ed\nfrom the time. Simply divide the size of the transfer by the time i t took:\nRI/O=SizeTransfer\nTI/O(37.2)\nTo get a better feel for I/O time, let us perform the following calc u-\nlation. Assume there are two workloads we are interested in. The ﬁrst,\nknown as the random workload, issues small (e.g., 4KB) reads to random\nlocations on the disk. Random workloads are common in many impor-\ntant applications, including database management systems. The second,\nknown as the sequential workload, simply reads a large number of sec-\ntors consecutively from the disk, without jumping around. Sequent ial\naccess patterns are quite common and thus important as well.\nTo understand the difference in performance between random an d se-\nquential workloads, we need to make a few assumptions about the di sk\ndrive ﬁrst. Let’s look at a couple of modern disks from Seagate. The ﬁrs t,\nknown as the Cheetah 15K.5 [S09b], is a high-performance SCSI driv e.\nThe second, the Barracuda [S09a], is a drive built for capacity. Details on\nboth are found in Figure 37.5.\nAs you can see, the drives have quite different characteristi cs, and\nin many ways nicely summarize two important components of the dis k\ndrive market. The ﬁrst is the “high performance” drive market , where\ndrives are engineered to spin as fast as possible, deliver low s eek times,\nand transfer data quickly. The second is the “capacity” marke t, where\ncost per byte is the most important aspect; thus, the drives are s lower but\npack as many bits as possible into the space available.\nFrom these numbers, we can start to calculate how well the drive s\nwould do under our two workloads outlined above. Let’s start by looking\nat the random workload. Assuming each 4 KB read occurs at a random\nlocation on disk, we can calculate how long each such read would take .\nOn the Cheetah:\nTseek= 4ms, T rotation = 2ms, T transfer = 30microsecs (37.3)\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 HARD DISKDRIVES\nTIP: USEDISKS SEQUENTIALLY\nWhen at all possible, transfer data to and from disks in a sequen tial man-\nner. If sequential is not possible, at least think about transfe rring data\nin large chunks: the bigger, the better. If I/O is done in littl e random\npieces, I/O performance will suffer dramatically. Also, user s will suffer.\nAlso, you will suffer, knowing what suffering you have wrought wit h\nyour careless random I/Os.\nThe average seek time (4 milliseconds) is just taken as the ave rage time\nreported by the manufacturer; note that a full seek (from one end of the\nsurface to the other) would likely take two or three times longer. The\naverage rotational delay is calculated from the RPM directly. 1 5000 RPM\nis equal to 250 RPS (rotations per second); thus, each rotation tak es 4 ms.\nOn average, the disk will encounter a half rotation and thus 2 ms i s the\naverage time. Finally, the transfer time is just the size of th e transfer over\nthe peak transfer rate; here it is vanishingly small (30 microseconds ; note\nthat we need 1000 microseconds just to get 1 millisecond!).\nThus, from our equation above, TI/Ofor the Cheetah roughly equals\n6 ms. To compute the rate of I/O, we just divide the size of the tran sfer\nby the average time, and thus arrive at RI/Ofor the Cheetah under the\nrandom workload of about 0.66MB/s. The same calculation for the Bar-\nracuda yields a TI/Oof about 13.2 ms, more than twice as slow, and thus\na rate of about 0.31MB/s.\nNow let’s look at the sequential workload. Here we can assume there\nis a single seek and rotation before a very long transfer. For simpl icity,\nassume the size of the transfer is 100 MB. Thus, TI/Ofor the Cheetah and\nBarracuda is about 800 ms and 950 ms, respectively. The rates of I/O\nare thus very nearly the peak transfer rates of 125 MB/s and 105 MB/s,\nrespectively. Figure 37.6 summarizes these numbers.\nThe ﬁgure shows us a number of important things. First, and most\nimportantly, there is a huge gap in drive performance between r andom\nand sequential workloads, almost a factor of 200 or so for the Cheetah\nand more than a factor 300 difference for the Barracuda. And thus we\narrive at the most obvious design tip in the history of computing.\nA second, more subtle point: there is a large difference in perfor mance\nbetween high-end “performance” drives and low-end “capacity ” drives.\nFor this reason (and others), people are often willing to pay top doll ar for\nthe former while trying to get the latter as cheaply as possible .\nCheetah Barracuda\nRI/O Random 0.66 MB/s 0.31 MB/s\nRI/O Sequential 125 MB/s 105 MB/s\nFigure 37.6: Disk Drive Performance: SCSI Versus SATA\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nHARD DISKDRIVES 9\nASIDE : COMPUTING THE“AVERAGE ” SEEK\nIn many books and papers, you will see average disk-seek time cit ed\nas being roughly one-third of the full seek time. Where does this c ome\nfrom?\nTurns out it arises from a simple calculation based on average see k\ndistance , not time. Imagine the disk as a set of tracks, from 0toN. The\nseek distance between any two tracks xandyis thus computed as the\nabsolute value of the difference between them: |x−y|.\nTo compute the average seek distance, all you need to do is to ﬁrs t add\nup all possible seek distances:\nN/summationdisplay\nx=0N/summationdisplay\ny=0|x−y|. (37.4)\nThen, divide this by the number of different possible seeks: N2. To\ncompute the sum, we’ll just use the integral form:\n/integraldisplayN\nx=0/integraldisplayN\ny=0|x−y|dydx. (37.5)\nTo compute the inner integral, let’s break out the absolute value :\n/integraldisplayx\ny=0(x−y)dy+/integraldisplayN\ny=x(y−x)dy. (37.6)\nSolving this leads to (xy−1\n2y2)/vextendsingle/vextendsinglex\n0+ (1\n2y2−xy)/vextendsingle/vextendsingleN\nxwhich can be sim-\npliﬁed to (x2−Nx+1\n2N2).Now we have to compute the outer integral:\n/integraldisplayN\nx=0(x2−Nx+1\n2N2)dx, (37.7)\nwhich results in:\n(1\n3x3−N\n2x2+N2\n2x)/vextendsingle/vextendsingle/vextendsingle/vextendsingleN\n0=N3\n3. (37.8)\nRemember that we still have to divide by the total number of seek s\n(N2) to compute the average seek distance: (N3\n3)/(N2) =1\n3N. Thus the\naverage seek distance on a disk, over all possible seeks, is one-t hird the\nfull distance. And now when you hear that an average seek is one-t hird\nof a full seek, you’ll know where it came from.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 HARD DISKDRIVES\n011109\n8\n7\n6\n5\n4\n32112232221\n20\n19\n18\n17\n16\n1514132435343332\n31\n30\n29\n28272625SpindleRotates this way\nFigure 37.7: SSTF: Scheduling Requests 21 And 2\n37.5 Disk Scheduling\nBecause of the high cost of I/O, the OS has historically played a rol e in\ndeciding the order of I/Os issued to the disk. More speciﬁcally, given a\nset of I/O requests, the disk scheduler examines the requests and decides\nwhich one to schedule next [SCO90, JW91].\nUnlike job scheduling, where the length of each job is usually un -\nknown, with disk scheduling, we can make a good guess at how long\na “job” (i.e., disk request) will take. By estimating the seek and possi-\nble rotational delay of a request, the disk scheduler can know how l ong\neach request will take, and thus (greedily) pick the one that w ill take the\nleast time to service ﬁrst. Thus, the disk scheduler will try to follow the\nprinciple of SJF (shortest job ﬁrst) in its operation.\nSSTF: Shortest Seek Time First\nOne early disk scheduling approach is known as shortest-seek-time-ﬁrst\n(SSTF ) (also called shortest-seek-ﬁrst orSSF). SSTF orders the queue of\nI/O requests by track, picking requests on the nearest track t o complete\nﬁrst. For example, assuming the current position of the head is ove r the\ninner track, and we have requests for sectors 21 (middle track) and 2\n(outer track), we would then issue the request to 21 ﬁrst, wait f or it to\ncomplete, and then issue the request to 2 (Figure 37.7).\nSSTF works well in this example, seeking to the middle track ﬁrst and\nthen the outer track. However, SSTF is not a panacea, for the following\nreasons. First, the drive geometry is not available to the host OS; rather,\nit sees an array of blocks. Fortunately, this problem is rather ea sily ﬁxed.\nInstead of SSTF, an OS can simply implement nearest-block-ﬁrst (NBF ),\nwhich schedules the request with the nearest block address ne xt.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nHARD DISKDRIVES 11\nThe second problem is more fundamental: starvation . Imagine in\nour example above if there were a steady stream of requests to the in-\nner track, where the head currently is positioned. Requests to any other\ntracks would then be ignored completely by a pure SSTF approach. And\nthus the crux of the problem:\nCRUX: HOWTOHANDLE DISKSTARVATION\nHow can we implement SSTF-like scheduling but avoid starvation?\nElevator (a.k.a. SCAN or C-SCAN)\nThe answer to this query was developed some time ago (see [CKR72 ]\nfor example), and is relatively straightforward. The algorith m, originally\ncalled SCAN , simply moves back and forth across the disk servicing re-\nquests in order across the tracks. Let’s call a single pass across the disk\n(from outer to inner tracks, or inner to outer) a sweep . Thus, if a request\ncomes for a block on a track that has already been serviced on this sw eep\nof the disk, it is not handled immediately, but rather queued un til the next\nsweep (in the other direction).\nSCAN has a number of variants, all of which do about the same thing.\nFor example, Coffman et al. introduced F-SCAN , which freezes the queue\nto be serviced when it is doing a sweep [CKR72]; this action plac es re-\nquests that come in during the sweep into a queue to be serviced later.\nDoing so avoids starvation of far-away requests, by delaying the servic-\ning of late-arriving (but nearer by) requests.\nC-SCAN is another common variant, short for Circular SCAN . In-\nstead of sweeping in both directions across the disk, the algorith m only\nsweeps from outer-to-inner, and then resets at the outer track to begin\nagain. Doing so is a bit more fair to inner and outer tracks, as pur e back-\nand-forth SCAN favors the middle tracks, i.e., after servicing the outer\ntrack, SCAN passes through the middle twice before coming back to the\nouter track again.\nFor reasons that should now be clear, the SCAN algorithm (and its\ncousins) is sometimes referred to as the elevator algorithm, because it\nbehaves like an elevator which is either going up or down and not jus t\nservicing requests to ﬂoors based on which ﬂoor is closer. Imagine h ow\nannoying it would be if you were going down from ﬂoor 10 to 1, and\nsomebody got on at 3 and pressed 4, and the elevator went up to 4 be-\ncause it was “closer” than 1! As you can see, the elevator algorith m, when\nused in real life, prevents ﬁghts from taking place on elevators . In disks,\nit just prevents starvation.\nUnfortunately, SCAN and its cousins do not represent the best sch edul-\ning technology. In particular, SCAN (or SSTF even) do not actually ad here\nas closely to the principle of SJF as they could. In particular, th ey ignore\nrotation. And thus, another crux:\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 HARD DISKDRIVES\nCRUX: HOWTOACCOUNT FORDISKROTATION COSTS\nHow can we implement an algorithm that more closely approximates SJ F\nby taking both seek and rotation into account?\n011109\n8\n7\n6\n5\n4\n32112232221\n20\n19\n18\n17\n16\n1514132435343332\n31\n30\n29\n28272625SpindleRotates this way\nFigure 37.8: SSTF: Sometimes Not Good Enough\nSPTF: Shortest Positioning Time First\nBefore discussing shortest positioning time ﬁrst orSPTF scheduling (some-\ntimes also called shortest access time ﬁrst orSATF ), which is the solution\nto our problem, let us make sure we understand the problem in more d e-\ntail. Figure 37.8 presents an example.\nIn the example, the head is currently positioned over sector 30 on t he\ninner track. The scheduler thus has to decide: should it sched ule sector 16\n(on the middle track) or sector 8 (on the outer track) for its next req uest.\nSo which should it service next?\nThe answer, of course, is “it depends”. In engineering, it turn s out\n“it depends” is almost always the answer, reﬂecting that trad e-offs are\npart of the life of the engineer; such maxims are also good in a pinc h,\ne.g., when you don’t know an answer to your boss’s question, you might\nwant to try this gem. However, it is almost always better to know why it\ndepends, which is what we discuss here.\nWhat it depends on here is the relative time of seeking as compare d\nto rotation. If, in our example, seek time is much higher than rota tional\ndelay, then SSTF (and variants) are just ﬁne. However, imagine i f seek is\nquite a bit faster than rotation. Then, in our example, it would ma ke more\nsense to seek further to service request 8 on the outer track than it would\nto perform the shorter seek to the middle track to service 16, wh ich has to\nrotate all the way around before passing under the disk head.\nOn modern drives, as we saw above, both seek and rotation are roughly\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nHARD DISKDRIVES 13\nTIP: ITALWAYS DEPENDS (LIVNY ’SLAW)\nAlmost any question can be answered with “it depends”, as our coll eague\nMiron Livny always says. However, use with caution, as if you answ er\ntoo many questions this way, people will stop asking you questions a lto-\ngether. For example, somebody asks: “want to go to lunch?” You rep ly:\n“it depends, are youcoming along?”\nequivalent (depending, of course, on the exact requests), and t hus SPTF\nis useful and improves performance. However, it is even more difﬁ cult\nto implement in an OS, which generally does not have a good idea wher e\ntrack boundaries are or where the disk head currently is (in a rot ational\nsense). Thus, SPTF is usually performed inside a drive, descri bed below.\nOther Scheduling Issues\nThere are many other issues we do not discuss in this brief descr iption\nof basic disk operation, scheduling, and related topics. One suc h is-\nsue is this: where is disk scheduling performed on modern systems? In\nolder systems, the operating system did all the scheduling; af ter looking\nthrough the set of pending requests, the OS would pick the best one , and\nissue it to the disk. When that request completed, the next one w ould be\nchosen, and so forth. Disks were simpler then, and so was life.\nIn modern systems, disks can accommodate multiple outstanding r e-\nquests, and have sophisticated internal schedulers themsel ves (which can\nimplement SPTF accurately; inside the disk controller, all rel evant details\nare available, including exact head position). Thus, the OS sc heduler usu-\nally picks what it thinks the best few requests are (say 16) an d issues them\nall to disk; the disk then uses its internal knowledge of head pos ition and\ndetailed track layout information to service said requests in t he best pos-\nsible (SPTF) order.\nAnother important related task performed by disk schedulers is I/O\nmerging . For example, imagine a series of requests to read blocks 33,\nthen 8, then 34, as in Figure 37.8. In this case, the scheduler should merge\nthe requests for blocks 33 and 34 into a single two-block request; any re-\nordering that the scheduler does is performed upon the merged req uests.\nMerging is particularly important at the OS level, as it reduc es the num-\nber of requests sent to the disk and thus lowers overheads.\nOne ﬁnal problem that modern schedulers address is this: how long\nshould the system wait before issuing an I/O to disk? One might n aively\nthink that the disk, once it has even a single I/O, should immedi ately\nissue the request to the drive; this approach is called work-conserving , as\nthe disk will never be idle if there are requests to serve. Howe ver, research\nonanticipatory disk scheduling has shown that sometimes it is better to\nwait for a bit [ID01], in what is called a non-work-conserving approach.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 HARD DISKDRIVES\nBy waiting, a new and “better” request may arrive at the disk, and thus\noverall efﬁciency is increased. Of course, deciding when to wa it, and for\nhow long, can be tricky; see the research paper for details, or che ck out\nthe Linux kernel implementation to see how such ideas are trans itioned\ninto practice (if you are the ambitious sort).\n37.6 Summary\nWe have presented a summary of how disks work. The summary is\nactually a detailed functional model; it does not describe the am azing\nphysics, electronics, and material science that goes into act ual drive de-\nsign. For those interested in even more details of that nature, we suggest\na different major (or perhaps minor); for those that are happy with this\nmodel, good! We can now proceed to using the model to build more in-\nteresting systems on top of these incredible devices.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nHARD DISKDRIVES 15\nReferences\n[ADR03] “More Than an Interface: SCSI vs. ATA” by Dave Anderson, Jim Dyke s, Erik Riedel.\nFAST ’03, 2003. One of the best recent-ish references on how modern disk drives really work; a must\nread for anyone interested in knowing more.\n[CKR72] “Analysis of Scanning Policies for Reducing Disk Seek Times” E.G . Coffman, L.A.\nKlimko, B. Ryan SIAM Journal of Computing, September 1972, Vol 1 . No 3. Some of the early\nwork in the ﬁeld of disk scheduling.\n[HK+17] “The Unwritten Contract of Solid State Drives” by Jun He, Sud arsun Kannan, Andrea\nC. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. EuroSys ’17, Belgr ade, Serbia, April 2017. We\ntake the idea of the unwritten contract, and extend it to SSDs. Using SSDs well s eems as complicated\nthan hard drives, and sometimes more so.\n[ID01] “Anticipatory Scheduling: A Disk-scheduling Framework To Overco me Deceptive Idle-\nness In Synchronous I/O” by Sitaram Iyer, Peter Druschel. SOSP ’01, Octobe r 2001. A cool paper\nshowing how waiting can improve disk scheduling: better requests may be on their way!\n[JW91] “Disk Scheduling Algorithms Based On Rotational Position” by D. Jacobson, J. Wilkes.\nTechnical Report HPL-CSP-91-7rev1, Hewlett-Packard, February 1991. A more modern take on\ndisk scheduling. It remains a technical report (and not a published pap er) because the authors were\nscooped by Seltzer et al. [S90].\n[RW92] “An Introduction to Disk Drive Modeling” by C. Ruemmler, J. W ilkes. IEEE Computer,\n27:3, March 1994. A terriﬁc introduction to the basics of disk operation. Some pieces are out of date,\nbut most of the basics remain.\n[SCO90] “Disk Scheduling Revisited” by Margo Seltzer, Peter Chen, John Ou sterhout. USENIX\n1990. A paper that talks about how rotation matters too in the world of disk scheduling.\n[SG04] “MEMS-based storage devices and standard disk interfaces: A squ are peg in a round\nhole?” Steven W. Schlosser, Gregory R. Ganger FAST ’04, pp. 87-100, 2004 While the MEMS\naspect of this paper hasn’t yet made an impact, the discussion of the contract be tween ﬁle systems and\ndisks is wonderful and a lasting contribution. We later build on this work to stu dy the “Unwritten\nContract of Solid State Drives” [HK+17]\n[S09a] “Barracuda ES.2 data sheet” by Seagate, Inc.. Available at this website, at least, it was:\nhttp://www.seagate.com/docs/pdf/datasheet/disc/ds_b arracuda_es.pdf .A\ndata sheet; read at your own risk. Risk of what? Boredom.\n[S09b] “Cheetah 15K.5” by Seagate, Inc.. Available at this websit e, we’re pretty sure it is:\nhttp://www.seagate.com/docs/pdf/datasheet/disc/ds-c heetah-15k-5-us.pdf .\nSee above commentary on data sheets.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 HARD DISKDRIVES\nHomework (Simulation)\nThis homework uses disk.py to familiarize you with how a modern\nhard drive works. It has a lot of different options, and unlike most of\nthe other simulations, has a graphical animator to show you exactl y what\nhappens when the disk is in action. See the README for details.\n1. Compute the seek, rotation, and transfer times for the follow ing sets of re-\nquests:-a 0 ,-a 6 ,-a 30 ,-a 7,30,8 , and ﬁnally -a 10,11,12,13 .\n2. Do the same requests above, but change the seek rate to differe nt values: -S\n2,-S 4 ,-S 8 ,-S 10 ,-S 40 ,-S 0.1 . How do the times change?\n3. Do the same requests above, but change the rotation rate: -R 0.1 ,-R 0.5 ,\n-R 0.01 . How do the times change?\n4. FIFO is not always best, e.g., with the request stream -a 7,30,8 , what or-\nder should the requests be processed in? Run the shortest seek-t ime ﬁrst\n(SSTF) scheduler ( -p SSTF ) on this workload; how long should it take\n(seek, rotation, transfer) for each request to be served?\n5. Now use the shortest access-time ﬁrst (SATF) scheduler ( -p SATF ). Does it\nmake any difference for -a 7,30,8 workload? Find a set of requests where\nSATF outperforms SSTF; more generally, when is SATF better than SSTF?\n6. Here is a request stream to try: -a 10,11,12,13 . What goes poorly when\nit runs? Try adding track skew to address this problem ( -o skew ). Given\nthe default seek rate, what should the skew be to maximize performan ce?\nWhat about for different seek rates (e.g., -S 2 ,-S 4 )? In general, could\nyou write a formula to ﬁgure out the skew?\n7. Specify a disk with different density per zone, e.g., -z 10,20,30 , which\nspeciﬁes the angular difference between blocks on the outer, mi ddle, and\ninner tracks. Run some random requests (e.g., -a -1 -A 5,-1,0 , which\nspeciﬁes that random requests should be used via the -a -1 ﬂag and that\nﬁve requests ranging from 0 to the max be generated), and compute t he\nseek, rotation, and transfer times. Use different random seed s. What is the\nbandwidth (in sectors per unit time) on the outer, middle, and inn er tracks?\n8. A scheduling window determines how many requests the disk can e xamine\nat once. Generate random workloads (e.g., -A 1000,-1,0 , with different\nseeds) and see how long the SATF scheduler takes when the sched uling win-\ndow is changed from 1 up to the number of requests. How big of a windo w\nis needed to maximize performance? Hint: use the -cﬂag and don’t turn\non graphics ( -G) to run these quickly. When the scheduling window is set\nto 1, does it matter which policy you are using?\n9. Create a series of requests to starve a particular request, as suming an SATF\npolicy. Given that sequence, how does it perform if you use a bounded\nSATF (BSATF ) scheduling approach? In this approach, you specify the\nscheduling window (e.g., -w 4 ); the scheduler only moves onto the next\nwindow of requests when allrequests in the current window have been ser-\nviced. Does this solve starvation? How does it perform, as comp ared to\nSATF? In general, how should a disk make this trade-off between perfor-\nmance and starvation avoidance?\n10. All the scheduling policies we have looked at thus far are greedy ; they pick\nthe next best option instead of looking for an optimal schedule. Can you\nﬁnd a set of requests in which greedy is not optimal?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",37258
44-38. Redundant Disk Arrays RAID.pdf,44-38. Redundant Disk Arrays RAID,"38\nRedundant Arrays of Inexpensive Disks\n(RAIDs)\nWhen we use a disk, we sometimes wish it to be faster; I/O operati ons\nare slow and thus can be the bottleneck for the entire system. Whe n we\nuse a disk, we sometimes wish it to be larger; more and more data is being\nput online and thus our disks are getting fuller and fuller. Whe n we use\na disk, we sometimes wish for it to be more reliable; when a disk fa ils, if\nour data isn’t backed up, all that valuable data is gone.\nCRUX: HOWTOMAKE A L ARGE , FAST, RELIABLE DISK\nHow can we make a large, fast, and reliable storage system? What are\nthe key techniques? What are trade-offs between different ap proaches?\nIn this chapter, we introduce the Redundant Array of Inexpensive\nDisks better known as RAID [P+88], a technique to use multiple disks in\nconcert to build a faster, bigger, and more reliable disk syste m. The term\nwas introduced in the late 1980s by a group of researchers at U.C. Berke-\nley (led by Professors David Patterson and Randy Katz and then st udent\nGarth Gibson); it was around this time that many different rese archers si-\nmultaneously arrived upon the basic idea of using multiple disk s to build\na better storage system [BG88, K86,K88,PB86,SG86].\nExternally, a RAID looks like a disk: a group of blocks one can read\nor write. Internally, the RAID is a complex beast, consisting of m ultiple\ndisks, memory (both volatile and non-), and one or more processors to\nmanage the system. A hardware RAID is very much like a computer\nsystem, specialized for the task of managing a group of disks.\nRAIDs offer a number of advantages over a single disk. One advan-\ntage is performance . Using multiple disks in parallel can greatly speed\nup I/O times. Another beneﬁt is capacity . Large data sets demand large\ndisks. Finally, RAIDs can improve reliability ; spreading data across mul-\ntiple disks (without RAID techniques) makes the data vulnera ble to the\nloss of a single disk; with some form of redundancy , RAIDs can tolerate\nthe loss of a disk and keep operating as if nothing were wrong.\n1\n2 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S)\nTIP: TRANSPARENCY ENABLES DEPLOYMENT\nWhen considering how to add new functionality to a system, one shou ld\nalways consider whether such functionality can be added transparently ,\nin a way that demands no changes to the rest of the system. Requi ring a\ncomplete rewrite of the existing software (or radical hardware c hanges)\nlessens the chance of impact of an idea. RAID is a perfect exampl e, and\ncertainly its transparency contributed to its success; admi nistrators could\ninstall a SCSI-based RAID storage array instead of a SCSI disk, and t he\nrest of the system (host computer, OS, etc.) did not have to change on e bit\nto start using it. By solving this problem of deployment , RAID was made\nmore successful from day one.\nAmazingly, RAIDs provide these advantages transparently to systems\nthat use them, i.e., a RAID just looks like a big disk to the host sy stem. The\nbeauty of transparency, of course, is that it enables one to simpl y replace\na disk with a RAID and not change a single line of software; the oper at-\ning system and client applications continue to operate without m odiﬁca-\ntion. In this manner, transparency greatly improves the deployability of\nRAID, enabling users and administrators to put a RAID to use wi thout\nworries of software compatibility.\nWe now discuss some of the important aspects of RAIDs. We begin\nwith the interface, fault model, and then discuss how one can eva luate a\nRAID design along three important axes: capacity, reliabilit y, and perfor-\nmance. We then discuss a number of other issues that are importan t to\nRAID design and implementation.\n38.1 Interface And RAID Internals\nTo a ﬁle system above, a RAID looks like a big, (hopefully) fast, an d\n(hopefully) reliable disk. Just as with a single disk, it pres ents itself as\na linear array of blocks, each of which can be read or written by the ﬁle\nsystem (or other client).\nWhen a ﬁle system issues a logical I/O request to the RAID, the RAID\ninternally must calculate which disk (or disks) to access in or der to com-\nplete the request, and then issue one or more physical I/Os to do so. The\nexact nature of these physical I/Os depends on the RAID level, a s we will\ndiscuss in detail below. However, as a simple example, consider a RAID\nthat keeps two copies of each block (each one on a separate disk); wh en\nwriting to such a mirrored RAID system, the RAID will have to perform\ntwo physical I/Os for every one logical I/O it is issued.\nA RAID system is often built as a separate hardware box, with a st an-\ndard connection (e.g., SCSI, or SATA) to a host. Internally, however,\nRAIDs are fairly complex, consisting of a microcontroller that run s ﬁrmware\nto direct the operation of the RAID, volatile memory such as DRAM\nto buffer data blocks as they are read and written, and in some ca ses,\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 3\nnon-volatile memory to buffer writes safely and perhaps even sp ecial-\nized logic to perform parity calculations (useful in some RAID le vels, as\nwe will also see below). At a high level, a RAID is very much a spe cial-\nized computer system: it has a processor, memory, and disks; howev er,\ninstead of running applications, it runs specialized software designed to\noperate the RAID.\n38.2 Fault Model\nTo understand RAID and compare different approaches, we must h ave\na fault model in mind. RAIDs are designed to detect and recover f rom\ncertain kinds of disk faults; thus, knowing exactly which faul ts to expect\nis critical in arriving upon a working design.\nThe ﬁrst fault model we will assume is quite simple, and has bee n\ncalled the fail-stop fault model [S84]. In this model, a disk can be in\nexactly one of two states: working or failed. With a working disk, a ll\nblocks can be read or written. In contrast, when a disk has failed , we\nassume it is permanently lost.\nOne critical aspect of the fail-stop model is what it assumes abou t fault\ndetection. Speciﬁcally, when a disk has failed, we assume that this is\neasily detected. For example, in a RAID array, we would assume t hat the\nRAID controller hardware (or software) can immediately observe w hen a\ndisk has failed.\nThus, for now, we do not have to worry about more complex “silent”\nfailures such as disk corruption. We also do not have to worry about a sin-\ngle block becoming inaccessible upon an otherwise working disk (s ome-\ntimes called a latent sector error). We will consider these more c omplex\n(and unfortunately, more realistic) disk faults later.\n38.3 How To Evaluate A RAID\nAs we will soon see, there are a number of different approaches to\nbuilding a RAID. Each of these approaches has different charac teristics\nwhich are worth evaluating, in order to understand their stren gths and\nweaknesses.\nSpeciﬁcally, we will evaluate each RAID design along three axe s. The\nﬁrst axis is capacity ; given a set of Ndisks each with Bblocks, how much\nuseful capacity is available to clients of the RAID? Without re dundancy,\nthe answer is N·B; in contrast, if we have a system that keeps two copies\nof each block (called mirroring ), we obtain a useful capacity of (N·B)/2.\nDifferent schemes (e.g., parity-based ones) tend to fall in b etween.\nThe second axis of evaluation is reliability . How many disk faults can\nthe given design tolerate? In alignment with our fault model, we assume\nonly that an entire disk can fail; in later chapters (i.e., on da ta integrity),\nwe’ll think about how to handle more complex failure modes.\nFinally, the third axis is performance . Performance is somewhat chal-\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S)\nlenging to evaluate, because it depends heavily on the workload pre-\nsented to the disk array. Thus, before evaluating performance , we will\nﬁrst present a set of typical workloads that one should consider.\nWe now consider three important RAID designs: RAID Level 0 (stri p-\ning), RAID Level 1 (mirroring), and RAID Levels 4/5 (parity-b ased re-\ndundancy). The naming of each of these designs as a “level” stem s from\nthe pioneering work of Patterson, Gibson, and Katz at Berkeley [P+ 88].\n38.4 RAID Level 0: Striping\nThe ﬁrst RAID level is actually not a RAID level at all, in that t here is\nno redundancy. However, RAID level 0, or striping as it is better known,\nserves as an excellent upper-bound on performance and capacity and\nthus is worth understanding.\nThe simplest form of striping will stripe blocks across the disks of the\nsystem as follows (assume here a 4-disk array):\nDisk 0 Disk 1 Disk 2 Disk 3\n0 1 2 3\n4 5 6 7\n8 9 10 11\n12 13 14 15\nFigure 38.1: RAID-0: Simple Striping\nFrom Figure 38.1, you get the basic idea: spread the blocks of the a rray\nacross the disks in a round-robin fashion. This approach is design ed to\nextract the most parallelism from the array when requests are m ade for\ncontiguous chunks of the array (as in a large, sequential read, f or exam-\nple). We call the blocks in the same row a stripe ; thus, blocks 0, 1, 2, and\n3 are in the same stripe above.\nIn the example, we have made the simplifying assumption that on ly 1\nblock (each of say size 4KB) is placed on each disk before moving on to\nthe next. However, this arrangement need not be the case. For exa mple,\nwe could arrange the blocks across disks as in Figure 38.2:\nDisk 0 Disk 1 Disk 2 Disk 3\n0 2 4 6 chunk size:\n1 3 5 7 2 blocks\n8 10 12 14\n9 11 13 15\nFigure 38.2: Striping With A Bigger Chunk Size\nIn this example, we place two 4KB blocks on each disk before moving\non to the next disk. Thus, the chunk size of this RAID array is 8KB, and\na stripe thus consists of 4 chunks or 32KB of data.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 5\nASIDE : THERAID M APPING PROBLEM\nBefore studying the capacity, reliability, and performance c haracteristics\nof the RAID, we ﬁrst present an aside on what we call the mapping prob-\nlem. This problem arises in all RAID arrays; simply put, given a log ical\nblock to read or write, how does the RAID know exactly which physica l\ndisk and offset to access?\nFor these simple RAID levels, we do not need much sophistication i n\norder to correctly map logical blocks onto their physical locations . Take\nthe ﬁrst striping example above (chunk size = 1 block = 4KB). In t his case,\ngiven a logical block address A, the RAID can easily compute the d esired\ndisk and offset with two simple equations:\nDisk = A % number_of_disks\nOffset = A / number_of_disks\nNote that these are all integer operations (e.g., 4 / 3 = 1 not 1.333 33...).\nLet’s see how these equations work for a simple example. Imagine in the\nﬁrst RAID above that a request arrives for block 14. Given that th ere are\n4 disks, this would mean that the disk we are interested in is (1 4 % 4 = 2):\ndisk 2. The exact block is calculated as (14 / 4 = 3): block 3. Thus , block\n14 should be found on the fourth block (block 3, starting at 0) of the th ird\ndisk (disk 2, starting at 0), which is exactly where it is.\nYou can think about how these equations would be modiﬁed to support\ndifferent chunk sizes. Try it! It’s not too hard.\nChunk Sizes\nChunk size mostly affects performance of the array. For example, a small\nchunk size implies that many ﬁles will get striped across many disks, thus\nincreasing the parallelism of reads and writes to a single ﬁle ; however, the\npositioning time to access blocks across multiple disks increas es, because\nthe positioning time for the entire request is determined by the maximum\nof the positioning times of the requests across all drives.\nA big chunk size, on the other hand, reduces such intra-ﬁle para l-\nlelism, and thus relies on multiple concurrent requests to ach ieve high\nthroughput. However, large chunk sizes reduce positioning time ; if, for\nexample, a single ﬁle ﬁts within a chunk and thus is placed on a s ingle\ndisk, the positioning time incurred while accessing it will ju st be the po-\nsitioning time of a single disk.\nThus, determining the “best” chunk size is hard to do, as it req uires a\ngreat deal of knowledge about the workload presented to the disk sy stem\n[CL95]. For the rest of this discussion, we will assume that the a rray uses\na chunk size of a single block (4KB). Most arrays use larger chunk sizes\n(e.g., 64 KB), but for the issues we discuss below, the exact chu nk size\ndoes not matter; thus we use a single block for the sake of simplicit y.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S)\nBack To RAID-0 Analysis\nLet us now evaluate the capacity, reliability, and performanc e of striping.\nFrom the perspective of capacity, it is perfect: given Ndisks each of size\nBblocks, striping delivers N·Bblocks of useful capacity. From the stand-\npoint of reliability, striping is also perfect, but in the bad w ay: any disk\nfailure will lead to data loss. Finally, performance is excell ent: all disks\nare utilized, often in parallel, to service user I/O requests .\nEvaluating RAID Performance\nIn analyzing RAID performance, one can consider two different p erfor-\nmance metrics. The ﬁrst is single-request latency . Understanding the la-\ntency of a single I/O request to a RAID is useful as it reveals how much\nparallelism can exist during a single logical I/O operation. Th e second\nissteady-state throughput of the RAID, i.e., the total bandwidth of many\nconcurrent requests. Because RAIDs are often used in high-per formance\nenvironments, the steady-state bandwidth is critical, and t hus will be the\nmain focus of our analyses.\nTo understand throughput in more detail, we need to put forth some\nworkloads of interest. We will assume, for this discussion, that t here\nare two types of workloads: sequential and random . With a sequential\nworkload, we assume that requests to the array come in large conti gu-\nous chunks; for example, a request (or series of requests) that ac cesses\n1 MB of data, starting at block xand ending at block (x+1 MB), would be\ndeemed sequential. Sequential workloads are common in many envir on-\nments (think of searching through a large ﬁle for a keyword), and t hus\nare considered important.\nFor random workloads, we assume that each request is rather small ,\nand that each request is to a different random location on disk. For exam-\nple, a random stream of requests may ﬁrst access 4KB at logical ad dress\n10, then at logical address 550,000, then at 20,100, and so fort h. Some im-\nportant workloads, such as transactional workloads on a database ma n-\nagement system (DBMS), exhibit this type of access pattern, an d thus it is\nconsidered an important workload.\nOf course, real workloads are not so simple, and often have a mix\nof sequential and random-seeming components as well as behaviors in-\nbetween the two. For simplicity, we just consider these two possi bilities.\nAs you can tell, sequential and random workloads will result in wi dely\ndifferent performance characteristics from a disk. With sequ ential access,\na disk operates in its most efﬁcient mode, spending little time s eeking and\nwaiting for rotation and most of its time transferring data. With r andom\naccess, just the opposite is true: most time is spent seeking and waiting\nfor rotation and relatively little time is spent transferring d ata. To capture\nthis difference in our analysis, we will assume that a disk can transfer\ndata atSMB/s under a sequential workload, and RMB/s when under a\nrandom workload. In general, Sis much greater than R(i.e.,S≫R).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 7\nTo make sure we understand this difference, let’s do a simple e xercise.\nSpeciﬁcally, let’s calculate SandRgiven the following disk characteris-\ntics. Assume a sequential transfer of size 10 MB on average, and a random\ntransfer of 10 KB on average. Also, assume the following disk chara cter-\nistics:\nAverage seek time 7 ms\nAverage rotational delay 3 ms\nTransfer rate of disk 50 MB/s\nTo compute S, we need to ﬁrst ﬁgure out how time is spent in a typical\n10 MB transfer. First, we spend 7 ms seeking, and then 3 ms rotat ing.\nFinally, transfer begins; 10 MB @ 50 MB/s leads to 1/5th of a sec ond, or\n200 ms, spent in transfer. Thus, for each 10 MB request, we spen d 210 ms\ncompleting the request. To compute S, we just need to divide:\nS=Amount of Data\nTime to access=10MB\n210ms= 47.62MB/s\nAs we can see, because of the large time spent transferring dat a,Sis\nvery near the peak bandwidth of the disk (the seek and rotational costs\nhave been amortized).\nWe can compute Rsimilarly. Seek and rotation are the same; we then\ncompute the time spent in transfer, which is 10 KB @ 50 MB/s, or 0. 195\nms.\nR=Amount of Data\nTime to access=10KB\n10.195ms= 0.981MB/s\nAs we can see, Ris less than 1 MB/s, and S/R is almost 50.\nBack To RAID-0 Analysis, Again\nLet’s now evaluate the performance of striping. As we said above, i t is\ngenerally good. From a latency perspective, for example, the lat ency of a\nsingle-block request should be just about identical to that of a s ingle disk;\nafter all, RAID-0 will simply redirect that request to one of it s disks.\nFrom the perspective of steady-state throughput, we’d expect to get\nthe full bandwidth of the system. Thus, throughput equals N(the num-\nber of disks) multiplied by S(the sequential bandwidth of a single disk).\nFor a large number of random I/Os, we can again use all of the disks,\nand thus obtain N·RMB/s. As we will see below, these values are both\nthe simplest to calculate and will serve as an upper bound in com parison\nwith other RAID levels.\n38.5 RAID Level 1: Mirroring\nOur ﬁrst RAID level beyond striping is known as RAID level 1, or\nmirroring. With a mirrored system, we simply make more than one cop y\nof each block in the system; each copy should be placed on a separate\ndisk, of course. By doing so, we can tolerate disk failures.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S)\nIn a typical mirrored system, we will assume that for each logica l\nblock, the RAID keeps two physical copies of it. Here is an exampl e:\nDisk 0 Disk 1 Disk 2 Disk 3\n0 0 1 1\n2 2 3 3\n4 4 5 5\n6 6 7 7\nFigure 38.3: Simple RAID-1: Mirroring\nIn the example, disk 0 and disk 1 have identical contents, and d isk 2\nand disk 3 do as well; the data is striped across these mirror pai rs. In fact,\nyou may have noticed that there are a number of different ways to p lace\nblock copies across the disks. The arrangement above is a common one\nand is sometimes called RAID-10 or (RAID 1+0 ) because it uses mirrored\npairs (RAID-1) and then stripes (RAID-0) on top of them; another c om-\nmon arrangement is RAID-01 (orRAID 0+1 ), which contains two large\nstriping (RAID-0) arrays, and then mirrors (RAID-1) on top of the m. For\nnow, we will just talk about mirroring assuming the above layout.\nWhen reading a block from a mirrored array, the RAID has a choice: i t\ncan read either copy. For example, if a read to logical block 5 is is sued to\nthe RAID, it is free to read it from either disk 2 or disk 3. When wr iting\na block, though, no such choice exists: the RAID must update both copies\nof the data, in order to preserve reliability. Do note, though, th at these\nwrites can take place in parallel; for example, a write to logic al block 5\ncould proceed to disks 2 and 3 at the same time.\nRAID-1 Analysis\nLet us assess RAID-1. From a capacity standpoint, RAID-1 is exp ensive;\nwith the mirroring level = 2, we only obtain half of our peak useful c a-\npacity. With Ndisks ofBblocks, RAID-1 useful capacity is (N·B)/2.\nFrom a reliability standpoint, RAID-1 does well. It can tolerate the fail-\nure of any one disk. You may also notice RAID-1 can actually do bett er\nthan this, with a little luck. Imagine, in the ﬁgure above, tha t disk 0 and\ndisk 2 both failed. In such a situation, there is no data loss! More gen-\nerally, a mirrored system (with mirroring level of 2) can tolerat e 1 disk\nfailure for certain, and up to N/2 failures depending on which d isks fail.\nIn practice, we generally don’t like to leave things like this t o chance; thus\nmost people consider mirroring to be good for handling a single failu re.\nFinally, we analyze performance. From the perspective of the la tency\nof a single read request, we can see it is the same as the latency on a single\ndisk; all the RAID-1 does is direct the read to one of its copies. A w rite\nis a little different: it requires two physical writes to comp lete before it\nis done. These two writes happen in parallel, and thus the time will be\nroughly equivalent to the time of a single write; however, becau se the\nlogical write must wait for both physical writes to complete, it s uffers the\nworst-case seek and rotational delay of the two requests, and thu s (on\naverage) will be slightly higher than a write to a single disk .\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 9\nASIDE : THERAID C ONSISTENT -UPDATE PROBLEM\nBefore analyzing RAID-1, let us ﬁrst discuss a problem that ari ses in\nany multi-disk RAID system, known as the consistent-update problem\n[DAA05]. The problem occurs on a write to any RAID that has to up-\ndate multiple disks during a single logical operation. In this c ase, let us\nassume we are considering a mirrored disk array.\nImagine the write is issued to the RAID, and then the RAID deci des that\nit must be written to two disks, disk 0 and disk 1. The RAID then issues\nthe write to disk 0, but just before the RAID can issue the reque st to disk\n1, a power loss (or system crash) occurs. In this unfortunate case, let us\nassume that the request to disk 0 completed (but clearly the re quest to\ndisk 1 did not, as it was never issued).\nThe result of this untimely power loss is that the two copies of the b lock\nare now inconsistent ; the copy on disk 0 is the new version, and the copy\non disk 1 is the old. What we would like to happen is for the state of bot h\ndisks to change atomically , i.e., either both should end up as the new\nversion or neither.\nThe general way to solve this problem is to use a write-ahead log of some\nkind to ﬁrst record what the RAID is about to do (i.e., update two disks\nwith a certain piece of data) before doing it. By taking this appr oach, we\ncan ensure that in the presence of a crash, the right thing will happen; by\nrunning a recovery procedure that replays all pending transactions to the\nRAID, we can ensure that no two mirrored copies (in the RAID-1 ca se)\nare out of sync.\nOne last note: because logging to disk on every write is prohibiti vely\nexpensive, most RAID hardware includes a small amount of non-vola tile\nRAM (e.g., battery-backed) where it performs this type of loggi ng. Thus,\nconsistent update is provided without the high cost of logging to di sk.\nTo analyze steady-state throughput, let us start with the seq uential\nworkload. When writing out to disk sequentially, each logical wr ite must\nresult in two physical writes; for example, when we write logic al block\n0 (in the ﬁgure above), the RAID internally would write it to both disk\n0 and disk 1. Thus, we can conclude that the maximum bandwidth ob -\ntained during sequential writing to a mirrored array is (N\n2·S), or half the\npeak bandwidth.\nUnfortunately, we obtain the exact same performance during a se -\nquential read. One might think that a sequential read could do better,\nbecause it only needs to read one copy of the data, not both. However,\nlet’s use an example to illustrate why this doesn’t help much. Im agine we\nneed to read blocks 0, 1, 2, 3, 4, 5, 6, and 7. Let’s say we issue the read of\n0 to disk 0, the read of 1 to disk 2, the read of 2 to disk 1, and the re ad of\n3 to disk 3. We continue by issuing reads to 4, 5, 6, and 7 to disks 0, 2, 1,\nand 3, respectively. One might naively think that because we are utilizing\nall disks, we are achieving the full bandwidth of the array.\nTo see that this is not (necessarily) the case, however, conside r the\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S)\nrequests a single disk receives (say disk 0). First, it gets a request for\nblock 0; then, it gets a request for block 4 (skipping block 2). In f act, each\ndisk receives a request for every other block. While it is rotatin g over the\nskipped block, it is not delivering useful bandwidth to the cli ent. Thus,\neach disk will only deliver half its peak bandwidth. And thus, the se-\nquential read will only obtain a bandwidth of (N\n2·S) MB/s.\nRandom reads are the best case for a mirrored RAID. In this case, w e\ncan distribute the reads across all the disks, and thus obtain t he full pos-\nsible bandwidth. Thus, for random reads, RAID-1 delivers N·RMB/s.\nFinally, random writes perform as you might expect:N\n2·RMB/s. Each\nlogical write must turn into two physical writes, and thus whi le all the\ndisks will be in use, the client will only perceive this as half the available\nbandwidth. Even though a write to logical block xturns into two parallel\nwrites to two different physical disks, the bandwidth of many small re-\nquests only achieves half of what we saw with striping. As we wil l soon\nsee, getting half the available bandwidth is actually prett y good!\n38.6 RAID Level 4: Saving Space With Parity\nWe now present a different method of adding redundancy to a disk a r-\nray known as parity . Parity-based approaches attempt to use less capac-\nity and thus overcome the huge space penalty paid by mirrored sys tems.\nThey do so at a cost, however: performance.\nDisk 0 Disk 1 Disk 2 Disk 3 Disk 4\n0 1 2 3 P0\n4 5 6 7 P1\n8 9 10 11 P2\n12 13 14 15 P3\nFigure 38.4: RAID-4 With Parity\nHere is an example ﬁve-disk RAID-4 system (Figure 38.4). For e ach\nstripe of data, we have added a single parity block that stores the redun-\ndant information for that stripe of blocks. For example, parity bloc k P1\nhas redundant information that it calculated from blocks 4, 5, 6, and 7.\nTo compute parity, we need to use a mathematical function that e n-\nables us to withstand the loss of any one block from our stripe. It tur ns\nout the simple function XOR does the trick quite nicely. For a given set of\nbits, the XOR of all of those bits returns a 0 if there are an even nu mber of\n1’s in the bits, and a 1 if there are an odd number of 1’s. For example:\nC0 C1 C2 C3 P\n0 0 1 1 XOR(0,0,1,1) = 0\n0 1 0 0 XOR(0,1,0,0) = 1\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 11\nIn the ﬁrst row (0,0,1,1), there are two 1’s (C2, C3), and thus XO R of\nall of those values will be 0 (P); similarly, in the second row ther e is only\none 1 (C1), and thus the XOR must be 1 (P). You can remember this in a\nsimple way: that the number of 1s in any row must be an even (not odd)\nnumber; that is the invariant that the RAID must maintain in order for\nparity to be correct.\nFrom the example above, you might also be able to guess how parity\ninformation can be used to recover from a failure. Imagine the colu mn la-\nbeled C2 is lost. To ﬁgure out what values must have been in the col umn,\nwe simply have to read in all the other values in that row (includ ing the\nXOR’d parity bit) and reconstruct the right answer. Speciﬁcally, assume\nthe ﬁrst row’s value in column C2 is lost (it is a 1); by reading the ot her\nvalues in that row (0 from C0, 0 from C1, 1 from C3, and 0 from the parit y\ncolumn P), we get the values 0, 0, 1, and 0. Because we know that XO R\nkeeps an even number of 1’s in each row, we know what the missing dat a\nmust be: a 1. And that is how reconstruction works in a XOR-based pa r-\nity scheme! Note also how we compute the reconstructed value: we j ust\nXOR the data bits and the parity bits together, in the same way t hat we\ncalculated the parity in the ﬁrst place.\nNow you might be wondering: we are talking about XORing all of\nthese bits, and yet from above we know that the RAID places 4KB (or\nlarger) blocks on each disk; how do we apply XOR to a bunch of blocks\nto compute the parity? It turns out this is easy as well. Simply pe rform a\nbitwise XOR across each bit of the data blocks; put the result of ea ch bit-\nwise XOR into the corresponding bit slot in the parity block. For ex ample,\nif we had blocks of size 4 bits (yes, this is still quite a bit smal ler than a\n4KB block, but you get the picture), they might look something like this:\nBlock0 Block1 Block2 Block3 Parity\n00 10 11 10 11\n10 01 00 01 10\nAs you can see from the ﬁgure, the parity is computed for each bit of\neach block and the result placed in the parity block.\nRAID-4 Analysis\nLet us now analyze RAID-4. From a capacity standpoint, RAID-4 us es 1\ndisk for parity information for every group of disks it is protecting . Thus,\nour useful capacity for a RAID group is (N−1)·B.\nReliability is also quite easy to understand: RAID-4 tolerat es 1 disk\nfailure and no more. If more than one disk is lost, there is simply n o way\nto reconstruct the lost data.\nFinally, there is performance. This time, let us start by anal yzing steady-\nstate throughput. Sequential read performance can utilize all of the disks\nexcept for the parity disk, and thus deliver a peak effective b andwidth of\n(N−1)·SMB/s (an easy case).\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S)\nDisk 0 Disk 1 Disk 2 Disk 3 Disk 4\n0 1 2 3 P0\n4 5 6 7 P1\n8 9 10 11 P2\n12 13 14 15 P3\nFigure 38.5: Full-stripe Writes In RAID-4\nTo understand the performance of sequential writes, we must ﬁr st un-\nderstand how they are done. When writing a big chunk of data to dis k,\nRAID-4 can perform a simple optimization known as a full-stripe write .\nFor example, imagine the case where the blocks 0, 1, 2, and 3 have been\nsent to the RAID as part of a write request (Figure 38.5).\nIn this case, the RAID can simply calculate the new value of P0 ( by\nperforming an XOR across the blocks 0, 1, 2, and 3) and then write a ll of\nthe blocks (including the parity block) to the ﬁve disks above in parallel\n(highlighted in gray in the ﬁgure). Thus, full-stripe write s are the most\nefﬁcient way for RAID-4 to write to disk.\nOnce we understand the full-stripe write, calculating the p erformance\nof sequential writes on RAID-4 is easy; the effective bandwidt h is also\n(N−1)·SMB/s. Even though the parity disk is constantly in use during\nthe operation, the client does not gain performance advantage from it.\nNow let us analyze the performance of random reads. As you can also\nsee from the ﬁgure above, a set of 1-block random reads will be sprea d\nacross the data disks of the system but not the parity disk. Thus, the\neffective performance is: (N−1)·RMB/s.\nRandom writes, which we have saved for last, present the most in-\nteresting case for RAID-4. Imagine we wish to overwrite block 1 i n the\nexample above. We could just go ahead and overwrite it, but that w ould\nleave us with a problem: the parity block P0 would no longer accura tely\nreﬂect the correct parity value of the stripe; in this example, P0 must also\nbe updated. How can we update it both correctly and efﬁciently?\nIt turns out there are two methods. The ﬁrst, known as additive parity ,\nrequires us to do the following. To compute the value of the new par ity\nblock, read in all of the other data blocks in the stripe in paralle l (in the\nexample, blocks 0, 2, and 3) and XOR those with the new block (1). T he\nresult is your new parity block. To complete the write, you can the n write\nthe new data and new parity to their respective disks, also in parallel.\nThe problem with this technique is that it scales with the numb er of\ndisks, and thus in larger RAIDs requires a high number of reads to com-\npute parity. Thus, the subtractive parity method.\nFor example, imagine this string of bits (4 data bits, one parity ):\nC0 C1 C2 C3 P\n0 0 1 1 XOR(0,0,1,1) = 0\nLet’s imagine that we wish to overwrite bit C2 with a new value wh ich\nwe will call C2 new. The subtractive method works in three steps. First,\nwe read in the old data at C2 (C2 old= 1) and the old parity (P old= 0).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 13\nThen, we compare the old data and the new data; if they are the sam e\n(e.g., C2 new = C2old), then we know the parity bit will also remain the\nsame (i.e., P new= Pold). If, however, they are different, then we must ﬂip\nthe old parity bit to the opposite of its current state, that is, if (Pold== 1),\nPnewwill be set to 0; if (P old== 0), P newwill be set to 1. We can express\nthis whole mess neatly with XOR (where ⊕is the XOR operator):\nPnew= (Cold⊕Cnew)⊕Pold (38.1)\nBecause we are dealing with blocks, not bits, we perform this cal cula-\ntion over all the bits in the block (e.g., 4096 bytes in each block m ultiplied\nby 8 bits per byte). Thus, in most cases, the new block will be dif ferent\nthan the old block and thus the new parity block will too.\nYou should now be able to ﬁgure out when we would use the additive\nparity calculation and when we would use the subtractive method . Think\nabout how many disks would need to be in the system so that the addi tive\nmethod performs fewer I/Os than the subtractive method; what is the\ncross-over point?\nFor this performance analysis, let us assume we are using the su btrac-\ntive method. Thus, for each write, the RAID has to perform 4 physi cal\nI/Os (two reads and two writes). Now imagine there are lots of wri tes\nsubmitted to the RAID; how many can RAID-4 perform in parallel? To\nunderstand, let us again look at the RAID-4 layout (Figure 38.6) .\nDisk 0 Disk 1 Disk 2 Disk 3 Disk 4\n0 1 2 3 P0\n∗4 5 6 7+P1\n8 9 10 11 P2\n12∗13 14 15+P3\nFigure 38.6: Example: Writes To 4, 13, And Respective Parity Blocks\nNow imagine there were 2 small writes submitted to the RAID-4 a t\nabout the same time, to blocks 4 and 13 (marked with∗in the diagram).\nThe data for those disks is on disks 0 and 1, and thus the read and wr ite\nto data could happen in parallel, which is good. The problem that a rises\nis with the parity disk; both the requests have to read the rela ted parity\nblocks for 4 and 13, parity blocks 1 and 3 (marked with+). Hopefully, the\nissue is now clear: the parity disk is a bottleneck under this ty pe of work-\nload; we sometimes thus call this the small-write problem for parity-\nbased RAIDs. Thus, even though the data disks could be accessed in\nparallel, the parity disk prevents any parallelism from mate rializing; all\nwrites to the system will be serialized because of the parity d isk. Because\nthe parity disk has to perform two I/Os (one read, one write) per l ogical\nI/O, we can compute the performance of small random writes in RAID -4\nby computing the parity disk’s performance on those two I/Os, and t hus\nwe achieve (R/2)MB/s. RAID-4 throughput under random small writes\nis terrible; it does not improve as you add disks to the system.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S)\nWe conclude by analyzing I/O latency in RAID-4. As you now know,\na single read (assuming no failure) is just mapped to a single disk, and\nthus its latency is equivalent to the latency of a single disk r equest. The\nlatency of a single write requires two reads and then two write s; the reads\ncan happen in parallel, as can the writes, and thus total laten cy is about\ntwice that of a single disk (with some differences because we ha ve to wait\nfor both reads to complete and thus get the worst-case positioning t ime,\nbut then the updates don’t incur seek cost and thus may be a better -than-\naverage positioning cost).\n38.7 RAID Level 5: Rotating Parity\nTo address the small-write problem (at least, partially), Pa tterson, Gib-\nson, and Katz introduced RAID-5. RAID-5 works almost identicall y to\nRAID-4, except that it rotates the parity block across drives (Figure 38.7).\nDisk 0 Disk 1 Disk 2 Disk 3 Disk 4\n0 1 2 3 P0\n5 6 7 P1 4\n10 11 P2 8 9\n15 P3 12 13 14\nP4 16 17 18 19\nFigure 38.7: RAID-5 With Rotated Parity\nAs you can see, the parity block for each stripe is now rotated across\nthe disks, in order to remove the parity-disk bottleneck for RAID -4.\nRAID-5 Analysis\nMuch of the analysis for RAID-5 is identical to RAID-4. For examp le, the\neffective capacity and failure tolerance of the two levels are identical. So\nare sequential read and write performance. The latency of a sin gle request\n(whether a read or a write) is also the same as RAID-4.\nRandom read performance is a little better, because we can now ut ilize\nall disks. Finally, random write performance improves noticeab ly over\nRAID-4, as it allows for parallelism across requests. Imagine a write to\nblock 1 and a write to block 10; this will turn into requests to di sk 1 and\ndisk 4 (for block 1 and its parity) and requests to disk 0 and disk 2 (for\nblock 10 and its parity). Thus, they can proceed in parallel. In fact, we\ncan generally assume that given a large number of random reques ts, we\nwill be able to keep all the disks about evenly busy. If that is t he case,\nthen our total bandwidth for small writes will beN\n4·RMB/s. The factor\nof four loss is due to the fact that each RAID-5 write still genera tes 4 total\nI/O operations, which is simply the cost of using parity-based RA ID.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 15\nRAID-0 RAID-1 RAID-4 RAID-5\nCapacity N·B(N·B)/2 (N−1)·B(N−1)·B\nReliability 0 1 (for sure) 1 1\nN\n2(if lucky)\nThroughput\nSequential Read N·S(N/2)·S(N−1)·S(N−1)·S\nSequential Write N·S(N/2)·S(N−1)·S(N−1)·S\nRandom Read N·R N ·R(N−1)·R N ·R\nRandom Write N·R(N/2)·R1\n2·RN\n4R\nLatency\nRead T T T T\nWrite T T 2T 2T\nFigure 38.8: RAID Capacity, Reliability, and Performance\nBecause RAID-5 is basically identical to RAID-4 except in th e few cases\nwhere it is better, it has almost completely replaced RAID-4 in the market-\nplace. The only place where it has not is in systems that know they will\nnever perform anything other than a large write, thus avoiding t he small-\nwrite problem altogether [HLM94]; in those cases, RAID-4 is some times\nused as it is slightly simpler to build.\n38.8 RAID Comparison: A Summary\nWe now summarize our simpliﬁed comparison of RAID levels in Fig-\nure 38.8. Note that we have omitted a number of details to simplif y our\nanalysis. For example, when writing in a mirrored system, the a verage\nseek time is a little higher than when writing to just a single disk, because\nthe seek time is the max of two seeks (one on each disk). Thus, rand om\nwrite performance to two disks will generally be a little less than random\nwrite performance of a single disk. Also, when updating the pari ty disk\nin RAID-4/5, the ﬁrst read of the old parity will likely cause a f ull seek\nand rotation, but the second write of the parity will only result in rotation.\nHowever, the comparison in Figure 38.8 does capture the essentia l dif-\nferences, and is useful for understanding tradeoffs across RAI D levels.\nFor the latency analysis, we simply use Tto represent the time that a\nrequest to a single disk would take.\nTo conclude, if you strictly want performance and do not care about\nreliability, striping is obviously best. If, however, you want r andom I/O\nperformance and reliability, mirroring is the best; the cost you pay is in\nlost capacity. If capacity and reliability are your main goals, then RAID-\n5 is the winner; the cost you pay is in small-write performance. F inally,\nif you are always doing sequential I/O and want to maximize capa city,\nRAID-5 also makes the most sense.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S)\n38.9 Other Interesting RAID Issues\nThere are a number of other interesting ideas that one could (and p er-\nhaps should) discuss when thinking about RAID. Here are some thi ngs\nwe might eventually write about.\nFor example, there are many other RAID designs, including Leve ls 2\nand 3 from the original taxonomy, and Level 6 to tolerate multiple d isk\nfaults [C+04]. There is also what the RAID does when a disk fail s; some-\ntimes it has a hot spare sitting around to ﬁll in for the failed disk. What\nhappens to performance under failure, and performance during recon-\nstruction of the failed disk? There are also more realistic faul t models,\nto take into account latent sector errors orblock corruption [B+08], and\nlots of techniques to handle such faults (see the data integrit y chapter for\ndetails). Finally, you can even build RAID as a software layer: such soft-\nware RAID systems are cheaper but have other problems, including the\nconsistent-update problem [DAA05].\n38.10 Summary\nWe have discussed RAID. RAID transforms a number of independen t\ndisks into a large, more capacious, and more reliable single ent ity; impor-\ntantly, it does so transparently, and thus hardware and softwa re above is\nrelatively oblivious to the change.\nThere are many possible RAID levels to choose from, and the exact\nRAID level to use depends heavily on what is important to the end -user.\nFor example, mirrored RAID is simple, reliable, and generally provides\ngood performance but at a high capacity cost. RAID-5, in contrast, is\nreliable and better from a capacity standpoint, but performs qu ite poorly\nwhen there are small writes in the workload. Picking a RAID and s etting\nits parameters (chunk size, number of disks, etc.) properly for a particular\nworkload is challenging, and remains more of an art than a science .\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 17\nReferences\n[B+08] “An Analysis of Data Corruption in the Storage Stack” by La kshmi N. Bairavasun-\ndaram, Garth R. Goodson, Bianca Schroeder, Andrea C. Arpaci-Dusseau , Remzi H. Arpaci-\nDusseau. FAST ’08, San Jose, CA, February 2008. Our own work analyzing how often disks actu-\nally corrupt your data. Not often, but sometimes! And thus something a reliabl e storage system must\nconsider.\n[BJ88] “Disk Shadowing” by D. Bitton and J. Gray. VLDB 1988. One of the ﬁrst papers to discuss\nmirroring, herein called “shadowing”.\n[CL95] “Striping in a RAID level 5 disk array” by Peter M. Chen and Edw ard K. Lee. SIGMET-\nRICS 1995. A nice analysis of some of the important parameters in a RAID-5 disk array.\n[C+04] “Row-Diagonal Parity for Double Disk Failure Correction” by P . Corbett, B. English, A.\nGoel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar. FAST ’04, February 2004. Though not the ﬁrst\npaper on a RAID system with two disks for parity, it is a recent and highly-un derstandable version of\nsaid idea. Read it to learn more.\n[DAA05] “Journal-guided Resynchronization for Software RAID” by Timo thy E. Denehy, A.\nArpaci-Dusseau, R. Arpaci-Dusseau. FAST 2005. Our own work on the consistent-update problem.\nHere we solve it for Software RAID by integrating the journaling machinery of the ﬁle system above\nwith the software RAID beneath it.\n[HLM94] “File System Design for an NFS File Server Appliance” by Da ve Hitz, James Lau,\nMichael Malcolm. USENIX Winter 1994, San Francisco, California, 1994. The sparse paper intro-\nducing a landmark product in storage, the write-anywhere ﬁle layout or WAFL ﬁle system that underlies\nthe NetApp ﬁle server.\n[K86] “Synchronized Disk Interleaving” by M.Y. Kim. IEEE Transactions o n Computers, Vol-\nume C-35: 11, November 1986. Some of the earliest work on RAID is found here.\n[K88] “Small Disk Arrays – The Emerging Approach to High Performance” b y F. Kurzweil.\nPresentation at Spring COMPCON ’88, March 1, 1988, San Francisco, Californi a.Another early\nRAID reference.\n[P+88] “Redundant Arrays of Inexpensive Disks” by D. Patterson, G. Gi bson, R. Katz. SIG-\nMOD 1988. This is considered theRAID paper, written by famous authors Patterson, Gibson, and\nKatz. The paper has since won many test-of-time awards and ushered in the RAID era, i ncluding the\nname RAID itself!\n[PB86] “Providing Fault Tolerance in Parallel Secondary Storage Syst ems” by A. Park, K. Bal-\nasubramaniam. Department of Computer Science, Princeton, CS-TR-O57-86, N ovember 1986.\nAnother early work on RAID.\n[SG86] “Disk Striping” by K. Salem, H. Garcia-Molina. IEEE Internati onal Conference on Data\nEngineering, 1986. And yes, another early RAID work. There are a lot of these, which kind of came\nout of the woodwork when the RAID paper was published in SIGMOD.\n[S84] “Byzantine Generals in Action: Implementing Fail-Stop Processor s” by F.B. Schneider.\nACM Transactions on Computer Systems, 2(2):145154, May 1984. Finally, a paper that is not\nabout RAID! This paper is actually about how systems fail, and how to make something behave in a\nfail-stop manner.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n18 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S)\nHomework (Simulation)\nThis section introduces raid.py , a simple RAID simulator you can\nuse to shore up your knowledge of how RAID systems work. See the\nREADME for details.\nQuestions\n1. Use the simulator to perform some basic RAID mapping tests. Run wit h\ndifferent levels (0, 1, 4, 5) and see if you can ﬁgure out the mappi ngs of a set\nof requests. For RAID-5, see if you can ﬁgure out the difference be tween left-\nsymmetric and left-asymmetric layouts. Use some different random s eeds\nto generate different problems than above.\n2. Do the same as the ﬁrst problem, but this time vary the chunk size w ith-C.\nHow does chunk size change the mappings?\n3. Do the same as above, but use the -rﬂag to reverse the nature of each\nproblem.\n4. Now use the reverse ﬂag but increase the size of each request wi th the\n-Sﬂag. Try specifying sizes of 8k, 12k, and 16k, while varying t he RAID\nlevel. What happens to the underlying I/O pattern when the siz e of the re-\nquest increases? Make sure to try this with the sequential workl oad too (-W\nsequential ); for what request sizes are RAID-4 and RAID-5 much more\nI/O efﬁcient?\n5. Use the timing mode of the simulator ( -t) to estimate the performance of\n100 random reads to the RAID, while varying the RAID levels, usin g 4 disks.\n6. Do the same as above, but increase the number of disks. How does t he\nperformance of each RAID level scale as the number of disks increa ses?\n7. Do the same as above, but use all writes ( -w 100 ) instead of reads. How\ndoes the performance of each RAID level scale now? Can you do a ro ugh\nestimate of the time it will take to complete the workload of 100 r andom\nwrites?\n8. Run the timing mode one last time, but this time with a sequential wo rkload\n(-W sequential ). How does the performance vary with RAID level, and\nwhen doing reads versus writes? How about when varying the size of each\nrequest? What size should you write to a RAID when using RAID-4 or\nRAID-5?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",47645
45-39. Files and Directories.pdf,45-39. Files and Directories,"39\nInterlude: Files and Directories\nThus far we have seen the development of two key operating system ab-\nstractions: the process, which is a virtualization of the CPU, an d the ad-\ndress space, which is a virtualization of memory. In tandem, the se two\nabstractions allow a program to run as if it is in its own private, i solated\nworld; as if it has its own processor (or processors); as if it has its ow n\nmemory. This illusion makes programming the system much easier and\nthus is prevalent today not only on desktops and servers but increa singly\non all programmable platforms including mobile phones and the lik e.\nIn this section, we add one more critical piece to the virtualiza tion puz-\nzle:persistent storage . A persistent-storage device, such as a classic hard\ndisk drive or a more modern solid-state storage device , stores informa-\ntion permanently (or at least, for a long time). Unlike memory, whos e\ncontents are lost when there is a power loss, a persistent-storage device\nkeeps such data intact. Thus, the OS must take extra care with such a\ndevice: this is where users keep data that they really care ab out.\nCRUX: HOWTOMANAGE A P ERSISTENT DEVICE\nHow should the OS manage a persistent device? What are the APIs?\nWhat are the important aspects of the implementation?\nThus, in the next few chapters, we will explore critical techn iques for\nmanaging persistent data, focusing on methods to improve perform ance\nand reliability. We begin, however, with an overview of the API: the in-\nterfaces you’ll expect to see when interacting with a U NIXﬁle system.\n39.1 Files And Directories\nTwo key abstractions have developed over time in the virtualiza tion\nof storage. The ﬁrst is the ﬁle. A ﬁle is simply a linear array of bytes,\neach of which you can read or write. Each ﬁle has some kind of low-level\nname , usually a number of some kind; often, the user is not aware of\n1\n2 INTERLUDE : FILES AND DIRECTORIES\nthis name (as we will see). For historical reasons, the low-level name of a\nﬁle is often referred to as its inode number . We’ll be learning a lot more\nabout inodes in future chapters; for now, just assume that each ﬁl e has an\ninode number associated with it.\nIn most systems, the OS does not know much about the structure of\nthe ﬁle (e.g., whether it is a picture, or a text ﬁle, or C code); ra ther, the\nresponsibility of the ﬁle system is simply to store such data per sistently\non disk and make sure that when you request the data again, you get\nwhat you put there in the ﬁrst place. Doing so is not as simple as it seems!\nThe second abstraction is that of a directory . A directory, like a ﬁle,\nalso has a low-level name (i.e., an inode number), but its conten ts are\nquite speciﬁc: it contains a list of (user-readable name, low-l evel name)\npairs. For example, let’s say there is a ﬁle with the low-level na me “10”,\nand it is referred to by the user-readable name of “foo”. The dire ctory\nthat “foo” resides in thus would have an entry (“foo”, “10”) that ma ps\nthe user-readable name to the low-level name. Each entry in a d irectory\nrefers to either ﬁles or other directories. By placing directori es within\nother directories, users are able to build an arbitrary directory tree (or\ndirectory hierarchy ), under which all ﬁles and directories are stored.\n/\nfoo\nbar.txtbar\nfoo bar\nbar.txt\nFigure 39.1: An Example Directory Tree\nThe directory hierarchy starts at a root directory (in U NIX-based sys-\ntems, the root directory is simply referred to as /) and uses some kind\nofseparator to name subsequent sub-directories until the desired ﬁle or\ndirectory is named. For example, if a user created a directory foo in the\nroot directory /, and then created a ﬁle bar.txt in the directory foo,\nwe could refer to the ﬁle by its absolute pathname , which in this case\nwould be/foo/bar.txt . See Figure 39.1 for a more complex directory\ntree; valid directories in the example are /, /foo, /bar, /bar/bar,\n/bar/foo and valid ﬁles are /foo/bar.txt and/bar/foo/bar.txt .\nDirectories and ﬁles can have the same name as long as they are in dif-\nferent locations in the ﬁle-system tree (e.g., there are two ﬁl es named\nbar.txt in the ﬁgure, /foo/bar.txt and/bar/foo/bar.txt ).\nYou may also notice that the ﬁle name in this example often has two\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 3\nTIP: THINK CAREFULLY ABOUT NAMING\nNaming is an important aspect of computer systems [SK09]. In U NIX\nsystems, virtually everything that you can think of is named th rough the\nﬁle system. Beyond just ﬁles, devices, pipes, and even process es [K84]\ncan be found in what looks like a plain old ﬁle system. This uniformi ty\nof naming eases your conceptual model of the system, and makes the\nsystem simpler and more modular. Thus, whenever creating a sys tem or\ninterface, think carefully about what names you are using.\nparts:bar andtxt, separated by a period. The ﬁrst part is an arbitrary\nname, whereas the second part of the ﬁle name is usually used to i ndi-\ncate the type of the ﬁle, e.g., whether it is C code (e.g., .c), or an image\n(e.g.,.jpg ), or a music ﬁle (e.g., .mp3 ). However, this is usually just a\nconvention : there is usually no enforcement that the data contained in a\nﬁle named main.c is indeed C source code.\nThus, we can see one great thing provided by the ﬁle system: a conv e-\nnient way to name all the ﬁles we are interested in. Names are important\nin systems as the ﬁrst step to accessing any resource is being a ble to name\nit. In U NIXsystems, the ﬁle system thus provides a uniﬁed way to access\nﬁles on disk, USB stick, CD-ROM, many other devices, and in fact m any\nother things, all located under the single directory tree.\n39.2 The File System Interface\nLet’s now discuss the ﬁle system interface in more detail. We’ll s tart\nwith the basics of creating, accessing, and deleting ﬁles. You may think\nthis is straightforward, but along the way we’ll discover the mys terious\ncall that is used to remove ﬁles, known as unlink() . Hopefully, by the\nend of this chapter, this mystery won’t be so mysterious to you!\n39.3 Creating Files\nWe’ll start with the most basic of operations: creating a ﬁle. This can be\naccomplished with the open system call; by calling open() and passing\nit theOCREAT ﬂag, a program can create a new ﬁle. Here is some exam-\nple code to create a ﬁle called “foo” in the current working direct ory.\nint fd = open(""foo"", O_CREAT|O_WRONLY|O_TRUNC, S_IRUSR|S _IWUSR);\nThe routine open() takes a number of different ﬂags. In this exam-\nple, the second parameter creates the ﬁle ( OCREAT ) if it does not exist,\nensures that the ﬁle can only be written to ( OWRONLY ), and, if the ﬁle\nalready exists, truncates it to a size of zero bytes thus removi ng any exist-\ning content ( OTRUNC ). The third parameter speciﬁes permissions, in this\ncase making the ﬁle readable and writable by the owner.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 INTERLUDE : FILES AND DIRECTORIES\nASIDE : THECREAT()SYSTEM CALL\nThe older way of creating a ﬁle is to call creat() , as follows:\nint fd = creat(""foo""); // option: add second flag to set permi ssions\nYou can think of creat() asopen() with the following ﬂags:\nOCREAT | O WRONLY | O TRUNC . Because open() can create a ﬁle,\nthe usage of creat() has somewhat fallen out of favor (indeed, it could\njust be implemented as a library call to open() ); however, it does hold a\nspecial place in U NIXlore. Speciﬁcally, when Ken Thompson was asked\nwhat he would do differently if he were redesigning U NIX, he replied:\n“I’d spell creat with an e.”\nOne important aspect of open() is what it returns: a ﬁle descriptor . A\nﬁle descriptor is just an integer, private per process, and is u sed in U NIX\nsystems to access ﬁles; thus, once a ﬁle is opened, you use the ﬁle de-\nscriptor to read or write the ﬁle, assuming you have permission to do so.\nIn this way, a ﬁle descriptor is a capability [L84], i.e., an opaque handle\nthat gives you the power to perform certain operations. Another way to\nthink of a ﬁle descriptor is as a pointer to an object of type ﬁle; once you\nhave such an object, you can call other “methods” to access the ﬁle , like\nread() andwrite() (we’ll see how to do so below).\nAs stated above, ﬁle descriptors are managed by the operating sy stem\non a per-process basis. This means some kind of simple structure ( e.g., an\narray) is kept in the proc structure on U NIXsystems. Here is the relevant\npiece from the xv6 kernel [CK+08]:\nstruct proc {\n...\nstruct file *ofile[NOFILE]; // Open files\n...\n};\nA simple array (with a maximum of NOFILE open ﬁles) tracks which\nﬁles are opened on a per-process basis. Each entry of the array is a ctually\njust a pointer to a struct file , which will be used to track information\nabout the ﬁle being read or written; we’ll discuss this further b elow.\n39.4 Reading And Writing Files\nOnce we have some ﬁles, of course we might like to read or write them .\nLet’s start by reading an existing ﬁle. If we were typing at a com mand\nline, we might just use the program cat to dump the contents of the ﬁle\nto the screen.\nprompt> echo hello > foo\nprompt> cat foo\nhello\nprompt>\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 5\nTIP: USESTRACE (ANDSIMILAR TOOLS )\nThestrace tool provides an awesome way to see what programs are up\nto. By running it, you can trace which system calls a program make s, see\nthe arguments and return codes, and generally get a very good ide a of\nwhat is going on.\nThe tool also takes some arguments which can be quite useful. For e x-\nample,-ffollows any fork’d children too; -treports the time of day\nat each call; -e trace=open,close,read,write only traces calls to\nthose system calls and ignores all others. There are many more powe rful\nﬂags — read the man pages and ﬁnd out how to harness this wonderful\ntool.\nIn this code snippet, we redirect the output of the program echo to\nthe ﬁlefoo, which then contains the word “hello” in it. We then use cat\nto see the contents of the ﬁle. But how does the cat program access the\nﬁlefoo?\nTo ﬁnd this out, we’ll use an incredibly useful tool to trace the sy s-\ntem calls made by a program. On Linux, the tool is called strace ; other\nsystems have similar tools (see dtruss on a Mac, or truss on some older\nUNIXvariants). What strace does is trace every system call made by a\nprogram while it runs, and dump the trace to the screen for you to s ee.\nHere is an example of using strace to ﬁgure out what cat is doing\n(some calls removed for readability):\nprompt> strace cat foo\n...\nopen(""foo"", O_RDONLY|O_LARGEFILE) = 3\nread(3, ""hello\n"", 4096) = 6\nwrite(1, ""hello\n"", 6) = 6\nhello\nread(3, """", 4096) = 0\nclose(3) = 0\n...\nprompt>\nThe ﬁrst thing that cat does is open the ﬁle for reading. A couple\nof things we should note about this; ﬁrst, that the ﬁle is only opened for\nreading (not writing), as indicated by the ORDONLY ﬂag; second, that\nthe 64-bit offset be used ( OLARGEFILE ); third, that the call to open()\nsucceeds and returns a ﬁle descriptor, which has the value of 3.\nWhy does the ﬁrst call to open() return 3, not 0 or perhaps 1 as you\nmight expect? As it turns out, each running process already has three\nﬁles open, standard input (which the process can read to receiv e input),\nstandard output (which the process can write to in order to dump i nfor-\nmation to the screen), and standard error (which the process can write\nerror messages to). These are represented by ﬁle descriptors 0, 1, and 2,\nrespectively. Thus, when you ﬁrst open another ﬁle (as cat does above),\nit will almost certainly be ﬁle descriptor 3.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 INTERLUDE : FILES AND DIRECTORIES\nAfter the open succeeds, cat uses theread() system call to repeat-\nedly read some bytes from a ﬁle. The ﬁrst argument to read() is the ﬁle\ndescriptor, thus telling the ﬁle system which ﬁle to read; a pr ocess can of\ncourse have multiple ﬁles open at once, and thus the descriptor en ables\nthe operating system to know which ﬁle a particular read refers to. The\nsecond argument points to a buffer where the result of the read() will be\nplaced; in the system-call trace above, strace shows the resul ts of the read\nin this spot (“hello”). The third argument is the size of the buff er, which\nin this case is 4 KB. The call to read() returns successfully as well, here\nreturning the number of bytes it read (6, which includes 5 for th e letters\nin the word “hello” and one for an end-of-line marker).\nAt this point, you see another interesting result of the strace: a single\ncall to thewrite() system call, to the ﬁle descriptor 1. As we mentioned\nabove, this descriptor is known as the standard output, and thus i s used\nto write the word “hello” to the screen as the program cat is meant to\ndo. But does it call write() directly? Maybe (if it is highly optimized).\nBut if not, what cat might do is call the library routine printf() ; in-\nternally,printf() ﬁgures out all the formatting details passed to it, and\neventually writes to standard output to print the results to t he screen.\nThecat program then tries to read more from the ﬁle, but since there\nare no bytes left in the ﬁle, the read() returns 0 and the program knows\nthat this means it has read the entire ﬁle. Thus, the program ca llsclose()\nto indicate that it is done with the ﬁle “foo”, passing in the corre sponding\nﬁle descriptor. The ﬁle is thus closed, and the reading of it thus complete.\nWriting a ﬁle is accomplished via a similar set of steps. First, a ﬁle\nis opened for writing, then the write() system call is called, perhaps\nrepeatedly for larger ﬁles, and then close() . Usestrace to trace writes\nto a ﬁle, perhaps of a program you wrote yourself, or by tracing the dd\nutility, e.g., dd if=foo of=bar .\n39.5 Reading And Writing, But Not Sequentially\nThus far, we’ve discussed how to read and write ﬁles, but all acc ess\nhas been sequential ; that is, we have either read a ﬁle from the beginning\nto the end, or written a ﬁle out from beginning to end.\nSometimes, however, it is useful to be able to read or write to a spe -\nciﬁc offset within a ﬁle; for example, if you build an index over a t ext\ndocument, and use it to look up a speciﬁc word, you may end up reading\nfrom some random offsets within the document. To do so, we will use\nthelseek() system call. Here is the function prototype:\noff_t lseek(int fildes, off_t offset, int whence);\nThe ﬁrst argument is familiar (a ﬁle descriptor). The second ar gu-\nment is the offset , which positions the ﬁle offset to a particular location\nwithin the ﬁle. The third argument, called whence for historical reasons,\ndetermines exactly how the seek is performed. From the man page:\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 7\nASIDE : DATA STRUCTURE — T HEOPEN FILETABLE\nEach process maintains an array of ﬁle descriptors, each of which refers\nto an entry in the system-wide open ﬁle table . Each entry in this table\ntracks which underlying ﬁle the descriptor refers to, the curr ent offset,\nand other relevant details such as whether the ﬁle is readable or writable.\nIf whence is SEEK_SET, the offset is set to offset bytes.\nIf whence is SEEK_CUR, the offset is set to its current\nlocation plus offset bytes.\nIf whence is SEEK_END, the offset is set to the size of\nthe file plus offset bytes.\nAs you can tell from this description, for each ﬁle a process opens, t he\nOS tracks a “current” offset, which determines where the next read or\nwrite will begin reading from or writing to within the ﬁle. Thus , part\nof the abstraction of an open ﬁle is that it has a current offset, whi ch\nis updated in one of two ways. The ﬁrst is when a read or write of N\nbytes takes place, Nis added to the current offset; thus each read or write\nimplicitly updates the offset. The second is explicitly withlseek , which\nchanges the offset as speciﬁed above.\nThe offset, as you might have guessed, is kept in that struct file\nwe saw earlier, as referenced from the struct proc . Here is a (simpli-\nﬁed) xv6 deﬁnition of the structure:\nstruct file {\nint ref;\nchar readable;\nchar writable;\nstruct inode *ip;\nuint off;\n};\nAs you can see in the structure, the OS can use this to determine\nwhether the opened ﬁle is readable or writable (or both), which un der-\nlying ﬁle it refers to (as pointed to by the struct inode pointerip),\nand the current offset ( off). There is also a reference count ( ref), which\nwe will discuss further below.\nThese ﬁle structures represent all of the currently opened ﬁle s in the\nsystem; together, they are sometimes referred to as the open ﬁle table .\nThe xv6 kernel just keeps these as an array as well, with one lock per\nentry, as shown here:\nstruct {\nstruct spinlock lock;\nstruct file file[NFILE];\n} ftable;\nLet’s make this a bit clearer with a few examples. First, let’s t rack a\nprocess that opens a ﬁle (of size 300 bytes) and reads it by callin g the\nread() system call repeatedly, each time reading 100 bytes. Here is a\ntrace of the relevant system calls, along with the values retur ned by each\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 INTERLUDE : FILES AND DIRECTORIES\nsystem call, and the value of the current offset in the open ﬁle ta ble for\nthis ﬁle access:\nReturn Current\nSystem Calls Code Offset\nfd = open(""file"", O RDONLY); 3 0\nread(fd, buffer, 100); 100 100\nread(fd, buffer, 100); 100 200\nread(fd, buffer, 100); 100 300\nread(fd, buffer, 100); 0 300\nclose(fd); 0 –\nThere are a couple of items of interest to note from the trace. First ,\nyou can see how the current offset gets initialized to zero when t he ﬁle is\nopened. Next, you can see how it is incremented with each read() by\nthe process; this makes it easy for a process to just keep calling read()\nto get the next chunk of the ﬁle. Finally, you can see how at the end , an\nattempted read() past the end of the ﬁle returns zero, thus indicating to\nthe process that it has read the ﬁle in its entirety.\nSecond, let’s trace a process that opens the same ﬁle twice and issues a\nread to each of them.\nOFT[10] OFT[11]\nReturn Current Current\nSystem Calls Code Offset Offset\nfd1 = open(""file"", O RDONLY); 3 0 –\nfd2 = open(""file"", O RDONLY); 4 0 0\nread(fd1, buffer1, 100); 100 100 0\nread(fd2, buffer2, 100); 100 100 100\nclose(fd1); 0 – 100\nclose(fd2); 0 – –\nIn this example, two ﬁle descriptors are allocated ( 3and4), and each\nrefers to a different entry in the open ﬁle table (in this example, entries 10\nand11, as shown in the table heading; OFT stands for Open File Table).\nIf you trace through what happens, you can see how each current offs et\nis updated independently.\nIn one ﬁnal example, a process uses lseek() to reposition the current\noffset before reading; in this case, only a single open ﬁle table e ntry is\nneeded (as with the ﬁrst example).\nReturn Current\nSystem Calls Code Offset\nfd = open(""file"", O RDONLY); 3 0\nlseek(fd, 200, SEEK SET); 200 200\nread(fd, buffer, 50); 50 250\nclose(fd); 0 –\nHere, thelseek() call ﬁrst sets the current offset to 200. The subse-\nquentread() then reads the next 50 bytes, and updates the current offset\naccordingly.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 9\nASIDE : CALLING LSEEK()DOES NOTPERFORM A D ISKSEEK\nThe poorly-named system call lseek() confuses many a student try-\ning to understand disks and how the ﬁle systems atop them work. Do\nnot confuse the two! The lseek() call simply changes a variable in OS\nmemory that tracks, for a particular process, at which offset its next read\nor write will start. A disk seek occurs when a read or write issued to the\ndisk is not on the same track as the last read or write, and thus nec es-\nsitates a head movement. Making this even more confusing is the f act\nthat calling lseek() to read or write from/to random parts of a ﬁle, and\nthen reading/writing to those random parts, will indeed lead t o more\ndisk seeks. Thus, calling lseek() can certainly lead to a seek in an up-\ncoming read or write, but absolutely does not cause any disk I/O to oc cur\nitself.\n39.6 Shared File Table Entries: fork() Anddup()\nIn many cases (as in the examples shown above), the mapping of ﬁle\ndescriptor to an entry in the open ﬁle table is a one-to-one mapping . For\nexample, when a process runs, it might decide to open a ﬁle, read it, and\nthen close it; in this example, the ﬁle will have a unique entry in the open\nﬁle table. Even if some other process reads the same ﬁle at the sam e time,\neach will have its own entry in the open ﬁle table. In this way, ea ch logical\nreading or writing of a ﬁle is independent, and each has its own cu rrent\noffset while it accesses the given ﬁle.\nHowever, there are a few interesting cases where an entry in th e open\nﬁle table is shared . One of those cases occurs when a parent process creates\na child process with fork() . Figure 39.2 shows a small code snippet in\nwhich a parent creates a child and then waits for it to complete. The child\nadjusts the current offset via a call to lseek() and then exits. Finally the\nparent, after waiting for the child, checks the current offset and prints out\nits value.\nint main(int argc, char *argv[]) {\nint fd = open(""file.txt"", O_RDONLY);\nassert(fd >= 0);\nint rc = fork();\nif (rc == 0) {\nrc = lseek(fd, 10, SEEK_SET);\nprintf(""child: offset %d\n"", rc);\n} else if (rc > 0) {\n(void) wait(NULL);\nprintf(""parent: offset %d\n"", (int) lseek(fd, 0, SEEK_CUR ));\n}\nreturn 0;\n}\nFigure 39.2: Shared Parent/Child File Table Entries ( fork-seek.c )\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 INTERLUDE : FILES AND DIRECTORIES\nParent\nFile\nDescriptors\n3:\nChild\nFile\nDescriptors\n3:Open File Table\nrefcnt: 2\noff: 10\ninode: Inode #1000\n(file.txt)\nFigure 39.3: Processes Sharing An Open File Table Entry\nWhen we run this program, we see the following output:\nprompt> ./fork-seek\nchild: offset 10\nparent: offset 10\nprompt>\nFigure 39.3 shows the relationships that connect each processes private\ndescriptor arrays, the shared open ﬁle table entry, and the ref erence from\nit to the underlying ﬁle-system inode. Note that we ﬁnally make use of\nthereference count here. When a ﬁle table entry is shared, its reference\ncount is incremented; only when both processes close the ﬁle (or exi t) will\nthe entry be removed.\nSharing open ﬁle table entries across parent and child is occasion ally\nuseful. For example, if you create a number of processes that are c ooper-\natively working on a task, they can write to the same output ﬁle wi thout\nany extra coordination. For more on what is shared by processes when\nfork() is called, please see the man pages.\nOne other interesting, and perhaps more useful, case of sharing occurs\nwith thedup() system call (and its very similar cousins, dup2() and\nevendup3() ).\nThedup() call allows a process to create a new ﬁle descriptor that\nrefers to the same underlying open ﬁle as an existing descript or. Figure\n39.4 shows a small code snippet that shows how dup() can be used.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 11\nint main(int argc, char *argv[]) {\nint fd = open(""README"", O_RDONLY);\nassert(fd >= 0);\nint fd2 = dup(fd);\n// now fd and fd2 can be used interchangeably\nreturn 0;\n}\nFigure 39.4: Shared File Table Entry With dup() (dup.c )\nThedup() call (and, in particular, dup2() ) are useful when writing\na U NIX shell and performing operations like output redirection; spend\nsome time and think about why! And now, you are thinking: why didn’t\nthey tell me this when I was doing the shell project? Oh well, you c an’t get\neverything in the right order, even in an incredible book about ope rating\nsystems. Sorry!\n39.7 Writing Immediately With fsync()\nMost times when a program calls write() , it is just telling the ﬁle\nsystem: please write this data to persistent storage, at some p oint in the\nfuture. The ﬁle system, for performance reasons, will buffer such writes\nin memory for some time (say 5 seconds, or 30); at that later point in\ntime, the write(s) will actually be issued to the storage devi ce. From the\nperspective of the calling application, writes seem to complet e quickly,\nand only in rare cases (e.g., the machine crashes after the write() call\nbut before the write to disk) will data be lost.\nHowever, some applications require something more than this even -\ntual guarantee. For example, in a database management system (DBMS),\ndevelopment of a correct recovery protocol requires the ability to f orce\nwrites to disk from time to time.\nTo support these types of applications, most ﬁle systems provide s ome\nadditional control APIs. In the U NIXworld, the interface provided to ap-\nplications is known as fsync(int fd) . When a process calls fsync()\nfor a particular ﬁle descriptor, the ﬁle system responds by forci ng all dirty\n(i.e., not yet written) data to disk, for the ﬁle referred to by t he speciﬁed\nﬁle descriptor. The fsync() routine returns once all of these writes are\ncomplete.\nHere is a simple example of how to use fsync() . The code opens\nthe ﬁlefoo, writes a single chunk of data to it, and then calls fsync()\nto ensure the writes are forced immediately to disk. Once the fsync()\nreturns, the application can safely move on, knowing that the dat a has\nbeen persisted (if fsync() is correctly implemented, that is).\nint fd = open(""foo"", O_CREAT|O_WRONLY|O_TRUNC, S_IRUSR|S _IWUSR);\nassert(fd > -1);\nint rc = write(fd, buffer, size);\nassert(rc == size);\nrc = fsync(fd);\nassert(rc == 0);\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 INTERLUDE : FILES AND DIRECTORIES\nInterestingly, this sequence does not guarantee everything t hat you\nmight expect; in some cases, you also need to fsync() the directory that\ncontains the ﬁle foo. Adding this step ensures not only that the ﬁle itself\nis on disk, but that the ﬁle, if newly created, also is durably a part of the\ndirectory. Not surprisingly, this type of detail is often overlooke d, leading\nto many application-level bugs [P+13,P+14].\n39.8 Renaming Files\nOnce we have a ﬁle, it is sometimes useful to be able to give a ﬁle a\ndifferent name. When typing at the command line, this is accomp lished\nwithmvcommand; in this example, the ﬁle foo is renamed bar:\nprompt> mv foo bar\nUsingstrace , we can see that mvuses the system call rename(char\n*old, char *new) , which takes precisely two arguments: the original\nname of the ﬁle ( old) and the new name ( new).\nOne interesting guarantee provided by the rename() call is that it is\n(usually) implemented as an atomic call with respect to system crashes;\nif the system crashes during the renaming, the ﬁle will eithe r be named\nthe old name or the new name, and no odd in-between state can arise .\nThus,rename() is critical for supporting certain kinds of applications\nthat require an atomic update to ﬁle state.\nLet’s be a little more speciﬁc here. Imagine that you are using a ﬁ le ed-\nitor (e.g., emacs), and you insert a line into the middle of a ﬁle. The ﬁle’s\nname, for the example, is foo.txt . The way the editor might update the\nﬁle to guarantee that the new ﬁle has the original contents plus the line\ninserted is as follows (ignoring error-checking for simplicity) :\nint fd = open(""foo.txt.tmp"", O_WRONLY|O_CREAT|O_TRUNC,\nS_IRUSR|S_IWUSR);\nwrite(fd, buffer, size); // write out new version of file\nfsync(fd);\nclose(fd);\nrename(""foo.txt.tmp"", ""foo.txt"");\nWhat the editor does in this example is simple: write out the new\nversion of the ﬁle under a temporary name ( foo.txt.tmp ), force it to\ndisk with fsync() , and then, when the application is certain the new\nﬁle metadata and contents are on the disk, rename the temporary ﬁ le to\nthe original ﬁle’s name. This last step atomically swaps the new ﬁle into\nplace, while concurrently deleting the old version of the ﬁle, an d thus an\natomic ﬁle update is achieved.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 13\n39.9 Getting Information About Files\nBeyond ﬁle access, we expect the ﬁle system to keep a fair amount\nof information about each ﬁle it is storing. We generally call such data\nabout ﬁles metadata . To see the metadata for a certain ﬁle, we can use the\nstat() orfstat() system calls. These calls take a pathname (or ﬁle\ndescriptor) to a ﬁle and ﬁll in a stat structure as seen here:\nstruct stat {\ndev_t st_dev; / *ID of device containing file */\nino_t st_ino; / *inode number */\nmode_t st_mode; / *protection */\nnlink_t st_nlink; / *number of hard links */\nuid_t st_uid; / *user ID of owner */\ngid_t st_gid; / *group ID of owner */\ndev_t st_rdev; / *device ID (if special file) */\noff_t st_size; / *total size, in bytes */\nblksize_t st_blksize; / *blocksize for filesystem I/O */\nblkcnt_t st_blocks; / *number of blocks allocated */\ntime_t st_atime; / *time of last access */\ntime_t st_mtime; / *time of last modification */\ntime_t st_ctime; / *time of last status change */\n};\nYou can see that there is a lot of information kept about each ﬁle, in-\ncluding its size (in bytes), its low-level name (i.e., inode nu mber), some\nownership information, and some information about when the ﬁle was\naccessed or modiﬁed, among other things. To see this information, y ou\ncan use the command line tool stat :\nprompt> echo hello > file\nprompt> stat file\nFile: ‘file’\nSize: 6 Blocks: 8 IO Block: 4096 regular file\nDevice: 811h/2065d Inode: 67158084 Links: 1\nAccess: (0640/-rw-r-----) Uid: (30686/ remzi) Gid: (30686 / remzi)\nAccess: 2011-05-03 15:50:20.157594748 -0500\nModify: 2011-05-03 15:50:20.157594748 -0500\nChange: 2011-05-03 15:50:20.157594748 -0500\nAs it turns out, each ﬁle system usually keeps this type of inform ation\nin a structure called an inode1. We’ll be learning a lot more about inodes\nwhen we talk about ﬁle system implementation. For now, you should ju st\nthink of an inode as a persistent data structure kept by the ﬁle s ystem that\nhas information like we see above inside of it. All inodes reside on d isk;\na copy of active ones are usually cached in memory to speed up acces s.\n1Some ﬁle systems call these structures similar, but slightly dif ferent, names, such as\ndnodes; the basic idea is similar however.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 INTERLUDE : FILES AND DIRECTORIES\n39.10 Removing Files\nAt this point, we know how to create ﬁles and access them, either s e-\nquentially or not. But how do you delete ﬁles? If you’ve used U NIX, you\nprobably think you know: just run the program rm. But what system call\ndoesrmuse to remove a ﬁle?\nLet’s use our old friend strace again to ﬁnd out. Here we remove\nthat pesky ﬁle “foo”:\nprompt> strace rm foo\n...\nunlink(""foo"") = 0\n...\nWe’ve removed a bunch of unrelated cruft from the traced output,\nleaving just a single call to the mysteriously-named system c allunlink() .\nAs you can see, unlink() just takes the name of the ﬁle to be removed,\nand returns zero upon success. But this leads us to a great puzz le: why\nis this system call named “unlink”? Why not just “remove” or “del ete”.\nTo understand the answer to this puzzle, we must ﬁrst underst and more\nthan just ﬁles, but also directories.\n39.11 Making Directories\nBeyond ﬁles, a set of directory-related system calls enable you t o make,\nread, and delete directories. Note you can never write to a direc tory di-\nrectly; because the format of the directory is considered ﬁle sys tem meta-\ndata, you can only update a directory indirectly by, for example, creating\nﬁles, directories, or other object types within it. In this way, t he ﬁle system\nmakes sure that the contents of the directory always are as expec ted.\nTo create a directory, a single system call, mkdir() , is available. The\neponymous mkdir program can be used to create such a directory. Let’s\ntake a look at what happens when we run the mkdir program to make a\nsimple directory called foo:\nprompt> strace mkdir foo\n...\nmkdir(""foo"", 0777) = 0\n...\nprompt>\nWhen such a directory is created, it is considered “empty”, alt hough it\ndoes have a bare minimum of contents. Speciﬁcally, an empty direc tory\nhas two entries: one entry that refers to itself, and one entry t hat refers\nto its parent. The former is referred to as the “.” (dot) director y, and the\nlatter as “..” (dot-dot). You can see these directories by passin g a ﬂag (-a)\nto the program ls:\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 15\nTIP: BEWARY OFPOWERFUL COMMANDS\nThe program rmprovides us with a great example of powerful com-\nmands, and how sometimes too much power can be a bad thing. For\nexample, to remove a bunch of ﬁles at once, you can type something li ke:\nprompt> rm *\nwhere the *will match all ﬁles in the current directory. But sometimes\nyou want to also delete the directories too, and in fact all of their contents.\nYou can do this by telling rmto recursively descend into each directory,\nand remove its contents too:\nprompt> rm -rf *\nWhere you get into trouble with this small string of characters i s when\nyou issue the command, accidentally, from the root directory of a ﬁle sys-\ntem, thus removing every ﬁle and directory from it. Oops!\nThus, remember the double-edged sword of powerful commands; whil e\nthey give you the ability to do a lot of work with a small number of\nkeystrokes, they also can quickly and readily do a great deal of harm.\nprompt> ls -a\n./ ../\nprompt> ls -al\ntotal 8\ndrwxr-x--- 2 remzi remzi 6 Apr 30 16:17 ./\ndrwxr-x--- 26 remzi remzi 4096 Apr 30 16:17 ../\n39.12 Reading Directories\nNow that we’ve created a directory, we might wish to read one too.\nIndeed, that is exactly what the program lsdoes. Let’s write our own\nlittle tool like lsand see how it is done.\nInstead of just opening a directory as if it were a ﬁle, we instead use\na new set of calls. Below is an example program that prints the cont ents\nof a directory. The program uses three calls, opendir() ,readdir() ,\nandclosedir() , to get the job done, and you can see how simple the\ninterface is; we just use a simple loop to read one directory entry at a time,\nand print out the name and inode number of each ﬁle in the directory .\nint main(int argc, char *argv[]) {\nDIR*dp = opendir(""."");\nassert(dp != NULL);\nstruct dirent *d;\nwhile ((d = readdir(dp)) != NULL) {\nprintf(""%lu %s\n"", (unsigned long) d->d_ino, d->d_name);\n}\nclosedir(dp);\nreturn 0;\n}\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 INTERLUDE : FILES AND DIRECTORIES\nThe declaration below shows the information available within eac h\ndirectory entry in the struct dirent data structure:\nstruct dirent {\nchar d_name[256]; / *filename */\nino_t d_ino; / *inode number */\noff_t d_off; / *offset to the next dirent */\nunsigned short d_reclen; / *length of this record */\nunsigned char d_type; / *type of file */\n};\nBecause directories are light on information (basically, just m apping\nthe name to the inode number, along with a few other details), a pr ogram\nmay want to call stat() on each ﬁle to get more information on each,\nsuch as its length or other detailed information. Indeed, this is exactly\nwhatlsdoes when you pass it the -lﬂag; trystrace onlswith and\nwithout that ﬂag to see for yourself.\n39.13 Deleting Directories\nFinally, you can delete a directory with a call to rmdir() (which is\nused by the program of the same name, rmdir ). Unlike ﬁle deletion,\nhowever, removing directories is more dangerous, as you could poten-\ntially delete a large amount of data with a single command. Thus, rmdir()\nhas the requirement that the directory be empty (i.e., only has “.” and “..”\nentries) before it is deleted. If you try to delete a non-empty di rectory, the\ncall tormdir() simply will fail.\n39.14 Hard Links\nWe now come back to the mystery of why removing a ﬁle is performed\nviaunlink() , by understanding a new way to make an entry in the\nﬁle system tree, through a system call known as link() . Thelink()\nsystem call takes two arguments, an old pathname and a new one; w hen\nyou “link” a new ﬁle name to an old one, you essentially create anoth er\nway to refer to the same ﬁle. The command-line program lnis used to\ndo this, as we see in this example:\nprompt> echo hello > file\nprompt> cat file\nhello\nprompt> ln file file2\nprompt> cat file2\nhello\nHere we created a ﬁle with the word “hello” in it, and called the ﬁ le\nfile2. We then create a hard link to that ﬁle using the lnprogram. After\nthis, we can examine the ﬁle by either opening file orfile2 .\n2Note how creative the authors of this book are. We also used to have a ca t named “Cat”\n(true story). However, she died, and we now have a hamster named “Hammy .” Update:\nHammy is now dead too. The pet bodies are piling up.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 17\nThe waylink works is that it simply creates another name in the di-\nrectory you are creating the link to, and refers it to the same inode number\n(i.e., low-level name) of the original ﬁle. The ﬁle is not copied in any way;\nrather, you now just have two human names ( file andfile2 ) that both\nrefer to the same ﬁle. We can even see this in the directory itse lf, by print-\ning out the inode number of each ﬁle:\nprompt> ls -i file file2\n67158084 file\n67158084 file2\nprompt>\nBy passing the -iﬂag tols, it prints out the inode number of each ﬁle\n(as well as the ﬁle name). And thus you can see what link really h as done:\njust make a new reference to the same exact inode number (67158 084 in\nthis example).\nBy now you might be starting to see why unlink() is calledunlink() .\nWhen you create a ﬁle, you are really doing twothings. First, you are\nmaking a structure (the inode) that will track virtually all r elevant infor-\nmation about the ﬁle, including its size, where its blocks are on d isk, and\nso forth. Second, you are linking a human-readable name to that ﬁle, and\nputting that link into a directory.\nAfter creating a hard link to a ﬁle, to the ﬁle system, there is no dif-\nference between the original ﬁle name ( file ) and the newly created ﬁle\nname (file2 ); indeed, they are both just links to the underlying meta-\ndata about the ﬁle, which is found in inode number 67158084.\nThus, to remove a ﬁle from the ﬁle system, we call unlink() . In the\nexample above, we could for example remove the ﬁle named file , and\nstill access the ﬁle without difﬁculty:\nprompt> rm file\nremoved ‘file’\nprompt> cat file2\nhello\nThe reason this works is because when the ﬁle system unlinks ﬁle , it\nchecks a reference count within the inode number. This reference count\n(sometimes called the link count ) allows the ﬁle system to track how\nmany different ﬁle names have been linked to this particular inode. When\nunlink() is called, it removes the “link” between the human-readable\nname (the ﬁle that is being deleted) to the given inode number, and decre-\nments the reference count; only when the reference count reache s zero\ndoes the ﬁle system also free the inode and related data blocks, a nd thus\ntruly “delete” the ﬁle.\nYou can see the reference count of a ﬁle using stat() of course. Let’s\nsee what it is when we create and delete hard links to a ﬁle. In t his exam-\nple, we’ll create three links to the same ﬁle, and then delete t hem. Watch\nthe link count!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n18 INTERLUDE : FILES AND DIRECTORIES\nprompt> echo hello > file\nprompt> stat file\n... Inode: 67158084 Links: 1 ...\nprompt> ln file file2\nprompt> stat file\n... Inode: 67158084 Links: 2 ...\nprompt> stat file2\n... Inode: 67158084 Links: 2 ...\nprompt> ln file2 file3\nprompt> stat file\n... Inode: 67158084 Links: 3 ...\nprompt> rm file\nprompt> stat file2\n... Inode: 67158084 Links: 2 ...\nprompt> rm file2\nprompt> stat file3\n... Inode: 67158084 Links: 1 ...\nprompt> rm file3\n39.15 Symbolic Links\nThere is one other type of link that is really useful, and it is cal led a\nsymbolic link or sometimes a soft link . As it turns out, hard links are\nsomewhat limited: you can’t create one to a directory (for fear that you\nwill create a cycle in the directory tree); you can’t hard link to ﬁles in\nother disk partitions (because inode numbers are only unique wit hin a\nparticular ﬁle system, not across ﬁle systems); etc. Thus, a ne w type of\nlink called the symbolic link was created.\nTo create such a link, you can use the same program ln, but with the\n-sﬂag. Here is an example:\nprompt> echo hello > file\nprompt> ln -s file file2\nprompt> cat file2\nhello\nAs you can see, creating a soft link looks much the same, and the orig -\ninal ﬁle can now be accessed through the ﬁle name file as well as the\nsymbolic link name file2 .\nHowever, beyond this surface similarity, symbolic links are ac tually\nquite different from hard links. The ﬁrst difference is that a symbolic\nlink is actually a ﬁle itself, of a different type. We’ve alread y talked about\nregular ﬁles and directories; symbolic links are a third type t he ﬁle system\nknows about. A stat on the symlink reveals all:\nprompt> stat file\n... regular file ...\nprompt> stat file2\n... symbolic link ...\nRunninglsalso reveals this fact. If you look closely at the ﬁrst char-\nacter of the long-form of the output from ls, you can see that the ﬁrst\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 19\ncharacter in the left-most column is a -for regular ﬁles, a dfor directo-\nries, and an lfor soft links. You can also see the size of the symbolic link\n(4 bytes in this case), as well as what the link points to (the ﬁl e named\nfile ).\nprompt> ls -al\ndrwxr-x--- 2 remzi remzi 29 May 3 19:10 ./\ndrwxr-x--- 27 remzi remzi 4096 May 3 15:14 ../\n-rw-r----- 1 remzi remzi 6 May 3 19:10 file\nlrwxrwxrwx 1 remzi remzi 4 May 3 19:10 file2 -> file\nThe reason that file2 is 4 bytes is because the way a symbolic link is\nformed is by holding the pathname of the linked-to ﬁle as the data of the\nlink ﬁle. Because we’ve linked to a ﬁle named file , our link ﬁle file2\nis small (4 bytes). If we link to a longer pathname, our link ﬁle w ould be\nbigger:\nprompt> echo hello > alongerfilename\nprompt> ln -s alongerfilename file3\nprompt> ls -al alongerfilename file3\n-rw-r----- 1 remzi remzi 6 May 3 19:17 alongerfilename\nlrwxrwxrwx 1 remzi remzi 15 May 3 19:17 file3 -> alongerfilen ame\nFinally, because of the way symbolic links are created, they le ave the\npossibility for what is known as a dangling reference :\nprompt> echo hello > file\nprompt> ln -s file file2\nprompt> cat file2\nhello\nprompt> rm file\nprompt> cat file2\ncat: file2: No such file or directory\nAs you can see in this example, quite unlike hard links, removin g the\noriginal ﬁle named file causes the link to point to a pathname that no\nlonger exists.\n39.16 Permission Bits And Access Control Lists\nThe abstraction of a process provided two central virtualization s: of\nthe CPU and of memory. Each of these gave the illusion to a process th at\nit had its own private CPU and its own private memory; in reality, the OS\nunderneath used various techniques to share limited physica l resources\namong competing entities in a safe and secure manner.\nThe ﬁle system also presents a virtual view of a disk, transform ing it\nfrom a bunch of raw blocks into much more user-friendly ﬁles and di -\nrectories, as described within this chapter. However, the abs traction is\nnotably different from that of the CPU and memory, in that ﬁles are com-\nmonly shared among different users and processes and are not (always)\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n20 INTERLUDE : FILES AND DIRECTORIES\nprivate. Thus, a more comprehensive set of mechanisms for enabli ng var-\nious degrees of sharing are usually present within ﬁle systems .\nThe ﬁrst form of such mechanisms is the classic U NIXpermission bits .\nTo see permissions for a ﬁle foo.txt , just type:\nprompt> ls -l foo.txt\n-rw-r--r-- 1 remzi wheel 0 Aug 24 16:29 foo.txt\nWe’ll just pay attention to the ﬁrst part of this output, namely th e\n-rw-r--r-- . The ﬁrst character here just shows the type of the ﬁle: -for\na regular ﬁle (which foo.txt is),dfor a directory, lfor a symbolic link,\nand so forth; this is (mostly) not related to permissions, so we’ll ignore it\nfor now.\nWe are interested in the permission bits, which are represent ed by the\nnext nine characters ( rw-r--r-- ). These bits determine, for each regular\nﬁle, directory, and other entities, exactly who can access it a nd how.\nThe permissions consist of three groupings: what the owner of the ﬁle\ncan do to it, what someone in a group can do to the ﬁle, and ﬁnally, what\nanyone (sometimes referred to as other ) can do. The abilities the owner,\ngroup member, or others can have include the ability to read the ﬁ le, write\nit, or execute it.\nIn the example above, the ﬁrst three characters of the output of ls\nshow that the ﬁle is both readable and writable by the owner ( rw-), and\nonly readable by members of the group wheel and also by anyone else\nin the system ( r-- followed by r--).\nThe owner of the ﬁle can readily change these permissions, for exa m-\nple by using the chmod command (to change the ﬁle mode ). To remove\nthe ability for anyone except the owner to access the ﬁle, you could type:\nprompt> chmod 600 foo.txt\nThis command enables the readable bit (4) and writable bit (2) for the\nowner (OR’ing them together yields the 6 above), but set the group a nd\nother permission bits to 0 and 0, respectively, thus setting th e permissions\ntorw------- .\nThe execute bit is particularly interesting. For regular ﬁle s, its presence\ndetermines whether a program can be run or not. For example, if we h ave\na simple shell script called hello.csh , we may wish to run it by typing:\nprompt> ./hello.csh\nhello, from shell world.\nHowever, if we don’t set the execute bit properly for this ﬁle, the f ol-\nlowing happens:\nprompt> chmod 600 hello.csh\nprompt> ./hello.csh\n./hello.csh: Permission denied.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 21\nASIDE : SUPERUSER FORFILESYSTEMS\nWhich user is allowed to do privileged operations to help admini ster the\nﬁle system? For example, if an inactive user’s ﬁles need to be de leted to\nsave space, who has the rights to do so?\nOn local ﬁle systems, the common default is for there to be some kind of\nsuperuser (i.e., root) who can access all ﬁles regardless of privileges. In\na distributed ﬁle system such as AFS (which has access control l ists), a\ngroup called system:administrators contains users that are trusted\nto do so. In both cases, these trusted users represent an inhere nt secu-\nrity risk; if an attacker is able to somehow impersonate such a us er, the\nattacker can access all the information in the system, thus viol ating ex-\npected privacy and protection guarantees.\nFor directories, the execute bit behaves a bit differently. Spe ciﬁcally,\nit enables a user (or group, or everyone) to do things like change d i-\nrectories (i.e., cd) into the given directory, and, in combination with the\nwritable bit, create ﬁles therein. The best way to learn more a bout this:\nplay around with it yourself! Don’t worry, you (probably) won’t mess\nanything up too badly.\nBeyond permissions bits, some ﬁle systems, including the distr ibuted\nﬁle system known as AFS (discussed in a later chapter), includ ing more\nsophisticated controls. AFS, for example, does this in the form of an ac-\ncess control list (ACL ) per directory. Access control lists are a more gen-\neral and powerful way to represent exactly who can access a giv en re-\nsource. In a ﬁle system, this enables a user to create a very spe ciﬁc list of\nwho can and cannot read a set of ﬁles, in contrast to the somewhat li mited\nowner/group/everyone model of permissions bits described above.\nFor example, here are the access controls for a private directory i n one\nauthor’s AFS account, as shown by the fs listacl command:\nprompt> fs listacl private\nAccess list for private is\nNormal rights:\nsystem:administrators rlidwka\nremzi rlidwka\nThe listing shows that both the system administrators and the us er\nremzi can lookup, insert, delete, and administer ﬁles in this direct ory, as\nwell as read, write, and lock those ﬁles.\nTo allow someone (in this case, the other author) to access to this d i-\nrectory, user remzi can just type the following command.\nprompt> fs setacl private/ andrea rl\nThere goes remzi ’s privacy! But now you have learned an even more\nimportant lesson: there can be no secrets in a good marriage, even within\nthe ﬁle system3.\n3Married happily since 1996, if you were wondering. We know, you weren’ t.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n22 INTERLUDE : FILES AND DIRECTORIES\nTIP: BEWARY OFTOCTTOU\nIn 1974, McPhee noticed a problem in computer systems. Speciﬁ-\ncally, McPhee noted that “... if there exists a time interval b etween\na validity-check and the operation connected with that validit y-check,\n[and,] through multitasking, the validity-check variables can deliberately\nbe changed during this time interval, resulting in an invali d operation be-\ning performed by the control program.” We today call this the Time Of\nCheck To Time Of Use (TOCTTOU ) problem, and alas, it still can occur.\nA simple example, as described by Bishop and Dilger [BD96], sh ows how\na user can trick a more trusted service and thus cause trouble. I magine,\nfor example, that a mail service runs as root (and thus has privil ege to\naccess all ﬁles on a system). This service appends an incoming m essage\nto a user’s inbox ﬁle as follows. First, it calls lstat() to get informa-\ntion about the ﬁle, speciﬁcally ensuring that it is actually ju st a regular\nﬁle owned by the target user, and not a link to another ﬁle that the mail\nserver should not be updating. Then, after the check succeeds, the server\nupdates the ﬁle with the new message.\nUnfortunately, the gap between the check and the update leads to a prob-\nlem: the attacker (in this case, the user who is receiving the mail, and thus\nhas permissions to access the inbox) switches the inbox ﬁle (via a call\ntorename() ) to point to a sensitive ﬁle such as /etc/passwd (which\nholds information about users and their passwords). If this switc h hap-\npens at just the right time (between the check and the access) , the server\nwill blithely update the sensitive ﬁle with the contents of the mail. The\nattacker can now write to the sensitive ﬁle by sending an email , an esca-\nlation in privilege; by updating /etc/passwd , the attacker can add an\naccount with root privileges and thus gain control of the system.\nThere are not any simple and great solutions to the TOCTTOU proble m\n[T+08]. One approach is to reduce the number of services that ne ed root\nprivileges to run, which helps. The ONOFOLLOW ﬂag makes it so that\nopen() will fail if the target is a symbolic link, thus avoiding attack s\nthat require said links. More radicial approaches, such as usi ng a trans-\nactional ﬁle system [H+18], would solve the problem, there aren’t many\ntransactional ﬁle systems in wide deployment. Thus, the usual (lame)\nadvice: careful when you write code that runs with high privile ges!\n39.17 Making And Mounting A File System\nWe’ve now toured the basic interfaces to access ﬁles, directorie s, and\ncertain types of special types of links. But there is one more topic we\nshould discuss: how to assemble a full directory tree from many un der-\nlying ﬁle systems. This task is accomplished via ﬁrst making ﬁ le systems,\nand then mounting them to make their contents accessible.\nTo make a ﬁle system, most ﬁle systems provide a tool, usually re-\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 23\nferred to as mkfs (pronounced “make fs”), that performs exactly this task.\nThe idea is as follows: give the tool, as input, a device (such as a d isk par-\ntition, e.g., /dev/sda1 ) and a ﬁle system type (e.g., ext3), and it simply\nwrites an empty ﬁle system, starting with a root directory, onto t hat disk\npartition. And mkfs said, let there be a ﬁle system!\nHowever, once such a ﬁle system is created, it needs to be made ac -\ncessible within the uniform ﬁle-system tree. This task is ach ieved via the\nmount program (which makes the underlying system call mount() to do\nthe real work). What mount does, quite simply is take an existing direc-\ntory as a target mount point and essentially paste a new ﬁle system onto\nthe directory tree at that point.\nAn example here might be useful. Imagine we have an unmounted\next3 ﬁle system, stored in device partition /dev/sda1 , that has the fol-\nlowing contents: a root directory which contains two sub-directori es,a\nandb, each of which in turn holds a single ﬁle named foo. Let’s say we\nwish to mount this ﬁle system at the mount point /home/users . We\nwould type something like this:\nprompt> mount -t ext3 /dev/sda1 /home/users\nIf successful, the mount would thus make this new ﬁle system ava il-\nable. However, note how the new ﬁle system is now accessed. To look at\nthe contents of the root directory, we would use lslike this:\nprompt> ls /home/users/\na b\nAs you can see, the pathname /home/users/ now refers to the root\nof the newly-mounted directory. Similarly, we could access direc toriesa\nandbwith the pathnames /home/users/a and/home/users/b . Fi-\nnally, the ﬁles named foo could be accessed via /home/users/a/foo\nand/home/users/b/foo . And thus the beauty of mount: instead of\nhaving a number of separate ﬁle systems, mount uniﬁes all ﬁle sy stems\ninto one tree, making naming uniform and convenient.\nTo see what is mounted on your system, and at which points, simply\nrun themount program. You’ll see something like this:\n/dev/sda1 on / type ext3 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\n/dev/sda5 on /tmp type ext3 (rw)\n/dev/sda7 on /var/vice/cache type ext3 (rw)\ntmpfs on /dev/shm type tmpfs (rw)\nAFS on /afs type afs (rw)\nThis crazy mix shows that a whole number of different ﬁle systems ,\nincluding ext3 (a standard disk-based ﬁle system), the proc ﬁ le system (a\nﬁle system for accessing information about current processes), t mpfs (a\nﬁle system just for temporary ﬁles), and AFS (a distributed ﬁle system)\nare all glued together onto this one machine’s ﬁle-system tree.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n24 INTERLUDE : FILES AND DIRECTORIES\nASIDE : KEYFILESYSTEM TERMS\n•Aﬁleis an array of bytes which can be created, read, written, and\ndeleted. It has a low-level name (i.e., a number) that refers t o it\nuniquely. The low-level name is often called an i-number .\n•Adirectory is a collection of tuples, each of which contains a\nhuman-readable name and low-level name to which it maps. Each\nentry refers either to another directory or to a ﬁle. Each direct ory\nalso has a low-level name (i-number) itself. A directory alway s has\ntwo special entries: the .entry, which refers to itself, and the ..\nentry, which refers to its parent.\n•Adirectory tree ordirectory hierarchy organizes all ﬁles and direc-\ntories into a large tree, starting at the root.\n•To access a ﬁle, a process must use a system call (usually, open() )\nto request permission from the operating system. If permission i s\ngranted, the OS returns a ﬁle descriptor , which can then be used\nfor read or write access, as permissions and intent allow.\n•Each ﬁle descriptor is a private, per-process entity, which re fers to\nan entry in the open ﬁle table . The entry therein tracks which ﬁle\nthis access refers to, the current offset of the ﬁle (i.e., which part\nof the ﬁle the next read or write will access), and other relevant\ninformation.\n•Calls toread() andwrite() naturally update the current offset;\notherwise, processes can use lseek() to change its value, enabling\nrandom access to different parts of the ﬁle.\n•To force updates to persistent media, a process must use fsync()\nor related calls. However, doing so correctly while maintaining\nhigh performance is challenging [P+14], so think carefully w hen\ndoing so.\n•To have multiple human-readable names in the ﬁle system refe r to\nthe same underlying ﬁle, use hard links orsymbolic links . Each\nis useful in different circumstances, so consider their stre ngths and\nweaknesses before usage. And remember, deleting a ﬁle is just per-\nforming that one last unlink() of it from the directory hierarchy.\n•Most ﬁle systems have mechanisms to enable and disable sharin g.\nA rudimentary form of such controls are provided by permissions\nbits; more sophisticated access control lists allow for more precise\ncontrol over exactly who can access and manipulate information.\n39.18 Summary\nThe ﬁle system interface in U NIXsystems (and indeed, in any system)\nis seemingly quite rudimentary, but there is a lot to understa nd if you\nwish to master it. Nothing is better, of course, than simply usin g it (a lot).\nSo please do so! Of course, read more; as always, Stevens [SR05] is th e\nplace to begin.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nINTERLUDE : FILES AND DIRECTORIES 25\nReferences\n[BD96] “Checking for Race Conditions in File Accesses” by Matt Bishop, Michae l Dilger. Com-\nputing Systems 9:2, 1996. A great description of the TOCTTOU problem and its presence in ﬁle\nsystems.\n[CK+08] “The xv6 Operating System” by Russ Cox, Frans Kaashoek, Robe rt Morris, Nickolai\nZeldovich. From: https://github.com/mit-pdos/xv6-public. As mentioned before, a cool and\nsimple Unix implementation. We have been using an older version (2012-0 1-30-1-g1c41342) and hence\nsome examples in the book may not match the latest in the source.\n[H+18] “TxFS: Leveraging File-System Crash Consistency to Provide A CID Transactions” by\nY. Hu, Z. Zhu, I. Neal, Y. Kwon, T. Cheng, V . Chidambaram, E. Witchel. U SENIX ATC ’18, June\n2018. The best paper at USENIX ATC ’18, and a good recent place to start to learn about transactional\nﬁle systems.\n[K84] “Processes as Files” by Tom J. Killian. USENIX, June 1984. The paper that introduced the\n/proc ﬁle system, where each process can be treated as a ﬁle within a pseud o ﬁle system. A clever idea\nthat you can still see in modern UNIXsystems.\n[L84] “Capability-Based Computer Systems” by Henry M. Levy. Dig ital Press, 1984. Available:\nhttp://homes.cs.washington.edu/˜levy/capabook. An excellent overview of early capability-based\nsystems.\n[P+13] “Towards Efﬁcient, Portable Application-Level Consistency” by Thanumalayan S. Pil-\nlai, Vijay Chidambaram, Joo-Young Hwang, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-\nDusseau. HotDep ’13, November 2013. Our own work that shows how readily applications can\nmake mistakes in committing data to disk; in particular, assumptions about the ﬁle sy stem creep into\napplications and thus make the applications work correctly only if they are runnin g on a speciﬁc ﬁle\nsystem.\n[P+14] “All File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent\nApplications” by Thanumalayan S. Pillai, Vijay Chidambaram, Ramnat than Alagappan, Samer\nAl-Kiswany, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. OSDI ’14, Broom-\nﬁeld, Colorado, October 2014. The full conference paper on this topic – with many more details and\ninteresting tidbits than the ﬁrst workshop paper above.\n[SK09] “Principles of Computer System Design” by Jerome H. Saltzer and M. Frans Kaashoek.\nMorgan-Kaufmann, 2009. This tour de force of systems is a must-read for anybody interested in the\nﬁeld. It’s how they teach systems at MIT. Read it once, and then read it a few more times to let it all\nsoak in.\n[SR05] “Advanced Programming in the U NIXEnvironment” by W. Richard Stevens and Stephen\nA. Rago. Addison-Wesley, 2005. We have probably referenced this book a few hundred thousand\ntimes. It is that useful to you, if you care to become an awesome systems programm er.\n[T+08] “Portably Solving File TOCTTOU Races with Hardness Ampliﬁcatio n” by D. Tsafrir, T.\nHertz, D. Wagner, D. Da Silva. FAST ’08, San Jose, California, 200 8.Not the paper that introduced\nTOCTTOU, but a recent-ish and well-done description of the problem and a w ay to solve the problem\nin a portable manner.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n26 INTERLUDE : FILES AND DIRECTORIES\nHomework (Code)\nIn this homework, we’ll just familiarize ourselves with how the AP Is\ndescribed in the chapter work. To do so, you’ll just write a few dif ferent\nprograms, mostly based on various U NIXutilities.\nQuestions\n1.Stat: Write your own version of the command line program stat , which\nsimply calls the stat() system call on a given ﬁle or directory. Print out ﬁle\nsize, number of blocks allocated, reference (link) count, and s o forth. What\nis the link count of a directory, as the number of entries in the di rectory\nchanges? Useful interfaces: stat()\n2.List Files: Write a program that lists ﬁles in the given directory. When c alled\nwithout any arguments, the program should just print the ﬁle names. When\ninvoked with the -lﬂag, the program should print out information about\neach ﬁle, such as the owner, group, permissions, and other infor mation ob-\ntained from the stat() system call. The program should take one addi-\ntional argument, which is the directory to read, e.g., myls -l directory .\nIf no directory is given, the program should just use the current w orking di-\nrectory. Useful interfaces: stat() ,opendir() ,readdir() ,getcwd() .\n3.Tail: Write a program that prints out the last few lines of a ﬁle. The p ro-\ngram should be efﬁcient, in that it seeks to near the end of the ﬁ le, reads in\na block of data, and then goes backwards until it ﬁnds the reques ted num-\nber of lines; at this point, it should print out those lines from beginning to\nthe end of the ﬁle. To invoke the program, one should type: mytail -n\nfile , wherenis the number of lines at the end of the ﬁle to print. Useful\ninterfaces: stat() ,lseek() ,open() ,read() ,close() .\n4.Recursive Search: Write a program that prints out the names of each ﬁle\nand directory in the ﬁle system tree, starting at a given poin t in the tree. For\nexample, when run without arguments, the program should start with t he\ncurrent working directory and print its contents, as well as t he contents of\nany sub-directories, etc., until the entire tree, root at the C WD, is printed. If\ngiven a single argument (of a directory name), use that as the root of the tree\ninstead. Reﬁne your recursive search with more fun options, simil ar to the\npowerfulfind command line tool. Useful interfaces: you ﬁgure it out.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",63428
46-40. File System Implementation.pdf,46-40. File System Implementation,"40\nFile System Implementation\nIn this chapter, we introduce a simple ﬁle system implementat ion, known\nasvsfs (the Very Simple File System ). This ﬁle system is a simpliﬁed\nversion of a typical U NIX ﬁle system and thus serves to introduce some\nof the basic on-disk structures, access methods, and various poli cies that\nyou will ﬁnd in many ﬁle systems today.\nThe ﬁle system is pure software; unlike our development of CPU and\nmemory virtualization, we will not be adding hardware features to make\nsome aspect of the ﬁle system work better (though we will want to pa y at-\ntention to device characteristics to make sure the ﬁle system works well).\nBecause of the great ﬂexibility we have in building a ﬁle syste m, many\ndifferent ones have been built, literally from AFS (the Andrew File Sys-\ntem) [H+88] to ZFS (Sun’s Zettabyte File System) [B07]. All of the se ﬁle\nsystems have different data structures and do some things bet ter or worse\nthan their peers. Thus, the way we will be learning about ﬁle sy stems is\nthrough case studies: ﬁrst, a simple ﬁle system (vsfs) in this chapter to\nintroduce most concepts, and then a series of studies of real ﬁle sy stems\nto understand how they can differ in practice.\nTHECRUX: HOWTOIMPLEMENT A S IMPLE FILESYSTEM\nHow can we build a simple ﬁle system? What structures are neede d\non the disk? What do they need to track? How are they accessed?\n40.1 The Way To Think\nTo think about ﬁle systems, we usually suggest thinking about t wo\ndifferent aspects of them; if you understand both of these aspect s, you\nprobably understand how the ﬁle system basically works.\nThe ﬁrst is the data structures of the ﬁle system. In other words, what\ntypes of on-disk structures are utilized by the ﬁle system to org anize its\ndata and metadata? The ﬁrst ﬁle systems we’ll see (including v sfs below)\nemploy simple structures, like arrays of blocks or other objects, w hereas\n1\n2 FILESYSTEM IMPLEMENTATION\nASIDE : M ENTAL MODELS OFFILESYSTEMS\nAs we’ve discussed before, mental models are what you are really t rying\nto develop when learning about systems. For ﬁle systems, your men tal\nmodel should eventually include answers to questions like: wha t on-disk\nstructures store the ﬁle system’s data and metadata? What happ ens when\na process opens a ﬁle? Which on-disk structures are accessed dur ing a\nread or write? By working on and improving your mental model, you\ndevelop an abstract understanding of what is going on, instead of j ust\ntrying to understand the speciﬁcs of some ﬁle-system code (thoug h that\nis also useful, of course!).\nmore sophisticated ﬁle systems, like SGI’s XFS, use more complicate d\ntree-based structures [S+96].\nThe second aspect of a ﬁle system is its access methods . How does\nit map the calls made by a process, such as open() ,read() ,write() ,\netc., onto its structures? Which structures are read during t he execution\nof a particular system call? Which are written? How efﬁciently are all of\nthese steps performed?\nIf you understand the data structures and access methods of a ﬁle sys-\ntem, you have developed a good mental model of how it truly works, a\nkey part of the systems mindset. Try to work on developing your ment al\nmodel as we delve into our ﬁrst implementation.\n40.2 Overall Organization\nWe now develop the overall on-disk organization of the data struc-\ntures of the vsfs ﬁle system. The ﬁrst thing we’ll need to do is di vide the\ndisk into blocks ; simple ﬁle systems use just one block size, and that’s\nexactly what we’ll do here. Let’s choose a commonly-used size of 4 KB.\nThus, our view of the disk partition where we’re building our ﬁle sy s-\ntem is simple: a series of blocks, each of size 4 KB. The blocks are a d-\ndressed from 0 to N−1, in a partition of size N4-KB blocks. Assume we\nhave a really small disk, with just 64 blocks:\n0 78 1516 2324 31\n32 3940 4748 5556 63\nLet’s now think about what we need to store in these blocks to build\na ﬁle system. Of course, the ﬁrst thing that comes to mind is user data.\nIn fact, most of the space in any ﬁle system is (and should be) user data.\nLet’s call the region of the disk we use for user data the data region , and,\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFILESYSTEM IMPLEMENTATION 3\nagain for simplicity, reserve a ﬁxed portion of the disk for these b locks,\nsay the last 56 of 64 blocks on the disk:\n0 7D\n8DDDDDDD\n15D\n16DDDDDDD\n23D\n24DDDDDDD\n31\nD\n32DDDDDDD\n39D\n40DDDDDDD\n47D\n48DDDDDDD\n55D\n56DDDDDDD\n63Data Region\nData Region\nAs we learned about (a little) last chapter, the ﬁle system has to track\ninformation about each ﬁle. This information is a key piece of metadata ,\nand tracks things like which data blocks (in the data region) com prise a\nﬁle, the size of the ﬁle, its owner and access rights, access and modify\ntimes, and other similar kinds of information. To store this inform ation,\nﬁle systems usually have a structure called an inode (we’ll read more\nabout inodes below).\nTo accommodate inodes, we’ll need to reserve some space on the disk\nfor them as well. Let’s call this portion of the disk the inode table , which\nsimply holds an array of on-disk inodes. Thus, our on-disk image now\nlooks like this picture, assuming that we use 5 of our 64 blocks for in odes\n(denoted by I’s in the diagram):\n0IIIII\n7D\n8DDDDDDD\n15D\n16DDDDDDD\n23D\n24DDDDDDD\n31\nD\n32DDDDDDD\n39D\n40DDDDDDD\n47D\n48DDDDDDD\n55D\n56DDDDDDD\n63Data Region\nData RegionInodes\nWe should note here that inodes are typically not that big, for exam ple\n128 or 256 bytes. Assuming 256 bytes per inode, a 4-KB block can hol d 16\ninodes, and our ﬁle system above contains 80 total inodes. In our simp le\nﬁle system, built on a tiny 64-block partition, this number repr esents the\nmaximum number of ﬁles we can have in our ﬁle system; however, do\nnote that the same ﬁle system, built on a larger disk, could simpl y allocate\na larger inode table and thus accommodate more ﬁles.\nOur ﬁle system thus far has data blocks (D), and inodes (I), but a few\nthings are still missing. One primary component that is still n eeded, as\nyou might have guessed, is some way to track whether inodes or data\nblocks are free or allocated. Such allocation structures are thus a requisite\nelement in any ﬁle system.\nMany allocation-tracking methods are possible, of course. For exam -\nple, we could use a free list that points to the ﬁrst free block, which then\npoints to the next free block, and so forth. We instead choose a simp le and\npopular structure known as a bitmap , one for the data region (the data\nbitmap ), and one for the inode table (the inode bitmap ). A bitmap is a\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 FILESYSTEM IMPLEMENTATION\nsimple structure: each bit is used to indicate whether the cor responding\nobject/block is free (0) or in-use (1). And thus our new on-disk lay out,\nwith an inode bitmap (i) and a data bitmap (d):\n0idIIIII\n7D\n8DDDDDDD\n15D\n16DDDDDDD\n23D\n24DDDDDDD\n31\nD\n32DDDDDDD\n39D\n40DDDDDDD\n47D\n48DDDDDDD\n55D\n56DDDDDDD\n63Data Region\nData RegionInodes\nYou may notice that it is a bit of overkill to use an entire 4-KB block for\nthese bitmaps; such a bitmap can track whether 32K objects are allocated,\nand yet we only have 80 inodes and 56 data blocks. However, we just u se\nan entire 4-KB block for each of these bitmaps for simplicity.\nThe careful reader (i.e., the reader who is still awake) may h ave no-\nticed there is one block left in the design of the on-disk structur e of our\nvery simple ﬁle system. We reserve this for the superblock , denoted by\nan S in the diagram below. The superblock contains information abou t\nthis particular ﬁle system, including, for example, how many i nodes and\ndata blocks are in the ﬁle system (80 and 56, respectively in th is instance),\nwhere the inode table begins (block 3), and so forth. It will like ly also\ninclude a magic number of some kind to identify the ﬁle system ty pe (in\nthis case, vsfs).\nS\n0idIIIII\n7D\n8DDDDDDD\n15D\n16DDDDDDD\n23D\n24DDDDDDD\n31\nD\n32DDDDDDD\n39D\n40DDDDDDD\n47D\n48DDDDDDD\n55D\n56DDDDDDD\n63Data Region\nData RegionInodes\nThus, when mounting a ﬁle system, the operating system will rea d\nthe superblock ﬁrst, to initialize various parameters, and th en attach the\nvolume to the ﬁle-system tree. When ﬁles within the volume are a ccessed,\nthe system will thus know exactly where to look for the needed on-di sk\nstructures.\n40.3 File Organization: The Inode\nOne of the most important on-disk structures of a ﬁle system is the\ninode ; virtually all ﬁle systems have a structure similar to this. The name\ninode is short for index node , the historical name given to it in U NIX\n[RT74] and possibly earlier systems, used because these nodes were orig-\ninally arranged in an array, and the array indexed into when accessing a\nparticular inode.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFILESYSTEM IMPLEMENTATION 5\nASIDE : DATA STRUCTURE — T HEINODE\nThe inode is the generic name that is used in many ﬁle systems to de-\nscribe the structure that holds the metadata for a given ﬁle, su ch as its\nlength, permissions, and the location of its constituent blocks. T he name\ngoes back at least as far as U NIX (and probably further back to Multics\nif not earlier systems); it is short for index node , as the inode number is\nused to index into an array of on-disk inodes in order to ﬁnd the inod e\nof that number. As we’ll see, design of the inode is one key part of ﬁle\nsystem design. Most modern systems have some kind of structure li ke\nthis for every ﬁle they track, but perhaps call them different things (such\nas dnodes, fnodes, etc.).\nEach inode is implicitly referred to by a number (called the i-number ),\nwhich we’ve earlier called the low-level name of the ﬁle. In vsfs (and\nother simple ﬁle systems), given an i-number, you should direct ly be able\nto calculate where on the disk the corresponding inode is located. For ex-\nample, take the inode table of vsfs as above: 20-KB in size (5 4-KB blocks)\nand thus consisting of 80 inodes (assuming each inode is 256 bytes ); fur-\nther assume that the inode region starts at 12KB (i.e, the super block starts\nat 0KB, the inode bitmap is at address 4KB, the data bitmap at 8K B, and\nthus the inode table comes right after). In vsfs, we thus have th e following\nlayout for the beginning of the ﬁle system partition (in closeup vi ew):\nSuper i-bmap d-bmap\n0KB 4KB 8KB 12KB 16KB 20KB 24KB 28KB 32KBThe Inode Table (Closeup)\n0123\n4567\n891011\n1213141516171819\n20212223\n24252627\n2829303132333435\n36373839\n40414243\n4445464748495051\n52535455\n56575859\n6061626364656667\n68697071\n72737475\n76777879iblock 0 iblock 1 iblock 2 iblock 3 iblock 4\nTo read inode number 32, the ﬁle system would ﬁrst calculate the off-\nset into the inode region ( 32·sizeof(inode)or8192 ), add it to the start\naddress of the inode table on disk ( inodeStartAddr =12KB), and thus\narrive upon the correct byte address of the desired block of inodes: 20KB.\nRecall that disks are not byte addressable, but rather consist of a large\nnumber of addressable sectors, usually 512 bytes. Thus, to fet ch the block\nof inodes that contains inode 32, the ﬁle system would issue a read t o sec-\ntor20×1024\n512, or 40, to fetch the desired inode block. More generally, the\nsector address sector of the inode block can be calculated as follows:\nblk = (inumber *sizeof(inode_t)) / blockSize;\nsector = ((blk *blockSize) + inodeStartAddr) / sectorSize;\nInside each inode is virtually all of the information you need about a\nﬁle: its type (e.g., regular ﬁle, directory, etc.), its size, the number of blocks\nallocated to it, protection information (such as who owns the ﬁle, as well\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 FILESYSTEM IMPLEMENTATION\nSize Name What is this inode ﬁeld for?\n2 mode can this ﬁle be read/written/executed?\n2 uid who owns this ﬁle?\n4 size how many bytes are in this ﬁle?\n4 time what time was this ﬁle last accessed?\n4 ctime what time was this ﬁle created?\n4 mtime what time was this ﬁle last modiﬁed?\n4 dtime what time was this inode deleted?\n2 gid which group does this ﬁle belong to?\n2 links count how many hard links are there to this ﬁle?\n4 blocks how many blocks have been allocated to this ﬁle?\n4 ﬂags how should ext2 use this inode?\n4 osd1 an OS-dependent ﬁeld\n60 block a set of disk pointers (15 total)\n4 generation ﬁle version (used by NFS)\n4 ﬁle acl a new permissions model beyond mode bits\n4 dir acl called access control lists\nFigure 40.1: Simpliﬁed Ext2 Inode\nas who can access it), some time information, including when the ﬁle was\ncreated, modiﬁed, or last accessed, as well as information about w here its\ndata blocks reside on disk (e.g., pointers of some kind). We refer t o all\nsuch information about a ﬁle as metadata ; in fact, any information inside\nthe ﬁle system that isn’t pure user data is often referred to as s uch. An\nexample inode from ext2 [P09] is shown in Figure 40.11.\nOne of the most important decisions in the design of the inode is how\nit refers to where data blocks are. One simple approach would be t o\nhave one or more direct pointers (disk addresses) inside the inode; each\npointer refers to one disk block that belongs to the ﬁle. Such an app roach\nis limited: for example, if you want to have a ﬁle that is really b ig (e.g.,\nbigger than the block size multiplied by the number of direct poi nters in\nthe inode), you are out of luck.\nThe Multi-Level Index\nTo support bigger ﬁles, ﬁle system designers have had to introd uce dif-\nferent structures within inodes. One common idea is to have a spe cial\npointer known as an indirect pointer . Instead of pointing to a block that\ncontains user data, it points to a block that contains more pointers , each\nof which point to user data. Thus, an inode may have some ﬁxed numbe r\nof direct pointers (e.g., 12), and a single indirect pointer. If a ﬁle grows\nlarge enough, an indirect block is allocated (from the data-block region of\nthe disk), and the inode’s slot for an indirect pointer is set to poin t to it.\nAssuming 4-KB blocks and 4-byte disk addresses, that adds anot her 1024\npointers; the ﬁle can grow to be (12+1024) ·4Kor 4144KB.\n1Type info is kept in the directory entry, and thus is not found in the inode it self.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFILESYSTEM IMPLEMENTATION 7\nTIP: CONSIDER EXTENT -BASED APPROACHES\nA different approach is to use extents instead of pointers. An extent is\nsimply a disk pointer plus a length (in blocks); thus, instead of requiring\na pointer for every block of a ﬁle, all one needs is a pointer and a leng th\nto specify the on-disk location of a ﬁle. Just a single extent is li miting, as\none may have trouble ﬁnding a contiguous chunk of on-disk free space\nwhen allocating a ﬁle. Thus, extent-based ﬁle systems often al low for\nmore than one extent, thus giving more freedom to the ﬁle system du ring\nﬁle allocation.\nIn comparing the two approaches, pointer-based approaches are t he most\nﬂexible but use a large amount of metadata per ﬁle (particularl y for large\nﬁles). Extent-based approaches are less ﬂexible but more compa ct; in par-\nticular, they work well when there is enough free space on the dis k and\nﬁles can be laid out contiguously (which is the goal for virtually a ny ﬁle\nallocation policy anyhow).\nNot surprisingly, in such an approach, you might want to support\neven larger ﬁles. To do so, just add another pointer to the inode: t hedou-\nble indirect pointer . This pointer refers to a block that contains pointers\nto indirect blocks, each of which contain pointers to data blocks. A dou-\nble indirect block thus adds the possibility to grow ﬁles with an additional\n1024·1024 or 1-million 4KB blocks, in other words supporting ﬁles that\nare over 4GB in size. You may want even more, though, and we bet you\nknow where this is headed: the triple indirect pointer .\nOverall, this imbalanced tree is referred to as the multi-level index ap-\nproach to pointing to ﬁle blocks. Let’s examine an example with tw elve\ndirect pointers, as well as both a single and a double indirect bl ock. As-\nsuming a block size of 4 KB, and 4-byte pointers, this structure c an accom-\nmodate a ﬁle of just over 4 GB in size (i.e., (12+1024+10242)×4KB).\nCan you ﬁgure out how big of a ﬁle can be handled with the addition of\na triple-indirect block? (hint: pretty big)\nMany ﬁle systems use a multi-level index, including commonly- used\nﬁle systems such as Linux ext2 [P09] and ext3, NetApp’s WAFL, a s well as\nthe original U NIXﬁle system. Other ﬁle systems, including SGI XFS and\nLinux ext4, use extents instead of simple pointers; see the earlier aside for\ndetails on how extent-based schemes work (they are akin to segme nts in\nthe discussion of virtual memory).\nYou might be wondering: why use an imbalanced tree like this? Wh y\nnot a different approach? Well, as it turns out, many researcher s have\nstudied ﬁle systems and how they are used, and virtually every time they\nﬁnd certain “truths” that hold across the decades. One such ﬁnd ing is\nthat most ﬁles are small . This imbalanced design reﬂects such a reality; if\nmost ﬁles are indeed small, it makes sense to optimize for this ca se. Thus,\nwith a small number of direct pointers (12 is a typical number), an inode\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 FILESYSTEM IMPLEMENTATION\nMost ﬁles are small ˜2K is the most common size\nAverage ﬁle size is growing Almost 200K is the average\nMost bytes are stored in large ﬁles A few big ﬁles use most of space\nFile systems contains lots of ﬁles Almost 100K on average\nFile systems are roughly half full Even as disks grow, ﬁle systems\nremain ˜50% full\nDirectories are typically small Many have few entries; most\nhave 20 or fewer\nFigure 40.2: File System Measurement Summary\ncan directly point to 48 KB of data, needing one (or more) indirect b locks\nfor larger ﬁles. See Agrawal et. al [A+07] for a recent-ish study ; Figure\n40.2 summarizes those results.\nOf course, in the space of inode design, many other possibilities e x-\nist; after all, the inode is just a data structure, and any data structure that\nstores the relevant information, and can query it effectively, is sufﬁcient.\nAs ﬁle system software is readily changed, you should be willing to ex-\nplore different designs should workloads or technologies change.\n40.4 Directory Organization\nIn vsfs (as in many ﬁle systems), directories have a simple orga niza-\ntion; a directory basically just contains a list of (entry name, i node num-\nber) pairs. For each ﬁle or directory in a given directory, there i s a string\nand a number in the data block(s) of the directory. For each string , there\nmay also be a length (assuming variable-sized names).\nFor example, assume a directory dir (inode number 5) has three ﬁles\nin it (foo,bar, andfoobarisaprettylongname ), with inode num-\nbers 12, 13, and 24 respectively. The on-disk data for dir might look like:\ninum | reclen | strlen | name\n5 12 2 .\n2 12 3 ..\n12 12 4 foo\n13 12 4 bar\n24 36 28 foobar_is_a_pretty_longname\nIn this example, each entry has an inode number, record length ( the\ntotal bytes for the name plus any left over space), string length (the actual\nlength of the name), and ﬁnally the name of the entry. Note that ea ch di-\nrectory has two extra entries, .“dot” and ..“dot-dot”; the dot directory\nis just the current directory (in this example, dir), whereas dot-dot is the\nparent directory (in this case, the root).\nDeleting a ﬁle (e.g., calling unlink() ) can leave an empty space in\nthe middle of the directory, and hence there should be some way to m ark\nthat as well (e.g., with a reserved inode number such as zero). Su ch a\ndelete is one reason the record length is used: a new entry may reu se an\nold, bigger entry and thus have extra space within.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFILESYSTEM IMPLEMENTATION 9\nASIDE : LINKED -BASED APPROACHES\nAnother simpler approach in designing inodes is to use a linked list .\nThus, inside an inode, instead of having multiple pointers, you j ust need\none, to point to the ﬁrst block of the ﬁle. To handle larger ﬁles, ad d an-\nother pointer at the end of that data block, and so on, and thus you can\nsupport large ﬁles.\nAs you might have guessed, linked ﬁle allocation performs poorly for\nsome workloads; think about reading the last block of a ﬁle, for examp le,\nor just doing random access. Thus, to make linked allocation work be tter,\nsome systems will keep an in-memory table of link information, ins tead\nof storing the next pointers with the data blocks themselves. The table\nis indexed by the address of a data block D; the content of an entry is\nsimplyD’s next pointer, i.e., the address of the next block in a ﬁle which\nfollowsD. A null-value could be there too (indicating an end-of-ﬁle), or\nsome other marker to indicate that a particular block is free. Ha ving such\na table of next pointers makes it so that a linked allocation schem e can\neffectively do random ﬁle accesses, simply by ﬁrst scanning t hrough the\n(in memory) table to ﬁnd the desired block, and then accessing ( on disk)\nit directly.\nDoes such a table sound familiar? What we have described is the b asic\nstructure of what is known as the ﬁle allocation table , orFAT ﬁle system.\nYes, this classic old Windows ﬁle system, before NTFS [C94], is b ased on a\nsimple linked-based allocation scheme. There are other differ ences from\na standard U NIXﬁle system too; for example, there are no inodes per se,\nbut rather directory entries which store metadata about a ﬁle an d refer\ndirectly to the ﬁrst block of said ﬁle, which makes creating har d links\nimpossible. See Brouwer [B02] for more of the inelegant details.\nYou might be wondering where exactly directories are stored. Oft en,\nﬁle systems treat directories as a special type of ﬁle. Thus, a d irectory has\nan inode, somewhere in the inode table (with the type ﬁeld of the in ode\nmarked as “directory” instead of “regular ﬁle”). The directory has data\nblocks pointed to by the inode (and perhaps, indirect blocks); th ese data\nblocks live in the data block region of our simple ﬁle system. Our on- disk\nstructure thus remains unchanged.\nWe should also note again that this simple linear list of director y en-\ntries is not the only way to store such information. As before, any da ta\nstructure is possible. For example, XFS [S+96] stores directorie s in B-tree\nform, making ﬁle create operations (which have to ensure that a ﬁ le name\nhas not been used before creating it) faster than systems with s imple lists\nthat must be scanned in their entirety.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 FILESYSTEM IMPLEMENTATION\nASIDE : FREE SPACE MANAGEMENT\nThere are many ways to manage free space; bitmaps are just one w ay.\nSome early ﬁle systems used free lists , where a single pointer in the super\nblock was kept to point to the ﬁrst free block; inside that block th e next\nfree pointer was kept, thus forming a list through the free blocks of the\nsystem. When a block was needed, the head block was used and the l ist\nupdated accordingly.\nModern ﬁle systems use more sophisticated data structures. For e xample,\nSGI’s XFS [S+96] uses some form of a B-tree to compactly represent which\nchunks of the disk are free. As with any data structure, differ ent time-\nspace trade-offs are possible.\n40.5 Free Space Management\nA ﬁle system must track which inodes and data blocks are free, an d\nwhich are not, so that when a new ﬁle or directory is allocated, it c an ﬁnd\nspace for it. Thus free space management is important for all ﬁle systems.\nIn vsfs, we have two simple bitmaps for this task.\nFor example, when we create a ﬁle, we will have to allocate an inod e\nfor that ﬁle. The ﬁle system will thus search through the bitmap for an in-\node that is free, and allocate it to the ﬁle; the ﬁle system will h ave to mark\nthe inode as used (with a 1) and eventually update the on-disk bi tmap\nwith the correct information. A similar set of activities take pl ace when a\ndata block is allocated.\nSome other considerations might also come into play when allocating\ndata blocks for a new ﬁle. For example, some Linux ﬁle systems, suc h\nas ext2 and ext3, will look for a sequence of blocks (say 8) that are f ree\nwhen a new ﬁle is created and needs data blocks; by ﬁnding such a se-\nquence of free blocks, and then allocating them to the newly-cre ated ﬁle,\nthe ﬁle system guarantees that a portion of the ﬁle will be contigu ous on\nthe disk, thus improving performance. Such a pre-allocation policy is\nthus a commonly-used heuristic when allocating space for data bl ocks.\n40.6 Access Paths: Reading and Writing\nNow that we have some idea of how ﬁles and directories are stored on\ndisk, we should be able to follow the ﬂow of operation during the activ ity\nof reading or writing a ﬁle. Understanding what happens on this access\npath is thus the second key in developing an understanding of how a ﬁle\nsystem works; pay attention!\nFor the following examples, let us assume that the ﬁle system has been\nmounted and thus that the superblock is already in memory. Every thing\nelse (i.e., inodes, directories) is still on the disk.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFILESYSTEM IMPLEMENTATION 11\ndata inode root foo bar root foo bar bar bar\nbitmap bitmap inode inode inode data data data data data\n[0] [1] [2]\nread\nread\nopen(bar) read\nread\nread\nread\nread() read\nwrite\nread\nread() read\nwrite\nread\nread() read\nwrite\nFigure 40.3: File Read Timeline (Time Increasing Downward)\nReading A File From Disk\nIn this simple example, let us ﬁrst assume that you want to simp ly open\na ﬁle (e.g., /foo/bar ), read it, and then close it. For this simple example,\nlet’s assume the ﬁle is just 12KB in size (i.e., 3 blocks).\nWhen you issue an open(""/foo/bar"", O RDONLY) call, the ﬁle sys-\ntem ﬁrst needs to ﬁnd the inode for the ﬁle bar, to obtain some basic in-\nformation about the ﬁle (permissions information, ﬁle size, etc.) . To do so,\nthe ﬁle system must be able to ﬁnd the inode, but all it has right now is\nthe full pathname. The ﬁle system must traverse the pathname and thus\nlocate the desired inode.\nAll traversals begin at the root of the ﬁle system, in the root directory\nwhich is simply called /. Thus, the ﬁrst thing the FS will read from disk\nis the inode of the root directory. But where is this inode? To ﬁnd an\ninode, we must know its i-number. Usually, we ﬁnd the i-number of a ﬁle\nor directory in its parent directory; the root has no parent (by deﬁ nition).\nThus, the root inode number must be “well known”; the FS must know\nwhat it is when the ﬁle system is mounted. In most U NIX ﬁle systems,\nthe root inode number is 2. Thus, to begin the process, the FS reads in the\nblock that contains inode number 2 (the ﬁrst inode block).\nOnce the inode is read in, the FS can look inside of it to ﬁnd pointers to\ndata blocks, which contain the contents of the root directory. The FS will\nthus use these on-disk pointers to read through the directory, in this case\nlooking for an entry for foo. By reading in one or more directory data\nblocks, it will ﬁnd the entry for foo; once found, the FS will also hav e\nfound the inode number of foo (say it is 44) which it will need next.\nThe next step is to recursively traverse the pathname until t he desired\ninode is found. In this example, the FS reads the block containing the\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 FILESYSTEM IMPLEMENTATION\nASIDE : READS DON’TACCESS ALLOCATION STRUCTURES\nWe’ve seen many students get confused by allocation structures s uch as\nbitmaps. In particular, many often think that when you are simp ly read-\ning a ﬁle, and not allocating any new blocks, that the bitmap will still\nbe consulted. This is not true! Allocation structures, such as bi tmaps,\nare only accessed when allocation is needed. The inodes, director ies, and\nindirect blocks have all the information they need to complete a r ead re-\nquest; there is no need to make sure a block is allocated when the inode\nalready points to it.\ninode offoo and then its directory data, ﬁnally ﬁnding the inode number\nofbar. The ﬁnal step of open() is to readbar’s inode into memory; the\nFS then does a ﬁnal permissions check, allocates a ﬁle descriptor for this\nprocess in the per-process open-ﬁle table, and returns it to the user.\nOnce open, the program can then issue a read() system call to read\nfrom the ﬁle. The ﬁrst read (at offset 0 unless lseek() has been called)\nwill thus read in the ﬁrst block of the ﬁle, consulting the inode to ﬁnd\nthe location of such a block; it may also update the inode with a new l ast-\naccessed time. The read will further update the in-memory open ﬁle table\nfor this ﬁle descriptor, updating the ﬁle offset such that the ne xt read will\nread the second ﬁle block, etc.\nAt some point, the ﬁle will be closed. There is much less work to be\ndone here; clearly, the ﬁle descriptor should be deallocated, bu t for now,\nthat is all the FS really needs to do. No disk I/Os take place.\nA depiction of this entire process is found in Figure 40.3 (page 11 );\ntime increases downward in the ﬁgure. In the ﬁgure, the open cau ses\nnumerous reads to take place in order to ﬁnally locate the inode of t he ﬁle.\nAfterwards, reading each block requires the ﬁle system to ﬁrs t consult the\ninode, then read the block, and then update the inode’s last-acce ssed-time\nﬁeld with a write. Spend some time and understand what is going on.\nAlso note that the amount of I/O generated by the open is propor-\ntional to the length of the pathname. For each additional director y in the\npath, we have to read its inode as well as its data. Making this w orse\nwould be the presence of large directories; here, we only have to r ead one\nblock to get the contents of a directory, whereas with a large dire ctory, we\nmight have to read many data blocks to ﬁnd the desired entry. Ye s, life\ncan get pretty bad when reading a ﬁle; as you’re about to ﬁnd out, wr iting\nout a ﬁle (and especially, creating a new one) is even worse.\nWriting A File To Disk\nWriting to a ﬁle is a similar process. First, the ﬁle must be open ed (as\nabove). Then, the application can issue write() calls to update the ﬁle\nwith new contents. Finally, the ﬁle is closed.\nUnlike reading, writing to the ﬁle may also allocate a block (unless\nthe block is being overwritten, for example). When writing out a n ew\nﬁle, each write not only has to write data to disk but has to ﬁrst d ecide\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFILESYSTEM IMPLEMENTATION 13\ndata inode root foo bar root foo bar bar bar\nbitmap bitmap inode inode inode data data data data data\n[0] [1] [2]\nread\nread\nread\nread\ncreate read\n(/foo/bar) write\nwrite\nread\nwrite\nwrite\nread\nread\nwrite() write\nwrite\nwrite\nread\nread\nwrite() write\nwrite\nwrite\nread\nread\nwrite() write\nwrite\nwrite\nFigure 40.4: File Creation Timeline (Time Increasing Downward)\nwhich block to allocate to the ﬁle and thus update other structur es of the\ndisk accordingly (e.g., the data bitmap and inode). Thus, each write to a\nﬁle logically generates ﬁve I/Os: one to read the data bitmap (w hich is\nthen updated to mark the newly-allocated block as used), one to w rite the\nbitmap (to reﬂect its new state to disk), two more to read and th en write\nthe inode (which is updated with the new block’s location), and ﬁna lly\none to write the actual block itself.\nThe amount of write trafﬁc is even worse when one considers a sim-\nple and common operation such as ﬁle creation. To create a ﬁle, the ﬁ le\nsystem must not only allocate an inode, but also allocate space wit hin\nthe directory containing the new ﬁle. The total amount of I/O trafﬁ c to\ndo so is quite high: one read to the inode bitmap (to ﬁnd a free inod e),\none write to the inode bitmap (to mark it allocated), one write to t he new\ninode itself (to initialize it), one to the data of the directory ( to link the\nhigh-level name of the ﬁle to its inode number), and one read and w rite\nto the directory inode to update it. If the directory needs to grow to ac-\ncommodate the new entry, additional I/Os (i.e., to the data bitm ap, and\nthe new directory block) will be needed too. All that just to creat e a ﬁle!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 FILESYSTEM IMPLEMENTATION\nLet’s look at a speciﬁc example, where the ﬁle /foo/bar is created,\nand three blocks are written to it. Figure 40.4 (page 13) shows w hat hap-\npens during the open() (which creates the ﬁle) and during each of three\n4KB writes.\nIn the ﬁgure, reads and writes to the disk are grouped under whi ch\nsystem call caused them to occur, and the rough ordering they mig ht take\nplace in goes from top to bottom of the ﬁgure. You can see how much\nwork it is to create the ﬁle: 10 I/Os in this case, to walk the pat hname\nand then ﬁnally create the ﬁle. You can also see that each alloca ting write\ncosts 5 I/Os: a pair to read and update the inode, another pair to r ead\nand update the data bitmap, and then ﬁnally the write of the dat a itself.\nHow can a ﬁle system accomplish any of this with reasonable efﬁcie ncy?\nTHECRUX: HOWTOREDUCE FILESYSTEM I/O C OSTS\nEven the simplest of operations like opening, reading, or writing a ﬁle\nincurs a huge number of I/O operations, scattered over the disk. W hat\ncan a ﬁle system do to reduce the high costs of doing so many I/Os?\n40.7 Caching and Buffering\nAs the examples above show, reading and writing ﬁles can be expe n-\nsive, incurring many I/Os to the (slow) disk. To remedy what wou ld\nclearly be a huge performance problem, most ﬁle systems aggress ively\nuse system memory (DRAM) to cache important blocks.\nImagine the open example above: without caching, every ﬁle open\nwould require at least two reads for every level in the directory hierarchy\n(one to read the inode of the directory in question, and at least one t o read\nits data). With a long pathname (e.g., /1/2/3/ ... /100/ﬁle.t xt), the ﬁle\nsystem would literally perform hundreds of reads just to open the ﬁle!\nEarly ﬁle systems thus introduced a ﬁxed-size cache to hold popular\nblocks. As in our discussion of virtual memory, strategies such as LRU\nand different variants would decide which blocks to keep in cac he. This\nﬁxed-size cache would usually be allocated at boot time to be rough ly\n10% of total memory.\nThis static partitioning of memory, however, can be wasteful; what\nif the ﬁle system doesn’t need 10% of memory at a given point in time?\nWith the ﬁxed-size approach described above, unused pages in t he ﬁle\ncache cannot be re-purposed for some other use, and thus go to waste .\nModern systems, in contrast, employ a dynamic partitioning approach.\nSpeciﬁcally, many modern operating systems integrate virtual memory\npages and ﬁle system pages into a uniﬁed page cache [S00]. In this way,\nmemory can be allocated more ﬂexibly across virtual memory and ﬁle\nsystem, depending on which needs more memory at a given time.\nNow imagine the ﬁle open example with caching. The ﬁrst open may\ngenerate a lot of I/O trafﬁc to read in directory inode and data, bu t sub-\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFILESYSTEM IMPLEMENTATION 15\nTIP: UNDERSTAND STATIC VS. DYNAMIC PARTITIONING\nWhen dividing a resource among different clients/users, you ca n use\neither static partitioning ordynamic partitioning . The static approach\nsimply divides the resource into ﬁxed proportions once; for exampl e, if\nthere are two possible users of memory, you can give some ﬁxed fract ion\nof memory to one user, and the rest to the other. The dynamic approac h\nis more ﬂexible, giving out differing amounts of the resource over t ime;\nfor example, one user may get a higher percentage of disk bandwid th for\na period of time, but then later, the system may switch and decid e to give\na different user a larger fraction of available disk bandwidth .\nEach approach has its advantages. Static partitioning ensures each user\nreceives some share of the resource, usually delivers more predi ctable\nperformance, and is often easier to implement. Dynamic partit ioning can\nachieve better utilization (by letting resource-hungry user s consume oth-\nerwise idle resources), but can be more complex to implement, an d can\nlead to worse performance for users whose idle resources get consum ed\nby others and then take a long time to reclaim when needed. As is of -\nten the case, there is no best method; rather, you should think ab out the\nproblem at hand and decide which approach is most suitable. Inde ed,\nshouldn’t you always be doing that?\nsequent ﬁle opens of that same ﬁle (or ﬁles in the same directory) w ill\nmostly hit in the cache and thus no I/O is needed.\nLet us also consider the effect of caching on writes. Whereas rea d I/O\ncan be avoided altogether with a sufﬁciently large cache, writ e trafﬁc has\nto go to disk in order to become persistent. Thus, a cache does not s erve\nas the same kind of ﬁlter on write trafﬁc that it does for reads. Tha t said,\nwrite buffering (as it is sometimes called) certainly has a number of per-\nformance beneﬁts. First, by delaying writes, the ﬁle system c anbatch\nsome updates into a smaller set of I/Os; for example, if an inode bi tmap\nis updated when one ﬁle is created and then updated moments late r as\nanother ﬁle is created, the ﬁle system saves an I/O by delaying the write\nafter the ﬁrst update. Second, by buffering a number of writes in memory,\nthe system can then schedule the subsequent I/Os and thus increase per-\nformance. Finally, some writes are avoided altogether by delayi ng them;\nfor example, if an application creates a ﬁle and then deletes it , delaying\nthe writes to reﬂect the ﬁle creation to disk avoids them entirely. In this\ncase, laziness (in writing blocks to disk) is a virtue.\nFor the reasons above, most modern ﬁle systems buffer writes in mem -\nory for anywhere between ﬁve and thirty seconds, representing y et an-\nother trade-off: if the system crashes before the updates have b een prop-\nagated to disk, the updates are lost; however, by keeping write s in mem-\nory longer, performance can be improved by batching, scheduling , and\neven avoiding writes.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 FILESYSTEM IMPLEMENTATION\nTIP: UNDERSTAND THEDURABILITY /PERFORMANCE TRADE -OFF\nStorage systems often present a durability/performance trade- off to\nusers. If the user wishes data that is written to be immediate ly durable,\nthe system must go through the full effort of committing the newly -\nwritten data to disk, and thus the write is slow (but safe). Howe ver, if\nthe user can tolerate the loss of a little data, the system can buf fer writes\nin memory for some time and write them later to the disk (in the bac k-\nground). Doing so makes writes appear to complete quickly, thus im-\nproving perceived performance; however, if a crash occurs, writ es not\nyet committed to disk will be lost, and hence the trade-off. To un derstand\nhow to make this trade-off properly, it is best to understand wha t the ap-\nplication using the storage system requires; for example, whil e it may be\ntolerable to lose the last few images downloaded by your web browser ,\nlosing part of a database transaction that is adding money to your b ank\naccount may be less tolerable. Unless you’re rich, of course; in tha t case,\nwhy do you care so much about hoarding every last penny?\nSome applications (such as databases) don’t enjoy this trade-off. T hus,\nto avoid unexpected data loss due to write buffering, they simp ly force\nwrites to disk, by calling fsync() , by using direct I/O interfaces that\nwork around the cache, or by using the raw disk interface and avoiding\nthe ﬁle system altogether2. While most applications live with the trade-\noffs made by the ﬁle system, there are enough controls in place to g et the\nsystem to do what you want it to, should the default not be satisfyi ng.\n40.8 Summary\nWe have seen the basic machinery required in building a ﬁle sy stem.\nThere needs to be some information about each ﬁle (metadata), usu ally\nstored in a structure called an inode. Directories are just a spe ciﬁc type\nof ﬁle that store name →inode-number mappings. And other structures\nare needed too; for example, ﬁle systems often use a structure suc h as a\nbitmap to track which inodes or data blocks are free or allocated.\nThe terriﬁc aspect of ﬁle system design is its freedom; the ﬁle s ystems\nwe explore in the coming chapters each take advantage of this fre edom\nto optimize some aspect of the ﬁle system. There are also clearly many\npolicy decisions we have left unexplored. For example, when a new ﬁle\nis created, where should it be placed on disk? This policy and othe rs will\nalso be the subject of future chapters. Or will they?3\n2Take a database class to learn more about old-school databases and their former insis-\ntence on avoiding the OS and controlling everything themselves. But watch o ut! Those\ndatabase types are always trying to bad mouth the OS. Shame on you, database people. Shame.\n3Cue mysterious music that gets you even more intrigued about the topic of ﬁle systems.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFILESYSTEM IMPLEMENTATION 17\nReferences\n[A+07] “A Five-Year Study of File-System Metadata” by Nitin Agra wal, William J. Bolosky,\nJohn R. Douceur, Jacob R. Lorch. FAST ’07, San Jose, California, Februar y 2007. An excellent\nrecent analysis of how ﬁle systems are actually used. Use the bibliography within to follow the trail of\nﬁle-system analysis papers back to the early 1980s.\n[B07] “ZFS: The Last Word in File Systems” by Jeff Bonwick and Bill Moor e. Available from:\nhttp://www.ostep.org/Citations/zfs_last.pdf .One of the most recent important ﬁle\nsystems, full of features and awesomeness. We should have a chapter on it, and perhaps soon will.\n[B02] “The FAT File System” by Andries Brouwer. September, 2002. Available online at:\nhttp://www.win.tue.nl/˜aeb/linux/fs/fat/fat.html .A nice clean description of\nFAT. The ﬁle system kind, not the bacon kind. Though you have to admit, bacon fat p robably tastes\nbetter.\n[C94] “Inside the Windows NT File System” by Helen Custer. Microsoft Press, 1994. A short\nbook about NTFS; there are probably ones with more technical details elsewher e.\n[H+88] “Scale and Performance in a Distributed File System” by John H. H oward, Michael\nL. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Rober t N. Sidebotham,\nMichael J. West.. ACM TOCS, Volume 6:1, February 1988. A classic distributed ﬁle system; we’ll\nbe learning more about it later, don’t worry.\n[P09] “The Second Extended File System: Internal Layout” by Dave Poirie r. 2009.Available:\nhttp://www.nongnu.org/ext2-doc/ext2.html .Some details on ext2, a very simple Linux\nﬁle system based on FFS, the Berkeley Fast File System. We’ll be readin g about it in the next chapter.\n[RT74] “The U NIX Time-Sharing System” by M. Ritchie, K. Thompson. CACM Volume 17:7,\n1974. The original paper about UNIX. Read it to see the underpinnings of much of modern operating\nsystems.\n[S00] “UBC: An Efﬁcient Uniﬁed I/O and Memory Caching Subsystem for NetB SD” by Chuck\nSilvers. FREENIX, 2000. A nice paper about NetBSD’s integration of ﬁle-system buffer caching and\nthe virtual-memory page cache. Many other systems do the same type of thing.\n[S+96] “Scalability in the XFS File System” by Adan Sweeney, Doug Doucette, Wei Hu, Curtis\nAnderson, Mike Nishimoto, Geoff Peck. USENIX ’96, January 1996, San D iego, California.\nThe ﬁrst attempt to make scalability of operations, including things like havin g millions of ﬁles in a\ndirectory, a central focus. A great example of pushing an idea to the extrem e. The key idea behind this\nﬁle system: everything is a tree. We should have a chapter on this ﬁle sys tem too.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n18 FILESYSTEM IMPLEMENTATION\nHomework (Simulation)\nUse this tool, vsfs.py , to study how ﬁle system state changes as var-\nious operations take place. The ﬁle system begins in an empty sta te, with\njust a root directory. As the simulation takes place, various opera tions are\nperformed, thus slowly changing the on-disk state of the ﬁle syst em. See\nthe README for details.\nQuestions\n1. Run the simulator with some different random seeds (say 17, 18 , 19,\n20), and see if you can ﬁgure out which operations must have taken\nplace between each state change.\n2. Now do the same, using different random seeds (say 21, 22, 23,\n24), except run with the -rﬂag, thus making you guess the state\nchange while being shown the operation. What can you conclude\nabout the inode and data-block allocation algorithms, in terms of\nwhich blocks they prefer to allocate?\n3. Now reduce the number of data blocks in the ﬁle system, to very\nlow numbers (say two), and run the simulator for a hundred or so\nrequests. What types of ﬁles end up in the ﬁle system in this hig hly-\nconstrained layout? What types of operations would fail?\n4. Now do the same, but with inodes. With very few inodes, what\ntypes of operations can succeed? Which will usually fail? What i s\nthe ﬁnal state of the ﬁle system likely to be?\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG",45826
47-41. Fast File System FFS.pdf,47-41. Fast File System FFS,"41\nLocality and The Fast File System\nWhen the U NIXoperating system was ﬁrst introduced, the U NIXwizard\nhimself Ken Thompson wrote the ﬁrst ﬁle system. Let’s call that th e “old\nUNIXﬁle system”, and it was really simple. Basically, its data st ructures\nlooked like this on the disk:\nS Inodes Data\nThe super block (S) contained information about the entire ﬁle syst em:\nhow big the volume is, how many inodes there are, a pointer to the hea d\nof a free list of blocks, and so forth. The inode region of the disk conta ined\nall the inodes for the ﬁle system. Finally, most of the disk was tak en up\nby data blocks.\nThe good thing about the old ﬁle system was that it was simple, and\nsupported the basic abstractions the ﬁle system was trying to d eliver: ﬁles\nand the directory hierarchy. This easy-to-use system was a rea l step for-\nward from the clumsy, record-based storage systems of the past, a nd the\ndirectory hierarchy was a true advance over simpler, one-level hierarchies\nprovided by earlier systems.\n41.1 The Problem: Poor Performance\nThe problem: performance was terrible. As measured by Kirk McK u-\nsick and his colleagues at Berkeley [MJLF84], performance sta rted off bad\nand got worse over time, to the point where the ﬁle system was deliv ering\nonly 2% of overall disk bandwidth!\nThe main issue was that the old U NIXﬁle system treated the disk like it\nwas a random-access memory; data was spread all over the place wi thout\nregard to the fact that the medium holding the data was a disk, a nd thus\nhad real and expensive positioning costs. For example, the data b locks of\na ﬁle were often very far away from its inode, thus inducing an exp ensive\nseek whenever one ﬁrst read the inode and then the data blocks of a ﬁ le\n(a pretty common operation).\n1\n2 LOCALITY AND THEFAST FILESYSTEM\nWorse, the ﬁle system would end up getting quite fragmented , as the\nfree space was not carefully managed. The free list would end up point-\ning to a bunch of blocks spread across the disk, and as ﬁles got alloc ated,\nthey would simply take the next free block. The result was that a logi-\ncally contiguous ﬁle would be accessed by going back and forth acros s\nthe disk, thus reducing performance dramatically.\nFor example, imagine the following data block region, which contai ns\nfour ﬁles (A, B, C, and D), each of size 2 blocks:\nA1 A2 B1 B2 C1 C2 D1 D2\nIf B and D are deleted, the resulting layout is:\nA1 A2 C1 C2\nAs you can see, the free space is fragmented into two chunks of tw o\nblocks, instead of one nice contiguous chunk of four. Let’s say you now\nwish to allocate a ﬁle E, of size four blocks:\nA1 A2 E1 E2 C1 C2 E3 E4\nYou can see what happens: E gets spread across the disk, and as a\nresult, when accessing E, you don’t get peak (sequential) perfor mance\nfrom the disk. Rather, you ﬁrst read E1 and E2, then seek, then re ad E3\nand E4. This fragmentation problem happened all the time in the old\nUNIX ﬁle system, and it hurt performance. A side note: this problem is\nexactly what disk defragmentation tools help with; they reorganize on-\ndisk data to place ﬁles contiguously and make free space for one or a few\ncontiguous regions, moving data around and then rewriting inodes a nd\nsuch to reﬂect the changes.\nOne other problem: the original block size was too small (512 bytes ).\nThus, transferring data from the disk was inherently inefﬁci ent. Smaller\nblocks were good because they minimized internal fragmentation (waste\nwithin the block), but bad for transfer as each block might requi re a posi-\ntioning overhead to reach it. Thus, the problem:\nTHECRUX:\nHOWTOORGANIZE ON-DISK DATA TOIMPROVE PERFORMANCE\nHow can we organize ﬁle system data structures so as to improve pe r-\nformance? What types of allocation policies do we need on top of those\ndata structures? How do we make the ﬁle system “disk aware”?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCALITY AND THEFAST FILESYSTEM 3\n41.2 FFS: Disk Awareness Is The Solution\nA group at Berkeley decided to build a better, faster ﬁle syste m, which\nthey cleverly called the Fast File System (FFS) . The idea was to design\nthe ﬁle system structures and allocation policies to be “disk aw are” and\nthus improve performance, which is exactly what they did. FFS t hus ush-\nered in a new era of ﬁle system research; by keeping the same interface\nto the ﬁle system (the same APIs, including open() ,read() ,write() ,\nclose() , and other ﬁle system calls) but changing the internal implemen-\ntation , the authors paved the path for new ﬁle system construction, work\nthat continues today. Virtually all modern ﬁle systems adhere t o the ex-\nisting interface (and thus preserve compatibility with appl ications) while\nchanging their internals for performance, reliability, or othe r reasons.\n41.3 Organizing Structure: The Cylinder Group\nThe ﬁrst step was to change the on-disk structures. FFS divide s the\ndisk into a number of cylinder groups . A single cylinder is a set of tracks\non different surfaces of a hard drive that are the same distance from the\ncenter of the drive; it is called a cylinder because of its clear resemblance\nto the so-called geometrical shape. FFS aggregates Nconsecutive cylin-\nders into a group, and thus the entire disk can thus be viewed as a collec-\ntion of cylinder groups. Here is a simple example, showing the four outer\nmost tracks of a drive with six platters, and a cylinder group tha t consists\nof three cylinders:\nSingle track (e.g., dark gray)Cylinder:\nTracks at same distance from center\nof drive across different surfaces\n(all tracks with same color)\nCylinder Group:\nSet of N consecutive cylinders\n(if N=3, first group does not include black track)\nNote that modern drives do not export enough information for the\nﬁle system to truly understand whether a particular cylinde r is in use;\nas discussed previously [AD14a], disks export a logical addres s space of\nblocks and hide details of their geometry from clients. Thus, mode rn ﬁle\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 LOCALITY AND THEFAST FILESYSTEM\nsystems (such as Linux ext2, ext3, and ext4) instead organize the drive\ninto block groups , each of which is just a consecutive portion of the disk’s\naddress space. The picture below illustrates an example wher e every 8\nblocks are organized into a different block group (note that real g roups\nwould consist of many more blocks):\nGroup 0 Group 1 Group 2\nWhether you call them cylinder groups or block groups, these groups\nare the central mechanism that FFS uses to improve performance . Crit-\nically, by placing two ﬁles within the same group, FFS can ensu re that\naccessing one after the other will not result in long seeks across t he disk.\nTo use these groups to store ﬁles and directories, FFS needs to ha ve the\nability to place ﬁles and directories into a group, and track al l necessary\ninformation about them therein. To do so, FFS includes all the str uctures\nyou might expect a ﬁle system to have within each group, e.g., sp ace for\ninodes, data blocks, and some structures to track whether each of those\nare allocated or free. Here is a depiction of what FFS keeps within a single\ncylinder group:\nSib db Inodes Data\nLet’s now examine the components of this single cylinder group in\nmore detail. FFS keeps a copy of the super block (S) in each group for\nreliability reasons. The super block is needed to mount the ﬁle s ystem;\nby keeping multiple copies, if one copy becomes corrupt, you can sti ll\nmount and access the ﬁle system by using a working replica.\nWithin each group, FFS needs to track whether the inodes and dat a\nblocks of the group are allocated. A per-group inode bitmap (ib) and\ndata bitmap (db) serve this role for inodes and data blocks in each group.\nBitmaps are an excellent way to manage free space in a ﬁle syst em be-\ncause it is easy to ﬁnd a large chunk of free space and allocate it to a ﬁle,\nperhaps avoiding some of the fragmentation problems of the free lis t in\nthe old ﬁle system.\nFinally, the inode and data block regions are just like those in the pre-\nvious very-simple ﬁle system (VSFS). Most of each cylinder group, a s\nusual, is comprised of data blocks.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCALITY AND THEFAST FILESYSTEM 5\nASIDE : FFS F ILECREATION\nAs an example, think about what data structures must be update d when\na ﬁle is created; assume, for this example, that the user creat es a new ﬁle\n/foo/bar.txt and that the ﬁle is one block long (4KB). The ﬁle is new,\nand thus needs a new inode; thus, both the inode bitmap and the new ly-\nallocated inode will be written to disk. The ﬁle also has data in it and\nthus it too must be allocated; the data bitmap and a data block wil l thus\n(eventually) be written to disk. Hence, at least four writes t o the current\ncylinder group will take place (recall that these writes may b e buffered\nin memory for a while before they take place). But this is not all! I n\nparticular, when creating a new ﬁle, you must also place the ﬁl e in the\nﬁle-system hierarchy, i.e., the directory must be updated. Sp eciﬁcally, the\nparent directory foo must be updated to add the entry for bar.txt ; this\nupdate may ﬁt in an existing data block of foo or require a new block to\nbe allocated (with associated data bitmap). The inode of foo must also\nbe updated, both to reﬂect the new length of the directory as well as to\nupdate time ﬁelds (such as last-modiﬁed-time). Overall, it i s a lot of work\njust to create a new ﬁle! Perhaps next time you do so, you should be m ore\nthankful, or at least surprised that it all works so well.\n41.4 Policies: How To Allocate Files and Directories\nWith this group structure in place, FFS now has to decide how to pl ace\nﬁles and directories and associated metadata on disk to improve p erfor-\nmance. The basic mantra is simple: keep related stuff together (and its corol-\nlary, keep unrelated stuff far apart ).\nThus, to obey the mantra, FFS has to decide what is “related” an d\nplace it within the same block group; conversely, unrelated ite ms should\nbe placed into different block groups. To achieve this end, FFS makes use\nof a few simple placement heuristics.\nThe ﬁrst is the placement of directories. FFS employs a simple ap -\nproach: ﬁnd the cylinder group with a low number of allocated direc -\ntories (to balance directories across groups) and a high number of free\ninodes (to subsequently be able to allocate a bunch of ﬁles), and put the\ndirectory data and inode in that group. Of course, other heuristic s could\nbe used here (e.g., taking into account the number of free data b locks).\nFor ﬁles, FFS does two things. First, it makes sure (in the gener al case)\nto allocate the data blocks of a ﬁle in the same group as its inode, th us\npreventing long seeks between inode and data (as in the old ﬁle sy stem).\nSecond, it places all ﬁles that are in the same directory in the cy linder\ngroup of the directory they are in. Thus, if a user creates four ﬁle s,/a/b ,\n/a/c ,/a/d , andb/f, FFS would try to place the ﬁrst three near one\nanother (same group) and the fourth far away (in some other group).\nLet’s look at an example of such an allocation. In the example, as-\nsume that there are only 10 inodes and 10 data blocks in each group ( both\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 LOCALITY AND THEFAST FILESYSTEM\nunrealistically small numbers), and that the three director ies (the root di-\nrectory/,/a, and/b) and four ﬁles ( /a/c, /a/d, /a/e, /b/f ) are\nplaced within them per the FFS policies. Assume the regular ﬁl es are each\ntwo blocks in size, and that the directories have just a single b lock of data.\nFor this ﬁgure, we use the obvious symbols for each ﬁle or directory (i .e.,\n/for the root directory, afor/a,ffor/b/f , and so forth).\ngroup inodes data\n0 /--------- /---------\n1 acde------ accddee---\n2 bf-------- bff-------\n3 ---------- ----------\n4 ---------- ----------\n5 ---------- ----------\n6 ---------- ----------\n7 ---------- ----------\n...\nNote that the FFS policy does two positive things: the data blocks of\neach ﬁle are near each ﬁle’s inode, and ﬁles in the same directory are\nnear one another (namely, /a/c ,/a/d , and/a/e are all in Group 1, and\ndirectory/band its ﬁle /b/f are near one another in Group 2).\nIn contrast, let’s now look at an inode allocation policy that simply\nspreads inodes across groups, trying to ensure that no group’s inod e table\nﬁlls up quickly. The ﬁnal allocation might thus look something lik e this:\ngroup inodes data\n0 /--------- /---------\n1 a--------- a---------\n2 b--------- b---------\n3 c--------- cc--------\n4 d--------- dd--------\n5 e--------- ee--------\n6 f--------- ff--------\n7 ---------- ----------\n...\nAs you can see from the ﬁgure, while this policy does indeed keep ﬁl e\n(and directory) data near its respective inode, ﬁles within a d irectory are\narbitrarily spread around the disk, and thus name-based local ity is not\npreserved. Access to ﬁles /a/c ,/a/d , and/a/e now spans three groups\ninstead of one as per the FFS approach.\nThe FFS policy heuristics are not based on extensive studies of ﬁl e-\nsystem trafﬁc or anything particularly nuanced; rather, the y are based on\ngood old-fashioned common sense (isn’t that what CS stands for after\nall?)1. Files in a directory areoften accessed together: imagine compil-\ning a bunch of ﬁles and then linking them into a single executab le. Be-\ncause such namespace-based locality exists, FFS will often im prove per-\nformance, making sure that seeks between related ﬁles are nic e and short.\n1Some people refer to common sense as horse sense , especially people who work regu-\nlarly with horses. However, we have a feeling that this idiom may be l ost as the “mechanized\nhorse”, a.k.a. the car, gains in popularity. What will they invent next ? A ﬂying machine??!!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCALITY AND THEFAST FILESYSTEM 7\n0 2 4 6 8 100%20%40%60%80%100%FFS Locality\nPath DifferenceCumulative FrequencyTrace\nRandom\nFigure 41.1: FFS Locality For SEER Traces\n41.5 Measuring File Locality\nTo understand better whether these heuristics make sense, l et’s ana-\nlyze some traces of ﬁle system access and see if indeed there is n amespace\nlocality. For some reason, there doesn’t seem to be a good study of this\ntopic in the literature.\nSpeciﬁcally, we’ll use the SEER traces [K94] and analyze how “far\naway” ﬁle accesses were from one another in the directory tree. For ex-\nample, if ﬁle fis opened, and then re-opened next in the trace (before\nany other ﬁles are opened), the distance between these two opens in the\ndirectory tree is zero (as they are the same ﬁle). If a ﬁle fin directory\ndir (i.e.,dir/f ) is opened, and followed by an open of ﬁle gin the same\ndirectory (i.e., dir/g ), the distance between the two ﬁle accesses is one,\nas they share the same directory but are not the same ﬁle. Our dis tance\nmetric, in other words, measures how far up the directory tree you h ave\nto travel to ﬁnd the common ancestor of two ﬁles; the closer they are in the\ntree, the lower the metric.\nFigure 41.1 shows the locality observed in the SEER traces over all\nworkstations in the SEER cluster over the entirety of all traces. T he graph\nplots the difference metric along the x-axis, and shows the cumu lative\npercentage of ﬁle opens that were of that difference along the y-a xis.\nSpeciﬁcally, for the SEER traces (marked “Trace” in the graph), you can\nsee that about 7% of ﬁle accesses were to the ﬁle that was opened pr evi-\nously, and that nearly 40% of ﬁle accesses were to either the sam e ﬁle or\nto one in the same directory (i.e., a difference of zero or one). Thu s, the\nFFS locality assumption seems to make sense (at least for these t races).\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 LOCALITY AND THEFAST FILESYSTEM\nInterestingly, another 25% or so of ﬁle accesses were to ﬁles tha t had a\ndistance of two. This type of locality occurs when the user has stru ctured\na set of related directories in a multi-level fashion and consist ently jumps\nbetween them. For example, if a user has a src directory and builds\nobject ﬁles ( .oﬁles) into an obj directory, and both of these directories\nare sub-directories of a main proj directory, a common access pattern\nwill beproj/src/foo.c followed by proj/obj/foo.o . The distance\nbetween these two accesses is two, as proj is the common ancestor. FFS\ndoes notcapture this type of locality in its policies, and thus more seeki ng\nwill occur between such accesses.\nFor comparison, the graph also shows locality for a “Random” trace.\nThe random trace was generated by selecting ﬁles from within an existing\nSEER trace in random order, and calculating the distance metric between\nthese randomly-ordered accesses. As you can see, there is less n amespace\nlocality in the random traces, as expected. However, because ev entually\nevery ﬁle shares a common ancestor (e.g., the root), there is some loc ality,\nand thus random is useful as a comparison point.\n41.6 The Large-File Exception\nIn FFS, there is one important exception to the general policy of ﬁle\nplacement, and it arises for large ﬁles. Without a different ru le, a large\nﬁle would entirely ﬁll the block group it is ﬁrst placed within (a nd maybe\nothers). Filling a block group in this manner is undesirable, as it prevents\nsubsequent “related” ﬁles from being placed within this block group, and\nthus may hurt ﬁle-access locality.\nThus, for large ﬁles, FFS does the following. After some number of\nblocks are allocated into the ﬁrst block group (e.g., 12 blocks, or t he num-\nber of direct pointers available within an inode), FFS places th e next “large”\nchunk of the ﬁle (e.g., those pointed to by the ﬁrst indirect block ) in an-\nother block group (perhaps chosen for its low utilization). Then, th e next\nchunk of the ﬁle is placed in yet another different block group, an d so on.\nLet’s look at some diagrams to understand this policy better. With out\nthe large-ﬁle exception, a single large ﬁle would place all of it s blocks into\none part of the disk. We investigate a small example of a ﬁle ( /a) with 30\nblocks in an FFS conﬁgured with 10 inodes and 40 data blocks per grou p.\nHere is the depiction of FFS without the large-ﬁle exception:\ngroup inodes data\n0 /a-------- /aaaaaaaaa aaaaaaaaaa aaaaaaaaaa a---------\n1 ---------- ---------- ---------- ---------- ----------\n2 ---------- ---------- ---------- ---------- ----------\n...\nAs you can see in the picture, /aﬁlls up most of the data blocks in\nGroup 0, whereas other groups remain empty. If some other ﬁles are n ow\ncreated in the root directory ( /), there is not much room for their data in\nthe group.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCALITY AND THEFAST FILESYSTEM 9\nWith the large-ﬁle exception (here set to ﬁve blocks in each chu nk), FFS\ninstead spreads the ﬁle spread across groups, and the resultin g utilization\nwithin any one group is not too high:\ngroup inodes data\n0 /a-------- /aaaaa---- ---------- ---------- ----------\n1 ---------- aaaaa----- ---------- ---------- ----------\n2 ---------- aaaaa----- ---------- ---------- ----------\n3 ---------- aaaaa----- ---------- ---------- ----------\n4 ---------- aaaaa----- ---------- ---------- ----------\n5 ---------- aaaaa----- ---------- ---------- ----------\n6 ---------- ---------- ---------- ---------- ----------\n...\nThe astute reader (that’s you) will note that spreading blocks of a ﬁle\nacross the disk will hurt performance, particularly in the rel atively com-\nmon case of sequential ﬁle access (e.g., when a user or applicati on reads\nchunks 0 through 29 in order). And you are right, oh astute reader of\nours! But you can address this problem by choosing chunk size caref ully.\nSpeciﬁcally, if the chunk size is large enough, the ﬁle system w ill spend\nmost of its time transferring data from disk and just a (relative ly) little\ntime seeking between chunks of the block. This process of reducin g an\noverhead by doing more work per overhead paid is called amortization\nand is a common technique in computer systems.\nLet’s do an example: assume that the average positioning time (i .e.,\nseek and rotation) for a disk is 10 ms. Assume further that the dis k trans-\nfers data at 40 MB/s. If your goal was to spend half our time seekin g\nbetween chunks and half our time transferring data (and thus a chieve\n50% of peak disk performance), you would thus need to spend 10 ms\ntransferring data for every 10 ms positioning. So the question bec omes:\nhow big does a chunk have to be in order to spend 10 ms in transfer?\nEasy, just use our old friend, math, in particular the dimension al analysis\nmentioned in the chapter on disks [AD14a]:\n40✘✘MB\n✟✟sec·1024KB\n1✘✘MB·1✟✟sec\n1000✟✟ms·10✟✟ms= 409.6KB (41.1)\nBasically, what this equation says is this: if you transfer dat a at 40\nMB/s, you need to transfer only 409.6KB every time you seek in orde r to\nspend half your time seeking and half your time transferring. Si milarly,\nyou can compute the size of the chunk you would need to achieve 90%\nof peak bandwidth (turns out it is about 3.69MB), or even 99% of peak\nbandwidth (40.6MB!). As you can see, the closer you want to get to p eak,\nthe bigger these chunks get (see Figure 41.2 for a plot of these va lues).\nFFS did not use this type of calculation in order to spread large ﬁl es\nacross groups, however. Instead, it took a simple approach, based on the\nstructure of the inode itself. The ﬁrst twelve direct blocks wer e placed\nin the same group as the inode; each subsequent indirect block, a nd all\nthe blocks it pointed to, was placed in a different group. With a bl ock\nsize of 4KB, and 32-bit disk addresses, this strategy implies that every\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 LOCALITY AND THEFAST FILESYSTEM\n0% 25% 50% 75% 100%1K32K1M10MThe Challenges of Amortization\nPercent Bandwidth (Desired)Log(Chunk Size Needed)50%, 409.6K90%, 3.69M\nFigure 41.2: Amortization: How Big Do Chunks Have To Be?\n1024 blocks of the ﬁle (4MB) were placed in separate groups, the l one\nexception being the ﬁrst 48KB of the ﬁle as pointed to by direct poi nters.\nNote that the trend in disk drives is that transfer rate improve s fairly\nrapidly, as disk manufacturers are good at cramming more bits in to the\nsame surface, but the mechanical aspects of drives related to seeks (disk\narm speed and the rate of rotation) improve rather slowly [P98]. Th e\nimplication is that over time, mechanical costs become relative ly more\nexpensive, and thus, to amortize said costs, you have to transfe r more\ndata between seeks.\n41.7 A Few Other Things About FFS\nFFS introduced a few other innovations too. In particular, the desi gn-\ners were extremely worried about accommodating small ﬁles; as it turned\nout, many ﬁles were 2KB or so in size back then, and using 4KB block s,\nwhile good for transferring data, was not so good for space efﬁciency .\nThis internal fragmentation could thus lead to roughly half the disk be-\ning wasted for a typical ﬁle system.\nThe solution the FFS designers hit upon was simple and solved the\nproblem. They decided to introduce sub-blocks , which were 512-byte\nlittle blocks that the ﬁle system could allocate to ﬁles. Thus, i f you created\na small ﬁle (say 1KB in size), it would occupy two sub-blocks and t hus not\nwaste an entire 4KB block. As the ﬁle grew, the ﬁle system will c ontinue\nallocating 512-byte blocks to it until it acquires a full 4KB of d ata. At that\npoint, FFS will ﬁnd a 4KB block, copy the sub-blocks into it, and free the\nsub-blocks for future use.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCALITY AND THEFAST FILESYSTEM 11\n0111098\n7\n6\n5\n4321Spindle\n0115104\n9\n3\n8\n2716Spindle\nFigure 41.3: FFS: Standard Versus Parameterized Placement\nYou might observe that this process is inefﬁcient, requiring a l ot of ex-\ntra work for the ﬁle system (in particular, a lot of extra I/O to per form the\ncopy). And you’d be right again! Thus, FFS generally avoided this pes-\nsimal behavior by modifying the libc library; the library would buffer\nwrites and then issue them in 4KB chunks to the ﬁle system, thu s avoid-\ning the sub-block specialization entirely in most cases.\nA second neat thing that FFS introduced was a disk layout that was\noptimized for performance. In those times (before SCSI and other more\nmodern device interfaces), disks were much less sophisticate d and re-\nquired the host CPU to control their operation in a more hands-on way.\nA problem arose in FFS when a ﬁle was placed on consecutive sectors of\nthe disk, as on the left in Figure 41.3.\nIn particular, the problem arose during sequential reads. FFS would\nﬁrst issue a read to block 0; by the time the read was complete, an d FFS\nissued a read to block 1, it was too late: block 1 had rotated under t he\nhead and now the read to block 1 would incur a full rotation.\nFFS solved this problem with a different layout, as you can see on th e\nright in Figure 41.3. By skipping over every other block (in the e xample),\nFFS has enough time to request the next block before it went past t he\ndisk head. In fact, FFS was smart enough to ﬁgure out for a particu lar\ndisk how many blocks it should skip in doing layout in order to avoid the\nextra rotations; this technique was called parameterization , as FFS would\nﬁgure out the speciﬁc performance parameters of the disk and use those\nto decide on the exact staggered layout scheme.\nYou might be thinking: this scheme isn’t so great after all. In f act, you\nwill only get 50% of peak bandwidth with this type of layout, becau se\nyou have to go around each track twice just to read each block once. For-\ntunately, modern disks are much smarter: they internally rea d the entire\ntrack in and buffer it in an internal disk cache (often called a track buffer\nfor this very reason). Then, on subsequent reads to the track, th e disk will\njust return the desired data from its cache. File systems thus no longer\nhave to worry about these incredibly low-level details. Abstra ction and\nhigher-level interfaces can be a good thing, when designed prop erly.\nSome other usability improvements were added as well. FFS was one\nof the ﬁrst ﬁle systems to allow for long ﬁle names , thus enabling more\nexpressive names in the ﬁle system instead of the traditional ﬁ xed-size\napproach (e.g., 8 characters). Further, a new concept was intr oduced\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 LOCALITY AND THEFAST FILESYSTEM\nTIP: M AKE THESYSTEM USABLE\nProbably the most basic lesson from FFS is that not only did it intro-\nduce the conceptually good idea of disk-aware layout, but it also a dded\na number of features that simply made the system more usable. Lon g ﬁle\nnames, symbolic links, and a rename operation that worked atomica lly\nall improved the utility of a system; while hard to write a resea rch pa-\nper about (imagine trying to read a 14-pager about “The Symbolic L ink:\nHard Link’s Long Lost Cousin”), such small features made FFS more u se-\nful and thus likely increased its chances for adoption. Making a system\nusable is often as or more important than its deep technical innova tions.\ncalled a symbolic link . As discussed in a previous chapter [AD14b] ,\nhard links are limited in that they both could not point to director ies (for\nfear of introducing loops in the ﬁle system hierarchy) and that th ey can\nonly point to ﬁles within the same volume (i.e., the inode number m ust\nstill be meaningful). Symbolic links allow the user to create an “alias” to\nany other ﬁle or directory on a system and thus are much more ﬂexible .\nFFS also introduced an atomic rename() operation for renaming ﬁles.\nUsability improvements, beyond the basic technology, also like ly gained\nFFS a stronger user base.\n41.8 Summary\nThe introduction of FFS was a watershed moment in ﬁle system his-\ntory, as it made clear that the problem of ﬁle management was one of t he\nmost interesting issues within an operating system, and showed how one\nmight begin to deal with that most important of devices, the hard disk.\nSince that time, hundreds of new ﬁle systems have developed, but still\ntoday many ﬁle systems take cues from FFS (e.g., Linux ext2 and e xt3 are\nobvious intellectual descendants). Certainly all modern syst ems account\nfor the main lesson of FFS: treat the disk like it’s a disk.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLOCALITY AND THEFAST FILESYSTEM 13\nReferences\n[AD14a] “Operating Systems: Three Easy Pieces” (Chapter: Hard Disk Dr ives) by Remzi\nArpaci-Dusseau and Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2 014. There is no way\nyou should be reading about FFS without having ﬁrst understood hard drives in some detail. If you try\nto do so, please instead go directly to jail; do not pass go, and, critically, d o not collect 200 much-needed\nsimoleons.\n[AD14b] “Operating Systems: Three Easy Pieces” (Chapter: File System Implementation) by\nRemzi Arpaci-Dusseau and Andrea Arpaci-Dusseau . Arpaci-Dusseau Bo oks, 2014. As above,\nit makes little sense to read this chapter unless you have read (and understood) th e chapter on ﬁle\nsystem implementation. Otherwise, we’ll be throwing around terms like “i node” and “indirect block”\nand you’ll be like “huh?” and that is no fun for either of us.\n[K94] “The Design of the SEER Predictive Caching System” by G. H. Kuenning. MOBICOMM\n’94, Santa Cruz, California, December 1994. According to Kuenning, this is the best overview of the\nSEER project, which led to (among other things) the collection of these traces.\n[MJLF84] “A Fast File System for U NIX” by Marshall K. McKusick, William N. Joy, Sam J.\nLefﬂer, Robert S. Fabry. ACM TOCS, 2:3, August 1984. McKusick was recently honored with the\nIEEE Reynold B. Johnson award for his contributions to ﬁle systems, much of whi ch was based on\nhis work building FFS. In his acceptance speech, he discussed the ori ginal FFS software: only 1200\nlines of code! Modern versions are a little more complex, e.g., the BSD FF S descendant now is in the\n50-thousand lines-of-code range.\n[P98] “Hardware Technology Trends and Database Opportunities” by Dav id A. Patterson.\nKeynote Lecture at SIGMOD ’98, June 1998. A great and simple overview of disk technology trends\nand how they change over time.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 LOCALITY AND THEFAST FILESYSTEM\nHomework (Simulation)\nThis section introduces ffs.py , a simple FFS simulator you can use\nto understand better how FFS-based ﬁle and directory allocation w ork.\nSee the README for details on how to run the simulator.\nQuestions\n1. Examine the ﬁle in.largefile , and then run the simulator with ﬂag -f\nin.largefile and-L 4 . The latter sets the large-ﬁle exception to 4 blocks.\nWhat will the resulting allocation look like? Run with -cto check.\n2. Now run with -L 30 . What do you expect to see? Once again, turn on -c\nto see if you were right. You can also use -Sto see exactly which blocks\nwere allocated to the ﬁle /a.\n3. Now we will compute some statistics about the ﬁle. The ﬁrst is so mething\nwe call ﬁlespan , which is the max distance between any two data blocks of\nthe ﬁle or between the inode and any data block. Calculate the ﬁ lespan of\n/a. Runffs.py -f in.largefile -L 4 -T -c to see what it is. Do\nthe same with -L 100 . What difference do you expect in ﬁlespan as the\nlarge-ﬁle exception parameter changes from low values to high v alues?\n4. Now let’s look at a new input ﬁle, in.manyfiles . How do you think the\nFFS policy will lay these ﬁles out across groups? (you can run wit h-vto\nsee what ﬁles and directories are created, or just cat in.manyfiles ). Run\nthe simulator with -cto see if you were right.\n5. A metric to evaluate FFS is called dirspan . This metric calculates the spread\nof ﬁles within a particular directory, speciﬁcally the max dis tance between\nthe inodes and data blocks of all ﬁles in the directory and the inode and data\nblock of the directory itself. Run with in.manyfiles and the-Tﬂag, and\ncalculate the dirspan of the three directories. Run with -cto check. How\ngood of a job does FFS do in minimizing dirspan?\n6. Now change the size of the inode table per group to 5 ( -I 5 ). How do you\nthink this will change the layout of the ﬁles? Run with -cto see if you were\nright. How does it affect the dirspan?\n7. Which group should FFS place inode of a new directory in? The d efault\n(simulator) policy looks for the group with the most free inodes. A different\npolicy looks for a set of groups with the most free inodes. For exa mple, if\nyou run with -A 2 , when allocating a new directory, the simulator will look\nat groups in pairs and pick the best pair for the allocation. Run ./ffs.py\n-f in.manyfiles -I 5 -A 2 -c to see how allocation changes with\nthis strategy. How does it affect dirspan? Why might this poli cy be good?\n8. One last policy change we will explore relates to ﬁle fragmen tation. Run\n./ffs.py -f in.fragmented -v and see if you can predict how the\nﬁles that remain are allocated. Run with -cto conﬁrm your answer. What\nis interesting about the data layout of ﬁle /i? Why is it problematic?\n9. A new policy, which we call contiguous allocation (-C), tries to ensure that\neach ﬁle is allocated contiguously. Speciﬁcally, with -C n , the ﬁle system\ntries to ensure that ncontiguous blocks are free within a group before al-\nlocating a block. Run ./ffs.py -f in.fragmented -v -C 2 -c to\nsee the difference. How does layout change as the parameter pas sed to-C\nincreases? Finally, how does -Caffect ﬁlespan and dirspan?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",34058
48-42. FSCK and Journaling.pdf,48-42. FSCK and Journaling,"42\nCrash Consistency: FSCK and Journaling\nAs we’ve seen thus far, the ﬁle system manages a set of data struc tures to\nimplement the expected abstractions: ﬁles, directories, and all of the other\nmetadata needed to support the basic abstraction that we expec t from a\nﬁle system. Unlike most data structures (for example, those foun d in\nmemory of a running program), ﬁle system data structures must persist ,\ni.e., they must survive over the long haul, stored on devices that retain\ndata despite power loss (such as hard disks or ﬂash-based SSDs).\nOne major challenge faced by a ﬁle system is how to update persis -\ntent data structures despite the presence of a power loss orsystem crash .\nSpeciﬁcally, what happens if, right in the middle of updating on -disk\nstructures, someone trips over the power cord and the machine loses\npower? Or the operating system encounters a bug and crashes? Bec ause\nof power losses and crashes, updating a persistent data structu re can be\nquite tricky, and leads to a new and interesting problem in ﬁle system\nimplementation, known as the crash-consistency problem .\nThis problem is quite simple to understand. Imagine you have to up-\ndate two on-disk structures, AandB, in order to complete a particular\noperation. Because the disk only services a single request at a t ime, one\nof these requests will reach the disk ﬁrst (either AorB). If the system\ncrashes or loses power after one write completes, the on-disk struc ture\nwill be left in an inconsistent state. And thus, we have a problem that all\nﬁle systems need to solve:\nTHECRUX: HOWTOUPDATE THEDISKDESPITE CRASHES\nThe system may crash or lose power between any two writes, and\nthus the on-disk state may only partially get updated. After th e crash,\nthe system boots and wishes to mount the ﬁle system again (in order to\naccess ﬁles and such). Given that crashes can occur at arbitra ry points\nin time, how do we ensure the ﬁle system keeps the on-disk image i n a\nreasonable state?\n1\n2 C RASH CONSISTENCY : FSCK AND JOURNALING\nIn this chapter, we’ll describe this problem in more detail, and look\nat some methods ﬁle systems have used to overcome it. We’ll begin by\nexamining the approach taken by older ﬁle systems, known as fsck or the\nﬁle system checker . We’ll then turn our attention to another approach,\nknown as journaling (also known as write-ahead logging ), a technique\nwhich adds a little bit of overhead to each write but recovers more quickly\nfrom crashes or power losses. We will discuss the basic machinery of\njournaling, including a few different ﬂavors of journaling that Linux ext3\n[T98,PAA05] (a relatively modern journaling ﬁle system) impl ements.\n42.1 A Detailed Example\nTo kick off our investigation of journaling, let’s look at an example.\nWe’ll need to use a workload that updates on-disk structures in some\nway. Assume here that the workload is simple: the append of a sing le\ndata block to an existing ﬁle. The append is accomplished by open ing the\nﬁle, calling lseek() to move the ﬁle offset to the end of the ﬁle, and then\nissuing a single 4KB write to the ﬁle before closing it.\nLet’s also assume we are using standard simple ﬁle system stru ctures\non the disk, similar to ﬁle systems we have seen before. This tin y example\nincludes an inode bitmap (with just 8 bits, one per inode), a data bitmap\n(also 8 bits, one per data block), inodes (8 total, numbered 0 to 7, and\nspread across four blocks), and data blocks (8 total, numbered 0 to 7).\nHere is a diagram of this ﬁle system:\nInode\nBmapData\nBmapInodes Data Blocks\nI[v1]Da\nIf you look at the structures in the picture, you can see that a sing le inode\nis allocated (inode number 2), which is marked in the inode bitma p, and a\nsingle allocated data block (data block 4), also marked in the da ta bitmap.\nThe inode is denoted I[v1], as it is the ﬁrst version of this inode; i t will\nsoon be updated (due to the workload described above).\nLet’s peek inside this simpliﬁed inode too. Inside of I[v1], we see :\nowner : remzi\npermissions : read-write\nsize : 1\npointer : 4\npointer : null\npointer : null\npointer : null\nIn this simpliﬁed inode, the size of the ﬁle is 1(it has one block al-\nlocated), the ﬁrst direct pointer points to block 4(the ﬁrst data block of\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCRASH CONSISTENCY : FSCK AND JOURNALING 3\nthe ﬁle, Da), and all three other direct pointers are set to null (indicating\nthat they are not used). Of course, real inodes have many more ﬁeld s; see\nprevious chapters for more information.\nWhen we append to the ﬁle, we are adding a new data block to it, an d\nthus must update three on-disk structures: the inode (which mu st point\nto the new block and record the new larger size due to the append) , the\nnew data block Db, and a new version of the data bitmap (call it B[v 2]) to\nindicate that the new data block has been allocated.\nThus, in the memory of the system, we have three blocks which we\nmust write to disk. The updated inode (inode version 2, or I[v2] for short)\nnow looks like this:\nowner : remzi\npermissions : read-write\nsize : 2\npointer : 4\npointer : 5\npointer : null\npointer : null\nThe updated data bitmap (B[v2]) now looks like this: 00001100. F inally,\nthere is the data block (Db), which is just ﬁlled with whatever it is users\nput into ﬁles. Stolen music perhaps?\nWhat we would like is for the ﬁnal on-disk image of the ﬁle system to\nlook like this:\nInode\nBmapData\nBmapInodes Data Blocks\nI[v2]Da Db\nTo achieve this transition, the ﬁle system must perform three s epa-\nrate writes to the disk, one each for the inode (I[v2]), bitmap (B [v2]), and\ndata block (Db). Note that these writes usually don’t happen imme di-\nately when the user issues a write() system call; rather, the dirty in-\node, bitmap, and new data will sit in main memory (in the page cache\norbuffer cache ) for some time ﬁrst; then, when the ﬁle system ﬁnally\ndecides to write them to disk (after say 5 seconds or 30 seconds), the ﬁle\nsystem will issue the requisite write requests to the disk. U nfortunately,\na crash may occur and thus interfere with these updates to the d isk. In\nparticular, if a crash happens after one or two of these writes ha ve taken\nplace, but not all three, the ﬁle system could be left in a funny s tate.\nCrash Scenarios\nTo understand the problem better, let’s look at some example crash sce-\nnarios. Imagine only a single write succeeds; there are thus th ree possible\noutcomes, which we list here:\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 C RASH CONSISTENCY : FSCK AND JOURNALING\n•Just the data block (Db) is written to disk. In this case, the data is\non disk, but there is no inode that points to it and no bitmap that\neven says the block is allocated. Thus, it is as if the write neve r\noccurred. This case is not a problem at all, from the perspective of\nﬁle-system crash consistency1.\n•Just the updated inode (I[v2]) is written to disk. In this case, the\ninode points to the disk address (5) where Db was about to be writ-\nten, but Db has not yet been written there. Thus, if we trust tha t\npointer, we will read garbage data from the disk (the old contents\nof disk address 5).\nFurther, we have a new problem, which we call a ﬁle-system in-\nconsistency . The on-disk bitmap is telling us that data block 5 has\nnot been allocated, but the inode is saying that it has. The disag ree-\nment between the bitmap and the inode is an inconsistency in the\ndata structures of the ﬁle system; to use the ﬁle system, we mus t\nsomehow resolve this problem (more on that below).\n•Just the updated bitmap (B[v2]) is written to disk. In this case, the\nbitmap indicates that block 5 is allocated, but there is no inode that\npoints to it. Thus the ﬁle system is inconsistent again; if left unre-\nsolved, this write would result in a space leak , as block 5 would\nnever be used by the ﬁle system.\nThere are also three more crash scenarios in this attempt to wri te three\nblocks to disk. In these cases, two writes succeed and the last one fails:\n•The inode (I[v2]) and bitmap (B[v2]) are written to disk, but not\ndata (Db). In this case, the ﬁle system metadata is completely con-\nsistent: the inode has a pointer to block 5, the bitmap indicates that\n5 is in use, and thus everything looks OK from the perspective of\nthe ﬁle system’s metadata. But there is one problem: 5 has garbag e\nin it again.\n•The inode (I[v2]) and the data block (Db) are written, but not the\nbitmap (B[v2]). In this case, we have the inode pointing to the cor-\nrect data on disk, but again have an inconsistency between the i n-\node and the old version of the bitmap (B1). Thus, we once again\nneed to resolve the problem before using the ﬁle system.\n•The bitmap (B[v2]) and data block (Db) are written, but not th e\ninode (I[v2]). In this case, we again have an inconsistency between\nthe inode and the data bitmap. However, even though the block\nwas written and the bitmap indicates its usage, we have no ide a\nwhich ﬁle it belongs to, as no inode points to the ﬁle.\n1However, it might be a problem for the user, who just lost some data !\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCRASH CONSISTENCY : FSCK AND JOURNALING 5\nThe Crash Consistency Problem\nHopefully, from these crash scenarios, you can see the many proble ms\nthat can occur to our on-disk ﬁle system image because of crashes: we can\nhave inconsistency in ﬁle system data structures; we can have space leaks;\nwe can return garbage data to a user; and so forth. What we’d like to do\nideally is move the ﬁle system from one consistent state (e.g., be fore the\nﬁle got appended to) to another atomically (e.g., after the inode, bitmap,\nand new data block have been written to disk). Unfortunately, w e can’t\ndo this easily because the disk only commits one write at a time, a nd\ncrashes or power loss may occur between these updates. We call thi s\ngeneral problem the crash-consistency problem (we could also call it the\nconsistent-update problem ).\n42.2 Solution #1: The File System Checker\nEarly ﬁle systems took a simple approach to crash consistency. Ba si-\ncally, they decided to let inconsistencies happen and then ﬁx them later\n(when rebooting). A classic example of this lazy approach is found in a\ntool that does this: fsck2.fsck is a U NIX tool for ﬁnding such inconsis-\ntencies and repairing them [M86]; similar tools to check and re pair a disk\npartition exist on different systems. Note that such an approach can’t ﬁx\nall problems; consider, for example, the case above where the ﬁle system\nlooks consistent but the inode points to garbage data. The only real goal\nis to make sure the ﬁle system metadata is internally consiste nt.\nThe toolfsck operates in a number of phases, as summarized in\nMcKusick and Kowalski’s paper [MK96]. It is run before the ﬁle system\nis mounted and made available ( fsck assumes that no other ﬁle-system\nactivity is on-going while it runs); once ﬁnished, the on-disk ﬁl e system\nshould be consistent and thus can be made accessible to users.\nHere is a basic summary of what fsck does:\n•Superblock: fsck ﬁrst checks if the superblock looks reasonable,\nmostly doing sanity checks such as making sure the ﬁle system si ze\nis greater than the number of blocks that have been allocated. Us u-\nally the goal of these sanity checks is to ﬁnd a suspect (corrupt)\nsuperblock; in this case, the system (or administrator) may dec ide\nto use an alternate copy of the superblock.\n•Free blocks: Next,fsck scans the inodes, indirect blocks, double\nindirect blocks, etc., to build an understanding of which block s are\ncurrently allocated within the ﬁle system. It uses this knowle dge\nto produce a correct version of the allocation bitmaps; thus, if the re\nis any inconsistency between bitmaps and inodes, it is resolved by\ntrusting the information within the inodes. The same type of chec k\nis performed for all the inodes, making sure that all inodes that l ook\nlike they are in use are marked as such in the inode bitmaps.\n2Pronounced either “eff-ess-see-kay”, “eff-ess-check”, or, if you don’t like the tool, “eff-\nsuck”. Yes, serious professional people use this term.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 C RASH CONSISTENCY : FSCK AND JOURNALING\n•Inode state: Each inode is checked for corruption or other prob-\nlems. For example, fsck makes sure that each allocated inode has\na valid type ﬁeld (e.g., regular ﬁle, directory, symbolic link , etc.). If\nthere are problems with the inode ﬁelds that are not easily ﬁxed, the\ninode is considered suspect and cleared by fsck ; the inode bitmap\nis correspondingly updated.\n•Inode links: fsck also veriﬁes the link count of each allocated in-\node. As you may recall, the link count indicates the number of dif-\nferent directories that contain a reference (i.e., a link) to t his par-\nticular ﬁle. To verify the link count, fsck scans through the en-\ntire directory tree, starting at the root directory, and builds i ts own\nlink counts for every ﬁle and directory in the ﬁle system. If ther e\nis a mismatch between the newly-calculated count and that foun d\nwithin an inode, corrective action must be taken, usually by ﬁxi ng\nthe count within the inode. If an allocated inode is discovered but\nno directory refers to it, it is moved to the lost+found directory.\n•Duplicates: fsck also checks for duplicate pointers, i.e., cases where\ntwo different inodes refer to the same block. If one inode is obvi-\nously bad, it may be cleared. Alternately, the pointed-to block could\nbe copied, thus giving each inode its own copy as desired.\n•Bad blocks: A check for bad block pointers is also performed while\nscanning through the list of all pointers. A pointer is considered\n“bad” if it obviously points to something outside its valid range,\ne.g., it has an address that refers to a block greater than the p arti-\ntion size. In this case, fsck can’t do anything too intelligent; it just\nremoves (clears) the pointer from the inode or indirect block.\n•Directory checks: fsck does not understand the contents of user\nﬁles; however, directories hold speciﬁcally formatted informat ion\ncreated by the ﬁle system itself. Thus, fsck performs additional\nintegrity checks on the contents of each directory, making sure t hat\n“.” and “..” are the ﬁrst entries, that each inode referred to i n a\ndirectory entry is allocated, and ensuring that no directory is linked\nto more than once in the entire hierarchy.\nAs you can see, building a working fsck requires intricate knowledge\nof the ﬁle system; making sure such a piece of code works correctly i n all\ncases can be challenging [G+08]. However, fsck (and similar a pproaches)\nhave a bigger and perhaps more fundamental problem: they are too slow .\nWith a very large disk volume, scanning the entire disk to ﬁnd a ll the\nallocated blocks and read the entire directory tree may take man y minutes\nor hours. Performance of fsck , as disks grew in capacity and RAIDs\ngrew in popularity, became prohibitive (despite recent advan ces [M+13]).\nAt a higher level, the basic premise of fsck seems just a tad irra-\ntional. Consider our example above, where just three blocks are wr itten\nto the disk; it is incredibly expensive to scan the entire dis k to ﬁx prob-\nlems that occurred during an update of just three blocks. This si tuation is\nakin to dropping your keys on the ﬂoor in your bedroom, and then com-\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCRASH CONSISTENCY : FSCK AND JOURNALING 7\nmencing a search-the-entire-house-for-keys recovery algorithm, starting in\nthe basement and working your way through every room. It works but is\nwasteful. Thus, as disks (and RAIDs) grew, researchers and p ractitioners\nstarted to look for other solutions.\n42.3 Solution #2: Journaling (or Write-Ahead Logging)\nProbably the most popular solution to the consistent update problem\nis to steal an idea from the world of database management systems . That\nidea, known as write-ahead logging , was invented to address exactly this\ntype of problem. In ﬁle systems, we usually call write-ahead log ging jour-\nnaling for historical reasons. The ﬁrst ﬁle system to do this was Cedar\n[H87], though many modern ﬁle systems use the idea, including L inux\next3 and ext4, reiserfs, IBM’s JFS, SGI’s XFS, and Windows NTFS.\nThe basic idea is as follows. When updating the disk, before over-\nwriting the structures in place, ﬁrst write down a little note ( somewhere\nelse on the disk, in a well-known location) describing what you are about\nto do. Writing this note is the “write ahead” part, and we write i t to a\nstructure that we organize as a “log”; hence, write-ahead loggi ng.\nBy writing the note to disk, you are guaranteeing that if a crash takes\nplaces during the update (overwrite) of the structures you are u pdating,\nyou can go back and look at the note you made and try again; thus, you\nwill know exactly what to ﬁx (and how to ﬁx it) after a crash, inst ead\nof having to scan the entire disk. By design, journaling thus ad ds a bit\nof work during updates to greatly reduce the amount of work require d\nduring recovery.\nWe’ll now describe how Linux ext3 , a popular journaling ﬁle system,\nincorporates journaling into the ﬁle system. Most of the on-disk st ruc-\ntures are identical to Linux ext2 , e.g., the disk is divided into block groups,\nand each block group contains an inode bitmap, data bitmap, inodes , and\ndata blocks. The new key structure is the journal itself, which occupies\nsome small amount of space within the partition or on another device.\nThus, an ext2 ﬁle system (without journaling) looks like this:\nSuper Group 0 Group 1 . . . Group N\nAssuming the journal is placed within the same ﬁle system imag e\n(though sometimes it is placed on a separate device, or as a ﬁle wit hin\nthe ﬁle system), an ext3 ﬁle system with a journal looks like this :\nSuper Journal Group 0 Group 1 . . . Group N\nThe real difference is just the presence of the journal, and of cou rse,\nhow it is used.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 C RASH CONSISTENCY : FSCK AND JOURNALING\nData Journaling\nLet’s look at a simple example to understand how data journaling works.\nData journaling is available as a mode with the Linux ext3 ﬁle sy stem,\nfrom which much of this discussion is based.\nSay we have our canonical update again, where we wish to write the\ninode (I[v2]), bitmap (B[v2]), and data block (Db) to disk agai n. Before\nwriting them to their ﬁnal disk locations, we are now ﬁrst going to write\nthem to the log (a.k.a. journal). This is what this will look like i n the log:JournalTxB I[v2] B[v2] Db TxE\nYou can see we have written ﬁve blocks here. The transaction begi n\n(TxB) tells us about this update, including information about th e pend-\ning update to the ﬁle system (e.g., the ﬁnal addresses of the bl ocks I[v2],\nB[v2], and Db), and some kind of transaction identiﬁer (TID ). The mid-\ndle three blocks just contain the exact contents of the blocks them selves;\nthis is known as physical logging as we are putting the exact physical\ncontents of the update in the journal (an alternate idea, logical logging ,\nputs a more compact logical representation of the update in the jour nal,\ne.g., “this update wishes to append data block Db to ﬁle X”, whi ch is a\nlittle more complex but can save space in the log and perhaps impr ove\nperformance). The ﬁnal block (TxE) is a marker of the end of this tr ansac-\ntion, and will also contain the TID.\nOnce this transaction is safely on disk, we are ready to overwrit e the\nold structures in the ﬁle system; this process is called checkpointing .\nThus, to checkpoint the ﬁle system (i.e., bring it up to date with the pend-\ning update in the journal), we issue the writes I[v2], B[v2], a nd Db to\ntheir disk locations as seen above; if these writes complete succ essfully,\nwe have successfully checkpointed the ﬁle system and are basi cally done.\nThus, our initial sequence of operations:\n1.Journal write: Write the transaction, including a transaction-begin\nblock, all pending data and metadata updates, and a transacti on-\nend block, to the log; wait for these writes to complete.\n2.Checkpoint: Write the pending metadata and data updates to their\nﬁnal locations in the ﬁle system.\nIn our example, we would write TxB, I[v2], B[v2], Db, and TxE to t he\njournal ﬁrst. When these writes complete, we would complete the u pdate\nby checkpointing I[v2], B[v2], and Db, to their ﬁnal locations on disk.\nThings get a little trickier when a crash occurs during the wri tes to\nthe journal. Here, we are trying to write the set of blocks in the t ransac-\ntion (e.g., TxB, I[v2], B[v2], Db, TxE) to disk. One simple way to do this\nwould be to issue each one at a time, waiting for each to complete, a nd\nthen issuing the next. However, this is slow. Ideally, we’d like to issue\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCRASH CONSISTENCY : FSCK AND JOURNALING 9\nASIDE : FORCING WRITES TODISK\nTo enforce ordering between two disk writes, modern ﬁle systems have\nto take a few extra precautions. In olden times, forcing ordering between\ntwo writes, AandB, was easy: just issue the write of Ato the disk, wait\nfor the disk to interrupt the OS when the write is complete, and t hen issue\nthe write of B.\nThings got slightly more complex due to the increased use of write caches\nwithin disks. With write buffering enabled (sometimes calle dimmediate\nreporting ), a disk will inform the OS the write is complete when it simply\nhas been placed in the disk’s memory cache, and has not yet reache d\ndisk. If the OS then issues a subsequent write, it is not guaran teed to\nreach the disk after previous writes; thus ordering between wr ites is not\npreserved. One solution is to disable write buffering. However , more\nmodern systems take extra precautions and issue explicit write barriers ;\nsuch a barrier, when it completes, guarantees that all writes issued before\nthe barrier will reach disk before any writes issued after the barrier.\nAll of this machinery requires a great deal of trust in the correc t oper-\nation of the disk. Unfortunately, recent research shows that some disk\nmanufacturers, in an effort to deliver “higher performing” di sks, explic-\nitly ignore write-barrier requests, thus making the disks se emingly run\nfaster but at the risk of incorrect operation [C+13, R+11]. As Kah an said,\nthe fast almost always beats out the slow, even if the fast is wrong .\nall ﬁve block writes at once, as this would turn ﬁve writes into a s ingle\nsequential write and thus be faster. However, this is unsafe, for the fol-\nlowing reason: given such a big write, the disk internally may p erform\nscheduling and complete small pieces of the big write in any orde r. Thus,\nthe disk internally may (1) write TxB, I[v2], B[v2], and TxE a nd only later\n(2) write Db. Unfortunately, if the disk loses power between (1) and (2),\nthis is what ends up on disk:\nJournalTxB\nid=1I[v2] B[v2] ?? TxE\nid=1\nWhy is this a problem? Well, the transaction looks like a valid tra ns-\naction (it has a begin and an end with matching sequence number s). Fur-\nther, the ﬁle system can’t look at that fourth block and know it is wron g;\nafter all, it is arbitrary user data. Thus, if the system now re boots and\nruns recovery, it will replay this transaction, and ignorantly copy the con-\ntents of the garbage block ’??’ to the location where Db is supposed t o\nlive. This is bad for arbitrary user data in a ﬁle; it is much wors e if it hap-\npens to a critical piece of ﬁle system, such as the superblock, w hich could\nrender the ﬁle system unmountable.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 C RASH CONSISTENCY : FSCK AND JOURNALING\nASIDE : OPTIMIZING LOGWRITES\nYou may have noticed a particular inefﬁciency of writing to the l og.\nNamely, the ﬁle system ﬁrst has to write out the transaction-be gin block\nand contents of the transaction; only after these writes complete can the\nﬁle system send the transaction-end block to disk. The performa nce im-\npact is clear, if you think about how a disk works: usually an extra rota-\ntion is incurred (think about why).\nOne of our former graduate students, Vijayan Prabhakaran, had a simple\nidea to ﬁx this problem [P+05]. When writing a transaction to th e journal,\ninclude a checksum of the contents of the journal in the begin and e nd\nblocks. Doing so enables the ﬁle system to write the entire tran saction at\nonce, without incurring a wait; if, during recovery, the ﬁle sys tem sees\na mismatch in the computed checksum versus the stored checksum in\nthe transaction, it can conclude that a crash occurred during th e write\nof the transaction and thus discard the ﬁle-system update. Thu s, with a\nsmall tweak in the write protocol and recovery system, a ﬁle syste m can\nachieve faster common-case performance; on top of that, the system is\nslightly more reliable, as any reads from the journal are now prote cted by\na checksum.\nThis simple ﬁx was attractive enough to gain the notice of Linux ﬁ le sys-\ntem developers, who then incorporated it into the next generati on Linux\nﬁle system, called (you guessed it!) Linux ext4 . It now ships on mil-\nlions of machines worldwide, including the Android handheld pla tform.\nThus, every time you write to disk on many Linux-based systems, a little\ncode developed at Wisconsin makes your system a little faster and more\nreliable.\nTo avoid this problem, the ﬁle system issues the transactional w rite in\ntwo steps. First, it writes all blocks except the TxE block to th e journal,\nissuing these writes all at once. When these writes complete, t he journal\nwill look something like this (assuming our append workload again) :JournalTxB\nid=1I[v2] B[v2] Db\nWhen those writes complete, the ﬁle system issues the write of th e TxE\nblock, thus leaving the journal in this ﬁnal, safe state:\nJournalTxB\nid=1I[v2] B[v2] Db TxE\nid=1\nAn important aspect of this process is the atomicity guarantee pr o-\nvided by the disk. It turns out that the disk guarantees that an y 512-byte\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCRASH CONSISTENCY : FSCK AND JOURNALING 11\nwrite will either happen or not (and never be half-written); th us, to make\nsure the write of TxE is atomic, one should make it a single 512-byt e block.\nThus, our current protocol to update the ﬁle system, with each of it s three\nphases labeled:\n1.Journal write: Write the contents of the transaction (including TxB,\nmetadata, and data) to the log; wait for these writes to complete .\n2.Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for write to complete; transaction is said to be\ncommitted .\n3.Checkpoint: Write the contents of the update (metadata and data)\nto their ﬁnal on-disk locations.\nRecovery\nLet’s now understand how a ﬁle system can use the contents of the jour -\nnal to recover from a crash. A crash may happen at any time during this\nsequence of updates. If the crash happens before the transacti on is writ-\nten safely to the log (i.e., before Step 2 above completes), then our job\nis easy: the pending update is simply skipped. If the crash ha ppens af-\nter the transaction has committed to the log, but before the check point is\ncomplete, the ﬁle system can recover the update as follows. When the\nsystem boots, the ﬁle system recovery process will scan the log and look\nfor transactions that have committed to the disk; these transac tions are\nthus replayed (in order), with the ﬁle system again attempting to write\nout the blocks in the transaction to their ﬁnal on-disk locations. T his form\nof logging is one of the simplest forms there is, and is called redo logging .\nBy recovering the committed transactions in the journal, the ﬁle system\nensures that the on-disk structures are consistent, and thus c an proceed\nby mounting the ﬁle system and readying itself for new requests .\nNote that it is ﬁne for a crash to happen at any point during check-\npointing, even after some of the updates to the ﬁnal locations of the blocks\nhave completed. In the worst case, some of these updates are simpl y per-\nformed again during recovery. Because recovery is a rare operati on (only\ntaking place after an unexpected system crash), a few redund ant writes\nare nothing to worry about3.\nBatching Log Updates\nYou might have noticed that the basic protocol could add a lot of extra\ndisk trafﬁc. For example, imagine we create two ﬁles in a row, ca lled\nfile1 andfile2 , in the same directory. To create one ﬁle, one has\nto update a number of on-disk structures, minimally including : the in-\node bitmap (to allocate a new inode), the newly-created inode of th e ﬁle,\n3Unless you worry about everything, in which case we can’t help you. Stop worrying so\nmuch, it is unhealthy! But now you’re probably worried about over-wo rrying.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 C RASH CONSISTENCY : FSCK AND JOURNALING\nthe data block of the parent directory containing the new director y en-\ntry, and the parent directory inode (which now has a new modiﬁcati on\ntime). With journaling, we logically commit all of this informati on to\nthe journal for each of our two ﬁle creations; because the ﬁles are i n the\nsame directory, and assuming they even have inodes within the s ame in-\node block, this means that if we’re not careful, we’ll end up writin g these\nsame blocks over and over.\nTo remedy this problem, some ﬁle systems do not commit each update\nto disk one at a time (e.g., Linux ext3); rather, one can buffer a ll updates\ninto a global transaction. In our example above, when the two ﬁles are\ncreated, the ﬁle system just marks the in-memory inode bitmap, inodes\nof the ﬁles, directory data, and directory inode as dirty, and add s them to\nthe list of blocks that form the current transaction. When it is ﬁn ally time\nto write these blocks to disk (say, after a timeout of 5 seconds), t his single\nglobal transaction is committed containing all of the updates des cribed\nabove. Thus, by buffering updates, a ﬁle system can avoid exces sive write\ntrafﬁc to disk in many cases.\nMaking The Log Finite\nWe thus have arrived at a basic protocol for updating ﬁle-system on -disk\nstructures. The ﬁle system buffers updates in memory for some ti me;\nwhen it is ﬁnally time to write to disk, the ﬁle system ﬁrst car efully writes\nout the details of the transaction to the journal (a.k.a. write-a head log);\nafter the transaction is complete, the ﬁle system checkpoints t hose blocks\nto their ﬁnal locations on disk.\nHowever, the log is of a ﬁnite size. If we keep adding transactions to\nit (as in this ﬁgure), it will soon ﬁll. What do you think happens t hen?JournalTx1 Tx2 Tx3 Tx4 Tx5 ...\nTwo problems arise when the log becomes full. The ﬁrst is simpler ,\nbut less critical: the larger the log, the longer recovery will t ake, as the\nrecovery process must replay all the transactions within the log (in order)\nto recover. The second is more of an issue: when the log is full (or nea rly\nfull), no further transactions can be committed to the disk, th us making\nthe ﬁle system “less than useful” (i.e., useless).\nTo address these problems, journaling ﬁle systems treat the log as a\ncircular data structure, re-using it over and over; this is why the journal\nis sometimes referred to as a circular log . To do so, the ﬁle system must\ntake action some time after a checkpoint. Speciﬁcally, once a tran saction\nhas been checkpointed, the ﬁle system should free the space it w as occu-\npying within the journal, allowing the log space to be reused. Th ere are\nmany ways to achieve this end; for example, you could simply mark the\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCRASH CONSISTENCY : FSCK AND JOURNALING 13\noldest and newest non-checkpointed transactions in the log in a journal\nsuperblock ; all other space is free. Here is a graphical depiction:JournalJournal\nSuperTx1 Tx2 Tx3 Tx4 Tx5 ...\nIn the journal superblock (not to be confused with the main ﬁle sys tem\nsuperblock), the journaling system records enough information to know\nwhich transactions have not yet been checkpointed, and thus red uces re-\ncovery time as well as enables re-use of the log in a circular fash ion. And\nthus we add another step to our basic protocol:\n1.Journal write: Write the contents of the transaction (containing TxB\nand the contents of the update) to the log; wait for these writes to\ncomplete.\n2.Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for the write to complete; the transaction is\nnow committed .\n3.Checkpoint: Write the contents of the update to their ﬁnal locations\nwithin the ﬁle system.\n4.Free: Some time later, mark the transaction free in the journal by\nupdating the journal superblock.\nThus we have our ﬁnal data journaling protocol. But there is still a\nproblem: we are writing each data block to the disk twice , which is a\nheavy cost to pay, especially for something as rare as a system cr ash. Can\nyou ﬁgure out a way to retain consistency without writing data twi ce?\nMetadata Journaling\nAlthough recovery is now fast (scanning the journal and replayin g a few\ntransactions as opposed to scanning the entire disk), normal oper ation\nof the ﬁle system is slower than we might desire. In particular, for each\nwrite to disk, we are now also writing to the journal ﬁrst, thus d oubling\nwrite trafﬁc; this doubling is especially painful during seq uential write\nworkloads, which now will proceed at half the peak write bandwidt h of\nthe drive. Further, between writes to the journal and writes t o the main\nﬁle system, there is a costly seek, which adds noticeable overhe ad for\nsome workloads.\nBecause of the high cost of writing every data block to disk twice, peo-\nple have tried a few different things in order to speed up perfor mance.\nFor example, the mode of journaling we described above is often call ed\ndata journaling (as in Linux ext3), as it journals all user data (in addition\nto the metadata of the ﬁle system). A simpler (and more common) form\nof journaling is sometimes called ordered journaling (or just metadata\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 C RASH CONSISTENCY : FSCK AND JOURNALING\njournaling ), and it is nearly the same, except that user data is notwrit-\nten to the journal. Thus, when performing the same update as abov e, the\nfollowing information would be written to the journal:\nJournalTxB I[v2] B[v2] TxE\nThe data block Db, previously written to the log, would instead be\nwritten to the ﬁle system proper, avoiding the extra write; giv en that most\nI/O trafﬁc to the disk is data, not writing data twice substant ially reduces\nthe I/O load of journaling. The modiﬁcation does raise an interesti ng\nquestion, though: when should we write data blocks to disk?\nLet’s again consider our example append of a ﬁle to understand the\nproblem better. The update consists of three blocks: I[v2], B[v2 ], and\nDb. The ﬁrst two are both metadata and will be logged and then che ck-\npointed; the latter will only be written once to the ﬁle system. W hen\nshould we write Db to disk? Does it matter?\nAs it turns out, the ordering of the data write does matter for metad ata-\nonly journaling. For example, what if we write Db to disk after the trans-\naction (containing I[v2] and B[v2]) completes? Unfortunately, this ap-\nproach has a problem: the ﬁle system is consistent but I[v2] may e nd up\npointing to garbage data. Speciﬁcally, consider the case where I[v2] and\nB[v2] are written but Db did not make it to disk. The ﬁle system w ill then\ntry to recover. Because Db is notin the log, the ﬁle system will replay\nwrites to I[v2] and B[v2], and produce a consistent ﬁle system ( from the\nperspective of ﬁle-system metadata). However, I[v2] will be p ointing to\ngarbage data, i.e., at whatever was in the slot where Db was hea ded.\nTo ensure this situation does not arise, some ﬁle systems (e.g., L inux\next3) write data blocks (of regular ﬁles) to the disk ﬁrst, before related\nmetadata is written to disk. Speciﬁcally, the protocol is as follow s:\n1.Data write: Write data to ﬁnal location; wait for completion\n(the wait is optional; see below for details).\n2.Journal metadata write: Write the begin block and metadata to the\nlog; wait for writes to complete.\n3.Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for the write to complete; the transaction (i n-\ncluding data) is now committed .\n4.Checkpoint metadata: Write the contents of the metadata update\nto their ﬁnal locations within the ﬁle system.\n5.Free: Later, mark the transaction free in journal superblock.\nBy forcing the data write ﬁrst, a ﬁle system can guarantee that a pointer\nwill never point to garbage. Indeed, this rule of “write the poin ted-to\nobject before the object that points to it” is at the core of crash cons is-\ntency, and is exploited even further by other crash consistency schemes\n[GP94] (see below for details).\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCRASH CONSISTENCY : FSCK AND JOURNALING 15\nIn most systems, metadata journaling (akin to ordered journalin g of\next3) is more popular than full data journaling. For example, Win dows\nNTFS and SGI’s XFS both use some form of metadata journaling. Linux\next3 gives you the option of choosing either data, ordered, or unordere d\nmodes (in unordered mode, data can be written at any time). All of t hese\nmodes keep metadata consistent; they vary in their semantics f or data.\nFinally, note that forcing the data write to complete (Step 1) bef ore is-\nsuing writes to the journal (Step 2) is not required for correctnes s, as indi-\ncated in the protocol above. Speciﬁcally, it would be ﬁne to concurre ntly\nissue writes to data, the transaction-begin block, and journal ed metadata;\nthe only real requirement is that Steps 1 and 2 complete before the issuing\nof the journal commit block (Step 3).\nTricky Case: Block Reuse\nThere are some interesting corner cases that make journaling mor e tricky,\nand thus are worth discussing. A number of them revolve around bloc k\nreuse; as Stephen Tweedie (one of the main forces behind ext3) sai d:\n“What’s the hideous part of the entire system? ... It’s deletin g ﬁles.\nEverything to do with delete is hairy. Everything to do with d elete...\nyou have nightmares around what happens if blocks get deleted a nd\nthen reallocated.” [T00]\nThe particular example Tweedie gives is as follows. Suppose you ar e\nusing some form of metadata journaling (and thus data blocks for ﬁle s\narenotjournaled). Let’s say you have a directory called foo. The user\nadds an entry to foo (say by creating a ﬁle), and thus the contents of\nfoo (because directories are considered metadata) are written to the log;\nassume the location of the foo directory data is block 1000. The log thus\ncontains something like this:\nJournalTxB\nid=1I[foo]\nptr:1000D[foo]\n[final addr:1000]TxE\nid=1\nAt this point, the user deletes everything in the directory and the di-\nrectory itself, freeing up block 1000 for reuse. Finally, the us er creates a\nnew ﬁle (say foobar ), which ends up reusing the same block (1000) that\nused to belong to foo. The inode of foobar is committed to disk, as is\nits data; note, however, because metadata journaling is in use, only the\ninode offoobar is committed to the journal; the newly-written data in\nblock 1000 in the ﬁle foobar isnotjournaled.JournalTxB\nid=1I[foo]\nptr:1000D[foo]\n[final addr:1000]TxE\nid=1TxB\nid=2I[foobar]\nptr:1000TxE\nid=2\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 C RASH CONSISTENCY : FSCK AND JOURNALING\nJournal File System\nTxB Contents TxE Metadata Data\n(metadata) (data)\nissue issue issue\ncomplete\ncomplete\ncomplete\nissue\ncomplete\nissue issue\ncomplete\ncomplete\nFigure 42.1: Data Journaling Timeline\nNow assume a crash occurs and all of this information is still in the\nlog. During replay, the recovery process simply replays everyt hing in\nthe log, including the write of directory data in block 1000; the r eplay\nthus overwrites the user data of current ﬁle foobar with old directory\ncontents! Clearly this is not a correct recovery action, and certa inly it will\nbe a surprise to the user when reading the ﬁle foobar .\nThere are a number of solutions to this problem. One could, for ex-\nample, never reuse blocks until the delete of said blocks is chec kpointed\nout of the journal. What Linux ext3 does instead is to add a new type\nof record to the journal, known as a revoke record. In the case above,\ndeleting the directory would cause a revoke record to be written t o the\njournal. When replaying the journal, the system ﬁrst scans for s uch re-\nvoke records; any such revoked data is never replayed, thus avoid ing the\nproblem mentioned above.\nWrapping Up Journaling: A Timeline\nBefore ending our discussion of journaling, we summarize the protoc ols\nwe have discussed with timelines depicting each of them. Figu re 42.1\nshows the protocol when journaling data and metadata, whereas Fig ure\n42.2 shows the protocol when journaling only metadata.\nIn each ﬁgure, time increases in the downward direction, and ea ch row\nin the ﬁgure shows the logical time that a write can be issued or mi ght\ncomplete. For example, in the data journaling protocol (Figure 42. 1), the\nwrites of the transaction begin block (TxB) and the contents of the trans-\naction can logically be issued at the same time, and thus can be c ompleted\nin any order; however, the write to the transaction end block (TxE ) must\nnot be issued until said previous writes complete. Similarly, th e check-\npointing writes to data and metadata blocks cannot begin until t he trans-\naction end block has committed. Horizontal dashed lines show where\nwrite-ordering requirements must be obeyed.\nA similar timeline is shown for the metadata journaling protocol. N ote\nthat the data write can logically be issued at the same time as t he writes\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCRASH CONSISTENCY : FSCK AND JOURNALING 17\nJournal File System\nTxB Contents TxE Metadata Data\n(metadata)\nissue issue issue\ncomplete\ncomplete\ncomplete\nissue\ncomplete\nissue\ncomplete\nFigure 42.2: Metadata Journaling Timeline\nto the transaction begin and the contents of the journal; however, it must\nbe issued and complete before the transaction end has been issue d.\nFinally, note that the time of completion marked for each write in t he\ntimelines is arbitrary. In a real system, completion time is de termined by\nthe I/O subsystem, which may reorder writes to improve performa nce.\nThe only guarantees about ordering that we have are those that mus t\nbe enforced for protocol correctness (and are shown via the horizontal\ndashed lines in the ﬁgures).\n42.4 Solution #3: Other Approaches\nWe’ve thus far described two options in keeping ﬁle system metad ata\nconsistent: a lazy approach based on fsck , and a more active approach\nknown as journaling. However, these are not the only two approaches .\nOne such approach, known as Soft Updates [GP94], was introduced by\nGanger and Patt. This approach carefully orders all writes to t he ﬁle sys-\ntem to ensure that the on-disk structures are never left in an i nconsis-\ntent state. For example, by writing a pointed-to data block to di skbefore\nthe inode that points to it, we can ensure that the inode never poin ts to\ngarbage; similar rules can be derived for all the structures of the ﬁle sys-\ntem. Implementing Soft Updates can be a challenge, however; whe reas\nthe journaling layer described above can be implemented with r elatively\nlittle knowledge of the exact ﬁle system structures, Soft Update s requires\nintricate knowledge of each ﬁle system data structure and thus adds a fair\namount of complexity to the system.\nAnother approach is known as copy-on-write (yes, COW ), and is used\nin a number of popular ﬁle systems, including Sun’s ZFS [B07]. Thi s tech-\nnique never overwrites ﬁles or directories in place; rather, it places new\nupdates to previously unused locations on disk. After a number of u p-\ndates are completed, COW ﬁle systems ﬂip the root structure of the ﬁle\nsystem to include pointers to the newly updated structures. D oing so\nmakes keeping the ﬁle system consistent straightforward. We’l l be learn-\ning more about this technique when we discuss the log-structure d ﬁle\nsystem (LFS) in a future chapter; LFS is an early example of a COW .\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n18 C RASH CONSISTENCY : FSCK AND JOURNALING\nAnother approach is one we just developed here at Wisconsin. In thi s\ntechnique, entitled backpointer-based consistency (orBBC ), no ordering\nis enforced between writes. To achieve consistency, an additi onal back\npointer is added to every block in the system; for example, each data\nblock has a reference to the inode to which it belongs. When acces sing\na ﬁle, the ﬁle system can determine if the ﬁle is consistent by c hecking if\nthe forward pointer (e.g., the address in the inode or direct block ) points\nto a block that refers back to it. If so, everything must have saf ely reached\ndisk and thus the ﬁle is consistent; if not, the ﬁle is inconsiste nt, and an\nerror is returned. By adding back pointers to the ﬁle system, a n ew form\nof lazy crash consistency can be attained [C+12].\nFinally, we also have explored techniques to reduce the numbe r of\ntimes a journal protocol has to wait for disk writes to complete. Ent itled\noptimistic crash consistency [C+13], this new approach issues as many\nwrites to disk as possible by using a generalized form of the transaction\nchecksum [P+05], and includes a few other techniques to detect incon-\nsistencies should they arise. For some workloads, these optimisti c tech-\nniques can improve performance by an order of magnitude. However, to\ntruly function well, a slightly different disk interface is r equired [C+13].\n42.5 Summary\nWe have introduced the problem of crash consistency, and discuss ed\nvarious approaches to attacking this problem. The older approach of\nbuilding a ﬁle system checker works but is likely too slow to recov er on\nmodern systems. Thus, many ﬁle systems now use journaling. Journ aling\nreduces recovery time from O(size-of-the-disk-volume) to O(si ze-of-the-\nlog), thus speeding recovery substantially after a crash and r estart. For\nthis reason, many modern ﬁle systems use journaling. We have als o seen\nthat journaling can come in many different forms; the most commonly\nused is ordered metadata journaling, which reduces the amount of trafﬁc\nto the journal while still preserving reasonable consistency g uarantees for\nboth ﬁle system metadata and user data. In the end, strong guara ntees\non user data are probably one of the most important things to provide;\noddly enough, as recent research has shown, this area remains a w ork in\nprogress [P+14].\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCRASH CONSISTENCY : FSCK AND JOURNALING 19\nReferences\n[B07] “ZFS: The Last Word in File Systems” by Jeff Bonwick and Bill Moor e. Available online:\nhttp://www.ostep.org/Citations/zfs_last.pdf .ZFS uses copy-on-write and journal-\ning, actually, as in some cases, logging writes to disk will perform better .\n[C+12] “Consistency Without Ordering” by Vijay Chidambaram, Tushar S harma, Andrea C.\nArpaci-Dusseau, Remzi H. Arpaci-Dusseau. FAST ’12, San Jose, Cal ifornia. A recent paper of\nours about a new form of crash consistency based on back pointers. Read it for the exciting details!\n[C+13] “Optimistic Crash Consistency” by Vijay Chidambaram, Thanu S. Pillai, Andrea C.\nArpaci-Dusseau, Remzi H. Arpaci-Dusseau . SOSP ’13, Nemacolin Woo dlands Resort, PA,\nNovember 2013. Our work on a more optimistic and higher performance journaling protocol. For\nworkloads that call fsync() a lot, performance can be greatly improved.\n[GP94] “Metadata Update Performance in File Systems” by Gregory R. G anger and Yale N.\nPatt. OSDI ’94. A clever paper about using careful ordering of writes as the main way to achie ve\nconsistency. Implemented later in BSD-based systems.\n[G+08] “SQCK: A Declarative File System Checker” by Haryadi S. Guna wi, Abhishek Ra-\njimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. OSDI ’08, San Diego, Califor-\nnia. Our own paper on a new and better way to build a ﬁle system checker using SQL queries. We also\nshow some problems with the existing checker, ﬁnding numerous bugs and odd behaviors, a direct result\nof the complexity of fsck .\n[H87] “Reimplementing the Cedar File System Using Logging and Group Commit” by Robert\nHagmann. SOSP ’87, Austin, Texas, November 1987. The ﬁrst work (that we know of) that applied\nwrite-ahead logging (a.k.a. journaling) to a ﬁle system.\n[M+13] “ffsck: The Fast File System Checker” by Ao Ma, Chris Dragga, Andre a C. Arpaci-\nDusseau, Remzi H. Arpaci-Dusseau. FAST ’13, San Jose, California , February 2013. A recent\npaper of ours detailing how to make fsck an order of magnitude faster. Some of the ide as have already\nbeen incorporated into the BSD ﬁle system checker [MK96] and are deployed tod ay.\n[MK96] “Fsck – The U NIX File System Check Program” by Marshall Kirk McKusick and T. J.\nKowalski. Revised in 1996. Describes the ﬁrst comprehensive ﬁle-system checking tool, the epony-\nmousfsck . Written by some of the same people who brought you FFS.\n[MJLF84] “A Fast File System for UNIX” by Marshall K. McKusick, William N. Joy, Sam J.\nLefﬂer, Robert S. Fabry. ACM Transactions on Computing Systems, Volume 2:3, August 1984.\nYou already know enough about FFS, right? But come on, it is OK to re-referenc e important papers.\n[P+14] “All File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent\nApplications” by Thanumalayan Sankaranarayana Pillai, Vijay Chidamb aram, Ramnatthan\nAlagappan, Samer Al-Kiswany, Andrea C. Arpaci-Dusseau, Remzi H. A rpaci-Dusseau. OSDI\n’14, Broomﬁeld, Colorado, October 2014. A paper in which we study what ﬁle systems guarantee\nafter crashes, and show that applications expect something different, leadi ng to all sorts of interesting\nproblems.\n[P+05] “IRON File Systems” by Vijayan Prabhakaran, Lakshmi N. Baira vasundaram, Nitin\nAgrawal, Haryadi S. Gunawi, Andrea C. Arpaci-Dusseau, Remzi H. A rpaci-Dusseau. SOSP\n’05, Brighton, England, October 2005. A paper mostly focused on studying how ﬁle systems react\nto disk failures. Towards the end, we introduce a transaction checksum to spe ed up logging, which was\neventually adopted into Linux ext4.\n[PAA05] “Analysis and Evolution of Journaling File Systems” by Vij ayan Prabhakaran, Andrea\nC. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. USENIX ’05, Anaheim, California, April 2005.\nAn early paper we wrote analyzing how journaling ﬁle systems work.\n[R+11] “Coerced Cache Eviction and Discreet-Mode Journaling” by Abhishek R ajimwale, Vijay\nChidambaram, Deepak Ramamurthi, Andrea C. Arpaci-Dusseau, Remz i H. Arpaci-Dusseau.\nDSN ’11, Hong Kong, China, June 2011. Our own paper on the problem of disks that buffer writes in\na memory cache instead of forcing them to disk, even when explicitly told not to do that! Our solution\nto overcome this problem: if you want Ato be written to disk before B, ﬁrst write A, then send a lot of\n“dummy” writes to disk, hopefully causing Ato be forced to disk to make room for them in the cache. A\nneat if impractical solution.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n20 C RASH CONSISTENCY : FSCK AND JOURNALING\n[T98] “Journaling the Linux ext2fs File System” by Stephen C. Tweedie. The Fourth Annual\nLinux Expo, May 1998. Tweedie did much of the heavy lifting in adding journaling to the Linux e xt2\nﬁle system; the result, not surprisingly, is called ext3. Some nice d esign decisions include the strong\nfocus on backwards compatibility, e.g., you can just add a journaling ﬁle to an e xisting ext2 ﬁle system\nand then mount it as an ext3 ﬁle system.\n[T00] “EXT3, Journaling Filesystem” by Stephen Tweedie. Talk at the Ot tawa Linux Sympo-\nsium, July 2000. olstrans.sourceforge.net/release/OLS2000-ext 3/OLS2000-ext3.html A tran-\nscript of a talk given by Tweedie on ext3.\n[T01] “The Linux ext2 File System” by Theodore Ts’o, June, 2001.. Avail able online here:\nhttp://e2fsprogs.sourceforge.net/ext2.html .A simple Linux ﬁle system based on\nthe ideas found in FFS. For a while it was quite heavily used; now it is re ally just in the kernel as an\nexample of a simple ﬁle system.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nCRASH CONSISTENCY : FSCK AND JOURNALING 21\nHomework (Simulation)\nThis section introduces fsck.py , a simple simulator you can use to\nbetter understand how ﬁle system corruptions can be detected (a nd po-\ntentially repaired). Please see the associated README for det ails on how\nto run the simulator.\nQuestions\n1. First, run fsck.py -D ; this ﬂag turns off any corruption, and thus you can\nuse it to generate a random ﬁle system, and see if you can determin e which\nﬁles and directories are in there. So, go ahead and do that! Use the-pﬂag\nto see if you were right. Try this for a few different randomly- generated ﬁle\nsystems by setting the seed ( -s) to different values, like 1, 2, and 3.\n2. Now, let’s introduce a corruption. Run fsck.py -S 1 to start. Can you\nsee what inconsistency is introduced? How would you ﬁx it in a re al ﬁle\nsystem repair tool? Use -cto check if you were right.\n3. Change the seed to -S 3 or-S 19 ; which inconsistency do you see? Use\n-cto check your answer. What is different in these two cases?\n4. Change the seed to -S 5 ; which inconsistency do you see? How hard\nwould it be to ﬁx this problem in an automatic way? Use -cto check\nyour answer. Then, introduce a similar inconsistency with -S 38 ; is this\nharder/possible to detect? Finally, use -S 642 ; is this inconsistency de-\ntectable? If so, how would you ﬁx the ﬁle system?\n5. Change the seed to -S 6 or-S 13 ; which inconsistency do you see? Use -c\nto check your answer. What is the difference across these two c ases? What\nshould the repair tool do when encountering such a situation?\n6. Change the seed to -S 9 ; which inconsistency do you see? Use -cto check\nyour answer. Which piece of information should a check-and-rep air tool\ntrust in this case?\n7. Change the seed to -S 15 ; which inconsistency do you see? Use -cto check\nyour answer. What can a repair tool do in this case? If no repair is possible,\nhow much data is lost?\n8. Change the seed to -S 10 ; which inconsistency do you see? Use -cto check\nyour answer. Is there redundancy in the ﬁle system structure here that can\nhelp a repair?\n9. Change the seed to -S 16 and-S 20 ; which inconsistency do you see? Use\n-cto check your answer. How should the repair tool ﬁx the problem?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",55413
49-43. Log-structured File System LFS.pdf,49-43. Log-structured File System LFS,"43\nLog-structured File Systems\nIn the early 90’s, a group at Berkeley led by Professor John Ousterh out\nand graduate student Mendel Rosenblum developed a new ﬁle syst em\nknown as the log-structured ﬁle system [RO91]. Their motivation to do\nso was based on the following observations:\n•System memories are growing : As memory gets bigger, more data\ncan be cached in memory. As more data is cached, disk trafﬁc in-\ncreasingly consists of writes, as reads are serviced by the cac he.\nThus, ﬁle system performance is largely determined by its wri te\nperformance.\n•There is a large gap between random I/O performance and se-\nquential I/O performance : Hard-drive transfer bandwidth has in-\ncreased a great deal over the years [P98]; as more bits are packe d\ninto the surface of a drive, the bandwidth when accessing said bits\nincreases. Seek and rotational delay costs, however, have decrea sed\nslowly; it is challenging to make cheap and small motors spin the\nplatters faster or move the disk arm more quickly. Thus, if you are\nable to use disks in a sequential manner, you gain a sizeable pe rfor-\nmance advantage over approaches that cause seeks and rotations.\n•Existing ﬁle systems perform poorly on many common workload s:\nFor example, FFS [MJLF84] would perform a large number of writes\nto create a new ﬁle of size one block: one for a new inode, one to\nupdate the inode bitmap, one to the directory data block that the\nﬁle is in, one to the directory inode to update it, one to the new dat a\nblock that is a part of the new ﬁle, and one to the data bitmap to\nmark the data block as allocated. Thus, although FFS places all of\nthese blocks within the same block group, FFS incurs many short\nseeks and subsequent rotational delays and thus performance fa lls\nfar short of peak sequential bandwidth.\n•File systems are not RAID-aware : For example, both RAID-4 and\nRAID-5 have the small-write problem where a logical write to a\nsingle block causes 4 physical I/Os to take place. Existing ﬁl e sys-\ntems do not try to avoid this worst-case RAID writing behavior.\n1\n2 LOG-STRUCTURED FILESYSTEMS\nTIP: DETAILS MATTER\nAll interesting systems are comprised of a few general ideas an d a num-\nber of details. Sometimes, when you are learning about these syste ms,\nyou think to yourself “Oh, I get the general idea; the rest is jus t details,”\nand you use this to only half-learn how things really work. Don’t do t his!\nMany times, the details are critical. As we’ll see with LFS, the general idea\nis easy to understand, but to really build a working system, you have to\nthink through allof the tricky cases.\nAn ideal ﬁle system would thus focus on write performance, and try\nto make use of the sequential bandwidth of the disk. Further, it would\nperform well on common workloads that not only write out data but also\nupdate on-disk metadata structures frequently. Finally, it would work\nwell on RAIDs as well as single disks.\nThe new type of ﬁle system Rosenblum and Ousterhout introduced\nwas called LFS, short for the Log-structured File System . When writ-\ning to disk, LFS ﬁrst buffers all updates (including metadat a!) in an in-\nmemory segment ; when the segment is full, it is written to disk in one\nlong, sequential transfer to an unused part of the disk. LFS nev er over-\nwrites existing data, but rather always writes segments to free locations.\nBecause segments are large, the disk (or RAID) is used efﬁcien tly, and\nperformance of the ﬁle system approaches its zenith.\nTHECRUX:\nHOWTOMAKE ALLWRITES SEQUENTIAL WRITES ?\nHow can a ﬁle system transform all writes into sequential write s? For\nreads, this task is impossible, as the desired block to be read m ay be any-\nwhere on disk. For writes, however, the ﬁle system always has a ch oice,\nand it is exactly this choice we hope to exploit.\n43.1 Writing To Disk Sequentially\nWe thus have our ﬁrst challenge: how do we transform all updates t o\nﬁle-system state into a series of sequential writes to disk? T o understand\nthis better, let’s use a simple example. Imagine we are writin g a data block\nDto a ﬁle. Writing the data block to disk might result in the follow ing\non-disk layout, with Dwritten at disk address A0:\nD\nA0\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOG-STRUCTURED FILESYSTEMS 3\nHowever, when a user writes a data block, it is not only data that ge ts\nwritten to disk; there is also other metadata that needs to be updated.\nIn this case, let’s also write the inode (I) of the ﬁle to disk, and have it\npoint to the data block D. When written to disk, the data block and inode\nwould look something like this (note that the inode looks as big as the\ndata block, which generally isn’t the case; in most systems, dat a blocks\nare 4 KB in size, whereas an inode is much smaller, around 128 byt es):\nD\nA0Iblk[0]:A0\nThis basic idea, of simply writing all updates (such as data bl ocks,\ninodes, etc.) to the disk sequentially, sits at the heart of LFS. If you un-\nderstand this, you get the basic idea. But as with all complicat ed systems,\nthe devil is in the details.\n43.2 Writing Sequentially And Effectively\nUnfortunately, writing to disk sequentially is not (alone) enou gh to\nguarantee efﬁcient writes. For example, imagine if we wrote a s ingle\nblock to address A, at time T. We then wait a little while, and write to\nthe disk at address A+ 1 (the next block address in sequential order),\nbut at time T+δ. In-between the ﬁrst and second writes, unfortunately,\nthe disk has rotated; when you issue the second write, it will thu s wait\nfor most of a rotation before being committed (speciﬁcally, if the rot ation\ntakes time Trotation , the disk will wait Trotation−δbefore it can commit\nthe second write to the disk surface). And thus you can hopefully see\nthat simply writing to disk in sequential order is not enough to a chieve\npeak performance; rather, you must issue a large number of contiguous\nwrites (or one large write) to the drive in order to achieve good wri te\nperformance.\nTo achieve this end, LFS uses an ancient technique known as write\nbuffering1. Before writing to the disk, LFS keeps track of updates in\nmemory; when it has received a sufﬁcient number of updates, it w rites\nthem to disk all at once, thus ensuring efﬁcient use of the disk.\nThe large chunk of updates LFS writes at one time is referred to b y\nthe name of a segment . Although this term is over-used in computer\nsystems, here it just means a large-ish chunk which LFS uses t o group\nwrites. Thus, when writing to disk, LFS buffers updates in an in-memory\n1Indeed, it is hard to ﬁnd a good citation for this idea, since it was like ly invented by many\nand very early on in the history of computing. For a study of the beneﬁt s of write buffering,\nsee Solworth and Orji [SO90]; to learn about its potential harms, see Mogul [M94].\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 LOG-STRUCTURED FILESYSTEMS\nsegment, and then writes the segment all at once to the disk. As long as\nthe segment is large enough, these writes will be efﬁcient.\nHere is an example, in which LFS buffers two sets of updates int o a\nsmall segment; actual segments are larger (a few MB). The ﬁrs t update is\nof four block writes to ﬁle j; the second is one block being added to ﬁle k.\nLFS then commits the entire segment of seven blocks to disk at once . The\nresulting on-disk layout of these blocks is as follows:\nD[j,0]\nA0D[j,1]\nA1D[j,2]\nA2D[j,3]\nA3blk[0]:A0\nblk[1]:A1\nblk[2]:A2\nblk[3]:A3\nInode[j]D[k,0]\nA5blk[0]:A5\nInode[k]\n43.3 How Much To Buffer?\nThis raises the following question: how many updates should LFS\nbuffer before writing to disk? The answer, of course, depends on t he disk\nitself, speciﬁcally how high the positioning overhead is in compa rison to\nthe transfer rate; see the FFS chapter for a similar analysis.\nFor example, assume that positioning (i.e., rotation and seek over -\nheads) before each write takes roughly Tposition seconds. Assume further\nthat the disk transfer rate is Rpeak MB/s. How much should LFS buffer\nbefore writing when running on such a disk?\nThe way to think about this is that every time you write, you pay a\nﬁxed overhead of the positioning cost. Thus, how much do you have\nto write in order to amortize that cost? The more you write, the better\n(obviously), and the closer you get to achieving peak bandwidth.\nTo obtain a concrete answer, let’s assume we are writing out DMB.\nThe time to write out this chunk of data ( Twrite ) is the positioning time\nTposition plus the time to transfer D(D\nRpeak), or:\nTwrite=Tposition+D\nRpeak(43.1)\nAnd thus the effective rate of writing ( Reffective ), which is just the\namount of data written divided by the total time to write it, is:\nReffective =D\nTwrite=D\nTposition+D\nRpeak. (43.2)\nWhat we’re interested in is getting the effective rate ( Reffective ) close\nto the peak rate. Speciﬁcally, we want the effective rate to be some fraction\nFof the peak rate, where 0< F <1(a typical Fmight be 0.9, or 90% of\nthe peak rate). In mathematical form, this means we want Reffective =\nF×Rpeak.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOG-STRUCTURED FILESYSTEMS 5\nAt this point, we can solve for D:\nReffective =D\nTposition+D\nRpeak=F×Rpeak (43.3)\nD=F×Rpeak×(Tposition+D\nRpeak) (43.4)\nD= (F×Rpeak×Tposition)+(F×Rpeak×D\nRpeak) (43.5)\nD=F\n1−F×Rpeak×Tposition (43.6)\nLet’s do an example, with a disk with a positioning time of 10 mil-\nliseconds and peak transfer rate of 100 MB/s; assume we want an e f-\nfective bandwidth of 90% of peak ( F= 0.9). In this case, D=0.9\n0.1×\n100MB/s×0.01seconds = 9MB . Try some different values to see\nhow much we need to buffer in order to approach peak bandwidth. How\nmuch is needed to reach 95% of peak? 99%?\n43.4 Problem: Finding Inodes\nTo understand how we ﬁnd an inode in LFS, let us brieﬂy review how\nto ﬁnd an inode in a typical U NIXﬁle system. In a typical ﬁle system such\nas FFS, or even the old U NIX ﬁle system, ﬁnding inodes is easy, because\nthey are organized in an array and placed on disk at ﬁxed locations .\nFor example, the old U NIXﬁle system keeps all inodes at a ﬁxed por-\ntion of the disk. Thus, given an inode number and the start addres s, to\nﬁnd a particular inode, you can calculate its exact disk addres s simply by\nmultiplying the inode number by the size of an inode, and adding t hat\nto the start address of the on-disk array; array-based indexin g, given an\ninode number, is fast and straightforward.\nFinding an inode given an inode number in FFS is only slightly more\ncomplicated, because FFS splits up the inode table into chunks and places\na group of inodes within each cylinder group. Thus, one must know how\nbig each chunk of inodes is and the start addresses of each. After that, the\ncalculations are similar and also easy.\nIn LFS, life is more difﬁcult. Why? Well, we’ve managed to scatte r the\ninodes all throughout the disk! Worse, we never overwrite in place , and\nthus the latest version of an inode (i.e., the one we want) keeps mov ing.\n43.5 Solution Through Indirection: The Inode Map\nTo remedy this, the designers of LFS introduced a level of indirection\nbetween inode numbers and the inodes through a data structure ca lled\ntheinode map (imap) . The imap is a structure that takes an inode number\nas input and produces the disk address of the most recent version of the\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 LOG-STRUCTURED FILESYSTEMS\nTIP: USEA L EVEL OFINDIRECTION\nPeople often say that the solution to all problems in Computer Scienc e is\nsimply a level of indirection . This is clearly not true; it is just the solution\ntomost problems (yes, this is still too strong of a comment, but you get the\npoint). You certainly can think of every virtualization we have s tudied,\ne.g., virtual memory, or the notion of a ﬁle, as simply a level of indi rection.\nAnd certainly the inode map in LFS is a virtualization of inode num bers.\nHopefully you can see the great power of indirection in these examp les,\nallowing us to freely move structures around (such as pages in th e VM\nexample, or inodes in LFS) without having to change every referen ce to\nthem. Of course, indirection can have a downside too: extra overhead. So\nnext time you have a problem, try solving it with indirection, but make\nsure to think about the overheads of doing so ﬁrst. As Wheeler famou sly\nsaid, “All problems in computer science can be solved by another l evel of\nindirection, except of course for the problem of too many indirection s.”\ninode. Thus, you can imagine it would often be implemented as a sim ple\narray , with 4 bytes (a disk pointer) per entry. Any time an inode is wri tten\nto disk, the imap is updated with its new location.\nThe imap, unfortunately, needs to be kept persistent (i.e., w ritten to\ndisk); doing so allows LFS to keep track of the locations of inodes acr oss\ncrashes, and thus operate as desired. Thus, a question: where s hould the\nimap reside on disk?\nIt could live on a ﬁxed part of the disk, of course. Unfortunately, as it\ngets updated frequently, this would then require updates to ﬁ le structures\nto be followed by writes to the imap, and hence performance would s uffer\n(i.e., there would be more disk seeks, between each update and t he ﬁxed\nlocation of the imap).\nInstead, LFS places chunks of the inode map right next to where i t is\nwriting all of the other new information. Thus, when appending a d ata\nblock to a ﬁle k, LFS actually writes the new data block, its inode, and a\npiece of the inode map all together onto the disk, as follows:\nD\nA0I[k]blk[0]:A0\nA1imapmap[k]:A1\nIn this picture, the piece of the imap array stored in the block ma rked\nimap tells LFS that the inode kis at disk address A1; this inode, in turn,\ntells LFS that its data block Dis at address A0.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOG-STRUCTURED FILESYSTEMS 7\n43.6 Completing The Solution: The Checkpoint Region\nThe clever reader (that’s you, right?) might have noticed a probl em\nhere. How do we ﬁnd the inode map, now that pieces of it are also now\nspread across the disk? In the end, there is no magic: the ﬁle sy stem must\nhave some ﬁxed and known location on disk to begin a ﬁle lookup.\nLFS has just such a ﬁxed place on disk for this, known as the check-\npoint region (CR) . The checkpoint region contains pointers to (i.e., ad-\ndresses of) the latest pieces of the inode map, and thus the inode m ap\npieces can be found by reading the CR ﬁrst. Note the checkpoint re gion\nis only updated periodically (say every 30 seconds or so), and thus perfor-\nmance is not ill-affected. Thus, the overall structure of the on- disk layout\ncontains a checkpoint region (which points to the latest pieces of the in-\node map); the inode map pieces each contain addresses of the inodes ; the\ninodes point to ﬁles (and directories) just like typical U NIXﬁle systems.\nHere is an example of the checkpoint region (note it is all the way a t\nthe beginning of the disk, at address 0), and a single imap chun k, inode,\nand data block. A real ﬁle system would of course have a much bigger\nCR (indeed, it would have two, as we’ll come to understand later), many\nimap chunks, and of course many more inodes, data blocks, etc.\nimap\n[k...k+N]:\nA2\nCR\n0D\nA0I[k]blk[0]:A0\nA1imapmap[k]:A1\nA2\n43.7 Reading A File From Disk: A Recap\nTo make sure you understand how LFS works, let us now walk through\nwhat must happen to read a ﬁle from disk. Assume we have nothing i n\nmemory to begin. The ﬁrst on-disk data structure we must read is the\ncheckpoint region. The checkpoint region contains pointers (i.e. , disk ad-\ndresses) to the entire inode map, and thus LFS then reads in the entire in-\node map and caches it in memory. After this point, when given an in ode\nnumber of a ﬁle, LFS simply looks up the inode-number to inode-disk -\naddress mapping in the imap, and reads in the most recent versi on of the\ninode. To read a block from the ﬁle, at this point, LFS proceeds exac tly\nas a typical U NIXﬁle system, by using direct pointers or indirect pointers\nor doubly-indirect pointers as need be. In the common case, LFS shou ld\nperform the same number of I/Os as a typical ﬁle system when read ing a\nﬁle from disk; the entire imap is cached and thus the extra work L FS does\nduring a read is to look up the inode’s address in the imap.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 LOG-STRUCTURED FILESYSTEMS\n43.8 What About Directories?\nThus far, we’ve simpliﬁed our discussion a bit by only considering in-\nodes and data blocks. However, to access a ﬁle in a ﬁle system (suc h as\n/home/remzi/foo , one of our favorite fake ﬁle names), some directo-\nries must be accessed too. So how does LFS store directory data?\nFortunately, directory structure is basically identical to c lassic U NIX\nﬁle systems, in that a directory is just a collection of (name, inod e number)\nmappings. For example, when creating a ﬁle on disk, LFS must both write\na new inode, some data, as well as the directory data and its inode t hat\nrefer to this ﬁle. Remember that LFS will do so sequentially on the disk\n(after buffering the updates for some time). Thus, creating a ﬁ lefoo in a\ndirectory would lead to the following new structures on disk:\nD[k]\nA0I[k]blk[0]:A0\nA1(foo, k)\nD[dir]\nA2I[dir]blk[0]:A2\nA3map[k]:A1\nmap[dir]:A3\nimap\nThe piece of the inode map contains the information for the location of\nboth the directory ﬁle diras well as the newly-created ﬁle f. Thus, when\naccessing ﬁle foo (with inode number k), you would ﬁrst look in the\ninode map (usually cached in memory) to ﬁnd the location of the inode\nof directory dir(A3); you then read the directory inode, which gives you\nthe location of the directory data ( A2); reading this data block gives you\nthe name-to-inode-number mapping of ( foo,k). You then consult the\ninode map again to ﬁnd the location of inode number k(A1), and ﬁnally\nread the desired data block at address A0.\nThere is one other serious problem in LFS that the inode map solves,\nknown as the recursive update problem [Z+12]. The problem arises\nin any ﬁle system that never updates in place (such as LFS), but rather\nmoves updates to new locations on the disk.\nSpeciﬁcally, whenever an inode is updated, its location on disk ch anges.\nIf we hadn’t been careful, this would have also entailed an upda te to\nthe directory that points to this ﬁle, which then would have mand ated\na change to the parent of that directory, and so on, all the way up t he ﬁle\nsystem tree.\nLFS cleverly avoids this problem with the inode map. Even though\nthe location of an inode may change, the change is never reﬂected i n the\ndirectory itself; rather, the imap structure is updated whil e the directory\nholds the same name-to-inode-number mapping. Thus, through ind irec-\ntion, LFS avoids the recursive update problem.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOG-STRUCTURED FILESYSTEMS 9\n43.9 A New Problem: Garbage Collection\nYou may have noticed another problem with LFS; it repeatedly write s\nthe latest version of a ﬁle (including its inode and data) to new l ocations\non disk. This process, while keeping writes efﬁcient, implies that LFS\nleaves old versions of ﬁle structures scattered throughout the di sk. We\n(rather unceremoniously) call these old versions garbage .\nFor example, let’s imagine the case where we have an existing ﬁl e re-\nferred to by inode number k, which points to a single data block D0.\nWe now update that block, generating both a new inode and a new data\nblock. The resulting on-disk layout of LFS would look something like t his\n(note we omit the imap and other structures for simplicity; a new c hunk\nof imap would also have to be written to disk to point to the new inod e):\nD0\nA0I[k]blk[0]:A0\n(both garbage)D0\nA4I[k]blk[0]:A4\nIn the diagram, you can see that both the inode and data block have\ntwo versions on disk, one old (the one on the left) and one current and\nthus live (the one on the right). By the simple act of (logically) updating\na data block, a number of new structures must be persisted by LFS, thus\nleaving old versions of said blocks on the disk.\nAs another example, imagine we instead append a block to that ori g-\ninal ﬁlek. In this case, a new version of the inode is generated, but the\nold data block is still pointed to by the inode. Thus, it is still li ve and very\nmuch part of the current ﬁle system:\nD0\nA0I[k]blk[0]:A0\n(garbage)D1\nA4I[k]blk[0]:A0\nblk[1]:A4\nSo what should we do with these older versions of inodes, data blocks,\nand so forth? One could keep those older versions around and allow\nusers to restore old ﬁle versions (for example, when they acciden tally\noverwrite or delete a ﬁle, it could be quite handy to do so); such a ﬁ le\nsystem is known as a versioning ﬁle system because it keeps track of the\ndifferent versions of a ﬁle.\nHowever, LFS instead keeps only the latest live version of a ﬁle; t hus\n(in the background), LFS must periodically ﬁnd these old dead ve rsions\nof ﬁle data, inodes, and other structures, and clean them; cleaning should\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 LOG-STRUCTURED FILESYSTEMS\nthus make blocks on disk free again for use in subsequent writes. Note\nthat the process of cleaning is a form of garbage collection , a technique\nthat arises in programming languages that automatically free unused mem-\nory for programs.\nEarlier we discussed segments as important as they are the mec hanism\nthat enables large writes to disk in LFS. As it turns out, they ar e also quite\nintegral to effective cleaning. Imagine what would happen if the LFS\ncleaner simply went through and freed single data blocks, inode s, etc.,\nduring cleaning. The result: a ﬁle system with some number of fr eeholes\nmixed between allocated space on disk. Write performance would d rop\nconsiderably, as LFS would not be able to ﬁnd a large contiguous reg ion\nto write to disk sequentially and with high performance.\nInstead, the LFS cleaner works on a segment-by-segment basis, thus\nclearing up large chunks of space for subsequent writing. The b asic clean-\ning process works as follows. Periodically, the LFS cleaner reads in a\nnumber of old (partially-used) segments, determines which bl ocks are\nlive within these segments, and then write out a new set of segme nts\nwith just the live blocks within them, freeing up the old ones for w riting.\nSpeciﬁcally, we expect the cleaner to read in Mexisting segments, com-\npact their contents into Nnew segments (where N < M ), and then write\ntheNsegments to disk in new locations. The old Msegments are then\nfreed and can be used by the ﬁle system for subsequent writes.\nWe are now left with two problems, however. The ﬁrst is mechanism :\nhow can LFS tell which blocks within a segment are live, and whic h are\ndead? The second is policy: how often should the cleaner run, and wh ich\nsegments should it pick to clean?\n43.10 Determining Block Liveness\nWe address the mechanism ﬁrst. Given a data block Dwithin an on-\ndisk segment S, LFS must be able to determine whether Dis live. To do\nso, LFS adds a little extra information to each segment that desc ribes each\nblock. Speciﬁcally, LFS includes, for each data block D, its inode number\n(which ﬁle it belongs to) and its offset (which block of the ﬁle this is). This\ninformation is recorded in a structure at the head of the segment k nown\nas the segment summary block .\nGiven this information, it is straightforward to determine whe ther a\nblock is live or dead. For a block Dlocated on disk at address A, look\nin the segment summary block and ﬁnd its inode number Nand offset\nT. Next, look in the imap to ﬁnd where Nlives and read Nfrom disk\n(perhaps it is already in memory, which is even better). Final ly, using\nthe offset T, look in the inode (or some indirect block) to see where the\ninode thinks the Tth block of this ﬁle is on disk. If it points exactl y to disk\naddressA, LFS can conclude that the block Dis live. If it points anywhere\nelse, LFS can conclude that Dis not in use (i.e., it is dead) and thus know\nthat this version is no longer needed. Here is a pseudocode summar y:\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOG-STRUCTURED FILESYSTEMS 11\n(N, T) = SegmentSummary[A];\ninode = Read(imap[N]);\nif (inode[T] == A)\n// block D is alive\nelse\n// block D is garbage\nHere is a diagram depicting the mechanism, in which the segme nt\nsummary block (marked SS) records that the data block at address A0\nis actually a part of ﬁle kat offset 0. By checking the imap for k, you can\nﬁnd the inode, and see that it does indeed point to that location.\nD\nA0I[k]blk[0]:A0\nA1imapmap[k]:A1\nssA0:\n(k,0)\nThere are some shortcuts LFS takes to make the process of determin ing\nliveness more efﬁcient. For example, when a ﬁle is truncated or d eleted,\nLFS increases its version number and records the new version number in\nthe imap. By also recording the version number in the on-disk seg ment,\nLFS can short circuit the longer check described above simply by compar-\ning the on-disk version number with a version number in the imap, thus\navoiding extra reads.\n43.11 A Policy Question: Which Blocks To Clean, And When?\nOn top of the mechanism described above, LFS must include a set of\npolicies to determine both when to clean and which blocks are wort h\ncleaning. Determining when to clean is easier; either period ically, dur-\ning idle time, or when you have to because the disk is full.\nDetermining which blocks to clean is more challenging, and has been\nthe subject of many research papers. In the original LFS paper [ RO91], the\nauthors describe an approach which tries to segregate hotand cold seg-\nments. A hot segment is one in which the contents are being freque ntly\nover-written; thus, for such a segment, the best policy is to wai t a long\ntime before cleaning it, as more and more blocks are getting over-w ritten\n(in new segments) and thus being freed for use. A cold segment, i n con-\ntrast, may have a few dead blocks but the rest of its contents are r elatively\nstable. Thus, the authors conclude that one should clean cold segm ents\nsooner and hot segments later, and develop a heuristic that does ex actly\nthat. However, as with most policies, this policy isn’t perfect; l ater ap-\nproaches show how to do better [MR+97].\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 LOG-STRUCTURED FILESYSTEMS\n43.12 Crash Recovery And The Log\nOne ﬁnal problem: what happens if the system crashes while LFS is\nwriting to disk? As you may recall in the previous chapter about j our-\nnaling, crashes during updates are tricky for ﬁle systems, an d thus some-\nthing LFS must consider as well.\nDuring normal operation, LFS buffers writes in a segment, and th en\n(when the segment is full, or when some amount of time has elapsed) ,\nwrites the segment to disk. LFS organizes these writes in a log, i.e., the\ncheckpoint region points to a head and tail segment, and each seg ment\npoints to the next segment to be written. LFS also periodically updates the\ncheckpoint region. Crashes could clearly happen during either of these\noperations (write to a segment, write to the CR). So how does LFS han dle\ncrashes during writes to these structures?\nLet’s cover the second case ﬁrst. To ensure that the CR update hap pens\natomically, LFS actually keeps two CRs, one at either end of the d isk, and\nwrites to them alternately. LFS also implements a careful pr otocol when\nupdating the CR with the latest pointers to the inode map and othe r infor-\nmation; speciﬁcally, it ﬁrst writes out a header (with timesta mp), then the\nbody of the CR, and then ﬁnally one last block (also with a timestam p). If\nthe system crashes during a CR update, LFS can detect this by s eeing an\ninconsistent pair of timestamps. LFS will always choose to use th e most\nrecent CR that has consistent timestamps, and thus consistent update of\nthe CR is achieved.\nLet’s now address the ﬁrst case. Because LFS writes the CR every 30\nseconds or so, the last consistent snapshot of the ﬁle system may be q uite\nold. Thus, upon reboot, LFS can easily recover by simply reading in the\ncheckpoint region, the imap pieces it points to, and subsequent ﬁ les and\ndirectories; however, the last many seconds of updates would be los t.\nTo improve upon this, LFS tries to rebuild many of those segments\nthrough a technique known as roll forward in the database community.\nThe basic idea is to start with the last checkpoint region, ﬁnd t he end of\nthe log (which is included in the CR), and then use that to read t hrough\nthe next segments and see if there are any valid updates withi n it. If there\nare, LFS updates the ﬁle system accordingly and thus recovers m uch of\nthe data and metadata written since the last checkpoint. See Ros enblum’s\naward-winning dissertation for details [R92].\n43.13 Summary\nLFS introduces a new approach to updating the disk. Instead of ove r-\nwriting ﬁles in places, LFS always writes to an unused portion of the\ndisk, and then later reclaims that old space through cleaning. This ap-\nproach, which in database systems is called shadow paging [L77] and in\nﬁle-system-speak is sometimes called copy-on-write , enables highly efﬁ-\ncient writing, as LFS can gather all updates into an in-memory segment\nand then write them out together sequentially.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOG-STRUCTURED FILESYSTEMS 13\nTIP: TURN FLAWS INTO VIRTUES\nWhenever your system has a fundamental ﬂaw, see if you can turn i t\naround into a feature or something useful. NetApp’s WAFL does this\nwith old ﬁle contents; by making old versions available, WAFL no l onger\nhas to worry about cleaning quite so often (though it does delete old ver-\nsions, eventually, in the background), and thus provides a cool fe ature\nand removes much of the LFS cleaning problem all in one wonderful\ntwist. Are there other examples of this in systems? Undoubtedly , but\nyou’ll have to think of them yourself, because this chapter is over with a\ncapital “O”. Over. Done. Kaput. We’re out. Peace!\nThe large writes that LFS generates are excellent for performa nce on\nmany different devices. On hard drives, large writes ensure that posi-\ntioning time is minimized; on parity-based RAIDs, such as RAID -4 and\nRAID-5, they avoid the small-write problem entirely. Recent r esearch\nhas even shown that large I/Os are required for high performance on\nFlash-based SSDs [H+17]; thus, perhaps surprisingly, LFS-sty le ﬁle sys-\ntems may be an excellent choice even for these new mediums.\nThe downside to this approach is that it generates garbage; old c opies\nof the data are scattered throughout the disk, and if one wants to r e-\nclaim such space for subsequent usage, one must clean old segmen ts pe-\nriodically. Cleaning became the focus of much controversy in LFS, a nd\nconcerns over cleaning costs [SS+95] perhaps limited LFS’s initial impact\non the ﬁeld. However, some modern commercial ﬁle systems, includi ng\nNetApp’s WAFL [HLM94], Sun’s ZFS [B07], and Linux btrfs [R+13], and\neven modern ﬂash-based SSDs [AD14], adopt a similar copy-on-write\napproach to writing to disk, and thus the intellectual legacy of LFS lives\non in these modern ﬁle systems. In particular, WAFL got around cle an-\ning problems by turning them into a feature; by providing old ver sions of\nthe ﬁle system via snapshots , users could access old ﬁles whenever they\ndeleted current ones accidentally.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 LOG-STRUCTURED FILESYSTEMS\nReferences\n[AD14] “Operating Systems: Three Easy Pieces” (Chapter: Flash-based So lid State Drives)\nby Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2014. A bit\ngauche to refer you to another chapter in this very book, but who are we to judge?\n[B07] “ZFS: The Last Word in File Systems” by Jeff Bonwick and Bill Moor e. Copy Available:\nhttp://www.ostep.org/Citations/zfs_last.pdf .Slides on ZFS; unfortunately, there\nis no great ZFS paper (yet). Maybe you will write one, so we can cite it here?\n[H+17] “The Unwritten Contract of of Solid State Drives” by Jun He, Su darsun Kannan, An-\ndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. EuroSys ’17, A pril 2017. Which unwritten\nrules one must follow to extract high performance from an SSD? Interestingl y, both request scale (large\nor parallel requests) and locality still matter, even on SSDs. The more things change ...\n[HLM94] “File System Design for an NFS File Server Appliance” by Da ve Hitz, James Lau,\nMichael Malcolm. USENIX Spring ’94. WAFL takes many ideas from LFS and RAID and puts it\ninto a high-speed NFS appliance for the multi-billion dollar storage company N etApp.\n[L77] “Physical Integrity in a Large Segmented Database” by R. Lori e. ACM Transactions on\nDatabases, Volume 2:1, 1977. The original idea of shadow paging is presented here.\n[MJLF84] “A Fast File System for UNIX” by Marshall K. McKusick, William N. Joy, Sam J.\nLefﬂer, Robert S. Fabry. ACM TOCS, Volume 2:3, August 1984. The original FFS paper; see the\nchapter on FFS for more details.\n[MR+97] “Improving the Performance of Log-structured File Systems wit h Adaptive Meth-\nods” by Jeanna Neefe Matthews, Drew Roselli, Adam M. Costello, Randol ph Y. Wang, Thomas\nE. Anderson. SOSP 1997, pages 238-251, October, Saint Malo, France. A more recent paper detail-\ning better policies for cleaning in LFS.\n[M94] “A Better Update Policy” by Jeffrey C. Mogul. USENIX ATC ’94, June 1 994. In this paper,\nMogul ﬁnds that read workloads can be harmed by buffering writes for too long and then sending them\nto the disk in a big burst. Thus, he recommends sending writes more fr equently and in smaller batches.\n[P98] “Hardware Technology Trends and Database Opportunities” by Dav id A. Patterson.\nACM SIGMOD ’98 Keynote, 1998. Available online here: http://www.cs.berkeley.edu/\n˜pattrsn/talks/keynote.html .A great set of slides on technology trends in computer sys-\ntems. Hopefully, Patterson will create another of these sometime soon.\n[R+13] “BTRFS: The Linux B-Tree Filesystem” by Ohad Rodeh, Josef Bacik, C hris Mason. ACM\nTransactions on Storage, Volume 9 Issue 3, August 2013. Finally, a good paper on BTRFS, a\nmodern take on copy-on-write ﬁle systems.\n[RO91] “Design and Implementation of the Log-structured File Syste m” by Mendel Rosen-\nblum and John Ousterhout. SOSP ’91, Paciﬁc Grove, CA, October 1991. The original SOSP\npaper about LFS, which has been cited by hundreds of other papers and insp ired many real systems.\n[R92] “Design and Implementation of the Log-structured File Syste m” by Mendel Rosenblum.\nhttp://www.eecs.berkeley.edu/Pubs/TechRpts/1992/CSD-92-696 .pdf. The award-winning dis-\nsertation about LFS, with many of the details missing from the paper.\n[SS+95] “File system logging versus clustering: a performance compa rison” by Margo Seltzer,\nKeith A. Smith, Hari Balakrishnan, Jacqueline Chang, Sara McMains, Venkata Padmanabhan.\nUSENIX 1995 Technical Conference, New Orleans, Louisiana, 1995. A paper that showed the LFS\nperformance sometimes has problems, particularly for workloads with many call s tofsync() (such as\ndatabase workloads). The paper was controversial at the time.\n[SO90] “Write-Only Disk Caches” by Jon A. Solworth, Cyril U. Orji. SIGMOD ’ 90, Atlantic\nCity, New Jersey, May 1990. An early study of write buffering and its beneﬁts. However, buffering\nfor too long can be harmful: see Mogul [M94] for details.\n[Z+12] “De-indirection for Flash-based SSDs with Nameless Writes” by Yiying Zhang, Leo\nPrasath Arulraj, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusse au. FAST ’13, San Jose,\nCalifornia, February 2013. Our paper on a new way to build ﬂash-based storage devices, to avoid\nredundant mappings in the ﬁle system and FTL. The idea is for the devic e to pick the physical location\nof a write, and return the address to the ﬁle system, which stores the mapping .\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nLOG-STRUCTURED FILESYSTEMS 15\nHomework (Simulation)\nThis section introduces lfs.py , a simple LFS simulator you can use\nto understand better how an LFS-based ﬁle system works. Read the\nREADME for details on how to run the simulator.\nQuestions\n1. Run./lfs.py -n 3 , perhaps varying the seed ( -s). Can you ﬁgure out\nwhich commands were run to generate the ﬁnal ﬁle system contents ? Can\nyou tell which order those commands were issued? Finally, can yo u deter-\nmine the liveness of each block in the ﬁnal ﬁle system state? Use -oto show\nwhich commands were run, and -cto show the liveness of the ﬁnal ﬁle sys-\ntem state. How much harder does the task become for you as you incr ease\nthe number of commands issued (i.e., change -n 3 to-n 5 )?\n2. If you ﬁnd the above painful, you can help yourself a little bi t by show-\ning the set of updates caused by each speciﬁc command. To do so, run\n./lfs.py -n 3 -i . Now see if it is easier to understand what each com-\nmand must have been. Change the random seed to get different comman ds\nto interpret (e.g., -s 1 ,-s 2 ,-s 3 , etc.).\n3. To further test your ability to ﬁgure out what updates are made to d isk by\neach command, run the following: ./lfs.py -o -F -s 100 (and per-\nhaps a few other random seeds). This just shows a set of commands a nd\ndoes NOT show you the ﬁnal state of the ﬁle system. Can you reaso n about\nwhat the ﬁnal state of the ﬁle system must be?\n4. Now see if you can determine which ﬁles and directories are l ive after a\nnumber of ﬁle and directory operations. Run tt ./lfs.py -n 20 -s\n1and then examine the ﬁnal ﬁle system state. Can you ﬁgure out which\npathnames are valid? Run tt ./lfs.py -n 20 -s 1 -c -v to see the\nresults. Run with -oto see if your answers match up given the series of\nrandom commands. Use different random seeds to get more problems.\n5. Now let’s issue some speciﬁc commands. First, let’s create a ﬁl e and write\nto it repeatedly. To do so, use the -Lﬂag, which lets you specify speciﬁc\ncommands to execute. Let’s create the ﬁle ”/foo” and write to it fo ur times:\n-L c,/foo:w,/foo,0,1:w,/foo,1,1:w,/foo,2,1:w,/foo,3, 1 -o .\nSee if you can determine the liveness of the ﬁnal ﬁle system sta te; use-cto\ncheck your answers.\n6. Now, let’s do the same thing, but with a single write operatio n instead of\nfour. Run ./lfs.py -o -L c,/foo:w,/foo,0,4 to create ﬁle ”/foo”\nand write 4 blocks with a single write operation. Compute the li veness\nagain, and check if you are right with -c. What is the main difference be-\ntween writing a ﬁle all at once (as we do here) versus doing it on e block at a\ntime (as above)? What does this tell you about the importance of b uffering\nupdates in main memory as the real LFS does?\n7. Let’s do another speciﬁc example. First, run the following: ./lfs.py -L\nc,/foo:w,/foo,0,1 . What does this set of commands do? Now, run\n./lfs.py -L c,/foo:w,/foo,7,1 . What does this set of commands\ndo? How are the two different? What can you tell about the size ﬁeld in\nthe inode from these two sets of commands?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 LOG-STRUCTURED FILESYSTEMS\n8. Now let’s look explicitly at ﬁle creation versus directory c reation. Run simu-\nlations./lfs.py -L c,/foo and./lfs.py -L d,/foo to create a ﬁle\nand then a directory. What is similar about these runs, and what i s different?\n9. The LFS simulator supports hard links as well. Run the followin g to study\nhow they work:\n./lfs.py -L c,/foo:l,/foo,/bar:l,/foo,/goo -o -i .\nWhat blocks are written out when a hard link is created? How is t his similar\nto just creating a new ﬁle, and how is it different? How does the reference\ncount ﬁeld change as links are created?\n10. LFS makes many different policy decisions. We do not explore many of\nthem here – perhaps something left for the future – but here is a simp le one\nwe do explore: the choice of inode number. First, run ./lfs.py -p c100\n-n 10 -o -a s to show the usual behavior with the ”sequential” alloca-\ntion policy, which tries to use free inode numbers nearest to zer o. Then,\nchange to a ”random” policy by running ./lfs.py -p c100 -n 10 -o\n-a r (the-p c100 ﬂag ensures 100 percent of the random operations are\nﬁle creations). What on-disk differences does a random poli cy versus a se-\nquential policy result in? What does this say about the importanc e of choos-\ning inode numbers in a real LFS?\n11. One last thing we’ve been assuming is that the LFS simulator al ways up-\ndates the checkpoint region after each update. In the real LFS , that isn’t the\ncase: it is updated periodically to avoid long seeks. Run ./lfs.py -N -i\n-o -s 1000 to see some operations and the intermediate and ﬁnal states of\nthe ﬁle system when the checkpoint region isn’t forced to dis k. What would\nhappen if the checkpoint region is never updated? What if it is updated pe-\nriodically? Could you ﬁgure out how to recover the ﬁle system to t he latest\nstate by rolling forward in the log?\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG",41423
50-44. Flash-based SSDs.pdf,50-44. Flash-based SSDs,"44\nFlash-based SSDs\nAfter decades of hard-disk drive dominance, a new form of persist ent\nstorage device has recently gained signiﬁcance in the world. G eneri-\ncally referred to as solid-state storage , such devices have no mechani-\ncal or moving parts like hard drives; rather, they are simply bu ilt out of\ntransistors, much like memory and processors. However, unlike ty pical\nrandom-access memory (e.g., DRAM), such a solid-state storage device\n(a.k.a., an SSD ) retains information despite power loss, and thus is an\nideal candidate for use in persistent storage of data.\nThe technology we’ll focus on is known as ﬂash (more speciﬁcally,\nNAND-based ﬂash ), which was created by Fujio Masuoka in the 1980s\n[M+14]. Flash, as we’ll see, has some unique properties. For exam ple, to\nwrite to a given chunk of it (i.e., a ﬂash page ), you ﬁrst have to erase a big-\nger chunk (i.e., a ﬂash block ), which can be quite expensive. In addition,\nwriting too often to a page will cause it to wear out . These two properties\nmake construction of a ﬂash-based SSD an interesting challenge:\nCRUX: HOWTOBUILD A F LASH -BASED SSD\nHow can we build a ﬂash-based SSD? How can we handle the expen-\nsive nature of erasing? How can we build a device that lasts a long time,\ngiven that repeated overwrite will wear the device out? Will th e march of\nprogress in technology ever cease? Or cease to amaze?\n44.1 Storing a Single Bit\nFlash chips are designed to store one or more bits in a single trans is-\ntor; the level of charge trapped within the transistor is mapped to a binary\nvalue. In a single-level cell (SLC ) ﬂash, only a single bit is stored within\na transistor (i.e., 1 or 0); with a multi-level cell (MLC ) ﬂash, two bits are\nencoded into different levels of charge, e.g., 00, 01, 10, and 1 1 are repre-\nsented by low, somewhat low, somewhat high, and high levels. Ther e is\neven triple-level cell (TLC ) ﬂash, which encodes 3 bits per cell. Overall,\nSLC chips achieve higher performance and are more expensive.\n1\n2 FLASH -BASED SSD S\nTIP: BECAREFUL WITHTERMINOLOGY\nYou may have noticed that some terms we have used many times before\n(blocks, pages) are being used within the context of a ﬂash, but i n slightly\ndifferent ways than before. New terms are not created to make you r life\nharder (although they may be doing just that), but arise becaus e there is\nno central authority where terminology decisions are made. What is a\nblock to you may be a page to someone else, and vice versa, dependin g\non the context. Your job is simple: to know the appropriate terms wit hin\neach domain, and use them such that people well-versed in the di scipline\ncan understand what you are talking about. It’s one of those times wh ere\nthe only solution is simple but sometimes painful: use your memory.\nOf course, there are many details as to exactly how such bit-lev el stor-\nage operates, down at the level of device physics. While beyond th e scope\nof this book, you can read more about it on your own [J10].\n44.2 From Bits to Banks/Planes\nAs they say in ancient Greece, storing a single bit (or a few) does not\na storage system make. Hence, ﬂash chips are organized into banks or\nplanes which consist of a large number of cells.\nA bank is accessed in two different sized units: blocks (sometimes\ncalled erase blocks ), which are typically of size 128 KB or 256 KB, and\npages , which are a few KB in size (e.g., 4KB). Within each bank there are\na large number of blocks; within each block, there are a large num ber of\npages. When thinking about ﬂash, you must remember this new ter mi-\nnology, which is different than the blocks we refer to in disks an d RAIDs\nand the pages we refer to in virtual memory.\nFigure 44.1 shows an example of a ﬂash plane with blocks and pages ;\nthere are three blocks, each containing four pages, in this simp le exam-\nple. We’ll see below why we distinguish between blocks and pages ; it\nturns out this distinction is critical for ﬂash operations such as reading\nand writing, and even more so for the overall performance of the dev ice.\nThe most important (and weird) thing you will learn is that to wri te to\na page within a block, you ﬁrst have to erase the entire block; thi s tricky\ndetail makes building a ﬂash-based SSD an interesting and worth while\nchallenge, and the subject of the second-half of the chapter.\n0 1 2 Block:\nPage:\nContent:000102030405060708091011\nFigure 44.1: A Simple Flash Chip: Pages Within Blocks\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 3\n44.3 Basic Flash Operations\nGiven this ﬂash organization, there are three low-level operati ons that\na ﬂash chip supports. The read command is used to read a page from the\nﬂash; erase and program are used in tandem to write. The details:\n•Read (a page) : A client of the ﬂash chip can read any page (e.g.,\n2KB or 4KB), simply by specifying the read command and appro-\npriate page number to the device. This operation is typically q uite\nfast, 10s of microseconds or so, regardless of location on the device,\nand (more or less) regardless of the location of the previous request\n(quite unlike a disk). Being able to access any location uniform ly\nquickly means the device is a random access device.\n•Erase (a block): Before writing to a page within a ﬂash, the nature\nof the device requires that you ﬁrst erase the entire block the page\nlies within. Erase, importantly, destroys the contents of the bl ock\n(by setting each bit to the value 1); therefore, you must be sure that\nany data you care about in the block has been copied elsewhere\n(to memory, or perhaps to another ﬂash block) before executing the\nerase. The erase command is quite expensive, taking a few mill isec-\nonds to complete. Once ﬁnished, the entire block is reset and eac h\npage is ready to be programmed.\n•Program (a page): Once a block has been erased, the program com-\nmand can be used to change some of the 1’s within a page to 0’s,\nand write the desired contents of a page to the ﬂash. Program-\nming a page is less expensive than erasing a block, but more costl y\nthan reading a page, usually taking around 100s of microseconds\non modern ﬂash chips.\nOne way to think about ﬂash chips is that each page has a state as so-\nciated with it. Pages start in an INVALID state. By erasing the block that\na page resides within, you set the state of the page (and all page s within\nthat block) to ERASED , which resets the content of each page in the block\nbut also (importantly) makes them programmable. When you progra m a\npage, its state changes to VALID , meaning its contents have been set and\ncan be read. Reads do not affect these states (although you should only\nread from pages that have been programmed). Once a page has been pro-\ngrammed, the only way to change its contents is to erase the enti re block\nwithin which the page resides. Here is an example of states tra nsition\nafter various erase and program operations within a 4-page block:\niiii Initial: pages in block are invalid (i)\nErase() →EEEE State of pages in block set to erased (E)\nProgram(0) →VEEE Program page 0; state set to valid (V)\nProgram(0) → error Cannot re-program page after programming\nProgram(1) →VVEE Program page 1\nErase() →EEEE Contents erased; all pages programmable\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 FLASH -BASED SSD S\nA Detailed Example\nBecause the process of writing (i.e., erasing and programming) is so un-\nusual, let’s go through a detailed example to make sure it makes sense.\nIn this example, imagine we have the following four 8-bit pages, within\na 4-page block (both unrealistically small sizes, but useful w ithin this ex-\nample); each page is VALID as each has been previously programmed.\nPage 0 Page 1 Page 2 Page 3\n00011000 11001110 00000001 00111111\nVALID VALID VALID VALID\nNow say we wish to write to page 0, ﬁlling it with new contents. To\nwrite any page, we must ﬁrst erase the entire block. Let’s assum e we do\nso, thus leaving the block in this state:\nPage 0 Page 1 Page 2 Page 3\n11111111 11111111 11111111 11111111\nERASED ERASED ERASED ERASED\nGood news! We could now go ahead and program page 0, for exam-\nple with the contents 00000011 , overwriting the old page 0 (contents\n00011000 ) as desired. After doing so, our block looks like this:\nPage 0 Page 1 Page 2 Page 3\n00000011 11111111 11111111 11111111\nVALID ERASED ERASED ERASED\nAnd now the bad news: the previous contents of pages 1, 2, and 3\nare all gone! Thus, before overwriting any page within a block, we must\nﬁrst move any data we care about to another location (e.g., memory, or\nelsewhere on the ﬂash). The nature of erase will have a strong imp act on\nhow we design ﬂash-based SSDs, as we’ll soon learn about.\nSummary\nTo summarize, reading a page is easy: just read the page. Flas h chips\ndo this quite well, and quickly; in terms of performance, they of fer the\npotential to greatly exceed the random read performance of modern disk\ndrives, which are slow due to mechanical seek and rotation costs.\nWriting a page is trickier; the entire block must ﬁrst be erase d (taking\ncare to ﬁrst move any data we care about to another location), and th en\nthe desired page programmed. Not only is this expensive, but fre quent\nrepetitions of this program/erase cycle can lead to the biggest reliability\nproblem ﬂash chips have: wear out . When designing a storage system\nwith ﬂash, the performance and reliability of writing is a cent ral focus.\nWe’ll soon learn more about how modern SSDs attack these issues, deliv-\nering excellent performance and reliability despite these l imitations.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 5\nRead Program Erase\nDevice ( µs) ( µs) ( µs)\nSLC 25 200-300 1500-2000\nMLC 50 600-900 ˜3000\nTLC ˜75 ˜900-1350 ˜4500\nFigure 44.2: Raw Flash Performance Characteristics\n44.4 Flash Performance And Reliability\nBecause we’re interested in building a storage device out of raw ﬂ ash\nchips, it is worthwhile to understand their basic performance character-\nistics. Figure 44.2 presents a rough summary of some numbers foun d in\nthe popular press [V12]. Therein, the author presents the basi c operation\nlatency of reads, programs, and erases across SLC, MLC, and TLC ﬂa sh,\nwhich store 1, 2, and 3 bits of information per cell, respectively .\nAs we can see from the table, read latencies are quite good, takin g just\n10s of microseconds to complete. Program latency is higher and more\nvariable, as low as 200 microseconds for SLC, but higher as you pack\nmore bits into each cell; to get good write performance, you will ha ve\nto make use of multiple ﬂash chips in parallel. Finally, erase s are quite\nexpensive, taking a few milliseconds typically. Dealing wit h this cost is\ncentral to modern ﬂash storage design.\nLet’s now consider reliability of ﬂash chips. Unlike mechanical disks,\nwhich can fail for a wide variety of reasons (including the grues ome and\nquite physical head crash , where the drive head actually makes contact\nwith the recording surface), ﬂash chips are pure silicon and in that sense\nhave fewer reliability issues to worry about. The primary conce rn is wear\nout; when a ﬂash block is erased and programmed, it slowly accrues a\nlittle bit of extra charge. Over time, as that extra charge bui lds up, it\nbecomes increasingly difﬁcult to differentiate between a 0 a nd a 1. At the\npoint where it becomes impossible, the block becomes unusable.\nThe typical lifetime of a block is currently not well known. Manuf ac-\nturers rate MLC-based blocks as having a 10,000 P/E (Program/E rase)\ncycle lifetime; that is, each block can be erased and programme d 10,000\ntimes before failing. SLC-based chips, because they store only a single bit\nper transistor, are rated with a longer lifetime, usually 100, 000 P/E cycles.\nHowever, recent research has shown that lifetimes are much long er than\nexpected [BD10].\nOne other reliability problem within ﬂash chips is known as distur-\nbance . When accessing a particular page within a ﬂash, it is possibl e that\nsome bits get ﬂipped in neighboring pages; such bit ﬂips are know n as\nread disturbs orprogram disturbs , depending on whether the page is\nbeing read or programmed, respectively.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 FLASH -BASED SSD S\nTIP: THEIMPORTANCE OFBACKWARDS COMPATIBILITY\nBackwards compatibility is always a concern in layered system s. By\ndeﬁning a stable interface between two systems, one enables i nnovation\non each side of the interface while ensuring continued interoper ability.\nSuch an approach has been quite successful in many domains: opera ting\nsystems have relatively stable APIs for applications, disks p rovide the\nsame block-based interface to ﬁle systems, and each layer in t he IP net-\nworking stack provides a ﬁxed unchanging interface to the laye r above.\nNot surprisingly, there can be a downside to such rigidity, as i nterfaces\ndeﬁned in one generation may not be appropriate in the next. In some\ncases, it may be useful to think about redesigning the entire s ystem en-\ntirely. An excellent example is found in the Sun ZFS ﬁle system [ B07];\nby reconsidering the interaction of ﬁle systems and RAID, the cr eators of\nZFS envisioned (and then realized) a more effective integrate d whole.\n44.5 From Raw Flash to Flash-Based SSDs\nGiven our basic understanding of ﬂash chips, we now face our next\ntask: how to turn a basic set of ﬂash chips into something that looks like\na typical storage device. The standard storage interface is a s imple block-\nbased one, where blocks (sectors) of size 512 bytes (or larger) can be read\nor written, given a block address. The task of the ﬂash-based SSD is to\nprovide that standard block interface atop the raw ﬂash chips in side it.\nInternally, an SSD consists of some number of ﬂash chips (for persis-\ntent storage). An SSD also contains some amount of volatile (i.e., non-\npersistent) memory (e.g., SRAM); such memory is useful for cachi ng and\nbuffering of data as well as for mapping tables, which we’ll lear n about\nbelow. Finally, an SSD contains control logic to orchestrate device op era-\ntion. See Agrawal et. al for details [A+08]; a simpliﬁed block dia gram is\nseen in Figure 44.3 (page 7).\nOne of the essential functions of this control logic is to satisfy cl ient\nreads and writes, turning them into internal ﬂash operations a s need be.\nThe ﬂash translation layer , or FTL, provides exactly this functionality.\nThe FTL takes read and write requests on logical blocks (that comprise the\ndevice interface) and turns them into low-level read, erase, and program\ncommands on the underlying physical blocks and physical pages (that com-\nprise the actual ﬂash device). The FTL should accomplish this t ask with\nthe goal of delivering excellent performance and high reliabil ity.\nExcellent performance, as we’ll see, can be realized through a c om-\nbination of techniques. One key will be to utilize multiple ﬂas h chips\ninparallel ; although we won’t discuss this technique much further, suf-\nﬁce it to say that all modern SSDs use multiple chips internally t o obtain\nhigher performance. Another performance goal will be to reduce write\nampliﬁcation , which is deﬁned as the total write trafﬁc (in bytes) issued\nto the ﬂash chips by the FTL divided by the total write trafﬁc (i n bytes) is-\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 7Host Interface LogicFlash\nControllerMemoryFlash Flash Flash\nFlash Flash Flash\nFigure 44.3: A Flash-based SSD: Logical Diagram\nsued by the client to the SSD. As we’ll see below, naive approaches t o FTL\nconstruction will lead to high write ampliﬁcation and low perform ance.\nHigh reliability will be achieved through the combination of a fe w dif-\nferent approaches. One main concern, as discussed above, is wear out . If\na single block is erased and programmed too often, it will become un us-\nable; as a result, the FTL should try to spread writes across the blocks of\nthe ﬂash as evenly as possible, ensuring that all of the blocks of t he device\nwear out at roughly the same time; doing so is called wear leveling and\nis an essential part of any modern FTL.\nAnother reliability concern is program disturbance. To minimi ze such\ndisturbance, FTLs will commonly program pages within an erased block\nin order , from low page to high page. This sequential-programming ap-\nproach minimizes disturbance and is widely utilized.\n44.6 FTL Organization: A Bad Approach\nThe simplest organization of an FTL would be something we call di-\nrect mapped . In this approach, a read to logical page Nis mapped di-\nrectly to a read of physical page N. A write to logical page Nis more\ncomplicated; the FTL ﬁrst has to read in the entire block that pa geNis\ncontained within; it then has to erase the block; ﬁnally, the FT L programs\nthe old pages as well as the new one.\nAs you can probably guess, the direct-mapped FTL has many prob-\nlems, both in terms of performance as well as reliability. The pe rformance\nproblems come on each write: the device has to read in the entire b lock\n(costly), erase it (quite costly), and then program it (costly). The end re-\nsult is severe write ampliﬁcation (proportional to the number of p ages\nin a block) and as a result, terrible write performance, even sl ower than\ntypical hard drives with their mechanical seeks and rotationa l delays.\nEven worse is the reliability of this approach. If ﬁle system met adata\nor user ﬁle data is repeatedly overwritten, the same block is era sed and\nprogrammed, over and over, rapidly wearing it out and potentially los-\ning data. The direct mapped approach simply gives too much contr ol\nover wear out to the client workload; if the workload does not spread\nwrite load evenly across its logical blocks, the underlying phys ical blocks\ncontaining popular data will quickly wear out. For both reliabili ty and\nperformance reasons, a direct-mapped FTL is a bad idea.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 FLASH -BASED SSD S\n44.7 A Log-Structured FTL\nFor these reasons, most FTLs today are log structured , an idea useful\nin both storage devices (as we’ll see now) and ﬁle systems above the m (as\nwe’ll see in the chapter on log-structured ﬁle systems ). Upon a write to\nlogical block N, the device appends the write to the next free spot in the\ncurrently-being-written-to block; we call this style of writ inglogging . To\nallow for subsequent reads of block N, the device keeps a mapping table\n(in its memory, and persistent, in some form, on the device); this table\nstores the physical address of each logical block in the system.\nLet’s go through an example to make sure we understand how the\nbasic log-based approach works. To the client, the device looks li ke a\ntypical disk, in which it can read and write 512-byte sectors ( or groups of\nsectors). For simplicity, assume that the client is reading or w riting 4-KB\nsized chunks. Let us further assume that the SSD contains some lar ge\nnumber of 16-KB sized blocks, each divided into four 4-KB pages; these\nparameters are unrealistic (ﬂash blocks usually consist of more pages) but\nwill serve our didactic purposes quite well.\nAssume the client issues the following sequence of operations:\n•Write(100) with contents a1\n•Write(101) with contents a2\n•Write(2000) with contents b1\n•Write(2001) with contents b2\nThese logical block addresses (e.g., 100) are used by the client of the\nSSD (e.g., a ﬁle system) to remember where information is located.\nInternally, the device must transform these block writes into the erase\nand program operations supported by the raw hardware, and somehow\nrecord, for each logical block address, which physical page of the SSD\nstores its data. Assume that all blocks of the SSD are currently not v alid,\nand must be erased before any page can be programmed. Here we show\nthe initial state of our SSD, with all pages marked INVALID (i):\n0 1 2 Block:\nPage:\nContent:\nState:00\ni01\ni02\ni03\ni04\ni05\ni06\ni07\ni08\ni09\ni10\ni11\ni\nWhen the ﬁrst write is received by the SSD (to logical block 100), t he\nFTL decides to write it to physical block 0, which contains four p hysical\npages: 0, 1, 2, and 3. Because the block is not erased, we cannot wr ite to\nit yet; the device must ﬁrst issue an erase command to block 0. Doi ng so\nleads to the following state:\n0 1 2 Block:\nPage:\nContent:\nState:00\nE01\nE02\nE03\nE04\ni05\ni06\ni07\ni08\ni09\ni10\ni11\ni\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 9\nBlock 0 is now ready to be programmed. Most SSDs will write pages\nin order (i.e., low to high), reducing reliability problems rel ated to pro-\ngram disturbance . The SSD then directs the write of logical block 100\ninto physical page 0:\n0 1 2 Block:\nPage:\nContent:\nState:00\na1\nV01\nE02\nE03\nE04\ni05\ni06\ni07\ni08\ni09\ni10\ni11\ni\nBut what if the client wants to read logical block 100? How can it ﬁnd\nwhere it is? The SSD must transform a read issued to logical block 10 0\ninto a read of physical page 0. To accommodate such functionality , when\nthe FTL writes logical block 100 to physical page 0, it records th is fact in\nanin-memory mapping table . We will track the state of this mapping\ntable in the diagrams as well:\nMemory\nFlash\nChipTable: 100 0\n0 1 2 Block:\nPage:\nContent:\nState:00\na1\nV01\nE02\nE03\nE04\ni05\ni06\ni07\ni08\ni09\ni10\ni11\ni\nNow you can see what happens when the client writes to the SSD.\nThe SSD ﬁnds a location for the write, usually just picking the next free\npage; it then programs that page with the block’s contents, and re cords\nthe logical-to-physical mapping in its mapping table. Subsequ ent reads\nsimply use the table to translate the logical block address presented by\nthe client into the physical page number required to read the data.\nLet’s now examine the rest of the writes in our example write strea m:\n101, 2000, and 2001. After writing these blocks, the state of th e device is:\nMemory\nFlash\nChipTable: 100 0 101 1 2000 2 2001 3\n0 1 2 Block:\nPage:\nContent:\nState:00\na1\nV01\na2\nV02\nb1\nV03\nb2\nV04\ni05\ni06\ni07\ni08\ni09\ni10\ni11\ni\nThe log-based approach by its nature improves performance (eras es\nonly being required once in a while, and the costly read-modify-w rite of\nthe direct-mapped approach avoided altogether), and greatly e nhances\nreliability. The FTL can now spread writes across all pages, pe rforming\nwhat is called wear leveling and increasing the lifetime of the device;\nwe’ll discuss wear leveling further below.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 FLASH -BASED SSD S\nASIDE : FTL M APPING INFORMATION PERSISTENCE\nYou might be wondering: what happens if the device loses power? Doe s\nthe in-memory mapping table disappear? Clearly, such informa tion can-\nnot truly be lost, because otherwise the device would not function a s a\npersistent storage device. An SSD must have some means of recoverin g\nmapping information.\nThe simplest thing to do is to record some mapping information wit h\neach page, in what is called an out-of-band (OOB ) area. When the device\nloses power and is restarted, it must reconstruct its mapping ta ble by\nscanning the OOB areas and reconstructing the mapping table i n mem-\nory. This basic approach has its problems; scanning a large SSD to ﬁ nd\nall necessary mapping information is slow. To overcome this limit ation,\nsome higher-end devices use more complex logging and checkpointing\ntechniques to speed up recovery; learn more about logging by read ing\nchapters on crash consistency and log-structured ﬁle systems [ AD14].\nUnfortunately, this basic approach to log structuring has some d own-\nsides. The ﬁrst is that overwrites of logical blocks lead to someth ing we\ncall garbage , i.e., old versions of data around the drive and taking up\nspace. The device has to periodically perform garbage collection (GC) to\nﬁnd said blocks and free space for future writes; excessive gar bage collec-\ntion drives up write ampliﬁcation and lowers performance. The se cond\nis high cost of in-memory mapping tables; the larger the device, the more\nmemory such tables need. We now discuss each in turn.\n44.8 Garbage Collection\nThe ﬁrst cost of any log-structured approach such as this one is tha t\ngarbage is created, and therefore garbage collection (i.e., dead-block recla-\nmation) must be performed. Let’s use our continued example to make\nsense of this. Recall that logical blocks 100, 101, 2000, and 200 1 have been\nwritten to the device.\nNow, let’s assume that blocks 100 and 101 are written to again, wi th\ncontentsc1andc2. The writes are written to the next free pages (in this\ncase, physical pages 4 and 5), and the mapping table is update d accord-\ningly. Note that the device must have ﬁrst erased block 1 to make such\nprogramming possible:\nMemory\nFlash\nChipTable: 100 4 101 5 2000 2 2001 3\n0 1 2 Block:\nPage:\nContent:\nState:00\na1\nV01\na2\nV02\nb1\nV03\nb2\nV04\nc1\nV05\nc2\nV06\nE07\nE08\ni09\ni10\ni11\ni\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 11\nThe problem we have now should be obvious: physical pages 0 and\n1, although marked VALID , have garbage in them, i.e., the old versions\nof blocks 100 and 101. Because of the log-structured nature of the d e-\nvice, overwrites create garbage blocks, which the device must reclaim to\nprovide free space for new writes to take place.\nThe process of ﬁnding garbage blocks (also called dead blocks ) and\nreclaiming them for future use is called garbage collection , and it is an\nimportant component of any modern SSD. The basic process is simple:\nﬁnd a block that contains one or more garbage pages, read in the live\n(non-garbage) pages from that block, write out those live pages to the\nlog, and (ﬁnally) reclaim the entire block for use in writing.\nLet’s now illustrate with an example. The device decides it wan ts to\nreclaim any dead pages within block 0 above. Block 0 has two dead b locks\n(pages 0 and 1) and two lives blocks (pages 2 and 3, which contain blocks\n2000 and 2001, respectively). To do so, the device will:\n•Read live data (pages 2 and 3) from block 0\n•Write live data to end of the log\n•Erase block 0 (freeing it for later usage)\nFor the garbage collector to function, there must be enough informa -\ntion within each block to enable the SSD to determine whether each page\nis live or dead. One natural way to achieve this end is to store, a t some\nlocation within each block, information about which logical blocks a re\nstored within each page. The device can then use the mapping ta ble to\ndetermine whether each page within the block holds live data or n ot.\nFrom our example above (before the garbage collection has taken pla ce),\nblock 0 held logical blocks 100, 101, 2000, 2001. By checking the mapping\ntable (which, before garbage collection, contained 100->4, 101->5,\n2000->2, 2001->3 ), the device can readily determine whether each of\nthe pages within the SSD block holds live information. For example, 2 000\nand 2001 clearly are still pointed to by the map; 100 and 101 are not and\ntherefore are candidates for garbage collection.\nWhen this garbage collection process is complete in our example, t he\nstate of the device is:\nMemory\nFlash\nChipTable: 100 4 101 5 2000 6 2001 7\n0 1 2 Block:\nPage:\nContent:\nState:00\nE01\nE02\nE03\nE04\nc1\nV05\nc2\nV06\nb1\nV07\nb2\nV08\ni09\ni10\ni11\ni\nAs you can see, garbage collection can be expensive, requiring r eading\nand rewriting of live data. The ideal candidate for reclamation is a block\nthat consists of only dead pages; in this case, the block can immed iately\nbe erased and used for new data, without expensive data migrati on.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 FLASH -BASED SSD S\nASIDE : A N EWSTORAGE API K NOWN ASTRIM\nWhen we think of hard drives, we usually just think of the most ba-\nsic interface to read and write them: read and write (there is also usu-\nally some kind of cache ﬂush command, ensuring that writes have actu-\nally been persisted, but sometimes we omit that for simplicity) . With\nlog-structured SSDs, and indeed, any device that keeps a ﬂexibl e and\nchanging mapping of logical-to-physical blocks, a new interfac e is use-\nful, known as the trim operation.\nThe trim operation takes an address (and possibly a length) and s imply\ninforms the device that the block(s) speciﬁed by the address (a nd length)\nhave been deleted; the device thus no longer has to track any in forma-\ntion about the given address range. For a standard hard drive, tr im isn’t\nparticularly useful, because the drive has a static mapping of block ad-\ndresses to speciﬁc platter, track, and sector(s). For a log-str uctured SSD,\nhowever, it is highly useful to know that a block is no longer neede d, as\nthe SSD can then remove this information from the FTL and later recla im\nthe physical space during garbage collection.\nAlthough we sometimes think of interface and implementation as s epa-\nrate entities, in this case, we see that the implementation sh apes the inter-\nface. With complex mappings, knowledge of which blocks are no long er\nneeded makes for a more effective implementation.\nTo reduce GC costs, some SSDs overprovision the device [A+08]; by\nadding extra ﬂash capacity, cleaning can be delayed and push ed to the\nbackground , perhaps done at a time when the device is less busy. Adding\nmore capacity also increases internal bandwidth, which can b e used for\ncleaning and thus not harm perceived bandwidth to the client. Many\nmodern drives overprovision in this manner, one key to achieving e xcel-\nlent overall performance.\n44.9 Mapping Table Size\nThe second cost of log-structuring is the potential for extremely l arge\nmapping tables, with one entry for each 4-KB page of the device. W ith a\nlarge 1-TB SSD, for example, a single 4-byte entry per 4-KB page r esults\nin 1 GB of memory needed the device, just for these mappings! Thus , this\npage-level FTL scheme is impractical.\nBlock-Based Mapping\nOne approach to reduce the costs of mapping is to only keep a pointer per\nblock of the device, instead of per page, reducing the amount of mapping\ninformation by a factor ofSizeblock\nSizepage. This block-level FTL is akin to having\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 13\nbigger page sizes in a virtual memory system; in that case, you u se fewer\nbits for the VPN and have a larger offset in each virtual address .\nUnfortunately, using a block-based mapping inside a log-based FTL\ndoes not work very well for performance reasons. The biggest problem\narises when a “small write” occurs (i.e., one that is less than t he size of\na physical block). In this case, the FTL must read a large amount of live\ndata from the old block and copy it into a new one (along with the data\nfrom the small write). This data copying increases write ampli ﬁcation\ngreatly and thus decreases performance.\nTo make this issue more clear, let’s look at an example. Assume the\nclient previously wrote out logical blocks 2000, 2001, 2002, and 2 003 (with\ncontents,a, b, c, d ), and that they are located within physical block\n1 at physical pages 4, 5, 6, and 7. With per-page mappings, the transla-\ntion table would have to record four mappings for these logical block s:\n2000→4, 2001→5, 2002→6, 2003→7.\nIf, instead, we use block-level mapping, the FTL only needs to r ecord\na single address translation for all of this data. The address ma pping,\nhowever, is slightly different than our previous examples. Spec iﬁcally,\nwe think of the logical address space of the device as being choppe d into\nchunks that are the size of the physical blocks within the ﬂash. Thus,\nthe logical block address consists of two portions: a chunk number a nd\nan offset. Because we are assuming four logical blocks ﬁt within e ach\nphysical block, the offset portion of the logical addresses requir es 2 bits;\nthe remaining (most signiﬁcant) bits form the chunk number.\nLogical blocks 2000, 2001, 2002, and 2003 all have the same chun k\nnumber (500), and have different offsets (0, 1, 2, and 3, respe ctively).\nThus, with a block-level mapping, the FTL records that chunk 50 0 maps\nto block 1 (starting at physical page 4), as shown in this diagra m:\nMemory\nFlash\nChipTable: 500 4\n0 1 2 Block:\nPage:\nContent:\nState:00\ni01\ni02\ni03\ni04\na\nV05\nb\nV06\nc\nV07\nd\nV08\ni09\ni10\ni11\ni\nIn a block-based FTL, reading is easy. First, the FTL extracts the chunk\nnumber from the logical block address presented by the client, b y taking\nthe topmost bits out of the address. Then, the FTL looks up the chunk-\nnumber to physical-page mapping in the table. Finally, the F TL computes\nthe address of the desired ﬂash page by adding the offset from the logical\naddress to the physical address of the block.\nFor example, if the client issues a read to logical address 2002 , the de-\nvice extracts the logical chunk number (500), looks up the trans lation in\nthe mapping table (ﬁnding 4), and adds the offset from the logica l ad-\ndress (2) to the translation (4). The resulting physical-pag e address (6) is\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 FLASH -BASED SSD S\nwhere the data is located; the FTL can then issue the read to tha t physical\naddress and obtain the desired data ( c).\nBut what if the client writes to logical block 2002 (with content sc’)?\nIn this case, the FTL must read in 2000, 2001, and 2003, and the n write\nout all four logical blocks in a new location, updating the mapping t able\naccordingly. Block 1 (where the data used to reside) can then be erased\nand reused, as shown here.\nMemory\nFlash\nChipTable: 500 8\n0 1 2 Block:\nPage:\nContent:\nState:00\ni01\ni02\ni03\ni04\nE05\nE06\nE07\nE08\na\nV09\nb\nV10\nc’\nV11\nd\nV\nAs you can see from this example, while block level mappings grea tly\nreduce the amount of memory needed for translations, they cause si gnif-\nicant performance problems when writes are smaller than the ph ysical\nblock size of the device; as real physical blocks can be 256KB or la rger,\nsuch writes are likely to happen quite often. Thus, a better sol ution is\nneeded. Can you sense that this is the part of the chapter where w e tell\nyou what that solution is? Better yet, can you ﬁgure it out yourself, before\nreading on?\nHybrid Mapping\nTo enable ﬂexible writing but also reduce mapping costs, many modern\nFTLs employ a hybrid mapping technique. With this approach, the FTL\nkeeps a few blocks erased and directs all writes to them; these are called\nlog blocks . Because the FTL wants to be able to write any page to any\nlocation within the log block without all the copying required by a p ure\nblock-based mapping, it keeps per-page mappings for these log blocks.\nThe FTL thus logically has two types of mapping table in its memor y: a\nsmall set of per-page mappings in what we’ll call the log table , and a larger\nset of per-block mappings in the data table . When looking for a particular\nlogical block, the FTL will ﬁrst consult the log table; if the logic al block’s\nlocation is not found there, the FTL will then consult the data tabl e to ﬁnd\nits location and then access the requested data.\nThe key to the hybrid mapping strategy is keeping the number of log\nblocks small. To keep the number of log blocks small, the FTL has to pe-\nriodically examine log blocks (which have a pointer per page) and switch\nthem into blocks that can be pointed to by only a single block pointe r.\nThis switch is accomplished by one of three main techniques, bas ed on\nthe contents of the block [KK+02].\nFor example, let’s say the FTL had previously written out logical p ages\n1000, 1001, 1002, and 1003, and placed them in physical block 2 (physical\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 15\npages 8, 9, 10, 11); assume the contents of the writes to 1000, 10 01, 1002,\nand 1003 are a,b,c, andd, respectively.\nMemory\nFlash\nChipLog Table:\nData Table: 250 8\n0 1 2 Block:\nPage:\nContent:\nState:00\ni01\ni02\ni03\ni04\ni05\ni06\ni07\ni08\na\nV09\nb\nV10\nc\nV11\nd\nV\nNow assume that the client overwrites each of these blocks (with d ata\na’,b’,c’, andd’), in the exact same order, in one of the currently avail-\nable log blocks, say physical block 0 (physical pages 0, 1, 2, and 3). In this\ncase, the FTL will have the following state:\nMemory\nFlash\nChipLog Table: 1000 0 1001 1 1002 2 1003 3\nData Table: 250 8\n0 1 2 Block:\nPage:\nContent:\nState:00\na’\nV01\nb’\nV02\nc’\nV03\nd’\nV04\ni05\ni06\ni07\ni08\na\nV09\nb\nV10\nc\nV11\nd\nV\nBecause these blocks have been written exactly in the same man ner as\nbefore, the FTL can perform what is known as a switch merge . In this\ncase, the log block (0) now becomes the storage location for blocks 0, 1, 2,\nand 3, and is pointed to by a single block pointer; the old block (2) i s now\nerased and used as a log block. In this best case, all the per-pag e pointers\nrequired replaced by a single block pointer.\nMemory\nFlash\nChipLog Table:\nData Table: 250 0\n0 1 2 Block:\nPage:\nContent:\nState:00\na’\nV01\nb’\nV02\nc’\nV03\nd’\nV04\ni05\ni06\ni07\ni08\ni09\ni10\ni11\ni\nThis switch merge is the best case for a hybrid FTL. Unfortunate ly,\nsometimes the FTL is not so lucky. Imagine the case where we have\nthe same initial conditions (logical blocks 1000 ... 1003 stored i n physi-\ncal block 2) but then the client overwrites logical blocks 1000 an d 1001.\nWhat do you think happens in this case? Why is it more challengin g\nto handle? (think before looking at the result on the next page)\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 FLASH -BASED SSD S\nMemory\nFlash\nChipLog Table: 1000 0 1001 1\nData Table: 250 8\n0 1 2 Block:\nPage:\nContent:\nState:00\na’\nV01\nb’\nV02\ni03\ni04\ni05\ni06\ni07\ni08\na\nV09\nb\nV10\nc\nV11\nd\nV\nTo reunite the other pages of this physical block, and thus be abl e to re-\nfer to them by only a single block pointer, the FTL performs what is called\napartial merge . In this operation, logical blocks 1002 and 1003 are read\nfrom physical block 2, and then appended to the log. The resultin g state\nof the SSD is the same as the switch merge above; however, in this cas e,\nthe FTL had to perform extra I/O to achieve its goals, thus incre asing\nwrite ampliﬁcation.\nThe ﬁnal case encountered by the FTL known as a full merge , and re-\nquires even more work. In this case, the FTL must pull together pa ges\nfrom many other blocks to perform cleaning. For example, imagine t hat\nlogical blocks 0, 4, 8, and 12 are written to log block A. To switch this log\nblock into a block-mapped page, the FTL must ﬁrst create a data b lock\ncontaining logical blocks 0, 1, 2, and 3, and thus the FTL must rea d 1, 2,\nand 3 from elsewhere and then write out 0, 1, 2, and 3 together. Nex t, the\nmerge must do the same for logical block 4, ﬁnding 5, 6, and 7 and re con-\nciling them into a single physical block. The same must be done f or logi-\ncal blocks 8 and 12, and then (ﬁnally), the log block Acan be freed. Fre-\nquent full merges, as is not surprising, can seriously harm per formance\nand thus should be avoided when at all possible [GY+09].\nPage Mapping Plus Caching\nGiven the complexity of the hybrid approach above, others have sug -\ngested simpler ways to reduce the memory load of page-mapped FTL s.\nProbably the simplest is just to cache only the active parts of th e FTL in\nmemory, thus reducing the amount of memory needed [GY+09].\nThis approach can work well. For example, if a given workload only\naccesses a small set of pages, the translations of those pages wil l be stored\nin the in-memory FTL, and performance will be excellent without high\nmemory cost. Of course, the approach can also perform poorly. If mem-\nory cannot contain the working set of necessary translations, each access\nwill minimally require an extra ﬂash read to ﬁrst bring in the missing\nmapping before being able to access the data itself. Even worse , to make\nroom for the new mapping, the FTL might have to evict an old map-\nping, and if that mapping is dirty (i.e., not yet written to the ﬂash per-\nsistently), an extra write will also be incurred. However, in many cases,\nthe workload will display locality, and this caching approach wi ll both\nreduce memory overheads and keep performance high.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 17\n44.10 Wear Leveling\nFinally, a related background activity that modern FTLs must i mple-\nment is wear leveling , as introduced above. The basic idea is simple:\nbecause multiple erase/program cycles will wear out a ﬂash bloc k, the\nFTL should try its best to spread that work across all the blocks of t he de-\nvice evenly. In this manner, all blocks will wear out at roughly t he same\ntime, instead of a few “popular” blocks quickly becoming unusabl e.\nThe basic log-structuring approach does a good initial job of spread ing\nout write load, and garbage collection helps as well. However, some times\na block will be ﬁlled with long-lived data that does not get over-wr itten;\nin this case, garbage collection will never reclaim the block, a nd thus it\ndoes not receive its fair share of the write load.\nTo remedy this problem, the FTL must periodically read all the l ive\ndata out of such blocks and re-write it elsewhere, thus making th e block\navailable for writing again. This process of wear leveling incr eases the\nwrite ampliﬁcation of the SSD, and thus decreases performance as e xtra\nI/O is required to ensure that all blocks wear at roughly the sam e rate.\nMany different algorithms exist in the literature [A+08, M+1 4]; read more\nif you are interested.\n44.11 SSD Performance And Cost\nBefore closing, let’s examine the performance and cost of modern SSDs,\nto better understand how they will likely be used in persisten t storage\nsystems. In both cases, we’ll compare to classic hard-disk driv es (HDDs),\nand highlight the biggest differences between the two.\nPerformance\nUnlike hard disk drives, ﬂash-based SSDs have no mechanical com po-\nnents, and in fact are in many ways more similar to DRAM, in that they\nare “random access” devices. The biggest difference in perfor mance, as\ncompared to disk drives, is realized when performing random rea ds and\nwrites; while a typical disk drive can only perform a few hundre d ran-\ndom I/Os per second, SSDs can do much better. Here, we use some data\nfrom modern SSDs to see just how much better SSDs perform; we’re par-\nticularly interested in how well the FTLs hide the performance issues of\nthe raw chips.\nTable 44.4 shows some performance data for three different SSDs and\none top-of-the-line hard drive; the data was taken from a few diff erent\nonline sources [S13, T15]. The left two columns show random I/O per-\nformance, and the right two columns sequential; the ﬁrst three rows show\ndata for three different SSDs (from Samsung, Seagate, and Intel), a nd the\nlast row shows performance for a hard disk drive (orHDD ), in this case\na Seagate high-end drive.\nWe can learn a few interesting facts from the table. First, and most\ndramatic, is the difference in random I/O performance between the SSDs\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n18 FLASH -BASED SSD S\nRandom Sequential\nReads Writes Reads Writes\nDevice (MB/s) (MB/s) (MB/s) (MB/s)\nSamsung 840 Pro SSD 103 287 421 384\nSeagate 600 SSD 84 252 424 374\nIntel SSD 335 SSD 39 222 344 354\nSeagate Savvio 15K.3 HDD 2 2 223 223\nFigure 44.4: SSDs And Hard Drives: Performance Comparison\nand the lone hard drive. While the SSDs obtain tens or even hundreds of\nMB/s in random I/Os, this “high performance” hard drive has a pe ak of\njust a couple MB/s (in fact, we rounded up to get to 2 MB/s). Second, you\ncan see that in terms of sequential performance, there is much l ess of a dif-\nference; while the SSDs perform better, a hard drive is still a good choice\nif sequential performance is all you need. Third, you can see tha t SSD ran-\ndom read performance is not as good as SSD random write performance.\nThe reason for such unexpectedly good random-write performance is\ndue to the log-structured design of many SSDs, which transforms ra n-\ndom writes into sequential ones and improves performance. Final ly, be-\ncause SSDs exhibit some performance difference between sequent ial and\nrandom I/Os, many of the techniques we will learn in subsequent chap-\nters about how to build ﬁle systems for hard drives are still appl icable to\nSSDs; although the magnitude of difference between sequential a nd ran-\ndom I/Os is smaller, there is enough of a gap to carefully consider how\nto design ﬁle systems to reduce random I/Os.\nCost\nAs we saw above, the performance of SSDs greatly outstrips modern har d\ndrives, even when performing sequential I/O. So why haven’t SSDs c om-\npletely replaced hard drives as the storage medium of choice? Th e an-\nswer is simple: cost, or more speciﬁcally, cost per unit of capacit y. Cur-\nrently [A15], an SSD costs something like $150 for a 250-GB drive; s uch\nan SSD costs 60 cents per GB. A typical hard drive costs roughly $50 f or\n1-TB of storage, which means it costs 5 cents per GB. There is stil l more\nthan a 10 ×difference in cost between these two storage media.\nThese performance and cost differences dictate how large-scal e stor-\nage systems are built. If performance is the main concern, SSDs ar e a\nterriﬁc choice, particularly if random read performance is imp ortant. If,\non the other hand, you are assembling a large data center and wish to\nstore massive amounts of information, the large cost difference wi ll drive\nyou towards hard drives. Of course, a hybrid approach can make sen se\n– some storage systems are being assembled with both SSDs and hard\ndrives, using a smaller number of SSDs for more popular “hot” data and\ndelivering high performance, while storing the rest of the “cold er” (less\nused) data on hard drives to save on cost. As long as the price gap ex ists,\nhard drives are here to stay.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 19\n44.12 Summary\nFlash-based SSDs are becoming a common presence in laptops, desk-\ntops, and servers inside the datacenters that power the world’s e conomy.\nThus, you should probably know something about them, right?\nHere’s the bad news: this chapter (like many in this book) is just the\nﬁrst step in understanding the state of the art. Some places to ge t some\nmore information about the raw technology include research on actua l\ndevice performance (such as that by Chen et al. [CK+09] and Gru pp et\nal. [GC+09]), issues in FTL design (including works by Agrawa l et al.\n[A+08], Gupta et al. [GY+09], Huang et al. [H+14], Kim et al. [ KK+02],\nLee et al. [L+07], and Zhang et al. [Z+12]), and even distribu ted systems\ncomprised of ﬂash (including Gordon [CG+09] and CORFU [B+12]). A nd,\nif we may say so, a really good overview of all the things you need to do\nto extract high performance from an SSD can be found in a paper on the\n“unwritten contract” [HK+17].\nDon’t just read academic papers; also read about recent advance s in\nthe popular press (e.g., [V12]). Therein you’ll learn more pract ical (but\nstill useful) information, such as Samsung’s use of both TLC and SLC c ells\nwithin the same SSD to maximize performance (SLC can buffer write s\nquickly) as well as capacity (TLC can store more bits per cell). And this\nis, as they say, just the tip of the iceberg. Dive in and learn mor e about\nthis “iceberg” of research on your own, perhaps starting with Ma e t al.’s\nexcellent (and recent) survey [M+14]. Be careful though; ice bergs can sink\neven the mightiest of ships [W15].\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n20 FLASH -BASED SSD S\nASIDE : KEYSSD T ERMS\n•Aﬂash chip consists of many banks, each of which is organized into\nerase blocks (sometimes just called blocks ). Each block is further\nsubdivided into some number of pages .\n•Blocks are large (128KB–2MB) and contain many pages, which are\nrelatively small (1KB–8KB).\n•To read from ﬂash, issue a read command with an address and\nlength; this allows a client to read one or more pages.\n•Writing ﬂash is more complex. First, the client must erase the en-\ntire block (which deletes all information within the block). The n,\nthe client can program each page exactly once, thus completing the\nwrite.\n•A new trim operation is useful to tell the device when a particular\nblock (or range of blocks) is no longer needed.\n•Flash reliability is mostly determined by wear out ; if a block is\nerased and programmed too often, it will become unusable.\n•A ﬂash-based solid-state storage device (SSD ) behaves as if it were\na normal block-based read/write disk; by using a ﬂash translation\nlayer (FTL), it transforms reads and writes from a client into reads,\nerases, and programs to underlying ﬂash chips.\n•Most FTLs are log-structured , which reduces the cost of writing\nby minimizing erase/program cycles. An in-memory translation\nlayer tracks where logical writes were located within the phys ical\nmedium.\n•One key problem with log-structured FTLs is the cost of garbage\ncollection , which leads to write ampliﬁcation .\n•Another problem is the size of the mapping table, which can be-\ncome quite large. Using a hybrid mapping or just caching hot\npieces of the FTL are possible remedies.\n•One last problem is wear leveling ; the FTL must occasionally mi-\ngrate data from blocks that are mostly read in order to ensure said\nblocks also receive their share of the erase/program load.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 21\nReferences\n[A+08] “Design Tradeoffs for SSD Performance” by N. Agrawal, V . Prabha karan, T. Wobber, J.\nD. Davis, M. Manasse, R. Panigrahy. USENIX ’08, San Diego California, June 2008. An excellent\noverview of what goes into SSD design.\n[AD14] “Operating Systems: Three Easy Pieces” by Chapters: Crash Consistency: FSCK and Jour-\nnaling and Log-Structured File Systems . Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. A\nlot more detail here about how logging can be used in ﬁle systems; some of the s ame ideas can be applied\ninside devices too as need be.\n[A15] “Amazon Pricing Study” by Remzi Arpaci-Dusseau. February, 2 015. This is not an actual\npaper, but rather one of the authors going to Amazon and looking at current prices of h ard drives and\nSSDs. You too can repeat this study, and see what the costs are today. Do it!\n[B+12] “CORFU: A Shared Log Design for Flash Clusters” by M. Balakr ishnan, D. Malkhi, V .\nPrabhakaran, T. Wobber, M. Wei, J. D. Davis. NSDI ’12, San Jose, Cali fornia, April 2012. A new\nway to think about designing a high-performance replicated log for cluster s using Flash.\n[BD10] “Write Endurance in Flash Drives: Measurements and Analysis” by Simona Boboila,\nPeter Desnoyers. FAST ’10, San Jose, California, February 2010. A cool paper that reverse en-\ngineers ﬂash-device lifetimes. Endurance sometimes far exceeds man ufacturer predictions, by up to\n100×.\n[B07] “ZFS: The Last Word in File Systems” by Jeff Bonwick and Bill Moor e. Available here:\nhttp://www.ostep.org/Citations/zfs_last.pdf .Was this the last word in ﬁle sys-\ntems? No, but maybe it’s close.\n[CG+09] “Gordon: Using Flash Memory to Build Fast, Power-efﬁcient Cl usters for Data-intensive\nApplications” by Adrian M. Caulﬁeld, Laura M. Grupp, Steven Swans on. ASPLOS ’09, Wash-\nington, D.C., March 2009. Early research on assembling ﬂash into larger-scale clusters; deﬁnitel y\nworth a read.\n[CK+09] “Understanding Intrinsic Characteristics and System Implicat ions of Flash Memory\nbased Solid State Drives” by Feng Chen, David A. Koufaty, and Xiaod ong Zhang. SIGMET-\nRICS/Performance ’09, Seattle, Washington, June 2009. An excellent overview of SSD performance\nproblems circa 2009 (though now a little dated).\n[G14] “The SSD Endurance Experiment” by Geoff Gasior. The Tech Report, S eptember 19,\n2014. Available: http://techreport.com/review/27062 .A nice set of simple experiments\nmeasuring performance of SSDs over time. There are many other similar stud ies; use google to ﬁnd\nmore.\n[GC+09] “Characterizing Flash Memory: Anomalies, Observations, and Applications” by L.\nM. Grupp, A. M. Caulﬁeld, J. Coburn, S. Swanson, E. Yaakobi, P . H. Sie gel, J. K. Wolf. IEEE\nMICRO ’09, New York, New York, December 2009. Another excellent characterization of ﬂash\nperformance.\n[GY+09] “DFTL: a Flash Translation Layer Employing Demand-Based S elective Caching of\nPage-Level Address Mappings” by Aayush Gupta, Youngjae Kim, Bhuva n Urgaonkar. ASP-\nLOS ’09, Washington, D.C., March 2009. This paper gives an excellent overview of different strategies\nfor cleaning within hybrid SSDs as well as a new scheme which saves map ping table space and improves\nperformance under many workloads.\n[HK+17] “The Unwritten Contract of Solid State Drives” by Jun He, Sud arsun Kannan, Andrea\nC. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. EuroSys ’17, Belgr ade, Serbia, April 2017. Our\nown paper which lays out ﬁve rules clients should follow in order to get the best performance out of\nmodern SSDs. The rules are request scale, locality, aligned sequenti ality, grouping by death time, and\nuniform lifetime. Read the paper for details!\n[H+14] “An Aggressive Worn-out Flash Block Management Scheme To Allev iate SSD Perfor-\nmance Degradation” by Ping Huang, Guanying Wu, Xubin He, Weijun Xiao. EuroSys ’14,\n2014. Recent work showing how to really get the most out of worn-out ﬂash blocks; neat!\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n22 FLASH -BASED SSD S\n[J10] “Failure Mechanisms and Models for Semiconductor Devices” by Unknown a uthor. Re-\nport JEP122F, November 2010. Available on the internet at this excit ing so-called web site:\nhttp://www.jedec.org/sites/default/files/docs/JEP12 2F.pdf .A highly detailed\ndiscussion of what is going on at the device level and how such devices fai l. Only for those not faint of\nheart. Or physicists. Or both.\n[KK+02] “A Space-Efﬁcient Flash Translation Layer For Compact Flas h Systems” by Jesung\nKim, Jong Min Kim, Sam H. Noh, Sang Lyul Min, Yookun Cho. IEEE Transactions on Con-\nsumer Electronics, Volume 48, Number 2, May 2002. One of the earliest proposals to suggest\nhybrid mappings.\n[L+07] “A Log Buffer-Based Flash Translation Layer by Using Fully -Associative Sector Trans-\nlation. ” Sang-won Lee, Tae-Sun Chung, Dong-Ho Lee, Sangwon Park, Ha-Joo S ong. ACM\nTransactions on Embedded Computing Systems, Volume 6, Number 3, July 2007 A terriﬁc paper\nabout how to build hybrid log/block mappings.\n[M+14] “A Survey of Address Translation Technologies for Flash Memor ies” by Dongzhe Ma,\nJianhua Feng, Guoliang Li. ACM Computing Surveys, Volume 46, Numbe r 3, January 2014.\nProbably the best recent survey of ﬂash and related technologies.\n[S13] “The Seagate 600 and 600 Pro SSD Review” by Anand Lal Shimpi. AnandT ech, May 7,\n2013. Available: http://www.anandtech.com/show/6935/seagate-600-ssd- review .\nOne of many SSD performance measurements available on the internet. Haven’t heard of the internet?\nNo problem. Just go to your web browser and type “internet” into the search tool . You’ll be amazed at\nwhat you can learn.\n[T15] “Performance Charts Hard Drives” by Tom’s Hardware. January 201 5. Available here:\nhttp://www.tomshardware.com/charts/enterprise-hdd-c harts .Yet another site\nwith performance data, this time focusing on hard drives.\n[V12] “Understanding TLC Flash” by Kristian Vatto. AnandTech, Septemb er, 2012. Available:\nhttp://www.anandtech.com/show/5067/understanding-tl c-nand .A short descrip-\ntion about TLC ﬂash and its characteristics.\n[W15] “List of Ships Sunk by Icebergs” by Many authors. Available at t his location on the\n“web”:http://en.wikipedia.org/wiki/List ofshipssunkbyicebergs .Yes, there\nis a wikipedia page about ships sunk by icebergs. It is a really boring page and basically everyone knows\nthe only ship the iceberg-sinking-maﬁa cares about is the Titanic.\n[Z+12] “De-indirection for Flash-based SSDs with Nameless Writes” by Yiying Zhang, Leo\nPrasath Arulraj, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusse au. FAST ’13, San Jose,\nCalifornia, February 2013. Our research on a new idea to reduce mapping table space; the key is to\nre-use the pointers in the ﬁle system above to store locations of blocks, instead of add ing another level of\nindirection.\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG\nFLASH -BASED SSD S 23\nHomework (Simulation)\nThis section introduces ssd.py , a simple SSD simulator you can use\nto understand better how SSDs work. Read the README for details on\nhow to run the simulator. It is a long README, so boil a cup of tea (caf -\nfeinated likely necessary), put on your reading glasses, let t he cat curl up\non your lap1, and get to work.\nQuestions\n1. The homework will mostly focus on the log-structured SSD, which\nis simulated with the “-T log” ﬂag. We’ll use the other types of\nSSDs for comparison. First, run with ﬂags -T log -s 1 -n 10\n-q. Can you ﬁgure out which operations took place? Use -cto\ncheck your answers (or just use -Cinstead of -q -c ). Use different\nvalues of-sto generate different random workloads.\n2. Now just show the commands and see if you can ﬁgure out the\nintermediate states of the Flash. Run with ﬂags -T log -s 2 -n\n10 -C to show each command. Now, determine the state of the\nFlash between each command; use -Fto show the states and see if\nyou were right. Use different random seeds to test your burgeonin g\nexpertise.\n3. Let’s make this problem ever so slightly more interesting by a dding\nthe-r 20 ﬂag. What differences does this cause in the commands?\nUse-cagain to check your answers.\n4. Performance is determined by the number of erases, programs, and\nreads (we assume here that trims are free). Run the same workloa d\nagain as above, but without showing any intermediate states (e. g.,\n-T log -s 1 -n 10 ). Can you estimate how long this workload\nwill take to complete? (default erase time is 1000 microseconds ,\nprogram time is 40, and read time is 10) Use the -Sﬂag to check\nyour answer. You can also change the erase, program, and read\ntimes with the -E, -W, -R ﬂags.\n5. Now, compare performance of the log-structured approach and the\n(very bad) direct approach ( -T direct instead of -T log ). First,\nestimate how you think the direct approach will perform, then che ck\nyour answer with the -Sﬂag. In general, how much better will the\nlog-structured approach perform than the direct one?\n6. Let us next explore the behavior of the garbage collector. To do\nso, we have to set the high ( -G) and low ( -g) watermarks appro-\npriately. First, let’s observe what happens when you run a large r\nworkload to the log-structured SSD but without any garbage col-\nlection. To do this, run with ﬂags -T log -n 1000 (the high wa-\n1Now you might complain, “But I’m a dog person!” To this, we say, too b ad! Get a cat,\nput it on your lap, and do the homework! How else will you learn, if y ou can’t even follow\nthe most basic of instructions?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n24 FLASH -BASED SSD S\ntermark default is 10, so the GC won’t run in this conﬁguration).\nWhat do you think will happen? Use -Cand perhaps -Fto see.\n7. To turn on the garbage collector, use lower values. The high wat er-\nmark (-G N ) tells the system to start collecting once Nblocks have\nbeen used; the low watermark ( -G M ) tells the system to stop col-\nlecting once there are only Mblocks in use. What watermark values\ndo you think will make for a working system? Use -Cand-Fto\nshow the commands and intermediate device states and see.\n8. One other useful ﬂag is -J, which shows what the collector is doing\nwhen it runs. Run with ﬂags -T log -n 1000 -C -J to see both\nthe commands and the GC behavior. What do you notice about the\nGC? The ﬁnal effect of GC, of course, is performance. Use -Sto\nlook at ﬁnal statistics; how many extra reads and writes occur due\nto garbage collection? Compare this to the ideal SSD ( -T ideal );\nhow much extra reading, writing, and erasing is there due to th e\nnature of Flash? Compare it also to the direct approach; in what\nway (erases, reads, programs) is the log-structured approach s upe-\nrior?\n9. One last aspect to explore is workload skew . Adding skew to the\nworkload changes writes such that more writes occur to some smalle r\nfraction of the logical block space. For example, running with -K\n80/20 makes 80% of the writes go to 20% of the blocks. Pick some\ndifferent skews and perform many randomly-chosen operations (e. g.,\n-n 1000 ), using ﬁrst -T direct to understand the skew, and then\n-T log to see the impact on a log-structured device. What do you\nexpect will happen? One other small skew control to explore is -k\n100; by adding this ﬂag to a skewed workload, the ﬁrst 100 writes\nare not skewed. The idea is to ﬁrst create a lot of data, but then onl y\nupdate some of it. What impact might that have upon a garbage\ncollector?\nOPERATING\nSYSTEMS\n[VERSION 1.01]WWW .OSTEP .ORG",61699
51-45. Data Integrity and Protection.pdf,51-45. Data Integrity and Protection,"45\nData Integrity and Protection\nBeyond the basic advances found in the ﬁle systems we have studi ed thus\nfar, a number of features are worth studying. In this chapter, w e focus on\nreliability once again (having previously studied storage sys tem reliabil-\nity in the RAID chapter). Speciﬁcally, how should a ﬁle system or s torage\nsystem ensure that data is safe, given the unreliable nature of modern\nstorage devices?\nThis general area is referred to as data integrity ordata protection .\nThus, we will now investigate techniques used to ensure that t he data\nyou put into your storage system is the same when the storage syste m\nreturns it to you.\nCRUX: HOWTOENSURE DATA INTEGRITY\nHow should systems ensure that the data written to storage is pro-\ntected? What techniques are required? How can such technique s be made\nefﬁcient, with both low space and time overheads?\n45.1 Disk Failure Modes\nAs you learned in the chapter about RAID, disks are not perfect, a nd\ncan fail (on occasion). In early RAID systems, the model of failure was\nquite simple: either the entire disk is working, or it fails comp letely, and\nthe detection of such a failure is straightforward. This fail-stop model of\ndisk failure makes building RAID relatively simple [S90].\nWhat you didn’t learn is about all of the other types of failure modes\nmodern disks exhibit. Speciﬁcally, as Bairavasundaram et al. studied\nin great detail [B+07, B+08], modern disks will occasionally se em to be\nmostly working but have trouble successfully accessing one or more blocks.\nSpeciﬁcally, two types of single-block failures are common and wor thy of\nconsideration: latent-sector errors (LSEs ) and block corruption . We’ll\nnow discuss each in more detail.\n1\n2 DATA INTEGRITY AND PROTECTION\nCheap Costly\nLSEs 9.40% 1.40%\nCorruption 0.50% 0.05%\nFigure 45.1: Frequency Of LSEs And Block Corruption\nLSEs arise when a disk sector (or group of sectors) has been damaged\nin some way. For example, if the disk head touches the surface for s ome\nreason (a head crash , something which shouldn’t happen during nor-\nmal operation), it may damage the surface, making the bits unre adable.\nCosmic rays can also ﬂip bits, leading to incorrect contents. For tunately,\nin-disk error correcting codes (ECC ) are used by the drive to determine\nwhether the on-disk bits in a block are good, and in some cases, to ﬁx\nthem; if they are not good, and the drive does not have enough informa-\ntion to ﬁx the error, the disk will return an error when a request i s issued\nto read them.\nThere are also cases where a disk block becomes corrupt in a way not\ndetectable by the disk itself. For example, buggy disk ﬁrmwar e may write\na block to the wrong location; in such a case, the disk ECC indicate s the\nblock contents are ﬁne, but from the client’s perspective the wron g block\nis returned when subsequently accessed. Similarly, a block ma y get cor-\nrupted when it is transferred from the host to the disk across a fa ulty bus;\nthe resulting corrupt data is stored by the disk, but it is not wha t the client\ndesires. These types of faults are particularly insidious bec ause they are\nsilent faults ; the disk gives no indication of the problem when returning\nthe faulty data.\nPrabhakaran et al. describes this more modern view of disk failu re as\nthefail-partial disk failure model [P+05]. In this view, disks can still fail\nin their entirety (as was the case in the traditional fail-stop model); how-\never, disks can also seemingly be working and have one or more block s\nbecome inaccessible (i.e., LSEs) or hold the wrong contents (i.e., corrup-\ntion). Thus, when accessing a seemingly-working disk, once in a while\nit may either return an error when trying to read or write a given block\n(a non-silent partial fault), and once in a while it may simply r eturn the\nwrong data (a silent partial fault).\nBoth of these types of faults are somewhat rare, but just how rare? F ig-\nure 45.1 summarizes some of the ﬁndings from the two Bairavasund aram\nstudies [B+07,B+08].\nThe ﬁgure shows the percent of drives that exhibited at least one LSE\nor block corruption over the course of the study (about 3 years, over\n1.5 million disk drives). The ﬁgure further sub-divides the r esults into\n“cheap” drives (usually SATA drives) and “costly” drives (usu ally SCSI\nor FibreChannel). As you can see, while buying better drives re duces\nthe frequency of both types of problem (by about an order of magnitude ),\nthey still happen often enough that you need to think carefully a bout how\nto handle them in your storage system.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDATA INTEGRITY AND PROTECTION 3\nSome additional ﬁndings about LSEs are:\n•Costly drives with more than one LSE are as likely to develop ad-\nditional errors as cheaper drives\n•For most drives, annual error rate increases in year two\n•The number of LSEs increase with disk size\n•Most disks with LSEs have less than 50\n•Disks with LSEs are more likely to develop additional LSEs\n•There exists a signiﬁcant amount of spatial and temporal localit y\n•Disk scrubbing is useful (most LSEs were found this way)\nSome ﬁndings about corruption:\n•Chance of corruption varies greatly across different drive model s\nwithin the same drive class\n•Age effects are different across models\n•Workload and disk size have little impact on corruption\n•Most disks with corruption only have a few corruptions\n•Corruption is not independent within a disk or across disks in RAID\n•There exists spatial locality, and some temporal locality\n•There is a weak correlation with LSEs\nTo learn more about these failures, you should likely read the orig inal\npapers [B+07,B+08]. But hopefully the main point should be clea r: if you\nreally wish to build a reliable storage system, you must includ e machin-\nery to detect and recover from both LSEs and block corruption.\n45.2 Handling Latent Sector Errors\nGiven these two new modes of partial disk failure, we should now tr y\nto see what we can do about them. Let’s ﬁrst tackle the easier of th e two,\nnamely latent sector errors.\nCRUX: HOWTOHANDLE LATENT SECTOR ERRORS\nHow should a storage system handle latent sector errors? How much\nextra machinery is needed to handle this form of partial failur e?\nAs it turns out, latent sector errors are rather straightforward to han-\ndle, as they are (by deﬁnition) easily detected. When a storage system\ntries to access a block, and the disk returns an error, the storag e system\nshould simply use whatever redundancy mechanism it has to ret urn the\ncorrect data. In a mirrored RAID, for example, the system should a ccess\nthe alternate copy; in a RAID-4 or RAID-5 system based on parity, the\nsystem should reconstruct the block from the other blocks in the par ity\ngroup. Thus, easily detected problems such as LSEs are readily r ecovered\nthrough standard redundancy mechanisms.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 DATA INTEGRITY AND PROTECTION\nThe growing prevalence of LSEs has inﬂuenced RAID designs over th e\nyears. One particularly interesting problem arises in RAID- 4/5 systems\nwhen both full-disk faults and LSEs occur in tandem. Speciﬁcally , when\nan entire disk fails, the RAID tries to reconstruct the disk (say, onto a\nhot spare) by reading through all of the other disks in the parity g roup\nand recomputing the missing values. If, during reconstruction , an LSE\nis encountered on any one of the other disks, we have a problem: the\nreconstruction cannot successfully complete.\nTo combat this issue, some systems add an extra degree of redunda ncy.\nFor example, NetApp’s RAID-DP has the equivalent of two parity disks\ninstead of one [C+04]. When an LSE is discovered during reconstruc tion,\nthe extra parity helps to reconstruct the missing block. As alw ays, there is\na cost, in that maintaining two parity blocks for each stripe is m ore costly;\nhowever, the log-structured nature of the NetApp WAFL ﬁle system mit-\nigates that cost in many cases [HLM94]. The remaining cost is sp ace, in\nthe form of an extra disk for the second parity block.\n45.3 Detecting Corruption: The Checksum\nLet’s now tackle the more challenging problem, that of silent fail ures\nvia data corruption. How can we prevent users from getting bad dat a\nwhen corruption arises, and thus leads to disks returning bad d ata?\nCRUX: HOWTOPRESERVE DATA INTEGRITY DESPITE CORRUPTION\nGiven the silent nature of such failures, what can a storage sys tem do\nto detect when corruption arises? What techniques are needed? How can\none implement them efﬁciently?\nUnlike latent sector errors, detection of corruption is a key problem.\nHow can a client tell that a block has gone bad? Once it is known that a\nparticular block is bad, recovery is the same as before: you need to have\nsome other copy of the block around (and hopefully, one that is not cor-\nrupt!). Thus, we focus here on detection techniques.\nThe primary mechanism used by modern storage systems to preser ve\ndata integrity is called the checksum . A checksum is simply the result\nof a function that takes a chunk of data (say a 4KB block) as input an d\ncomputes a function over said data, producing a small summary of th e\ncontents of the data (say 4 or 8 bytes). This summary is referred t o as the\nchecksum. The goal of such a computation is to enable a system to de tect\nif data has somehow been corrupted or altered by storing the checks um\nwith the data and then conﬁrming upon later access that the data ’s cur-\nrent checksum matches the original storage value.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDATA INTEGRITY AND PROTECTION 5\nTIP: THERE ’SNOFREE LUNCH\nThere’s No Such Thing As A Free Lunch, or TNSTAAFL for short, is\nan old American idiom that implies that when you are seemingly ge t-\nting something for free, in actuality you are likely paying some c ost for\nit. It comes from the old days when diners would advertise a free lu nch\nfor customers, hoping to draw them in; only when you went in, did you\nrealize that to acquire the “free” lunch, you had to purchase on e or more\nalcoholic beverages. Of course, this may not actually be a problem , partic-\nularly if you are an aspiring alcoholic (or typical undergraduat e student).\nCommon Checksum Functions\nA number of different functions are used to compute checksums, a nd\nvary in strength (i.e., how good they are at protecting data integ rity) and\nspeed (i.e., how quickly can they be computed). A trade-off that is com-\nmon in systems arises here: usually, the more protection you get, t he\ncostlier it is. There is no such thing as a free lunch.\nOne simple checksum function that some use is based on exclusive\nor (XOR). With XOR-based checksums, the checksum is computed b y\nXOR’ing each chunk of the data block being checksummed, thus prod uc-\ning a single value that represents the XOR of the entire block.\nTo make this more concrete, imagine we are computing a 4-byte che ck-\nsum over a block of 16 bytes (this block is of course too small to really be a\ndisk sector or block, but it will serve for the example). The 16 dat a bytes,\nin hex, look like this:\n365e c4cd ba14 8a92 ecef 2c3a 40be f666\nIf we view them in binary, we get the following:\n0011 0110 0101 1110 1100 0100 1100 1101\n1011 1010 0001 0100 1000 1010 1001 0010\n1110 1100 1110 1111 0010 1100 0011 1010\n0100 0000 1011 1110 1111 0110 0110 0110\nBecause we’ve lined up the data in groups of 4 bytes per row, it is ea sy\nto see what the resulting checksum will be: perform an XOR over e ach\ncolumn to get the ﬁnal checksum value:\n0010 0000 0001 1011 1001 0100 0000 0011\nThe result, in hex, is 0x201b9403.\nXOR is a reasonable checksum but has its limitations. If, for exa mple,\ntwo bits in the same position within each checksummed unit chan ge, the\nchecksum will not detect the corruption. For this reason, people ha ve\ninvestigated other checksum functions.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 DATA INTEGRITY AND PROTECTION\nAnother basic checksum function is addition. This approach has t he\nadvantage of being fast; computing it just requires performing 2’s-complement\naddition over each chunk of the data, ignoring overﬂow. It can detec t\nmany changes in data, but is not good if the data, for example, is sh ifted.\nA slightly more complex algorithm is known as the Fletcher check-\nsum , named (as you might guess) for the inventor, John G. Fletcher [F8 2].\nIt is quite simple to compute and involves the computation of two ch eck\nbytes,s1ands2. Speciﬁcally, assume a block Dconsists of bytes d1...\ndn;s1is deﬁned as follows: s1 = (s1 +di)mod255(computed over all\ndi);s2in turn is: s2 = (s2 +s1)mod255(again over all di) [F04]. The\nFletcher checksum is almost as strong as the CRC (see below), det ecting\nall single-bit, double-bit errors, and many burst errors [F04] .\nOne ﬁnal commonly-used checksum is known as a cyclic redundancy\ncheck (CRC ). Assume you wish to compute the checksum over a data\nblockD. All you do is treat Das if it is a large binary number (it is just\na string of bits after all) and divide it by an agreed upon value ( k). The\nremainder of this division is the value of the CRC. As it turns out, one\ncan implement this binary modulo operation rather efﬁciently, and hence\nthe popularity of the CRC in networking as well. See elsewhere for m ore\ndetails [M13].\nWhatever the method used, it should be obvious that there is no per -\nfect checksum: it is possible two data blocks with non-identica l contents\nwill have identical checksums, something referred to as a collision . This\nfact should be intuitive: after all, computing a checksum is ta king some-\nthing large (e.g., 4KB) and producing a summary that is much sm aller\n(e.g., 4 or 8 bytes). In choosing a good checksum function, we are thu s\ntrying to ﬁnd one that minimizes the chance of collisions while re main-\ning easy to compute.\nChecksum Layout\nNow that you understand a bit about how to compute a checksum, let’s\nnext analyze how to use checksums in a storage system. The ﬁrst q uestion\nwe must address is the layout of the checksum, i.e., how should che ck-\nsums be stored on disk?\nThe most basic approach simply stores a checksum with each disk s ec-\ntor (or block). Given a data block D, let us call the checksum over that\ndataC(D). Thus, without checksums, the disk layout looks like this:\nD0 D1 D2 D3 D4 D5 D6\nWith checksums, the layout adds a single checksum for every bloc k:\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDATA INTEGRITY AND PROTECTION 7C[D0]D0\nC[D1]D1\nC[D2]D2\nC[D3]D3\nC[D4]D4\nBecause checksums are usually small (e.g., 8 bytes), and dis ks only can\nwrite in sector-sized chunks (512 bytes) or multiples thereof, one problem\nthat arises is how to achieve the above layout. One solution employe d by\ndrive manufacturers is to format the drive with 520-byte sect ors; an extra\n8 bytes per sector can be used to store the checksum.\nIn disks that don’t have such functionality, the ﬁle system must ﬁgure\nout a way to store the checksums packed into 512-byte blocks. One such\npossibility is as follows:\nC[D0]\nC[D1]\nC[D2]\nC[D3]\nC[D4]D0 D1 D2 D3 D4\nIn this scheme, the nchecksums are stored together in a sector, fol-\nlowed by ndata blocks, followed by another checksum sector for the\nnextnblocks, and so forth. This approach has the beneﬁt of working\non all disks, but can be less efﬁcient; if the ﬁle system, for exa mple, wants\nto overwrite block D1, it has to read in the checksum sector containing\nC(D1), update C(D1)in it, and then write out the checksum sector and\nnew data block D1(thus, one read and two writes). The earlier approach\n(of one checksum per sector) just performs a single write.\n45.4 Using Checksums\nWith a checksum layout decided upon, we can now proceed to actu-\nally understand how to usethe checksums. When reading a block D, the\nclient (i.e., ﬁle system or storage controller) also reads its ch ecksum from\ndiskCs(D), which we call the stored checksum (hence the subscript Cs).\nThe client then computes the checksum over the retrieved block D, which\nwe call the computed checksum Cc(D). At this point, the client com-\npares the stored and computed checksums; if they are equal (i.e .,Cs(D)\n==Cc(D), the data has likely not been corrupted, and thus can be safely\nreturned to the user. If they do notmatch (i.e., Cs(D)!=Cc(D)), this im-\nplies the data has changed since the time it was stored (since t he stored\nchecksum reﬂects the value of the data at that time). In this ca se, we have\na corruption, which our checksum has helped us to detect.\nGiven a corruption, the natural question is what should we do about\nit? If the storage system has a redundant copy, the answer is eas y: try to\nuse it instead. If the storage system has no such copy, the likel y answer is\nto return an error. In either case, realize that corruption dete ction is not a\nmagic bullet; if there is no other way to get the non-corrupted da ta, you\nare simply out of luck.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 DATA INTEGRITY AND PROTECTION\n45.5 A New Problem: Misdirected Writes\nThe basic scheme described above works well in the general case of\ncorrupted blocks. However, modern disks have a couple of unusual fa il-\nure modes that require different solutions.\nThe ﬁrst failure mode of interest is called a misdirected write . This\narises in disk and RAID controllers which write the data to disk correctly,\nexcept in the wrong location. In a single-disk system, this means that the\ndisk wrote block Dxnot to address x(as desired) but rather to address\ny(thus “corrupting” Dy); in addition, within a multi-disk system, the\ncontroller may also write Di,xnot to address xof diskibut rather to\nsome other disk j. Thus our question:\nCRUX: HOWTOHANDLE MISDIRECTED WRITES\nHow should a storage system or disk controller detect misdirected\nwrites? What additional features are required from the checks um?\nThe answer, not surprisingly, is simple: add a little more infor mation\nto each checksum. In this case, adding a physical identiﬁer (physical ID )\nis quite helpful. For example, if the stored information now contai ns the\nchecksum C(D)and both the disk and sector numbers of the block, it is\neasy for the client to determine whether the correct information resides\nwithin a particular locale. Speciﬁcally, if the client is read ing block 4 on\ndisk 10 ( D10,4), the stored information should include that disk number\nand sector offset, as shown below. If the information does not match, a\nmisdirected write has taken place, and a corruption is now detec ted. Here\nis an example of what this added information would look like on a two-\ndisk system. Note that this ﬁgure, like the others before it, is n ot to scale,\nas the checksums are usually small (e.g., 8 bytes) whereas th e blocks are\nmuch larger (e.g., 4 KB or bigger):\nDisk 0Disk 1\nC[D0]\ndisk=0\nblock=0D0\nC[D1]\ndisk=0\nblock=1D1\nC[D2]\ndisk=0\nblock=2D2C[D0]\ndisk=1\nblock=0D0\nC[D1]\ndisk=1\nblock=1D1\nC[D2]\ndisk=1\nblock=2D2\nYou can see from the on-disk format that there is now a fair amount of\nredundancy on disk: for each block, the disk number is repeated w ithin\neach block, and the offset of the block in question is also kept next to the\nblock itself. The presence of redundant information should be no s ur-\nprise, though; redundancy is the key to error detection (in this case) and\nrecovery (in others). A little extra information, while not stric tly needed\nwith perfect disks, can go a long ways in helping detect problem atic situ-\nations should they arise.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDATA INTEGRITY AND PROTECTION 9\n45.6 One Last Problem: Lost Writes\nUnfortunately, misdirected writes are not the last problem we w ill\naddress. Speciﬁcally, some modern storage devices also have an i ssue\nknown as a lost write , which occurs when the device informs the up-\nper layer that a write has completed but in fact it never is pers isted; thus,\nwhat remains is the old contents of the block rather than the updat ed new\ncontents.\nThe obvious question here is: do any of our checksumming strategie s\nfrom above (e.g., basic checksums, or physical identity) help t o detect\nlost writes? Unfortunately, the answer is no: the old block likely has a\nmatching checksum, and the physical ID used above (disk numbe r and\nblock offset) will also be correct. Thus our ﬁnal problem:\nCRUX: HOWTOHANDLE LOST WRITES\nHow should a storage system or disk controller detect lost writes?\nWhat additional features are required from the checksum?\nThere are a number of possible solutions that can help [K+08]. One\nclassic approach [BS04] is to perform a write verify orread-after-write ;\nby immediately reading back the data after a write, a system c an ensure\nthat the data indeed reached the disk surface. This approach, however, is\nquite slow, doubling the number of I/Os needed to complete a write .\nSome systems add a checksum elsewhere in the system to detect los t\nwrites. For example, Sun’s Zettabyte File System (ZFS) includes a check-\nsum in each ﬁle system inode and indirect block for every block inc luded\nwithin a ﬁle. Thus, even if the write to a data block itself is los t, the check-\nsum within the inode will not match the old data. Only if the write s to\nboth the inode and the data are lost simultaneously will such a sch eme\nfail, an unlikely (but unfortunately, possible!) situation.\n45.7 Scrubbing\nGiven all of this discussion, you might be wondering: when do thes e\nchecksums actually get checked? Of course, some amount of checki ng\noccurs when data is accessed by applications, but most data is ra rely\naccessed, and thus would remain unchecked. Unchecked data is prob-\nlematic for a reliable storage system, as bit rot could eventuall y affect all\ncopies of a particular piece of data.\nTo remedy this problem, many systems utilize disk scrubbing of var-\nious forms [K+08]. By periodically reading through every block of the\nsystem, and checking whether checksums are still valid, the disk system\ncan reduce the chances that all copies of a certain data item bec ome cor-\nrupted. Typical systems schedule scans on a nightly or weekly b asis.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 DATA INTEGRITY AND PROTECTION\n45.8 Overheads Of Checksumming\nBefore closing, we now discuss some of the overheads of using check-\nsums for data protection. There are two distinct kinds of overhead s, as is\ncommon in computer systems: space and time.\nSpace overheads come in two forms. The ﬁrst is on the disk (or other\nstorage medium) itself; each stored checksum takes up room on the d isk,\nwhich can no longer be used for user data. A typical ratio might b e an 8-\nbyte checksum per 4 KB data block, for a 0.19% on-disk space overhe ad.\nThe second type of space overhead comes in the memory of the sys-\ntem. When accessing data, there must now be room in memory for the\nchecksums as well as the data itself. However, if the system si mply checks\nthe checksum and then discards it once done, this overhead is shor t-lived\nand not much of a concern. Only if checksums are kept in memory (for\nan added level of protection against memory corruption [Z+13]) wil l this\nsmall overhead be observable.\nWhile space overheads are small, the time overheads induced by check-\nsumming can be quite noticeable. Minimally, the CPU must compu te the\nchecksum over each block, both when the data is stored (to determi ne the\nvalue of the stored checksum) and when it is accessed (to compute the\nchecksum again and compare it against the stored checksum). On e ap-\nproach to reducing CPU overheads, employed by many systems that use\nchecksums (including network stacks), is to combine data copyi ng and\nchecksumming into one streamlined activity; because the copy is needed\nanyhow (e.g., to copy the data from the kernel page cache into a us er\nbuffer), combined copying/checksumming can be quite effecti ve.\nBeyond CPU overheads, some checksumming schemes can induce ex-\ntra I/O overheads, particularly when checksums are stored dis tinctly from\nthe data (thus requiring extra I/Os to access them), and for an y extra I/O\nneeded for background scrubbing. The former can be reduced by de sign;\nthe latter can be tuned and thus its impact limited, perhaps b y control-\nling when such scrubbing activity takes place. The middle of t he night,\nwhen most (not all!) productive workers have gone to bed, may be a\ngood time to perform such scrubbing activity and increase the rob ustness\nof the storage system.\n45.9 Summary\nWe have discussed data protection in modern storage systems, focu s-\ning on checksum implementation and usage. Different checksum s protect\nagainst different types of faults; as storage devices evolve, n ew failure\nmodes will undoubtedly arise. Perhaps such change will force th e re-\nsearch community and industry to revisit some of these basic app roaches,\nor invent entirely new approaches altogether. Time will tell. O r it won’t.\nTime is funny that way.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDATA INTEGRITY AND PROTECTION 11\nReferences\n[B+07] “An Analysis of Latent Sector Errors in Disk Drives” by L. B airavasundaram, G. Good-\nson, S. Pasupathy, J. Schindler. SIGMETRICS ’07, San Diego, CA. The ﬁrst paper to study latent\nsector errors in detail. The paper also won the Kenneth C. Sevcik Outstandin g Student Paper award,\nnamed after a brilliant researcher and wonderful guy who passed away too soon . To show the OSTEP\nauthors it was possible to move from the U.S. to Canada, Ken once sang us the Canadian national\nanthem, standing up in the middle of a restaurant to do so. We chose the U.S., b ut got this memory.\n[B+08] “An Analysis of Data Corruption in the Storage Stack” by La kshmi N. Bairavasun-\ndaram, Garth R. Goodson, Bianca Schroeder, Andrea C. Arpaci-Dusseau , Remzi H. Arpaci-\nDusseau. FAST ’08, San Jose, CA, February 2008. The ﬁrst paper to truly study disk corruption in\ngreat detail, focusing on how often such corruption occurs over three years for over 1.5 million drives.\n[BS04] “Commercial Fault Tolerance: A Tale of Two Systems” by Wend y Bartlett, Lisa Spainhower.\nIEEE Transactions on Dependable and Secure Computing, Vol. 1:1, Janua ry 2004. This classic\nin building fault tolerant systems is an excellent overview of the state of th e art from both IBM and\nTandem. Another must read for those interested in the area.\n[C+04] “Row-Diagonal Parity for Double Disk Failure Correction” by P . Corbett, B. English, A.\nGoel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar. FAST ’04, San Jose, C A, February 2004. An\nearly paper on how extra redundancy helps to solve the combined full-disk-f ailure/partial-disk-failure\nproblem. Also a nice example of how to mix more theoretical work with practical .\n[F04] “Checksums and Error Control” by Peter M. Fenwick. Copy availabl e online here:\nhttp://www.ostep.org/Citations/checksums-03.pdf .A great simple tutorial on check-\nsums, available to you for the amazing cost of free.\n[F82] “An Arithmetic Checksum for Serial Transmissions” by John G. Fle tcher. IEEE Trans-\nactions on Communication, Vol. 30:1, January 1982. Fletcher’s original work on his eponymous\nchecksum. He didn’t call it the Fletcher checksum, rather he just did n’t call it anything; later, others\nnamed it after him. So don’t blame old Fletch for this seeming act of braggadoc io. This anecdote might\nremind you of Rubik; Rubik never called it “ Rubik’s cube ”; rather, he just called it “my cube.”\n[HLM94] “File System Design for an NFS File Server Appliance” by Da ve Hitz, James Lau,\nMichael Malcolm. USENIX Spring ’94. The pioneering paper that describes the ideas and product at\nthe heart of NetApp’s core. Based on this system, NetApp has grown into a multi -billion dollar storage\ncompany. To learn more about NetApp, read Hitz’s autobiography “How to Castrate a Bul l” (which is\nthe actual title, no joking). And you thought you could avoid bull castration by going i nto CS.\n[K+08] “Parity Lost and Parity Regained” by Andrew Krioukov, Lakshm i N. Bairavasun-\ndaram, Garth R. Goodson, Kiran Srinivasan, Randy Thelen, Andrea C. Ar paci-Dusseau, Remzi\nH. Arpaci-Dusseau. FAST ’08, San Jose, CA, February 2008. This work explores how different\nchecksum schemes work (or don’t work) in protecting data. We reveal a number of interesting ﬂaws in\ncurrent protection strategies.\n[M13] “Cyclic Redundancy Checks” by unknown. Available: http://www.mathpages.com/\nhome/kmath458.htm .A super clear and concise description of CRCs. The internet is full of i nfor-\nmation, as it turns out.\n[P+05] “IRON File Systems” by V . Prabhakaran, L. Bairavasundaram, N . Agrawal, H. Gunawi,\nA. Arpaci-Dusseau, R. Arpaci-Dusseau. SOSP ’05, Brighton, England. Our paper on how disks\nhave partial failure modes, and a detailed study of how modern ﬁle systems reac t to such failures. As it\nturns out, rather poorly! We found numerous bugs, design ﬂaws, and other oddi ties in this work. Some\nof this has fed back into the Linux community, thus improving ﬁle system re liability. You’re welcome!\n[RO91] “Design and Implementation of the Log-structured File Syste m” by Mendel Rosen-\nblum and John Ousterhout. SOSP ’91, Paciﬁc Grove, CA, October 1991. So cool we cite it again.\n[S90] “Implementing Fault-Tolerant Services Using The State Machine Appr oach: A Tutorial”\nby Fred B. Schneider. ACM Surveys, Vol. 22, No. 4, December 1990. How to build fault tolerant\nservices. A must read for those building distributed systems.\n[Z+13] “Zettabyte Reliability with Flexible End-to-end Data Inte grity” by Y. Zhang, D. Myers,\nA. Arpaci-Dusseau, R. Arpaci-Dusseau. MSST ’13, Long Beach, Californi a, May 2013. How to\nadd data protection to the page cache of a system. Out of space, otherwise we would w rite something...\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 DATA INTEGRITY AND PROTECTION\nHomework (Simulation)\nIn this homework, you’ll use checksum.py to investigate various as-\npects of checksums.\nQuestions\n1. First just run checksum.py with no arguments. Compute the additive,\nXOR-based, and Fletcher checksums. Use -cto check your answers.\n2. Now do the same, but vary the seed ( -s) to different values.\n3. Sometimes the additive and XOR-based checksums produce the same c heck-\nsum (e.g., if the data value is all zeroes). Can you pass in a 4-by te data value\n(using the -Dﬂag, e.g.,-D a,b,c,d ) that does not contain only zeroes and\nleads the additive and XOR-based checksum having the same value? In\ngeneral, when does this occur? Check that you are correct with the-cﬂag.\n4. Now pass in a 4-byte value that you know will produce a differe nt checksum\nvalues for additive and XOR. In general, when does this occur?\n5. Use the simulator to compute checksums twice (once each for a diffe rent set\nof numbers). The two number strings should be different (e.g., -D a1,b1,c1,d1\nthe ﬁrst time and -D a2,b2,c2,d2 the second) but should produce the\nsame additive checksum. In general, when will the additive chec ksum be\nthe same, even though the data values are different? Check your sp eciﬁc\nanswer with the -cﬂag.\n6. Now do the same for the XOR checksum.\n7. Now let’s look at a speciﬁc set of data values. The ﬁrst is: -D 1,2,3,4 .\nWhat will the different checksums (additive, XOR, Fletcher) be for this data?\nNow compare it to computing these checksums over -D 4,3,2,1 . What\ndo you notice about these three checksums? How does Fletcher comp are to\nthe other two? How is Fletcher generally “better” than someth ing like the\nsimple additive checksum?\n8. No checksum is perfect. Given a particular input of your choosi ng, can you\nﬁnd other data values that lead to the same Fletcher checksum? Whe n, in\ngeneral, does this occur? Start with a simple data string (e.g. ,-D 0,1,2,3 )\nand see if you can replace one of those numbers but end up with the sa me\nFletcher checksum. As always, use -cto check your answers.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDATA INTEGRITY AND PROTECTION 13\nHomework (Code)\nIn this part of the homework, you’ll write some of your own code to\nimplement various checksums.\nQuestions\n1. Write a short C program (called check-xor.c ) that computes an XOR-\nbased checksum over an input ﬁle, and prints the checksum as output . Use\na 8-bit unsigned char to store the (one byte) checksum. Make some t est ﬁles\nto see if it works as expected.\n2. Now write a short C program (called check-fletcher.c ) that computes\nthe Fletcher checksum over an input ﬁle. Once again, test your pr ogram to\nsee if it works.\n3. Now compare the performance of both: is one faster than the ot her? How\ndoes performance change as the size of the input ﬁle changes? Use internal\ncalls togettimeofday to time the programs. Which should you use if you\ncare about performance? About checking ability?\n4. Read about the 16-bit CRC and then implement it. Test it on a numbe r of\ndifferent inputs to ensure that it works. How is its performance as compared\nto the simple XOR and Fletcher? How about its checking ability?\n5. Now build a tool ( create-csum.c ) that computes a single-byte checksum\nfor every 4KB block of a ﬁle, and records the results in an output ﬁ le (speci-\nﬁed on the command line). Build a related tool ( check-csum.c ) that reads\na ﬁle, computes the checksums over each block, and compares the res ults\nto the stored checksums stored in another ﬁle. If there is a prob lem, the\nprogram should print that the ﬁle has been corrupted. Test the p rogram by\nmanually corrupting the ﬁle.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",33977
52-46. Summary Dialogue on Persistence.pdf,52-46. Summary Dialogue on Persistence,"46\nSummary Dialogue on Persistence\nStudent: Wow, ﬁle systems seem interesting(!), and yet complicated.\nProfessor: That’s why my spouse and I do our research in this space.\nStudent: Hold on. Are you one of the professors who wrote this book? I thoug ht\nwe were both just fake constructs, used to summarize some main poin ts, and\nperhaps add a little levity in the study of operating systems.\nProfessor: Uh... er... maybe. And none of your business! And who did you\nthink was writing these things? (sighs) Anyhow, let’s get on with it: w hat did\nyou learn?\nStudent: Well, I think I got one of the main points, which is that it is much\nharder to manage data for a long time (persistently) than it is to mana ge data\nthat isn’t persistent (like the stuff in memory). After all, if your mach ines crashes,\nmemory contents disappear! But the stuff in the ﬁle system needs to live forever.\nProfessor: Well, as my friend Kevin Hultquist used to say, “Forever is a long\ntime”; while he was talking about plastic golf tees, it’s especially true fo r the\ngarbage that is found in most ﬁle systems.\nStudent: Well, you know what I mean! For a long time at least. And even simple\nthings, such as updating a persistent storage device, are complica ted, because you\nhave to care what happens if you crash. Recovery, something I ha d never even\nthought of when we were virtualizing memory, is now a big deal!\nProfessor: Too true. Updates to persistent storage have always been, and r e-\nmain, a fun and challenging problem.\nStudent: I also learned about cool things like disk scheduling, and about data\nprotection techniques like RAID and even checksums. That stuff is c ool.\nProfessor: I like those topics too. Though, if you really get into it, they can get a\nlittle mathematical. Check out some the latest on erasure codes if yo u want your\nbrain to hurt.\nStudent: I’ll get right on that.\n1\n2 SUMMARY DIALOGUE ON PERSISTENCE\nProfessor: (frowns) I think you’re being sarcastic. Well, what else did you like?\nStudent: And I also liked all the thought that has gone into building technology-\naware systems, like FFS and LFS. Neat stuff! Being disk aware seems c ool. But\nwill it matter anymore, with Flash and all the newest, latest technolo gies?\nProfessor: Good question! And a reminder to get working on that Flash chap-\nter... (scribbles note down to self) ... But yes, even with Flash, all of this stuff\nis still relevant, amazingly. For example, Flash Translation Layers (F TLs) use\nlog-structuring internally, to improve performance and reliability of F lash-based\nSSDs. And thinking about locality is always useful. So while the technolog y\nmay be changing, many of the ideas we have studied will continue to be useful,\nfor a while at least.\nStudent: That’s good. I just spent all this time learning it, and I didn’t want it\nto all be for no reason!\nProfessor: Professors wouldn’t do that to you, would they?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",2995
53-47. A Dialogue on Distribution.pdf,53-47. A Dialogue on Distribution,"47\nA Dialogue on Distribution\nProfessor: And thus we reach our ﬁnal little piece in the world of operating\nsystems: distributed systems. Since we can’t cover much here, w e’ll sneak in a\nlittle intro here in the section on persistence, and focus mostly on dist ributed ﬁle\nsystems. Hope that is OK!\nStudent: Sounds OK. But what is a distributed system exactly, oh glorious and\nall-knowing professor?\nProfessor: Well, I bet you know how this is going to go...\nStudent: There’s a peach?\nProfessor: Exactly! But this time, it’s far away from you, and may take some\ntime to get the peach. And there are a lot of them! Even worse, som etimes a\npeach becomes rotten. But you want to make sure that when anyb ody bites into\na peach, they will get a mouthful of deliciousness.\nStudent: This peach analogy is working less and less for me.\nProfessor: Come on! It’s the last one, just go with it.\nStudent: Fine.\nProfessor: So anyhow, forget about the peaches. Building distributed systems\nis hard, because things fail all the time. Messages get lost, machines go down,\ndisks corrupt data. It’s like the whole world is working against you!\nStudent: But I use distributed systems all the time, right?\nProfessor: Yes! You do. And... ?\nStudent: Well, it seems like they mostly work. After all, when I send a search\nrequest to Google, it usually comes back in a snap, with some great re sults! Same\nthing when I use Facebook, Amazon, and so forth.\n1\n2 A D IALOGUE ON DISTRIBUTION\nProfessor: Yes, it is amazing. And that’s despite all of those failures taking\nplace! Those companies build a huge amount of machinery into their sy stems so\nas to ensure that even though some machines have failed, the entire system stays\nup and running. They use a lot of techniques to do this: replication, r etry, and\nvarious other tricks people have developed over time to detect and recover from\nfailures.\nStudent: Sounds interesting. Time to learn something for real?\nProfessor: It does seem so. Let’s get to work! But ﬁrst things ﬁrst ...\n(bites into peach he has been holding, which unfortunately is rotten)\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",2166
54-48. Distributed Systems.pdf,54-48. Distributed Systems,"48\nDistributed Systems\nDistributed systems have changed the face of the world. When you r web\nbrowser connects to a web server somewhere else on the planet, it i s par-\nticipating in what seems to be a simple form of a client/server distributed\nsystem. When you contact a modern web service such as Google or Face-\nbook, you are not just interacting with a single machine, however; be-\nhind the scenes, these complex services are built from a large c ollection\n(i.e., thousands) of machines, each of which cooperate to provide t he par-\nticular service of the site. Thus, it should be clear what makes studying\ndistributed systems interesting. Indeed, it is worthy of an en tire class;\nhere, we just introduce a few of the major topics.\nA number of new challenges arise when building a distributed s ystem.\nThe major one we focus on is failure ; machines, disks, networks, and\nsoftware all fail from time to time, as we do not (and likely, will never)\nknow how to build “perfect” components and systems. However, when\nwe build a modern web service, we’d like it to appear to clients a s if it\nnever fails; how can we accomplish this task?\nTHECRUX:\nHOWTOBUILD SYSTEMS THAT WORK WHEN COMPONENTS FAIL\nHow can we build a working system out of parts that don’t work correctly\nall the time? The basic question should remind you of some of the topic s\nwe discussed in RAID storage arrays; however, the problems here tend\nto be more complex, as are the solutions.\nInterestingly, while failure is a central challenge in const ructing dis-\ntributed systems, it also represents an opportunity. Yes, mac hines fail;\nbut the mere fact that a machine fails does not imply the entire s ystem\nmust fail. By collecting together a set of machines, we can build a sys-\ntem that appears to rarely fail, despite the fact that its comp onents fail\nregularly. This reality is the central beauty and value of dis tributed sys-\ntems, and why they underly virtually every modern web service you use,\nincluding Google, Facebook, etc.\n1\n2 DISTRIBUTED SYSTEMS\nTIP: COMMUNICATION ISINHERENTLY UNRELIABLE\nIn virtually all circumstances, it is good to view communication as a\nfundamentally unreliable activity. Bit corruption, down or non- working\nlinks and machines, and lack of buffer space for incoming packet s all lead\nto the same result: packets sometimes do not reach their destin ation. To\nbuild reliable services atop such unreliable networks, we mus t consider\ntechniques that can cope with packet loss.\nOther important issues exist as well. System performance is often crit-\nical; with a network connecting our distributed system together , system\ndesigners must often think carefully about how to accomplish the ir given\ntasks, trying to reduce the number of messages sent and furthe r make\ncommunication as efﬁcient (low latency, high bandwidth) as poss ible.\nFinally, security is also a necessary consideration. When connecting\nto a remote site, having some assurance that the remote party is w ho\nthey say they are becomes a central problem. Further, ensuring that third\nparties cannot monitor or alter an on-going communication between tw o\nothers is also a challenge.\nIn this introduction, we’ll cover the most basic aspect that is new in\na distributed system: communication . Namely, how should machines\nwithin a distributed system communicate with one another? We’ll start\nwith the most basic primitives available, messages, and buil d a few higher-\nlevel primitives on top of them. As we said above, failure will be a central\nfocus: how should communication layers handle failures?\n48.1 Communication Basics\nThe central tenet of modern networking is that communication is fu n-\ndamentally unreliable. Whether in the wide-area Internet, or a local-area\nhigh-speed network such as Inﬁniband, packets are regularly lost, cor-\nrupted, or otherwise do not reach their destination.\nThere are a multitude of causes for packet loss or corruption. Some-\ntimes, during transmission, some bits get ﬂipped due to electr ical or other\nsimilar problems. Sometimes, an element in the system, such as a net-\nwork link or packet router or even the remote host, are somehow dam-\naged or otherwise not working correctly; network cables do acciden tally\nget severed, at least sometimes.\nMore fundamental however is packet loss due to lack of buffering\nwithin a network switch, router, or endpoint. Speciﬁcally, even i f we\ncould guarantee that all links worked correctly, and that all th e compo-\nnents in the system (switches, routers, end hosts) were up and r unning as\nexpected, loss is still possible, for the following reason. Imagin e a packet\narrives at a router; for the packet to be processed, it must be pla ced in\nmemory somewhere within the router. If many such packets arrive at\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDISTRIBUTED SYSTEMS 3\n// client code\nint main(int argc, char *argv[]) {\nint sd = UDP_Open(20000);\nstruct sockaddr_in addrSnd, addrRcv;\nint rc = UDP_FillSockAddr(&addrSnd, ""machine.cs.wisc.ed u"", 10000);\nchar message[BUFFER_SIZE];\nsprintf(message, ""hello world"");\nrc = UDP_Write(sd, &addrSnd, message, BUFFER_SIZE);\nif (rc > 0) {\nint rc = UDP_Read(sd, &addrRcv, message, BUFFER_SIZE);\n}\nreturn 0;\n}\n// server code\nint main(int argc, char *argv[]) {\nint sd = UDP_Open(10000);\nassert(sd > -1);\nwhile (1) {\nstruct sockaddr_in addr;\nchar message[BUFFER_SIZE];\nint rc = UDP_Read(sd, &addr, message, BUFFER_SIZE);\nif (rc > 0) {\nchar reply[BUFFER_SIZE];\nsprintf(reply, ""goodbye world"");\nrc = UDP_Write(sd, &addr, reply, BUFFER_SIZE);\n}\n}\nreturn 0;\n}\nFigure 48.1: Example UDP/IP Client/Server Code\nonce, it is possible that the memory within the router cannot accomm o-\ndate all of the packets. The only choice the router has at that point is\ntodrop one or more of the packets. This same behavior occurs at end\nhosts as well; when you send a large number of messages to a single ma-\nchine, the machine’s resources can easily become overwhelmed, a nd thus\npacket loss again arises.\nThus, packet loss is fundamental in networking. The question th us\nbecomes: how should we deal with it?\n48.2 Unreliable Communication Layers\nOne simple way is this: we don’t deal with it. Because some appli-\ncations know how to deal with packet loss, it is sometimes useful to let\nthem communicate with a basic unreliable messaging layer, an example\nof the end-to-end argument one often hears about (see the Aside at end\nof chapter). One excellent example of such an unreliable layer is found\nin the UDP/IP networking stack available today on virtually all modern\nsystems. To use UDP , a process uses the sockets API in order to create a\ncommunication endpoint ; processes on other machines (or on the same\nmachine) send UDP datagrams to the original process (a datagram is a\nﬁxed-sized message up to some max size).\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 DISTRIBUTED SYSTEMS\nint UDP_Open(int port) {\nint sd;\nif ((sd = socket(AF_INET, SOCK_DGRAM, 0)) == -1) { return -1; }\nstruct sockaddr_in myaddr;\nbzero(&myaddr, sizeof(myaddr));\nmyaddr.sin_family = AF_INET;\nmyaddr.sin_port = htons(port);\nmyaddr.sin_addr.s_addr = INADDR_ANY;\nif (bind(sd, (struct sockaddr *) &myaddr, sizeof(myaddr)) == -1) {\nclose(sd);\nreturn -1;\n}\nreturn sd;\n}\nint UDP_FillSockAddr(struct sockaddr_in *addr, char *hostName, int port) {\nbzero(addr, sizeof(struct sockaddr_in));\naddr->sin_family = AF_INET; // host byte order\naddr->sin_port = htons(port); // short, network byte order\nstruct in_addr *inAddr;\nstruct hostent *hostEntry;\nif ((hostEntry = gethostbyname(hostName)) == NULL) { retur n -1; }\ninAddr = (struct in_addr *) hostEntry->h_addr;\naddr->sin_addr = *inAddr;\nreturn 0;\n}\nint UDP_Write(int sd, struct sockaddr_in *addr, char *buffer, int n) {\nint addrLen = sizeof(struct sockaddr_in);\nreturn sendto(sd, buffer, n, 0, (struct sockaddr *) addr, addrLen);\n}\nint UDP_Read(int sd, struct sockaddr_in *addr, char *buffer, int n) {\nint len = sizeof(struct sockaddr_in);\nreturn recvfrom(sd, buffer, n, 0, (struct sockaddr *) addr,\n(socklen_t *) &len);\n}\nFigure 48.2: A Simple UDP Library\nFigures 48.1 and 48.2 show a simple client and server built on top of\nUDP/IP . The client can send a message to the server, which the n responds\nwith a reply. With this small amount of code, you have all you need to\nbegin building distributed systems!\nUDP is a great example of an unreliable communication layer. If y ou\nuse it, you will encounter situations where packets get lost (drop ped) and\nthus do not reach their destination; the sender is never thus in formed of\nthe loss. However, that does not mean that UDP does not guard against\nany failures at all. For example, UDP includes a checksum to detect some\nforms of packet corruption.\nHowever, because many applications simply want to send data to a\ndestination and not worry about packet loss, we need more. Speciﬁcal ly,\nwe need reliable communication on top of an unreliable network.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDISTRIBUTED SYSTEMS 5\nTIP: USECHECKSUMS FORINTEGRITY\nChecksums are a commonly-used method to detect corruption quickl y\nand effectively in modern systems. A simple checksum is addit ion: just\nsum up the bytes of a chunk of data; of course, many other more sophis-\nticated checksums have been created, including basic cycli c redundancy\ncodes (CRCs), the Fletcher checksum, and many others [MK09].\nIn networking, checksums are used as follows. Before sending a me ssage\nfrom one machine to another, compute a checksum over the bytes of the\nmessage. Then send both the message and the checksum to the des ti-\nnation. At the destination, the receiver computes a checksum ove r the\nincoming message as well; if this computed checksum matches th e sent\nchecksum, the receiver can feel some assurance that the data l ikely did\nnot get corrupted during transmission.\nChecksums can be evaluated along a number of different axes. Ef fective-\nness is one primary consideration: does a change in the data lead t o a\nchange in the checksum? The stronger the checksum, the harder it is for\nchanges in the data to go unnoticed. Performance is the other imp ortant\ncriterion: how costly is the checksum to compute? Unfortunately, effec-\ntiveness and performance are often at odds, meaning that checks ums of\nhigh quality are often expensive to compute. Life, again, isn’t perfect.\n48.3 Reliable Communication Layers\nTo build a reliable communication layer, we need some new mech-\nanisms and techniques to handle packet loss. Let us consider a s imple\nexample in which a client is sending a message to a server over a n unreli-\nable connection. The ﬁrst question we must answer: how does the sen der\nknow that the receiver has actually received the message?\nThe technique that we will use is known as an acknowledgment , or\nackfor short. The idea is simple: the sender sends a message to the r e-\nceiver; the receiver then sends a short message back to acknowledge its\nreceipt. Figure 48.3 depicts the process.\nSender\n[send message]Receiver\n[receive message]\n[send ack]\n[receive ack]\nFigure 48.3: Message Plus Acknowledgment\nWhen the sender receives an acknowledgment of the message, it c an\nthen rest assured that the receiver did indeed receive the ori ginal mes-\nsage. However, what should the sender do if it does not receive an a c-\nknowledgment?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 DISTRIBUTED SYSTEMS\nSender\n[send message;\n keep copy;\n set timer]Receiver\n...\n (waiting for ack)\n...\n[timer goes off;\n set timer/retry]\n[receive message]\n[send ack]\n[receive ack;\n delete copy/timer off]\nFigure 48.4: Message Plus Acknowledgment: Dropped Request\nTo handle this case, we need an additional mechanism, known as a\ntimeout . When the sender sends a message, the sender now sets a timer\nto go off after some period of time. If, in that time, no acknowledgm ent\nhas been received, the sender concludes that the message has b een lost.\nThe sender then simply performs a retry of the send, sending the same\nmessage again with hopes that this time, it will get through. For this\napproach to work, the sender must keep a copy of the message around,\nin case it needs to send it again. The combination of the timeout an d\nthe retry have led some to call the approach timeout/retry ; pretty clever\ncrowd, those networking types, no? Figure 48.4 shows an example.\nUnfortunately, timeout/retry in this form is not quite enough. Fi gure\n48.5 shows an example of packet loss which could lead to trouble. In this\nexample, it is not the original message that gets lost, but the ac knowledg-\nment. From the perspective of the sender, the situation seems th e same:\nno ack was received, and thus a timeout and retry are in order. Bu t from\nthe perspective of the receiver, it is quite different: now the same message\nhas been received twice! While there may be cases where this i s OK, in\ngeneral it is not; imagine what would happen when you are downloadi ng\na ﬁle and extra packets are repeated inside the download. Thus, when we\nare aiming for a reliable message layer, we also usually want t o guarantee\nthat each message is received exactly once by the receiver.\nTo enable the receiver to detect duplicate message transmis sion, the\nsender has to identify each message in some unique way, and the receiver\nneeds some way to track whether it has already seen each messag e be-\nfore. When the receiver sees a duplicate transmission, it simp ly acks the\nmessage, but (critically) does notpass the message to the application that\nreceives the data. Thus, the sender receives the ack but the m essage is not\nreceived twice, preserving the exactly-once semantics ment ioned above.\nThere are myriad ways to detect duplicate messages. For examp le, the\nsender could generate a unique ID for each message; the receive r could\ntrack every ID it has ever seen. This approach could work, but it i s pro-\nhibitively costly, requiring unbounded memory to track all IDs .\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDISTRIBUTED SYSTEMS 7\nSender\n[send message;\n keep copy;\n set timer]Receiver\n[receive message]\n[send ack]\n...\n (waiting for ack)\n...\n[timer goes off;\n set timer/retry]\n[receive message]\n[send ack]\n[receive ack;\n delete copy/timer off]\nFigure 48.5: Message Plus Acknowledgment: Dropped Reply\nA simpler approach, requiring little memory, solves this proble m, and\nthe mechanism is known as a sequence counter . With a sequence counter,\nthe sender and receiver agree upon a start value (e.g., 1) for a c ounter\nthat each side will maintain. Whenever a message is sent, the current\nvalue of the counter is sent along with the message; this counter v alue\n(N) serves as an ID for the message. After the message is sent, the sender\nthen increments the value (to N+1).\nThe receiver uses its counter value as the expected value for th e ID\nof the incoming message from that sender. If the ID of a received me s-\nsage (N) matches the receiver’s counter (also N), it acks the message and\npasses it up to the application; in this case, the receiver conc ludes this\nis the ﬁrst time this message has been received. The receiver then incre-\nments its counter (to N+1), and waits for the next message.\nIf the ack is lost, the sender will timeout and re-send message N. This\ntime, the receiver’s counter is higher ( N+1), and thus the receiver knows\nit has already received this message. Thus it acks the messag e but does\nnotpass it up to the application. In this simple manner, sequence counters\ncan be used to avoid duplicates.\nThe most commonly used reliable communication layer is known as\nTCP/IP , or just TCP for short. TCP has a great deal more sophistication\nthan we describe above, including machinery to handle congest ion in the\nnetwork [VJ88], multiple outstanding requests, and hundreds of other\nsmall tweaks and optimizations. Read more about it if you’re curious ;\nbetter yet, take a networking course and learn that material we ll.\n48.4 Communication Abstractions\nGiven a basic messaging layer, we now approach the next question\nin this chapter: what abstraction of communication should we use w hen\nbuilding a distributed system?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 DISTRIBUTED SYSTEMS\nTIP: BECAREFUL SETTING THETIMEOUT VALUE\nAs you can probably guess from the discussion, setting the timeout value\ncorrectly is an important aspect of using timeouts to retry messa ge sends.\nIf the timeout is too small, the sender will re-send messages ne edlessly,\nthus wasting CPU time on the sender and network resources. If the time-\nout is too large, the sender waits too long to re-send and thus perc eived\nperformance at the sender is reduced. The “right” value, from t he per-\nspective of a single client and server, is thus to wait just long enough to\ndetect packet loss but no longer.\nHowever, there are often more than just a single client and serve r in a\ndistributed system, as we will see in future chapters. In a sc enario with\nmany clients sending to a single server, packet loss at the ser ver may be\nan indicator that the server is overloaded. If true, clients mig ht retry in\na different adaptive manner; for example, after the ﬁrst time out, a client\nmight increase its timeout value to a higher amount, perhaps tw ice as\nhigh as the original value. Such an exponential back-off scheme, pio-\nneered in the early Aloha network and adopted in early Ethernet [ A70],\navoids situations where resources are being overloaded by an exce ss of\nre-sends. Robust systems strive to avoid overload of this nature.\nThe systems community developed a number of approaches over the\nyears. One body of work took OS abstractions and extended them to\noperate in a distributed environment. For example, distributed shared\nmemory (DSM ) systems enable processes on different machines to share\na large, virtual address space [LH89]. This abstraction turn s a distributed\ncomputation into something that looks like a multi-threaded appl ication;\nthe only difference is that these threads run on different mach ines instead\nof different processors within the same machine.\nThe way most DSM systems work is through the virtual memory sys-\ntem of the OS. When a page is accessed on one machine, two things can\nhappen. In the ﬁrst (best) case, the page is already local on the machine,\nand thus the data is fetched quickly. In the second case, the pa ge is cur-\nrently on some other machine. A page fault occurs, and the page fau lt\nhandler sends a message to some other machine to fetch the page, install\nit in the page table of the requesting process, and continue exec ution.\nThis approach is not widely in use today for a number of reasons. The\nlargest problem for DSM is how it handles failure. Imagine, for exa mple,\nif a machine fails; what happens to the pages on that machine? W hat if\nthe data structures of the distributed computation are spread a cross the\nentire address space? In this case, parts of these data struct ures would\nsuddenly become unavailable. Dealing with failure when part s of your\naddress space go missing is hard; imagine a linked list where a “next”\npointer points into a portion of the address space that is gone. Yike s!\nA further problem is performance. One usually assumes, when wr it-\ning code, that access to memory is cheap. In DSM systems, some acce sses\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDISTRIBUTED SYSTEMS 9\nare inexpensive, but others cause page faults and expensive f etches from\nremote machines. Thus, programmers of such DSM systems had to be\nvery careful to organize computations such that almost no communi ca-\ntion occurred at all, defeating much of the point of such an approach .\nThough much research was performed in this space, there was lit tle prac-\ntical impact; nobody builds reliable distributed systems usi ng DSM today.\n48.5 Remote Procedure Call (RPC)\nWhile OS abstractions turned out to be a poor choice for building dis -\ntributed systems, programming language (PL) abstractions ma ke much\nmore sense. The most dominant abstraction is based on the idea of a re-\nmote procedure call , orRPC for short [BN84]1.\nRemote procedure call packages all have a simple goal: to make th e\nprocess of executing code on a remote machine as simple and straigh t-\nforward as calling a local function. Thus, to a client, a procedur e call is\nmade, and some time later, the results are returned. The serve r simply\ndeﬁnes some routines that it wishes to export. The rest of the magi c is\nhandled by the RPC system, which in general has two pieces: a stub gen-\nerator (sometimes called a protocol compiler ), and the run-time library .\nWe’ll now take a look at each of these pieces in more detail.\nStub Generator\nThe stub generator’s job is simple: to remove some of the pain of packi ng\nfunction arguments and results into messages by automating it . Numer-\nous beneﬁts arise: one avoids, by design, the simple mistakes th at occur\nin writing such code by hand; further, a stub compiler can perha ps opti-\nmize such code and thus improve performance.\nThe input to such a compiler is simply the set of calls a server wi shes\nto export to clients. Conceptually, it could be something as simp le as this:\ninterface {\nint func1(int arg1);\nint func2(int arg1, int arg2);\n};\nThe stub generator takes an interface like this and generates a few dif-\nferent pieces of code. For the client, a client stub is generated, which\ncontains each of the functions speciﬁed in the interface; a clie nt program\nwishing to use this RPC service would link with this client stu b and call\ninto it in order to make RPCs.\nInternally, each of these functions in the client stub do all of t he work\nneeded to perform the remote procedure call. To the client, the c ode just\n1In modern programming languages, we might instead say remote method invocation\n(RMI ), but who likes these languages anyhow, with all of their fancy objects?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 DISTRIBUTED SYSTEMS\nappears as a function call (e.g., the client calls func1(x) ); internally, the\ncode in the client stub for func1() does this:\n•Create a message buffer. A message buffer is usually just a con-\ntiguous array of bytes of some size.\n•Pack the needed information into the message buffer. This infor-\nmation includes some kind of identiﬁer for the function to be calle d,\nas well as all of the arguments that the function needs (e.g., in our\nexample above, one integer for func1 ). The process of putting all\nof this information into a single contiguous buffer is sometimes re -\nferred to as the marshaling of arguments or the serialization of the\nmessage.\n•Send the message to the destination RPC server. The communi-\ncation with the RPC server, and all of the details required to ma ke\nit operate correctly, are handled by the RPC run-time library, de-\nscribed further below.\n•Wait for the reply. Because function calls are usually synchronous ,\nthe call will wait for its completion.\n•Unpack return code and other arguments. If the function just re-\nturns a single return code, this process is straightforward; how ever,\nmore complex functions might return more complex results (e.g., a\nlist), and thus the stub might need to unpack those as well. Thi s\nstep is also known as unmarshaling ordeserialization .\n•Return to the caller. Finally, just return from the client stub back\ninto the client code.\nFor the server, code is also generated. The steps taken on the ser ver\nare as follows:\n•Unpack the message. This step, called unmarshaling ordeserial-\nization , takes the information out of the incoming message. The\nfunction identiﬁer and arguments are extracted.\n•Call into the actual function. Finally! We have reached the point\nwhere the remote function is actually executed. The RPC runtim e\ncalls into the function speciﬁed by the ID and passes in the des ired\narguments.\n•Package the results. The return argument(s) are marshaled back\ninto a single reply buffer.\n•Send the reply. The reply is ﬁnally sent to the caller.\nThere are a few other important issues to consider in a stub compil er.\nThe ﬁrst is complex arguments, i.e., how does one package and send\na complex data structure? For example, when one calls the write()\nsystem call, one passes in three arguments: an integer ﬁle des criptor, a\npointer to a buffer, and a size indicating how many bytes (start ing at the\npointer) are to be written. If an RPC package is passed a pointer , it needs\nto be able to ﬁgure out how to interpret that pointer, and perform t he\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDISTRIBUTED SYSTEMS 11\ncorrect action. Usually this is accomplished through either wel l-known\ntypes (e.g., a buffertthat is used to pass chunks of data given a size,\nwhich the RPC compiler understands), or by annotating the data s truc-\ntures with more information, enabling the compiler to know which b ytes\nneed to be serialized.\nAnother important issue is the organization of the server with reg ards\nto concurrency. A simple server just waits for requests in a sim ple loop,\nand handles each request one at a time. However, as you might have\nguessed, this can be grossly inefﬁcient; if one RPC call blocks ( e.g., on\nI/O), server resources are wasted. Thus, most servers are const ructed in\nsome sort of concurrent fashion. A common organization is a thread pool .\nIn this organization, a ﬁnite set of threads are created when the server\nstarts; when a message arrives, it is dispatched to one of these worker\nthreads, which then does the work of the RPC call, eventually rep lying;\nduring this time, a main thread keeps receiving other request s, and per-\nhaps dispatching them to other workers. Such an organization enab les\nconcurrent execution within the server, thus increasing its u tilization; the\nstandard costs arise as well, mostly in programming complexity, as the\nRPC calls may now need to use locks and other synchronization primi -\ntives in order to ensure their correct operation.\nRun-Time Library\nThe run-time library handles much of the heavy lifting in an RP C system;\nmost performance and reliability issues are handled herein. W e’ll now\ndiscuss some of the major challenges in building such a run-time layer.\nOne of the ﬁrst challenges we must overcome is how to locate a re-\nmote service. This problem, of naming , is a common one in distributed\nsystems, and in some sense goes beyond the scope of our current discu s-\nsion. The simplest of approaches build on existing naming system s, e.g.,\nhostnames and port numbers provided by current internet protocols . In\nsuch a system, the client must know the hostname or IP address of th e\nmachine running the desired RPC service, as well as the port nu mber it is\nusing (a port number is just a way of identifying a particular com munica-\ntion activity taking place on a machine, allowing multiple commu nication\nchannels at once). The protocol suite must then provide a mechanis m to\nroute packets to a particular address from any other machine in t he sys-\ntem. For a good discussion of naming, you’ll have to look elsewhere, e.g .,\nread about DNS and name resolution on the Internet, or better yet ju st\nread the excellent chapter in Saltzer and Kaashoek’s book [SK09].\nOnce a client knows which server it should talk to for a particula r re-\nmote service, the next question is which transport-level protocol should\nRPC be built upon. Speciﬁcally, should the RPC system use a relia ble pro-\ntocol such as TCP/IP , or be built upon an unreliable communication l ayer\nsuch as UDP/IP?\nNaively the choice would seem easy: clearly we would like for a re-\nquest to be reliably delivered to the remote server, and clear ly we would\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 DISTRIBUTED SYSTEMS\nlike to reliably receive a reply. Thus we should choose the relia ble trans-\nport protocol such as TCP , right?\nUnfortunately, building RPC on top of a reliable communication lay er\ncan lead to a major inefﬁciency in performance. Recall from the d iscus-\nsion above how reliable communication layers work: with acknowledg -\nments plus timeout/retry. Thus, when the client sends an RPC r equest\nto the server, the server responds with an acknowledgment so th at the\ncaller knows the request was received. Similarly, when the ser ver sends\nthe reply to the client, the client acks it so that the server k nows it was\nreceived. By building a request/response protocol (such as RPC) on top\nof a reliable communication layer, two “extra” messages are sen t.\nFor this reason, many RPC packages are built on top of unreliable com -\nmunication layers, such as UDP . Doing so enables a more efﬁcient RPC\nlayer, but does add the responsibility of providing reliability to the RPC\nsystem. The RPC layer achieves the desired level of responsibi lity by us-\ning timeout/retry and acknowledgments much like we described above.\nBy using some form of sequence numbering, the communication layer\ncan guarantee that each RPC takes place exactly once (in the ca se of no\nfailure), or at most once (in the case where failure arises).\nOther Issues\nThere are some other issues an RPC run-time must handle as well. For\nexample, what happens when a remote call takes a long time to com-\nplete? Given our timeout machinery, a long-running remote call m ight\nappear as a failure to a client, thus triggering a retry, and t hus the need\nfor some care here. One solution is to use an explicit acknowledgme nt\n(from the receiver to sender) when the reply isn’t immediately generated;\nthis lets the client know the server received the request. The n, after some\ntime has passed, the client can periodically ask whether the s erver is still\nworking on the request; if the server keeps saying “yes”, the cl ient should\nbe happy and continue to wait (after all, sometimes a procedure c all can\ntake a long time to ﬁnish executing).\nThe run-time must also handle procedure calls with large argu ments,\nlarger than what can ﬁt into a single packet. Some lower-level ne twork\nprotocols provide such sender-side fragmentation (of larger packets into\na set of smaller ones) and receiver-side reassembly (of smaller parts into\none larger logical whole); if not, the RPC run-time may have to imp lement\nsuch functionality itself. See Birrell and Nelson’s excellent R PC paper for\ndetails [BN84].\nOne issue that many systems handle is that of byte ordering . As you\nmay know, some machines store values in what is known as big endian\nordering, whereas others use little endian ordering. Big endian stores\nbytes (say, of an integer) from most signiﬁcant to least signiﬁc ant bits,\nmuch like Arabic numerals; little endian does the opposite. Both are\nequally valid ways of storing numeric information; the question h ere is\nhow to communicate between machines of different endianness.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDISTRIBUTED SYSTEMS 13\nAside: T HEEND-TO-ENDARGUMENT\nThe end-to-end argument makes the case that the highest level in a\nsystem, i.e., usually the application at “the end”, is ultima tely the only\nlocale within a layered system where certain functionality ca n truly be\nimplemented. In their landmark paper [SRC84], Saltzer et al. a rgue\nthis through an excellent example: reliable ﬁle transfer bet ween two ma-\nchines. If you want to transfer a ﬁle from machine Ato machine B, and\nmake sure that the bytes that end up on Bare exactly the same as those\nthat began on A, you must have an “end-to-end” check of this; lower-\nlevel reliable machinery, e.g., in the network or disk, provide s no such\nguarantee.\nThe contrast is an approach which tries to solve the reliable-ﬁl e-\ntransfer problem by adding reliability to lower layers of the sy stem. For\nexample, say we build a reliable communication protocol and use it to\nbuild our reliable ﬁle transfer. The communication protocol guara ntees\nthat every byte sent by a sender will be received in order by the receiver,\nsay using timeout/retry, acknowledgments, and sequence numb ers. Un-\nfortunately, using such a protocol does not a reliable ﬁle transfer make;\nimagine the bytes getting corrupted in sender memory before the com-\nmunication even takes place, or something bad happening when th e re-\nceiver writes the data to disk. In those cases, even though the b ytes were\ndelivered reliably across the network, our ﬁle transfer was ult imately\nnot reliable. To build a reliable ﬁle transfer, one must includ e end-to-\nend checks of reliability, e.g., after the entire transfer is complete, read\nback the ﬁle on the receiver disk, compute a checksum, and compar e that\nchecksum to that of the ﬁle on the sender.\nThe corollary to this maxim is that sometimes having lower layers pro-\nvide extra functionality can indeed improve system performanc e or oth-\nerwise optimize a system. Thus, you should not rule out having such\nmachinery at a lower-level in a system; rather, you should caref ully con-\nsider the utility of such machinery, given its eventual usage in an overall\nsystem or application.\nRPC packages often handle this by providing a well-deﬁned endi -\nanness within their message formats. In Sun’s RPC package, the XDR\n(eXternal Data Representation ) layer provides this functionality. If the\nmachine sending or receiving a message matches the endiannes s of XDR,\nmessages are just sent and received as expected. If, however, the machine\ncommunicating has a different endianness, each piece of inform ation in\nthe message must be converted. Thus, the difference in endian ness can\nhave a small performance cost.\nA ﬁnal issue is whether to expose the asynchronous nature of com-\nmunication to clients, thus enabling some performance optimiza tions.\nSpeciﬁcally, typical RPCs are made synchronously , i.e., when a client\nissues the procedure call, it must wait for the procedure call to return\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 DISTRIBUTED SYSTEMS\nbefore continuing. Because this wait can be long, and because th e client\nmay have other work it could be doing, some RPC packages enable you\nto invoke an RPC asynchronously . When an asynchronous RPC is is-\nsued, the RPC package sends the request and returns immediat ely; the\nclient is then free to do other work, such as call other RPCs or other use-\nful computation. The client at some point will want to see the resu lts of\nthe asynchronous RPC; it thus calls back into the RPC layer, tel ling it to\nwait for outstanding RPCs to complete, at which point return argu ments\ncan be accessed.\n48.6 Summary\nWe have seen the introduction of a new topic, distributed systems , and\nits major issue: how to handle failure which is now a commonplace ev ent.\nAs they say inside of Google, when you have just your desktop machine ,\nfailure is rare; when you’re in a data center with thousands of mac hines,\nfailure is happening all the time. The key to any distributed system is\nhow you deal with that failure.\nWe have also seen that communication forms the heart of any dis-\ntributed system. A common abstraction of that communication is foun d\nin remote procedure call (RPC), which enables clients to make r emote\ncalls on servers; the RPC package handles all of the gory details , includ-\ning timeout/retry and acknowledgment, in order to deliver a ser vice that\nclosely mirrors a local procedure call.\nThe best way to really understand an RPC package is of course to u se\none yourself. Sun’s RPC system, using the stub compiler rpcgen , is an\nolder one; Google’s gRPC and Apache Thrift are modern takes on the\nsame. Try one out, and see what all the fuss is about.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nDISTRIBUTED SYSTEMS 15\nReferences\n[A70] “The ALOHA System — Another Alternative for Computer Communicati ons” by Nor-\nman Abramson. The 1970 Fall Joint Computer Conference. The ALOHA network pioneered some\nbasic concepts in networking, including exponential back-off and retransmit, which formed the basis for\ncommunication in shared-bus Ethernet networks for years.\n[BN84] “Implementing Remote Procedure Calls” by Andrew D. Birrell , Bruce Jay Nelson.\nACM TOCS, Volume 2:1, February 1984. The foundational RPC system upon which all others build.\nYes, another pioneering effort from our friends at Xerox P ARC.\n[MK09] “The Effectiveness of Checksums for Embedded Control Networks” b y Theresa C.\nMaxino and Philip J. Koopman. IEEE Transactions on Dependable and Secure Co mputing,\n6:1, January ’09. A nice overview of basic checksum machinery and some performance and robu stness\ncomparisons between them.\n[LH89] “Memory Coherence in Shared Virtual Memory Systems” by Kai Li and Paul Hudak.\nACM TOCS, 7:4, November 1989. The introduction of software-based shared memory via virtual\nmemory. An intriguing idea for sure, but not a lasting or good one in the end .\n[SK09] “Principles of Computer System Design” by Jerome H. Saltzer and M. Frans Kaashoek.\nMorgan-Kaufmann, 2009. An excellent book on systems, and a must for every bookshelf. One of the\nfew terriﬁc discussions on naming we’ve seen.\n[SRC84] “End-To-End Arguments in System Design” by Jerome H. Saltzer , David P . Reed,\nDavid D. Clark. ACM TOCS, 2:4, November 1984. A beautiful discussion of layering, abstraction,\nand where functionality must ultimately reside in computer systems.\n[VJ88] “Congestion Avoidance and Control” by Van Jacobson. SIGCOMM ’88 . A pioneering\npaper on how clients should adjust to perceived network congestion; deﬁni tely one of the key pieces of\ntechnology underlying the Internet, and a must read for anyone serious about s ystems, and for Van\nJacobson’s relatives because well relatives should read all of your papers.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 DISTRIBUTED SYSTEMS\nHomework (Code)\nIn this section, we’ll write some simple communication code to get\nyou familiar with the task of doing so. Have fun!\nQuestions\n1. Using the code provided in the chapter, build a simple UDP-base d server\nand client. The server should receive messages from the client , and reply\nwith an acknowledgment. In this ﬁrst attempt, do not add any ret ransmis-\nsion or robustness (assume that communication works perfectly). R un this\non a single machine for testing; later, run it on two different ma chines.\n2. Turn your code into a communication library . Speciﬁcally, make your own\nAPI, with send and receive calls, as well as other API calls as ne eded. Rewrite\nyour client and server to use your library instead of raw socket c alls.\n3. Add reliable communication to your burgeoning communication libra ry, in\nthe form of timeout/retry . Speciﬁcally, your library should make a copy of\nany message that it is going to send. When sending it, it should s tart a timer,\nso it can track how long it has been since the message was sent. O n the re-\nceiver, the library should acknowledge received messages. The client send\nshould block when sending, i.e., it should wait until the message has been\nacknowledged before returning. It should also be willing to re try sending\nindeﬁnitely. The maximum message size should be that of the largest single\nmessage you can send with UDP . Finally, be sure to perform timeout/ retry\nefﬁciently by putting the caller to sleep until either an ack ar rives or the\ntransmission times out; do notspin and waste the CPU!\n4. Make your library more efﬁcient and feature-ﬁlled. First, ad d very-large\nmessage transfer. Speciﬁcally, although the network limit maximum mes-\nsage size, your library should take a message of arbitrarily lar ge size and\ntransfer it from client to server. The client should transmit t hese large mes-\nsages in pieces to the server; the server-side library code s hould assemble re-\nceived fragments into the contiguous whole, and pass the single large buffer\nto the waiting server code.\n5. Do the above again, but with high performance. Instead of sen ding each\nfragment one at a time, you should rapidly send many pieces, thus al lowing\nthe network to be much more highly utilized. To do so, carefully mark each\npiece of the transfer so that the re-assembly on the receiver s ide does not\nscramble the message.\n6. A ﬁnal implementation challenge: asynchronous message send w ith in-\norder delivery. That is, the client should be able to repeated ly call send\nto send one message after the other; the receiver should call re ceive and get\neach message in order, reliably; many messages from the sender s hould be\nable to be in ﬂight concurrently. Also add a sender-side call th at enables a\nclient to wait for all outstanding messages to be acknowledged .\n7. Now, one more pain point: measurement. Measure the bandwidth of each\nof your approaches; how much data can you transfer between two di fferent\nmachines, at what rate? Also measure latency: for single packet s end and\nacknowledgment, how quickly does it ﬁnish? Finally, do your numbe rs look\nreasonable? What did you expect? How can you better set your expe ctations\nso as to know if there is a problem, or that your code is working we ll?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",41627
55-49. Network File System NFS.pdf,55-49. Network File System NFS,"49\nSun’s Network File System (NFS)\nOne of the ﬁrst uses of distributed client/server computing was in the\nrealm of distributed ﬁle systems. In such an environment, ther e are a\nnumber of client machines and one server (or a few); the server st ores the\ndata on its disks, and clients request data through well-formed protocol\nmessages. Figure 49.1 depicts the basic setup.\nClient 0\nClient 1\nClient 2\nClient 3Server\nRAIDNetwork\nFigure 49.1: A Generic Client/Server System\nAs you can see from the picture, the server has the disks, and cli ents\nsend messages across a network to access their directories and ﬁ les on\nthose disks. Why do we bother with this arrangement? (i.e., why don’t\nwe just let clients use their local disks?) Well, primarily th is setup allows\nfor easy sharing of data across clients. Thus, if you access a ﬁle on one\nmachine (Client 0) and then later use another (Client 2), you wi ll have the\nsame view of the ﬁle system. Your data is naturally shared across these\ndifferent machines. A secondary beneﬁt is centralized administration ;\nfor example, backing up ﬁles can be done from the few server machi nes\ninstead of from the multitude of clients. Another advantage could be\nsecurity ; having all servers in a locked machine room prevents certain\ntypes of problems from arising.\n1\n2 SUN’SNETWORK FILESYSTEM (NFS)\nCRUX: HOWTOBUILD A D ISTRIBUTED FILESYSTEM\nHow do you build a distributed ﬁle system? What are the key aspec ts\nto think about? What is easy to get wrong? What can we learn from\nexisting systems?\n49.1 A Basic Distributed File System\nWe now will study the architecture of a simpliﬁed distributed ﬁ le sys-\ntem. A simple client/server distributed ﬁle system has more c omponents\nthan the ﬁle systems we have studied so far. On the client side , there are\nclient applications which access ﬁles and directories through theclient-\nside ﬁle system . A client application issues system calls to the client-side\nﬁle system (such as open() ,read() ,write() ,close() ,mkdir() ,\netc.) in order to access ﬁles which are stored on the server. Thus , to client\napplications, the ﬁle system does not appear to be any different than a lo-\ncal (disk-based) ﬁle system, except perhaps for performance; in this way,\ndistributed ﬁle systems provide transparent access to ﬁles, an obvious\ngoal; after all, who would want to use a ﬁle system that required a differ-\nent set of APIs or otherwise was a pain to use?\nThe role of the client-side ﬁle system is to execute the actions n eeded\nto service those system calls. For example, if the client issue s aread()\nrequest, the client-side ﬁle system may send a message to the server-side\nﬁle system (or, as it is commonly called, the ﬁle server ) to read a partic-\nular block; the ﬁle server will then read the block from disk (or it s own\nin-memory cache), and send a message back to the client with th e re-\nquested data. The client-side ﬁle system will then copy the da ta into the\nuser buffer supplied to the read() system call and thus the request will\ncomplete. Note that a subsequent read() of the same block on the client\nmay be cached in client memory or on the client’s disk even; in the best\nsuch case, no network trafﬁc need be generated.\nClient Application\nClient-side File System\nNetworking LayerFile Server\nNetworking LayerDisks\nFigure 49.2: Distributed File System Architecture\nFrom this simple overview, you should get a sense that there are tw o\nimportant pieces of software in a client/server distributed ﬁl e system: the\nclient-side ﬁle system and the ﬁle server. Together their beh avior deter-\nmines the behavior of the distributed ﬁle system. Now it’s time to study\none particular system: Sun’s Network File System (NFS).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSUN’SNETWORK FILESYSTEM (NFS) 3\nASIDE : W HYSERVERS CRASH\nBefore getting into the details of the NFSv2 protocol, you might be\nwondering: why do servers crash? Well, as you might guess, ther e are\nplenty of reasons. Servers may simply suffer from a power outage (tem-\nporarily); only when power is restored can the machines be restar ted.\nServers are often comprised of hundreds of thousands or even millions\nof lines of code; thus, they have bugs (even good software has a few\nbugs per hundred or thousand lines of code), and thus they eventua lly\nwill trigger a bug that will cause them to crash. They also hav e memory\nleaks; even a small memory leak will cause a system to run out of me m-\nory and crash. And, ﬁnally, in distributed systems, there is a network\nbetween the client and the server; if the network acts strange ly (for ex-\nample, if it becomes partitioned and clients and servers are working but\ncannot communicate), it may appear as if a remote machine has cra shed,\nbut in reality it is just not currently reachable through the ne twork.\n49.2 On To NFS\nOne of the earliest and quite successful distributed systems was devel-\noped by Sun Microsystems, and is known as the Sun Network File Sys-\ntem (or NFS) [S86]. In deﬁning NFS, Sun took an unusual approach: in-\nstead of building a proprietary and closed system, Sun instead de veloped\nanopen protocol which simply speciﬁed the exact message formats that\nclients and servers would use to communicate. Different groups could\ndevelop their own NFS servers and thus compete in an NFS marketpl ace\nwhile preserving interoperability. It worked: today there are many com-\npanies that sell NFS servers (including Oracle/Sun, NetApp [ HLM94],\nEMC, IBM, and others), and the widespread success of NFS is like ly at-\ntributed to this “open market” approach.\n49.3 Focus: Simple And Fast Server Crash Recovery\nIn this chapter, we will discuss the classic NFS protocol (versi on 2,\na.k.a. NFSv2), which was the standard for many years; small cha nges\nwere made in moving to NFSv3, and larger-scale protocol changes we re\nmade in moving to NFSv4. However, NFSv2 is both wonderful and frus-\ntrating and thus serves as our focus.\nIn NFSv2, the main goal in the design of the protocol was simple and\nfast server crash recovery . In a multiple-client, single-server environment,\nthis goal makes a great deal of sense; any minute that the server is down\n(or unavailable) makes allthe client machines (and their users) unhappy\nand unproductive. Thus, as the server goes, so goes the entire sy stem.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 SUN’SNETWORK FILESYSTEM (NFS)\n49.4 Key To Fast Crash Recovery: Statelessness\nThis simple goal is realized in NFSv2 by designing what we refer to\nas a stateless protocol. The server, by design, does not keep track of any-\nthing about what is happening at each client. For example, the s erver\ndoes not know which clients are caching which blocks, or which ﬁles are\ncurrently open at each client, or the current ﬁle pointer position for a ﬁle,\netc. Simply put, the server does not track anything about what cli ents are\ndoing; rather, the protocol is designed to deliver in each protocol r equest\nall the information that is needed in order to complete the request. If it\ndoesn’t now, this stateless approach will make more sense as we dis cuss\nthe protocol in more detail below.\nFor an example of a stateful (not stateless) protocol, consider the open()\nsystem call. Given a pathname, open() returns a ﬁle descriptor (an inte-\nger). This descriptor is used on subsequent read() orwrite() requests\nto access various ﬁle blocks, as in this application code (note tha t proper\nerror checking of the system calls is omitted for space reasons):\nchar buffer[MAX];\nint fd = open(""foo"", O_RDONLY); // get descriptor ""fd""\nread(fd, buffer, MAX); // read MAX bytes from foo (via fd)\nread(fd, buffer, MAX); // read MAX bytes from foo\n...\nread(fd, buffer, MAX); // read MAX bytes from foo\nclose(fd); // close file\nFigure 49.3: Client Code: Reading From A File\nNow imagine that the client-side ﬁle system opens the ﬁle by sen ding\na protocol message to the server saying “open the ﬁle ’foo’ and give me\nback a descriptor”. The ﬁle server then opens the ﬁle locally on it s side\nand sends the descriptor back to the client. On subsequent rea ds, the\nclient application uses that descriptor to call the read() system call; the\nclient-side ﬁle system then passes the descriptor in a messag e to the ﬁle\nserver, saying “read some bytes from the ﬁle that is referred to by the\ndescriptor I am passing you here”.\nIn this example, the ﬁle descriptor is a piece of shared state between\nthe client and the server (Ousterhout calls this distributed state [O91]).\nShared state, as we hinted above, complicates crash recovery. Im agine\nthe server crashes after the ﬁrst read completes, but before th e client\nhas issued the second one. After the server is up and running aga in,\nthe client then issues the second read. Unfortunately, the ser ver has no\nidea to which ﬁle fdis referring; that information was ephemeral (i.e.,\nin memory) and thus lost when the server crashed. To handle this situa-\ntion, the client and server would have to engage in some kind of recovery\nprotocol , where the client would make sure to keep enough information\naround in its memory to be able to tell the server what it needs to know\n(in this case, that ﬁle descriptor fdrefers to ﬁle foo).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSUN’SNETWORK FILESYSTEM (NFS) 5\nIt gets even worse when you consider the fact that a stateful serv er has\nto deal with client crashes. Imagine, for example, a client th at opens a ﬁle\nand then crashes. The open() uses up a ﬁle descriptor on the server; how\ncan the server know it is OK to close a given ﬁle? In normal operation , a\nclient would eventually call close() and thus inform the server that the\nﬁle should be closed. However, when a client crashes, the server never\nreceives aclose() , and thus has to notice the client has crashed in order\nto close the ﬁle.\nFor these reasons, the designers of NFS decided to pursue a state less\napproach: each client operation contains all the information need ed to\ncomplete the request. No fancy crash recovery is needed; the se rver just\nstarts running again, and a client, at worst, might have to ret ry a request.\n49.5 The NFSv2 Protocol\nWe thus arrive at the NFSv2 protocol deﬁnition. Our problem state-\nment is simple:\nTHECRUX: HOWTODEFINE A S TATELESS FILEPROTOCOL\nHow can we deﬁne the network protocol to enable stateless operation?\nClearly, stateful calls like open() can’t be a part of the discussion (as it\nwould require the server to track open ﬁles); however, the clien t appli-\ncation will want to call open() ,read() ,write() ,close() and other\nstandard API calls to access ﬁles and directories. Thus, as a r eﬁned ques-\ntion, how do we deﬁne the protocol to both be stateless andsupport the\nPOSIX ﬁle system API?\nOne key to understanding the design of the NFS protocol is under-\nstanding the ﬁle handle . File handles are used to uniquely describe the\nﬁle or directory a particular operation is going to operate upon; thu s,\nmany of the protocol requests include a ﬁle handle.\nYou can think of a ﬁle handle as having three important components: a\nvolume identiﬁer , aninode number , and a generation number ; together, these\nthree items comprise a unique identiﬁer for a ﬁle or directory tha t a client\nwishes to access. The volume identiﬁer informs the server whic h ﬁle sys-\ntem the request refers to (an NFS server can export more than one ﬁ le\nsystem); the inode number tells the server which ﬁle within th at partition\nthe request is accessing. Finally, the generation number is n eeded when\nreusing an inode number; by incrementing it whenever an inode n um-\nber is reused, the server ensures that a client with an old ﬁle h andle can’t\naccidentally access the newly-allocated ﬁle.\nHere is a summary of some of the important pieces of the protocol; the\nfull protocol is available elsewhere (see Callaghan’s book for an ex cellent\nand detailed overview of NFS [C00]).\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 SUN’SNETWORK FILESYSTEM (NFS)\nNFSPROC_GETATTR\nexpects: file handle\nreturns: attributes\nNFSPROC_SETATTR\nexpects: file handle, attributes\nreturns: nothing\nNFSPROC_LOOKUP\nexpects: directory file handle, name of file/directory to l ook up\nreturns: file handle\nNFSPROC_READ\nexpects: file handle, offset, count\nreturns: data, attributes\nNFSPROC_WRITE\nexpects: file handle, offset, count, data\nreturns: attributes\nNFSPROC_CREATE\nexpects: directory file handle, name of file, attributes\nreturns: nothing\nNFSPROC_REMOVE\nexpects: directory file handle, name of file to be removed\nreturns: nothing\nNFSPROC_MKDIR\nexpects: directory file handle, name of directory, attribu tes\nreturns: file handle\nNFSPROC_RMDIR\nexpects: directory file handle, name of directory to be remo ved\nreturns: nothing\nNFSPROC_READDIR\nexpects: directory handle, count of bytes to read, cookie\nreturns: directory entries, cookie (to get more entries)\nFigure 49.4: The NFS Protocol: Examples\nWe brieﬂy highlight the important components of the protocol. First ,\nthe LOOKUP protocol message is used to obtain a ﬁle handle, which i s\nthen subsequently used to access ﬁle data. The client passes a directory\nﬁle handle and name of a ﬁle to look up, and the handle to that ﬁle (or\ndirectory) plus its attributes are passed back to the client f rom the server.\nFor example, assume the client already has a directory ﬁle hand le for\nthe root directory of a ﬁle system ( /) (indeed, this would be obtained\nthrough the NFS mount protocol , which is how clients and servers ﬁrst\nare connected together; we do not discuss the mount protocol here for\nsake of brevity). If an application running on the client opens th e ﬁle\n/foo.txt , the client-side ﬁle system sends a lookup request to the serve r,\npassing it the root ﬁle handle and the name foo.txt ; if successful, the\nﬁle handle (and attributes) for foo.txt will be returned.\nIn case you are wondering, attributes are just the metadata tha t the ﬁle\nsystem tracks about each ﬁle, including ﬁelds such as ﬁle crea tion time,\nlast modiﬁcation time, size, ownership and permissions informat ion, and\nso forth, i.e., the same type of information that you would get back i f you\ncalledstat() on a ﬁle.\nOnce a ﬁle handle is available, the client can issue READ and W RITE\nprotocol messages on a ﬁle to read or write the ﬁle, respectively. T he\nREAD protocol message requires the protocol to pass along the ﬁle han dle\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSUN’SNETWORK FILESYSTEM (NFS) 7\nof the ﬁle along with the offset within the ﬁle and number of bytes t o read.\nThe server then will be able to issue the read (after all, the h andle tells the\nserver which volume and which inode to read from, and the offset an d\ncount tells it which bytes of the ﬁle to read) and return the data to the\nclient (or an error if there was a failure). WRITE is handled sim ilarly,\nexcept the data is passed from the client to the server, and jus t a success\ncode is returned.\nOne last interesting protocol message is the GETATTR request; g iven a\nﬁle handle, it simply fetches the attributes for that ﬁle, inc luding the last\nmodiﬁed time of the ﬁle. We will see why this protocol request is imp or-\ntant in NFSv2 below when we discuss caching (can you guess why?).\n49.6 From Protocol To Distributed File System\nHopefully you are now getting some sense of how this protocol is\nturned into a ﬁle system across the client-side ﬁle system and the ﬁle\nserver. The client-side ﬁle system tracks open ﬁles, and gene rally trans-\nlates application requests into the relevant set of protocol mess ages. The\nserver simply responds to protocol messages, each of which contain s all\ninformation needed to complete request.\nFor example, let us consider a simple application which reads a ﬁ le.\nIn the diagram (Figure 49.5), we show what system calls the app lication\nmakes, and what the client-side ﬁle system and ﬁle server do i n respond-\ning to such calls.\nA few comments about the ﬁgure. First, notice how the client track s all\nrelevant state for the ﬁle access, including the mapping of the integer ﬁle\ndescriptor to an NFS ﬁle handle as well as the current ﬁle pointe r. This\nenables the client to turn each read request (which you may hav e noticed\ndonotspecify the offset to read from explicitly) into a properly-form atted\nread protocol message which tells the server exactly which byte s from\nthe ﬁle to read. Upon a successful read, the client updates the current\nﬁle position; subsequent reads are issued with the same ﬁle han dle but a\ndifferent offset.\nSecond, you may notice where server interactions occur. When the ﬁl e\nis opened for the ﬁrst time, the client-side ﬁle system sends a L OOKUP\nrequest message. Indeed, if a long pathname must be traversed (e.g.,\n/home/remzi/foo.txt ), the client would send three LOOKUPs: one\nto look up home in the directory /, one to look up remzi inhome , and\nﬁnally one to look up foo.txt inremzi .\nThird, you may notice how each server request has all the informat ion\nneeded to complete the request in its entirety. This design poi nt is critical\nto be able to gracefully recover from server failure, as we will now discuss\nin more detail; it ensures that the server does not need state to b e able to\nrespond to the request.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 SUN’SNETWORK FILESYSTEM (NFS)\nClient Server\nfd = open(”/foo”, ...);\nSend LOOKUP (rootdir FH, ”foo”)\nReceive LOOKUP request\nlook for ”foo” in root dir\nreturn foo’s FH + attributes\nReceive LOOKUP reply\nallocate ﬁle desc in open ﬁle table\nstore foo’s FH in table\nstore current ﬁle position (0)\nreturn ﬁle descriptor to application\nread(fd, buffer, MAX);\nIndex into open ﬁle table with fd\nget NFS ﬁle handle (FH)\nuse current ﬁle position as offset\nSend READ (FH, offset=0, count=MAX)\nReceive READ request\nuse FH to get volume/inode num\nread inode from disk (or cache)\ncompute block location (using offset)\nread data from disk (or cache)\nreturn data to client\nReceive READ reply\nupdate ﬁle position (+bytes read)\nset current ﬁle position = MAX\nreturn data/error code to app\nread(fd, buffer, MAX);\nSame except offset=MAX and set current ﬁle position = 2*MAX\nread(fd, buffer, MAX);\nSame except offset=2*MAX and set current ﬁle position = 3*MAX\nclose(fd);\nJust need to clean up local structures\nFree descriptor ”fd” in open ﬁle table\n(No need to talk to server)\nFigure 49.5: Reading A File: Client-side And File Server Actions\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSUN’SNETWORK FILESYSTEM (NFS) 9\nTIP: IDEMPOTENCY ISPOWERFUL\nIdempotency is a useful property when building reliable systems. When\nan operation can be issued more than once, it is much easier to hand le\nfailure of the operation; you can just retry it. If an operation is notidem-\npotent, life becomes more difﬁcult.\n49.7 Handling Server Failure With Idempotent Operations\nWhen a client sends a message to the server, it sometimes does not re-\nceive a reply. There are many possible reasons for this failure t o respond.\nIn some cases, the message may be dropped by the network; networks do\nlose messages, and thus either the request or the reply could be l ost and\nthus the client would never receive a response.\nIt is also possible that the server has crashed, and thus is not c urrently\nresponding to messages. After a bit, the server will be rebooted and start\nrunning again, but in the meanwhile all requests have been los t. In all of\nthese cases, clients are left with a question: what should they do when\nthe server does not reply in a timely manner?\nIn NFSv2, a client handles all of these failures in a single, uni form, and\nelegant way: it simply retries the request. Speciﬁcally, after sending the\nrequest, the client sets a timer to go off after a speciﬁed time period. If a\nreply is received before the timer goes off, the timer is cancele d and all is\nwell. If, however, the timer goes off before any reply is received, the client\nassumes the request has not been processed and resends it. If th e server\nreplies, all is well and the client has neatly handled the prob lem.\nThe ability of the client to simply retry the request (regardl ess of what\ncaused the failure) is due to an important property of most NFS req uests:\nthey are idempotent . An operation is called idempotent when the effect\nof performing the operation multiple times is equivalent to the e ffect of\nperforming the operation a single time. For example, if you store a v alue\nto a memory location three times, it is the same as doing so once; thu s\n“store value to memory” is an idempotent operation. If, however, you in-\ncrement a counter three times, it results in a different amount than doing\nso just once; thus, “increment counter” is not idempotent. More ge ner-\nally, any operation that just reads data is obviously idempotent; an oper-\nation that updates data must be more carefully considered to det ermine\nif it has this property.\nThe heart of the design of crash recovery in NFS is the idempotency\nof most common operations. LOOKUP and READ requests are trivially\nidempotent, as they only read information from the ﬁle server and d o not\nupdate it. More interestingly, WRITE requests are also idemp otent. If,\nfor example, a WRITE fails, the client can simply retry it. The WRITE\nmessage contains the data, the count, and (importantly) the exa ct offset\nto write the data to. Thus, it can be repeated with the knowledge that the\noutcome of multiple writes is the same as the outcome of a single one.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 SUN’SNETWORK FILESYSTEM (NFS)\nCase 1: Request Lost\nClient\n[send request]Server\n(no mesg)\nCase 2: Server Down\nClient\n[send request]Server\n(down)\nCase 3: Reply lost on way back from Server\nClient\n[send request]Server\n[recv request]\n[handle request]\n[send reply]\nFigure 49.6: The Three Types Of Loss\nIn this way, the client can handle all timeouts in a uniﬁed way. If a\nWRITE request was simply lost (Case 1 above), the client will re try it, the\nserver will perform the write, and all will be well. The same wi ll happen\nif the server happened to be down while the request was sent, bu t back\nup and running when the second request is sent, and again all wor ks\nas desired (Case 2). Finally, the server may in fact receive t he WRITE\nrequest, issue the write to its disk, and send a reply. This re ply may get\nlost (Case 3), again causing the client to re-send the request . When the\nserver receives the request again, it will simply do the exac t same thing:\nwrite the data to disk and reply that it has done so. If the client this time\nreceives the reply, all is again well, and thus the client has handled both\nmessage loss and server failure in a uniform manner. Neat!\nA small aside: some operations are hard to make idempotent. For\nexample, when you try to make a directory that already exists, y ou are\ninformed that the mkdir request has failed. Thus, in NFS, if the ﬁle server\nreceives a MKDIR protocol message and executes it successfully but the\nreply is lost, the client may repeat it and encounter that failu re when in\nfact the operation at ﬁrst succeeded and then only failed on the re try.\nThus, life is not perfect.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSUN’SNETWORK FILESYSTEM (NFS) 11\nTIP: PERFECT ISTHEENEMY OFTHEGOOD (VOLTAIRE ’SLAW)\nEven when you design a beautiful system, sometimes all the corne r cases\ndon’t work out exactly as you might like. Take the mkdir example abov e;\none could redesign mkdir to have different semantics, thus mak ing it\nidempotent (think about how you might do so); however, why bother?\nThe NFS design philosophy covers most of the important cases, and ove r-\nall makes the system design clean and simple with regards to f ailure.\nThus, accepting that life isn’t perfect and still building th e system is a sign\nof good engineering. Apparently, this wisdom is attributed to Vol taire,\nfor saying “... a wise Italian says that the best is the enemy of t he good”\n[V72], and thus we call it Voltaire’s Law .\n49.8 Improving Performance: Client-side Caching\nDistributed ﬁle systems are good for a number of reasons, but sendi ng\nall read and write requests across the network can lead to a big p erfor-\nmance problem: the network generally isn’t that fast, especial ly as com-\npared to local memory or disk. Thus, another problem: how can we im-\nprove the performance of a distributed ﬁle system?\nThe answer, as you might guess from reading the big bold words in\nthe sub-heading above, is client-side caching . The NFS client-side ﬁle\nsystem caches ﬁle data (and metadata) that it has read from the server in\nclient memory. Thus, while the ﬁrst access is expensive (i.e. , it requires\nnetwork communication), subsequent accesses are serviced qui te quickly\nout of client memory.\nThe cache also serves as a temporary buffer for writes. When a cl ient\napplication ﬁrst writes to a ﬁle, the client buffers the data i n client mem-\nory (in the same cache as the data it read from the ﬁle server) bef ore writ-\ning the data out to the server. Such write buffering is useful because it de-\ncouples application write() latency from actual write performance, i.e.,\nthe application’s call to write() succeeds immediately (and just puts\nthe data in the client-side ﬁle system’s cache); only later does the data get\nwritten out to the ﬁle server.\nThus, NFS clients cache data and performance is usually great and\nwe are done, right? Unfortunately, not quite. Adding caching in to any\nsort of system with multiple client caches introduces a big and i nteresting\nchallenge which we will refer to as the cache consistency problem .\n49.9 The Cache Consistency Problem\nThe cache consistency problem is best illustrated with two cli ents and\na single server. Imagine client C1 reads a ﬁle F, and keeps a cop y of the\nﬁle in its local cache. Now imagine a different client, C2, overw rites the\nﬁle F, thus changing its contents; let’s call the new version of th e ﬁle F\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 SUN’SNETWORK FILESYSTEM (NFS)\nC1\ncache: F[v1]C2\ncache: F[v2]C3\ncache: empty\nServer S\ndisk: F[v1] at first\n      F[v2] eventually\nFigure 49.7: The Cache Consistency Problem\n(version 2), or F[v2] and the old version F[v1] so we can keep the tw o\ndistinct (but of course the ﬁle has the same name, just differen t contents).\nFinally, there is a third client, C3, which has not yet accesse d the ﬁle F.\nYou can probably see the problem that is upcoming (Figure 49.7). I n\nfact, there are two subproblems. The ﬁrst subproblem is that th e client C2\nmay buffer its writes in its cache for a time before propagating t hem to the\nserver; in this case, while F[v2] sits in C2’s memory, any acces s of F from\nanother client (say C3) will fetch the old version of the ﬁle (F[v1 ]). Thus,\nby buffering writes at the client, other clients may get stale versions of the\nﬁle, which may be undesirable; indeed, imagine the case wher e you log\ninto machine C2, update F, and then log into C3 and try to read th e ﬁle,\nonly to get the old copy! Certainly this could be frustrating. Thu s, let us\ncall this aspect of the cache consistency problem update visibility ; when\ndo updates from one client become visible at other clients?\nThe second subproblem of cache consistency is a stale cache ; in this\ncase, C2 has ﬁnally ﬂushed its writes to the ﬁle server, and th us the server\nhas the latest version (F[v2]). However, C1 still has F[v1] in i ts cache; if a\nprogram running on C1 reads ﬁle F, it will get a stale version (F[v 1]) and\nnot the most recent copy (F[v2]), which is (often) undesirable.\nNFSv2 implementations solve these cache consistency problems in two\nways. First, to address update visibility, clients impleme nt what is some-\ntimes called ﬂush-on-close (a.k.a., close-to-open ) consistency semantics;\nspeciﬁcally, when a ﬁle is written to and subsequently closed by a client\napplication, the client ﬂushes all updates (i.e., dirty page s in the cache)\nto the server. With ﬂush-on-close consistency, NFS ensures tha t a subse-\nquent open from another node will see the latest ﬁle version.\nSecond, to address the stale-cache problem, NFSv2 clients ﬁrst c heck\nto see whether a ﬁle has changed before using its cached content s. Specif-\nically, before using a cached block, the client-side ﬁle syste m will issue a\nGETATTR request to the server to fetch the ﬁle’s attributes. T he attributes,\nimportantly, include information as to when the ﬁle was last modi ﬁed on\nthe server; if the time-of-modiﬁcation is more recent than the ti me that the\nﬁle was fetched into the client cache, the client invalidates the ﬁle, thus\nremoving it from the client cache and ensuring that subsequent reads will\ngo to the server and retrieve the latest version of the ﬁle. If, on the other\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSUN’SNETWORK FILESYSTEM (NFS) 13\nhand, the client sees that it has the latest version of the ﬁle, i t will go\nahead and use the cached contents, thus increasing performanc e.\nWhen the original team at Sun implemented this solution to the sta le-\ncache problem, they realized a new problem; suddenly, the NFS s erver\nwas ﬂooded with GETATTR requests. A good engineering principle t o\nfollow is to design for the common case , and to make it work well; here,\nalthough the common case was that a ﬁle was accessed only from a sin-\ngle client (perhaps repeatedly), the client always had to se nd GETATTR\nrequests to the server to make sure no one else had changed the ﬁ le. A\nclient thus bombards the server, constantly asking “has anyone changed\nthis ﬁle?”, when most of the time no one had.\nTo remedy this situation (somewhat), an attribute cache was added\nto each client. A client would still validate a ﬁle before acces sing it, but\nmost often would just look in the attribute cache to fetch the attri butes.\nThe attributes for a particular ﬁle were placed in the cache wh en the ﬁle\nwas ﬁrst accessed, and then would timeout after a certain amount of time\n(say 3 seconds). Thus, during those three seconds, all ﬁle acces ses would\ndetermine that it was OK to use the cached ﬁle and thus do so wit h no\nnetwork communication with the server.\n49.10 Assessing NFS Cache Consistency\nA few ﬁnal words about NFS cache consistency. The ﬂush-on-close be -\nhavior was added to “make sense”, but introduced a certain perf ormance\nproblem. Speciﬁcally, if a temporary or short-lived ﬁle was creat ed on a\nclient and then soon deleted, it would still be forced to the serve r. A more\nideal implementation might keep such short-lived ﬁles in memor y until\nthey are deleted and thus remove the server interaction entire ly, perhaps\nincreasing performance.\nMore importantly, the addition of an attribute cache into NFS mad e\nit very hard to understand or reason about exactly what version of a ﬁle\none was getting. Sometimes you would get the latest version; sometim es\nyou would get an old version simply because your attribute cache ha dn’t\nyet timed out and thus the client was happy to give you what was in\nclient memory. Although this was ﬁne most of the time, it would (and\nstill does!) occasionally lead to odd behavior.\nAnd thus we have described the oddity that is NFS client cachin g.\nIt serves as an interesting example where details of an implem entation\nserve to deﬁne user-observable semantics, instead of the other way around.\n49.11 Implications On Server-Side Write Buffering\nOur focus so far has been on client caching, and that is where most\nof the interesting issues arise. However, NFS servers tend to b e well-\nequipped machines with a lot of memory too, and thus they have cachi ng\nconcerns as well. When data (and metadata) is read from disk, NF S\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 SUN’SNETWORK FILESYSTEM (NFS)\nservers will keep it in memory, and subsequent reads of said dat a (and\nmetadata) will not go to disk, a potential (small) boost in perform ance.\nMore intriguing is the case of write buffering. NFS servers abs olutely\nmay notreturn success on a WRITE protocol request until the write has\nbeen forced to stable storage (e.g., to disk or some other persiste nt device).\nWhile they can place a copy of the data in server memory, returnin g suc-\ncess to the client on a WRITE protocol request could result in incorr ect\nbehavior; can you ﬁgure out why?\nThe answer lies in our assumptions about how clients handle serve r\nfailure. Imagine the following sequence of writes as issued by a client:\nwrite(fd, a_buffer, size); // fill first block with a’s\nwrite(fd, b_buffer, size); // fill second block with b’s\nwrite(fd, c_buffer, size); // fill third block with c’s\nThese writes overwrite the three blocks of a ﬁle with a block of a’s,\nthen b’s, and then c’s. Thus, if the ﬁle initially looked like this :\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxx\nyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy yyyyyyyyy\nzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz zzzzzzzzz\nWe might expect the ﬁnal result after these writes to be like t his, with the\nx’s, y’s, and z’s, would be overwritten with a’s, b’s, and c’s, respect ively.\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaa\nbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb bbbbbbbbb\nccccccccccccccccccccccccccccccccccccccccccccccccccc ccccccccc\nNow let’s assume for the sake of the example that these three clien t\nwrites were issued to the server as three distinct WRITE protoc ol mes-\nsages. Assume the ﬁrst WRITE message is received by the serve r and\nissued to the disk, and the client informed of its success. Now as sume\nthe second write is just buffered in memory, and the server also reports\nit success to the client before forcing it to disk; unfortunately, the server\ncrashes before writing it to disk. The server quickly restart s and receives\nthe third write request, which also succeeds.\nThus, to the client, all the requests succeeded, but we are su rprised\nthat the ﬁle contents look like this:\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaa\nyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy yyyyyyyyy <--- oops\nccccccccccccccccccccccccccccccccccccccccccccccccccc ccccccccc\nYikes! Because the server told the client that the second write was\nsuccessful before committing it to disk, an old chunk is left in t he ﬁle,\nwhich, depending on the application, might be catastrophic.\nTo avoid this problem, NFS servers must commit each write to stable\n(persistent) storage before informing the client of success; doi ng so en-\nables the client to detect server failure during a write, and thus retry until\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSUN’SNETWORK FILESYSTEM (NFS) 15\nASIDE : INNOVATION BREEDS INNOVATION\nAs with many pioneering technologies, bringing NFS into the worl d also\nrequired other fundamental innovations to enable its success. Probably\nthe most lasting is the Virtual File System (VFS ) /Virtual Node (vnode )\ninterface, introduced by Sun to allow different ﬁle systems to b e readily\nplugged into the operating system [K86].\nThe VFS layer includes operations that are done to an entire ﬁle s ystem,\nsuch as mounting and unmounting, getting ﬁle-system wide stat istics,\nand forcing all dirty (not yet written) writes to disk. The vnode layer\nconsists of all operations one can perform on a ﬁle, such as open, close,\nreads, writes, and so forth.\nTo build a new ﬁle system, one simply has to deﬁne these “methods ”; the\nframework then handles the rest, connecting system calls to th e particular\nﬁle system implementation, performing generic functions common to all\nﬁle systems (e.g., caching) in a centralized manner, and thu s providing a\nway for multiple ﬁle system implementations to operate simulta neously\nwithin the same system.\nAlthough some of the details have changed, many modern systems ha ve\nsome form of a VFS/vnode layer, including Linux, BSD variants, macO S,\nand even Windows (in the form of the Installable File System). Eve n if\nNFS becomes less relevant to the world, some of the necessary found a-\ntions beneath it will live on.\nit ﬁnally succeeds. Doing so ensures we will never end up with ﬁ le con-\ntents intermingled as in the above example.\nThe problem that this requirement gives rise to in NFS server i m-\nplementation is that write performance, without great care, ca n be the\nmajor performance bottleneck. Indeed, some companies (e.g., Net work\nAppliance) came into existence with the simple objective of bu ilding an\nNFS server that can perform writes quickly; one trick they use i s to ﬁrst\nput writes in a battery-backed memory, thus enabling to quick ly reply\nto WRITE requests without fear of losing the data and without the c ost\nof having to write to disk right away; the second trick is to use a ﬁle sys-\ntem design speciﬁcally designed to write to disk quickly whe n one ﬁnally\nneeds to do so [HLM94, RO91].\n49.12 Summary\nWe have seen the introduction of the NFS distributed ﬁle system. NFS\nis centered around the idea of simple and fast recovery in the fac e of\nserver failure, and achieves this end through careful protocol d esign. Idem-\npotency of operations is essential; because a client can safely r eplay a\nfailed operation, it is OK to do so whether or not the server has exe cuted\nthe request.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n16 SUN’SNETWORK FILESYSTEM (NFS)\nASIDE : KEYNFS T ERMS\n•The key to realizing the main goal of fast and simple crash recove ry\nin NFS is in the design of a stateless protocol. After a crash, the\nserver can quickly restart and begin serving requests again ; clients\njustretry requests until they succeed.\n•Making requests idempotent is a central aspect of the NFS protocol.\nAn operation is idempotent when the effect of performing it multi-\nple times is equivalent to performing it once. In NFS, idempotenc y\nenables client retry without worry, and uniﬁes client lost-mes sage\nretransmission and how the client handles server crashes.\n•Performance concerns dictate the need for client-side caching and\nwrite buffering , but introduces a cache consistency problem .\n•NFS implementations provide an engineering solution to cache\nconsistency through multiple means: a ﬂush-on-close (close-to-\nopen ) approach ensures that when a ﬁle is closed, its contents are\nforced to the server, enabling other clients to observe the upda tes\nto it. An attribute cache reduces the frequency of checking wi th the\nserver whether a ﬁle has changed (via GETATTR requests).\n•NFS servers must commit writes to persistent media before retu rn-\ning success; otherwise, data loss can arise.\n•To support NFS integration into the operating system, Sun intro-\nduced the VFS/Vnode interface, enabling multiple ﬁle system im-\nplementations to coexist in the same operating system.\nWe also have seen how the introduction of caching into a multiple-\nclient, single-server system can complicate things. In part icular, the sys-\ntem must resolve the cache consistency problem in order to behave rea-\nsonably; however, NFS does so in a slightly ad hoc fashion which can\noccasionally result in observably weird behavior. Finally, we s aw how\nserver caching can be tricky: writes to the server must be forc ed to stable\nstorage before returning success (otherwise data can be lost).\nWe haven’t talked about other issues which are certainly releva nt, no-\ntably security. Security in early NFS implementations was rem arkably\nlax; it was rather easy for any user on a client to masquerade as ot her\nusers and thus gain access to virtually any ﬁle. Subsequent in tegration\nwith more serious authentication services (e.g., Kerberos [NT9 4]) have\naddressed these obvious deﬁciencies.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nSUN’SNETWORK FILESYSTEM (NFS) 17\nReferences\n[AKW88] “The AWK Programming Language” by Alfred V . Aho, Brian W. Kerni ghan, Peter\nJ. Weinberger. Pearson, 1988 (1st edition). A concise, wonderful book about awk. We once had the\npleasure of meeting Peter Weinberger; when he introduced himself, he said “I’m Peter Weinberger, you\nknow, the ’W’ in awk?” As huge awk fans, this was a moment to savor. One of us (Remzi ) then said,\n“I love awk! I particularly love the book, which makes everything so wonderful ly clear.” Weinberger\nreplied (crestfallen), “Oh, Kernighan wrote the book.”\n[C00] “NFS Illustrated” by Brent Callaghan. Addison-Wesley Profess ional Computing Series,\n2000. A great NFS reference; incredibly thorough and detailed per the protocol i tself.\n[ES03] “New NFS Tracing Tools and Techniques for System Analysis” by Da niel Ellard and\nMargo Seltzer. LISA ’03, San Diego, California. An intricate, careful analysis of NFS done via\npassive tracing. By simply monitoring network trafﬁc, the authors show how to de rive a vast amount\nof ﬁle system understanding.\n[HLM94] “File System Design for an NFS File Server Appliance” by Da ve Hitz, James Lau,\nMichael Malcolm. USENIX Winter 1994. San Francisco, California, 1994. Hitz et al. were greatly\ninﬂuenced by previous work on log-structured ﬁle systems.\n[K86] “Vnodes: An Architecture for Multiple File System Types in Sun UNIX” by Steve R.\nKleiman. USENIX Summer ’86, Atlanta, Georgia. This paper shows how to build a ﬂexible ﬁle\nsystem architecture into an operating system, enabling multiple differe nt ﬁle system implementations\nto coexist. Now used in virtually every modern operating system in some form .\n[NT94] “Kerberos: An Authentication Service for Computer Networks ” by B. Clifford Neu-\nman, Theodore Ts’o. IEEE Communications, 32(9):33-38, September 199 4.Kerberos is an early\nand hugely inﬂuential authentication service. We probably should write a b ook chapter about it some-\ntime...\n[O91] “The Role of Distributed State” by John K. Ousterhout. 1991. Ava ilable at this site:\nftp://ftp.cs.berkeley.edu/ucb/sprite/papers/state.p s.A rarely referenced dis-\ncussion of distributed state; a broader perspective on the problems and chall enges.\n[P+94] “NFS Version 3: Design and Implementation” by Brian Pawlows ki, Chet Juszczak, Peter\nStaubach, Carl Smith, Diane Lebel, Dave Hitz. USENIX Summer 1994 , pages 137-152. The small\nmodiﬁcations that underlie NFS version 3.\n[P+00] “The NFS version 4 protocol” by Brian Pawlowski, David Noveck , David Robinson,\nRobert Thurlow. 2nd International System Administration and Networ king Conference (SANE\n2000). Undoubtedly the most literary paper on NFS ever written.\n[RO91] “The Design and Implementation of the Log-structured File Syst em” by Mendel Rosen-\nblum, John Ousterhout. Symposium on Operating Systems Principles (SOSP), 1991. LFS again.\nNo, you can never get enough LFS.\n[S86] “The Sun Network File System: Design, Implementation and Exp erience” by Russel\nSandberg. USENIX Summer 1986. The original NFS paper; though a bit of a challenging read,\nit is worthwhile to see the source of these wonderful ideas.\n[Sun89] “NFS: Network File System Protocol Speciﬁcation” by Sun Micro systems, Inc. Request\nfor Comments: 1094, March 1989. Available: http://www.ietf.org/rfc/rfc1094.txt .\nThe dreaded speciﬁcation; read it if you must, i.e., you are getting paid to r ead it. Hopefully, paid a lot.\nCash money!\n[V72] “La Begueule” by Francois-Marie Arouet a.k.a. Voltaire. Pub lished in 1772. Voltaire said\na number of clever things, this being but one example. For example, Vol taire also said “If you have two\nreligions in your land, the two will cut each others throats; but if you have thi rty religions, they will\ndwell in peace.” What do you say to that, Democrats and Republicans?\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n18 SUN’SNETWORK FILESYSTEM (NFS)\nHomework (Measurement)\nIn this homework, you’ll do a little bit of NFS trace analysis using real\ntraces. The source of these traces is Ellard and Seltzer’s effort [ ES03].\nMake sure to read the related README and download the relevant t ar-\nball from the OSTEP homework page (as usual) before starting.\nQuestions\n1. A ﬁrst question for your trace analysis: using the timestamps fo und in the\nﬁrst column, determine the period of time the traces were taken fr om. How\nlong is the period? What day/week/month/year was it? (does th is match\nthe hint given in the ﬁle name?) Hint: Use the tools head -1 andtail -1\nto extract the ﬁrst and last lines of the ﬁle, and do the calculat ion.\n2. Now, let’s do some operation counts. How many of each type of op era-\ntion occur in the trace? Sort these by frequency; which operati on is most\nfrequent? Does NFS live up to its reputation?\n3. Now let’s look at some particular operations in more detail. F or example,\nthe GETATTR request returns a lot of information about ﬁles, incl uding\nwhich user ID the request is being performed for, the size of the ﬁ le, and\nso forth. Make a distribution of ﬁle sizes accessed within the trace; what\nis the average ﬁle size? Also, how many different users access ﬁl es in the\ntrace? Do a few users dominate trafﬁc, or is it more spread out? Wha t other\ninteresting information is found within GETATTR replies?\n4. You can also look at requests to a given ﬁle and determine how ﬁ les are be-\ning accessed. For example, is a given ﬁle being read or written s equentially?\nOr randomly? Look at the details of READ and WRITE requests/repl ies to\ncompute the answer.\n5. Trafﬁc comes from many machines and goes to one server (in this trace).\nCompute a trafﬁc matrix, which shows how many different clients th ere are\nin the trace, and how many requests/replies go to each. Do a few ma chines\ndominate, or is it more evenly balanced?\n6. The timing information, and the per-request/reply unique ID, s hould allow\nyou to compute the latency for a given request. Compute the latenci es of all\nrequest/reply pairs, and plot them as a distribution. What is t he average?\nMaximum? Minimum?\n7. Sometimes requests are retried, as the request or its reply coul d be lost or\ndropped. Can you ﬁnd any evidence of such retrying in the trace sample?\n8. There are many other questions you could answer through more ana lysis.\nWhat questions do you think are important? Suggest them to us, and per-\nhaps we’ll add them here!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",46867
56-50. Andrew File System AFS.pdf,56-50. Andrew File System AFS,"50\nThe Andrew File System (AFS)\nThe Andrew File System was introduced at Carnegie-Mellon Unive rsity\n(CMU)1in the 1980’s [H+88]. Led by the well-known Professor M. Satya-\nnarayanan of Carnegie-Mellon University (“Satya” for short), th e main\ngoal of this project was simple: scale . Speciﬁcally, how can one design a\ndistributed ﬁle system such that a server can support as many c lients as\npossible?\nInterestingly, there are numerous aspects of design and imple menta-\ntion that affect scalability. Most important is the design of the protocol be-\ntween clients and servers. In NFS, for example, the protocol forces clients\nto check with the server periodically to determine if cached c ontents have\nchanged; because each check uses server resources (includin g CPU and\nnetwork bandwidth), frequent checks like this will limit the number of\nclients a server can respond to and thus limit scalability.\nAFS also differs from NFS in that from the beginning, reasonable user-\nvisible behavior was a ﬁrst-class concern. In NFS, cache consist ency is\nhard to describe because it depends directly on low-level impl ementa-\ntion details, including client-side cache timeout intervals . In AFS, cache\nconsistency is simple and readily understood: when the ﬁle is ope ned, a\nclient will generally receive the latest consistent copy from t he server.\n50.1 AFS Version 1\nWe will discuss two versions of AFS [H+88, S+85]. The ﬁrst version\n(which we will call AFSv1, but actually the original system was called\nthe ITC distributed ﬁle system [S+85]) had some of the basic desi gn in\nplace, but didn’t scale as desired, which led to a re-design an d the ﬁnal\nprotocol (which we will call AFSv2, or just AFS) [H+88]. We now discus s\nthe ﬁrst version.\n1Though originally referred to as “Carnegie-Mellon University”, CMU l ater dropped\nthe hyphen, and thus was born the modern form, “Carnegie Mellon Universit y.” As AFS\nderived from work in the early 80’s, we refer to CMU in its original fu lly-hyphenated form. See\nhttps://www.quora.com/When-did-Carnegie-Mellon-Univ ersity-remove-the-\nhyphen-in-the-university-name for more details, if you are into really boring minutiae.\n1\n2 THEANDREW FILESYSTEM (AFS)\nTestAuth Test whether a file has changed\n(used to validate cached entries)\nGetFileStat Get the stat info for a file\nFetch Fetch the contents of file\nStore Store this file on the server\nSetFileStat Set the stat info for a file\nListDir List the contents of a directory\nFigure 50.1: AFSv1 Protocol Highlights\nOne of the basic tenets of all versions of AFS is whole-ﬁle caching on\nthelocal disk of the client machine that is accessing a ﬁle. When you\nopen() a ﬁle, the entire ﬁle (if it exists) is fetched from the server a nd\nstored in a ﬁle on your local disk. Subsequent application read() and\nwrite() operations are redirected to the local ﬁle system where the ﬁle i s\nstored; thus, these operations require no network communication a nd are\nfast. Finally, upon close() , the ﬁle (if it has been modiﬁed) is ﬂushed\nback to the server. Note the obvious contrasts with NFS, which cach es\nblocks (not whole ﬁles, although NFS could of course cache every block of\nan entire ﬁle) and does so in client memory (not local disk).\nLet’s get into the details a bit more. When a client application ﬁ rst calls\nopen() , the AFS client-side code (which the AFS designers call Venus )\nwould send a Fetch protocol message to the server. The Fetch protocol\nmessage would pass the entire pathname of the desired ﬁle (for ex am-\nple,/home/remzi/notes.txt ) to the ﬁle server (the group of which\nthey called Vice ), which would then traverse the pathname, ﬁnd the de-\nsired ﬁle, and ship the entire ﬁle back to the client. The clie nt-side code\nwould then cache the ﬁle on the local disk of the client (by writing it to\nlocal disk). As we said above, subsequent read() andwrite() system\ncalls are strictly local in AFS (no communication with the server occurs);\nthey are just redirected to the local copy of the ﬁle. Because the read()\nandwrite() calls act just like calls to a local ﬁle system, once a block\nis accessed, it also may be cached in client memory. Thus, AFS a lso uses\nclient memory to cache copies of blocks that it has in its local disk . Fi-\nnally, when ﬁnished, the AFS client checks if the ﬁle has been modiﬁed\n(i.e., that it has been opened for writing); if so, it ﬂushes the n ew version\nback to the server with a Store protocol message, sending the entir e ﬁle\nand pathname to the server for permanent storage.\nThe next time the ﬁle is accessed, AFSv1 does so much more efﬁ-\nciently. Speciﬁcally, the client-side code ﬁrst contacts the s erver (using\nthe TestAuth protocol message) in order to determine whether the ﬁle\nhas changed. If not, the client would use the locally-cached copy , thus\nimproving performance by avoiding a network transfer. The ﬁgure above\nshows some of the protocol messages in AFSv1. Note that this early ver-\nsion of the protocol only cached ﬁle contents; directories, for exampl e,\nwere only kept at the server.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEANDREW FILESYSTEM (AFS) 3\nTIP: M EASURE THEN BUILD (PATTERSON ’SLAW)\nOne of our advisors, David Patterson (of RISC and RAID fame), used to\nalways encourage us to measure a system and demonstrate a proble m\nbefore building a new system to ﬁx said problem. By using experimen-\ntal evidence, rather than gut instinct, you can turn the proces s of system\nbuilding into a more scientiﬁc endeavor. Doing so also has the fr inge ben-\neﬁt of making you think about how exactly to measure the system bef ore\nyour improved version is developed. When you do ﬁnally get around to\nbuilding the new system, two things are better as a result: ﬁr st, you have\nevidence that shows you are solving a real problem; second, you now\nhave a way to measure your new system in place, to show that it act ually\nimproves upon the state of the art. And thus we call this Patterson’s Law .\n50.2 Problems with Version 1\nA few key problems with this ﬁrst version of AFS motivated the de-\nsigners to rethink their ﬁle system. To study the problems in d etail, the\ndesigners of AFS spent a great deal of time measuring their exis ting pro-\ntotype to ﬁnd what was wrong. Such experimentation is a good thing,\nbecause measurement is the key to understanding how systems work\nand how to improve them; obtaining concrete, good data is thus a nece s-\nsary part of systems construction. In their study, the authors fou nd two\nmain problems with AFSv1:\n•Path-traversal costs are too high : When performing a Fetch or Store\nprotocol request, the client passes the entire pathname (e.g., /home/\nremzi/notes.txt ) to the server. The server, in order to access the\nﬁle, must perform a full pathname traversal, ﬁrst looking in the root\ndirectory to ﬁnd home , then inhome to ﬁndremzi , and so forth,\nall the way down the path until ﬁnally the desired ﬁle is located .\nWith many clients accessing the server at once, the designers of AFS\nfound that the server was spending much of its CPU time simply\nwalking down directory paths.\n•The client issues too many TestAuth protocol messages : Much\nlike NFS and its overabundance of GETATTR protocol messages,\nAFSv1 generated a large amount of trafﬁc to check whether a lo-\ncal ﬁle (or its stat information) was valid with the TestAuth prot o-\ncol message. Thus, servers spent much of their time telling cli ents\nwhether it was OK to used their cached copies of a ﬁle. Most of the\ntime, the answer was that the ﬁle had not changed.\nThere were actually two other problems with AFSv1: load was not\nbalanced across servers, and the server used a single distinc t process per\nclient thus inducing context switching and other overheads. Th e load\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 THEANDREW FILESYSTEM (AFS)\nimbalance problem was solved by introducing volumes , which an ad-\nministrator could move across servers to balance load; the context -switch\nproblem was solved in AFSv2 by building the server with threads i nstead\nof processes. However, for the sake of space, we focus here on the main\ntwo protocol problems above that limited the scale of the system.\n50.3 Improving the Protocol\nThe two problems above limited the scalability of AFS; the server CPU\nbecame the bottleneck of the system, and each server could only se r-\nvice 20 clients without becoming overloaded. Servers were receiv ing too\nmany TestAuth messages, and when they received Fetch or Store me s-\nsages, were spending too much time traversing the directory hi erarchy.\nThus, the AFS designers were faced with a problem:\nTHECRUX: HOWTODESIGN A S CALABLE FILEPROTOCOL\nHow should one redesign the protocol to minimize the number of\nserver interactions, i.e., how could they reduce the number of Te stAuth\nmessages? Further, how could they design the protocol to make thes e\nserver interactions efﬁcient? By attacking both of these issue s, a new pro-\ntocol would result in a much more scalable version AFS.\n50.4 AFS Version 2\nAFSv2 introduced the notion of a callback to reduce the number of\nclient/server interactions. A callback is simply a promise fr om the server\nto the client that the server will inform the client when a ﬁle t hat the\nclient is caching has been modiﬁed. By adding this state to the system,\nthe client no longer needs to contact the server to ﬁnd out if a cac hed ﬁle\nis still valid. Rather, it assumes that the ﬁle is valid until the server tells it\notherwise; notice the analogy to polling versus interrupts .\nAFSv2 also introduced the notion of a ﬁle identiﬁer (FID) (similar to\nthe NFS ﬁle handle ) instead of pathnames to specify which ﬁle a client\nwas interested in. An FID in AFS consists of a volume identiﬁer, a ﬁle\nidentiﬁer, and a “uniquiﬁer” (to enable reuse of the volume and ﬁle IDs\nwhen a ﬁle is deleted). Thus, instead of sending whole pathname s to\nthe server and letting the server walk the pathname to ﬁnd the desired\nﬁle, the client would walk the pathname, one piece at a time, cac hing the\nresults and thus hopefully reducing the load on the server.\nFor example, if a client accessed the ﬁle /home/remzi/notes.txt ,\nandhome was the AFS directory mounted onto /(i.e.,/was the local root\ndirectory, but home and its children were in AFS), the client would ﬁrst\nFetch the directory contents of home , put them in the local-disk cache,\nand set up a callback on home . Then, the client would Fetch the directory\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEANDREW FILESYSTEM (AFS) 5\nClient (C 1) Server\nfd = open(“/home/remzi/notes.txt”, ...);\nSend Fetch (home FID, “remzi”)\nReceive Fetch request\nlook for remzi in home dir\nestablish callback(C 1) on remzi\nreturn remzi’s content and FID\nReceive Fetch reply\nwrite remzi to local disk cache\nrecord callback status of remzi\nSend Fetch (remzi FID, “notes.txt”)\nReceive Fetch request\nlook for notes.txt in remzi dir\nestablish callback(C 1) on notes.txt\nreturn notes.txt’s content and FID\nReceive Fetch reply\nwrite notes.txt to local disk cache\nrecord callback status of notes.txt\nlocalopen() of cached notes.txt\nreturn ﬁle descriptor to application\nread(fd, buffer, MAX);\nperform local read() on cached copy\nclose(fd);\ndo localclose() on cached copy\nif ﬁle has changed, ﬂush to server\nfd = open(“/home/remzi/notes.txt”, ...);\nForeach dir (home, remzi)\nif (callback(dir) == VALID)\nuse local copy for lookup(dir)\nelse\nFetch (as above)\nif (callback(notes.txt) == VALID)\nopen local cached copy\nreturn ﬁle descriptor to it\nelse\nFetch (as above) then open and return fd\nFigure 50.2: Reading A File: Client-side And File Server Actions\nremzi , put it in the local-disk cache, and set up a callback on remzi .\nFinally, the client would Fetch notes.txt , cache this regular ﬁle in the\nlocal disk, set up a callback, and ﬁnally return a ﬁle descript or to the\ncalling application. See Figure 50.2 for a summary.\nThe key difference, however, from NFS, is that with each fetch of a\ndirectory or ﬁle, the AFS client would establish a callback with the server,\nthus ensuring that the server would notify the client of a change in its\ncached state. The beneﬁt is obvious: although the ﬁrst access to/home/\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 THEANDREW FILESYSTEM (AFS)\nASIDE : CACHE CONSISTENCY ISNOTA P ANACEA\nWhen discussing distributed ﬁle systems, much is made of the c ache con-\nsistency the ﬁle systems provide. However, this baseline consi stency does\nnot solve all problems with regards to ﬁle access from multiple cl ients.\nFor example, if you are building a code repository, with multiple c lients\nperforming check-ins and check-outs of code, you can’t simply rely on\nthe underlying ﬁle system to do all of the work for you; rather, you h ave\nto use explicit ﬁle-level locking in order to ensure that the “right” thing\nhappens when such concurrent accesses take place. Indeed, an y applica-\ntion that truly cares about concurrent updates will add extra ma chinery\nto handle conﬂicts. The baseline consistency described in thi s chapter and\nthe previous one are useful primarily for casual usage, i.e., wh en a user\nlogs into a different client, they expect some reasonable versi on of their\nﬁles to show up there. Expecting more from these protocols is settin g\nyourself up for failure, disappointment, and tear-ﬁlled frust ration.\nremzi/notes.txt generates many client-server messages (as described\nabove), it also establishes callbacks for all the directories a s well as the\nﬁle notes.txt, and thus subsequent accesses are entirely loca l and require\nno server interaction at all. Thus, in the common case where a ﬁle is\ncached at the client, AFS behaves nearly identically to a loca l disk-based\nﬁle system. If one accesses a ﬁle more than once, the second access should\nbe just as fast as accessing a ﬁle locally.\n50.5 Cache Consistency\nWhen we discussed NFS, there were two aspects of cache consisten cy\nwe considered: update visibility and cache staleness . With update visi-\nbility, the question is: when will the server be updated with a new version\nof a ﬁle? With cache staleness, the question is: once the server h as a new\nversion, how long before clients see the new version instead of an old er\ncached copy?\nBecause of callbacks and whole-ﬁle caching, the cache consiste ncy pro-\nvided by AFS is easy to describe and understand. There are two im-\nportant cases to consider: consistency between processes on different ma-\nchines, and consistency between processes on the same machine.\nBetween different machines, AFS makes updates visible at th e server\nand invalidates cached copies at the exact same time, which is when the\nupdated ﬁle is closed. A client opens a ﬁle, and then writes to it (perhaps\nrepeatedly). When it is ﬁnally closed, the new ﬁle is ﬂushed to the server\n(and thus visible). At this point, the server then “breaks” ca llbacks for\nany clients with cached copies; the break is accomplished by con tacting\neach client and informing it that the callback it has on the ﬁle i s no longer\nvalid. This step ensures that clients will no longer read stal e copies of\nthe ﬁle; subsequent opens on those clients will require a re-fet ch of the\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEANDREW FILESYSTEM (AFS) 7\nClient 1 Client 2 Server Comments\nP1 P2 Cache P 3 Cache Disk\nopen(F) - - - File created\nwrite(A) A - -\nclose() A - A\nopen(F) A - A\nread()→A A - A\nclose() A - A\nopen(F) A - A\nwrite(B) B - A\nopen(F) B - A Local processes\nread()→B B - A see writes immediately\nclose() B - A\nB open(F) A A Remote processes\nB read() →A A A do not see writes...\nB close() A A\nclose() B ✁A B ... until close()\nB open(F) B B has taken place\nB read() →B B B\nB close() B B\nB open(F) B B\nopen(F) B B B\nwrite(D) D B B\nD write(C) C B\nD close() C C\nclose() D ✁C D\nD open(F) D D Unfortunately for P 3\nD read() →D D D the last writer wins\nD close() D D\nFigure 50.3: Cache Consistency Timeline\nnew version of the ﬁle from the server (and will also serve to rees tablish\na callback on the new version of the ﬁle).\nAFS makes an exception to this simple model between processes on\nthe same machine. In this case, writes to a ﬁle are immediatel y visible to\nother local processes (i.e., a process does not have to wait until a ﬁ le is\nclosed to see its latest updates). This makes using a single ma chine be-\nhave exactly as you would expect, as this behavior is based upon ty pical\nUNIXsemantics. Only when switching to a different machine would y ou\nbe able to detect the more general AFS consistency mechanism.\nThere is one interesting cross-machine case that is worthy of fur ther\ndiscussion. Speciﬁcally, in the rare case that processes on diff erent ma-\nchines are modifying a ﬁle at the same time, AFS naturally empl oys what\nis known as a last writer wins approach (which perhaps should be called\nlast closer wins ). Speciﬁcally, whichever client calls close() last will\nupdate the entire ﬁle on the server last and thus will be the “wi nning”\nﬁle, i.e., the ﬁle that remains on the server for others to see. Th e result is\na ﬁle that was generated in its entirety either by one client or t he other.\nNote the difference from a block-based protocol like NFS: in NFS, writ es\nof individual blocks may be ﬂushed out to the server as each clien t is up-\ndating the ﬁle, and thus the ﬁnal ﬁle on the server could end up as a mix\nof updates from both clients. In many cases, such a mixed ﬁle outpu t\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 THEANDREW FILESYSTEM (AFS)\nwould not make much sense, i.e., imagine a JPEG image getting mod i-\nﬁed by two clients in pieces; the resulting mix of writes would n ot likely\nconstitute a valid JPEG.\nA timeline showing a few of these different scenarios can be seen in\nFigure 50.3. The columns show the behavior of two processes (P 1and P 2)\non Client 1and its cache state, one process (P 3) on Client 2and its cache\nstate, and the server (Server), all operating on a single ﬁle cal led, imag-\ninatively,F. For the server, the ﬁgure simply shows the contents of the\nﬁle after the operation on the left has completed. Read through it a nd see\nif you can understand why each read returns the results that it does. A\ncommentary ﬁeld on the right will help you if you get stuck.\n50.6 Crash Recovery\nFrom the description above, you might sense that crash recovery is\nmore involved than with NFS. You would be right. For example, imagin e\nthere is a short period of time where a server (S) is not able to contac t\na client (C1), for example, while the client C1 is rebooting. Whi le C1\nis not available, S may have tried to send it one or more callback re call\nmessages; for example, imagine C1 had ﬁle F cached on its local di sk, and\nthen C2 (another client) updated F, thus causing S to send mess ages to all\nclients caching the ﬁle to remove it from their local caches. Bec ause C1\nmay miss those critical messages when it is rebooting, upon rejoin ing the\nsystem, C1 should treat all of its cache contents as suspect. Thu s, upon\nthe next access to ﬁle F, C1 should ﬁrst ask the server (with a Te stAuth\nprotocol message) whether its cached copy of ﬁle F is still valid; i f so, C1\ncan use it; if not, C1 should fetch the newer version from the serve r.\nServer recovery after a crash is also more complicated. The proble m\nthat arises is that callbacks are kept in memory; thus, when a s erver re-\nboots, it has no idea which client machine has which ﬁles. Thus, upon\nserver restart, each client of the server must realize that th e server has\ncrashed and treat all of their cache contents as suspect, and (a s above)\nreestablish the validity of a ﬁle before using it. Thus, a serve r crash is a\nbig event, as one must ensure that each client is aware of the cra sh in a\ntimely manner, or risk a client accessing a stale ﬁle. There ar e many ways\nto implement such recovery; for example, by having the server s end a\nmessage (saying “don’t trust your cache contents!”) to each clien t when\nit is up and running again, or by having clients check that the s erver is\nalive periodically (with a heartbeat message, as it is called). As you can\nsee, there is a cost to building a more scalable and sensible cac hing model;\nwith NFS, clients hardly noticed a server crash.\n50.7 Scale And Performance Of AFSv2\nWith the new protocol in place, AFSv2 was measured and found to be\nmuch more scalable that the original version. Indeed, each serv er could\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEANDREW FILESYSTEM (AFS) 9\nWorkload NFS AFS AFS/NFS\n1. Small ﬁle, sequential read Ns·Lnet Ns·Lnet 1\n2. Small ﬁle, sequential re-read Ns·Lmem Ns·Lmem 1\n3. Medium ﬁle, sequential read Nm·Lnet Nm·Lnet 1\n4. Medium ﬁle, sequential re-read Nm·Lmem Nm·Lmem 1\n5. Large ﬁle, sequential read NL·Lnet NL·Lnet 1\n6. Large ﬁle, sequential re-read NL·Lnet NL·LdiskLdisk\nLnet\n7. Large ﬁle, single read Lnet NL·Lnet NL\n8. Small ﬁle, sequential write Ns·Lnet Ns·Lnet 1\n9. Large ﬁle, sequential write NL·Lnet NL·Lnet 1\n10. Large ﬁle, sequential overwrite NL·Lnet2·NL·Lnet 2\n11. Large ﬁle, single write Lnet 2·NL·Lnet2·NL\nFigure 50.4: Comparison: AFS vs. NFS\nsupport about 50 clients (instead of just 20). A further beneﬁt w as that\nclient-side performance often came quite close to local performa nce, be-\ncause in the common case, all ﬁle accesses were local; ﬁle reads u sually\nwent to the local disk cache (and potentially, local memory). Onl y when a\nclient created a new ﬁle or wrote to an existing one was there need to send\na Store message to the server and thus update the ﬁle with new cont ents.\nLet us also gain some perspective on AFS performance by compar-\ning common ﬁle-system access scenarios with NFS. Figure 50.4 (pa ge 9)\nshows the results of our qualitative comparison.\nIn the ﬁgure, we examine typical read and write patterns anal ytically,\nfor ﬁles of different sizes. Small ﬁles have Nsblocks in them; medium\nﬁles have Nmblocks; large ﬁles have NLblocks. We assume that small\nand medium ﬁles ﬁt into the memory of a client; large ﬁles ﬁt on a loc al\ndisk but not in client memory.\nWe also assume, for the sake of analysis, that an access across th e net-\nwork to the remote server for a ﬁle block takes Lnettime units. Access\nto local memory takes Lmem , and access to local disk takes Ldisk. The\ngeneral assumption is that Lnet> Ldisk> Lmem .\nFinally, we assume that the ﬁrst access to a ﬁle does not hit in an y\ncaches. Subsequent ﬁle accesses (i.e., “re-reads”) we assum e will hit in\ncaches, if the relevant cache has enough capacity to hold the ﬁl e.\nThe columns of the ﬁgure show the time a particular operation (e.g. , a\nsmall ﬁle sequential read) roughly takes on either NFS or AFS. The right-\nmost column displays the ratio of AFS to NFS.\nWe make the following observations. First, in many cases, the per -\nformance of each system is roughly equivalent. For example, when ﬁrst\nreading a ﬁle (e.g., Workloads 1, 3, 5), the time to fetch the ﬁle from the re-\nmote server dominates, and is similar on both systems. You might th ink\nAFS would be slower in this case, as it has to write the ﬁle to local disk;\nhowever, those writes are buffered by the local (client-side) ﬁ le system\ncache and thus said costs are likely hidden. Similarly, you migh t think\nthat AFS reads from the local cached copy would be slower, again be-\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 THEANDREW FILESYSTEM (AFS)\ncause AFS stores the cached copy on disk. However, AFS again beneﬁ ts\nhere from local ﬁle system caching; reads on AFS would likely hit i n the\nclient-side memory cache, and performance would be similar to N FS.\nSecond, an interesting difference arises during a large-ﬁle s equential\nre-read (Workload 6). Because AFS has a large local disk cache, i t will\naccess the ﬁle from there when the ﬁle is accessed again. NFS, in contrast,\nonly can cache blocks in client memory; as a result, if a large ﬁle (i.e., a ﬁle\nbigger than local memory) is re-read, the NFS client will have t o re-fetch\nthe entire ﬁle from the remote server. Thus, AFS is faster than N FS in this\ncase by a factor ofLnet\nLdisk, assuming that remote access is indeed slower\nthan local disk. We also note that NFS in this case increases ser ver load,\nwhich has an impact on scale as well.\nThird, we note that sequential writes (of new ﬁles) should perfor m\nsimilarly on both systems (Workloads 8, 9). AFS, in this case, will write\nthe ﬁle to the local cached copy; when the ﬁle is closed, the AFS cl ient\nwill force the writes to the server, as per the protocol. NFS will b uffer\nwrites in client memory, perhaps forcing some blocks to the serve r due\nto client-side memory pressure, but deﬁnitely writing them t o the server\nwhen the ﬁle is closed, to preserve NFS ﬂush-on-close consistenc y. You\nmight think AFS would be slower here, because it writes all data to local\ndisk. However, realize that it is writing to a local ﬁle system; those writes\nare ﬁrst committed to the page cache, and only later (in the back ground)\nto disk, and thus AFS reaps the beneﬁts of the client-side OS me mory\ncaching infrastructure to improve performance.\nFourth, we note that AFS performs worse on a sequential ﬁle over-\nwrite (Workload 10). Thus far, we have assumed that the workloads that\nwrite are also creating a new ﬁle; in this case, the ﬁle exists , and is then\nover-written. Overwrite can be a particularly bad case for AFS, because\nthe client ﬁrst fetches the old ﬁle in its entirety, only to subs equently over-\nwrite it. NFS, in contrast, will simply overwrite blocks and thus avoid the\ninitial (useless) read2.\nFinally, workloads that access a small subset of data within lar ge ﬁles\nperform much better on NFS than AFS (Workloads 7, 11). In these cas es,\nthe AFS protocol fetches the entire ﬁle when the ﬁle is opened; unf ortu-\nnately, only a small read or write is performed. Even worse, if the ﬁle is\nmodiﬁed, the entire ﬁle is written back to the server, doubling the per-\nformance impact. NFS, as a block-based protocol, performs I/O that i s\nproportional to the size of the read or write.\nOverall, we see that NFS and AFS make different assumptions an d not\nsurprisingly realize different performance outcomes as a resu lt. Whether\nthese differences matter is, as always, a question of workload.\n2We assume here that NFS writes are block-sized and block-aligned; if t hey were not, the\nNFS client would also have to read the block ﬁrst. We also assume the ﬁ le was notopened\nwith the O TRUNC ﬂag; if it had been, the initial open in AFS would not fetch the soon to be\ntruncated ﬁle’s contents.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEANDREW FILESYSTEM (AFS) 11\nASIDE : THEIMPORTANCE OFWORKLOAD\nOne challenge of evaluating any system is the choice of workload . Be-\ncause computer systems are used in so many different ways, the re are a\nlarge variety of workloads to choose from. How should the storage sys-\ntem designer decide which workloads are important, in order to ma ke\nreasonable design decisions?\nThe designers of AFS, given their experience in measuring how ﬁl e sys-\ntems were used, made certain workload assumptions; in particul ar, they\nassumed that most ﬁles were not frequently shared, and accesse d sequen-\ntially in their entirety. Given those assumptions, the AFS des ign makes\nperfect sense.\nHowever, these assumptions are not always correct. For example, i mag-\nine an application that appends information, periodically, to a log. These\nlittle log writes, which add small amounts of data to an existing large ﬁle,\nare quite problematic for AFS. Many other difﬁcult workloads exist as\nwell, e.g., random updates in a transaction database.\nOne place to get some information about what types of workloads are\ncommon are through various research studies that have been perfor med.\nSee any of these studies for good examples of workload analysis [B+91,\nH+11, R+00, V99], including the AFS retrospective [H+88].\n50.8 AFS: Other Improvements\nLike we saw with the introduction of Berkeley FFS (which added sy m-\nbolic links and a number of other features), the designers of AFS t ook the\nopportunity when building their system to add a number of featur es that\nmade the system easier to use and manage. For example, AFS provi des a\ntrue global namespace to clients, thus ensuring that all ﬁles were named\nthe same way on all client machines. NFS, in contrast, allows each client\nto mount NFS servers in any way that they please, and thus only by con-\nvention (and great administrative effort) would ﬁles be named s imilarly\nacross clients.\nAFS also takes security seriously, and incorporates mechanism s to au-\nthenticate users and ensure that a set of ﬁles could be kept priv ate if a\nuser so desired. NFS, in contrast, had quite primitive support f or security\nfor many years.\nAFS also includes facilities for ﬂexible user-managed acces s control.\nThus, when using AFS, a user has a great deal of control over who exac tly\ncan access which ﬁles. NFS, like most U NIX ﬁle systems, has much less\nsupport for this type of sharing.\nFinally, as mentioned before, AFS adds tools to enable simpler ma n-\nagement of servers for the administrators of the system. In think ing about\nsystem management, AFS was light years ahead of the ﬁeld.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 THEANDREW FILESYSTEM (AFS)\n50.9 Summary\nAFS shows us how distributed ﬁle systems can be built quite diff er-\nently than what we saw with NFS. The protocol design of AFS is partic -\nularly important; by minimizing server interactions (through whole-ﬁle\ncaching and callbacks), each server can support many clients and thus\nreduce the number of servers needed to manage a particular sit e. Many\nother features, including the single namespace, security, a nd access-control\nlists, make AFS quite nice to use. The consistency model provide d by AFS\nis simple to understand and reason about, and does not lead to the oc ca-\nsional weird behavior as one sometimes observes in NFS.\nPerhaps unfortunately, AFS is likely on the decline. Because N FS be-\ncame an open standard, many different vendors supported it, and , along\nwith CIFS (the Windows-based distributed ﬁle system protocol), NFS\ndominates the marketplace. Although one still sees AFS install ations\nfrom time to time (such as in various educational institutions, i ncluding\nWisconsin), the only lasting inﬂuence will likely be from the id eas of AFS\nrather than the actual system itself. Indeed, NFSv4 now adds se rver state\n(e.g., an “open” protocol message), and thus bears an increasing similar-\nity to the basic AFS protocol.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nTHEANDREW FILESYSTEM (AFS) 13\nReferences\n[B+91] “Measurements of a Distributed File System” by Mary Baker , John Hartman, Martin\nKupfer, Ken Shirriff, John Ousterhout. SOSP ’91, Paciﬁc Grove, Californi a, October 1991. An\nearly paper measuring how people use distributed ﬁle systems. Matches m uch of the intuition found in\nAFS.\n[H+11] “A File is Not a File: Understanding the I/O Behavior of Apple Desktop Applications”\nby Tyler Harter, Chris Dragga, Michael Vaughn, Andrea C. Arpaci-Dusse au, Remzi H. Arpaci-\nDusseau. SOSP ’11, New York, New York, October 2011. Our own paper studying the behavior of\nApple Desktop workloads; turns out they are a bit different than many of the serv er-based workloads the\nsystems research community usually focuses upon. Also a good recent re ference which points to a lot of\nrelated work.\n[H+88] “Scale and Performance in a Distributed File System” by John H. H oward, Michael\nL. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Rober t N. Sidebotham,\nMichael J. West. ACM Transactions on Computing Systems (ACM TOCS), Volum e 6:1, Febru-\nary 1988. The long journal version of the famous AFS system, still in use in a number of places\nthroughout the world, and also probably the earliest clear thinking on how to bui ld distributed ﬁle\nsystems. A wonderful combination of the science of measurement and princ ipled engineering.\n[R+00] “A Comparison of File System Workloads” by Drew Rosell i, Jacob R. Lorch, Thomas E.\nAnderson. USENIX ’00, San Diego, California, June 2000. A more recent set of traces as compared\nto the Baker paper [B+91], with some interesting twists.\n[S+85] “The ITC Distributed File System: Principles and Design” by M. Sa tyanarayanan, J.H.\nHoward, D.A. Nichols, R.N. Sidebotham, A. Spector, M.J. West. SOSP ’ 85, Orcas Island, Wash-\nington, December 1985. The older paper about a distributed ﬁle system. Much of the basic design of\nAFS is in place in this older system, but not the improvements for scale. T he name change to “Andrew”\nis an homage to two people both named Andrew, Andrew Carnegie and Andrew Me llon. These two\nrich dudes started the Carnegie Institute of Technology and the Mellon Institute of Industrial Research,\nrespectively, which eventually merged to become what is now known as Carn egie Mellon University.\n[V99] “File system usage in Windows NT 4.0” by Werner Vogels. SOSP ’99, Kiawah Island\nResort, South Carolina, December 1999. A cool study of Windows workloads, which are inherently\ndifferent than many of the UNIX-based studies that had previously been done.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n14 THEANDREW FILESYSTEM (AFS)\nHomework (Simulation)\nThis section introduces afs.py , a simple AFS simulator you can use\nto shore up your knowledge of how the Andrew File System works. Read\nthe README ﬁle for more details.\nQuestions\n1. Run a few simple cases to make sure you can predict what values wil l be\nread by clients. Vary the random seed ﬂag ( -s) and see if you can trace\nthrough and predict both intermediate values as well as the ﬁnal values\nstored in the ﬁles. Also vary the number of ﬁles ( -f), the number of clients\n(-C), and the read ratio ( -r, from between 0 to 1) to make it a bit more\nchallenging. You might also want to generate slightly longer traces to make\nfor more interesting interactions, e.g., ( -n 2 or higher).\n2. Now do the same thing and see if you can predict each callback that the AFS\nserver initiates. Try different random seeds, and make sure to use a high\nlevel of detailed feedback (e.g., -d 3 ) to see when callbacks occur when\nyou have the program compute the answers for you (with -c). Can you\nguess exactly when each callback occurs? What is the precise con dition for\none to take place?\n3. Similar to above, run with some different random seeds and see if you can\npredict the exact cache state at each step. Cache state can be o bserved by\nrunning with -cand-d 7 .\n4. Now let’s construct some speciﬁc workloads. Run the simulation with-A\noa1:w1:c1,oa1:r1:c1 ﬂag. What are different possible values observed\nby client 1 when it reads the ﬁle a, when running with the random sched-\nuler? (try different random seeds to see different outcomes)? O f all the\npossible schedule interleavings of the two clients’ operati ons, how many of\nthem lead to client 1 reading the value 1, and how many reading th e value\n0?\n5. Now let’s construct some speciﬁc schedules. When running with the-A\noa1:w1:c1,oa1:r1:c1 ﬂag, also run with the following schedules: -S\n01,-S 100011 ,-S 011100 , and others of which you can think. What\nvalue will client 1 read?\n6. Now run with this workload: -A oa1:w1:c1,oa1:w1:c1 , and vary the\nschedules as above. What happens when you run with -S 011100 ? What\nabout when you run with -S 010011 ? What is important in determining\nthe ﬁnal value of the ﬁle?\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",36079
57-51. Summary Dialogue on Distribution.pdf,57-51. Summary Dialogue on Distribution,"51\nSummary Dialogue on Distribution\nStudent: Well, that was quick. Too quick, in my opinion!\nProfessor: Yes, distributed systems are complicated and cool and well worth\nyour study; just not in this book (or course).\nStudent: That’s too bad; I wanted to learn more! But I did learn a few things.\nProfessor: Like what?\nStudent: Well, everything can fail.\nProfessor: Good start.\nStudent: But by having lots of these things (whether disks, machines, or wha t-\never), you can hide much of the failure that arises.\nProfessor: Keep going!\nStudent: Some basic techniques like retrying are really useful.\nProfessor: That’s true.\nStudent: And you have to think carefully about protocols: the exact bits that\nare exchanged between machines. Protocols can affect everything , including how\nsystems respond to failure and how scalable they are.\nProfessor: You really are getting better at this learning stuff.\nStudent: Thanks! And you’re not a bad teacher yourself!\nProfessor: Well thank you very much too.\nStudent: So is this the end of the book?\nProfessor: I’m not sure. They don’t tell me anything.\nStudent: Me neither. Let’s get out of here.\nProfessor: OK.\nStudent: Go ahead.\nProfessor: No, after you.\nStudent: Please, professors ﬁrst.\n1\n2 SUMMARY DIALOGUE ON DISTRIBUTION\nProfessor: No, please, after you.\nStudent: (exasperated) Fine!\nProfessor: (waiting) ... so why haven’t you left?\nStudent: I don’t know how. Turns out, the only thing I can do is participate in\nthese dialogues.\nProfessor: Me too. And now you’ve learned our ﬁnal lesson...\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",1620
58-Appendices.pdf,58-Appendices,,0
59-Appendix B Virtual Machine Monitors.pdf,59-Appendix B Virtual Machine Monitors,"A\nA Dialogue on Virtual Machine Monitors\nStudent: So now we’re stuck in the Appendix, huh?\nProfessor: Yes, just when you thought things couldn’t get any worse.\nStudent: Well, what are we going to talk about?\nProfessor: An old topic that has been reborn: virtual machine monitors , also\nknown as hypervisors .\nStudent: Oh, like VMware? That’s cool; I’ve used that kind of software before.\nProfessor: Cool indeed. We’ll learn how VMMs add yet another layer of virtu-\nalization into systems, this one beneath the OS itself! Crazy and amaz ing stuff,\nreally.\nStudent: Sounds neat. Why not include this in the earlier part of the book, the n,\non virtualization? Shouldn’t it really go there?\nProfessor: That’s above our pay grade, I’m afraid. But my guess is this: there\nis already a lot of material there. By moving this small aside on VMMs int o\nthe appendix, a particular instructor can choose whether to includ e it or skip it.\nBut I do think it should be included, because if you can understand ho w VMMs\nwork, then you really understand virtualization quite well.\nStudent: Alright then, let’s get to work!\n1\nB\nVirtual Machine Monitors\nB.1 Introduction\nYears ago, IBM sold expensive mainframes to large organizations , and\na problem arose: what if the organization wanted to run different oper-\nating systems on the machine at the same time? Some applications h ad\nbeen developed on one OS, and some on others, and thus the problem.\nAs a solution, IBM introduced yet another level of indirection in th e form\nof a virtual machine monitor (VMM ) (also called a hypervisor ) [G74].\nSpeciﬁcally, the monitor sits between one or more operating systems\nand the hardware and gives the illusion to each running OS that it con-\ntrols the machine. Behind the scenes, however, the monitor actua lly is\nin control of the hardware, and must multiplex running OSes across the\nphysical resources of the machine. Indeed, the VMM serves as an operat-\ning system for operating systems, but at a much lower level; the O S must\nstill think it is interacting with the physical hardware. Th us,transparency\nis a major goal of VMMs.\nThus, we ﬁnd ourselves in a funny position: the OS has thus far ser ved\nas the master illusionist, tricking unsuspecting applicati ons into thinking\nthey have their own private CPU and a large virtual memory, whil e se-\ncretly switching between applications and sharing memory as w ell. Now,\nwe have to do it again, but this time underneath the OS, who is us ed to\nbeing in charge. How can the VMM create this illusion for each OS r un-\nning on top of it?\nTHECRUX:\nHOW TO VIRTUALIZE THE MACHINE UNDERNEATH THE OS\nThe virtual machine monitor must transparently virtualize th e ma-\nchine underneath the OS; what are the techniques required to d o so?\n1\n2 VIRTUAL MACHINE MONITORS\nB.2 Motivation: Why VMMs?\nToday, VMMs have become popular again for a multitude of reasons.\nServer consolidation is one such reason. In many settings, people ru n\nservices on different machines which run different operating systems (or\neven OS versions), and yet each machine is lightly utilized. I n this case,\nvirtualization enables an administrator to consolidate multiple OSes onto\nfewer hardware platforms, and thus lower costs and ease adminis tration.\nVirtualization has also become popular on desktops, as many users\nwish to run one operating system (say Linux or Mac OS X) but still h ave\naccess to native applications on a different platform (say Wind ows). This\ntype of improvement in functionality is also a good reason.\nAnother reason is testing and debugging. While developers writ e code\non one main platform, they often want to debug and test it on the many\ndifferent platforms that they deploy the software to in the ﬁeld . Thus,\nvirtualization makes it easy to do so, by enabling a developer to run many\noperating system types and versions on just one machine.\nThis resurgence in virtualization began in earnest the mid-t o-late 1990’s,\nand was led by a group of researchers at Stanford headed by Professor\nMendel Rosenblum. His group’s work on Disco [B+97], a virtual mach ine\nmonitor for the MIPS processor, was an early effort that revived VMM s\nand eventually led that group to the founding of VMware [V98], now a\nmarket leader in virtualization technology. In this chapter, w e will dis-\ncuss the primary technology underlying Disco and through that w indow\ntry to understand how virtualization works.\nB.3 Virtualizing the CPU\nTo run a virtual machine (e.g., an OS and its applications) on top of a\nvirtual machine monitor, the basic technique that is used is limited direct\nexecution , a technique we saw before when discussing how the OS vir-\ntualizes the CPU. Thus, when we wish to “boot” a new OS on top of the\nVMM, we simply jump to the address of the ﬁrst instruction and le t the\nOS begin running. It is as simple as that (well, almost).\nAssume we are running on a single processor, and that we wish to\nmultiplex between two virtual machines, that is, between tw o OSes and\ntheir respective applications. In a manner quite similar to a n operating\nsystem switching between running processes (a context switch ), a virtual\nmachine monitor must perform a machine switch between running vir-\ntual machines. Thus, when performing such a switch, the VMM mu st\nsave the entire machine state of one OS (including registers, P C, and un-\nlike in a context switch, any privileged hardware state), res tore the ma-\nchine state of the to-be-run VM, and then jump to the PC of the to-be -run\nVM and thus complete the switch. Note that the to-be-run VM’s PC ma y\nbe within the OS itself (i.e., the system was executing a syst em call) or it\nmay simply be within a process that is running on that OS (i.e., a user-\nmode application).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nVIRTUAL MACHINE MONITORS 3\nWe get into some slightly trickier issues when a running appli cation\nor OS tries to perform some kind of privileged operation . For example,\non a system with a software-managed TLB, the OS will use special priv-\nileged instructions to update the TLB with a translation before restarting\nan instruction that suffered a TLB miss. In a virtualized envi ronment, the\nOS cannot be allowed to perform privileged instructions, becaus e then it\ncontrols the machine rather than the VMM beneath it. Thus, the V MM\nmust somehow intercept attempts to perform privileged operation s and\nthus retain control of the machine.\nA simple example of how a VMM must interpose on certain operations\narises when a running process on a given OS tries to make a system call.\nFor example, the process may be trying to call open() on a ﬁle, or may be\ncallingread() to get data from it, or may be calling fork() to create a\nnew process. In a system without virtualization, a system call i s achieved\nwith a special instruction; on MIPS, it is a trap instruction, and on x86, it\nis theint (an interrupt) instruction with the argument 0x80 . Here is the\nopen library call on FreeBSD [B00] (recall that your C code ﬁrst mak es a\nlibrary call into the C library, which then executes the prope r assembly\nsequence to actually issue the trap instruction and make a sys tem call):\nopen:\npush dword mode\npush dword flags\npush dword path\nmov eax, 5\npush eax\nint 80h\nOn U NIX-based systems, open() takes just three arguments: int\nopen(char *path, int flags, mode t mode) . You can see in the\ncode above how the open() library call is implemented: ﬁrst, the ar-\nguments get pushed onto the stack ( mode, flags, path ), then a 5\ngets pushed onto the stack, and then int 80h is called, which trans-\nfers control to the kernel. The 5, if you were wondering, is the pre -agreed\nupon convention between user-mode applications and the kernel for the\nopen() system call in FreeBSD; different system calls would place dif fer-\nent numbers onto the stack (in the same position) before calling t he trap\ninstruction int and thus making the system call1.\nWhen a trap instruction is executed, as we’ve discussed before, it usu-\nally does a number of interesting things. Most important in our exa mple\nhere is that it ﬁrst transfers control (i.e., changes the PC) to a well-deﬁned\ntrap handler within the operating system. The OS, when it is ﬁrst start-\ning up, establishes the address of such a routine with the hardw are (also\na privileged operation) and thus upon subsequent traps, the har dware\n1Just to make things confusing, the Intel folks use the term “interrupt” for w hat almost\nany sane person would call a trap instruction. As Patterson said abou t the Intel instruction\nset: “It’s an ISA only a mother could love.” But actually, we kind of like it, and we’re not its\nmother.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 VIRTUAL MACHINE MONITORS\nProcess Hardware Operating System\n1.Execute instructions\n(add, load, etc.)\n2.System call:\nTrap to OS\n3.Switch to kernel mode;\nJump to trap handler\n4.In kernel mode;\nHandle system call;\nReturn from trap\n5.Switch to user mode;\nReturn to user code\n6.Resume execution\n(@PC after trap)\nTable B.1: Executing a System Call\nknows where to start running code to handle the trap. At the same time\nof the trap, the hardware also does one other crucial thing: it cha nges the\nmode of the processor from user mode tokernel mode . In user mode, op-\nerations are restricted, and attempts to perform privileged op erations will\nlead to a trap and likely the termination of the offending process ; in ker-\nnel mode, on the other hand, the full power of the machine is availab le,\nand thus all privileged operations can be executed. Thus, in a t raditional\nsetting (again, without virtualization), the ﬂow of control would b e like\nwhat you see in Table B.1.\nOn a virtualized platform, things are a little more interestin g. When an\napplication running on an OS wishes to perform a system call, it d oes the\nexact same thing: executes a trap instruction with the argume nts carefully\nplaced on the stack (or in registers). However, it is the VMM that controls\nthe machine, and thus the VMM who has installed a trap handler that\nwill ﬁrst get executed in kernel mode.\nSo what should the VMM do to handle this system call? The VMM\ndoesn’t really know how to handle the call; after all, it does not know\nthe details of each OS that is running and therefore does not know wh at\neach call should do. What the VMM does know, however, is where the\nOS’s trap handler is. It knows this because when the OS booted up, it\ntried to install its own trap handlers; when the OS did so, it was trying\nto do something privileged, and therefore trapped into the VMM ; at that\ntime, the VMM recorded the necessary information (i.e., where t his OS’s\ntrap handlers are in memory). Now, when the VMM receives a trap f rom\na user process running on the given OS, it knows exactly what to do: i t\njumps to the OS’s trap handler and lets the OS handle the system c all as\nit should. When the OS is ﬁnished, it executes some kind of privil eged\ninstruction to return from the trap ( rett on MIPS, iret on x86), which\nagain bounces into the VMM, which then realizes that the OS is t rying to\nreturn from the trap and thus performs a real return-from-trap a nd thus\nreturns control to the user and puts the machine back in user mode . The\nentire process is depicted in Tables B.2 and B.3, both for the norm al case\nwithout virtualization and the case with virtualization (we le ave out the\nexact hardware operations from above to save space).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nVIRTUAL MACHINE MONITORS 5\nProcess Operating System\n1.System call:\nTrap to OS\n2.OS trap handler:\nDecode trap and execute\nappropriate syscall routine;\nWhen done: return from trap\n3.Resume execution\n(@PC after trap)\nTable B.2: System Call Flow Without Virtualization\nProcess Operating System VMM\n1.System call:\nTrap to OS\n2.Process trapped:\nCall OS trap handler\n(at reduced privilege)\n3.OS trap handler:\nDecode trap and\nexecute syscall;\nWhen done: issue\nreturn-from-trap\n4.OS tried return from trap:\nDo real return from trap\n5.Resume execution\n(@PC after trap)\nTable B.3: System Call Flow with Virtualization\nAs you can see from the ﬁgures, a lot more has to take place when\nvirtualization is going on. Certainly, because of the extra jump ing around,\nvirtualization might indeed slow down system calls and thus coul d hurt\nperformance.\nYou might also notice that we have one remaining question: what\nmode should the OS run in? It can’t run in kernel mode, because then\nit would have unrestricted access to the hardware. Thus, it mu st run in\nsome less privileged mode than before, be able to access its own da ta\nstructures, and simultaneously prevent access to its data st ructures from\nuser processes.\nIn the Disco work, Rosenblum and colleagues handled this problem\nquite neatly by taking advantage of a special mode provided by th e MIPS\nhardware known as supervisor mode. When running in this mode, one\nstill doesn’t have access to privileged instructions, but one ca n access a\nlittle more memory than when in user mode; the OS can use this extr a\nmemory for its data structures and all is well. On hardware that doesn’t\nhave such a mode, one has to run the OS in user mode and use memory\nprotection (page tables and TLBs) to protect OS data structures appro-\npriately. In other words, when switching into the OS, the monitor w ould\nhave to make the memory of the OS data structures available to th e OS via\npage-table protections; when switching back to the running ap plication,\nthe ability to read and write the kernel would have to be removed .\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 VIRTUAL MACHINE MONITORS\nVirtual Address Space ""Physical Memory"" Machine Memory\n0\n1\n2\n3OS Page Table\nVPN 0 to PFN 10\nVPN 2 to PFN 03\nVPN 3 to PFN 08\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15VMM Page Table\nPFN 03 to MFN 06\nPFN 08 to MFN 10\nPFN 10 to MFN 05\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nFigure B.1: VMM Memory Virtualization\nB.4 Virtualizing Memory\nYou should now have a basic idea of how the processor is virtualized:\nthe VMM acts like an OS and schedules different virtual machi nes to run,\nand some interesting interactions occur when privilege levels change. But\nwe have left out a big part of the equation: how does the VMM virtuali ze\nmemory?\nEach OS normally thinks of physical memory as a linear array of pag es,\nand assigns each page to itself or user processes. The OS itself , of course,\nalready virtualizes memory for its running processes, such tha t each pro-\ncess has the illusion of its own private address space. Now we must add\nanother layer of virtualization, so that multiple OSes can share the actual\nphysical memory of the machine, and we must do so transparently .\nThis extra layer of virtualization makes “physical” memory a vi rtual-\nization on top of what the VMM refers to as machine memory , which is\nthe real physical memory of the system. Thus, we now have an addit ional\nlayer of indirection: each OS maps virtual-to-physical addres ses via its\nper-process page tables; the VMM maps the resulting physical mappings\nto underlying machine addresses via its per-OS page tables. Figure B.1\ndepicts this extra level of indirection.\nIn the ﬁgure, there is just a single virtual address space wit h four\npages, three of which are valid (0, 2, and 3). The OS uses its pag e ta-\nble to map these pages to three underlying physical frames (1 0, 3, and\n8, respectively). Underneath the OS, the VMM performs a furthe r level\nof indirection, mapping PFNs 3, 8, and 10 to machine frames 6, 10 , and\n5 respectively. Of course, this picture simpliﬁes things qui te a bit; on a\nreal system, there would be Voperating systems running (with Vlikely\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nVIRTUAL MACHINE MONITORS 7\nProcess Operating System\n1.Load from memory:\nTLB miss: Trap\n2.OS TLB miss handler:\nExtract VPN from VA;\nDo page table lookup;\nIf present and valid:\nget PFN, update TLB;\nReturn from trap\n3.Resume execution\n(@PC of trapping instruction);\nInstruction is retried;\nResults in TLB hit\nTable B.4: TLB Miss Flow without Virtualization\ngreater than one), and thus VVMM page tables; further, on top of each\nrunning operating system OS i, there would be a number of processes Pi\nrunning ( Pilikely in the tens or hundreds), and hence Pi(per-process)\npage tables within OS i.\nTo understand how this works a little better, let’s recall how address\ntranslation works in a modern paged system. Speciﬁcally, let’s discuss\nwhat happens on a system with a software-managed TLB during add ress\ntranslation. Assume a user process generates an address (for an instruc-\ntion fetch or an explicit load or store); by deﬁnition, the process ge nerates\navirtual address , as its address space has been virtualized by the OS. As\nyou know by now, it is the role of the OS, with help from the hardware,\nto turn this into a physical address and thus be able to fetch the desired\ncontents from physical memory.\nAssume we have a 32-bit virtual address space and a 4-KB page s ize.\nThus, our 32-bit address is chopped into two parts: a 20-bit vir tual page\nnumber (VPN), and a 12-bit offset. The role of the OS, with help from the\nhardware TLB, is to translate the VPN into a valid physical pa ge frame\nnumber (PFN) and thus produce a fully-formed physical address which\ncan be sent to physical memory to fetch the proper data. In the com mon\ncase, we expect the TLB to handle the translation in hardware, thus mak-\ning the translation fast. When a TLB miss occurs (at least, on a sy stem\nwith a software-managed TLB), the OS must get involved to servi ce the\nmiss, as depicted here in Table B.4.\nAs you can see, a TLB miss causes a trap into the OS, which handles\nthe fault by looking up the VPN in the page table and installing t he trans-\nlation in the TLB.\nWith a virtual machine monitor underneath the OS, however, thing s\nagain get a little more interesting. Let’s examine the ﬂow of a TLB miss\nagain (see Table B.5 for a summary). When a process makes a virtu al\nmemory reference and misses in the TLB, it is not the OS TLB miss h an-\ndler that runs; rather, it is the VMM TLB miss handler, as the V MM is\nthe true privileged owner of the machine. However, in the normal c ase,\nthe VMM TLB handler doesn’t know how to handle the TLB miss, so it\nimmediately jumps into the OS TLB miss handler; the VMM knows t he\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 VIRTUAL MACHINE MONITORS\nProcess Operating System Virtual Machine Monitor\n1.Load from memory\nTLB miss: Trap\n2.VMM TLB miss handler:\nCall into OS TLB handler\n(reducing privilege)\n3. OS TLB miss handler:\nExtract VPN from VA;\nDo page table lookup;\nIf present and valid,\nget PFN, update TLB\n4.Trap handler:\nUnprivileged code trying to\nupdate the TLB;\nOS is trying to install\nVPN-to-PFN mapping;\nUpdate TLB instead with\nVPN-to-MFN (privileged);\nJump back to OS\n(reducing privilege)\n5.Return from trap\n6.Trap handler:\nUnprivileged code trying\nto return from a trap;\nReturn from trap\n7.Resume execution\n(@PC of instruction);\nInstruction is retried;\nResults in TLB hit\nTable B.5: TLB Miss Flow with Virtualization\nlocation of this handler because the OS, during “boot”, tried to ins tall its\nown trap handlers. The OS TLB miss handler then runs, does a page ta-\nble lookup for the VPN in question, and tries to install the VPN-to- PFN\nmapping in the TLB. However, doing so is a privileged operation, a nd\nthus causes another trap into the VMM (the VMM gets notiﬁed when any\nnon-privileged code tries to do something that is privileged, of course).\nAt this point, the VMM plays its trick: instead of installing th e OS’s VPN-\nto-PFN mapping, the VMM installs its desired VPN-to-MFN mappi ng.\nAfter doing so, the system eventually gets back to the user-lev el code,\nwhich retries the instruction, and results in a TLB hit, fetch ing the data\nfrom the machine frame where the data resides.\nThis set of actions also hints at how a VMM must manage the virtu-\nalization of physical memory for each running OS; just like the OS h as a\npage table for each process, the VMM must track the physical-to- machine\nmappings for each virtual machine it is running. These per-ma chine page\ntables need to be consulted in the VMM TLB miss handler in order t o de-\ntermine which machine page a particular “physical” page map s to, and\neven, for example, if it is present in machine memory at the curr ent time\n(i.e., the VMM could have swapped it to disk).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nVIRTUAL MACHINE MONITORS 9\nASIDE : HYPERVISORS ANDHARDWARE -MANAGED TLB S\nOur discussion has centered around software-managed TLBs and t he\nwork that needs to be done when a miss occurs. But you might be\nwondering: how does the virtual machine monitor get involved with a\nhardware-managed TLB? In those systems, the hardware walks t he page\ntable on each TLB miss and updates the TLB as need be, and thus th e\nVMM doesn’t have a chance to run on each TLB miss to sneak its trans -\nlation into the system. Instead, the VMM must closely monitor cha nges\nthe OS makes to each page table (which, in a hardware-managed sys-\ntem, is pointed to by a page-table base register of some kind), an d keep a\nshadow page table that instead maps the virtual addresses of each pro-\ncess to the VMM’s desired machine pages [AA06]. The VMM instal ls a\nprocess’s shadow page table whenever the OS tries to install the process’s\nOS-level page table, and thus the hardware chugs along, transl ating vir-\ntual addresses to machine addresses using the shadow table, w ithout the\nOS even noticing.\nFinally, as you might notice from this sequence of operations, TLB\nmisses on a virtualized system become quite a bit more expensive than\nin a non-virtualized system. To reduce this cost, the designer s of Disco\nadded a VMM-level “software TLB”. The idea behind this data st ructure\nis simple. The VMM records every virtual-to-physical mapping that it\nsees the OS try to install; then, on a TLB miss, the VMM ﬁrst consu lts\nits software TLB to see if it has seen this virtual-to-physical mapping be-\nfore, and what the VMM’s desired virtual-to-machine mapping sh ould\nbe. If the VMM ﬁnds the translation in its software TLB, it simpl y installs\nthe virtual-to-machine mapping directly into the hardware T LB, and thus\nskips all the back and forth in the control ﬂow above [B+97].\nB.5 The Information Gap\nJust like the OS doesn’t know too much about what application pro-\ngrams really want, and thus must often make general policies th at hope-\nfully work for all programs, the VMM often doesn’t know too much about\nwhat the OS is doing or wanting; this lack of knowledge, sometimes\ncalled the information gap between the VMM and the OS, can lead to\nvarious inefﬁciencies [B+97]. For example, an OS, when it has not hing\nelse to run, will sometimes go into an idle loop just spinning and waiting\nfor the next interrupt to occur:\nwhile (1)\n; // the idle loop\nIt makes sense to spin like this if the OS in charge of the entire machine\nand thus knows there is nothing else that needs to run. However, w hen a\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 VIRTUAL MACHINE MONITORS\nASIDE : PARA -VIRTUALIZATION\nIn many situations, it is good to assume that the OS cannot be modiﬁe d in\norder to work better with virtual machine monitors (for example, b ecause\nyou are running your VMM under an unfriendly competitor’s operatin g\nsystem). However, this is not always the case, and when the OS ca n be\nmodiﬁed (as we saw in the example with demand-zeroing of pages), it\nmay run more efﬁciently on top of a VMM. Running a modiﬁed OS to\nrun on a VMM is generally called para-virtualization [WSG02], as the\nvirtualization provided by the VMM isn’t a complete one, but rathe r a\npartial one requiring OS changes to operate effectively. Rese arch shows\nthat a properly-designed para-virtualized system, with jus t the right OS\nchanges, can be made to be nearly as efﬁcient a system without a VMM\n[BD+03].\nVMM is running underneath two different OSes, one in the idle loop and\none usefully running user processes, it would be useful for the VM M to\nknow that one OS is idle so it can give more CPU time to the OS doing\nuseful work.\nAnother example arises with demand zeroing of pages. Most oper-\nating systems zero a physical frame before mapping it into a pr ocess’s\naddress space. The reason for doing so is simple: security. If th e OS\ngave one process a page that another had been using without zeroing it,\nan information leak across processes could occur, thus potentially leak-\ning sensitive information. Unfortunately, the VMM must zero pa ges that\nit gives to each OS, for the same reason, and thus many times a page will\nbe zeroed twice, once by the VMM when assigning it to an OS, and once\nby the OS when assigning it to a process. The authors of Disco had n o\ngreat solution to this problem: they simply changed the OS (IRIX ) to not\nzero pages that it knew had been zeroed by the underlying VMM [B +97].\nThere are many other similar problems to these described here. One\nsolution is for the VMM to use inference (a form of implicit information )\nto overcome the problem. For example, a VMM can detect the idle loop b y\nnoticing that the OS switched to low-power mode. A different appr oach,\nseen in para-virtualized systems, requires the OS to be changed. This\nmore explicit approach, while harder to deploy, can be quite eff ective.\nB.6 Summary\nVirtualization is in a renaissance. For a multitude of reasons, u sers\nand administrators want to run multiple OSes on the same machine at\nthe same time. The key is that VMMs generally provide this serv icetrans-\nparently ; the OS above has little clue that it is not actually controlling t he\nhardware of the machine. The key method that VMMs use to do so is\nto extend the notion of limited direct execution; by setting up th e hard-\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nVIRTUAL MACHINE MONITORS 11\nTIP: USEIMPLICIT INFORMATION\nImplicit information can be a powerful tool in layered systems whe re\nit is hard to change the interfaces between systems, but more i nforma-\ntion about a different layer of the system is needed. For example, a\nblock-based disk device might like to know more about how a ﬁle sys-\ntem above it is using it; Similarly, an application might want to know\nwhat pages are currently in the ﬁle-system page cache, but th e OS pro-\nvides no API to access this information. In both these cases, res earchers\nhave developed powerful inferencing techniques to gather the needed in-\nformation implicitly, without requiring an explicit interface between lay-\ners [AD+01,S+03]. Such techniques are quite useful in a virtua l machine\nmonitor, which would like to learn more about the OSes running above i t\nwithout requiring an explicit API between the two layers.\nware to enable the VMM to interpose on key events (such as traps) , the\nVMM can completely control how machine resources are allocated whi le\npreserving the illusion that the OS requires.\nYou might have noticed some similarities between what the OS does\nfor processes and what the VMM does for OSes. They both virtualize\nthe hardware after all, and hence do some of the same things. Howe ver,\nthere is one key difference: with the OS virtualization, a numb er of new\nabstractions and nice interfaces are provided; with VMM-leve l virtual-\nization, the abstraction is identical to the hardware (and thu s not very\nnice). While both the OS and VMM virtualize hardware, they do s o by\nproviding completely different interfaces; VMMs, unlike the OS, are not\nparticularly meant to make the hardware easier to use.\nThere are many other topics to study if you wish to learn more about\nvirtualization. For example, we didn’t even discuss what happe ns with\nI/O, a topic that has its own new and interesting issues when it c omes to\nvirtualized platforms. We also didn’t discuss how virtualizat ion works\nwhen running “on the side” with your OS in what is sometimes calle d a\n“hosted” conﬁguration. Read more about both of these topics if you’re in -\nterested [SVL01]. We also didn’t discuss what happens when a col lection\nof operating systems running on a VMM uses too much memory.\nFinally, hardware support has changed how platforms support vir tu-\nalization. Companies like Intel and AMD now include direct supp ort for\nan extra level of virtualization, thus obviating many of the softw are tech-\nniques in this chapter. Perhaps, in a chapter yet-to-be-writ ten, we will\ndiscuss these mechanisms in more detail.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 VIRTUAL MACHINE MONITORS\nReferences\n[AA06] “A Comparison of Software and Hardware Techniques\nfor x86 Virtualization”\nKeith Adams and Ole Agesen\nASPLOS ’06, San Jose, California\nA terriﬁc paper from two VMware engineers about the surprisingly small beneﬁts of having hardware\nsupport for virtualization. Also an excellent general discussion about vi rtualization in VMware, includ-\ning the crazy binary-translation tricks they have to play in order to virtualize the di fﬁcult-to-virtualize\nx86 platform.\n[AD+01] “Information and Control in Gray-box Systems”\nAndrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau\nSOSP ’01, Banff, Canada\nOur own work on how to infer information and even exert control over the OS from app lication level,\nwithout any change to the OS. The best example therein: determining whi ch ﬁle blocks are cached in the\nOS using a probabilistic probe-based technique; doing so allows applicati ons to better utilize the cache,\nby ﬁrst scheduling work that will result in hits.\n[B00] “FreeBSD Developers’ Handbook:\nChapter 11 x86 Assembly Language Programming”\nhttp://www.freebsd.org/doc/en/books/developers-handbook/\nA nice tutorial on system calls and such in the BSD developers handbook.\n[BD+03] “Xen and the Art of Virtualization”\nPaul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harri s, Alex Ho, Rolf Neuge-\nbauer, Ian Pratt, Andrew Warﬁeld\nSOSP ’03, Bolton Landing, New York\nThe paper that shows that with para-virtualized systems, the overheads of virtuali zed systems can be\nmade to be incredibly low. So successful was this paper on the Xen virtual machine monitor that it\nlaunched a company.\n[B+97] “Disco: Running Commodity Operating Systems\non Scalable Multiprocessors”\nEdouard Bugnion, Scott Devine, Kinshuk Govil, Mendel Rosenblum\nSOSP ’97\nThe paper that reintroduced the systems community to virtual machine resear ch; well, perhaps this is\nunfair as Bressoud and Schneider [BS95] also did, but here we began to u nderstand why virtualization\nwas going to come back. What made it even clearer, however, is when this grou p of excellent researchers\nstarted VMware and made some billions of dollars.\n[BS95] “Hypervisor-based Fault-tolerance”\nThomas C. Bressoud, Fred B. Schneider\nSOSP ’95\nOne the earliest papers to bring back the hypervisor , which is just another term for a virtual machine\nmonitor. In this work, however, such hypervisors are used to improve system tolerance of hardware\nfaults, which is perhaps less useful than some of the more practical scenari os discussed in this chapter;\nhowever, still quite an intriguing paper in its own right.\n[G74] “Survey of Virtual Machine Research”\nR.P . Goldberg\nIEEE Computer, Volume 7, Number 6\nA terriﬁc survey of a lot of old virtual machine research.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nVIRTUAL MACHINE MONITORS 13\n[SVL01] “Virtualizing I/O Devices on VMware Workstation’s\nHosted Virtual Machine Monitor”\nJeremy Sugerman, Ganesh Venkitachalam and Beng-Hong Lim\nUSENIX ’01, Boston, Massachusetts\nProvides a good overview of how I/O works in VMware using a hosted architecture which exploits many\nnative OS features to avoid reimplementing them within the VMM.\n[V98] VMware corporation.\nAvailable: http://www.vmware.com/\nThis may be the most useless reference in this book, as you can clearly look thi s up yourself. Anyhow,\nthe company was founded in 1998 and is a leader in the ﬁeld of virtualization.\n[S+03] “Semantically-Smart Disk Systems”\nMuthian Sivathanu, Vijayan Prabhakaran, Florentina I. Popovici, Timot hy E. Denehy, Andrea\nC. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’03, San Francisco, California, March 2003\nOur work again, this time showing how a dumb block-based device can infer mu ch about what the ﬁle\nsystem above it is doing, such as deleting a ﬁle. The technology used ther ein enables interesting new\nfunctionality within a block device, such as secure delete, or more reli able storage.\n[WSG02] “Scale and Performance in the Denali Isolation Kernel”\nAndrew Whitaker, Marianne Shaw, and Steven D. Gribble\nOSDI ’02, Boston, Massachusetts\nThe paper that introduces the term para-virtualization. Although one can argue that Bu gnion et al.\n[B+97] introduce the idea of para-virtualization in the Disco paper, Whitaker et al . take it further and\nshow how the idea can be more general than what was thought before.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",33626
60-Appendix D Monitors Deprecated.pdf,60-Appendix D Monitors Deprecated,"C\nA Dialogue on Monitors\nProfessor: So it’s you again, huh?\nStudent: I bet you are getting quite tired by now, being so, well you know, old?\nNot that 50 years old is that old, really.\nProfessor: I’m not 50! I’ve just turned 40, actually. But goodness, I guess to\nyou, being 20-something ...\nStudent: ... 19, actually ...\nProfessor: (ugh) ... yes, 19, whatever, I guess 40 and 50 seem kind of similar.\nBut trust me, they’re not. At least, that’s what my 50-year old frie nds tell me.\nStudent: Anyhow ...\nProfessor: Ah yes! What are we talking about again?\nStudent: Monitors. Not that I know what a monitor is, except for some kind of\nold-fashioned name for the computer display sitting in front of me.\nProfessor: Yes, this is a whole different type of thing. It’s an old concurrency\nprimitive, designed as a way to incorporate locking automatically into o bject-\noriented programs.\nStudent: Why not include it in the section on concurrency then?\nProfessor: Well, most of the book is about C programming and the POSIX\nthreads libraries, where there are no monitors, so there’s that. But there are some\nhistorical reasons to at least include the information on the topic, so here it is, I\nguess.\nStudent: Ah, history. That’s for old people, like you, right?\nProfessor: (glares)\nStudent: Oh take it easy. I kid!\nProfessor: I can’t wait until you take the ﬁnal exam...\n1\nD\nMonitors (Deprecated)\nAround the time concurrent programming was becoming a big deal, ob ject-\noriented programming was also gaining ground. Not surprisingly , peo-\nple started to think about ways to merge synchronization into a m ore\nstructured programming environment.\nOne such approach that emerged was the monitor . First described by\nPer Brinch Hansen [BH73] and later reﬁned by Tony Hoare [H74], t he\nidea behind a monitor is quite simple. Consider the following pret end\nmonitor written in C++ notation:\nmonitor class account {\nprivate:\nint balance = 0;\npublic:\nvoid deposit(int amount) {\nbalance = balance + amount;\n}\nvoid withdraw(int amount) {\nbalance = balance - amount;\n}\n};\nFigure D.1: A Pretend Monitor Class\nNote: this is a “pretend” class because C++ does not support moni-\ntors, and hence the monitor keyword does not exist. However, Java does\nsupport monitors, with what are called synchronized methods. Below,\nwe will examine both how to make something quite like a monitor in\nC/C++, as well as how to use Java synchronized methods.\nIn this example, you may notice we have our old friend the account\nand some routines to deposit and withdraw an amount from the balanc e.\nAs you also may notice, these are critical sections ; if they are called by\nmultiple threads concurrently, you have a race condition and the poten-\ntial for an incorrect outcome.\nIn a monitor class, you don’t get into trouble, though, because the\nmonitor guarantees that only one thread can be active within the mon-\nitor at a time . Thus, our above example is a perfectly safe and working\n1\n2 MONITORS (DEPRECATED )\npiece of code; multiple threads can call deposit() or withdraw() and know\nthat mutual exclusion is preserved.\nHow does the monitor do this? Simple: with a lock. Whenever a\nthread tries to call a monitor routine, it implicitly tries to ac quire the mon-\nitor lock. If it succeeds, then it will be able to call into the rou tine and run\nthe method’s code. If it does not, it will block until the thread that is in\nthe monitor ﬁnishes what it is doing. Thus, if we wrote a C++ class t hat\nlooked like the following, it would accomplish the exact same goal as the\nmonitor class above:\nclass account {\nprivate:\nint balance = 0;\npthread_mutex_t monitor;\npublic:\nvoid deposit(int amount) {\npthread_mutex_lock(&monitor);\nbalance = balance + amount;\npthread_mutex_unlock(&monitor);\n}\nvoid withdraw(int amount) {\npthread_mutex_lock(&monitor);\nbalance = balance - amount;\npthread_mutex_unlock(&monitor);\n}\n};\nFigure D.2: A C++ Class that acts like a Monitor\nThus, as you can see from this example, the monitor isn’t doing too\nmuch for you automatically. Basically, it is just acquiring a loc k and re-\nleasing it. By doing so, we achieve what the monitor requires: only one\nthread will be active within deposit() or withdraw(), as desir ed.\nD.1 Why Bother with Monitors?\nYou might wonder why monitors were invented at all, instead of just\nusing explicit locking. At the time, object-oriented programmi ng was\njust coming into fashion. Thus, the idea was to gracefully blen d some\nof the key concepts in concurrent programming with some of the basic\napproaches of object orientation. Nothing more than that.\nD.2 Do We Get More Than Automatic Locking?\nBack to business. As we know from our discussion of semaphores,\njust having locks is not quite enough; for example, to implement t he pro-\nducer/consumer solution, we previously used semaphores to both pu t\nthreads to sleep when waiting for a condition to change (e.g., a p roducer\nwaiting for a buffer to be emptied), as well as to wake up a threa d when\na particular condition has changed (e.g., a consumer signaling that it has\nindeed emptied a buffer).\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMONITORS (DEPRECATED ) 3\nmonitor class BoundedBuffer {\nprivate:\nint buffer[MAX];\nint fill, use;\nint fullEntries = 0;\ncond_t empty;\ncond_t full;\npublic:\nvoid produce(int element) {\nif (fullEntries == MAX) // line P0\nwait(&empty); // line P1\nbuffer[fill] = element; // line P2\nfill = (fill + 1) % MAX; // line P3\nfullEntries++; // line P4\nsignal(&full); // line P5\n}\nint consume() {\nif (fullEntries == 0) // line C0\nwait(&full); // line C1\nint tmp = buffer[use]; // line C2\nuse = (use + 1) % MAX; // line C3\nfullEntries--; // line C4\nsignal(&empty); // line C5\nreturn tmp; // line C6\n}\n}\nFigure D.3: Producer/Consumer with Monitors and Hoare Semantics\nMonitors support such functionality through an explicit construc t known\nas a condition variable . Let’s take a look at the producer/consumer so-\nlution, here written with monitors and condition variables.\nIn this monitor class, we have two routines, produce() and consume ().\nA producer thread would repeatedly call produce() to put data in to the\nbounded buffer, while a consumer() would repeatedly call consum e().\nThe example is a modern paraphrase of Hoare’s solution [H74].\nYou should notice some similarities between this code and the sema phore-\nbased solution in the previous note. One major difference is how cond i-\ntion variables must be used in concert with an explicit state variable ; in\nthis case, the integer fullEntries determines whether a producer or\nconsumer must wait, depending on its state. Semaphores, in contra st,\nhave an internal numeric value which serves this same purpose . Thus,\ncondition variables must be paired with some kind of external sta te value\nin order to achieve the same end.\nThe most important aspect of this code, however, is the use of the\ntwo condition variables, empty and full, and the respective wait() and\nsignal() calls that employ them. These operations do exactly what\nyou might think: wait() blocks the calling thread on a given condi tion;\nsignal() wakes one waiting thread that is waiting on the condition.\nHowever, there are some subtleties in how these calls operate; un der-\nstanding the semantics of these calls is critically important to understand-\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 MONITORS (DEPRECATED )\ning why this code works. In what researchers in operating system s call\nHoare semantics (yes, a somewhat unfortunate name), the signal()\nimmediately wakes one waiting thread and runs it; thus, the mon itor\nlock, which is implicitly held by the running thread, immedia tely is trans-\nferred to the woken thread which then runs until it either block s or ex-\nits the monitor. Note that there may be more than one thread waiting ;\nsignal() only wakes one waiting thread and runs it, while the others\nmust wait for a subsequent signal.\nA simple example will help us understand this code better. Ima gine\nthere are two threads, one a producer and the other a consumer. The con-\nsumer gets to run ﬁrst, and calls consume() , only to ﬁnd that fullEntries\n= 0(C0) , as there is nothing in the buffer yet. Thus, it calls wait(&full)\n(C1) , and waits for a buffer to be ﬁlled. The producer then runs, ﬁnds\nit doesn’t have to wait (P0) , puts an element into the buffer (P2) , in-\ncrements the ﬁll index (P3) and the fullEntries count (P4) , and calls\nsignal(&full)(P5) . In Hoare semantics, the producer does not con-\ntinue running after the signal; rather, the signal immediat ely transfers\ncontrol to the waiting consumer, which returns from wait() (C1) and\nimmediately consumes the element produced by the producer (C2) and\nso on. Only after the consumer returns will the producer get to ru n again\nand return from the produce() routine.\nD.3 Where Theory Meets Practice\nTony Hoare, who wrote the solution above and came up with the ex-\nact semantics for signal() andwait() , was a theoretician. Clearly a\nsmart guy, too; he came up with quicksort after all [H61]. However , the\nsemantics of signaling and waiting, as it turns out, were not ide al for a\nreal implementation. As the old saying goes, in theory, there is n o differ-\nence between theory and practice, but in practice, there is.\nOLDSAYING : THEORY VS . PRACTICE\nThe old saying is “in theory, there is no difference between the ory and\npractice, but in practice, there is.” Of course, only practiti oners tell you\nthis; a theory person could undoubtedly prove that it is not true.\nA few years later, Butler Lampson and David Redell of Xerox PARC\nwere building a concurrent language known as Mesa , and decided to use\nmonitors as their basic concurrency primitive [LR80]. They wer e well-\nknown systems researchers, and they soon found that Hoare semanti cs,\nwhile more amenable to proofs, were hard to realize in a real syst em\n(there are a lot of reasons for this, perhaps too many to go through he re).\nIn particular, to build a working monitor implementation, Lamps on\nand Redell decided to change the meaning of signal() in a subtl e but crit-\nical way. The signal() routine now was just considered a hint [L83]; it\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMONITORS (DEPRECATED ) 5\nwould move a single waiting thread from the blocked state to a runn able\nstate, but it would not run it immediately. Rather, the signali ng thread\nwould retain control until it exited the monitor and was deschedul ed.\nD.4 Oh Oh, A Race\nGiven these new Mesa semantics , let us again reexamine the code\nabove. Imagine again a consumer (consumer 1) who enters the moni-\ntor and ﬁnds the buffer empty and thus waits (C1) . Now the producer\ncomes along and ﬁlls the buffer and signals that a buffer has bee n ﬁlled,\nmoving the waiting consumer from blocked on the full condition varia ble\nto ready. The producer keeps running for a while, and eventuall y gives\nup the CPU.\nBut Houston, we have a problem. Can you see it? Imagine a differ-\nent consumer (consumer 2) now calls into the consume() routine; it will\nﬁnd a full buffer, consume it, and return, setting fullEntrie s to 0 in the\nmeanwhile. Can you see the problem yet? Well, here it comes. Our ol d\nfriend consumer 1 now ﬁnally gets to run, and returns from wait() , ex-\npecting a buffer to be full (C1...) ; unfortunately, this is no longer true,\nas consumer 2 snuck in and consumed the buffer before consumer 1 ha d\na chance to consume it. Thus, the code doesn’t work, because in the t ime\nbetween the signal() by the producer and the return from wait() by con-\nsumer 1, the condition has changed. This timeline illustrates the problem:\nProducer Consumer1 Consumer2\nC0 (fullEntries=0)\nC1 (Consumer 1: blocked)\nP0 (fullEntries=0)\nP2\nP3\nP4 (fullEntries=1)\nP5 (Consumer1: ready)\nC0 (fullEntries=1)\nC2\nC3\nC4 (fullEntries=0)\nC5\nC6\nC2 (using a buffer,\nfullEntries=0!)\nFigure D.4: Why the Code doesn’t work with Hoare Semantics\nFortunately, the switch from Hoare semantics to Mesa semantics re-\nquires only a small change by the programmer to realize a working so-\nlution. Speciﬁcally, when woken, a thread should recheck the condition\nit was waiting on; because signal() is only a hint, it is possible that the\ncondition has changed (even multiple times) and thus may not be i n the\ndesired state when the waiting thread runs. In our example, tw o lines of\ncode must change, lines P0 and C0:\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 MONITORS (DEPRECATED )\npublic:\nvoid produce(int element) {\nwhile (fullEntries == MAX) // line P0 (CHANGED IF->WHILE)\nwait(&empty); // line P1\nbuffer[fill] = element; // line P2\nfill = (fill + 1) % MAX; // line P3\nfullEntries++; // line P4\nsignal(&full); // line P5\n}\nint consume() {\nwhile (fullEntries == 0) // line C0 (CHANGED IF->WHILE)\nwait(&full); // line C1\nint tmp = buffer[use]; // line C2\nuse = (use + 1) % MAX; // line C3\nfullEntries--; // line C4\nsignal(&empty); // line C5\nreturn tmp; // line C6\n}\nFigure D.5: Producer/Consumer with Monitors and Mesa Semantics\nNot too hard after all. Because of the ease of this implementation,\nvirtually any system today that uses condition variables with s ignaling\nand waiting uses Mesa semantics. Thus, if you remember nothing else at\nall from this class, you can just remember: always recheck the condition\nafter being woken! Put in even simpler terms, use while loops and not\nif statements when checking conditions. Note that this is alway s correct,\neven if somehow you are running on a system with Hoare semantics; in\nthat case, you would just needlessly retest the condition an extr a time.\nD.5 Peeking Under The Hood A Bit\nTo understand a bit better why Mesa semantics are easier to im ple-\nment, let’s understand a little more about the implementation of M esa\nmonitors. In their work [LR80], Lampson and Redell describe thre e differ-\nent types of queues that a thread can be a part of at a given time: t heready\nqueue, a monitor lock queue, and a condition variable queue. Note that\na program might have multiple monitor classes and multiple condi tion\nvariable instances; there is a queue per instance of said item s.\nWith a single bounded buffer monitor, we thus have four queues to\nconsider: the ready queue, a single monitor queue, and two condit ion\nvariable queues (one for the full condition and one for the empty). T o\nbetter understand how a thread library manages these queues, what we\nwill do is show how a thread transitions through these queues in th e pro-\nducer/consumer example.\nIn this example, we walk through a case where a consumer might be\nwoken up but ﬁnd that there is nothing to consume. Let us consider t he\nfollowing timeline. On the left are two consumers (Con1 and Con2) a nd a\nproducer (Prod) and which line of code they are executing; on the ri ght is\nthe state of each of the four queues we are following for this example :\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMONITORS (DEPRECATED ) 7\nt | Con1 Con2 Prod | Mon | Empty | Full | FE | Comment\n--------------------------------------------------- ---------------------\n0 C0 0\n1 C1 Con1 0 Con1 waiting on full\n2 <Context switch> Con1 0 switch: Con1 to Prod\n3 P0 Con1 0\n4 P2 Con1 0 Prod doesn’t wait (FE=0)\n5 P3 Con1 0\n6 P4 Con1 1 Prod updates fullEntries\n7 P5 1 Prod signals: Con1 now ready\n8 <Context switch> 1 switch: Prod to Con2\n9 C0 1 switch to Con2\n10 C2 1 Con2 doesn’t wait (FE=1)\n11 C3 1\n12 C4 0 Con2 changes fullEntries\n13 C5 0 Con2 signals empty (no waiter)\n14 C6 0 Con2 done\n15 <Context switch> 0 switch: Con2 to Con1\n16 C0 0 recheck fullEntries: 0!\n17 C1 Con1 0 wait on full again\nFigure D.6: Tracing Queues during a Producer/Consumer Run\nthe ready queue of runnable processes, the monitor lock queue call ed\nMonitor, and the empty and full condition variable queues. We als o track\ntime (t), the thread that is running (square brackets around t he thread on\nthe ready queue that is running), and the value of fullEntries (FE).\nAs you can see from the timeline, consumer 2 (Con2) sneaks in and\nconsumes the available data (t=9..14) before consumer 1 (Con1), who was\nwaiting on the full condition to be signaled (since t=1), gets a c hance to\ndo so. However, Con1 does get woken by the producer’s signal (t=7),\nand thus runs again even though the buffer is empty by the time i t does\nso. If Con1 didn’t recheck the state variable fullEntries (t=16 ), it would\nhave erroneously tried to consume data when no data was present t o\nconsume. Thus, this natural implementation is exactly what le ads us to\nMesa semantics (and not Hoare).\nD.6 Other Uses Of Monitors\nIn their paper on Mesa, Lampson and Redell also point out a few\nplaces where a different kind of signaling is needed. For examp le, con-\nsider the following memory allocator (Figure D.7).\nMany details are left out of this example, in order to allow us to foc us\non the conditions for waking and signaling. It turns out the signal /wait\ncode above does not quite work; can you see why?\nImagine two threads call allocate. The ﬁrst calls allocate(20 ) and the\nsecond allocate(10). No memory is available, and thus both threa ds call\nwait() and block. Some time later, a different thread comes along a nd calls\nfree(p, 15), and thus frees up 15 bytes of memory. It then signal s that it\nhas done so. Unfortunately, it wakes the thread waiting for 20 byt es; that\nthread rechecks the condition, ﬁnds that only 15 bytes are avail able, and\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 MONITORS (DEPRECATED )\nmonitor class allocator {\nint available; // how much memory is available?\ncond_t c;\nvoid*allocate(int size) {\nwhile (size > available)\nwait(&c);\navailable -= size;\n// and then do whatever the allocator should do\n// and return a chunk of memory\n}\nvoid free(void *pointer, int size) {\n// free up some memory\navailable += size;\nsignal(&c);\n}\n};\nFigure D.7: A Simple Memory Allocator\ncalls wait() again. The thread that could have beneﬁted from th e free of\n15 bytes, i.e., the thread that called allocate(10), is not woke n.\nLampson and Redell suggest a simple solution to this problem. In-\nstead of a signal() which wakes a single waiting thread, they e mploy a\nbroadcast() which wakes allwaiting threads. Thus, all threads are woken\nup, and in the example above, the thread waiting for 10 bytes wil l ﬁnd 15\navailable and succeed in its allocation.\nIn Mesa semantics, using a broadcast() is always correct, as all threads\nshould recheck the condition of interest upon waking anyhow. Howeve r,\nit may be a performance problem, and thus should only be used when\nneeded. In this example, a broadcast() might wake hundreds of w aiting\nthreads, only to have one successfully continue while the rest i mmedi-\nately block again; this problem, sometimes known as a thundering herd ,\nis costly, due to all the extra context switches that occur.\nD.7 Using Monitors To Implement Semaphores\nYou can probably see a lot of similarities between monitors and sema phores.\nNot surprisingly, you can use one to implement the other. Here, we show\nhow you might implement a semaphore class using a monitor (Figure\nD.8).\nAs you can see, wait() simply waits for the value of the semaphore t o\nbe greater than 0, and then decrements its value, whereas post () incre-\nments the value and wakes one waiting thread (if there is one). I t’s as\nsimple as that.\nTo use this class as a binary semaphore (i.e., a lock), you just in itialize\nthe semaphore to 1, and then put wait()/post() pairs around crit ical sec-\ntions. And thus we have shown that monitors can be used to implemen t\nsemaphores.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMONITORS (DEPRECATED ) 9\nmonitor class Semaphore {\nint s; // value of the semaphore\nSemaphore(int value) {\ns = value;\n}\nvoid wait() {\nwhile (s <= 0)\nwait();\ns--;\n}\nvoid post() {\ns++;\nsignal();\n}\n};\nFigure D.8: Implementing a Semaphore with a Monitor\nD.8 Monitors in the Real World\nWe already mentioned above that we were using “pretend” monitors ;\nC++ has no such concept. We now show how to make a monitor-like C++\nclass, and how Java uses synchronized methods to achieve a simil ar end.\nA C++ Monitor of Sorts\nHere is the producer/consumer code written in C++ with locks and c on-\ndition variables (Figure D.9). You can see in this code example t hat there\nis little difference between the pretend monitor code and the wor king\nC++ class we have above. Of course, one obvious difference is the ex plicit\nuse of a lock ”monitor”. More subtle is the switch to the POSIX standa rd\npthread condsignal() andpthread condwait() calls. In partic-\nular, notice that when calling pthread condwait() , one also passes\nin the lock that is held at the time of waiting. The lock is needed i nside\npthread condwait() because it must be released when this thread is\nput to sleep and re-acquired before it returns to the caller (t he same be-\nhavior as within a monitor but again with explicit locks).\nA Java Monitor\nInterestingly, the designers of Java decided to use monitors as they thought\nthey were a graceful way to add synchronization primitives int o a lan-\nguage. To use them, you just use add the keyword synchronized to the\nmethod or set of methods that you wish to use as a monitor (here is an\nexample from Sun’s own documentation site [S12a,S12b]):\nThis code does exactly what you think it should: provide a counter\nthat is thread safe. Because only one thread is allowed into the m onitor\nat a time, only one thread can update the value of ”c”, and thus a ra ce\ncondition is averted.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 MONITORS (DEPRECATED )\nclass BoundedBuffer {\nprivate:\nint buffer[MAX];\nint fill, use;\nint fullEntries;\npthread_mutex_t monitor; // monitor lock\npthread_cond_t empty;\npthread_cond_t full;\npublic:\nBoundedBuffer() {\nuse = fill = fullEntries = 0;\n}\nvoid produce(int element) {\npthread_mutex_lock(&monitor);\nwhile (fullEntries == MAX)\npthread_cond_wait(&empty, &monitor);\nbuffer[fill] = element;\nfill = (fill + 1) % MAX;\nfullEntries++;\npthread_cond_signal(&full);\npthread_mutex_unlock(&monitor);\n}\nint consume() {\npthread_mutex_lock(&monitor);\nwhile (fullEntries == 0)\npthread_cond_wait(&full, &monitor);\nint tmp = buffer[use];\nuse = (use + 1) % MAX;\nfullEntries--;\npthread_cond_signal(&empty);\npthread_mutex_unlock(&monitor);\nreturn tmp;\n}\n}\nFigure D.9: C++ Producer/Consumer with a “Monitor”\nJava and the Single Condition Variable\nIn the original version of Java, a condition variable was also supp lied with\neach synchronized class. To use it, you would call either wait() ornotify()\n(sometimes the term notify is used instead of signal, but they me an the\nsame thing). Oddly enough, in this original implementation, th ere was no\nway to have two (or more) condition variables. You may have noticed i n\nthe producer/consumer solution, we always use two: one for signalin g a\nbuffer has been emptied, and another for signaling that a buffe r has been\nﬁlled.\nTo understand the limitations of only providing a single condition\nvariable, let’s imagine the producer/consumer solution with only a sin-\ngle condition variable. Imagine two consumers run ﬁrst, and both get\nstuck waiting. Then, a producer runs, ﬁlls a single buffer, wa kes a single\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMONITORS (DEPRECATED ) 11\npublic class SynchronizedCounter {\nprivate int c = 0;\npublic synchronized void increment() {\nc++;\n}\npublic synchronized void decrement() {\nc--;\n}\npublic synchronized int value() {\nreturn c;\n}\n}\nFigure D.10: A Simple Java Class with Synchronized Methods\nconsumer, and then tries to ﬁll again but ﬁnds the buffer full ( MAX=1).\nThus, we have a producer waiting for an empty buffer, a consumer w ait-\ning for a full buffer, and a consumer who had been waiting about to run\nbecause it has been woken.\nThe consumer then runs and consumes the buffer. When it calls no-\ntify(), though, it wakes a single thread that is waiting on the c ondition.\nBecause there is only a single condition variable, the consumer m ight\nwake the waiting consumer, instead of the waiting producer. Thus, the\nsolution does not work.\nTo remedy this problem, one can again use the broadcast solution. I n\nJava, one calls notifyAll() to wake all waiting threads. In this case, the\nconsumer would wake a producer and a consumer, but the consumer\nwould ﬁnd that fullEntries is equal to 0 and go back to sleep, wh ile the\nproducer would continue. As usual, waking all waiters can lead t o the\nthundering herd problem.\nBecause of this deﬁciency, Java later added an explicit Condit ion class,\nthus allowing for a more efﬁcient solution to this and other similar con-\ncurrency problems.\nD.9 Summary\nWe have seen the introduction of monitors, a structuring concept de -\nveloped by Brinch Hansen and and subsequently Hoare in the earl y sev-\nenties. When running inside the monitor, a thread implicitly h olds a mon-\nitor lock, and thus prevents other threads from entering the monit or, al-\nlowing the ready construction of mutual exclusion.\nWe also have seen the introduction of explicit condition variable s, which\nallow threads to signal() and wait() much like we saw with sema phores\nin the previous note. The semantics of signal() and wait() are cr itical; be-\ncause all modern systems implement Mesa semantics, a recheck of the\ncondition that the thread went to sleep on is required for correct e xecu-\ntion. Thus, signal() is just a hint that something has changed; it is the\nresponsibility of the woken thread to make sure the conditions are right\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 MONITORS (DEPRECATED )\nfor its continued execution.\nFinally, because C++ has no monitor support, we saw how to emulate\nmonitors with explicit pthread locks and condition variables. We also saw\nhow Java supports monitors with its synchronized routines, and some of\nthe limitations of only providing a single condition variable in su ch an\nenvironment.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nMONITORS (DEPRECATED ) 13\nReferences\n[BH73] “Operating System Principles”\nPer Brinch Hansen, Prentice-Hall, 1973\nAvailable: http://portal.acm.org/citation.cfm?id=540365\nOne of the ﬁrst books on operating systems; certainly ahead of its time. Introduced m onitors as a\nconcurrency primitive.\n[H74] “Monitors: An Operating System Structuring Concept”\nC.A.R. Hoare\nCACM, Volume 17:10, pages 549–557, October 1974\nAn early reference to monitors; however, Brinch Hansen probably was the tr ue inventor.\n[H61] “Quicksort: Algorithm 64”\nC.A.R. Hoare\nCACM, Volume 4:7, July 1961\nThe famous quicksort algorithm.\n[LR80] “Experience with Processes and Monitors in Mesa”\nB.W. Lampson and D.R. Redell\nCACM, Volume 23:2, pages 105–117, February 1980\nAn early and important paper highlighting the differences between theor y and practice.\n[L83] “Hints for Computer Systems Design”\nButler Lampson\nACM Operating Systems Review, 15:5, October 1983\nLampson, a famous systems researcher, loved using hints in the design of computer systems. A hint is\nsomething that is often correct but can be wrong; in this use, a signal() is te lling a waiting thread that it\nchanged the condition that the waiter was waiting on, but not to trust that the condition will be in the\ndesired state when the waiting thread wakes up. In this paper about hints for desi gning systems, one of\nLampson’s general hints is that you should use hints. It is not as confusing as it sounds.\n[S12a] “Synchronized Methods”\nSun documentation\nhttp://java.sun.com/docs/books/tutorial/essential/concurrency /syncmeth.html\n[S12b] “Condition Interface”\nSun documentation\nhttp://java.sun.com/j2se/1.5.0/docs/api/java/util/concurre nt/locks/Condition.html\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",28247
61-Appendix E A Dialogue on Labs.pdf,61-Appendix E A Dialogue on Labs,"E\nA Dialogue on Labs\nStudent: Is this our ﬁnal dialogue?\nProfessor: I hope so! You’ve been becoming quite a pain, you know!\nStudent: Yes, I’ve enjoyed our conversations too. What’s up here?\nProfessor: It’s about the projects you should be doing as you learn this materia l;\nyou know, actual programming, where you do some real work inste ad of this\nincessant talking and reading. The real way to learn!\nStudent: Sounds important. Why didn’t you tell me earlier?\nProfessor: Well, hopefully those using this book actually do look at this part\nearlier, all throughout the course. If not, they’re really missing so mething.\nStudent: Seems like it. So what are the projects like?\nProfessor: Well, there are two types of projects. The ﬁrst set are what you migh t\ncallsystems programming projects, done on machines running Linux and in\nthe C programming environment. This type of programming is quite us eful to\nknow, as when you go off into the real world, you very well might have to do\nsome of this type of hacking yourself.\nStudent: What’s the second type of project?\nProfessor: The second type is based inside a real kernel, a cool little teaching\nkernel developed at MIT called xv6. It is a “port” of an old version of UNIX\nto Intel x86, and is quite neat! With these projects, instead of writ ing code that\ninteracts with the kernel (as you do in systems programming), you actually get\nto re-write parts of the kernel itself!\nStudent: Sounds fun! So what should we do in a semester? You know, there are\nonly so many hours in the day, and as you professors seem to forget, we students\ntake four or ﬁve courses, not just yours!\nProfessor: Well, there is a lot of ﬂexibility here. Some classes just do all systems\nprogramming, because it is so practical. Some classes do all xv6 hack ing, because\nit really gets you to see how operating systems work. And some, as y ou may have\nguessed, do a mix, starting with some systems programming, and th en doing xv6\nat the end. It’s really up to the professor of a particular class.\n1\n2 A D IALOGUE ON LABS\nStudent: (sighing) Professors have all the control, it seems...\nProfessor: Oh, hardly! But that little control they do get to exercise is one of\nthe fun parts of the job. Deciding on assignments is important you kno w — and\nnot something any professor takes lightly.\nStudent: Well, that is good to hear. I guess we should see what these projects\nare all about...\nProfessor: OK. And one more thing: if you’re interested in the systems pro-\ngramming part, there is also a little tutorial about the UNIXand C programming\nenvironment.\nStudent: Sounds almost too useful to be true.\nProfessor: Well, take a look. You know, classes are supposed to be about useful\nthings, sometimes!\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",2817
62-Appendix F Laboratory - Tutorial.pdf,62-Appendix F Laboratory - Tutorial,"F\nLaboratory: Tutorial\nThis is a very brief document to familiarize you with the basics of the C\nprogramming environment on U NIXsystems. It is not comprehensive or\nparticularly detailed, but should just give you enough to get you going.\nA couple of general points of advice about programming: if you want\nto become an expert programmer, you need to master more than just t he\nsyntax of a language. Speciﬁcally, you should know your tools ,know\nyour libraries , and know your documentation . The tools that are rel-\nevant to C compilation are gcc,gdb, and maybe ld. There are tons of\nlibrary routines that are also available to you, but fortunatel y a lot of\nfunctionality is included in libc, which is linked with all C programs by\ndefault — all you need to do is include the right header ﬁles. Fi nally,\nknowing how to ﬁnd the library routines you need (e.g., learning t o ﬁnd\nand read man pages) is a skill worth acquiring. We’ll talk about e ach of\nthese in more detail later on.\nLike (almost) everything worth doing in life, becoming an expert in\nthese domains takes time. Spending the time up-front to learn mor e about\nthe tools and environment is deﬁnitely well worth the effort.\nF.1 A Simple C Program\nWe’ll start with a simple C program, perhaps saved in the ﬁle “hw .c”.\nUnlike Java, there is not necessarily a connection between the ﬁ le name\nand the contents of the ﬁle; thus, use your common sense in naming ﬁl es\nin a manner that is appropriate.\nThe ﬁrst line speciﬁes a ﬁle to include, in this case stdio.h , which\n“prototypes” many of the commonly used input/output routines; the\none we are interested in is printf() . When you use the #include di-\nrective, you are telling the C preprocessor ( cpp) to ﬁnd a particular ﬁle\n(e.g.,stdio.h ) and to insert it directly into your code at the spot of the\n#include . By default, cpp will look in the directory /usr/include/\nto try to ﬁnd the ﬁle.\nThe next part speciﬁes the signature of the main() routine, namely\nthat it returns an integer ( int), and will be called with two arguments,\n1\n2 LABORATORY : TUTORIAL\n/*header files go up here */\n/*note that C comments are enclosed within a slash and a star, an d\nmay wrap over lines */\n// if you use gcc, two slashes will work too (and may be preferr ed)\n#include <stdio.h>\n/*main returns an integer */\nint main(int argc, char *argv[]) {\n/*printf is our output function;\nby default, writes to standard out */\n/*printf returns an integer, but we ignore that */\nprintf(""hello, world\n"");\n/*return 0 to indicate all went well */\nreturn(0);\n}\nan integer argc , which is a count of the number of arguments on the com-\nmand line, and an array of pointers to characters ( argv ), each of which\ncontain a word from the command line, and the last of which is null.\nThere will be more on pointers and arrays below.\nThe program then simply prints the string “hello, world” and ad-\nvances the output stream to the next line, courtesy of the backsl ash fol-\nlowed by an “n” at the end of the call to printf() . Afterwards, the pro-\ngram completes by returning a value, which is passed back to th e shell\nthat executed the program. A script or the user at the terminal c ould\ncheck this value (in csh and tcsh shells, it is stored in the status vari-\nable), to see whether the program exited cleanly or with an error .\nF.2 Compilation and Execution\nWe’ll now learn how to compile the program. Note that we will use\ngcc as our example, though on some platforms you may be able to use a\ndifferent (native) compiler, cc.\nAt the shell prompt, you just type:\nprompt> gcc hw.c\ngcc is not really the compiler, but rather the program called a “com-\npiler driver”; thus it coordinates the many steps of the compilat ion. Usu-\nally there are four to ﬁve steps. First, gcc will execute cpp, the C pre-\nprocessor, to process certain directives (such as #define and#include .\nThe program cpp is just a source-to-source translator, so its end-product\nis still just source code (i.e., a C ﬁle). Then the real compilati on will begin,\nusually a command called cc1. This will transform source-level C code\ninto low-level assembly code, speciﬁc to the host machine. The a ssem-\nbleraswill then be executed, generating object code (bits and things that\nmachines can really understand), and ﬁnally the link-editor (or linker) ld\nwill put it all together into a ﬁnal executable program. Fortuna tely(!), for\nmost purposes, you can blithely be unaware of how gcc works, and just\nuse it with the proper ﬂags.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLABORATORY : TUTORIAL 3\nThe result of your compilation above is an executable, named (by de -\nfault)a.out . To then run the program, we simply type:\nprompt> ./a.out\nWhen we run this program, the OS will set argc and argv properly\nso that the program can process the command-line arguments as ne ed be.\nSpeciﬁcally, argc will be equal to 1,argv[0] will be the string “./a.out”,\nandargv[1] will be null, indicating the end of the array.\nF.3 Useful Flags\nBefore moving on to the C language, we’ll ﬁrst point out some useful\ncompilation ﬂags for gcc.\nprompt> gcc -o hw hw.c # -o: to specify the executable name\nprompt> gcc -Wall hw.c # -Wall: gives much better warnings\nprompt> gcc -g hw.c # -g: to enable debugging with gdb\nprompt> gcc -O hw.c # -O: to turn on optimization\nOf course, you may combine these ﬂags as you see ﬁt (e.g., gcc -o\nhw -g -Wall hw.c ). Of these ﬂags, you should always use -Wall ,\nwhich gives you lots of extra warnings about possible mistakes. Don’t\nignore the warnings! Instead, ﬁx them and thus make them blissfully\ndisappear.\nF.4 Linking with Libraries\nSometimes, you may want to use a library routine in your program.\nBecause so many routines are available in the C library (which is auto-\nmatically linked with every program), all you usually have to d o is ﬁnd\nthe right#include ﬁle. The best way to do that is via the manual pages ,\nusually just called the man pages .\nFor example, let’s say you want to use the fork() system call1. By\ntypingman fork at the shell prompt, you will get back a text description\nof howfork() works. At the very top will be a short code snippet, and\nthat will tell you which ﬁles you need to #include in your program in\norder to get it to compile. In the case of fork() , you need to #include\nbothsys/types.h andunistd.h , which would be accomplished as\nfollows:\n#include <sys/types.h>\n#include <unistd.h>\n1Note thatfork() is a system call, and not just a library routine. However, the C libr ary\nprovides C wrappers for all the system calls, each of which simply tr ap into the operating\nsystem.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 LABORATORY : TUTORIAL\nHowever, some library routines do not reside in the C library, and\ntherefore you will have to do a little more work. For example, the ma th\nlibrary has many useful routines, such as sines, cosines, tang ents, and the\nlike. If you want to include the routine tan() in our code, you should\nagain ﬁrst check the man page. At the top of the Linux man page for t an,\nyou will see the following two lines:\n#include <math.h>\n...\nLink with -lm.\nThe ﬁrst line you already should understand — you need to #include\nthe math library, which is found in the standard location in the ﬁ le system\n(i.e.,/usr/include/math.h ). However, what the next line is telling\nyou is how to “link” your program with the math library. A number\nof useful libraries exist and can be linked with; many of those re side in\n/usr/lib ; it is indeed where the math library is found.\nThere are two types of libraries: statically-linked librari es (which end\nin.a), and dynamically-linked ones (which end in .so). Statically-linked\nlibraries are combined directly into your executable; that is , the low-level\ncode for the library is inserted into your executable by the link er, and re-\nsults in a much larger binary object. Dynamic linking improves on this\nby just including the reference to a library in your program exe cutable;\nwhen the program is run, the operating system loader dynamicall y links\nin the library. This method is preferred over the static approac h because\nit saves disk space (no unnecessarily large executables are made) and al-\nlows applications to share library code and static data in memory . In the\ncase of the math library, both static and dynamic versions are av ailable,\nwith the static version called /usr/lib/libm.a and the dynamic one\n/usr/lib/libm.so .\nIn any case, to link with the math library, you need to specify t he li-\nbrary to the link-editor; this can be achieved by invoking gcc with the\nright ﬂags.\nprompt> gcc -o hw hw.c -Wall -lm\nThe-lXXX ﬂag tells the linker to look for libXXX.so orlibXXX.a ,\nprobably in that order. If for some reason you insist on the static lib rary\nover the dynamic one, there is another ﬂag you can use — see you if\nyou can ﬁnd out what it is. People sometimes prefer the static vers ion\nof a library because of the slight performance cost associated wit h using\ndynamic libraries.\nOne ﬁnal note: if you want the compiler to search for headers in a di f-\nferent path than the usual places, or want it to link with libra ries that you\nspecify, you can use the compiler ﬂag -I/foo/bar to look for headers in\nthe directory /foo/bar , and the-L/foo/bar ﬂag to look for libraries in\nthe/foo/bar directory. One common directory to specify in this manner\nis “.” (called “dot”), which is U NIX shorthand for the current directory.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLABORATORY : TUTORIAL 5\nNote that the -Iﬂag should go on a compile line, and the -Lﬂag on the\nlink line.\nF.5 Separate Compilation\nOnce a program starts to get large enough, you may want to split\nit into separate ﬁles, compiling each separately, and then li nk them to-\ngether. For example, say you have two ﬁles, hw.c andhelper.c , and\nyou wish to compile them individually, and then link them togeth er.\n# we are using -Wall for warnings, -O for optimization\nprompt> gcc -Wall -O -c hw.c\nprompt> gcc -Wall -O -c helper.c\nprompt> gcc -o hw hw.o helper.o -lm\nThe-cﬂag tells the compiler just to produce an object ﬁle — in this\ncase, ﬁles called hw.o andhelper.o . These ﬁles are not executables,\nbut just machine-level representations of the code within each source\nﬁle. To combine the object ﬁles into an executable, you have to “l ink”\nthem together; this is accomplished with the third line gcc -o hw hw.o\nhelper.o ). In this case, gcc sees that the input ﬁles speciﬁed are not\nsource ﬁles ( .c), but instead are object ﬁles ( .o), and therefore skips right\nto the last step and invoked the link-editor ldto link them together into a\nsingle executable. Because of its function, this line is often c alled the “link\nline”, and would be where you specify link-speciﬁc commands suc h as\n-lm. Analogously, ﬂags such as -Wall and-Oare only needed in the\ncompile phase, and therefore need not be included on the link line but\nrather only on compile lines.\nOf course, you could just specify all the C source ﬁles on a single li ne\ntogcc (gcc -Wall -O -o hw hw.c helper.c ), but this requires the\nsystem to recompile every source-code ﬁle, which can be a time-c onsuming\nprocess. By compiling each individually, you can save time by onl y re-\ncompiling those ﬁles that have changed during your editing, and thus\nincrease your productivity. This process is best managed by anot her pro-\ngram,make , which we now describe.\nF.6 Makeﬁles\nThe program make lets you automate much of your build process,\nand is thus a crucially important tool for any serious program (and p ro-\ngrammer). Let’s take a look at a simple example, saved in a ﬁle cal led\nMakefile .\nTo build your program, now all you have to do is type:\nprompt> make\nThis will (by default) look for Makefile ormakefile , and use that\nas its input (you can specify a different makeﬁle with a ﬂag; re ad the\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n6 LABORATORY : TUTORIAL\nhw: hw.o helper.o\ngcc -o hw hw.o helper.o -lm\nhw.o: hw.c\ngcc -O -Wall -c hw.c\nhelper.o: helper.c\ngcc -O -Wall -c helper.c\nclean:\nrm -f hw.o helper.o hw\nman pages to ﬁnd out which). The gnu version of make ,gmake , is more\nfully featured than traditional make, so we will focus upon it for the rest\nof this discussion (though we will use the two terms interchange ably).\nMost of these notes are based on the gmake info page; to see how to ﬁnd\nthose pages, see the Documentation section below. Also note: on Linu x\nsystems,gmake andmake are one and the same.\nMakeﬁles are based on rules, which are used to decide what need s to\nhappen. The general form of a rule:\ntarget: prerequisite1 prerequisite2 ...\ncommand1\ncommand2\n...\nAtarget is usually the name of a ﬁle that is generated by a program;\nexamples of targets are executable or object ﬁles. A target can a lso be the\nname of an action to carry out, such as “clean” in our example.\nAprerequisite is a ﬁle that is used as input to create the target. A\ntarget often depends on several ﬁles. For example, to build the e xecutable\nhw, we need two object ﬁles to be built ﬁrst: hw.o andhelper.o .\nFinally, a command is an action that make carries out. A rule may have\nmore than one command, each on its own line. Important: You have to\nput a single tab character at the beginning of every command lin e! If you\njust put spaces, make will print out some obscure error message and exit.\nUsually a command is in a rule with prerequisites and serves to cre-\nate a target ﬁle if any of the prerequisites change. However, th e rule that\nspeciﬁes commands for the target need not have prerequisites. F or ex-\nample, the rule containing the delete command associated with t he target\n“clean” does not have prerequisites.\nGoing back to our example, when make is executed, it roughly works\nlike this: First, it comes to the target hw, and it realizes that to build it, it\nmust have two prerequisites, hw.o andhelper.o . Thus,hwdepends on\nthose two object ﬁles. Make then will examine each of those target s. In\nexamining hw.o , it will see that it depends on hw.c . Here is the key: if\nhw.c has been modiﬁed more recently than hw.o has been created, make\nwill know that hw.o is out of date and should be generated anew; in that\ncase, it will execute the command line, gcc -O -Wall -c hw.c , which\ngenerates hw.o . Thus, if you are compiling a large program, make will\nknow which object ﬁles need to be re-generated based on their dep en-\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLABORATORY : TUTORIAL 7\ndencies, and will only do the necessary amount of work to recreate the\nexecutable. Also note that hw.o will be created in the case that it does\nnot exist at all.\nContinuing along, helper.o may also be regenerated or created, based\non the same criteria as deﬁned above. When both of the object ﬁles\nhave been created, make is now ready to execute the command to cr e-\nate the ﬁnal executable, and goes back and does so: gcc -o hw hw.o\nhelper.o -lm .\nUp until now, we’ve been ignoring the clean target in the makeﬁle.\nTo use it, you have to ask for it explicitly. Type\nprompt> make clean\nThis will execute the command on the command line. Because there\nare no prerequisites for the clean target, typing make clean will al-\nways result in the command(s) being executed. In this case, th eclean\ntarget is used to remove the object ﬁles and executable, quite h andy if\nyou wish to rebuild the entire program from scratch.\nNow you might be thinking, “well, this seems OK, but these makeﬁ les\nsure are cumbersome!” And you’d be right — if they always had to be\nwritten like this. Fortunately, there are a lot of shortcuts that makemake\neven easier to use. For example, this makeﬁle has the same func tionality\nbut is a little nicer to use:\n# specify all source files here\nSRCS = hw.c helper.c\n# specify target here (name of executable)\nTARG = hw\n# specify compiler, compile flags, and needed libs\nCC = gcc\nOPTS = -Wall -O\nLIBS = -lm\n# this translates .c files in src list to .o’s\nOBJS = $(SRCS:.c=.o)\n# all is not really needed, but is used to generate the target\nall: $(TARG)\n# this generates the target executable\n$(TARG): $(OBJS)\n$(CC) -o $(TARG) $(OBJS) $(LIBS)\n# this is a generic rule for .o files\n%.o: %.c\n$(CC) $(OPTS) -c $< -o $@\n# and finally, a clean line\nclean:\nrm -f $(OBJS) $(TARG)\nThough we won’t go into the details of make syntax, as you can see,\nthis makeﬁle can make your life somewhat easier. For example, it allows\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n8 LABORATORY : TUTORIAL\nyou to easily add new source ﬁles into your build, simply by addin g them\nto theSRCS variable at the top of the makeﬁle. You can also easily change\nthe name of the executable by changing the TARG line, and the compiler,\nﬂags, and library speciﬁcations are all easily modiﬁed.\nOne ﬁnal word about make : ﬁguring out a target’s prerequisites is not\nalways trivial, especially in large and complex programs. Not s urpris-\ningly, there is another tool that helps with this, called makedepend . Read\nabout it on your own and see if you can incorporate it into a makeﬁle.\nF.7 Debugging\nFinally, after you have created a good build environment, and a cor -\nrectly compiled program, you may ﬁnd that your program is buggy. On e\nway to ﬁx the problem(s) is to think really hard — this method is s ome-\ntimes successful, but often not. The problem is a lack of information ; you\njust don’t know exactly what is going on within the program, and ther e-\nfore cannot ﬁgure out why it is not behaving as expected. Fortunate ly,\nthere is some help: gdb, the GNU debugger.\nLet’s take the following buggy code, saved in the ﬁle buggy.c , and\ncompiled into the executable buggy.\n#include <stdio.h>\nstruct Data {\nint x;\n};\nint\nmain(int argc, char *argv[])\n{\nstruct Data *p = NULL;\nprintf(""%d\n"", p->x);\n}\nIn this example, the main program dereferences the variable pwhen\nit is NULL, which will lead to a segmentation fault. Of course, t his prob-\nlem should be easy to ﬁx by inspection, but in a more complex program ,\nﬁnding such a problem is not always easy.\nTo prepare yourself for a debugging session, recompile your progra m\nand make sure to pass the -gﬂag to each compile line. This includes extra\ndebugging information in your executable that will be useful du ring your\ndebugging session. Also, don’t turn on optimization ( -O); though this\nmay work, it may also lead to confusion during debugging.\nAfter re-compiling with -g, you are ready to use the debugger. Fire\nupgdb at the command prompt as follows:\nprompt> gdb buggy\nThis puts you inside an interactive session with the debugger. Note\nthat you can also use the debugger to examine “core” ﬁles that we re pro-\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLABORATORY : TUTORIAL 9\nduced during bad runs, or to attach to an already-running progr am; read\nthe documentation to learn more about this.\nOnce inside, you may see something like this:\nprompt> gdb buggy\nGNU gdb ...\nCopyright 2008 Free Software Foundation, Inc.\n(gdb)\nThe ﬁrst thing you might want to do is to go ahead and run the pro-\ngram. To do this, simply type run at gdb command prompt. In this case,\nthis is what you might see:\n(gdb) run\nStarting program: buggy\nProgram received signal SIGSEGV, Segmentation fault.\n0x8048433 in main (argc=1, argv=0xbffff844) at buggy.cc:1 9\n19 printf(""%d\n"", p->x);\nAs you can see from the example, in this case, gdb immediately pin-\npoints where the problem occurred; a “segmentation fault” was ge ner-\nated at the line where we tried to dereference p. This just means that we\naccessed some memory that we weren’t supposed to access. At this p oint,\nthe astute programmer can examine the code, and say “aha! it mus t be\nthatpdoes not point to anything valid, and thus should not be derefer-\nenced!”, and then go ahead and ﬁx the problem.\nHowever, if you didn’t know what was going on, you might want to\nexamine some variable. gdb allows you to do this interactively during\nthe debug session.\n(gdb) print p\n1 = (Data *) 0x0\nBy using the print primitive, we can examine p, and see both that it is\na pointer to a struct of type Data, and that it is currently set to NULL (or\nzero, or hex zero which is shown here as “0x0”).\nFinally, you can also set breakpoints within your program to have the\ndebugger stop the program at a certain routine. After doing this, it is\noften useful to step through the execution (one line at a time), an d see\nwhat is happening.\n(gdb) break main\nBreakpoint 1 at 0x8048426: file buggy.cc, line 17.\n(gdb) run\nStarting program: /homes/hacker/buggy\nBreakpoint 1, main (argc=1, argv=0xbffff844) at buggy.cc: 17\n17 struct Data *p = NULL;\n(gdb) next\n19 printf(""%d\n"", p->x);\n(gdb)\nProgram received signal SIGSEGV, Segmentation fault.\n0x8048433 in main (argc=1, argv=0xbffff844) at buggy.cc:1 9\n19 printf(""%d\n"", p->x);\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n10 LABORATORY : TUTORIAL\nIn the example above, a breakpoint is set at the main() routine; thus,\nwhen we run the program, the debugger almost immediately stops e xe-\ncution at main. At that point in the example, a “next” command is i ssued,\nwhich executes the next source-level command. Both “next” and “ step”\nare useful ways to advance through a program — read about them in t he\ndocumentation for more details2.\nThis discussion really does not do gdb justice; it is a rich and ﬂexi-\nble debugging tool, with many more features than can be describe d in\nthe limited space here. Read more about it on your own and become an\nexpert in your copious spare time.\nF.8 Documentation\nTo learn a lot more about all of these things, you have to do two thing s:\nthe ﬁrst is to use these tools, and the second is to read more about th em\non your own. One way to ﬁnd out more about gcc,gmake , andgdb is to\nread their man pages; type man gcc ,man gmake , orman gdb at your\ncommand prompt. You can also use man -k to search the man pages for\nkeywords, though that doesn’t always work as well as it might; googli ng\nis probably a better approach here.\nOne tricky thing about man pages: typing man XXX may not result\nin the thing you want, if there is more than one thing called XXX. For\nexample, if you are looking for the kill() system call man page, and\nif you just type man kill at the prompt, you will get the wrong man\npage, because there is a command-line program called kill . Man pages\nare divided into sections , and by default, man will return the man page\nin the lowest section that it ﬁnds, which in this case is section 1 . Note that\nyou can tell which man page you got by looking at the top of the page:\nif you see kill(2) , you know you are in the right man page in Section\n2, where system calls live. Type man man to learn more about what is\nstored in each of the different sections of the man pages. Also note that\nman -a kill can be used to cycle through all of the different man pages\nnamed “kill”.\nMan pages are useful for ﬁnding out a number of things. In particu lar,\nyou will often want to look up what arguments to pass to a library ca ll,\nor what header ﬁles need to be included to use a library call. Al l of this\nshould be available in the man page. For example, if you look up the\nopen() system call, you will see:\nSYNOPSIS\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <fcntl.h>\nint open(const char *path, int oflag, / *mode_t mode */...);\n2In particular, you can use the interactive “help” command while debugg ing withgdb\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLABORATORY : TUTORIAL 11\nThat tells you to include the headers sys/types.h ,sys/stat.h ,\nandfcntl.h in order to use the open call. It also tells you about the\nparameters to pass to open, namely a string called path , and integer ﬂag\noflag , and an optional argument to specify the mode of the ﬁle. If there\nwere any libraries you needed to link with to use the call, it wou ld tell\nyou that here too.\nMan pages require some effort to use effectively. They are often di-\nvided into a number of standard sections. The main body will desc ribe\nhow you can pass different parameters in order to have the functi on be-\nhave differently.\nOne particularly useful section is called the RETURN VALUES part of\nthe man page, and it tells you what the function will return unde r success\nor failure. From the open() man page again:\nRETURN VALUES\nUpon successful completion, the open() function opens the\nfile and return a non-negative integer representing the\nlowest numbered unused file descriptor. Otherwise, -1 is\nreturned, errno is set to indicate the error, and no files\nare created or modified.\nThus, by checking what open returns, you can see if the open suc-\nceeded or not. If it didn’t, open (and many standard library routin es) will\nset a global variable called errno to a value to tell you about the error.\nSee theERRORS section of the man page for more details.\nAnother thing you might want to do is to look for the deﬁnition of a\nstructure that is not speciﬁed in the man page itself. For examp le, the\nman page for gettimeofday() has the following synopsis:\nSYNOPSIS\n#include <sys/time.h>\nint gettimeofday(struct timeval *restrict tp,\nvoid*restrict tzp);\nFrom this page, you can see that the time is put into a structure of\ntypetimeval , but the man page may not tell you what ﬁelds that struct\nhas! (in this case, it does, but you may not always be so lucky) Thu s, you\nmay have to hunt for it. All include ﬁles are found under the dire ctory\n/usr/include , and thus you can use a tool like grep to look for it. For\nexample, you might type:\nprompt> grep ’struct timeval’ /usr/include/sys/ *.h\nThis lets you look for the deﬁnition of the structure in all ﬁles that\nend with.hin/usr/include/sys . Unfortunately, this may not always\nwork, as that include ﬁle may include others which are found else where.\nA better way to do this is to use a tool at your disposal, the com-\npiler. Write a program that includes the header time.h , let’s say called\nmain.c . Then, instead of compiling it, use the compiler to invoke the\npreprocessor. The preprocessor processes all the directives in y our ﬁle,\nsuch as#define commands and #include commands. To do this, type\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n12 LABORATORY : TUTORIAL\ngcc -E main.c . The result of this is a C ﬁle that has all of the needed\nstructures and prototypes in it, including the deﬁnition of the t imeval\nstruct.\nProbably an even better way to ﬁnd these things out: google. You\nshould always google things you don’t know about — it’s amazing how\nmuch you can learn simply by looking it up!\nInfo Pages\nAlso quite useful in the hunt for documentation are the info pages , which\nprovide much more detailed documentation on many GNU tools. You\ncan access the info pages by running the program info , or viaemacs ,\nthe preferred editor of hackers, by executing Meta-x info . A program\nlikegcc has hundreds of ﬂags, and some of them are surprisingly useful\nto know about. gmake has many more features that will improve your\nbuild environment. Finally, gdb is quite a sophisticated debugger. Read\nthe man and info pages, try out features that you hadn’t tried bef ore, and\nbecome a power user of your programming tools.\nF.9 Suggested Readings\nOther than the man and info pages, there are a number of useful b ooks\nout there. Note that a lot of this information is available for free on- line;\nhowever, sometimes having something in book form seems to make it\neasier to learn. Also, always look for O’Reilly books on topics you are\ninterested in; they are almost always of high quality.\n•“The C Programming Language”, by Brian Kernighan and Dennis\nRitchie. This is thedeﬁnitive C book to have.\n•“Managing Projects with make”, by Andrew Oram and Steve Tal-\nbott. A reasonable and short book on make.\n•“Debugging with GDB: The GNU Source-Level Debugger”, by Richa rd\nM. Stallman, Roland H. Pesch. A little book on using GDB.\n•“Advanced Programming in the UNIX Environment”, by W. Richard\nStevens and Steve Rago. Stevens wrote some excellent books, and\nthis is a must for U NIXhackers. He also has an excellent set of books\non TCP/IP and Sockets programming.\n•“Expert C Programming”, by Peter Van der Linden. A lot of the\nuseful tips about compilers, etc., above are stolen directly from here.\nRead this! It is a great and eye-opening book, even though a little\nout of date.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",28843
63-Appendix G Laboratory - Systems Projects.pdf,63-Appendix G Laboratory - Systems Projects,"G\nLaboratory: Systems Projects\nThis chapter presents some ideas for systems projects. We usual ly do\nabout six or seven projects in a 15-week semester, meaning one eve ry two\nweeks or so. The ﬁrst few are usually done by a single student, and the\nlast few in groups of size two.\nEach semester, the projects follow this same outline; however, we vary\nthe details to keep it interesting and make “sharing” of code ac ross semesters\nmore challenging (not that anyone would do that!). We also use the M oss\ntool [M94] to look for this kind of “sharing”.\nAs for grading, we’ve tried a number of different approaches, eac h\nof which have their strengths and weaknesses. Demos are fun but time\nconsuming. Automated test scripts are less time intensive but require a\ngreat deal of care to get them to carefully test interesting cor ner cases.\nCheck the book web page for more details on these projects; if you’d lik e\nthe automated test scripts, we’d be happy to share.\nG.1 Intro Project\nThe ﬁrst project is an introduction to systems programming. Typi cal\nassignments have been to write some variant of the sort utility, with\ndifferent constraints. For example, sorting text data, sorting binary data,\nand other similar projects all make sense. To complete the projec t, one\nmust get familiar with some system calls (and their return err or codes),\nuse a few simple data structures, and not much else.\nG.2 U NIXShell\nIn this project, students build a variant of a U NIXshell. Students learn\nabout process management as well as how mysterious things like pi pes\nand redirects actually work. Variants include unusual featu res, like a\nredirection symbol that also compresses the output via gzip. Anot her\nvariant is a batch mode which allows the user to batch up a few req uests\nand then execute them, perhaps using different scheduling d isciplines.\n1\n2 LABORATORY : SYSTEMS PROJECTS\nG.3 Memory-allocation Library\nThis project explores how a chunk of memory is managed, by building\nan alternative memory-allocation library (like malloc() andfree()\nbut with different names). The project teaches students how to usemmap()\nto get a chunk of anonymous memory, and then about pointers in great\ndetail in order to build a simple (or perhaps, more complex) free l ist to\nmanage the space. Variants include: best/worst ﬁt, buddy, an d various\nother allocators.\nG.4 Intro to Concurrency\nThis project introduces concurrent programming with POSIX threa ds.\nBuild some simple thread-safe libraries: a list, hash table, and some more\ncomplicated data structures are good exercises in adding locks t o real-\nworld code. Measure the performance of coarse-grained versus ﬁne -grained\nalternatives. Variants just focus on different (and perhaps m ore complex)\ndata structures.\nG.5 Concurrent Web Server\nThis project explores the use of concurrency in a real-world appli ca-\ntion. Students take a simple web server (or build one) and add a thr ead\npool to it, in order to serve requests concurrently. The thread pool should\nbe of a ﬁxed size, and use a producer/consumer bounded buffer to pa ss\nrequests from a main thread to the ﬁxed pool of workers. Learn how\nthreads, locks, and condition variables are used to build a real server.\nVariants include scheduling policies for the threads.\nG.6 File System Checker\nThis project explores on-disk data structures and their consist ency.\nStudents build a simple ﬁle system checker. The debugfs tool can be\nused on Linux to make real ﬁle-system images; crawl through the m and\nmake sure all is well. To make it more difﬁcult, also ﬁx any probl ems that\nare found. Variants focus on different types of problems: pointers , link\ncounts, use of indirect blocks, etc.\nG.7 File System Defragmenter\nThis project explores on-disk data structures and their perform ance\nimplications. The project should give some particular ﬁle-syst em images\nto students with known fragmentation problems; students should then\ncrawl through the image, and look for ﬁles that are not laid out seque n-\ntially. Write out a new “defragmented” image that ﬁxes this pr oblem,\nperhaps reporting some statistics.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLABORATORY : SYSTEMS PROJECTS 3\nG.8 Concurrent File Server\nThis project combines concurrency and ﬁle systems and even a lit tle\nbit of networking and distributed systems. Students build a sim ple con-\ncurrent ﬁle server. The protocol should look something like NFS, with\nlookups, reads, writes, and stats. Store ﬁles within a single dis k image\n(designed as a ﬁle). Variants are manifold, with different su ggested on-\ndisk formats and network protocols.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES\n4 LABORATORY : SYSTEMS PROJECTS\nReferences\n[M94] “Moss: A System for Detecting Software Plagiarism”\nAlex Aiken\nAvailable: http://theory.stanford.edu/˜aiken/moss/\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG",4962
64-Appendix H Laboratory - xv6 Projects.pdf,64-Appendix H Laboratory - xv6 Projects,"H\nLaboratory: xv6 Projects\nThis chapter presents some ideas for projects related to the xv6 kernel.\nThe kernel is available from MIT and is quite fun to play with; d oing\nthese projects also make the in-class material more directly r elevant to\nthe projects. These projects (except perhaps the ﬁrst couple) a re usually\ndone in pairs, making the hard task of staring at the kernel a lit tle easier.\nH.1 Intro Project\nThe introduction adds a simple system call to xv6. Many variant s are\npossible, including a system call to count how many system calls have\ntaken place (one counter per system call), or other information-ga thering\ncalls. Students learn about how a system call actually takes pla ce.\nH.2 Processes and Scheduling\nStudents build a more complicated scheduler than the default rou nd\nrobin. Many variants are possible, including a Lottery schedul er or multi-\nlevel feedback queue. Students learn how schedulers actually work, as\nwell as how a context switch takes place. A small addendum is to a lso\nrequire students to ﬁgure out how to make processes return a prope r error\ncode when exiting, and to be able to access that error code through the\nwait() system call.\nH.3 Intro to Virtual Memory\nThe basic idea is to add a new system call that, given a virtual address,\nreturns the translated physical address (or reports that the a ddress is not\nvalid). This lets students see how the virtual memory system se ts up page\ntables without doing too much hard work. Another variant explores h ow\nto transform xv6 so that a null-pointer dereference actually g enerates a\nfault.\n1\n2 LABORATORY :XV6 PROJECTS\nH.4 Copy-on-write Mappings\nThis project adds the ability to perform a lightweight fork() , called\nvfork() , to xv6. This new call doesn’t simply copy the mappings but\nrather sets up copy-on-write mappings to shared pages. Upon ref erence\nto such a page, the kernel must then create a real copy and updat e page\ntables accordingly.\nH.5 Memory mappings\nAn alternate virtual memory project is to add some form of memory-\nmapped ﬁles. Probably the easiest thing to do is to perform a laz y page-in\nof code pages from an executable; a more full-blown approach is to bu ild\nanmmap() system call and all of the requisite infrastructure needed to\nfault in pages from disk upon dereference.\nH.6 Kernel Threads\nThis project explores how to add kernel threads to xv6. A clone()\nsystem call operates much like fork but uses the same address sp ace. Stu-\ndents have to ﬁgure out how to implement such a call, and thus how\nto create a real kernel thread. Students also should build a lit tle thread\nlibrary on top of that, providing simple locks.\nH.7 Advanced Kernel Threads\nStudents build a full-blown thread library on top of their kernel t hreads,\nadding different types of locks (spin locks, locks that sleep whe n the pro-\ncessor is not available) as well as condition variables. Requisi te kernel\nsupport is added as well.\nH.8 Extent-based File System\nThis ﬁrst ﬁle system project adds some simple features to the ba sic\nﬁle system. For ﬁles of type EXTENT, students change the inode to store\nextents (i.e., pointer, length pairs) instead of just pointers . Serves as a\nrelatively light introduction to the ﬁle system.\nH.9 Fast File System\nStudents transform the basic xv6 ﬁle system into the Berkeley F ast File\nSystem (FFS). Students build a new mkfs tool, introduce block groups\nand a new block-allocation policy, and build the large-ﬁle excep tion. The\nbasics of how ﬁle systems work are understood at a deeper level.\nOPERATING\nSYSTEMS\n[VERSION 1.00] WWW .OSTEP .ORG\nLABORATORY :XV6 PROJECTS 3\nH.10 Journaling File System\nStudents add a rudimentary journaling layer to xv6. For each wri te to\na ﬁle, the journaling FS batches up all dirtied blocks and write s a record of\ntheir pending update to an on-disk log; only then are the blocks mod iﬁed\nin place. Students demonstrate the correctness of their system b y intro-\nducing crash points and showing that the ﬁle system always recov ers to\na consistent state.\nH.11 File System Checker\nStudents build a simple ﬁle system checker for the xv6 ﬁle syste m.\nStudents learn about what makes a ﬁle system consistent and how ex actly\nto check for it.\nc/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE\nEASY\nPIECES",4343
