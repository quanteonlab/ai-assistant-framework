filename,title,text,len
01-Cover.pdf,01-Cover,"Data Governance  \n  The Definitive Guide\nPeople, Processes, and Tools to Operationalize \nData Trustworthiness\nEvren Eryurek, Uri Gilad,  \nValliappa Lakshmanan,  \nAnita Kibunguchy-Grant  \n& Jessi Ashdown\n\nPraise for Data Governance: The Definitive  Guide\nWe live in a digital world. Whether we realize it or not, we are living in the throes of one\nof the biggest economic and social revolutions since the Industrial Revolution. It’s about\ntransforming traditional business processes—many of them previously nondigital or\nmanual—into processes that will fundamentally change how we live, how we operate our\nbusinesses, and how we deliver value to our customers. Data defines, informs, and\npredicts—it is used to penetrate new markets, control costs, drive revenues, manage risk,\nand help us discover the world around us. But to realize these benefits, data must be\nproperly managed and stewarded. Data Governance: The Definitive  Guide  walks you\nthrough the many facets of data management and data governance—people, process and\ntools, data ownership, data quality, data protection, privacy and security—and does it in a\nway that is practical and easy to follow. A must-read for the data professional!\n—John Bottega, president of the EDM Council\nEnterprises are increasingly evolving as insight-driven businesses, putting pressure on\ndata to satisfy new use cases and business ecosystems. Add to this business complexity,\nmarket disruption, and demand for speed, and data governance is front and center to\nmake data trusted, secure, and relevant. This is not your grandfather’s slow and\nbureaucratic data governance either. This book shares the secrets into how modern data\ngovernance ensures data is the cornerstone to your business resilience, elasticity, speed,\nand growth opportunity and not an afterthought.\n—Michele Goetz, vice president/principal analyst–business\ninsights at Forrester\nData governance has evolved from a discipline focused on cost and compliance to one\nthat propels organizations to grow and innovate. Today’s data governance solutions can\nbenefit from technological advances that establish a continuous, autonomous, and\nvirtuous cycle. This in turn becomes an ecosystem—a community in which data is used\nfor good, and doing the right thing is also the easy thing. Executives looking to use data as\nan asset and deliver positive business outcomes need to rethink governance’s role and\nadopt the modern and transformative approach Data Governance: The Definitive  Guide\nprovides.\n—Jim Cushman, CPO of Collibra\nEvren Eryurek, Uri Gilad, Valliappa Lakshmanan,\nAnita Kibunguchy-Grant, and Jessi AshdownData Governance:\nThe Definitive  Guide\nPeople, Processes, and Tools to Operationalize\nData Trustworthiness\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing",2852
02-Table of Contents.pdf,02-Table of Contents,"978-1-492-06349-0\n[LSI]Data Governance: The Definitive  Guide\nby Evren Eryurek, Uri Gilad, Valliappa Lakshmanan, Anita Kibunguchy-Grant, and Jessi Ashdown\nCopyright © 2021 Uri Gilad, Jessi Ashdown, Valliappa Lakshmanan, Evren Eryurek, and Anita\nKibunguchy-Grant. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com .\nAcquisitions Editor:  Jessica Haberman\nDevelopment Editor:  Gary O’Brien\nProduction Editor:  Kate Galloway\nCopyeditor:  Piper Editorial Consulting, LLC\nProofreader:  Arthur JohnsonIndexer:  WordCo Indexing Services, Inc.\nInterior Designer:  David Futato\nCover Designer:  Karen Montgomery\nIllustrator:  Kate Dullea\nMarch 2021:  First Edition\nRevision History for the First Edition\n2021-03-08: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492063490  for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Data Governance: The Definitive  Guide ,\nthe cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\n1.What Is Data Governance?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nWhat Data Governance Involves                                                                                     2\nHolistic Approach to Data Governance                                                                      4\nEnhancing Trust in Data                                                                                               5\nClassification and Access Control                                                                                6\nData Governance Versus Data Enablement and Data Security                               8\nWhy Data Governance Is Becoming More Important                                                 9\nThe Size of Data Is Growing                                                                                         9\nThe Number of People Working and/or Viewing the Data Has Grown\nExponentially                                                                                                             10\nMethods of Data Collection Have Advanced                                                           10\nMore Kinds of Data (Including More Sensitive Data) Are Now\nBeing Collected                                                                                                         13\nThe Use Cases for Data Have Expanded                                                                   14\nNew Regulations and Laws Around the Treatment of Data                                   16\nEthical Concerns Around the Use of Data                                                               16\nExamples of Data Governance in Action                                                                     17\nManaging Discoverability, Security, and Accountability                                        18\nImproving Data Quality                                                                                              19\nThe Business Value of Data Governance                                                                      23\nFostering Innovation                                                                                                   23\nThe Tension Between Data Governance and Democratizing Data Analysis       24\nManage Risk (Theft, Misuse, Data Corruption)                                                      25\nRegulatory Compliance                                                                                               26\nConsiderations for Organizations as They Think About Data Governance       28\nWhy Data Governance Is Easier in the Public Cloud                                                 30\nv\nLocation                                                                                                                         31\nReduced Surface Area                                                                                                  32\nEphemeral Compute                                                                                                    32\nServerless and Powerful                                                                                               32\nLabeled Resources                                                                                                        33\nSecurity in a Hybrid World                                                                                         34\nSummary                                                                                                                           34\n2.Ingredients of Data Governance: Tools. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\nThe Enterprise Dictionary                                                                                              37\nData Classes                                                                                                                   38\nData Classes and Policies                                                                                            40\nData Classification and Organization                                                                       43\nData Cataloging and Metadata Management                                                           44\nData Assessment and Profiling                                                                                  45\nData Quality                                                                                                                  46\nLineage Tracking                                                                                                          46\nKey Management and Encryption                                                                             47\nData Retention and Data Deletion                                                                             50\nWorkflow Management for Data Acquisition                                                          52\nIAM—Identity and Access Management                                                                  52\nUser Authorization and Access Management                                                          54\nSummary                                                                                                                           55\n3.Ingredients of Data Governance: People and Processes. . . . . . . . . . . . . . . . . . . . . . . . . . .  57\nThe People: Roles, Responsibilities, and Hats                                                              57\nUser Hats Defined                                                                                                        58\nData Enrichment and Its Importance                                                                        65\nThe Process: Diverse Companies, Diverse Needs and Approaches to Data\nGovernance                                                                                                                   65\nLegacy                                                                                                                             66\nCloud Native/Digital Only                                                                                          67\nRetail                                                                                                                              67\nHighly Regulated                                                                                                          69\nSmall Companies                                                                                                          71\nLarge Companies                                                                                                          72\nPeople and Process Together: Considerations, Issues, and Some Successful\nStrategies                                                                                                                        73\nConsiderations and Issues                                                                                           74\nProcesses and Strategies with Varying Success                                                        77\nSummary                                                                                                                           84\nvi | Table of Contents\n4.Data Governance over a Data Life Cycle. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  85\nWhat Is a Data Life Cycle?                                                                                              85\nPhases of a Data Life Cycle                                                                                             86\nData Creation                                                                                                                87\nData Processing                                                                                                            88\nData Storage                                                                                                                  88\nData Usage                                                                                                                     88\nData Archiving                                                                                                              89\nData Destruction                                                                                                          89\nData Life Cycle Management                                                                                         90\nData Management Plan                                                                                               90\nApplying Governance over the Data Life Cycle                                                          92\nData Governance Framework                                                                                    92\nData Governance in Practice                                                                                      94\nExample of How Data Moves Through a Data Platform                                        97\nOperationalizing Data Governance                                                                            100\nWhat Is a Data Governance Policy?                                                                         101\nImportance of a Data Governance Policy                                                               102\nDeveloping a Data Governance Policy                                                                    103\nData Governance Policy Structure                                                                           103\nRoles and Responsibilities                                                                                         105\nStep-by-Step Guidance                                                                                              106\nConsiderations for Governance Across a Data Life Cycle                                   108\nSummary                                                                                                                         111\n5.Improving Data Quality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  113\nWhat Is Data Quality?                                                                                                   113\nWhy Is Data Quality Important?                                                                                 114\nData Quality in Big Data Analytics                                                                          116\nData Quality in AI/ML Models                                                                                117\nWhy Is Data Quality a Part of a Data Governance Program?                                 121\nTechniques for Data Quality                                                                                        121\nScorecard                                                                                                                     123\nPrioritization                                                                                                               123\nAnnotation                                                                                                                  123\nProfiling                                                                                                                       124\nSummary                                                                                                                         131\n6.Governance of Data in Flight. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  133\nData Transformations                                                                                                   133\nLineage                                                                                                                            134\nWhy Lineage Is Useful                                                                                               135\nTable of Contents | vii\nHow to Collect Lineage                                                                                             135\nTypes of Lineage                                                                                                         136\nThe Fourth Dimension                                                                                              138\nHow to Govern Data in Flight                                                                                  139\nPolicy Management, Simulation, Monitoring, Change Management                    141\nAudit, Compliance                                                                                                         141\nSummary                                                                                                                         142\n7.Data Protection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  143\nPlanning Protection                                                                                                       143\nLineage and Quality                                                                                                   144\nLevel of Protection                                                                                                     145\nClassification                                                                                                               146\nData Protection in the Cloud                                                                                       146\nMulti-Tenancy                                                                                                            147\nSecurity Surface                                                                                                          147\nVirtual Machine Security                                                                                          148\nPhysical Security                                                                                                            149\nNetwork Security                                                                                                        151\nSecurity in Transit                                                                                                      151\nData Exfiltration                                                                                                             153\nVirtual Private Cloud Service Controls (VPC-SC)                                               155\nSecure Code                                                                                                                156\nZero-Trust Model                                                                                                       157\nIdentity and Access Management                                                                                158\nAuthentication                                                                                                            158\nAuthorization                                                                                                              159\nPolicies                                                                                                                         160\nData Loss Prevention                                                                                                 161\nEncryption                                                                                                                  162\nDifferential Privacy                                                                                                    164\nAccess Transparency                                                                                                  165\nKeeping Data Protection Agile                                                                                    165\nSecurity Health Analytics                                                                                          165\nData Lineage                                                                                                               166\nEvent Threat Detection                                                                                             167\nData Protection Best Practices                                                                                     167\nSeparated Network Designs                                                                                      168\nPhysical Security                                                                                                         168\nPortable Device Encryption and Policy                                                                  170\nData Deletion Process                                                                                                170\nSummary                                                                                                                         173\nviii | Table of Contents\n8.Monitoring. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  175\nWhat Is Monitoring?                                                                                                     175\nWhy Perform Monitoring?                                                                                           176\nWhat Should Y ou Monitor?                                                                                         179\nData Quality Monitoring                                                                                          179\nData Lineage Monitoring                                                                                          180\nCompliance Monitoring                                                                                            182\nProgram Performance Monitoring                                                                          183\nSecurity Monitoring                                                                                                   185\nWhat Is a Monitoring System?                                                                                     187\nAnalysis in Real Time                                                                                                187\nSystem Alerts                                                                                                              187\nNotifications                                                                                                                187\nReporting/Analytics                                                                                                   188\nGraphic Visualization                                                                                                188\nCustomization                                                                                                            188\nMonitoring Criteria                                                                                                       189\nImportant Reminders for Monitoring                                                                        190\nSummary                                                                                                                         191\n9.Building a Culture of Data Privacy and Security. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  193\nData Culture: What It Is and Why It’s Important                                                      193\nStarting at the Top—Benefits of Data Governance to the Business                       194\nAnalytics and the Bottom Line                                                                                195\nCompany Persona and Perception                                                                          195\nIntention, Training, and Communications                                                                196\nA Data Culture Needs to Be Intentional                                                                 197\nTraining: Who Needs to Know What                                                                      197\nBeyond Data Literacy                                                                                                    200\nMotivation and Its Cascading Effects                                                                      200\nMaintaining Agility                                                                                                       201\nRequirements, Regulations, and Compliance                                                        202\nThe Importance of Data Structure                                                                           202\nScaling the Governance Process Up and Down                                                     203\nInterplay with Legal and Security                                                                                203\nStaying on Top of Regulations                                                                                  204\nCommunication                                                                                                         204\nInterplay in Action                                                                                                     204\nAgility Is Still Key                                                                                                       205\nIncident Handling                                                                                                          205\nWhen “Everyone” Is Responsible, No One Is Responsible                                  205\nImportance of Transparency                                                                                        206\nTable of Contents | ix\nWhat It Means to Be Transparent                                                                            206\nBuilding Internal Trust                                                                                              206\nBuilding External Trust                                                                                             207\nSetting an Example                                                                                                     208\nSummary                                                                                                                         208\nA.Google’s Internal Data Governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  209\nB.Additional Resources. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  217\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  219\nx | Table of Contents",25528
03-Data Governance Framework.pdf,03-Data Governance Framework,"Preface\nIn recent years, the ease of moving to the cloud has motivated and energized a fast-\ngrowing community of data consumers to collect, capture, store, and analyze data for\ninsights and decision making. For a number of reasons, as adoption of cloud comput‐\ning continues to grow, information management stakeholders have questions about\nthe potential risks involved in managing their data in the cloud. Evren faced such\nquestions for the first time when he worked in healthcare and had to put in place the\nprocesses and technologies to govern data. Now at Google Cloud, Uri and Lak also\nanswer these questions nearly every week and dispense advice on getting value from\ndata, breaking down data silos, preserving anonymity, protecting sensitive informa‐\ntion, and improving the trustworthiness of data.\nWe noticed that GDPR  was what precipitated a sea change in customers’ behavior.\nSome customers even deleted their data, thinking it was the right thing to do. That\nreaction, more than any other, prompted us to write this book capturing the advice\nwe have provided over the years to Google Cloud customers. If data is the new cur‐\nrency, we do not want enterprises to be scared of it. If the data is locked away or is not\ntrustworthy, it is of no value.\nWe all pride ourselves on helping Google Cloud customers get value for their techni‐\ncal expenditures. Data is a huge investment, and we felt obligated to provide our cus‐\ntomers with the best way to get value from it.\nCustomers’ questions usually involve one of three risk factors:\nSecuring the data\nStoring data in a public cloud infrastructure might concern large enterprises that\ntypically deploy their systems on-premises and expect tight security. With a sig‐\nnificant number of security threats and breaches in the news, organizations are\nconcerned that they might be the next victim. These factors contribute to risk\nmanagement concerns for protecting against unauthorized access to or exposure\nof sensitive data, ranging from personally identifiable information (PII) to corpo‐\nrate confidential information, trade secrets, or intellectual property.\nxi\nRegulations and compliance\nThere is a growing set of regulations, including the California Consumer Privacy\nAct (CCPA), the European Union’s General Data Protection Regulation (GDPR),\nand industry-specific standards such as global Legal Entity Identifier (LEI) num‐\nbers in the financial industry and ACORD data standards in the insurance indus‐\ntry. Compliance teams responsible for adhering to these regulations and\nstandards may have concerns about oversight and control of data stored in the\ncloud.\nVisibility and control\nData management professionals and data consumers sometimes lack visibility\ninto their own data landscape: which data assets are available, where those assets\nare located and how and if they can be used, and who has access to the data and\nwhether they should  have access to it. This uncertainty limits their ability to fur‐\nther leverage their own data to improve productivity or drive business value.\nThese risk factors clearly highlight the need for increased data assessment, cataloging\nof metadata, access control management, data quality, and information security as\ncore data governance competencies that the cloud provider should not only provide\nbut also continuously upgrade in a transparent way. In essence, addressing these risks\nwithout abandoning the benefits provided by cloud computing has elevated the\nimportance of not only understanding data governance in the cloud, but also know‐\ning what is important. Good data governance can inspire customer trust and lead to\nvast improvements in customer experience.\nWhy Your Business Needs Data Governance in the Cloud\nAs your business generates more data and moves it into the cloud, the dynamics of\ndata management change in a number of fundamental ways. Organizations should\ntake note of the following:\nRisk management\nThere are concerns about potential exposure of sensitive information to unau‐\nthorized individuals or systems, security breaches, or known personnel accessing\ndata under the wrong circumstances. Organizations are looking to minimize this\nrisk, so additional forms of protection (such as encryption) to obfuscate the data\nobject’s embedded information are required to safeguard the data should a sys‐\ntem breach occur. In addition, other tools are required in order to support access\nmanagement, identify sensitive data assets, and create a policy around their\nprotection.\nData proliferation\nThe speed at which businesses create, update, and stream their data assets has\nincreased, and while cloud-based platforms are capable of handling increased\nxii | Preface\n1Craig Stedman and Jack Vaughan, “What Is Data Governance and Why Does It Matter?”  TechTarget, Decem‐\nber 2019. This article was updated in February 2020; the current version no longer includes this quote.data velocity, volume, and variety, it is important to introduce controls and\nmechanisms to rapidly validate the quality aspects of high-bandwidth data\nstreams.\nData management\nThe need to adopt externally produced data sources and data streams (including\npaid feeds from third parties) means that you should be prepared not to trust all\nexternal data sources. Y ou may need to introduce tools that document data line‐\nage, classification, and metadata to help your employees (data consumers, in par‐\nticular) to determine data usability based on their knowledge of how the data\nassets were produced.\nDiscovery (and data awareness)\nMoving data into any kind of data lake (cloud-based or on-premises) runs the\nrisk of losing track of which data assets have been moved, the characteristics of\ntheir content, and details about their metadata. The ability, therefore, to assess\ndata asset content and sensitivity (no matter where the data is) becomes very\nimportant.\nPrivacy and compliance\nRegulatory compliance demands auditable and measurable standards and proce‐\ndures that ensure compliance with internal data policies as well as external gov‐\nernment regulations. Migrating data to the cloud means that organizations need\ntools to enforce, monitor, and report compliance, as well as ensure that the right\npeople and services have access and permissions to the right data.\nFramework and Best Practices for Data Governance\nin the Cloud\nGiven the changing dynamics of data management, how should organizations think\nabout data governance in the cloud, and why is it important? According to\nTechTarget , data governance is\nthe overall management of the availability, usability, integrity, and security of data used\nin an enterprise. A sound data governance program includes a governing body or\ncouncil, a defined set of procedures and a plan to execute those procedures.1\nSimply put, data governance encompasses the ways that people, processes and tech‐\nnology can work together to enable auditable compliance with defined and agreed-\nupon data policies.\nPreface | xiii\nData Governance Framework\nEnterprises need to think about data governance comprehensively, from data intake\nand ingestion to cataloging, persistence, retention, storage management, sharing,\narchiving, backup, recovery, loss prevention, disposition, and removal and deletion:\nData discovery and assessment\nCloud-based environments often offer an economical option for creating and\nmanaging data lakes, but the risk remains for ungoverned migration of data\nassets. This risk represents a potential loss of knowledge of what data assets are in\nthe data lake, what information is contained within each object, and where those\ndata objects originated from. A best practice for data governance in the cloud is\ndata discovery and assessment in order to know what data assets you have. The\ndata discovery and assessment process is used to identify data assets within the\ncloud environment, and to trace and record each data asset’s origin and lineage,\nwhat transformations have been applied, and object metadata. (Often this meta‐\ndata describes the demographic details, such as the name of the creator, the size\nof the object, the number of records if it is a structured data object, or when it\nwas last updated.)\nData classification  and organization\nProperly evaluating a data asset and scanning the content of its different\nattributes can help categorize the data asset for subsequent organization. This\nprocess can also infer whether the object contains sensitive data and, if so, clas‐\nsify it in terms of the level of data sensitivity, such as personal and private data,\nconfidential data, or intellectual property. To implement data governance in the\ncloud, you’ll need to profile and classify sensitive data to determine which gover‐\nnance policies and procedures apply to the data.\nData cataloging and metadata management\nOnce your data assets are assessed and classified, it is crucial that you document\nyour learnings so that your communities of data consumers have visibility into\nyour organization’s data landscape. Y ou need to maintain a data catalog that con‐\ntains structural metadata, data object metadata, and the assessment of levels of\nsensitivity in relation to the governance directives (such as compliance with one\nor more data privacy regulations). The data catalog not only allows data consum‐\ners to view this information but can also serve as part of a reverse index for\nsearch and discovery, both by phrase and (given the right ontologies) by concept.\nIt is also important to understand the format of structured and semi-structured\ndata objects and allow your systems to handle these data types differently, as\nnecessary.\nxiv | Preface\nData quality management\nDifferent data consumers may have different data quality requirements, so it’s\nimportant to provide a means of documenting data quality expectations as well as\ntechniques and tools for supporting the data validation and monitoring process.\nData quality management processes include creating controls for validation,\nenabling  quality monitoring and reporting, supporting the triage process for\nassessing the level of incident severity, enabling root cause analysis and recom‐\nmendation of remedies to data issues, and data incident tracking. The right pro‐\ncesses for data quality management will provide measurably trustworthy data for\nanalysis.\nData access management\nThere are two aspects of governance for data access. The first aspect is the provi‐\nsioning of access to available assets. It’s important to provide data services that\nallow data consumers to access their data, and fortunately, most cloud platforms\nprovide methods for developing data services. The second aspect is prevention of\nimproper or unauthorized access. It’s important to define identities, groups, and\nroles and assign access rights to establish a level of managed access. This best\npractice involves managing access services as well as interoperating with the\ncloud provider’s identity and access management (IAM) services by defining\nroles, specifying access rights, and managing and allocating access keys to ensure\nthat only authorized and authenticated individuals and systems are able to access\ndata assets according to defined rules.\nAuditing\nOrganizations must be able to assess their systems to make sure that they are\nworking as designed. Monitoring, auditing, and tracking (who did what and\nwhen and with what information) helps security teams gather data, identify\nthreats, and act on those threats before they result in business damage or loss. It’s\nimportant to perform regular audits to check the effectiveness of controls in\norder to quickly mitigate threats and evaluate overall security health.\nData protection\nDespite the efforts of information technology security groups to establish perim‐\neter security as a way to prevent unauthorized individuals from accessing data,\nperimeter security is not and never has been sufficient for protecting sensitive\ndata. While you might be successful in preventing someone from breaking into\nyour system, you are not protected from an insider security breach or even from\nexfiltration (data theft). It’s important to institute additional methods of data pro‐\ntection—including encryption at rest, encryption in transit, data masking, and\npermanent deletion—to ensure that exposed data cannot be read.\nPreface | xv",12477
04-Who Is This Book For.pdf,04-Who Is This Book For,"Operationalizing Data Governance in Your Organization\nTechnology certainly helps support the data governance principles presented in the\npreceding section, but data governance goes beyond the selection and implementa‐\ntion of products and tools. The success of a data governance program depends on a\ncombination of:\n•People  to build the business case, develop the operating model, and take on\nappropriate roles\n•Processes  that operationalize policy development, implementation, and\nenforcement\n•Technology  used to facilitate the ways that people execute those processes\nThe following steps are critical in planning, launching, and supporting a data gover‐\nnance program:\n1.Build the business case . Establish the business case by identifying critical busi‐\nness drivers to justify the effort and investment associated with data governance.\nOutline perceived data risks (such as the storage of data on cloud-based plat‐\nforms) and indicate how data governance helps the organization mitigate those\nrisks.\n2.Document guiding principles . Assert core principles associated with governance\nand oversight of enterprise data. Document those principles in a data governance\ncharter to present to senior management.\n3.Get management buy-in . Engage data governance champions and get buy-in\nfrom the key senior stakeholders. Present your business case and guiding princi‐\nples to C-level management for approval.\n4.Develop an operating model . Once you have management approval, define the\ndata governance roles and responsibilities, and then describe the processes and\nprocedures for the data governance council and data stewardship teams who will\ndefine processes for defining and implementing policies as well as reviewing and\nremediating identified data issues.\n5.Establish a framework for accountability . Establish a framework for assigning\ncustodianship and responsibility for critical data domains. Make sure there is vis‐\nibility to the “data owners” across the data landscape. Provide a methodology to\nensure that everyone is accountable for contributing to data usability.\n6.Develop taxonomies and ontologies . There may be a number of governance\ndirectives associated with data classification, organization, and—in the case of\nsensitive information—data protection. To enable your data consumers to com‐\nply with those directives, there must be a clear definition of the categories (for\norganizational structure) and classifications (for assessing data sensitivity).\nxvi | Preface\n7.Assemble the right technology stack . Once you’ve assigned data governance\nroles to your staff and defined and approved your processes and procedures, you\nshould assemble a suite of tools that facilitate ongoing validation of compliance\nwith data policies and accurate compliance reporting.\n8.Establish education and training . Raise awareness of the value of data gover‐\nnance by developing educational materials highlighting data governance practi‐\nces and procedures, and the use of supporting technology. Plan for regular\ntraining sessions to reinforce good data governance practices.\nThe Business Benefits  of Robust Data Governance\nData security, data protection, data accessibility and usability, data quality, and other\naspects of data governance will continue to emerge and grow as critical priorities for\norganizations. And as more organizations migrate their data assets to the cloud, the\nneed for auditable practices for ensuring data utility will also continue to grow. To\naddress these directives, businesses should frame their data governance practices\naround three key components:\n•A framework that enables people  to define, agree to, and enforce data policies\n•Effective processes  for control, oversight, and stewardship over all data assets\nacross on-premises systems, cloud storage, and data warehouse platforms\n•The right tools and technologies  for operationalizing data policy compliance\nWith this framework in mind, an effective data governance strategy and operating\nmodel provides a path for organizations to establish control and maintain visibility\ninto their data assets, providing a competitive advantage over their peers. Organiza‐\ntions will likely reap immense benefits as they promote a data-driven culture within\ntheir organizations—specifically:\nImproved decision making\nBetter data discovery means that users can find the data they need when they\nneed it, which makes them more efficient. Data-driven decision making plays a\nhuge role in improving business planning within an organization.\nBetter risk management\nA good data governance operating model helps organizations audit their pro‐\ncesses more easily so that they reduce the risk of fines, increase customer trust,\nand improve operations. Downtime can be minimized while productivity still\ngrows.\nPreface | xvii",4866
05-OReilly Online Learning.pdf,05-OReilly Online Learning,"Regulatory compliance\nIncreasing governmental regulation has made it even more important for organi‐\nzations to establish data governance practices. With a good data governance\nframework, organizations can embrace the changing regulatory environment\ninstead of simply reacting to it.\nAs you migrate more of your data to the cloud, data governance provides a level of\nprotection against data misuse. At the same time, auditable compliance with defined\ndata policies helps demonstrate to your customers that you protect their private\ninformation, alleviating their concerns about information risks.\nWho Is This Book For?\nThe current growth in data is unprecedented and, when coupled with increased regu‐\nlations and fines, has meant that organizations are forced to look into their data gov‐\nernance plans to make sure that they do not become the next statistic. Therefore,\nevery organization will need to establish an understanding of the data it collects, the\nliability and regulation associated with that data, and who has access to it. This book\nis for you if you want to know what that entails, the risks to be aware of, and the con‐\nsiderations to keep in mind.\nThis book is for anyone who needs to implement the processes or technology that\nenables data to become trustworthy. This book covers the ways that people, processes,\nand technology can work together to enable auditable compliance with defined and\nagreed-upon data policies.\nThe benefits of data governance are multifaceted, ranging from legal and regulatory\ncompliance to better risk management and the ability to drive top-line revenue and\ncost savings by creating new products and services. Read this book to learn how to\nestablish control and maintain visibility into your data assets, which will provide you\nwith a competitive advantage over your peers.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program ele‐\nments such as variable or function names, databases, data types, environment\nvariables, statements, and keywords.\nxviii | Preface",2259
06-What Data Governance Involves.pdf,06-What Data Governance Involves,"This element signifies a tip or suggestion.\nThis element signifies a general note.\nThis element indicates a warning or caution.\nO’Reilly Online Learning\nFor more than 40 years, O’Reilly Media  has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nOur unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com .\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. Y ou can access this page at https://oreil.ly/data-governance-TDG .\nPreface | xix\nEmail bookquestions@oreilly.com  to comment or ask technical questions about this\nbook.\nFor news and information about our books and courses, visit http://oreilly.com .\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\nAcknowledgments\nThank you to our respective families, teammates, and managers. Gary O’Brien, our\neditor at O’Reilly, was a force of nature—this book would not exist without his con‐\nstant prodding and invaluable advice. Thanks also to our technical reviewers for their\ninvaluable suggestions.\nxx | Preface\nCHAPTER 1\nWhat Is Data Governance?\nData governance is, first and foremost, a data management function to ensure the\nquality, integrity, security, and usability of the data collected by an organization. Data\ngovernance needs to be in place from the time a factoid of data is collected or gener‐\nated until the point in time at which that data is destroyed or archived. Along the way\nin this full life cycle of the data, data governance focuses on making the data available\nto all stakeholders in a form that they can readily access. In addition, it must be one\nthey can use in a manner that generates the desired business outcomes (insights, anal‐\nysis) and conforms to regulatory standards, if relevant. These regulatory standards\nare often an intersection of industry (e.g., healthcare), government (e.g., privacy), and\ncompany (e.g., nonpartisan) rules and codes of behavior. Moreover, data governance\nneeds to ensure that the stakeholders get a high-quality integrated view of all the data\nwithin the enterprise. There are many facets to high-quality data—the data needs to\nbe correct, up to date, and consistent. Finally, data governance needs to be in place to\nensure that the data is secure, by which we mean that:\n•It is accessed only by permitted users in permitted ways\n•It is auditable, meaning all accesses, including changes, are logged\n•It is compliant with regulations\nThe purpose of data governance is to enhance trust in the data. Trustworthy data is\nnecessary to enable users to employ enterprise data to support decision making, risk\nassessment, and management using key performance indicators (KPIs). Using data,\nyou can increase confidence in the decision-making process by showing supporting\nevidence. The principles of data governance are the same regardless of the size of the\nenterprise or the quantity of data. However, data governance practitioners will make\nchoices with regard to tools and implementation based on practical considerations\ndriven by the environment within which they operate.\n1\n1Leandro DalleMule and Thomas H. Davenport, “What’s Y our Data Strategy?”  Harvard Business Review  (May–\nJune 2017): 112–121.What Data Governance Involves\nThe advent of big data analytics, powered by the ease of moving to the cloud and the\never-increasing capability and capacity of compute power, has motivated and ener‐\ngized a fast-growing community of data consumers to collect, store, and analyze data\nfor insights and decision making. Nearly every computer application these days is\ninformed by business data. It is not surprising, therefore, that new ideas inevitably\ninvolve the analysis of existing data in new ways, as well as the collection of new data‐\nsets, whether through new systems or by purchase from external vendors. Does your\norganization have a mechanism to vet new data analysis techniques and ensure that\nany data collected is stored securely, that the data collected is of high quality, and that\nthe resulting capabilities accrue to your brand value? While it’s tempting to look only\ntoward the future power and possibilities of data collection and big data analytics,\ndata governance is a very real and very important consideration that cannot be\nignored. In 2017, Harvard Business Review  reported that more than 70% of employees\nhave access to data they should not.1 This is not to say that companies should adopt a\ndefensive posture; it’s only to illustrate the importance of governance to prevent data\nbreaches and improper use of the data. Well-governed data can lead to measurable\nbenefits for an organization.\nSpotify Creates Discover Weekly\nAs an example of how well-governed data can lead to measurable benefits to an orga‐\nnization and how the availability of data can completely change an entire industry,\nconsider the Spotify Discover Weekly feature. In the early 2010s, the way most people\nlistened to music was to purchase physical/digital albums and rip them to create cus‐\ntom playlists. These playlists, consisting of songs you owned, was what you listened\nto.\nThere was also a large and thriving illegal music-sharing ecosystem that consisted of\npirated singles that could be added to your playlists. In an effort to get the pirated\nmusic system under control, music labels allowed the sales of digital singles. As the\nsize of people’s digital libraries grew, and as internet connections became more relia‐\nble, consumers became willing to keep their purchased tracks online and stream them\nto their audio devices. Music labels were also willing to “rent” out music when it was\nstreamed. Instead of selling the song, the music labels would be paid each time the\nsong was played.\n2 | Chapter 1: What Is Data Governance?\nThis was how Spotify (which is now the world’s largest music streaming service) got\nstarted. It’s worth noting that Spotify owes its very existence to data governance. It got\nstarted as a way for music labels to get paid for their work—music piracy was deci‐\nmating the music industry. Spotify’s entire business model was built around tracking\nthe songs users played and reimbursing artists for those songs. The ability to prove\nthat its handling of the data was trustworthy is the reason that Spotify became a viable\nmusic service in the first place.\nThe fact that Spotify was keeping tabs on which songs users played meant that it had\ndata on what people listened to. Thus it was now possible for Spotify to recommend\nnew songs to its listeners. Such recommendation algorithms key off of three things:\n•Find other songs by the artists you listen to, or songs in the same genre (e.g.,\n1940s jazz). This is called content-based recommendation .\n•Find users who like the same songs you do and recommend the songs that those\nusers like. This is called collaborative filtering .\n•Use models that analyze the raw audio files of songs you like and recommend\nsongs that are similar. The raw audio captures many inherent features, such as\nthe beat. If you tend to like music with a fast beat and with repeating tonal\nphrases, the algorithm can recommend other songs with a similar structure. This\nis called similarity matching .\nAt that point, Edward Newett, an engineer at Spotify, had an interesting idea : instead\nof recommending songs one at a time, what if Spotify created a playlist of recommen‐\ndations? And so, every Monday, Spotify would recommend what they thought each\nindividual user would like. This was called Discover Weekly.\nDiscover Weekly was a huge hit—within a year after its launch, more than 40 million\npeople had used the service and streamed nearly five billion tracks. The deep person‐\nalization had worked. The music sounded familiar but was still novel. The service\nallowed music lovers to discover new titles, and new artists to find audiences, and it\ngave Spotify’s customers an event to look forward to every week.\nSpotify was able to use its recommendation algorithm to provide artists and music\nlabels with insights about fans’ preferences. It could leverage the recommendation\nalgorithm to expand users’ music preferences and introduce novel bands. This extra\nknowledge and marketing ability has allowed Spotify to negotiate with music publish‐\ners from a position of strength.\nNone of this would have been possible if Spotify had not assured its users that the\ninformation about their listening habits was being used in a responsible way to\nimprove their own music-listening experience. European regulators are very protec‐\ntive of the privacy of EU citizens. Spotify, being based in Europe, could not have got‐\nten its recommendation systems off the ground if it had not proven that it had robust\nprivacy controls in place, and that it was ensuring that data scientists could devise\nalgorithms but not breach data that could be tied to individuals.\nWhat Data Governance Involves | 3",9730
07-Classification and Access Control.pdf,07-Classification and Access Control,"2This application is the Meteorological Phenomena Identification Near the Ground (mPING) Project , devel‐\noped through a partnership between NSSL, the University of Oklahoma, and the Cooperative Institute for\nMesoscale Meteorological Studies.Discover Weekly illustrates how data, properly governed, can create a well-loved\nbrand and change the power dynamic in an entire industry. Spotify extended its rec‐\nommendations with Spotify Wrapped, where listeners everywhere get a deep dive into\ntheir most memorable listening moments of the year. This is a great way to have peo‐\nple remember and share their most listened-to songs and artists (see Figure 1-1 ).\nFigure 1-1. Anita Kibunguchy-Grant’s 2020 Wrapped playlist\nHolistic Approach to Data Governance\nSeveral years ago, when smartphones with GPS sensors were becoming ubiquitous,\none of the authors of this book was working on machine learning algorithms to pre‐\ndict the occurence of hail. Machine learning requires labeled data—something that\nwas in short supply at the temporal and spatial resolution the research team needed.\nOur team hit on the idea of creating a mobile application that would allow citizen sci‐\nentists to report hail at their location.2 This was our first encounter with making\nchoices about what data to collect—until then, we had mostly been at the receiving\n4 | Chapter 1: What Is Data Governance?\n3It was on the radio, but you can read about it on NPR’s All Tech Considered  blog .end of whatever data the National Weather Service was collecting. Considering the\nrudimentary state of information security tools in an academic setting, we decided to\nforego all personally identifying information and make the reporting totally anony‐\nmous, even though this meant that certain types of reported information became\nsomewhat unreliable. Even this anonymous data brought tremendous benefits—we\nstarted to evaluate hail algorithms at greater resolutions, and this improved the qual‐\nity of our forecasts. This new dataset allowed us to calibrate existing datasets, thus\nenhancing the data quality of other datasets as well. The benefits went beyond data\nquality and started to accrue toward trustworthiness—involvement of citizen scien‐\ntists was novel enough that National Public Radio carried a story about the project,\nemphasizing the anonymous nature of the data collection.3 The data governance lens\nhad allowed us to carefully think about which report data to collect, improve the\nquality of enterprise data, enhance the quality of forecasts produced by the National\nWeather Service, and even contribute to the overall brand of our weather enterprise.\nThis combination of effects—regulatory compliance, better data quality, new business\nopportunities, and enhanced trustworthiness—was the result of a holistic approach to\ndata governance.\nFast-forward a few years, and now, at Google Cloud, we are all part of a team that\nbuilds technology for scalable cloud data warehouses and data lakes. One of the\nrecurring concerns that our enterprise customers have is around what best practices\nand policies they should put in place to manage the classification, discovery, availabil‐\nity, accessibility, integrity, and security of their data—data governance—and custom‐\ners approach it with the same sort of apprehension that our small team in academia\ndid.\nY et the tools and capabilities that an enterprise has at its disposal to carry out data\ngovernance are quite powerful and diverse. We hope to convince you that you should\nnot be afraid of data governance, and that properly applying data governance can\nopen up new worlds of possibility. While you might initially approach data gover‐\nnance purely from a legal or regulatory compliance standpoint, applying governance\npolicies can drive growth and lower costs.\nEnhancing Trust in Data\nUltimately, the purpose of data governance is to build trust in data. Data governance\nis valuable to the extent that it adds to stakeholders’ trust in the data—specifically, in\nhow that data is collected, analyzed, published, or used.\nEnsuring trust in data requires that a data governance strategy address three key\naspects: discoverability , security , and accountability  (see Figure 1-2 ). Discoverability\nWhat Data Governance Involves | 5\nitself requires data governance to make technical metadata, lineage information, and\na business glossary readily available. In addition, business critical data needs to be\ncorrect and complete. Finally, master data management is necessary to guarantee that\ndata is finely classified and thus ensure appropriate protection against inadvertent or\nmalicious changes or leakage. In terms of security, regulatory compliance, manage‐\nment of sensitive data (personally identifiable information, for example), and data\nsecurity and exfiltration prevention may all be important depending on the business\ndomain and the dataset in question. If discoverability and security are in place, then\nyou can start treating the data itself as a product. At that point, accountability\nbecomes important, and it is necessary to provide an operating model for ownership\nand accountability around boundaries of data domains.\nFigure 1-2. The three key aspects of data governance that must be addressed to enhance\ntrust in data\nClassification  and Access Control\nWhile the purpose of data governance is to increase the trustworthiness of enterprise\ndata so as to derive business benefits, it remains the case that the primary activity\nassociated with data governance involves classification and access control. Therefore,\nto understand the roles involved in data governance, it is helpful to consider a typical\nclassification and access control setup.\nLet’s take the case of protecting the human resources information of employees, as\nshown in Figure 1-3 .\n6 | Chapter 1: What Is Data Governance?\nFigure 1-3. Protecting the human resources information of employees\nThe human resources information includes several data elements: each employee’s\nname, their date of hire, past salary payments, the bank account into which those sal‐\nary payments were deposited, current salary, etc. Each of these data elements is pro‐\ntected in different ways, depending on the classification level. Potential classification\nlevels might be public (things accessible by people not associated with the enterprise),\nexternal (things accessible by partners and vendors with authorized access to the\nenterprise internal systems), internal (things accessible by any employee of the orga‐\nnization), and restricted. For example, information about each employee’s salary pay‐\nments and which bank account they were deposited into would be restricted to\nmanagers in the payroll processing group only. On the other hand, the restrictions\ncould be more dynamic. An employee’s current salary might be visible only to their\nmanager, and each manager might be able to see salary information only for their\nrespective reports. The access control policy would specify what users can do when\nthey access the data—whether they can create a new record, or read, update, or delete\nexisting records.\nThe governance policy is typically specified by the group that is accountable for the\ndata (here, the human resources department)—this group is often referred to as the\ngovernors . The policy itself might be implemented by the team that operates the data‐\nbase system or application (here, the information technology department), and so\nchanges such as adding users to permitted groups are often carried out by the IT\nteam—hence, members of that team are often referred to as approvers  or data stew‐\nards. The people whose actions are being circumscribed or enabled by data gover‐\nnance are often referred to as users . In businesses where not all employees have access\nWhat Data Governance Involves | 7",7903
08-Why Data Governance Is Becoming More Important.pdf,08-Why Data Governance Is Becoming More Important,"to enterprise data, the set of employees with access might be called knowledge workers\nto differentiate them from those without access..\nSome enterprises default to open —for example, when it comes to business data, the\ndomain of authorized users may involve all knowledge workers in the enterprise.\nOther enterprises default to closed —business data may be available only to those with\na need to know. Policies such as these are within the purview of the data governance\nboard in the organization—there is no uniquely correct answer on which approach is\nbest.\nData Governance Versus Data Enablement and Data Security\nData governance is often conflated with data enablement and with data security.\nThose topics intersect but have different emphases:\n•Data governance is mostly focused on making data accessible, reachable, and\nindexed for searching across the relevant constituents, usually the entire organi‐\nzation’s knowledge-worker population. This is a crucial part of data governance\nand will require tools such as a metadata index, a data catalog to “shop for” data.\nData governance extends data enablement into including a workflow  in which\ndata acquisition can take place. Users can search for data by context and descrip‐\ntion, find the relevant data stores, and ask for access, including the desired use\ncase as justification. An approver (data steward) will need to review the ask,\ndetermine whether the ask is justified and whether the data being requested can\nactually serve the use case, and kick off a process through which the data can be\nmade accessible.\n•Data enablement goes further than making data accessible and discoverable; it\nextends into tooling that allows rapid analysis and processing of the data to\nanswer business-related questions: “how much is the business spending on this\ntopic, ” “can we optimize this supply chain, ” and so on. The topic is crucial and\nrequires knowledge of how to work with data, as well as what the data actually\nmeans —best addressed by including, from the get-go, metadata that describes the\ndata and includes value proposition, origin, lineage, and a contact person who\ncurates and owns the data in question, to allow for further inquiry.\n•Data security, which intersects with both data enablement and data governance,\nis normally thought about as a set of mechanics put in place to prevent and block\nunauthorized access. Data governance relies on data security mechanics to be in\nplace but goes beyond just prevention of unauthorized access and into policies\nabout the data itself, its transformation according to data class (see Chapter 7 ),\nand the ability to prove that the policies set to access and transform the data over\ntime are being complied with. The correct implementation of security mechanics\npromotes the trust required to share data broadly or “democratize access” to the\ndata.\n8 | Chapter 1: What Is Data Governance?",2934
09-The Number of People Working andor Viewing the Data Has Grown Exponentially.pdf,09-The Number of People Working andor Viewing the Data Has Grown Exponentially,"4David Reinsel, John Gantz, and John Rydning, “The Digitization of the World: From Edge to Core” , Novem‐\nber 2018.Why Data Governance Is Becoming More Important\nData governance has been around since there was data to govern, although it was\noften restricted to IT departments in regulated industries, and to security concerns\naround specific datasets such as authentication credentials. Even legacy data process‐\ning systems needed a way to not only ensure data quality but also control access to\ndata.\nTraditionally, data governance was viewed as an IT function that was performed in\nsilos related to data source type. For example, a company’s HR data and financial\ndata, typically highly controlled data sources with strictly controlled access and spe‐\ncific usage guidelines, would be controlled by one IT silo, whereas sales data would be\nin a different, less restrictive silo. Holistic or “centralized” data governance may have\nexisted within some organizations, but the majority of companies viewed data gover‐\nnance as a departmental concern.\nData governance has come into prominence because of the recent introductions of\nGDPR-  and CCPA-type  regulations that affect every industry, beyond just healthcare,\nfinance, and a few other regulated industries. There has also been a growing realiza‐\ntion about the business value of data. Because of this, the data landscape is vastly dif‐\nferent today.\nThe following are just a few ways in which the topography has changed over time,\nwarranting very different approaches to and methods for data governance.\nThe Size of Data Is Growing\nThere is almost no limit to the kinds and amount of data that can now be collected. In\na whitepaper published in November 2018, International Data Corporation  predicts\nthat the global datasphere will balloon to 175 ZB by 2025 (see Figure 1-4 ).4\nThis rise in data captured via technology, coupled with predictive analyses, results in\nsystems nearly knowing more about today’s users than the users themselves.\nWhy Data Governance Is Becoming More Important | 9",2076
10-Methods of Data Collection Have Advanced.pdf,10-Methods of Data Collection Have Advanced,"5“The Best Jobs in the US: 2019” , Indeed, March 19, 2019.\n6Reinsel et al. “The Digitization of the World. ”\nFigure 1-4. The size of the global datasphere is expected to exhibit dramatic growth\nThe Number of People Working and/or Viewing the Data Has Grown\nExponentially\nA report by Indeed shows that the demand for data science jobs had jumped 78%\nbetween 2015 and 2018.5 IDC also reports that there are now over five billion people\nin the world interacting with data, and it projects this number to increase to six bil‐\nlion (nearly 75% of the world’s population) in 2025. Companies are obsessed with\nbeing able to make “data-driven decisions, ” requiring an inordinate amount of head‐\ncount: from the engineers setting up data pipelines to analysts doing data curation\nand analyses, and business stakeholders viewing dashboards and reports. The more\npeople working and viewing data, the greater the need for complex systems to man‐\nage access, treatment, and usage of data because of the greater chance of misuse of the\ndata.\nMethods of Data Collection Have Advanced\nNo longer must data only be batch processed and loaded for analysis. Companies are\nleveraging real-time or near real-time streaming data and analytics to provide their\ncustomers with better, more personalized engagements. Customers now expect to\naccess products and services wherever they are, over whatever connection they have,\nand on any device. IDC predicts that this infusion of data into business workflows\nand personal streams of life will result in nearly 30% of the global datasphere to be\nreal-time by 2025, as shown in Figure 1-5 .6\n10 | Chapter 1: What Is Data Governance?\nFigure 1-5. More than 25% of the global datasphere will be real-time data by 2025\nThe advent of streaming, however, while greatly increasing the speed to analytics, also\ncarries with it the potential risk of infiltration, bringing about the need for complex\nsetup and monitoring for protection.\nAdvanced Data Collection in Sports\nIt used to be that when you talked about sport statistics, you were talking about rela‐\ntively coarse-grained data—things like wins and losses. In some sports, you might\nhave information about a player’s performance (for example, average number of runs\na cricketer gets per inning). However, the quantity and type of data collected in sports\nhas changed dramatically because teams are looking to better understand what levers\nthey can pull to be successful in these highly competitive fields.\nIt’s therefore no surprise that the National Football League (NFL) wanted to better\nquantify the value of specific plays and actions within a play, which is why it started\nusing league-wide analytics in 2015. If you’re not familiar with American football, it is\na complex sport, primarily governed by the NFL. The NFL is a professional American\nfootball league consisting of 32 teams that are divided equally between the National\nFootball Conference (NFC) and the American Football Conference (AFC).\nTraditional metrics such as “yards per carry” or “total rushing yards” can be flawed;\nrecognizing a need to grow its analytics and data collection process, the NFL created \nNext Gen Stats (NGS). NGS is a league program wherein the pads of every player and\nofficial, along with game balls, pylons, and first down chains are all tagged with a\nradio frequency identification (RFID) chip, which allows for a very robust set of\nWhy Data Governance Is Becoming More Important | 11\nstatistics  after every game. These statistics constitute real-time data for every player\non every play, in every situation, anywhere on the field, including location, speed,\nacceleration, and velocity ( Figure 1-6 ).\nFigure 1-6. An example of analysis and visualization of the NFL data in Kaggle. Note‐\nbook by Rob Mulla\nThe types of questions the NFL wanted an answer to included, for example, “What\ncontributes to a successful run play?” It wanted to know whether success depends\nmostly on the ball carrier who takes the hand off, or on their teammates (by way of\nblocking), or on the coach (by way of the play call). And could the data even show\nwhat part was played by the opposing defense and the actions it took? The NFL\nwould also want to predict how many yards a team might gain on given rushing plays\n12 | Chapter 1: What Is Data Governance?",4362
11-The Use Cases for Data Have Expanded.pdf,11-The Use Cases for Data Have Expanded,"7Kaggle: NFL Big Data Bowl .\n8Reinsel et al. “The Digitization of the World. ”as they happen. Deeper insight into rushing plays ultimately helps teams, media, and\nfans better understand the skillsets of players and the strategies of coaches.\nBecause of this, each year the league hosts the NFL ’s Big Data Bowl, a sports analytics\ncontest that challenges talented members of the analytics community—from college\nstudents to professionals—to contribute to the NFL ’s continuing evolution in the use\nof advanced analytics.7 Contestants are asked to analyze and rethink trends and player\nperformance in order to innovate on the way football is played and coached.\nThis NFL example showcases just how much the methods of data collection have\nadvanced. It’s no surprise that this is what’s really accelerating the amount of data\nbeing generated in the global datasphere.\nMore Kinds of Data (Including More Sensitive Data) Are Now\nBeing Collected\nIt’s projected that by 2025 every person using technology and generating data will\nhave more than 4,900 digital data engagements per day; that’s about about one digital\ninteraction every eighteen seconds (see Figure 1-7 ).8\nFigure 1-7. By 2025, a person will interact with data-creating technology more than\n4,900 times a day\nMany of those interactions will include the generation and resulting collection of a\nmyriad of sensitive data such as social security numbers, credit card numbers, names,\naddresses, and health conditions, to name a few categories. The proliferation of the\ncollection of these extremely sensitive types of data carries with it great customer\n(and regulator) concern about how that data is used and treated, and who gets to view\nit.\nWhy Data Governance Is Becoming More Important | 13\nThe Use Cases for Data Have Expanded\nCompanies are striving to use data to make better business decisions, coined data-\ndriven decision making . They not only are using data internally to drive day-to-day\nbusiness execution, but are also using data to help their customers  make better deci‐\nsions. Amazon is an example of a company doing this via collecting and analyzing\nitems in customers’ past purchases, items the customers have viewed, and items in\ntheir virtual shopping carts, as well as the items they’ve ranked/reviewed after pur‐\nchase, to drive targeted messaging and recommendations for future purchases.\nWhile this Amazon use case makes perfect business sense, there are types of data\n(sensitive) coupled with specific use cases for that data that are not appropriate (or\neven legal). For sensitive types of data, it matters not only how that data is treated but\nalso how it’s used. For example, employee data may be used/viewed internally by a\ncompany’s HR department, but it would not be appropriate for that data to be used/\nviewed by the marketing department.\nUse of Data to Make Better Decisions\nMany of us enjoy the suggestions our friends and family make for books, movies,\nmusic, and so on. Have you noticed how some of these suggestions pop up while you\nare searching for something online? Here we would like to highlight a few examples\nthat show how use of data resulted in better decisions and better business outcomes.\nAn example from Safari Books Online  shows the use of modern analytics tools to\nachieve improved sales. Safari Books Online is known for its very large and diverse\ncustomer base and for having more than 30,000 books and videos accessible from\nvarious platforms. Safari Books Online wanted to unlock value from its massive\namounts of usage data, user search results, and trends, and to connect all this data to\ndrive better sales intelligence and higher sales.\nThe key was to achieve this in near real-time—after all, none of us likes to wait 10\nminutes for the results of an online search. In order to provide real-time insights, the\nSafari Books Online team routinely transferred the usage data corresponding to the\ncontent delivery network (CDN) to a cloud native data warehouse to make the infor‐\nmation available outside the original silo (see Figure 1-8 ).\nThe Safari Books Online team wanted to drill into the data, deliver various dash‐\nboards, provide a better user experience, and deliver faster ad hoc queries. Using the\nnew analytics made supporting users much quicker and simpler and achieved higher\ncustomer satisfaction. This was because the team could get to relevant information\nabout users (such as their IP addresses, or the title of a book they were querying) in\nnear real-time.\n14 | Chapter 1: What Is Data Governance?\nFigure 1-8. Bringing the usage data from content delivery network (CDN) and web\napplication logs into a smart data analytics platform to achieve better data-driven\ndecisions\nAchieving better sales intelligence was among the most important use cases for Safari\nBooks Online when it began its journey into data-driven decision making. All the\ndata it had that was once buried, or not even available from its web logs, became sales\nleads. Assessment of interest among likely readers was integrated into the CRM sys‐\ntem and quickly turned into actionable information (see Figure 1-9 ).\nFigure 1-9. A dashboard that is used to monitor titles and trends\nAnother good example  is from California Design Den , which transformed its\ndecision-making process with data on pricing and inventory management. By leaning\non smart analytics platforms, they were able to achieve much faster pricing decisions,\nsell their inventory, and achieve better profitability.\nThe ability to aggregate different types of data for decision making (while balancing\nwhich data to retain and which to get rid of) is key. Not all data can be valuable for\nbetter decision making. It is equally important to guard against biases when you are\ntrying to establish your data-driven decision-making process. Y ou need to define your\nWhy Data Governance Is Becoming More Important | 15",5959
12-New Regulations and Laws Around the Treatment of Data.pdf,12-New Regulations and Laws Around the Treatment of Data,,0
13-Improving Data Quality.pdf,13-Improving Data Quality,"9Aarian Marshall and Alex Davies, “Uber’s Self-Driving Car Saw the Woman It Killed, Report Says” , Wired ,\nMay 24, 2018.objectives—maybe set some easy-to-measure goals at first—and create a list of high-\nvalue questions  to which you want to elicit answers. It is OK to go back and revisit\nyour starting point, objectives, and metrics. The answers are to be found in your data,\nbut looking at it from different perspectives will help you decide which data is\nrelevant.\nThe world is your oyster when it comes to gaining insight from your data. Asking the\nhigh-value questions to obtain deeper insights is the essential part of the value chain\nwhen it comes to data-driven decision making. Whether you want to drive sales intel‐\nligence and increase your revenue, improve support and customer experience, or\ndetect malicious use to prevent operational issues, data-driven decision making is key\nfor any business and any operation, and there are various smart tools out there to\nhelp you start taking advantage of your invaluable data.\nNew Regulations and Laws Around the Treatment of Data\nThe increase in data and data availability has resulted in the desire and need for regu‐\nlations on data, data collection, data access, and data use. Some regulations that have\nbeen around for quite some time—for example, the Health Insurance Portability and\nAccountability Act of 1996 (HIPAA), the law protecting the collection and use of per‐\nsonal health data—not only are well known, but companies that have had to comply\nwith them have been doing so for decades—meaning their processes and methodol‐\nogy for treatment of this sensitive data are fairly sophisticated. New regulations, such\nas the EU’s General Data Protection Regulation (GDPR) and the California Con‐\nsumer Privacy Act (CCPA) in the US, are just two examples of usage and collection\ncontrols that apply to myriad companies, for many of whom such governance of data\nwas not baked into their original data architecture strategy. Because of this, compa‐\nnies that have not had to worry about regulatory compliance before have a more dif‐\nficult time modifying their technology and business processes to maintain\ncompliance with these new regulations.\nEthical Concerns Around the Use of Data\nWhile use cases themselves can fit into the category of ethical use of data, new tech‐\nnology around machine learning and artificial intelligence has spawned new concerns\naround the ethical use of data.\nOne recent example from 2018 is that of Elaine Herzberg, who, while wheeling her\nbike across a street in Tempe, Arizona, was struck and killed by a self-driving car.9\nThis incident raised questions about responsibility. Who was responsible for Elaine’s\n16 | Chapter 1: What Is Data Governance?\n10Jonathan Shaw, “ Artificial Intelligence and Ethics” , Harvard Magazine , January–February 2019, 44-49, 74.death? The person in the driver’s seat? The company testing the car’s capabilities? The\ndesigners of the AI system?\nWhile not deadly, consider the following additional examples:\n•In 2014, Amazon developed a recruiting tool for identifying software engineers it\nmight want to hire; however, it was found that the tool discriminated against\nwomen. Amazon eventually had to abandon the tool in 2017.\n•In 2016, ProPublica analyzed a commercially developed system that was created\nto help judges make better sentencing decisions by predicting the likelihood that\ncriminals would reoffend, and it found that it was biased against Black people.10\nIncidents such as these are enormous PR nightmares for companies.\nConsequently, regulators have published guidelines on the ethical use of data. For\nexample, EU regulators published a set of seven requirements  that must be met for AI\nsystems to be considered trustworthy:\n•AI systems should be under human oversight.\n•They need to have a fall-back plan in case something goes wrong. They also need\nto be accurate, reliable, and reproducible.\n•They must ensure full respect for privacy and data protection.\n•Data, system, and AI business models should be transparent and offer\ntraceability.\n•AI systems must avoid unfair bias.\n•They must benefit all human beings.\n•They must ensure responsibility and accountability.\nHowever, the drive for data-driven decisions, fueled by more data and robust analyt‐\nics, calls for a necessary consideration of and focus on the ethics of data and data use\nthat goes beyond these regulatory requirements.\nExamples of Data Governance in Action\nThis section takes a closer look at several enterprises and how they were able to derive\nbenefits from their governance efforts. These examples demonstrate that data gover‐\nnance is being used to manage accessibility and security, that it addresses the issue of\ntrust by tackling data quality head-on, and that the governance structure makes these\nendeavors successful.\nExamples of Data Governance in Action | 17\n11“Information on the Capital One Cyber Incident” , Capital One, updated September 23, 2019; Brian Krebs,\n“What We Can Learn from the Capital One Hack” , Krebs on Security  (blog), August 2, 2019.Managing Discoverability, Security, and Accountability\nIn July 2019, Capital One, one of the largest issuers of consumer and small business\ncredit cards, discovered that an outsider had been able to take advantage of a miscon‐\nfigured web application firewall in its Apache web server. The attacker was able to\nobtain temporary credentials and access files containing personal information for\nCapital One customers.11 The resulting leak of information affected more than 100\nmillion individuals who had applied for Capital One credit cards.\nTwo aspects of this leak limited the blast radius. First, the leak was of application data\nsent to Capital One, and so, while the information included names, social security\nnumbers, bank account numbers, and addresses, it did not include log-in credentials\nthat would have allowed the attacker to steal money. Second, the attacker was swiftly\ncaught by the FBI, and the reason for the attacker being caught is why we include this\nanecdote in this book.\nBecause the files in question were stored in a public cloud storage bucket where every\naccess to the files was logged, access logs were available to investigators after the fact.\nThey were able to figure out the IP routes and narrow down the source of the attack\nto a few houses. While misconfigured IT systems that create security vulnerabilities\ncan happen anywhere, attackers who steal admin credentials from on-premises sys‐\ntems will usually cover their tracks by modifying the system access logs. On the pub‐\nlic cloud, though, these access logs are not modifiable because the attacker doesn’t\nhave access to them.\nThis incident highlights a handful of lessons:\n•Make sure that your data collection is purposeful. In addition, store as narrow a\nslice of the data as possible. It was fortunate that the data store of credit card\napplications did not also include the details of the resulting credit card accounts.\n•Turn on organizational-level audit logs in your data warehouse. Had this not\nbeen done, it would not have been possible to catch the culprit.\n•Conduct periodic security audits of all open ports. If this is not done, no alerts\nwill be raised about attempts to get past security safeguards.\n•Apply an additional layer of security to sensitive data within documents. Social\nsecurity numbers, for example, should have been masked or tokenized using an\nartificial intelligence service capable of identifying PII data and redacting it.\nThe fourth best practice is an additional safeguard—arguably, if only absolutely nec‐\nessary data is collected and stored, there would be no need for masking. However,\n18 | Chapter 1: What Is Data Governance?\n12See, for example, the book Dark Data: Why What You Don’t Know Matters  by David Hand (Princeton Univer‐\nsity Press).most organizations have multiple uses of the data, and in some use cases, the decryp‐\nted social security number might be needed. In order to do such multi-use effectively,\nit is necessary to tag or label each attribute based on multiple categories to ensure the\nappropriate controls and security are placed on it. This tends to be a collaborative\neffort among many organizations within the company. It is worth noting that systems\nlike these that remove data from consideration come with their own challenges and\nrisks.12\nAs the data collected and retained by enterprises has grown, ensuring that best practi‐\nces like these are well understood and implemented correctly has become more and\nmore important. Such best practices and the policies and tools to implement them are\nat the heart of data governance.\nImproving Data Quality\nData governance is not just about security breaches. For data to be useful to an orga‐\nnization, it is necessary that the data be trustworthy. The quality of data matters, and\nmuch of data governance focuses on ensuring that the integrity of data can be trusted\nby downstream applications. This is especially hard when data is not owned by your\norganization and when that data is moving around.\nA good example of data governance activities improving data quality comes from the\nUS Coast Guard (USCG). The USCG focuses on maritime search and rescue, ocean\nspill cleanup, maritime safety, and law enforcement. Our colleague Dom Zippilli was\npart of the team that proved the data governance concepts and techniques behind\nwhat became known as the Authoritative Vessel Identification Service (AVIS). The\nfollowing sidebar about AVIS is in his words.\nHow the US Coast Guard Improved Data Quality\nDom Zippilli\nFigure 1-10  illustrates what AVIS looked like when looking at a vessel with no data\ndiscrepancies. The data from Automatic Identification Systems (AIS) corresponds\nwell with what was in AVIS, which is best described as “what we think we know”\nabout a ship—an amalgam of information from other USCG systems that handled\nvessel registration, integration with the International Maritime Organization (IMO),\ncitations, and so on.\nExamples of Data Governance in Action | 19\nFigure 1-10. What AVIS looked like. Figure courtesy of NAVCEN.\nUnfortunately, not all data corresponded this cleanly. Figure 1-11  illustrates a patho‐\nlogical case: no ship image, mismatched name, mismatched Maritime Mobile Service\nIdentity (MMSI), mismatched IMO number, mismatched everything.\nFigure 1-11. A pathological case with a lot of inconsistencies between what’s tracked in\nAIS and what’s known from an amalgam of other sources. Figure courtesy of NAVCEN.\nSuch mismatches make knowing which ships are where, and information about those\nships, much harder to figure out for USCG in the field. The vessels that cropped up in\n20 | Chapter 1: What Is Data Governance?\nthe AVIS UI were the ones that couldn’t be resolved using automated tooling and thus\nrequired some human intervention. Automating is nice (and this was almost 10 years\nago), but even surfacing the work that had to be done by a human was a huge  step\nforward. In almost all cases, the issues were the result of innocent mistakes, but get‐\nting things back on track required identifying the issues and reaching out to the mari‐\ntime community.\nThe business value of such corrections comes down to maritime domain awareness\n(MDA), a key part of the USCG’s mission. Domain awareness is pretty hard to come\nby when your data quality is poor. Here are some qualitative examples of how AVIS\nhelped.\nFor example, imagine a scenario in which a vessel needs to be investigated for some\nkind of violation, or interdicted for any reason. If that vessel is among many broad‐\ncasting with the same MMSI number, our track for that vessel looks like Figure 1-12 .\nThis could be even more serious in a search and rescue situation in which we need to\nlocate nearby vessels that could render aid faster than a USCG vessel (cooperation is a\nmajor theme of maritime life).\nFigure 1-12. Effects  of duplicate vehicle id numbers. Figure courtesy of NAVCEN.\nOver time, in the pilot program, as shown in Figure 1-13 , we saw a drastic reduction\nin the number of ambiguous vessel tracks received each day. While zero was always\nthe goal, this is by nature a community effort, so it requires constant upkeep.\nExamples of Data Governance in Action | 21\n13David Winkler, “ AIS Data Quality and the Authoritative Vessel Identification Service (AVIS)”  (PowerPoint\npresentation, National GMDSS Implementation Task Force, Arlington, V A, January 10, 2012).\nFigure 1-13. Improvements in data quality due to pilot program to correct vessel IDs\nThe closest I have to a quantitative result (though it doesn’t spell out the mission\nvalue exactly, as that was expected to be obvious to the reader) is this highlight from a\nwhitepaper that is unfortunately no longer available publicly:\nOver the course of the project, the AVIS team was able to virtually eliminate uniden‐\ntified and uncorrelated AIS vessel signals broadcasting unregistered MMSI numbers\nsuch as 1, 2, 123456789, etc. Specifically, 863 out of 866 vessels were corrected by\nSeptember 2011, eliminating nearly 100% of incorrect broadcasts.13\n863 might not seem like a lot, but keep in mind the global merchant fleet is something\non the order of 50,000 vessels. So, just for US waters, this is actually a big part of the\npopulation, and as you know, it doesn’t take a lot of bad data to make all the data\nuseless.\nThe USCG program is a handy reminder that data quality is something to strive for\nand constantly be on the watch for. The cleaner the data, the more likely it is to be\nusable for more critical use cases.  In the USCG case, we see this in the usability of the\ndata for search and rescue tasks as well.\n22 | Chapter 1: What Is Data Governance?",13897
14-The Business Value of Data Governance.pdf,14-The Business Value of Data Governance,,0
15-Regulatory Compliance.pdf,15-Regulatory Compliance,"The Business Value of Data Governance\nData governance is not solely a control practice. When implemented cohesively, data\ngovernance addresses the strategic need to get knowledge workers the insights they\nrequire with a clear process to “shop for data. ” This makes possible the extraction of\ninsights from multiple sources that were previously siloed off within different busi‐\nness units.\nIn organizations where data governance is a strategic process, knowledge workers can\nexpect to easily find all the data required to fulfill their mission, safely apply for\naccess, and be granted access to the data under a simple process with clear timelines\nand a transparent approval process. Approvers and governors of data can expect to\neasily pull up a picture of what data is accessible to whom, and what data is “outside”\nthe governance zone of control (and what to do about any discrepancies there). CIOs\ncan expect to be able to review a high-level analysis of the data in the organization in\norder to holistically review quantifiable metrics such as “total amount of data” or\n“data out of compliance” and even understand (and mitigate) risks to the organiza‐\ntion due to data leakage.\nFostering Innovation\nA good data governance strategy, when set in motion, combines several factors that\nallow a business to extract more value from the data. Whether the goal is to improve\noperations, find additional sources of revenue, or even monetize data directly, a data\ngovernance strategy is an enabler of various value drivers in enterprises.\nA data governance strategy, if working well, is a combination of process (to make data\navailable under governance), people (who manage policies and usher in data access\nacross the organization, breaking silos where needed), and tools that facilitate the\nabove by applying machine learning techniques to categorize data and indexing the\ndata available for discovery.\nData governance ideally will allow all employees in the organization to access all data\n(subject to a governance process) under a set of governance rules (defined in greater\ndetail below), while preserving the organization’s risk posture (i.e., no additional\nexposure or risks are introduced due to making data accessible under a governance\nstrategy). Since the risk posture is maintained and possibly even improved with the\nadditional controls data governance brings, one could argue there is only an upside to\nmaking data accessible. Giving all knowledge workers access to data, in a governed\nmanner, can foster innovation by allowing individuals to rapidly prototype answers\nto questions based on the data that exists within the organization. This can lead to\nbetter decision making, better opportunity discovery, and a more productive organi‐\nzation overall.\nThe Business Value of Data Governance | 23\nThe quality of the data available is another way to ascertain whether governance is\nwell implemented in the organization. A part of data governance is a well-understood\nway to codify and inherit a “quality signal” on the data. This signal should tell poten‐\ntial data users and analysts whether the data was curated, whether it was normalized\nor missing, whether corrupt data was removed, and potentially how trustworthy the\nsource for the data is. Quality signals are crucial when making decisions on potential\nuses of the data; for example, within machine learning training datasets.\nThe Tension Between Data Governance and Democratizing\nData Analysis\nVery often, complete data democratization is thought of as conflicting with data gov‐\nernance. This conflict is not necessarily an axiom. Data democratization, in its most\nextreme interpretation, can mean that all analysts or knowledge workers can access\nall data, whatever class it may belong to. The access described here makes a modern\norganization uncomfortable when you consider specific examples, such as employee\ndata (e.g., salaries) and customer data (e.g., customer names and addresses). Clearly,\nonly specific people should be able to access data of the aforementioned types, and\nthey should do so only within their specific job-related responsibilities.\nData governance is actually an enabler here, solving this tension. The key concept to\nkeep in mind is that there are two layers to the data: the data itself (e.g., salaries) and\nthe metadata (data about the data—e.g., “I have a table that contains salaries, but I\nwon’t tell you anything further”).\nWith data governance, you can accomplish three things:\n•Access a metadata catalog, which includes an index of all the data managed (full\ndemocratization, in a way) and allows you to search for the existence  of certain\ndata. A good data catalog also includes certain access control rules that limit the\nbounds of the search (for example, I will be able to search “sales-related data, ” but\n“HR” is out of my purview completely, and therefore even HR-metadata is inac‐\ncessible to me).\n•Govern access to the data, which includes an acquisition process (described\nabove) and a way to adhere to the principle of least access: once access is reques‐\nted, provide access limited to the boundaries of the specific resource; don’t\novershare.\n•Independently of the other steps, make an “audit trail” available to the data access\nrequest, the data access approval cycle, and the approver (data steward), as well\nas to all the subsequent access operations. This audit trail is data itself and there‐\nfore must comply with data governance.\n24 | Chapter 1: What Is Data Governance?\nIn a way, data governance becomes the facility where you can enable data democra‐\ntization, allowing more of your data to be accessible to more of the knowledge\nemployee population, and therefore be an accelerator to the business in making the\nuse of data easier and faster.\nBusiness outcomes, such as visibility into all parts of a supply chain, understanding of\ncustomer behavior on every online asset, tracking the success of a multipronged cam‐\npaign, and the resulting customer journeys, are becoming more and more possible.\nUnder governance, different business units will be able to pull data together, analyze\nit to achieve deeper insight, and react quickly to both local and global changes.\nManage Risk (Theft, Misuse, Data Corruption)\nThe key concerns CIOs and responsible data stewards have had for a long time (and\nthis has not changed with the advent of big data analytics) have always been: What\nare my risk factors, what is my mitigation plan, and what is the potential damage?\nCIOs have been using these concerns to assign resources based on the answer to\nthose questions. Data governance comes to provide a set of tools, processes, and posi‐\ntions for personnel to manage the risk to data, among other topics presented therein\n(for example, data efficiency, or getting value from data). Those risks include:\nTheft\nData theft is a concern in those organizations where data is either the product or\na key factor in generating value. Theft of data about parts, suppliers, or price in\nan electronics manufacturer supply chain can cause a crippling blow to the busi‐\nness if competition uses that information to negotiate with those very same sup‐\npliers, or to derive a product roadmap from the supply-chain information. Theft\nof a customer list can be very damaging to any organization. Setting data gover‐\nnance around information that the organization considers to be sensitive can\nencourage confidence in the sharing of surrounding data, aggregates, and so on,\ncontributing to business efficiency and breaking down barriers to sharing and\nreusing data.\nMisuse\nMisuse is often the unknowing use of data in a way that’s different from the pur‐\npose it was collected for—sometimes to support the wrong conclusions. This is\noften a result of a lack of information about the data source, its quality, or even\nwhat it means. There is sometimes malicious misuse of data as well, meaning that\ninformation gathered with consent for benign purposes is used for other unin‐\ntended and sometimes nefarious purposes. An example is AT&T’s payout to the\nFCC in 2015 , after its call center employees were found to have disclosed con‐\nsumers’ personal information to third parties for financial gain. Data governance\ncan protect against misuse with several layers. First, establish trust before sharing\ndata. Another way to protect against misuse is declarative—declare the source of\nThe Business Value of Data Governance | 25\nthe data within the container, the way it was collected, and what it was intended\nfor. Finally, limiting the length of time for which data is accessible can prevent\npossible misuse. This does not mean placing a lid on the data and making it inac‐\ncessible. Remember, the fact that the data exists should be shared alongside its\npurpose and description—which should make data democratization a reality.\nData corruption\nData corruption is an insidious risk because it is hard to detect and hard to pro‐\ntect against. The risk materializes when deriving operational business conclu‐\nsions from corrupt (and therefore incorrect) data. Data corruption often occurs\noutside of data governance control and can be due to errors on data ingest, join‐\ning “clean” data with corrupt data (creating a new, corrupt product). Partial data,\nautocorrected to include some default values, can be misinterpeted, for example,\nas curated data. Data governance can step into the fray here and allow recording,\neven at the structured data column level, of the processes and lineage of the data,\nand the level of confidence, or quality, of the top-level source of the data.\nRegulatory Compliance\nData governance is often leveraged when a set of regulations are applicable to the\nbusiness, and specifically to the data the business processes. Regulations are, in\nessence, policies that must be adhered to in order to play within the business environ‐\nment the organization operates in. GDPR is often referred to as an example regula‐\ntion around data. This is because, among other things, GDPR mandates a separation\nof (European citizens') personal data from other data, and treatment of that data in a\ndifferent way, especially around data that can be used to identify a person. This\nmanuscript does not intend to go into the specifics of GDPR.\nRegulation will usually refer to one or more of the following specifics:\n•Fine-grained access control\n•Data retention and data deletion\n•Audit logging\n•Sensitive data classes\nLet’s discuss these one by one.\nRegulation around fine-grained  access control\nAccess control is already an established topic that relates to security most of all. Fine-\ngrained  access control adds the following considerations to access control:\n26 | Chapter 1: What Is Data Governance?\nWhen providing access, are you providing access to the right size of container?\nThis means making sure you provide the minimal size of the container of the\ndata (table, dataset, etc.) that includes the requested information. In structured\nstorage this will most commonly be a single table, rather than the whole dataset\nor project-wide permission.\nWhen providing access, are you providing the right level of access?\nDifferent levels of access to data are possible. A common access pattern is being\nable to either read the data or write the data, but there are additional levels: you\ncan choose to allow a contributor to append (but possibly not change) the data,\nor an editor may have access to modify or even delete data. In addition, consider\nprotected systems in which some data is transformed on access. Y ou could redact\ncertain columns (e.g., US social security numbers, which serve as a national ID)\nto expose just the last four digits, or coarsen GPS coordinates to city and country.\nA useful way to share data without exposing too much is to tokenize (encrypt)\nthe data with symmetric (reversible) encryption such that key data values (for\nexample, a person’s ID) preserve uniqueness (and thus you can count how many\ndistinct persons you have in your dataset) without being exposed to the specific\ndetails of a person’s ID.\nAll the levels of access mentioned here should be considered (read/write/delete/\nupdate and redact/mask/tokenize).\nWhen providing access, for how long should access remain open?\nRemember that access is usually requested for a reason (a specific project must be\ncompleted), and permissions granted should not “dangle” without appropriate\njustification. The regulator will be asking “who has access to what, ” and thus lim‐\niting the number of personnel who have access to a certain class of data will make\nsense and can prove efficient.\nData retention and data deletion\nA significant body of regulation deals with the deletion and the preservation of data.\nA requirement to preserve data for a set period, and no less than that period, is com‐\nmon. For example, in the case of financial transaction regulations, it is not uncom‐\nmon to find a requirement that all business transaction information be kept for a\nduration of as much as seven years to allow financial fraud investigators to backtrack.\nConversely, an organization may want to limit the time it retains certain information,\nallowing it to draw quick conclusions while limiting liability. For example, having\nconstantly up-to-date information about the location of all delivery trucks is useful\nfor making rapid decisions about “just-in-time” pickups and deliveries, but it\nbecomes a liability if you maintain that information over a period of time and can, in\ntheory, plot a picture of the location of a specific delivery driver over the course of\nseveral weeks.\nThe Business Value of Data Governance | 27",13796
16-Considerations for Organizations as They Think About Data Governance.pdf,16-Considerations for Organizations as They Think About Data Governance,"Audit logging\nBeing able to bring up audit logs for a regulator is useful as evidence that policies are\ncomplied with. Y ou cannot present data that has been deleted, but you can show an\naudit trail of the means by which the data was created, manipulated, shared (and with\nwhom), accessed (and by whom), and later expired or deleted. The auditor will be\nable to verify that policies are being adhered to. Audit logs can serve as a useful foren‐\nsic tool as well.\nTo be useful for data governance purposes, audit logs need to be immutable, write-\nonly (unchangeable by internal or external parties), and preserved, by themselves, for\na lengthy period—as long as the most demanding data preservation policy (and\nbeyond that, in order to show the data being deleted).\nAudit logs need to include information not only about data and data operations by\nthemselves but also about operations that happen around the data management\nfacility. Policy changes need to be logged, and data schema changes need to be logged.\nPermission management and permission changes need to be logged, and the logging\ninformation should contain not only the subject of the change (be it a data container\nor a person to be granted permission) but also the originator of the action (the\nadministrator or the service process that initiated the activity).\nSensitive data classes\nVery often, a regulator will determine that a class of data should be treated differently\nthan other data. This is the heart of the regulation that is most commonly concerned\nwith a group of protected people, or a kind of activity. The regulator will be using\nlegal language (e.g., personally identifiable data about European Union residents, or\n“financial transaction history”). It will be up to the organization to correctly identify\nwhat portion of that data it actually processes, and how this data compares to the data\nstored in structured or unstructured storage. For structured data it is sometimes eas‐\nier to bind a data class into a set of columns (PII is stored in these columns) and tag\nthe columns so that certain policies apply to these columns specifically, including\naccess and retention. This supports the principles of fine-grained access control as\nwell as adhering to the regulation about the data (not the data store or the personnel\nmanipulating that data).\nConsiderations for Organizations as They Think About Data\nGovernance\nWhen an organization sits down and begins to define a data governance program and\nthe goals of such a program, it should take into consideration the environment in\nwhich it operates. Specifically, it should consider what regulations are relevant and\nhow often these change, whether or not a cloud deployment makes sense for the\n28 | Chapter 1: What Is Data Governance?\norganization, and what expertise is required from IT and data analysts/owners. We\ndiscuss these factors next.\nChanging regulations and compliance needs\nIn past years, data governance regulations have garnered more attention. With GDPR\nand CCPA joining the ranks of HIPAA- and PCI-related regulations, the affected\norganizations are reacting.\nThe changing regulation environment has meant that organizations need to remain\nvigilant when it comes to governance. No organization wants to be in the news for\ngetting sued for failing to handle customer information as per a set of regulations. In\na world where customer information is very precious, firms need to be careful how\nthey handle customer data. Not only should firms know about existing regulations,\nbut they also need to keep up with any changing mandates or stipulations, as well as\nany new regulations that might affect how they do business. In addition, changes to\ntechnology have also created additional challenges. Machine learning and AI have\nallowed organizations to predict future outcomes and probabilities. These technolo‐\ngies also create a ton of new datasets as a part of this process. With these new\npredicted values, how do companies think about governance? Should these new data‐\nsets assume the same policies and governance that the original datasets had, or\nshould they have their own set of policies for governance? Who should have access to\nthis data? How long should it be retained for? These are all questions that need to be\nconsidered and answered.\nData accumulation and organization growth\nWith infrastructure cost rapidly decreasing, and organizations growing both organi‐\ncally and through acquisition of additional business units (with their own data\nstores), the topic of data accumulation, and how to properly react to quickly amassing\nlarge amounts of data, becomes important. With data accumulation, an organization\nis collecting more data from more sources and for more purposes.\nBig data  is a term you will keep hearing, and it alludes to the vast amounts of data\n(structured and unstructured) now collected from connected devices, sensors, social\nnetworks, clickstreams, and so on. The volume, variety, and velocity of data has\nchanged and accelerated over the past decade. The effort to manage and even consoli‐\ndate this data has created data swamps (disorganized and inconsistent collections of\ndata without clear curation) and even more silos—i.e., customers decided to consoli‐\ndate on System Applications and Products (SAP), and then they decided to consoli‐\ndate on Hive Metastore, and some consolidated on the cloud, and so on. Given these\nchallenges, knowing what you have and applying governance to this data is compli‐\ncated, but it’s a task that organizations need to undertake. Organizations thought that\nbuilding a data lake would solve all their issues, but now these data lakes are becom‐\ning data swamps with so much data that is impossible to understand and govern. In\nThe Business Value of Data Governance | 29",5864
17-Reduced Surface Area.pdf,17-Reduced Surface Area,"an environment in which IDC predicts that more than a quarter of the data generated\nby 2025 will be real-time in nature, how do organizations make sure that they are\nready for this changing paradigm?\nMoving data to the cloud\nTraditionally, all data resided in infrastructure provided and maintained by the orga‐\nnization. This meant the organization had full control over access, and there was no\ndynamic sharing of resources. With the emergence of cloud computing—which in\nthis context implies cheap but shared infrastructure—organizations need to think\nabout their response and investment in on-premises versus cloud infrastructure.\nMany large enterprises still mention that they have no plans to move their core data,\nor governed data, to the cloud anytime soon. Even though the largest cloud compa‐\nnies have invested money and resources to protect customer data in the cloud, most\ncustomers still feel the need to keep this data on-prem. This is understandable,\nbecause data breaches in the cloud feel more consequential. The potential for damage,\nmonetary as well as to reputation, explains why enterprises want more transparency\nin how governance works to protect their data on the cloud. With this pressure,\nyou’re seeing cloud companies put more guardrails in place. They need to “show” and\n“open the hood” to how governance is being implemented, as well as provide controls\nthat not only engender trust among customers, but also put some power into custom‐\ners’ hands. We discuss these topics in Chapter 7 .\nData infrastructure expertise\nAnother consideration for organizations is the sheer complexity of the infrastructure\nlandscape. How do you think about governance in a hybrid and multi-cloud world?\nHybrid computing allows organizations to have both on-premise and cloud infra‐\nstructure, while multicloud allows organizations to utilize more than one cloud pro‐\nvider. How do you implement governance across the organization when the data\nresides on-premises and on other clouds? This makes governance complicated and\ntherefore goes beyond the tools used to implement it. When organizations start\nthinking about the people, the processes, and the tools and define a framework that\nencompasses these facets, then it becomes a little easier to extend governance across\non-prem and in the cloud.\nWhy Data Governance Is Easier in the Public Cloud\nData governance involves managing risk. The practitioner is always trading off the\nsecurity inherent in never allowing access to the data against the agility that is possi‐\nble if data is readily available within the organization to support different types of\ndecisions and products. Regulatory compliance often dictates the minimal require‐\nments for access control, lineage, and retention policies. As we discussed in the\n30 | Chapter 1: What Is Data Governance?\nprevious  sections, the implementation of these can be challenging as a result of\nchanging regulations and organic growth.\nThe public cloud has several features that make data governance easier to implement,\nmonitor, and update. In many cases, these features are unavailable or cost-prohibitive\nin on-premises systems.\nLocation\nData locality  is mostly relevant for global organizations that store and use data across\nthe globe, but a deeper look into regulation reveals that the situation is not so simple.\nFor example, if, for business reasons, you want to leverage a data center in a central\nlocation (say, in the US, next to your potential customers) but your company is a Ger‐\nman company, regulation requires that data about employees remains on German\nsoil; thus your data strategy just became more involved.\nThe need to store user data within sovereign boundaries is an increasingly common\nregulatory requirement. In 2016, the EU Parliament approved data sovereignty meas‐\nures within GDPR, wherein the storage and processing of records about EU citizens\nand residents must be carried out in a manner that follows EU law. Specific classes of\ndata (e.g., health records in Australia, telecommunications metadata in Germany, or\npayment data in India) may also be subject to data locality regulations; these go\nbeyond mere sovereignty measures by requiring that all data processing and storage\noccur within the national boundaries. The major public cloud providers offer the\nability to store your data in accordance with these regulations. It can be convenient to\nsimply mark a dataset as being within the EU multi-region and know that you have\nboth redundancy (because it’s a multi-region) and compliance (because data never\nleaves the EU). Implementing such a solution in your on-premises data center can be\nquite difficult, since it can be cost-prohibitive to build data centers in every sovereign\nlocation that you wish to do business in and that has locality regulations.\nAnother reason that location matters is that secure transaction-aware global access\nmatters. As your customers travel or locate their own operations, they will require\nyou to provide access to data and applications wherever they are. This can be difficult\nif your regulatory compliance begins and ends with colocating applications and data\nin regional silos. Y ou need the ability to seamlessly apply compliance roles based on\nusers, not just on applications. Running your applications in a public cloud that runs\nits own private fiber and offers end-to-end physical network security and global time\nsynchronization (not all clouds do this) simplifies the architecture of your\napplications.\nWhy Data Governance Is Easier in the Public Cloud | 31",5624
18-Ephemeral Compute.pdf,18-Ephemeral Compute,,0
19-Security in a Hybrid World.pdf,19-Security in a Hybrid World,"Reduced Surface Area\nIn heavily regulated industries, there are huge advantages if there is a single “golden”\nsource of truth for datasets, especially for data that requires auditability. Having your\nenterprise data warehouse (EDW) in a public cloud, particularly in a setting in which\nyou can separate compute from storage and access the data from ephemeral clusters,\nprovides you with the ability to create different data marts for different use cases.\nThese data marts are provided data through views of the EDW that are created on the\nfly. There is no need to maintain copies, and examination of the views is enough to\nensure auditability in terms of data correctness.\nIn turn, the lack of permanent storage in these data marts greatly simplifies their gov‐\nernance. Since there is no storage, complying with rules around data deletion is trivial\nat the data mart level. All such rules have to be enforced only at the EDW . Other rules\naround proper use and control of the data still have to be enforced, of course. That’s\nwhy we think of this as a reduced surface area, not zero governance.\nEphemeral Compute\nIn order to have a single source of data and still be able to support enterprise applica‐\ntions, current and future, we need to make sure that the data is not stored within a\ncompute cluster, or scaled in proportion to it. If our business is spiky, or if we require\nthe ability to support interactive or occasional workloads, we will require infinitely\nscalable and readily burstable compute capability that is separate from storage archi‐\ntecture. This is possible only if our data processing and analytics architecture is serv‐\nerless and/or clearly separates computes and storage.\nWhy do we need both data processing and analytics to be serverless? Because the util‐\nity of data is often realized only after a series of preparation, cleanup, and intelligence\ntools are applied to it. All these tools need to support separation of compute and stor‐\nage and autoscaling in order to realize the benefits of a serverless analytics platform.\nIt is not sufficient just to have a serverless data warehouse or application architecture\nthat is built around serverless functions. Y ou need your tooling frameworks them‐\nselves to be serverless. This is available only in the cloud.\nServerless and Powerful\nIn many enterprises, lack of data is not the problem—it’s the availability of tools to\nprocess data at scale. Google’s mission of organizing the world’s information has\nmeant that Google needed to invent data processing methods, including methods to\nsecure and govern the data being processed. Many of these research tools have been\nhardened through production use at Google and are available on Google Cloud as\nserverless tools (see Figure 1-14 ). Equivalents exist on other public clouds as well. For\nexample, the Aurora database on Amazon Web Services (AWS) and Microsoft’s Azure\n32 | Chapter 1: What Is Data Governance?\nCosmos DB are serverless; S3 on AWS and Azure Cloud Storage are the equivalent of\nGoogle Cloud Storage. Similarly, Lambda on AWS and Azure Functions provide the\nability to carry out stateless serverless data processing. Elastic Map Reduce (EMR) on\nAWS and HDInsight on Azure are the equivalent of Google Cloud Dataproc . At the\ntime of writing, serverless stateful processing (Dataflow on Google Cloud) is not yet\navailable on other public clouds, but this will no doubt be remedied over time. These\nsorts of capabilities are cost-prohibitive to implement on-premises because of the\nnecessity to implement serverless tools in an efficient manner while evening out the\nload and traffic spikes across thousands of workloads.\nFigure 1-14. Many of the data-processing techniques invented at Google (top panel; see\nalso http://research.google.com/pubs/papers.html ) exist as managed services on Google\nCloud (bottom panel).\nLabeled Resources\nPublic cloud providers provide granular resource labeling and tagging in order to\nsupport a variety of billing considerations. For example, the organization that owns\nthe data in a data mart may not be the one carrying out (and therefore paying for) the\ncompute. This gives you the ability to implement regulatory compliance on top of the\nsophisticated labeling and tagging features of these platforms.\nWhy Data Governance Is Easier in the Public Cloud | 33",4387
20-Summary.pdf,20-Summary,"These capabilities might include the ability to discover, label, and catalog items (ask\nyour cloud provider whether this is the case). It is important to be able to label\nresources, not just in terms of identity and access management but also in terms of\nattributes, such as whether a specific column is considered PII in certain jurisdic‐\ntions. Then it is possible to apply consistent policies to all such fields everywhere in\nyour enterprise.\nSecurity in a Hybrid World\nThe last point about having consistent policies that are easily applicable is key. Con‐\nsistency and a single security pane are key benefits to hosting your enterprise soft‐\nware infrastructure on the cloud. However, such an all-or-nothing approach is\nunrealistic for most enterprises. If your business operates equipment (handheld devi‐\nces, video cameras, point-of-sale registers, etc.) “on the edge, ” it is often necessary to\nhave some of your software infrastructure there as well. Sometimes, as with voting\nmachines, regulatory compliance might require physical control of the equipment\nbeing used. Y our legacy systems may not be ready to take advantage of the separation\nof compute and storage that the cloud offers. In these cases, you’ d like to continue to\noperate on-premises. Systems that involve components that live in a public cloud and\none other place—in two public clouds, or in a public cloud and on the edge, or a in\npublic cloud and on-premises—are termed hybrid cloud systems .\nIt is possible to greatly expand the purview of your cloud security posture and poli‐\ncies by employing solutions that allow you to control both on-premises and cloud\ninfrastructure using the same tooling. For example, if you have audited an on-\npremises application and its use of data, it is easier to approve that identical applica‐\ntion running in the cloud than it is to reaudit a rewritten application. The cost of\nentry to this capability is to containerize your applications, and this might be a cost\nwell worth paying for, for the governance benefits alone.\nSummary\nWhen discussing a successful data-governance strategy, you must consider more than\njust the data architecture/data pipeline structure or the tools that perform “gover‐\nnance” tasks. Consideration of the actual humans behind the governance tools as well\nas the “people processes” put into place is also highly important and should not be\ndiscounted. A truly successful governance strategy must address not only the tools\ninvolved but the people and processes as well. In Chapters 2 and 3, we will discuss\nthese ingredients of data governance.\nIn Chapter 4 , we take an example corpus of data and consider how data governance is\ncarried out over the entire life cycle of that data; from ingest to preparation and stor‐\nage, to incorporation into reports, dashboards, and machine learning models, and on\nto updates and eventual deletion. A key concern here is that data quality is an\n34 | Chapter 1: What Is Data Governance?\nongoing  concern; new data-processing methods are invented, and business rules\nchange. How to handle the ongoing improvement of data quality is addressed in\nChapter 5 .\nBy 2025, more than 25% of enterprise data is expected to be streaming data. In Chap‐\nter 6 , we address the challenges of governing data that is on the move. Data in flight\ninvolves governing data at the source and at the destination, and any aggregations\nand manipulations that are carried in flight. Data governance also has to address the\nchallenges of late-arriving data and what it means for the correctness of calculations if\nstorage systems are only eventually correct.\nIn Chapter 7 , we delve into data protection and the solutions available for authentica‐\ntion, security, backup, and so on. The best data governance is of no use if monitoring\nis not carried out and leaks, misuse, and accidents are not discovered early enough to\nbe mitigated. Monitoring is covered in Chapter 8 .\nFinally, in Chapter 9 , we bring together the topics in this book and cover best practi‐\nces in building a data culture—a culture in which both the user and the opportunity\nis respected.\nOne question we often get asked is how Google does data governance internally. In\nAppendix A , we use Google as an example (one that we know well) of a data gover‐\nnance system, and point out the benefits and challenges of the approaches that Goo‐\ngle takes and the ingredients that make it all possible.\nSummary | 35",4493
21-Chapter 2. Ingredients of Data Governance Tools.pdf,21-Chapter 2. Ingredients of Data Governance Tools,,0
22-The Enterprise Dictionary.pdf,22-The Enterprise Dictionary,"CHAPTER 2\nIngredients of Data Governance: Tools\nA lot of the tasks related to data governance can benefit from automation. Machine\nlearning tools and automatic policy applications or suggestions can accelerate data\ngovernance tasks. In this chapter we will review some of the tools commonly referred\nto when discussing data governance.\nWhen evaluating a data governance process/system, pay attention to the capabilities\nmentioned in this chapter. The following discussion concerns tasks and tools that can\naugment complete end-to-end support for the processes involved in, and the person‐\nnel responsible for, data governance organization. We’ll dive into the various pro‐\ncesses and solutions in more detail in later chapters.\nThe Enterprise Dictionary\nTo begin, it is important to understand how an organization works with data and\nenables data governance. Usually, there is an enterprise dictionar y or an enterprise\npolicy book  of some kind.\nThe first of these documents, the enterprise dictionary, is one that can take many\nshapes, from a paper document to a tool that encodes or automates certain policies. It\nis an agreed-upon repository of the information types ( infotypes ) used by the organi‐\nzation—that is, data elements that the organization processes and derives insights\nfrom. An infotype will be a piece of information with a singular meaning—“email\naddress” or “street address, ” for example, or even “salary amount. ”\nIn order to refer to individual fields of information and drive a governance policy\naccordingly, you need to name those pieces of information.\n37\nAn organization’s enterprise dictionary is normally owned by either the legal depart‐\nment (whose focus would be compliance) or the data office (whose focus would be\nstandardization of the data elements used).\nIn Figure 2-1 , you can see examples of infotypes, and a potential organization of\nthose infotypes into organization-specific data classes  to be used in policy making.\nFigure 2-1. Data classes and infotypes\nOnce the enterprise dictionary is defined, the various individual infotypes within it\ncan be grouped into data classes, and a policy can be defined for each data class. The\nenterprise dictionary generally contains data classes, policies related to the data\nclasses, and additional metadata. The following sections expand on this.\nData Classes\nA good enterprise dictionary will contain a listing of the classes of data the organiza‐\ntion processes. Those will be infotypes collected into groups that are treated in a com‐\nmon way from the policy management aspect. For example, an organization will not\nwant to treat “street addresses, ” “phone numbers, ” “city, state, ” and “zip code” differ‐\nently in a granular manner but must rather be able to set a policy that “all location\ninformation for consumers must be accessible only to a privileged group of personnel\nand be kept only for a maximum of 30 days. ” This means that the enterprise dictio‐\nnary we’ve just described, will actually contain a hierarchy of infotypes—at the leaf\nnodes  there will be the individual infotypes (e.g., “address, ” “email”), and at the root\nnodes  you will find a data class, or a sensitivity classification (or sometimes both).\nFigure 2-2  shows an example of such a hierarchy from a fictional organization.\n38 | Chapter 2: Ingredients of Data Governance: Tools\nFigure 2-2. A data class hierarchy\nIn the data class hierarchy detailed in Figure 2-2 , you can see how infotypes such as\nIMEI (cellular device hardware ID), phone number, and IP address are grouped\ntogether under personally identifiable information (PII). For this organization, these\nare easily identifiable automatically, and policies are defined on “all PII data ele‐\nments. ” PII is paired with PHI ( protected health information ) in the “restricted data”\ncategory. It is likely that there are further policies defined on all data grouped under\nthe “restricted” heading.\nData classes are usually maintained by a central body within the organization,\nbecause policies on “types of data classes” usually affect compliance to regulation.\nSome example data classes seen across many organizations are:\nThe Enterprise Dictionary | 39\nPII\nThis is data such as name, address, and personal phone number that can be used\nto uniquely identify a person. For a retailer, this can be a customer list. Other\nexamples include lists of employee data, a list of third-party vendors, and similar\ninformation.\nFinancial information\nThis is data such as transactions, salaries, benefits, or any kind of data that can\ninclude information of financial value.\nBusiness intellectual property\nThis is information related to the success and differentiation of the business.\nThe variety and kind of data classes will change with the business vertical and inter‐\nest. The preceding example data classes (PII, financial, business intellectual property)\nwill not work for every organization, and the meaning of data collected and the clas‐\nsification will differ between businesses and between countries. Do note that data\nclasses are a combination of information elements belonging to one topic. For exam‐\nple, a phone number is usually not a data class, but PII (of which phone number is a\nmember) is normally a data class.\nThe characteristics of a data class are twofold:\n•A data class references a set of policies : the same retention rules and access rules\nare required on this data.\n•A data class references a set of individual infotypes, as in the example in\nFigure 2-1 .\nData Classes and Policies\nOnce the data the organization handles is defined in an enterprise dictionary, policies\nthat govern the data classes can be assigned to data across multiple containers. The\ndesired relationship is between a policy (e.g., access control , retention ) and a data class\nrather than a policy and an individual container. Associating policies with the mean‐\ning of data allows for better human understanding (“ Analysts cannot access PII” ver‐\nsus “ Analysts cannot access column #3 on Table B”) and supports scaling to larger\namounts of data.\nAbove, we have discussed the relationship between data classes and policies. Fre‐\nquently, along with the data class specification, the central data office, or legal, will\ndefine an enterprise policy book. An organization needs to be able to answer the\nquestion, “What kinds of data do we process?” An enterprise policy book records\nthat. It specifies the data classes the organization uses, the kinds of data that are\n40 | Chapter 2: Ingredients of Data Governance: Tools\nprocessed , and how they are processed, and it elaborates on “what are we allowed and\nnot allowed to do” with the data. This is a crucial element in the following respects:\n•For compliance, the organization needs to be able to prove to a regulator that it\nhas the right policies in place around the handling of data.\n•A regulator will require the organization to submit the policy book, as well as\nproof (usually from audit logs) of compliance with the policies.\n•The regulator will require evidence of procedures to ensure that the policy book\nis enforced and may even comment on the policies themselves.\nWe’ d like to stress the importance of having a well-thought-out and\nwell-documented enterprise policy book. Not only is it incredibly\nhelpful for understanding, organizing, and enforcing your policies,\nbut it also enables you to quickly and effectively react to changing\nrequirements and regulations. Additionally, it should be noted that\nhaving the ability to quickly and easily provide documentation and\nproof of compliance—not only for external audits but also for\ninternal ones—should not be discounted. Knowing at a glance how\nyour company is doing with its policies and its management of\nthose policies is critical for ensuring a successful and comprehen‐\nsive governance program. During the course of our research and\ninterviews, many companies expressed their struggles with being\nable to conduct quick internal audits to know how their gover‐\nnance strategy was going. This often resulted in much time and\neffort being spent backtracking and documenting policies, how\nthey were enforced, on what data they were attached, and so on.\nCreating an enterprise policy book and setting yourself up to more\neasily audit internally (and thus externally) will help you avoid this\npain.\nTo limit liability, risk management, and exposure to legal action, an organization will\nusually define a maximum (and a minimum) retention rate for data. This is impor‐\ntant because during an investigation, certain law enforcement agencies will require\ncertain kinds of data, which the organization must therefore be able to supply. In the\ncase of financial institutions, for example, it is common to find requirements for\nholding certain kinds of data (e.g., transactions) for a minimum of seven years. Other\nkinds of data pose a liability: you cannot leak or lose control of data that you don’t\nhave.\nAnother kind of policy will be access control . For data, access control goes beyond\n“yes/no” and into no access , partial access , or full access . Partial access is accessing the\ndata when some bits have been “starred out, ” or accessing the data after a determinis‐\ntic encryption transformation, which will still allow acting on distinct values, or\nThe Enterprise Dictionary | 41\ngrouping by these values, without being exposed to the underlying cleartext. Y ou can\nthink of partial access as a spectrum of access, ranging from zero access to\never-increasing details about the data in question (format only, number of digits only,\ntokenized rendition, to full access). See Figure 2-3 .\nFigure 2-3. Examples of varying levels of access for sensitive data\nNormally, a policy book will specify:\n•Who (inside or outside the organization) can access a data class\n•The retention policy for the data class (how long data is preserved)\n•Data residency/locality rules, if applicable\n•How the data can be processed (OK/Not OK for analytics, machine learning,\netc.)\n•Other considerations by the organization\nThe enterprise policy book—and, with it, the enterprise dictionary—describe the data\nmanaged by the organization. Now let’s discuss specific tools and functionality that\ncan accelerate data governance work and optimize personnel time.\nPer-Use-Case Data Policies\nData can have different meanings and require different policies when the data use\ncase is taken into consideration. An illustrative example might be a furniture manu‐\nfacturer that collects personal data (names, addresses, contact numbers) in order to\nensure delivery. The very same data can potentially be used for marketing purposes,\nbut consent, in most cases, probably has not been granted for the purpose of market‐\ning. However, at the same time, I would very much like that sofa to be delivered to my\nhome, so I want the manufacturer to store the data! The use case, or purpose, of the\ndata access ideally should be an overlay on top of your organizational membership\nand organizational roles. One way to think about this would be as a “window”\nthrough which the analyst can select data, specifying the purpose ahead of time, and\npotentially moving the data into a different container for that purpose (the marketing\n42 | Chapter 2: Ingredients of Data Governance: Tools\ndatabase, for example), all with an audit artifact and lineage tracking that will be used\nfor tracking purposes.\nImportance of Use Case and Policy Management\nIncreasingly, as requirements and regulations change to accommodate the new and\ndifferent types of data that are collected, the use case of data becomes an important\naspect of policy management. One of the things that we’ve heard companies say over\nand over is that the use case of data needs to be a part of policies—the simple “data\nclass to role-based access type” isn’t sufficient. We’ve given the example of a furniture\nmanufacturer and access to, and usage of, customer address for delivery versus mar‐\nketing purposes. Another example we’ve heard about throughout our research is that\nof a company that has employees who may perform multiple roles. In this case, a sim‐\nple role-based access policy is inadequate, because these employees may be allowed\naccess to a particular set of data for performing tasks relating to one role but may not\nhave access for tasks relating to a different role. From this you can see how it’s more\nefficient (and effective) to consider access related to what the data will be used for—its\nuse case—rather than only considering infotype/data class and employee role.\nData Classification  and Organization\nTo control the governance of data, it is beneficial to automate, at least in part, the\nclassification of data into, at the very least, infotypes—although an even greater auto‐\nmation is sometimes adopted. A data classifier will look at unstructured data, or even\na set of columns in structured data, and infer “what” the data is—e.g., it will identify\nvarious representations of phone numbers, bank accounts, addresses, location indica‐\ntors, and more.\nAn example classifier would be Google’s Cloud Data Loss Prevention (DLP) . Another\nclassifier is Amazon’s Macie service .\nAutomation of data classification  can be accomplished in two main ways:\n•Identify data classes on ingest, triggering a classification job on the addition of\ndata sources\n•Trigger a data-classification job periodically, reviewing samples of your data\nWhen it is possible, identifying new data sources and classifying them as they are\nadded to the data warehouse is most efficient, but sometimes this is not possible with\nlegacy or federated data.\nUpon classifying data, you can do the following, depending on the desired level of\nautomation:\nThe Enterprise Dictionary | 43\n•Tag the data as “belonging to a class” (see “The Enterprise Dictionary” on page\n37)\n•Automatically (or manually) apply policies that control access to and retention of\nthe data according to the definition of the data class, “purpose, ” or context for\nwhich the data is accessed or manipulated\nData Cataloging and Metadata Management\nWhen talking about data, data classification, and data classes, we need to discuss the\nmetadata,  or the “information about information”—specifically, where it’s stored and\nwhat governance controls there are on it. It would be naive to think that metadata\nobeys the same policies and controls as the underlying data itself. There are many\ncases, in fact, where this can be a hindrance. Consider, for example, searching in a\nmetadata catalog for a specific table containing customer names. While you may not\nhave access to the table itself, knowing such a table exists is valuable (you can then\nrequest access, you can attempt to review the schema and figure out if this table is\nrelevant, and you can avoid creating another iteration of this information if it already\nexists). Another example is data residency–sensitive information, which must not\nleave a certain national border, but at the same time that restriction does not neces‐\nsarily apply to information about the existence of the data itself, which may be rele‐\nvant in a global search. A final example is information about a listing of phone calls\n(who called whom, from where, and when), which can be potentially more sensitive\nthan the actual calls themselves, because a call list places certain people at certain\nlocations at certain times.\nCrucial to metadata management is a data catalog , a tool to manage this metadata.\nWhereas enterprise data warehouses, such as Google BigQuery, are efficient at pro‐\ncessing data, you probably want a tool that spans multiple storage systems to hold the\ninformation about the data. This includes where the data is and what technical infor‐\nmation is associated with it (table schema, table name, column name, column\ndescription)—but you also should allow for the attachment of additional “business”\nmetadata, such as who in the organization owns the data, whether the data is locally\ngenerated or externally purchased, whether it relates to production use cases or test‐\ning, and so on.\nAs your data governance strategy grows, you will want to attach the particulars of\ndata governance information to the data in a data catalog: data class , data quality ,\nsensitivity , and so on. It is useful to have these dimensions of information schema‐\ntized, so that you can run a faceted search like “Show me all data of type:table and\nclass:X in the “production” environment” .\n44 | Chapter 2: Ingredients of Data Governance: Tools\nThe data catalog clearly needs to efficiently index all this information and must be\nable to present it to the users whose permissions allow it, using high-performing\nsearch and discovery tooling.\nData Assessment and Profiling\nA key step in most insight generation workflows, as you sift through data, is to review\nthat data for outliers. There are many possible reasons for outliers, and best practice\nis to review before making sweeping/automated decisions. Outliers can be the result\nof data-entry errors or may just be inconsistent with the rest of the data, but they can\nalso be weak signals or less-represented new segments or patterns. In many cases, you\nwill need to normalize the data for the general case before driving insights. This nor‐\nmalization (keeping or removing outliers) should be done in the context of the busi‐\nness purpose the data is being used for—for example, if you are looking for unusual\npatterns or not.\nThe reason for normalizing data is to ensure data quality and consistency (sometimes\ndata entry errors lead to data inconsistencies). Again, this must be done in the con‐\ntext of the business purpose of the data use case. Data quality cannot be performed\nwithout a directing business case, because “reviewing transactions” is not the same\nfor a marketing team (which is looking for big/influential customers) and a fraud\nanalysis team (which is looking for provable indications of fraud).\nNote that machine learning models, for example, are susceptible to arriving at the\nwrong conclusions by extracting generalizations from erroneous data. This is also\ntrue for many other types of analytics.\nData engineers are usually responsible for producing a report that contains data outli‐\ners and other suspected quality issues. Unless the data errors are obviously caused by\ndata entry or data processing, the data errors are fixed/cleansed by data analysts who\nare part of the organization that owns/produces the data, as previously mentioned.\nThis is done in light of the expected use of the data. However, the data engineers can\nleverage a programmatic approach in fixing the data errors, as per the data owners’\nrequests and requirements. The data engineers will look for empty fields, out-of-\nbounds values (e.g., people of ages over 200 or under 0), or just plain errors (a string\nwhere a number is expected). There are tools to easily review a sample of the data and\nmake the cleanup process easier, for example, Dataprep  by Trifacta and Stitch .\nThese cleanup processes work to ensure that using the data in applications, such as\ngenerating a machine learning model, does not result in it being skewed by data outli‐\ners. Ideally, data should be profiled in order to detect anomalies per column and\nmake a determination on whether anomalies are making sense in the relevant context\n(e.g., customers shopping in a physical store outside of store hours are probably an\nerror, while late-night online ordering is very much a reality). The bounds for what\nThe Enterprise Dictionary | 45\nkinds of data are acceptable for each field are set, and automated rules prepare and\nclean up any batch of data, or any event stream, for ingestion. Care must be taken to\navoid introducing bias into the data, such as by eliminating outliers where they\nshould not be eliminated.\nData Quality\nData quality is an important parameter in determining the relevant use cases for a\ndata source, as is being able to rely on data for further calculations/inclusions with\nother datasets. Y ou can identify data quality by looking at the data source—i.e.,\nunderstanding where it physically came from. (Error-prone human entry? Fuzzy IoT\ndevices optimizing for quantity, not quality? Highly exact mobile app event stream?)\nKnowing the quality of data sources should guide the decision of whether to join\ndatasets of varying quality, because low-quality data will reduce confidence in higher-\nquality sources. Data quality management processes include creating controls for val‐\nidation, enabling quality monitoring and reporting, supporting the triage process for\nassessing the level of incident severity, enabling root cause analysis and recommenda‐\ntion of remedies to data issues, and data incident tracking.\nThere should be different confidence levels assigned to different quality datasets.\nThere should also be considerations around allowing (or at least curating) resultant\ndatasets with mixed-quality ancestors. The right processes for data quality manage‐\nment will provide measurably trustworthy data for analysis.\nOne possible process that can be implemented to improve data quality is a sense of\nownership: making sure the business unit responsible for generating the data also\nowns the quality of that data and does not leave it behind for users downstream. The\norganization can create a data acceptance process wherein data is not allowed to be\nused until the owners of the data prove the data is of a quality that passes the organi‐\nzation’s quality standards.\nLineage Tracking\nData does not live in a vacuum; it is generated by certain sources, undergoes various\ntransformations, aggregates additionals, and is eventually supporting certain insights.\nA lot of valuable context is generated from the source of the data and how it was\nmanipulated along the way, which is crucial to track. This is data lineage .\nWhy is lineage tracking important? One reason is understanding the quality of a\nresulting dashboard/aggregate. If that end product was generated from high-quality\ndata, but later the information is merged into lower-quality data, that leads to a differ‐\nent interpretation of the dashboard. Another example will be viewing, in a holistic\n46 | Chapter 2: Ingredients of Data Governance: Tools\nmanner, the movement of a sensitive data class across the organization data scape, to\nmake sure sensitive data is not inadvertently exposed into unauthorized containers.\nLineage tracking should be able, first and foremost, to present a calculation on the\nresultant metrics, such as “quality, ” or on whether or not the data was “tainted” with\nsensitive information. And later it must be able to present a graphical “graph” of the\ndata traversal itself. This graph is very useful for debugging purposes but is less so for\nother purposes.\nLineage Tracking and Time/Cost\nWhen describing how lineage tracking—especially doing so visually—is so important,\ncompanies have often referred to the level of effort they have to put into debugging\nand troubleshooting. They have stated how much time is spent not on fixing prob‐\nlems or errors but just on trying to find out if there are any errors or problems at all.\nWe have heard time and time again that better tracking (e.g., notifications about\nwhat’s going on and when things are “on fire” and should be looked at) not only\nwould help solve issues better but also would save valuable time—and thus expense—\nin the long run. Often when lineage is talked about, the focus is on knowing where\ndata came from and where it’s going to, but there is also value in visually seeing/\nknowing when and where something breaks and being able to take immediate action\non it.\nLineage tracking is also important when thinking about explaining decisions later on.\nBy identifying input information into a decision-making algorithm (think about a\nneural net, or a machine learning model), you can rationalize later why some busi‐\nness decisions (e.g., loan approval) were made in a certain way in the past and will be\nmade in a certain way in the future. By making business decisions explainable (past\ntransactions explaining a current decision) and keeping this information transparent\nto the data users, you practice good data governance.\nThis also brings up the importance of the temporal dimension of lineage. The more\nsophisticated solutions track lineage across time—tracking not only what the current\ninputs to a dashboard are but also what those inputs were in the past, and how the\nlandscape has evolved.\nKey Management and Encryption\nOne consideration when storing data in any kind of system is whether to store it in a\nplain-text format or whether to encrypt it. Data encryption provides another layer of\nprotection (beyond protecting all data traffic itself), as only the systems or users that\nThe Enterprise Dictionary | 47\nhave the keys can derive meaning from the data. There are several implementations\nof data encryptions:\n•Data encryption where the underlying storage can access the key. This allows the\nunderlying storage system to effect efficient storage via data compression\n(encrypted data usually does not compress well). When the data is accessed out‐\nside the bounds of the storage system—for example, if a physical disk is taken out\nof a data center—the data should be unreadable and therefore secure.\n•Data encryption where the data is encrypted by a key inaccessible to the storage\nsystem, usually managed separately by the customer. This provides protection\nfrom a bad actor within the storage provider itself in some cases but results in\ninefficient storage and performance impact.\n•Just-in-time decryption, where, in some cases and for some users, it is useful to\ndecrypt certain data as it is being accessed as a form of access control. In this\ncase, encryption works to protect some data classes (e.g., “customer name”) while\nstill allowing insights such as “total aggregate revenues from all customers” or\n“top 10 customers by revenue, ” or even identifying subjects who meet some con‐\ndition, with the option to ask for de-masking of these subjects later via a trouble\nticket.\nAll data in Google Cloud is encrypted by default, both in transit and at rest, ensuring\nthat customer data is always protected from intrusions and attacks. Customers can\nalso choose customer-managed encryption keys (CMEK) using Cloud KMS , or they\ncan opt for customer-supplied encryption keys (CSEK) when they need more control\nover their data.\nTo provide the strongest protections, your encryption options should be native to the\ncloud platform or data warehouse you choose. The big cloud platforms all have a\nnative key management that usually allows you to perform operations on keys\nwithout revealing the actual keys. In this case, there are actually two keys in play:\nA data encryption key (DEK)\nThis key is used to directly encrypt the data by the storage system.\nA key encryption key (KEK)\nThis key is used to protect the data encryption key and resides within a protected\nservice, a key management service.\n48 | Chapter 2: Ingredients of Data Governance: Tools\n1Protection of data at rest is a broad topic; a good starter book would be Applied Cryptography  by Bruce Schne‐\nier (John Wiley and Sons).A Sample Key Management Scenario\nIn the scenario depicted in Figure 2-4 , the table on the right is encrypted in chunks\nusing the plain data encryption key.1 The data encryption key is not stored with the\ntable but instead is stored in a protected form (wrapped) by a striped key encryption\nkey. The key encryption key resides (only) in the key management service.\nFigure 2-4. Key management scenario\nTo access the data, a user (or process) follows these steps:\n1.The user/process requests the data, instructing the data warehouse (BigQuery) to\nuse the “striped key” to unwrap the data encryption key, basically passing the key\nID.\n2.BigQuery retrieves the protected DEK from the table metadata and accesses the\nkey management service, supplying the wrapped key.\n3.The key management service unwraps the data protection key, while the KEK\nnever leaves the vault of the key management service.\n4.BigQuery uses the DEK to access the data and then discards it, never storing it in\na persistent manner.\nThis scenario ensures that the key encryption key never leaves a secure separate store\n(the KMS) and that the data encryption key never resides on disk—only in memory,\nand only when needed.\nThe Enterprise Dictionary | 49\n2Dan Moshkovich, “ Accidental Data Deletion: The Cautionary Tale of Toy Story 2  and MySpace” , HubStor\n(blog), August 17, 2020.Data Retention and Data Deletion\nAnother important item in the data governance tool chest is the ability to control\nhow long data is kept—that is, setting maximal and minimal values. There are clear\nadvantages to identifying data that should survive occasional storage space optimiza‐\ntion as being more valuable to retain, but setting a maximum amount of time on data\nretention for a less-valuable data class and automatically deleting it seems less obvi‐\nous. Consider that retaining PII presents the challenges of proposer disclosure,\ninformed consent, and transparency. Getting rid of PII after a short duration (e.g.,\nretaining location only while on the commute) simplifies the above.\nWhen talking about data retention and data deletion, we’re often thinking about them\nin the context of how to treat sensitive data—that is, whether to retain it, encrypt it,\ndelete it, or handle it some other way. But there are also scenarios around which your\ngovernance policies not only can save you from being out of compliance but also can\nprotect you from lost work.\nAlthough it’s a bit old, an example that comes to mind around the subject of protec‐\ntion against data deletion is the accidental deletion of the movie Toy Story 2  in 1998.2\nDuring the film’s production process, Oren Jacob, Pixar’s CTO, and Technical Direc‐\ntor Galyn Susman were looking at a directory that was holding the assets for several\ncharacters when they encountered an error—something along the lines of “directory\nno longer valid, ” meaning the location of those characters in the directory had been\ndeleted. During their effort to walk back through the directory to find where the\nproblem occurred, they witnessed, in real time, assets for several of the characters\nvanish before their eyes.\nWhen all was said and done, an erroneous 10-character command had mistakenly\ndeleted 90% of the movie. They were able to recover most of the movie, but unfortu‐\nnately about a week’s worth of work was lost forever.\nY ou can see from this example that even though sensitive data wasn’t leaked or lost, a\nmultitude of data was still accidentally deleted—some of which could never be recov‐\nered. We challenge you to consider in your governance program not only how you\nwill deal with and treat sensitive data in terms of where or how long you retain it, and\nwhether or not you delete it, but also how that same program can be implemented on\nother classes and categories of data that are important for you to back up. While a loss\nof data may not result in a breach in compliance, it could certainly result in other\ncatastrophic business consequences that, if planned for, will hopefully not happen to\nyou.\n50 | Chapter 2: Ingredients of Data Governance: Tools\n3For more about this case study, see coverage by the law firm Gorrissen Federspiel .Case Study: The Importance of Clear Data Retention Rules for\nData Governance\nA lesson about data retention was learned by a Danish taxi company that actually had\na data governance policy in place.3\nTo understand the story, we need a little bit of context about GDPR Article 5 , the reg‐\nulation covering treatment of European citizens’ data by tech companies. GDPR\ndetails the standards that organizations have to follow when processing personal data.\nThese standards state that data must be handled in a way that is transparent to its sub‐\nject (the European citizen), collected for a specific purpose, and used only for that\npurpose. GDPR Article 5(1)(c) addresses data minimization by requiring that per‐\nsonal data be limited to what is necessary relative to the purpose for which it is pro‐\ncessed. And Article 5(1)(e)—the one that’s most relevant to the taxi company example\n—specifies that data cannot be stored any longer than is necessary for the purposes\nfor which it was gathered.\nThe Danish taxi company in this example ( Figure 2-5 ) processed taxi ridership data\nfor legitimate purposes, making sure there was a record of every ride, and of the asso‐\nciated fare, for example, for chargeback and accounting purposes.\nFigure 2-5. Danish taxis (photo courtesy of Håkan Dahlström, Creative Commons\nLicense 2.0\nThe Danish taxi company, as mentioned, had a data retention policy in place: after\ntwo years, the data from a taxi ride was made anonymous by deleting the name of the\npassenger. However, that action did not make the data completely  anonymous, as mul‐\ntiple additional details were (albeit transparently) collected. These included the geolo‐\ncation of the taxi ride’s start and end and the phone number of the passenger. With\nthese details, even without the name of the passenger, it was easy to identify the\nThe Enterprise Dictionary | 51\npassenger , and therefore the company was not in compliance with its own statement\nof anonymization; thus the data retention policy was actually not effective.\nThe lesson learned is that considering the business goal of a data retention policy, and\nwhether or not that goal is actually achieved by the policy set in place, is essential. In\nour case, the taxi company was fined by the European Union and was criticized for\nthe fact that telephone numbers are actually used as unique identifiers within the taxi\nride management system.\nWorkflow  Management for Data Acquisition\nOne of the key workflows tying together all the tools mentioned so far is data acquisi‐\ntion. This workflow usually begins with an analyst seeking data to perform a task.\nThe analyst, through the power of a well-implemented data governance plan, is able\nto access the data catalog for the organization and, through a multifaceted search\nquery, is able to review relevant data sources. Data acquisition continues with identi‐\nfying the relevant data source and seeking an access grant to it. The governance con‐\ntrols send the access request to the right authorizing personnel, and access is granted\nto the relevant data warehouse, enforced through the native controls of that ware‐\nhouse. This workflow—identifying a task, shopping for relevant data, identifying rele‐\nvant data, and acquiring access to it—constitutes a data access workflow that is safe.\nThe level of data access requested—that is, access to metadata for search, access to the\ndata itself, querying the data in aggregate—these are all data governance decisions.\nIAM—Identity and Access Management\nWhen talking about data acquisition, it’s important to detail how access control\nworks. The topic of access control relies on user authentication and, per the user, the\nauthorization of the user to access certain data and the conditions of access.\nThe objective of authenticating a user is to determine that “you are who you say you\nare. ” Any user (and, for that matter, any service or application) operates under a set of\npermissions and roles tied to the identity of a service. The importance of securely\nauthenticating a user is clear: if I can impersonate a different user, there is a risk of\nassuming that user’s roles and privileges and breaking data governance.\nAuthentication used to be traditionally accomplished by supplying a password tied to\nthe user requesting access. This method has the obvious drawback that anyone who\nhas somehow gained access to the password can gain access to everything that user\nhas access to. Nowadays, proper authentication requires:\n52 | Chapter 2: Ingredients of Data Governance: Tools\n4An example book about identity and access management is Identity and Access Management  by Ertem Osma‐\nnoglu (Elsevier Science, Syngress).•Something you know—this will be your password or passphrase; it should be\nhard to guess and changed regularly.\n•Something you have—this serves as a second factor of authentication. After pro‐\nviding the right passphrase, a user will be prompted to prove that they have a cer‐\ntain device (a cell phone able to accept single-use codes, a hardware token),\nadding another layer of security. The underlying assumption is that if you mis‐\nplace that “object” you would report it promptly, ensuring the token cannot be\nused by others.\n•Something you are—sometimes, for another layer of security, the user will add\nbiometric information to the authentication request: a fingerprint, a facial scan,\nor something similar.\n•Additional context—another oft-used layer of security is ensuring that an\nauthenticated user can access only certain information from within a specific\nsanctioned application or device, or other conditions. Such additional context\noften includes:\n—Being able to access corporate information only from corporate hardware\n(sanctioned and cleared by central IT). This, for example, eliminates the risk\nof “using the spouse’s device to check for email” without enjoying the default\ncorporate anti-malware software installed by default on corporate hardware.\n—Being able to access certain information only during working hours—thus\neliminating the risk of personnel using their off-hours time to manipulate\nsensitive data, maybe when those employees are not in appropriate surround‐\nings or are not alert to risk.\n—Having limited access to sensitive information when not logged in to the cor‐\nporate network—when using an internet café, for example, and risking net‐\nwork eavesdropping.\nThe topic of authentication is the cornerstone of access control, and each organiza‐\ntion will define its own balance between risk aversion and user-authentication fric‐\ntion. It is a known maxim that the more “hoops” employees have to jump through in\norder to access data, the more these employees will seek to avoid complexity, leading\nto shadow IT and information siloing—directions opposed to data governance (data\ngovernance seeks to promote data access to all, under proper restrictions). There are\ndetailed volumes written on this topic.4\nThe Enterprise Dictionary | 53\nUser Authorization and Access Management\nOnce the user is properly authenticated, access is determined by a process of checking\nwhether the user is authorized to access or otherwise perform an operation on the\ndata object in question, be it a table, a dataset, a pipeline, or streaming data.\nData is a rich medium, and sample access policies can be:\n•For reading the data directly (performing “select” SQL statement on a table, read‐\ning a file).\n•For reading or editing the metadata associated with the data. For a table, this\nwould be the schema (the names and types of columns, the table name). For a\nfile, this would be the filename. In addition, metadata also refers to the creation\ndate, update date, and “last read” dates.\n•For updating the content, without adding new content.\n•For copying the data or exporting it.\n•There are also access controls associated with workflows, such as performing an\nextract-transform-load (ETL) operation to move and reshape the data (replacing\nrows/columns with others).\nWe have expanded here on the policies previously mentioned for data classes, which\nalso detail partial-read access—which can be its own authorized function.\nIt’s important to define identities, groups, and roles and assign access rights to estab‐\nlish a level of managed access.\nIdentity and access management (IAM) should provide role management for every\nuser, with the capability to flexibly add custom roles that group together meaningful\npermissions relevant to your organization, ensuring that only authorized and authen‐\nticated individuals and systems are able to access data assets according to defined\nrules. Enterprise-scale IAM should also provide context (IP , device, the time the\naccess request is being generated from, and, if possible, use case for access). As good\ngovernance results in context-specific role and permission determination before any\ndata access, the IAM system should scale to millions of users, issuing multiple data\naccess requests per second.\n54 | Chapter 2: Ingredients of Data Governance: Tools",40873
23-Summary.pdf,23-Summary,"Summary\nIn this chapter, we have gone through the basic ingredients of data governance: the\nimportance of having a policy book containing the data classes managed, and how to\nclean up the data, secure it, and control access. Now it is time to go beyond the\ntooling  and discuss the importance of the additional ingredients of people  and pro‐\ncesses  to a successful data governance program.\nSummary | 55",410
24-Chapter 3. Ingredients of Data Governance People and Processes.pdf,24-Chapter 3. Ingredients of Data Governance People and Processes,,0
25-User Hats Defined.pdf,25-User Hats Defined,"CHAPTER 3\nIngredients of Data Governance:\nPeople and Processes\nAs mentioned in the preceding chapters, companies want to be able to derive more\ninsights from their data. They want to make “data-driven decisions. ” Gone are the\ndays of business decisions being based exclusively on intuition or observation. Big\ndata and big data analytics now allow for decisions to be made based on collecting\ndata and extracting patterns and facts from that data.\nWe have spent much time explaining how this movement into using big data has\nbrought with it a host of considerations around the governance of that data, and we\nhave outlined tools that aid in this process. Tools, however, are not the only factor to\nevaluate when designing a data governance strategy—the people  involved and the pro‐\ncess by which data governance is implemented are key to a successful  implementation\nof a data governance strategy.\nThe people and process are aspects of a data governance strategy that often get over‐\nlooked or oversimplified. There is an exceedingly heavier reliance on governance\ntools, and though tools are getting better and more robust, they are not enough by\nthemselves; how the tools are implemented, an understanding of the people using\nthem, and the process set up for their proper use are all critical to governance success\nas well.\nThe People: Roles, Responsibilities, and Hats\nMany data governance frameworks revolve around a complex interplay of many roles\nand responsibilities. These frameworks rely heavily on each role playing its part in\nkeeping the well-oiled data governance machine running smoothly.\n57\nThe problem with this is that most companies are rarely able to exactly or even semi-\nfully match these frameworks, due to lack of employee skill set or, more commonly, a\nsimple lack of headcount. For this reason, employees working in the information and\ndata space of their company often wear different user hats. We use the term hat to\ndelineate the difference between an actual role or job title  and tasks  that are done. The\nsame person can perform tasks that align with many different roles or wear many dif‐\nferent hats as part of their day-to-day job.\nUser Hats Defined\nIn Chapter 1 , we outlined three broad categories of governors, approvers, and users.\nHere we will look in depth at the different hats (versus roles) within each category\n(and expand on an additional category of ancillary hats), the tasks associated with\neach hat, and finally, the implications and considerations when looking at these hats\nfrom a task-oriented perspective versus a role-based approach. In Table 3-1  we’ve lis‐\nted out each hat with its respective category and key tasks for quick reference, with\nmore detailed descriptions following.\nTable 3-1. Different  hats with their respective categories and the tasks associated with them\nHat Category Key tasks\nLegal Ancillary Knows of and communicates legal requirements for compliance\nPrivacy tsar Governor Ensures compliance and oversees company’s governance strategy/process\nData owner Approver (can also be\ngovernor)Physically implements company’s governance strategy \n(e.g., data architecture, tooling, data pipelining, etc.)\nData steward Governor Performs categorization and classification  of data\nData analyst/data\nscientistUser Runs complex data analytics/queries\nBusiness analyst User Runs simple data analyses\nCustomer support\nspecialistAncillary (can also be\na user)Views customer data (but does not use this data for any kind of analytical\npurpose)\nC-suite Ancillary Funds company’s governance strategy\nExternal auditor Ancillary Audits a company’s compliance with legal regulations\nLegal (ancillary)\nContrary to the title of legal , this hat may or may not be an actual attorney. This hat\nincludes the tasks of ensuring the company is up to date in terms of compliance with\nthe legal requirements for its data handling and communicating this information\ninternally. Depending on the company, the person with this hat may actually be an\nattorney (this is especially true for highly regulated companies, which will be covered\nin depth later) who must have a deep knowledge of the type of data collected and how\nit’s treated to ensure that the company is in compliance in the event of an external\naudit. Other companies, especially those dealing with sensitive but not highly\n58 | Chapter 3: Ingredients of Data Governance: People and Processes\n1See Google’s “Community Mobility Reports”  for more on this.regulated data, are more likely to simply have someone whose task is to be up to date\non regulations and have a deep understanding of which ones apply to the data the\ncompany collects.\nPrivacy tsar (governor)\nWe chose to title this hat privacy tsar  because this is a term we use internally at Goo‐\ngle, but this hat has also been called governance manager , director of privacy , and\ndirector of data governance  in other literature. The key tasks of this hat are those\nwhich ensure that the regulations the legal department has deemed appropriate are\nfollowed. Additionally, the privacy tsar also generally oversees the entire governance\nprocess at the company, which includes defining which governance processes should\nbe followed and how. We will discuss other process approaches later in this chapter.\nIt’s important to note that the privacy tsar may or may not have an extremely techni‐\ncal background. On the surface it might seem that this hat would come from a techni‐\ncal background, but depending on the company and the resources it has dedicated to\nits data governance efforts, these tasks are often performed by people who sit more\non the business side of the company rather than on the technical side.\nUnderstanding the movement of people is of utmost importance when it comes to\nbattling COVID-19. Google, a company that processes significant amounts of highly\npersonal data that includes location information, was torn between helping health‐\ncare providers and authorities to battle the deadly pandemic more effectively and pre‐\nserving the trust of the billions of people worldwide who use Google’s services.\nPrivacy tsar, work example 1: Community mobility reports.    The challenge of preserving\nprivacy while at the same time providing useful, actionable  data to health authorities\nrequired the full attention of Google’s privacy tsars, the people entrusted with creating\nthe internal regulations that make sure that technology does not intrude into users’\npersonal data and privacy.\nThe solution they found was to provide information in an aggregated form, based on\nanonymized sets of data from only those users who have turned on “location history”\nin Google’s services.1 This setting is off by default, and users need to “opt in” to enable\nit. Location information history can always be deleted by the user at any time. In\naddition, differential privacy (a technique covered in Chapter 7 ) was further used to\nidentify small groups of users with outlier results and eliminate those groups com‐\npletely from the provided solution. Another differential privacy technique was\nemployed to add statistical noise to the results; the noise, statistically irrelevant for\naggregates, helps ensure that no individual can be tracked back through the data.\nThe People: Roles, Responsibilities, and Hats | 59\n2For more about exposure notifications, see “Exposure Notifications: Using Technology to Help Public Health\nAuthorities Fight COVID ‑19” and “Privacy: Preserving Contact Tracing” .The result is a useful set of reports tracking communities and changes in behavior\nover time. Health officials can then assess whether “stay at home” orders are being\ncomplied with, and trace back sources of infection due to people congregating.\nIn Figure 3-1 , we see the results of that work. A sample from the report for San Fran‐\ncisco County shows increased people presence in residential areas (bottom right) but\na reduced presence across the board in retail sites, grocery stores, parks, transit sta‐\ntions, and workplaces. Note that no individual location is named, yet the data is use‐\nful for health officials in estimating where people congregate. For example, an order\nabout the reopening of retail stores can be considered.\nFigure 3-1. Example from the Google mobility reports\nPrivacy tsar, work example 2: Exposure notifications .    An even more daunting task related\nto COVID-19 is how to safely (from a privacy perspective) inform people about pro‐\nlonged exposure to a person diagnosed with COVID-19.2 Because the virus is highly\ncontagious and can be transmitted through the air, identifying exposures and making\nsure people who were inadvertently exposed to a positively diagnosed person get tes‐\nted (and isolate themselves if testing positive), is crucial to breaking infection chains\nand limiting outbreaks. This process is a recognized technique in battling infections\n60 | Chapter 3: Ingredients of Data Governance: People and Processes\nand is otherwise known as contact tracing . Technology can augment this technique by\nimmediately alerting the individual as an alternative to a prolonged phone investiga‐\ntion in which public health authorities question a positively diagnosed individual as\nto their whereabouts over the incubation period. (Many people cannot accurately\nidentify where they have been over the course of the past few days, nor can everyone\neasily produce a list of all the people they have interacted with over the course of the\npast week.)\nHowever, a positive COVID-19 diagnosis is highly personal, and having this informa‐\ntion delivered to everyone that a diagnosed person came in contact with is an emo‐\ntionally loaded process. Furthermore, having your technology do that for you is\nhighly intrusive and will cause resistance to the point of not enabling this technology.\nSo how does a privacy tsar thread the needle between preserving personal informa‐\ntion and privacy and combating a deadly disease?\nThe solution that was found maintains the principles required to ensure privacy:\n•It must be an “opt-in” solution—the people must enable it, and the product pro‐\nvides information to ensure consent is acquired after being informed.\n•Since the topic is whether or not the subject was next to a diagnosed person,\nlocation information, though it might be useful to health authorities to under‐\nstand where the incident occurred, is not collected. This is a decision made by\nthe privacy tsar in favor of preserving privacy.\n•The information is shared only with public health authorities and not with Goo‐\ngle or Apple.\nSo how does the solution work? Every phone beams a unique yet random and fre‐\nquently changing identifier to all nearby phones; the phones collect the list of bea‐\ncons, and this list is matched with anyone who has reported their diagnosis. If your\nphone was in proximity to the phone of someone who has uploaded a positive diag‐\nnosis, the match will be reported to you (see Figure 3-2 ). Note that the identity of the\ninfected individual is not reported, nor is the specific time and place. Thus the crucial\ninformation (you have been near a positively diagnosed individual so get tested!) is\nshared without sacrificing the privacy of the infected individual.\nThe People: Roles, Responsibilities, and Hats | 61\n3While “classical literature” on data governance often separates data owners and data custodians (the former\nresiding more on the business side of things, and the latter more on the technical side), during the course of\nour research and interviews with many companies we found that, in practice, these two “sides of the coin” are\noften conflated, and the actual tasks of data ownership tend to fall on those with technical expertise.\nFigure 3-2. Excerpt from the Google/Apple guide to the exposure notification  technology\nData owner (approver/governor)\nIn order for the privacy tsar’s governance strategy/process to be realized, the data\nowner is needed.3 The tasks of the data owner include physically implementing the\n62 | Chapter 3: Ingredients of Data Governance: People and Processes\nprocesses and/or strategies laid out by the privacy tsar. This most often includes the\nideation and creation of the data architecture of the company, along with choosing\nand implementing tooling and data pipeline and storage creation, monitoring, and\nmaintenance—in other words, “owning the data. ” It’s clear from the task descriptions\nthat these must be performed by someone with quite a bit of technical background\nand expertise; hence people who wear the data owner hat largely are engineers or\nfolks with an engineering background.\nData steward (governor)\nWhen researching data governance, you will find that there is likely to be much\nweight given to the role of data steward , and that’s with good reason—the tasks of a\ndata steward include categorization and classification of data. For any sort of gover‐\nnance to be implemented, data must be defined and labeled to clearly identify what\nthe data is: sensitive, restricted, health related, and so on. A large part of the reason\nwe advocate the usage of the term hats versus roles  is exemplified by the fact that it’s\nvery rare to find a singular person doing the “role” of data steward in the wild. The\nact of data “stewardship” is highly, highly manual and extremely time consuming. In\nfact, one company we spoke to said that for a short while it actually had a “full time”\ndata steward who quit after several months, citing the job as being “completely soul\nsucking. ” Because of the manual, time-consuming nature of the role—coupled with\nthe fact that in most cases there is no dedicated person  to perform stewardship\nduties— these duties often fall to many different people across the company or to\nsomeone who has another role/other duties they perform as well. As such, full data\ncategorization/classification is often not done well, not done fully, or, in the worst\ncases, just not done at all. This is an important item to note, because without steward‐\nship, governance is incomplete at best. We will cover this in more depth later, but\nhere is where we begin to see a recurring theme when looking at the people and the\nprocess. Many of the governance processes that most companies employ right now\nare undertaken to get around  the fact that stewardship falls short. As we outlined in\nChapter 1 , the quick growth and expansion of data collected by companies has resul‐\nted in an overwhelm of data, and companies simply don’t have the time or headcount\nto dedicate to being able to categorize, classify, and label ALL of their data, so they\ncreate and utilize other methods and strategies to “do their best given the limitations. ”\nData analyst/data scientist (user)\nData analysts and data scientists are, in general, some of the primary or key users of\ndata within a company and are largely who data governance efforts are for. Compa‐\nnies struggle with governance and security of their data versus the democratization of\ntheir data. In order to be data driven, companies must collect and analyze large\namounts of data. The more data that can be made available to analysts and scientists,\nthe better—unless of course that data is sensitive and should have limited access.\nThe People: Roles, Responsibilities, and Hats | 63\nTherefore, the better the governance execution, the better (and the more safely) ana‐\nlysts or scientists are able to do their job and provide valuable business insights.\nBusiness analyst (user)\nWhile data analysts and data scientists are the main users or consumers of data, there\nare a few people in the periphery of a company who also use and view data. In mov‐\ning toward being more data driven, companies have folks who sit on the business side\nof things who are very interested in the data analyses produced by analysts and scien‐\ntists. In some companies, data engineers aid in the creation and maintenance of much\nsimpler analytics platforms to aid in “self-service” for business users. More and more\nbusiness people in companies have questions that they hope crunching some data will\nhelp them to answer. Analysts/scientists thus end up fielding many, many inquiries,\nsome of which they simply don’t have time to answer. By enabling business users to\ndirectly answer some of their own questions, analysts/scientists are able to free up\ntheir time to answer more complex analysis questions.\nCustomer support specialists (user/ancillary)\nWhile a customer support specialist is technically only a “viewer” of data, it’s worth\nnoting that there are folks with this role who will need access to some sensitive data,\neven though they don’t have any tasks around manipulating that data. In terms of\nhats, customer support specialists do tend to have this as their sole role and are not\nalso doing other things; however, they are consumers of data, and their needs, and\nhow to grant them appropriate access, must be considered and managed by the other\nhats executing a company’s governance strategy.\nC-suite (ancillary)\nIn many companies, members of the C-suite have limited tasks in relation to the\nactual execution of a data governance strategy. They are nonetheless a critical hat in\nthe grand scheme of governance because they “hold the purse strings. ” As we men‐\ntioned earlier, tools and headcount are critical factors when considering a successful\ndata governance strategy. It thus makes sense that the person who actually funds  that\nstrategy must understand it and be on board with the funding that makes it a reality.\nExternal auditor (ancillary)\nWe have included the hat of external auditor in this section despite the fact that exter‐\nnal auditors are not within a particular company. Many of the companies we’ve spo‐\nken to have mentioned the importance of the external auditor in their governance\nstrategy. No longer is it good enough to simply be “compliant” with regulations—\ncompanies now often need to prove  their compliance, which has direct implications\nfor the way a governance strategy and process is employed. Oftentimes, companies\nneed to prove who has access to what data as well as all the different locations and\n64 | Chapter 3: Ingredients of Data Governance: People and Processes",18380
26-Data Enrichment and Its Importance.pdf,26-Data Enrichment and Its Importance,,0
27-Cloud NativeDigital Only.pdf,27-Cloud NativeDigital Only,"permutations of that data (its lineage). While internal tool reporting can generally\nhelp with providing proof of compliance, the way that a governance strategy is set up\nand tended to can help, or hinder, the production of this documentation.\nData Enrichment and Its Importance\nAs we mentioned at the beginning of this section, one person may wear many hats—\nthat is, perform many of the tasks involved in carrying out a data governance strategy\nat a company. As can be seen in the list of hats and the (fairly long) list of tasks within\neach hat shown in Table 3-1 , it’s easy to see how many of these tasks may not be com‐\npleted well, if at all.\nWhile there are many tasks that are important to successful implementation of a data\ngovernance strategy, it could be argued that the most critical  are data categorization,\nclassification, and labeling. As we pointed out, these tasks are manual and highly time\nconsuming, which means that they rarely get executed fully. There is a saying: “In\norder to govern data, you must first know what it is. ” Without proper data enrich‐\nment (the process of attaching metadata to data), the central task of the data steward\nhat, proper data governance falls short. This central task of the data steward is so key,\nhowever, that what we commonly see is that the person who wears the data steward\nhat also often wears the privacy tsar hat as well as the data owner hat, and/or they\nmay even wear a hat in a completely different part of the company (a common one we\nsee is business analyst). Wearing many hats results in a limited amount of tasks that\ncan be done (one person can only do so much!), and most often the majority of the\ntime-consuming task of data enrichment falls off the list.\nIn the next section we will cover some common governance processes we’ve observed\nover the years. One thing to keep in mind while reading through the processes is how\nthe hats play a role in the execution of these processes. We will discuss their interplay\nlater in this chapter.\nThe Process: Diverse Companies, Diverse Needs and\nApproaches to Data Governance\nIt’s important to note that in the discussion of the people and processes around data\ngovernance, there is no “one size fits all” approach. As mentioned, in this section we\nwill begin to examine some broad company categories, outline their specific con‐\ncerns, needs, and considerations, and explore how those interplay with a company’s\napproach to data governance.\nThe Process: Diverse Companies, Diverse Needs and Approaches to Data Governance | 65\nWe’ d like to reemphasize the point that there is no “one size fits all”\napproach to governance; and you need to fit your program to your\nneeds—whether those be mitigated by headcount, the kind of data\nyou collect, your industry, etc. We have seen many governance\napproaches and frameworks that are somewhat inflexible and thus\nmight be difficult to implement if you don’t fit their particular\nparameters. We hope that through exploration of elements to con‐\nsider you’re able to create a framework for yourself that not only\nmatches your governance needs and goals but also matches where\nyour company is right now, and where you’ d like to be in the future.\nLegacy\nLegacy companies are defined as companies that have been around for quite some\ntime and most certainly have, or have had, legacy on-premises (on-prem) systems—\nmost often many different systems, which bring with them a host of issues. The time\nand level of effort this work requires often leads to it not being done fully, or simply\nnot being done at all. Further, for consistency (and arguably for the most effective use\nof data and data analytics), every company should have a central data dictionary ,\ndefining all data names, classes, and categories that is standardized and used through‐\nout the company. Many legacy companies lack this central data dictionary because\ntheir data is spread out through these various on-prem systems. More often than not,\nthese on-prem systems and the data within them are associated with a particular\nbranch or line of business. And that branch, in and of itself, is agnostic of the data\nthat resides in the other lines of the business’s systems. As such, there ends up being a\ndictionary for each system and line of business that may not align to any other line of\nbusiness or system, which makes cross-system governance and analytics nearly\nimpossible.\nA prime example of this would be a large retailer that has a system that houses its\nonline sales and another system that handles its brick-and-mortar sales. In one sys‐\ntem the income the company receives from a sale is called “revenue, ” while in the\nother system it’s simply called “sales. ” Between the two systems, the same enterprise\ndictionary is not being used—one can see where this becomes problematic if execu‐\ntives are trying to figure out what the total income for the company is. Analytics are\nnearly impossible to run when the same data does not have the same metadata ver‐\nnacular attached. This becomes an even larger issue when considering sensitive data\nand its treatment. If there is no agreed-upon, company-wide terminology (as outlined\nin the enterprise dictionary), and if that terminology is not implemented for all sour‐\nces of data, governance is rendered incomplete and ineffective.\nThe power of the cloud and big data analytics drives many companies to want to\nmigrate their data, or a portion of their data, to the cloud—but the past pain of incon‐\nsistent enterprise dictionaries and haphazard governance gives them pause. They\n66 | Chapter 3: Ingredients of Data Governance: People and Processes",5702
28-Retail.pdf,28-Retail,"don’t want to “repeat their past mistakes”; they don’t want to simply replicate all of\ntheir current issues. While tools can help to right past wrongs, they simply aren’t\nenough. Companies need (and desire) a framework for how to move their data and\nhave it properly governed from the beginning, with the right people working the right\nprocess.\nCloud Native/Digital Only\nCloud-native  companies (sometimes referred to as digital only ) are defined as compa‐\nnies who have, and have always had, all of their data stored in the cloud. Not always,\nbut in general, these tend to be much younger companies who have never had on-\npremises systems and thus have never had to “migrate” their data to a cloud environ‐\nment. Based on that alone, it’s easy to see why cloud-native companies don’t face the\nsame issues that legacy companies do.\nDespite the fact that cloud-native companies do not have to deal with on-prem sys‐\ntems and the “siloing” of data that often comes along with that, they still may deal\nwith different clouds as well as different storage solutions within and between those\nclouds that have their own flavor of “siloing. ” For one, a centralized data dictionary, as\nwe’ve explored, is already a challenge, and having one that spans multiple clouds is\neven more daunting. Even if a centralized data dictionary is established, the process\nand tools (because some clouds require that only certain tools be used) by which data\nis enriched and governed in each cloud will probably be slightly different. And that is\nsomething that hinders consistency (in both process and personnel) and thus hinders\neffectiveness. Further, even within a cloud, there can be different storage solutions\nthat also may carry with them different structures of data (e.g., files versus tables ver‐\nsus jpegs). These structures can be difficult to attach metadata to, which makes gover‐\nnance additionally difficult.\nIn terms of cloud—and data governance within clouds specifically—cloud-native\ncompanies tend to have better governance and data management processes set up\nfrom the beginning. Because of their relatively young “age” in dealing with data, there\nis often less data overall, and they have more familiarity with some common data-\nhandling best practices. Additionally, cloud-native companies by definition have all\nof their data in the cloud, including their most sensitive data—which means that\nthey’ve most likely been dealing with the need for governance since the beginning,\nand they most likely have some processes in place for governing, if not all of their\ndata, at least their most sensitive data.\nRetail\nRetail companies are an interesting category, as not only do they often ingest quite a\nbit of data from their own stores (or online stores), but they also tend to ingest and\nutilize quite a bit of third-party data. This presents yet another instance in which data\nThe Process: Diverse Companies, Diverse Needs and Approaches to Data Governance | 67\ngovernance is only as good as the process set up for it and the people who are there to\nexecute it. The oft-mentioned importance of creating and implementing a data dictio‐\nnary applies, as well as having a process around how this third-party data is ingested,\nwhere it lands, and how governance can be applied.\nOne twist that we have not discussed but that very much applies in the case of retail is\nthat of governance beyond simple classification of data. So far we have discussed the\nimportance of classifying data (especially sensitive data) so that it can be known and\nthus governed appropriately. That governance most often relates to the treatment of\nand/or access to said data. But data access and data treatment are not the only aspects\nto consider in some instances; the use case for that data is also important. In the case\nof retail, there may be a certain class of data—email for example—that was collected\nfor the purpose of sending a customer their receipt from an in-store purchase. In the\ncontext (use case) of accessing this data for the purposes of sending a customer a\nreceipt, this is completely acceptable. If, however, one wanted to access this same  data\nfor the purpose of sending a customer some marketing material around the item they\njust purchased, this use case would not be acceptable unless the customer has given\nexplicit consent for their email to be used for marketing. Now, depending on the\nemployee structure at a company, this problem may not be solvable with simple role-\nbased access controls (fulfillment gets access, marketing does not) if the same\nemployee may cover many different roles. This warrants the need for a more complex\nprocess that includes establishing use cases for data classes.\nCombo of Legacy and Retail\nA particularly interesting use case we’ve come across in our customer interviews is\nthat of a very large legacy retail company. In business for over 75 years, this company\nis looking to leverage powerful data analytics tools to help it move toward being a\nmore data-driven company.\nThe struggle, interestingly, is not only in its old legacy on-prem data-storage systems\nbut also in its internal processes around data and data management.\nIt currently has its data separated into several data marts, a configuration aimed at\ndistributing responsibility for segments of data: a mart for marketing for in-store\nsales, for third-party sales, and so on. This, however, has become problematic for the\ncompany, as not only is there no “central source of truth” in terms of an enterprise\ndictionary, but it also cannot run any kind of analytics across its data marts, resulting\nin duplication of data from one mart to another since analytics can only be run within\na mart.\nHistorically, the company was very focused on keeping all of its data on-prem; how‐\never, this view has changed with the increased security that is now available in the\ncloud, and the company is now looking to migrate all of its data off-premises.\nThrough this migration effort, not only is the company looking to change the basic\n68 | Chapter 3: Ingredients of Data Governance: People and Processes",6167
29-Highly Regulated.pdf,29-Highly Regulated,"infrastructure of its data story from a decentralized one (multiple marts) to a central‐\nized one (centralized data warehouse), but it is also using this as an opportunity to\nrestructure its internal processes around data management and governance (see\nFigure 3-3 ).\nFigure 3-3. Breaking down data silos by restructuring internal processes is often  a key\nstage after  enterprises consolidate their on-premises data into the cloud\nData centralization enables the company to have one enterprise dictionary that allows\nfor all new data—namely sensitive data—to be quickly marked and treated accord‐\ningly. Quick and easy handling of sensitive data also enables quicker and easier access\ncontrols (as they need to be applied only once, as opposed to each mart). This not\nonly saves overhead effort but also allows for the implementation of more easy-to-use\nself-service analytics tools for employees who may not be analysts by trade but, with\nthe right tools and access to data, are able to run simple analytics. This new process,\nhowever, is really only good for new incoming data.\nThis company, like many legacy companies with a lot of historical data, is struggling\nwith how to handle the data it already has stored. It currently has 15 years of data\n(~25 terabytes) stored on-premises, and while it knows there is a wealth of informa‐\ntion there, the time and effort required to migrate, enrich, and curate all this data\nseems daunting at best, especially when the payoff is not known.\nHighly Regulated\nHighly regulated companies represent the sector of companies that deal with\nextremely sensitive data—data that often carries additional compliance requirements\nbeyond the usual handling of sensitive information. Some examples of highly\nregulated  companies would be those dealing with financial, pharmaceutical, or\nhealthcare services.\nThe Process: Diverse Companies, Diverse Needs and Approaches to Data Governance | 69\nHighly regulated companies deal with multiple kinds of sensitive data. They have to\njuggle not only basic data governance best practices but also the additional regula‐\ntions related to the data they collect and deal in, and they face regular audits to make\nsure that they are aboveboard and compliant. As a result of this, many highly regula‐\nted companies are more sophisticated in their data-governance process. As they’ve\nhad to deal with compliance related to their data from the get-go, they often have bet‐\nter systems in place to identify and classify their sensitive data and treat it\nappropriately.\nAlso, for many of these kinds of companies, their business is based solely around sen‐\nsitive data, so not only do they tend to have better processes in place from the begin‐\nning, but also those processes most often include a more well-funded and\nsophisticated organization of the people who handle that sensitive data. These com‐\npanies tend to actually have people dedicated to each of the hats discussed earlier, and\nthat additional headcount, as we pointed out, can be the deciding factor in whether a\ndata-governance strategy is successful or unsuccessful.\nOne final note about highly regulated companies is that, as a result of the legal\nrequirements they face on the handling of certain data, they can function similarly to\nlegacy companies in that they have difficulty moving off-prem and/or trying out new\ntools. Any tool that will touch sensitive data under a regulation must meet the stand‐\nards (fully) of that regulation. As an example, a tool or product in beta may be used\nby a healthcare company only if it is HIPAA compliant. Many product betas, because\nthey’re used for early access and as a way to work out bugs, are not designed to meet\nthe most stringent compliance standards. While the final product may be compliant,\nthe beta generally is not, meaning that highly regulated companies often don’t get to\nexperiment with these new tools/products and thus have trouble migrating to new,\npotentially better tools than the ones they’re currently using.\nUnique Highly Regulated Organization: Hospital/University\nWhen discussing highly regulated industries, finance and healthcare are the ones\nmost commonly referenced. In our discussions with customers we came across\nanother highly regulated industry that had some unique challenges: hospital/universi‐\nties. These “companies” are unique in that they collect a lot of clinical data from their\nhospitals, but they also collect (and produce) a lot of research data through\nuniversity-sponsored research studies.\nEach of these types of data comes with its own specific regulations and standards for\nresearch—e.g., HIPAA covers clinical data, and the Institutional Review Board (IRB)\nprotects the rights and welfare of human research subjects recruited to participate in\nresearch activities.\n70 | Chapter 3: Ingredients of Data Governance: People and Processes",4922
30-Considerations and Issues.pdf,30-Considerations and Issues,"One particular hospital/university we spoke to was looking at the use case of being\nable to run secondary analytics across its clinical and research data. Currently it has\ntwo main pipeline solutions for its data: one for clinical and one for research.\nFor its clinical pipelines, data is stored on-prem in a Clarity data warehouse for each\nhospital, and only analysts with access to that database are able to run analytics.\nFor its research pipelines, each “lab” within the university has its own on-prem stor‐\nage, and again, only analysts with access to that server can run analytics. It can be\nseen that not only does this structure not allow for secondary analyses to be run\nacross labs, but they also can’t be run across clinical and research.\nKnowing that there is a wealth of value in these secondary analytics, this hospital/\nuniversity decided to migrate a large portion of its clinical and research data to the\ncloud, to get it all in one central location. In order to do this, however, much needs to\nbe done to make the data comply with healthcare and research regulations. As such,\nthe hospital/university created a specialized team dedicated to this migration effort;\nthat team’s role included tasks such as: creating an enterprise dictionary, enriching\ndata, reviewing the presence of sensitive data, reviewing policies attached to data,\napplying new policies to data, and applying a standardized file structure.\nFortunately for this organization, it was able to secure funding for such an elaborate\nundertaking, and yet it is still looking at ways to automate its process. Migration and\nthe setup of data in the cloud is but one hurdle—maintaining and managing a data\nstore going forward will require effort as well, and as such, tools that enable automa‐\ntion are top of mind for this organization.\nSmall Companies\nFor our purposes, we define a small company as one that has fewer than one thou‐\nsand employees. One of the benefits of small companies is their smaller employee\nfootprint.\nSmaller companies often have small data-analytics teams, which means that there are\nfewer people who actually need to touch data. This means that there is less risk over‐\nall. One of the primary reasons to govern data is to make sure that sensitive data does\nnot fall into the wrong hands. Fewer employees also makes for a much less arduous\nand less complicated process of setting up and maintaining access controls. As we\ndiscussed earlier, access controls and policy management can get increasingly compli‐\ncated, especially when factors such as use case for data come into play.\nAnother benefit of a small amount of people touching data is that there is often less\nproliferation of datasets. Data analysts and scientists, as part of their jobs, create many\ndifferent views of datasets and joins (combining tables from different dataset sour‐\nces). This proliferation of data makes it hard to track where the data came from and\nwhere it’s going to (not to mention who had and now has access). Fewer analysts/\nThe Process: Diverse Companies, Diverse Needs and Approaches to Data Governance | 71\nscientists mean fewer datasets/tables/joins, resulting in data that’s much easier to\ntrack and thus easier to govern.\nLarge Companies\nWhile there are many more company types, we will end our exploration of broad cat‐\negories with large companies, defined as companies with more than one thousand\nemployees.\nIf small companies have the governance benefit of dealing with less data, large com‐\npanies deal with the reverse, in that they often deal with a lot  of data. Large compa‐\nnies not only generate a great deal of data themselves but also often deal with a lot of\nthird-party data. This results in immense difficulty in wrapping their arms around it\nall; they are overwhelmed by the data and often struggle to govern even a portion of\nit. As such, only some data gets enriched and curated, which means that only some of\ntheir data is able to be used to drive insights.\nLarge companies often put processes in place to limit this overwhelm by choosing\nonly select data to enrich, curate, and thus govern. A common strategy for limiting\nthe data that a data steward must enrich is to select a number of categories and gov‐\nern only the data that falls within those categories. Another is to only govern known\npipelines of data (these are the pipelines consisting of the primary data a company\ndeals with, such as daily sales numbers from a retail store) and to handle “ad hoc”\npipeline data (engineers at times are asked to create new pipelines to address one-\ntime or infrequent data analysis use cases) only as time allows or if absolutely\nnecessary.\nIt’s easy to see that these strategies (which we will discuss in more depth later) result\nin a sort of iceberg of data where enriched ( known ), curated data sits at the top, and\nbelow that is a mound of data that is, in the main, un-nriched and thus unknown .\nCompanies don’t know what this data is, which means they can’t govern it, and\nungoverned data is quite scary. It could be sensitive, it could be noncompliant, and\nthere could be dire consequences if it happens to get leaked. This causes much fear\nand forces companies with this problem to use strategies to help mitigate their risk.\nNot only do large companies deal with copious amounts of data, but they also tend to\nhave a much larger workforce of data analysts, data scientists, and data engineers—all\nof whom need access to data to do their jobs. More data, plus more people who need\naccess to data, results in much more complicated (and often poorly managed) pro‐\ncesses around access control. Access control is generally based on user role, which\nshould determine the data (and only that data) that they need access to. This strategy\nmay seem simple, but it is a difficult process to implement for two main reasons.\nFirst, in order to know what data is critical to a particular user’s role, data must be\nknown. And we know from previous discussion in this book that the majority of data\na company has is unknown. This results in not only the inability to govern (it’s\n72 | Chapter 3: Ingredients of Data Governance: People and Processes\nimpossible to govern what you don’t know you have) but also the inability to know\nwhat data is appropriate for whom. Data specialists still need to do their job, however,\nso (too much) access is often granted at the expense of risk. Companies try to offset\nthis risk by creating a “culture of security, ” which puts the onus on the employee to do\nthe right thing and not expose or misuse potentially sensitive information. Another\nissue with role-based access is that roles and their ensuing uses for data are not always\nblack and white; the use case for data must also be considered. Depending on the\ncompany, a user in the same role may be able to use data for some use cases and not\nfor others (an example being the use case we outlined in the retail section). As\ntouched on Chapter 2 , and as will be covered in depth later, the addition of use case\nas a parameter during policy creation helps to mitigate this issue.\nLike legacy companies, large companies often also deal with the issue of having many\ndifferent storage systems, the majority of which are legacy. Large companies tend to\nbe built up over time, and with time comes more data and more storage systems.\nWe’ve already discussed how the siloed nature of different storage systems makes\ngovernance difficult. Different storage systems naturally create data silos, but large\ncompanies have another factor that results in even more complex “silos”: acquisitions.\nLarge companies are sometimes built completely from within, but others become\nlarge (or larger) due to acquisitions. When companies acquire other, smaller compa‐\nnies, they also acquire all their data (and its underlying storage), which brings along a\nwhole host of potential problems—primarily, how the acquired company handled its\ndata, as well as its overall governance process and approach. This includes how the\ncompany managed its data: its method of data classification, its enterprise dictionary\n(or lack thereof), its process and methods around access control, and its overall cul‐\nture of privacy and security. For these reasons, many large companies find it nearly\nimpossible to marry their central governance process with that of their acquisitions,\nwhich often results in acquired data sitting in storage and not being used for\nanalytics.\nPeople and Process Together: Considerations, Issues, and\nSome Successful Strategies\nWe have now outlined the different people involved in various kinds of companies, as\nwell as some specific processes and approaches utilized by the different company\ntypes.\nThere is obviously a synergy between people and process, of which there are some\nconsiderations and issues. We will review several of the issues we’ve observed, along\nwith outlining a few strategies we have seen in which the right people and process\ntogether have resulted in moving toward a successfully implemented data-governance\nstrategy.\nPeople and Process Together: Considerations, Issues, and Some Successful Strategies | 73\nConsiderations and Issues\nThis certainly is not an exhaustive list of all the potential issues with implementing a\nsuccessful data-governance strategy; we are simply highlighting some of the top\nissues we’ve observed. We only briefly note the mitigation efforts to combat these\nissues; the latter half of this text will go into these in much more detail.\n“Hats” versus “roles” and company structure\nWe previously discussed our intentional use of the terms hats versus roles  and the\nimpact that wearing many hats has on the data governance process. To expand on\nthat idea further, an additional issue that arises with hats versus roles is that responsi‐\nbility and accountability become unclear. When looking at the different kinds of\napproaches that different companies take to achieve governance, an underlying need\nis for actual people to take responsibility for the parts of that process. This is easy\nwhen it is clearly someone’s job to conduct a piece of the process, but when the lines\nbetween what is and what is not within a person’s purview are blurred, these fuzzy\nlines often result in inadequate work, miscommunication, and overall mismanage‐\nment. It’s clear that a successful governance strategy will rely not simply on roles but\non tasks, and on who is responsible or accountable for these tasks.\nTribal knowledge and subject matter experts (SMEs)\nWhen talking with customers about their pain points with data governance, one of\nthe things we hear over and over again is that they need tools to help their analysts\nfind which datasets are “good, ” so that when an analyst is searching for data, they\nknow that this dataset is of the best quality and is the most useful one for their use\ncase. The companies state that this would help their analysts save time searching for\nthe “right/best” dataset, as well as assisting them in producing better analytics. Cur‐\nrently, through most companies, the way analysts know which datasets they should\nwork with is by word of mouth, or “tribal knowledge. ” This is an obvious problem for\ncompanies because roles change, people move on, etc. Companies request “crowd‐\nsourcing” functionality, such as allowing analysts to comment on or rank datasets to\nhelp give them a “usefulness” score for others to see when searching. This suggestion\nis not without merit but uncovers the larger problems of findability  and quality .\nCompanies are relying on people  to know the usefulness of a dataset and to transfer\nthat knowledge on to others, yet this strategy is fallible and difficult (if not impossi‐\nble) to scale. This is where tools that lessen (or negate) the effort placed on a particu‐\nlar user or users aid in the process. A tool that can detectthe most-used datasets and\nsurface these first in a search, for example, can help minimize the reliance on tribal\nknowledge and SMEs.\n74 | Chapter 3: Ingredients of Data Governance: People and Processes\nDefinition  of data\nRegardless of their type, all companies want to be able to collect data that can be used\nto drive informed business decisions. They are certain that more data, and the analyt‐\nics that could be run on that data, could result in key insights—insights that have the\npotential to skyrocket the success of their business. The problem, however, is that in\norder for data to be used, it must be known . It must be known what the letters or\nnumbers or characters in a column of a table mean. And now it must also be known\nwhether those numbers, letters, or characters represent information that is sensitive\nin nature and thus needs to be treated in a specific way. Data enrichment is key to\n“knowing” data, yet it is largely a manual process. It generally requires actual people\nto look at each and every piece of data to determine what it is. As we discussed, this\nprocess is cumbersome on its own, and it becomes almost impossible when the extra\ncomplexity of disparate data storage systems and different data definitions and cata‐\nlogs are considered. The “impossible” nature of this work in general means that it just\nnever gets done; this leaves companies scrambling to use a few tools and to imple‐\nment some half strategies to make up for it—and also hoping that educating people\non how data should  be treated and handled will somehow be enough.\nOld access methods\nGone are the days of simple access controls—i.e., these users/roles get access and\nthese users/roles do not. Historically, there were not many users or roles who even\nhad the need  to view or interact with data, which meant that only a small portion of\nemployees in a company needed to be granted access in the first place. In today’s\ndata-driven businesses there is the potential for many, many users who may need to\ntouch data in a variety of ways, as seen in the beginning of this chapter. Each hat has\ndifferent types of tasks that it may need to do in relation to data that requires varying\nlevels of access and security privilege.\nWe already discussed the problematic nature of unknown data; another layer to that\nis the implementation of access controls. There is a synergy between knowing which\ndata even needs  access restrictions (remember, data must be known in order for it to\nbe governed) and knowing what those restrictions should be for what users. As dis‐\ncussed in Chapter 2 , there are varying levels of access control, all the way from access\nto plain text to access to hashed or aggregated data.\nA further complication around access is that of the intent  of the user accessing data.\nThere may be use cases for which access can and should be granted, and other use\ncases for which access should strictly be denied. A prime example (and one we’ve\nheard from more than one company) is a customer’s shipping address. Imagine the\ncustomer just bought a new couch, and their address was recorded in order to fulfill\nshipment of that couch. A user working to fulfill shipping orders should undoubtedly\nget access to this information. Now let’s say that the slipcovers for the couch this cus‐\ntomer just bought go on sale, and the company would like to send a marketing flyer\nPeople and Process Together: Considerations, Issues, and Some Successful Strategies | 75\nto the customer to let them know about this sale. It may be the case that the user in\nthe company who handles shipping data also happens to handle marketing data\n(remember hats?). If the customer has opted OUT of promotional mail, the user in\nthis example would not be able to send the marketing material even though  they have\naccess to that data. This means that access controls and policies need to be sensitive\nenough to account not only for a black-and-white “gets access/doesn’t get access” rule\nfor a particular user, but also for what purpose  the user is using the data.\nRegulation compliance\nAnother struggle that companies have is around compliance with regulations. Some\nregulations, such as those in financial and healthcare industries, have been around for\nquite a while, and as we pointed out before, companies who deal with these kinds of\ndata tend to have better governance strategies, for two main reasons: one, they’ve\nbeen dealing with these regulations for some time and have built in processes to\naddress them, and two, these regulations are fairly established and don’t change\nmuch.\nThe advent of a proliferation of data collection has brought about new regulations\nsuch as GDPR and CCPA that aim to protect all of a person’s data, not just their most\nsensitive data (i.e., health or financial data). Companies in all types of industries, not\njust highly regulated ones, now must comply with these new regulations or face seri‐\nous financial consequences. This is a difficult endeavor for companies that previously\ndid not have regulations to comply with and thus perhaps did not address this during\nthe setting up of their data infrastructure. As an example, one of the main compo‐\nnents of GDPR is the “right to be forgotten, ” or the ability for a person to request that\nall their data collected by a company be deleted. If the company does not have its data\nset up to find all the permutations of a person’s individual data, it will struggle to be\ncompliant. Thus, it can be seen how findability is important not only from the per‐\nspective of finding the right data to analyze but also from the perspective of finding\nthe right data to delete.\nCase Study: Gaming of Metrics and Its Effect  on Data Governance\nBe aware of the human element when you assign regulations and compliance. A rele‐\nvant case study about how, when introducing metrics, you should factor in the human\nresponse to these metrics is in Washington, DC’s school system. In 2009, Washington,\nDC introduced a new ranking system called IMPACT through which teachers were\nassessed and scored. The system, born out of the intention to promote “good” teach‐\ners and generally improve education—and, even further, to allow “low scoring” teach‐\ners to be dismissed—actually did not achieve the desired result.\nThe ranking system was based on an algorithm that took into account standardized\ntest scores of students, with the underlying assumption that test scores for students\n76 | Chapter 3: Ingredients of Data Governance: People and Processes",18567
31-Processes and Strategies with Varying Success.pdf,31-Processes and Strategies with Varying Success,"4Perry Stein, “Chancellor Pledges to Review D.C. ’s Controversial Teacher Evaluation System” , Washington Post ,\nOctober 20, 2019.with “good” teachers should show improvement year after year. However, the ranking\nsystem did not include social and familial circumstances, nor did it include any kind\nof feedback or training from a controlled set.\nThe result of implementing the system was that teachers were let go based only on the\nresults of the standardized tests, and without regard to feedback from administrators.\nThe immediate reaction was that teachers pivoted to focusing on how to pass the\nstandardized tests rather than on the underlying principles of the subjects. This resul‐\nted in high scores and advancement for teachers whose students passed the tests, but\nit did not, sadly, promote excellence among the students.\nIn fact, in 2018 a city-commissioned investigation revealed that one in three students\ngraduating in 2017 received a diploma despite missing classes. After being challenged,\nand after a detailed account of the usage of the algorithm and its usefulness had been\ncarried out, the system was eventually “reconsidered” in 2019.4\nThe process lessons here are that, although the motivation (improving teaching, pro‐\nmoting excellent teachers) should have been easily agreed upon by both teachers’\nunions and the school district administrators, the implementation was lacking. A\nproper way to introduce a process is to first have a discussion with the affected peo‐\nple. Take feedback to heart and act on it. Run the process for the first time as a trial,\nwith transparent results, open discussion, and a willingness to pivot if it isn’t working\nas intended.\nProcesses and Strategies with Varying Success\nThe issues with people and process are many, and we have observed some strategies\nthat have been implemented with varying degrees of success. While we will explore\nsome of these strategies here, the latter part of this book dives deeper into how these\nstrategies (and others) can be implemented to achieve greater data governance\nsuccess.\nData segregation within storage systems\nWe’ve reviewed some of the issues that arise from having multiple data-storage sys‐\ntems; however, some companies use multiple storage systems and even different stor‐\nage “areas” within the same storage system in an advantageous and systematic way.\nTheir process is to separate curated/known data from uncurated/unknown data. They\nmay do this in a variety of ways, but we have heard two common, prevailing\nstrategies.\nPeople and Process Together: Considerations, Issues, and Some Successful Strategies | 77\nThe first strategy is to keep all uncurated data in an on-prem storage system and to\npush curated data that can be used for analytics to the cloud. The benefit that compa‐\nnies see in this strategy is that the “blast radius, ” or the potential for data to be leaked\neither by mistake or by a bad actor, is greatly diminished if only known, clean, and\ncurated data is moved into a public cloud. To be clear, companies often indicate that it\nis not their total distrust of cloud security that is the problem (although that can play\na role as well); it is their concern that their own employees  may unwittingly leak data if\nit’s in a public cloud versus an on-prem environment.\nFigure 3-4  shows an example of data stored on-premises and in the cloud. As you can\nsee, even though both storage systems have structured datasets, only the cloud dataset\nhas been enriched with metadata and thus has been treated with the appropriate gov‐\nernance controls (hashing in this case). The on-premises dataset, while it has the\nsame information, hasn’t been curated or enriched; we don’t know just by looking at it\nthat the numbers in the first column are credit card numbers or that the words in the\nsecond column are customer names. Once these columns have been curated and the\nappropriate metadata (credit card number, customer name, etc.) has been attached,\nwe can attach governance controls, so that even if the data gets leaked, it will be the\nprotected version and not plain text.\nFigure 3-4. Enriched versus not enriched datasets on-premises and in the public cloud\nAs we’ve stated, an obvious benefit of this strategy is that if sensitive data resides only\non-premises, then if it’s leaked or incorrectly accessed, that could only have been\ndone by someone within the company. On the other hand, if that same sensitive data\nis in a public cloud, it could potentially have been accessed by anyone if leaked or\nhacked into.\n78 | Chapter 3: Ingredients of Data Governance: People and Processes\nThere are a few drawbacks to this strategy, however. One is that when data is segrega‐\nted this way—some residing on premise and some residing in the cloud—cross-\nstorage analytics are difficult if not impossible to complete. Since one of the main\ndrivers of collecting so much data is being able to run powerful analytics, hindering\nthis seems counterproductive. The other drawback is that segregation also requires\nupkeep of and attendance to these multiple storage systems and data pipelines, not to\nmention the creation, maintenance, and enforcement of additional access controls, all\nof which are difficult to properly manage and stay on top of over time.\nThe second strategy is similar to the first in that there is a separation between curated\nand uncurated data, but this is done within the same cloud environment. Companies\nwill create different layers or zones (as can be seen in Table 3-2 ) within their cloud\nenvironment and base access on these; the bottom, uncurated zone may be accessed\nby only a few users, while the uppermost zone, curated and cleaned (of sensitive\ndata), may be accessed by any user.\nTable 3-2. A “data lake” within cloud storage with multiple tiers showing what kind of data\nresides in each and who has access\nTypes of data Access\nInsights\nzoneKnown, enriched, curated, and cleaned data. Data also has likely had\ngovernance controls such as encryption, hashing, redaction, etc.\nExample: Well-labeled, structured datasets.Highest level of access. Most if not all\ndata analysts/scientists and others in a\nuser role.\nStaging\nzoneMore known and structured data. Data from multiple sources is likely\nto be joined here. This is also where data engineers prep data, cleanse\nit, and get it ready to drop into the insights zone.More access. Mostly data engineers—\nthose in charge of putting together\ndatasets for analytics.\nRaw zone Any kind of data. Unstructured and uncurated. Could also include\nthings such as videos, text files,  etc. Example: Video files,  unstructured\ndatasetsVery restricted access. Likely a handful\nof people or just an admin.\nClearly the benefits and drawbacks of this strategy are nearly a reverse of those of the\nprevious strategy. In this strategy, management and upkeep of the storage system,\ndata pipelines, and policies are limited to one system, which makes it far simpler and\nmore streamlined to stay on top of. Analytics are also largely easier to run because\nthey can all be run within one central storage system, as opposed to trying to run\nthem across multiple systems, or dealing with the constant moving of data from one\nstorage system to another for the purposes of analysis.\nAs we’ve noted, all data residing within a public cloud does carry the potential draw‐\nback of data leaking (whether intentionally or unintentionally) out onto the public\ninternet—a cause for trepidation to be sure.\nPeople and Process Together: Considerations, Issues, and Some Successful Strategies | 79\nData segregation and ownership by line of business\nAs we’ve mentioned more than a few times, data enrichment is a key challenge in suc‐\ncessful data governance for many reasons, the primary ones being level of effort and\nlack of accountability/ownership.\nOne way we’ve observed companies handle this issue is by segregating their data by\nline of business. In this strategy, each line of business has dedicated people to do the\nwork of governance on just that data. While this is often not actually delineated by\nrole, each line of business has a deep knowledge of the kind of business it has and\nhandles the ingress and egress of data (pipelines), data enrichment, enforcement/\nmanagement of access controls and governance policies, and data analysis. Depend‐\ning on the company size and/or complexities of data within each line of business,\nthese tasks may be handled by just one person, a handful of people, or a large team.\nThere are several reasons this process tends to be quite successful. The first is that the\namount of data any given “team” must wrap its arms around is smaller. Instead of\nhaving to look at and manage a company’s entire  data story, the team needs to under‐\nstand and work on only a portion of it. Not only does this result in less work, but it\nalso allows for deeper knowledge of that data. Deeper knowledge of data has a multi‐\ntude of advantages, a few being quicker data enrichment and the ability to run\nquicker, more robust analytics.\nThe second reason this process is successful is that there is clear, identifiable owner‐\nship and accountability for data: there is a specific person (or persons) to go to when\nsomething goes wrong or when something needs to change (such as the addition of a\nnew data source or the implementation of a new compliance policy). When there is\nno clear accountability and responsibility for data, it’s easy for that data to be lost or\nforgotten or worse—mismanaged.\nIn Figure 3-5 , we’ve tried to show an example flow of different lines of business into a\ncentral repository. As you can see, retail store sales, marketing, online sales, and HR\nnot only all feed into the central enterprise data repository in this example, but they\nare also fed by  this repository.\nTo give you an idea of the different key hats and their tasks in each line of business,\nlet’s take marketing as an example. In this line of business in our example, we have\nthe hats of data owner, data steward, and business analyst.\nThe data owner sets up and manages the pipelines in this line of business, along with\nmanaging requests for new pipelines and ingestion sources. They also perform the\ntasks of monitoring, troubleshooting, and fixing any data quality issues that arise, as\nwell as implementing any of the technical aspects of the company’s governance poli‐\ncies and strategies.\n80 | Chapter 3: Ingredients of Data Governance: People and Processes\nThe data steward is the subject matter expert  (SME) in this line of business; knowing\nwhat data resides here, what it means, how it should be categorized/classed, and what\ndata is sensitive and what isn’t. They also serve as the point of contact between their\nline of business and the central governing body at their company for staying up to\ndate on compliance and regulations, and they’re responsible for ensuring that the data\nin their line of business is in compliance.\nFinally, the business analyst is the expert on the business implications for the data in\nthis line of business. They’re also responsible for knowing how their data fits into the\nbroader enterprise, and for communicating which data from their line of business\nshould be used in enterprise analytics. In addition, they need to know what addi‐\ntional/new data will need to be collected for this particular line of business to help\nanswer whatever the current or future business questions are.\nFrom this example you can see how each hat has their role and tasks for just this one\nline of business, and how a breakdown of any of those tasks can result in a less than\nefficient flow/implementation of a governance program.\nFigure 3-5. Flowchart of an enterprise with four lines of business and their data ingress/\negress, as well as a close-up of one line of business, its key hats\nThis process, while successful, does come with some pitfalls, however. The primary\none is that segregating data by line of business encourages the siloing of data and can,\ndepending on how it’s set up, inhibit cross-company analytics.\nWe recently worked with a large retail company that was dealing with this exact issue.\nPeople and Process Together: Considerations, Issues, and Some Successful Strategies | 81\nThis large US retailer (with over 350,000 employees), in an effort to better deal with\nthe tremendous amount of data it collects, divided up its data by several lines of busi‐\nness. These included air shipping, ground shipping, retail store sales, and marketing.\nHaving the data separated in this way greatly enabled the company to devote specific\ndata analytics teams to each line of business, whereby it could more quickly enrich its\ndata, allowing it to apply governance policies to aid with CCPA compliance. This\nstrategy, however, created issues when the company began to want to run analytics\nacross  its different lines of business. To separate its data, the company created infra‐\nstructure and storage solutions with data pipelines that fed directly (and only) into\none or another line of business. We won’t go into too much depth on data pipelines\nhere, but in short, it is a common practice to have data “land” in only one storage\nsolution, because duplicating data and transferring it to additional storage areas is\ncostly and difficult to maintain. Because specific data resided only in one storage\narea/silo, the company could not run analytics across its lines of business to see what\npatterns might emerge between, for example, air shipping and retail store sales\n(Figure 3-6 ).\nFigure 3-6. The preceding example company’s two data silos: air shipping and ground\nshipping. Each silo has its own data pipeline and stores this data within its own bucket,\nmeaning analytics can be run only within silos unless data from that pipeline is duplica‐\nted and piped into another silo.\nThis process of segregation by line of business to aid with accountability and respon‐\nsibility is not a bad strategy, and while it’s obvious there are pitfalls, there are also\nways to make it more successful, which we will cover later in the book.\nCreation of “views” of datasets\nA classic strategy employed by many companies is to create different “views” of\ndatasets . As seen in Table 3-3 , these views are really just different versions of the same\ndataset and/or table that have sensitive information sanitized or removed.\n82 | Chapter 3: Ingredients of Data Governance: People and Processes\nTable 3-3. Three  potentially different  types of “views” of data: one plain text, one with\nsensitive data hashed, and another with sensitive data redacted\nPlain text customer name Hashed customer name Redacted customer name\nAnderson, Dan Anderson, ##### ********\nBuchanan, Cynthia Buchanan, ##### ********\nDrexel, Frieda Drexel, ##### ********\nHarris, Javiar Harris, ##### ********\nThis strategy is classic (and works) because it allows analytics to easily be run, worry\nfree, by virtually anyone on the “clean” view (the one with sensitive data either hashed\nor removed). It takes away the risk of access to and usage of sensitive data.\nWhile this strategy works, it is problematic in the long run for several reasons. The\nfirst is that it takes quite a bit of effort and manpower to create these views. The clean\nview has to be manually created by someone who does have access to all of the data.\nThey must go in and identify any and all columns, rows, or cells that contain sensitive\ndata and decide how they should be treated: hashed, aggregated, completely removed,\netc. They then must create an entirely new dataset/table with these treatments in place\nand make it available to be used for analytics.\nThe second issue is that new views constantly need to be created as fresher data\ncomes in. This results not only in much time and effort being put into creating “fresh”\nviews but also in a proliferation of datasets/tables that are difficult to manage. Once\nall these views are created (and re-created), it’s hard to know which is the most fresh\nand should be used. Past datasets/tables often can’t be immediately deprecated, as\nthere may be a need down the line for that specific data.\nWhile views have some success and merit in aiding in access controls, we will pro‐\npose and discuss some strategies we have seen that not only are easier to manage but\nscale better as a company collects more and more data.\nA culture of privacy and security\nThe final process we’ d like to discuss is that of creating a culture of privacy and secu‐\nrity. While certainly every company and employee should respect data privacy and\nsecurity, the ways in which this strategy is thought through and implemented are\ntruly a special ingredient in creating not just a good data governance strategy but a\nsuccessful one.\nWe have dedicated an entire chapter to the strategy of building a successful data cul‐\nture (see Chapter 9 ), so more on that is to come; however, as a short introduction to\nthe concept, we will note that we have seen companies driven to really look at their\ndata culture and work to implement a new (successful) one because of one (or all) of\nthe following occurring within their organization:\nPeople and Process Together: Considerations, Issues, and Some Successful Strategies | 83",17414
32-Chapter 4. Data Governance over a Data Life Cycle.pdf,32-Chapter 4. Data Governance over a Data Life Cycle,"•Their governance/data management tools are not working sufficiently (meaning\nthe tools do not in and of themselves provide all of the governance “functional‐\nity” desired by a company).\n•People are accessing data they should not be accessing and/or using data in ways\nthat are not appropriate (whether intentionally or not).\n•People are not following processes and procedures put into place (again, whether\nintentionally or not).\n•The company knows that governance standards/data compliance are not being\nmet and don’t know what else to do, but they hope that educating people on\ndoing the “right thing” will help.\nTo be sure, throughout this text we have already covered in great detail governance\ntools, the people involved, and the processes that are/can be followed. This is where\nthe importance of putting all the pieces together can be seen—i.e., tools, people, and\nprocess. One or even two pieces are insufficient to achieve a successful  governance\nstrategy.\nAs evidenced by the four reasons we’ve just listed even though a few tools, some peo‐\nple, and several processes are in place, some gaps may still remain.\nThis is where it must all come together—in a collective data culture  that encompasses\nand embodies how the company thinks about and will execute its their governance\nstrategy. That encompasses what tools it will use, what people it will need, and the\nexact processes that will bring it all together.\nSummary\nIn this chapter, we have reviewed multiple unique considerations regarding the peo‐\nple and process of data governance for different kinds of companies. We’ve also cov‐\nered some issues commonly faced by companies, as well as some strategies we’ve seen\nimplemented with varying success.\nFrom our discussion it should be clear that data governance is not simply an imple‐\nmentation of tools but that the overall process and consideration of the people\ninvolved—while it may vary slightly from company to company or industry to indus‐\ntry—is important and necessary for a successful data governance program.\nThe process for how to think about data, how it should be handled and classified\nfrom the beginning, how it continually needs to be (re)classified and (re)categorized,\nand who will do this work and be responsible for it— coupled  with the tools that\nenable these tasks to be done efficiently and effectively—is key, if not mandatory, for\nsuccessful data governance.\n84 | Chapter 3: Ingredients of Data Governance: People and Processes",2507
33-Data Processing.pdf,33-Data Processing,"CHAPTER 4\nData Governance over a Data Life Cycle\nIn previous chapters, we introduced governance, what it means, and the tools and\nprocesses that make governance a reality, as well as the people and process aspects of\ngovernance. This chapter will bring together those concepts and provide a data life\ncycle approach to operationalize governance within your organization.\nY ou will learn about a data life cycle, the different phases of a data life cycle, data life\ncycle management, applying data governance over a data life cycle, crafting a data\ngovernance policy, best practices along each life cycle phase, applicable examples, and\nconsiderations for implementing governance. For some, this chapter will validate\nwhat you already know; for others, it will help you ponder, plant seeds, and consider\nhow these learnings can be applied within your organization. This chapter will intro‐\nduce and address a lot of concepts that will help you get started on the journey to\nmaking governance a reality. Before getting into the detailed aspects of governance,\nit’s important to center our understanding on data life cycle management and what it\nmeans for governance.\nWhat Is a Data Life Cycle?\nDefining what a data life cycle is should be easy—but in reality, it’s quite complex. If\nyou look up the definition of a data life cycle and its phases, you quickly realize that it\nvaries from one author to another, and from one organization to another. There’s\nhonestly not one right way to think about the different stages a piece of data goes\nthrough; however, we can all agree that each phase that is defined has certain charac‐\nteristics that are important in distinguishing it from the other phases. And because of\nthese different characteristics within each phase, the way to think about governance\nwill also vary as each piece of data moves through the data life cycle. In this chapter,\nwe will define a data life cycle as the order of stages a piece of data goes through from\n85\nits initial generation or capture to its eventual archival or deletion at the end of its\nuseful life.\nIt’s important to quickly point out that this definition tries to capture the essence of\nwhat happens to a piece of data; however, not all data goes through each phase, and\nthese phases are simply logical dependencies and not actual data flows.\nOrganizations work with transactional data  as well as with analytical data . In this\nchapter, we will primarily focus on the analytics data life cycle, from the point when\ndata is ingested into a platform all the way to when it is analyzed, visualized, purged,\nand archived.\nTransactional systems  are databases that are optimized to run day-to-day transactional\noperations. These are fully optimized systems that allow for a high number of concur‐\nrent users and transaction types. Even though these systems generate data, most are\nnot optimized to run analytics processes. On the other hand, analytical systems  are\noptimized to run analytical processes. These databases store historical data from vari‐\nous sources, including CRM, IOT sensors, logs, transactional data (sales, inventory),\nand many more. These systems allow data analysts, business analysts, and even exec‐\nutives to run queries and reports against the data stored in the analytic database.\nAs you can quickly see, transactional data and analytical data can have completely\ndifferent data life cycles depending on what an organization chooses to do. That said,\nfor many organizations, transactional data is usually moved to an analytics system for\nanalysis and will therefore undergo the phases of a data life cycle that we will outline\nin the following section.\nProper oversight of data throughout its life cycle is essential for optimizing its useful‐\nness and minimizing the potential for errors. Data governance is at the core of mak‐\ning data work for businesses. Defining this process end-to-end across the data life\ncycle is needed to operationalize data governance and make it a reality. And because\neach phase has distinct governance needs, this ultimately helps the mission of data\ngovernance.\nPhases of a Data Life Cycle\nAs mentioned earlier, you will see a data life cycle represented in many different ways,\nand there’s no right or wrong answer. Whichever framework you choose to use for\nyour organization has to be the one guiding the processes and procedures you put in\nplace. Each phase of the data life cycle as shown in Figure 4-1  has distinct characteris‐\ntics. In this section, we will go through each phase of the life cycle as we define it,\ndelve into what each phase means, and walk through the implications for each phase\nas you think about governance.\n86 | Chapter 4: Data Governance over a Data Life Cycle\n1Andy Patrizio, “IDC: Expect 175 Zettabytes of Data Worldwide by 2025” , Network World , December 3, 2018.\nFigure 4-1. Phases of a data life cycle\nData Creation\nThe first phase of the data life cycle is the creation or capture of data. Data is gener‐\nated from multiple sources, in different formats such as structured or unstructured\ndata, and in different frequencies (batch or stream). Customers can choose to use\nexisting data connectors, build ETL pipelines, and/or leverage third-party ingestion\ntools to load data into a data platform or storage system. Metadata—data about\ndata—can  also be created and captured in this phase. Y ou will notice data creation\nand data capture  used interchangeably, mostly because of the source of data. When\nnew data is created, that is referred to as data creation, and when existing data is fun‐\nneled into a system, it is referred to as data capture.\nIn Chapter 1 , we mentioned that the rate at which data is generated is growing expo‐\nnentially, with IDC predicting that worldwide data will grow to 175 zettabytes by\n2025.1 This is enormous! Data is typically created in one of these three ways:\nData acquisition\nWhen an organization acquires data that has been produced by a third-party\norganization\nData entry\nWhen new data is manually entered by humans or devices within the\norganization\nData capture\nWhen data generated by various devices in an organization, such as IOT sensors,\nis captured\nIt’s important to mention that each of these ways of generating data offers significant\ndata governance challenges. For example, what are the different checks and balances\nfor data acquired from outside your organization? There are probably contracts and\nagreements that outline how the enterprise is allowed to use this data and for what\npurposes. There might also be limitations as to who can access that specific data. All\nthese offer considerations and implications for governance. Later in the chapter, we\nwill look at how to think about governance during this phase, and we will call out the\ndifferent tools you should think about when designing your governance strategy.\nPhases of a Data Life Cycle | 87",6978
34-Data Storage.pdf,34-Data Storage,,0
35-Data Archiving.pdf,35-Data Archiving,"Data Processing\nOnce data has been captured, it is then processed, without yet deriving any value\nfrom it for the enterprise. This is done prior to its use. Data processing  is also referred\nto as data maintenance , and this is when data goes through processes such as integra‐\ntion, cleaning , scrubbing , or extract-transform-load  (ETL) to get it ready for storage\nand eventual analysis.\nIn this phase, some of the governance implications that you will come across are data\nlineage, data quality, and data classification. All these have been discussed in much\nmore detail in Chapter 2 . To make governance a reality, how do you make sure that as\ndata is being processed, its lineage is tracked and maintained? In addition, checking\ndata quality is very important to make sure you’re not missing any important values\nbefore storing this data. Y ou should also think about data classification. How are you\ndealing with sensitive information? What is it? How are you ensuring management of\nand access to this data so it doesn’t get into the wrong hands? Finally, as this data is\nmoving, it needs to be encrypted in transit and then later at rest. There are a lot of\ngovernance considerations during this phase. We will delve into these concepts later\nin the chapter.\nData Storage\nThe third phase in the data life cycle is data storage , where both data and metadata are\nstored on storage systems and devices with the appropriate levels of protection.\nBecause we’re focusing on the analytics data life cycle, a storage system could be a\ndata warehouse, a data mart, or a data lake. Data should be encrypted at rest to pro‐\ntect it from intrusions and attacks. In addition, data needs to be backed up to ensure\nredundancy in the event of a data loss, accidental deletion, or disaster.\nData Usage\nThe data usage  phase is important to understanding how data is consumed within an\norganization to support the organization’s objectives and operations. In this phase,\ndata becomes truly useful and empowers the organization to make informed business\ndecisions when it can be viewed, analyzed, and/or visualized for insights. In this\nphase, users get to ask all types of questions of the data, via a user interface or busi‐\nness intelligence tools, with the hope of getting “good” answers. This is where the\nrubber meets the road, especially when confirming whether the governance processes\nalready instituted in previous phases truly work. If data quality is not implemented\ncorrectly, the types of answers you receive will be incorrect or might not make too\nmuch sense, and this could potentially jeopardize your business operations.\n88 | Chapter 4: Data Governance over a Data Life Cycle",2718
36-Data Life Cycle Management.pdf,36-Data Life Cycle Management,"In this phase, data itself may be the product or service that the organization offers. If\ndata is indeed the product, then different governance policies need to be enacted to\nensure proper handling of this data.\nBecause data is consumed by multiple internal and external stakeholders and pro‐\ncesses during this phase, proper access management and audits are key. In addition,\nthere might be regulatory or contractual constraints on how data may actually be\nused, and part of the role of data governance is to ensure that these constraints are\nobserved accordingly.\nData Archiving\nIn the data archiving  phase, data is removed from all active production environments\nand copied to another environment. It is no longer processed, used, or published, but\nis stored in case it is needed again in an active production environment. Because the\nvolume of data generated is growing, it’s inevitable that the volume of archived data\ngrows. In this phase, no maintenance or general usage occurs. A data governance\nplan should guide the retention of this data and define the length of time it will be\nstored, including the different controls that will be applied to this data.\nData Destruction\nIn this final phase, data is destroyed. Data destruction , or purging , refers to the\nremoval of every copy of data from an organization, typically done from an archive\nstorage location. Even if you wanted to save all your data forever, it’s just not feasible.\nIt’s very expensive to store data that is not in use, and compliance issues create the\nneed to get rid of data you no longer need. The primary challenge of this phase is\nensuring that all the data is properly destroyed and at the right time.\nBefore destroying any data, it is critical to confirm whether there are any policies in\nplace that would require you to retain the data for a certain period of time. Coming\nup with the right timeline for this cycle means understanding state and federal regu‐\nlations, industry standards, and governance policies to ensure that the right steps are\ntaken. Y ou will also need to prove that the purge has been done properly, which\nensures that data doesn’t consume more resources than necessary at the end of its\nuseful life.\nY ou should now have a solid understanding about the different phases of a data life\ncycle and what some of the governance implications are. As stated previously, these\nphases are logical dependencies and not necessarily actual data flows. Some pieces of\ndata might go back and forth between different processing systems before being\nstored. And some that are stored in a data lake might skip processing altogether and\nget stored first, and then get processed later. Data does not need to pass through all\nthe phases.\nPhases of a Data Life Cycle | 89\nWe’re sure you’ve heard the phrase “Rome was not built in a day, ” but that’s really\nwhat this data life cycle is trying to do. Applying data governance in an organization\nis a daunting task and can be very overwhelming. However, if you think about your\ndata within these logical data life cycle phases, implementing governance can be a\ntask that can be broken down into each phase and then thought through and imple‐\nmented accordingly.\nData Life Cycle Management\nNow that you understand the data life cycle, another common term you will run into\nis data life cycle management  (DLM). What’s interesting is that many authors will\nrefer to data life cycle and data life cycle management interchangeably. Even though\nthere might be a need or desire to bundle them together, it’s important to realize that\na data life cycle can exist without data life cycle management. DLM, therefore, refers\nto a comprehensive policy-based approach to manage the flow of data throughout its\nlife cycle, from creation to the time when it becomes obsolete and is purged. When an\norganization is able to define and organize the life cycle processes and practices into\nrepeatable steps for the company, this refers to DLM. As you start learning about\nDLM, you will quickly run into a data management plan . So let’s quickly look at what\nthat means and what it entails.\nData Management Plan\nA data management plan (DMP) defines how data will be managed, described, and\nstored. In addition, it defines standards you will use and how data will be handled\nand protected throughout its life cycle. Y ou will primarily see data management plans\nrequired to drive research projects within institutions, but the concepts of the process\nare fundamental to implementing governance. Because of this, it’s worth us doing a\ndeep dive into them and seeing how these could be applied to implement governance\nwithin an organization.\nWith governance, you will quickly realize that there is no a lack of templates and\nframeworks—see, for example, the DMPTool from Massachusetts Institute of Tech‐\nnology . Y ou simply need to pick a plan or framework that works for your project and\norganization and march ahead; there’s not one right or wrong way to do it. If you\nchoose to use a data management plan, here is some quick guidance to get you\nstarted. The concepts here are much more fundamental than the template or frame‐\nwork, so if you were able to capture these in a document, then you’ d be ahead of the\ncurve.\n90 | Chapter 4: Data Governance over a Data Life Cycle\nGuidance 1: Identify the data to be captured or collected\nData volume is important to helping you determine infrastructure costs and people\ntime. Y ou need to know how much data you’re expecting and the types of data you\nwill be collecting:\nTypes\nOutline the various types of data you will be collecting. Are they structured or\nunstructured? This will help determine the right infrastructure to use.\nSources\nWhere is the data coming from? Are there restrictions on how this data can be\nused or manipulated? What are those rules? All of these need to be documented.\nVolume\nThis can be a little difficult to predict, especially with the exponential growth in\ndata; however, planning for that increase early on and projecting what it could be\nwould set you apart and help you be prepared for the future.\nGuidance 2: Define  how the data will be organized\nNow that you know the type, sources, and volume of data you’re collecting, you need\nto determine how that data will be managed. What tools do you need across the data\nlife cycle? Do you need a data warehouse? Which type, and from which vendor? Or\ndo you need a data lake? Or do you need both? Understanding these implications and\nwhat each means will allow you to better define what your governance policies should\nbe. There are many regulations that govern how data can and cannot be used, and\nunderstanding them is vital.\nGuidance 3: Document a data storage and preservation strategy\nDisasters happen, and ensuring that you’ve adequately prepared for one is very\nimportant. How long will a piece of data be accessible, and by whom? How will the\ndata be stored and protected over its life? As we mentioned previously, data purging\nneeds to happen according to the rules set forth. In addition, understanding what\nyour systems’ backup and retention policies are, is important.\nGuidance 4: Define  data policies\nIt’s important to document how data will be managed and shared. Identify the licens‐\ning and sharing agreements that pertain to the data you’re collecting. Are there\nrestrictions that the organization should adhere to? What are the legal and ethical\nrestrictions on access to and use of sensitive data, for example? Regulations like\nGDPR and CCPA can easily get confusing and can even become contradictory. In this\nData Life Cycle Management | 91",7709
37-Applying Governance over the Data Life Cycle.pdf,37-Applying Governance over the Data Life Cycle,,0
38-Data Governance Framework.pdf,38-Data Governance Framework,"step, ensure that all the applicable data policies are captured accordingly. This also\nhelps in case you’re audited.\nGuidance 5: Define  roles and responsibilities\nChapter 3  defined roles and responsibilities. With those roles in mind, determine\nwhich are the right ones for your organization and what each one means for you.\nWhich teams will be responsible for metadata management and data discovery? Who\nwill ensure governance policies are followed all the way? And there are many more\nroles that you can define.\nA DMP should provide your organization with an easy-to-follow roadmap that will\nguide others and explain how data will be treated throughout its life cycle. Think of\nthis as a living document that evolves with your organization as new datasets are cap‐\ntured, and as new laws and regulations are enacted.\nIf this was a data management plan for a research project, it would have included a\nlot more steps and items for consideration. Those plans tend to be more robust\nbecause they guide the entire research project and data end-to-end. We will cover a\nlot more concepts later in the chapter, so we chose to select items that were easily\ntransferable to creating a governance policy and plan for your organization.\nApplying Governance over the Data Life Cycle\nWe’ve gone through fundamental concepts thus far; now let’s bring everything\ntogether and look at how you can apply governance over the data life cycle. Gover‐\nnance needs to bring together people, processes, and technology to govern data\nthroughout its life cycle. In Chapter 2 , we outlined a robust set of tools to make gov‐\nernance a reality, and Chapter 3  focused on the people and process side of things. It’s\nimportant to point out that implementing governance is complicated; there’s no easy\nway to simply apply everything and consider he job done. Most technologies need to\nbe stitched together, and as you can imagine, they’re all coming from different ven‐\ndors with different implementations. Y ou would need to integrate the best-in-class\nsuite of products and services to make things work. Another option is to purchase a\nfully integrated data platform or governance platform. This is not a trivial task.\nData Governance Framework\nFrameworks help you visualize the plan, and there are several frameworks that can\nhelp you think about governance across the data life cycle. Figure 4-2  is one such\nframework in which we highlight all the concepts from Chapter 2 , overlaid with the\nconcepts we’ve discussed in this chapter.\n92 | Chapter 4: Data Governance over a Data Life Cycle\nFigure 4-2. Governance over a data life cycle\nThis framework oversimplifies things to make them easier to understand; it assumes\nthings are linear, from left to right, which is usually not the case. When data is inges‐\nted from various sources on the left, this is simply at the point of data creation or cap‐\nture. That data is then processed and stored, and then it is consumed by the different\nstakeholders, including data analysts, data engineers, data stewards, and so on.\nData archiving and data destruction are not reflected in this framework because those\ntake place beyond the point when data is used. As we previously outlined, during\narchiving, data is removed from all active production environments. It is no longer\nprocessed, used, or published but is stored in case it is needed again in the future.\nDestruction is when data comes to the end of its life and is removed according to\nguidelines and procedures that have been set forth.\nOne discrepancy that you will quickly notice is that metadata management should be\nconsidered from the point of data creation—where enterprises need to discover and\ncurate the data as it’s ingested (especially for sensitive data)—to when data is stored\nand discovered in the applicable storage system. Archiving, even though mentioned\nwithin data management, tends to happen when the data’s usefulness is done and it is\nremoved from production environments. Though archiving is an important part of\ngovernance, this diagram implies that it is taking place in the middle of the data life\ncycle. That said, it’s also possible to have an archiving strategy when data is simply\nstored in the applicable storage systems, so we cannot completely rule this out.\nApplying Governance over the Data Life Cycle | 93",4377
39-Data Governance in Practice.pdf,39-Data Governance in Practice,"We want to reiterate that Figure 4-2  provides a logical representation of the phases a\npiece of data goes through, from left to right, and not necessarily the actual step-by-\nstep flow of the data. There’s a lot of back and forth that happens between each phase,\nand not all pieces of data go through each one of these phases.\nFrameworks are good at providing a holistic view of things. However, they are not the\nbe-all, end-all. Make sure whichever framework you select works for your organiza‐\ntion and your data.\nWe’ve mentioned it already, but would like to emphasize again the\nidea of selecting a framework that work for your organization. This\ncan include considerations around the kind of data you collect or\nwork with, as well as what kind of personnel you have dedicated to\nyour data governance efforts. One thing we challenge you to con‐\nsider is how to take what you have and fit enough framework\naround it. Take these ideas as laid out (noting that not each step is\nrequired or even necessary) and layer on what you have to work\nwith currently as a place to start. More pieces (and people, for that\nmatter) can be added later, but if you focus on at least laying the\ngroundwork—the foundation—you will be in a much better posi‐\ntion if or when you do have more pieces to add to your framework.\nData Governance in Practice\nOpenStreetMap (OSM)  was created by Steve Coast in the UK in 2004 and was\ninspired by the success of Wikipedia. It is open source, which means it is created by\npeople like you and is free to use under an open license. It was a response to the pro‐\nliferation of siloed, proprietary international geographical data sources and dozens of\nmapping software products that didn’t talk to each other. OSM has grown signifi‐\ncantly to over two million contributors, and what’s amazing is that it works. In fact, it\nworks well enough to be the trusted source of data for a number of Fortune 500 com‐\npanies, including other small and medium-sized businesses. With so many contribu‐\ntors, OSM is successful because it was able to establish data standards early in the\nprocess and ensured contributors adhered to them. As you can imagine, a crowd‐\nsourced mapping system without a way to standardize contributor data could go\nwrong very quickly. Defining governance standards can bring value to your organiza‐\ntion and provide trusted data for your users.\nAnd now that you have an understanding of the data life cycle with an overlay of the\ndifferent governance tools, let’s delve further into how the different data governance\ntools we outlined in Chapters 1 and 2 can be applied and used across this life cycle.\nThis section also includes best practices, which can help you start to define your\norganization’s data standards.\n94 | Chapter 4: Data Governance over a Data Life Cycle\nData creation\nAs previously mentioned, this is the initial phase of the data life cycle, where data is\ncreated or captured. During this phase, an organization can choose to capture both\nthe metadata, and the lineage of the data. Metadata describes the data, while the line‐\nage describes the where  of the data and how it will flow and be transformed and used\ndownstream. Trying to capture these during this initial phase sets you up well for the\nlater phases.\nIn addition, processes such as classification and profiling can be employed, especially\nif you’re dealing with sensitive data assets. Data should also be encrypted in transit to\noffer protection from intrusions and attacks. Cloud service providers such as Google\nCloud offer encryption in transit and at rest by default.\nDefine  the Type of Data\nEstablish a set of guidelines for categorizing data that takes into\naccount the sensitivity of the information as well its criticality and\nvalue to the organization. Profiling and classifying data helps\ninform which governance policies and procedures apply to the\ndata.\nData processing\nDuring this phase, data goes through processes such as integration, cleaning, scrub‐\nbing, or extract-transform-load (ETL) prior to its use, to get it ready for storage and\neventual analysis. It’s important that the integrity of the data is preserved during this\nphase; that is why data quality plays a critical role.\nLineage needs to be captured and tracked here also, to ensure that the end users\nunderstand which processes led to which transformation and where the data origina‐\nted from. We heard this from one user: “It would be nice to have a better understand‐\ning of the lineage of data. When finding where a certain column in a table comes\nfrom, I need to manually dig through the source code of that table and follow that\ntrail (if I have access). Automate this process. ” This is a common pain point felt by\nmany, and one in which DLM and governance are critical.\nDocument Data Quality Expectations\nDifferent data consumers may have different data quality require‐\nments, so it’s important to provide a means to document data qual‐\nity expectations while the data is being captured and processed, as\nwell as techniques and tools for supporting the data’s validation and\nmonitoring as it goes through the data life cycle. The right pro‐\ncesses for data quality management will provide measurable and\ntrustworthy data for analysis.\nApplying Governance over the Data Life Cycle | 95\nData storage\nIn this phase, both data and metadata are stored and made ready for analysis. Data\nshould be encrypted at rest to protect it from intrusions and attacks. In addition, data\nneeds to be backed up to ensure redundancy.\nAutomated Data Protection and Recovery\nBecause data is stored in storage devices in this phase, find solu‐\ntions and products that provide automated data protection to\nensure that exposed data cannot be read, including encryption at\nrest, encryption in transit, data masking, and permanent deletion.\nIn addition, implement a robust recovery plan to protect your busi‐\nness when a disaster strikes.\nData usage\nIn this phase, data is analyzed and consumed for insights and consumed by multiple\ninternal and external stakeholders and processes in the organization. In addition,\nanalyzed data is visualized and used to support the organization’s objectives and oper‐\nations; business intelligence tools play a critical role in this phase.\nA data catalog is vital to helping users discover data assets using captured metadata.\nPrivacy, access management, and auditing are paramount at this stage, which ensures\nthat the right people and systems are accessing and sharing the data for analysis. Fur‐\nthermore, there might be regulatory or contractual constraints on how data may\nactually be used, and part of the role of data governance is to ensure that these con‐\nstraints are observed.\nData Access Management\nIt is important to provide data services that allow data consumers\nto access their data with ease. Documenting what and how the data\nwill be used, and for what purposes, can help you define identities,\ngroups, and roles and assign access rights to establish a level of\nmanaged access. This ensures that only authorized and authentica‐\nted individuals and systems are able to access data assets according\nto defined rules.\nData archiving\nIn this phase, data is removed from all active production environments. It is no\nlonger processed, used, or published but is stored in case it is needed again in the\nfuture. Data classification should guide the retention and disposal method of data.\n96 | Chapter 4: Data Governance over a Data Life Cycle",7578
40-Example of How Data Moves Through a Data Platform.pdf,40-Example of How Data Moves Through a Data Platform,"Automated Data Protection Plan\nBeyond being a way to prevent unauthorized individuals from\naccessing data, perimeter security is not and never has been suffi‐\ncient for protecting data. The same protections applied in data stor‐\nage would apply here as well to ensure that exposed data cannot be\nread, including encryption at rest, data masking, and permanent\ndeletion. In addition, in case of a disaster, or in the event that\narchive data is needed again in a production environment, it’s\nimportant to have a well-defined process to revive this data and\nmake it useful.\nData destruction\nFinally, data is destroyed, or rather, it is removed from the enterprise at the end of its\nuseful life. Before purging any data, it is critical to confirm whether there are any pol‐\nicies in place that would require you to retain the data for a certain period of time.\nData classification should guide the retention and disposal method of data.\nCreate a Compliance Policy\nComing up with the right timeline for this cycle means under‐\nstanding state and federal regulations, industry standards, and gov‐\nernance policies and staying up to date on any changes. Doing so\nhelps to ensure that the right steps are taken and that the purge has\nbeen done properly. It also ensures that data doesn’t consume more\nresources than necessary at the end of its useful life.\nIT stakeholders are urged to revisit the guidelines for destroying\ndata every 12–18 months to ensure compliance, since rules change\noften.\nExample of How Data Moves Through a Data Platform\nHere’s an example scenario of how data could move through a data platform with the\nframework in Figure 4-2 .\nScenario\nLet’s say that a business wants to ingest data onto a cloud-data platform, like Google\nCloud, AWS, or Azure, and share it with data analysts. This data may include sensi‐\ntive elements such as US social security numbers, phone numbers, and email\naddresses. Here are the different pieces it might go through:\nApplying Governance over the Data Life Cycle | 97\n1.Business configures an ingestion data pipeline using a batch or streaming service:\na.Goal: As they move raw data into the platform, it will need to be scanned,\nclassified, and tagged before it can be processed, manipulated, and stored.\nb.Staged ingestion buckets:\ni.Ingest: heavily restricted\nii.Released: processed data\niii.Admin quarantine: needs review\n2.Data is then scanned and classified for sensitive information such as PII.\n3.Some data may be redacted, obfuscated, or anonymized/de-identified. This pro‐\ncess may generate new metadata, such as what keys were used for tokenization.\nThis metadata would be captured at this stage.\n4.Data is tagged with PII tags/labels.\n5.Aspects of data quality can be accessed—that is, are there any missing values, are\nprimary keys in the right format, etc.\n6.Start to capture data provenance information for lineage.\n7.As data moves between the different services along the life cycle, it is encrypted\nin transit.\n8.Once ingestion and processing are complete, the data will need to be stored in a\ndata warehouse and/or a data lake, where it is encrypted at rest. Backup and\nrecovery processes need to be employed as well, in case of a disaster.\n9.While in storage, additional business and technical metadata can be added to the\ndata and cataloged, and users need to be able to discover and find the data.\n10.Audit trails need to be captured throughout this data life cycle and made visible\nas needed. Audits allow you to check the effectiveness of controls in order to\nquickly mitigate threats and evaluate overall security health.\n11.Throughout this process, it is important to ensure that the right people and serv‐\nices have access and permissions to the right data across the data platform using a\nrobust identity and access management (IAM) solution.\n12.Y ou need to be able to run analytics and visualize the results for use. In addition\nto access management, additional privacy, de-identification, and anonymization\ntools may be employed.\n13.Once this data is no longer needed in a production environment, it is archived\nfor a determined period of time to maintain compliance.\n14.At the end of its useful life, it is completely removed from the data platform and\ndestroyed.\n98 | Chapter 4: Data Governance over a Data Life Cycle\nCase Study: Nike and the 4% Improving Running Shoes\nIn 2018, Nike launched a new running shoe, and the related advertising campaign\nclaimed that the “Nike Zoom Vaporfly 4%” will make you run 4% faster. While a 4%\nspeed increase does not sound like a lot, over a marathon run that averages to 4–5\nhours, this can potentially lead to 10–12 minutes’ improvement in the time it takes to\nfinish the race.\nThe claims were significant and were met with skepticism. A Nike-sponsored study\ndid not help because it was based, by necessity, on a small dataset. Many athletes\nwould pay for an expensive shoe to improve their result by that margin, if the claim\nwere true, but the source of this information (the vendor) sowed doubt. Running an\nindependent scientific experiment to conclude whether or not the claim was true\nwould have been challenging, as this would have required significant investment, and\ngetting the runner(s) to use different kinds of shoes over the same courses in the same\nconditions to truly eliminate all possible variables and challenges.\nHappily, many athletes use a popular record-keeping app called Strava. Strava makes\nthe data publicly available, and many athletes also record the shoes they use when\nrunning ( Figure 4-3 ). This created a natural experiment in which you could look over\nexisting data and, with enough data, could potentially tease out patterns.\nFigure 4-3. Strava data. Does the Nike Vaporfly  make you run 4% faster?\nThe New York Times  investigated, collecting half a million real-life performance\nrecords from Strava.Keven Quealy and Josh Katz, “Nike Says Its $250 Running Shoes\nWill Make Y ou Run Much Faster. What If That’s Actually True?”  New York Times , July\n18, 2018. The next step was to determine whether or not the data was useful. While\nthe ideal way would have been to measure runs of identical runners on the same\ncourse with different shoes, this was not possible at this scale. However, the large\nApplying Governance over the Data Life Cycle | 99",6393
41-Developing a Data Governance Policy.pdf,41-Developing a Data Governance Policy,"2A natural experiment  is a situation in which you can identify experimental and control groups determined by\nfactors outside the control of the investigators. In our example here, the runners naturally fell into groups\ndefined by the shoes they wore, rather than being assigned shoes externally. The groups of runners were large\nenough to qualify for good “experimental” and “control” groups with controlled number of external factors.amount of data did enable the finding of natural experiments .2 This was not a small-\nscale lab experiment, but an actual account of (admittedly amateur, for the most part)\nweekend runners reporting and sharing their race results.\nThe New York Times  was able to compile 280,000 marathon and 215,000 half mara‐\nthon results, and then compared running conditions (same races) and concurrent\nresults from the same athletes (different shoes, different races or dates). These com‐\nparisons ensured that conditions similar to the ideal experiment were met, and by\nincluding knowledge about the races themselves (weather, course layout and diffi‐\nculty), the data was curated to keep the quality records in while eliminating outliers\n(less-frequented races, extreme conditions).\nThe New York Times  was able to conclude that the Nike shoes were very often part of\na more successful run for many runners. The paper pointed out that these results\nwere not gathered through a lab experiment in controlled conditions, but their results\nwere consistent with the Nike-funded study.\nThis effort would not have been possible without the availability of an open dataset,\ncontributed to freely by runners globally, and preserved and made accessible to\nresearchers. This example of a dataset in which data is made available under a con‐\ntrolled environment (Strava does protect runners’ personal data and allows runners\nfull control over how their data is shared, including the ability to opt out and delete\ntheir own data) is an excellent example of proper information cycle and data\ngovernance.\nOperationalizing Data Governance\nIt’s one thing to have a plan, but it’s something else to ensure that plan works for your\norganization. NASA learned things the hard way. In September 1999, after almost 10\nmonths of travel to Mars, the $125-million Mars Climate Orbiter  lost communication\nand then burned and broke into pieces a mere 37 miles away from the planet’s sur‐\nface. The analysis found out that, while NASA had used the metric system, one of its\npartners had used the International System of Units (SI). This inconsistency was not\ndiscovered until it was time to land the orbiter, leading to a complete loss of the satel‐\nlite. This of course was crushing to the team. After this incident, proper checks and\nbalances were implemented to ensure that something similar did not happen again.\nBringing things together so that issues such as the one NASA experienced are caught\nearly and rectified before a disaster happens starts with the creatiion of a data gover‐\n100 | Chapter 4: Data Governance over a Data Life Cycle\nnance policy. A data governance policy is a living, breathing document that provides\na set of rules, policies, and guidance for safeguarding an organization’s data assets.\nWhat Is a Data Governance Policy?\nA data governance policy is a documented set of guidelines for ensuring that an\norganization’s data and information assets are managed consistently and used prop‐\nerly. A data governance policy is essential in order to implement governance . The guide‐\nlines will include individual policies for data quality, access, security, privacy, and\nusage, which are paramount for managing data across its life cycle. In addition, data\ngovernance policies center on establishing roles and responsibilities for data that\ninclude access, disposal, storage, backup, and protection, which should all be familiar\nconcepts. This document helps to bring everything together toward a common goal.\nThe data governance policy is usually created by a data governance committee or data\ngovernance council, which is made up of business executives and other data owners.\nThis policy document defines a clear data governance structure for the executive\nteam, managers, and line workers to follow in their daily operations.\nTo get started operationalizing governance, a data governance charter template could\nbe useful. Figure 4-4  shows an example template that could help you socialize your\nideas across the organization and get the conversation started. Information in this\ntemplate will funnel directly into your data governance policy.\nUse the data governance charter template to kick off the conversation and get your\nteam assembled. Once it has bought into your vision, mission, and goals, that is the\nteam that will help you create and define your governance policy.\nOperationalizing Data Governance | 101\nFigure 4-4. Data governance charter template\nImportance of a Data Governance Policy\nWhen you have a business idea and are going to friends to socialize the idea and pos‐\nsibly get them to buy into it, you will quickly run into someone who asks for a busi‐\nness plan. “Do you have a business plan you can share so I can read more about this\nidea and what your plans are?” A data governance policy allows you to have all the\nimportant elements of operationalizing governance documented according to your\norganization’s needs and objectives. It also allows consistency within the organization\nover a long period of time. It is the document that everyone will refer to when ques‐\ntions and issues arise. It should be reviewed regularly and updated when things in the\norganization change. Y ou can consider it your business plan—or to another extreme,\nit can also be your governance bible.\nWhen a data governance policy is well drafted, it will ensure:\n•Consistent, efficient, and effective management of the data assets throughout the\norganization and data life cycle and over time.\n•The appropriate level of protection of the organization’s data assets based on their\nvalue and risk as determined by the data governance committee.\n•The appropriate protection and security levels for different categories of data as\nestablished by the governance committee.\n102 | Chapter 4: Data Governance over a Data Life Cycle",6327
42-Data Governance Policy Structure.pdf,42-Data Governance Policy Structure,"Developing a Data Governance Policy\nA data governance policy is usually authored by the data governance committee or\nappointed data governance council. This committee will establish comprehensive\npolicies for the data program that outline how data will be collected, stored, used, and\nprotected. The committee will identify risks and regulatory requirements and look\ninto how they will impact or disrupt the business.\nOnce all the risks and assessments have been identified, the data governance commit‐\ntee will then draft policy guidelines and procedures that will ensure the organization\nhas the data program that was envisioned. When a policy is well written, it helps cap‐\nture the strategic vision of the data program. The vision for the governance program\ncould be to drive digital transformation for the organization, or possibly to get\ninsights to drive new revenue or even to use data to provide new products or services.\nWhichever is the case for your organization, the policies drafted should all coalesce\ntoward the articulated vision and mission as outlined in the data governance charter\ntemplate.\nPart of the process of developing a data governance policy is establishing the expecta‐\ntions, wants, and needs of key stakeholders through interviews, meetings, and infor‐\nmal conversations. This will help you get valuable input, but it’s also an opportunity\nto secure additional buy-in for the program.\nData Governance Policy Structure\nA well-crafted policy should be unique to your organization’s vision, mission, and\ngoals. Don’t get hung up on every single piece of information on this template, how‐\never; use it more like a guide to help you think things through. With that in mind,\nyour governance policy should address:\nVision and mission for the program\nIf you used a data governance charter template as outlined in Figure 4-4  to get\nbuy-in from other stakeholders, that means you already have this information\nreadily available. As mentioned before, the vision for the governance program\ncould be to drive digital transformation for the organization, or to get insights to\ndrive new revenue, or even to use data to provide new products or services.\nPolicy purpose\nCapture goals for your organization’s data governance program, as well as metrics\nfor determining success. The mission and vision of the program should drive the\ngoals and success metrics.\nPolicy scope\nDocument the data assets covered by this governance policy. In addition, inven‐\ntory the data sources and determine data classifications based on whether data is\nOperationalizing Data Governance | 103\nsensitive, confidential, or publicly available, along with the levels of security and\nprotection required at the different levels.\nDefinitions  and terms\nThe data governance policy is usually viewed by stakeholders across the organi‐\nzation who might not be familiar with certain terms. Use this section to docu‐\nment terms and definitions to ensure everyone is on the same page.\nPolicy principles\nDefine rules and standards for the governance program you’re looking to set up\nalong with the procedures and programs to enforce them. The rules could cover\ndata access (who has access to what data), data usage (how the data will be used\nand details around what’s acceptable), data integration (what transformations the\ndata will undergo), and data integrity (expectations around data quality).\nDevelop best practices to protect data and to ensure regulations and compliance\nare effectively documented.\nProgram structure\nDefine roles and responsibilities (R&Rs), which are positions within the organi‐\nzation that will oversee elements of the governance program. A RACI chart could\nhelp you map out who is responsible, who is accountable, who needs to be con‐\nsulted, and who should be kept informed about changes. Information on gover‐\nnance R&Rs can be found in Chapter 3  of the book.\nPolicy review\nDetermine when the policy will be reviewed and updated and how adherence to\nthe policy will be monitored, measured, and remedied.\nFurther assistance\nDocument the right people to address questions from the team and other stake‐\nholders.\nIt’s not enough to document a data governance policy as outlined in Figure 4-5 , com‐\nmunicating it to all stakeholders is equally important. This could happen through a\ncombination of group meetings and training, one-on-one conversations, recorded\ntraining videos, and written communication.\n104 | Chapter 4: Data Governance over a Data Life Cycle",4543
43-Step-by-Step Guidance.pdf,43-Step-by-Step Guidance,"Figure 4-5. Example data governance policy template\nIn addition, review performance regularly with your data governance team to ensure\nthat you’re still on the right track. This also means regularly reviewing your data gov‐\nernance policy to make sure it still reflects the current needs of the organization and\nprogram.\nRoles and Responsibilities\nWhen operationalizing governance over a data life cycle, you will interact with many\nstakeholders within the organization, and you will need to bring them together to\nwork on this common goal. While it might be tempting to definitively say which roles\ndo what at which part of the data life cycle, as outlined in Chapter 3 , many data gov‐\nernance frameworks revolve around a complex interplay of roles and responsibilities.\nThe reality is that most companies rarely are able to exactly or fully staff governance\nroles due to lack of employee skill set or, more commonly, a simple lack of headcount.\nFor this reason, employees working in the information and data space of their com‐\npany often wear different user “hats. ”\nWe will not go into detail about roles and responsibilities in this chapter, because\nthey’re well outlined in Chapter 3 . Y ou still need to define what these look like within\nyour organization and how they will interplay with each other to make governance a\nreality for you. This will typically be outlined in a RACI matrix describing who is\n“responsible, accountable, to be consulted, and to be informed” within a certain\nenforcement, process, policy, or standard.\nOperationalizing Data Governance | 105\nStep-by-Step Guidance\nBy this section of the book, you should know that data governance goes beyond the\nselection and implementation of products and tools. The success of a data governance\nprogram depends on the combination of people, processes, and tools all working\ntogether to make governance a reality. This section will feel very familiar, because it\ngathers all the elements discussed in the previous section on data governance policy\nand puts them in a step-by-step process to show you how to get started. It further\ndouble clicks into the concepts as well.\nBuild the business case\nAs previously mentioned, data governance takes time and is expensive. If done\ncorrectly, it can be automated as part of the application design done at the source\nwith a focus on business value. That said, data governance initiatives will often\nvary in scope and objectives. Depending on where the initiative is originating\nfrom, you need to be able to build a business case that will identify critical busi‐\nness drivers and justify the effort and investment of data governance. It should\nidentify the pain points, outline perceived data risks, and indicate how gover‐\nnance helps the organization mitigate those risks and enable better business out‐\ncomes. It’s OK to start small, strive for quick wins, and build up ambitions over\ntime. Set clear, measurable, and specific goals. Y ou cannot control what you can‐\nnot measure; therefore you need to outline success metrics. The data governance\ncharter template in Figure 4-4  is perfect for helping you get started.\nDocument guiding principles\nDevelop and document core principles associated with governance and, of\ncourse, associated with the project you’re looking to get off the ground. A core\nprinciple of your governance strategy could be to make consistent and confident\nbusiness decisions based on trustworthy data aligned with all the various pur‐\nposes for the use of the data assets. Another core principle could be to meet regu‐\nlatory requirements and avoid fines or even to optimize staff effectiveness by\nproviding data assets that meet the desired data quality thresholds. Define princi‐\nples that are core to your business and project. If you’re still new to this area,\nthere are a lot of resources available. If you are looking online, there are several\nvendor-agnostic, not-for-profit associations, such as the Data Governance Insti‐\ntute (DGI) , the Data Management Association (DAMA) , the Data Governance\nProfessionals Organization (DGPO) , and the Enterprise Data Management\nCouncil , all of which provide great resources for business, IT, and data professio‐\nnals dedicated to advancing the discipline of data governance. In addition, iden‐\ntify whether there are any local data governance meetup groups or conferences\nthat you can possibly attend, such as the Data Governance and Information\nQuality Conference, DAMA International Events, or a Financial Information\nSummit.\n106 | Chapter 4: Data Governance over a Data Life Cycle\nGet management buy-in\nIt should be no surprise that without management buy-in, your governance ini‐\ntiative can easily be dead from the get-go. Management controls the big decisions\nand funding that you need. Outlining important KPIs, and how your plan helps\nto move them, will get management to be all ears. Engage data governance cham‐\npions and get buy-in from the key senior stakeholders. Present your business\ncase and guiding principles to C-level management for approval. Y ou need allies\non your side to help make the case. And once the project has gotten off the\nground, communicate frequently.\nDevelop an operating model\nOnce you have management approval, it’s time to get to work. How do you inte‐\ngrate this governance plan into the way of doing business in your enterprise? We\nintroduced you to the data governance policy, which can come in very handy\nduring this process. During this stage, define the data governance roles and\nresponsibilities, and then describe the processes and procedures for the data gov‐\nernance council and data stewardship teams who will define processes for defin‐\ning and implementing policies as well as for reviewing and remediating identified\ndata issues. Leverage the content from the data management policy plan to help\nyou define your operating model. Data governance is a team sport, with delivera‐\nbles from all parts of the business.\nDevelop a framework for accountability\nAs with any project you’re looking to bring to market, establishing a framework\nfor assigning custodianship and responsibility for critical data domains is para‐\nmount. Define ownership. Make sure there is visibility to the “data owners”\nacross the data landscape. Provide a methodology to ensure that everyone is\naccountable for contributing to data usability. Refer back to your data manage‐\nment policy, as it probably started to capture some of these dependencies.\nDevelop taxonomies and ontologies\nThis is where a lot of the education you’ve collected thus far comes in handy.\nWorking closely with governance associations, leaning in on your peers, and sim‐\nply learning about things online will help you with this step. There may be a\nnumber of governance directives associated with data classification, organization,\nand, in the case of sensitive information, data protection. To enable your data\nconsumers to comply with those directives, there must be a clear definition of the\ncategories (for organizational structure) and classifications (for assessing data\nsensitivity). These should be captured in your data governance policy.\nAssemble the right technology stack\nOnce you’ve assigned data governance roles to your staff and defined and\napproved your processes and procedures, you should then assemble a suite of\ntools that facilitates implementation and ongoing validation of compliance with\nOperationalizing Data Governance | 107",7541
44-Considerations for Governance Across a Data Life Cycle.pdf,44-Considerations for Governance Across a Data Life Cycle,"data policies and accurate compliance reporting. Map infrastructure, architec‐\nture, and tools. Y our data governance framework must be a sensible part of your\nenterprise architecture, the IT landscape, and the tools needed. We talked about\ntechnology in previous sections, so we won’t go into detail about it here. Finding\ntools and technology that work for you and satisfy the organizational objectives\nyou laid out is what’s important.\nEstablish education and training\nAs highlighted earlier, for data governance to work, it needs buy-in across the\norganization. Y ou need to ensure that your organization is keeping up and is still\nbuying into the project you presented. It’s therefore important to raise awareness\nof the value of data governance by developing educational materials highlighting\ndata governance practices, procedures, and the use of supporting technology.\nPlan for regular training sessions to reinforce good data governance practices.\nWherever possible, use business terms, and translate the academic parts of the\ndata governance discipline into meaningful content in the business context.\nConsiderations for Governance Across a Data Life Cycle\nData governance has been around since there was data to govern, but it was mostly\nviewed as an IT function. Implementing data governance across the data life cycle is\nno walk in the park. Here are some considerations you will need to think about as\nyou implement governance in your organization. These should not be surprising to\nyou, because you will quickly notice that they touch on a lot of aspects we introduced\nin Chapters 1 and 2, as well as in this chapter.\nDeployment time\nCrafting and setting up governance processes across the data life cycle takes a lot of\ntime, effort, and resources. In this chapter, we have introduced a lot of concepts,\nideas, and ways to think about operationalizing governance across the data life cycle,\nand you can see it gets overwhelming very quickly. There’s not a one-size-fits-all solu‐\ntion; you need to identify what is unique about your business and then forge a plan\nthat works for you. Automation can reduce the deployment time compared with\nhand-coded governance processes. In addition, artificial intelligence is seen as a way\nto get arms around data governance in the future, especially for things like autodis‐\ncovery of sensitive data and metadata management. That means that as you look for\nsolutions in the market, you will need to find out how much automation and integra‐\ntion is built into it, how well it works for your environment and situation, and\nwhether that is the most difficult part of the workflow that could use automation. In a\nhybrid or even a multi-cloud world, this becomes even more complex and further\nincreases the deployment time.\n108 | Chapter 4: Data Governance over a Data Life Cycle\nComplexity and cost\nComplexity comes in many forms. In Chapter 1 , we talked about how much the data\nlandscape is and about just how quickly data was being produced in the world.\nAnother complexity is a lack of defined industry standards for things like metadata.\nWe touched on this in Chapter 2 . In most cases, metadata does not obey the same\npolicies and controls as the underlying data itself, and a lack of standardized meta‐\ndata specifications means that different products and processes will have different\nways of presenting this information. Still another complexity is the sheer amount of\ntools, processes, and infrastructure needed to make governance a reality. In order to\ndeliver comprehensive governance, organizations must either integrate best-of-breed\nsolutions, which are often complex and very expensive (with high license and mainte‐\nnance costs), or buy turnkey, integrated solutions, which are expensive and fewer in\nthe market. With this in mind, cloud service providers (CSPs) are building data plat‐\nforms with all these governance capabilities built in, thus creating a one-stop shop\nand simplifying the process for customers. As an organization, research and compare\nthe different data platforms provided by CSPs and see which one works for you. Some\nbusinesses choose to leave some of their data on-premises; however, for the data that\ncan move to the cloud, these CSPs are now building robust tools and processes to\nhelp customers govern their data end-to-end on the platform. In addition, companies\nsuch as Informatica, Alation, and Collibra offer governance-specific platforms and\nproducts that can be implemented in your organization.\nChanging regulation environment\nIn previous chapters, we’ve clearly outlined the implications of a constantly changing\nregulatory environment with the introduction of GDPR and CCPA. We will not go\ninto the same detail here; however, regulations define a lot of what must be done and\nimplemented to ensure governance. They will outline how certain types of data need\nto be handled and which types of controls need to be in place, and they sometimes\nwill even go as far as outlining what the repercussions are when these things are not\ncomplied with. Complying with regulations is absolutely something your organiza‐\ntion needs to think about as you implement data governance over the data life cycle.\nOperationalizing Data Governance | 109\nIn our discussions with many different companies, we’ve heard of\ntwo very different philosophies when it comes to considering\nchanges to the regulatory environment. One strategy is to assume\nthat, in the future, the most restrictive regulations that are present\nnow will cascade and be required everywhere (like CCPA being\nrequired across the entire US and not just in California), and that\nensuring compliance now, even though not required, is a top prior‐\nity. Conversely, we’ve also heard the strategy of complying only\nwith what’s required right now and dealing with regulations only if\nthey become required. We strongly suggest you take the former\napproach, because a proper and well-thought-out governance pro‐\ngram not only ensures compliance with ever-changing regulations;\nit also enables many of the other benefits we’ve outlined thus far,\nsuch as better findability, better security, and more accurate analyt‐\nics from higher-quality data.\nLocation of data\nIn order to fully implement governance over a data life cycle, understanding which\ndata is on-premises versus in the cloud is very important. Furthermore, understand‐\ning how data will interact with other data along the life cycle does create complexity.\nIn the current paradigm, most organizational data lives both on-premises and in the\ncloud, and having systems and tools that allow for hybrid and even multicloud sce‐\nnarios is paramount. In Chapter 1 , we talked about why governance is easier in the\npublic cloud—it’sprimarily because the public cloud has several features that make\ndata governance easier to implement, monitor, and update. In many cases, these fea‐\ntures are unavailable or cost-prohibitive in on-premises systems. Data should be pro‐\ntected no matter where it is located, so a viable data life cycle management plan will\nincorporate governance for all data at all times.\nOrganizational culture\nAs you know, culture is one of those intangible things in an organization that plays an\nimportant role in how the organization functions. In Chapter 3 , we touched on how\nan organization can create a culture of privacy and security, which allows employees\nto understand how data should be managed and treated so that they are good stew‐\nards of proper data handling and usage. In this section, we’re referring to organiza‐\ntional culture, which often dictates what people do and how they behave. Y our\norganization might be free, allowing folks to easily raise questions and concerns, and\nin such an environment, when something goes wrong, people are more likely to speak\nup. In organizations in which people are reprimanded for every little thing, they will\nbe more afraid to speak up and report when things are not working or even when\nthings go wrong. In these environments, governance is a little difficult to implement,\nbecause without transparency and proper reporting, mistakes are usually not discov‐\nered until much later. In the NASA example we provided earlier in this chapter, there\n110 | Chapter 4: Data Governance over a Data Life Cycle",8384
45-Summary.pdf,45-Summary,"were a couple of people within the organization who noticed the discrepancy in the\ndata and even reported it. Their reports were ignored by management, and we all\nknow what happened. Things did not end well for NASA. Remember, instituting gov‐\nernance in an organization is often met with resistance, especially if the organization\nis accustomed to decentralized operations. Creating an environment in which func‐\ntions are centralized across the data life cycle simply means that these areas have to\nadhere to processes that they might not have been used to in the past but that are for\nthe larger good of the organization.\nSummary\nData life cycle management is paramount to implementing governance and ensures\nthat useful data is clean, accurate, and readily available to users. In addition, it\nensures that your organization remains compliant at all times.\nIn this chapter, we introduced you to data life cycle management, and how to apply\ngovernance over the data life cycle. We then looked into operationalizing governance\nand how the role of a data governance policy is to ensure that an organization’s data\nand information assets are managed consistently and used properly. Finally, we pro‐\nvided step-by-step guidance for implementing governance and finished with the con‐\nsiderations for governance across the data life cycle, including deployment time,\ncomplexity and cost, and organizational culture.\nSummary | 111",1444
46-Chapter 5. Improving Data Quality.pdf,46-Chapter 5. Improving Data Quality,,0
47-Why Is Data Quality Important.pdf,47-Why Is Data Quality Important,"CHAPTER 5\nImproving Data Quality\nWhen most people hear the words data quality , they think about data that is correct\nand factual. In data analytics and data governance, data quality has a more nuanced\nset of qualifiers. Being correct is not enough, if all of the details are not available (e.g.,\nfields in a transaction). Data quality is also measured in the context of a use case, as\nwe will explain. Let’s begin by exploring the characteristics of data quality.\nWhat Is Data Quality?\nPut simply, data quality is the ranking of certain data according to accuracy, com‐\npleteness (all columns have values), and timeliness. When you are working with large\namounts of data, the data is usually acquired and processed in an automated way.\nWhen thinking about data quality, it is good to discuss:\nAccuracy\nWhether the data captured was actually correct. For example, an error in data\nentry causing multiple zeros to be entered ahead of a decimal point, is an accu‐\nracy issue. Duplicate data is also an example of inaccurate data.\nCompleteness\nWhether all records captured were complete—i.e., there are no columns with\nmissing information. If you are managing customer records, for example, make\nsure you capture or otherwise reconcile a complete customer details record (e.g.,\nname/address/phone number). Missing fields will cause issues if you are looking\nfor customer records in a specific zip code, for example.\nTimeliness\nTransactional data is affected by timeliness. The order of events in buying and\nselling shares, for example, can have an impact on the buyer’s available credit.\nTimeliness also should take into account the fact that some data can get stale.\n113\nIn addition, the data quality can be affected by outlier values. If you are looking at\nretail transactions, for example, very large purchase sums are likely indications of\ndata-entry issues (e.g., forgetting a decimal point) and not indicators that revenues\nwent up by two orders of magnitude. This will be an accuracy issue.\nMake sure to take all possible values into account. In the above retail example, nega‐\ntive values are likely indications of returns and not “purchasing a product for nega‐\ntive $” and should be accounted for differently (e.g., a possible impact will be the\naverage transaction size—with the purchase and the return each accounting for a sin‐\ngle purchase).\nFinally, there is the trustworthiness of the source of the data. Not all data sources are\nequal—there is a difference, for example, between a series of temperature values col‐\nlected over time from a connected thermometer and a series of human reads of a\nmercury thermometer collected over time in handwriting. The machine will likely\ncontrol variables such as the time the sample was taken and will sync to a global\natomic clock. The human recording in a notebook will possibly add variance to the\ntime the sample was taken, may smudge the text, or may have hard-to-read handwrit‐\ning. It is dangerous to take data from both sources and treat them as the same.\nWhy Is Data Quality Important?\nFor many organizations, data directly leads to decision making: a credit score com‐\npiled from transaction data can lead to a decision by a banker to approve a mortgage.\nA company share price is instantly computed from the amount offered by multiple\nbuyers and sellers. These kinds of decisions are very often regulated—clear evidence\nmust be collected on credit-related decisions, for example. It is important to the cus‐\ntomer and the lender that mortgage decisions are made according to high-quality\ndata. Lack of data quality is the source of lack of trust and of biased, unethical auto‐\nmated decisions being made. A train schedule you cannot trust (based on erroneous\nor untimely station visits and past performance) can lead to you making decisions\nabout your commute that can potentially result in your always taking your own car,\nthus negating the very reason for the existence of the mass transit train.\nWhen collecting data from multiple sources or domains, data accuracy and context\nbecome a challenge: not only is it possible that the central repository does not have\nthe same understanding of the data as the data sources (e.g., how outliers are defined\nand how partial data is treated), but it is also possible that data sources do not agree\nwith each other on the meaning of certain values (e.g., how to handle negative values,\nor how to fill in for missing values). Reconciliation of data meaning can be done by\nmaking sure that when a new data source is added, the accuracy, completeness, and\ntimeliness of the data in the data source is examined—sometimes manually—and\neither described in a way that the data analysts using the resource can use or directly\nnormalized according to the rules in the central repository.\n114 | Chapter 5: Improving Data Quality\nWhen an error or unexpected data is introduced into the system, there are usually no\nhuman curators who will detect and react to it. In a data processing pipeline, each\nstep can introduce and amplify errors in the next steps, until presented to the busi‐\nness user:\n•In the data-gathering endpoints, gathering data from low-quality sources which\nare potentially irrelevant to the business task can, if not eliminated early, cause\nissues. Consider, for example, mobile ad impressions, or usage details, where\nsome of the data is gathered for an engineering lab source and does not represent\nreal users (and can potentially be very large in volume).\n•In the landing zone step, upon normalization/aggregation, the wrong data can be\naggregated into a summation and sway the result.\n•In the data analytics workload, joining tables of different qualities can introduce\nunexpected outcomes.\nAll of the above can be presented in a way such that the business user is kept unaware\nof decisions/operations happening earlier in the data acquisition chain (see\nFigure 5-1 ) and is presented with the wrong data.\nFigure 5-1. Simple data acquisition chain\nAny step in the (very simple) chain shown in Figure 5-1  can result in erroneous data,\nwhich will eventually drive the wrong business decisions. Let’s look at a couple of\nexamples.\nIn the early days of internet mapping, the mapping provider of the time, a company\ncalled MaxMind, was the sole provider of IP addresses to location services. This com‐\npany made an arguably reasonable decision to have a “default” location right at the\ncenter of the United States, in northern Kansas near the Nebraska border. From the\ntime of making this decision until very recently, whenever the service could not find a\nmap location for an IP address, it would provide this default location. The problem\nwith this decision became evident when systems and persons downstream did not\nrealize the meaning of this default, and when illegal activity was detected from\nWhy Is Data Quality Important? | 115",6943
48-Data Quality in AIML Models.pdf,48-Data Quality in AIML Models,"1Kashmir Hill, “How an Internet Mapping Glitch Turned a Random Kansas Farm into a Digital Hell” , Splinter ,\nApril 10, 2016.\n2Fiona Kelliher and Nico Savidge, “With Data Backlog Cleared, California Coronavirus Cases Officially\nDecreasing, Newsom Says” , Mercury News , August 14, 2020.\n3Thomas Redman, “Seizing Opportunity in Data Quality” , MIT Sloan Management Review , November 27,\n2017.\n4IBM, “The Four V’s of Big Data” , Big Data & Analytics Hub  (blog).“default” (aka unknown) IP addresses—law enforcement would show up at this loca‐\ntion in the central US and serve warrants to people who actually live there.1\nA more current data quality challenge example is within California’s collection of\nCOVID-19 cases. California moved from rapid per-county reports to state data\n(which caused an inconsistency). Later, California moved from “people tested” to\n“specimens tested” (some people are likely tested more than once, causing another\ndata quality issue), and then in August 2020 the data quality became an even more\nserious issue\nafter a series of errors—among them, a failed link to one of the country’s largest testing\nlabs—had led the state to underreport coronavirus cases starting in late July. Counties\nwere forced to comb spreadsheets in search of reliable data, while the state worked to\nunderstand the scope of the problem and fix it. A total of 296,000 records were\naffected.2\nIt took nearly a month to recover from this data-quality issue.\nA study performed by Experian and published by MIT Sloan researchers estimates\nthat the cost of bad data (and by that they mean bad, or unmanaged data quality) is\naround 15–20% of revenue for most companies.3 The study sampled one hundred\n“records” (or units of work) from enterprise divisions and then manually calculated\nthe error range of these records. The result was an astonishing error rate of 50%. The\nData Warehousing Institute (TDWI) estimated in 2002 that poor data quality costs\nbusinesses in the US over $700 billion annually, and since then this figure has grown\ndramatically. Currently, IBM estimates that the yearly cost of “bad data” is $3.1\ntrillion .4\nData Quality in Big Data Analytics\nData warehouses—databases that are used to performing data analytics in petabyte\nscale—are vulnerable to data quality issues. Typically, to get data in a big data ware‐\nhouse, you will extract, clean, transform, and integrate data from multiple operational\ndatabases to create a comprehensive database. A set of processes termed extract-\ntransform-load (ETL) is used to facilitate construction of data warehouses. The data\nin the warehouses, though rarely updated, is refreshed periodically and is intended\nfor a read-only mode of operation. The desired use of the data also has an impact on\n116 | Chapter 5: Improving Data Quality\nthe kinds of analysis and desired uses of the data. A data warehouse built to support\ndecision making in retail, where transactions are collected hourly and rounded to the\nnearest five-cent value, has different quality needs than a data warehouse built to sup‐\nport stock trades, where the transaction time needs to be accurate to the\nmicrosecond , and values range from microtransactions of below one cent to transac‐\ntions spanning millions of dollars.\nCompared to operational database environments, data warehouses pose additional\nchallenges to data quality. Since data warehouses integrate data from multiple sour‐\nces, quality issues related to data acquisition, cleaning, transformation, linking, and\nintegration become critical.\nWe touched on this earlier, but it’s important to keep in mind that\nproper data quality management not only assists in the ability to\nrun big data analytics but also helps to save on cost and prevent the\nloss of productivity. The harder it is for analysts to find and use\nhigh-quality data, coupled with extra engineering time spent hunt‐\ning down and solving data issues, the more cost you will incur, and\nyour analytics output will suffer. The downstream effects and cost\nimplications of poorly managed data quality should not be dis‐\ncounted.\nData Quality in AI/ML Models\nOf specific note is data quality within AI/ML models. To broadly generalize, machine\nlearning models work by extrapolating from existing data using a model to predict\nfuture data (e.g., transaction volume). If the input data has errors within it, the\nmachine learning model will likely amplify these errors. If the machine learning\nmodel is used to make predictions, and those predictions are then further input into\nthe model (once acted upon), the predictions become the reality, and the machine\nlearning model becomes compromised because it will generate, by force of a positive\nfeedback loop, more and more errors.\nThe data available for building machine learning models is usually divided into three\nnonoverlapping datasets: training , validation , and test. The machine learning model is\ndeveloped using the training dataset. Next, the validation dataset is used to adjust the\nmodel parameters so that overfitting is avoided. Last, the test dataset is used to evalu‐\nate the model performance. Errors in one or more datasets can lead to a badly trained\nmachine learning model, which will yield bad output. Note that there is a fine line\nbetween cleaning all the errors out manually (which can be cost prohibitive over a\nlarge amount of records) and allowing some level of error in a robust model.\nWhy Is Data Quality Important? | 117\nQuality Beats Quantity, Even in AI\nIt is a truth universally acknowledged that an AI product manager in possession of a\ngood idea must be in want of copious amounts of data. The resurgence of AI, starting\nin around 2014, has been driven by the ability to train ML models on ever-larger\ndatasets. The explosion of smartphones, the rise of ecommerce, and the prevalence of\nconnected devices are some of the trends that have fostered an explosion in the\namount of data that a business has available to it when designing new services or opti‐\nmizing existing ones. With larger datasets comes the ability to use larger and more\nsophisticated AI models. For example, a typical image classification model now has\nhundreds of layers, whereas an AI model from the 1990s had only one layer. Such\nmodels are practical because of the availability of custom ML hardware like GPUs and\nTPUs, and the ability to distribute the work across many machines in the public\ncloud. Thus, any AI product manager with a good idea will be on the hunt to use as\nmuch data as possible. The accuracy of the AI model and its ability to represent the\nreal world depends on it being trained with the widest, most representative data\npossible.\nIn our effort to gather more data, however, we should be careful to make sure that the\ncollected data is of good quality. A small amount of high-quality data yields much\nbetter results than does a large amount of low-quality or outright wrong data. A fasci‐\nnating example of this second truth (not as universally acknowledged) comes from an\neffort to reduce the overhead of testing water meters  in northern Italy.\nThe goal was to identify malfunctioning mechanical water meters based on their\nreadings alone. Going out to a site to test a water meter (see Figure 5-2 ) is pretty\nexpensive; if the team could use AI to identify potential malfunctions from the water\nreadings alone, it would be a huge cost savings. For example, if a water meter ran\nbackwards, or if the amount of water read as being consumed was wholly unreasona‐\nble (it was more, perhaps, than the amount of water supplied to the entire neighbor‐\nhood), we could be sure the meter was faulty. Of course, this also depended on the\nhistorical water usage at any meter—a water meter attached to a one-bath house with\na small garden would tend to consume a certain amount of water that varies season‐\nally. A large deviation from this amount would be suspicious.\n118 | Chapter 5: Improving Data Quality\n5Marco Roccetti et al., “Is Bigger Always Better? A Controversial Journey to the Center of Machine Learning\nDesign, with Uses and Misuses of Big Data for Predicting Water Meter Failures” , Journal of Big Data  6, no. 70\n(2019).\nFigure 5-2. A mechanical water meter of the sort used in Italy, meant to be read by a\nhuman looking at the dial. Photo courtesy of Andrevruas, Creative Commons License\n3.0\nThe team started out with 15 million readings from 1 million mechanical water\nmeters. This was enough data to train a recurrent neural network (RNN), the fanciest\ntime-series prediction method (with less data, they’ d have to settle for something like\nARIMA [an autoregressive integrated moving average model]), and so the team got to\nwork. The data was already numeric, so there was no need for any data preprocessing\n—the team could just feed it into the RNN and do lots of hyperparameter tuning. The\nidea was that any large differences between what the RNN predicted and what the\nmeter actually read would be attributable to faulty water meters.\nHow did that go? The team notes:\nOur initial attempts to train a recurrent neural network, without a specific attention\nto the quality, and to the limitations, of those data used for training, led to unexpec‐\nted and negative prediction outcomes.5\nWhy Is Data Quality Important? | 119\nThey went back to the drawing board and looked more closely at those 15 million\nreadings that they had. It turned out that there were two problems with the data: erro‐\nneous data and made-up data.\nFirst, some of the data was simply wrong. How could a water meter reading have been\nwrong? Wouldn’t a customer complain? It turns that the process of a water meter\nreading going to the customer consists of three steps:\n1.The mechanical water meter being read by the technician onsite\n2.The reading being entered from the technician’s logbook into the company ERP\nsystem\n3.The ERP system calculating the bill\nIn the case of errors in either data entry, the customer might call to complain, but\nonly the bill was corrected. The water meter readings might still remain wrong! Thus,\nthe AI model was getting trained on wrong data and being told the meters were not\nfaulty. Such wrong data turned out to be about 1% of all observations, which is about\nthe proportion of actually faulty water meters. So, if the meter did return such a bad\nvalue, the model was simply tossing a coin—when the historical data did have these\nwild swings, including negative values, half the time it was because the observation\nwas wrong, and the other half of the time it was because the meter was faulty. The\nRNN was being trained on noise.\nSecond, some of the data was simply made up. It costs so much to send a technician\nout to a site that sometimes past readings were simply extrapolated and a reasonable\ncharge was made to the customer. This was then made up the next time by a true\nreading. For example, an actual measurement might be made in January, skipped in\nMarch, and caught up in May. The value for March wasn’t real. Thus the AI model\nwas being trained on data that wasn’t real—31% of the “measurements” were actually\ninterpolated values. Further, the readjustments added a lot of noise to the dataset.\nAfter correcting all the billing-related errors and the interpolated values, faulty water\nmeters were detected by the RNN with an accuracy of 85% (a simpler linear regres‐\nsion model would have given them 79%). In order to get there, though, they had to\nthrow away a large percentage of the original data. Quality, in other words, trumped\nquantity.\nA good data governance regime would have been careful to propagate billing correc‐\ntions back to the original source data and to classify measurements and interpolated\nvalues differently. A good data governance regime would have enforced dataset qual‐\nity from the get-go.\n120 | Chapter 5: Improving Data Quality",11972
49-Why Is Data Quality a Part of a Data Governance Program.pdf,49-Why Is Data Quality a Part of a Data Governance Program,,0
50-Techniques for Data Quality.pdf,50-Techniques for Data Quality,"6Poppy Noor, “Overzealous Profanity Filter Bans Paleontologists from Talking About Bones” , Guardian , Octo‐\nber 16, 2020.Why Is Data Quality a Part of a Data Governance Program?\nTo summarize, data quality is absolutely essential when planning a data program.\nOrganizations very often overestimate the quality of the data they have and underes‐\ntimate the impact of bad data quality. The same program that governs data life cycle,\ncontrols, and usage should be leveraged to govern the quality of data (and plan for\nthe impact and response to bad data quality incidents).\nTechniques for Data Quality\nHaving discussed the importance of data quality, let’s review a few strategies for clean‐\ning up data, assessing quality, and improving data quality. As a general rule, the ear‐\nlier in the pipeline that data can be prepared, sanitized, disambiguated, and cleaned,\nthe better. It is important to note, in parallel, that data processing pipelines are not the\nsame for different business purposes/different teams; thus it may be hard to clean up\nthe data upstream, and that task may need to move downstream for the individual\nteams. There is a balance here: if further down the data analytics pipeline aggregates\nare performed, causing a coarsening of the data, the crucial meaning needed for\ncleanup may be lost as discrete values are aggregated. We will highlight three key\ntechniques for data quality: prioritization, annotation, and profiling.\nThe Importance of Matching Business Case to the Data Use\nParticipants in a virtual symposium for the Society of Vertebrate Paleontology in\nOctober 2020 were baffled by the fact that the transcription, captions, and chat mes‐\nsages in digital Q&A sessions were oddly garbled. After some investigation, a com‐\nmon denominator was determined: the words “bone, ” “knob, ” and “jerk”—all very\nrelevant and appropriate when discussing fossils—were banned and made invisible to\nthe participants.6\nThe underlying issue was that the online meeting platform used to power the conven‐\ntion was designed for education and not necessarily for science. While on the surface\nthis does not immediately pose an issue (when you consider teenagers), a built-in \n“naughty-word filter” seemed to be the culprit.\nThe filter automatically went over all text data presented in the platform and filtered\nout certain words considered “not appropriate for school. ” Words that are essential\nwhen discussing paleontology, such as, well, “bone, ” are inappropriate in other set‐\ntings. To top it off, even “dinosaur” was apparently inappropriate in the business con‐\ntext originally envisioned for the meeting platform.\nWhy Is Data Quality a Part of a Data Governance Program? | 121\nThis filtering caused a stir when one researcher found out that “Wang” was one of the\nbanned words, while “Johnson” was not. This issue introduced serious (unintended)\nbias as both are common surnames as well as slang terms.\nAt the end of the day, the issue was quickly fixed, and people accepted the error in\ngood humor, laughing at the unintended consequences and sharing them over social\nmedia. For the purpose of this book, however, this story presents an important lesson:\nany system that acts on data (e.g., the data modification system discussed in this side‐\nbar) must be developed with as full a business use case as possible in mind. Clearly,\nthe intended audience is part of the business context, and you obviously cannot treat\ninternational science symposium participants like you would treat schoolchildren.\nFigure 5-3  drives home the point that data quality must be tied to the specific busi‐\nness case, and it also conveys a more subtle point about data governance. Sharing\n“banned word lists” between use cases also informs the use case participants of the\nexistence of those other cases as well as of the relevant word lists—not always a\ndesired scenario!\nFigure 5-3. Architectures of a chat filter\n122 | Chapter 5: Improving Data Quality",4006
51-Scorecard.pdf,51-Scorecard,,0
52-Prioritization.pdf,52-Prioritization,,0
53-Profiling.pdf,53-Profiling,"Scorecard\nIn your organization, create a scorecard for the data sources. A useful scorecard\nincludes information about the origin of the data and its accuracy , completeness,  and\ntimeliness . This scorecard will be used by data pipeline builders to make decisions\nabout how and where to use the data and for what purpose. More mundane informa‐\ntion, such as who is the administrative owner of the data, who asked for it, and so on,\ncan also be useful in a scorecard.\nPrioritization\nFirst, prioritize— there are different sources of data and different uses for each source.\nA data source used for prioritizing healthcare actions is very different from a data\nsource used to drive graphics for a lobby “heat map” display. Prioritization should be\nperformed with the eventual business goal in mind. Lineage can help with backtrac‐\ning the data and repurposing the origin for a different business purpose. By monitor‐\ning the lineage of data (more on that in “Lineage Tracking” on page 46) for both its\nsources as well as its eventual uses—you can prioritize and expend resources on the\nmore critical data sources first.\nAnnotation\nSecond, annotate— make sure you have a standardized way to attach “quality infor‐\nmation” to data sources. Even if you cannot provide a detailed scorecard for each\nsource, being able to attest that “This data has been vetted” or (just as importantly)\n“This data is not vetted” is valuable, even if there are differences between the datasets\nand their meaning. As you evolve your data quality program, you can further attach\nmore detailed information to datasets. Begin small, however, and clearly annotate the\ninformation at hand so that it does not become “tribal knowledge” and get lost when\npeople in the know move on.\nA common technique for annotation can be to “crowdsource” the quality information\nby allowing people using the data to “vote” on or “star” data according to its quality as\nthey observe it through usage. This allows multiple human eyes on the data, and if\nyou start with a good default (data is normally untrusted) and assign a curator to\nreview data that is highly rated before issuing a quality signal to the rest of the organi‐\nzation, you can effectively practice a fair data quality program. (This is not the only\nrecommendation in this chapter, though!)\nTechniques for Data Quality | 123\nCascading Problems with Tribal Knowledge\nWhile you are certainly very familiar with the struggle of overcoming reliance on\ntribal knowledge, the following is an interesting use case that further illustrates just\nhow problematic disregarding annotation can be.\nWe’ve spoken many times with a healthcare company that is going through a myriad\nof issues related to lack of annotation and reliance on tribal knowledge. This company\nis quickly growing and has recently taken in several new acquisitions. Part of the ben‐\nefit of these acquisitions is the data that they have. This company has the vision of\nbeing able to leverage the data from these new acquisitions along with its own data to\nrun some extremely powerful analytics that are sure to have prolific business impact.\nThe company, however, encountered an enormous snag: the enterprise dictionaries,\nmetadata management, and data quality management were anemic or nonexistent at\nthe companies they acquired. The majority of these other companies relied on tribal\nknowledge, and, post-acquisition, many of the employees who had this knowledge are\nno longer at the company. This has resulted in most of the acquired data (since it’s not\nproperly managed, and no one can know what it is without spending a lot of time and\neffort to curate it) sitting in storage, just taking up space and not providing any busi‐\nness value whatsoever.\nThrough this issue the company has come to realize the importance of a centralized\ngovernance strategy that it can quickly scale—even for acquisitions—so that in the\nfuture this problem hopefully doesn’t occur again, or at least such problems can be\nsomewhat mitigated.\nProfiling\nData profiling  begins by generating a data profile: information about a range of data\nvalues (e.g., min, max, cardinality), highlighting values that are missing and values\nthat are out-of-bounds (versus the average distribution) data outliers. Reviewing the\ndata profile enables a determination of what are considered legal values (e.g., am I\nhappy with the data outliers, or should I exclude those records?) and of value mean‐\ning (e.g., is a negative value in the income column an appropriate value, or should I\nexclude it?).\nWe proceed to detail several data profiling and cleanup techniques.\nData deduplication\nIn a quantitative system, each record should have only one voice. However, there are\nmany cases in which the same record, or the same value, actually gets duplicated,\nresulting in data quality issues (and potentially in increased cost). Think about a\nredundant transaction system, where each transaction has an ID, and sometimes the\n124 | Chapter 5: Improving Data Quality\nsame transaction can appear twice (e.g., due to a transmission issue). Given the ID,\nyou can easily deduplicate the transactions and resolve. But think about a more chal‐\nlenging use case in which you have support cases (a list of customer issues in a ticket‐\ning system), each expressed by a user input title. When writing knowledge base\narticles to address these support cases, it is important to merge different user requests\nthat refer to the same source issue. Since a “user request” is expressed in natural lan‐\nguage and can be ambiguous, this is more challenging.\nDeduplicating Names and Places\nThink of an entity, and it probably needs resolving or disambiguation. Two of the\nmost common entities that you have in datasets are names and addresses. Both of\nthese need to be resolved if you want to deduplicate records. For example, the same\nperson could be addressed as “Dr. Jill Biden, ” “Jill Biden, ” or “Mrs. Biden. ” In all three\nsets of records, it may be necessary to replace the name by a consistent identifier.\nTake, for example, the impact of deduplicating author names in bibliographies, as\nshown in Figure 5-4 . Robert Spence is referenced as Bob Spence and as R. Spence.\nCombining all these records and replacing the different variants with the canonical\nversion of his name greatly simplifies the set of relationships and makes deriving\ninsights much easier.\nFigure 5-4. Deduplication can dramatically simplify the complexity of a dataset. For\nexample, Lisa Tweedie is also referenced as L. Tweedie. Figure adapted from a paper by\nBilgic et al., 2004 .\nFor deduplicating names, consider using a tool such as the Google Knowledge Graph\nSearch API , or building one from your set of stakeholders using an open source API\nsuch as Akutan .\nSimilar to names, the same place can be referred to as “the New Y ork Stock\nExchange, ” “11 Wall St, ” “Wall St. & Broad Street, ” “Wall and Broad, ” or any of a\nmyriad number of combinations. If packages are noted as having been delivered to\nmultiple versions of this location, you might want to consolidate them into a canoni‐\ncal representation of the location.\nTechniques for Data Quality | 125\nAddress resolution of this form is provided by the Google Places API  which returns a\nplace ID, a textual identifier that uniquely identifies a place. Place IDs are available for\nmost locations, including businesses, landmarks, parks, and intersections, and will\nchange over time as businesses close or relocate. It can be helpful, therefore, to com‐\nbine the Places API with the Google Maps Geocoding API  to yield the actual location\nin time. Thus, while “the New Y ork Stock Exchange” and “11 Wall Street” yield differ‐\nent place IDs, as shown in Figure 5-5  (after all, the NYSE could relocate!), geocoding\nthem will return the same location.\nFigure 5-5. The place IDs for “the New York Stock Exchange” and “11 Wall Street” are\ndifferent  from each other but geolocate to the same location\n126 | Chapter 5: Improving Data Quality\nData outliers\nAnother tactic is to identify outliers of the data early on and eliminate them. For\nexample, in a system that accepts only natural numbers (the range between 1 and\ninfinity, excluding fractions), such as a house number in a street address book, it\nwould be odd to find negative or fractional numbers, and thus it may make more\nsense to delete the entire record containing the outlier value rather than manually fix‐\ning it. Discretion is advised, as (for example) for 221b Baker Street, the fictional home\nof Sherlock Holmes. The UK’s Royal Mail has had to recognize “221b Baker Street,\nLondon” as a real postal address because of all the Sherlock fans expecting it to be\ngenuine! But the mail to 221b redirects to the Sherlock Holmes Museum at 239 Baker\nStreet.\nTo generalize and scale: when building a dataset, make sure you can determine, for as\nmany fields as possible, the minimum, maximum, and expected values (fractions,\nnegatives, strings, zero...), and include logic to clean up records with unexpected val‐\nues (or otherwise treat them). These actions are better done early in the processing\ntimeline rather than later, when values have been aggregated or used in machine\nlearning models. It is hard to backtrack/root out issues that are discovered after data\nhas been processed.\nExtreme values are not necessarily outliers, and care must be taken there. For exam‐\nple, a perfect SAT score is possible; however, in the US the SAT score range is 400–\n1,600, and values outside this range are suspect. Look at the distribution of the values\nand how the curve is shaped. The extreme edges of a “bell” curve should be treated\ndifferently than a distribution that has two peak clusters.\nConsider the example distribution in Figure 5-6  in light of the expected use case of\ndata. In our example, an expected bell curve with a sudden peak at the edge should\nprompt a more manual investigation and understanding to rule out potential data\nquality issues. Be wary of automatically dismissing such values without investigating\nthem, as sometimes these outliers are the result not of low-quality data, but of phe‐\nnomena that should be accounted for in the use case.\nTechniques for Data Quality | 127\nFigure 5-6. Example distributions of data\nLineage tracking\nAs mentioned earlier, lineage tracking  for data is a force multiplier. If you can identify\nsource datasets of high quality, you can follow the use of those high-quality sources\nand express an opinion of the result derivatives. In addition, if you can identify low-\nquality datasets, you can assume any computed derivative that is a product of one (or\nmore) low-quality source is also a low-quality result—a useful conclusion that should\nguide the use of the data.\nFurthermore, with lineage tracking you can backtrack from high-criticality use cases\nand results (e.g., dashboards) and learn what data sources are feeding these. At a min‐\nimum, you should prioritize the quality assessment of all sources of critical decision-\nsupporting outputs.\nThis process of monitoring quality by source should not be a one-time thing but\nshould be used every time a new dashboard/end product is set up. And it should be\non periodic review, because the benefit of early detection can be significant if man‐\naged correctly. The impact of bad data, which can often go unnoticed, has been dis‐\ncussed above.\n128 | Chapter 5: Improving Data Quality\nData completeness\nIn some cases, there are data records with missing information, such as a customer\nrecord without an address or a transaction without a tracking number. Special con‐\nsideration should be given to such cases, and an informed decision must be made\nwhether or not to eliminate the records that are incomplete (resulting in “less data”\nbut more “complete” data). Alternatively, if accepting incomplete records, make sure\nto include an annotation on the dataset indicating that it contains such records and\nnoting which fields are “OK” to be missing and the default values (if any) used as\ninput for those fields that are missing. This is especially important in the case of\nmerging datasets .\nMerging datasets\nDuring ETL processes, and in general, you should be aware of special values used in\nthe source datasets and make room for them during transformation/aggregation. If\none dataset uses “null” to indicate no data and another dataset uses “zero, ” make sure\nthis information is available to future users. Ensure that the joining process equalizes\nthese two values into one consistent value (either null or zero; they have the same\nmeaning in our example). And of course, record this special new value.\nDataset source quality ranking for conflict  resolution\nWhen merging multiple datasets from different vendors, another topic comes to\nmind: how to react if multiple-source datasets contain the same fields but with cer‐\ntain fields having different values in them. This is a common issue in financial sys‐\ntems, for example, where transactions are collected from multiple sources but\nsometimes differ in the meaning of special values such as zero, negative 1. One way to\nresolve that is to attach a ranking to each data source, and, in case of conflict, record\nthe data from the highest-ranked source.\nUnexpected Sources for Data and Data Quality\nMany a McDonald’s fan is disappointed when the ice cream maker in their local\nbranch is broken. In fact, this issue troubled Rashiq, a young McDonald’s fan in Ger‐\nmany, so much that he reverse-engineered the McDonald’s app, found its API, and\ndiscovered a way to figure out whether or not the ice cream maker in a particular\nbranch was operational.\nRashiq tested out the code and later built a site, mcbroken.com , that reports on the\nstatus of the ice cream makers in every branch on a global scale ( Figure 5-7 ).\nTechniques for Data Quality | 129\nFigure 5-7. Rashiq’s Twitter feed\nHowever, there was a downside to Rashiq’s code. In order to identify whether or not\nthe ice cream maker was operational, Rashiq had a bot create an ice cream order and\nadd an ice cream to the shopping cart. If that operation was successful, Rashiq would\nmark the ice cream maker in that branch as “operational. ” This resulted in thousands\nof half-completed orders of ice cream at McDonald’s locations, globally. Apparently,\nhowever, the McDonald’s data team was able to control for Rashiq’s orders, as evi‐\ndenced by the reply from McDonald’s’ director of analytics: “I’m Lovin’ It. ” McDo‐\nnald’s head of communications tweeted a response ( Figure 5-8 ).\n130 | Chapter 5: Improving Data Quality",14747
54-Summary.pdf,54-Summary,"Figure 5-8. McDonald’s responds to Rashiq’s work\nSummary\nData quality is making sure that the data’s accuracy, completeness, and timeliness are\nrelevant to the business use case in mind. Different types of business use necessitate\ndifferent levels of the above, and you should strive to keep a scorecard of your data\nsources when creating an analytics workload composed of descendants of these data\nsources.\nWe have reviewed the importance and real-life impact of data quality, through exam‐\nples of bad data. We have discussed several techniques to improve data quality. If\nthere is one key piece of advice that should be taken into account from this chapter, it\nis this: for data quality, handle it early, as close to the data source as possible, and\nmonitor resultant products of your data. When repurposing data for a different ana‐\nlytics workload, revisit the sources and see if they are up to the new business task.\nSummary | 131",949
55-Chapter 6. Governance of Data in Flight.pdf,55-Chapter 6. Governance of Data in Flight,,0
56-Why Lineage Is Useful.pdf,56-Why Lineage Is Useful,"CHAPTER 6\nGovernance of Data in Flight\nData, especially data used for insights via data analytics, is a “living” medium. As data\ngets collected from multiple sources, it is reshaped, transformed, and molded into\nvarious patterns for different use cases: from a standardized “transactions table” to\nallow for forecasting the next season’s business demand, to a dashboard presenting\nthe past yield of a new crop, and more.\nData governance should be consistent across these transformations and allow more\nefficiency and frictionless security. Data governance should not introduce labor by\nforcing users to register and annotate new data containers as they work to reshape\nand collect data for their needs.\nThis chapter will discuss possible techniques and tools to enable seamless data gover‐\nnance through analysis of data “in flight. ”\nData Transformations\nThere are different ways to transform data, all of which impact governance, and we\nshould be aware of these before we dig in deeper. It is common to refer to these pro‐\ncesses as extract-transform-load (ETL). This is a generic phrase used to indicate the\nvarious stages of moving data between systems.\nExtracting  data means retrieving it from the source system in which it is stored, e.g., a\nlegacy DB, a file, or the results a web crawler operation. Data extraction is a separate\nstep, as the act of extracting data is a time-consuming retrieval process. It is advanta‐\ngeous to consider the extraction phase as the first step in a pipeline, allowing subse‐\nquent steps to operate in batches in parallel to the continued extraction. As data is\nextracted from the sources, it’s useful to perform data validation , making sure the val‐\nues retrieved are “as expected” (that the completeness of records and their accuracy\nmatch the expected values; see Chapter 5 ). If you perform data validation while still\n133\noperating within the context of the source system, you will be unencumbered by the\ndifferent computed results performed in later stages, which may be unknown (at this\nstage), and as you progress, you may lose the context of the source data. In an earlier\nchapter, we discussed data preparation, which is an example of a data validation pro‐\ncess. The data being extracted and validated often lands in a staging area  not normally\naccessible to the business users, which is where the data owners and data stewards\nperform the aforementioned validation checks.\nTransforming data usually involves normalization of the data: eliminating outliers,\njoining from multiple sources into a single record (row), aggregating where relevant,\nor even splitting a single compound column into multiple columns. Be wary of the\nfact that any normalization done early, as well as any kind of general purpose clean‐\ning, is also removing information—information whose value may not have been\nanticipated for a case unknown at the cleaning level. The implications are that you\nshould have the business context in mind when extracting data and that you may\nneed to revisit the source data in case the extraction process removed information\nneeded for a new, unforeseen use case. At this stage, if you are extracting data from a\nnew source, it may be worthwhile to create a scorecard for the source, describing\nsome of the information contexts and those that are potentially not carried over dur‐\ning transformation. See “Scorecard” on page 123  for more about scorecards.\nFinally, the load process situates the data into its final destination, usually a data-\nanalytics-capable warehouse such as Google’s BigQuery, Snowflake, or Amazon’s\nRedshift.\nIt is important to note that as data warehousing solutions grew to be more powerful,\nthe transformation process sometimes moved into the data warehousing solution,\nrenaming ETL as ELT.\nAs data undergoes transformations, it is crucial not only to keep context as expressed\nby the origin but also to maintain consistency and completeness. Keeping origin con‐\ntext, consistency, and completeness will allow a measure of trustworthiness in the\ndata, which is critical for data governance.\nLineage\nFollowing the discussion of data transformations, it is important to note the role\nplayed by data lineage. Lineage, or provenance , is the recording of the “path” that data\ntakes as it travels through extract-transform-load, and other movement of data, as\nnew datasets and tables are created, discarded, restored, and generally used through‐\nout the data life cycle. Lineage can be a visual representation of the data origins (cre‐\nation, transformation, import) and should help answer the questions “Why does this\ndataset exist?” and “Where did the data come from?”\n134 | Chapter 6: Governance of Data in Flight",4763
57-Types of Lineage.pdf,57-Types of Lineage,"Why Lineage Is Useful\nAs data moves around your data lake, it intermingles and interacts with other data\nfrom other sources in order to produce insight . However, metadata—the information\nabout the data sources and their classification—is at risk of getting lost as data travels.\nFor example, you can potentially ask, for a given data source, “What is the quality of\nthe data coming from that source?” It could be a highly reliable automatic process, or\nit could be a human-curated/validated dataset. There are other sources that you\npotentially trust less. As data from different sources intermingle, this information can\nsometimes be lost. Sometimes it is even desirable to forgo mixing certain data sources\nin order to preserve authenticity.\nIn addition to data quality, another common signal potentially available for source\ndata is sensitivity . Census information and a recently acquired client phone list are of\na certain level of sensitivity, while data scraped off a publicly available web page may\nbe of a different sensitivity.\nThus, data sensitivity and quality, and other information potentially available on the\norigin, should filter down to the final data products.\nThe metadata information of the data sources (e.g., sensitivity, quality, whether or not\nthe data contains PII, etc.) can support decisions about whether or not to allow cer‐\ntain data products to mix, whether or not to allow access to that data and to whom,\nand so on. When mixing data products, you will need to keep track of where the data\ncame from. The business purpose for the data is of particular importance, as the cre‐\nated data products should be useful in achieving that purpose. If the business purpose\nrequires, for example, a certain level of accuracy for time units, make sure the lineage\nof your data does not coarsen time values as the data is processed. Lineage is there‐\nfore crucial for an effective data governance strategy.\nHow to Collect Lineage\nIdeally, your data warehouse or data catalog will have a facility that starts from the\ndata origin and ends with whatever data products, dashboards, or models are used,\nand that can collect lineage for every transaction along the way. This is far from com‐\nmon, and there will likely be blindspots. Y ou will have to either infer the information\nthat you need on by-products or otherwise manually curate information to close the\ngap. Once you have this information, depending on how trustworthy it is, you can use\nit for governance purposes.\nAs your data grows (and it is common for successful businesses to accumulate data at\nan exponential rate) it is important to allow more and more automation into the pro‐\ncess of lineage collection and to rely less and less on human curation. Automation is\nimportant because, as we will see in this chapter, lineage can make a huge difference\nin data governance, and placing human bottlenecks in the path to governance blocks\nLineage | 135\nan organization’s ability to make data accessible. Also, a broken chain of lineage (e.g.,\ndue to human error) can have a larger impact on derivative data products, as trust\nbecomes harder to come by.\nAnother way to collect/create lineage information is to connect to the API log for\nyour data warehouse. The API log is expected to contain all SQL jobs and also all pro‐\ngrammatic pipelines (R, Python, etc.). If there is a good job audit log, you can use it to\ncreate a lineage graph. This allows backtracking table creation statements to the\ntable’s predecessors, for example. This is not as effective as just-in-time lineage\nrecording, as it requires backtracking the logs and batch processing. But assuming\n(not always true!) that you are focusing on lineage within the data warehouse, this\nmethod can be extremely useful.\nTypes of Lineage\nThe level of granularity of the lineage is important when discussing possible applica‐\ntions for that lineage. Normally, you would want lineage at least in the table/file\nlevel—i.e., this table is a product of this process and this other table.\nIn Figure 6-1 , we see a very simple lineage graph of two tables joined by a SQL state‐\nment to create a third table.\nFigure 6-1. Table-level lineage\nA column/field-level granularity is more useful: “This table consists of the following\ncolumns from this other table and another column from that table. ” When talking\nabout column-level granularity, you can start to talk about specific types of data being\ntracked. In structured (tabular) data, a column is normally only a single data type. An\nexample business case would be tracking PII: if you mark at the sources which col‐\numns are PII, you can continue tracking this PII as data is moved and new tables are\ncreated, and you can confidently answer which tables have PII. In Figure 6-2 , two col‐\numns from Table A are joined with two columns from Table B to create Table C. If\n136 | Chapter 6: Governance of Data in Flight\nthose source columns were “known PII” (as an example), you can use lineage to\ndetermine that Table C now contains PII as well.\nFigure 6-2. Column-level lineage\nRow-level lineage allows the expression of information about transactions. Dataset-\nlevel lineage allows the expression of coarse information about data sources.\nMore Granular Access Controls\nOne of the most common use cases we have heard during our research and interviews\nis that, while project/file-level access controls work, being able to more granularly\ncontrol access is highly desired. For example, column-level access control (“Provide\naccess to columns 1–3 and 5–8 of this table but not column 4”) allows you to lock\ndown, if you will, a particular column of data and still allow access to the rest of the\ntable.\nThis method works especially well for tables that contain much usable, relevant ana‐\nlytics data but may also contain some sensitive data. A prime example of this would\nbe a telecommunications company and its retail store transactions. Each retail store’s\ntransaction logs not only would contain information about each item purchased,\nalong with date, price, etc., but also would likely contain the purchaser’s name (in the\nevent the purchaser is a customer of the telecom company and put the item onto their\naccount). An example system leveraging labels-based (or attribute-based) granular\naccess controls is depicted in Figure 6-3 .\nLineage | 137",6415
58-How to Govern Data in Flight.pdf,58-How to Govern Data in Flight,"Figure 6-3. Label-based security  in Oracle\nAn analyst certainly wouldn’t need the customer’s name or account information, but\nthey would need the other relevant information of item purchased, location, time,\nprice, and so on. Instead of having to either grant full access to this table or rewrite\nthe table with the sensitive information removed, the sensitive column itself can have\nan access control so that only certain individuals are allowed access, and all others\nsimply see the table with that column removed or redacted/hashed/encrypted in some\nway.\nAs we talked about in Chapter 3 , most companies do not have enough headcount to\nsupport the constant monitoring, tagging, and rewriting of tables to remove or\nrestrict sensitive information. Granular access controls can enable you to get the same\nresult (sensitive information is guarded) while also allowing greater data\ndemocratization.\nThe Fourth Dimension\nAs data gets created, coarsened, and discarded, it is important to realize that lineage is\na temporal state—while a certain table is currently a product of other tables, it is pos‐\nsible that in a previous iteration the very same table was generated out of other data.\nLooking at the “state of now” is useful for certain applications, which we will discuss\nshortly, but it is important to remember that the past versions of certain data objects\nare relevant to gaining a true understanding of how data is being used and accessed\n138 | Chapter 6: Governance of Data in Flight\nthroughout an enterprise. This requires versioning information that needs to be\naccessible on the object.\nHow to Govern Data in Flight\nWorking off the assumption that lineage information is preserved and is reliable to a\ncertain extent, there are key governance applications that rely on lineage.\nA common need that gets answered by lineage is debugging or understanding sudden\nchanges in data. Why has a certain dashboard stopped displaying correctly? What\ndata has caused a shift in the accuracy of a certain machine learning algorithm? And\nso on. Finding from an end product which information feeds into that end product\nand looking for changes (e.g., a sudden drop in quality, missing fields, unavailability\nof certain data) could significantly speed up tasks related to end products. Under‐\nstanding the path and transformations of the data can also help troubleshoot data\ntransformation errors.\nAnother need that commonly requires at least field-level lineage (and said lineage\nneeds to be reliable) is the ability to infer data-class-level policies. For a certain col‐\numn that contains PII, I want to mark all future descendants of this column as PII\nand to effect the same access/retention/masking policies of the source column to the\nderivatives. Y ou will likely need some algorithmic intelligence that allows you to effect\nan identical policy if the column is copied precisely and to effect a different (or no)\npolicy if the column contents are nullified as a result of the creation of a derivative.\nIn Figure 6-4 , we see a slightly more involved lineage graph. While the operations\nperformed to create derivatives are marked as “SQL, ” note that this is not always the\ncase; sometimes there are other ways to transform the data (for example, different\nscripting languages). As data moves from the external data source into Table D, and\nas Table D is merged with Table C (the by-product of Tables A and B) and finally pre‐\nsented in a dashboard, you can see how keeping lineage, especially column-level line‐\nage, is important.\nFor example, a question asked by the business user would be “Who can I show this\ndashboard to?” Let’s say that just one of the sources (Table A, Table B, the external\ndata source) contains PII in one of the columns, and that the desired policy on PII is\n“available only to full-time employees”; if PII made it into the dashboard, that should\neffect an access policy on the dashboard itself.\nLineage | 139\nFigure 6-4. Lineage workflow—if  Table B contains sensitive data, that data can poten‐\ntially be found in the dashboard as well\nThese use cases are rather specific, but broader use cases have been expressed by\nCIOs and CISOs for a long time:\n“Show me all the sensitive data within our data warehouse, and what systems contain\nsensitive data”\nThis is a simple-to-express ask, and with the combination of lineage and the abil‐\nity to classify data sources, it is much simpler to answer than understanding the\nmultitude of data objects and systems that may be part of an organization’s data\nlake.\n“I want to identify certain systems as trusted, and make sure data does not exist in other,\nless trusted systems without manual oversight”\nThis need can be addressed by enforcing egress controls on the trusted systems, a\ntask which is potentially simpler with a good lineage solution. In this way, you\ncan, for example, eliminate data being ingested from unapproved systems,\nbecause those sources will show up in the lineage information.\n“I need to report and audit all systems that process PII”\nThis is a common need in a post-GDPR world, and if you mark all sources of PII,\nyou can leverage the lineage graph to identify where PII gets processed, allowing\na new level of control.\n140 | Chapter 6: Governance of Data in Flight",5336
59-Policy Management Simulation Monitoring Change Management.pdf,59-Policy Management Simulation Monitoring Change Management,,0
60-Chapter 7. Data Protection.pdf,60-Chapter 7. Data Protection,"Policy Management, Simulation, Monitoring, Change\nManagement\nWe already provided one example for policy management: inheritance . In essence,\ndata governance policies should be derived from the meaning of the data. If in a cer‐\ntain organization we want to govern PII, and PII is defined as all of personal phone\nnumbers, email addresses, and street addresses, these individual infotypes can be\nautomatically detected and associated with the data class. However, scanning a table\nand determining which columns contain these infotypes is expensive computation‐\nally. To correctly identify infotypes, the system will need to sample the relevant col‐\numns (without foreknowledge about which columns are sensitive, the entire table will\nneed to be sampled), process those through a pattern matching and/or a machine\nlearning model that will provide a level of confidence as to the underlying infotype,\nand tag the columns appropriately.\nHowever, this process is made much more efficient if you can just identify the event\nof a column creation without changing in values from an already-tagged column.\nThis is where lineage comes in.\nAnother use for lineage information is when data change management is considered.\nLet’s imagine that you want to delete a certain table; or alternatively, you want to\nchange the access policy on a data class; or maybe you want to set up a data-retention\ncontrol that will coarsen the data over a period of time (for example, change the data\nfrom “GPS Coordinates” to city/state after 30 days). With lineage, you can follow the\naffected data to its end products and analyze the impact of this change. Let’s say you\nlimit access to a certain number of fields in a table. Y ou can now see which dash‐\nboards or other systems use the data and assess the impact. A desirable result will be\nhighlighting the change in access to end users accessing end products, so a warning\ncould be issued at the time of changing the policy, alerting the administrator to the\nfact certain users will lose access and potentially even allowing a level of drill down so\nthose users can be inspected and a more informed decision can be made.\nAudit, Compliance\nWe often need to point at lineage information to be able to prove to an auditor or a\nregulator that a certain end product (machine learning model, dashboard, etc.) was\nfed into by specific and preapproved transactional information. This is necessary\nbecause regulators will want to be able to explain the reasoning behind certain\ndecision-making algorithms and make sure these rely on data that was captured\naccording to the enterprise’s charter—for example, making sure loan approvals rely\nonly on specific credit information that was collected according to regulations.\nPolicy Management, Simulation, Monitoring, Change Management | 141\nIt is increasingly common for regulated organizations to be able to prove that even\nmachine learning models are not biased, that decisions that are derived from data are\ndone so without manipulation, and that there is an unbroken “chain of trust” between\nproperly acquired data sources and the end-user tools (e.g., dashboards, expert sys‐\ntems) that are guiding those decisions. For example, in the credit scenario just\ndescribed, a decision on a new line of credit must be traced exclusively to a list of\ntransactions from a trusted source, without other influences guiding the decision.\nWhile it’s clearly incredibly important to be able to “prove” compli‐\nance in the event of an external audit, these same tools can be used\nfor internal auditing purposes as well. In fact, it’s best practice (as\nwe’ve mentioned several times in earlier chapters) to continually\nassess and reassess your governance program—how it’s going,\nwhere it may need to be modified, and how it may or may not need\nto be updated as regulations and/or business needs change.\nSummary\nWe have seen how collecting data lineage can enable data governance policies, infer‐\nence, and automation. With a lineage graph, organizations can trace data variations\nand life cycle, promoting control and allowing a complete picture of the various sys‐\ntems involved in data collection and manipulation. For the business user, governing\ndata while it is “in flight” through lineage allows a measure of trustworthiness to be\ninherited from trusted sources or trusted processors in the data path. This enriched\ncontext of lineage allows matching the right datasets (by sensitivity) to the right peo‐\nple or processes.\nWhile lineage is essentially a technical construct, we should always keep the end busi‐\nness goal in mind. This could be “ensuring decisions are made with high-quality\ndata, ” in which case we need to show lineage to the result from high-quality sources\n(and be able to discuss the transformations along the way), or a specific business case\nsuch as “tracing sensitive data, ” as we’ve already discussed.\nThe technical lineage and the business lineage use cases discussed here are both\nimportant, and we should strive to present the many technical details (e.g., intermedi‐\nate processing tables) to analysts while at the same time serving a simplified “business\nview” to the business users.\n142 | Chapter 6: Governance of Data in Flight",5286
61-Classification.pdf,61-Classification,"CHAPTER 7\nData Protection\nOne of the key concerns of data governance is that of protecting data. Owners of data\nmight be concerned about the potential exposure of sensitive information to individ‐\nuals or applications without authorization. Leadership might be wary of security\nbreaches or even of known personnel accessing data for the wrong reasons (e.g., to\ncheck on the purchase history of a celebrity). Users of the data might be concerned\nabout how the data they rely on has been processed, or whether it has been tampered\nwith.\nData protection has to be carried out at multiple levels to provide defense in depth. It\nis necessary to protect the physical data centers where the data is stored and the net‐\nwork infrastructure over which that traffic is carried. To do this, it is necessary to\nplan how authorized personnel and applications will be authenticated and how that\nauthorization will be provisioned. However, it is not enough to simply secure access\nto the premises and the network—there is risk involved with known personnel\naccessing data that they are not supposed to be accessing, and these personnel are\ninside the network. Additional forms of protection such as encryption are required so\nthat even if a security breach happens, the data is obfuscated.\nData protection needs to be agile because new threats and attack vectors continue to\nmaterialize.\nPlanning Protection\nA key aspect of data governance is determining the level of protection that needs to\nbe afforded to different types of data assets. Then all of the organization’s data assets\nmust be classified into these levels.\n143\nFor example, at the planning level , it might be mandated that data created by the\npayment-processing system needs to be secured because a malicious actor with access\nto individual transaction data might be able to make spurious orders and have them\nbilled to the original customer. This will then have to be implemented at the authenti‐\ncation level  by ensuring that access to any aspect of the payments data is available only\nto personnel and applications that are recognized as having a role of employee. How‐\never, not all data is made available to all employees. Instead, the payments data might\nbe classified at different levels. Only aggregate data by store location, date, inventory,\nand payment type might be made available to business planners. Access to individual\ntransactions would be authorized only to the payments support team, and even then\nis granted only for specific transactions on a timed basis, when authorization is pro‐\nvided in the form of a support ticket carrying the customer’s approval. Given this, it is\nnecessary to ensure that exfiltration of payments data is not possible. The data gover‐\nnance tools and systems must support these requirements, and violations and\nbreaches must be captured, and alerts carried out.\nIdeally, a catalog of all the data assets is created, although quite often planning, classi‐\nfication, and implementation can happen at higher levels of abstraction. To carry out\nthe above governance policy, for example, it is not necessary that you have an explicit\ncatalog of all the possible aggregates of payment data—only that any aggregates that\nare created are placed in a governance environment with strict boundaries.\nLineage and Quality\nAs discussed in Chapter 5 , data lineage and quality considerations are critical aspects\nof data governance. Therefore, they need to be part of the protection planning pro‐\ncess. It is not enough to consider data protection of only raw data; instead, protection\nof the data at each stage of its transformation needs to be considered. When aggre‐\ngates are computed from protected data, those aggregates need some level of protec‐\ntion that is typically equal to or less than the aggregate. When two datasets are joined,\nthe level of protection afforded to the joined data is often the intersection of authenti‐\ncation and authorization permissions. Aggregates and joins affect the data quality,\nand so governance needs to take this into account as well. If the data protection on\nthe raw data has restrictions on the volume of data that any person or partner is able\nto access, it might be necessary to revisit what the restriction on the aggregates and\njoins needs to be.\nLineage and quality checks also offer the capability to catch erroneous or malicious\nuse of data. Corrupt or fraudulent data may be clearer when aggregated. For example, \nBenford’s Law predicts the relative incidence of leading digits on numeric values that\nspan multiple orders of magnitude. The last digits of numeric values are expected to\nhave a normal distribution. Carrying out such statistical checks is easier on aggrega‐\nted, transformed data. Once such fraud is observed, it is necessary to have the ability\n144 | Chapter 7: Data Protection\nto trace back (through data lineage) where the change occurred and whether it was\nthe result of a data protection breach.\nIt is part of the data culture  of an organization that data published by the organization\nand used within the organization is trustworthy. Data quality, as discussed in Chap‐\nter 5 , remains a fundamental goal of data protection. For this purpose, it is important\nto have trust-but-verify safeguards built into data pipelines to catch quality errors\nwhen they arise.\nLevel of Protection\nThe level of protection to be afforded to an asset should reflect the cost and likeli‐\nhood of a security breach associated with that asset. This requires cataloging the types\nof security breaches and the costs associated with each breach. Different levels of pro‐\ntection also carry costs, and so it is necessary to identify the likelihood of a breach\noccurring with the given level of protection. Then a cost analysis that balances risk\nbetween different protection levels needs to be carried out, and a protection level\nchosen.\nFor example, consider the raw payments-processing information stored in a data lake.\nPotential security breaches might include individual transaction data being read by a\nmalicious actor, all the transactions within a certain time period being read, all the\ntransactions from a certain store being read, and so on. Similarly, there is a risk of\ndata being modified, corrupted, or deleted.\nWhen considering the risk of data being modified, corrupted, or\ndeleted, note that this may happen on purpose due to a malicious\nactor as we’ve described above, or it may happen unintentionally\nbecause of a mistake made by an internal employee. In many of our\ninterviews with companies we have found that both scenarios can\nand have resulted in unfavorable and at times disastrous outcomes,\nand thus both warrant your attention and consideration.\nThe cost of a security breach at a given level of protection needs to include the cost of\nthat data not being available, whether because of data protection or because of a secu‐\nrity breach. It is also important to realize that loss of data continuity itself can carry\ncosts, and so the cost of losing, say, an hour of data may affect the company’s ability to\nprovide accurate annual reports. The costs can also be quite indirect, such as down‐\ntime, legal exposure, loss of goodwill, or poor public relations. Because of all of these\nfactors, the costs will typically be different for different stakeholders, whether they are\nend users, line-of-business decision makers, or company executives. The costs might\nalso accrue outside the company to customers, suppliers, and shareholders. In cases\nwhere a granular-level cost estimate is impractical, a cost level of high/medium/low\ncan be assigned to guide the level of protection that is needed.\nPlanning Protection | 145",7782
62-Multi-Tenancy.pdf,62-Multi-Tenancy,"There are usually a variety of choices in terms of the data protection that can be\napplied. At one extreme, we might choose to not store the data at all. At the other\nextreme, we might choose to make the dataset publicly available. In between are\nchoices such as storing only aggregated data, storing only a subset of the data, or\ntokenizing certain fields. We can also choose where to store the data, perhaps driven\nby regulations around geographic locations and by which roles need to be provided\naccess to the data. Risk levels vary between these choices, because the likelihood of a\nbreach, data loss, or corruption is different with each.\nClassification\nAs covered in detail in Chapter 4 , implementing data governance requires being able\nto profile and classify sensitive data. This profile of the data is required to identify the\npotential security breaches, their cost, and their likelihood. This in turn will allow the\ndata governance practitioner to select the appropriate governance policies and proce‐\ndures that need to apply to the data.\nThere may be a number of governance directives associated with data classification,\norganization, and data protection. To enable data consumers to comply with those\ndirectives, there must be a clear definition of the categories (for organizational struc‐\nture) and classifications (for assessing data sensitivity).\nClassification requires properly evaluating a data asset, including the content of its\ndifferent attributes (e.g., does a free-form text field contain phone numbers?). This\nprocess has to take into account both the business use of the data (which parts of the\norganization need to be able to access the data) and the privacy and sensitivity impli‐\ncations. Each data asset can then be categorized in terms of business roles and in\nterms of the different levels of data sensitivity, such as personal and private data, con‐\nfidential data, and intellectual property.\nOnce the classification is determined and the protection level chosen by cost analysis,\nthe protection level is implemented through two aspects. The first aspect is the provi‐\nsioning of access to available assets. This can include determining the data services\nthat will allow data consumers to access the data. The second aspect is prevention of\nunauthorized access. This is done by defining identities, groups, and roles and assign‐\ning access rights to each.\nData Protection in the Cloud\nOrganizations have to rethink data protection when they move data from on-\npremises to the cloud or when they burst data from on-premises to the cloud for\nephemeral hybrid workloads.\n146 | Chapter 7: Data Protection",2674
63-Physical Security.pdf,63-Physical Security,"Multi-Tenancy\nWhen large enterprises that typically deploy their systems on-premises move to the\ncloud, one of the biggest changes they must come to terms with is being one of many\norganizations simultaneously using multi-tenant cloud architecture. This means that\nit is particularly important not to leak data by leaving it in unsecured locations due to\nunfounded confidence that malicious actors will not be able to authenticate into the\nphysical infrastructure. Whereas on-premises organizations have physical and net‐\nwork perimeter control, that control may be lost when going to the cloud.\nSome cloud providers provide “bare metal” infrastructure or “government cloud, ”\nessentially providing data center management to address this change. However, rely‐\ning on such single-tenant architecture often brings increased costs, greater silos, and\ntechnical debt.\nMany of the concepts and tools of on-premises security are implemented in the\ncloud. So it is possible to hew closely to the way data access, categorization, and clas‐\nsification are done on premises. However, such a lift-and-shift approach can involve\ngiving up many of the benefits of a public cloud in terms of elasticity, democratiza‐\ntion, and lower operating cost. Instead, we recommend applying cloud-native secu‐\nrity policies to data that is held in the cloud, because there are better and more\nmodern ways to achieve data protection goals.\nUse cloud identity and access management (IAM) systems rather than the Kerberos-\nbased or directory-based authentication that you may be using on premises. This best\npractice involves managing access services as well as interoperating with the cloud\nprovider’s IAM services by defining roles, specifying access rights, and managing and\nallocating access keys for ensuring that only authorized and authenticated individuals\nand systems are able to access data assets according to defined rules. There are tools\nthat simplify migration by providing authentication mapping during the period of\nmigration.\nSecurity Surface\nAnother big change from on premises to cloud is the sense of vulnerability. At any\nparticular time, there are a number of security threats and breaches in the news, and\nmany of these will involve the public cloud. As we discussed in Chapter 1 , much of\nthis is due to the increased security monitoring and auditability that public cloud sys‐\ntems offer—many on-premises breaches can go unnoticed for long periods of time.\nHowever, because of this increased media attention, organizations may be concerned\nthat they might be the next victim.\nOne of the benefits of the public cloud is the availability of dedicated, world-class\nsecurity teams. For example, data center employees undergo special screening and are\nspecifically trained on security. Also, a dedicated security team actively scans for\nData Protection in the Cloud | 147\nsecurity  threats using commercial and custom tools, penetration tests, quality assur‐\nance (QA) measures, and software security reviews. The security team includes\nworld-class researchers, and many software and hardware vulnerabilities are first dis‐\ncovered by these dedicated teams. At Google, for example, a full-time team known as\nProject Zero  aims to prevent targeted attacks by reporting bugs to software vendors\nand filing them in an external database. The final argument against this sense of vul‐\nnerability is that “security by obscurity” was never a good option.\nThe security surface is also changed by the scale and sophistication of the tools used\non the cloud. Whether it is the use of AI-enabled tools to quickly scan datasets for\nsensitive data or images for unsafe content, or the ability to process petabytes of data\nor to carry out processing in real time on streaming data, the cloud brings benefits in\nterms of being able to apply governance practices that may not be possible on prem‐\nises. Being able to benefit from widely used and well-understood systems also reduces\nthe chances of employee error. Finally, using a common set of tools to be able to com‐\nply with regulatory requirements in all the places where an organization does busi‐\nness leads to a dramatically simpler governance structure within that organization.\nIt is therefore worth having a conversation with your cloud vendor about security\nbest practices, because they vary by public cloud. A public cloud where much of the\ntraffic is on private fiber and where data is encrypted by default will have a different\nsurface than one where traffic is sent over public internet.\nVirtual Machine Security\nA necessary part of securing your data in the public cloud is to design an architecture\nthat limits the effects of a security compromise on the rest of the system. Because\nperimeter security is not an option, it is necessary to redesign the architecture to take\nadvantage of shielding and confidential computing capabilities.\nFor example, Google Cloud offers Shielded VM to provide verifiable integrity of\nCompute Engine virtual machine (VM) instances. This is a cryptographically pro‐\ntected baseline measurement of the VM’s image in order to make the virtual\nmachines tamperproof and provide alerts on changes in their runtime state.\nThis security precaution allows organizations to be confident that your instances\nhaven’t been compromised by boot- or kernel-level malware. It prevents the virtual\nmachine from being boot ed in a different context than it was originally deployed in—\nin other words, it prevents theft of VMs through “snapshotting” or other duplication.\nMicrosoft Azure offers Confidential Compute to allow applications running on Azure\nto keep data encrypted even when it’s in-memory. This allows organizations to keep\ndata secure even if someone is able to hack into the machine that is running the code.\n148 | Chapter 7: Data Protection\nAWS offers Nitro Enclaves to its customers to create isolated compute environments\nto further protect and securely process highly sensitive data such as PII and health‐\ncare, financial, and intellectual property data within Amazon EC2 instances.\nPart of data governance is to establish a strong detection and response infrastructure\nfor data exfiltration events. Such an infrastructure will give you rapid detection of\nrisky or improper activity, limit the “blast radius” of the activity, and minimize the\nwindow of opportunity for a malicious actor.\nThough we are discussing “blast radius” in terms of minimizing of\nthe effects caused by a malicious actor, the blast radius concerns\nalso apply to employees mistakenly (or purposefully) sharing data\non the public internet. One concern of companies moving from\non-premises to the cloud is the increased blast radius of such leaks.\nData housed on premises, if leaked, will only leak internally and\nnot publicly. Companies fear that if their data is in the cloud and\nleaks, it could potentially be accessible to anyone online. While this\nis a valid concern, we hope that by the end of this book you will be\nconvinced that through the implementation and execution of a\nwell-thought-out governance program you can head off and pre‐\nvent either scenario. Ideally, you can make your decisions about\nwhere your data resides based on your business goals and objec‐\ntives rather than on apprehension about the safety of cloud versus\non-premises data storage and warehousing.\nPhysical Security\nMake sure that data center physical security involves a layered security model using as\nmany safeguards as possible, such as electronic access cards, alarms, vehicle access\nbarriers, perimeter fencing, metal detectors, biometrics, and laser-beam intrusion\ndetection. The data center should be monitored 24/7 by high-resolution interior and\nexterior cameras that can detect and track intruders.\nBesides these automated methods, there needs to be good old-fashioned human secu‐\nrity as well. Ensure that the data center is routinely patrolled by experienced security\nguards who have undergone rigorous background checks and training. Not all\nemployees of the company need access to the data center—so the data center access\nneeds to be limited to a much smaller subset of approved employees with specific\nroles. Metal detectors and video surveillance need to be implemented to help make\nsure no equipment leaves the data center floor without authorization.\nAs you get closer to the data center floor, security measures should increase, with\nextra multifactor access control in every corridor that leads to the data center floor.\nPhysical security also needs to be concerned with what authorized personnel can do\nin the data center, and whether they can access only parts of the data center. Access\nPhysical Security | 149\ncontrol by sections is important in cases where regulatory compliance requires that\nall maintenance be performed by citizens of a certain country/countries and/or by\npersonnel who have passed security clearances.\nPhysical security also includes ensuring an uninterrupted power supply and reducing\nthe chance of damage. The data center needs redundant power systems, with every\ncritical component having a primary and an alternate power source. Environmental\ncontrols are also important to ensure smooth running and reduce the chances of\nmachine failure. Cooling systems should maintain a constant operating temperature\nfor servers and other hardware, reducing the risk of service outages. Fire detection\nand suppression equipment is needed to prevent damage to hardware. It is necessary\nto tie these systems in with the security operations console so that heat, fire, and\nsmoke detectors trigger audible and visible alarms in the affected zone, at security\noperations consoles, and at remote monitoring desks.\nAccess logs, activity records, and camera footage should be made available in case an\nincident occurs. A rigorous incident management process for security events is\nrequired so as to inform customers about data breaches that may affect the confiden‐\ntiality, integrity, or availability of systems or data. The US National Institute of Stand‐\nards and Technology (NIST) provides guidance on devising a security incident\nmanagement program (NIST SP 800–61) . Data center staff need to be trained in for‐\nensics and handling evidence.\nPhysical security involves tracking the equipment that is in the data center over its\nentire life cycle. The location and status of all equipment must be tracked from the\ntime it is acquired, through installation, and all the way to retirement and eventual\ndestruction. Stolen hard drives must be rendered useless by ensuring that all data is\nencrypted when stored. When a hard drive is retired, the disk should be verifiably\nerased by writing zeros to the drive and ensuring the drive contains no data. Malfunc‐\ntioning drives that cannot be erased have to be physically destroyed in a way that pre‐\nvents any sort of disk recovery—a multistage process that involves crushing,\ndeformation, shredding, breakage, and recycling is recommended.\nFinally, it is necessary to routinely exercise all aspects of the data center security sys‐\ntem, from disaster recovery to incident management. These tests should take into\nconsideration a variety of scenarios, including insider threats and software\nvulnerabilities.\nIf you are using the data center in a public cloud, the public cloud provider should be\nable to provide you (and any regulatory authorities) with documentation and pro‐\ncesses around all of these aspects.\n150 | Chapter 7: Data Protection",11610
64-Network Security.pdf,64-Network Security,,0
65-Security in Transit.pdf,65-Security in Transit,"Network Security\nThe simplest form of network security is a perimeter network security model—all\napplications and personnel within the network are trusted, and all others outside the\nnetwork are not. Unfortunately, perimeter security is not sufficient for protecting sen‐\nsitive data, whether on-premises or in the cloud. First of all, no perimeter is 100%\nsafe. At some point, some malicious actor will break into the system, and, at that\npoint data may become exposed. The second issue with perimeter security is that not\nall applications within the network can be trusted—an application may have suffered\na security breach, or a disgruntled employee may be trying to exfiltrate data or trying\nto access systems they shouldn’t have access to. Therefore, it is important to institute\nadditional methods of data protection to ensure that exposed data cannot be read—\nthis can include encrypting all stored and in-transit data, masking sensitive data, and\nhaving processes around deleting data when it is no longer needed.\nSecurity in Transit\nNetwork security is made difficult because application data often must make several\njourneys between devices, known as “hops, ” across the public internet. The number of\nhops depends on the distance between the customer’s ISP and the cloud provider’s\ndata center. Each additional hop introduces a new opportunity for data to be attacked\nor intercepted. A cloud provider or network solution that can limit the number of\nhops on the public internet and carry more of the traffic on private fiber can offer\nbetter security than a solution that requires use of the public internet for all hops.\nGoogle’s IP data network, for instance, consists of its own fiber, public fiber, and\nundersea cables. This is what allows Google to deliver highly available and low-\nlatency services across the globe. Google Cloud customers can optionally choose to\nhave their traffic on this private network or on the public internet. Choose between\nthese options depending on the speed your use case requires and the sensitivity of the\ndata being transmitted.\nBecause data is vulnerable to unauthorized access as it travels, it is important to\nsecure data in transit by taking advantage of strong encryption. It is also necessary to\nprotect the endpoints from illegal request structures. One example of this is to use\ngRPC (Google’s high-performance remote procedural call) as the application trans‐\nport layer. gRPC is based around the idea of defining a service, specifying the meth‐\nods that can be called remotely with their parameters and return types. gRPC can use\nprotocol buffers as both its interface definition language (IDL) and its underlying\nmessage interchange format. Protocol buffers are Google’s language-neutral,\nplatform-neutral, extensible mechanism for serializing structured data—think XML,\nbut smaller, faster, and simpler. Protocol buffers allow a program to introspect, that is,\nto examine the type or properties of an object at runtime, ensuring that only connec‐\ntions with correctly structured data will be allowed.\nPhysical Security | 151\nRegardless of the type of network, it is necessary to make sure applications serve only\ntraffic and protocols that meet security standards. This is usually done by the cloud\nprovider; firewalls, access control lists, and traffic analysis are used to enforce net‐\nwork segregation and to stop distributed denial-of-service (DDoS) attacks. Addition‐\nally, it is ideal if servers are allowed to communicate with only a controlled list of\nservers (“default deny”), rather than allowing access very broadly. Logs should be rou‐\ntinely examined to reveal any exploits and unusual activity.\nZoom Bombing\nCOVID-19 has changed how many of us work, and we have found ourselves attend‐\ning a ton of video meetings across Zoom, Google Meet, Webex, and other videocon‐\nferencing apps. It’s not surprising that this surge in popularity and usage has also\nbrought up many cautionary tales of “Zoom bombings, ” in which bad actors use open\nor poorly secured Zoom meetings to post malicious content .\nFirst the media and then, in quick succession, regulators and Congress began to scru‐\ntinize, publicize, and take legal action with respect to what were perceived as privacy\nor data security flaws in the products of some technology companies. The premise\nwas that some of these video calls were not as private as you might think, and that\ncustomer data was being collected and being used in ways that the customers did not\nintend. The issues have been a lot more complicated because the California Con‐\nsumer Privacy Act (CCPA), which took effect on January 1, 2020, instituted many\nregulations pertaining to consumer data and how it can be used.\nWhen the world changes and companies are forced to react quickly to these dynam‐\nics, things are bound to happen, and consumer privacy and data security are usually\nviolated. This is not new. We’ve seen this with Zoom, Facebook, Y ouTube, TikTok,\nand many other large organizations that grow quickly and then get caught up in the\nlimelight. Whenever any organization rises to prominence so quickly, it emerges from\nobscurity and can no longer treat privacy and data security as anything other than a\ntop priority.\nAnd even with all these violations, these are still difficult conversations to have in\nlight of the immense contributions Zoom is making to advance the greater good by\nscaling its operations so quickly to allow millions of Americans to communicate with\neach other during a public health crisis.\nThis truly reinforces the notion that security and governance should not be an after‐\nthought; they should be baked into the fabric of your organization. This chapter rein‐\nforces all the elements your organization should think about when it comes to data\nprotection and ensuring that your data is always protected, in transit and at rest. Hap‐\npily, Zoom made several security improvements, including alerting organizers of\nvideo calls if the link to their meeting is posted online (see Figure 7-1 ).\n152 | Chapter 7: Data Protection",6138
66-Data Exfiltration.pdf,66-Data Exfiltration,"Figure 7-1. Improvements being made after  a Zoom security debacle .\nData Exfiltration\nData exfiltration is a scenario in which an authorized person or application extracts\nthe data they are allowed to access and then shares it with unauthorized third parties\nor moves it to insecure systems. Data exfiltration can occur maliciously or acciden‐\ntally. It can also happen because the authorized account has been compromised by a\nmalicious actor.\nThe traditional method of addressing data exfiltration risk relied on hardening the\nphysical perimeter defenses of private networks. In a public cloud, though, the net‐\nwork fabric is shared among multiple tenants, and there is no perimeter in the tradi‐\ntional sense. Securing data in the cloud requires new security approaches and\nmethods of auditing data access.\nIt is possible to deploy specialized agents that produce telemetry about user and host\nactivity in virtual machines. Cloud providers also support the introduction of explicit\nchokepoints, such as network proxy servers, network egress servers, and cross-\nproject networks. These measures can reduce the risk of data exfiltration, but they\ncannot eliminate it completely.\nIt is important to recognize common data exfiltration mechanisms, identify data at\nrisk of exfiltration through these vectors, and put mitigation mechanisms in place.\nTable 7-1  summarizes the considerations.\nData Exfiltration  | 153\nTable 7-1. Data exfiltration  vectors and mitigation strategies\nExfiltration  vector Data at risk Mitigation mechanism\nUse business email or mobile\ndevices to transmit sensitive\ndata from secure systems to\nuntrusted third parties or\ninsecure systems.Contents of organization\nemails, calendars, databases,\nimages, planning documents,\nbusiness forecasts, and source\ncode.Limit volume and frequency of data transmission.\nAudit email metadata such as from- and to- addresses.\nScan email content using automated tools for common\nthreats.\nAlert on insecure channels and attempts.\nDownload sensitive data to\nunmonitored or insecure\ndevices.Files of sensitive data; any\nsensitive data accessed via\napplications that offer  a\ndownload feature.Avoid storing sensitive data in files  (“data lake”), and\ninstead keep it in managed storage such as an enterprise\ndata warehouse.\nEstablish a policy that prohibits downloads, and keep access\nlogs of data that is requested and served.\nRegulate connections between authorized clients and cloud\nservices using an access security broker.\nImplement dynamic watermarking in visualizations to\nrecord the user responsible for screenshots or photographs\nof sensitive information.\nAdd permissions-aware security and encryption on each file\nusing digital rights management (DRM).\nRequisition or modify virtual\nmachines (VMs), deploy\ncode, or make requests to\ncloud storage or\ncomputation services.\nAnyone with sufficient\npermission can initiate\noutbound transmission of\nsensitive data.Any sensitive data that is\naccessible to employees in\norganizations’ IT departments.Maintain precise, narrowly scoped permissions, and\ncomprehensive, immutable audit logs.\nMaintain separate development and test datasets that\nconsist of simulated or tokenized data, and limit access to\nproduction datasets.\nProvide data access to service accounts with narrow\npermissions, not to user credentials.\nScan all data sent to the broader internet to identify\nsensitive information.\nProhibit outgoing connections to unknown addresses.\nAvoid giving your VMs public IP addresses.\nDisable remote management software like remote desktop\nprotocol (RDP).\nEmployee termination All types of data, even\nnormally benign data like\nhistorical company memos, are\nat risk of data exfiltration  by\nemployees anticipating\nimminent termination.aConnect logging and monitoring systems to HR software and\nset more conservative thresholds for alerting security teams\nto abnormal behavior by these users.\na Michael Hanley and Joji Montelibano, “Insider Threat Control: Using Centralized Logging to Detect Data Exfiltration  Near\nInsider Termination” , Software Engineering Institute, Carnegie Mellon University, 2011.\nIn summary, because perimeter security is not an option, use of public cloud infra‐\nstructure requires increased vigilance and new approaches to secure data from exfil‐\ntration. We recommend that organizations:\n154 | Chapter 7: Data Protection",4444
67-Identity and Access Management.pdf,67-Identity and Access Management,"•Minimize the blast radius of data exfiltration events by compartmentalizing data\nand permissions to access that data, perhaps through line of business or by com‐\nmon workloads that access that data.\n•Use fine-grained access control lists and grant access to sensitive data sparingly\nand in a time-bound manner.\n•Provide only simulated or tokenized data to development teams, because the\nability to create cloud infrastructure creates special risks.\n•Use immutable logging trails to increase transparency into the access and move‐\nment of data in your organization.\n•Restrict and monitor ingress and egress to machines in your organization using\nnetworking rules, IAM, and bastion hosts.\n•Create a baseline of normal data flows, such as amounts of data accessed or trans‐\nferred, and geographical locations of access against which to compare abnormal\nbehaviors.\nVirtual Private Cloud Service Controls (VPC-SC)\nVirtual Private Cloud Service Controls (VPC-SC) improves the ability of an organiza‐\ntion to mitigate the risk of data exfiltration from cloud-native data lakes and enter‐\nprise data warehouses. With VPC-SC, organizations create perimeters that protect\nthe resources and data of an explicitly specified set of services ( Figure 7-2 ).\nFigure 7-2. VPC-SC creates a perimeter around a set of services and data so that the\ndata is inaccessible from outside the perimeter, even to personnel and applications with\nvalid credentials.\nData Exfiltration  | 155\nVPC-SC thus extends private access and perimeter security to cloud services and\nallows unfettered access to data for applications within the perimeter without open‐\ning up that data to access from malicious insiders, compromised code, or malicious\nactors with stolen authentication credentials. Resources within a perimeter can be\naccessed only from clients within authorized VPC networks (either the public cloud\nnetwork or an explicitly allowed on-premises one). It is possible to restrict internet\naccess to resources within a perimeter through allowed IP addresses or ranges of\naddresses.\nClients within a perimeter that have private access to resources do not have access to\nunauthorized (potentially public) resources outside the perimeter—this is what limits\nthe data exfiltration risk. Data cannot be copied to unauthorized resources outside\nthe perimeter.\nVPC-SC, when used in conjunction with a restricted virtual IP , can be used to prevent\naccess from a trusted network to storage services that are not integrated with VPC\nservice controls. The restricted VIP (virtual IP) also allows requests to be made to\nservices supported by VPC Service Controls without exposing those requests to the\ninternet. VPC Service Controls provides an additional layer of security by denying\naccess from unauthorized networks, even if the data is exposed by misconfigured\nCloud IAM policies.\nWe recommend that you use the dry-run features of VPC-SC to monitor requests to\nprotected services without preventing access. This will enable you to monitor\nrequests to gain a better understanding of request traffic to your projects and provide\na way to create honeypot perimeters to identify unexpected or malicious attempts to\nprobe accessible services.\nNote that VPC-SC limits the movement of data, rather than metadata, across a ser‐\nvice perimeter via supported services. While in many cases VPC-SC also controls\naccess to metadata, there may be scenarios in which metadata can be copied and\naccessed without VPC-SC policy checks. It is necessary to rely on Cloud IAM to\nensure appropriate control over access to metadata.\nSecure Code\nData lineage is of no effect if the application code that produces or transforms the\ndata is not trusted. Binary authorization mechanisms such as Kritis  provide software\nsupply-chain security when deploying container-based applications. The idea is to\nextend the Kubernetes-managed runtime and enforce security policies at deploy time.\nIn Google Cloud, binary authorization works with container images from Container\nRegistry or another container image registry and extends Google Kubernetes Engine\n(GKE). This allows the organization to scan built containers for vulnerabilities before\ndeploying to systems that can access sensitive data.\n156 | Chapter 7: Data Protection\nBinary authorization implements a policy model, where a policy is a set of rules that\ngoverns the deployment of container images to an operational cluster. Rules in a pol‐\nicy specify the criteria that an image must pass before it can be deployed. A typical\npolicy requires a container image to have a verified digital signature before it is\ndeployed.\nIn this type of policy, a rule specifies which trusted authorities, called signers , must\nassert that required processes have completed and that an image is ready to move to\nthe next stage of deployment. A signer may be a human user or more often it’s a\nmachine process such as a build-and-test system or a part of your continuous deploy‐\nment pipeline.\nDuring the development life cycle, signers digitally sign globally unique container\nimage descriptors, thereby creating certificated statements called attestations . Later,\nduring the deploy phase, Binary Authorization uses attestors  to verify the certificate\nindicating that required processes in your pipeline have been completed.\nZero-Trust Model\nA perimeter-based security model is problematic, because when that perimeter is\nbreached, an attacker has relatively easy access to a company’s privileged intranet. As\ncompanies adopt mobile and cloud technologies, the perimeter becomes increasingly\ndifficult to enforce. By shifting access controls from the network perimeter to indi‐\nvidual users and devices, a zero-trust security model allows users to access enterprise\napplications from virtually any location without the need for a traditional VPN.\nThe zero-trust model assumes that an internal network is untrustworthy and builds\nenterprise applications based on the assumption that all network traffic emanates\nfrom a zero-trust network, i.e., the public internet. Instead of relying on the IP\naddress of the origin, access to an application depends solely on device and user cre‐\ndentials. All access to enterprise resources is authenticated, authorized, and encrypted\nbased on device state and user credentials. Fine-grained access to different parts of\nenterprise resources is enforced, and the goal is to make the user experience of\naccessing enterprise resources effectively identical between internal and public\nnetworks.\nThe zero-trust model  consists of a few specific parts:\n•Only a device that is procured and actively managed by the enterprise is allowed\nto access corporate applications.\n•All managed devices need to be uniquely identified using a device certificate that\nreferences the record in a device inventory database , which needs to be\nmaintained.\nData Exfiltration  | 157",6969
68-Encryption.pdf,68-Encryption,"•All users are tracked and managed in a user database  and a group database  that\ntightly integrate with HR processes that manage job categorization, usernames,\nand group memberships for all users.\n•A centralized user-authentication portal validates two-factor credentials for users\nrequesting access to enterprise resources.\n•An unprivileged network that very closely resembles an external network,\nalthough within a private address space, is defined and deployed. The unprivi‐\nleged network only connects to the internet, and to limited infrastructure and\nconfiguration management systems. All managed devices are assigned to this\nnetwork while physically located in the office, and there needs to be a strictly\nmanaged access control list (ACL) between this network and other parts of the\nnetwork.\n•Enterprise applications are exposed via an Internet-facing access proxy that\nenforces encryption between the client and the application.\n•An access control manager interrogates multiple data sources to determine the\nlevel of access given to a single user and/or a single device at any point in time.\nThe latter is called endpoint verification .\nIdentity and Access Management\nAccess control encompasses authentication , authorization , and auditing . Authentica‐\ntion determines who you are, authorization determines what you can do, and audit‐\ning logs record what you did.\nAuthentication\nThe identity specifies who has access. This could be an end user who is identified by a\nusername, or an application identified by a service account.\nUser accounts represent a data scientist or business analyst, or an administrator. They\nare intended for scenarios (e.g., notebooks, dashboard tools, or administration tools)\nin which an application needs to access resources interactively on behalf of a human\nuser.\nService accounts are managed by Cloud IAM and represent nonhuman users. They\nare intended for scenarios in which an application needs to access resources automat‐\nically. Service accounts are essentially robot accounts that are defined to have a subset\nof the permissions held by the creator of the service account. Typically, they are cre‐\nated to embody the limited set of permissions required by applications that are run\non the account creator’s behalf.\n158 | Chapter 7: Data Protection\nCloud APIs reject requests that do not contain a valid application credential (no\nanonymous requests are processed). Application credentials need to provide the\nrequired information about the caller making the request. Valid credential types are:\nAPI keys\nNote that an API key says only that this is a registered application. If the applica‐\ntion requires user-specific data, the user needs to authenticate themselves as well.\nThis can be difficult, and so API keys are often used only to access services (e.g.,\nrequests for stock market quotes) that do not need user credentials but just a way\nto ensure that this is a paid subscriber.\nAccess tokens such as OAuth 2.0 client credentials\nThis credential type involves two-factor authentication and is the recommended\napproach for authenticating interactive users. The end user allows the application\nto access data on their behalf.\nService account keys\nService accounts provide both the application credential and an identity.\nAuthorization\nThe role determines what access is allowed to the identity in question. A role consists\nof a set of permissions. It is possible to create a custom role to provide granular access\nto a custom list of permissions. For example, the role roles/bigquery.metadata\nViewer , when assigned to an identity, allows that person to access metadata (and only\nthe metadata, not the table data) of a BigQuery dataset.\nTo grant multiple roles to allow a particular task, create a group, grant the roles to\nthat group, and then add users or other groups to that group. Y ou might find it help‐\nful to create groups for different job functions within your organization and to give\neveryone in those groups a set of predefined roles. For example, all members of your\ndata science team might be given dataViewer  and jobUser  permissions on data ware‐\nhousing datasets. This way, if people change jobs, you’ll just need to update their\nmembership in the appropriate groups instead of updating their access to datasets\nand projects one dataset or project at a time.\nAnother reason to create a custom role is to subtract permissions from the predefined\nroles. For example, the predefined role dataEditor  allows the possessor to create,\nmodify, and delete tables. However, you might want to allow your data suppliers to\ncreate tables but not to modify or delete any existing tables. In such a case, you would\ncreate a new role named dataSupplier  and provide it with the specific list of\npermissions.\nIdentity and Access Management | 159\nNormally, access to resources is managed individually, resource by resource. An iden‐\ntity does not get the dataViewer  role or the tables.getData  permission on all\nresources in a project; rather, the permission should be granted on specific datasets or\ntables. As much as possible, avoid permission/role creep; err on the side of providing\nthe least amount of privileges to identities. This includes restricting both the roles\nand the resources on which they are provided. Balance this against the burden of\nupdating permissions on new resources as they are created. One reasonable compro‐\nmise is to set trust boundaries that map projects to your organizational structure and\nset roles at the project level—IAM policies can then propagate down from projects to\nresources within the project, thus automatically applying to new datasets in the\nproject. The problem with such individualized access is that it can quickly get out of\nhand.\nAnother option for implementing authorization is to use Identity-Aware Proxy (IAP).\nIAP lets you establish a central authorization layer for applications accessed by\nHTTPS, so you can use an application-level access control model instead of relying\non network-level firewalls. IAP policies scale across your organization. Y ou can define\naccess policies centrally and apply them to all of your applications and resources.\nWhen you assign a dedicated team to create and enforce policies, you protect your\nproject from incorrect policy definition or implementation in any application. Use\nIAP when you want to enforce access control policies for applications and resources.\nWith IAP , you can set up group-based application access: a resource could be accessi‐\nble for employees and inaccessible for contractors, or accessible only to a specific\ndepartment.\nPolicies\nPolicies are rules or guardrails that enable your developers to move quickly, but\nwithin the boundaries of security and compliance. There are policies that apply to\nusers—authentication and security policies, such as two-factor authentication, or\nauthorization policies that determine who can do what on which resources—and\nthere are also policies that apply to resources and are valid for all users.\nWhere possible, define policies hierarchically. Hierarchical policies allow you to cre‐\nate and enforce a consistent policy across your organization. Y ou can assign hierarch‐\nical policies to the organization as a whole or to individual business units, projects, or\nteams. These policies contain rules that can explicitly deny or allow roles. Lower-level\nrules cannot override a rule from a higher place in the resource hierarchy. This allows\norganization-wide admins to manage critical rules in one place. Some hierarchical\npolicy mechanisms allow the ability to delegate evaluation of a rule to lower levels.\nMonitor the use of the rules to see which one is being applied to a specific network or\ndata resource. This can help with compliance.\n160 | Chapter 7: Data Protection\nContext-Aware Access is an approach that works with the zero-trust network security\nmodel to enforce granular access control based on a user’s identity and the context of\nthe request. Use Context-Aware Access when you want to establish fine-grained\naccess control based on a wide range of attributes and conditions, including what\ndevice is being used and from what IP address. Making your corporate resources\ncontext-aware improves your security posture. For example, depending on the policy\nconfiguration, it is possible to provide edit access to an employee using a managed\ndevice from the corporate network, but provide them only view access to the data if\nthe access is from a device that has not been fully patched.\nInstead of securing your resources at the network level, Context-Aware Access puts\ncontrols at the level of individual devices and users. Context-Aware Access works by\nleveraging four key pieces of technology that have all been discussed in this chapter:\nIdentity-Aware Proxy (IAP)\nA service that enables employees to access corporate apps and resources from\nuntrusted networks without the use of a VPN.\nCloud identity and access management (Cloud IAM)\nA service that manages permissions for cloud resources\nAccess context manager\nA rules engine that enables fine-grained access control.\nEndpoint verification\nA way of collecting user device details.\nData Loss Prevention\nIn some cases, especially when you have free-form text or images (such as with\nrecords of support conversations), you might not even know where sensitive data\nexists. It is possible that the customer has revealed their home address or credit card\nnumber. It can therefore be helpful to scan data stores looking for known patterns\nsuch as credit card numbers, company confidential project codes, and medical infor‐\nmation. The result of a scan can be used as a first step toward ensuring that such sen‐\nsitive data is properly secured and managed, thus reducing the risk of exposing\nsensitive details. It can also be important to carry out such scans periodically to keep\nup with growth in data and changes in use.\nAI methods such as Cloud Data Loss Prevention  can be used to scan tables and files\nin order to protect your sensitive data (see Figure 7-3 ). These tools come with built-in\ninformation type detectors to identify patterns, formats, and checksums. They may\nalso provide the ability to define custom information type detectors using dictionar‐\nies, regular expressions, and contextual elements. Use the tool to de-identify your\nIdentity and Access Management | 161\ndata, including masking, tokenization, pseudonymization, date shifting, and more, all\nwithout replicating customer data.\nFigure 7-3. Scanning a BigQuery table using Cloud DLP .\nTo redact or otherwise de-identify sensitive data that the Cloud DLP scan found, pro‐\ntect the data through encryption, as discussed in the next section.\nEncryption\nEncryption helps to ensure that if the data accidentally falls into an attacker’s hands,\nthey cannot access the data without also having access to the encryption keys. Even if\nan attacker obtains the storage devices containing your data, they won’t be able to\nunderstand or decrypt it. Encryption also acts as a “chokepoint”—centrally managed\nencryption keys create a single place where access to data is enforced and can be\naudited . Finally, encryption contributes to the privacy of customer data—it allows\nsystems to manipulate data, for example, for backup, and it allows engineers to sup‐\nport the infrastructure without providing access to content.\nIt is possible to use a public cloud provider’s encryption-at-rest and encryption-in-\ntransit mechanisms to ensure that low-level infrastructure access (to hard drives or\nnetwork traffic, for example) does not afford the ability to read your data. Sometimes,\n162 | Chapter 7: Data Protection\nhowever, regulatory compliance might require you to make sure that your data is\nencrypted with your own keys. In such cases, you can use customer-managed encryp‐\ntion keys (CMEK), and you can manage your keys in a key management system\n(KMS), even in Cloud KMS, GCP’s central key management service. Then you can\ndesignate datasets or tables that you want to be encrypted using those keys.\nMultiple layers of key wrapping are used so that the master keys aren’t exposed out‐\nside of KMS (see Figure 7-4 ). Every CMEK-protected table has a wrapped key as part\nof the table metadata. When the native cloud tools access the table, they send a\nrequest to Cloud KMS to unwrap the key. The unwrapped table key is then used to\nunwrap separate keys for each record or file. There are a number of advantages to this\nkey-wrapping protocol that reduce the risk should an unwrapped key be leaked. If\nyou have an unwrapped file key, you can’t read any other files. If you have an unwrap‐\nped table key, you can only unwrap file keys after you pass access control checks. And\nCloud KMS never discloses the master key. If you delete the key from KMS, the other\nkeys can never be unwrapped.\nFigure 7-4. Envelope encryption with data encryption key (DEK) and key encryption key\n(KEK). The KEKs are managed centrally in a KMS, which rotates keys through the use\nof a key ring.\nA common use of encryption is to delete all records associated with a specific user,\noften in response to a legal request. It is possible to plan for such deletion by assign‐\ning a unique encryption key to each userId  and encrypting all sensitive data corre‐\nsponding to a user with that encryption key. In addition to maintaining user privacy,\nyou can remove the records for a user simply by deleting the encryption key. This\napproach has the advantage of immediately making the user records unusable (as\nlong as the deleted key is not recoverable) in all the tables in your data warehouse,\nincluding backups and temporary tables. To query the encrypted table, you need to\ndecrypt the data before querying. This approach is called crypto-shredding .\nIdentity and Access Management | 163",13986
69-Access Transparency.pdf,69-Access Transparency,"Differential  Privacy\nAnother concept for keeping data secure, especially when discussing private data, is\ndifferential privacy. Differential privacy is necessary when you want to share a dataset\nthat contains highly personal or otherwise sensitive information without exposing\nany of the involved parties to identification. This means describing aggregate data\nwhile withholding information about the individuals. However, this is not a simple\ntask—there are ways to statistically re-identify individuals in the dataset by cross-\nmatching different dimensions of aggregate data (and knowing something about the\nindividual). For example, you can extract a particular salary from an aggregate aver‐\nage—if you run multiple averages across the salary recipient’s home neighborhood,\ntheir age group, and so on, you will eventually be able to compute the salary.\nThere are common techniques for ensuring differential privacy:\nk-anonymity\nk-anonymity means that aggregates returned from queries to the datasets are rep‐\nresenting groups of at least k individuals, or are otherwise expanded to include k\nindividuals ( Figure 7-5 ). The value of k is determined by the size of the dataset\nand other considerations relevant to the particular data represented.\nFigure 7-5. Example of a k-anonymized table in which the age was replaced with a k-\nanonymized value.\nAdding “statistically insignificant  noise” to the dataset\nFor fields such as age or gender where there is a discrete list of possible values,\nyou can add statistical noise to the dataset so that aggregates are skewed slightly\nto preserve privacy but the data remains useful. Examples of such tech niques that\ngeneralize the data and reduce granularity are l-diversity and t-distance.\n164 | Chapter 7: Data Protection",1795
70-Keeping Data Protection Agile.pdf,70-Keeping Data Protection Agile,,0
71-Event Threat Detection.pdf,71-Event Threat Detection,"Access Transparency\nIt is important for safeguarding access to the data that any access to the data is trans‐\nparent . Only a small number of on-call engineers should be able to access user data in\nthe production system, and even then it should only be to ensure safe running of the\nsystem. Whenever someone in the IT department or cloud provider accesses your\ndata, you should be notified.\nIn Google Cloud, for example, Access Transparency provides you with logs that cap‐\nture the actions Google personnel take when accessing your content. Cloud Audit\nLogs helps you answer questions about “who did what, where, and when?” in your\ncloud projects. While Cloud Audit Logs provides these logs about the actions taken\nby members within your own organization, Access Transparency provides logs of the\nactions taken by personnel who work for the cloud provider.\nY ou might need Access Transparency logs data for the following reasons:\n•To verify that cloud provider personnel are accessing your content only for valid\nbusiness reasons, such as fixing an outage or in response to a support request\n•To verify and track compliance with legal or regulatory obligations\n•To collect and analyze tracked access events through an automated security infor‐\nmation and event management (SIEM) tool\nNote that the Access Transparency logs have to be used in conjunction with Cloud\nAudit Logs because the Access Transparency logs do not include access that origi‐\nnates from a standard workload allowed through Cloud IAM policies.\nKeeping Data Protection Agile\nData protection cannot be rigid and unchanging. Instead, it has to be agile to take\naccount of changes in business processes and respond to observed new threats.\nSecurity Health Analytics\nIt is important to continually monitor the set of permissions given to users to see\nwhether any of those are unnecessarily broad. For example, we can periodically scan\nuser-role combinations to find how many of the granted permissions are being used.\nIn Figure 7-6 , the second user has been granted a BigQuery Admin role but is using\nonly 5 of the 31 permissions that the role grants. In such cases, it is better to either\nreduce the role or create a more granular custom role.\nKeeping Data Protection Agile | 165\nFigure 7-6. Scan the use of permissions by users to narrow down role definitions  or pro‐\nvide the users more fine-grained  access.\nData Lineage\nA key attribute of keeping data protection agile is to understand the lineage of every\npiece of data. Where did it come from? When was it ingested? What transformations\nhave been carried out? Who carried out these transformations? Were there any errors\nthat resulted in records being skipped?\nIt is important to ensure that data fusion and transformation tools (see Figure 7-7 )\nprovide such lineage information, and that this linear information is used to analyze\nerrors being encountered by production systems.\nFigure 7-7. It is important to maintain the data lineage of all enterprise data and\naddress errors in ingest processes.\n166 | Chapter 7: Data Protection",3102
72-Separated Network Designs.pdf,72-Separated Network Designs,"1Roger Collier, “NHS Ransomware Attack Spreads Worldwide” , Canadian Medical Association Journal  189, no.\n22 (June 2017): E786–E787.\n2David Floyd, “Was I Hacked? Find Out If the Equifax Breach Affects Y ou” , Investopedia , updated June 25,\n2019.Event Threat Detection\nThe overall security health needs to be continually monitored as well. Network secu‐\nrity logs need to be analyzed to find the most frequent causes of security incidents.\nAre a number of users trying (and failing) to access a specific file or table? It is possi‐\nble that the metadata about the file or table has been breached. It is worth searching\nfor the source of the metadata leak and plugging it. It is also advisable to secure the\ntable before one of the attacks succeeds.\nInstead of limiting ourselves to scanning security logs, it is worth modeling network\ntraffic and looking for anomalous activity in order to identify suspicious behavior\nand uncover threats. For example, an unusual number of SSH logins by an authorized\nemployee might be a sign of data exfiltration, and the definition of “unusual” in this\ncase can be learned by an AI model that compares the activity of an employee against\ntheir peers who are doing a similar role and working on similar projects.\nData Protection Best Practices\nWhile data protection is top of mind for all industries and users, there are different\napproaches from one industry to another, including some stricter approaches in\nindustries such as healthcare, government, and financial services that routinely deal\nin sensitive data.\nThe healthcare industry has been dealing with medical records for decades. As\nhealthcare providers have moved more and more to digital tools for record keeping,\nthe industry has experienced globally known incidents. The ransomware cyberattack\nthat affected more than 60 trusts within the United Kingdom’s National Health Ser‐\nvice (NHS) spread to more than 200,000 computer systems in 150 countries, and the\nlist continues to grow.1\nSimilarly, financial institutions have been dealing with cyberattacks. We still remem‐\nber when Equifax announced a massive cybersecurity breach in 2017 in which cyber‐\ncriminals accessed the personal data of some 145.5 million Equifax customers,\nincluding full names, social security numbers, birth dates, addresses, and driver’s\nlicense numbers. At least 209,000 customers’ credit card credentials were taken in the\nattack.2 Since then, a number of more visible data breaches have taken place, and\nmany if not all of them affected managed systems that were part of the companies’\nown data centers.\nData Protection Best Practices | 167",2655
73-Physical Security.pdf,73-Physical Security,"The natural question that comes to mind is: why are these data breaches still taking\nplace. even with the effective processes and tools that are available, and despite a con‐\ncerted focus to protect data? It comes down to how the best practices are imple‐\nmented, and whether the institutions are staying on top of their data governance\nprocesses day in and day out, 24/7, with no complacency. In the previous sections, we\nhave highlighted a comprehensive way of protecting the data; in cloud, with physical\nand network security, and with advanced IAM.\nProfessionals in each industry—healthcare, financial institutions, retail, and others—\nare trying to establish what they believe to be the best practices for their world when\nit comes to data protection. As an example of those best practices, let’s start with what\ndata protection experts in the healthcare industry are suggesting to their users.\nA data breach in healthcare can occur in several forms. It could be a criminal cyberat‐\ntack to access protected health data for the purpose of committing medical identity\ntheft, or it could be an instance of a healthcare employee viewing patient records\nwithout authorization.\nOrganizations in the healthcare industry have to be very diligent in protecting sensi‐\ntive patient, financial, and other types of datasets, and must stay on top of this 24/7,\nthroughout their entire operations, by educating their employees and by utilizing\nbest-in-class security tools and best practices for the industry. Here are some of the\nrecommended best practices for the healthcare industry.\nSeparated Network Designs\nHackers utilize various methods to gain access to healthcare organizations’ networks.\nIT departments in the healthcare industry should rigorously deploy tools such as fire‐\nwalls and antivirus and anti-malware software rigorously. However, focusing on\nperimeter security is not enough. Healthcare firms should adopt network design\napproaches that separate networks so that intruders cannot access the patient data\neven if they are able to obtain access to parts of their network. Some firms are already\npracticing and benefitting from these practices at the network-design level.\nPhysical Security\nPaper-based record keeping of much of the sensitive data is still a very common prac‐\ntice in the healthcare industry, and it is imperative that healthcare providers provide\nphysical security with locked cabinets and doors, cameras, and so on, while physically\nsecuring the IT equipment where sensitive data is stored, including providing cable\nlocks to the laptops within offices.\n168 | Chapter 7: Data Protection\n3Jim Forsyth, “Records of 4.9 Million Stolen from Car in Texas Data Breach” , Reuters , September 29, 2011.Physical Data Breach in Texas\nThroughout this book we have discussed (and will discuss further) security around\nyour data in on-premises storage systems and/or in cloud-based storage. There are\nuse cases, however, around securing actual physical data (like papers or tapes), as we\npointed out in the introduction to this section.\nWhile it’s a bit of an older example, an event that occurred in Texas in 2011  illustrates\nthe importance of physical security.\nA data contractor was working for Science Applications International Corporation\n(SAIC), the firm that handles data for TRICARE, the federal government and military\nhealthcare program. In his car, the contractor had the backup tapes of electronic\nhealthcare records for over 4.6 million active and retired military personnel. All of\nthese tapes were stolen from the car, compromising the health information of not\nonly the active and retired military personnel, but also their families.\nSAIC clarified in a press release that while the tapes medical record data such as social\nsecurity numbers, diagnoses, and lab reports), financial data was not included on the\ntapes and thus was not leaked.\nSAIC and TRICARE set up incident response call centers to help patients deal with\nthe security breach and to help them to place a fraud alert on their credit reports if\nneeded, but they also made a statement:\nIf you are a citizen in the modern society, if you have a credit card, if you shop\nonline, if you have information stored, you should anticipate that some day your\ninformation will get stolen.3\nThis statement isn’t wholly inaccurate even a decade later, but what it fails to consider\nis the role of governance in preventing these sorts of things from happening in the\nfirst place—which is likely to be the main reason you are reading this book.\nY ou should be aware of and consider implementing additional security measures for\nany physical data that you may have. Healthcare, as in this example, is an obvious sce‐\nnario, but there are many instances in which there may be physical sensitive data that\nneeds to be treated and/or needs to be something that you educate your employees on\nand build into your data culture (see Chapter 9  for more on this).\nAs in this example, even the methods by which you transport physical data from one\nlocation to another should be considered. Often we think about how data is transfer‐\nred between applications, where it’s stored, whether or not it’s encrypted, and so on.\nBut as this example shows, you should also consider how you are physically trans‐\nporting data, if this is something you or your company will be doing. Will it be by car?\nWhat are the safeguards that are in place to ensure that data is always being watched\nData Protection Best Practices | 169",5560
74-Portable Device Encryption and Policy.pdf,74-Portable Device Encryption and Policy,,0
75-Data Deletion Process.pdf,75-Data Deletion Process,"over or protected? Do you have any safeguards in place in the event of this data being\nstolen? Is it unreadable? Is it encrypted?\nHopefully, if you note these areas of consideration and generate a plan for how to\ntreat and protect your physical data, what happened to SAIC and TRICARE in 2011\nwon’t happen to you.\nPortable Device Encryption and Policy\nData breaches in the healthcare industry in recent years have occurred largely\nbecause a laptop or storage device containing protected health information was lost\nor stolen. A key measure that healthcare organizations should always undertake to\nprevent a data breach due to a stolen portable device is to encrypt all devices that\nmight hold patient data, including laptops, smartphones, tablets, and portable USB\ndrives. Also, in addition to providing encrypted devices for their employees, health‐\ncare organizations should establish a strong policy against carrying data on an unen‐\ncrypted personal device. We are seeing more and more bring-your-own-device\n(BYOD) policies adopted by various institutions, and many healthcare providers are\nnow using mobile device management (MDM) software to enforce those policies.\nData Deletion Process\nA key lesson that data-breach victims have learned is the need for a data-deletion pol‐\nicy, because as more data is held by an organization, there is more for intruders to\nsteal. Healthcare institutions should deploy a policy mandating the deletion of patient\nand other information that’s no longer needed, while complying with regulations that\nrequire records to be kept for a certain duration. Furthermore, regular audits must be\nexercised to ensure that policies are followed and that organizations know what data\nis where, what might be deleted, and when it can be deleted.\nElectronic medical device and OS software upgrades\nOne of the areas that healthcare providers and their IT organizations need to pay\ncloser attention to is medical device software and OS upgrades. Intruders have dis‐\ncovered that healthcare providers were not always so diligent and still utilize outdated\nOS-based medical devices that become easy targets to hack, despite update recom‐\nmendations from the healthcare device vendors’ . While these updates may seem dis‐\nruptive to the providers and their employees, a data breach is much worse for a\nprovider. Keeping these devices patched and up to date will minimize these\nvulnerabilities.\n170 | Chapter 7: Data Protection\n4Emsisoft Malware Lab, “The State of Ransomware in the US: Report and Statistics 2019” , December 12, 2019.Data breach readiness\nThere is no such thing as preventing every possible IT security incident; this is exactly\nwhy institutions should have a well-established plan to deploy when and if a data\nbreach occurs. Educating employees on what a HIPAA violation is, how to avoid\nphishing, avoiding target attacks, and choosing strong passwords are among a few\nsimple steps that healthcare institutions can take.\nThe healthcare industry best practices and suggestions we’ve highlighted apply to\nother industries as well. Institutions at large should implement processes and proce‐\ndures to reduce in-the-moment thinking when it comes to dealing with data\nbreaches. Automation and well-planned and commented response are key in dealing\nwith a potential data breach. Establishing internal data security controls will reduce\nthe risk of data breaches while improving regulatory compliance. In summary, organ‐\nizations should establish the following steps in their company-wide process:\n•Identify systems and data that need to be protected.\n•Continuously evaluate possible internal and external threats.\n•Establish data security measures to manage identified risks.\n•Educate and train employees regularly.\n•Monitor, test, and revise—and remember that the IT world risks change all the\ntime.\nWhy Do Hackers Target the Healthcare Industry?\nEvery year, we see study after study on the impact of security breaches in the health‐\ncare industry, where hackers demand large ransoms from healthcare vendors and\nproviders. 2019 was no different; in fact, according to a study by Emsisoft, it has\nreached record levels, costing healthcare industry members over $7.5 billion just in\nthe US, where over 100 state and local government agencies, over 750 providers,\nnearly 90 universities, and more than 1,200 schools have been affected.4 The results\nwere not just an inconvenience of expense but a massive disruption to healthcare\ndelivery: surgeries were postponed, and in some cases patients had to be transferred\nto other hospitals to receive the urgent care they needed—not to mention the disrup‐\ntion to payment systems and collection systems. Even student grades were lost. 2020\nwas no better in terms of healthcare data breaches, according to HealthITSecurity .\nThe World Privacy Forum website has an interactive map  that is a great resource for\ngetting the latest on medical data breaches (see Figure 7-8 ).\nData Protection Best Practices | 171\nFigure 7-8. An example from the World Privacy Forum website shows medical data\nbreaches in the state of Kentucky in 2018.\nMany of us regularly wonder, “Why is this happening, and why is this happening\nespecially in the healthcare industry?” We are alarmed to discover that a large\npercentage  of these institutions did not even know how often these breaches were\nhappening, and only a small fraction had the processes in place to recover and protect\ntheir data. It is especially worrying to know that many of the breaches were happen‐\ning via very smart and digitally connected IoT devices. The industry has to do much\nbetter going forward.\n172 | Chapter 7: Data Protection",5734
76-Summary.pdf,76-Summary,"Hackers target the healthcare industry largely because it is one of the few industries\nwith legacy operating systems like Microsoft XP or Windows 7 in their medical\ndevices , and it has not been industry practice to stay on top of OS patches when they\nbecome available. In fact, some of these older OS-based medical devices will no\nlonger be supported by Microsoft, meaning the vendors need to ensure their users\nupgrade to a device that is up to date and better prepared for today’s security\nbreaches. Failing to patch or renew their devices will continue to increase the risk.\nManufacturing a medical device is not easy; it takes many years to build one, and then\nit is in use for 15 years or longer. So these devices become easy targets during their\n20+ years of use, not to mention that many of them are shipped to other countries for\nsecondary and tertiary use. Considering how often the electronics and software in\nour daily lives change (e.g., cell phones have an average life cycle of two years before\nwe refresh them), you can see why hackers target healthcare devices.\nThe healthcare industry must be much more proactive in its approach to data protec‐\ntion. It must utilize best practices around data protection; have total control of assets;\ncontinuously assess gaps and processes; leverage best practices and experts from\nNIST, HITRUST, and others in the cybersecurity industry; and ensure that appropri‐\nate budgets are allocated for upgrading legacy medical device software and endpoints\nthat are open to hackers. A single weak point is enough for a hacker to impact an\nentire network.\nSummary\nIn this chapter, we looked at what is perhaps the key concern of data governance—\nprotecting data. A necessary part of planning protection is ensuring that lineage is\ntracked and quality monitored so that the data remains trustworthy. The planning\nprocess should identify the levels of protection and decide the kind of protection\nafforded at each level. Then the classification process itself—classifying data assets\ninto levels—needs to be planned for.\nA key concern with data stored in the public cloud is multi-tenancy. There could be\nother workloads running on the same machines that are carrying out data processing\nfor your company. Therefore, it is important to consider the security surface and take\ninto account virtual machine security. Typically, physical security is provided by the\ncloud provider, and your responsibility for network security is limited to getting the\ndata to/from the cloud (security in transit) and configuring the appropriate security\ncontrols, such as VPC-SC. Important aspects to consider are data exfiltration, zero\ntrust, and how to set up Cloud IAM. These involve setting up governance of authenti‐\ncation, authorization, and access policies.\nBecause your data may be used by data science users who need access to all of the\ndata, Cloud IAM and row-based security may be insufficient. Y ou must also deter‐\nmine the need for the masking, tokenization, or anonymization of sensitive\nSummary | 173\ninformation that may not be needed to train machine learning models. A clear under‐\nstanding of what to encrypt and how to implement differential privacy, comprehen‐\nsive access monitoring, and the ablity to change security profiles in response to\nchanging risks all become more important the more the data gets used for machine\nlearning.\nThe data protection governance process should also create policies on when separate\nnetwork designs are necessary, how portable devices have to be treated, when to\ndelete data, and what to do in the case of a data breach. Finally, data protection needs\nto be agile, because new threats and attack vectors continue to materialize. So the\nentire set of policies should be periodically revisited and fine-tuned.\n174 | Chapter 7: Data Protection",3874
77-Chapter 8. Monitoring.pdf,77-Chapter 8. Monitoring,,0
78-Why Perform Monitoring.pdf,78-Why Perform Monitoring,"CHAPTER 8\nMonitoring\nIn previous chapters, we explained what governance is; discussed the tools, people,\nand processes of governance; looked at data governance over a data life cycle; and\neven did a deeper dive into governance concepts, including data quality and data\nprotection.\nIn this chapter, we will do a deep dive into monitoring  as a way to understand how\nyour governance implementations are performing day-to-day, and even on a longer-\nterm basis. Y ou will learn what monitoring is, why it is important, what to look for in\na monitoring system, what components of governance to monitor, the benefits of\nmonitoring, and the implications of doing so. Y ou have implemented governance in\nyour organization, so how do you know what’s working and what’s not? It’s important\nto be able to monitor and track the performance of your governance initiatives so you\ncan report to all the stakeholders the impact the program has had on the organiza‐\ntion. This allows you to ask for additional resources, course-correct if/as needed,\nlearn from the wins and failures, and really showcase the impact of the governance\nprogram—not to mention making the most of potential growth opportunities that\nmight become more visible to the chief data and digital officers of your organization.\nSo what is monitoring? Let’s start by introducing the concept.\nWhat Is Monitoring?\nMonitoring allows you to know what is happening as soon as it happens so you can\nact quickly. We’re in a world in which companies and individuals are made and\ndestroyed on social networks; that’s how powerful these platforms are. User expecta‐\ntions are fueling complexity in applications and infrastructure, so much so that over\n50% of mobile users abandon sites that take more than three seconds to load. Most\nweb pages take a lot longer than that to load, creating a significant gap between con‐\nsumers’ expectations and most businesses’ mobile capabilities . If you’re an\n175\norganization that services customers and you know of this stat, then you must ensure\nthat you have a system that’s constantly monitoring your website load times and alert‐\ning you when numbers are outside acceptable bounds. Y ou want to make sure that\nyour team can resolve issues before they become bigger problems.\nSo what is monitoring? In simple terms, monitoring is a comprehensive operations,\npolicies and performance management framework. The aim is to detect and alert\nabout possible errors of a program or a system in a timely manner and deliver value\nto the business. Organizations use monitoring systems to monitor devices, infrastruc‐\nture, applications, services, policies, and even business processes. Because monitoring\napplies to many areas of the business beyond what we can cover, in this chapter we\nwill primarily focus on monitoring as it relates to governance.\nMonitoring governance involves capturing and measuring the value generated from\ndata governance initiatives, compliance, and exceptions to defined policies and pro‐\ncedures—and finally, enabling transparency and auditability into datasets across their\nlife cycle.\nWhat’s interesting about monitoring is that when everything is working well and\nthere are no issues, efforts usually go unnoticed. When issues arise and things go\nwrong—for example, with data quality or compliance exceptions—then governance is\none of the first areas that gets blamed because these areas fall right within governance\nareas that should be constantly monitored. (We will do a deeper dive into each of\nthese areas later in the chapter.) Because of this, it’s important for you to define met‐\nrics that allow you to demonstrate to the business that your efforts and investments in\ndata governance are benefiting the business in reducing costs, increasing revenue,\nand providing business value. Y ou need to implement metrics that you will track\naccordingly, enabling you to understand and showcase the improvements you are\nmaking to the bottom line. In later sections of this chapter, we will provide you with a\nlist of some of the key metrics you can track to show the efficacy of your governance\nprogram.\nWhy Perform Monitoring?\nMonitoring allows you to review and assess performance for your data assets, intro‐\nduce policy changes within the organization, and learn from what’s working and\nwhat’s not, with the ultimate goal of creating value for the business. If your organiza‐\ntion is used to hearing about incidents via its customer base and is spending too\nmuch time and money on manual support, then a monitoring system is vital. Moni‐\ntoring serves many different functions, and for the majority of the use cases, a moni‐\ntoring system will help you with alerting, accounting, auditing, and compliance. Let’s\ndo a deep dive into these core areas:\n176 | Chapter 8: Monitoring\nAlerting\nIn its simplest terms, an alert warns someone or something of a danger, threat, or\nproblem—typically with the intention of avoiding it or dealing with it. An alert\nsystem can help you prevent incidents, and when incidents happen, they are\ndetected earlier and faster. A governance monitoring system that is specifically\ndesigned to monitor data quality can alert you when data quality thresholds fall\noutside the allowable limits, allowing you to avoid falls in service or to minimize\nthe amount of time needed to resolve the issue.\nAccounting\nAn account refers to a report or description of an event or experience. In this\ncore area of monitoring, you want to get an in-depth analysis of your applica‐\ntions, infrastructure, and policies. This enables you to create more appropriate\nstrategies, set realistic goals, and understand where improvements are needed,\nallowing you to discover the effectiveness and value generated from data gover‐\nnance and stewardship efforts.\nAuditing\nAuditing refers to a systematic review or assessment of something to ensure that\nit is performing as designed. Audits also enable transparency into data assets and\ntheir life cycle. Auditing allows you to understand the ins and outs of your busi‐\nness so that you can make improvements to processes and internal controls, with\nthe aim of reducing organizational risk and preventing unexpected costs from\nexternal auditors.\nCompliance\nRegulatory compliance refers to the efforts required to help you meet relevant\npolicies, laws, standards, and regulations. When your systems, thresholds, or pol‐\nicies are outside the defined rules, resulting in out-of-compliance processes,\nmonitoring can help you stay in compliance; your business can be alerted as soon\nas these exceptions are detected, giving you the opportunity to resolve the issues\nin order to stay compliant.\nAlthough this is not the first time we’re covering this subject, we\nwant to reiterate just how important alerting  is to your governance\nprogram. In countless interviews, and during our research, the lack\nof sufficient alerting has been mentioned as a top pain point among\ndata scientists, data engineers, and the like. While there are many\ningredients we’ve discussed that are key to a successful governance\nstrategy, proper alerting is something that often gets overlooked.\nImprovements in this area not only aid in prevention and early\ndetection of incidents but also help to streamline tasks and the time\nspent on those tasks by your employees.\nWhy Perform Monitoring? | 177\n1Tim Wilson, “ A Cautionary Tale” , Dark Reading , August 17, 2007.Monitoring use cases will tend to fall within the four buckets we’ve just highlighted,\nand in most cases they will overlap. For example, as you monitor compliance in your\norganization, this can also help you when it comes to audits and proving that your\norganization is doing the right things and performing as designed.\nMonitoring is often done using a monitoring system that can be built in-house by\npurchasing a system and configuring it to your other internal systems. It can be out‐\nsourced to other vendors as a managed service for ease of setup and expense, or if\nyou’re using cloud solutions, it can be embedded within your organization’s work‐\nflows. Open source tools also provide some monitoring solutions to be considered.\nWe will delve more deeply into monitoring systems later in this chapter; for now, let’s\nfocus on what areas of governance you should monitor, why, and how.\nWhy Alerting Is a Critical Monitoring Function\nThis is a cautionary tale about a small website management services company that\nserved 50 major hospitals around the country. The story sounds a little too familiar,\nand honestly, this continues to happen to many organizations that store customer\ninformation. For the company, things were going so well that it decided to introduce a\nnew service that allowed patients and partners to do online bill payment.\nOne day during a maintenance procedure, an employee accidentally turned off the\ncompany’s firewall, and all of the patient data stored on its online billing service sys‐\ntem (for at least five hospitals and approximately 100,000 patients) was left exposed to\nthe world.1 And because the hospitals reported the breaches as separate incidents, the\nnature of the error was not immediately apparent.\nUnfortunately for this small company, there was no way to recover from such an inci‐\ndent. It simply closed its doors and shut down its website. There’s so much to learn\nfrom this experience, because if customer data had been properly governed and\nstored—that is, if the data had been encrypted and the right access controls had been\nenforced—it would have been easier to catch the incident right when it happened and\nalert the hospitals as needed. In addition, this reinforces the importance of monitor‐\ning and, more specifically, of alerting. It’s scary to think that an employee turned off a\nfirewall system without any system alerts being triggered or the right folks being\nalerted.\n178 | Chapter 8: Monitoring",9987
79-What Should You Monitor.pdf,79-What Should You Monitor,,0
80-Data Lineage Monitoring.pdf,80-Data Lineage Monitoring,"What Should You Monitor?\nThere are many areas of an organization that are monitored: operating systems and\nhardware, network and connectivity, servers, processes, governance, and more. In\norder to stay close to the core of this book and chapter, we will now do a deeper dive\ninto monitoring as it relates to data governance. A lot of the concepts we will explore\nnext have already been covered in previous chapters, and so we will focus on which\nparts need to be monitored and how to go about it. We will keep referring back to\nwhat you’ve learned in order to solidify these concepts across the key areas.\nData Quality Monitoring\nIn Chapters 1 and 2, we introduced the concept of data quality, and we went into that\nin more detail in Chapter 5 . Data quality allows the organization to trust the data and\nthe results. High-quality data means that the organization can rely on that data for\nfurther calculations/inclusions with other datasets. Because of how important data\nquality is, it should be monitored proactively, and compliance exceptions should be\nidentified and flagged in real time. This will allow the organization to move quickly to\nidentify and mitigate critical issues that can cause process breakdowns.\nThere are critical attributes of data quality that should be tracked and measured,\nincluding completeness, accuracy, duplication, and conformity. These are outlined in\ndetail in Table 8-1  and should map to specific business requirements you have set\nforth for your governance initiative. Monitoring can help you create controls for vali‐\ndation and can provide alerts as needed when these are outside the defined thresh‐\nolds. The attributes in Table 8-1  are common problem areas within the data\nmanagement space that make it difficult to trust data for analysis.\nTable 8-1. Data quality attributes\nAttribute Description\nCompleteness This identifies  what data is missing and/or not usable.\nAccuracy This identifies  the correctness and consistency of the data and whether the correct values are stored for an\nobject.\nDuplication This identifies  which data is repeated. Duplicate records make it difficult  to know the right data to use for\nanalysis.\nConformity This identifies  which data is stored in a nonstandard format that will not allow analysis.\nProcess and tools for monitoring data quality\nA data quality monitoring system routinely monitors and maintains data quality\nstandards across a data life cycle and ensures they are met. It involves creating con‐\ntrols for validation, enabling quality monitoring and reporting, accessing the level of\nWhat Should You Monitor? | 179\nincident severity, enabling root cause analysis, and providing remedy\nrecommendations.\nWhen setting up a data quality monitoring process, some of the things you need to\nconsider are:\nEstablishing a baseline\nEstablish a baseline of the current state of data quality. This will help you identify\nwhere quality is failing and help you determine what is “good” quality and what\nthose targets are. These targets must be tied to the business objectives you have\nset forth for your governance initiatives. Comparing results over time is essential\nto proactive management of ongoing data quality improvement and governance.\nQuality signals\nThese are usually monitored over a period of time or by the source of the data.\nThe monitoring system will be looking to verify data fields for completeness,\naccuracy, duplicates, conformity, statistical anomalies, and more. When data\nquality falls below a specified threshold, an alert would be triggered with more\ninformation about the quality issue observed. These quality signal rules are usu‐\nally set forth by the data governance committee, which ensures compliance with\ndata policies, rules, and standards. More details around this are outlined in Chap‐\nter 4 . These policy guidelines and procedures ensure that the organization has\nthe data program that was envisioned by the organization.\nIn order to get started with monitoring data quality, determine a set of baseline met‐\nrics for levels of data quality and use this to help you build a business case to justify\nthe investment and, over time, help you make improvements to the governance\nprogram.\nData Lineage Monitoring\nWe introduced the concept of data lineage in Chapter 2  and talked about why track‐\ning lineage is important. The natural life cycle of data is that it is generated/created by\nmultiple different sources and then undergoes various transformations to support\norganizational insights. There is a lot of valuable context generated from the source of\nthe data and all along the way that is crucial to track. This is what data lineage is all\nabout. Monitoring lineage is important to ensure data integrity, quality, usability, and\nthe security of the resulting analysis and dashboards.\nLet’s be honest: tracking and monitoring lineage is no simple task; for many organiza‐\ntions, their data life cycle can be quite complex, as data flows from different sources,\nfrom files to databases, reports, and dashboards, while going through different trans‐\nformation processes. Lineage can help you track why a certain dashboard has differ‐\nent results than expected and can help you see the movement of sensitive data classes\nacross the organization.\n180 | Chapter 8: Monitoring\nWhen looking to track and monitor lineage, it’s important to understand certain key\nareas that you will come across. Table 8-2  has some of these attributes. These are not\nall encompassing but are just some of the more important ones for you to know.\nTable 8-2. Data lineage attributes\nAttribute Description\nData\ntransformationsThese are the various changes and hops (aggregates, additionals, removals, functions, and more) as\ndata moves along the data life cycle. Monitoring helps you understand details of the data points and\ntheir historical behavior, as data gets transformed along the way.\nTechnical metadata Metadata is important for understanding more about the data elements. Enabling automatic tagging\nof data based on its source can help provide more understanding of the data asset.\nData quality test\nresultsThese represent data quality measurements that are tracked at specific  points of the data life cycle, in\norder to take action as/if needed.\nReference data\nvaluesReference data values can be used to understand backward data lineage and transformation from that\nspecific  data point, and/or the intermediate transformation following that point with forward data\nlineage. Understanding reference data values can be useful in performing root cause analysis.\nActor An actor is an entity that transforms data. It may be a MapReduce job or an entire data pipeline. Actors\ncan sometimes be black boxes, and the inputs and outputs of an actor are tapped to capture lineage in\nthe form of associations.\nProcess and tools for monitoring data lineage\nWhat makes monitoring lineage so complicated is that it needs to be captured at mul‐\ntiple levels and granularities—this is tedious and time consuming because of all the\nintricacies and dependencies. As mentioned earlier in this chapter, monitoring serves\ndifferent purposes; for lineage it can be used to alert, audit, and comply with a set of\ndefined rules and policies. As described in Table 8-2 , one of the things you could\nmonitor is the behavior of the actors; when their resulting transformed outputs are\nincorrect, an alert function can be set up that the inputs are investigated, and the\nactors are augmented and corrected to behave as expected or are removed from the\nprocess flow of the data.\nMonitoring lineage can also provide a lineage audit trail, which can be used to deter‐\nmine the who, what, where, and when of a successful or attempted data breach in\norder to understand which areas of the business were or might have been affected by\nthe breach. In addition, the important details tracked by data lineage are the best way\nto provide regulatory compliance and improve risk management for businesses.\nLineage is about providing a record of where data came from, how it was used, who\nviewed it, and whether it was sent, copied, transformed, or received and it is also\nabout ensuring this information is available. Y ou will need to identify the best way to\ndo this for your organization, depending on the use cases and needs of the business.\nIdentify the data elements, track the listed elements back to their origin, create a\nrepository that labels the sources and their elements, and finally, build visual maps for\neach system and a master map for the whole system.\nWhat Should You Monitor? | 181",8684
81-Program Performance Monitoring.pdf,81-Program Performance Monitoring,"Compliance Monitoring\nCompliance has been covered in great detail in several of the previous chapters.\nUnderstanding state and federal regulations, industry standards, and governance pol‐\nicies and staying up to date on any changes ensures that compliance monitoring is\neffective.\nThe changing nature of laws and regulations can make monitoring compliance diffi‐\ncult and time consuming. Noncompliance is not an option because it often results in\nconsiderable fines—sometimes more than twice the cost of maintaining or meeting\ncompliance requirements. In a study from the Ponemon Institute , the average cost for\norganizations that experience noncompliance problems is $14.82 million. That\nincludes fines, forced compliance costs, lack of trust from customers, and lost busi‐\nness.\nMonitoring compliance means having an internal legal representative (attorney),\nthough at times this task falls on the privacy tsar or someone else in security, who\nmust continually keep up with laws and regulations and how they impact your busi‐\nness. It also requires that changes are made according to the new information gath‐\nered in order to stay in compliance. In addition, to stay in compliance requires\nauditing and tracking access to data and resources within the organization. All of this\nis done to ensure your business is compliant in case of an audit by the government.\nProcess and tools for monitoring compliance\nTo be successful in monitoring regulatory compliance, you need to evaluate which\nregulations apply to your data governance efforts and what compliance looks like\nwith these regulations. Once this is done, do an audit to understand your current\ngovernance structure with regard to the relevant regulations. Consider this a bench‐\nmark exercise to understand your current standing, future requirements, and how\nyou can build a plan to get to the end state, and then continuously monitor that end\nstate to ensure compliance.\nOnce you’ve completed the audit, you should jump to creating a monitoring plan;\nhere you should address all risks identified in the audit stage and prioritize those that\nare the greatest threat to the organization. Next, decide how you are going to imple‐\nment the monitoring program, including roles and responsibilities.\nThe resulting output will be dependent on the level and frequency of regulatory\nchanges and updates from the relevant regulatory boards. Make sure you have a way\nto inform the regulatory agencies of failed audits and of how you’re looking to miti‐\ngate them.\nHere are some ways you can be proactive about compliance:\n182 | Chapter 8: Monitoring\n•Keep on top of regulatory changes and make sure that you’re checking for up\ndated standards and regulations. Of course, this is easier said than done.\n•Be transparent so your employees understand the importance of compliance and\nthe regulations they are complying with. Offer training sessions to explain the\nregulations and their significance.\n•Build a culture of compliance within the organization—and yes, there must be\nsomeone who has the task of staying up to date on regulatory requirements that\nthe company may (or may not) need to be in compliance with. Most big compa‐\nnies have large compliance teams; even if you’re a small organization; however,\nit’s still important to designate someone to handle compliance, including moni‐\ntoring, checking for updates in regulations and standards, and more.\n•Cultivate a strong relationship between your compliance person/team and the\nlegal department so that when incidents occur, those teams are in lockstep with\neach other and are already used to working together.\nMonitoring compliance can be a manual process, with checklists and all, but that can\nalso make things even more complicated. There are automated compliance tools that\nyour organization can look at that provide compliance in real time, giving you con‐\ntinuous assurance and minimizing the chance that human error may lead to a gap in\ncompliance.\nProgram Performance Monitoring\nMonitoring and managing the performance of a governance program is integral to\ndemonstrating program success to business leadership. This type of monitoring\nallows you to track progress against the program’s aims and objectives, ensuring the\ngovernance program delivers the right outcomes for the organization, accounting for\nefficient and effective use of funding, and identifying improvement opportunities to\ncontinue creating impact for the business.\nIn order to monitor program performance, some of the items you might want to\nmeasure include:\n•Number of lines of business, functional areas, and project teams that have com‐\nmitted resources and sponsorship\n•Status of all issues that come into the governance function, how they were han‐\ndled, and what the resulting impact was\n•Level of engagement, participation, and influence the governance program is\nhaving across the organization, which will help people understand the value of\nthe governance program\nWhat Should You Monitor? | 183\n2Grosvenor Performance Group, “How Is Y our Program Going…Really? Performance Monitoring” , May 15,\n2018.•Value-added interactions, including training and project support, and what the\nimpact is to the business\n•Business value ROI from data governance investments, including reducing pen‐\nalties by ensuring regulatory compliance, reducing enterprise risk (e.g., contrac‐\ntual, legal, financial, brand), improving operational efficiencies, increasing top-\nline revenue growth, and optimizing customer experience and satisfaction\nProcess and tools for monitoring program performance\nProgram performance monitoring needs to be continuous and ongoing, helping you\nidentify areas that are not performing to expectations and determining what types of\nprogram adjustments are needed. Most performance management frameworks con‐\nsist of the sets of activities outlined in Figure 8-1 .2\nFigure 8-1. Performance management framework\nLet’s look more closely at each of these areas:\nAlignment with existing governance frameworks\nThis ensures that your program performance is aligned with the established gov‐\nernance framework. In Chapter 4 , we did a deep dive into governance frame‐\nworks, groups, and what is needed for effective governance.\nDeveloping performance indicators\nNow that you are aligned with existing governance frameworks, develop key per‐\nformance indicators (KPIs) for your governance program. These should be well\ndefined, relevant, and informative.\nReporting progress and performance\nDocumenting the governance performance objectives and how they’re being met\nand sharing this information with leadership will ensure that people see the value\nof the program. Providing reports becomes an essential way for people to con‐\nsume this information.\n184 | Chapter 8: Monitoring",6869
82-Security Monitoring.pdf,82-Security Monitoring,"3“29 Must-Know Cybersecurity Statistics for 2020” , Cyber Observer , December 27, 2019.\n4“5 Reasons Why Y ou Need 24x7 Cyber Security Monitoring” , Cipher  (blog), May 15, 2018.\n5Paul R. La Monica, “ After Equifax Apologizes, Stock Falls Another 15%” , CNNMoney , September 13, 2017.Taking action based on performance results\nIt’s important to identify ways to ensure that the performance results are used to\ninform decision making in your organization; otherwise, what is the point?\nAs you can see in Figure 8-1 , this is an ongoing and iterative process, with one area\nfeeding and informing the other.\nSecurity Monitoring\nCyberattacks are becoming bigger than ever before, with new threats from state\nactors and increasingly sophisticated methods. The damage related to cybercrime is\nprojected to hit $6 trillion annually by 2021, according to Cybersecurity Ventures.3 In\naddition, many countries are now taking consumer data privacy and protection more\nseriously by introducing new legislation to hold businesses accountable.\nAttacks cost more than money. They can affect a business’s brand and shareholder\nreputation. In the recent Equifax data breach, in which over 140 million records were\nexposed, the company most likely incurred a cost of more than $32 billion to resolve\nthe issue.4 This was reflected in their stock price, which fell more than 30% after the\nbreach. Perhaps even worse, adjacent firms in Equifax’s industry who did not get\nbreached felt a 9% stock drop, likely due to loss of confidence in security measures.5\nSo even if you’re doing everything right, you can still be impacted by a breach.\nThat’s why security monitoring is so important. It is the process of collecting and ana‐\nlyzing information to detect suspicious behavior, or unauthorized system changes on\nthe network in order to take action on alerts as needed. Most companies are routinely\nexposed to security threats of varying severity; the causes of security breaches include\nhackers and malware, careless employees, and vulnerable devices and operating sys‐\ntems. Security threats are part of the normal course of conducting business; therefore\nit’s important to be prepared and act on threats and breaches before they cause dam‐\nage and disruption.\nThere are many areas of the business where you can monitor security. Table 8-3  high‐\nlights some of those areas.\nWhat Should You Monitor? | 185\nTable 8-3. Security monitoring items, showing some areas of the organization that you can\nmonitor security on\nItem Description\nSecurity alerts and\nincidentsThese are any alerts or incidents generated from an IT environment. They could be data exfiltration  or\nunusual port activity, acceptable use policy violations, or privileged user-activity violations.\nNetwork events This involves the ability to monitor network activity and receive alarms or reports of the occurrence of\nselected events, including device statuses and their IP addresses, new device alerts, and network\nstatus.\nServer logs This involves monitoring and detecting server activities continuously, examining alerts before a server\nmishap occurs, recording server logs and reports for easy tracking of errors, performing log analysis,\nand monitoring server performance and capacity.\nApplication events This involves monitoring events surrounding your software and applications in their stack, ensuring\nthey are accessible and performing smoothly.\nServer patch\ncomplianceThis involves installing and patching all the servers in your IT environment and staying compliant. This\nhelps mitigate vulnerabilities, server downtimes and crashes, and slowing down.\nEndpoint events This is a list of all events that can be emitted by an instance of an application, a process, or an event.\nIdentity access\nmanagementThis involves defining  and managing the roles and access privileges of individual network users and\nmaintaining, modifying, and monitoring this access throughout each user’s access life cycle.\nData loss This involves detecting potential data breaches/data exfiltration  transmissions and employing\nmonitoring techniques and prevention when data is in use, in motion, and at rest.\nProcess and tools for monitoring security\nAn effective process of monitoring security is continuous security monitoring, pro‐\nviding real-time visibility into an organization’s security posture and constantly look‐\ning for cyber threats, security misconfigurations, and other vulnerabilities. This\nallows an organization to stay a step ahead of cyber threats, reducing the time it takes\nto respond to attacks while complying with industry and regulatory requirements.\nCyber security can be conducted at the network level or the endpoint level. With net‐\nwork security monitoring, tools aggregate and analyze security logs from different\nsources to detect any failures. Endpoint security technologies, on the other hand, pro‐\nvide security visibility at the host level, allowing for threats to be detected earlier in\nthe process flow.\nSecurity monitoring is an area with many players: from companies that offer solu‐\ntions you can implement within your organization to full-fledged companies that you\ncan outsource the entire service to. The solution you choose to go with depends on\nyour business, the size of your in-house team, your budget, the technologies at your\ndisposal, and the level of security-monitoring sophistication that you’re looking for.\nBoth options have pros and cons that need to be weighed before you can make an\neffective decision for your business.\n186 | Chapter 8: Monitoring",5609
83-What Is a Monitoring System.pdf,83-What Is a Monitoring System,,0
84-Analysis in Real Time.pdf,84-Analysis in Real Time,,0
85-System Alerts.pdf,85-System Alerts,,0
86-ReportingAnalytics.pdf,86-ReportingAnalytics,"Y ou should now have an understanding of what governance items to monitor, the\nhow, and the why. The next section will look more closely at monitoring systems,\ntheir features, and which criteria to monitor.\nWhat Is a Monitoring System?\nMonitoring systems are the core set of tools, technologies, and processes used to ana‐\nlyze operations and performance in order to alert, account, audit, and maintain the\ncompliance of organizational programs and resources. A robust monitoring system is\nparamount to the success of a program and needs to be optimal with regard to what\nthe business needs.\nMonitoring can be done in-house by purchasing a system and configuring it to your\nother internal systems; it can be outsourced as a managed service to other vendors\nbecause of an internal lack of expertise and expense; or if you’re using cloud solu‐\ntions, it can be embedded within your organization’s workflows. Open source tools\nalso provide some monitoring solutions that can be considered as well. Whatever\noption you choose to go with, here are common features of a good monitoring\nsystem.\nAnalysis in Real Time\nReal time is real money. In a world in which things change so fast and people need\ninformation at their fingertips, you must have a monitoring system that does analysis\nin real time. A good system should offer continuous monitoring with minimal delays,\nallowing you to make changes and improvements on the fly.\nSystem Alerts\nA monitoring system needs to have the ability to signal when something is happening\nso that it can be actioned. A system that allows multiple people to be informed with\nthe right information will go a long way toward ensuring that issues are addressed as\nquickly as possible. A system should allow configuration for multiple events and have\nthe ability to set different sets of actions depending on the alert. The alert should con‐\ntain information about what is wrong and where to find additional information.\nNotifications\nA good monitoring system needs to have a robust, built-in notification system. We’re\nno longer in the age of pagers, so your system needs to be able to send SMS, email,\nchat, and more to ensure that the message gets to the right folks. And once the mes‐\nsage is received, the right people need to be able to communicate back to the team\nthat the alert has been received and that the issue is being investigated—and when\nissues are resolved, communicate that the system or process is back to normal\nWhat Is a Monitoring System? | 187",2527
87-Graphic Visualization.pdf,87-Graphic Visualization,,0
88-Summary.pdf,88-Summary,"operations . Notifications could kick off additional processes automatically, where cer‐\ntain systems take an action.\nReporting/Analytics\nMonitoring systems are big data collection and aggregation units given all the alerts\nand trigger events collected over a period of time. Y our monitoring system needs to\nallow for robust reporting that will allow you to present the data to clients or different\ndepartments in the organization. Reporting allows you to identify trends, correlate\npatterns, and even predict future events.\nGraphic Visualization\nCollecting data should be augmented with the ability to analyze and visualize a situa‐\ntion. Dashboards play a critical role in ensuring that everyone has a place to look to\nsee how things are going, and even to observe trends over time. A visual representa‐\ntion of what’s happening is easier for people to understand and absorb and is one of\nthe top customer requests in a data governance solution. A good monitoring system\nshould have friendly and easy-to-understand graphs that allow the organization to\nmake decisions.\nCustomization\nIt’s no surprise that different organizations have different business requirements. Y ou\nneed to have the ability to be able to customize your monitoring system by function,\nuser type, permissions, and more, allowing you to have the right alerts triggered, and\nto have them actioned by the right folks.\nMonitoring systems need to run independently from production services, and they\nshould not put a burden on the systems they are tracking. This simply allows the\nmonitoring system to continue running in the case of a production outage or other\nfailure event. Otherwise, a failure could take down a monitoring system (when the\nproduction environment is down), and that defeats the purpose of its existence. In\naddition, as you would with any system, ensure you have a failover for your monitor‐\ning system in case it is the one that’s affected by a blackout or failure. Figure 8-2  high‐\nlights a simple monitoring system, given some of the features we’ve just highlighted.\n188 | Chapter 8: Monitoring\nFigure 8-2. Example monitoring system\nAs you can imagine, monitoring systems are becoming even more sophisticated with\nthe introduction of machine learning capabilities, so even though this list of features\nis robust, it’s not by any measure the be-all and end-all. Use this list as a starting point\nto select or build the right monitoring system for your organization, and, depending\non your use case and company needs, augment your system as needed.\nMonitoring Criteria\nNow that you have selected a monitoring system, what are the types of criteria that\nyou can set up to monitor? Monitoring systems collect data in two distinct ways: pas‐\nsive systems,  where the tools observe data created by the application and system under\nnormal conditions (i.e., log files, output messages, etc.); and active systems , which are\nmore proactive, use agents and other tools to capture data through a monitoring\nmodule, and are often integrated within production systems.\nHere’s some common criteria you can follow as you set up your monitoring system:\nBasic details\nIdentify basic details for the items you’re looking to monitor. Capture a list of\nrules, their purpose, and a distinct name for each one. If your system has prede‐\nfined alerts and queries, select the ones you want to monitor and label them\naccordingly so they can be easily identified.\nMonitoring Criteria | 189\nNotification  condition\nProgram your notification condition. Once the query/criteria you set is identified\nand the result is evaluated against configurable thresholds, an alert should be set\noff if there’s a violation on the criteria set forth. Users should be identified by\nemail or SMS, and the information should be made available on the monitoring\ndashboard. In addition, it’s not enough to simply send alerts; you need to ensure\nthat someone acknowledges that they’ve received the alert and that they are\nworking on it. Also, the alert needs to go to someone on call, and if that person,\nfor whatever reason, is not available, it needs to be routed to the right person who\ncan respond to the alert. If the alert is not responded to, the system should send\nanother notification within a specified amount of time as long as the metric con‐\ntinues to remain outside the threshold.\nMonitoring times\nIn this instance, specify the frequency of the running of a certain validation\n(daily/weekly/monthly), and then how long and how frequently it should occur\nwithin the day (business hours within the day and frequency within the day).\nThis is more of a gut check system that looks to make sure processes and systems\nare operating correctly; it is more useful for passive systems in which things are\nmonitored to ensure ongoing operations.\nGiven the complexity of all these items and the level of sophistication needed to\nmaintain them, monitoring systems are what make all this possible. The next section\noffers some reminders and touches on other things you should keep in mind.\nImportant Reminders for Monitoring\nFrom reading this chapter, you will have discerned some recurring themes when it\ncomes to monitoring. Here are some considerations to keep in mind:\nGetting started with a monitoring system\nJust like any software development process, monitoring can be done in-house by\npurchasing a system and configuring it to your systems; it can be outsourced as a\nmanaged service to other vendors because of an internal lack of expertise and\nexpense,; or if you’re using cloud solutions, it can be embedded within your\norganization’s workflows. Open source tools also provide some monitoring solu‐\ntions that can be considered as well.\nReal-time improves decision making\nFor the majority of the governance items outlined in the previous section, a con‐\ntinuous, real-time monitoring system is paramount to improving decision-\nmaking and staying on top of compliance. In addition, having an alert system\nthat is robust and allows people to take action as needed will ensure that things\nare remedied within the outlined SLAs.\n190 | Chapter 8: Monitoring\nData culture is key to success\nEmployee training needs to be embedded into the fiber of your business—your\ndata culture , as discussed in Chapter 9 . Many governance concerns are under‐\ntaken by employees who might not be aware that they’re doing something that\nwill compromise the business. Find ways to make education personal, because\npeople are most likely to apply education when they see it as important to their\nlives.\nSummary\nThis chapter was designed to give you a foundation on monitoring and how to think\nabout implementing it for your organization. Monitoring is vital to understanding\nhow your governance implementations are performing on both a day-to-day and a\nlonger-term basis. Monitoring is where the rubber meets the road, allowing you to\nask for additional resources, course-correct as needed, learn from the wins and fail‐\nures, and really showcase the impact of your governance program.\nTake an audit of your current governance and monitoring initiatives and augment\nthem as needed, because doing this can only reap more benefits for your\norganization.\nSummary | 191",7303
89-Chapter 9. Building a Culture of Data Privacy and Security.pdf,89-Chapter 9. Building a Culture of Data Privacy and Security,,0
90-Analytics and the Bottom Line.pdf,90-Analytics and the Bottom Line,"CHAPTER 9\nBuilding a Culture of Data Privacy\nand Security\nIn this book we have covered a lot: considerations in data governance, the people and\nprocesses involved, the data life cycle, tools, and beyond. These are all pieces of a puz‐\nzle that need to come together in the end for data governance to be successful.\nAs alluded to earlier, data governance is not just about the products, tools, and people\nwho carry out the process—there is also the need to build a data culture . The careful\ncreation and implementation of a data culture—especially one focused on privacy\nand security—not only aids in the establishment of a successful data governance pro‐\ngram but also ensures that the program is maintained over time.\nAs we discussed in Chapter 3 , the people and processes around data governance—in\nconjunction  with data governance tools—are essential components of a data gover‐\nnance program. In this chapter we will go one step further than thatby including the\ndata culture (the culture within a company around  data) as a key final component in\ncreating a highly successful data governance program.\nData Culture: What It Is and Why It’s Important\nThe data culture is the set of values, goals, attitudes, and practices around data, data\ncollection, and data handling within a company or organization. While many compa‐\nnies or organizations spend quite a bit of time vetting data governance tools and cre‐\nating processes, they often fail to set up a culture  within the company around data.\nThis culture defines and influences things like:\n193\n1Peter Bisson, Bryce Hall, Brian McCarthy, and Khaled Rifai, “Breaking Away: The Secrets to Scaling Analyt‐\nics”, McKinsey & Company, May 22, 2018.•How data is thought about within the company/organization (Is it an asset?\nNeeded to make decisions? The most important part of the company? Just some‐\nthing to be managed?)\n•How data should be collected and handled\n•Who should be handling data and when\n•Who is responsible for data during its life cycle\n•How much money and/or resources will be committed to serving the company/\norganization’s data goals\nWhile this is certainly not an exhaustive list, it begins to show you the vast amount of\nconsiderations that go into the definition of a data culture.\nHaving a set and defined data culture is important for a multitude of reasons, but the\nmost important is that it sets the stage and serves as the glue that holds everything\nelse together within a data governance program.\nWe will go into more detail in this chapter about what the “North Star” of a data cul‐\nture should look like and about what considerations you should be making when\ndesigning your own.\nStarting at the Top—Benefits  of Data Governance to the\nBusiness\nA key aspect of building a successful data culture is getting buy-in from the top down,\nwhich generally requires that decision makers within a company see how a data gov‐\nernance program will work and agree on why implementation of a data culture bene‐\nfits the company’s bottom line. An efficient data culture aids in ensuring reliable,\nhigh-quality data that not only produces better analytics but also reduces compliance\nviolations and penalties. All of this will result in better business performance, as high‐\nlighted in various studies including a 2018 McKinsey report that found that “break‐\naway companies” are twice as likely to claim they had a strong data governance\nstrategy.1 In many of our interviews with companies, one of the most common com‐\nplaints we hear about initiating and implementing a data governance program is get‐\nting buy-in from those that have the power to fund data governance initiatives, as\nwell as to support and enforce a data culture.\n194 | Chapter 9: Building a Culture of Data Privacy and Security",3820
91-A Data Culture Needs to Be Intentional.pdf,91-A Data Culture Needs to Be Intentional,"Often the investment in a data governance program and building a data culture (both\nin terms of the purchasing of tools and infrastructure and in terms of headcount) is\nseen as a cost with no ROI other than hoped-for assurance that a company won’t be\nhit with fines for being out of compliance with a regulation. A well-thought-out and\nwell-executed governance program can, however, provide the cost savings of proper\ndata handling, and it also has the ability to increase the value of already existing assets\n(“old data” that may be sitting around).\nWe have found and will discuss several areas that can be quite persuasive in helping\ndecision makers to see the value and importance of building a data culture.\nAnalytics and the Bottom Line\nWe’ve discussed in depth the implications of governance and how knowing what data\nthere is, and where it is, not only aids in locking down and handling sensitive data\nbut also helps analysts to run better, more actionable analytics. Better analytics—ana‐\nlytics that are based on higher-quality data or come from a synthesis of data from\nmultiple sources, for example—enable us to make better data-driven decisions. All\ncompanies are driven by a desire to increase their profitability, whether by increasing\nrevenue and/or by decreasing waste or expenditure. The entire push to data-driven\ndecision making has at its heart the hope and promise of driving revenue. When deci‐\nsion makers are able to see that a data governance program and data culture generate\nbetter analytics, which in turn positively affects the bottom line, they are not only a\nbit more likely to be passively “bought in, ” but they may also rise to be champions\nand drivers of the data culture.\nCompany Persona and Perception\nWhile not directly related to the bottom line, there is much to be said for the public\nperception of a company and how it handles data.\nIn the last five years, there have been several companies that have fallen under great\nscrutiny regarding the data they collect and how that data is used. The public’s per‐\nception that these companies have used data unethically carries a host of cascading\nnegative repercussions—ranging from decreased employee morale to financial effects\nsuch as dropped sponsors and/or losing customers to a competitor—and these do\nindeed affect a company’s bottom line.\nStarting at the Top—Benefits  of Data Governance to the Business | 195\nTop-Down Buy-In Success Story\nOne company we’ve interviewed has had a particularly successful execution of a data\ngovernance program, in large part due to the buy-in it received from the top of the\norganization. This company works in research and in healthcare and is looking to lev‐\nerage analytics across data collected on each side of the business. Currently its data\nresides in separate storage for each line of business. To marry this data together into a\ncentral repository, it recognized the need for a comprehensive data governance\nstrategy.\nWe hope we’ve impressed upon you in this book that all companies should have a data\ngovernance strategy and program, but of course there are certain business types that,\ndue to their high level of regulation, need to be even more conscious of their imple‐\nmentation of governance. Healthcare is one of those. This company knew that a com‐\nprehensive governance program would need to be broad and be executed at many\ndifferent levels within the organization. The company deals almost exclusively in sen‐\nsitive data, and for this data to be moved, manipulated, joined with other datasets,\nand so on, the governance on it had to be top-notch.\nTo achieve the level of buy-in that would be required, the company set out to create a\ncharter that outlined exactly what the governance program within the company\nshould look like: the framework/philosophy that should be followed, tools that would\nbe needed, the headcount needed to execute these tools, and notably, the establish‐\nment and ongoing reinforcement of a data culture. Is this exhaustive? Lofty? Idealis‐\ntic? Perhaps. But through this charter, the company was able to gain that buy-in from\nthe top, and it is now executing one of the most well-thought-out and structured gov‐\nernance programs we’ve seen (complete with headcount for governance-only related\ntasks—a true rarity).\nWe include this example not to imply that every single one of these “boxes” must be\nticked or that a smaller-scale or fledgling governance program is not worth it. On the\ncontrary: we hope that this example (while extreme) serves to show you just how\nmuch documenting a well-thought-out strategy, including how this strategy will be\nembedded into the culture of the company, can help you in getting not only initial\nbuy-in, but also the level of buy-in that will maintain and support your governance\nprogram in the long run.\nIntention, Training, and Communications\nPerhaps one of the most important aspects of building a data culture is the internal\ndata literacy, communications, and training. In the overview of this chapter, we men‐\ntioned that a key to successful governance and data culture is not just the establish‐\nment of a program but maintenance over time to ensure its longevity. Integral to this\n196 | Chapter 9: Building a Culture of Data Privacy and Security",5330
92-Training Who Needs to Know What.pdf,92-Training Who Needs to Know What,"end is intention, data literacy (a deep understanding of and ability to derive meaning‐\nful information from all kinds of data), training, and communications.\nA Data Culture Needs to Be Intentional\nJust as we discussed that a successful governance program needs to have a set process,\nthe building and maintenance of a data culture is very much the same.\nWhat’s important\nWhen creating and establishing a data culture, a company first needs to decide what’s\nimportant to it—what its tenets are. For example, a company that deals with a lot of\nsensitive data (like a healthcare company) may decide that the proper treatment and\nhandling of PII is a primary tenet, whereas a small gaming app company may decide\nthat ensuring data quality is its primary focus. Alternatively, a company may decide\nthat many tenets are important to it, which is fine—the key aspect here is that these\ntenets need to be clearly defined and collectively agreed upon as the rest of the estab‐\nlishment and maintenance of a data culture stems from this step.\nAside from internal tenets, there also exist tenets that are nonnegotiable in nature,\nsuch as legal requirements and compliance standards that all or most companies need\nto integrate as tenets, no matter what.\nIt’s also worth noting here that in addition to tenets and requirements/compliance\nstandards, we would argue that an important and perhaps even crucial tenet is that of\ncaring. This may seem like a strange tenet to have as part of a data governance pro‐\ngram, but it is an essential component of a data culture . For a governance program to\nfunction at its highest level, there must be an intrinsic desire on the part of the com‐\npany and its employees to do the right thing. The protection of and respect for data\nhas to be an integral part of the fabric of the company and its data-handling ethos. It\nmust be something that is touted and supported from the top so that it cascades down\nto the rest of the company. Without this concern, the company is one that merely\ndeals with data, and perhaps has a data governance program but has no data culture.\nWhile no company wants to rely solely on its employees to do the right thing, foster‐\ning a data culture helps to fill in the gaps for when things invariably go sideways.\nTraining: Who Needs to Know What\nA successful implementation of a data culture not only needs to have its tenets well\ndefined; it also needs to identify who will be carrying these out, how they will do this,\nand whether or not they have the knowledge and skills  needed for proper execution.\nIntention, Training, and Communications | 197\nThe “who,” the “how,” and the “knowledge”\nToo often we have seen that one or more of these three areas is glossed over or taken\nfor granted. As when we discussed the different “hats” involved in data governance,\nthere is a high likelihood that several of the folks carrying out these tasks have: little-\nto-no technical knowledge; some may be doing other tasks as part of their role, leav‐\ning little time to dedicate to data governance; and there are often breakdowns around\nwho is responsible for what tasks. Each of these components is important and should\nnot be overlooked. Each component needs to be thoroughly considered when plan‐\nning and implementing the data culture aspect of a data governance program.\nPart of the data governance and data culture execution strategy includes determining\nwho will do what. Not only is this important in terms of defining roles and responsi‐\nbilities, but it’s also important that the people who will be fulfilling these duties have\nthe skills and the knowledge to perform their tasks. It is simply not enough to enlist a\nperson to a task, give them a tool, and hope for the best. A plan for how knowledge\nand skills will be not only acquired but also maintained  over time is critical.\nA solid plan for what training will be required and how it will be disseminated is\nessential. Again, this is a place where we’ve seen companies falter—they may have\ndecided who does what and even provided some initial training to get people up to\nspeed, but they forget that as technology and the data collected grow and change, new\nskills and knowledge may be necessary. Training is often not a “one-and-done” event\n—it should be seen as ongoing.\nIn devising your own training strategy, you should think in terms of what’s initially\nneeded to teach your staff and what should continue to be reinforced and/or intro‐\nduced in subsequent training. For example, we have seen companies have success\nwith setting up events such as “Privacy Week” or “Data Week, ” in which essential\ntraining about proper data handling and governance considerations are reviewed and\nnew considerations and/or regulations are introduced. What makes these “events” so\nmuch more successful than simply providing required virtual click-through training\nis that you can center your event around a particular topic of recent importance (per‐\nhaps an internal issue that occurred, a new regulation that’s been launched, or even a\nhighly publicized issue from an external company). This event structure gives you\nsome freedom and flexibility around how to go about your training, depending on\nwhat’s most important to either reinforce or disseminate to your staff.\nAn example of what this type of event could look like is shown in Table 9-1 .\n198 | Chapter 9: Building a Culture of Data Privacy and Security\nTable 9-1. Sample schedule\nMonday “Basics of Proper Data Handling and Governance” (one hour)\nTuesday “How to Use Our Governance Tools 101” \nor “Advanced ‘How to’ on Our Governance Tools. Did You Know You Can Do…?!” (one hour)\nWednesday “Governance and Ethics: How and Why Governance Is Everyone’s Responsibility” (one hour)\nThursday “How Do I Handle That? A Guide On What to Do When You Encounter a Governance Concern” (one hour)\nFriday Guest speaker on an aspect of governance and/or data culture you find  particularly important to your\norganization (one hour)\nOf course you will have to evaluate what sort of training strategy works best for your\norganization, but the takeaway here should be that you need to have an ongoing  strat‐\negy—a strategy to address not only the initial training and expertise that your staff\nwill need, but also how you’re going to continue to reinforce past learning and intro‐\nduce new learning.\nCommunication\nAnother area we often see overlooked is an intentional plan around communication.\nAs mentioned with training, communication is also not a “one-and-done” activity. It\nis something that should not only be ongoing and consistent but also strategic and\nexhaustive. In fact, we would argue that proper communication and an intentional\nstrategy around it are what fuels a powerfully effective data culture.\nTop-down, bottom-up, and everything in between\nWhen thinking about communication in terms of a data governance program, there\nare multiple facets to consider. Two common facets of focus are top-down  communi‐\ncation of the governance program itself and its practices, standards, and expectations,\nand bottom-up  communication encompassing breaches and problems in governance\nbeing bubbled up for resolution.\nThese two facets, while clearly important, are only pieces  of the communication nec‐\nessary to foster a company-wide data culture. These facets are focused simply on the\npassage of governance information back and forth, not on how to develop and enrich\nthe culture  of data privacy and security. For data culture, communication needs to\ncenter around the tenets—what the culture for data is within the company, and how\neach of those responsible for its success matter and are a part of it.\nAdditionally, while not specifically training per se, communication that bolsters a\ncompany’s data culture can serve as a reminder and reinforcement not only of the\ninformation covered in said trainings but also of the company’s overall vision and\ncommitment to its data culture.\nIntention, Training, and Communications | 199",8087
93-Beyond Data Literacy.pdf,93-Beyond Data Literacy,,0
94-Requirements Regulations and Compliance.pdf,94-Requirements Regulations and Compliance,"Beyond Data Literacy\nIn the previous section we briefly discussed the value and impact of fostering caring\nwithin the data culture. Here we will explore that more deeply and expand on why\nthis is such a fundamental part of a successful data culture.\nMotivation and Its Cascading Effects\nTo be sure, education around what  data is (namely, what different kinds of data are),\nand how data should be treated is essential, but so too is the why. The question of why\ndata should be handled and treated with respect is an overlooked component of data\nculture. Of course it is part and parcel for several of the user hats in the governance\nspace (legal and privacy tsars, to name a few), but it should be so for other hats as\nwell.\nMotivation and adoption\nThis isn’t a text on psychology by any means; however, the power of motivation and\nits impact on people’s likeliness to do this over that influences greatly whether a data\nculture will be adopted or fall by the wayside.\nFor a data culture to be fully adopted, the motivation really needs to begin at the top\nand permeate down from there. Take, for example, a company that needs to put into\nplace a governance program to abide by new data compliance and security laws. Sev‐\neral people (or teams) within the company know that their data collection and han‐\ndling needs to meet higher standards to be in compliance and bring this need to the\nC-level. For a governance program to be fully implemented (as we discussed earlier in\nthis chapter), the C-level folks need to buy into the idea that a governance program is\nimportant and worth their support (including funding). Without C-level support for\na governance program, it’s highly likely that the program would fall short due to lack\nof budgetary support and the absence of an ongoing advocacy of a data culture.\nC-level involvement and belief in a governance program and implementation of a\ndata culture has many downstream effects.\nFirst, there is the financial aspect. Without proper funding, a company does not have\nthe resources in terms of both headcount and tools but also in terms of education of\nthe data governance space and how to use said tools effectively. Not only does this\ninfluence whether or not a governance program is executed effectively, but it also\naffects how well people are able to do their jobs and how happy they are doing those\njobs. As we discussed in Chapter 3 , people in the governance space wearing many dif‐\nferent hats and spreading their time across many different tasks—some of which\nthey’re ill-equipped to do—can lead to decreased job satisfaction and productivity\nand ultimately high job turnover (which has its own implications in terms of sunk\ncost and lost time).\n200 | Chapter 9: Building a Culture of Data Privacy and Security\nWhile C-level buy-in is clearly critical for its consequential effects, there is still the\nneed to cultivate motivation for best practices within the rest of the workforce. Here\nagain is where data culture becomes so important. First, the workforce needs to be\ninformed/educated on why proper data treatment and handling is not only necessary\nbut ethical. Of note here is that this is not where the reinforcement should stop. A\ndata culture continues to reinforce this value through the behavior of the company—\nthe training that is offered/required (and how often), the communications that are\nsent out, the behavior of decision makers/those in positions of influence, and finally,\neven the way the company is structured. A prime example of this would be a com‐\npany that touts a data culture of privacy and security and yet does not have dedicated\nteams and/or resources that support that culture. If there is a disconnect between\nwhat is internally marketed  as the data culture and what actually exists to support  that\nculture, motivation and adoption of the data culture is not only less likely—it’s\nimprobable at best.\nMaintaining Agility\nHopefully in the preceding sections we’ve driven home the need for a very thoughtful,\nthorough, and structured approach to creating and cultivating a data culture. An\nimportant aspect of this, one that should not be ignored in inception and creation of\na data culture, is how agility will be maintained. We have seen this be a highly prob‐\nlematic area for many companies and would be remiss if we didn’t address its impor‐\ntance, as it’s much easier to maintain agility than to create it after the fact.\nAgility and Its Benefits  on Ever-Changing Regulations\nDuring the course of our research we encountered a very interesting use case relating\nto the CCPA. While you may not be encountering compliance with this particular\nregulation, the struggles one company is facing may give you some food for thought.\nThis company is a large retailer that has many different ingestion streams, as it sells\nits products in many different locations across the United States. This company has\nalways struggled to keep track of not only the data that it collects itself, but also the\ndata it collects from third parties (e.g., a retailer sells its product directly to consumers\non its website but may also sell its product via other retailers, either in their stores or\nvia their websites).\nPart of compliance with CCPA is to track down PII data for a California resident\nregardless of where the item was purchased. For example, Ben lives in California but\npurchased this company’s product at a large retailer while he was on vacation in Flor‐\nida. The transaction itself occurred in Florida, but because Ben is a California resi‐\ndent, he can request that the product company find his purchase data and delete it.\nMaintaining Agility | 201",5719
95-Scaling the Governance Process Up and Down.pdf,95-Scaling the Governance Process Up and Down,"While many companies are now facing this conundrum, the company in this case\nstudy has a highly structured and inflexible governance strategy—one that does not\neasily lend itself to quickly adapting to these new requirements. As such, it recognizes\nthe need to revise its governance strategy, to allow the company to be more agile. The\nmain way it is spearheading this effort is to focus on building and nurturing a strong\ndata culture. With so many moving parts to its business (i.e., so much data coming\nfrom so many different places), and with the likelihood that more regulations like\nCCPA are on the horizon, this company feels that defining and building a culture that\nwill support (ever-changing) governance initiatives will be the key to its success. In\nthis way the company is creating (and has begun enacting) a comprehensive data\nculture.\nRequirements, Regulations, and Compliance\nThe most obvious reason for creating a culture around agility is the ever-changing\nlegal requirements and regulations that data must be in compliance with. To be sure,\nthe data regulations of today are likely to be less robust than the regulations of tomor‐\nrow. In Chapter 1  we discussed how the explosion of available data and its collection\nhas resulted in heaps and heaps of data just sitting there—much of it uncurated.\nIt is shortsighted and unfeasible to take an approach of simply “handling” the regula‐\ntion(s) of the present. The ability to pivot once regulations change or pop up (because\nthey will) is not just a necessity—it can be made much easier if steps are taken at the\noutset.\nThe Importance of Data Structure\nA key aspect to consider when cultivating agility is to “set yourself up for success”—\nagain, this is a component that can be relied upon and enforced by the data culture.\nNote that being “set up for success” is an intersection of how the data warehouse is\nstructured (the metadata that’s collected, the tags/labels/classification used, etc.) with\nthe culture that supports or enables the structure to run smoothly. These two in con‐\njunction help to make it much easier to pivot when new regulations arise.\nThis is an area in which we’ve seen many companies struggle. Take, for example, data\nthat is stored based on the application from which it comes—say, sales data from a\nspecific company’s retail stores across the US. In the company’s current analytics\nstructure, this data is tagged in a certain way—perhaps with metadata relating to the\nstore, purchase amount, time of purchase, and so on.\nNow let’s say that a new regulation comes along stating that any data from a customer\nwho lives in a certain state can be retained for only 15 days. How would this company\neasily find all the sales data from customers who live in a certain state? Note that the\ncriteria is not sales  with a certain state but customers , so a customer from state X\n202 | Chapter 9: Building a Culture of Data Privacy and Security",2969
96-Staying on Top of Regulations.pdf,96-Staying on Top of Regulations,"could make a purchase in state Y , and if state X is part of the regulation, then that\ncustomer’s data will need to be deleted after 15 days. The company in our example\nwill have a hard time complying quickly and easily with this regulation if it doesn’t\nhave the data structure set up to record the location of the purchaser (for the sake of\nsimplicity, we are assuming here that these are all credit card or mobile pay transac‐\ntions that can be traced to each purchaser’s state of residence).\nA process and culture from the outset to collect this kind of metadata (whether or not\nit is labeled or tagged initially) will make it that much easier to find this class of data\nand then tag/label it, and thus attach a retention policy.\nScaling the Governance Process Up and Down\nWhile we have touted the importance of not only the right tools and process but also\nthe right players (hopefully provided by the buy-in from C-level), there undeniably\nare times when things don’t go according to plan, and the process needs to scale (usu‐\nally down, although there is the use case that it could need to scale up).\nIt could be the case that a company loses headcount due to reorganization, restructur‐\ning, an acquisition, or even a change in data collection (in terms of actual data collec‐\nted and/or in the platform[s] used, tools for transformation, storage types and\nlocations, or analytics tools). Any of these will likely affect how a governance pro‐\ngram functions, and that program needs to be elastic to accommodate for such\nchanges. The data culture—agreed upon, supported, and reinforced—will aid in suc‐\ncessful elasticity.\nOne strategy we’ d like to mention here is one we touched on in Chapter 2 : making\nsure that the most critical data is tended to first (again, this should be a part of the\ndata culture). It’s impossible to predict what requirements and regulations may come\nin the future, but prioritizing data that is most critical to the business (as well as data\nthat has any relation back to a person or identifiable entity) will aid in your ability to\nscale to accommodate whatever changes may come. At the absolute minimum there\nshould be a process in place (for example, all types of critical data are always tagged\nor classified upon ingestion and/or are always stored in a certain location) that can\nquickly address and tend to this category of data.\nInterplay with Legal and Security\nIn this book we’ve discussed at length the different roles and/or “hats” involved in\ndata governance. When looking at the data culture specifically, two hats of note are\nlegal and security/privacy tsar, and the interplay between the two and their respective\nteams is important.\nInterplay with Legal and Security | 203",2759
97-Communication.pdf,97-Communication,,0
98-Agility Is Still Key.pdf,98-Agility Is Still Key,"Staying on Top of Regulations\nRegardless of how a company is organized in terms of roles and who does what, there\nmust be someone who has the task of staying up to date on regulatory requirements\nthat the company may (or may not) need to be in compliance with. As we’ve previ‐\nously discussed, sometimes this is done by an internal legal representative (attorney)\nand at other times this task falls on the privacy tsar or on someone else in security.\nWhat’s important about this task is that it needs to be done early and often, not only\nto ensure current compliance but also to help aid in the ability to ensure future  com‐\npliance. As we’ve seen over the past 10 years, data handling standards and regulations\nhave greatly changed—the ability to be aware of what these changes might be and\nhow best to set up a data culture to flex as needed and be in compliance is critical.\nAs we’ve discussed several times throughout the book, and especially in Chapter 8 ,\nhaving an auditing system in place will greatly aid you in monitoring your gover‐\nnance strategy over time and also facilitate the task of being on top of compliance and\nregulations, so that if (or when) you face an external audit, you will know that you are\nin compliance and that nothing unexpected will be found.\nCommunication\nStaying on top of regulations is, however, only half of the story. There must be a pro‐\ncess in place for how changes in regulations will be discovered and how these will be\ncommunicated to those who decide on how to proceed. In essence, there needs to be\nconstant communication back to the decision-making body about what regulations\nmight be coming up and/or whether there are any changes that need to be made to\ncurrent data handling practices in order to be in compliance.\nIt’s easy to see how this is a process that very much should be a part of the data cul‐\nture within a company.\nInterplay in Action\nAn excellent recent example of this process in action is the reaction to GDPR. All\ncompanies in the EU were well aware that this new regulation was coming and that\nchanges were needed in order to be compliant. The story was different for companies\nin the US, however. During many of our interviews with US companies about their\ndata governance practices and their plans for the future in terms of changing regula‐\ntions, we heard two different approaches: the first was to simply ignore new regula‐\ntions until they become a requirement (so in the case of GDPR, not to address\ncompliance until it is a requirement for US companies); the second was to assume\nthat the most restrictive regulations in the world could  become a requirement and\nthus work toward being in compliance now, even before it’s required.\n204 | Chapter 9: Building a Culture of Data Privacy and Security",2811
99-Incident Handling.pdf,99-Incident Handling,,0
100-Importance of Transparency.pdf,100-Importance of Transparency,"This takes a strong data culture that includes a robust interplay between the gathering\nof legal requirements, a group deciding which requirements the company will comply\nwith, and another group actually carrying out the work of ensuring that compliance\nis being met.\nAgility Is Still Key\nThis point really hearkens back to the previous section on agility. It is likely that new\nregulations will continue to emerge, and the flexibility of the data structure and sys‐\ntem that a company builds would do well to have the potential to be easily modified\nor to pivot when needed to accommodate such requirements.\nIncident Handling\nWe’ve talked at length about the importance of process and communication in a suc‐\ncessful data culture. One particular process we’ d like to spend time unpacking is that\nof incident handling—how are breaches in data governance handled, and who is held\nresponsible?\nWhen “Everyone” Is Responsible, No One Is Responsible\nDuring some of our initial research into how companies structure their data gover‐\nnance strategy, one of the questions we asked was, “Who, at the end of the day, is\nresponsible for improper governance? Whose job is on the line?” Surprisingly, many\ncompanies danced around this question and struggled to give us a direct answer of a\nparticular person and/or group who would be held accountable in the event of some‐\nthing going wrong.\nThis might seem like an unimportant part of the data culture, but it is actually quite\ncritical. Earlier in this chapter we spoke about the importance of fostering an envi‐\nronment of caring and responsibility (and indeed this is important), but it also needs\nto have a backbone—there must be a person(s) or group(s) who is culpable when\nthings go wrong. When this structure is lacking, governance becomes “everyone’s”\nand yet “no one’s” responsibility. The culture needs to support everyone doing their\npart in proper data handling and owning their portion of governance responsibilities,\nand it also needs to identify who is the go-to—the end of the line, so to speak, for\nspecific aspects of the governance strategy.\nFor example, as part of the data culture, it is on every data analyst in a company to\nknow what is PII data and whether or not it should be accessed, and/or for what pur‐\nposes it can be accessed. A good data culture will support these analysts with educa‐\ntion and reinforcements for proper data handling. In the event that an analyst\nmistakenly (or malevolently as the case may be) accesses and improperly uses PII\ndata, someone should be held accountable for that breach. It may be a data engineer\nIncident Handling | 205",2662
101-What It Means to Be Transparent.pdf,101-What It Means to Be Transparent,,0
102-Setting an Example.pdf,102-Setting an Example,"in charge of enforcing access controls, or even a privacy tsar whose responsibility it is\nto set up and manage access policies. In any event, there needs to be someone who\nhas, as part of their job, the responsibility for ensuring that things go right and\naccepting the consequences when things go wrong.\nWe may be overstating this, but implementing and enforcing responsibility  is key here.\nPeople should also be trained  in how to be responsible and what this looks like (and\nwhat it doesn’t look like), and this should also be outlined in specific roles from the\noutset. Literally listed within a role description’s key tasks should be what that role is\nspecifically responsible for in terms of data handling, and what the consequences are\nfor failure to carry out this task. Responsibility should be something that is defined\nand agreed upon, as well as trained and communicated on, so that people are taught\nhow to be  responsible and are also held responsible.\nImportance of Transparency\nTransparency is an oft-forgotten (or intentionally sidestepped) ingredient of gover‐\nnance that warrants a deeper dive into not only why it’s important, but also why you\nshould keep it in mind and most certainly address it as part of building your data\nculture.\nWhat It Means to Be Transparent\nTo be sure, many companies and organizations do not want to disclose everything\nabout the ins and outs of their data governance structure, which is to be expected and\nis not entirely problematic. There is value, however, in a certain amount of transpar‐\nency from an organization, in terms of what data it collects, how it uses the data (and\nwhat for), and what steps/measures it takes to protect data and ensure proper data\nhandling.\nBuilding Internal Trust\nEarlier we touched on the importance of trust in building a data culture, and that\ntrust goes both ways—bottom up and top down. A key aspect of building that trust\nand truly showing  employees that it is part of the company’s data culture is to have full\ntransparency: transparency not only in the items related to data noted above (what\ndata is collected, how it’s used, governance processes, etc.) but also in what the\nincident-handling strategy is. In the previous section we mentioned how important it\nis to define the specific person or group that is responsible when things go wrong,\nand just as there should be consequences for these folks, there should also be conse‐\nquences (albeit different ones) for improper handling by anyone who touches data.\nWhile it may make an organization feel uncomfortable to share so broadly about\nwhen something has gone wrong and how it was handled internally, doing so builds\n206 | Chapter 9: Building a Culture of Data Privacy and Security\nincredible trust within the company that what is being touted as the data culture is an\nintegral part of the entire  company culture.\nAnother strategy that can help you build internal trust is that of enabling two-way\ncommunication via a user forum, wherein users of data within your company are able\nto voice their concerns and needs. We discussed earlier in this chapter the importance\nof communication, but this is an additional facet that is not just informational (you\nget to hear from the people actually using the data about why it might be wrong or\nwhat could be better)—it also bolsters the data culture by making all in the organiza‐\ntion feel that they’re being heard and are pieces of the greater whole that keeps the\ngovernance program running smoothly.\nBuilding External Trust\nIn creating a solid and successful data culture, focusing on the internal—the company\nnuts and bolts—is obviously extremely important, but the data culture does not and\nshould not end there. The external perception and trust of a company/organization\nshould also be considered. Just as showing full transparency internally helps to build\nand reinforce the data culture, what is communicated externally about data collec‐\ntion, handling, and protection practices is also highly important.\nIn a sense, the customers of a company or organization are an additional extension of\nits “culture. ” Customers or consumers should also be considered when thinking about\nbuilding a data culture. It’s not just the actions and perceptions of your staff or\nemployees; it’s also the actions and perceptions of your consumers and/or customers,\nand it’s their actions (generally driven by trust) that dictate whether or not they inter‐\nact with your company or buy your product.\nProviding full transparency externally regarding what data is collected, how it’s used,\nhow it’s protected, and what has been done to mitigate wrongdoing is critical in\nbuilding trust in a company/organization. There are, to be sure, cases in which cus‐\ntomers/people have no choice but to choose one company or organization for a ser‐\nvice/purchase/etc., but in the event that there is choice, customers/people are much\nmore likely to choose a company that they trust  over one they don’t trust or are\nunsure of.\nThe data culture in essence should not be just an internally decided-upon practice but\none that also includes how the company or organization sits in the world—what it\nwants the world to know about how it handles data and its commitment to compli‐\nance and proper data handling.\nImportance of Transparency | 207",5380
103-Appendix A. Googles Internal Data Governance.pdf,103-Appendix A. Googles Internal Data Governance,"Setting an Example\nIt may sound lofty, but another aspect around the importance and maybe even the\npower  of transparency is that it can teach and/or inspire others to adopt similar data\ngovernance practices and data culture.\nIf every company or organization were to create, implement, and enforce the gover‐\nnance principles and practices we’ve laid out, not only would there be excellent gov‐\nernance and data culture within each organization, but there would also be a cross-\ncompany, cross-organizational, cross-product, cross-geographical data culture.\nSummary\nThroughout the course of this text you have learned about all the aspects and facets to\nbe considered when creating your own successful data governance program. We hope\nthat we have educated you not only on all of the facets and features of data itself, gov‐\nernance tools, and the people and processes to consider, but also on the importance\nof looking at your governance program in a much broader sense to include things\nsuch as long-term monitoring and creating a data culture to ensure governance\nsuccess.\nY ou should walk away from this book feeling that you know what makes up a gover‐\nnance program (some aspects that you may have already heard of, and hopefully\nsome others that you hadn’t considered before) and that you know how to put those\npieces together to create and maintain  a powerful, flexible, and enduring program\nthat not only meets but also exceeds regulation, compliance, and ethical and societal\nstandards.\n208 | Chapter 9: Building a Culture of Data Privacy and Security",1584
104-Googles Governance Process.pdf,104-Googles Governance Process,"APPENDIX A\nGoogle’s Internal Data Governance\nIn order to understand data governance, it is good to look at a practical example of a\ncompany with a deep investment in the topic. We (the authors) are all Google\nemployees, and we believe that in Google we have a great example of a deeply\ningrained process, and a fine example of tooling.\nThe Business Case for Google’s Data Governance\nGoogle has user privacy at the top of its priorities and has published strong privacy\nprinciples  that guide us throughout all product development cycles. These privacy\nprinciples include, as top priorities, respect for user’s privacy, being transparent about\ndata collection, and taking the utmost care with respecting the protection of users’\ndata, and they have ensured that good data governance is at the core of Google.\nBefore we dive into the specifics of data governance and management at Google, it is\ncrucial to understand the motivations behind Google’s data collection, and the use\ncase. This is a good approach for any undertaking. Google provides access to search\nresults and videos and presents advertisements alongside the search results. Google’s\nincome (while more diversified than a few years ago) is largely attributable to ads.\nGiven the importance of ads, a large portion of the effort in Google is focused on\nmaking ads relevant. Google does this by collecting data about end users, indexing\nthis data, and personalizing the ads served for each user.\nGoogle is transparent  about this information: when someone uses Google services—\nfor example, when they do a search on Google, get directions on Maps, or watch a\nvideo on Y ouTube—Google collects data to personalize these services. This can\ninclude highlighting videos the user has watched in the past, showing ads that are\nmore relevant to the user depending on their location or the websites they frequent,\nand updating the apps, browsers, and devices they use to access Google services. For\n209\n1Carrie Mihalcik, “Google to Spend $10 Billion on Offices, Data Centers in US This Y ear” , CNET , February 26,\n2020.\n2James Zetlen, “Google’s Datacenters on Punch Cards” , XKCD .example, the results and corresponding ads might be different when a search is made\nby a user on a mobile device and navigating in Google Maps versus a search made\nfrom a desktop using a web browser. The personal information associated with the\nuser’s Google account is available if the user is signed in. This can include the user’s\nname, birthday, gender, password, and phone number. Depending on the Google\nproperty they are using, this can also include emails they have written and received (if\nthey are using Gmail); photos and videos they have saved (if they are using Google\nPhotos); documents, spreadsheets, and slides they have created (if they are using\nGoogle Drive); comments they have made on Y ouTube; contacts they added in Goo‐\ngle Contacts; and/or events on their Google Calendar. It is necessary that all this user\ninformation is protected when using Google services.\nGoogle, therefore, provides each user with transparency and control of how their per‐\nsonal information is used. Users can find out how their ads are personalized by going\nto Google Ad Settings  and can control the personalization therein. They can also turn\noff personalization and even take out their data . They can review their activity within\nGoogle domains  and delete or control activity collection. This level of transparency\nand control is something that users expect in order to be comfortable with providing\na business with their personal information. Google maintains transparency around\nwhat data it collects  and how this information is used  to generate the revenues men‐\ntioned above. If you are collecting personal information and using it to personalize\nyour services, you should provide similar mechanisms for your customers to view,\ncontrol, and redact your use of their personal data.\nGiven all the information Google collects, it is not surprising that Google is publicly\ncommitted to protecting that information and ensuring privacy . Google is meticulous\nabout external certifications and accreditations  and provides tools for individual con‐\nsumers to control the data collected about them .\nThe Scale of Google’s Data Governance\nGoogle keeps some information about itself private—for example, how much data it\nactually collects and manages. Some public information provides a general sense,\nsuch as Google’s reported investment of $10 billion in offices and data centers in\n2020.1 A third-party attempt at estimating Google’s data storage capacity using public\nsources of information came up with 10EB ( exabytes ) of information.2\n210 | Appendix A: Google’s Internal Data Governance\n3Alon Halevy et al., “Goods: Organizing Google’s Dataset”  (presented at SIGMOD/PODS’16: International\nConference on Management of Data, San Francisco, CA, June 2016).Some further information about the Google data cataloging effort, its scale, and the\napproaches taken to organizing data is described in the Google “Goods” paper.3 This\npaper discusses the Goo gle Dataset Search (GOODS) approach, which does not rely\non stakeholder support but works in the background to gather metadata and index\nthat metadata. The resulting catalog can be leveraged to further annotate technical\nmetadata with business information.\nSo, with this significant amount and variety of information, much of it likely sensi‐\ntive, how does Google protect the data that it has collected and ensure that privacy\nwhile keeping the data usable?\nGoogle has published certain papers about the tools used, and we can discuss these.\nGoogle’s Governance Process\nThere are various privacy commitments Google needs to respect and comply with, in\nparticular around user data: regulations, privacy policies, external communications,\nand best practices. However, it is generally hard to:\n•Make nontrivial global statements (e.g., all data is deleted on time)\n•Answer specific questions (e.g., do you never record user location if setting X is\nswitched off?)\n•Make informed decisions (e.g., is it OK to add this googler to that group?)\n•Consistently assert rules or invariants at all times\nThe ideal state is one in which we have a comprehensive understanding of Google’s\ndata and production systems and automatically enforced data policies and obliga‐\ntions. In the ideal state:\nThe lives of Google employees are easier\n•Taking the privacy- and security-preserving path is easier than taking an insecure\npath.\n•Privacy bureaucracy is reduced via automation.\nGoogle can do more…\n•Developers use data without worrying about introducing security and privacy\nrisks.\n•Developers can make privacy a product feature.\n•Google can make data-supported external privacy statements with confidence.\nGoogle’s Internal Data Governance | 211\n…while ensuring security and privacy\n•Google can prevent privacy problems before they happen.\n•Data obligations (policies, contracts, best practices) are objective and enforceable.\nEvery single feature Google produces and releases undergoes scrutiny by specialized\nteams external to the core development team. These teams review the product from\nthe following aspects:\nPrivacy\n•The team looks carefully at any user data collected and reviews justification for\nthe collection of this data. This includes reviewing what data, if collected, is going\nto be visible to Google employees, and under what constraints. The team also\nensures that consent was provided for the data collected, and whether encryption\nis applied and audit logging is enabled for that data.\n•In addition, we look at compliance—we make sure that when users choose to\ndelete data, it is verifiably removed within Google’s committed SLAs, and that\nGoogle has monitoring on all data retained (so that we can ensure minimal\nretention is preserved).\n•By verifying compliance and policies before even starting the collection of data,\nwe limit a significant number of challenges ahead of time.\nSecurity\nThis is a technical review targeted at scrutinizing the design and architecture of\nthe code according to best practices to potentially head off future incidents\nresulting from security flaws. Since most capabilities Google launches are\nexposed to the web, and despite the fact that there are always multiple layers of\nsecurity, we always provide for an additional review. Web threats are present and\nevolving, and a review by subject matter experts is beneficial to all.\nLegal\nThis is a review from a corporate governance standpoint, making sure the prod‐\nuct or service launched is compliant with export regulations and explicitly getting\na review from a regulation perspective.\n(There are other approvers for a release, naturally, but we will focus on those related\nto data governance.)\nGoogle maintains additional certifications , with a common theme of most of those\nbeing third-party verified.\n212 | Appendix A: Google’s Internal Data Governance",9064
105-How Does Google Handle Data.pdf,105-How Does Google Handle Data,,0
106-Privacy SafeADH as a Case Study.pdf,106-Privacy SafeADH as a Case Study,"How Does Google Handle Data?\nMuch of the information Google holds goes into a central database, where it is held\nunder strict controls. We have already shared information about the likely contents of\nthis database; now let’s focus on the controls around this database.\nPrivacy Safe—ADH as a Case Study\nADH—or Ads Data Hub —is a tool Google provides that allows you to join data you\ncollect yourself (e.g., Google ad campaign events) with Google’s own data about the\nsame constituents. Y et it does so without breaching the privacy or trust of the individ‐\nuals inspected. The ways ADH accomplishes this are indicative of the care Google\ntakes with respect to data. There are several mechanisms working in conjunction to\nprovide multiple layers of protection:\nStatic checks\nADH looks for obvious breaches, such as listing out user IDs, and blocks certain\nanalytics functions that can potentially expose user IDs or distill a single user’s\ninformation.\nAggregations\nADH makes sure to respond only with aggregates, so that each row in the\nresponse to a query corresponds to multiple users, beyond a minimal threshold.\nThis prevents the identification of any individual user. For most queries, you can\nonly receive reporting data on 50 or more users. Filtered rows are those omitted\nfrom the results without notification.\nDifferential  requirements\nDifferential requirements compare results from the current query operation\nyou’re running to your previous results, as well as rows from the same result set.\nThis is designed to help prevent the user from gathering information about indi‐\nvidual users by comparing data from multiple sets of users that meet our aggre‐\ngation requirements. Differential requirement violations can be triggered by\nchanges to your underlying data between two jobs.\nGoogle’s Internal Data Governance | 213\n4Some of the these techniques are described in a paper Google published entitled “Differentially Private SQL\nwith Bounded User Contribution” .ADH Uses Differential  Privacy\nBusinesses that advertise on Google often want to measure how well their marketing\nis working. To do so, it is essential to be able to measure the performance of their ads.\nA local restaurant that has advertised on Google will want to know how many people\nwho were served a restaurant ad actually came to the restaurant. How can Google\nprovide advertisers the ability to do customized analysis that aligns with the business\n(e.g., how many patrons placed an online order after seeing the ad on Google), con‐\nsidering such analysis will require joining information on who the ads are served to\nwith the restaurant’s own customer transactions database?\nADH uses differential checks to enable customized analysis while respecting user pri‐\nvacy and upholding Google’s high standards of data security. Differential checks are\napplied to ensure that users can’t be identified through the comparison of multiple\nsufficiently aggregated results. When comparing a job’s results to previous results,\nADH needs to look for vulnerabilities on the level of individual users. Because of this,\neven results from different campaigns, or results that report the same number of\nusers, might have to be filtered if they have a large number of overlapping users. On\nthe other hand, two aggregated result sets may have the same number of users—\nappearing identical—but not share individual users and would therefore be privacy-\nsafe, in which case they wouldn’t be filtered. ADH uses data from historical results\nwhen considering the vulnerability of a new result. This means that running the same\nquery over and over again creates more data for differential checks to use when con‐\nsidering a new result’s vulnerability. Additionally, the underlying data can change,\nleading to privacy check violations on queries thought to be stable.\nThese techniques are sometimes referred to as differential  privacy .4\nThe ADH case study exemplifies Google’s cultural approach to handling data: beyond\nthe process part mentioned previously, Google has built a system that provides value\nwhile at the same time ensuring safeguards and prevention techniques that put user\nprivacy at the forefront. Google has captured some of the capabilities in a “differential\nprivacy library” .\nFor another case study, consider the capabilities of Gmail. Google has built tools to\nextract structured data from emails. These tools enable assistive experiences, such as\nreminding the user when a bill payment is due or answering queries about the depar‐\nture time of a booked flight. They can also be combined with other information to do\nseemingly magical things like proactively surfacing an emailed discount coupon\n214 | Appendix A: Google’s Internal Data Governance\nwhile the user is at that store. All of the above is accomplished by scanning the user’s\npersonal email while still maintaining that user’s privacy. Remember, Google person‐\nnel are not allowed to view any single email. This is presented in the paper “ Anatomy\nof a Privacy-Safe Large-Scale Information Extraction System over Email” . This capa‐\nbility to scan information and make it accessible to the information’s owner, while at\nthe same time maintaining privacy, is accomplished through the fact that most emails\nare business-to-consumer emails, and those emails from a single business to many\nconsumers share the same template. Y ou can, without human intervention, backtrack\ngroups of emails to the business, generate a template that is devoid of any potential\ninformation (differentiating between the boilerplate portions and the transient sec‐\ntion), and then build an extraction template. The paper goes into more detail.\nGoogle’s Internal Data Governance | 215",5772
107-Appendix B. Additional Resources.pdf,107-Appendix B. Additional Resources,"APPENDIX B\nAdditional Resources\nThe following are some of the works we consulted while writing this book. This is not\nintended to be a complete list of the resources we used, but we hope that some of you\nwill find this selection helpful as you learn more about data governance.\nChapter 4: Data Governance over a Data Life Cycke\n•Association Analytics. “How to Develop a Data Governance Policy” . September\n27, 2016.\n•Australian Catholic University. “Data and Information Governance Policy” .\nRevised January 1, 2018.\n•Michener, William K. “Ten Simple Rules for Creating a Good Data Management\nPlan” . PLOS Computational Biology  11, no. 10 (October 2015): e1004525.\n•Mohan, Sanjeev. “ Applying Effective Data Governance to Secure Y our Data Lake” .\nGartner, Inc. April 17, 2018.\n•Pratt, Mary K. “What Is a Data Governance Policy?” . TechTarget. Updated Febru‐\nary 2020.\n•Profisee Group, Inc. “Data Governance—What, Why, How, Who & 15 Best Prac‐\ntices” . April 12, 2019.\n•Smartsheet, Inc. “How to Create a Data Governance Plan to Gain Control of\nY our Data Assets” . Accessed February 26, 2021.\n•TechTarget. “What Is Data Life Cycle Management (DLM)?” . Updated August\n2010.\n•USGS. “Data Management Plans” . Accessed February 26, 2021.\n217\n•Watts, Stephen. “Data Lifecycle Management (DLM) Explained” . The Business of\nIT (blog). BMC. June 26, 2018.\n•Wikipedia. “Data governance” . Last modified February 4, 2021.\n•Wing, Jeannette M. “The Data Life Cycle” . Harvard Data Science Review  1, no. 1\n(Summer 2019).\nChapter 8: Monitoring\n•Alm, Jens, ed. Action for Good Governance in International Sport Organisations .\nCopenhagen: Play the Game/Danish Institute for Sports Studies . March 2013.\n•Cyborg Institute. “Infrastructure Monitoring for Everyone” . Accessed February\n26, 2020.\n•Ellingwood, Justin. “ An Introduction to Metrics, Monitoring, and Alerting” . Dig‐\nitalOcean. December 5, 2017.\n•Goldman, Todd. “LESSON—Data Quality Monitoring: The Basis for Ongoing\nInformation Quality Management” . Transforming Data with Intelligence. May 8,\n2007.\n•Grosvenor Performance Group. “How Is Y our Program Going…Really? Perfor‐\nmance Monitoring” . MAy 15, 2018.\n•Henderson, Liz. “35 Metrics Y ou Should Use to Monitor Data Governance” .\nDatafloq. October 28, 2015.\n•Karel, Rob. “Monitoring Data with Data Monitoring Tools | Informatica US” .\nInformatica  (blog). January 2, 2014.\n•Pandora FMS. “The Importance of Having a Good Monitoring System? Offer the\nBest Service for Y our Clients”  (blog post). September 19, 2017.\n•Redscan. “Cyber Security Monitoring” . Accessed February 26, 2021.\n•Wells, Charles. “Leveraging Monitoring Governance: How Service Providers Can\nBoost Operational Efficiency and Scalability… ” . CA Technologies. January 19,\n2018.\n•Wikipedia. “Data Lineage” . Last modified November 17, 2020.\n218 | Appendix B: Additional Resources",2899
108-Index.pdf,108-Index,"Index\nA\naccess control, 6-8\nevolution of methods, 75\ngranularity, 137\nidentity and access management, 52\nas policy, 41\nregulation around fine-grained access con‐\ntrol, 26\naccess logs, 18\naccess management, 158-165\nAccess Transparency, 165\nauthentication, 158\nauthorization, 159\ndata loss prevention, 161\ndifferential privacy, 164\nencryption, 162\nidentity and access management, 52\npolicies, 160\nuser authorization and, 54\nAccess Transparency, 165\naccountability, in Capital One data leak, 18-19\naccounting (monitoring), 177\nacquisitions, large companies and, 73\nactive monitoring systems, 189\naddresses, deduplication of, 125\nAds Data Hub (ADH), 213-215\nadvertising, Google and, 209\nagile data protection, 165-167\ndata lineage, 166\nevent threat detection, 167\nsecurity health analytics, 165\nagility, data culture and\ndata structure and, 202\nmaintaining agility, 201-203regulation and, 201\nrequirements, regulations, and compliance,\n202, 205\nscaling of governance, 203\nAI (artificial intelligence)\ncase study: water meters, 118-120\ndata quality in, 117-120\nalerting, 177\nAmazon\nbias in automated recruiting tool, 17\ndata-driven decision making, 14\nAmazon Web Services (AWS) Nitro Enclaves,\n149\nanalysis, real-time, 187\nanalytical data, life cycle of (see data life cycle)\nanalytics\nbusiness case for data culture, 195\nin monitoring system, 188\nancillaries, 58\nannotation of data, 123\nAPI log, 136\napprovers, defined, 7\nartificial intelligence (AI)\ncase study: water meters, 118-120\ndata quality in, 117-120\nattestations, in binary authorization, 157\nattestors, in binary authorization, 157\naudit logging, regulatory compliance and, 28\naudit trail, 24\nauditing (in monitoring), 177\nauditing (of system)\ndata lineage and, 141\nin framework, xv\nauthentication, 52, 158\n219\nAuthoritative Vessel Identification Service\n(AVIS), 19-22\nauthorization, 54, 159\nautomation\ndata protection, 96, 97\nlineage collection, 135\nAWS (Amazon Web Services) Nitro Enclaves,\n149\nB\nBenford's Law, 144\nbest practices\ndata breach readiness, 171\ndata deletion process, 170\ndata protection, 167-173\nelectronic medical devices and OS software\nupgrades, 170\nphysical security, 168\nportable device encryption/policy, 170\nseparated network designs for data protec‐\ntion, 168\nbig data analytics, 116\nbig data, defined, 29\nbinary authorization, 156\nbottom-up communication, 199\nbusiness analyst, 64, 81\nbusiness case for data governance, xii, 194-196\nanalytics and bottom line, 195\nbuilding, 106\ncompany persona/perception, 195\nfor Google's internal data governance, 209\nimportance of matching to data use, 121\ntop-down buy-in success story, 196\nbusiness value of data governance, 23-30\ndata governance considerations for organi‐\nzations, 28-30\nfostering innovation, 23\nregulatory compliance, 26-28\nrisk management, 25\ntension between data governance and\ndemocratizing data analysis, 24\nC\nC-suite, 64\nCalifornia Design Den, 15\nCapital One data leak case study, 18-19\ncaring, as essential component of data culture,\n197, 200-201\nCCPA (California Consumer Privacy Act), 152effect on data governance, 9\nuse case, 201\ncentral data dictionary, 66\nclassification (see data classification and orga‐\nnization)\ncloud\ndata governance advantages in public cloud,\n30-34\ndata protection in, 146-149\nmoving data to, 30\nmulti-tenancy, 147\nsecurity surface, 147\nvirtual machine security, 148\nvirtual private cloud service controls, 155\nCloud Data Loss Prevention (DLP), 161\ncloud-native companies, 67\nCMEK (Customer Managed Encryption Keys),\n163\nCoast Guard, US, 19-22\nCoast, Steve, 94\ncollection of data (see data collection)\ncompany persona, 195\ncompany, public perception of, 195\ncompleteness of data, 129\ncompliance\naudit logging, 28\nbusiness value of data governance, 26-28\nchallenges for companies, 76\ndata destruction and, 89\ndata governance considerations for organi‐\nzations, 29\ndata lineage and, 141\ndata retention/deletion, 27, 97\ngaming of metrics, 76\nmonitoring and, 177, 182\nregulation around fine-grained access con‐\ntrol, 26\nsensitive data classes, 28\nConfidential Compute, 148\ncontact tracing, 60\nContext-Aware Access, 161\ncosts\nof bad data, 116\nof cyberattacks, 185\nof security breaches, 145\nCOVID-19\nCalifornia's collection of data on, 116\nlocation history privacy issues, 59-61\nculture (see data culture)\n220 | Index\nCustomer Managed Encryption Keys (CMEK),\n163\ncustomer support specialists, 64\ncustomers, as extension of company culture,\n207\ncustomization, of monitoring system, 188\nD\ndata\nin business context, 75\ndecision-making and, 14-16\nenhancing trust in, 5\nevolution of access methods, 75\nuse case expansion, 14-16\ndata access management, xv\ndata accumulation, 29\ndata acquisition\ndefined, 87\nidentity and access management, 52\nworkflow management for, 52\ndata analysis\ndata quality in big data analytics, 116\ntension between data governance and\ndemocratizing data analysis, 24\ndata analyst, 63\ndata archiving\nautomated data protection plan, 97\nas data life cycle phase, 89\nin practice, 96\ndata assessment, 45\ndata breaches\nEquifax, 167\nhealthcare industry, 171-173\nphysical data breach in Texas, 169\nportable devices and, 170\nreadiness, 171\ndata capture, 87\ndata catalog, 24\ndata cataloging, xiv, 44\ndata change management, 141\ndata classes\nin enterprise dictionary, 38-40\npolicies and, 40-43\ndata classification and organization, 43\naccess control and, 6-8\nautomation of, 43\ndata classification and organization, 43\ndata protection and, 146\nin framework, xivdata collection\nadvances in methods of, 10\nincrease in types of data collected, 13\nin sports, 11-13\ndata completeness, 129\ndata corruption, 26\ndata creation\nas data life cycle phase, 87\ndefining type of data, 95\nin practice, 95\ndata culture\nanalytics and bottom line, 195\nbenefits of data governance to the business,\n194-196\nbuilding a, 193-208\ncaring and, 200-201\ncommunication planning, 199\ndata governance policy, 110\ndefinition/importance of, 193\nincident handling, 205\nas intentional, 197\ninterplay with legal and security, 203\nmaintaining agility, 201-203\nmonitoring and, 191\nmotivation and adoption, 200\nmotivation and its cascading effects, 200\nprivacy/security and, 83, 193-208\nstaying on top of regulations, 204\ntraining for, 197-199\ntransparency and, 206\ndata deduplication, 124\ndata deletion, 50-52\nbest practices, 170\nregulatory compliance and, 27\ndata democratization, 24\ndata destruction\ncompliance policy/timeline for, 97\nas data life cycle phase, 89\nin practice, 97\ndata discovery and assessment, xiv\ndata enablement, 8\ndata encryption (see encryption)\ndata enrichment, 65\ncloud dataset, 78\nmanual process of, 75\ndata entry, 87\ndata exfiltration, 153-158\nsecure code, 156\nvirtual private cloud service controls, 155\nIndex | 221\nzero-trust model, 157\ndata governance (generally)\nin action, 17-22\nadvances in data collection methods, 10\nbasics, 1-35\nbusiness benefits of robust governance, xvii\nbusiness case for (see business case for data\ngovernance)\nbusiness need for, xii\nbusiness value of, 23-30\nclassification/access control, 6-8\ncloud-based data storage advantages, 30-34\ndata enablement/security versus, 8\ndefined, xiii\nelements of, 2-8\nenhancing trust in data, 5\nethical concerns around use of data, 16\ngrowth of number of people working with\ndata, 10\ngrowth of size of data, 9\nholistic approach to, 4\nimproving data quality, 19-22\nincreased importance of, 9-17\ninternal data governance at Google, 209-215\nmanaging discoverability/security/account‐\nability, 18-19\nnew laws/regulations around treatment of\ndata, 16\noperationalizing (see operationalizing data\ngovernance)\npolicy (see data governance policy)\nprocess (see process of data governance)\nprogram performance monitoring, 183-185\npurpose of, 1\nscaling of process, 203\ntension between data governance and\ndemocratizing data analysis, 24\ntools (see tools of data governance)\ndata governance charter template, 101\ndata governance framework (see framework)\ndata governance policy\naccess management and, 160\nchanging regulatory environment, 109\ncomplexity and cost, 109\nconsiderations across a data life cycle,\n108-111\ndata destruction and, 89\ndefined, 101\ndeployment time, 108developing a policy, 103\nimportance of, 102\nlocation of data, 110\norganizational culture, 110\nroles and responsibilities, 105\nstep-by-step guidance, 106-108\nstructure of, 103-105\ndata handling, at Google, 213-215\ndata in flight, governance of, 133-142\ndata transformations, 133\nlineage, 134-140\npolicy management, simulation, monitor‐\ning, change management, 141\nsecurity, 151\ndata infrastructure, complexity of, 30\ndata life cycle, 85-111\napplying governance over, 92-100\ncase study: information governance, 99\ndata archiving phase, 89, 96\ndata creation phase, 87, 95\ndata destruction phase, 89, 97\ndata governance frameworks, 92-94\ndata governance in practice, 94-97\ndata governance policy considerations,\n108-111\ndata governance throughout, 85-111\ndata management plan, 90-92\ndata processing phase, 88, 95\ndata storage phase, 88, 96\ndata usage phase, 88, 96\ndefined, 85\nexample of how data moves through a data\nplatform, 97\nmanagement of, 90-92\noperationalizing data governance, 100\nphases of, 86-90\ndata lifecycle management (DLM), 90-92\ndata lineage, 134-140\nagile data protection and, 166\nauditing and compliance, 141\ncollecting, 135\ndata protection planning and, 144\ndefined, 46\nkey governance applications that rely on lin‐\neage, 139\nmonitoring, 180\npolicy management, simulation, monitor‐\ning, change management, 141\nas temporal state, 138\n222 | Index\ntracking of, 46, 128\ntypes of, 136\nusefulness of, 135\ndata locality, 31\ndata loss prevention, access management and,\n161\ndata maintenance (see data processing)\ndata management plan (DMP), 90-92\ndata outliers (see outliers)\ndata owner, 62, 80\ndata policies\ndata classes and, 40-43\nper-use case, 42\ndata processing\nas data life cycle phase, 88\ndocumenting data quality expectations, 95\nin practice, 95\ndata profiling, 45\ndata completeness, 129\ndata deduplication, 124\ndata quality and, 124-129\ndataset source quality ranking for conflict\nresolution, 129\nlineage tracking, 128\nmerging datasets, 129\noutliers, 127\ndata protection, 143-174\nas framework element, xv\nautomated, 96\nautomated data protection plan, 97\nbest practices, 167-173\nclassification and, 146\nin the cloud, 146-149\ndata breach readiness, 171\ndata deletion process, 170\ndata exfiltration, 153-158\nidentity/access management, 158-165\nkeeping data protection agile, 165-167\nlevel of protection, 145\nlineage and quality, 144\nnetwork security, 151\nphysical security, 149-152\nplanning, 143-146\nportable device encryption/policy, 170\nsecurity in transit, 151\ndata provenance (see data lineage)\ndata quality, 113-131\nAI/ML models and, 117-120\nannotation, 123big data analytics and, 116\ncascading problems with tribal knowledge,\n124\ndata protection planning and, 144\ndata quality management as part of frame‐\nwork, xv\ndefined, 113\ndocumenting data quality expectations, 95\nimportance of, 114-120\nimproving, 19-22\nmanagement processes in enterprise dictio‐\nnary, 46\nmatching business case to data use, 121\nmonitoring, 179\nprioritization, 123\nprofiling, 124-129\nreasons for including in data governance\nprogram, 121\nscorecard for data sources, 123\ntechniques for, 121-130\nunexpected sources for, 129\nat US Coast Guard, 19-22\ndata recovery, automated, 96\ndata retention, 50-52\ncase study, 51\nregulatory compliance and, 27\ndata scientist, 63\ndata security, 8\nCapital One data leak case study, 18-19\ncreating culture of, 83\ndata governance versus, 8\nhealthcare industry security breaches,\n171-173\nkey management and encryption, 47-49\nphysical, 149-152\nsecurity monitoring, 185\nZoom bombing, 152\ndata segregation\nownership by line of business, 80-82\nwithin storage systems, 77-79\ndata sensitivity, 135\ndata steward, 7, 63, 81\ndata storage\nautomated data protection and recovery, 96\nas data life cycle phase, 88\nin practice, 96\nkey management and encryption, 47-49\ndata structure, agility and, 202\ndata theft, managing risk of, 25\nIndex | 223\ndata transformations, 133\ndata usage\naccess management and, 96\nas data life cycle phase, 88\nin practice, 96\ndata validation, 133\ndata-driven decision making, 14\ndatasets\n""views"" of, 82\nmerging, 129\nsource quality ranking for conflict resolu‐\ntion, 129\ndebugging, 139\ndecision-making, data and, 14-16\ndeduplication, 124\ndeletion of data (see data deletion)\ndemocratization of data, 24\ndifferential privacy, 59, 164, 214\ndigital-only (cloud-native) companies, 67\ndirector of data governance (see privacy tsar)\ndiscoverability, 18-19\nDLM (data life cycle management), 90-92\nDLP (Cloud Data Loss Prevention), 161\nDMP (data management plan), 90-92\ndocumenting data quality expectations, 95\nE\neducation, when operationalizing data gover‐\nnance, 108\nelectronic medical devices, 170\nemployees (see people, data governance and)\nencryption, 151\naccess management and, 162\nkey management and, 47-49\nportable devices and, 170\nenterprise dictionary, 37\ndata assessment/profiling, 45\ndata cataloging/metadata management, 44\ndata classes and policies, 40-43\ndata classes in, 38-40\ndata classification and organization, 43\ndata quality management processes, 46\ndata retention/deletion, 50-52\nkey management and encryption, 47-49\nlineage tracking, 46\nworkflow management for data acquisition,\n52\nEquifax data breach, 167, 185\nethics, data use and, 16ETL (extract-transform-load), 116, 133\nEuropean Union (EU), 3\nAI regulations, 17\nSpotify regulations, 3\nevent threat detection, 167\nexfiltration (see data exfiltration)\nExperian, 116\nexternal auditor, 64\nexternal trust, 207\nextract-transform-load (ETL), 116, 133\nextraction, defined, 133\nF\nfilters, 121\nfine-grained access control, 26\nframework, xiv\naccountability, 107\nand data life cycle, 92-94\nG\nGDPR (General Data Protection Regulation),\nxi, 26\ndata retention rules, 51\ndata sovereignty measures, 31\neffect on data governance, 9\nright to be forgotten, 76\nUS companies and, 204\nGmail, 214\nGoogle\nAds Data Hub case study, 213-215\nbusiness case for internal data governance,\n209\ndata handling at, 32, 213-215\ngovernance process, 211-212\ninternal data governance at, 209-215\nprotocol buffers, 151\nscale of internal data governance, 210\nGoogle Cloud\nencrypted data in, 48\nShielded VM, 148\nGoogle Dataset Search (GOODS), 211\ngovernance manager (see privacy tsar)\ngovernance process (see process of data gover‐\nnance)\ngovernors, defined, 7\ngraphic visualization, in monitoring system,\n188\ngRPC framework, 151\n224 | Index\nguiding principles, developing/documenting,\n106\nH\n""hats""\nand company structure, 74\nHealth Insurance Portability and Accountabil‐\nity Act of 1996 (HIPAA), 16\nhealthcare industry\ncascading data quality problems with tribal\nknowledge, 124\ndata protection best practices, 167\nneed for comprehensive data governance,\n196\nsecurity breaches, 171-173\nHerzberg, Elaine, 16\nhighly regulated companies, 69-71\nI\nidentity and access management (IAM) sys‐\ntems, 52, 147\nidentity-aware proxy (IAP), 160\nIMPACT (teacher ranking system), 76\nincident handling, 205\ninfotypes, 37\ninheritance, 141\ninnovation, fostering with data governance, 23\ninternal data governance, at Google, 209-215\ninternal trust, building, 206\nJ\nJacob, Oren, 50\nK\nk-anonymity, 164\nkey management, 47-49\nkey management system (KMS), 163\nknowledge workers, 8\nL\nlabeling of resources in public cloud, 33\nlaptops, 170\nlarge companies, 72\nlegacy companies, 66\nlegal ""hat"", 58\nlegal issues (see regulations; compliance)\nlife cycle (see data life cycle)\nlineage (see data lineage)lineage graph, 136\nlineage tracking, 128\nin enterprise dictionary, 46\ntime/cost of, 47\nlocation of data, 110\nM\nmachine learning (ML)\ndata governance and, 4\ndata quality in, 117-120\nmanagement buy-in, 107\nmaritime domain awareness (MDA), 21\nMars Climate Orbiter, 100\nMaxMind, 115\nmedical records, 167\nmergers and acquisitions, large companies and,\n73\nmetadata\nanalytics in legacy companies and, 66\ndata enrichment and, 65\ndata lineage and, 135\nmetadata catalog, 24\nmetadata management\nin enterprise dictionary, 44\nas part of framework, xiv\nmetrics, gaming of, 76\nMicrosoft Azure, 148\nmisuse of data, 25\nML (machine learning)\ndata governance and, 4\ndata quality in, 117-120\nmonitoring, 175-191\ncompliance monitoring, 182\ncriteria for, 189\ndata lineage monitoring, 180\ndata quality monitoring, 179\ndefined, 175\nimportant reminders for, 190\nkey areas to monitor, 179-187\nprogram performance monitoring, 183-185\nreasons to perform, 176-178\nsecurity monitoring, 185\nsystem features, 187-189\nmonitoring system\nanalysis in real-time, 187\ncustomization in, 188\nfeatures of, 187-189\ngraphic visualization in, 188\nnotifications in, 187\nIndex | 225\nreporting/analytics in, 188\nsystem alerts, 187\nmulti-tenancy, 147, 153\nN\nnames, deduplication of, 125\nNASA, 100\nNational Football League (NFL), 11-13\nNational Health Service (NHS), 167\nnatural experiments, 99\nnetwork security, 151\nNewett, Edward, 3\nNext Gen Stats (NGS), 11\nNFL (National Football League), 11-13\nNHS (National Health Service), 167\nNike Zoom Vaporfly 4% ad campaign, 99\nNitro Enclaves, 149\nnormalization, 45, 134\nnotifications, in monitoring system, 187\nO\nontologies, 107\nOpenStreetMap (OSM), 94\noperating model, 107\noperationalizing data governance, xvi, 100\nconsiderations across a data life cycle,\n108-111\ndata governance policy defined, 101\ndeveloping a policy, 103\nimportance of a data governance policy, 102\npolicy structure, 103-105\nroles and responsibilities, 105\nstep-by-step guidance, 106-108\nOS software upgrades, 170\nOSM (OpenStreetMap), 94\noutliers, 45, 114, 127\nP\npassive monitoring systems, 189\npeople, data governance and, 57-65\nconsiderations and issues, 74-77\ncreation of ""views"" of datasets, 82\nculture of privacy/security, 83\ndata definition, 75\ndata enrichment, 65\ndata segregation and ownership by line of\nbusiness, 80-82data segregation within storage systems,\n77-79\nfor physical security, 149\ngrowth of number of people working with\ndata, 10\n""hats,"" defined, 58\n""hats"" versus ""roles"" and company structure,\n74\nlegal ""hat"", 58\npeople–data governance process synergy,\n73-84\nregulation compliance, 76\nroles, responsibilities, and ""hats"", 57\ntribal knowledge and subject matter exerts,\n74\nper-use case data policies, 42\nperimeter network security model, 151\npersonally identifiable information (PII) (see\nPII)\npersonnel (see people, data governance and)\nphysical security, 149-152\nbest practices, 168\nnetwork security, 151\nphysical data breach in Texas, 169\nsecurity in transit, 151\nPII (personally identifiable information)\ndata class hierarchy and, 39\ndata deletion and, 50\ndefined, 40\nlineage collection, 136, 139\npolicy book (see enterprise dictionary)\npolicy(ies) (see data governance policy)\nportable devices, 170\nprioritization of data, 123\nprivacy\ncreating culture of, 83\ndifferential privacy, 164\nprivacy tsar, 59\ncommunity mobility reports, 59\nexposure notifications, 60\nprocess of data governance, 65-73\ncloud-native companies and, 67-69\nat Google, 211-212\nhighly regulated companies, 69-71\nlarge companies, 72\nlegacy companies and, 66\nlegacy retail companies, 68\npeople and (see people, data governance\nand)\n226 | Index\nscaling of, 203\nsmall companies, 71\nprofiling (see data profiling)\nprogram performance monitoring, 183-185\nProPublica, 17\nprotocol buffers, 151\nprovenance (see data lineage)\npublic cloud\nCapital One data leak, 18\ndata governance in, 30-34\ndata locality and, 31\nephemeral computing, 32\nlabeled resources, 33\nreduced surface area, 32\nsecurity issues, 34\nas serverless and powerful, 32\npublic perception of company, 195, 207\npurging (see data destruction)\nQ\nquality (see data quality)\nquality ranking, 129\nR\nRACI matrix (responsible, accountable, consul‐\nted, and informed), 105\nregulations\nagility and compliance, 201, 205\naround treatment of data, 16\ncommunicating regulatory changes, 204\ncompliance (see compliance)\ndata destruction and, 89\ndata governance considerations for organi‐\nzations, 29\ndata governance policy regulatory environ‐\nment, 109\nhighly regulated companies, 69-71\nstaying on top of, 204\nregulatory compliance (see compliance)\nreporting, in monitoring system, 188\nresponsibility, incident handling and, 205\nretail companies\ncompliance use case, 201\ndata governance process for, 67-69\nlegacy companies, 68\nregulation issues, 202\nretention of data (see data retention)\nright to be forgotten, 76\nrisk management, 25S\nSafari Books Online, 14-15\nscaling of governance process, 203\nScience Applications International Corp (SAIC)\ndata breach, 169\nscorecard, for data sources, 123\nsecurity (see data protection; data security)\nsecurity breaches (see data breaches)\nsecurity monitoring, 185\nsecurity teams, 147\nself-driving cars, 16\nsensitive data classes, 28\nsensitivity of source data, 135\nShielded VM, 148\nsigners, in binary authorization, 157\nsmall companies, data governance in, 71\nSMEs (subject matter exerts), 74, 81\nSociety of Vertebrate Paleontology, 121\nsoftware upgrades, 170\nsports, advanced data collection in, 11-13\nSpotify, 2-4\nstorage systems, data segregation within, 77-79\nStrava, 99\nstreaming data (see data in flight, governance\nof)\nstreet addresses, deduplication of, 125\nsubject matter exerts (SMEs), 74, 81\nSusman, Gayln, 50\nsystem access logs, 18\nsystem alerts, 187\nT\ntagging, 33\ntaxonomies, developing, 107\nteacher rating systems, 76\ntechnology stack, 107\nTelco, 25\ntenets, of company, 197\ntest dataset, 117\ntext data filters, 121\ntheft (see data theft)\ntools of data governance, 37-55\ndata assessment/profiling, 45\ndata cataloging/metadata management, 44\ndata classes and policies, 40-43\ndata classes in, 38-40\ndata classification and organization, 43\ndata quality management processes, 46\ndata retention/deletion, 50-52\nIndex | 227\nenterprise dictionary, 37\nidentity and access management, 52\nkey management and encryption, 47-49\nlineage tracking, 46\nuser authorization and access management,\n54\nworkflow management for data acquisition,\n52\ntop-down communication, 199\nToy Story 2 (movie), 50\ntraining, 108\ntraining dataset, 117\ntransactional systems, 86\ntransformations (see data transformations)\ntransparency\nbuilding external trust, 207\nbuilding internal trust, 206\ndata culture and, 206\nmeaning of, 206\nsetting an example, 208\ntribal knowledge, 74, 124\nTRICARE data breach, 169\ntrust\nand purpose of data governance, 1\nenhancing trust in data, 5\nexternal, 207\ninternal, 206\nU\nuse cases, for dataexpansion in, 14-16\nimportance of use case and policy manage‐\nment, 43\nper-use case data policies, 42\nuser authentication, 52, 158\nuser forums, 207\nuser roles, company structure and, 74\nusers, defined, 7\nV\nvalidation dataset, 117\nvalidation of data, 133\nviews of datasets, 82\nvirtual machine (VM), 148\nvirtual private cloud service controls (VPC-\nSC), 155\nW\nWashington, DC, 76\nwater meter case study, 118-120\nworkflow management, for data acquisition, 52\nworkforce (see people, data governance and)\nZ\nzero-trust model, 157\nZippilli, Dom, 19\nZoom bombing, 152\n228 | Index",23711
109-Colophon.pdf,109-Colophon,"About the Authors\nEvren Eryurek, PhD , is the leader of the data analytics and data management portfo‐\nlio of Google Cloud, covering Streaming Analytics, Dataflow, Beam, Messaging\n(Pub/Sub & Confluent Kafka), Data Governance, Data Catalog & Discovery, and Data\nMarketplace as the director of product management.\nHe joined Google Cloud as the technical director in the CTO Office of Google Cloud,\nleading Google Cloud in its efforts toward Industrial Enterprise Solutions. Google\nCloud business established the CTO Office and is still building a team of the world’s\nforemost experts on cloud computing, analytics, AI, and machine learning to work\nwith global companies as trusted advisors and partners. Evren joined Google as the\nfirst external member to take a leadership role as a technical director within the CTO\nOffice of Google Cloud.\nPrior to joining Google, he was the SVP & software chief technology officer for GE\nHealthcare, a nearly $20 billion segment of GE. GE Healthcare is a global leader in\ndelivering clinical, business, and operational solutions, with its medical equipment,\ninformation technologies, and life science and service technologies covering settings\nfrom physician offices to integrated delivery networks.\nEvren began his GE career at GE Transportation, where he served as general manager\nof the software and solutions business. Evren was with Emerson Process Management\ngroup for over 11 years, where he held several leadership positions and was responsi‐\nble for developing new software-based growth technologies for process control sys‐\ntems and field devices, and coordinating cross-divisional product execution and\nimplementation.\nA graduate of the University of Tennessee, Evren holds master’s and doctorate\ndegrees in nuclear engineering. Evren holds over 60 US patents.\nUri Gilad  is leading data governance efforts for the data analytics within Google\nCloud. As part of his role, Uri is spearheading a cross-functional effort to create the\nrelevant controls, management tools, and policy workflows that enable a GCP cus‐\ntomer to apply data governance policies in a unified fashion wherever their data may\nbe in their GCP deployment.\nPrior to Google, Uri served as an executive in multiple data security companies, most\nrecently as the VP of product in MobileIron, a public zero trust/endpoint security\nplatform. Uri was an early employee and a manager in CheckPoint and Forescout,\ntwo well-known security brands. Uri holds an MS from Tel Aviv University and a BS\nfrom the Technion—Israel Institute of Technology.\nValliappa (Lak) Lakshmanan  is a tech lead for Big Data and Machine Learning Pro‐\nfessional Services on Google Cloud Platform. His mission is to democratize machine\nlearning so that it can be done by anyone anywhere using Google’s amazing infra‐\nstructure (i.e., without deep knowledge of statistics or programming or ownership of\nlots of hardware).\nAnita Kibunguchy-Grant  is a product marketing manager for Google Cloud, specifi‐\ncally focusing on BigQuery, Google’s data warehousing solution. She also led thought-\nleadership marketing content for Data Security & Governance at Google Cloud.\nBefore Google, she worked for VMware, where she managed awareness and go-to\nmarket programs for VMware’s core Hyper-Converged Infrastructure (HCI) product,\nvSAN.\nShe has an MBA from MIT Sloan School of Management and is passionate about\nhelping customers use technology to transform their businesses.\nJessi Ashdown  is a user experience researcher for Google Cloud specifically focused\non data governance. She conducts user studies with Google Cloud customers from all\nover the world and uses the findings and feedback from these studies to help inform\nand shape Google’s data governance products to best serve the users’ needs.\nPrior to joining Google, Jessi led the enterprise user experience research team at T-\nMobile, which was focused on bringing best-in-class user experiences to T-Mobile\nretail and customer care employees.\nA graduate of both the University of Washington and Iowa State University, Jessi\nholds a bachelor’s in psychology and a master’s in human computer interaction.\nColophon\nThe animal on the cover of Data Governance: The Definitive  Guide  is the Pakistan\ntawny owl ( Strix aluco biddulphi ). While tawny owls are common throughout Europe,\nAsia, and northern Africa, this subspecies is specifically found in the northern parts\nof Afghanistan, Pakistan, and India, as well as in Tajikistan and Kyrgyzstan. These\nowls prefer temperate deciduous forests, or mixed forests with some clearings. They\nmay also roost in shrub land, orchards, pastures, or urban parks with large trees—\nanywhere with enough foliage to keep them well hidden during the day.\nTawny owls tend to have medium-sized brown or brown-gray bodies with large\nround heads and deep black eyes. Pakistan tawny owls have a distinctive gray color‐\ning with a whitish base and strong herringbone pattern below the head and mantle.\nThey are also believed to be the largest subspecies, with wingspans around 11 to 13\ninches; females are often slightly larger than their male counterparts. These owls are\nstrictly nocturnal and are not often seen in daylight. They are carnivorous and hunt\nfor small mammals, rodents, reptiles, birds, insects, and fish from dusk to dawn. They\ndo not migrate and are considered mature at the end of their first year.\nTawny owls are monogamous and pair for life. Breeding season is from February to\nJuly, and they nest in tree holes, among rocks, or in the crevices of old buildings. The\nfemale incubates two to four eggs for a month. New hatchlings are altricial and can‐\nnot fend for themselves or leave the nest for the first 35 to 40 days; they are fed by the\nmother with food brought by the father. Juveniles stay with their parents for about\nthree months after fledging, at which point they may disperse after breeding to estab‐\nlish new territory within their local range. Pakistan tawny owls can be highly territo‐\nrial and defend an area of about one thousand square meters year-round.\nTawny owls are excellent hunters, and while their eyesight may not be much better\nthan a human’s, they have excellent directional hearing and can swivel their heads\nalmost 360 degrees when tracking prey. They typically live 4 years in the wild—the\noldest wild tawny owl ever recorded lived more than 21 years! Tawny owls are consid‐\nered a species of least concern by the IUCN. Many of the animals on O’Reilly covers\nare endangered; all of them are important to the world.\nThe cover illustration is by Karen Montgomery, based on a black and white engraving\nfrom Meyers Kleines Lexicon . The cover fonts are Gilroy Semibold and Guardian\nSans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Con‐\ndensed; and the code font is Dalton Maag’s Ubuntu Mono.\nThere’s much more  \nwhere this came from.\nExperience books, videos, live online  \ntraining courses, and more from O’Reilly  \nand our 200+ partners—all in one place.\nLearn more at oreilly.com/online-learning\n©2019 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175",7223
