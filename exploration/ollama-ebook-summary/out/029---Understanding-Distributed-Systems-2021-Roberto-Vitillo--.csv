filename,title,text,len
01-Coordination.pdf,01-Coordination,"Copyright\nUnderstanding Distributed Systems byRobertoVitillo\nCopyright©RobertoVitillo. Allrightsreserved.\nThebook’sdiagramshavebeencreatedwithExcalidraw.\nWhiletheauthorhasusedgoodfaitheffortstoensurethatthein-\nformation and instructions in this work are accurate, the author\ndisclaimsallresponsibilityforerrorsoromissions,includingwith-\noutlimitationresponsibilityfordamagesresultingfromtheuseof\nor reliance on this work. The use of the information and instruc-\ntionscontainedinthisworkisatyourownrisk. Ifanycodesam-\nplesorothertechnologythisworkcontainsordescribesissubject\ntoopensourcelicensesortheintellectualpropertyrightsofothers,\nit is your responsibility to ensure that your use thereof complies\nwithsuchlicensesand/orrights.\nAbout the author\nAuthorsgenerallywritethispageinthethirdpersonasifsomeone\nelseiswritingaboutthem. Iliketodothingsalittlebitdifferently.\nIhaveover10yearsofexperienceinthetechindustryasasoftware\nengineer,technicallead,andmanager.\nIn 2017, I joined Microsoft to work on an internal SaaS data plat-\nform. Sincethen,IhavehelpedlaunchtwopublicSaaSproducts,\nProductInsightsandPlayfab. ThedatapipelineIamresponsible\nforisoneofthelargestintheworld. Itprocessesmillionsofevents\npersecondfrombillionsofdevicesworldwide.\nBefore that, I worked at Mozilla, where I set the direction of the\ndataplatformfromitsveryearlydaysandbuiltalargepartofit,\nincludingtheteam.\nAfter getting my master’s degree in computer science, I worked\nonscientiﬁccomputingapplicationsattheBerkeleyLab. Thesoft-\nwareIcontributedisusedtothisdaybytheATLASexperimentat\ntheLargeHadronCollider.\nAcknowledgements\nWriting a book is an incredibly challenging but rewarding expe-\nrience. I wanted to share what I have learned about distributed\nsystemsforaverylongtime.\nI appreciate the colleagues who inspired and believed in me.\nThanks to Chiara Roda, Andrea Dotti, Paolo Calaﬁura, Vladan\nDjeric,MarkReid,PawelChodarcewicz,andNunoCerqueira.\nDoug Warren, Vamis Xhagjika, Gaurav Narula, Alessio Placitelli,\nKoﬁSarfo,StefaniaVitilloandAlbertoSottilewereallkindenough\ntoprovideinvaluablefeedback. Withoutthem,thebookwouldn’t\nbewhatitistoday.\nFinally,andaboveall,thankstomyfamily: RachellandLeonardo.\nYoualwaysbelievedinme. Thatmadeallthedifference.\nPreface\nAccording to Stack Overﬂow’s 2020 developer survey1, the best-\npaidengineeringrolesrequiredistributedsystemsexpertise. That\ncomes as no surprise as modern applications are distributed sys-\ntems.\nLearningtobuilddistributedsystemsishard,especiallyiftheyare\nlargescale. It’snotthatthereisalackofinformationoutthere. You\ncan ﬁnd academic papers, engineering blogs, and even books on\nthesubject. Theproblemisthattheavailableinformationisspread\noutallovertheplace,andifyouweretoputitonaspectrumfrom\ntheorytopractice,youwouldﬁndalotofmaterialatthetwoends,\nbutnotmuchinthemiddle.\nWhen I ﬁrst started learning about distributed systems, I spent\nhoursconnectingthemissingdotsbetweentheoryandpractice. I\nwaslookingforanaccessibleandpragmaticintroductiontoguide\nmethroughthemazeofinformationandsettingmeonthepathto\nbecomingapractitioner. Buttherewasnothinglikethatavailable.\nThat is why I decided to write a book to teach the fundamentals\nof distributed systems so that you don’t have to spend countless\nhoursscratchingyourheadtounderstandhoweverythingﬁtsto-\ngether. ThisistheguideIwishedexistedwhenIﬁrststartedout,\nandit’sbasedonmyexperiencebuildinglargedistributedsystems\nthatscaletomillionsofrequestspersecondandbillionsofdevices.\n1https://insights.stackoverflow.com/survey/2020#work-salary-by-develope\nr-type-united-states\nCONTENTS 10\nIplantoupdatethebookregularly,whichiswhyithasaversion\nnumber. You can subscribe to receive updates from the book’s\nlanding page2. As no book is ever perfect, I’m always happy to\nreceivefeedback. Soifyouﬁndanerror,haveanideaforimprove-\nment,orsimplywanttocommentonsomething,alwaysfeelfree\ntowriteme3.\n0.1 Who should read this book\nIf you develop the back-end of web or mobile applications (or\nwould like to!), this book is for you. When building distributed\nsystems, you need to be familiar with the network stack, data\nconsistencymodels,scalabilityandreliabilitypatterns,andmuch\nmore. Although you can build applications without knowing\nany of that, you will end up spending hours debugging and\nre-designing their architecture, learning lessons that you could\nhaveacquiredinamuchfasterandlesspainfulway. Evenifyou\nare an experienced engineer, this book will help you ﬁll gaps\nin your knowledge that will make you a better practitioner and\nsystemarchitect.\nThebookalsomakesforagreatstudycompanionforasystemde-\nsign interview if you want to land a job at a company that runs\nlarge-scale distributed systems, like Amazon, Google, Facebook,\norMicrosoft. Ifyouareinterviewingforaseniorrole,youareex-\npectedtobeabletodesigncomplexnetworkedservicesanddive\ndeepintoanyvertical. Youcanbeaworldchampionatbalancing\ntrees, but if you fail the design round, you are out. And if you\njustmeetthebar,don’tbesurprisedwhenyourofferiswellbelow\nwhatyouexpected,evenifyouacedeverythingelse.\n2https://understandingdistributed.systems/\n3roberto@understandingdistributed.systems\nChapter 1\nIntroduction\nAdistributedsystemisoneinwhichthefailureofacomputer\nyoudidn’tevenknowexistedcanrenderyourowncomputer\nunusable.\n–LeslieLamport\nLooselyspeaking,adistributedsystemiscomposedofnodesthat\ncooperatetoachievesometaskbyexchangingmessagesovercom-\nmunication links. A node can generically refer to a physical ma-\nchine(e.g.,aphone)orasoftwareprocess(e.g.,abrowser).\nWhydowebotherbuildingdistributedsystemsintheﬁrstplace?\nSome applications are inherently distributed. For example, the\nwebisadistributedsystemyouareveryfamiliarwith. Youaccess\nit with a browser, which runs on your phone, tablet, desktop, or\nXbox. Togetherwithotherbillionsofdevicesworldwide,itforms\nadistributedsystem.\nAnotherreasonforbuildingdistributedsystemsisthatsomeappli-\ncationsrequirehighavailabilityandneedtoberesilienttosingle-\nnodefailures. Dropboxreplicatesyourdataacrossmultiplenodes\nso thatthe lossofasingle node doesn’tcause allyourdata to be\nlost.\nCHAPTER1. INTRODUCTION 12\nSomeapplicationsneedtotackleworkloadsthatarejusttoobigto\nﬁtonasinglenode,nomatterhowpowerful. Forexample,Google\nreceiveshundredsofthousandsofsearchrequestspersecondfrom\nallovertheglobe. Thereisnowayasinglenodecouldhandlethat.\nAnd ﬁnally, some applications have performance requirements\nthat would be physically impossible to achieve with a single\nnode. NetﬂixcanseamlesslystreammoviestoyourTVwithhigh\nresolutionsbecauseithasadatacenterclosetoyou.\nThisbookwillguideyouthroughthefundamentalchallengesthat\nneed to be solved to design, build and operate distributed sys-\ntems: communication,coordination,scalability,resiliency,andop-\nerations.\n1.1 Communication\nTheﬁrstchallengecomesfromthefactthatnodesneedtocommu-\nnicateoverthenetworkwitheachother. Forexample,whenyour\nbrowser wants to load a website, it resolves the server’s address\nfromtheURLandsendsanHTTPrequesttoit. Inturn,theserver\nreturnsaresponsewiththecontentofthepagetotheclient.\nHowarerequestandresponsemessagesrepresentedonthewire?\nWhathappenswhenthereisatemporarynetworkoutage,orsome\nfaultynetworkswitchﬂipsafewbitsinthemessages? Howcan\nyouguaranteethatnointermediarycansnoopintothecommuni-\ncation?\nAlthoughitwouldbeconvenienttoassumethatsomenetworking\nlibrary is going to abstract all communication concerns away, in\npractice it’s not that simple because abstractions leak1, and you\nneedtounderstandhowthestackworkswhenthathappens.\n1https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractio\nns/",7705
02-Scalability.pdf,02-Scalability,"CHAPTER1. INTRODUCTION 13\n1.2 Coordination\nAnotherhardchallengeofbuildingdistributedsystemsiscoordi-\nnating nodes into a single coherent whole in the presence of fail-\nures. Afaultisacomponentthatstoppedworking,andasystemis\nfault-tolerantwhenitcancontinuetooperatedespiteoneormore\nfaults. The “two generals” problem is a famous thought experi-\nmentthatshowcaseswhythisisachallengingproblem.\nSupposetherearetwogenerals(nodes),eachcommandingitsown\narmy,thatneedtoagreeonatimetojointlyattackacity. Thereis\nsomedistancebetweenthearmies,andtheonlywaytocommuni-\ncate is by sending a messenger (messages). Unfortunately, these\nmessengerscanbecapturedbytheenemy(networkfailure).\nIsthereawayforthegeneralstoagreeonatime? Well,general1\ncouldsendamessagewithaproposedtimetogeneral2andwait\nfor a response. What if no response arrives, though? Was one\nof the messengers captured? Perhaps a messenger was injured,\nand it’s taking longer than expected to arrive at the destination?\nShouldthegeneralsendanothermessenger?\nYoucanseethatthisproblemismuchharderthanitoriginallyap-\npeared. Asitturnsout,nomatterhowmanymessengersaredis-\npatched, neither general can be completely certain that the other\narmywillattackthecityatthesametime. Althoughsendingmore\nmessengersincreasesthegeneral’sconﬁdence,itneverreachesab-\nsolutecertainty.\nBecause coordination is such a key topic, the second part of this\nbookisdedicatedtodistributedalgorithmsusedtoimplementco-\nordination.\n1.3 Scalability\nTheperformanceofadistributedsystemrepresentshowefﬁciently\nit handles load, and it’s generally measured with throughput and\nresponsetime . Throughputisthenumberofoperationsprocessed\nper second, and response time is the total time between a client\nCHAPTER1. INTRODUCTION 14\nrequestanditsresponse.\nLoadcanbemeasuredindifferentwayssinceit’sspeciﬁctothesys-\ntem’susecases. Forexample,numberofconcurrentusers,number\nofcommunicationlinks,orratioofwritestoreadsarealldifferent\nformsofload.\nAs the load increases, it will eventually reach the system’s capac-\nity—themaximumloadthesystemcanwithstand. Atthatpoint,\nthesystem’sperformanceeitherplateausorworsens,asshownin\nFigure1.1. Iftheloadonthesystemcontinuestogrow,itwilleven-\ntuallyhitapointwheremostoperationsfailortimeout.\nFigure 1.1: The system throughput on the y axis is the subset of\nclientrequests(xaxis)thatcanbehandledwithouterrorsandwith\nlowresponsetimes,alsoreferredtoasitsgoodput.\nThe capacity of a distributed system depends on its architecture\nandanintricatewebofphysicallimitationslikethenodes’memory\nsize and clock cycle, and the bandwidth and latency of network\nlinks.\nAquickandeasywaytoincreasethecapacityisbuyingmoreex-\npensive hardware with better performance, which is referred to",2760
03-Anatomy of a distributed system.pdf,03-Anatomy of a distributed system,"CHAPTER1. INTRODUCTION 15\nasscaling up. But thatwill hita brick wallsooner orlater. When\nthatoptionisnolongeravailable, thealternativeis scalingout by\naddingmoremachinestothesystem.\nInthebook’sthirdpart,wewillexplorethemainarchitecturalpat-\nterns that you can leverage to scale out applications: functional\ndecomposition,duplication,andpartitioning.\n1.4 Resiliency\nAdistributedsystemisresilientwhenitcancontinuetodoitsjob\nevenwhenfailureshappen. Andatscale,anyfailurethatcanhap-\npen will eventually occur. Every component of a system has a\nprobabilityoffailing—nodescancrash,networklinkscanbesev-\nered,etc. Nomatterhowsmallthatprobabilityis,themorecom-\nponents there are, and the more operations the system performs,\nthe higher the absolute number of failures becomes. And it gets\nworse,sincefailurestypicallyarenotindependent,thefailureofa\ncomponentcanincreasetheprobabilitythatanotheronewillfail.\nFailuresthatareleftuncheckedcanimpactthesystem’s availability,\nwhich isdeﬁned asthe amount oftimethe application can serve\nrequestsdividedbythedurationoftheperiodmeasured. Inother\nwords,it’sthepercentageoftimethesystemiscapableofservicing\nrequestsanddoingusefulwork.\nAvailabilityisoftendescribedwithnines, ashorthandwayofex-\npressingpercentagesofavailability. Threeninesaretypicallycon-\nsidered acceptable, and anything above four is considered to be\nhighlyavailable.\nAvailability% Downtimeperday\n90%(“onenine”) 2.40hours\n99%(“twonines”) 14.40minutes\n99.9%(“threenines”) 1.44minutes\n99.99%(“fournines”) 8.64seconds\nCHAPTER1. INTRODUCTION 16\nAvailability% Downtimeperday\n99.999%(“ﬁvenines”) 864milliseconds\nIfthesystem isn’tresilienttofailures, which onlyincreaseasthe\napplicationscalesouttohandlemoreload,itsavailabilitywillin-\nevitablydrop. Becauseofthat, adistributedsystemneedstoem-\nbracefailureandworkarounditusingtechniquessuchasredun-\ndancyandself-healingmechanisms.\nAsanengineer,youneedtobeparanoidandassesstheriskthata\ncomponentcanfailbyconsideringthelikelihoodofithappening\nanditsresultingimpactwhenitdoes. Iftheriskishigh,youwill\nneedtomitigateit. Part4ofthebookisdedicatedtofaulttolerance\nanditintroducesvariousresiliencypatterns,suchasratelimiting\nandcircuitbreakers.\n1.5 Operations\nDistributedsystemsneedtobetested,deployed,andmaintained.\nItusedtobethatoneteamdevelopedanapplication,andanother\nwasresponsibleforoperatingit. TheriseofmicroservicesandDe-\nvOps has changed that. The same team that designs a system is\nalsoresponsibleforitslive-siteoperation. That’sagoodthingas\nthereisnobetterwaytoﬁndoutwhereasystemfallsshortthan\nexperiencingitbybeingon-callforit.\nNew deployments need to be rolled out continuously in a safe\nmanner without affecting the system’s availability. The system\nneedstobeobservablesothatit’seasytounderstandwhat’shap-\npeningatanytime. Alertsneedtoﬁrewhenitsservicelevelobjec-\ntivesareatriskofbeingbreached,andahumanneedstobelooped\nin. Thebook’sﬁnalpartexploresbestpracticestotestandoperate\ndistributedsystems.\nCHAPTER1. INTRODUCTION 17\n1.6 Anatomy of a distributed system\nDistributed systems come in all shapes and sizes. The book an-\nchorsthediscussiontothebackendofsystemscomposedofcom-\nmoditymachinesthatworkinunisontoimplementabusinessfea-\nture. Thiscomprisesthemajorityoflargescalesystemsbeingbuilt\ntoday.\nBeforewecanstarttacklingthefundamentals,weneedtodiscuss\nthe different ways a distributed system can be decomposed into\nparts and relationships, or in other words, its architecture. The\narchitecturediffersdependingontheangleyoulookatit.\nPhysically, a distributed system is an ensemble of physical ma-\nchinesthatcommunicateovernetworklinks.\nAt run-time, a distributed system is composed of software pro-\ncesses that communicate via inter-process communication (IPC)\nmechanismslikeHTTP,andarehostedonmachines.\nFromanimplementationperspective,adistributedsystemisaset\nof loosely-coupled components that can be deployed and scaled\nindependentlycalledservices.\nAserviceimplements one speciﬁc part of the overall system’s ca-\npabilities. Atthecoreofitsimplementationisthebusinesslogic,\nwhich exposes interfaces used to communicate with the outside\nworld. Byinterface,Imeanthekindofferedbyyourlanguageof\nchoice,likeJavaorC#. An“inbound”interfacedeﬁnestheopera-\ntionsthataserviceofferstoitsclients. Incontrast,an“outbound”\ninterfacedeﬁnesoperationsthattheserviceusestocommunicate\nwithexternalservices,likedatastores,messagingservices,andso\non.\nRemote clients can’t just invoke an interface, which is why\nadapters2are required to hook up IPC mechanisms with the\nservice’s interfaces. An inbound adapter is part of the service’s\nApplication Programming Interface (API); it handles the requests\nreceived from an IPC mechanism, like HTTP, by invoking oper-\n2http://wiki.c2.com/?PortsAndAdaptersArchitecture\nCHAPTER1. INTRODUCTION 18\nations deﬁned in the inbound interfaces. In contrast, outbound\nadapters implement the service’s outbound interfaces, granting\nthebusinesslogicaccesstoexternalservices,likedatastores. This\nisillustratedinFigure 1.2.\nFigure1.2: Thebusinesslogicusesthemessaginginterfaceimple-\nmented by the Kafka producer to send messages and the reposi-\ntoryinterfacetoaccesstheSQLstore. Incontrast, theHTTPcon-\ntrollerhandlesincomingrequestsusingtheserviceinterface.\nAprocessrunningaserviceisreferredtoasa server,whileaprocess\nthatsendsrequeststoaserverisreferredtoasa client. Sometimes,\naprocessisbothaclientandaserver,sincethetwoaren’tmutually\nexclusive.\nFor simplicity, I will assume that an individual instance of a ser-\nvicerunsentirelywithintheboundariesofasingleserverprocess.\nSimilarly,Iassumethataprocesshasasinglethread. Thisallows\nme to neglect some implementation details that only complicate\nourdiscussionwithoutaddingmuchvalue.\nIn the rest of the book, I will switch between the different archi-\nCHAPTER1. INTRODUCTION 19\ntectural points of view (see Figure 1.3), depending on which one\nismoreappropriatetodiscussaparticulartopic. Rememberthat\ntheyarejustdifferentwaystolookatthesamesystem.\nFigure1.3: Thedifferentarchitecturalpointsofviewusedinthis\nbook.",6138
04-I Communication.pdf,04-I Communication,"Part I\nCommunication\nIntroduction\nCommunication between processes over the network, or inter-\nprocesscommunication (IPC),isattheheartofdistributedsystems.\nNetwork protocols are arranged in a stack3, where each layer\nbuildsontheabstractionprovidedbythelayerbelow,andlower\nlayers are closer to the hardware. When a process sends data to\nanotherthroughthenetwork,itmovesthroughthestackfromthe\ntop layer to the bottom one and vice-versa on the other end, as\nshowninFigure 1.4.\nFigure1.4: Internetprotocolsuite\nThelink layerconsists of network protocols that operate on local\nnetworklinks,likeEthernetorWi-Fi,andprovidesaninterfaceto\ntheunderlyingnetworkhardware. Switchesoperateatthislayer\nandforwardEthernetpacketsbasedontheirdestinationMACad-\ndress.\nTheinternetlayer usesaddressestoroutepacketsfromonemachine\n3https://en.wikipedia.org/wiki/Internet_protocol_suite\n22\ntoanotheracrossthenetwork. TheInternetProtocol(IP)isthecore\nprotocolofthislayer,whichdeliverspacketsonabest-effortbasis.\nRoutersoperateatthislayerandforwardIPpacketsbasedontheir\ndestinationIPaddress.\nThetransport layer transmits data between two processes using\nport numbers to address the processes on either end. The most\nimportant protocol in this layer is the Transmission Control\nProtocol(TCP).\nTheapplication layer deﬁnes high-level communication protocols,\nlikeHTTPorDNS.Typicallyyourcodewilltargetthislevelofab-\nstraction.\nEven though each protocol builds up on top of the other, some-\ntimestheabstractionsleak. Ifyoudon’tknowhowthebottomlay-\ners work, you will have a hard time troubleshooting networking\nissuesthatwillinevitablyarise.\nChapter2describeshowtobuildareliablecommunicationchan-\nnel(TCP)ontopofanunreliableone(IP),whichcandrop,dupli-\ncateanddeliverdataoutoforder. Buildingreliableabstractionson\ntopofunreliableonesisacommonpatternthatwewillencounter\nmanytimesasweexplorefurtherhowdistributedsystemswork.\nChapter3describeshowtobuildasecurechannel(TLS)ontopof\na reliable one (TCP), which provides encryption, authentication,\nandintegrity.\nChapter4dives into how the phone book of the Internet (DNS)\nworks,whichallowsnodestodiscoverothersusingnames. Atits\nheart,DNSisadistributed,hierarchical,andeventuallyconsistent\nkey-valuestore. Bystudyingit,wewillgetaﬁrsttasteofeventu-\nallyconsistency.\nChapter5concludes this part by discussing how services can ex-\npose APIs that other nodes can use to send commands or notiﬁ-\ncationsto. Speciﬁcally,wewilldiveintotheimplementationofa\nRESTfulHTTPAPI.",2524
05-Reliable links.pdf,05-Reliable links,,0
06-Flow control.pdf,06-Flow control,"Chapter 2\nReliable links\nTCP1isatransport-layerprotocolthatexposesareliablecommuni-\ncationchannelbetweentwoprocessesontopofIP.TCPguarantees\nthatastreamofbytesarrivesinorder,withoutanygaps,duplica-\ntionorcorruption. TCPalsoimplementsasetofstabilitypatterns\ntoavoidoverwhelmingthenetworkorthereceiver.\n2.1 Reliability\nTo create the illusion of a reliable channel, TCP partitions a byte\nstream into discrete packets called segments. The segments are\nsequentiallynumbered, whichallowsthereceivertodetectholes\nand duplicates. Every segment sent needs to be acknowledged\nby the receiver. When that doesn’t happen, a timer ﬁres on the\nsendingside,andthesegmentisretransmitted. Toensurethatthe\ndatahasn’tbeencorruptedintransit,thereceiverusesachecksum\ntoverifytheintegrityofadeliveredsegment.\n1https://tools.ietf.org/html/rfc793\nCHAPTER2. RELIABLELINKS 24\n2.2 Connection lifecycle\nAconnectionneedstobeopenedbeforeanydatacanbetransmit-\ntedonaTCPchannel. Thestateoftheconnectionismanagedby\nthe operating system on both ends through a socket. The socket\nkeeps track of the state changes of the connection during its life-\ntime. Atahighlevel,therearethreestatestheconnectioncanbe\nin:\n•Theopeningstate,inwhichtheconnectionisbeingcreated.\n•The established state, in which the connection is open and\ndataisbeingtransferred.\n•Theclosingstate,inwhichtheconnectionisbeingclosed.\nThisisasimpliﬁcation,though,astherearemorestates2thanthe\nthreeabove.\nAservermustbelisteningforconnectionrequestsfromclientsbe-\nforeaconnectionisestablished. TCPusesathree-wayhandshake\ntocreateanewconnection,asshowninFigure 2.1:\n1.Thesenderpicksarandomsequencenumber xandsendsa\nSYNsegmenttothereceiver.\n2.Thereceiverincrements x,choosesarandomsequencenum-\nberyandsendsbackaSYN/ACKsegment.\n3.The sender increments both sequence numbers and replies\nwithanACKsegmentandtheﬁrstbytesofapplicationdata.\nThe sequence numbers are used by TCP to ensure the data is de-\nliveredinorderandwithoutholes.\nThe handshake introduces a full round-trip in which no applica-\ntiondataissent. Untiltheconnectionhasbeenopened, itsband-\nwidth is essentially zero. The lower the round trip time is, the\nfaster the connection can be established. Putting servers closer\ntotheclientsandreusingconnectionshelpsreducethiscold-start\npenalty.\nAfter data transmission is complete, the connection needs to be\n2https://en.wikipedia.org/wiki/Transmission_Control_Protocol#/media/Fi\nle:Tcp_state_diagram_fixed_new.svg\nCHAPTER2. RELIABLELINKS 25\nFigure2.1: Three-wayhandshake\nclosed to release all resources on both ends. This termination\nphaseinvolvesmultipleround-trips.\n2.3 Flow control\nFlowcontrolisabackoffmechanismimplementedtopreventthe\nsenderfromoverwhelmingthereceiver. Thereceiverstoresincom-\ning TCP segments waiting to be processed by the process into a\nreceivebuffer,asshowninFigure 2.2.\nThereceiveralsocommunicatesbacktothesenderthesizeofthe\nbuffer whenever it acknowledges a segment, as shown in Figure\n2.3. Thesender,ifit’srespectingtheprotocol,avoidssendingmore\ndatathatcanﬁtinthereceiver’sbuffer.\nThismechanismisnottoodissimilartorate-limiting3attheservice\nlevel. But, rather than rate-limiting on an API key or IP address,\nTCPisrate-limitingonaconnectionlevel.\n3https://en.wikipedia.org/wiki/Rate_limiting\nCHAPTER2. RELIABLELINKS 26\nFigure 2.2: The receive buffer stores data that hasn’t been pro-\ncessedyetbytheapplication.\nFigure 2.3: The size of the receive buffer is communicated in the\nheadersofacknowledgmentssegments.",3537
07-Custom protocols.pdf,07-Custom protocols,"CHAPTER2. RELIABLELINKS 27\n2.4 Congestion control\nTCPnotonlyguardsagainstoverwhelmingthereceiver,butalso\nagainstﬂoodingtheunderlyingnetwork.\nThe sender estimates the available bandwidth of the underlying\nnetwork empirically through measurements. The sender main-\ntainsaso-called congestionwindow ,whichrepresentsthetotalnum-\nberofoutstandingsegmentsthatcanbesentwithoutanacknowl-\nedgmentfromtheotherside. Thesizeofthereceiverwindowlim-\nitsthemaximumsizeofthecongestionwindow. Thesmallerthe\ncongestionwindowis,thefewerbytescanbein-ﬂightatanygiven\ntime,andthelessbandwidthisutilized.\nWhen a new connection is established, the size of the congestion\nwindow is set to a system default. Then, for every segment\nacknowledged, the window increases its size exponentially\nuntil reaching an upper limit. This means that we can’t use the\nnetwork’sfullcapacityrightafteraconnectionisestablished. The\nlower the round trip time (RTT) is, the quicker the sender can\nstart utilizing the underlying network’s bandwidth, as shown in\nFigure2.4.\nWhat happens if a segment is lost? When the sender detects a\nmissed acknowledgment through a timeout, a mechanism called\ncongestion avoidance kicks in, and the congestion window size is\nreduced. From there onwards, the passing of time increases the\nwindowsize4byacertainamount,andtimeoutsdecreaseitbyan-\nother.\nAs mentioned earlier, the size of the congestion window deﬁnes\nthemaximumnumberofbytesthatcanbesentwithoutreceiving\nan acknowledgment. Because the sender needs to wait for a full\nroundtriptogetanacknowledgment,wecanderivethemaximum\ntheoreticalbandwidthbydividingthesizeofthecongestionwin-\ndowbytheroundtriptime:\n4https://en.wikipedia.org/wiki/CUBIC_TCP\nCHAPTER2. RELIABLELINKS 28\nFigure2.4: ThelowertheRTTis,thequickerthesendercanstart\nutilizingtheunderlyingnetwork’sbandwidth.\nBandwidth =WinSize\nRTT\nTheequation5showsthatbandwidthisafunctionoflatency. TCP\nwill try very hard to optimize the window size since it can’t do\nanythingabouttheroundtriptime. However,thatdoesn’talways\nyieldtheoptimalconﬁguration. Duetothewaycongestioncontrol\nworks,thelowertheroundtriptimeis,thebettertheunderlying\nnetwork’sbandwidthisutilized. Thisismorereasontoputservers\ngeographicallyclosetotheclients.\n2.5 Custom protocols\nTCP’sreliabilityandstabilitycomeatthepriceoflowerbandwidth\nandhigherlatenciesthantheunderlyingnetworkisactuallycapa-\nble of delivering. If you drop the stability and reliability mecha-\n5https://en.m.wikipedia.org/wiki/Bandwidth-delay_product\nCHAPTER2. RELIABLELINKS 29\nnismsthatTCPprovides,whatyougetisasimpleprotocolnamed\nUserDatagramProtocol6(UDP)—aconnectionlesstransportlayer\nprotocolthatcanbeusedasanalternativetoTCP.\nUnlike TCP, UDP does not expose the abstraction of a byte\nstream to its clients. Clients can only send discrete packets,\ncalled datagrams, with a limited size. UDP doesn’t offer any\nreliability as datagrams don’t have sequence numbers and are\nnotacknowledged. UDPdoesn’timplementﬂowandcongestion\ncontroleither. Overall,UDPisaleanandbareboneprotocol. It’s\nusedtobootstrapcustomprotocols,whichprovidesome,butnot\nall,ofthestabilityandreliabilityguaranteesthatTCPdoes7.\nFor example, in modern multi-player games, clients sample\ngamepad, mouse and keyboard events several times per second\nandsendthemtoaserverthatkeepstrackoftheglobalgamestate.\nSimilarly, the server samples the game state several times per\nsecondandsendsthesesnapshotsbacktotheclients. Ifasnapshot\nislostintransmission,thereisnovalueinretransmittingitasthe\ngameevolvesinreal-time;bythetimetheretransmittedsnapshot\nwould get to the destination, it would be obsolete. This is a use\ncase where UDP shines, as TCP would attempt to redeliver the\nmissingdataandconsequentlyslowdowntheclient’sexperience.\n6https://en.wikipedia.org/wiki/User_Datagram_Protocol\n7As we will later see, HTTP 3 is based on UDP to avoid some of TCP’s short-\ncomings.",3940
08-Secure links.pdf,08-Secure links,,0
09-Authentication.pdf,09-Authentication,"Chapter 3\nSecure links\nWenowknowhowtoreliablysendbytesfromoneprocesstoan-\notheroverthenetwork. Theproblemisthesebytesaresentinthe\nclear, and any middle-man can intercept our communication. To\nprotectagainstthat,wecanusethe TransportLayerSecurity1(TLS)\nprotocol. TLS runs on top of TCP and encrypts the communica-\ntion channel so that application layer protocols, like HTTP, can\nleverage it to communicate securely. In a nutshell, TLS provides\nencryption,authentication,andintegrity.\n3.1 Encryption\nEncryptionguaranteesthatthedatatransmittedbetweenaclient\nandaserverisobfuscatedandcanonlybereadbythecommuni-\ncatingprocesses.\nWhentheTLSconnectionisﬁrstopened,theclientandtheserver\nnegotiate a shared encryption secret using asymmetric encryption .\nBothpartiesgenerateakey-pairconsistingofaprivateandpublic\npart. The processes are then able to create a shared secret by ex-\nchangingtheirpublickeys. Thisispossiblethankstosomemath-\n1https://en.wikipedia.org/wiki/Transport_Layer_Security\nCHAPTER3. SECURELINKS 31\nematicalproperties2ofthekey-pairs. Thebeautyofthisapproach\nisthatthesharedsecretisnevercommunicatedoverthewire.\nAlthoughasymmetricencryptionisslowandexpensive,it’sonly\nusedtocreatethesharedencryptionkey. Afterthat, symmetricen-\ncryptionis used, which is fast and cheap. The shared key is peri-\nodicallyrenegotiatedtominimizetheamountofdatathatcanbe\ndecipheredifthesharedkeyisbroken.\nEncrypting in-ﬂight data has a CPU penalty, but it’s negligible\nsincemodernprocessorsactuallycomewithcryptographicinstruc-\ntions. Unless you have a very good reason, you should use TLS\nforallcommunications,eventhosethatarenotgoingthroughthe\npublicInternet.\n3.2 Authentication\nAlthoughwehaveawaytoobfuscatedatatransmittedacrossthe\nwire, the client still needs to authenticate the server to verify it’s\nwho itclaimsto be. Similarly, theservermightwantto authenti-\ncatetheidentityoftheclient.\nTLSimplementsauthenticationusingdigitalsignaturesbasedon\nasymmetriccryptography. Theservergeneratesakey-pairwitha\nprivateandapublickey,andsharesitspublickeywiththeclient.\nWhen the server sends a message to the client, it signs it with its\nprivate key. The clientuses the public key ofthe server to verify\nthatthedigitalsignaturewasactuallysignedwiththeprivatekey.\nThisispossiblethankstomathematicalproperties3ofthekey-pair.\nTheproblemwiththisnaiveapproachisthattheclienthasnoidea\nwhether the public key shared by the server is authentic, so we\nhavecertiﬁcatestoprovetheownershipofapublickeyforaspe-\nciﬁc entity. A certiﬁcate includes information about the owning\nentity, expiration date, public key, and a digital signature of the\nthird-partyentitythatissuedthecertiﬁcate. Thecertiﬁcate’sissu-\n2https://blog.cloudflare.com/a-relatively-easy-to-understand-primer-on-\nelliptic-curve-cryptography/\n3https://en.wikipedia.org/wiki/Digital_signature\nCHAPTER3. SECURELINKS 32\ning entity is called a certiﬁcate authority (CA), which is also repre-\nsented with a certiﬁcate. This creates a chain of certiﬁcates that\nendswithacertiﬁcateissuedbyarootCA,asshowninFigure 3.1,\nwhichself-signsitscertiﬁcate.\nForaTLScertiﬁcatetobetrustedbyadevice,thecertiﬁcate,orone\nof its ancestors, must be present in the trusted store of the client.\nTrusted root CA’s, such as Let’s Encrypt4, are typically included\nintheclient’strustedstorebydefaultbytheoperatingsystemven-\ndor.\nFigure 3.1: A certiﬁcate chain ends with a self-signed certiﬁcate\nissuedbyarootCA.\nWhen a TLS connection is opened, the server sends the full cer-\ntiﬁcatechaintotheclient,startingwiththeserver’scertiﬁcateand\nendingwiththerootCA.Theclientveriﬁestheserver’scertiﬁcate\n4https://letsencrypt.org/",3693
10-Discovery.pdf,10-Discovery,"CHAPTER3. SECURELINKS 33\nbyscanningthecertiﬁcatechainuntilacertiﬁcateisfoundthatit\ntrusts. Thenthecertiﬁcatesareveriﬁedinthereverseorderfrom\nthatpointinthechain. Theveriﬁcationchecksseveralthings,like\nthecertiﬁcate’sexpirationdateandwhetherthedigitalsignature\nwasactuallysignedbytheissuingCA.Iftheveriﬁcationreaches\nthe last certiﬁcate in the path without errors, the path is veriﬁed,\nandtheserverisauthenticated.\nOneofthemostcommonmistakeswhenusingTLSislettingacer-\ntiﬁcateexpire. Whenthathappens,theclientwon’tbeabletover-\nify the server’s identity, and opening a connection to the remote\nprocesswillfail. Thiscanbringanentireservicedownasclients\narenolongerabletoconnectwithit. Automationtomonitorand\nauto-renewcertiﬁcatesclosetoexpirationiswellworththeinvest-\nment.\n3.3 Integrity\nEven if the data is obfuscated, a middle man could still tamper\nwith it; for example, random bits within the messages could be\nswapped. Toprotectagainsttampering,TLSveriﬁestheintegrity\nofthedatabycalculatingamessagedigest. Asecurehashfunction\nisusedtocreateamessageauthenticationcode5(HMAC).Whena\nprocessreceivesamessage,itrecomputesthedigestofthemessage\nandcheckswhetheritmatchesthedigestincludedinthemessage.\nIf not, then the message has either been corrupted during trans-\nmission or has been tampered with. In this case, the message is\ndropped.\nTheTLSHMACprotectsagainstdatacorruptionaswell,notjust\ntampering. Youmightbewonderinghowdatacanbecorruptedif\nTCPissupposedtoguaranteeitsintegrity. WhileTCPdoesusea\nchecksumtoprotectagainstdatacorruption,it’snot100%reliable6\nbecause it fails to detect errors for roughly 1 in 16 million to 10\nbillionpackets. Withpacketsof1KB,thiscanhappenevery16GB\nto10TBtransmitted.\n5https://en.wikipedia.org/wiki/HMAC\n6https://dl.acm.org/doi/10.1145/347057.347561\nCHAPTER3. SECURELINKS 34\n3.4 Handshake\nWhenanewTLSconnectionisestablished,ahandshakebetween\ntheclientandserveroccursduringwhich:\n1.The parties agree on the cipher suite to use. A cipher suite\nspeciﬁes the different algorithms that the client and the\nserverintendtousetocreateasecurechannel,likethe:\n•key exchange algorithm used to generate shared\nsecrets;\n•signaturealgorithmusedtosigncertiﬁcates;\n•symmetricencryptionalgorithmusedtoencrypttheap-\nplicationdata;\n•HMAC algorithm used to guarantee the integrity and\nauthenticityoftheapplicationdata.\n2.Thepartiesusethenegotiatedkeyexchangealgorithmtocre-\nateasharedsecret. Thesharedsecretisusedbythechosen\nsymmetricencryptionalgorithmtoencryptthecommunica-\ntionofthesecurechannelgoingforwards.\n3.Theclientveriﬁesthecertiﬁcateprovidedbytheserver. The\nveriﬁcationprocessconﬁrmsthattheserveriswhoitsaysit\nis. If the veriﬁcation is successful, the client can start send-\ningencryptedapplicationdatatotheserver. Theservercan\noptionallyalsoverifytheclientcertiﬁcateifoneisavailable.\nTheseoperationsdon’tnecessarilyhappeninthisorderasmodern\nimplementationsuseseveraloptimizationstoreduceroundtrips.\nThe handshake typically requires 2 round trips with TLS 1.2 and\njustonewithTLS1.37. Thebottomlineiscreatinganewconnec-\ntionisexpensive;yetanotherreasontoputyourserversgeograph-\nicallyclosertotheclientsandreuseconnectionswhenpossible.\n7https://tools.ietf.org/html/rfc8446\nChapter 4\nDiscovery\nSo far, we explored how to create a reliable and secure channel\nbetween two processes located on different machines. However,\ntocreateanewconnectionwitharemoteprocess,westillneedto\ndiscoveritsIPaddress. ToresolvehostnamesintoIPaddresses,we\ncanusethephonebookoftheInternet: the DomainNameSystem1\n(DNS)—adistributed,hierarchical,andeventuallyconsistentkey-\nvaluestore.\nIn this chapter, we will look at how DNS resolution works in a\nbrowser, but the process is the same for any other client. When\nyou enter a URL in your browser, the ﬁrst step is to resolve the\nhostname’sIPaddress,whichisthenusedtoopenanewTLScon-\nnection.\nConcretely,let’stakealookathowtheDNSresolutionworkswhen\nyoutypewww.example.com inyourbrowser(seeFigure 4.1).\n1.The browser checks whether it has resolved the hostname\nbeforeinitslocalcache. Ifso,itreturnsthecachedIPaddress;\notherwiseitroutestherequesttoaDNSresolver. TheDNS\nresolver is typically a DNS server hosted by your Internet\nServiceProvider.\n1https://en.wikipedia.org/wiki/Domain_Name_System\nCHAPTER4. DISCOVERY 36\n2.Theresolverisresponsibleforiterativelytranslatingthehost-\nnamefortheclient. Thereasonwhyit’siterativewillbecome\nevidentinamoment. Theresolverﬁrstchecksitslocalcache\nfor a cached entry, and if one is found, it’s returned to the\nclient. If not, the query is sent to a root name server (root\nNS).\n3.Therootnameservermapsthe top-leveldomain (TLD)ofan\nincomingrequest,like .com,tothenameserver’saddressre-\nsponsibleforit.\n4.Theresolver,armedwiththeaddressoftheTLD,sendsthe\nresolutionrequesttotheTLDnameserverforthedomain,in\nourcase.com.\n5.The TLD name server maps the domain name of a request\ntotheaddressofthe authoritativenameserver responsiblefor\nit. Anauthoritativenameserverisresponsibleforaspeciﬁc\ndomainandholdsallrecordsthatmapthehostnamestoIP\naddresseswithinthatdomain.\n6.Theresolverﬁnallyqueriestheauthoritativenameserverfor\nwww.example.com , which checks its entries for the www\nhostnameandreturnstheIPaddressassociatedwithitback\ntotheresolver.\nIf the query included a subdomain of example.com , like, e.g.,\nnews.example.com , the authoritative name server would have\nreturned the address of the name server responsible for the\nsubdomain.\nThe resolution process involves several round trips in the worst\ncase,butitsbeautyisthattheaddressofarootnameserverisall\nthat’sneededtoresolveanyhostname. Giventhecostsinvolved\nresolvingahostname,itcomesasnosurprisethatthedesignersof\nDNSthoughtofwaystoreducethem.\nDNS uses UDP to serve DNS queries as it’s lean and has a low\noverhead. UDPatthetimewasagreatchoiceasthereisnoprice\nto be paid to open a new connection. That said, it’s not secure,\nas requests are sent in the clear over the Internet, allowing third\nCHAPTER4. DISCOVERY 37\nFigure4.1: DNSresolutionprocess\npartiestosnoopin. Hence,theindustryispushingslowlytowards\nrunningDNSontopofTLS2.\nTheresolutionwouldbeslowifeveryrequesthadtogothrough\nseveralnameserverlookups. Notonlythat,butthinkofthescale\nrequirementsonthenameserverstohandletheglobalresolution\nload. Caching is used to speed up the resolution process, as the\nmapping of domain names to IP addresses doesn’t change often\n—thebrowser,operatingsystem,andDNSresolverallusecaches\ninternally.\nHowdothesecachesknowwhentoexpirearecord? EveryDNS\nrecordhasa timetolive (TTL)thatinformsthecachehowlongthe\nentryisvalid. But,thereisnoguaranteethattheclientplaysnicely\nandenforcestheTTL.Don’tbesurprisedwhenyouchangeaDNS\nentryandﬁndoutthatasmallfractionofclientsarestilltryingto\nconnecttotheoldaddressdaysafterthechange.\nSetting a TTL requires making a tradeoff. If you use a long TTL,\n2https://en.wikipedia.org/wiki/DNS_over_TLS\nCHAPTER4. DISCOVERY 38\nmanyclientswon’tseeachangeforalongtime. Butifyousetittoo\nshort,youincreasetheloadonthenameserversandtheaverage\nresponsetimeofrequestsbecausetheclientswillhavetoresolve\ntheentrymoreoften.\nIf your name server becomes unavailable for any reason, the\nsmaller the record’s TTL is and the higher the number of clients\nimpactedwillbe. DNScaneasilybecomeasinglepointoffailure\n—ifyourDNSnameserverisdownandtheclientscan’tﬁndthe\nIP address of your service, they won’t have a way to connect it.\nThiscanleadtomassiveoutages3.\n3https://en.wikipedia.org/wiki/2016_Dyn_cyberattack",7579
11-APIs.pdf,11-APIs,"Chapter 5\nAPIs\nAserviceexposesoperationstoitsconsumersviaasetofinterfaces\nimplementedbyitsbusinesslogic. Asremoteclientscan’taccess\nthesedirectly,adapters—whichmakeuptheservice’sapplication\nprogramminginterface(API)—translatemessagesreceivedfrom\nIPCmechanismstointerfacecalls,asshowninFigure 5.1.\nThe communication style between a client and a service can be\ndirectorindirect, depending on whether the client communicates\ndirectlywiththeserviceorindirectlywithitthroughabroker. Di-\nrect communication requires that both processes are up and run-\nningforthecommunicationtosucceed. However,sometimesthis\nguarantee is either not needed or very hard to achieve, in which\ncaseindirectcommunicationcanbeused.\nIn this chapter, we will focus our attention on a direct communi-\ncationstylecalled request-response ,inwhichaclientsendsa request\nmessagetotheservice,andtheservicerepliesbackwitha response\nmessage. Thisissimilartoafunctioncall,butacrossprocessbound-\nariesandoverthenetwork.\nTherequestandresponsemessagescontaindatathatisserialized\nin a language-agnostic format. The format impacts a message’s\nserialization and deserialization speed, whether it’s human-\nreadable, and how hard it is to evolve it over time. A textual\nCHAPTER5. APIS 40\nFigure5.1: AdapterstranslatemessagesreceivedfromIPCmech-\nanismstointerfacecalls.\nformat like JSON1is self-describing and human-readable, at the\nexpense of increased verbosity and parsing overhead. On the\nother hand, a binary format like Protocol Buffers2is leaner and\nmore performant than a textual one at the expense of human\nreadability.\nWhen a client sends a request to a service, it can block and wait\nfortheresponsetoarrive,makingthecommunication synchronous .\nAlternatively,itcanasktheoutboundadaptertoinvokeacallback\nwhen it receives the response, making the communication asyn-\nchronous.\nSynchronous communication is inefﬁcient, as it blocks threads\nthat could be used to do something else. Some languages, like\n1https://www.json.org\n2https://developers.google.com/protocol-buffers",2067
12-HTTP.pdf,12-HTTP,"CHAPTER5. APIS 41\nJavaScript and C#, can completely hide callbacks3through\nlanguage primitives such as async/await. These primitives\nmakewritingasynchronouscodeasstraightforwardaswritinga\nsynchronousone.\nThe most commonly used IPC technologies for request-response\ninteractionsaregRPC4,REST5,andGraphQL6.Typically,internal\nAPIsusedforservice-to-servicecommunicationswithinanorgani-\nzationareimplementedwithahigh-performanceRPCframework\nlikegRPC.Incontrast,externalAPIsavailabletothepublictendto\nbebasedonREST.Intherestofthechapter,wewillwalkthrough\ntheprocessofcreatingaRESTfulHTTPAPI.\n5.1 HTTP\nHTTP7isarequest-responseprotocolusedtoencodeandtransport\ninformationbetweenaclientandaserver. Inan HTTPtransaction ,\ntheclientsendsa requestmessage totheserver’sAPIendpoint,and\ntheserverrepliesbackwitha responsemessage ,asshowninFigure\n5.2.\nIn HTTP 1.1, a message is a textual block of data that contains a\nstartline,asetofheaders,andanoptionalbody:\n•Inarequestmessage,the startlineindicateswhattherequest\nis for, and in a response message, it indicates what the re-\nsponse’sresultis.\n•Theheadersare key-value pairs with meta-information that\ndescribethemessage.\n•Themessage’s bodyisacontainerfordata.\nHTTPisastatelessprotocol,whichmeansthateverythingneeded\nby a server to process a request needs to be speciﬁed within the\nrequestitself,withoutcontextfrompreviousrequests. HTTPuses\n3https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/co\nncepts/async/\n4https://grpc.io/\n5https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm\n6https://graphql.org/\n7https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol\nCHAPTER5. APIS 42\nFigure5.2: AnexampleHTTPtransactionbetweenabrowserand\nawebserver.\nTCPforthereliabilityguaranteesdiscussedinchapter 2. Whenit\nridesontopofTLS,it’salsoreferredtoasHTTPS.Needlesstosay,\nyoushoulduseHTTPSbydefault.\nHTTP1.1keepsaconnectiontoaserveropenbydefaulttoavoid\ncreating a new one when the next transaction occurs. Unfortu-\nnately,anewrequestcan’tbeissueduntiltheresponseofthepre-\nviousonehasbeenreceived;inotherwords,thetransactionshave\ntobeserialized. Forexample,abrowserthatneedstofetchseveral\nimagestorenderanHTMLpagehastodownloadthemoneatthe\ntime,whichcanbeveryinefﬁcient.\nAlthoughHTTP1.1technicallyallowssometypeofrequeststobe\npipelined8,ithasneverbeenwidelyadoptedduetoitslimitations.\nWithHTTP1.1,thetypicalwaytoimprovethethroughputofout-\n8https://en.wikipedia.org/wiki/HTTP_pipelining",2494
13-Resources.pdf,13-Resources,"CHAPTER5. APIS 43\ngoing requests is by creating multiple connections. Although it\ncomes with a price because connections consume resources like\nmemoryandsockets.\nHTTP 29was designed from the ground up to address the main\nlimitationsofHTTP1.1. Itusesabinaryprotocolratherthanatex-\ntual one, which allows HTTP 2 to multiplex multiple concurrent\nrequest-response transactions on the same connection. In early\n2020abouthalfofthemost-visitedwebsitesontheInternetwere\nusingthenewHTTP2standard. HTTP310isthelatestiterationof\ntheHTTPstandard,whichisslowlybeingrolledouttobrowsers\nasIwritethis—it’sbasedonUDPandimplementsitsowntrans-\nportprotocoltoaddresssomeofTCP’sshortcomings.\nGiventhatneitherHTTP2norHTTP3areubiquitousyet,youstill\nneedtobefamiliarwithHTTP1.1,whichisthestandardthebook\nusesgoingforwardasitsplaintextformatiseasiertodepict.\n5.2 Resources\nSuppose we are responsible for implementing a service to man-\nagetheproductcatalogofane-commerceapplication. Theservice\nmustallowuserstobrowsethecatalogandadminstocreate,up-\ndate, or delete products. Sounds simple enough; the interface of\ntheservicecouldbedeﬁnedlikethis:\ninterface CatalogService\n{\nList<Product> GetProducts (...);\nProduct GetProduct (...);\nvoid AddProduct (...);\nvoid DeleteProduct (...);\nvoid UpdateProduct (...)\n}\nExternal clients can’t invoke interface methods directly, which is\nwhere the HTTP adapter comes in. It handles an HTTP request\n9https://tools.ietf.org/html/rfc7540\n10https://www.youtube.com/watch?v=rlN4F1oyaRM\nCHAPTER5. APIS 44\nbyinvokingthemethodsdeﬁnedintheserviceinterfaceandcon-\nvertstheirreturnvaluesintoHTTPresponses. Buttoperformthis\nmapping,weﬁrstneedtounderstandhowtomodeltheAPIwith\nHTTPintheﬁrstplace.\nAn HTTP server hosts resources. A resourceis an abstraction of\ninformation, like a document, an image, or a collection of other\nresources. It’sidentiﬁedbyaURL,whichdescribesthelocationof\ntheresourceontheserver.\nIn our catalog service, the collection of products is a type of re-\nsource,whichcouldbeaccessedwithaURLlike https://www.exam\nple.com/products?sort=price ,where:\n•httpsistheprotocol;\n•www.example.com isthehostname;\n•productsisthenameoftheresource;\n•?sort=price isthequerystring,whichcontainsadditionalpa-\nrameters that affect the way the request is handled by the\nservice; inthiscase, thesortorderofthelistofproductsre-\nturnedintheresponse.\nTheURLwithoutthequerystringisalsoreferredtoastheAPI’s\n/productsendpoint.\nHTTPgivesusalotofﬂexibilityonhowtodesignourAPI.Noth-\ning forbids us from creating a resource name that looks like a re-\nmoteprocedure,like /getProducts ,whichexpectstheadditionalpa-\nrameters to be speciﬁed in the request’s body, rather than in the\nquerystring. Butifweweretodothis,wecouldnolongercache\nthe list of products by its URL. This is where REST comes in —\nit’sasetofconventionsandconstraintsfordesigningelegantand\nscalableHTTPAPIs. Intherestofthischapter,wewilluseREST\nprincipleswhereitmakessense.\nHow should we model relationships? For example, a speciﬁc\nproduct is a resource that belongs to the collection of products,\nandthatshouldideallybereﬂectedinitsURL.Hence,theproduct\nwiththeuniqueidentiﬁer42couldbeidentiﬁedwiththerelative\nURL/products/42 . The product could also have a list of reviews",3292
14-Response status codes.pdf,14-Response status codes,"CHAPTER5. APIS 45\nassociatedwithit,whichwecanmodelbyappendingthenested\nresource name, reviews, after the parent one, /products/42 , e.g.,\n/products/42/reviews . If we were to continue to add more nested\nresources, the API would become complex. As a rule of thumb,\nURLsshouldbekeptsimple,evenifitmeansthattheclientmight\nhavetoperformmultiplerequeststogettheinformationitneeds.\nNowthatweknowhowtorefertoresources,let’sseehowtorep-\nresentthemonthewirewhentheyaretransmittedinthebodyof\nrequestandresponsemessages. Aresourcecanberepresentedin\ndifferent ways; for example, a product can be represented either\nwithanXMLoraJSONdocument. JSONistypicallyusedtorep-\nresentnon-binaryresourcesinRESTAPIs:\n{\n""id"" :42,\n""category"" :""Laptop"" ,\n""price"" :999,\n}\nWhenaclientsendsarequesttoaservertogetaresource,itadds\nseveralheaderstothemessagetodescribeitspreferredrepresen-\ntation. Theserverusestheseheaderstopickthemostappropriate\nrepresentation11fortheresourceanddecoratestheresponsemes-\nsagewithheadersthatdescribeit.\n5.3 Request methods\nHTTP requests can create, read, update, and delete (CRUD) re-\nsourcesbyusingrequestmethods. Whenaclientmakesarequest\ntoaserverforaparticularresource, itspeciﬁeswhichmethod to\nuse. Youcanthinkofarequestmethodastheverboractiontouse\nonaresource.\nThe most commonly used methods are POST,GET,PUT, and\nDELETE. For example, the API of our catalog service could be\ndeﬁnedasfollows:\n11https://developer.mozilla.org/en-US/docs/Web/HTTP/Content_negotiati\non\nCHAPTER5. APIS 46\n•POST/products —CreateanewproductandreturntheURI\nofthenewresource.\n•GET/products —Retrievealistofproducts. Thequerystring\ncanbeusedtoﬁlter,paginate,andsortthecollection. Pagina-\ntionshouldbeusedtoreturnalimitednumberofresources\npercalltopreventdenialofserviceattacks.\n•GET/products/42 —Retrieveproduct42.\n•PUT/products/42 —Updateproduct42.\n•DELETE/products/42 —Deleteproduct42.\nRequestmethodscanbeclassiﬁeddependingonwhethertheyare\nsafe and idempotent. A safemethod should not have any visible\nside effects and can be safely cached. An idempotent method can\nbeexecutedmultipletimes,andtheendresultshouldbethesame\nasifitwasexecutedjustasingletime.\nMethod Safe Idempotent\nGET Yes Yes\nPUT No Yes\nPOST No No\nDELETE No Yes\nTheconceptofidempotencyiscrucialandwillcomeuprepeatedly\nintherestofthebook,notjustinthecontextofHTTPrequests. An\nidempotentrequestmakesitpossibletosafelyretryrequeststhat\nhavesucceeded,butforwhichtheclientneverreceivedaresponse;\nforexample,becauseitcrashedandrestartedbeforereceivingit.\n5.4 Response status codes\nAftertheservicehasreceivedarequest,itneedstosendaresponse\nback to the client. The HTTP response contains a status code12to\n12https://httpstatuses.com/\nCHAPTER5. APIS 47\ncommunicate to the client whether the request succeeded or not.\nDifferentstatuscoderangeshavedifferentmeanings.\nStatus codes between 200 and 299 are used to communicate suc-\ncess. Forexample, 200(OK)meansthattherequestsucceeded,and\nthebodyoftheresponsecontainstherequestedresource.\nStatuscodesbetween300and399areusedforredirection. Forex-\nample,301(MovedPermanently) meansthattherequestedresource\nhasbeenmovedtoadifferentURL,speciﬁedintheresponsemes-\nsageLocationheader.\nStatuscodesbetween400and499arereservedforclienterrors. A\nrequestthatfailswithaclienterrorwillusuallycontinuetoreturn\nthesameerrorifit’sretried,astheerroriscausedbyanissuewith\nthe client, not the server. Because of that, it shouldn’t be retried.\nTheseclienterrorsarecommon:\n•400 (Bad Request) — Validating the client-side input has\nfailed.\n•401 (Unauthorized) — The client isn’t authorized to access a\nresource.\n•403 (Forbidden) — The user is authenticated, but it’s not al-\nlowedtoaccessaresource.\n•404(NotFound) —Theservercouldn’tﬁndtherequestedre-\nsource.\nStatuscodesbetween500and599arereservedforservererrors. A\nrequestthatfailswithaservererrorcanberetriedastheissuethat\ncaused it to fail might be ﬁxed by the time the retry is processed\nbytheserver. Thesearesometypicalserverstatuscodes:\n•500(InternalServerError) —Agenericservererror.\n•502 (Bad Gateway) — Indicates an invalid response from an\nupstreamserver.\n•503(ServiceUnavailable) —Indicatesthattheservercan’tcur-\nrentlyservetherequest,butmightbeabletointhefuture.",4276
15-Evolution.pdf,15-Evolution,"CHAPTER5. APIS 48\n5.5 OpenAPI\nNowthatwehavelearnedhowtomaptheoperationsdeﬁnedby\nourservice’sinterfaceontoRESTfulHTTPendpoints,wecanfor-\nmally deﬁne the API with an interface deﬁnition language (IDL), a\nlanguageindependentdescriptionofit. TheIDLdeﬁnitioncanbe\nused to generate boilerplate code for the IPC adapter and client\nSDKsinyourlanguagesofchoice.\nThe OpenAPI13speciﬁcation, which evolved from the Swagger\nproject, is one of the most popular IDL for RESTful APIs based\non HTTP. With it, we can formally describe our API in a YAML\ndocument, including the available endpoints, supported request\nmethods and response status codes for each endpoint, and the\nschemaoftheresources’JSONrepresentation.\nForexample,thisishowpartofthe /productsendpointofthecata-\nlogservice’sAPIcouldbedeﬁned:\nopenapi :3.0.0\ninfo :\nversion :""1.0.0""\ntitle :Catalog Service API\npaths :\n/products :\nget:\nsummary :List products\nparameters :\n-in:query\nname :sort\nrequired :false\nschema :\ntype :string\nresponses :\n'200' :\ndescription :list of products in catalog\ncontent :\n13https://swagger.io/specification/\nCHAPTER5. APIS 49\napplication/json :\nschema :\ntype :array\nitems :\n$ref :'#/components/schemas/ProductItem'\n'400' :\ndescription :bad input\ncomponents :\nschemas :\nProductItem :\ntype :object\nrequired :\n-id\n-name\n-category\nproperties :\nid:\ntype :number\nname :\ntype :string\ncategory :\ntype :string\nAlthoughthisisaverysimpleexampleandwewon’tspendtime\ndescribingOpenAPIfurtherasit’smostlyanimplementationde-\ntail,itshouldgiveyouanideaofitsexpressiveness. Withthisdeﬁ-\nnition,wecanthenrunatooltogeneratetheAPI’sdocumentation,\nboilerplateadapters,andclientSDKsforourlanguagesofchoice.\n5.6 Evolution\nAPIs start out as beautifully-designed interfaces. Slowly, but\nsurely, they will need to change over time to adapt to new use\ncases. The last thing you want to do when evolving your API is\nto introduce a breaking change that requires modifying all the\nclientsin unison, someofwhich youmighthavenocontrolover\nCHAPTER5. APIS 50\nintheﬁrstplace.\nTherearetwotypesofchangesthatcanbreakcompatibility,oneat\ntheendpointlevelandanotheratthemessagelevel. Forexample,\nif you were to change the /productsendpoint to /fancy-products , it\nwould obviously break clients that haven’t been updated to sup-\nportthenewendpoint. Thesamegoeswhenmakingapreviously\noptionalqueryparametermandatory.\nChangingtheschemaofrequestandresponsemessagesinaback-\nwardincompatiblewaycanalsowreakhavoc. Forexample,chang-\ning the type of the categoryproperty in the Productschema from\nstring to number is a breaking change as the old deserialization\nlogicwouldblowupinclients. Similararguments14canbemade\nformessagesrepresentedwithotherserializationformats,likePro-\ntocolBuffers.\nTosupportbreakingchanges,RESTAPIsshouldbeversionedby\neitherpreﬁxingaversionnumberintheURLs(e.g., /v1/products/ ),\nusing a custom header (e.g., Accept-Version: v1 ) or the Ac-\nceptheader with content negotiation (e.g., Accept: applica-\ntion/vnd.example.v1+json ).\nAs a general rule of thumb, you should try to evolve your API\ninabackwards-compatiblewayunlessyouhaveaverygoodrea-\nson,inwhichcaseyouneedtobepreparedtodealwiththeconse-\nquences. Backwards-compatible APIs tend to be not particularly\nelegant, but they are a necessary evil. There are tools15that can\ncompare the IDL speciﬁcations of your API and check for break-\ningchanges;usetheminyourcontinuousintegrationpipelines.\n14https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-\nprotocol-buffers-thrift.html\n15https://github.com/Azure/openapi-diff",3627
16-II Coordination.pdf,16-II Coordination,"Part II\nCoordination\nIntroduction\nSofar,wehavelearnedhowwecangettwoprocessestocommu-\nnicatereliablyandsecurelywitheachother. Wedidn’tgointoall\nthistroublejustforthesakeofit,though. Theendgoalhasalways\nbeentousemultipleprocesses,andservices,tobuildadistributed\napplication that gives its clients the illusion they interact with a\nsinglenode.\nAlthoughachievingaperfectillusionisnotalwayspossibleorde-\nsirable, it’s clear that some degree of coordination is needed to\nbuild a distributed application. In this part, we will explore the\ncoredistributedalgorithmsattheheartoflargescaleservices.\nChapter6introducesformalmodelsthatencodeourassumptions\nabout the behavior of nodes, communication links, and timing;\nthink of them as abstractions that allow us to reason about dis-\ntributedsystemsbyignoringthecomplexityoftheactualtechnolo-\ngiesusedtoimplementthem.\nChapter7describeshowtodetectthataremoteprocessisunreach-\nable. Sincethenetworkisunreliableandprocessescancrashatany\ntime, a process trying to communicate with another could hang\nforeverwithoutfailuredetection.\nChapter8divesintotheconceptoftimeandorder. Inthischapter,\nwewillﬁrstlearnwhyagreeingonthetimeaneventhappenedina\ndistributedsystemismuchharderthanitlooks,andthenpropose\nasolutionbasedonclocksthatdon’tmeasurethepassingoftime.\nChapter9describes how a group of processes can elect a leader\n53\nwho can perform operations that others can’t, like accessing a\nsharedresourceorcoordinatingotherprocesses’actions.\nChapter10introduces one of the fundamental challenges in dis-\ntributed systems, namely keeping replicated data in sync across\nmultiple nodes. This chapter explores why there is a tradeoff\nbetween consistency and availability and describes how the Raft\nreplicationalgorithmworks.\nChapter11dives into how to implement transactions that span\ndata partitioned among multiple nodes or services. Transactions\nrelieveyoufromawholerangeofpossiblefailurescenariossothat\nyoucanfocusontheactualapplicationlogicratherthanallpossi-\nblethingsthatcangowrong.",2051
17-System models.pdf,17-System models,"Chapter 6\nSystem models\nTo reason about distributed systems, we need to deﬁne precisely\nwhatcanandcan’thappen. A systemmodel encodesassumptions\nabout the behavior of nodes, communication links, and timing;\nthink of it as a set of assumptions that allow us to reason about\ndistributedsystemsbyignoringthecomplexityoftheactualtech-\nnologiesusedtoimplementthem.\nLet’sstartbyintroducingsomemodelsforcommunicationlinks:\n•Thefair-loss link model assumes that messages may be lost\nand duplicated. If the sender keeps retransmitting a mes-\nsage,eventuallyitwillbedeliveredtothedestination.\n•Thereliable link model assumes that a message is delivered\nexactlyonce,withoutlossorduplication. Areliablelinkcan\nbe implemented on top of a fair-loss one by de-duplicating\nmessagesatthereceivingside.\n•Theauthenticatedreliablelink modelmakesthesameassump-\ntions as the reliable link, but additionally assumes that the\nreceivercanauthenticatethemessage’ssender.\nEven though these models are just abstractions of real communi-\ncationlinks,theyareusefultoverifythecorrectnessofalgorithms.\nAswehaveseeninthepreviouschapters, it’spossibletobuilda\nreliableandauthenticatedcommunicationlinkontopofafair-loss\nCHAPTER6. SYSTEMMODELS 55\none. Forexample,TCPdoespreciselythat(andmore),whileTLS\nimplementsauthentication(andmore).\nWe can also model the different types of node failures we expect\ntohappen:\n•Thearbitrary-fault model assumes that a node can deviate\nfrom its algorithm in arbitrary ways, leading to crashes or\nunexpectedbehaviorduetobugsormaliciousactivity. The\narbitrary fault model is also referred to as the “Byzantine”\nmodelforhistoricalreasons. Interestingly,itcanbetheoreti-\ncallyproventhatasystemwithByzantinenodescantolerate\nupto1\n3offaultynodes1andstilloperatecorrectly.\n•Thecrash-recovery model assumes that a node doesn’t devi-\natefromitsalgorithm,butcancrashandrestartatanytime,\nlosingitsin-memorystate.\n•Thecrash-stopmodel assumes that a node doesn’t deviate\nfrom its algorithm, but if it crashes it never comes back on-\nline.\nWhile it’s possible to take an unreliable communication link and\nconvert it into a more reliable one using a protocol (e.g., keep re-\ntransmittinglostmessages),theequivalentisn’tpossiblefornodes.\nBecauseofthat,algorithmsfordifferentnodemodelslookverydif-\nferentfromeachother.\nByzantinenodemodelsaretypicallyusedtomodelsafety-critical\nsystemslikeairplaneenginesystems,nuclearpowerplants,ﬁnan-\ncialsystems,andothersystemswhereasingleentitydoesn’tfully\ncontrol all the nodes2. These use cases are outside of the book’s\nscope,andthealgorithmspresentedwillgenerallyassumeacrash-\nrecoverymodel.\nFinally,wecanalsomodelthetimingassumptions:\n•Thesynchronous model assumes that sending a message or\nexecutinganoperationnevertakesoveracertainamountof\ntime. This is very unrealistic in the real world, where we\n1https://en.wikipedia.org/wiki/Byzantine_fault\n2For example, digital cryptocurrencies such as Bitcoin implement algorithms\nthatassumeByzantinenodes.\nCHAPTER6. SYSTEMMODELS 56\nknow sending messages over the network can potentially\ntake a very long time, and nodes can be stopped by, e.g.,\ngarbagecollectioncyclesorpagefaults.\n•Theasynchronous modelassumesthatsendingamessageor\nexecuting an operation on a node can take an unbounded\namount of time. Unfortunately, many problems can’t be\nsolvedunderthisassumption;ifsendingmessagescantake\naninﬁniteamountoftime,algorithmscangetstuckandnot\nmakeanyprogressatall.\n•Thepartiallysynchronous modelassumesthatthesystembe-\nhavessynchronouslymostofthetime,butoccasionallyitcan\nregress to an asynchronous mode. This model is typically\nrepresentativeenoughofpracticalsystems.\nIntherestofthebook,wewillgenerallyassumeasystemmodel\nwithfair-losslinks, nodeswithcrash-recoverybehavior, andpar-\ntialsynchrony. Fortheinterestedreader,“IntroductiontoReliable\nandSecureDistributedProgramming”3isanexcellenttheoretical\nbookthatexploresdistributedalgorithmsforavarietyofothersys-\ntemmodelsnotconsideredinthistext.\nButremember,modelsarejustanabstractionofreality4,andsome-\ntimes abstractions leak. As you read along, question the models’\nassumptionsandtrytoimaginehowalgorithmsthatrelyonthem\ncouldbreak.\n3https://www.distributedprogramming.net/\n4https://en.wikipedia.org/wiki/All_models_are_wrong",4328
18-Failure detection.pdf,18-Failure detection,"Chapter 7\nFailure detection\nSeveral things can go wrong when a client sends a request to a\nserver. Inthehappypath,theclientsendsarequestandreceives\naresponseback. But,whatifnoresponsecomesbackaftersome\ntime? Inthatcase,it’simpossibletotellwhethertheserverisjust\nveryslow,itcrashed,oramessagecouldn’tbedeliveredbecause\nofanetworkissue(seeFigure 7.1).\nFigure7.1: P1can’ttellwhetherP2isslow,crashedoramessage\nwasdelayed/droppedbecauseofanetworkissue.\nCHAPTER7. FAILUREDETECTION 58\nIn the worst case, the client will wait forever for a response that\nwillneverarrive. Thebestitcandoismakeaneducatedguesson\nwhethertheserverislikelytobedownorunreachableaftersome\ntime has passed. To do that, the client can conﬁgure a timeout\nto trigger if it hasn’t received a response from the server after a\ncertainamountoftime. Ifandwhenthetimeouttriggers,theclient\nconsiderstheserverunavailableandthrowsanerror.\nThetrickypartisdeﬁninghowlongtheamountoftimethattrig-\ngersthistimeoutshouldbe. Ifit’stooshortandtheserverisreach-\nable, the client will wrongly consider the server dead; if it’s too\nlongandtheserverisnotreachable, theclientwillblockwaiting\nforaresponse. Thebottomlineisthatit’snotpossibletobuilda\nperfectfailuredetector.\nA process doesn’t necessarily need to wait to send a message to\nﬁnd out that the destination is not reachable. It can also actively\ntrytomaintainalistofprocessesthatareavailableusingpingsor\nheartbeats.\nApingisaperiodicrequestthataprocesssendstoanothertocheck\nwhether it’s still available. The process expects a response to the\npingwithinaspeciﬁctimeframe. Ifthatdoesn’thappen, atime-\noutistriggeredthatmarksthedestinationasdead. However,the\nprocesswillkeepregularlysendingpingstoitsothatifandwhen\nitcomesbackonline,itwillreplytoapingandbemarkedasavail-\nableagain.\nAheartbeatisamessagethataprocessperiodicallysendstoanother\ntoinformitthatit’sstillupandrunning. Ifthedestinationdoesn’t\nreceiveaheartbeatwithinaspeciﬁctimeframe,ittriggersatime-\nout and marks the process that missed the heartbeat as dead. If\nthatprocesscomeslaterbacktolifeandstartssendingoutheart-\nbeats,itwilleventuallybemarkedasavailableagain.\nPings and heartbeats are typically used when speciﬁc processes\nfrequentlyinteractwitheachother,andanactionneedstobetaken\nassoonasoneofthemisnolongerreachable. Ifthat’snotthecase,\ndetectingfailuresjustatcommunicationtimeisgoodenough.",2401
19-Logical clocks.pdf,19-Logical clocks,"Chapter 8\nTime\nTimeisanessentialconceptinanyapplication,evenmoresoindis-\ntributedones. Wehavealreadyencounteredsomeuseforitwhen\ndiscussing the network stack (e.g., DNS record TTL) and failure\ndetection. Timealsoplaysanimportantroleinreconstructingthe\norderofoperationsbyloggingtheirtimestamps.\nThe ﬂow of execution of a single-threaded application is easy to\ngraspsinceeveryoperationexecutessequentiallyintime, oneaf-\ntertheother. Butinadistributedsystem,thereisnosharedglobal\nclock that all processes agree on and can be used to order their\noperations. And to make matters worse, processes can run con-\ncurrently.\nIt’s challenging to build distributed applications that work as in-\ntendedwithoutknowingwhetheroneoperationhappenedbefore\nanother. CanyouimaginedesigningaTCP-likeprotocolwithout\nusingsequencenumberstoorderthepackets? Inthischapter,we\nwilllearnaboutafamilyofclocksthatcanbeusedtoworkoutthe\norderofoperationsacrossprocessesinadistributedsystem.\nCHAPTER8. TIME 60\n8.1 Physical clocks\nAprocesshasaccesstoaphysicalwall-timeclock. Themostcom-\nmon type is based on a vibrating quartz crystal, which is cheap\nbutnotveryaccurate. Thedeviceyouareusingtoreadthisbook\nislikelyusingsuchaclock. Itcanrunslightlyfasterorslowerthan\nothers,dependingonmanufacturingdifferencesandtheexternal\ntemperature. The rate at which a clock runs is also called clock\ndrift. In contrast, the difference between two clocks at a speciﬁc\npointintimeisreferredtoasclockskew.\nBecause quartz clocks drift, they need to be synced periodically\nwith machines that have access to higher accuracy clocks, like\natomic ones. Atomic clocks1measure time based on quantum-\nmechanical properties of atoms and are signiﬁcantly more\nexpensive than quartz clocks and are accurate to 1 second in 3\nmillionyears.\nTheNetworkTimeProtocol(NTP2)isusedtosynchronizeclocks.\nThechallengeistodosodespitetheunpredictablelatenciesintro-\nducedbythenetwork. ANTPclientestimatestheclockskewby\ncorrecting the timestamp received by a NTP server with the esti-\nmatednetworklatency. Armedwithanestimateoftheclockskew,\ntheclientcanadjustitsclock,causingittojumpforwardorback-\nwardintime.\nThiscreatesaproblemasmeasuringtheelapsedtimebetweentwo\npoints in time becomes error-prone. For example, an operation\nthatisexecutedafteranothercouldappeartohavebeenexecuted\nbefore.\nLuckily,mostoperatingsystemsofferadifferenttypeofclockthat\nisnotaffectedbytimejumps: themonotonicclock. Amonotonic\nclockmeasuresthenumberofsecondselapsedsinceanarbitrary\npoint,likewhenthenodestartedup,andcanonlymoveforward\nin time. A monotonic clock isusefulto measurehow much time\nelapsed between two timestamps on the same node, but times-\n1https://en.wikipedia.org/wiki/Atomic_clock\n2https://en.wikipedia.org/wiki/Network_Time_Protocol\nCHAPTER8. TIME 61\ntampsofdifferentnodescan’tbecomparedwitheachother.\nSincewedon’thaveawaytosynchronizewall-timeclocksacross\nprocessesperfectly, wecan’tdependonthemfororderingopera-\ntions. To solve this problem, we need to look at it from another\nangle. We know that two operations can’t run concurrently in a\nsingle-threadedprocessasonemusthappenbeforetheother. This\nhappened-before3relationshipcreatesacausalbondbetweenthe\ntwooperations,astheonethathappensﬁrstcanchangethestateof\ntheprocessandaffecttheoperationthatcomesafterit. Wecanuse\nthisintuitiontobuildadifferenttypeofclock,onethatisn’ttiedto\nthephysicalconceptoftime, butcapturesthecausalrelationship\nbetweenoperations: alogicalclock.\n8.2 Logical clocks\nAlogicalclock4measuresthepassingoftimeintermsoflogicalop-\nerations, not wall-clock time. The simplest possible logical clock\nisacounter,whichisincrementedbeforeanoperationisexecuted.\nDoing so ensures that each operation has a distinct logical times-\ntamp. If two operations execute on the same process, then neces-\nsarilyonemustcomebeforetheother,andtheirlogicaltimestamps\nwillreﬂectthat. Butwhataboutoperationsexecutedondifferent\nprocesses?\nImaginesendinganemailtoafriend. Anyactionsyoudidbefore\nsendingthatemail,likedrinkingcoffee,musthavehappenedbe-\nfore the actions your friend took after receiving the email. Simi-\nlarly, when one process sends a message to another, a so-called\nsynchronization point is created. The operations executed by the\nsender before the message was sent musthave happened before\ntheoperationsthatthereceiverexecutedafterreceivingit.\nALamportclock5isalogicalclockbasedonthisidea. Everyprocess\nin the system has its own local logical clock implemented with a\n3https://en.wikipedia.org/wiki/Happened-before\n4https://en.wikipedia.org/wiki/Logical_clock\n5https://en.wikipedia.org/wiki/Lamport_timestamp\nCHAPTER8. TIME 62\nnumericalcounterthatfollowsspeciﬁcrules:\n•Thecounterisinitializedwith0.\n•Theprocessincrementsitscounterbeforeexecutinganoper-\nation.\n•Whentheprocesssendsamessage,itincrementsitscounter\nandsendsacopyofitinthemessage.\n•Whentheprocessreceivesamessage,itscounterisupdated\nto1plusthemaximumofitscurrentlogicaltimestampand\nthemessage’stimestamp.\nFigure 8.1: Three processes using Lamport clocks. For example,\nbecause D happened before F, D’s logical timestamp is less than\nF’s.\nThe Lamport clock assumes a crash-stop model, but a crash-\nrecovery one can be supported by persisting the clock’s state on\ndisk,forexample.\nThe rules guarantee that if operation 𝑂1happened-before opera-\ntion 𝑂2,thelogicaltimestampof 𝑂1islessthantheoneof 𝑂2. In\ntheexampleshowninFigure 8.1,operationDhappened-beforeF\nandtheirlogicaltimestamps,4and5,reﬂectthat.\nYou would think that the converse also applies — if the logical\ntimestamp of operation 𝑂3is less than 𝑂4, then 𝑂3happened-\nbefore 𝑂4. But, that can’t be guaranteed with Lamport times-\ntamps. Going back to the example in Figure 8.1, operation E",5798
20-Vector clocks.pdf,20-Vector clocks,"CHAPTER8. TIME 63\ndidn’t happen-before C, even if their timestamps seem to imply\nit. To guarantee the converse relationship, we will have to use a\ndifferenttypeoflogicalclock: the vectorclock .\n8.3 Vector clocks\nAvectorclock6isalogicalclockthatguaranteesthatiftwoopera-\ntions can be ordered by their logical timestamps, then one must\nhave happened-before the other. A vector clock is implemented\nwithanarrayofcounters,oneforeachprocessinthesystem. And\nsimilarlytohowLamportclocksareused,eachprocesshasitsown\nlocalcopyoftheclock.\nForexample,ifthesystemiscomposedof3processes 𝑃 1,𝑃 2,and\n𝑃 3,eachprocesshasalocalvectorclockimplementedwithanar-\nray7of3counters [𝐶𝑃 1, 𝐶𝑃 2, 𝐶𝑃 3]. Theﬁrstcounterinthearray\nisassociatedwith 𝑃 1,thesecondwith 𝑃 2,andthethirdwith 𝑃 3.\nA process updates its local vector clock based on the following\nrules:\n•Initially,thecountersinthearrayaresetto0.\n•When an operation occurs, the process increments its own\ncounterinthearraybyone.\n•When the process sends a message, it increments its own\ncounter in the array by one and sends a copy of the array\nwiththemessage.\n•Whentheprocessreceivesamessage,itmergesthearrayitre-\nceivedwiththelocalonebytakingthemaximumofthetwo\narrays element-wise. Finally, it increments its own counter\ninthearraybyone.\nThebeautyofvectorclocktimestampsisthattheycanbepartially\nordered;giventwooperations 𝑂1and 𝑂2withtimestamps 𝑇1and\n𝑇2,if:\n•every counter in 𝑇1is less than or equal the corresponding\n6https://en.wikipedia.org/wiki/Vector_clock\n7Inactualimplementationsadictionaryisusedratherthananarray.\nCHAPTER8. TIME 64\nFigure 8.2: Each process has a vector clock represented with an\narrayofthreecounters.\ncounterin 𝑇2,\n•andthereisatleastonecounterin 𝑇1thatisstrictlylessthan\nthecorrespondingcounterin 𝑇2,\nthen 𝑂1happened-before 𝑂2. For example, in Figure 8.2, B\nhappened-beforeC.\nIf𝑂1didn’t happen before 𝑂2and 𝑂2didn’t happen before 𝑂1,\nthenthetimestampscan’tbeordered,andtheoperationsarecon-\nsideredtobeconcurrent. Forexample,operationEandCinFigure\n8.2can’tbe ordered, and thereforetheyareconsidered to becon-\ncurrent.\nThisdiscussionaboutlogicalclocksmightfeelquiteabstract. Later\ninthebook,wewillencountersomepracticalapplicationsoflogi-\ncalclocks. Onceyoulearntospotthem,youwillrealizetheyareev-\nerywhere,astheycanbedisguisedunderdifferentnames. What’s\nimportant to internalize at this point is that generally, you can’t\nuse physical clocks to derive accurately the order of events that\nhappenedondifferentprocesses8.\n8That said, sometimes physical clocks are good enough. For example, using\nphysical clocks to timestamp logs is ﬁne as they are mostly used for debugging\npurposes.",2691
21-Leader election.pdf,21-Leader election,,0
22-Raft leader election.pdf,22-Raft leader election,"Chapter 9\nLeader election\nSometimes a single process in the system needs to have special\npowers,likebeingtheonlyonethatcanaccessasharedresource\norassignworktoothers. Tograntaprocessthesepowers,thesys-\ntemneedstoelecta leaderamongasetof candidateprocesses ,which\nremains in charge until it crashes or becomes otherwise unavail-\nable. Whenthathappens,theremainingprocessesdetectthatthe\nleaderisnolongeravailableandelectanewone.\nAleaderelectionalgorithmneedstoguaranteethatthereisatmost\noneleaderatanygiventimeandthatanelectioneventuallycom-\npletes. Thesetwopropertiesarealsoreferredtoas safetyandlive-\nness,respectively. Thischapterexploreshowaspeciﬁcalgorithm,\ntheRaftleaderelectionalgorithm,guaranteestheseproperties.\n9.1 Raft leader election\nRaft1’sleaderelectionalgorithmisimplementedwithastatema-\nchineinwhichaprocessisinoneofthreestates(seeFigure 9.1):\n•thefollowerstate ,inwhichtheprocessrecognizesanotherone\nastheleader;\n1https://raft.github.io/\nCHAPTER9. LEADERELECTION 66\n•thecandidatestate ,inwhichtheprocessstartsanewelection\nproposingitselfasaleader;\n•ortheleaderstate,inwhichtheprocessistheleader.\nInRaft, timeisdividedinto electionterms ofarbitrarylength. An\nelection term is represented with a logical clock, a numerical\ncounter that can only increase over time. A term begins with a\nnew election, during which one or more candidates attempt to\nbecome the leader. The algorithm guarantees that for any term\nthere is at most one leader. But what triggers an election in the\nﬁrstplace?\nWhenthesystemstartsup,allprocessesbegintheirjourneyasfol-\nlowers. Afollowerexpectstoreceiveaperiodicheartbeatfromthe\nleadercontainingtheelectiontermtheleaderwaselectedin. Ifthe\nfollowerdoesn’treceiveanyheartbeatwithinacertaintimeperiod,\natimeoutﬁresandtheleaderispresumeddead. Atthatpoint,the\nfollowerstartsanewelectionbyincrementingthecurrentelection\nterm and transitioning to the candidate state. It then votes for it-\nselfandsendsarequesttoalltheprocessesinthesystemtovote\nforit,stampingtherequestwiththecurrentelectionterm.\nTheprocessremainsinthecandidatestateuntiloneofthreethings\nhappens: itwinstheelection,anotherprocesswinstheelection,or\nsometimegoesbywithnowinner:\n•The candidate wins the election —Thecandidatewinsthe\nelectionifthemajorityoftheprocessesinthesystemvotefor\nit. Eachprocesscanvoteforatmostonecandidateinaterm\nonaﬁrst-come-ﬁrst-servedbasis. Thismajorityruleenforces\nthatatmostonecandidatecanwinaterm. Ifthecandidate\nwinstheelection,ittransitionstotheleaderstateandstarts\nsendingoutheartbeatstotheotherprocesses.\n•Another process wins the election — If the candidate re-\nceivesaheartbeatfromaprocessthatclaimstobetheleader\nwithatermgreaterthan,orequalthecandidate’sterm,itac-\nceptsthenewleaderandreturnstothefollowerstate. Ifnot,\nitcontinuesinthecandidatestate. Youmightbewondering\nhowthatcouldhappen;forexample,ifthecandidateprocess",2908
23-Practical considerations.pdf,23-Practical considerations,"CHAPTER9. LEADERELECTION 67\nwastostopforanyreason,likeforalongGCpause,bythe\ntimeitresumesanotherprocesscouldhavewontheelection.\n•A period of time goes by with no winner —It’sunlikelybut\npossiblethatmultiplefollowersbecomecandidatessimulta-\nneously, and none manages to receive a majority of votes;\nthis is referred to as a split vote. When that happens, the\ncandidatewilleventuallytimeoutandstartanewelection.\nThe election timeout is picked randomly from a ﬁxed inter-\nvaltoreducethelikelihoodofanothersplitvoteinthenext\nelection.\nFigure9.1: Raft’sleaderelectionalgorithmrepresentedasastate\nmachine.\n9.2 Practical considerations\nThere are many more leader election algorithms out there than\nthe one presented here, but Raft’s implementation is a modern\ntakeontheproblemoptimizedforsimplicityandunderstandabil-\nCHAPTER9. LEADERELECTION 68\nity, which is why I chose it. That said, you will rarely need to\nimplement leader election from scratch, as you can leverage lin-\nearizable key-value stores, like etcd2or ZooKeeper3, which offer\nabstractions that make it easy to implement leader election. The\nabstractions range from basic primitives like compare-and-swap\ntofull-ﬂedgeddistributedmutexes.\nIdeally,theexternalstoreshouldattheveryleastofferanatomic\ncompare-and-swapoperationwithanexpirationtime(TTL).The\ncompare-and-swap operation updates the value of a key if and\nonly if the value matches the expected one; the expiration time\ndeﬁnesthetimetoliveforakey,afterwhichthekeyexpiresandis\nremovedfromthestoreiftheleasehasn’tbeenextended. Theidea\nisthateachcompetingprocesstriestoacquirealeasebycreating\nanewkeywithcompare-and-swapusingaspeciﬁcTTL.Theﬁrst\nprocess to succeed becomes the leader and remains such until it\nstopsrenewingthelease,afterwhichanotherprocesscanbecome\ntheleader.\nThe TTL expiry logic can also be implemented on the client-side,\nlikethislockinglibrary4forDynamoDBdoes, buttheimplemen-\ntationismorecomplex,anditstillrequiresthedatastoretooffer\nacompare-and-swapoperation.\nYou might think that’s enough to guarantee there can’t be more\nthanoneleaderinyourapplication. Unfortunately,that’snotthe\ncase.\nToseewhy,supposetherearemultipleprocessesthatneedtoup-\ndateaﬁleonasharedblobstore,andyouwanttoguaranteethat\nonlyasingleprocessatatimecandosotoavoidraceconditions.\nToachievethat, youdecidetouseadistributedmutex, aformof\nleaderelection. Eachprocesstriestoacquirethelock,andtheone\nthatdoessosuccessfullyreadstheﬁle,updatesitinmemory,and\nwritesitbacktothestore:\n2https://etcd.io/\n3https://zookeeper.apache.org/\n4https://aws.amazon.com/blogs/database/building-distributed-locks-with-\nthe-dynamodb-lock-client/\nCHAPTER9. LEADERELECTION 69\niflock.acquire():\ntry:\ncontent =store.read(blob_name)\nnew_content =update(content)\nstore.write(blob_name, new_content)\nexcept :\nlock.release()\nTheproblemhereisthatbythetimetheprocesswritesthecontent\ntothestore,itmightnolongerbetheleaderandalotmighthave\nhappenedsinceitwaselected. Forexample,theoperatingsystem\nmight have preempted and stopped the process, and several sec-\nondswillhavepassedbythetimeit’srunningagain. Sohowcan\ntheprocessensurethatit’sstilltheleaderthen? Itcouldcheckone\nmore time before writing to the store, but that doesn’t eliminate\ntheracecondition,itjustmakesitlesslikely.\nToavoidthisissue,thedatastoredownstreamneedstoverifythat\nthe request has been sent by the current leader. One way to do\nthatisbyusingafencingtoken. A fencingtoken5isanumberthat\nincreaseseverytimethatadistributedlockisacquired—inother\nwords, it’s a logical clock. When the leader writes to the store, it\npassesdownthefencingtokentoit. Thestoreremembersthevalue\nofthelasttokenandacceptsonlywriteswithagreatervalue:\nsuccess, token =lock.acquire()\nifsuccess:\ntry:\ncontent =store.read(blob_name)\nnew_content =update(content)\nstore.write(blob_name, new_content, token)\nexcept :\nlock.release()\nThisapproachaddscomplexityasthedownstreamconsumer,the\nblobstore,needstosupportfencingtokens. Ifitdoesn’t,youare\noutofluck, andyouwillhavetodesignyoursystemaroundthe\n5https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-\nlocking.html\nCHAPTER9. LEADERELECTION 70\nfact that occasionally there will be more than one leader. For ex-\nample,iftherearemomentarilytwoleadersandtheybothperform\nthesameidempotentoperation,noharmisdone.\nAlthoughhavingaleadercansimplifythedesignofasystemasit\neliminates concurrency, it can become a scaling bottleneck if the\nnumber of operations performed by the leader increases to the\npoint where it can no longer keep up. When that happens, you\nmightbeforcedtore-designthewholesystem.\nAlso, having a leader introduces a single point of failure with a\nlarge blast radius; if the election process stops working or the\nleader isn’t working as expected, it can bring down the entire\nsystemwithit.\nYou can mitigate some of these downsides by introducing parti-\ntionsandassigningadifferentleaderperpartition,butthatcomes\nwithadditionalcomplexity. Thisisthesolutionmanydistributed\ndatastoresuse.\nBefore considering the use of a leader, check whether there are\nother ways of achieving the desired functionality without it. For\nexample, optimistic locking is one way to guarantee mutual ex-\nclusion at the cost of wasting some computing power. Or per-\nhapshighavailabilityisnotarequirementforyourapplication,in\nwhich case having just a single process that occasionally crashes\nandrestartsisnotabigdeal.\nAs a rule of thumb, if you must use leader election, you have to\nminimize the work it performs and be prepared to occasionally\nhavemorethanoneleaderifyoucan’tsupportfencingtokensend-\nto-end.",5668
24-State machine replication.pdf,24-State machine replication,"Chapter 10\nReplication\nDatareplicationisafundamentalbuildingblockofdistributedsys-\ntems. One reason to replicate data is to increase availability. If\nsome data is stored exclusively on a single node, and that node\ngoesdown,thedatawon’tbeaccessibleanymore. Butifthedata\nisreplicatedinstead,clientscanseamlesslyswitchtoareplica. An-\nother reason for replication is to increase scalability and perfor-\nmance;themorereplicasthereare,themoreclientscanaccessthe\ndataconcurrentlywithouthittingperformancedegradations.\nUnfortunately replicating data is not simple, as it’s challenging\nto keep replicas consistent with one another. In this chapter, we\nwillexploreRaft’sreplicationalgorithm1,whichisoneofthealgo-\nrithms that provide the strongest consistency guarantee possible\n—theguaranteethattotheclients,thedataappearstobelocated\nonasinglenode,evenifit’sactuallyreplicated.\nRaftisbasedonatechniqueknownas statemachinereplication . The\nmainideabehinditisthatasingleprocess,theleader,broadcasts\ntheoperationsthatchangeitsstatetootherprocesses,thefollow-\ners. If the followers execute the same sequence of operations as\ntheleader,thenthestateofeachfollowerwillmatchtheleader’s.\nUnfortunately,theleadercan’tsimplybroadcastoperationstothe\n1https://raft.github.io/\nCHAPTER10. REPLICATION 72\nfollowersandcallitaday,asanyprocesscanfailatanytime,and\nthenetworkcanlosemessages. Thisiswhyalargepartoftheal-\ngorithmisdedicatedtofault-tolerance.\n10.1 State machine replication\nWhenthesystemstartsup,aleaderiselectedusingRaft’sleader\nelectionalgorithm,whichwediscussedinchapter 9. Theleaderis\ntheonlyprocessthatcanmakechangestothereplicatedstate. It\ndoes so by storing the sequence of operations that alter the state\ninto a local ordered log, which it then replicates to the followers;\nit’sthereplicationofthelogthatallowsthestatetobereplicated\nacrossprocesses.\nAs shown in Figure 10.1, a log is an ordered list of entries where\neachentryincludes:\n•theoperationtobeappliedtothestate,liketheassignment\nof3tox;\n•theindexoftheentry’spositioninthelog;\n•andthetermnumber(thenumberineachbox).\nWhen the leader wants to apply an operation to its local state, it\nﬁrstappendsanewlogentryfortheoperationintoitslog. Atthis\npoint,theoperationhasn’tbeenappliedtothelocalstatejustyet;\nithasonlybeenlogged.\nThe leader then sends a so-called AppendEntries request to each\nfollowerwiththenewentrytobeadded. Thismessageisalsosent\noutperiodically,evenintheabsenceofnewentries,asitactsasa\nheartbeatfortheleader.\nWhenafollowerreceivesan AppendEntries request,itappendsthe\nentryitreceivedtoitslogandsendsbackaresponsetotheleader\ntoacknowledgethattherequestwassuccessful. Whentheleader\nhearsback successfullyfrom amajority offollowers, itconsiders\ntheentrytobecommittedandexecutestheoperationonitslocal\nstate.\nThecommittedlogentryisconsideredtobedurableandwilleven-\nCHAPTER10. REPLICATION 73\nFigure 10.1: The leader’s log is replicated to its followers. This\nﬁgureappearsinRaft’spaper.\ntually be executed by all available followers. The leader keeps\ntrack of the highest committed index in the log, which is sent in\nallfutureAppendEntries requests. Afolloweronlyappliesalogen-\ntrytoitslocalstatewhenitﬁndsoutthattheleaderhascommitted\ntheentry.\nBecausetheleaderneedstowait onlyforamajorityoffollowers,it\ncan make progress even if some processes are down, i.e., if there\nare 2𝑓 + 1followers,thesystemcantolerateupto 𝑓failures. The\nalgorithm guarantees that an entry that is committed is durable\nandwilleventuallybeexecutedbyalltheprocessesinthesystem,\nnotjustthosethatwerepartoftheoriginalmajority.\nSo far, we have assumed there are no failures, and the network\nisreliable. Let’srelaxtheseassumptions. Iftheleaderfails,afol-\nloweriselectedasthenewleader. But,thereisacaveat: because\nthe replication algorithm only needs a majority of the processes\ntomakeprogress,it’spossiblethatwhenaleaderfails,somepro-\nCHAPTER10. REPLICATION 74\ncessesarenotup-to-date.\nToavoidthatanout-of-dateprocessbecomestheleader,aprocess\ncan’t vote for one with a less up-to-date log. In other words, a\nprocess can’t win an election if it doesn’t contain all committed\nentries. Todeterminewhichoftwoprocesses’logsismoreup-to-\ndate,theindexandtermoftheirlastentriesarecompared. Ifthe\nlogsendwithdifferentterms,thelogwiththelatertermismoreup-\nto-date. Ifthelogsendwiththesameterm,whicheverlogislonger\nismoreup-to-date. Sincetheelectionrequiresamajorityvote,and\nacandidate’slogmustbeatleastasup-to-dateasanyotherprocess\ninthatmajoritytowintheelection,theelectedprocesswillcontain\nallcommittedentries.\nWhatifafollowerfails? Ifan AppendEntries requestcan’tbedeliv-\neredtooneormorefollowers,theleaderwillretrysendingitindef-\ninitelyuntilamajorityofthefollowerssuccessfullyappendeditto\ntheirlogs. Retriesareharmlessas AppendEntries requestsareidem-\npotent,andfollowersignorelogentriesthathavealreadybeenap-\npendedtotheirlogs.\nSo what happens when a follower that was temporarily unavail-\nablecomesbackonline? Theresurrectedfollowerwilleventually\nreceivean AppendEntries messagewithalogentryfromtheleader.\nTheAppendEntries message includes the index and term number\noftheentryinthelogthatimmediatelyprecedestheonetobeap-\npended. Ifthefollowercan’tﬁndalogentrywiththesameindex\nandtermnumber,itrejectsthemessage,ensuringthatanappend\ntoitslogcan’tcreateahole. It’sasiftheleaderissendingapuzzle\npiecethatthefollowercan’tﬁtinitsversionofthepuzzle.\nWhentheAppendEntries requestisrejected,theleaderretriessend-\ningthemessage,thistimeincludingthelasttwologentries—this\niswhywereferredtotherequestas AppendEntries ,andnotas Ap-\npendEntry. Thisdancecontinuesuntilthefollowerﬁnallyaccepts\nalistoflogentriesthatcanbeappendedtoitslogwithoutcreating\nahole. Althoughthenumberofmessagesexchangedcanbeopti-\nmized,theideabehinditisthesame: thefollowerwaitsforalist\nofpuzzlepiecesthatperfectlyﬁtitsversionofthepuzzle.",5928
25-Consensus.pdf,25-Consensus,,0
26-Consistency models.pdf,26-Consistency models,"CHAPTER10. REPLICATION 75\n10.2 Consensus\nState machine replication can be used for much more than just\nreplicatingdatasinceit’sasolutiontotheconsensusproblem. Con-\nsensus2is a fundamental problem studied in distributed systems\nresearch,whichrequiresasetofprocessestoagreeonavalueina\nfault-tolerantwaysothat:\n•everynon-faultyprocesseventuallyagreesonavalue;\n•theﬁnaldecisionofeverynon-faultyprocessisthesameev-\nerywhere;\n•andthevaluethathasbeenagreedonhasbeenproposedby\naprocess.\nConsensus has a large number of practical applications. For ex-\nample, a set of processes agreeing which one should hold a lock\nor commit a transaction are consensus problems in disguise. As\nitturnsout,decidingonavaluecanbesolvedwithstatemachine\nreplication. Hence, any problem that requires consensus can be\nsolvedwithstatemachinereplicationtoo.\nTypically, when you have a problem that requires consensus, the\nlastthingyouwanttodoistosolveitfromscratchbyimplement-\ning an algorithm like Raft. While it’s important to understand\nwhat consensus is and how it can be solved, many good open-\nsource projects implement state machine replication and expose\nsimpleAPIsontopofit,likeetcdandZooKeeper.\n10.3 Consistency models\nLet’stakeacloserlookatwhathappenswhenaclientsendsare-\nquesttoareplicatedstore. Inanidealworld,therequestexecutes\ninstantaneously,asshowninFigure 10.2.\nBut in reality, things are quite different — the request needs to\nreach the leader, which then needs to process it and ﬁnally send\nback a response to the client. As shown in Figure 10.3, all these\nactionstaketimeandarenotinstantaneous.\n2https://en.wikipedia.org/wiki/Consensus_(computer_science)\nCHAPTER10. REPLICATION 76\nFigure10.2: Awriterequestexecutinginstantaneously.\nFigure10.3: Awriterequestcan’texecuteinstantaneouslybecause\nittakestimetoreachtheleaderandbeexecuted.\nThe best guarantee the system can provide is that the request\nexecutessomewherebetweenitsinvocationandcompletiontime.\nYoumightthinkthatthisdoesn’tlooklikeabigdeal;afterall,it’s\nwhat you are used to when writing single-threaded applications.\nIfyouassign1toxandreaditsvaluerightafter,youexpecttoﬁnd\n1 in there, assuming there is no other thread writing to the same\nvariable. But, once you start dealing with systems that replicate\ntheir state on multiple nodes for high availability and scalability,\nallbetsareoff. Tounderstandwhythat’sthecase,wewillexplore\ndifferentwaystoimplementreadsinourreplicatedstore.",2475
27-Sequential consistency.pdf,27-Sequential consistency,"CHAPTER10. REPLICATION 77\nInsection10.1,welookedathowRaftreplicatestheleader’sstate\ntoitsfollowers. Sinceonlytheleadercanmakechangestothestate,\nanyoperationthatmodiﬁesitneedstonecessarilygothroughthe\nleader. Butwhataboutreads? Theydon’tnecessarilyhavetogo\nthrough the leader as they don’t affect the system’s state. Reads\ncanbeservedbytheleader,afollower,oracombinationofleader\nandfollowers. Ifallreadsweretogothroughtheleader,theread\nthroughput would be limited by that of a single process. But, if\nreads can be served by any follower instead, then two clients, or\nobservers, can have a different view of the system’s state, since\nfollowerscanlagbehindtheleader.\nIntuitively, there is a trade-off between how consistent the ob-\nservers’ views of the system are, and the system’s performance\nand availability. To understand this relationship, we need to\ndeﬁnepreciselywhatwemeanbyconsistency. Wewilldosowith\nthehelpof consistencymodels3,whichformallydeﬁnethepossible\nviewsofthesystem’sstateobserverscanexperience.\n10.3.1 Strong consistency\nIfclientssendwritesandreadsexclusivelytotheleader,thenevery\nrequestappearstotakeplaceatomicallyataveryspeciﬁcpointin\ntimeasiftherewasasinglecopyofthedata. Nomatterhowmany\nreplicasthereareorhowfarbehindtheyarelagging,aslongasthe\nclients always query the leader directly, from their point of view\nthereisasinglecopyofthedata.\nBecausearequestisnotservedinstantaneously,andthereisasin-\ngleprocessservingit,therequestexecutessomewherebetweenits\ninvocation and completion time. Another way to think about it\nis that once a request completes, it’s side-effects are visible to all\nobserversasshowninFigure 10.4.\nSince a request becomes visible to all other participants between\nitsinvocationandcompletiontime,thereisareal-timeguarantee\nthat must be enforced; this guarantee is formalized by a consis-\n3https://jepsen.io/consistency\nCHAPTER10. REPLICATION 78\nFigure10.4: Theside-effectsofastronglyconsistentoperationare\nvisibletoallobserversonceitcompletes.\ntencymodelcalled linearizability4,orstrongconsistency. Lineariz-\nabilityisthestrongestconsistencyguaranteeasystemcanprovide\nforsingle-objectrequests.\nWhat if the client sends a read request to the leader and by the\ntimetherequestgetsthere,theserverassumesit’stheleader,but\nit actually was just deposed? If the ex-leader was to process the\nrequest, the system would no longer be strongly consistent. To\nguardagainstthiscase,thepresumedleaderﬁrstneedstocontact\na majority of the replicas to conﬁrm whether it still is the leader.\nOnlythenit’sallowedtoexecutetherequestandsendbackthere-\nsponsetotheclient. Thisconsiderablyincreasesthetimerequired\ntoservearead.\n10.3.2 Sequential consistency\nSofar,wehavediscussedserializingallreadsthroughtheleader.\nButdoingsocreatesasinglechokepoint,whichlimitsthesystem’s\nthroughput. Ontopofthat,theleaderneedstocontactamajority\n4https://jepsen.io/consistency/models/linearizable\nCHAPTER10. REPLICATION 79\noffollowerstohandlearead,whichincreasesthetimeittakesto\nprocessarequest. Toincreasethereadperformance,wecouldal-\nlowthefollowerstohandlerequestsaswell.\nEven though a follower can lag behind the leader, it will always\nreceive new updates in the same order as the leader. Suppose a\nclientonlyeverqueriesfollower1,andanotheronlyeverqueries\nfollower2. Inthatcase,thetwoclientswillseethestateevolving\natdifferenttimes,asfollowersarenotentirelyinsync(seeFigure\n10.5).\nFigure 10.5: Although followers have a different view of the sys-\ntems’state,theyprocessupdatesinthesameorder.\nTheconsistency modelin which operationsoccurin thesameor-\nderforallobservers,butdoesn’tprovideanyreal-timeguarantee\naboutwhenanoperation’sside-effectbecomesvisibletothem,is\ncalledsequential consistency5. The lack of real-time guarantees is\nwhatdifferentiatessequentialconsistencyfromlinearizability.\n5https://jepsen.io/consistency/models/sequential",3919
28-Eventual consistency.pdf,28-Eventual consistency,,0
29-Practical considerations.pdf,29-Practical considerations,"CHAPTER10. REPLICATION 80\nAproducer/consumersystemsynchronizedwithaqueueisanex-\nample of this model you might be familiar with; a producer pro-\ncesswritesitemstothequeue,whichaconsumerreads. Thepro-\nducer and the consumer see the items in the same order, but the\nconsumerlagsbehindtheproducer.\n10.3.3 Eventual consistency\nAlthough we managed to increase the read throughput, we had\nto pin clients to followers — what if a follower goes down? We\ncouldincreasetheavailabilityofthestoreby allowinga clientto\nquery any follower. But, this comes at a steep price in terms of\nconsistency. Saytherearetwofollowers,1and2,wherefollower\n2 lags behind follower 1. If a client queries follower 1 and right\nafterfollower2,itwillseeastatefromthepast,whichcanbevery\nconfusing. Theonlyguaranteetheclienthasisthateventually,all\nfollowerswillconvergetotheﬁnalstateifthewritestothesystem\nstop. Thisconsistencymodeliscalled eventualconsistency .\nIt’schallengingtobuildapplicationsontopofaneventuallycon-\nsistent data store because the behavior is different from the one\nyou are used to when writing single-threaded applications. Sub-\ntlebugscancreepupthatarehardtodebugandreproduce. Yet,\nineventualconsistency’sdefense, notallapplicationsrequirelin-\nearizability. You need to make the conscious choice whether the\nguaranteesofferedbyyourdatastore,orlackthereof,satisfyyour\napplication’srequirements.\nAneventuallyconsistentstoreisperfectlyﬁneifyouwanttokeep\ntrackofthenumberofusersvisitingyourwebsite,asitdoesn’tre-\nally matter if a read returns a number that is slightly out of date.\nBut for a payment processor, you deﬁnitely want strong consis-\ntency.\n10.3.4 CAP theorem\nWhen a network partition happens, parts of the system become\ndisconnected from each other. For example, some clients might\nCHAPTER10. REPLICATION 81\nnolongerbeabletoreachtheleader. Thesystemhastwochoices\nwhenthishappens,itcaneither:\n•remainavailablebyallowingfollowerstoservereads,sacri-\nﬁcingstrongconsistency;\n•or guarantee strong consistency by failing reads that can’t\nreachtheleader.\nThisconceptisexpressedbythe CAPtheorem6,whichcanbesum-\nmarized as: “strong consistency, availability and partition toler-\nance: pick two out of three.” In reality, the choice really is only\nbetweenstrongconsistencyandavailability,asnetworkfaultsare\nagivenandcan’tbeavoided.\nEventhoughnetworkpartitionscanhappen,theyareusuallyrare.\nBut,thereisatrade-offbetweenconsistencyandlatencyintheab-\nsence of a network partition. The stronger the consistency guar-\nantee is, the higher the latency of individual operations must be.\nThis relationship is expressed by the PACELC theorem7. It states\nthatincaseofnetworkpartitioning(P)inadistributedcomputer\nsystem,onehastochoosebetweenavailability(A)andconsistency\n(C),butelse(E),evenwhenthesystemisrunningnormallyinthe\nabsence of partitions, one has to choose between latency (L) and\nconsistency(C).\n10.4 Practical considerations\nTo provide high availability and performance, off-the-shelf dis-\ntributed data stores — sometimes referred to as NoSQLstores\n— come with counter-intuitive consistency guarantees. Others\nhave knobs that allow you to choose whether you want better\nperformance or stronger consistency guarantees, like Azure’s\nCosmosDB8andCassandra9. Becauseofthat,youneedtoknow\nwhat the trade-offs are. With what you have learned here, you\n6https://en.wikipedia.org/wiki/CAP_theorem\n7https://en.wikipedia.org/wiki/PACELC_theorem\n8https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\n9https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlCon\nfigConsistency.html\nCHAPTER10. REPLICATION 82\nwillbeinamuchbetterplacetounderstandwhythetrade-offsare\nthereintheﬁrstplaceandwhattheymeanforyourapplication.",3782
30-Transactions.pdf,30-Transactions,,0
31-Isolation.pdf,31-Isolation,"Chapter 11\nTransactions\nTransactions provide the illusion that a group of operations that\nmodifysomedatahasexclusiveaccesstoitandthateitherallop-\nerations complete successfully, or none does. You can typically\nleverage transactions to modify data owned by a single service,\nasit’slikelytoresideina singledata storethatsupportstransac-\ntions. Ontheotherhand,transactionsthatupdatedataownedby\ndifferentservices,eachwithitsowndatastore,arechallengingto\nimplement. Thischapterwillexplorehowtoaddtransactionsto\nyourapplicationwhenyourdatamodelispartitioned.\n11.1 ACID\nConsider a money transfer from one bank account to another. If\nthewithdrawalsucceeds,butthedepositdoesn’t,thefundsneed\nto be deposited back into the source account — money can’t just\ndisappear into thin air. In other words, the transfer needs to ex-\necuteatomically; eitherboththewithdrawalandthedepositsuc-\nceed, or neither do. To achieve that, the withdrawal and deposit\nneedtobewrappedinaninseparableunit: atransaction.\nIn a traditional relational database, a transaction is a group of\noperations for which the database guarantees a set of properties,\nCHAPTER11. TRANSACTIONS 84\nknownasACID:\n•Atomicityguaranteesthatpartialfailuresaren’tpossible;ei-\nther all the operations in the transactions complete success-\nfully,ortheyarerolledbackasiftheyneverhappened.\n•Consistencyguaranteesthattheapplication-levelinvariants,\nlike a column that can’t be null, must always be true. Con-\nfusingly, the “C” in ACID has nothing to do with the con-\nsistency models we talked about so far, and according to\nJoeHellerstein,the“C”wastossedintomaketheacronym\nwork1. Therefore,wewillsafelyignorethispropertyinthe\nrestofthischapter.\n•Isolation guarantees that the concurrent execution of trans-\nactionsdoesn’tcauseanyraceconditions.\n•Durability guarantees that once the data store commits the\ntransaction, the changes are persisted on durable storage.\nTheuseofawrite-aheadlog2(WAL)isthestandardmethod\nused to ensure durability. When using a WAL, the data\nstore can update its state only after log entries describing\nthechangeshavebeenﬂushedtopermanentstorage. Most\nof the time, the database doesn’t read from this log at all.\nButifthedatabasecrashes,thelogcanbeusedtorecoverits\npriorstate.\nTransactionsrelieveyoufromawholerangeofpossiblefailuresce-\nnariossothatyoucanfocusontheactualapplicationlogicrather\nthanallpossiblethingsthatcangowrong. Thischapterexplores\nhow distributed transactions differ from ACID transactions and\nhowyoucanimplementtheminyoursystems. Wewillfocusour\nattentionmainlyonatomicityandisolation.\n11.2 Isolation\nA set of concurrently running transactions that access the same\ndatacanrunintoallsortsofraceconditions,likedirtywrites,dirty\nreads,fuzzyreads,andphantomreads:\n1http://www.bailis.org/blog/when-is-acid-acid-rarely/\n2https://www.postgresql.org/docs/9.1/wal-intro.html\nCHAPTER11. TRANSACTIONS 85\n•Adirty write happens when a transaction overwrites the\nvalue written by another transaction that hasn’t been\ncommittedyet.\n•Adirty readhappens when a transaction observes a write\nfromatransactionthathasn’tcompletedyet.\n•Afuzzy read happens when a transaction reads an object’s\nvalue twice, but sees a different value in each read because\nacommittedtransactionupdatedthevaluebetweenthetwo\nreads.\n•Aphantomread happenswhenatransactionreadsasetofob-\njectsmatchingaspeciﬁccondition,whileanothertransaction\nadds,updates,ordeletesanobjectmatchingthesamecondi-\ntion. Forexample,ifonetransactionissummingallemploy-\nees’ salaries while another deletes some employee records\nsimultaneously,thesumofthesalarieswillbewrongatcom-\nmittime.\nToprotectagainsttheseraceconditions,atransactionneedstobe\nisolatedfromothers. An isolationlevel protectsagainstoneormore\ntypes of race conditions and provides an abstraction that we can\nusetoreasonaboutconcurrency. Thestrongertheisolationlevel\nis,themoreprotectionitoffersagainstraceconditions,buttheless\nperformantitis.\nTransactions can have different types of isolation levels that are\ndeﬁnedbasedonthetypeofraceconditionstheyforbid,asshown\ninFigure11.1.\nSerializabilityistheonlyisolationlevelthatguardsagainstallpos-\nsibleraceconditions. Itguaranteesthatthesideeffectsofexecuting\nasetoftransactionsappeartobethesameasiftheyhadexecuted\nsequentially, one after the other. But, we still have a problem —\ntherearemanypossibleordersthatthetransactionscanappearto\nbeexecutedin,asserializabilitydoesn’tsayanythingaboutwhich\nonetopick.\nSupposewehavetwotransactionsAandB,andtransactionBcom-\npletes5minutesaftertransactionA.Asystemthatguaranteesseri-\nalizabilitycanreorderthemsothatB’schangesareappliedbefore\nCHAPTER11. TRANSACTIONS 86\nFigure11.1: Isolationlevelsdeﬁnewhichraceconditionstheyfor-\nbid.\nA’s. To add a real-time requirement on the order of transactions,\nwe need a stronger isolation level: strict serializability . This level\ncombinesserializabilitywiththereal-timeguaranteesthatlineariz-\nability provides so that when a transaction completes, its side ef-\nfectsbecomeimmediatelyvisibletoallfuturetransactions.\n(Strict)serializabilityisslowasitrequirescoordination,whichcre-\natescontentioninthesystem. Asaresult,therearemanydifferent\nisolationlevelsthataresimplertoimplementandalsoperformbet-\nter. Yourapplicationmightnotneedserializability,butyouneed\ntoconsciouslydecidewhichisolationleveltouseandunderstand\nitsimplications,oryourdatastorewillsilentlymakethedecision\nforyou;forexample,PostgreSQL’sdefaultisolation3isreadcom-\n3https://www.postgresql.org/docs/12/transaction-iso.html",5594
32-Two-phase commit.pdf,32-Two-phase commit,"CHAPTER11. TRANSACTIONS 87\nmitted. Whenindoubt,choosestrictserializability.\nTherearemoreisolationlevelsandraceconditionsthantheones\nwediscussedhere. Jepsen4providesagoodformalreferenceofthe\nexistingisolationlevels,howtheyrelatetooneanother,andwhich\nguarantees they offer. Although vendors typically document the\nisolation levels their products offer, these speciﬁcations don’t al-\nwaysmatch5theformaldeﬁnitions.\nNowthatweknowwhatserializabilityis,let’slookathowitcanbe\nimplementedandwhyit’ssoexpensiveintermsofperformance.\nSerializabilitycanbeachievedeitherwithapessimisticoranopti-\nmisticconcurrencycontrolmechanism.\n11.2.1 Concurrency control\nPessimistic concurrency control uses locks to block other transac-\ntions from accessing a data item. The most popular pessimistic\nprotocolis two-phaselocking6(2PL).2PLhastwotypesoflocks,one\nforreadsandoneforwrites. Areadlockcanbesharedbymulti-\nple transactions that access the data item in read-only mode, but\nitblockstransactionstryingtoacquireawritelock. Thelattercan\nbeheldonlybyasingletransactionandblocksanyoneelsetrying\ntoacquireeitherareadorwritelockonthedataitem.\nThere are two phases in 2PL, an expanding and a shrinking one.\nIntheexpandingphase,thetransactionisallowedonlytoacquire\nlocks,butnottoreleasethem. Intheshrinkingphase,thetransac-\ntionispermittedonlytoreleaselocks,butnottoacquirethem. If\ntheserulesareobeyed,itcanbeformallyproventhattheprotocol\nguaranteesserializability.\nTheoptimisticapproach to concurrency control doesn’t block, as\nit checks for conﬂicts only at the very end of a transaction. If a\nconﬂict is detected, the transaction is aborted or restarted from\nthe beginning. Generally, optimistic concurrency control is im-\n4https://jepsen.io/consistency\n5https://jepsen.io/analyses\n6https://en.wikipedia.org/wiki/Two-phase_locking\nCHAPTER11. TRANSACTIONS 88\nplemented with multi-version concurrency control7(MVCC). With\nMVCC,thedatastorekeepsmultipleversionsofadataitem. Read-\nonlytransactionsaren’tblockedbyothertransactions,astheycan\nkeepreadingtheversionofthedatathatwascommittedatthetime\nthetransactionstarted. But,atransactionthatwritestothestoreis\nabortedorrestartedwhenaconﬂictisdetected. WhileMVCCper\nse doesn’t guarantee serializability, there are variations of it that\ndo,likeSerializableSnapshotIsolation8(SSI).\nOptimistic concurrency makes sense when you have read-heavy\nworkloads that only occasionally perform writes, as reads don’t\nneed to take any locks. For write-heavy loads, a pessimistic pro-\ntocol is more efﬁcient as it avoids retrying the same transactions\nrepeatedly.\nI have deliberately not spent much time describing 2PL and\nMVCC,asit’sunlikelyyouwillhavetoimplementtheminyour\nsystems. But,thecommercialdatastoresyoursystemsdependon\nuseoneortheothertechniquetoisolatetransactions,soyoumust\nhaveabasicgraspofthetradeoffs.\n11.3 Atomicity\nGoing back to our original example of sending money from one\nbankaccounttoanother,supposethetwoaccountsbelongtotwo\ndifferentbanksthatuseseparatedatastores. Howshouldwego\nabout guaranteeing atomicity across the two accounts? We can’t\njust run two separate transactions to respectively withdraw and\ndeposit the funds — if the second transaction fails, then the sys-\ntem is left in an inconsistent state. We need atomicity: the guar-\nantee that either both transactions succeed and their changes are\ncommitted,orthattheyfailwithoutanysideeffects.\n7https://en.wikipedia.org/wiki/Multiversion_concurrency_control\n8https://wiki.postgresql.org/wiki/SSI\nCHAPTER11. TRANSACTIONS 89\n11.3.1 Two-phase commit\nTwo-phase commit9(2PC) is a protocol used to implement atomic\ntransaction commits across multiple processes. The protocol is\nsplitintotwophases, prepareandcommit. Itassumesaprocessacts\nascoordinator and orchestrates the actions of the other processes,\ncalledparticipants . Generally, the client application that initiates\nthetransactionactsasthecoordinatorfortheprotocol.\nWhen a coordinator wants to commit a transaction to the partici-\npants, itsendsapreparerequestaskingtheparticipantswhether\ntheyarepreparedtocommitthetransaction(seeFigure 11.2). Ifall\nparticipants reply that they are ready to commit, the coordinator\nsends out a commit message to all participants ordering them to\ndo so. In contrast, if any process replies that it’s unable to com-\nmit,ordoesn’trespondpromptly,thecoordinatorsendsanabort\nrequesttoallparticipants.\nFigure11.2: Thetwo-phasecommitprotocolconsistsofaprepare\nandacommitphase.\n9https://en.wikipedia.org/wiki/Two-phase_commit_protocol\nCHAPTER11. TRANSACTIONS 90\nTherearetwopointsofnon-returnintheprotocol. Ifaparticipant\nrepliestoapreparemessagethatit’sreadytocommit,itwillhave\ntodosolater,nomatterwhat. Theparticipantcan’tmakeprogress\nfromthatpointonwarduntilitreceivesamessagefromthecoor-\ndinatortoeithercommitorabortthetransaction. Thismeansthat\nifthecoordinatorcrashes,theparticipantiscompletelyblocked.\nThe other point of non-return is when the leader decides to com-\nmitorabortthetransactionafterreceivingaresponsetoitsprepare\nmessagefromallparticipants. Oncethecoordinatormakesthede-\ncision,itcan’tchangeitsmindlaterandhastoseethroughthatthe\ntransactioniscommittedoraborted, nomatterwhat. Ifapartici-\npantistemporarilydown,thecoordinatorwillkeepretryinguntil\ntherequesteventuallysucceeds.\nTwo-phase commit has a mixed reputation10. It’s slow, as it re-\nquires multiple round trips to complete a transaction and blocks\nwhen there is a failure. If either the coordinator or a participant\nfails, then all processes part of the transactions are blocked until\nthe failing process comes back online. On top of that, the partic-\nipants need to implement the protocol; you can’t just take Post-\ngreSQL and Cassandra and expect them to play ball with each\nother.\nIf we are willing to increase complexity, there is a way to make\nthe protocol more resilient to failures. Atomically committing a\ntransaction is a form of consensus, called “uniform consensus,”\nwhere all the processes have to agree on a value, even the faulty\nones. Incontrast,thegeneralformofconsensusintroducedinsec-\ntion10.2onlyguaranteesthatallnon-faultyprocessesagreeonthe\nproposedvalue. Therefore,uniformconsensusisactuallyharder11\nthanconsensus.\nYet,anoff-the-shelfconsensusimplementationcanstillbeusedto\nmake2PCmorerobusttofailures. Forexample,replicatingtheco-\nordinatorwithaconsensusalgorithmlikeRaftmakes2PCresilient\n10http://dbmsmusings.blogspot.com/2019/01/its-time-to-move-on-from-\ntwo-phase.html\n11https://infoscience.epfl.ch/record/88273/files/CBS04.pdf?version=1",6625
33-Asynchronous transactions.pdf,33-Asynchronous transactions,,0
34-Log-based transactions.pdf,34-Log-based transactions,"CHAPTER11. TRANSACTIONS 91\nto coordinator failures. Similarly, the participants could also be\nreplicated.\nAs a historical side-note, the ﬁrst versions of modern large-scale\ndatastoresthatcameoutinthelate2000susedtobereferredtoas\nNoSQLstoresastheircorefeatureswerefocusedentirelyonscala-\nbilityandlackedtheguaranteesoftraditionalrelationaldatabases,\nsuch as ACID transactions. But in recent years, that has mostly\nchangedasdistributeddatastoreshavecontinuedtoaddfeatures\nthat only traditional databases offered, and ACID transactions\nhave become the norm rather than the exception. For example,\nGoogle’s Spanner12implements transactions across partitions\nusingacombinationof2PCandstatemachinereplication.\n11.4 Asynchronous transactions\n2PCisasynchronousblockingprotocol;ifanyoftheparticipants\nisn’t available, the transaction can’t make any progress, and the\napplicationblockscompletely. Theassumptionisthatthecoordi-\nnator and the participants are available and that the duration of\nthetransactionisshort-lived. Becauseofitsblockingnature,2PC\nisgenerallycombinedwithablockingconcurrencycontrolmech-\nanism,like2PL,toprovideisolation.\nBut,sometypesoftransactionscantakehourstoexecute,inwhich\ncase blocking just isn’t an option. And some transactions don’t\nneedisolationintheﬁrstplace. Supposeweweretodroptheiso-\nlation requirement and the assumption that the transactions are\nshort-lived. Canwecomeupwithanasynchronousnon-blocking\nsolutionthatstillprovidesatomicity?\n11.4.1 Log-based transactions\nAtypicalpattern13inmodernapplicationsisreplicatingthesame\ndata in different data stores tailored to different use cases, like\n12https://static.googleusercontent.com/media/research.google.com/en//arc\nhive/spanner-osdi2012.pdf\n13https://queue.acm.org/detail.cfm?id=3321612\nCHAPTER11. TRANSACTIONS 92\nsearch or analytics. Suppose we own a product catalog service\nbacked by a relational database, and we decided to offer an ad-\nvanced full-text search capability in its API. Although some rela-\ntional databases offer basic full-text search functionality, a dedi-\ncateddatabasesuchasElasticsearchisrequiredformoreadvanced\nusecases.\nTointegratewiththesearchindex,thecatalogserviceneedstoup-\ndateboththerelationaldatabaseandthesearchindexwhenanew\nproductisaddedoranexistingproductismodiﬁedordeleted. The\nservicecouldjustupdatetherelationaldatabaseﬁrst,andthenthe\nsearchindex;butiftheservicecrashesbeforeupdatingthesearch\nindex,thesystemwouldbeleftinaninconsistentstate. Asyoucan\nguessbynow,weneedtowrapthetwoupdatesintoatransaction\nsomehow.\nWe could consider using 2PC, but while the relational database\nsupports the X/Open XA142PC standard, the search index\ndoesn’t, which means we would have to implement that func-\ntionality from scratch. We also don’t want the catalog service to\nblock if the search index is temporarily unavailable. Although\nwe want the two data stores to be in sync, we can accept some\ntemporary inconsistencies. In other words, eventual consistency\nisacceptableforourusecase.\nTosolvethisproblem,let’sintroduceanmessageloginourappli-\ncation. Alog15isanappend-only,totallyorderedsequenceofmes-\nsages, in which each message is assigned a unique sequential in-\ndex. Messagesareappendedattheendofthelog,andconsumers\nread from it in order. Kafka16and Azure Event Hubs17are two\npopularimplementationsoflogs.\nNow, when the catalog service receives a request from a client\nto create a new product, rather than writing to the relational\ndatabase, or the search index, it appends a product creation\n14https://en.wikipedia.org/wiki/X/Open_XA\n15https://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastruc\nture-or-why-dual-writes-are-a-bad-idea/\n16https://kafka.apache.org\n17https://azure.microsoft.com/en-gb/services/event-hubs/\nCHAPTER11. TRANSACTIONS 93\nmessage to the message log. The append acts as the atomic\ncommit step for the transaction. The relational database and the\nsearch index are asynchronous consumers of the message log,\nreading entries in the same order as they were appended and\nupdating their state at their own pace (see Figure 11.3). Because\nthe message log is ordered, it guarantees that the consumers see\ntheentriesinthesameorder.\nFigure 11.3: The producer appends entries at the end of the log,\nwhiletheconsumersreadtheentriesattheirownpace.\nThe consumers periodically checkpoint the index of the last mes-\nsagetheyprocessed. Ifaconsumercrashesandcomesbackonline\naftersometime,itreadsthelastcheckpointandresumesreading\nmessagesfromwhereitleftoff. Doingsoensuresthereisnodata\nlosseveniftheconsumerwasofﬂineforsometime.\nBut, there is a problem as the consumer can potentially read the\nsame message multiple times. For example, the consumer could\nprocessamessageandcrashbeforecheckpointingitsstate. When\nitcomesbackonline,itwilleventuallyre-readthesamemessage.\nTherefore,messagesneedtobeidempotentsothatnomatterhow\nmanytimestheyareread,theeffectshouldbethesameasifthey\nhadbeenprocessedonlyonce. Onewaytodothatistodecorate\neach message with a unique ID and ignore messages with dupli-\ncateIDsatreadtime.",5132
35-Sagas.pdf,35-Sagas,"CHAPTER11. TRANSACTIONS 94\nWe have already encountered the log abstraction in chapter 10\nwhen discussing state machine replication. If you squint a little,\nyou will see that what we have just implemented here is a form\nofstatemachinereplication, wherethestateisrepresentedbyall\nproducts in the catalog, and the replication happens across the\nrelationaldatabaseandthesearchindex.\nMessage logs are part of a more general communication interac-\ntion style referred to as messaging. In this model, the sender and\nthereceiverdon’tcommunicatedirectlywitheachother; theyex-\nchange messages through a channel that acts as a broker. The\nsendersendsmessagestothechannel, andontheotherside, the\nreceiverreadsmessagesfromit.\nAmessagechannelactsasatemporarybufferforthereceiver. Un-\nlikethedirectrequest-responsecommunicationstylewehavebeen\nusing so far, messaging is inherently asynchronous as sending a\nmessagedoesn’trequirethereceivingservicetobeonline.\nA message has a well-deﬁned format, consisting of a header and\nabody. Themessageheadercontainsmetadata,suchasaunique\nmessageID,whileitsbodycontainstheactualcontent. Typically,a\nmessagecaneitherbeacommand,whichspeciﬁesanoperationto\nbeinvokedbythereceiver,oranevent,whichsignalsthereceiver\nthatsomethingofinteresthappenedinthesender.\nServices use inbound adapters to receive messages from messag-\ning channels, which are part of their API surface, and outbound\nadapters to send messages, as shown in Figure 11.4. The log ab-\nstractionwehaveusedearlierisjustoneformofmessagingchan-\nnel. Laterinthebook,wewillencounterothertypesofchannels,\nlikequeues,thatdon’tguaranteeanyorderingofthemessages.\n11.4.2 Sagas\nSuppose we own a travel booking service. To book a trip, the\ntravelservicehastoatomicallybookaﬂightthroughadedicated\nserviceandahotelthroughanother. However,eitheroftheseser-\nvices can fail their respective requests. If one booking succeeds,\nCHAPTER11. TRANSACTIONS 95\nFigure 11.4: Inbound messaging adapters are part of a service’s\nAPIsurface.\nbut the other fails, then the former needs to be canceled to guar-\nantee atomicity. Hence, booking a trip requires multiple steps to\ncomplete,someofwhichareonlyrequiredincaseoffailure. Since\nappendingasinglemessagetoalogisnolongersufﬁcienttocom-\nmitthetransaction,wecan’tusethesimplelog-orientedsolution\npresentedearlier.\nTheSaga18patternprovidesasolutiontothisproblem. Asagais\na distributed transaction composed of a set of local transactions\n𝑇1, 𝑇2, ..., 𝑇𝑛, where 𝑇𝑖has a corresponding compensating local\ntransaction 𝐶𝑖usedtoundoitschanges. TheSagaguaranteesthat\neither all local transactions succeed, or in case of failure, that the\ncompensatinglocaltransactionsundothepartialexecutionofthe\ntransaction altogether. This guarantees the atomicity of the pro-\ntocol; eitheralllocaltransactionssucceed,ornoneofthemdo. A\nSaga can be implemented with an orchestrator, the transaction’s\ncoordinator, that manages the execution of the local transactions\nacrosstheprocessesinvolved,thetransaction’sparticipants.\n18https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf\nCHAPTER11. TRANSACTIONS 96\nInourexample, thetravelbookingserviceisthetransaction’sco-\nordinator,whiletheﬂightandhotelbookingservicesarethetrans-\naction’sparticipants. TheSagaiscomposedofthreelocaltransac-\ntions: 𝑇1booksaﬂight, 𝑇2booksahotel,and 𝐶1cancelstheﬂight\nbookedwith 𝑇1.\nAtahighlevel,theSagacanbeimplementedwiththe workﬂow19\ndepictedinFigure 11.5:\n1.Thecoordinatorinitiatesthetransactionbysendingabook-\ningrequesttotheﬂightservice( 𝑇1). Ifthebookingfails,no\nharmisdone, andthecoordinatormarksthetransactionas\naborted.\n2.Iftheﬂightbookingsucceeds,thecoordinatorsendsabook-\ningrequesttothehotelservice( 𝑇2). Iftherequestsucceeds,\nthetransactionismarkedassuccessful,andwearealldone.\n3.Ifthehotelbookingfails,thetransactionneedstobeaborted.\nThecoordinatorsendsacancellationrequesttotheﬂightser-\nvice to cancel the ﬂightit previously booked ( 𝐶1). Without\nthecancellation,thetransactionwouldbeleftinaninconsis-\ntentstate,whichwouldbreakitsatomicityguarantee.\nThe coordinator can communicate asynchronously with the par-\nticipantsviamessagechannelstotoleratetemporarilyunavailable\nones. As the transaction requires multiple steps to succeed, and\nthe coordinator can fail at any time, it needs to persist the state\nof the transaction as it advances. By modeling the transaction as\na state machine, the coordinator can durably checkpoint its state\ntoadatabaseasittransitionsfrom onestatetothenext. Thisen-\nsures that if the coordinator crashes and restarts, or another pro-\ncess is elected as the coordinator, it can resume the transaction\nfromwhereitleftoffbyreadingthelastcheckpoint.\nThereisacaveat,though;ifthecoordinatorcrashesaftersending\na request but before backing up its state, it will have to re-send\nthe requestwhen itcomes back online. Similarly, ifsending a re-\nquest times-out, the coordinator will have to retry it, causing the\n19http://web.archive.org/web/20161205130022/http://kellabyte.com:\n80/2012/05/30/clarifying-the-saga-pattern",5105
36-Isolation.pdf,36-Isolation,"CHAPTER11. TRANSACTIONS 97\nFigure11.5: Aworkﬂowimplementinganasynchronoustransac-\ntion.\nmessagetoappeartwiceatthereceivingend. Hence, thepartici-\npantshavetode-duplicatethemessagestheyreceivetomakethem\nidempotent.\nIn practice, you don’t need to build orchestration engines from\nscratchtoimplementsuchworkﬂows. Serverlesscloudcompute\nservices such as AWS Step Functions20or Azure Durable Func-\ntions21makeiteasytocreatefully-managedworkﬂows.\n11.4.3 Isolation\nWestartedourjourneyintoasynchronoustransactionsasawayto\ndesignaroundtheblockingnatureof2PC.Togethere,wehadto\nsacriﬁcetheisolationguaranteethattraditionalACIDtransactions\nprovide. Asitturnsout,wecanworkaroundthelackofisolation\n20https://aws.amazon.com/step-functions\n21https://docs.microsoft.com/en-us/azure/azure-functions/durable/durabl\ne-functions-orchestrations\nCHAPTER11. TRANSACTIONS 98\naswell. Forexample,onewaytodothatiswiththeuseof semantic\nlocks22. TheideaisthatanydatatheSagamodiﬁesismarkedwith\nadirty ﬂag. This ﬂag is only cleared at the end of the transaction\nwhen it completes. Another transaction trying to access a dirty\nrecord can either fail and roll back its changes, or block until the\ndirtyﬂagiscleared. Thelatterapproachcanintroducedeadlocks,\nthough,whichrequiresastrategytomitigatethem.\n22https://dl.acm.org/doi/10.5555/284472.284478",1339
37-III Scalability.pdf,37-III Scalability,"Part III\nScalability\nIntroduction\nNow that we understand how to coordinate processes, we\nare ready to dive into one of the main use cases for building\ndistributedsystems: scalability.\nAscalableapplicationcanincreaseitscapacityasitsloadincreases.\nThesimplestwaytodothatisby scalingupandrunningtheappli-\ncationonmoreexpensivehardware,butthatonlybringsyousofar\nsincetheapplicationwilleventuallyreachaperformanceceiling.\nThealternativetoscalingupis scalingout bydistributingtheload\novermultiplenodes. Thispartexplores3categories—ordimen-\nsions — of scalability patterns: functional decomposition ,partition-\ning,andduplication . Thebeautyofthesedimensionsisthattheyare\nindependentofeachotherandcanbecombinedwithinthesame\napplication.\nFunctional decomposition\nFunctional decomposition is the process of taking an application\nandbreakingitdownintoindividualparts. Thinkofthelasttime\nyou wrote some code; you most likely decomposed it into func-\ntions,classes,andmodules. Thesameideacanbetakenfurtherby\ndecomposing an application into separate services, each with its\nownwell-deﬁnedresponsibility.\nSection12.1discusses the advantages and pitfalls of splitting an\napplicationintoasetofindependentlydeployableservices.\nSection12.2describeshowexternalclientscancommunicatewith\n101\nanapplicationafterithasbeendecomposedintoservicesusingan\nAPIgateway. Thegatewayactsastheproxyfortheapplicationby\nrouting,composing,andtranslatingrequests.\nSection12.3discusseshowtodecoupleanAPI’sreadpathfromits\nwritepathsothattheirrespectiveimplementationscanusediffer-\nenttechnologiesthatﬁttheirspeciﬁcusecases.\nSection12.4divesintoasynchronousmessagingchannelsthatde-\ncoupleproducersononeendofachannelfromconsumersonthe\notherend. Thankstochannels,communicationbetweentwopar-\ntiesispossibleevenifthedestinationistemporarilynotavailable.\nMessagingprovidesseveralotherbeneﬁts,whichwewillexplore\nin this section, along with best practices and pitfalls you can run\ninto.\nPartitioning\nWhenadatasetnolongerﬁtsonasinglenode,itneedstobepar-\ntitionedacrossmultiplenodes. Partitioningisageneraltechnique\nthatcanbeusedinavarietyofcircumstances,likeshardingTCP\nconnectionsacrossbackendsinaloadbalancer.\nWewillexploredifferentshardingstrategiesinsection 13.1,suchas\nrangeandhashpartitioning. Then,insection 13.2,wewilldiscuss\nhowtorebalancepartitionseitherstaticallyordynamically.\nDuplication\nTheeasiestwaytoaddmorecapacitytoaserviceistocreatemore\ninstances of it and have some way of routing, or balancing, re-\nquests to them. This can be a fast and cheap way to scale out a\nstateless service, as long as you have considered the impact on\ntheservice’sdependencies. Scalingoutastatefulserviceissigniﬁ-\ncantlymorechallengingassomeformofcoordinationisrequired.\nSection14.1introduces the concept of load balancing requests\nacrossnodesanditsimplementationusingcommoditymachines.\nWe will start with DNS load balancing and then dive into the\nimplementationofloadbalancersthatoperateatthetransportand\napplication layer of the network stack. Finally, we will discuss\n102\ngeo load balancing that allows clients to communicate with the\ngeographicallyclosestdatacenter.\nSection14.2describeshowtoreplicatedataacrossnodesandkeep\nitinsync. Althoughwehavealreadydiscussedonewayofdoing\nthatwithRaftinchapter 10,inthissection,wewilltakeabroader\nlook at the topic and explore different approaches with varying\ntrade-offs(single-leader,multi-leader,andleaderless).\nSection14.3discussesthebeneﬁtsandpitfallsofcaching. Wewill\nstartbydiscussingin-processcachesﬁrst,whichareeasytoimple-\nmentbuthaveseveralpitfalls. Finally,wewilllookattheprosand\nconsofexternalcaches.",3674
38-Functional decomposition.pdf,38-Functional decomposition,,0
39-Microservices.pdf,39-Microservices,"Chapter 12\nFunctional\ndecomposition\n12.1 Microservices\nAnapplicationtypicallystartsitslifeasamonolith. Takeamodern\nbackend of a single-page JavaScript application (SPA), for exam-\nple. Itmightstartoutasasinglestatelesswebservicethatexposes\na RESTful HTTP API and uses a relational database as a backing\nstore. Theserviceislikelytobecomposedofanumberofcompo-\nnentsorlibrariesthatimplementdifferentbusinesscapabilities,as\nshowninFigure 12.1.\nAsthenumberoffeatureteamscontributingtothesamecodebase\nincreases,thecomponentsbecomeincreasinglycoupledovertime.\nThis leads the teams to step on each other’s toes more and more\nfrequently,decreasingtheirproductivity.\nThecodebasebecomescomplexenoughthatnobodyfullyunder-\nstands every part of it, and implementing new features or ﬁxing\nbugsbecomestime-consuming. Evenifthebackendiscomponen-\ntized into different libraries owned by different teams, a change\ntoalibraryrequirestheservicetoberedeployed. Andifachange\nCHAPTER12. FUNCTIONALDECOMPOSITION 104\nFigure12.1: Amonolithicbackendcomposedofmultiplecompo-\nnents.\nintroducesabuglikeamemoryleak,theentireservicecanpoten-\ntially be affected by it. Additionally, rolling back a faulty build\naffects the velocity of all teams, not just the one that introduced\nthebug.\nOnewaytomitigatethegrowingpainsofa monolithicbackendisto\nsplititintoasetofindependentlydeployableservicesthatcommu-\nnicate via APIs, as shown in Figure 12.2. The APIs decouple the\nservices from each other by creating boundaries that are hard to\nviolate,unliketheonesbetweencomponentsrunninginthesame\nprocess.\nThis architectural style is also referred to as the microservice archi-\ntecture. Theterm microcanbemisleading,though—theredoesn’t\nhavetobeanythingmicroabouttheservices. Infact,Iwouldargue\nthatifaservicedoesn’tdomuch, itjustcreatesmoreoperational\noverheadthanbeneﬁts. Amoreappropriatenameforthisarchitec-\ntureisservice-orientedarchitecture1,butunfortunately,thatname\ncomeswithsomeoldbaggageaswell. Perhapsin10years,wewill\ncallthesameconceptwithyetanothername,butfornowwewill\n1https://en.wikipedia.org/wiki/Service-oriented_architecture",2137
40-Costs.pdf,40-Costs,"CHAPTER12. FUNCTIONALDECOMPOSITION 105\nFigure 12.2: A backend split into independently deployable ser-\nvicesthatcommunicateviaAPIs.\nhavetosticktomicroservices.\n12.1.1 Beneﬁts\nBreaking down the backend by business capabilities into a set of\nservices with well-deﬁned boundaries allows each service to be\ndeveloped and operated by a single small team. Smaller teams\ncanincreasetheapplication’sdevelopmentspeedforavarietyof\nreasons:\n•They are more effective as the communication overhead\ngrowsquadratically2withtheteam’ssize.\n•Since each team dictates its own release schedule and has\ncompletecontroloveritscodebase, lesscross-teamcommu-\n2https://en.wikipedia.org/wiki/The_Mythical_Man-Month\nCHAPTER12. FUNCTIONALDECOMPOSITION 106\nnicationisrequired,andthereforedecisionscanbetakenin\nlesstime.\n•Thecodebaseofaserviceissmallerandeasiertodigestbyits\ndevelopers,reducingthetimeittakestorampupnewhires.\nAlso, a smaller codebase doesn’t slow down IDEs as much,\nwhichmakesdevelopersmoreproductive.\n•Theboundariesbetweenservicesaremuchstrongerthanthe\nboundaries between components in the same process. Be-\ncause of that, when a developer needs to change a part of\nthe backend, they only need to understand a small part of\nthewhole.\n•Eachservicecanbescaledindependentlyandadoptadiffer-\nenttechnologystackbasedonitsownneeds. Theconsumers\noftheAPIsdon’tcarehowthefunctionalityisimplemented\nafterall. Thismakesiteasytoexperimentandevaluatenew\ntechnologieswithoutaffectingotherpartsofthesystem.\n•Eachmicroservicecanhaveitsownindependentdatamodel\nanddatastore(s)thatbestﬁtitsuse-cases,allowingdevelop-\nerstochangeitsschemawithoutaffectingotherservices.\n12.1.2 Costs\nThemicroservicearchitectureaddsmoremovingpartstotheover-\nall system, and this doesn’t come for free. The cost of fully em-\nbracingmicroservicesisonlyworthpayingifitcanbeamortized\nacrossdozensofdevelopmentteams.\nDevelopment experience\nNothingforbidstheuseofdifferentlanguages,libraries,anddata-\nstores in each microservice, but doing so transforms the applica-\ntionintoanunmaintainablemess. Forexample,itbecomesmore\nchallenging for a developer to move from one team to another if\nthesoftwarestackiscompletelydifferent. Andthinkofthesheer\nnumberoflibraries,oneforeachlanguageadopted,thatneedtobe\nsupportedtoprovidecommonfunctionalitythatallservicesneed,\nlikelogging.\nIt’s only reasonable then that a certain degree of standardization\nCHAPTER12. FUNCTIONALDECOMPOSITION 107\nisneeded. Onewaytodothat,whilestillallowingsomedegreeof\nfreedom, is to loosely encourage speciﬁc technologies by provid-\ning a great development experience for the teams that stick with\ntherecommendedportfoliooflanguagesandtechnologies.\nResource provisioning\nTo support a large number of independent services, it should be\nsimpletospinupnewmachines,datastores,andothercommodity\nresources—youdon’twanteveryteamtocomeupwiththeirown\nwayofdoingit. Andoncetheseresourceshavebeenprovisioned,\nthey have to be conﬁgured. To be able to pull this off, you will\nneedafairamountofautomation.\nCommunication\nRemotecallsareexpensiveandcomewithallthecaveatswedis-\ncussed earlier in the book. You will need defense mechanisms\nto protectagainst failures and leverage asynchrony and batching\ntomitigate the performancehitofcommunicating acrossthenet-\nwork. Allofthisincreasesthesystem’scomplexity.\nMuchofwhatisdescribedinthisbookisaboutdealingwiththis\ncomplexity,andasitshouldbeclearbynow,itdoesn’tcomecheap.\nThatbeingsaid,evenamonolithdoesn’tliveinisolationsinceit’s\nbeingaccessedbyremoteclients,andit’slikelytousethird-party\nAPIsaswell. Soeventually,theseissuesneedtobesolvedthereas\nwell,albeitonasmallerscale.\nContinuous integration, delivery, and deployment\nContinuousintegrationensuresthatcodechangesaremergedinto\nthemainbranchafteranautomatedbuildandtestsuiteshaverun.\nOnceacodechangehasbeenmerged, itshouldbeautomatically\npublishedanddeployedtoaproduction-likeenvironment,where\nabatteryofintegrationandend-to-endtestsruntoensurethatthe\nservicedoesn’tbreakanydependenciesorusecases.\nWhile testing individual microservices is not more challenging\nthantestingamonolith,testingtheintegrationofallthemicroser-\nvicesisanorderofmagnitudeharder. Verysubtleandunexpected",4253
41-Practical considerations.pdf,41-Practical considerations,"CHAPTER12. FUNCTIONALDECOMPOSITION 108\nbehaviorcanemergewhenindividualservicesinteractwitheach\nother.\nOperations\nUnlike with a monolith, it’s much more expensive to staff each\nteam responsible for a service with its own operations team. As\na result, the team that develops a service is typically also on-call\nfor it. This creates friction between adding new features and op-\neratingtheserviceastheteamneedstodecidewhattoprioritize\nduringeachsprint.\nDebuggingsystemsfailuresbecomesmorechallengingaswell,as\nyou can’t just load the whole application on your local machine\nandstepthroughitwithadebugger. Thesystemhasmorewaysto\nfail,sincetherearemoremovingparts. Thisiswhygoodlogging\nandmonitoringbecomescrucial.\nEventual consistency\nAsideeffectofsplittinganapplicationintoseparateservicesisthat\nthedatamodelnolongerresidesinasingledatastore. Aswehave\nlearned in previouschapters, atomicallyupdatingrecordsstored\nin different data stores, and guaranteeing strong consistency, is\nslow, expensive, and hard to get right. Hence, this type of archi-\ntectureusuallyrequiresembracingeventualconsistency.\n12.1.3 Practical considerations\nSplitting an application into services adds a lot of complexity to\ntheoverallsystem. Becauseofthat,it’sgenerallybesttostartwith\namonolithandsplitituponlywhenthereisagoodreasontodo\nso3.\nGettingtheboundariesrightbetweentheservicesischallenging—\nit’smucheasiertomovethemaroundwithinamonolithuntilyou\nﬁndasweetspot. Oncethemonolithiswellmaturedandgrowing\npainsstarttorise, thenyoucanstarttopeeloffonemicroservice\natatimefromit.\n3https://martinfowler.com/bliki/MicroservicePremium.html\nCHAPTER12. FUNCTIONALDECOMPOSITION 109\nYoushouldonlystartwithamicroservice-ﬁrstapproachifyoual-\nreadyhaveexperiencewithit,andyoueitherhavebuiltoutaplat-\nformforitorhaveaccountedforthetimeitwilltakeyoutobuild\none.",1855
42-API gateway.pdf,42-API gateway,,0
43-Translation.pdf,43-Translation,"CHAPTER12. FUNCTIONALDECOMPOSITION 110\n12.2 API gateway\nAfteryouhavesplitanapplicationintoasetofservices,eachwith\nits own API, you need to rethink how clients communicate with\ntheapplication. Aclientmightneedtoperformmultiplerequests\ntodifferentservicestofetchalltheinformationitneedstocomplete\naspeciﬁcoperation. Thiscanbeveryexpensiveonmobiledevices\nwhereeverynetworkrequestconsumespreciousbatterylife.\nMoreover,clientsneedtobeawareofimplementationdetails,like\ntheDNSnamesofalltheinternalservices. Thismakesitchalleng-\ningtochangetheapplication’sarchitectureasitcouldrequireall\nclients to be upgraded. To make matters worse, if clients are dis-\ntributedtoindividualconsumers(e.g., anappontheAppStore),\nthere might not be an easy way to force them all to upgrade to a\nnewversion. ThebottomlineisthatonceapublicAPIisoutthere,\nyoubetterbepreparedtomaintainitforaverylongtime.\nAs is typical in computer science, we can solve this problem by\naddingalayerofindirection. TheinternalAPIscanbehiddenby\napubliconethatactsasafacade,orproxy,fortheinternalservices\n(seeFigure 12.3). TheservicethatexposesthepublicAPIiscalled\ntheAPIgateway ,whichistransparenttoitsclientssincetheyhave\nnoideatheyarecommunicatingthroughanintermediary.\nTheAPIgatewayprovidesmultiplefeatures,likerouting,compo-\nsition,andtranslation.\n12.2.1 Routing\nThe API gateway can route the requests it receives to the appro-\npriatebackendservice. Itdoessowiththehelpofaroutingmap,\nwhich maps the external APIs to the internal ones. For example,\nthemapmighthavea1:1mappingbetweenanexternalpathand\ninternalone. Ifinthefuturetheinternalpathchanges,thepublic\nAPI can continue to expose the old path to guarantee backward\ncompatibility.\nCHAPTER12. FUNCTIONALDECOMPOSITION 111\nFigure 12.3: The API gateway hides the internal APIs from its\nclients.\n12.2.2 Composition\nWhile data of a monolithic application typically resides in a sin-\ngledatastore,inadistributedsystem,it’sspreadacrossmultiple\nservices. As such, some use cases might require stitching data\nbacktogetherfrommultiplesources. TheAPIgatewaycanoffera\nhigher-levelAPIthatqueriesmultipleservicesandcomposestheir\nresponses within a single one that is then returned to the client.\nThisrelievestheclientfromknowingwhichservicestoqueryand\nreducesthenumberofrequestsitneedstoperformtogetthedata\nitneeds.\nCompositioncanbehardtogetright. Theavailabilityofthecom-\nposedAPIdecreasesasthenumberofinternalcallsincreasessince\neach has a non-zero probability of failure. Additionally, the data\nacross the services might be inconsistent as some updates might",2602
44-Cross-cutting concerns.pdf,44-Cross-cutting concerns,"CHAPTER12. FUNCTIONALDECOMPOSITION 112\nnot have propagated to all services yet; in that case, the gateway\nwillhavetosomehowresolvethisdiscrepancy.\n12.2.3 Translation\nThe API gateway can translate from one IPC mechanism to an-\nother. Forexample, itcantranslateaRESTfulHTTPrequestinto\naninternalgRPCcall.\nThe gateway can also expose different APIs to different types of\nclients. Forexample,awebAPIforadesktopapplicationcanpo-\ntentiallyreturnmoredatathantheoneforamobileapplication,as\nthescreenestateislargerandmoreinformationcanbepresented\natonce. Also,networkcallsareexpensiveformobileclients,and\nrequestsgenerallyneedtobebatchedtoreducebatteryusage.\nTomeetthesedifferentandcompetingrequirements,thegateway\ncanprovidedifferentAPIstailoredtodifferentusecasesandtrans-\nlate these APIs to the internal ones. An increasingly popular ap-\nproachtotailorAPIstoindividualusecasesistousegraph-based\nAPIs. Agraph-based API exposes a schema composed of types,\nﬁelds, and relationships across types. The API allows a client to\ndeclarewhatdataitneedsandletthegatewayﬁgureouthowto\ntranslatetherequestintoaseriesofinternalAPIcalls.\nThis approach reduces the development time as there is no need\ntointroducedifferentAPIsfordifferentusecases,andtheclients\narefreetospecifywhattheyneed. ThereisstillanAPI,though;it\njusthappensthatit’sdescribedwithagraphschema. Inaway,it’s\nasifthegatewaygrantstheclientstheabilitytoperformrestricted\nqueriesonitsbackendAPIs. GraphQL4isthemostpopulartech-\nnologyinthespaceatthetimeofwriting.\n12.2.4 Cross-cutting concerns\nAstheAPIgatewayisaproxy,ormiddleman,fortheservicesbe-\nhindit,itcanalsoimplementcross-cuttingfunctionalitythatoth-\nerwisewouldhavetobere-implementedineachservice. Forex-\n4https://graphql.org/\nCHAPTER12. FUNCTIONALDECOMPOSITION 113\nample,theAPIgatewaycouldcachefrequentlyaccessedresources\ntoimprovetheAPI’sperformancewhilereducingthebandwidth\nrequirements on the services or rate-limit requests to protect the\nservicesfrombeingoverwhelmed.\nAmongthemostcriticalcross-cuttingaspectsofsecuringaservice,\nauthentication and authorization are top-of-mind. Authentication\nis the process of validating that a so-called principal — a human\noranapplication—issuingarequestfromaclientiswhoitsays\nit is.Authorization instead is the process of granting the authen-\nticated principal permissions to perform speciﬁc operations, like\ncreating,reading,updating,ordeletingaparticularresource. Typ-\nically this is implemented by assigning a principal one or more\nrolesthatgrantspeciﬁcpermissions. Alternatively,anaccesscon-\ntrol list can be used to grant speciﬁc principals access to speciﬁc\nresources.\nAmonolithicapplicationcanimplementauthenticationandautho-\nrizationwithsessiontokens. Aclientsendsitscredentialstotheap-\nplicationAPI’sloginendpoint,whichvalidatesthecredentials. If\nthat’ssuccessful,theendpointreturnsa sessiontoken5totheclient,\ntypically through an HTTP cookie. The client then includes the\ntokeninallfuturerequests.\nTheapplicationusesthesessiontokentoretrieveasessionobject\nfromanin-memorycacheoranexternaldatastore. Theobjectcon-\ntainstheprincipal’sIDandtherolesgrantedtoit,whichareused\nbytheapplication’sAPIhandlerstodecidewhethertoallowthe\nprincipaltoperformanoperationornot.\nTranslating this approach to a microservice architecture is not\nthatstraightforward. Forexample,it’snotobviouswhichservice\nshouldberesponsibleforauthenticatingandauthorizingrequests,\nasthehandlingofrequestscanspanmultipleservices.\nOne approach is to have the API gateway be responsible for au-\nthenticatingexternalrequests,sincethat’stheirpointofentry. This\nallows centralizing the logic to support different authentication\n5e.g.,acryptographically-strongrandomnumber\nCHAPTER12. FUNCTIONALDECOMPOSITION 114\nmechanismsintoasinglecomponent,hidingthecomplexityfrom\ninternal services. In contrast, authorizing requests is best left to\nindividualservicestoavoidcouplingtheAPIgatewaywiththeir\ndomainlogic.\nWhen the API gateway has authenticated a request, it creates a\nsecurity token . The gateway passes this token to the internal ser-\nvicesresponsibleforhandlingtherequest,whichinturnwillpass\nitdownstreamtotheirdependencies(seeFigure 12.4).\nFigure12.4:\n1. APIclientsendsarequestwithcredentialstoAPIgateway\n2. APIgatewaytriestoauthenticatecredentialswithauthservice\n3. Auth service validates credentials and replies with a security\ntoken\n4. APIgatewaysendsarequesttoserviceAincludingthesecurity\ntoken\n5. APIgatewaysendsarequesttoserviceBincludingthesecurity\ntoken\n6. APIgatewaycomposestheresponsesfromAandBandreplies\ntotheclient",4624
45-Caveats.pdf,45-Caveats,"CHAPTER12. FUNCTIONALDECOMPOSITION 115\nWhenaninternalservicereceivesarequestwithasecuritytoken\nattachedtoit,itneedstohaveawaytovalidateitandobtainthe\nprincipal’sidentityanditsroles. Thevalidationdiffersdepending\nonthetypeoftokenused,whichcanbeeitheropaqueandnotcon-\ntainanyinformation(e.g.,anUUID),orbetransparentandembed\ntheprincipal’sinformationwithinthetokenitself.\nThedownsideofopaquetokensisthattheyrequireservicestocall\nanexternalauthservicetovalidateatokenandretrievetheprinci-\npal’sinformation. Transparenttokenseliminatethatcallattheex-\npenseofmakingithardertorevokeissuedtokensthathavefallen\nintothewronghands.\nThemostpopularstandardfortransparenttokensisthe JSONWeb\nToken6(JWT).AJWTisaJSONpayloadthatcontainsanexpiration\ndate,theprincipal’sidentity,roles,andothermetadata. Thepay-\nloadissignedwithacertiﬁcatetrustedbyinternalservices. Hence,\nnoexternalcallsareneededtovalidatethetoken.\nOpenIDConnect7andOAuth28aresecurityprotocolsthatyoucan\nuse to implement token-based authentication and authorization.\nWe have barely scratched the surface on the topic, and there are\nentire books9written on the subject you can read to learn more\naboutit.\nAnotherwidespreadmechanismtoauthenticateapplicationsisthe\nuseAPIkeys. AnAPIkeyisacustomkeythatallowstheAPIgate-\nway to identify which application is making a request and limit\nwhattheycando. ThisapproachispopularforpublicAPIs, like\ntheoneofferedbyGithuborTwitter.\n12.2.5 Caveats\nOne of the drawbacks of using an API gateway is that it can be-\ncomeadevelopmentbottleneck. Asit’scoupledwiththeservices\nit’shiding,everynewservicethatiscreatedneedstobewiredup\n6https://jwt.io/\n7https://openid.net/connect/\n8https://oauth.net/2/\n9https://www.manning.com/books/microservices-security-in-action\nCHAPTER12. FUNCTIONALDECOMPOSITION 116\ntoit. Additionally,whenevertheAPIofservicechanges,thegate-\nwayneedstobemodiﬁedaswell.\nThe other downside is that the API gateway is one more service\nthat needs to be developed, maintained, and operated. Also, it\nneedstobeabletoscaletowhatevertherequestrateisforallthe\nservices behind it. That said, if an application has dozens of ser-\nvices and APIs, the upside is greater than the downside and it’s\ngenerallyaworthwhileinvestment.\nSo how do you go about implementing a gateway? You can roll\nyour own API gateway, using a proxy framework as a starting\npoint, like NGINX10. Or better yet, you can use an off-the-shelf\nsolution,likeAzureAPIManagement11.\n10https://www.nginx.com/\n11https://azure.microsoft.com/en-gb/services/api-management/",2566
46-Messaging.pdf,46-Messaging,"CHAPTER12. FUNCTIONALDECOMPOSITION 117\n12.3 CQRS\nThe API’s gateway ability to compose internal APIs is quite lim-\nited,andqueryingdatadistributedacrossservicescanbeveryin-\nefﬁcientifthecompositionrequireslargein-memoryjoins.\nAccessingdatacanalsobeinefﬁcientforreasonsthathavenothing\ntodowithusingamicroservicearchitecture:\n•The data store used might not be well suited for speciﬁc\ntypesofqueries. Forexample,avanillarelationaldatastore\nisn’toptimizedforgeospatialqueries.\n•Thedatastoremightnotscaletohandlethenumberofreads,\nwhichcouldbeseveralordersofmagnitudehigherthanthe\nnumberofwrites.\nIn these cases, decoupling the read path from the write path can\nyieldsubstantialbeneﬁts. Thisapproachisalsoreferredtoasthe\nCommandQueryResponsibilitySegregation12(CQRS)pattern.\nThetwopathscanusedifferentdatamodelsanddatastoresthatﬁt\ntheirspeciﬁcusecases(seeFigure 12.5). Forexample,thereadpath\ncoulduseaspecializeddatastoretailoredtoaparticularquerypat-\nternrequiredbytheapplication,likegeospatialorgraph-based.\nTo keep the read and write data models synchronized, the write\npathpushesupdatestothereadpathwheneverthedatachanges.\nExternal clients could still use the write path for simple queries,\nbutcomplexqueriesareroutedtothereadpath.\nThis separation adds more complexity to the system. For exam-\nple, when the data model changes, both paths might need to be\nupdated. Similarly, operational costs increase as there are more\nmovingpartstomaintainandoperate. Also, thereisaninherent\nreplicationlagbetweenthetimeachangehasbeenappliedonthe\nwrite path and the read path has received and applied it, which\nmakesthesystemsequentiallyconsistent.\n12https://martinfowler.com/bliki/CQRS.html\nCHAPTER12. FUNCTIONALDECOMPOSITION 118\nFigure12.5: Inthisexample,thereadandwritepathsareseparated\noutintodifferentservices.\n12.4 Messaging\nWhenanapplicationisdecomposedintoservices, thenumberof\nnetworkcallsincreases,andwithit,theprobabilitythatarequest’s\ndestinationismomentarilyunavailable. Sofar,wehavemostlyas-\nsumedservicescommunicateusingadirectrequest-responsecom-\nmunication style, which requires the destination to be available\nandrespondpromptly. Messaging—aformofindirectcommuni-\ncation—doesn’thavethisrequirement,though.\nMessagingwasﬁrstintroducedwhenwediscussedtheimplemen-\ntation of asynchronous transactions in section 11.4.1. It is a form\nofindirectcommunicationinwhichaproducerwritesamessage\ntoachannel—ormessagebroker—thatdeliversthemessageto\naconsumerontheotherend.\nBydecouplingtheproducerfromtheconsumer,theformergains\nCHAPTER12. FUNCTIONALDECOMPOSITION 119\ntheabilitytocommunicatewiththelatterevenifit’stemporarily\nunavailable. Messagingprovidesseveralotherbeneﬁts:\n•It allows a client to execute an operation on a service asyn-\nchronously. This is particularly convenient for operations\nthat can take a long time to execute. For example, suppose\navideoneedstobeconvertedtomultipleformatsoptimized\nfor different devices. In that case, the client could write a\nmessagetoachanneltotriggertheconversionandbedone\nwithit.\n•It can load balance messages across a pool of consumers,\nwhich can dynamically be added or removed to match the\ncurrentload.\n•It helps to smooth out load spikes allowing consumers to\nreadmessagesattheirownpacewithoutgettingoverloaded.\nBecause there is an additional hop between the producer and\nconsumer, the communication latency is necessarily going to be\nhigher, more so if the channel has a large backlog of messages\nwaiting to be processed. Additionally, the system’s complexity\nincreases as there is one more service, the message broker, that\nneeds to be maintained and operated — as always, it’s all about\ntradeoffs.\nAny number of producers can write messages to a channel, and\nsimilarly,multipleconsumerscanreadfromit. Dependingonhow\nthechanneldeliversmessagestoconsumers,itcanbeclassiﬁedas\neitherpoint-to-pointorpublish-subscribe. Ina point-to-point chan-\nnel, a speciﬁc message is delivered to exactly one consumer. In-\nstead,ina publish-subscribe channel,acopyofthesamemessageis\ndeliveredtoallconsumers.\nAmessagechannelcanbeusedforavarietyofdifferentcommu-\nnicationstyles.\nOne-way messaging\nInthismessagingstyle,theproducerwritesamessagetoapoint-\nto-pointchannelwiththeexpectationthataconsumerwilleventu-\nallyreadandprocessit(seeFigure 12.6).\nCHAPTER12. FUNCTIONALDECOMPOSITION 120\nFigure12.6: One-waymessagingstyle\nRequest-response messaging\nThismessagingstyleissimilartothedirectrequest-responsestyle\nwe are familiar with, albeit with the difference that the request\nandresponsemessagesﬂowthroughchannels. Theconsumerhas\na point-to-point request channel from which it reads messages,\nwhileeveryproducerhasitsowndedicatedresponsechannel(see\nFigure12.7).\nWhenaproducerwritesamessagetotherequestchannel, itdec-\noratesitwitharequestidandareferencetoitsresponsechannel.\nAfter a consumer has read and processed the message, it writes\nareplytotheproducer’sresponsechannel,taggingitwiththere-\nquest’sid,whichallowstheproducertoidentifytherequestitbe-\nlongsto.\nFigure12.7: Request-responsemessagingstyle",5119
47-Guarantees.pdf,47-Guarantees,"CHAPTER12. FUNCTIONALDECOMPOSITION 121\nBroadcast messaging\nInthismessagingstyle,aproducerwritesamessagetoapublish-\nsubscribechanneltobroadcastittoallconsumers(seeFigure 12.8).\nThis mechanism is generally used to notify a group of processes\nthat a speciﬁc event has occurred. We have already encountered\nthis pattern when discussing log-based transactions in section\n11.4.1.\nFigure12.8: Broadcastmessagingstyle\n12.4.1 Guarantees\nA message channel is implemented by a messaging service, like\nAWS SQS13or Kafka14. The messaging service, or broker, acts as\nabufferformessages. Itdecouplesproducersfromconsumersso\nthattheydon’tneedtoknowtheconsumers’addresses,howmany\nofthemthereare,orwhethertheyareavailable.\nDifferentmessagebrokersimplementthechannelabstractiondif-\nferentlydependingonthetradeoffsandtheguaranteestheyoffer.\nFor example, you would think that a channel should respect the\ninsertion order of its messages, but you will ﬁnd that some im-\nplementations,likeSQSstandardqueues15,don’tofferanystrong\norderingguarantees. Whyisthat?\nBecauseamessagebrokerneedstoscaleoutjustliketheapplica-\ntionsthatuseit,itsimplementationisnecessarilydistributed. And\nwhen multiple nodes are involved, guaranteeing order becomes\n13https://aws.amazon.com/sqs/\n14https://kafka.apache.org\n15https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDevelo\nperGuide/standard-queues.html\nCHAPTER12. FUNCTIONALDECOMPOSITION 122\nchallenging as some form of coordination is required. Some bro-\nkers, like Kafka, partition a channel into multiple sub-channels,\neachsmallenoughtobehandledentirelybyasingleprocess. The\nidea is that if there is a single broker process responsible for the\nmessages of a sub-channel, then it should be trivial to guarantee\ntheirorder.\nInthiscase, whenmessagesaresenttothechannel, theyarepar-\ntitionedintosub-channelsbasedonapartitionkey. Toguarantee\nthatthemessageorderispreservedend-to-end,onlyasinglecon-\nsumerprocesscanbeallowedtoreadfromasub-channel16.\nBecause the channel is partitioned, it suffers from several draw-\nbacks. For example, a speciﬁc partition can become much hotter\nthantheothers,andthesingleconsumerreadingfromitmightnot\nbeabletokeepupwiththeload. Inthatcase,thechannelneedsto\nberepartitioned,whichcantemporarilydegradethebrokersince\nmessages need to be reshufﬂed across all partitions. Later in the\nchapter, we will learn more about the pros and cons of partition-\ning.\nNowyouseewhynothavingtoguaranteetheorderofmessages\nmakestheimplementationofabrokermuchsimpler. Orderingis\njustoneofthemanytradeoffsabrokerneedstomake,suchas:\n•deliveryguarantees,likeat-most-onceorat-least-once;\n•messagedurabilityguarantees;\n•latency;\n•messagingstandardssupported,likeAMQP17;\n•supportforcompetingconsumers;\n•broker limits, such asthe maximum supported size ofmes-\nsages.\nBecausetherearesomanydifferentwaystoimplementchannels,\nintherestofthissectionwewillmakesomeassumptionsforthe\nsakeofsimplicity:\n16Thisisalsoreferredtoasthe competingconsumerpattern ,whichisimplemented\nusingleaderelection\n17https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol",3135
48-Failures.pdf,48-Failures,"CHAPTER12. FUNCTIONALDECOMPOSITION 123\n•Channels are point-to-point and support an arbitrary num-\nberofproducersandconsumers.\n•Messagesaredeliveredtoconsumersat-least-once.\n•While a consumer is processing a message, the message re-\nmains persisted in the channel, but other consumers can’t\nreaditforthedurationofavisibilitytimeout. The visibility\ntimeoutguarantees that if the consumer crashes while pro-\ncessingthemessage,themessagewillbecomevisibletoother\nconsumersagainwhenthetimeouttriggers. Whenthecon-\nsumerisdoneprocessingthemessage,itdeletesitfromthe\nchannelpreventingitfrombeingreceivedbyanyothercon-\nsumerinthefuture.\nTheaboveguaranteesareverysimilartowhatcloudservicessuch\nasAmazon’sSQS18andAzureStorageQueues19offer.\n12.4.2 Exactly-once processing\nAsmentioned,aconsumerhastodeleteamessagefromthechan-\nnelonceit’sdoneprocessingitsothatitwon’tbereadbyanother\nconsumer.\nIftheconsumerdeletesthemessagebeforeprocessingit,thereisa\nriskitcouldcrashafterdeletingthemessageandbeforeprocessing\nit, causingthemessagetobelostforgood. Ontheotherhand, if\ntheconsumerdeletesthemessageonlyafterprocessingit,thereis\nariskthattheconsumermightcrashafterprocessingthemessage\nbutbeforedeletingit,causingthesamemessagetobereadagain\nlateron.\nBecauseofthat,thereisnosuchthing20asexactly-oncemessagedeliv-\nery. Thebestaconsumercandoistosimulate exactly-oncemessage\nprocessingbyrequiringmessagestobeidempotent.\n18https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDevelo\nperGuide/standard-queues.html\n19https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-\nbus-azure-and-service-bus-queues-compared-contrasted\n20https://bravenewgeek.com/you-cannot-have-exactly-once-delivery-redux/",1725
49-Reference plus blob.pdf,49-Reference plus blob,"CHAPTER12. FUNCTIONALDECOMPOSITION 124\n12.4.3 Failures\nWhenaconsumerfailstoprocessamessage,thevisibilitytimeout\ntriggers, and the message is eventually delivered to another con-\nsumer. Whathappensifprocessingaspeciﬁcmessageconsistently\nfails with an error, though? To guard against the message being\npickeduprepeatedlyinperpetuity,weneedtolimitthemaximum\nnumberoftimesthesamemessagecanbereadfromthechannel.\nTo enforce a maximum number of retries, the broker can stamp\nmessages with a counter that keeps track of the number of\ntimes the message has been delivered to a consumer. If the\nbrokerdoesn’tsupportthisfunctionalityoutofthebox,itcanbe\nimplementedbytheconsumers.\nOnceyouhaveawaytocountthenumberoftimesamessagehas\nbeen retried, you still have to decide what to do when the maxi-\nmumisreached. Aconsumershouldn’tdeleteamessagewithout\nprocessingit,asthatwouldcausedataloss. Butwhatitcandois\nremovethemessagefromthechannelafterwritingittoa deadlet-\nterchannel —achannelthatactsasabufferformessagesthathave\nbeenretriedtoomanytimes.\nThis way, messages that consistently fail are not lost forever but\nmerelyputonthesidesothattheydon’tpollutethemainchannel,\nwasting consumers’ processing resources. A human can then in-\nspectthesemessagestodebugthefailure,andoncetherootcause\nhasbeenidentiﬁedandﬁxed,movethembacktothemainchannel\ntobereprocessed.\n12.4.4 Backlogs\nOneofthemainadvantagesofusingamessagingbrokeristhatit\nmakesthesystemmorerobusttooutages. Producerscancontinue\ntowritemessagestoachannelevenifoneormoreconsumersare\nnot available or are degraded. As long as the rate of arrival of\nmessagesislowerorequaltotheratetheyarebeingdeletedfrom\nthechannel,everythingisgreat. Whenthatisnolongertrue,and\nconsumerscan’tkeepupwithproducers,abacklogstartstobuild\nCHAPTER12. FUNCTIONALDECOMPOSITION 125\nup.\nA messaging channel introduces a bi-modal behavior in the sys-\ntem. In one mode, there is no backlog, and everything works as\nexpected. In the other, a backlog builds up, and the system en-\ntersadegradedstate. Theissuewithabacklogisthatthelongerit\nbuildsup,themoreresourcesand/ortimeitwilltaketodrainit.\nThereareseveralreasonsforbacklogs,forexample:\n•more producers came online, and/or their throughput in-\ncreased,andtheconsumerscan’tmatchtheirrate;\n•the consumers have become slower to process individual\nmessages,whichinturndecreasedtheirdeletionrate;\n•the consumers fail to process a fraction of the messages,\nwhich are picked up again by other consumers until they\neventuallyendupinthedeadletterchannel. Thiscancause\na negative feedback loop that delays healthy messages and\nwastestheconsumers’processingtime.\nTo detect backlogs, you should measure the average time a mes-\nsage waits in the channel to be read for the ﬁrst time. Typically,\nbrokersattachatimestampofwhenthemessagewasﬁrstwritten\ntoit. Theconsumercanusethattimestamptocomputehowlong\nthemessagehasbeenwaitinginthechannelbycomparingittothe\ntimestamptakenwhenthemessagewasread. Althoughthetwo\ntimestampshavebeengeneratedbytwophysicalclocksthataren’t\nperfectlysynchronized(seesection 8.1),themeasurestillprovides\nagoodindicationofthebacklog.\n12.4.5 Fault isolation\nAspeciﬁcproducerthatemits“poisonous”messagesthatrepeat-\nedlyfailtobeprocessedcandegradethewholesystemandpoten-\ntiallycausebacklogs,sincemessagesareprocessedmultipletimes\nbefore they end up in the dead-letter channel. Therefore, it’s im-\nportant to ﬁnd ways to deal with problematic producers before\ntheystarttoaffecttherestofthesystem21.\n21Theseproducersarealsoreferredtoasnoisyneighbors\nCHAPTER12. FUNCTIONALDECOMPOSITION 126\nIf messages belong to different users22and are decorated with\nsome kind of identiﬁer, consumers can decide to treat “noisy”\nusersdifferently. Forexample, supposemessagesfromaspeciﬁc\nuser fail consistently. In that case, the consumers could decide\nto write these messages to an alternate low-priority channel and\nremove them from the main channel without processing them.\nThe consumers can continue to read from the slow channel, but\ndo so less frequently. This ensures that one single bad user can’t\naffectothers.\n12.4.6 Reference plus blob\nTransmitting a large binary object (blob) like images, audio ﬁles,\nor video can be challenging or simply impossible, depending on\nthe medium. For example, message brokers limit the maximum\nsize of messages that can be written to a channel; Azure Storage\nqueueslimitmessagesto64KB,AWSKinesisto1MB,etc. Sohow\ndo you transfer large blobs of hundreds of MBs with these strict\nlimits?\nYoucanuploadablobtoanobjectstorageservice,likeAWSS3or\nAzure Blob Storage, and then send the URL of the blob via mes-\nsage (this pattern is sometimes referred to as queue plus blob23).\nThedownsideisthatnowyouhavetodealwithtwoservices,the\nmessagebrokerandtheobjectstore,ratherthanjustthemessage\nbroker,whichincreasesthesystem’scomplexity.\nA similar approach can be used to store large blobs in databases\n—ratherthanstoringablobinadatabasedirectly,youonlystore\nsomemetadatacontaininganexternalreferencetotheactualblob.\nTheadvantageofthissolutionisthatitminimizesdatabeingtrans-\nferredbackandforthtoandfromthedatastore,improvingitsper-\nformance while reducing the required bandwidth. Also, the cost\nperbyteofanobjectstoredesignedtopersistlargeobjectsthatin-\nfrequentlychange,ifatall,islowerthantheoneofagenericdata\nstore.\n22Ausercanbeahumanoranapplication.\n23https://github.com/dotnet/orleans/issues/2328#issuecomment-255306449\nCHAPTER12. FUNCTIONALDECOMPOSITION 127\nOf course, the downside is that you lose the ability to transac-\ntionally update the blob with its metadata and potentially other\nrecords in the data store. For example, suppose a transaction\ninsertsanewrecordinthedatastorecontaininganimage. Inthis\ncase, the image won’t be visible until the transaction completes;\nthat won’t be the case if the image is stored in an external store,\nthough. Similarly, if the record is later deleted, the image is\nautomatically deleted as well; but if the image lives outside the\nstore,it’syourresponsibilitytodeleteit.\nWhether storing blobs outside of your data store is acceptable or\nnotdependsonyourspeciﬁcusecases.",6205
50-Partitioning.pdf,50-Partitioning,,0
51-Range partitioning.pdf,51-Range partitioning,"Chapter 13\nPartitioning\nNowit’stimetochangegearsanddiveintoanothertoolyouhave\natyourdisposaltoscaleoutapplication—partitioningorshard-\ning.\nWhenadatasetnolongerﬁtsonasinglenode,itneedstobepar-\ntitionedacrossmultiplenodes. Partitioningisageneraltechnique\nthatcanbeusedinavarietyofcircumstances,likeshardingTCP\nconnectionsacrossbackendsinaloadbalancer. Togroundthedis-\ncussioninthischapter,wewillanchorittotheimplementationof\nashardedkey-valuestore.\n13.1 Sharding strategies\nWhenaclientsendsarequesttoapartitioneddatastoretoreador\nwriteakey,therequestneedstoberoutedtothenoderesponsible\nforthepartitionthekeybelongsto. Onewaytodothatistousea\ngatewayservicethatcanroutetherequesttotherightplaceknow-\ninghowkeysaremappedtopartitionsandpartitionstonodes.\nThemappingbetweenkeysandpartitions,andothermetadata,is\ntypically maintained in a strongly-consistent conﬁguration store,\nlikeetcdorZookeeper. Buthowarekeysmappedtopartitionsin",944
52-Hash partitioning.pdf,52-Hash partitioning,"CHAPTER13. PARTITIONING 129\ntheﬁrstplace? Atahighlevel, therearetwowaystoimplement\nthemappingusingeitherrangepartitioningorhashpartitioning.\n13.1.1 Range partitioning\nWith range partitioning, the data is split into partitions by key\nrangeinlexicographicalorder,andeachpartitionholdsacontinu-\nousrangeofkeys,asshowninFigure 13.1. Thedatacanbestored\ninsortedorderondiskwithineachpartition,makingrangescans\nfast.\nFigure13.1: Arangepartitioneddataset\nSplittingthekey-rangeevenlydoesn’tmakemuchsensethoughif\nthe distribution of keys is not uniform, like in the English dictio-\nnary. Doing so creates unbalanced partitions that contain signiﬁ-\ncantlymoreentriesthanothers.\nAnotherissuewithrangepartitioningisthatsomeaccesspatterns\ncanleadtohotspots. Forexample,ifadatasetisrangepartitioned\nbydate,allwritesforthecurrentdayendupinthesamepartition,\nwhichdegradesthedatastore’sperformance.\n13.1.2 Hash partitioning\nTheideabehindhashpartitioningistouseahashfunctiontoas-\nsignkeystopartitions,whichshufﬂes—oruniformlydistributes\nCHAPTER13. PARTITIONING 130\n— keys across partitions, as shown in Figure 13.2. Another way\ntothinkaboutitisthatthehashfunctionmapsapotentiallynon-\nuniformly distributed key space to a uniformly distributed hash\nspace.\nFor example, a simple version of hash partitioning can be imple-\nmentedwithmodularhashing,i.e.,hash(key)modN.\nFigure13.2: Ahashpartitioneddataset\nAlthough this approach ensures that the partitions contain more\nor less the same number of entries, it doesn’t eliminate hotspots\nif theaccess pattern is not uniform. If there is a single key that is\naccessedsigniﬁcantlymoreoftenthanothers,thenallbetsareoff.\nInthiscase,thepartitionthatcontainsthehotkeyneedstobesplit\nfurtherdown. Alternatively,thekeyneedstobesplitintomultiple\nsub-keys,forexample,byaddinganoffsetattheendofit.\nUsingmodularhashingcanbecomeproblematicwhenanewpar-\ntitionisadded,asallkeyshavetobereshufﬂedacrosspartitions.\nShufﬂing data is extremely expensive as it consumes network\nbandwidth and other resources from nodes hosting partitions.\nIdeally, if a partition is added, only𝐾\n𝑁keys should be shufﬂed\naround, where 𝐾is the number of keys and 𝑁the number of\npartitions. A hashing strategy that guarantees this property is\ncalledstablehashing.\nCHAPTER13. PARTITIONING 131\nRinghashingisanexampleofstablehashing. Withringhashing,\nafunctionmapsakeytoapointonacircle. Thecircleisthensplit\nintopartitionsthatcanbeevenlyorpseudo-randomlyspaced,de-\npendingonthespeciﬁcalgorithm. Whenanewpartitionisadded,\nitcanbeshownthatmostkeysdon’tneedtobeshufﬂedaround.\nForexample,with consistenthashing1,boththepartitionidentiﬁers\nandkeysarerandomlydistributedonacircle,andeachkeyisas-\nsignedtothenextpartitionthatappearsonthecircleinclockwise\norder(seeFigure 13.3).\nFigure13.3: Withconsistenthashing,partitionidentiﬁersandkeys\nare randomly distributed on a circle, and each key is assigned to\nthenextpartitionthatappearsonthecircleinclockwiseorder.\n1https://en.wikipedia.org/wiki/Consistent_hashing\nCHAPTER13. PARTITIONING 132\nNow,whenanewpartitionisadded, onlythekeysmappedtoit\nneedtobereassigned,asshowninFigure 13.4.\nFigure13.4: AfterpartitionP4isadded,key’for’isreassignedto\nP4,buttheotherkeysarenotreassigned.\nThemaindrawbackofhashpartitioningcomparedtorangeparti-\ntioningisthatthesortorderoverthepartitionsislost. However,\nthedatawithinanindividualpartitioncanstillbesortedbasedon\nasecondarykey.",3452
53-Rebalancing.pdf,53-Rebalancing,,0
54-Static partitioning.pdf,54-Static partitioning,,0
55-Duplication.pdf,55-Duplication,"CHAPTER13. PARTITIONING 133\n13.2 Rebalancing\nWhenthenumberofrequeststothedatastorebecomestoolarge,\northedataset’ssizebecomestoolarge,thenumberofnodesserv-\ningpartitionsneedstobeincreased. Similarly,ifthedataset’ssize\nkeepsshrinking,thenumberofnodescanbedecreasedtoreduce\ncosts. The process of adding and removing nodes to balance the\nsystem’sloadiscalledrebalancing.\nRebalancingneedstobeimplementedinsuchawaytominimize\ndisruption to the data store, which needs to continue to serve re-\nquests. Hence,theamountofdatatransferredduringtherebalanc-\ningactneedstobeminimized.\n13.2.1 Static partitioning\nHere, the idea is to create way more partitions than necessary\nwhen the data store is ﬁrst initialized and assign multiple parti-\ntions per node. When a new node joins, some partitions move\nfromtheexistingnodestothenewonesothatthestoreisalways\ninabalancedstate.\nThe drawback of this approach is that the number of partitions\nis set when the data store is ﬁrst initialized and can’t be easily\nchangedafterthat. Gettingthenumberofpartitionswrongcanbe\nproblematic—toomanypartitionsaddoverheadanddecreasethe\ndata store’s performance, while too few partitions limit the data\nstore’sscalability.\n13.2.2 Dynamic partitioning\nAn alternative to creating partitions upfront is to create them on-\ndemand. One way to implementdynamic partitioning is to start\nwithasinglepartition. Whenitgrowsaboveacertainsizeorbe-\ncomes too hot, it’s split into two sub-partitions, each containing\napproximately half of the data. Then, one sub-partition is trans-\nferredtoanewnode. Similarly,iftwoadjacentpartitionsbecome\nsmallenough,theycanbemergedintoasingleone.\nCHAPTER13. PARTITIONING 134\n13.2.3 Practical considerations\nIntroducing partitions in the system adds a fair amount of com-\nplexity,evenifitappearsdeceptivelysimple. Partitionimbalance\ncaneasilybecomeaheadacheasasinglehotpartitioncanbottle-\nneckthesystemandlimititsabilitytoscale. Andaseachpartition\nis independent of the others, transactions are required to update\nmultiplepartitionsatomically.\nWe have merely scratched the surface on the topic; if you are in-\nterested to learn more about it, I recommend reading Designing\nData-IntensiveApplications2byMartinKleppmann.\n2https://www.amazon.co.uk/Designing-Data-Intensive-Applications-\nReliable-Maintainable/dp/1449373321",2350
56-Network load balancing.pdf,56-Network load balancing,"Chapter 14\nDuplication\nNowit’stimetochangegearsanddiveintoanothertoolyouhave\natyourdisposaltodesignhorizontallyscalableapplications—du-\nplication.\n14.1 Network load balancing\nArguably the easiest way to add more capacity to a service is to\ncreatemoreinstancesofitandhavesomewayofrouting, orbal-\nancing, requeststothem. Thethinkingisthatifoneinstancehas\nacertaincapacity, then2instancesshouldhaveacapacitythatis\ntwicethat.\nCreating more service instances can be a fast and cheap way to\nscaleoutastatelessservice,aslongasyouhavetakenintoaccount\nthe impact on its dependencies. For example, if every service in-\nstance needs to access a shared data store, eventually, the data\nstorewillbecomeabottleneck,andaddingmoreserviceinstances\ntothesystemwillonlystrainitfurther.\nThe routing, or balancing, of requests across a pool of servers is\nimplementedbyanetworkloadbalancer. A loadbalancer (LB)has\noneormorephysical networkinterfacecards (NIC)mappedtooneor\nmorevirtualIP(VIP)addresses. AVIP,inturn,isassociatedwitha\nCHAPTER14. DUPLICATION 136\npoolofservers. TheLBactsasamiddle-manbetweenclientsand\nservers—theclientsonlyseetheVIPexposedbytheLBandhave\nnovisibilityoftheindividualserversassociatedwithit.\nDistributing requests across servers has many beneﬁts. Because\nclients are decoupled from servers and don’t need to know their\nindividualaddresses,thenumberofserversbehindtheLBcanbe\nincreasedorreducedtransparently. Andsincemultipleredundant\nserverscaninterchangeablybeusedtohandlerequests,aLBcan\ndetect faulty ones and take them out of the pool, increasing the\nservice’savailability.\nAt a high level, a LB supports several core features beyond load\nbalancing,likeservicediscoveryandhealth-checks.\nLoad Balancing\nThe algorithms used for routing requests can vary from simple\nround-robin to more complex ones that take into account the\nservers’loadandhealth. ThereareseveralwaysforaLBtoinfer\nthe load of the servers. For example, the LB could periodically\nhitadedicated loadendpoint ofeachserverthatreturnsameasure\nof how busy the server is (e.g., CPU usage). Hitting the servers\nconstantly can be very costly though, so typically a LB caches\nthesemeasuresforsometime.\nUsingcached,andhencedelayed,metricstodistributerequeststo\nserverscancreateaherdingeffect. Supposetheloadmetricsarere-\nfreshedperiodically,andaserverthatjustjoinedthepoolreported\naloadof0—guesswhathappensnext? TheLBisgoingtohammer\nthatserveruntilthenexttimeitsloadissampled. Whenthathap-\npens,theserverismarkedasbusy,andtheLBstopssendingmore\nrequests to it, assuming it hasn’t become unavailable ﬁrst due to\nthe volume of requests sent its way. This creates a ping-pong ef-\nfectwheretheserveralternatesbetweenbeingverybusyandnot\nbusyatall.\nBecauseofthisherdingeffect,itturnsoutthatrandomlydistribut-\ningrequeststoserverswithoutaccountingfortheirloadactually\nachievesabetterloaddistribution. Doesthatmeanthatloadbal-\nCHAPTER14. DUPLICATION 137\nancingusingdelayedloadmetricsisnotpossible?\nActually, there is a way, but it requires combining load metrics\nwiththepowerofrandomness. Theideaistorandomlypicktwo\nserversfromthepoolandroutetherequesttotheleast-loadedone\nofthetwo. Thisapproach1worksremarkablywellasitcombines\ndelayedloadinformationwiththeprotectionagainstherdingthat\nrandomnessprovides.\nService Discovery\nServicediscoveryisthemechanismusedbytheLBtodiscoverthe\navailable servers in the pool it can route requests to. There are\nvariouswaystoimplementit. Forexample,asimpleapproachis\ntouseastaticconﬁgurationﬁlethatliststheIPaddressesofallthe\nservers. However, this is quite painful to manage and keep up-\nto-date. A more ﬂexible solution can be implemented with DNS.\nFinally,usingadatastoreprovidesthemaximumﬂexibilityatthe\ncostofincreasingthesystem’scomplexity.\nOne of the beneﬁts of using a dynamic service discovery mecha-\nnismisthatserverscanbeaddedandremovedfromtheLB’spool\nat any time. This is a crucial functionality that cloud providers\nleverage to implement autoscaling2, i.e., the ability to automati-\ncallyspinupandteardownserversbasedontheirload.\nHealth checks\nHealthchecksareusedbytheLBtodetectwhenaservercanno\nlongerserverequestsandneedstobetemporarilyremovedfrom\nthepool. Therearefundamentallytwocategoriesofhealthchecks:\npassiveandactive.\nApassivehealthcheck isperformedbytheLBasitroutesincoming\nrequeststotheserversdownstream. Ifaserverisn’treachable,the\nrequesttimesout,ortheserverreturnsanon-retriablestatuscode\n(e.g., 503),theLBcandecidetotakethatserveroutfromthepool.\n1https://brooker.co.za/blog/2012/01/17/two-random.html\n2https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-\nscaling",4683
57-DNS load balancing.pdf,57-DNS load balancing,,0
58-Transport layer load balancing.pdf,58-Transport layer load balancing,"CHAPTER14. DUPLICATION 138\nInstead, an active health check requires support from the down-\nstream servers, which need to expose a health endpoint signaling\nthe server’s health state. Later in the book, we will describe in\ngreaterdetailhowtoimplementsuchahealthendpoint.\n14.1.1 DNS load balancing\nNowthatweknowwhataloadbalancer’sjobis,let’stakeacloser\nlook at how it can be implemented. While you probably won’t\nhavetobuildyourownLBgiventheplethoraofoff-the-shelfsolu-\ntionsavailable,abasicknowledgeofhowloadbalancingworksis\ncrucial. LBfailuresareveryvisibletoyourservices’clientssince\nthey tend to manifest themselves as timeouts and connection re-\nsets. BecausetheLBsitsbetweenyourserviceanditsclients,italso\ncontributestotheend-to-endlatencyofrequest-responsetransac-\ntions.\nThe most basic form of load balancing can be implemented with\nDNS. Suppose you have a couple of servers that you would like\nto load balance requests over. If these servers have publicly-\nreachable IP addresses, you can add those to the service’s DNS\nrecord and have the clients pick one3when resolving the DNS\naddress,asshowninFigure 14.1.\nAlthough this works, it doesn’t deal well with failures. If one of\nthetwoserversgoesdown,theDNSserverwillhappilycontinue\nserving its IP address unaware of the failure. You can manually\nreconﬁguretheDNSrecordtotakeouttheproblematicIP,butas\nwehavelearnedinchapter 4,changesarenotappliedimmediately\nduetothenatureofDNScaching.\n14.1.2 Transport layer load balancing\nAmoreﬂexibleloadbalancingsolutioncanbeimplementedwith\naloadbalancerthatoperatesattheTCPlevelofthenetworkstack4,\n3https://en.wikipedia.org/wiki/Round-robin_DNS\n4Thisisalsoreferredtoaslayer4(L4)loadbalancingsincelayer4isthetransport\nlayerintheOSImodel.\nCHAPTER14. DUPLICATION 139\nFigure14.1: DNSloadbalancing\nthroughwhichalltrafﬁcbetweenclientsandserversneedstogo\nthrough.\nWhen a client creates a new TCP connection with a LB’s VIP,\nthe LB picks a server from the pool and henceforth shufﬂes the\npacketsbackandforthforthatconnectionbetweentheclientand\nthe server. How does the LB assign connections to the servers,\nthough?\nAconnectionisidentiﬁedbyatuple(sourceIP/port,destination\nIP/port). Typically,someformofhashingisusedtoassignacon-\nnectiontupletoaserver. Tominimizethedisruptioncausedbya\nserverbeingaddedorremovedfromthepool,consistenthashing5\nispreferredovermodularhashing.\nToforwardpacketsdownstream,theLBtranslates6eachpacket’s\n5https://www.youtube.com/watch?v=woaGu3kJ-xk\n6https://en.wikipedia.org/wiki/Network_address_translation\nCHAPTER14. DUPLICATION 140\nsourceaddresstotheLBaddressanditsdestinationaddresstothe\nserver’s address. Similarly, when the LB receives a packet from\ntheserver,ittranslatesitssourceaddresstotheLBaddressandits\ndestinationaddresstotheclient’saddress(seeFigure 14.2).\nFigure14.2: Transportlayerloadbalancing\nAsthedatagoingoutoftheserversusuallyhasagreatervolume\nthan the data coming in, there is a way for servers to bypass the\nLB and respond directly to the clients using a mechanism called\ndirectserverreturn7,butthisisbeyondthescopeofthissection.\nBecausetheLBiscommunicatingdirectlywiththeservers,itcan\ndetectunavailableones(e.g.,withapassivehealthcheck)andau-\n7https://blog.envoyproxy.io/introduction-to-modern-network-load-balancin\ng-and-proxying-a57f6ff80236",3339
59-Application layer load balancing.pdf,59-Application layer load balancing,"CHAPTER14. DUPLICATION 141\ntomaticallytakethemoutofthepoolimprovingthereliabilityof\nthebackendservice.\nAlthoughloadbalancingconnectionsattheTCPlevelisveryfast,\nthe drawback is that the LB is just shufﬂing bytes around with-\nout knowing what they actually mean. Therefore, L4 LBs gener-\nallydon’tsupportfeaturesthatrequirehigher-levelnetworkproto-\ncols,liketerminatingTLSconnectionsorbalancingHTTPsessions\nbasedoncookies. Aloadbalancerthatoperatesatahigherlevelof\nthenetworkstackisrequiredtosupporttheseadvancedusecases.\n14.1.3 Application layer load balancing\nAnapplicationlayerloadbalancer8isanHTTPreverseproxythat\nfarmsoutrequestsoverapoolofservers. TheLBreceivesanHTTP\nrequestfromaclient,inspectsit,andsendsittoabackendserver.\nTherearetwodifferentTCPconnectionsatplayhere,onebetween\nthe client and the L7 LB and another between the L7 LB and the\nserver. Because a L7 LB operates at the HTTP level, it can de-\nmultiplexindividualHTTPrequestssharingthesameTCPconnec-\ntion. This is even more important with HTTP 2, where multiple\nconcurrentstreamsaremultiplexedonthesameTCPconnection,\nand some connections can be several orders of magnitude more\nexpensivetohandlethanothers.\nThe LB can do smart things with application trafﬁc, like rate-\nlimiting requests based on HTTP headers, terminate TLS\nconnections,orforceHTTPrequestsbelongingtothesame logical\nsessiontoberoutedtothesamebackendserver.\nForexample,theLBcoulduseaspeciﬁccookietoidentifywhich\nlogicalsessionaspeciﬁcrequestbelongsto. JustlikewithaL4LB,\nthe session identiﬁer can be mapped to a server using consistent\nhashing. The caveatis thatsticky sessions can create hotspotsas\nsomesessionsaremoreexpensivetohandlethanothers.\nIfitsoundslikeaL7LBhassomeoverlappingfunctionalitywith\n8Alsoreferredtoasalayer7(L7)loadbalancersincelayer7istheapplication\nlayerintheOSImodel\nCHAPTER14. DUPLICATION 142\nan API gateway, it’s because they both are HTTP proxies, and\nthereforetheirresponsibilitiescanbeblurred.\nA L7 LB is typically used as the backend of a L4 LB to load bal-\nancerequestssentbyexternalclientsfromtheinternet(seeFigure\n14.3). AlthoughL7LBsoffermorefunctionalitythanL4LBs,they\nhavealowerthroughputincomparison,whichmakesL4LBsbet-\ntersuitedtoprotectagainstcertainDDoSattacks,likeSYNﬂoods.\nFigure14.3: AL7LBistypicallyusedasthebackendofaL4one\ntoloadbalancerequestssentbyexternalclientsfromtheinternet.\nAdrawbackofusingadedicatedload-balancingserviceisthatall\nthetrafﬁcneedstogothroughitandiftheLBgoesdown,theser-",2515
60-Geo load balancing.pdf,60-Geo load balancing,"CHAPTER14. DUPLICATION 143\nvice behind it is no longer reachable. Additionally, it’s one more\nservicethatneedstobeoperatedandscaledout.\nWhentheclientsareinternaltoanorganization,theL7LBfunction-\nalitycanalternativelybeboltedontotheclientsdirectlyusingthe\nsidecarpattern . Inthispattern,allnetworktrafﬁcfromaclientgoes\nthrough a process co-located on the same machine. This process\nimplementsloadbalancing,rate-limiting,authentication,monitor-\ning,andothergoodies.\nThesidecarprocessesformthedataplaneofa servicemesh9,which\nisconﬁguredbyacorrespondingcontrolplane. Thisapproachhas\nbeen gaining popularity with the rise of microservices in organi-\nzationsthathavehundredsofservicescommunicatingwitheach\nother. Popularsidecarproxyloadbalancersasofthiswritingare\nNGINX, HAProxy, and Envoy. The advantage of using this ap-\nproachisthatitdistributestheload-balancingfunctionalitytothe\nclients,removingtheneedforadedicatedservicethatneedstobe\nscaledoutandmaintained. Theconisasigniﬁcantincreaseinthe\nsystem’scomplexity.\n14.1.4 Geo load balancing\nWhen we ﬁrst discussed TCP in chapter 2, we talked about the\nimportanceofminimizingthelatencybetweenaclientandaserver.\nNomatterhowfasttheserveris,iftheclientislocatedontheother\nsideoftheworldfromit,theresponsetimeisgoingtobeover100\nmsjustbecauseofthenetworklatency,whichisphysicallylimited\nbythespeedoflight. Nottomentiontheincreasederrorratewhen\nsendingdataacrossthepublicinternetoverlongdistances.\nTomitigatetheseperformanceissues,youcandistributethetrafﬁc\ntodifferentdatacenterslocatedindifferentregions. Buthowdo\nyouensurethattheclientscommunicatewiththegeographically\nclosestL4loadbalancer?\nThisiswhereDNSgeoloadbalancing10comesin—it’sanexten-\n9https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-\n2774e720f7fc\n10https://landing.google.com/sre/sre-book/chapters/load-balancing-\nCHAPTER14. DUPLICATION 144\nsiontoDNSthatconsidersthelocationoftheclientinferredfrom\nits IP, and returns a list of the geographically closest L4 LB VIPs\n(seeFigure 14.4). TheLBalsoneedstotakeintoaccountthecapac-\nityofeachdatacenteranditshealthstatus.\nFigure 14.4: Geo load balancing infers the location of the client\nfromitsIP\nfrontend/",2214
61-Replication.pdf,61-Replication,,0
62-Single leader replication.pdf,62-Single leader replication,"CHAPTER14. DUPLICATION 145\n14.2 Replication\nIftheserversbehindaloadbalancerarestateless,scalingoutisas\nsimpleasaddingmoreservers. Butwhenthereisstateinvolved,\nsomeformofcoordinationisrequired.\nReplication is the process of storing a copy of the same data in\nmultiple nodes. If the data is static, replication is easy: just copy\nthedatatomultiplenodes,addaloadbalancerinfrontofit,and\nyouaredone. Thechallengeisdealingwithdynamicallychanging\ndata,whichrequirescoordinationtokeepitinsync.\nReplicationandshardingaretechniquesthatareoftencombined,\nbutareorthogonaltoeachother. Forexample,adistributeddata\nstorecandivideitsdataintoNpartitionsanddistributethemover\nKnodes. Then,astate-machinereplicationalgorithmlikeRaftcan\nbeusedtoreplicateeachpartitionRtimes(seeFigure 14.5).\nWe have already discussed one way of replicating data in chap-\nter10. This section will take a broader, but less detailed, look at\nreplication and explore different approaches with varying trade-\noffs. To keep things simple, we will assume that the dataset is\nsmallenoughtoﬁtonasinglenode,andthereforenopartitioning\nisneeded.\n14.2.1 Single leader replication\nThemostcommonapproachtoreplicatedataisthesingleleader,\nmultiplefollowers/replicasapproach(seeFigure 14.6). Inthisap-\nproach,theclientssendwritesexclusivelytotheleader,whichup-\ndatesitslocalstateandreplicatesthechangetothefollowers. We\nhaveseenanimplementationofthiswhenwediscussedtheRaft\nreplicationalgorithm.\nAt a high level, the replication can happen either fully syn-\nchronously, fully asynchronously, or as a combination of the\ntwo.\nAsynchronous replication\nInthismode,whentheleaderreceivesawriterequestfromaclient,\nCHAPTER14. DUPLICATION 146\nFigure 14.5: A replicated and partitioned data store. A node can\nbethereplicationleaderforapartitionwhilebeingafollowerfor\nanotherone.\nCHAPTER14. DUPLICATION 147\nFigure14.6: Singleleaderreplication\nitasynchronouslysendsoutrequeststothefollowerstoreplicateit\nandrepliestotheclientbeforethereplicationhasbeencompleted.\nAlthough this approach is fast, it’s not fault-tolerant. What hap-\npensiftheleadercrashesrightafteracceptingawrite,butbefore\nreplicatingittothefollowers? Inthiscase,anewleadercouldbe\nelected that doesn’t have the latest updates, leading to data loss,\nwhichisoneoftheworstpossibletrade-offsyoucanmake.\nTheotherissueisconsistency. Asuccessfulwritemightnotbevis-\niblebysomeorallreplicasbecausethereplicationhappensasyn-\nchronously. The client could send a write to the leader and later\nfailtoreadthedatafromareplicabecauseitdoesn’texistthereyet.\nTheonlyguaranteeisthatifthewritesstop,eventually,allreplicas\nwillcatchupandbeidentical(eventualconsistency).\nSynchronous replication\nSynchronousreplicationwaitsforawritetobereplicatedtoallfol-",2783
63-Multi-leader replication.pdf,63-Multi-leader replication,"CHAPTER14. DUPLICATION 148\nlowersbeforereturningaresponsetotheclient,whichcomeswith\naperformancepenalty. Ifareplicaisextremelyslow,everyrequest\nwillbeaffectedbyit. Totheextreme,ifanyreplicaisdownornot\nreachable,thestorebecomesunavailableanditcannolongerwrite\nanydata. Themorenodesthedatastorehas,themorelikelyafault\nbecomes.\nAsyoucansee,fullysynchronousorasynchronousreplicationare\nextremes that provide some advantages at the expense of others.\nMostdatastoreshavereplicationstrategiesthatuseacombination\nofthetwo. Forexample,inRaft,theleaderreplicatesitswritesto\namajoritybeforereturning aresponsetotheclient. Andin Post-\ngreSQL,youcanconﬁgureasubsetofreplicastoreceiveupdates\nsynchronously11ratherthanasynchronously.\n14.2.2 Multi-leader replication\nIn multi-leader replication, there is more than one node that can\naccept writes. This approach is used when the write throughput\nistoohighforasinglenodetohandle,orwhenaleaderneedsto\nbeavailableinmultipledatacenterstobegeographicallycloserto\nitsclients.\nThe replication happens asynchronously since the alternative\nwould defeat the purpose of using multiple leaders in the ﬁrst\nplace. This form of replication is generally best avoided when\npossibleasitintroducesalotofcomplexity. Themainissuewith\nmultiple leaders are conﬂicting writes; if the same data item is\nupdated concurrently by two leaders, which one should win?\nTo resolve conﬂicts, the data store needs to implement a conﬂict\nresolutionstrategy.\nThesimpleststrategyistodesignthesystemsothatconﬂictsare\nnot possible; this can be achieved under some circumstances if\nthe data has a homing region. For example, if all the European\ncustomerrequestsarealwaysroutedtotheEuropeandatacenter,\nwhich has a single leader, there won’t be any conﬂicting writes.\nThereisstillthepossibilityofadatacentergoingdown, butthat\n11https://www.postgresql.org/docs/9.6/runtime-config-replication.html\nCHAPTER14. DUPLICATION 149\nFigure14.7: Multi-leaderreplication\ncan be mitigated with a backup data center in the same region,\nreplicatedwithsingle-leaderreplication.\nIfassigningrequeststospeciﬁcleadersisnotpossible,andevery\nclientneedstobeabletowritetoeveryleader, conﬂictingwrites\nwillinevitablyhappen.\nOne way to deal with a conﬂict updating a record is to store the\nconcurrent writes and return them to the next client that reads\nthe record. The client will try to resolve the conﬂict and update\nthe data store with the resolution. In other words, the data store\n“pushesthecandowntheroad”totheclients.\nAlternatively,anautomaticconﬂictresolutionmethodneedstobe\nimplemented,forexample:\n•The data store could use the timestamps of the writes and\nlet the most recent one win. This is generally not reliable\nbecause the nodes’ physicalclocks aren’t perfectly synchro-\nnized. Logicalclocksarebettersuitedforthejobinthiscase.\n•Thedatastorecouldallowtheclienttouploadacustomcon-",2914
64-Leaderless replication.pdf,64-Leaderless replication,"CHAPTER14. DUPLICATION 150\nﬂictresolutionprocedure,whichcanbeexecutedbythedata\nstorewheneveraconﬂictisdetected.\n•Finally,thedatastorecouldleveragedatastructuresthatpro-\nvideautomaticconﬂictresolution,likea conﬂict-freereplicated\ndata type(CRDT). CRDTs12are data structures that can be\nreplicatedacrossmultiplenodes,allowingeachreplicatoup-\ndateitslocalversionindependentlyfromotherswhileresolv-\ninginconsistenciesinamathematicallysoundway.\n14.2.3 Leaderless replication\nWhatifanyreplicacouldacceptwritesfromclients? Inthatcase,\nthere wouldn’t be any leader(s), and the responsibility of repli-\ncating and resolving conﬂicts would be ofﬂoaded entirely to the\nclients.\nForthistowork, abasicinvariantneedstobesatisﬁed. Suppose\nthedatastorehasNreplicas. Whenaclientsendsawriterequestto\nthereplicas,itwaitsforatleastWreplicastoacknowledgeitbefore\nmovingon. Andwhenitreadsanentry,itdoessobyqueryingR\nreplicasandtakingthemostrecentonefromtheresponseset. Now,\naslongas 𝑊 + 𝑅 > 𝑁 ,thewriteandreplicasetintersect,which\nguarantees that at least one record in the read set will reﬂect the\nlatestwrite.\nThe writes arealways sentto all N replicasin parallel; the W pa-\nrameterdeterminesjustthenumberofresponsestheclienthasto\nreceive to complete the request. The data store’s read and write\nthroughputdependonhowlargeorsmallRandWare. Forexam-\nple,aworkloadwithmanyreadsbeneﬁtsfromasmallerR,butin\nturn,thatmakeswritesslowerandlessavailable.\nLike in multi-leader replication, a conﬂict resolution strategy\nneeds to be used when two or more writes to the same record\nhappenconcurrently.\nLeaderless replication is even more complex than multi-leader\nreplication, as it’s ofﬂoading the leader responsibilities to the\n12https://josephg.com/blog/crdts-are-the-future/\nCHAPTER14. DUPLICATION 151\nclients,andthereareedgecasesthataffectconsistencyevenwhen\n𝑊 + 𝑊 > 𝑁 is satisﬁed. For example, if a write succeeded on\nless than W replicas and failed on the others, the replicas areleft\ninaninconsistentstate.",2019
65-Caching.pdf,65-Caching,,0
66-Out-of-process cache.pdf,66-Out-of-process cache,"CHAPTER14. DUPLICATION 152\n14.3 Caching\nLet’stakealooknowataveryspeciﬁctypeofreplicationthatonly\noffersbesteffortguarantees: caching.\nSuppose a service requires retrieving data from a remote depen-\ndency,likeadatastore,tohandleitsrequests. Astheservicescales\nout, the dependency needs to do the same to keep up with the\never-increasingload. Acachecanbeintroducedtoreducetheload\nonthedependencyandimprovetheperformanceofaccessingthe\ndata.\nAcacheis a high-speed storage layer that temporarily buffers re-\nsponses from downstream dependencies so that future requests\ncanbeserveddirectlyfromit—it’saformofbesteffortreplication.\nForacachetobecost-effective,thereshouldbeahighprobability\nthatrequesteddatacanbefoundinit. Thisrequiresthedataaccess\npatterntohaveahighlocalityofreference,likeahighlikelihood\nofaccessingthesamedataagainandagainovertime.\n14.3.1 Policies\nWhen a cache miss occurs13, the missing data item has to be re-\nquestedfromtheremotedependency,andthecachehastobeup-\ndatedwithit. Thiscanhappenintwoways:\n•Theclient, aftergettingan“item-not-found”errorfromthe\ncache, requests the data item from the dependency and up-\ndates the cache. In this case, the cache is said to be a side\ncache.\n•Alternatively, ifthecacheis inline, thecachecommunicates\ndirectlywiththedependencyandrequeststhemissingdata\nitem. Inthiscase,theclientonlyeveraccessesthecache.\nBecause a cache has a maximum capacity for holding entries, an\nentryneedstobeevictedtomakeroomforanewonewhenitsca-\npacityisreached. Whichentrytoremovedependsontheeviction\n13Acachehitoccurswhentherequesteddatacanbefoundinthecache,whilea\ncachemissoccurswhenitcannot.\nCHAPTER14. DUPLICATION 153\npolicyusedbythecacheandtheclient’saccesspattern. Onecom-\nmonlyusedpolicyistoevicttheleastrecentlyused(LRU)entry.\nA cache also has an expiration policy that dictates for how long\ntostoreanentry. Forexample,asimpleexpirationpolicydeﬁnes\nthemaximumtimetolive(TTL)inseconds. Whenadataitemhas\nbeeninthecacheforlongerthanitsTTL,itexpiresandcansafely\nbeevicted.\nTheexpirationdoesn’tneedtooccurimmediately,though,andit\ncanbedeferredtothenexttimetheentryisrequested. Infact,that\nmight be preferable — if the dependency is temporarily unavail-\nable,andthecacheisinline,itcanopttoreturnanentrywithan\nexpiredTTLtotheclientratherthananerror.\n14.3.2 In-process cache\nThesimplestpossiblecacheyoucanbuildisanin-memorydictio-\nnarylocatedwithintheclients,suchasahash-tablewithalimited\nsizeandboundedtotheavailablememorythatthenodeoffers.\nFigure14.8: In-processcache\nBecauseeachcacheiscompletelyindependentoftheothers, con-\nsistency issues are inevitable since each client potentially sees a\nCHAPTER14. DUPLICATION 154\ndifferent version of the same entry. Additionally, an entry needs\nto be fetched once per cache, creating downstream pressure pro-\nportionaltothenumberofclients.\nThisissueisexacerbatedwhenaservicewithanin-processcacheis\nrestartedorscalesout,andeverynewlystartedinstancerequiresto\nfetchentriesdirectlyfromthedependency. Thiscancausea“thun-\nderingherd”effectwherethedownstreamdependencyishitwith\naspikeofrequests. Thesamecanhappenatrun-timeifaspeciﬁc\ndataitemthatwasn’taccessedbeforebecomesallofasuddenvery\npopular.\nRequestcoalescingcanbeusedtoreducetheimpactofathunder-\ningherd. Theideaisthatthereshouldbeatmostoneoutstanding\nrequestatthetimetofetchaspeciﬁcdataitemperin-processcache.\nForexample,ifaserviceinstanceisserving10concurrentrequests\nrequiringaspeciﬁcrecordthatisnotyetinthecache,theinstance\nwill send only a single request out to the remote dependency to\nfetchthemissingentry.\n14.3.3 Out-of-process cache\nAn external cache, shared across all service instances, addresses\nsomeofthedrawbacksofusinganin-processcacheattheexpense\nofgreatercomplexityandcost.\nBecausetheexternalcacheissharedamongtheserviceinstances,\nthere can be only a single version of each data item at any given\ntime. And although the cached item can be out-of-date, every\nclientaccessingthecachewillseethesameversion,whichreduces\nconsistency issues. The load on the dependency is also reduced\nsincethenumberoftimesanentryisaccessednolongergrowsas\nthenumberofclientsincreases.\nAlthough we have managed to decouple the clients from the de-\npendency, we have merely shifted the load to the external cache.\nIf the load increases, the cache will eventually need to be scaled\nout. As little data as possible should be moved around when\nthat happens to guarantee that the cache’s availability doesn’t\nCHAPTER14. DUPLICATION 155\nFigure14.9: Out-of-processcache\ndrop and that the number of cache misses doesn’t signiﬁcantly\nincrease. Consistent hashing, or a similar partitioning technique,\ncanbeusedtoreducetheamountofdatathatneedstobemoved\naround.\nMaintaininganexternalcachecomeswithapriceasit’syetanother\nservice that needs to be maintained and operated. Additionally,\nthelatencytoaccessitishigherthanaccessinganin-processcache\nbecauseanetworkcallisrequired.\nIf the external cache is down, how should the service react? You\nwould think it might be okay to temporarily bypass the cache\nanddirectlyhitthedependency. Butinthatcase,thedependency\nmight not be prepared to withstand a surge of trafﬁc since it’s\nusually shielded by the cache. Consequently, the external cache\nbecoming unavailable could easily cause a cascading failure\nresultinginthedependencytobecomeunavailableaswell.\nCHAPTER14. DUPLICATION 156\nThe clients can leverage an in-process cache as a defense against\nthe external cache becoming unavailable. Similarly, the depen-\ndencyalsoneedstobepreparedtohandlethesesudden“attacks.”\nLoadsheddingisatechniquethatcanbeusedhere,whichwewill\ndiscusslaterinthebook.\nWhat’s important to understand is that a cache introduces a bi-\nmodal behavior in the system14. Most of the time, the cache is\nworkingasexpected,andeverythingisﬁne;whenit’snotforwhat-\neverreason, thesystemneedstosurvivewithoutit. It’sadesign\nsmellifyoursystemcan’tcopeatallwithoutacache.\n14Rememberwhenwetalkedaboutthebi-modalbehaviorofmessagechannels\ninsection12.4? Aswewilllearnlater,youalwayswanttominimizethenumberof\nmodesinyourapplicationstomakethemsimpletounderstandandoperate.",6188
67-IV Resiliency.pdf,67-IV Resiliency,"Part IV\nResiliency\nIntroduction\nAsyouscaleoutyourapplications,anyfailurethatcanhappenwill\neventuallyhappen. Hardwarefailures,softwarecrashes,memory\nleaks — you name it. The more components you have, the more\nfailuresyouwillexperience.\nSupposeyouhaveabuggyservicethatleaks1MBofmemoryon\naverage every hundred requests. If the service does a thousand\nrequestsperday,chancesareyouwillrestarttheservicetodeploy\nanewbuildbeforetheleakreachesanysigniﬁcantsize. Butifyour\nserviceisdoing10millionrequestsperday,thenbytheendofthe\nday you lose 100 GB of memory! Eventually, the servers won’t\nhaveenoughmemoryavailableandtheywillstarttotrashdueto\ntheconstantswappingofpagesinandoutfromdisk.\nThisnastybehavioriscausedby cruelmath;givenanoperationthat\nhasacertainprobabilityoffailing,thetotalnumberoffailuresin-\ncreases with the total number of operations performed. In other\nwords, the more you scale out your system to handle more load,\nandthemoreoperationsandmovingpartsthereare,themorefail-\nuresyoursystemswillexperience.\nRememberwhenwetalkedaboutavailabilityand“nines”inchap-\nter1? Well, to guarantee just two nines, your system can be un-\navailableforupto15minaday. That’sverylittletimetotakeany\nmanualaction. Ifyoustrivefor3nines,thenyouonlyhave43min-\nutes permonthavailable. Although you can’t escape cruel math,\nyou can mitigate it by implementing self-healing mechanisms to\nreducetheimpactoffailures.\n159\nChapter15describes the causes of the most common failures in\ndistributedsystems: singlepointsoffailure,unreliablenetworks,\nslowprocesses,andunexpectedload.\nChapter16dives into resiliency patterns that help shield a ser-\nvice against failures in downstream dependencies, like timeouts,\nretries,andcircuitbreakers.\nChapter17discussesresiliencypatternsthathelpprotectaservice\nagainstupstreampressure,likeloadshedding,loadleveling,and\nrate-limiting.",1883
68-Common failure causes.pdf,68-Common failure causes,,0
69-Risk management.pdf,69-Risk management,"Chapter 15\nCommon failure causes\nInordertoprotectyoursystemsagainstfailures,youﬁrstneedto\nhaveanideaofwhatcangowrong. Themostcommonfailuresyou\nwillencounterarecausedbysinglepointsoffailure,thenetwork\nbeingunreliable,slowprocesses,andunexpectedload. Let’stake\nacloserlookatthese.\n15.1 Single point of failure\nA single point of failure is the most glaring cause of failure in a\ndistributed system; if it were to fail, that one component would\nbringdowntheentiresystemwithit. Inpractice,systemscanhave\nmultiplesinglepointsoffailure.\nAservicethatstartsupbyneedingtoreadaconﬁgurationfroma\nnon-replicateddatabaseisanexampleofasinglepointoffailure;\nifthedatabaseisn’treachable,theservicewon’tbeableto(re)start.\nAmoresubtleexampleisaservicethatexposesanHTTPAPIon\ntopofTLSusingacertiﬁcatethatneedstobemanuallyrenewed.\nIfthecertiﬁcateisn’trenewedbythetimeitexpires,thenallclients\ntryingtoconnecttoitwouldn’tbeabletoopenaconnectionwith\ntheservice.\nSingle points of failure should be identiﬁed when the system is\nCHAPTER15. COMMONFAILURECAUSES 161\narchitectedbeforetheycancauseanyharm. Thebestwaytodetect\nthemistoexamineeverycomponentofthesystemandaskwhat\nwouldhappenifthatcomponentweretofail. Somesinglepoints\noffailurecanbearchitectedaway,e.g.,byintroducingredundancy,\nwhileotherscan’t. Inthatcase,theonlyoptionleftistominimize\ntheblastradius.\n15.2 Unreliable network\nWhen a client makes a remote network call, it sends a request to\naserverandexpectstoreceivearesponsefromitawhilelater. In\nthe best case, the client receives a response shortly after sending\ntherequest. Butwhatiftheclientwaitsandwaitsandstilldoesn’t\nget a response? In that case, the client doesn’t know whether a\nresponse will eventually arrive or not. At that point it has only\ntwooptions: itcaneithercontinuetowait,orfailtherequestwith\nanexceptionorerror.\nAsdiscussedwhentheconceptoffailuredetectionwasintroduced\nin chapter 7, there are several reasons why the client hasn’t re-\nceivedaresponsesofar:\n•theserverisslow;\n•the client’s request has been dropped by a network switch,\nrouterorproxy;\n•theserverhascrashedwhileprocessingtherequest;\n•or the server’s response has been dropped by a network\nswitch,routerorproxy.\nSlownetworkcallsarethesilentkillers1ofdistributedsystems. Be-\ncausetheclientdoesn’tknowwhethertheresponseisonitswayor\nnot,itcanspendalongtimewaitingbeforegivingup,ifitgivesup\natall. Thewaitcaninturncausedegradationsthatareextremely\nhardtodebug. Inchapter 16wewillexplorewaystoprotectclients\nfromtheunreliabilityofthenetwork.\n1https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing\nCHAPTER15. COMMONFAILURECAUSES 162\n15.3 Slow processes\nFromanobserver’spointofview,averyslowprocessisnotvery\ndifferentfromonethatisn’trunningatall—neithercanperform\nusefulwork. Resourceleaksareoneofthemostcommoncauses\nofslowprocesses. Wheneveryouuseresources,especiallywhen\ntheyhavebeenleasedfromapool,thereisapotentialforleaks.\nMemoryisthemostwell-knownsourceofleaks. Amemoryleak\ncausesasteadyincreaseinmemoryconsumptionovertime. Run-\ntimeswithgarbagecollectiondon’thelpmucheither;ifareference\ntoanobjectthatisnolongerneedediskeptsomewhere,theobject\nwon’tbedeletedbythegarbagecollector.\nAmemoryleakkeepsconsumingmemoryuntilthereisnomore\nofit,atwhichpointtheoperatingsystemstartsswappingmemory\npagestodiskconstantly,whilethegarbagecollectorkicksinmore\nfrequentlytryingitsbesttoreleaseanyshredofmemory. Thecon-\nstantpagingandthegarbagecollectoreatingupCPUcyclesmake\nthe process slower. Eventually, when there is no more physical\nmemory, andthereisnomorespaceintheswapﬁle, theprocess\nwon’tbeabletoallocatemorememory,andmostoperationswill\nfail.\nMemoryisjustoneofthemanyresourcesthatcanleak. Forexam-\nple, ifyouareusingathreadpool, youcanloseathreadwhenit\nblocksonasynchronouscallthatneverreturns. Ifathreadmakes\nasynchronousblockingHTTPcallwithoutsettingatimeout,and\nthe call never returns, the thread won’t be returned to the pool.\nSince the pool has a ﬁxed size and keeps losing threads, it will\neventuallyrunoutofthreads.\nYou might think that making asynchronous calls, rather than syn-\nchronous ones, would help in the previous case. However, mod-\nernHTTPclientsusesocketpoolstoavoidrecreatingTCPconnec-\ntions and pay a hefty performance fee as discussed in chapter 2.\nIfarequestismadewithoutatimeout,theconnectionisneverre-\nturnedtothepool. Asthepoolhasalimitedsize,eventuallythere\nwon’tbeanyconnectionsleft.\nCHAPTER15. COMMONFAILURECAUSES 163\nOntopofthat,thecodeyouwriteisn’ttheonlyoneaccessingmem-\nory,threads,andsockets. Thelibrariesyourapplicationdepends\non access thesame resources, and they can do allkinds ofshady\nthings. Withoutdiggingintotheirimplementation,assumingit’s\nopenintheﬁrstplace, youcan’tbesurewhethertheycanwreak\nhavocornot.\n15.4 Unexpected load\nEverysystemhasalimittohowmuchloaditcanwithstandwith-\noutscaling. Dependingonhowtheloadincreases,youarebound\nto hit that brick wall sooner or later. One thing is an organic in-\ncreaseinloadthatgivesyouthetimetoscaleoutyourserviceac-\ncordingly,butanotherisasuddenandunexpectedspike.\nForexample,considerthenumberofrequestsreceivedbyaservice\nin a period of time. The rate and the type of incoming requests\ncan change over time, and sometimes suddenly, for a variety of\nreasons:\n•Therequestsmighthaveaseasonality—dependingonthe\nhour of the day, the service is going to get hit by users in\ndifferentcountries.\n•Some requests are much more expensive than others and\nabuse the system in ways you might have not anticipated,\nlikescrapersslurpingindatafromyoursiteatsuper-human\nspeed.\n•Some requests are malicious, like DDoS attacks that try to\nsaturateyourservice’sbandwidth,denyingaccesstotheser-\nviceforlegitimateusers.\nTo withstand unexpected load, you need to prepare beforehand.\nThepatternsinchapter 17willteachyousometechniquesonhow\ntodojustthat2.\n2Thesetechniquesmightlooksimplebutareveryeffective. DuringtheCOVID-\n19outbreak,IhavewitnessedmanyofthesystemsIwasresponsibleforatthetime\ndoublingtrafﬁcnearlyovernightwithoutcausinganyincidents.\nCHAPTER15. COMMONFAILURECAUSES 164\n15.5 Cascading failures\nYouwouldthinkthatifyoursystemhashundredsofprocesses,it\nshouldn’tmakemuchdifferenceifasmallpercentageareslowor\nunreachable. The thing about failures is that they tend to spread\nlike cancer, propagating from one process to the other until the\nwholesystemcrumblestoitsknees. Thiseffectisalsoreferredto\nas acascading failure , which occurs when a portion of an overall\nsystemfails,increasingtheprobabilitythatotherportionsfail.\nFor example, suppose there are multiple clients querying two\ndatabasereplicasAandB,whicharebehindaloadbalancer. Each\nreplica is handling about 50 transactions per second (see Figure\n15.1).\nFigure15.1: TworeplicasbehindanLB;eachishandlinghalfthe\nload.\nSuddenly, replica B becomes unavailable because of a network\nfault. TheloadbalancerdetectsthatBisunavailableandremoves\nit from its pool. Because of that, replica A has to pick up the\nslackforreplicaB,doublingtheloaditwaspreviouslyunder(see\nFigure15.2).\nAs replica A starts to struggle to keep up with the incoming re-\nquests,theclientsexperiencemorefailuresandtimeouts. Inturn,\ntheyretrythesamefailingrequestsseveraltimes,addinginsultto\ninjury.\nCHAPTER15. COMMONFAILURECAUSES 165\nFigure 15.2: When replica B becomes unavailable, A will be hit\nwithmoreload,whichcanstrainitbeyonditscapacity.\nEventually, replicaAisundersomuchloadthatitcannolonger\nserverequestspromptlyandbecomesunavailable,causingreplica\nAtoberemovedfromtheloadbalancer’spool. Inthemeantime,\nreplica B becomes available again and the load balancer puts it\nbackinthepool,atwhichpointit’sﬂoodedwithrequeststhatkill\nthereplicainstantaneously. Thisfeedbackloopofdoomcanrepeat\nseveraltimes.\nCascading failures are very hard to get under control once they\nhavestarted. Thebestwaytomitigateoneistonothaveitinthe\nﬁrstplace. Thepatternsintroducedinthenextchapterswillhelp\nyoustopthecracksinthesystemfromspreading.\n15.6 Risk management\nAswehavejustseen,adistributedsystemneedstoembracethat\nfailureswillhappenandneedstobepreparedforit. Justbecausea\nfailurehasachanceofhappeningdoesn’talwaysmeanyouhave\nnecessarily to do something about it. The day has only so many\nhours,andyouwillneedtomaketoughdecisionsaboutwhereto\nspendyourengineeringtime.\nGiven a speciﬁc failure, you have to consider its probability of\nCHAPTER15. COMMONFAILURECAUSES 166\nhappeningandtheimpactitcausestoyoursystemifitdoeshap-\npen. Bymultiplyingthetwofactorstogether,yougetariskscore3,\nwhichyoucanthenusetodecidewhichfailurestoprioritizeand\nactupon(seeFigure 15.3). Afailurethatisverylikelytohappen,\nandhasanextensiveimpact,shouldbedealtwithswiftly;onthe\nother hand, a failure with a low likelihood and low impact can\nwait.\nFigure15.3: Riskmatrix\nToaddressafailure,youcaneitherﬁndawaytoreducetheprob-\nabilityofithappening,orreduceitsimpact.\n3https://en.wikipedia.org/wiki/Risk_matrix",8999
70-Downstream resiliency.pdf,70-Downstream resiliency,,0
71-Timeout.pdf,71-Timeout,"Chapter 16\nDownstream resiliency\nIn this chapter, we will explore patterns that shield a service\nagainstfailuresinitsdownstreamdependencies.\n16.1 Timeout\nWhenyoumakeanetworkcall,youcanconﬁgureatimeouttofail\ntherequestifthereisnoresponsewithinacertainamountoftime.\nIfyoumakethecallwithoutsettingatimeout,youtellyourcode\nthatyouare100%conﬁdentthatthecallwillsucceed. Wouldyou\nreallytakethatbet?\nUnfortunately,somenetworkAPIsdon’thaveawaytosetatime-\noutintheﬁrstplace. Whenthedefaulttimeoutisinﬁnity, it’sall\ntoo easy for a client to shoot itself in the foot. As mentioned ear-\nlier, network calls that don’t return lead to resource leaks at best.\nTimeoutslimitandisolatefailures,stoppingthemfromcascading\ntotherestofthesystem. Andtheyareusefulnotjustfornetwork\ncalls, but also for requesting a resource from a pool and for syn-\nchronizationprimitiveslikemutexes.\nTodrivethepointhomeontheimportanceofsettingtimeouts,let’s\ntake a look at some concrete examples. JavaScript’s XMLHttpRe-\nquestisthewebAPItoretrievedatafromaserverasynchronously.\nCHAPTER16. DOWNSTREAMRESILIENCY 168\nItsdefaulttimeoutiszero1,whichmeansthereisnotimeout:\nvar xhr =new XMLHttpRequest() ;\nxhr.open ('GET' ,'/api' ,true );\n// No timeout by default!\nxhr.timeout =10000 ;\nxhr.onload =function () {\n// Request finished\n};\nxhr.ontimeout =function (e) {\n// Request timed out\n};\nxhr.send (null );\nClient-sidetimeoutsareascrucialasserver-sideones. Thereisa\nmaximumnumberofsocketsyourbrowser2canopenforapartic-\nularhost. Ifyoumakenetworkrequeststhatneverreturn,youare\ngoingtoexhaustthesocketpool. Whenthepoolisexhausted,you\narenolongerabletoconnecttothehost.\nThefetchweb API is a modern replacement for XMLHttpRequest\nthatusesPromises. When the fetch APIwas initially introduced,\ntherewasnowaytosetatimeoutatall3. Browsershaverecently\nadded experimental support for the Abort API4to support time-\nouts.\nconst controller =new AbortController ();\nconst signal =controller .signal ;\nconst fetchPromise =fetch (url ,{signal}) ;\n// No timeout by default!\nsetTimeout (() =>controller .abort (),10000 );\nfetchPromise .then (response =>{\n// Request finished\n})\n1https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest/t\nimeout\n2https://hpbn.co/primer-on-browser-networking/#connection-manageme\nnt-and-optimization\n3https://github.com/whatwg/fetch/issues/951\n4https://developer.mozilla.org/en-US/docs/Web/API/AbortController\nCHAPTER16. DOWNSTREAMRESILIENCY 169\nThingsaren’tmuchrosierforPython. Thepopular requestslibrary\nusesadefaulttimeoutofinﬁnity5:\n# No timeout by default!\nresponse =requests.get( 'https://github.com/' , timeout =10)\nGo’sHTTPpackage doesn’tusetimeouts6bydefault,either:\nvar client = &http.Client{\n// No timeout by default!\nTimeout: time.Second * 10,\n}\nresponse, _ := client .Get(url)\nModernHTTPclientsforJavaand.NETdoamuchbetterjoband\nusuallycomewithdefaulttimeouts. Forexample,.NETCore Http-\nClienthasadefaulttimeoutof100seconds7. It’slaxbutbetterthan\nnotsettingatimeoutatall.\nAs a rule of thumb, always set timeouts when making network\ncalls,andbewaryofthird-partylibrariesthatdonetworkcallsor\nuseinternalresourcepoolsbutdon’texposesettingsfortimeouts.\nAndifyoubuildlibraries,alwayssetreasonabledefaulttimeouts\nandmakethemconﬁgurableforyourclients.\nIdeally, you should set your timeouts based on the desired false\ntimeoutrate8. Sayyouwanttohaveabout0.1%falsetimeouts;to\nachieve that, you should set the timeout to the 99.9th percentile\noftheremotecall’sresponsetime,whichyoucanmeasureempiri-\ncally.\nYoualsowanttohavegoodmonitoringinplacetomeasuretheen-\ntirelifecycleofyournetworkcalls,likethedurationofthecall,the\nstatuscodereceived,andifatimeoutwastriggered. Wewilltalk\naboutmonitoringlaterinthebook, butthepointIwanttomake\nhereisthatyouhavetomeasurewhathappensattheintegration\n5https://requests.readthedocs.io/en/master/user/quickstart/#timeouts\n6https://github.com/golang/go/issues/24138\n7https://docs.microsoft.com/en-us/dotnet/api/system.net.http.httpclient.t\nimeout?view=netcore-3.1#remarks\n8https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-\nwith-jitter/",4157
72-State machine.pdf,72-State machine,"CHAPTER16. DOWNSTREAMRESILIENCY 170\npointsofyoursystems,oryouwon’tbeabletodebugproduction\nissueswhentheyshowup.\nIdeally,youwanttoencapsulatearemotecallwithinalibrarythat\nsetstimeoutsandmonitorsitforyousothatyoudon’thavetore-\nmembertodothiseverytimeyoumakeanetworkcall. Nomatter\nwhichlanguageyouuse,thereislikelyalibraryouttherethatim-\nplements some of the resiliency and transient fault-handling pat-\nternsintroducedinthischapter,whichyoucanusetoencapsulate\nyoursystem’snetworkcalls.\nUsingalanguage-speciﬁclibraryisnottheonlywaytowrapyour\nnetworkcalls;youcanalsoleverageareverseproxyco-locatedon\nthe same machine which intercepts all the remote calls that your\nprocess makes9. The proxy enforces timeouts and also monitors\nthecalls,relinquishingyourprocessfromtheresponsibilitytodo\nso.\n16.2 Retry\nYou know by now thata clientshould conﬁgurea timeoutwhen\nmaking a network request. But, what should it do when the re-\nquestfails,orthetimeoutﬁres? Theclienthastwooptionsatthat\npoint: itcaneitherfailfastorretrytherequestatalatertime.\nIfthefailureortimeoutwascausedbyashort-livedconnectivity\nissue,thenretryingaftersome backofftime hasahighprobabilityof\nsucceeding. However,ifthedownstreamserviceisoverwhelmed,\nretryingimmediatelywillonlymakemattersworse. Thisiswhy\nretryingneedstobesloweddownwithincreasinglylongerdelays\nbetweentheindividualretriesuntileitheramaximumnumberof\nretriesisreachedoracertainamountoftimehaspassedsincethe\ninitialrequest.\n9Wetalkedaboutthisinsection 14.1.3whendiscussingthesidecarpatternand\ntheservicemesh.\nCHAPTER16. DOWNSTREAMRESILIENCY 171\n16.2.1 Exponential backoff\nTo set the delay between retries, you can use a capped exponential\nfunction,wherethedelayisderivedbymultiplyingtheinitialback-\noff duration by a constant after each attempt, up to some maxi-\nmumvalue(thecap):\ndelay = 𝑚𝑖𝑛(cap ,initial-backoff ⋅ 2attempt)\nForexample, ifthecapissetto8seconds, andtheinitialbackoff\nduration is 2 seconds, then the ﬁrst retry delay is 2 seconds, the\nsecondis4seconds,thethirdis8seconds,andanyfurtherdelay\nwillbecappedto8seconds.\nAlthough exponential backoff does reduce the pressure on the\ndownstream dependency, there is still a problem. When the\ndownstreamserviceistemporarilydegraded, it’slikelythatmul-\ntipleclientsseetheirrequestsfailingaroundthesametime. This\ncausestheclientstoretrysimultaneously,hittingthedownstream\nservice with load spikes that can further degrade it, as shown in\nFigure16.1.\nFigure16.1: Retrystorm\nCHAPTER16. DOWNSTREAMRESILIENCY 172\nToavoidthisherdingbehavior,youcanintroducerandomjitter10\ninthedelaycalculation. Withit, theretriesspreadoutovertime,\nsmoothingouttheloadtothedownstreamservice:\ndelay = 𝑟𝑎𝑛𝑑𝑜𝑚(0, 𝑚𝑖𝑛( cap ,initial-backoff ⋅ 2attempt))\nActively waiting and retrying failed network requests isn’t the\nonly way to implement retries. In batch applications that don’t\nhave strict real-time requirements, a process can park failed\nrequestsintoa retryqueue . Thesameprocess,orpossiblyanother,\nreadsfromthesamequeuelaterandretriestherequests.\nJustbecauseanetworkcallcanberetrieddoesn’tmeanitshould\nbe. Iftheerrorisnotshort-lived,forexample,becausetheprocess\nisnotauthorizedtoaccesstheremoteendpoint,thenitmakesno\nsense to retry the request since it will fail again. In this case, the\nprocessshouldfailfastandcancelthecallrightaway.\nYoushouldalsonotretryanetworkcallthatisn’tidempotent,and\nwhose side effects can affect your application’s correctness. Sup-\nposeaprocessismakingacalltoapaymentproviderservice,and\nthecalltimesout;shoulditretryornot? Theoperationmighthave\nsucceededandretryingwouldchargetheaccounttwice,unlessthe\nrequestisidempotent.\n16.2.2 Retry ampliﬁcation\nSuppose that handling a request from a client requires it to go\nthrough a chain of dependencies. The client makes a call to ser-\nvice A, which to handle the request talks to service B, which in\nturntalkstoserviceC.\nIftheintermediaterequestfromserviceBtoserviceCfails,should\nBretrytherequestornot? Well,ifBdoesretryit,Awillperceivea\nlongerexecutiontimeforitsrequest,whichinturnmakesitmore\nlikelytohitA’stimeout. Ifthathappens,Aretriesitsrequestagain,\nmakingitmorelikelyfortheclienttohititstimeoutandretry.\n10https://aws.amazon.com/blogs/architecture/exponential-backoff-and-\njitter/\nCHAPTER16. DOWNSTREAMRESILIENCY 173\nHavingretriesatmultiplelevelsofthedependencychaincanam-\nplifythenumberofretries;thedeeperaserviceisinthechain,the\nhighertheloaditwillbeexposedtoduetotheampliﬁcation(see\nFigure16.2).\nFigure16.2: Retryampliﬁcationinaction\nAndifthepressuregetsbadenough,thisbehaviorcaneasilybring\ndownthewholesystem. That’swhywhenyouhavelongdepen-\ndencychains,youshouldonlyretryatasinglelevelofthechain,\nandfailfastinalltheotherones.\n16.3 Circuit breaker\nSupposeyourserviceusestimeoutstodetectcommunicationfail-\nureswithadownstreamdependency,andretriestomitigatetran-\nsientfailures. Ifthefailuresaren’ttransientandthedownstream\ndependency keeps being unresponsive, what should it do then?\nIftheservicekeepsretryingfailedrequests,itwillnecessarilybe-\ncomeslowerforitsclients. Inturn,thisslownesscanpropagateto\ntherestofthesystemandcausecascadingfailures.\nTo deal with non-transient failures, we need a mechanism that\ndetectslong-termdegradationsofdownstreamdependenciesand\nstopsnewrequestsfrombeingsentdownstreamintheﬁrstplace.\nAfter all, the fastest network call is the one you don’t have to\nmake. Thismechanismisalsocalledacircuitbreaker,inspiredby\nthesamefunctionalityimplementedinelectricalcircuits.\nA circuit breaker’s goal is to allow a sub-system to fail without\nbringing down the whole system with it. To protect the system,\ncalls to the failing sub-system are temporarily blocked. Later,\nwhenthesub-systemrecoversandfailuresstop,thecircuitbreaker\nCHAPTER16. DOWNSTREAMRESILIENCY 174\nallowscallstogothroughagain.\nUnlike retries, circuit breakers prevent network calls entirely,\nwhich makes the pattern particularly useful for long-term degra-\ndations. Inotherwords, retriesarehelpfulwhentheexpectation\nisthatthenextcallwillsucceed,whilecircuitbreakersarehelpful\nwhentheexpectationisthatthenextcallwillfail.\n16.3.1 State machine\nThecircuitbreakerisimplementedasastatemachinethatcanbe\ninoneofthreestates: open,closedandhalf-open(seeFigure 16.3).\nFigure16.3: Circuitbreakerstatemachine\nIn the closed state, the circuit breaker is merely acting as a pass-\nthroughfornetworkcalls. Inthisstate, thecircuitbreakertracks\nthe number of failures, like errors and timeouts. If the number\ngoesoveracertainthresholdwithinapredeﬁnedtime-interval,the\ncircuitbreakertripsandopensthecircuit.\nWhen the circuit is open, network calls aren’t attempted and fail\nimmediately. As an open circuit breaker can have business im-\nplications,youneedtothinkcarefullywhatshouldhappenwhen\na downstream dependency is down. If the down-stream depen-\ndencyisnon-critical,youwantyourservicetodegradegracefully,\nCHAPTER16. DOWNSTREAMRESILIENCY 175\nratherthantostopentirely.\nThinkofanairplanethatlosesoneofitsnon-criticalsub-systems\ninﬂight;itshouldn’tcrash,butrathergracefullydegradetoastate\nwhere the plane can still ﬂy and land. Another example is Ama-\nzon’s front page; if the recommendation service is not available,\nthe page should render without recommendations. It’s a better\noutcomethantofailtherenderingofthewholepageentirely.\nAftersometimehaspassed,thecircuitbreakerdecidestogivethe\ndownstream dependency another chance, and transitions to the\nhalf-open state. In the half-open state, the next call is allowed to\npass-throughtothedownstreamservice. Ifthecallsucceeds,the\ncircuitbreakertransitionstotheclosedstate;ifthecallfailsinstead,\nittransitionsbacktotheopenstate.\nThat’sreallyallthereistounderstandhowacircuitbreakerworks,\nbut the devil is in the details. How many failures are enough to\nconsideradownstreamdependencydown? Howlongshouldthe\ncircuit breaker wait to transition from the open to the half-open\nstate? Itreallydependsonyourspeciﬁccase; onlybyusingdata\naboutpastfailurescanyoumakeaninformeddecision.",8075
73-Upstream resiliency.pdf,73-Upstream resiliency,,0
74-Rate-limiting.pdf,74-Rate-limiting,"Chapter 17\nUpstream resiliency\nSo far, we have discussed patterns that protect against down-\nstream failures, like failures to reach an external dependency.\nIn this chapter, we will shift gears and discuss mechanisms to\nprotectagainstupstreampressure.\n17.1 Load shedding\nAserverhasverylittlecontroloverhowmanyrequestsitreceives\natanygiventime,whichcandeeplyimpactitsperformance.\nTheoperatingsystemhasaconnectionqueueperportwithalim-\nitedcapacitythat,whenreached,causesnewconnectionattempts\ntoberejectedimmediately. Buttypically,underextremeload,the\nservercrawlstoahaltbeforethatlimitisreachedasitstarvesout\nofresourceslikememory,threads,sockets,orﬁles. Thiscausesthe\nresponsetimetoincreasetothepointtheserverbecomesunavail-\nabletotheoutsideworld.\nWhenaserveroperatesatcapacity,thereisnogoodreasonforitto\nkeepacceptingnewrequestssincethatwillonlyendupdegrading\nit. Inthatcase,theprocessshouldstartrejectingexcessrequests1\n1https://aws.amazon.com/builders-library/using-load-shedding-to-avoid-\nCHAPTER17. UPSTREAMRESILIENCY 177\nsothatitcanfocusontheonesitisalreadyprocessing.\nThe deﬁnition of overload depends on your system, but the gen-\neralideaisthatitshouldbemeasurableandactionable. Forexam-\nple,thenumberofconcurrentrequestsbeingprocessedisagood\ncandidatetomeasureaserver’sload;allyouhavetodoistoincre-\nmentacounterwhenanewrequestcomesinanddecreaseitwhen\ntheserverhasprocesseditandsentbackaresponsetotheclient.\nWhentheserverdetectsthatit’soverloaded,itcanrejectincoming\nrequestsbyfailingfastandreturninga 503(ServiceUnavailable) sta-\ntuscodeintheresponse. Thistechniqueisalsoreferredtoasload\nshedding. The server doesn’t necessarily have to reject arbitrary\nrequests though; for example, if different requests have different\npriorities,theservercouldrejectonlythelower-priorityones.\nUnfortunately,rejectingarequestdoesn’tcompletelyofﬂoadfrom\nthe server the cost of handling it. Depending on how the rejec-\ntionisimplemented,theservermightstillhavetopaythepriceof\nopeningaTLSconnectionandreadtherequestjusttoﬁnallyreject\nit. Hence, load shedding can only help so much, and if the load\nkeeps increasing, eventually, the cost of rejecting requests takes\nover,andtheservicestartstodegrade.\n17.2 Load leveling\nLoadlevelingisanalternativetoloadshedding,whichcanbeused\nwhenclientsdon’texpectaresponsewithinashorttimeframe.\nTheideaistointroduceamessagingchannelbetweentheclients\nand the service. The channel decouples the load directed to the\nservicefromitscapacity, allowingtheservicetoprocessrequests\natitsownpace—ratherthanrequestsbeingpushedtotheservice\nbytheclients,theyarepulledbytheservicefromthechannel. This\npattern is referred to as load leveling and it’s well suited to fend\noffshort-livedspikes,whichthechannelsmoothesout(seeFigure\n17.1).\noverload\nCHAPTER17. UPSTREAMRESILIENCY 178\nFigure17.1: Thechannelsmoothsouttheloadfortheconsuming\nservice.\nLoad-sheddingandloadlevelingdon’taddressanincreaseinload\ndirectly, but rather protect a service from getting overloaded. To\nhandlemoreload,theserviceneedstobescaledout. Thisiswhy\nthese protection mechanisms are typically combined with auto-\nscaling2, which detects that the service is running hot and auto-\nmaticallyincreasesitsscaletohandletheadditionalload.\n17.3 Rate-limiting\nRate-limiting, or throttling, is a mechanism that rejects a request\nwhen a speciﬁc quota is exceeded. A service can have multiple\nquotas,likeforthenumberofrequestsseen,orthenumberofbytes\nreceived within a time interval. Quotas are typically applied to\nspeciﬁcusers,APIkeys,orIPaddresses.\nFor example, if a service with a quota of 10 requests per second,\nper API key, receives on average 12 requests per second from a\nspeciﬁc API key, it will on average, reject 2 requests per second\ntaggedwiththatAPIkey.\nWhenaservicerate-limitsarequest,itneedstoreturnaresponse\nwithaparticularerrorcodesothatthesenderknowsthatitfailed\nbecauseaquotahasbeenbreached. ForserviceswithHTTPAPIs,\n2https://en.wikipedia.org/wiki/Autoscaling\nCHAPTER17. UPSTREAMRESILIENCY 179\nthemostcommonwaytodothatisbyreturningaresponsewith\nstatuscode 429(TooManyRequests) . Theresponseshouldinclude\nadditional details about which quota has been breached and by\nhowmuch;itcanalsoincludea Retry-After headerindicatinghow\nlongtowaitbeforemakinganewrequest:\nHTTP/1.1 429 Too Many Requests\nRetry-After: 60\nIftheclientapplicationplaysbytherules,itstopshammeringthe\nserviceforsometime,protectingitfromnon-malicioususersmo-\nnopolizingitbymistake. Thisprotectsagainstbugsintheclients\nthat, for one reason or another, cause a client to repeatedly hit a\ndownstreamservicefornogoodreason.\nRate-limiting is also used to enforce pricing tiers; if a user wants\ntousemoreresources,theyalsoneedtobepreparedtopaymore.\nThisishowyoucanofﬂoadyourservice’scosttoyourusers: have\nthem pay proportionally to their usage and enforce pricing tiers\nwithquotas.\nYou would think that rate-limiting also offers strong protection\nagainst a denial-of-service3(DDoS) attack, but it only partially\nprotects a service from it. Nothing forbids throttled clients from\ncontinuing to hammer a service after getting 429s. And no, rate-\nlimited requests aren’t free either — for example, to rate-limit a\nrequestbyAPIkey,theservicehastopaythepricetoopenaTLS\nconnection,andtotheveryleastdownloadpartoftherequestto\nreadthekey. Althoughrate-limitingdoesn’tfullyprotectagainst\nDDoSattacks,itdoeshelpreducetheirimpact.\nEconomies of scale are the only true protection against DDoS at-\ntacks. Ifyourunmultipleservicesbehindonelargefrontendser-\nvice, no matter which of the services behind it are attacked, the\nfrontend service will be able to withstand the attack by rejecting\nthe trafﬁc upstream. The beauty of this approach is that the cost\nofrunningthefrontendserviceisamortizedacrossalltheservices\nthatareusingit.\n3https://en.wikipedia.org/wiki/Denial-of-service_attack",5928
75-Single-process implementation.pdf,75-Single-process implementation,"CHAPTER17. UPSTREAMRESILIENCY 180\nAlthough rate-limiting has some similarities to load shedding,\nthey are different concepts. Load shedding rejects trafﬁc based\nonthelocalstateofaprocess,likethenumberofrequestsconcur-\nrentlyprocessedbyit;rate-limitinginsteadshedstrafﬁcbasedon\nthe global state of the system, like the total number of requests\nconcurrently processed for a speciﬁc API key across all service\ninstances.\n17.3.1 Single-process implementation\nTheimplementationofrate-limitingisinterestinginitsownright,\nandit’swellworthspendingsometimestudyingit,asasimilarap-\nproachcanbeappliedtootherusecases. Wewillstartwithsingle-\nprocessimplementationﬁrstandthenproceedwithadistributed\none.\nSupposewewanttoenforceaquotaof2requestsperminute,per\nAPI key. A naive approach would be to use a doubly-linked list\nper API key, where each list stores the timestamps of the last N\nrequests received. Every time a new request comes in, an entry\nis appended to the list with its corresponding timestamp. Then\nperiodically,entriesolderthanaminutearepurgedfromthelist.\nBy keeping track of the list’s length, the process can rate-limits\nincoming requests by comparing it with the quota. The problem\nwiththisapproachisthatitrequiresalistperAPIkey, whichbe-\ncomesquicklyexpensiveintermsofmemoryasitgrowswiththe\nnumberofrequestsreceived.\nToreducememoryconsumption,weneedtocomeupwithaway\ntocompressthestoragerequired. Onewaytodothisistodivide\ntimeintobucketsofﬁxedtimeduration,forexampleof1minute,\nandkeeptrackofhowmanyrequestshavebeenseenwithineach\nbucket(seeFigure 17.2).\nAbucketcontainsanumericalcounter. Whenanewrequestcomes\nin,itstimestampisusedtodeterminethebucketitbelongsto. For\nexample,ifarequestarrivesat12.00.18,thecounterofthebucket\nforminute“12.00”isincrementedby1(seeFigure 17.3).\nCHAPTER17. UPSTREAMRESILIENCY 181\nFigure 17.2: Buckets divide time into 1-minute intervals, which\nkeeptrackofthenumberofrequestsseen.\nFigure17.3: Whenanewrequestcomesin,itstimestampisused\ntodeterminethebucketitbelongsto.\nCHAPTER17. UPSTREAMRESILIENCY 182\nWithbucketing,wecancompresstheinformationaboutthenum-\nberofrequestsseeninawaythatdoesn’tgrowasthenumberof\nrequests does. Now that we have a memory-friendly representa-\ntion,howcanweuseittoimplementrate-limiting? Theideaisto\nuse a sliding window that moves in real-time across the buckets,\nkeepingtrackofthenumberofrequestswithinit.\nTheslidingwindowrepresentstheintervaloftimeusedtodecide\nwhethertorate-limitornot. Thewindow’slengthdependsonthe\ntimeunitusedtodeﬁnethequota,whichinourcaseis1minute.\nBut,thereisacaveat: aslidingwindowcanoverlapwithmultiple\nbuckets. Toderivethenumberofrequestsundertheslidingwin-\ndow,wehavetocomputeaweightedsumofthebucket’scounters,\nwhereeachbucket’sweightisproportionaltoitsoverlapwiththe\nslidingwindow(seeFigure 17.4).\nFigure17.4: Abucket’sweightisproportionaltoitsoverlapwith\ntheslidingwindow.\nAlthoughthisisanapproximation,it’sareasonablygoodonefor\nour purposes. And, it can be made more accurate by increasing\nthe granularity of the buckets. For example, you can reduce the\napproximationerrorusing30-secondbucketsratherthan1-minute\nones.\nWeonlyhavetostoreasmanybucketsastheslidingwindowcan\noverlapwithatanygiventime. Forexample,witha1-minutewin-\ndowanda1-minutebucketlength,theslidingwindowcantouch",3329
76-Bulkhead.pdf,76-Bulkhead,"CHAPTER17. UPSTREAMRESILIENCY 183\natmost2buckets. Andifitcantouchatmosttwobuckets,there\nis no point to store the third oldest bucket, the fourth oldest one,\nandsoon.\nTo summarize, this approach requires two counters per API key,\nwhich is much more efﬁcient in terms of memory than the naive\nimplementationstoringalistofrequestsperAPIkey.\n17.3.2 Distributed implementation\nWhen more than one process accepts requests, the local state no\nlongercutsit,asthequotaneedstobeenforcedonthetotalnumber\nofrequestsperAPIkeyacrossallserviceinstances. Thisrequires\nashareddatastoretokeeptrackofthenumberofrequestsseen.\nAs discussed earlier, we need to store two integers per API key,\none for each bucket. When a new request comes in, the process\nreceivingitcouldfetchthebucket,updateitandwriteitbacktothe\ndatastore. But, thatwouldn’tworkbecausetwoprocessescould\nupdatethesamebucketconcurrently,whichwouldresultinalost\nupdate. Toavoidanyraceconditions,thefetch,update,andwrite\noperationsneedtobepackagedintoasingletransaction.\nAlthough this approach is functionally correct, it’s costly. There\naretwoissueshere: transactionsareslow, andexecutingoneper\nrequestwouldbecrazyexpensiveasthedatabasewouldhaveto\nscalelinearlywiththenumberofrequests. Ontopofthat,foreach\nrequestaprocessreceives, itneedstodoanoutgoingcalltoare-\nmotedatastore. Whatshoulditdoifitfails?\nLet’s address these issues. Rather than using transactions, we\ncan use a single atomic get-and-increment operation that most\ndata stores provide. Alternatively, the same can be emulated\nwith acompare-and-swap4. Atomic operations have much better\nperformancethantransactions.\nNow,ratherthanupdatingthedatabaseoneachrequest,thepro-\ncesscanbatchbucketupdatesinmemoryforsometime,andﬂush\nthem asynchronously to the database at the end of it (see Figure\n4https://en.wikipedia.org/wiki/Compare-and-swap\nCHAPTER17. UPSTREAMRESILIENCY 184\n17.5). This reduces the shared state’s accuracy, but it’s a good\ntrade-off as it reduces the load on the database and the number\nofrequestssenttoit.\nFigure 17.5: Servers batch bucket updates in memory for some\ntime, and ﬂush them asynchronously to the database at the end\nofit.\nWhathappensifthedatabaseisdown? RemembertheCAPtheo-\nrem’sessence: whenthereisanetworkfault, wecaneithersacri-\nﬁceconsistencyandkeepoursystemup,ormaintainconsistency\nand stop serving requests. In our case, temporarily rejecting all\nincomingrequestsjustbecausethedatabaseusedforrate-limiting\nisnotreachablecouldbeverydamagingtothebusiness. Instead,\nit’ssafertokeepservingrequestsbasedonthelaststatereadfrom\nthestore.\n17.4 Bulkhead\nThegoalofthebulkheadpatternistoisolateafaultinonepartofa\nservicefromtakingtheentireservicedownwithit. Thepatternis\nnamedafterthepartitionsofaship’shull. Ifonepartitionisdam-\naged and ﬁlls up with water, the leak is isolated to that partition\nanddoesn’tspreadtotherestoftheship.\nSomeclientscancreatemuchmoreloadonaservicethanothers.\nWithoutanyprotections,asinglegreedyclientcanhammerthesys-\nCHAPTER17. UPSTREAMRESILIENCY 185\ntemanddegradeeveryotherclient. Wehaveseensomepatterns,\nlikerate-limiting,thathelppreventasingleclientfromusingmore\nresourcesthanitshould. Butrate-limitingisnotbulletproof. You\ncanrate-limitclientsbasedonthenumberofrequestspersecond;\nbut what if a client sends very heavy or poisonous requests that\ncausetheserverstodegrade? Inthatcase,rate-limitingwouldn’t\nhelp much as the issue is intrinsic with the requests sent by that\nclient, which could eventually lead to degrading the service for\neveryotherclient.\nWhen everything else fails, the bulkhead pattern provides guar-\nanteedfaultisolationbydesign. Theideaistopartitionashared\nresource, like a pool of service instances behind a load balancer,\nandassigneachuseroftheservicetoaspeciﬁcpartitionsothatits\nrequests can only utilize resources belonging to the partition it’s\nassignedto.\nConsequently,aheavyorpoisonoususercanonlydegradethere-\nquests of users within the same partition. For example, suppose\nthereare10instancesofaservicebehindaloadbalancer,whichare\ndividedinto5partitions(seeFigure 17.6). Inthatcase,aproblem-\naticusercanonlyeverimpact20percentoftheservice’sinstances.\nThe problem is that the unlucky users who happen to be on the\nsamepartitionastheproblematiconearefullyimpacted. Canwe\ndobetter?\nFigure17.6: Serviceinstancespartitionedinto5partitions\nWecanintroduce virtualpartitions thatarecomposedofarandom\nCHAPTER17. UPSTREAMRESILIENCY 186\nsubsetofinstances. Thiscan make itmuch moreunlikelyforan-\notherusertobeallocatedtotheexactsamevirtualpartition.\nInourexample,wecanextract45combinationsof2instances(vir-\ntual partitions) from a pool of 10 instances. When a virtual par-\ntition is degraded, other virtual partitions are only partially im-\npactedastheydon’tfullyoverlap(seeFigure 17.7). Ifyoucombine\nthis with a health check on the load balancer, and a retry mecha-\nnismontheclientside,whatyougetismuchbetterfaultisolation.\nFigure 17.7: Virtual partitions are far less likely to fully overlap\nwitheachother.\nYou need to be careful when applying the bulkhead pattern; if\nyoutakeittoofarandcreatetoomanypartitions,youloseallthe\neconomy-of-scalebeneﬁtsofsharingcostlyresourcesacrossaset\nofusersthatareactiveatdifferenttimes.\nYoualsointroduceascalingproblem. Scalingissimplewhenthere\narenopartitionsandeveryusercanbeservedbyanyinstance,as\nyou can just add more instances. It’s not that easy with a parti-\ntioned pool of instances as some partitions are much hotter than\nothers.",5551
77-Health endpoint.pdf,77-Health endpoint,,0
78-Watchdog.pdf,78-Watchdog,"CHAPTER17. UPSTREAMRESILIENCY 187\n17.5 Health endpoint\nSofar, wehaveexploredpatternsthatallowaprocesstoshedor\nrejectincomingrequests. Thosearemitigationsaservercanapply\nonly after it has received a request. Wouldn’t it be nice to have\nawaytocontroltheincomingtrafﬁcsothatitdoesn’treachade-\ngradedserverintheﬁrstplace?\nIftheserverisbehindaloadbalancerandcancommunicatethat\nit’soverloaded, thebalancercanstopsendingrequeststoit. The\nprocesscanexposeahealthendpointthatwhenqueriedperforms\nahealthcheckthateitherreturns 200(OK)iftheprocesscanserve\nrequests,oranerrorcodeifit’soverloadedanddoesn’thavemore\ncapacitytoserverequests.\nThe health endpoint is periodically queried by the load balancer.\nIf the endpoint returns an error, the load balancer considers the\nprocess unhealthy and takes it out of the pool. Similarly, if the\nrequesttothehealthendpointtimesout,theprocessisalsotaken\noutofthepool.\nHealthchecksarecriticaltoachievinghighavailability;ifyouhave\naservicewith10serversandoneisunresponsiveforsomereason,\nthen 10% of the requests will fail, which will cause the service’s\navailabilitytodropto90%.\nLet’s have a look at the different types of health checks that you\ncanleverageinyourservice.\n17.5.1 Health checks\nAlivenesshealthtest isthemostbasicformofcheckingthehealth\nof a process. The load balancer simply performs a basic HTTP\nrequest to see whether the process replies with a 200 (OK)status\ncode.\nAlocalhealthtest checkswhethertheprocessisdegradedorinsome\nfaulty state. The process’s performance typically degrades when\nalocalresource,likememory,CPU,ordisk,iseithercloseenough\ntobefullysaturated,oriscompletelysaturated. Todetectadegra-\nCHAPTER17. UPSTREAMRESILIENCY 188\ndation,theprocesscomparesoneormorelocalmetrics,likemem-\noryavailableorremainingdiskspace,withsomeﬁxedupperand\nlower-boundthresholds. Whenametricisaboveanupper-bound\nthreshold, orbelow a lower-bound one, the process reportsitself\nasunhealthy.\nAmoreadvanced,andalsoharderchecktogetright,isthe depen-\ndencyhealthcheck . Thistypeofhealthcheckdetectsadegradation\ncausedbyaremotedependency,likeadatabase,thatneedstobe\naccessedtohandleincomingrequests. Theprocessmeasuresthe\nresponsetime,timeouts,anderrorsoftheremotecallsdirectedto\nthe dependency. If any measure breaks a predeﬁned threshold,\nthe process reports itself as unhealthy to reduce the load on the\ndownstreamdependency.\nButherebedragons5: ifthedownstreamdependencyistemporar-\nily unreachable, or the health-check has a bug, then it’s possible\nthatalltheprocessesbehindtheloadbalancerfailthehealthcheck.\nIn that case, a naive load balancer would just take all service in-\nstancesoutofrotation,bringingtheentireservicedown!\nA smart load balancer instead detects that a large fraction of the\nserviceinstancesisbeingreportedasunhealthyandconsidersthe\nhealthchecktonolongerbereliable. Ratherthancontinuingtore-\nmoveprocessesfromthepool,itstartstoignorethehealth-checks\naltogether so that new requests can be sent to any process in the\npool.\n17.6 Watchdog\nOneofthemainreasonstobuilddistributedservicesistobeable\ntowithstandsingle-processfailures. Sinceyouaredesigningyour\nsystem under the assumption that any process can crash at any\ntime,yourserviceneedstobeabletodealwiththateventuality.\nForaprocess’scrashtonotaffectyourservice’shealth,youshould\nensureideallythat:\n5https://aws.amazon.com/builders-library/implementing-health-checks/\nCHAPTER17. UPSTREAMRESILIENCY 189\n•there are other processes that are identical to the one that\ncrashedthatcanhandleincomingrequests;\n•requestsarestatelessandcanbeservedbyanyprocess;\n•anynon-volatilestateisstoredonaseparateanddedicated\ndatastoresothatwhentheprocesscrashesitsstateisn’tlost;\n•all shared resources are leased so that when the process\ncrashes,theleasesexpireandtheresourcescanbeaccessed\nbyotherprocesses;\n•the service is always running slightly over-scaled to with-\nstandtheoccasionalindividualprocessfailures.\nBecause crashes are inevitable and your service is prepared for\nthem, you don’t have to come up with complex recovery logic\nwhenaprocessgetsintosomeweirddegradedstate—youcanjust\nletitcrash. Atransientbutrarefailurecanbehardtodiagnoseand\nﬁx. Crashing and restarting the affected process gives operators\nmaintainingtheservicesomebreathingroomuntiltheroot-cause\ncanbeidentiﬁed,givingthesystemakindofself-healingproperty.\nImaginethatalatentmemoryleakcausestheavailablememoryto\ndecrease over time. When a process doesn’t have more physical\nmemoryavailable,itstartstoswapbackandforthtothepageﬁle\nondisk. Thisswappingisextremelyexpensiveanddegradesthe\nprocess’sperformancedramatically. Ifleftunchecked,thememory\nleak would eventuallybring allprocessesrunningtheservice on\ntheirknees. Wouldyouratherhavetheprocessesdetecttheyare\ndegradedandrestartthemselves,ortrytodebugtherootcausefor\nthedegradationat3AM?\nToimplementthispattern,aprocessshouldhaveaseparateback-\nground thread that wakes up periodically — a watchdog — that\nmonitors its health. For example, the watchdog could monitor\nthe available physical memory left. When any monitored metric\nbreachesaconﬁguredthreshold,thewatchdogconsidersthepro-\ncessdegradedanddeliberatelyrestartsit.\nThewatchdog’simplementationneedstobewell-testedandmoni-\ntoredsinceabugcouldcausetheprocessestorestartcontinuously.",5341
79-V Testing and operations.pdf,79-V Testing and operations,"Part V\nTesting and operations\nIntroduction\nWhenallresiliencymechanismsfail,humansoperatorsarethelast\nline of defense. Historically, developers, testers, and operators\nwere part of different teams. The developers handed over their\nsoftwaretoateamofQAengineersresponsiblefortestingit. When\nthe software passed that stage, it moved to an operations team\nresponsible for deploying it to production, monitoring it, and re-\nspondingtoalerts.\nThis model is being phased out in the industry as it has become\ncommonplaceforthedevelopmentteamtoalsoberesponsiblefor\ntestingandoperatingthesoftwaretheywrite. Thisforcesthede-\nvelopers to embrace an end-to-end view of their applications, ac-\nknowledging that faults are inevitable and need to be accounted\nfor.\nChapter18describesthedifferenttypesoftests—unit,integration,\nandend-to-endtests—youcanleveragetoincreasetheconﬁdence\nthatyourdistributedapplicationsworkasexpected.\nChapter19dives into continuous delivery and deployment\npipelinesusedtoreleasechangessafelyandefﬁcientlytoproduc-\ntion.\nChapter20discusseshowtousemetricsandservice-levelindica-\ntorstomonitorthehealthofdistributedsystems. Itthendescribes\nhowtodeﬁneobjectivesthattriggeralertswhenbreached. Finally,\nthechapterlistsbestpracticesfordashboarddesign.\nChapter21introducestheconceptofobservabilityandhowitre-\n192\nlatestomonitoring. Thenitdescribeshowtracesandlogscanhelp\ndevelopersdebugtheirsystems.",1431
80-Scope.pdf,80-Scope,"Chapter 18\nTesting\nThelongerittakestodetectabug,themoreexpensiveitbecomes\ntoﬁxit. Testingisallaboutcatchingbugsasearlyaspossible,al-\nlowingdeveloperstochangetheimplementationwithconﬁdence\nthat existing functionality won’t break, increasing the speed of\nrefactorings, shipping new features, and other changes. As a\nwelcome side effect, testing also improves the system’s design\nsincedevelopershavetoputthemselvesintheusers’shoestotest\niteffectively. Testsalsoprovideup-to-datedocumentation.\nUnfortunately, because it’s impossible to predict all the ways a\ncomplex distributed application can fail, testing only provides\nbest-effort guarantees that the code being tested is correct and\nfault-tolerant. No matter how exhaustive the test coverage is,\ntests can only cover failures developers can imagine, not the\nkind of complex emergent behavior that manifests itself only in\nproduction1.\nAlthoughtestscan’tgiveyoucompleteconﬁdencethatyourcode\nisbug-free,theycertainlydoagoodjobatdetectingfailurescenar-\niosyouareawareofandvalidatingexpectedbehaviors. Asarule\nof thumb, if you want to be conﬁdent that your implementation\n1CindySridharanwroteagreatblogpostseriesonthetopicat https://copyco\nnstruct.medium.com/testing-microservices-the-sane-way-9bb31d158c16\nCHAPTER18. TESTING 194\nbehavesinacertainway,youhavetoaddatestforit.\n18.1 Scope\nTestscomeindifferentshapesandsizes. Tobeginwith, weneed\nto distinguish between code paths a test is actually testing (aka\nsystem under test or SUT) from the ones that are being run. The\nSUTrepresentsthescopeofthetest,anddependingonit,thetest\ncan be categorized as either a unit test, an integration test, or an\nend-to-endtest.\nAunit testvalidates the behavior of a small part of the codebase,\nlikeanindividualclass. Agoodunittestshouldberelativelystatic\nintimeandchangeonlywhenthebehavioroftheSUTchanges—\nrefactoring,ﬁxingabug,oraddinganewfeatureshouldn’trequire\naunittesttochange. Toachievethat,aunittestshould:\n•useonlythepublicinterfacesoftheSUT;\n•test for state changes in the SUT (not predetermined\nsequenceofactions);\n•test for behaviors, i.e., how the SUT handles a given input\nwhenit’sinaspeciﬁcstate.\nAnintegration test has a larger scope than a unit test, since it ver-\niﬁes that a service can interact with its external dependencies as\nexpected. This deﬁnition is not universal, though, because inte-\ngrationtestinghasdifferentmeaningsfordifferentpeople.\nMartinFowler2makesthedistinctionbetweennarrowandbroad\nintegrationtests. Anarrowintegrationtestexercisesonlythecode\npathsofaservicethatcommunicatewithanexternaldependency,\nliketheadaptersandtheirsupportingclasses. Incontrast,abroad\nintegrationtestexercisescodepathsacrossmultipleliveservices.\nIntherestofthechapter,wewillrefertothesebroaderintegration\ntestsasend-to-endtests. An end-to-endtest validatesbehaviorthat\nspans multiple services in the system, like a user-facing scenario.\n2https://martinfowler.com/bliki/IntegrationTest.html\nCHAPTER18. TESTING 195\nThesetestsusuallyruninsharedenvironments,likestagingorpro-\nduction. Becauseoftheirscope,theyareslowandmoreproneto\nintermittentfailures.\nEnd-to-end tests should not have any impact on other tests or\nusers sharing the same environment. Among other things, that\nrequires services to have good fault isolation mechanisms, like\nrate-limiting,topreventbuggytestsfromaffectingtherestofthe\nsystem.\nEnd-to-end tests can be painful and expensive to maintain. For\nexample, when an end-to-end test fails, it’s not always obvious\nwhichserviceisresponsibleanddeeperinvestigationisrequired.\nBut they are a necessary evil to ensure that user-facing scenarios\nworkasexpectedacrosstheentireapplication. Theycanuncover\nissues that tests with smaller scope can’t, like unanticipated side\neffectsandemergentbehaviors.\nOne way to minimize the number of end-to-end tests is to frame\nthem as user journey tests. A user journey test simulates a multi-\nstepinteractionofauserwiththesystem(e.g.fore-commerceser-\nvice: createanorder, modifyit, andﬁnallycancelit). Suchatest\nusuallyrequireslesstimetorunthansplittingthetestintoNsep-\narateend-to-endtests.\nAsthescopeofatestincreases,itbecomesmorebrittle,slow,and\ncostly. Intermittently-failing tests are nearly as bad as no tests at\nall,asdevelopersstophavinganyconﬁdenceinthemandeventu-\nallyignoretheirfailures. Whenpossible,prefertestswithsmaller\nscopeastheytendtobemorereliable,faster,andcheaper. Agood\ntrade-offistohavealargenumberofunittests,asmallerfraction\nof integration tests, and even fewer end-to-end tests (see Figure\n18.1).",4598
81-Size.pdf,81-Size,"CHAPTER18. TESTING 196\nFigure18.1: Testpyramid\n18.2 Size\nThesizeofatest3reﬂectshowmuchcomputingresourcesitneeds\nto run, like the number of nodes. Generally, that depends on\nhow realistic the environment is where the test runs. Although\nthescopeandsizeofatesttendtobecorrelated,theyaredistinct\nconcepts,andithelpstoseparatethem.\nAsmalltestrunsinasingleprocessanddoesn’tperformanyblock-\ningcallsorI/O.It’sveryfast,deterministic,andhasaverysmall\nprobabilityoffailingintermittently.\nAnintermediatetest runsonasinglenodeandperformslocalI/O,\nlikereadsfromdiskornetworkcallstolocalhost. Thisintroduces\nmoreroomfordelaysandnon-determinism,increasingthelikeli-\nhoodofintermittentfailures.\nAlargetestrequiresmultiplenodestorun,introducingevenmore\nnon-determinismandlongerdelays.\nUnsurprisingly, the larger a test is, the longer it takes to run and\ntheﬂakieritbecomes. Thisiswhyyoushouldwritethesmallest\npossibletestforagivenbehavior. Buthowdoyoureducethesize\n3https://www.amazon.co.uk/dp/B0859PF5HB\nCHAPTER18. TESTING 197\nofatest,whilenotreducingitsscope?\nYoucanusea testdoubleinplaceofarealdependencytoreducethe\ntest’ssize,makingitfasterandlesspronetointermittentfailures.\nTherearedifferenttypesoftestdoubles:\n•Afakeis a lightweight implementation of an interface that\nbehavessimilarlytoarealone. Forexample,anin-memory\nversionofadatabaseisafake.\n•Astubis a function that always returns the same value no\nmatterwhichargumentsarepassedtoit.\n•Finally, amockhas expectations on how it should be called,\nandit’susedtotesttheinteractionsbetweenobjects.\nThe problem with test doubles is that they don’t resemble how\ntherealimplementationbehaveswithallitsnuances. Thelessthe\nresemblance is, the less conﬁdence you should have that the test\nusing the double is actually useful. Therefore, when the real im-\nplementationisfast,deterministic,andhasfewdependencies,use\nthatratherthanadouble. Ifthat’snotthecase,youhavetodecide\nhowrealisticyouwantthetestdoubletobe,asthereisatradeoff\nbetweenitsﬁdelityandthetest’ssize.\nWhen using the real implementation is not an option, use a fake\nmaintained by the same developers of the dependency, if one is\navailable. Stubbing, or mocking, are last-resort options as they\noffer the least resemblance to the actual implementation, which\nmakesteststhatusethembrittle.\nFor integration tests, a good compromise is to use mocking with\ncontracttests4. Acontracttest deﬁnestherequestitintendstosend\nto an external dependency and the response it expects to receive\nfromit. Thiscontractisthenusedbythetesttomocktheexternal\ndependency. For example, a contract for a REST API consists of\nan HTTP request and response pair. To ensure that the contract\ndoesn’t break, the test suite of the external dependency uses the\nsamecontracttosimulateaclientandensurethattheexpectedre-\nsponseisreturned.\n4https://martinfowler.com/bliki/ContractTest.html",2903
82-Practical considerations.pdf,82-Practical considerations,"CHAPTER18. TESTING 198\n18.3 Practical considerations\nAswitheverythingelse,testingrequiresmakingtradeoffs.\nSupposewewanttotestthebehaviorofaspeciﬁcuser-facingAPI\nendpoint offered by a service. The service talks to a data store,\naninternalserviceownedbyanotherteam,andathird-partyAPI\nused for billing (see Figure 18.2). As mentioned earlier, the gen-\neralguidelineistowritethesmallesttestpossiblewiththedesired\nscope.\nFigure18.2: Howwouldyoutesttheservice?\nAs it turns out, the endpoint doesn’t need to communicate with\ntheinternalservice,sowecansafelyuseamockinitsplace. The\ndatastorecomeswithanin-memoryimplementation(afake)that\nwe can leverage to avoid issuing network calls to a remote data\nstore.\nFinally,wecan’tusethethird-partybillingAPI,asthatwouldre-\nCHAPTER18. TESTING 199\nquire the test to issue real transactions. Fortunately, the API has\nadifferentendpointthatoffersaplaygroundenvironment,which\nthetestcanusewithoutcreatingrealtransactions. Iftherewasno\nplayground environment available and no fake either, we would\nhavetoresorttostubbingormocking.\nInthiscase,wehavecutthetest’ssizeconsiderably,whilekeeping\nitsscopemostlyintact.\nHereisamorenuancedexample. Supposeweneedtotestwhether\npurgingthedatabelongingtoaspeciﬁcuseracrosstheentireap-\nplicationstackworksasexpected. InEurope,thisfunctionalityis\nmandatedbylaw(GDPR),andfailingtocomplywithitcanresult\ninﬁnesupto20millioneurosor4%annualturnover,whicheveris\ngreater. Inthiscase,becausetheriskforthefunctionalitysilently\nbreaking is too high, we want to be as conﬁdent as possible that\nthefunctionalityisworkingasexpected. Thiswarrantstheuseof\nan end-to-end test that runs in production and uses live services\nratherthantestdoubles.",1726
83-Review and build.pdf,83-Review and build,"Chapter 19\nContinuous delivery and\ndeployment\nOnce a change and its newly introduced tests have been merged\ntoarepository,itneedstobereleasedtoproduction.\nWhenreleasingachangerequiresamanualprocess,itwon’thap-\npenfrequently. Meaningthatseveralchanges,possiblyoverdays\norevenweeks,endupbeingbatchedandreleasedtogether. This\nmakesithardertopinpointthebreakingchange1whenadeploy-\nment fails, creating interruptions for the whole team. The devel-\noper who initiated the release also needs to keep an eye on it by\nmonitoring dashboards and alerts to ensure that it’s working as\nexpectedorrollitback.\nManual deployments are a terrible use of engineering time. The\nproblem gets further exacerbated when there are many services.\nEventually,theonlywaytoreleasechangessafelyandefﬁciently\nistoautomatetheentireprocess. Onceachangehasbeenmerged\ntoarepository,itshouldautomaticallyberolledouttoproduction\nsafely. The developer is then free to context-switch to their next\ntask,ratherthanshepherdingthedeployment. Thewholerelease\n1Therecouldbemultiplebreakingchangesactually.\nCHAPTER19. CONTINUOUSDELIVERYANDDEPLOYMENT 201\nprocess,includingrollbacks,canbeautomatedwithacontinuous\ndeliveryanddeployment(CD)pipeline.\nCD requires a signiﬁcant amount of investment in terms of safe-\nguards, monitoring, and automation. If a regression is detected,\nthe artifact being released — i.e., the deployable component that\nincludesthechange—iseitherrolledbacktothepreviousversion,\norforwardtothenextone,assumingitcontainsahotﬁx.\nThere is a balance between the safety of a rollout and the time\nit takes to release a change to production. A good CD pipeline\nshould strive to make a good trade-off between the two. In this\nchapter,wewillexplorehow.\n19.1 Review and build\nAt a high level, a code change needs to go through a pipeline\nof four stages to be released to production: review, build,\npre-productionrollout,andproductionrollout.\nFigure19.1: Continuousdeliveryanddeploymentpipelinestages\nItallstartswithapullrequest(PR)submittedforreviewbyadevel-\nopertoarepository. WhenthePRissubmittedforreview,itneeds\nCHAPTER19. CONTINUOUSDELIVERYANDDEPLOYMENT 202\ntobecompiled,staticallyanalyzed,andvalidatedwithabatteryof\ntests,allofwhichshouldn’ttakelongerthanafewminutes. Toin-\ncreasethetests’speedandminimizeintermittentfailures,thetests\nthatrunatthisstageshouldbesmallenoughtorunonasinglepro-\ncessornode,likee.g.,unittests,withlargertestsonlyrunlaterin\nthepipeline.\nThe PR needs to be reviewed and approved by a team member\nbefore it can be merged into the repository. The reviewer has to\nvalidate whether the change is correct and safe to be released to\nproductionautomaticallybytheCDpipeline. Achecklistcanhelp\nthereviewernottoforgetanythingimportant:\n•Does the change include unit, integration, and end-to-end\ntestsasneeded?\n•Doesthechangeincludemetrics,logs,andtraces?\n•Can this change break production by introducing a\nbackward-incompatible change, or hitting some service\nlimit?\n•Canthechangeberolledbacksafely,ifneeded?\nCode changes shouldn’t be the only ones going through this\nreview process. For example, cloud resource templates, static\nassets, end-to-end tests, and conﬁguration ﬁles should all be\nversion-controlled in a repository (not necessarily the same) and\nbetreatedjustlikecode. Thesameservicecanthenhavemultiple\nCD pipelines, one for each repository, that can potentially run in\nparallel.\nI can’t stress enough the importance of reviewing and releasing\nconﬁgurationchangeswithaCDpipeline. Oneofthemostcom-\nmoncausesofproductionfailuresareconﬁgurationchanges2ap-\npliedgloballywithoutanypriorreviewortesting.\nOnce the change has been merged into the repository’s main\nbranch, the CD pipeline moves to the build stage, in which the\nrepository’s content is built and packaged into a deployable\nreleaseartifact.\n2https://github.com/danluu/post-mortems",3910
84-Pre-production.pdf,84-Pre-production,,0
85-Rollbacks.pdf,85-Rollbacks,"CHAPTER19. CONTINUOUSDELIVERYANDDEPLOYMENT 203\n19.2 Pre-production\nDuring this stage, the artifact is deployed and released to a syn-\nthetic pre-production environment. Although this environment\nlacks the realism of production, it’s useful to verify that no hard\nfailuresaretriggered(e.g.,anullpointerexceptionatstartupdue\nto a missing conﬁguration setting) and that end-to-end tests suc-\nceed. Becausereleasinganewversiontopre-productionrequires\nsigniﬁcantlylesstimethanreleasingittoproduction,bugscanbe\ndetectedearlier.\nYou can even have multiple pre-production environments, start-\ningwithonecreatedfromscratchforeachartifactandusedtorun\nsimplesmoketests, toapersistentonesimilartoproductionthat\nreceivesasmallfractionofmirroredrequestsfromit. AWS,forex-\nample,usesmultiplepre-productionenvironments3(Alpha,Beta,\nandGamma).\nAservicereleasedtoapre-productionenvironmentshouldcallthe\nproductionendpointsofitsexternaldependenciestomaketheen-\nvironment as stable as possible; it could call the pre-production\nendpointsofotherservicesownedbythesameteam,though.\nIdeally, the CD pipeline should assess the artifact’s health in\npre-productionusingthesamehealthsignalsusedinproduction.\nMetrics,alerts,andtestsusedinpre-productionshouldbeequiv-\nalenttothoseusedinproductiontoavoidtheformertobecomea\nsecond-classcitizenwithsub-parhealthcoverage.\n19.3 Production\nOnceanartifacthasbeenrolledouttopre-productionsuccessfully,\ntheCDpipelinecanproceedtotheﬁnalstageandreleasethearti-\nfacttoproduction. Itshouldstartbyreleasingittoasmallnumber\nof production instances at ﬁrst4. The goal is to surface problems\n3https://aws.amazon.com/builders-library/automating-safe-hands-off-\ndeployments/\n4Thisisalsoreferredtoascanarytesting.\nCHAPTER19. CONTINUOUSDELIVERYANDDEPLOYMENT 204\nthathaven’tbeendetectedsofarasquicklyaspossiblebeforethey\nhavethechancetocausewidespreaddamageinproduction.\nIf that goes well and all the health checks pass, the artifact is in-\ncrementallyreleasedtotherestoftheﬂeet. Whiletherolloutisin\nprogress, a fraction of the ﬂeet can’t serve any trafﬁc due to the\nongoing deployment, and the remaining instances need to pick\nuptheslack. Toavoidthiscausinganyperformancedegradation,\nthere needs to be enough capacity left to sustain the incremental\nrelease.\nIf the service is available in multiple regions, the CD pipeline\nshould start with a low-trafﬁc region ﬁrst to reduce the impact\nof a faulty release. Releasing the remaining regions should be\ndivided into sequential stages to minimize risks further. Natu-\nrally, the more stages there are, the longer the CD pipeline will\ntaketoreleasetheartifacttoproduction. Onewaytomitigatethis\nproblem is by increasing the release speed once the early stages\ncomplete successfully and enough conﬁdence has been built up.\nFor example, the ﬁrst stage could release the artifact to a single\nregion, the second to a larger region, and the third to N regions\nsimultaneously.\n19.4 Rollbacks\nAftereachstep,theCDpipelineneedstoassesswhetherthearti-\nfactdeployedishealthy,orelsestopthereleaseandrollitback. A\nvariety of health signals can be used to make that decision, such\nas:\n•theresultofend-to-endtests;\n•healthmetricslikelatenciesanderrors;\n•alerts;\n•andhealthendpoints.\nMonitoring just the health signals of the service being rolled out\nisnotenough. TheCDpipelineshouldalsomonitorthehealthof\nupstreamanddownstreamservicestodetectanyindirectimpact\noftherollout. Thepipelineshouldallowenoughtimetopassbe-\nCHAPTER19. CONTINUOUSDELIVERYANDDEPLOYMENT 205\ntweenonestepandthenext(baketime)toensurethatitwassuc-\ncessful,assomeissuescanappearonlyaftersometimehaspassed.\nForexample, aperformancedegradationcouldbevisibleonlyat\npeaktime.\nTheCDpipelinecanfurthergatethebaketimeonthenumberof\nrequestsseenforspeciﬁcAPIendpointstoguaranteethattheAPI\nsurfacehasbeenproperlyexercised. Tospeeduptherelease,the\nbaketimecanbereducedaftereachstepsucceedsandconﬁdence\nisbuiltup.\nWhenahealthsignalreportsadegradation,theCDpipelinestops.\nAt that point, it can either roll back the artifact automatically, or\ntrigger an alert to engage the engineer on-call, who needs to de-\ncidewhetherarollbackiswarrantedornot5. Basedontheirinput,\ntheCDpipelineretriesthestagethatfailed(e.g.,perhapsbecause\nsomethingelsewasgoingintoproductionatthetime),orrollsback\nthe release entirely. The operator can also stop the pipeline and\nwait for a new artifact with a hotﬁx to be rolled forward. This\nmight be necessary if the release can’t be rolled back because a\nbackward-incompatiblechangehasbeenintroduced.\nSincerollingforwardismuchriskierthanrollingback,anychange\nintroduced should always be backward compatible as a rule of\nthumb. Themostcommoncauseforbackward-incompatibilityis\nchangingtheserializationformatusedeitherforpersistenceorIPC\npurposes.\nTo safely introduce a backward-incompatible change, it needs to\nbebrokendownintomultiplebackward-compatiblechanges6. For\nexample,supposethemessagingschemabetweenaproducerand\na consumer service needs to change in a backward incompatible\nway. In this case, the change is broken down into three smaller\nchangesthatcanindividuallyberolledbacksafely:\n•Inthepreparechange,theconsumerismodiﬁedtosupport\n5CDpipelinescanbeconﬁguredtorunonlyduringbusinesshourstominimize\nthedisruptiontoon-callengineers.\n6https://aws.amazon.com/builders-library/ensuring-rollback-safety-during-\ndeployments/\nCHAPTER19. CONTINUOUSDELIVERYANDDEPLOYMENT 206\nboththenewandoldmessagingformat.\n•Intheactivatechange,theproducerismodiﬁedtowritethe\nmessagesinthenewformat.\n•Finally,inthecleanupchange,theconsumerstopssupport-\ningtheoldmessagingformataltogether. Thischangeshould\nonlybereleasedoncethereisenoughconﬁdencethattheac-\ntivatedchangewon’tneedtoberolledback.\nAnautomatedupgrade-downgradetestpartoftheCDpipelinein\npre-production can be used to validate whether a change is actu-\nallysafetorollbackornot.",5940
86-Metrics.pdf,86-Metrics,"Chapter 20\nMonitoring\nMonitoring is primarily used to detect failures that impact users\nin production and trigger notiﬁcations (alerts) sent to human op-\neratorsresponsibleformitigatingthem. Theothercriticalusecase\nformonitoringistoprovideahigh-leveloverviewofthesystem’s\nhealththroughdashboards.\nIn the early days, monitoring was used mostly as a black-box\napproach to report whether a service was up or down, without\nmuch visibility of what was going on inside. Over the years, it\nhas evolved into a white-box approach as developers started to\ninstrument their code to emit application-level measurements\nto answer whether speciﬁc features worked as expected. This\nwas popularized with the introduction of statsd1by Etsy, which\nnormalizedcollectingapplication-levelmeasurements.\nBlackbox monitoring is still in use today to monitor external de-\npendencies,suchasthird-partyAPIs,andvalidatehowtheusers\nperceivetheperformanceandhealthofaservicefromtheoutside.\nA common approach is to periodically run scripts2that send test\nrequests to external API endpoints and monitor how long they\n1https://codeascraft.com/2011/02/15/measure-anything-measure-everythi\nng/\n2https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/C\nloudWatch_Synthetics_Canaries.html\nCHAPTER20. MONITORING 208\ntookandwhethertheyweresuccessful. Thesescriptsaredeployed\nin the same regions the application’s users are and hit the same\nendpointstheydo. Becausetheyexercisethesystem’spublicsur-\nfacefromtheoutside,theycancatchissuesthataren’tvisiblefrom\nwithin the application, like connectivity problems. These scripts\narealsousefultodetectissueswithAPIsthataren’texercisedof-\ntenbyusers.\nBlackbox monitoring is good at detecting the symptoms when\nsomething is broken; in contrast, white-box monitoring can help\nidentifytherootcauseofknownhard-failuremodesbeforeusers\nare impacted. As a rule of thumb, if you can’t design away a\nhard-failuremode,youshouldaddmonitoringforit. Thelonger\nasystemhasbeenaround, thebetteryouwillunderstandhowit\ncanfailandwhatneedstobemonitored.\n20.1 Metrics\nAmetricisanumericrepresentationofinformationmeasuredover\na time interval and represented as a time-series, like the number\nof requests handled by a service. Conceptually, a metric is a list\nofsamples, whereeachsampleisrepresentedbyaﬂoating-point\nnumberandatimestamp.\nModernmonitoringsystemsallowametrictobetaggedwithaset\nofkey-valuepairscalled labels,whichincreasesthedimensionality\nofthemetric. Essentially,everydistinctcombinationoflabelsisa\ndifferentmetric. Thishasbecomeanecessityasmodernservices\ncanhavealargeamountofmetadataassociatedwitheachmetric,\nlike datacenter, cluster, node, pod, service, etc. High-cardinality\nmetricsmakeiteasytosliceanddicethedata, andeliminatethe\ninstrumentationcostofmanuallycreatingametricforeachlabel\ncombination.\nA service should emit metrics about its load, internal state, and\navailability and performance of downstream service dependen-\ncies. Combinedwiththemetricsemittedbydownstreamservices,\nthis allows operators to identify problems quickly. This requires\nCHAPTER20. MONITORING 209\nexplicit code changes and a deliberate effort by developers to\ninstrumenttheircode.\nForexample,takeaﬁctitiousHTTPhandlerthatreturnsaresource.\nThere is a whole range of questions you will want to be able to\nansweronceit’srunninginproduction3:\ndef get_resource(id):\nresource =self ._cache.get(id) # in-process cache\n# Is the id valid?\n# Was there a cache hit?\n# How long has the resource been in the cache?\nifresource isnot None :\nreturn resource\nresource =self ._repository.get(id)\n# Did the remote call fail, and if so, why?\n# Did the remote call timeout?\n# How long did the call take?\nself ._cache[id] =resource\n# What's the size of the cache?\nreturn resource\n# How long did it take for the handler to run?\nNow,supposewewanttorecordthenumberofrequestsourser-\nvice failed to handle. One way to do that is with an event-based\napproach—wheneveraserviceinstancefailstohandlearequest,\nitreportsafailurecountof1inan event4toalocaltelemetryagent,\ne.g.:\n{\n""failureCount"" :1,\n""serviceRegion"" :""EastUs2"" ,\n3Ihaveomittederrorhandlingforsimplicity\n4Wewilltalkmoreabouteventlogsinsection 21.1,fornowassumeaneventis\njustadictionary.\nCHAPTER20. MONITORING 210\n""timestamp"" :1614438079\n}\nThe agent batches these events and emits them periodically to a\nremotetelemetryservice,whichpersiststheminadedicateddata\nstore for event logs. For example, this is the approach taken by\nAzureMonitor’slog-basedmetrics5.\nAsyou canimagine, thisisquiteexpensivesincetheloadonthe\nbackendincreaseswiththenumberofeventsingested. Eventsare\nalsocostlytoaggregateatquerytime—supposeyouwanttore-\ntrievethenumberoffailuresinNorthEuropeoverthepastmonth;\nyou would have to issue a query that requires fetching, ﬁltering,\nandaggregatingpotentiallytrillionsofeventswithinthattimepe-\nriod.\nIsthereawaytoreducecostsatquerytime? Becausemetricsare\ntime-series,theycanbemodeledandmanipulatedwithmathemat-\nicaltools. Thesamplesofatime-seriescanbepre-aggregatedover\npre-speciﬁedtimeperiods(e.g., 1second, 5minutes, 1hour, etc.)\nandrepresentedwithsummarystatisticssuchasthesum,average,\norpercentiles.\nFor example, the telemetry backend can pre-aggregate metrics\nover one or more time periods at ingestion time. Conceptually,\nif the aggregation (i.e., the sum in our example) were to happen\nwithaperiodofonehour,wewouldhaveone failureCount metric\nperserviceRegion ,eachcontainingonesampleperhour,e.g.:\n""00:00"", 561,\n""01:00"", 42,\n""02:00"", 61,\n...\nThe backend can create multiple pre-aggregates with different\nperiods. Then at query time, the pre-aggregated metric with the\nbestperiodthatsatisﬁesthequeryischosen. Forexample,Cloud-\n5https://docs.microsoft.com/en-us/azure/azure-monitor/app/pre-aggregat\ned-metrics-log-metrics#log-based-metrics",5896
87-Service-level indicators.pdf,87-Service-level indicators,"CHAPTER20. MONITORING 211\nWatch6(thetelemetrybackendusedbyAWS)pre-aggregatesdata\nasit’singested.\nWe can take this idea one step further and also reduce ingestion\ncosts by having the local telemetry agents pre-aggregate metrics\nontheclientside.\nClient and server-side pre-aggregation drastically reduces band-\nwidth,compute,andstoragerequirementsformetrics. However,\nitcomesatacost;operatorslosetheﬂexibilitytore-aggregatemet-\nricsaftertheyhavebeeningested,astheynolongerhaveaccessto\ntheoriginaleventsthatgeneratedthem. Forexample, ifametric\nispre-aggregatedoveraperiodoftimeof1hour,itcan’tlaterbe\nre-aggregatedoveraperiodof5minwithouttheoriginalevents.\nBecausemetricsaremainlyusedforalertingandvisualizationpur-\nposes,theyareusuallypersistedinpre-aggregatedforminatime-\nseriesdatastoresincequeryingpre-aggregateddatacanbeseveral\norderofmagnitudesmoreefﬁcientthanthealternative.\n20.2 Service-level indicators\nAsmentionedearlier,oneofthemainusecasesformetricsisalert-\ning. Thatdoesn’tmeanweshouldcreatealertsforeverypossible\nmetric out there — for example, it’s useless to be alerted in the\nmiddleofthenightbecauseaservicehadabigspikeinmemory\nconsumptionafewminutesearlier. Inthissection,wewilldiscuss\nonespeciﬁcmetriccategorythatlendsitselfwellforalerting.\nAservice-level indicator (SLI) is a metric that measures one aspect\nofthelevelofservice providedbyaservicetoitsusers,likethere-\nsponse time, error rate, or throughput. SLIs are typically aggre-\ngatedoverarollingtimewindowandrepresentedwithasummary\nstatistic,likeaverageorpercentile.\nSLIsarebestdeﬁnedwitharatiooftwometrics,goodeventsover\ntotal number of events, since they are easy to interpret: 0 means\n6https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/clo\nudwatch_concepts.html\nCHAPTER20. MONITORING 212\ntheserviceisbrokenand1thateverythingisworkingasexpected\n(see Figure 20.1). As we will see later in the chapter, ratios also\nsimplifytheconﬁgurationofalerts.\nThesearesomecommonlyusedSLIsforservices:\n•Responsetime —Thefractionofrequeststhatarecompleted\nfasterthanagiventhreshold.\n•Availability —Theproportionoftimetheservicewasusable,\ndeﬁned as the number of successful requests over the total\nnumberofrequests.\n•Quality— The proportion of requests served in an un-\ndegradedstate(assumingthesystemdegradesgracefully).\n•Datacompleteness (forstoragesystems)—Theproportionof\nrecordspersistedtoadatastorethatcanbesuccessfullyac-\ncessedlater.\nFigure 20.1: An SLI deﬁned as the ratio of good events over the\ntotalnumberofevents.\nOnce you have decided what to measure, you need to decide\nwheretomeasureit. Taketheresponsetime,forexample. Should\nyou use the metric reported by the service, load balancer, or\nclients? In general, you want to use the one that best represents\ntheexperienceoftheusers. Andifthat’stoocostlytocollect,pick\nthenextbestcandidate. Inthepreviousexample,theclientmetric\nis the more meaningful one, as that accounts for delays in the\nCHAPTER20. MONITORING 213\nentirepathoftherequest.\nNow, how should you measure response times? Measurements\ncan be affected by various factors that increase their variance,\nsuchasnetworktimeouts,pagefaults,orheavycontextswitching.\nSince every request does not take the same amount of time,\nresponse times are best represented with a distribution, which\ntendtoberight-skewedandlong-tailed7.\nAdistributioncanbesummarizedwithastatistic. Taketheaver-\nage, for example. While it has its uses, it doesn’t tell you much\nabouttheproportionofrequestsexperiencingaspeciﬁcresponse\ntime. Allittakesisoneextremeoutliertoskewtheaverage. For\nexample,if100requestsarehittingyourservice,99ofwhichhave\naresponsetimeof1secondandoneof10min,theaverageisnearly\n7seconds. Eventhough99%oftherequestsexperiencearesponse\ntimeof1second,theaverageis7timeshigherthanthat.\nAbetterwaytorepresentthedistributionofresponsetimesiswith\npercentiles. Apercentileisthevaluebelowwhichapercentageof\ntheresponsetimesfall. Forexample,ifthe99thpercentileis1sec-\nond,then99%ofrequestshavearesponsetimebeloworequalto1\nsecond. Theupperpercentilesofaresponsetimedistribution,like\nthe 99th and 99.9th percentiles, are also called long-tail latencies.\nIn general, the higher the variance of a distribution is, the more\nlikelytheaverageuserwillbeaffectedbylong-tailbehavior8.\nEventhoughonlyasmallfractionofrequestsexperiencetheseex-\ntremelatencies,itimpactsyourmostproﬁtableusers. Theyarethe\nones that make the highest number of requests and thus have a\nhigherchanceofexperiencingtaillatencies. Severalstudies9have\nshownthathighlatenciescannegativelyaffectrevenues. Amere\n100-milliseconddelayinloadtimecanhurtconversionratesby7\npercent.\nAlso, long-tail behaviors left unchecked can quickly bring a ser-\n7https://igor.io/latency/\n8This tends to be primarily caused by various queues in the request-response\npath.\n9https://www.akamai.com/uk/en/about/news/press/2017-press/akamai-\nreleases-spring-2017-state-of-online-retail-performance-report.jsp",5008
88-Service-level objectives.pdf,88-Service-level objectives,"CHAPTER20. MONITORING 214\nvice to its knees. Suppose a service is using 2K threads to serve\n10Krequestspersecond. ByLittle’sLaw10, theaverageresponse\ntime of a thread is 200 ms. Suddenly, a network switch becomes\ncongested,andasithappens,1%ofrequestsarebeingservedfrom\nanodebehindthatswitch. That1%ofrequests,or100requestsper\nsecondoutofthe10K,startstaking20secondstocomplete.\nHow many more threads does the service need to deal with the\nsmallfractionofrequestshavingahighresponsetime? If100re-\nquests per second take 20 seconds to process, then 2K additional\nthreadsareneededtodealjustwiththeslowrequests. Sothenum-\nberofthreadsusedbytheserviceneedstodoubletokeepupwith\ntheload!\nMeasuringlong-tailbehaviorandkeepingitundercheckdoesn’t\njust make your users happy, but also drastically improves the re-\nsiliencyofyourserviceandreducesoperationalcosts. Whenyou\nareforcedtoguardagainsttheworst-caselong-tailbehavior,you\nhappentoimprovetheaveragecaseaswell.\n20.3 Service-level objectives\nAservice-levelobjective (SLO)deﬁnesarangeofacceptablevalues\nforanSLIwithinwhichtheserviceisconsideredtobeinahealthy\nstate (see Figure 20.2). An SLO sets the expectation to its users\nofhowtheserviceshouldbehavewhenit’sfunctioningcorrectly.\nService owners can also use SLOs to deﬁne a service-level agree-\nment (SLA) with their users — a contractual agreement that dic-\ntateswhathappenswhenanSLOisn’tmet,typicallyresultingin\nﬁnancialconsequences.\nForexample,anSLOcoulddeﬁnethat99%ofAPIcallstoendpoint\nXshouldcompletebelow200ms,asmeasuredoverarollingwin-\ndow of 1 week. Another way to look at it, is that it’s acceptable\nfor up to 1% of requests within a rolling week to have a latency\nhigherthan200ms. That1%isalsocalledtheerrorbudget,which\nrepresentsthenumberoffailuresthatcanbetolerated.\n10https://robertovitillo.com/back-of-the-envelope-estimation-hacks/\nCHAPTER20. MONITORING 215\nFigure20.2: AnSLOdeﬁnestherangeofacceptablevaluesforan\nSLI.\nSLOsarehelpfulforalertingpurposesandhelptheteamprioritize\nrepair tasks with feature work. For example, the team can agree\nthat when an error budget has been exhausted, repair items will\ntakeprecedenceovernewfeaturesuntiltheSLOisrestored. Also,\nan incident’s importance can be measured by how much of the\nerrorbudgethasbeenburned. Anincidentthatburned20%ofthe\nerrorbudgetneedsmoreafterthoughtthanonethatburnedonly\n1%.\nSmaller time windows force the team to act quicker and priori-\ntize bug ﬁxes and repair items, while longer windows are better\nsuitedtomakelong-termdecisionsaboutwhichprojectstoinvest\nin. ThereforeitmakessensetohavemultipleSLOswithdifferent\nwindowsizes.\nHow strict should SLOs be? Choosing the right target range\nis harder than it looks. If it’s too loose, you won’t detect user-\nfacing issues; if it’s too strict, you will waste engineering time\nmicro-optimizing and get diminishing returns. Even if you\ncouldguarantee100%reliabilityforyoursystem,youcan’tmake\nguarantees for anything that your users depend on to access\nyour service that is outside your control, like their last-mile\nconnection. Thus, 100% reliability doesn’t translate into a 100%\nreliableexperienceforusers.\nCHAPTER20. MONITORING 216\nWhen setting the target range for your SLOs, start with comfort-\nable ranges and tighten them as you build up conﬁdence. Don’t\njustpicktargetsthatyourservicemeetstodaythatmightbecome\nunattainable in a year after the load increases; work backward\nfrom what users care about. In general, anything above 3 nines\nofavailabilityisverycostlytoachieveandprovidesdiminishing\nreturns.\nHow many SLOs should you have? You should strive to keep\nthings simple and have as few as possible that provide a good\nenough indication of the desired service level. SLOs should also\nbedocumentedandreviewedperiodically. Forexample,suppose\nyoudiscoverthataspeciﬁcuser-facingissuegeneratedlotsofsup-\nporttickets,butnoneofyourSLOsshowedanydegradations. In\nthat case, they are either too relaxed, or you are not measuring\nsomethingthatyoushould.\nSLOsneedtobeagreedonwithmultiplestakeholders. Engineers\nneedtoagreethatthetargetsareachievablewithoutexcessivetoil.\nIf the error budget is burning too rapidly or has been exhausted,\nrepair items will take priority over features. Product managers\nhave to agree that the targets guarantee a good user experience.\nAsGoogle’sSREbook11mentions: “ifyoucan’teverwinaconver-\nsation about priorities by quoting a particular SLO, it’s probably\nnotworthhavingthatSLO”.\nUserscanbecomeover-reliantontheactualbehaviorofyourser-\nviceratherthanthepublishedSLO.Tomitigatethat,youcancon-\nsiderinjectingcontrolledfailures12inproduction—alsoknownas\nchaostesting—to“shakethetree”andensurethedependencies\ncan cope with the targeted service level and are not making un-\nrealistic assumptions. As an added beneﬁt, injecting faults helps\nvalidatethatresiliencymechanismsworkasexpected.\n11https://sre.google/sre-book/service-level-objectives/\n12https://en.wikipedia.org/wiki/Chaos_engineering",5023
89-Alerts.pdf,89-Alerts,"CHAPTER20. MONITORING 217\n20.4 Alerts\nAlertingisthepartofamonitoringsystemthattriggersanaction\nwhenaspeciﬁcconditionhappens,likeametriccrossingathresh-\nold. Dependingontheseverityandthetypeofthealert,theaction\ntriggeredcanrangefromrunningsomeautomation,likerestarting\naserviceinstance,toringingthephoneofahumanoperatorwho\nison-call. Intherestofthissection,wewillbemostlyfocusingon\nthelattercase.\nFor an alert to be useful, it has to be actionable. The operator\nshouldn’tspendtimediggingintodashboardstoassessthealert’s\nimpact and urgency. For example, an alert signaling a spike in\nCPUusageisnotusefulasit’snotclearwhetherithasanyimpact\non the system without further investigation. On the other hand,\nan SLO is a good candidate for an alert because it quantiﬁes its\nimpactontheusers. TheSLO’serrorbudgetcanbemonitoredto\ntriggeranalertwheneveralargefractionofithasbeenconsumed.\nBeforewecandiscusshowtodeﬁneanalert,it’simportanttoun-\nderstand that thereis a trade-offbetween its precision and recall.\nFormally,precisionisthefractionofsigniﬁcanteventsovertheto-\ntal number of alerts, while recall is the ratio of signiﬁcant events\nthattriggeredanalert. Alertswithlowprecisionarenoisyandof-\ntennotactionable,whilealertswithlowrecalldon’talwaystrigger\nduringanoutage. Althoughitwouldbenicetohave100%preci-\nsionandrecall,youhavetomakeatrade-offsinceimprovingone\ntypicallylowerstheother.\nSuppose you have an availability SLO of 99% over 30 days, and\nyou would like to conﬁgure an alert for it. A naive way would\nbe to trigger an alert whenever the availability goes below 99%\nwithinarelativelyshorttimewindow,likeanhour. Buthowmuch\noftheerrorbudgethasactuallybeenburnedbythetimethealert\ntriggers?\nBecausethetimewindowofthealertisonehour,andtheSLOerror\nbudgetisdeﬁnedover30days,thepercentageoferrorbudgetthat\nhasbeenspentwhenthealerttriggersis1hour\n30days= 0.14. Isitreally\nCHAPTER20. MONITORING 218\ncriticaltobenotiﬁedthat0.14%oftheSLO’serrorbudgethasbeen\nburned? Probablynot. Inthiscase,youhavehighrecall,butlow\nprecision.\nYoucanimprovethealert’sprecisionbyincreasingtheamountof\ntime its condition needs to be true. The problem with it is that\nnow the alert will take longer to trigger, which will be an issue\nwhenthereisanactualoutage. Thealternativeistoalertbasedon\nhowfasttheerrorbudgetisburning,alsoknownastheburnrate,\nwhichlowersthedetectiontime.\nThe burn rate is deﬁned as the percentage of the error budget\nconsumed over the percentage of the SLO time window that has\nelapsed—it’stherateofincreaseoftheerrorbudget. Concretely,\nforourSLOexample,aburnrateof1meanstheerrorbudgetwill\nbeexhaustedpreciselyin30days;iftherateis2,thenitwillbe15\ndays;iftherateis3,itwillbe10days,andsoon.\nBy rearranging the burn rate’s equation, you can derive the alert\nthresholdthattriggerswhenaspeciﬁcpercentageoftheerrorbud-\ngethasbeenburned. Forexample, tohaveanalerttriggerwhen\nanerrorbudgetof2%hasbeenburnedinaone-hourwindow,the\nthresholdfortheburnrateshouldbesetto14.4:\nerrorbudgetconsumed = 0.02\ntimeperiodelapsed =alertwindow\nSLOperiod=1ℎ\n720ℎ\nburnrate =errorbudgetconsumed\ntimeperiodelapsed= 14.4\nTo improve recall, you can have multiple alerts with different\nthresholds. For example, a burn rate below 2 could be a low-\nseverity alert that sends an e-mail and is investigated during\nworkinghours. TheSREworkbookhassomegreatexamples13of\nhowtoconﬁgurealertsbasedonburnrates.\n13https://sre.google/workbook/alerting-on-slos/",3475
90-Dashboards.pdf,90-Dashboards,"CHAPTER20. MONITORING 219\nWhileyoushoulddeﬁnemostofyouralertsbasedonSLOs,some\nshouldtriggerforknownhard-failuremodesthatyouhaven’thad\nthetimetodesignordebugaway. Forexample,supposeyouknow\nyourservicesuffersfromamemoryleakthathasledtoanincident\ninthepast, butyouhaven’tmanagedyettotrackdowntheroot-\ncauseorbuildaresiliencymechanismtomitigateit. Inthiscase,it\ncouldbeusefultodeﬁneanalertthattriggersanautomatedrestart\nwhenaserviceinstanceisrunningoutofmemory.\n20.5 Dashboards\nAfteralerting,theothermainusecaseformetricsistopowerreal-\ntimedashboardsthatdisplaytheoverallhealthofasystem.\nUnfortunately,dashboardscaneasilybecomeadumpingground\nfor charts that end up being forgotten, have questionable useful-\nness,orarejustplainconfusing. Gooddashboardsdon’thappen\nbycoincidence. Inthissection, Iwillpresentsomebestpractices\nonhowtocreateusefuldashboards.\nThe ﬁrst decision you have to make when creating a dashboard\nis to decide who the audience is14and what they are looking for.\nGiven the audience, you can work backward to decide which\ncharts,andthereforemetrics,toinclude.\nThecategoriesofdashboardspresentedhere(seeFigure 20.3)are\nbynomeansstandardbutshouldgiveyouanideaofhowtoorga-\nnizedashboards.\nSLO dashboard\nThe SLO summary dashboard is designed to be used by various\nstakeholdersfromacrosstheorganizationtogainvisibilityintothe\nsystem’s health as represented by its SLOs. During an incident,\nthisdashboardquantiﬁestheimpactit’shavingonusers.\nPublic API dashboard\n14https://aws.amazon.com/builders-library/building-dashboards-for-\noperational-visibility\nCHAPTER20. MONITORING 220\nFigure20.3: Dashboardsshouldbetailoredtotheiraudience.\nThis dashboard displays metrics about the system’s public API\nendpoints, which helps operators identifying problematic paths\nduringanincident. Foreachendpoint,thedashboardexposessev-\neralmetricsrelatedtorequestmessages,requesthandlingandre-\nsponsemessages,like:\n•Numberofrequestsreceivedormessagedpulledfromames-\nsagingbroker,sizeofrequests,authenticationissues,etc.\n•Requesthandlingduration,availabilityandresponsetimeof\nexternaldependencies,etc.\n•Countsperresponsetype,sizeofresponses,etc.\nService dashboard",2193
91-On-call.pdf,91-On-call,"CHAPTER20. MONITORING 221\nAservicedashboarddisplaysservice-speciﬁcimplementationde-\ntails, which require a deep understanding of its inner workings.\nUnlikethepreviousdashboards,thisoneisprimarilyusedbythe\nteamthatownstheservice.\nBeyond service-speciﬁc metrics, a service dashboard should also\ncontain metrics for upstream dependencies like load balancers\nand messaging queues, and downstream dependencies like data\nstores.\nThisdashboardoffersaﬁrstentrypointintothebehaviorofaser-\nvicewhendebugging. Aswewilllaterlearnwhendiscussingob-\nservability, thishigh-levelview isjustthestartingpoint. Theop-\neratortypicallydrillsdownintothemetricsbysegmentingthem\nfurther,andeventuallyreachesforrawlogsandtracestogetmore\ndetail.\n20.5.1 Best practices\nAsnewmetricsareaddedandoldonesremoved,chartsanddash-\nboardsneedtobemodiﬁedandbekeptin-syncacrossmultipleen-\nvironmentslikestagingandproduction. Themosteffectivewayto\nachievethatisbydeﬁningdashboardsandchartswithadomain-\nspeciﬁclanguageandversion-controlthemjustlikecode. Thisal-\nlows updating dashboards from the same pull request that con-\ntainsrelatedcodechangeswithoutneedingtoupdatedashboards\nmanually,whichiserror-prone.\nAs dashboards render top to bottom, the most important charts\nshouldalwaysbelocatedattheverytop.\nCharts should be rendered with a default timezone, like UTC, to\neasethecommunicationbetweenpeoplelocatedindifferentparts\noftheworldwhenlookingatthesamedata.\nSimilarly, all charts in the same dashboard should use the same\ntimeresolution(e.g.,1min,5min,1hour,etc.) andrange(24hours,\n7days,etc.). Thismakesiteasytocorrelateanomaliesacrosscharts\ninthesamedashboardvisually. Youshouldpickthedefaulttime\nrange and resolution based on the most common use case for a\nCHAPTER20. MONITORING 222\ndashboard. For example, a 1-hour range with a 1-min resolution\nisbesttomonitoranongoingincident,whilea1-yearrangewith\na1-dayresolutionisbestforcapacityplanning.\nYou should keep the number of data points and metrics on the\nsamecharttoaminimum. Renderingtoomanypointsdoesn’tjust\nslow downloading charts, but also makes them hard to interpret\nandspotanomalies.\nAchartshouldcontainonlymetricswithsimilarranges(minand\nmaxvalues);otherwise,themetricwiththelargestrangecancom-\npletely hide the others with smaller ranges. For that reason, it\nmakessensetosplitrelatedstatisticsforthesamemetricintomul-\ntiple charts. For example, the 10th percentile, average and 90th\npercentileofametriccanbedisplayedinonechart,whilethe0.1th\npercentile,99.9thpercentile,minimumandmaximuminanother.\nAchartshouldalsocontainusefulannotations,like:\n•a description of the chart with links to runbooks, related\ndashboards,andescalationcontacts;\n•ahorizontallineforeachconﬁguredalertthreshold,ifany;\n•averticallineforeachrelevantdeployment.\nMetricsthatareonlyemittedwhenanerrorconditionoccurscan\nbe hard to interpret as charts will show wide gaps between the\ndata points, leaving the operator wondering whether the service\nstopped emitting that metric due to a bug. To avoid this, emit a\nmetricusingavalueofzerointheabsenceofanerrorandavalue\nof1inthepresenceofit.\n20.6 On-call\nAhealthyon-callrotationisonlypossiblewhenservicesarebuilt\nfrom the ground up with reliability and operability in mind. By\nmakingthedevelopersresponsibleforoperatingwhattheybuild,\ntheyareincentivizedtoreducetheoperationaltolltoaminimum.\nThey are also in the best position to be on-call since they are in-\ntimately familiar with the system’s architecture, brick walls, and\nCHAPTER20. MONITORING 223\ntrade-offs.\nBeing on-call can be very stressful. Even when there are no call-\nouts,justthethoughtofnothavingthesamefreedomusuallyen-\njoyedoutsideofregularworkinghourscancauseanxiety. Thisis\nwhybeingon-callshouldbecompensated,andthereshouldn’tbe\nanyexpectationsfortheon-callengineertomakeanyprogresson\nfeaturework. Sincetheywillbeinterruptedbyalerts,theyshould\nmakethemostoutofitandbegivenfreereintoimprovetheon-\ncallexperience,forexample,byrevisingdashboardsorimproving\nresiliencymechanisms.\nAchievingahealthyon-callisonlypossiblewhenalertsareaction-\nable. Whenanalerttriggers,totheveryleast,itshouldlinktorele-\nvantdashboardsandarun-bookthatliststheactionstheengineer\nshouldtake,asit’salltooeasytomissastepwhenyougetacall\ninthemiddleofthenight15. Unlessthealertwasafalsepositive,\nall actions taken by the operator should be communicated into a\nsharedchannellikeaglobalchat,that’saccessiblebyotherteams.\nThis allows others to chime in, track the incident’s progress, and\nmakeiteasiertohandoveranongoingincidenttosomeoneelse.\nTheﬁrststeptoaddressanalertistomitigateit,notﬁxtheunder-\nlyingrootcausethatcreatedit. Anewartifacthasbeenrolledout\nthatdegradestheservice? Rollitback. Theservicecan’tcopewith\ntheloadeventhoughithasn’tincreased? Scaleitout.\nOncetheincidenthasbeenmitigated,thenextstepistobrainstorm\nways to prevent it from happening again. The more widespread\ntheimpactwas,themoretimeyoushouldspendonthis. Incidents\nthatburnedasigniﬁcantfractionofanSLO’serrorbudgetrequire\napostmortem .\nApostmortem’sgoalistounderstandanincident’srootcauseand\ncome up with a set of repair items that will prevent it from hap-\npeningagain. Thereshouldalsobeanagreementintheteamthat\nifanSLO’serrorbudgetisburnedorthenumberofalertsspirals\n15Forthesamereason,youshouldautomatewhatyoucantominimizemanual\nactions that operators need to perform. Machines are good at following instruc-\ntions;usethattoyouradvantage.\nCHAPTER20. MONITORING 224\nout of control, the whole team stops working on new features to\nfocusexclusivelyonreliabilityuntilahealthyon-callrotationhas\nbeenrestored.\nThe SRE books16provide a wealth of information and best prac-\nticesregardingsettingupahealthyon-callrotation.\n16https://sre.google/books/",5801
92-Logs.pdf,92-Logs,"Chapter 21\nObservability\nA distributed system is never 100% healthy at any given time\nas there can always be something failing. A whole range of\nfailure modes can be tolerated, thanks to relaxed consistency\nmodels and resiliency mechanisms like rate limiting, retries, and\ncircuit breakers. Unfortunately, they also increase the system’s\ncomplexity. And with more complexity, it becomes increasingly\nhardertoreasonaboutthemultitudeofemergentbehavioursthe\nsystemmightexperience.\nAs we have discussed, human operators are still a fundamental\npart of operating a service as there are things that can’t be auto-\nmated,likemitigatinganincident. Debuggingisanotherexample\nofsuchatask. Whenasystemisdesignedtotoleratesomelevelof\ndegradationandself-heal,it’snotnecessaryorpossibletomonitor\neverywayitcangetintoanunhealthystate. Youstillneedtooling\nandinstrumentationtodebugcomplexemergentfailuresbecause\ntheyareimpossibletopredictup-front.\nWhen debugging, the operator makes an hypothesis and tries to\nvalidate it. For example, the operator might get suspicious after\nnoticing that the variance of her service’s response time has in-\ncreased slowly but steadily over the past weeks, indicating that\nsomerequeststakemuchlongerthanothers. Aftercorrelatingthe\nCHAPTER21. OBSERVABILITY 226\nincreaseinvariancewithanincreaseintrafﬁc,theoperatorhypoth-\nesizesthattheserviceisgettingclosertohittingaconstraint,likea\nlimitoraresourcecontention. Metricsandchartsalonewon’thelp\ntovalidatethishypothesis.\nObservability is a set of tools that provide granular insights into\na system in production, allowing us to understand its emergent\nbehaviours. Agoodobservabilityplatformstrivestominimizethe\ntimeittakestovalidatehypotheses. Thisrequiresgranularevents\nwithrichcontexts,sinceit’snotpossibletoknowup-frontwhat’s\ngoingtobeusefulinthefuture.\nAtthecoreofobservability,weﬁndtelemetrysourceslikemetrics,\neventlogs,andtraces. Metricsarestoredintime-seriesdatastores\nthathavehighthroughput,butstruggletodealwithmetricsthat\nhavemanydimensions. Conversely,eventlogsandtracesendup\nintransactionalstoresthatcanhandlehigh-dimensionaldatawell,\nbut struggle with high throughput. Metrics are mainly used for\nmonitoring,whileeventlogsandtracesmainlyfordebugging.\nObservabilityisasupersetofmonitoring. Whilemonitoringisfo-\ncusedexclusivelyontrackingthehealthofasystem,observability\nalsoprovidestoolstounderstandanddebugit. Monitoringonits\nownisgoodatdetectingfailuresymptoms,butlesssotoexplain\ntheirrootcause(seeFigure 21.1).\n21.1 Logs\nAlogisanimmutablelistoftime-stampedeventsthathappened\nover time. An eventcan have different formats. In its simplest\nform, it’s just free-form text. It can also be structured and repre-\nsentedwithatextualformatlikeJSON,orabinaryonelikeProto-\nbuf. Whenstructured,aneventistypicallyrepresentedwithabag\nofkey-valuepairs:\n{\n""failureCount"" :1,\n""serviceRegion"" :""EastUs2"" ,\n""timestamp"" :1614438079\nCHAPTER21. OBSERVABILITY 227\nFigure21.1: Observabilityisasupersetofmonitoring.\n}\nLogscanoriginatefromyourservicesandexternaldependencies,\nlikemessagebrokers,proxies,databases,etc. Mostlanguagesoffer\nlibraries that make it easy to emit structured logs. Logs are typi-\ncally dumped to disk ﬁles, which are rotated every so often, and\nforwardedbyanagenttoanexternallogcollectorasynchronously,\nlikeanELKstack1orAWSCloudWatchlogs.\nLogsprovideawealthofinformationabouteverythingthat’shap-\npening in a service. They are particularly helpful for debugging\npurposes,astheyallowustotracebacktherootcausefromasymp-\ntom, like a service instance crash. They also help to investigate\nlong-tail behaviors that are missed by metrics represented with\naveragesandpercentiles,whichcan’thelpexplainwhyaspeciﬁc\nuserrequestisfailing.\nLogs are very simple to emit, particularly free-form textual ones.\nBut that’s pretty much the only advantage they have compared\n1https://www.elastic.co/what-is/elk-stack\nCHAPTER21. OBSERVABILITY 228\ntometricsandotherinstrumentationtools. Logginglibrariescan\nadd overhead to your services if misused, especially when they\narenotasynchronousandloggingblockswhilewritingtostdout\nordisk. Also,ifthediskﬁllsupduetoexcessivelogging,theser-\nvice instance might get itself into a degraded state. At best, you\nloselogging,andatworst,theserviceinstancestopsworkingifit\nrequiresdiskaccesstohandlerequests.\nIngesting, processing, and storing a massive trove of data is not\ncheapeither,nomatterwhetheryouplantodothisin-houseoruse\na third-party service. Although structured binary logs are more\nefﬁcientthantextualones,theyarestillexpensiveduetotheirhigh\ndimensionality.\nFinally,butnotlessimportant,logshaveahighnoisetosignalratio\nbecausetheyareﬁne-grainedandservice-speciﬁc,whichmakesit\nchallengingtoextractusefulinformationfromthem.\nBest Practices\nTomakethejoboftheengineerdrillingintothelogslesspainful,\nallthedataaboutaspeciﬁc workunitshouldbestoredinasingle\nevent. Aworkunittypicallycorrespondstoarequestoramessage\npulledfromaqueue. Toeffectivelyimplementthispattern, code\npaths handling work units need to pass around a context object\ncontainingtheeventbeingbuilt.\nAneventshouldcontainusefulinformationabouttheworkunit,\nlike who created it, what it was for, and whether it succeeded or\nfailed. Itshouldincludemeasurementsaswell,likehowlongspe-\nciﬁc operations took. Every network call performed within the\nworkunitneedstobeinstrumentedtologitsresponsestatuscode\nand response time. Finally, data logged to the event should be\nsanitized and stripped of potentially sensitive properties that de-\nvelopersshouldn’thaveaccessto,likeusercontent.\nCollatingalldatawithinasingleeventforaworkunitminimizes\ntheneedforjoinsbutdoesn’tcompletelyeliminateit. Forexample,\nifaservicecallsanotherdownstream,youwillhavetoperforma\njoin to correlate the caller’s event log with the callee’s one to un-",5887
93-Traces.pdf,93-Traces,"CHAPTER21. OBSERVABILITY 229\nderstandwhytheremotecallfailed. Tomakethatpossible,every\neventshouldincludetheidoftherequestormessageforthework\nunit.\nCosts\nThere are various ways to keep the costs of logging under con-\ntrol. A simple approach is to have different logging levels (e.g.:\ndebug,info,warning,error)controlledbyadynamicknobthatde-\ntermineswhichonesareemitted. Thisallowsoperatorstoincrease\ntheloggingverbosityforinvestigationpurposesandreducecosts\nwhengranularlogsaren’tneeded.\nSampling2is another option to reduce verbosity. For example, a\nservicecouldlogonlyoneeveryn-thevent. Additionally,events\ncanalsobeprioritizedbasedontheirexpectedsignaltonoisera-\ntio;forexample,loggingfailedrequestsshouldhaveahighersam-\nplingfrequencythanloggingsuccessfulones.\nTheoptionsdiscussedsofaronlyreducetheloggingverbosityon\nasinglenode. Asyouscaleoutandaddmorenodes,loggingvol-\numewillnecessarilyincrease. Evenwiththebestintentions,some-\nonecouldcheck-inabugthatleadstoexcessivelogging. Toavoid\ncostssoaringthroughtherooforkillingyourloggingpipelineen-\ntirely, log collectors need to be able to rate-limit requests. If you\nuseathird-partyservicetoingest,store,andqueryyourlogs,there\nprobablyisaquotainplacealready.\nOf course, you can always opt to create in-memory aggregates\nfromthemeasurementscollectedinevents(e.g.,metrics)andemit\njustthoseratherthanrawlogs. Bydoingso,youtrade-offtheabil-\nitytodrilldownintotheaggregatesifneeded.\n21.2 Traces\nTracing captures the entire lifespan of a request as it propagates\nthroughout the services of a distributed system. A traceis a list\n2https://www.honeycomb.io/blog/dynamic-sampling-by-example/\nCHAPTER21. OBSERVABILITY 230\nofcausally-relatedspansthatrepresenttheexecutionﬂowofare-\nquestinasystem. A spanrepresentanintervaloftimethatmapsto\nalogicaloperationorworkunit,andcontainsabagofkey-value\npairs(seeFigure 21.2).\nFigure21.2: Anexecutionﬂowcanberepresentedwithspans.\nTracesallowdevelopersto:\n•debug issues affecting very speciﬁc requests, which can be\nusedtoinvestigatesupportrequests;\n•debugrareissuesthataffectonlyanextremelysmallfraction\nofrequests;\n•debugissuesthataffectalargefractionofrequests,likehigh\nresponse times for requests that hit a speciﬁc subset of ser-\nviceinstances;\n•Identifybottlenecksintheend-to-endrequestpath;\n•Identify which clients hit which downstream services and\ninwhatproportion(alsoreferredtoasresourceattribution),\nwhichcanbeusedforrate-limitingorbillingpurposes.\nWhenarequestbegins,it’sassignedauniquetraceid. Thetraceid\nispropagatedfromonestagetoanotherateveryforkinthelocal\nexecutionﬂowfromonethreadtoanotherandfromcallertocallee",2660
94-Putting it all together.pdf,94-Putting it all together,"CHAPTER21. OBSERVABILITY 231\ninanetworkcall(throughHTTPheaders,forexample). Eachstage\nisrepresentedwithaspan—aneventcontainingthetraceid.\nWhenaspanends,it’semittedtoacollectorservice,whichassem-\nblesitintoatracebystitchingittogetherwiththeotherspansbe-\nlonging to the same trace. Popular distributed tracing collectors\nincludeOpenZipkin3andAWSX-ray4.\nTracing is challenging to retroﬁt into an existing system as it re-\nquires every component in the request path to be modiﬁed and\npropagate the trace context from one stage to the other. And it’s\nnot just the components that are under your control that need to\nsupporttracing;theframeworksandopensourcelibrariesyouuse\nneedtosupportitaswell,justlikethird-partyservices5.\n21.3 Putting it all together\nThemaindrawbackofeventlogsisthattheyareﬁne-grainedand\nservice-speciﬁc.\nWhen a single user request ﬂows through a system, it can pass\nthrough several services. A speciﬁc event only contains informa-\ntionfortheworkunitofonespeciﬁcservice,soitcan’tbeofmuch\nuse to debug the entire request ﬂow. Similarly, a single event\ndoesn’ttellmuchaboutthehealthorstateofaspeciﬁcservice.\nThis is where metrics and traces come in. You can think of them\nasabstractions,orderivedviews,builtfromeventlogsandtuned\nto speciﬁc use cases. A metric is a time-series of summary statis-\nticsderivedbyaggregatingcountersorobservationsovermultiple\nworkunitsorevents. Youcouldemitcountersineventsandhave\nthebackendrollthemupintometricsastheyareingested. Infact,\nthisishowsomemetricscollectionsystemswork.\nSimilarly,atracecanbederivedbyaggregatingalleventsbelong-\ning to the lifecycle of a speciﬁc user request into an ordered list.\n3https://zipkin.io/\n4https://aws.amazon.com/xray/\n5Theservicemeshpatterncanhelpretroﬁttracing.\nCHAPTER21. OBSERVABILITY 232\nJustlikeinthepreviouscase,youcanemitindividualspanevents\nandhavethebackendaggregatethemtogetherintotraces.",1916
95-Final words.pdf,95-Final words,"Chapter 22\nFinal words\nCongratulations, you reached the end of the book! I hope you\nlearnedsomethingyoudidn’tknowbeforeandperhapsevenhad\na few “aha” moments. Although this is the end of the book, it’s\njustthebeginningofyourjourney. Oneofthebestwaystolearn\nhowtodesignlargescalesystemsisbystandingontheshoulders\nofthegiants.\nIndustrypapersprovideawealthofknowledgeaboutdistributed\nsystems that have stood the test of time. My recommendation\nis to start with “Windows Azure Storage: A Highly Available\nCloud Storage Service with Strong Consistency1,” which de-\nscribes Azure’s cloud storage system2. Azure’s cloud storage is\nthe core building block on top of which Microsoft built many\nother successful products. You will see many of the concepts\nintroduced in the book there. One of the key design decisions\nwastoguaranteestrongconsistency,unlikeAWSS33,makingthe\napplicationdevelopers’jobmucheasier.\nOnce you have digested that, I suggest reading “Azure Data Ex-\n1https://sigops.org/s/conferences/sosp/2011/current/2011-Cascais/printa\nble/11-calder.pdf\n2ThinkofitasAzure’sequivalentofAWSS3,whichunfortunatelydoesn’thave\napublicpaper\n3S3supportsstrongconsistencysinceDecember2020,though\nCHAPTER22. FINALWORDS 234\nplorer: a big data analytics cloud platform optimized for interac-\ntive,adhocqueriesoverstructured,semi-structuredandunstruc-\ntureddata4.” Thepaperdiscussestheimplementationofacloud-\nnativeeventstorebuiltontopofAzure’scloudstorage—agreat\nexampleofhowtheselargescalesystemscomposeontopofeach\nother5.\nFinally,ifyouarepreparingforthesystemdesigninterview,check\noutAlexXu’sbook“SystemDesignInterview6”. Thebookintro-\nducesaframeworktotackledesigninterviewsandincludesmore\nthan10casestudies.\n4https://azure.microsoft.com/mediahandler/files/resourcefiles/azure-data-\nexplorer/Azure_Data_Explorer_white_paper.pdf\n5Iworkedonatime-seriesdatastorethatbuildsontopofAzureDataExplorer\nandAzureStorage,unfortunately,nopublicpaperisavailableforitjustyet\n6https://www.amazon.co.uk/dp/B08CMF2CQF",2028
