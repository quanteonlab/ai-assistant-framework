filename,title,text,len
01-BRIEF CONTENTS.pdf,01-BRIEF CONTENTS,"THE BOOK OF KUBERNETES\nA Complete Guide to Container Orchestration\nby Alan Hohn\nSan Francisco\nTHE BOOK OF KUBERNETES.\n Copyright © 2022 by Alan Hohn.\nAll rights reserved. No part of this work may be reproduced or transmitted in any form or by any\nmeans, electronic or mechanical, including photocopying, recording, or by any information storage or\nretrieval system, without the prior written permission of the copyright owner and the publisher.\nFirst printing\n26 25 24 23 22          1 2 3 4 5\nISBN-13: 978-1-7185-0264-2 (print)\nISBN-13: 978-1-7185-0265-9 (ebook)\nPublisher: William Pollock\nManaging Editor: Jill Franklin\nProduction Manager: Rachel Monaghan\nProduction Editors: Paula Williamson and Jennifer Kepler\nDevelopmental Editor: Jill Franklin\nCover Illustrator: Gina Redman\nInterior Design: Octopod Studios\nTechnical Reviewer: Xander Soldaat\nProduction Services: Octal Publishing, Inc.\nFor information on distribution, bulk sales, corporate sales, or translations, please contact No Starch\nPress, Inc. directly at \ninfo@nostarch.com\n or:\nNo Starch Press, Inc.\n245 8th Street, San Francisco, CA 94103\nphone: 1.415.863.9900\nwww.nostarch.com\nLibrary of Congress Control Number: 2022020536\nNo Starch Press and the No Starch Press logo are registered trademarks of No Starch Press, Inc. Other\nproduct and company names mentioned herein may be the trademarks of their respective owners.\nRather than use a trademark symbol with every occurrence of a trademarked name, we are using the\nnames only in an editorial fashion and to the benefit of the trademark owner, with no intention of\ninfringement of the trademark.\nThe information in this book is distributed on an “As Is” basis, without warranty. While every\nprecaution has been taken in the preparation of this work, neither the author nor No Starch Press, Inc.\nshall have any liability to any person or entity with respect to any loss or damage caused or alleged to\nbe caused directly or indirectly by the information contained in it.\nFor my wife, Sheryl\nAbout the Author\nAlan Hohn is the director for software strategy for Lockheed Martin. He has\n25 years of experience as a Lockheed Martin Fellow, software developer,\narchitect, lead, and manager. He has delivered real applications to production\nin Ada, Java, Python, and Go, among others, and has worked with Linux\nsince the early 1990s. He is an Agile and DevSecOps coach and is an\nexperienced trainer for Java, Ansible, containers, software architecture, and\nKubernetes. Alan has a degree in computer science from Embry-Riddle\nAeronautical University, a master’s in business administration from the\nUniversity of Minnesota, and a master’s in industrial engineering from the\nGeorgia Institute of Technology.\nAbout the Technical Reviewer\nXander Soldaat started his Linux journey back in 1994 with a sports bag full\nof floppy disks, a 486DX2/66, and a spare weekend. He has a deep\nbackground in IT infrastructure architecture, as well as embedded systems,\ncompiler, and STEM curriculum development. He is currently an OpenShift\nCloud Success Architect at Red Hat. In his spare time, he likes to tinker with\nrobots, electronics, retro computers, and tabletop games.\nBRIEF CONTENTS\nAcknowledgments\nIntroduction\nPART I: MAKING AND USING CONTAINERS\nChapter 1: Why Containers Matter\nChapter 2: Process Isolation\nChapter 3: Resource Limiting\nChapter 4: Network Namespaces\nChapter 5: Container Images and Runtime Layers\nPART II: CONTAINERS IN KUBERNETES\nChapter 6: Why Kubernetes Matters\nChapter 7: Deploying Containers to Kubernetes\nChapter 8: Overlay Networks\nChapter 9: Service and Ingress Networks\nChapter 10: When Things Go Wrong\nChapter 11: Control Plane and Access Control\nChapter 12: Container Runtime\nChapter 13: Health Probes\nChapter 14: Limits and Quotas\nChapter 15: Persistent Storage\nChapter 16: Configuration and Secrets\nChapter 17: Custom Resources and Operators\nPART III: PERFORMANT KUBERNETES\nChapter 18: Affinity and Devices\nChapter 19: Tuning Quality of Service\nChapter 20: Application Resiliency\nIndex",4106
02-CONTENTS IN DETAIL.pdf,02-CONTENTS IN DETAIL,CONTENTS IN DETAIL\nACKNOWLEDGMENTS\nINTRODUCTION\nThe Approach\nRunning Examples\nWhat You Will Need\nRun in the Cloud or Local\nTerminal Windows\nPART I\nMAKING AND USING CONTAINERS\n1\nWHY CONTAINERS MATTER\nModern Application Architecture\nAttribute: Cloud Native\nAttribute: Modular\nAttribute: Microservice-Based\nBenefit: Scalability\nBenefit: Reliability\nBenefit: Resilience\nWhy Containers\nRequirements for Containers\nRequirements for Orchestration\nRunning Containers\nWhat Containers Look Like\nWhat Containers Really Are\nDeploying Containers to Kubernetes\nTalking to the Kubernetes Cluster\nApplication Overview\nKubernetes Features\nFinal Thoughts\n2\nPROCESS ISOLATION\nUnderstanding Isolation\nWhy Processes Need Isolation\nFile Permissions and Change Root\nContainer Isolation\nContainer Platforms and Container Runtimes\nInstalling containerd\nUsing containerd\nIntroducing Linux Namespaces\nContainers and Namespaces in CRI-O\nRunning Processes in Namespaces Directly\nFinal Thoughts\n3\nRESOURCE LIMITING\nCPU Priorities\nReal-Time and Non-Real-Time Policies\nSetting Process Priorities\nLinux Control Groups\nCPU Quotas with cgroups\nCPU Quota with CRI-O and crictl\nMemory Limits\nNetwork Bandwidth Limits\nFinal Thoughts\n4\nNETWORK NAMESPACES\nNetwork Isolation\nNetwork Namespaces\nInspecting Network Namespaces\nCreating Network Namespaces\nBridge Interfaces\nAdding Interfaces to a Bridge\nTracing Traffic\nMasquerade\nFinal Thoughts\n5\nCONTAINER IMAGES AND RUNTIME LAYERS\nFilesystem Isolation\nContainer Image Contents\nImage Versions and Layers\nBuilding Container Images\nUsing a Dockerfile\nTagging and Publishing Images\nImage and Container Storage\nOverlay Filesystems\nUnderstanding Container Layers\nPractical Image Building Advice\nOpen Container Initiative\nFinal Thoughts\nPART II\nCONTAINERS IN KUBERNETES\n6\nWHY KUBERNETES MATTERS\nRunning Containers in a Cluster\nCross-Cutting Concerns\nKubernetes Concepts\nCluster Deployment\nPrerequisite Packages\nKubernetes Packages\nCluster Initialization\nJoining Nodes to the Cluster\nInstalling Cluster Add-ons\nNetwork Driver\nInstalling Storage\nIngress Controller\nMetrics Server\nExploring a Cluster\nFinal Thoughts\n7\nDEPLOYING CONTAINERS TO KUBERNETES\nPods\nDeploying a Pod\nPod Details and Logging\nDeployments\nCreating a Deployment\nMonitoring and Scaling\nAutoscaling\nOther Controllers\nJobs and CronJobs\nStatefulSets\nDaemon Sets\nFinal Thoughts\n8\nOVERLAY NETWORKS\nCluster Networking\nCNI Plug-ins\nPod Networking\nCross-Node Networking\nCalico Networking\nWeaveNet\nChoosing a Network Plug-in\nNetwork Customization\nFinal Thoughts\n9\nSERVICE AND INGRESS NETWORKS\nServices\nCreating a Service\nService DNS\nName Resolution and Namespaces\nTraffic Routing\nExternal Networking\nExternal Services\nIngress Services\nIngress in Production\nFinal Thoughts\n10\nWHEN THINGS GO WRONG\nScheduling\nNo Available Nodes\nInsufficient Resources\nPulling Images\nRunning Containers\nDebugging Using Logs\nDebugging Using Exec\nDebugging Using Port Forwarding\nFinal Thoughts\n11\nCONTROL PLANE AND ACCESS CONTROL\nAPI Server\nAPI Server Authentication\nClient Certificates\nBootstrap Tokens\nService Accounts\nRole-Based Access Controls\nRoles and Cluster Roles\nRole Bindings and Cluster Role Bindings\nAssigning a Service Account to Pods\nBinding Roles to Users\nFinal Thoughts\n12\nCONTAINER RUNTIME\nNode Service\nKubelet Cluster Configuration\nKubelet Container Runtime Configuration\nKubelet Network Configuration\nStatic Pods\nNode Maintenance\nNode Draining and Cordoning\nUnhealthy Nodes\nNode Unreachable\nFinal Thoughts\n13\nHEALTH PROBES\nAbout Probes\nLiveness Probes\nExec Probes\nHTTP Probes\nTCP Probes\nStartup Probes\nReadiness Probes\nFinal Thoughts\n14\nLIMITS AND QUOTAS\nRequests and Limits\nProcessing and Memory Limits\nCgroup Enforcement\nNetwork Limits\nQuotas\nFinal Thoughts\n15\nPERSISTENT STORAGE\nStorage Classes\nStorage Class Definition\nCSI Plug-in Internals\nPersistent Volumes\nStateful Sets\nVolumes and Claims\nDeployments\nAccess Modes\nFinal Thoughts\n16\nCONFIGURATION AND SECRETS\nInjecting Configuration\nExternalizing Configuration\nProtecting Secrets\nInjecting Files\nCluster Configuration Repository\nUsing etcdctl\nDeciphering Data in etcd\nFinal Thoughts\n17\nCUSTOM RESOURCES AND OPERATORS\nCustom Resources\nCreating CRDs\nWatching CRDs\nOperators\nFinal Thoughts\nPART III\nPERFORMANT KUBERNETES\n18\nAFFINITY AND DEVICES\nAffinity and Anti-affinity\nAnti-affinity\nAffinity\nService Traffic Routing\nHardware Resources\nFinal Thoughts\n19\nTUNING QUALITY OF SERVICE\nAchieving Predictability\nQuality of Service Classes\nBestEffort\nBurstable\nGuaranteed\nQoS Class Eviction\nChoosing a QoS Class\nPod Priority\nFinal Thoughts\n20\nAPPLICATION RESILIENCY\nExample Application Stack\nDatabase\nApplication Deployment\nPod Autoscaling\nApplication Service\nApplication and Cluster Monitoring\nPrometheus Monitoring\nDeploying kube-prometheus\nCluster Metrics\nAdding Monitoring for Services\nFinal Thoughts\nINDEX,5064
03-INTRODUCTION.pdf,03-INTRODUCTION,"ACKNOWLEDGMENTS\nThanks to the many people who have been generous with knowledge and\nhelp in creating this book. First, thanks to my editor, Jill Franklin, my\ntechnical reviewer, Xander Soldaat, and my copyeditor, Bob Russell, for\nspotting errors I didn’t see and filling gaps in my knowledge. The remaining\nmistakes are mine. They would have been much more numerous without your\nhelp.\nThanks to my colleagues at Lockheed Martin, especially our Software\nFactory team. I have learned a great deal from you, and we have built many\ncool things together. Thanks to my Application Based Architecture\ncolleagues who explored Kubernetes with me in the early days. Thanks also\nto the many people who build the open source products and the community\naround containers and Kubernetes; I am humbled by the chance to contribute.\nI am most grateful to my family for helping to make this book possible\nand for listening patiently as I described each current challenge in writing it.\nMy thanks goes to all these, but \nSoli Deo Gloria.",1034
04-Running Examples.pdf,04-Running Examples,"INTRODUCTION\nContainers and Kubernetes together are changing the way that applications\nare architected, developed, and deployed. Containers ensure that software\nruns reliably no matter where it’s deployed, and Kubernetes lets you manage\nall of your containers from a single control plane.\nThis book is designed to help you take full advantage of these essential\nnew technologies, using hands-on examples not only to try out the major\nfeatures but also to explore how each feature works. In this way, beyond\nsimply being ready to deploy an application to Kubernetes, you’ll gain the\nskills to architect applications to be performant and reliable in a Kubernetes\ncluster, and to quickly diagnose problems when they arise.\nThe Approach\nThe biggest advantage of a Kubernetes cluster is that it hides the work of\nrunning containers across multiple hosts behind an abstraction layer. A\nKubernetes cluster is a “black box” that runs what we tell it to run, with\nautomatic scaling, failover, and upgrades to new versions of our application.\nEven though this abstraction makes it easier to deploy and manage\napplications, it also makes it difficult to understand what a cluster is doing.\nFor this reason, this book presents each feature of container runtimes and\nKubernetes clusters from a “debugging” perspective. Every good debugging\nsession starts by treating the application as a black box and observing its\nbehavior, but it doesn’t end there. Skilled problem solvers know how to open\nthe black box, diving below the current abstraction layer to see how the\nprogram runs, how data is stored, and how traffic flows across the network.\nSkilled architects use this deep knowledge of a system to avoid performance\nand reliability issues. This book provides the detailed understanding of\ncontainers and Kubernetes that only comes from exploring not only what\nthese technologies do but also how they work.\nIn \nPart I\n, we’ll begin by running a container, but then we’ll dive into the\ncontainer runtime to understand what a container is and how we can simulate\na container using normal operating system commands. In \nPart II\n, we’ll install\na Kubernetes cluster and deploy containers to it. We’ll also see how the\ncluster works, including how it interacts with the container runtime and how\npackets flow from container to container across the host network. The\npurpose is not to duplicate the reference documentation to show every option\noffered by every feature but to demonstrate how each feature is implemented\nso that all that documentation will make sense and be useful.\nA Kubernetes cluster is complicated, so this book includes extensive\nhands-on examples, with enough automation to allow you to explore each\nchapter independently. This automation, which is available at\nhttps://github.com/book-of-kubernetes/examples\n, is published under a\npermissive open source license, so you can explore, experiment, and use it in\nyour own projects.\nRunning Examples\nIn many of this book’s example exercises, you’ll be combining multiple hosts\ntogether to make a cluster, or working with low-level features of the Linux\nkernel. For this reason, and to help you feel more comfortable with\nexperimentation, you’ll be running examples entirely on temporary virtual\nmachines. That way, if you make a mistake, you can quickly delete the\nvirtual machine and start over.\nThe example repository for this book is available at\nhttps://github.com/book-of-kubernetes/examples\n. All of the instructions for\nsetting up to run examples are provided in a \nREADME.md\n file within the\nsetup\n folder of the example repository.\nWhat You Will Need\nEven though you’ll be working in virtual machines, you’ll need a control\nmachine to start from that can run Windows, macOS, or Linux. It can even be\na Chromebook that supports Linux. If you are running Windows, you’ll need\nto use the Windows Subsystem for Linux (WSL) in order to get Ansible\nworking. See the \nREADME.md\n in the \nsetup\n folder for instructions.\nRun in the Cloud or Local\nTo make these examples as accessible as possible, I’ve provided automation\nto run them either using Vagrant or Amazon Web Services (AWS). If you\nhave access to a Windows, macOS, or Linux computer with at least eight\ncores and 8GB of memory, try installing VirtualBox and Vagrant and work\nwith local virtual machines. If not, you can set yourself up to work with\nAWS.\nWe use Ansible to perform AWS setup and automate some of the tedious\nsteps. Each chapter includes a separate Ansible playbook that makes use of\ncommon roles and collections. This means that you can work examples from\nchapter to chapter, starting with a fresh installation each time. In some cases,\nI’ve also provided an “extra” provisioning playbook that you can optionally\nuse to skip some of the detailed installation steps and get straight to the\nlearning. See the \nREADME.md\n in each chapter’s directory for more\ninformation.\nTerminal Windows\nAfter you’ve used Ansible to provision your virtual machines, you’ll need to\nget at least one terminal window connected to run commands. The\nREADME.md\n file in each chapter will tell you how to do that. Before running\nany examples, you’ll first need to become the root user, as follows:\nsudo su -\nThis will give you a root shell and set up your environment and home\ndirectory to match.\nRUNNING AS ROOT\nIf you’ve worked with Linux before, you probably have a healthy\naversion to working as root on a regular basis, so it might surprise you\nthat all of the examples in this book are run as the root user. This is a\nbig advantage of using temporary virtual machines and containers;\nwhen we act as the root user, we are doing so in a temporary, confined\nspace that can’t reach out and affect anything else.\nAs you move from learning about containers and Kubernetes to running\napplications in production, you’ll be applying security controls to your\ncluster that will limit administrative access and will ensure that\ncontainers cannot break out of their isolated environment. This often\nincludes configuring your containers so that they run as a non-root user.\nIn some examples, you’ll need to open multiple terminal windows in order\nto leave one process running while you inspect it from another terminal. How\nyou do that is up to you; most terminal applications support multiple tabs or\nmultiple windows. If you need a way to open multiple terminals within a\nsingle tab, try exploring a terminal multiplexer application. All of the\ntemporary virtual machines used in the examples come with both \nscreen\n and\ntmux\n installed and ready to use.",6669
05-1 WHY CONTAINERS MATTER.pdf,05-1 WHY CONTAINERS MATTER,"PART I\nMAKING AND USING CONTAINERS\nContainers are essential to modern application architecture. They simplify\npackaging, deploying, and scaling application components. They enable\nbuilding reliable and resilient applications that handle failure gracefully.\nHowever, containers can also be confusing. They look like completely\ndifferent systems, with separate hostnames, networking, and storage, but they\ndo not have many of the features of a separate system, such as a separate\nconsole or system services. To understand how containers look like separate\nsystems without really being separate, let’s explore containers, container\nengines, and Linux kernel features.",674
06-Modern Application Architecture.pdf,06-Modern Application Architecture,"1\nWHY CONTAINERS MATTER\nIt’s a great time to be a software developer. Creating a brand-new application\nand making it available to millions of people has never been easier. Modern\nprogramming languages, open source libraries, and application platforms\nmake it possible to write a small amount of code and end up with lots of\nfunctionality. However, although it’s easy to get started and create a new\napplication quickly, the best application developers are those who move\nbeyond treating the application platform as a “black box” and really\nunderstand how it works. Creating a reliable, resilient, and scalable\napplication requires more than just knowing how to create a Deployment in\nthe browser or on the command line.\nIn this chapter, we’ll look at application architecture in a scalable, cloud\nnative world. We will show why containers are the preferred way to package\nand deploy application components, and how container orchestration\naddresses key needs for containerized applications. We’ll finish with an\nexample application deployed to Kubernetes to give you an introductory\nglimpse into the power of these technologies.\nModern Application Architecture\nThe main theme of modern software applications is \nscale\n. We live in a world\nof applications with millions of simultaneous users. What is remarkable is the\nability of these applications to achieve not only this scale but also a level of\nstability such that an outage makes headlines and serves as fodder for weeks\nor months of technical analysis.\nWith so many modern applications running at large scale, it can be easy to\nforget that a lot of hard work goes into architecting, building, deploying, and\nmaintaining applications of this caliber, whether the scale they’re designed\nfor is thousands, millions, or billions of users. Our job in this chapter is to\nidentify what we need from our application platform to run a scalable,\nreliable application, and to see how containerization and Kubernetes meet\nthose requirements. We’ll start by looking at three key attributes of modern\napplication architecture. Then we’ll move on to looking at three key benefits\nthese attributes bring.\nAttribute: Cloud Native\nThere are lots of ways to define \ncloud native\n technologies (and a good place\nto start is the Cloud Native Computing Foundation at \nhttps://cncf.io\n). I like to\nstart with an idea of what “the cloud” is and what it enables so that we can\nunderstand what kind of architecture can make best use of it.\nAt its heart, the cloud is an abstraction. We talked about abstractions in\nthe introduction, so you know that abstractions are essential to computing,\nbut we also need a deep understanding of our abstractions to use them\nproperly. In the case of the cloud, the provider is abstracting away the real\nphysical processors, memory, storage, and networking, allowing cloud users\nto simply declare a need for these resources and have them provisioned on\ndemand. To have a “cloud native” application, then, we need an application\nthat can take advantage of that abstraction. As much as possible, the\napplication shouldn’t be tied to a specific host or a specific network layout,\nbecause we don’t want to constrain our flexibility in how application\ncomponents are divided among hosts.\nAttribute: Modular\nModularity\n is nothing new to application architecture. The goal has always\nbeen \nhigh cohesion\n, where everything within a module relates to a single\npurpose, and \nlow coupling\n, where modules are organized to minimize\nintermodule communication. However, even though modularity remains a\nkey design goal, the definition of what makes a module is different. Rather\nthan just treat modularity as a way of organizing the code, modern\napplication architecture today prefers to carry modularity into the runtime,\nproviding each \nmodule with a separate operating system process and\ndiscouraging the use of a shared filesystem or shared memory for\ncommunication. Because modules are separate processes, communication\nbetween modules is standard network (socket) communication.\nThis approach seems wasteful of hardware resources. It is more compact\nand faster to share memory than it is to copy data over a socket. But there are\ntwo good reasons to prefer separate processes. First, modern hardware is fast\nand getting faster, and it would be a form of premature optimization to\nimagine that sockets are not fast enough for our application. Second, no\nmatter how large a server we have, there is going to be a limit to how many\nprocesses we can fit on it, so a shared memory model ultimately limits our\nability to grow.\nAttribute: Microservice-Based\nModern application architecture is based on modules in the form of separate\nprocesses—and these individual modules tend to be very small. In theory, a\ncloud can provide us with virtual servers that are as powerful as we need;\nhowever, in practice, using a few powerful servers is more expensive and less\nflexible than many small servers. If our modules are small enough, they can\nbe deployed to cheap commodity servers, which means that we can leverage\nour cloud provider’s hardware to best advantage. Although there is no single\nanswer as to how small a module needs to be in order to be a \nmicroservice\n,\n“small enough that we can be flexible regarding where it is deployed” is a\ngood first rule.\nA microservice architecture also has practical advantages for organizing\nteams. Ever since Fred Brooks wrote \nThe Mythical Man-Month\n, architects\nhave understood that organizing people is one of the biggest challenges to\ndeveloping large, complex systems. Building a system from many small\npieces reduces the complexity of testing but also makes it possible to\norganize a large team of people without everyone getting in everyone else’s\nway.\nWHAT ABOUT APPLICATION SERVERS?\nThe idea of modular services has a long history, and one popular way to\nimplement it was building modules to run in an application server, such\nas a Java Enterprise environment. Why not then just continue to follow\nthat pattern for applications?\nAlthough application servers were successful for many uses, they don’t\nhave the same degree of isolation that a microservice architecture has.\nAs a result, there are more issues with interdependency, leading to more\ncomplex testing and reduced team independence. Additionally, the\ntypical model of having a single application server per host, with many\napplications deployed to it and sharing the same process space, is much\nless flexible than the containerized approaches you will see in this book.\nThis is not to say that you should immediately throw away your\napplication server architecture to use containers. There are lots of\nbenefits to containerization for any architecture. But as you adopt a\ncontainerized architecture, over time it will make sense for you to move\nyour code toward a true microservice architecture to take best advantage\nof what containers and Kubernetes offer.\nWe’ve looked at three key attributes of modern architecture. Now, let’s\nlook at three key benefits that result.\nBenefit: Scalability\nLet’s begin by envisioning the simplest application possible. We create a\nsingle executable that runs on a single machine and interacts with only a\nsingle user at a time. Now, suppose that we want to grow this application so\nthat it can interact with thousands or millions of users at once. Obviously, no\nmatter how powerful a server we use, eventually some computing resource\nwill become a bottleneck. It doesn’t matter whether the bottleneck is\nprocessing, or memory, or storage, or network bandwidth; the moment we hit\nthat bottleneck, our application cannot handle any additional users without\nhurting performance for others.\nThe only possible way to solve this issue is to stop sharing the resource\nthat caused the bottleneck. This means that we need to find a way to\ndistribute our application across multiple servers. But if we’re really scaling\nup, we can’t stop there. We need to distribute across multiple networks as\nwell, or we’ll hit the limit of what one network switch can do. And\neventually, we will even need to distribute geographically, or we’ll saturate\nthe broader network.\nTo build applications with no limit to scalability, we need an architecture\nthat can run additional application instances at will. And because an\napplication is only as slow as its slowest component, we need to find a way to\nscale \neverything\n, including our data stores. It’s obvious that the only way to\ndo this effectively is to create our application from many independent pieces\nthat are not tied to specific hardware. In other words, cloud native\nmicroservices.\nBenefit: Reliability\nLet’s go back to our simplest possible application. In addition to scalability\nlimits, it has another flaw. It runs on one server, and if that server fails, the\nentire application fails. Our application is lacking reliability. As before, the\nonly possible way to solve this issue is to stop sharing the resource that could\npotentially fail. Fortunately, when we start distributing our application across\nmany servers, we have the opportunity to avoid a single point of failure in the\nhardware that would bring down our application. And as an application is\nonly as reliable as its least reliable component, we need to find a way to\ndistribute everything, including storage and networks. Again, we \nneed cloud\nnative microservices that are flexible about where they are run and about how\nmany instances are running at once.\nBenefit: Resilience\nThere is a third, subtler advantage to cloud native microservice architecture.\nThis time, imagine an application that runs on a single server, but it can\neasily be installed as a single package on as many servers as we like. Each\ninstance can serve a new user. In theory, this application would have good\nscalability, given that we can always install it on another server. And overall,\nthe application could be said to be reliable because a failure of a single server\nis going to affect only that one user, whereas the others can keep running as\nnormal.\nWhat is missing from this approach is the concept of resilience, or the\nability of an application to respond meaningfully to failure. A truly resilient",10355
07-Why Containers.pdf,07-Why Containers,"application can handle a hardware or software failure somewhere in the\napplication without an end user noticing at all. And although separate,\nunrelated instances of this application keep running when one instance fails,\nwe can’t really say that the application exhibits resilience, at least not from\nthe perspective of the unlucky user with the failed system.\nOn the other hand, if we construct our application out of separate\nmicroservices, each of which has the ability to communicate over a network\nwith other microservices on any server, the loss of a single server might cost\nus several microservice instances, but end users can be moved to other\ninstances on other servers transparently, such that they don’t even notice the\nfailure.\nWhy Containers\nI’ve made modern application architecture with its fancy cloud native\nmicroservices sound pretty appealing. Engineering is full of trade-offs,\nhowever, so experienced engineers will suspect that there must be some\npretty significant trade-offs, and, of course, there are.\nIt’s \nvery difficult\n to build an application from lots of small pieces.\nOrganizing teams around microservices so that they can work independently\nfrom one another might be great, but when it comes time to put those together\ninto a working application, the sheer number of pieces means worrying about\nhow to package them up, how to deliver them to the runtime environment,\nhow to configure them, how to provide them with (potentially conflicting)\ndependencies, how to update them, and how to monitor them to make sure\nthey are working.\nThis problem only grows worse when we consider the need to run\nmultiple instances of each microservice. Now, we need a microservice to be\nable to find a working instance of another microservice, balancing the load\nacross all of the working instances. We need that load balancing to\nreconfigure itself immediately if we have a hardware or software failure. We\nneed to fail over seamlessly and retry failed work in order to hide that failure\nfrom the end user. And we need to monitor not just each individual service,\nbut how all of them are working together to get the job done. After all, our\nusers don’t care if 99 percent of our microservices are working correctly if\nthe 1 percent failure prevents them from using our application.\nWe have lots of problems to solve if we want to build an application out\nof many individual microservices, and we do not want each of our\nmicroservice teams working those problems, or they would never have time\nto write code! We need a common way to manage the packaging,\ndeployment, configuration, and maintenance of our microservices. Let’s look\nat two categories of required attributes: those that apply to a single\nmicroservice, and those that apply to multiple microservices working\ntogether.\nRequirements for Containers\nFor a single microservice, we need the following:\nPackaging\n Bundle the application for delivery, which needs to include\ndependencies so that the package is portable and we avoid conflicts between\nmicroservices.\nVersioning\n Uniquely identify a version. We need to update microservices\nover time, and we need to know what version is running.\nIsolation\n Keep microservices from interfering with one another. This allows\nus to be flexible about what microservices are deployed together.\nFast startup\n Start new instances rapidly. We need this to scale and respond\nto failures.\nLow overhead\n Minimize required resources to run a microservice in order to\navoid limits on how small a microservice can be.\nContainers\n are designed to address exactly these needs. Containers\nprovide isolation together with low overhead and fast startup. And, as we’ll\nsee in \nChapter 5\n, a container runs from a container image, which provides a\nway to package an application with its dependencies and to uniquely identify\nthe version of that package.\nRequirements for Orchestration\nFor multiple microservices working together, we need:\nClustering\n Provide processing, memory, and storage for containers across\nmultiple servers.",4102
08-Running Containers.pdf,08-Running Containers,"Discovery\n Provide a way for one microservice to find another. Our\nmicroservices might run anywhere on the cluster, and they might move\naround.\nConfiguration\n Separate configuration from runtime, allowing us to\nreconfigure our application without rebuilding and redeploying our\nmicroservices.\nAccess control\n Manage authorization to create containers. This ensures that\nthe right containers run, and the wrong ones don’t.\nLoad balancing\n Spread requests among working instances in order to avoid\nthe need for end users or other microservices to track all microservice\ninstances and balance the load themselves.\nMonitoring\n Identify failed microservice instances. Load balancing won’t\nwork well if traffic is going to failed instances.\nResilience\n Automatically recover from failures. If we don’t have this ability,\na chain of failures could kill our application.\nThese requirements come into play only when we are running containers\non multiple servers. It’s a different problem from just packaging up and\nrunning a single container. To address these needs, we require a \ncontainer\norchestration\n environment. A container orchestration environment such as\nKubernetes allows us to treat multiple servers as a single set of resources to\nrun containers, dynamically allocating containers to available servers and\nproviding distributed communication and storage.\nRunning Containers\nBy now, hopefully you’re excited by the possibilities of building an\napplication using containerized microservices and Kubernetes. Let’s walk\nthrough the basics so that you can see what these ideas look like in practice,\nproviding a foundation for the deeper dive into container technology that\nyou’ll find in the rest of this book.\nWhat Containers Look Like\nIn \nChapter 2\n, we’ll look at the difference between a container platform and a\ncontainer runtime, and we’ll run containers using multiple container\nruntimes. For now, let’s begin with a simple example running in the most\npopular container platform, \nDocker\n. Our goal is to learn the basic Docker\ncommands, which align to universal container concepts.\nRunning a Container\nThe first command is \nrun\n, which creates a container and runs a command\ninside it. We will tell Docker the name of the container image to use. We\ndiscuss container images more in \nChapter 5\n; for now, it’s enough to know\nthat it provides a unique name and version so that Docker knows exactly\nwhat to run. Let’s get started using the example for this chapter.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nA key idea for this section is that containers look like a completely\nseparate system. To illustrate this, before we run a container, let’s look at the\nhost system:\nroot@host01:~# \ncat /etc/os-release\nNAME=""Ubuntu""\n...\nroot@host01:~# \nps -ef\nUID          PID    PPID  C STIME TTY          TIME CMD\nroot           1       0  0 12:59 ?        00:00:07 /sbin/init\n...\nroot@host01:~# \nuname -v\n#...-Ubuntu SMP ...\nroot@host01:~# \nip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 ...\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n...\n3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel ...\n    link/ether 08:00:27:bf:63:1f brd ff:ff:ff:ff:ff:ff\n    inet 192.168.61.11/24 brd 192.168.61.255 scope global enp0s8\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:febf:631f/64 scope link \n       valid_lft forever preferred_lft forever\n...\nThe first command looks at a file called \n/etc/os-release\n, which has\ninformation about the installed Linux distribution. In this case, our example\nvirtual machine is running Ubuntu. That matches the output of the next\ncommand, in which we see an Ubuntu-based Linux kernel. Finally, we list\nnetwork interfaces and see an IP address of \n192.168.61.11\n.\nThe example setup steps automatically installed Docker, so we have it\nready to go. First, let’s download and start a Rocky Linux container with a\nsingle command:\nroot@host01:~# \ndocker run -ti rockylinux:8\nUnable to find image 'rockylinux:8' locally\n8: Pulling from library/rockylinux\n...\nStatus: Downloaded newer image for rockylinux:8\nWe use \n-ti\n in our \ndocker run\n command to tell Docker that we need an\ninteractive terminal to run commands. The only other parameter to \ndocker run\n is\nthe container image, \nrockylinux:8\n, which specifies the name \nrockylinux\n and the\nversion \n8\n. Because we don’t provide a command to run, the default \nbash\ncommand for that container image is used.\nNow that we have a shell prompt inside the container, we can run a few\ncommands and then use \nexit\n to leave the shell and stop the container:\n➊\n [root@18f20e2d7e49 /]# \ncat /etc/os-release\n➋\n NAME=""Rocky Linux""\n  ...\n➌\n [root@18f20e2d7e49 /]# \nyum install -y procps iproute\n  ...\n  [root@18f20e2d7e49 /]# \nps -ef\n  UID          PID    PPID  C STIME TTY          TIME CMD\n  root        \n➍\n 1       0  0 13:30 pts/0    00:00:00 /bin/bash\n  root          19       1  0 13:46 pts/0    00:00:00 ps -ef\n  [root@18f20e2d7e49 /]# \nip addr\n  1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 ...\n  link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n  inet 127.0.0.1/8 scope host lo\n     valid_lft forever preferred_lft forever\n➎\n 18: eth0@if19: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 ... \n  link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n  inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0\n     valid_lft forever preferred_lft forever\n  [root@18f20e2d7e49 /]# \nuname -v\n➏\n #...-Ubuntu SMP ...\n  [root@18f20e2d7e49 /]# \nexit\nWhen we run commands within our container, it looks like we are running\nin a Rocky Linux system. Compared to the host system, there are multiple\ndifferences:\nA different hostname in the shell prompt \n➊\n (\n18f20e2d7e49\n for mine,\nthough yours will be different)\nDifferent filesystem contents \n➋\n, including basic files like \n/etc/os-\nrelease\nThe use of \nyum\n \n➌\n to install packages, and the need to install packages\neven for basic commands\nA limited set of running processes, with no base system services and our\nbash shell \n➍\n as process ID (PID) 1\nDifferent network devices \n➎\n, including a different MAC address and IP\naddress\nStrangely, however, when we run \nuname -v\n, we see the exact same Ubuntu\nLinux kernel \n➏\n as when we were on the host. Clearly, a container is not a\nwholly separate system as we might otherwise believe.\nImages and Volume Mounts\nAt first glance, a container looks like a mix between a regular process and a\nvirtual machine. And the way we interact with Docker only deepens that\nimpression. Let’s illustrate that by running an Alpine Linux container. We’ll\nstart by “pulling” the container image, which feels a lot like downloading a\nvirtual machine image:\nroot@host01:~# \ndocker pull alpine:3\n3: Pulling from library/alpine\n...\ndocker.io/library/alpine:3\nNext, we’ll run a container from the image. We’ll use a \nvolume mount\n to\nsee files from the host, a common task with a virtual machine. However,\nwe’ll also tell Docker to specify an environment variable, which is the kind\nof thing we would do when running a regular process:\nroot@host01:~# \ndocker run -ti -v /:/host -e hello=world alpine:3\n/ # \nhostname\n75b51510ab61\nWe can print the contents of \n/etc/os-release\n inside the container, as before\nwith Rocky Linux:\n/ # \ncat /etc/os-release\n \nNAME=""Alpine Linux""\nID=alpine\n...\nHowever, this time we can also print the host’s \n/etc/os-release\n file because\nthe host filesystem is mounted at \n/host\n:\n/ # \ncat /host/etc/os-release\n \nNAME=""Ubuntu""\n...\nAnd finally, within the container we also have access to the environment\nvariable we passed in:\n/ # \necho $hello\nworld\n/ # \nexit\nThis mix of ideas from virtual machines and regular processes sometimes\nleads new container users to ask questions like, “Why can’t I SSH into my\ncontainer?” A major goal of the next few chapters is to make clear what\ncontainers really are.\nWhat Containers Really Are\nDespite what a container looks like, with its own hostname, filesystem,\nprocess space, and networking, a container is not a virtual machine. It does\nnot have a separate kernel, so it cannot have separate kernel modules or\ndevice \ndrivers. A container can have multiple processes, but they must be\nstarted explicitly by the first process (PID 1). So a container will not have an\nSSH server in it by default, and most containers do not have any system\nservices running.\nIn the next several chapters, we’ll look at how a container manages to\nlook like a separate system while being a group of processes. For now, let’s\ntry one more Docker example to see what a container looks like from the host\nsystem.\nFirst, we’ll download and run NGINX with a single command:\nroot@host01:~# \ndocker run -d -p 8080:80 nginx\nUnable to find image 'nginx:latest' locally\nlatest: Pulling from library/nginx\n...\nStatus: Downloaded newer image for nginx:latest\ne9c5e87020372a23ce31ad10bd87011ed29882f65f97f3af8d32438a8340f936\nThis example illustrates a couple of additional useful Docker commands.\nAnd again, we are mixing ideas from virtual machines and regular processes.\nBy using the \n-d\n flag, we tell Docker to run this container in \ndaemon mode\n (in\nthe background), which is the kind of thing we would do for a regular\nprocess. Using \n-p 8080:80\n, however, brings in another concept from virtual\nmachines, as it instructs Docker to forward port 8080 on the host to port 80 in\nthe container, letting us connect to NGINX from the host even though the\ncontainer has its own network interfaces.\nNGINX is now running in the background in a Docker container. To see\nit, run the following:\nroot@host01:~# \ndocker ps\nCONTAINER ID IMAGE ... PORTS                  NAMES\ne9c5e8702037 nginx ... 0.0.0.0:8080->80/tcp   funny_montalcini\nBecause of the port forwarding, we can connect to it from our host system\nusing \ncurl\n:\nroot@host01:~# \ncurl http://localhost:8080/\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n...\nWith this example, we’re starting to see how containerization meets some",10530
09-Deploying Containers to Kubernetes.pdf,09-Deploying Containers to Kubernetes,"of the needs we identified earlier in this chapter. Because NGINX is\npackaged into a container image, we can download and run it with a single\ncommand, with no concern for any conflict with anything else that might be\ninstalled on our host.\nLet’s run one more command to explore our NGINX server:\nroot@host01:~# \nps -ef | grep nginx | grep -v grep\nroot     35729 35703 0 14:17 ? 00:00:00 nginx: master ...\nsystemd+ 35796 35729 0 14:17 ? 00:00:00 nginx: worker ...\nIf NGINX were running in a virtual machine, we would not see it in a \nps\nlisting on the host system. Clearly, NGINX in a container is running as a\nregular process. At the same time, we didn’t need to install NGINX onto our\nhost system to get it working. In other words, we are getting the benefits of a\nvirtual machine approach without the overhead of a virtual machine.\nDeploying Containers to Kubernetes\nTo have load balancing and resilience in our containerized applications, we\nneed a container orchestration framework like Kubernetes. Our example\nsystem also has a Kubernetes cluster automatically installed, with a web\napplication and database deployed to it. As a preparation for our deep dive\ninto Kubernetes in \nPart II\n, let’s look at that application.\nThere are many different options for installing and configuring a\nKubernetes cluster, with distributions available from many companies. We\ndiscuss multiple options for Kubernetes distributions in \nChapter 6\n. For this\nchapter, we’ll use a lightweight distribution called “K3s” from a company\ncalled Rancher.\nTo use a container orchestration environment like Kubernetes, we have to\ngive up some control over our containers. Rather than executing commands\ndirectly to run containers, we’ll tell Kubernetes what containers we want it to\nrun, and it will decide where to run each container. Kubernetes will then\nmonitor our containers for us and handle automatic restart, failover, updates\nto new versions, and even autoscaling based on load. This style of\nconfiguration is called \ndeclarative\n.\nTalking to the Kubernetes Cluster\nA Kubernetes cluster has an API server that we can use to get status and\nchange the cluster configuration. We interact with the API server using the\nkubectl\n client application. K3s comes with its own embedded \nkubectl\n command\nthat we’ll use. Let’s begin by getting some basic information about the\nKubernetes cluster:\nroot@host01:~# \nk3s kubectl version\nClient Version: version.Info{Major:""1"", ...\nServer Version: version.Info{Major:""1"", ...\nroot@host01:~# \nk3s kubectl get nodes\nNAME     STATUS   ROLES             AGE   VERSION\nhost01   Ready    control-plane...  2d    v1...\nAs you can see, we’re working with a single-node Kubernetes cluster. Of\ncourse, this would not meet our needs for high availability. Most Kubernetes\ndistributions, including K3s, support a multinode, highly available cluster,\nand we will look at how that works in detail in \nPart II\n.\nApplication Overview\nOur example application provides a “to-do” list with a web interface,\npersistent storage, and tracking of item state. It will take several minutes for\nthis to be running in Kubernetes, even after the automated scripts are\nfinished. After it’s running, we can access it in a browser and should see\nsomething like \nFigure 1-1\n.\nFigure 1-1: An example application in Kubernetes\nThis application is divided into two types of containers, one for each\napplication component. A Node.js application serves files to the browser and\nprovides a REST API. The Node.js application communicates with a\nPostgreSQL database. The Node.js component is stateless, so it is easy to\nscale up to as many instances as we need based on the number of users. In\nthis case, our application’s Deployment asked Kubernetes for three Node.js\ncontainers:\nroot@host01:~# \nk3s kubectl get pods\nNAME                       READY   STATUS    RESTARTS   AGE\ntodo-db-7df8b44d65-744mt   1/1     Running   0          2d\ntodo-655ff549f8-l4dxt      1/1     Running   0          2d\ntodo-655ff549f8-gc7b6      1/1     Running   1          2d\ntodo-655ff549f8-qq8ff      1/1     Running   1          2d\nThe command \nget pods\n tells Kubernetes to list \nPods\n. A Pod is a group of\none or more containers that Kubernetes treats as a single unit for scheduling\nand monitoring. We look at Pods more closely throughout \nPart II\n.\nHere, we have one Pod whose name starts with \ntodo-db\n, which is our\nPostgreSQL database. The other three Pods, with names starting with \ntodo\n, are\nthe Node.js containers. (We’ll explain later why the names have random\ncharacters after them; you can ignore that for now.)\nAccording to Kubernetes, our application component containers are\nrunning, so we should be able to access our application in a browser. How\nyou do this depends on whether you are running in AWS or Vagrant; the\nexample setup scripts will print out what URL you should use in your\nbrowser. If you visit that URL, you should see something like \nFigure 1-1\n.\nKubernetes Features\nIf our only goal were to run four containers, we could have done that just\nusing the Docker commands described earlier. Kubernetes is providing a lot\nmore functionality, though. Let’s take a quick tour of the most important\nfeatures.\nIn addition to running our containers, Kubernetes is also monitoring them.\nBecause we asked for three instances, Kubernetes will work to keep three\ninstances running. Let’s destroy one and watch Kubernetes automatically\nrecover:\nroot@host01:~# \nk3s kubectl delete pod\n \ntodo-655ff549f8-qq8ff\npod ""todo-655ff549f8-qq8ff"" deleted\nroot@host01:~# \nk3s kubectl get pods\nNAME                       READY   STATUS    RESTARTS   AGE\ntodo-db-7df8b44d65-744mt   1/1     Running   0          2d\ntodo-655ff549f8-l4dxt      1/1     Running   0          2d\ntodo-655ff549f8-gc7b6      1/1     Running   1          2d\ntodo-655ff549f8-rm8sh      1/1     Running   0          11s\nTo run this command, you will need to copy and paste the full name of\none of your three Pods. The name will be a little different from mine. When\nyou delete a Pod, you should see that Kubernetes immediately creates a new\none. (You can identify which one is brand new by the \nAGE\n field.)\nNext let’s explore how Kubernetes can automatically scale our\napplication. Later, we’ll see how to make Kubernetes do this automatically,\nbut for now, we will do it manually. Suppose that we decide we need five\nPods instead of three. We can do this with one command:\nroot@host01:~# \nk3s kubectl scale --replicas=5 deployment todo\ndeployment.apps/todo scaled\nroot@host01:~# \nk3s kubectl get pods\nNAME                       READY   STATUS    RESTARTS   AGE\ntodo-db-7df8b44d65-744mt   1/1     Running   0          2d\ntodo-655ff549f8-l4dxt      1/1     Running   0          2d\ntodo-655ff549f8-gc7b6      1/1     Running   1          2d\ntodo-655ff549f8-rm8sh      1/1     Running   0          5m13s\ntodo-655ff549f8-g7lxg      1/1     Running   0          6s\ntodo-655ff549f8-zsqp6      1/1     Running   0          6s\nWe tell Kubernetes to scale the \nDeployment\n that manages our Pods. For\nnow, you can think of the Deployment as the “owner” of the Pods; it\nmonitors them and controls how many there are. Here, two extra Pods are\nimmediately created. We just scaled up our application.\nBefore we close, let’s look at one more critically important Kubernetes\nfeature. When you load the application in your web browser, Kubernetes is\nsending your browser’s request to one of the available Pods. Each time you\nreload, the request might be routed to a different Pod because Kubernetes is\nautomatically balancing the application’s load. To make this happen, when\nwe deploy our application to Kubernetes, the application configuration\nincludes a \nService\n:\nroot@host01:~# \nk3s kubectl describe service todo",7910
10-2 PROCESS ISOLATION.pdf,10-2 PROCESS ISOLATION,"Name:       todo\n...\nIPs:        10.43.231.177\nPort:       <unset>  80/TCP\nTargetPort: 5000/TCP\nEndpoints:  10.42.0.10:5000,10.42.0.11:5000,10.42.0.14:5000 + 2 more...\n...\nA Service has its own IP address and routes traffic to one or more\nendpoints. In this case, because we scaled up to five Pods, the Service is\nbalancing traffic across all five endpoints.\nFinal Thoughts\nModern applications achieve scalability and reliability through an\narchitecture based on microservices that can be deployed independently and\ndynamically to available hardware, including cloud resources. By using\ncontainers and container orchestration to run our microservices, we achieve a\ncommon approach for packaging, scaling, monitoring, and maintaining\nmicroservices, enabling our development teams to focus on the hard work of\nactually building the application.\nIn this chapter, we saw how containerization can create the appearance of\na separate system while really being a regular process run in an isolated way.\nWe also saw how we can use Kubernetes to deploy an entire application as a\nset of containers, with scalability and self-healing. Of course, Kubernetes has\na lot more important features than what we’ve mentioned here, enough that it\nwill take the whole book for us to cover them all! With this brief overview, I\nhope you are excited to dive more deeply into containers and Kubernetes in\norder to understand how to build applications that perform well and are\nreliable.\nWe’ll come back to Kubernetes in \nPart II\n of this book. For now, let’s look\nclosely at how containers create the illusion of a separate system. We’ll start\nby looking at process isolation using Linux namespaces.",1707
11-Understanding Isolation.pdf,11-Understanding Isolation,"2\nPROCESS ISOLATION\nContainers build on a rich history of technologies designed to isolate one\ncomputer program from another while allowing many programs to share the\nsame CPU, memory, storage, and network resources. Containers use\nfundamental capabilities of the Linux kernel, particularly namespaces, which\ncreate separate views of process identifiers, users, the filesystem, and\nnetwork interfaces. Container runtimes use multiple types of namespaces to\ngive each container an isolated view of the system.\nIn this chapter, we’ll consider some of the reasons for process isolation\nand look at how Linux has historically isolated processes. We’ll then\nexamine how containers use namespaces to provide isolation. We’ll test this\nusing a couple of different container runtimes. Finally, we will use Linux\ncommands to create namespaces directly.\nUnderstanding Isolation\nBefore running some containers and inspecting their isolation, let’s look at\nthe motivation for process isolation. We’ll also consider traditional process\nisolation in Linux and how that has led to the isolation capabilities that\ncontainers use.\nWhy Processes Need Isolation\nThe whole idea of a computer is that it is a general-purpose machine that can\nrun many different kinds of programs. Ever since the beginning of\ncomputing, there has been a need to share a single computer between\nmultiple programs. It started with people taking turns submitting programs on\npunch cards, but as computer multitasking became more sophisticated, people\ncould start multiple programs, and the computer would make it seem as if\nthey were all running on the same CPU at once.\nOf course, as soon as something needs to be shared, there is a need to\nmake sure it is shared fairly, and computer programs are no different. So\nalthough we think of a \nprocess\n as an independent program with its own time\non the CPU and its own memory space, there are many ways that one process\ncan cause trouble for another, including:\nUsing too much CPU, memory, storage, or network\nOverwriting the memory or files of another process\nExtracting secret information from another process\nSending another process bad data to cause it to misbehave\nFlooding another process with requests so that it stops responding\nBugs can cause processes to do these same things by accident, but a\nbigger concern is a security vulnerability that allows a bad actor to use one\nprocess to cause problems for another. It takes only one vulnerability to\ncreate major problems in a system, so we need ways to isolate processes that\nlimit damage from both accidental and intentional behavior.\nPhysical isolation is best—\nair-gapped\n systems are regularly used to\nprotect government-classified information and safety-critical systems—but\nthis approach is also too expensive and inconvenient for many uses. Virtual\nmachines can give the appearance of separation while sharing physical\nhardware, but a virtual machine has the overhead of running its own\noperating system, services, and virtual devices, making it slower to start and\nless scalable. The solution is to run regular processes, but use process\nisolation to reduce the risk of affecting other processes.\nFile Permissions and Change Root\nMost of the effort in process isolation involves preventing one process from\nseeing things it shouldn’t. After all, if a process can’t even see another\nprocess, it will be far more difficult to cause trouble, either accidentally or on\npurpose. The traditional ways that Linux has controlled what processes can\nsee and do serve as the foundation for the ideas behind containers.\nOne of the most basic visibility controls is \nfilesystem permissions\n. Linux\nassociates an owner and group with each file and directory, and manages\nread, write, and execute permissions. This basic permission scheme works\nwell to ensure that user files are kept private, that a process cannot overwrite\nthe files of another process, and that only a privileged user like root can\ninstall new software or modify critical system configuration files.\nOf course, this permission scheme relies on us ensuring that each process\nis run as the authentic user and that users are in the appropriate groups.\nTypically, each new service install creates a user just for running that service.\nEven better, this \nservice user\n can be configured without a real login shell,\nwhich means that the user cannot be exploited to log in to the system. To\nmake this clear, let’s look at an example.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nThe Linux \nrsyslogd\n service provides logging services, so it needs to write to\nfiles in \n/var/log\n, but it should not have permissions to read or write all of the\nfiles in that directory. File permissions are used to control this, as shown in\nthis example:\n   root@host01:~# \nps -ef | grep rsyslogd | grep -v grep\n➊\n syslog  698  1  0 Mar05 ?   00:00:04 /usr/sbin/rsyslogd -n -iNONE\n   root@host01:~# \nsu syslog\n➋\n This account is currently not available.\n   root@host01:~# \nls -l /var/log/auth.log\n➌\n -rw-r----- 1 syslog adm 18396 Mar  6 01:27 /var/log/auth.log\n   root@host01:~# \nls -ld /var/log/private\n➍\n drwx------ 2 root root 4096 Mar  5 21:04 /var/log/private\nThe \nsyslog\n user \n➊\n exists specifically to run \nrsyslogd\n, and that user is\nconfigured with no login shell for security reasons \n➋\n. Because \nrsyslogd\n needs\nto be able to write to \nauth.log\n, it’s given write permission, as shown in the\nfile mode printout \n➌\n. Members of the admin (\nadm\n) group have read-only\naccess to this file.\nAn initial \nd\n in the file mode \n➍\n indicates that this is a directory. The\nfollowing \nrwx\n indicates that the root user has read, write, and execute\npermissions. The remaining dashes indicate that there are no rights for\nmembers of the \nroot\n group or for other system users, so we can conclude that\nthe \nrsyslogd\n process cannot see the contents of this directory.\nPermission control is important, but it doesn’t fully satisfy our goal of\nprocess isolation. One reason is that it is not enough to protect us from\nprivilege escalation\n, wherein a vulnerable process and a vulnerable system\nallow a bad actor to obtain root privileges. To help deal with this, some Linux\nservices go a step beyond by running in an isolated part of the filesystem.\nThis approach is known as \nchroot\n for “change root.” Running in a \nchroot\nenvironment requires quite a bit of setup, as you can see in this example:\n   root@host01:~# \nmkdir /tmp/newroot\n   root@host01:~# \n➊\n cp --parents /bin/bash /bin/ls /tmp/newroot\n   root@host01:~# \ncp --parents /lib64/ld-linux-x86-64.so.2 \\n  \n➋\n \n$(ldd /bin/bash /bin/ls | grep '=>' | awk '{print $3}') /tmp/newroot\n   ...\n   root@host01:~# \n➌\n chroot /tmp/newroot /bin/bash\n   bash-5.0# \nls -l /bin\n   total 1296\n➍\n -rwxr-xr-x 1 0 0 1183448 Mar  6 02:15 bash\n   -rwxr-xr-x 1 0 0  142144 Mar  6 02:15 ls\n   bash-5.0# \nexit\n   exit\nFirst, we need to copy in all of the executables that we intend to run \n➊\n.\nWe also need to copy in all of the shared libraries these executables use,\nwhich we specify with the \nldd | grep | awk\n command \n➋\n. When both binaries and\nlibraries are copied in, we can use the \nchroot\n command \n➌\n to move into our\nisolated environment. Only the files we copied in are visible \n➍\n.\nContainer Isolation\nFor experienced Linux system administrators, file permissions and change\nroot are basic-level knowledge. However, those concepts also serve as the\nfoundation for how containers work. Even though a running container\nappears like a completely separate system, with its own hostname, network,\nprocesses, and filesystem (as we saw in \nChapter 1\n), it’s really a regular Linux",7951
12-Container Platforms and Container Runtimes.pdf,12-Container Platforms and Container Runtimes,"process using isolation rather than a virtual machine.\nA container has multiple kinds of isolation, including several essential\nkinds of isolation that we haven’t seen before:\nMounted filesystems\nHostname and domain name\nInterprocess communication\nProcess identifiers\nNetwork devices\nThese separate kinds of isolation work together so that a process or\ncollection of processes looks like a completely separate system. Although\nthese processes still share the kernel and physical hardware, this isolation\ngoes a long way toward ensuring that they cannot cause trouble for other\nprocesses, especially when we configure containers correctly to control the\nCPU, memory, storage, and network resources available to them.\nContainer Platforms and Container Runtimes\nSpecifying all the binaries, libraries, and configuration files needed to run a\nprocess in an isolated filesystem would be laborious. Fortunately, as we saw\nin \nChapter 1\n, \ncontainer images\n come prepackaged with the needed\nexecutables and libraries. Using Docker, we were able to easily download\nand run NGINX in a container. Docker is an example of a \ncontainer platform\n,\nproviding not only the ability to run containers but also container storage,\nnetworking, and security.\nUnder the covers, modern versions of Docker are using \ncontainerd\n as the\ncontainer runtime\n, also known as a \ncontainer engine\n. A container runtime\nprovides low-level functionality to run processes in containers.\nTo explore isolation further, let’s experiment with two different container\nruntimes to start containers from preexisting images and then inspect how\nprocesses in containers are isolated from the rest of the system.\nInstalling containerd\nWe’ll be using \ncontainerd\n in \nPart II\n in support of our Kubernetes clusters, so\nlet’s begin by installing and interacting with this runtime directly. Interacting\ndirectly with \ncontainerd\n will also benefit our exploration of process isolation.\nYou can skip install commands by using the \nextra\n provisioning script\nprovided with this chapter’s examples. See the README file for this chapter\nfor instructions.\nEven though \ncontainerd\n is available in the standard Ubuntu package\nrepository, we’ll install it from the official Docker package registry so that\nwe get the latest stable version. To do that, we need Apt to support HTTP/S,\nso let’s do that first:\nroot@host01:~# \napt update\n...\nroot@host01:~# \napt -y install apt-transport-https\n...\nNow let’s add the package registry and install:\nroot@host01:~# \ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | \\n  \ngpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\nroot@host01:~# \necho ""deb [arch=amd64"" \\n  \n""signed-by=/usr/share/keyrings/docker-archive-keyring.gpg]"" \\n  \n""https://download.docker.com/linux/ubuntu focal stable"" > \\n  \n/etc/apt/sources.list.d/docker.list\nroot@host01:~# \napt update && apt install -y containerd.io\n...\nroot@host01:~# \nctr images ls\nREF TYPE DIGEST SIZE PLATFORMS LABELS\nThe final command just ensures that the package installed correctly, that\nthe service is running, and that the \nctr\n command is working. We don’t see any\nimages because we haven’t installed any yet.\nContainer runtimes are low-level libraries. They are typically not used\ndirectly but are used by a higher-level container platform or orchestration\nenvironment such as Docker or Kubernetes. This means that they put a lot of\nfocus into a quality application programming interface (API) but not as much\neffort into user-facing tools we can use from the command line. Fortunately,\ncommand line tools are still needed for testing, and \ncontainerd\n provides the \nctr\ntool that we’ll use for experimentation.\nUsing containerd\nOur initial \ncontainerd\n command showed that no images have been downloaded\nyet. Let’s download a small image with which we can run a container. We\nwill use \nBusyBox\n, a tiny container image that includes a shell and basic Linux\nutilities. To download the image, we use the \npull\n command:\nroot@host01:~# \nctr image pull docker.io/library/busybox:latest\n...\nroot@host01:~# \nctr images ls\nREF                              ...\ndocker.io/library/busybox:latest ...\nOur list of images is no longer empty. Let’s run a container from that\nimage:\nroot@host01:~# \nctr run -t --rm docker.io/library/busybox:latest v1\n/ #\nThis looks similar to using Docker. We use \n-t\n to create a TTY for this\ncontainer, allowing us to interact with it, and we use \n--rm\n to tell \ncontainerd\n to\ndelete the container when the main process stops. However, there are some\nimportant differences to note. When we used Docker in \nChapter 1\n, we didn’t\nworry about pulling the image before running it, and we were able to use\nsimpler names like \nnginx\n or \nrockylinux:8\n. The \nctr\n tool requires us to specify\ndocker.io/library/busybox:latest\n, the full path to the image, with registry\nhostname and tag included. Also, we are required to pull the image first\nbecause the runtime won’t do this for us automatically.\nNow that we’re inside this container, we can see that it has an isolated\nnetwork stack and process space:\n/ # \nip a\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n        valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n        valid_lft forever preferred_lft forever\n/ # \nps -ef\nPID   USER     TIME  COMMAND\n    1 root      0:00 sh\n    8 root      0:00 ps -ef\n/ #\nInside the container, we see a loopback network interface. We also see our\nshell process and the \nps\n command that we ran. As far as the processes in our\ncontainer are concerned, we are running on a separate system with no other\nprocesses running or listening on the network.\nWHY NO BRIDGE INTERFACE?\nIf you’ve worked with Docker, you might be surprised to see that this\ncontainer has only a loopback interface. Default networking on a\ncontainer platform also provides an additional interface that is attached\nto a bridge. This allows containers to see one another and also allows\ncontainers to use the host interface to access external networks via\nNetwork Address Translation (NAT).\nIn this case, we are talking directly to a lower-level container runtime.\nThis container runtime handles managing images and running\ncontainers only. If we want a bridge interface and a connection to the\ninternet, we’ll need to provide it ourselves (and we do exactly that in\nChapter 4\n).\nWe’ve illustrated that we can talk to the \ncontainerd\n runtime to run a\ncontainer, and that inside the container, we’re isolated from the rest of the\nsystem. How does that isolation work? To find out, let’s keep the container\nrunning and investigate it from the host system.\nIntroducing Linux Namespaces\nLike other container runtimes, \ncontainerd\n uses a Linux kernel feature called\nnamespaces\n to isolate the processes in the container. As mentioned earlier,\nmost of the effort in process isolation is to ensure that a process can’t see\nthings it shouldn’t. A process running in a namespace sees a limited view of a\nparticular system resource.\nEven though containerization seems like new technology, Linux\nnamespaces have been available for many years. Over time, more types of\nnamespaces were added. We can find out what namespaces are associated\nwith our container using the \nlsns\n command, but first we need to know the\nprocess ID (PID) on the host for our container’s shell process. While leaving\nthe container running, open another terminal tab or window. (See “Running\nExamples” on \npage xx\n for more information.) Then, use \nctr\n to list running\ncontainers:\nroot@host01:~# \nctr task ls\nTASK    PID      STATUS    \nv1      18088    RUNNING\nLet’s use \nps\n to verify that we have the correct PID. When you run these\ncommands yourself, be sure to use the PID that displays in your listing:\nroot@host01:~# \nps -ef | grep \n18088\n | grep -v grep\nroot       18088   18067  0 18:46 pts/0    00:00:00 sh\nroot@host01:~# \nps -ef | grep \n18067\n | grep -v grep\nroot       18067       1  0 18:46 ?        00:00:00 \n  /usr/bin/containerd-shim-runc-v2 -namespace default -id v1 -address \n  /run/containerd/containerd.sock\nroot       18088   18067  0 18:46 pts/0    00:00:00 sh\nAs expected, the parent of this PID is \ncontainerd\n. Next let’s use \nlsns\n to list the\nnamespaces that \ncontainerd\n has created to isolate this process:\nroot@host01:~# \nlsns | grep \n18088\n4026532180 mnt         1 18088 root            sh\n4026532181 uts         1 18088 root            sh\n4026532182 ipc         1 18088 root            sh\n4026532183 pid         1 18088 root            sh\n4026532185 net         1 18088 root            sh\nHere, \ncontainerd\n is using five different types of namespaces in order to fully\nisolate the processes running in the \nbusybox\n container:\nmnt\n Mount points\nuts\n Unix time sharing (hostname and network domain)\nipc\n Interprocess communication (for example, shared memory)\npid\n Process identifiers (and list of running processes)\nnet\n Network (including interfaces, routing table, and firewall)\nFinally, we’ll close out the BusyBox container by running \nexit\n from within\nthat container (first terminal window):\n/ # \nexit\nThis command returns us to a regular shell prompt so that we can be ready\nfor the next set of examples.\nContainers and Namespaces in CRI-O\nIn addition to \ncontainerd\n, Kubernetes supports other container runtimes.\nDepending on which Kubernetes distribution you use, you might find that the\ncontainer runtime is different. For example, Red Hat OpenShift uses \nCRI-O\n,\nan alternative container runtime. CRI-O is also used by the Podman, Buildah,\nand Skopeo suite of tools, which are the standard way to manage containers\non Red Hat 8 and related systems.\nLet’s run the same container image using CRI-O to get a better picture of\nhow container runtimes are different from one another but also to show how\nthey use the same underlying Linux kernel capabilities for process isolation.\nYou can skip these install commands by using the \nextra\n provisioning\nscript provided with this chapter’s examples. See the README file for this\nchapter for instructions.\nThe OpenSUSE Kubic project hosts repositories for CRI-O for various\nLinux distributions, including Ubuntu, so we will install from there. The\nexact URL is dependent on the version of CRI-O we want to install, and the\nURLs are long and challenging to type, so the automation installs a script to\nconfigure some useful environment variables. Before proceeding, we need to\nload that script:\nroot@host01:~# \nsource /opt/crio-ver\nWe can now use the environment variables to set up the CRI-O\nrepositories and install CRI-O:\nroot@host01:~# \necho ""deb $REPO/$OS/ /"" > /etc/apt/sources.list.d/kubic.list\nroot@host01:~# \necho ""deb $REPO:/cri-o:/$VERSION/$OS/ /"" \\n  \n> /etc/apt/sources.list.d/kubic.cri-o.list\nroot@host01:~# \ncurl -L $REPO/$OS/Release.key | apt-key add -\n...\nOK\nroot@host01:~# \napt update && apt install -y cri-o cri-o-runc\n...\nroot@host01:~# \nsystemctl enable crio && systemctl start crio\n...\nroot@host01:~# \ncurl -L -o /tmp/crictl.tar.gz $CRICTL_URL\n...\nroot@host01:~# \ntar -C /usr/local/bin -xvzf /tmp/crictl.tar.gz\ncrictl\nroot@host01:~# \nrm -f /tmp/crictl.tar.gz\nWe first add to the list of repositories for \napt\n by adding files to\n/etc/apt/sources.list.d\n. We then use \napt\n to install CRI-O packages. After CRI-\nO is installed, we use \nsystemd\n to enable and start its service.\nUnlike \ncontainerd\n, CRI-O does not ship with any command line tools that we\ncan use for testing, so the last command installs \ncrictl\n, which is part of the\nKubernetes project and is designed for testing any container runtime\ncompatible with the Container Runtime Interface (CRI) standard. CRI is the\nprogramming API that Kubernetes itself uses to communicate with container\nruntimes.\nBecause \ncrictl\n is compatible with any container runtime that supports CRI,\nit needs configuration to connect to CRI-O. CRI-O has installed a\nconfiguration file \n/etc/crictl.yaml\n to configure \ncrictl\n:\ncrictl.yaml\nruntime-endpoint: unix:///var/run/crio/crio.sock\nimage-endpoint: unix:///var/run/crio/crio.sock\n...\nThis configuration tells \ncrictl\n to connect to CRI-O’s socket.\nTo create and run containers, the \ncrictl\n command requires us to provide\ndefinition files in the JSON or YAML file format. The automated scripts for\nthis chapter added two \ncrictl\n definition files to \n/opt\n. The first file, shown in\nListing 2-1\n, creates a Pod:\npod.yaml\n---\nmetadata:\n  name: busybox\n  namespace: crio\nlinux:\n  security_context:\n    namespace_options:\n      network: 2\nListing 2-1: CRI-O Pod definition\nSimilar to the Kubernetes Pod we saw in \nChapter 1\n, the Pod is a group of\none or more containers that run in the same isolated space. In our case, we\nneed only one container in the Pod, and the second file, shown in \nListing 2-2\n,\ndefines the container process that CRI-O should start. We provide a name\n(\nbusybox\n) and namespace (\ncrio\n) to distinguish this Pod from any others.\nOtherwise, we need to provide only network configuration. CRI-O expects to\nuse a Container Network Interface (CNI) plug-in to configure the network\nnamespace. We cover CNI plug-ins in \nChapter 8\n, so for now, we’ll use\nnetwork: 2\n to tell CRI-O not to create a separate network namespace and instead\nuse the host network:\ncontainer.yaml\n---\nmetadata:\n  name: busybox\nimage:\n  image: docker.io/library/busybox:latest\nargs:\n  - ""/bin/sleep""\n  - ""36000""\nListing 2-2: CRI-O container definition\nAgain we are using BusyBox because its small size makes it fast and\nlightweight. However, because \ncrictl\n will create this container in the\nbackground without a terminal, we need to specify \n/bin/sleep\n as the command\nto be run \ninside the container; otherwise, the container will immediately\nterminate when the shell realizes that it doesn’t have a TTY.\nBefore we can run the container, we first need to pull the image:\nroot@host01:~# \ncrictl pull docker.io/library/busybox:latest\nImage is up to date for docker.io/library/busybox@sha256:...\nThen, we provide the \npod.yaml\n and \ncontainer.yaml\n files to \ncrictl\n to create\nand start our BusyBox container:\nroot@host01:~# \ncd /opt\nroot@host01:~# \nPOD_ID=$(crictl runp pod.yaml)\nroot@host01:~# \ncrictl pods\nPOD ID              CREATED                  STATE ...\n3bf297ace44b5       Less than a second ago   Ready ...\nroot@host01:~# \nCONTAINER_ID=$(crictl create $POD_ID container.yaml pod.yaml)\nroot@host01:~# \ncrictl start $CONTAINER_ID\n91394a7f37e3da3a557782ed6d6eb2cf8c23e5b3dd4e2febd415bba071d10734\nroot@host01:~# \ncrictl ps\nCONTAINER           ... STATE\n91394a7f37e3d       ... Running\nWe capture the Pod’s unique identifier and the container in \nPOD_ID\n and\nCONTAINER_ID\n variables, so we can use them here and upcoming commands.\nBefore looking at the Linux namespaces created by CRI-O, let’s look\ninside the \nbusybox\n container by using the \ncrictl exec\n command to start a new shell\nprocess inside it:\nroot@host01:~# \ncrictl exec -ti $CONTAINER_ID /bin/sh\n/ # \nip a\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000\n...\n2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel qlen 1000\n...\n3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel qlen 1000\n...\n/ # \nps -ef\nPID   USER     TIME  COMMAND\n    1 root      0:00 /pause\n    7 root      0:00 /bin/sleep 36000\n    13 root      0:00 /bin/sh\n    20 root      0:00 ps -ef\n/ # \nexit\nThis BusyBox container running in CRI-O looks a little different from\nBusyBox running in \ncontainerd\n. First, because we configured our Pod with\nnetwork: 2\n, the container can see the same network devices that a regular\nprocess would see. Second, we see a couple of additional processes. We look\nat the \npause\n process with PID 1 when we discuss container runtimes under\nKubernetes in \nChapter 12\n. The other extra process is \nsleep\n, which we created\nas the entry point for this container.\nCRI-O is also using Linux namespaces for process isolation, as we can see\nfrom examining the container processes and listing namespaces:\nroot@host01:~# \nPID=$(crictl inspect $CONTAINER_ID | jq '.info.pid')\nroot@host01:~# \nps -ef | grep $PID | grep -v grep\nroot       23906   23894  0 20:15 ?        00:00:00 /bin/sleep 36000\nroot@host01:/opt# \nps -ef | grep \n23894\n | grep -v grep\nroot       23894       1  0 20:15 ?        00:00:00 /usr/bin/conmon ...\nroot       23906   23894  0 20:15 ?        00:00:00 /bin/sleep 36000",16929
13-Running Processes in Namespaces Directly.pdf,13-Running Processes in Namespaces Directly,"The \ncrictl inspect\n command provides a wealth of information about the\ncontainer, but for the moment, we need only the PID. Because \ncrictl\n returns\nJSON-formatted output, we can use \njq\n to extract the \npid\n field from the \ninfo\nstructure and save it to an environment variable called \nPID\n. Try running \ncrictl\ninspect $CONTAINER_ID\n to see the full information.\nUsing the PID we discovered, we can see our \nsleep\n command. We then can\nuse its parent PID to verify that it is managed by \nconmon\n, a CRI-O utility.\nNext, let’s see the namespaces that CRI-O has created. The allocation of\nnamespaces to processes is more complex in CRI-O, so let’s just list all of the\nnamespaces on our Linux system and pick out the ones related to the\ncontainer:\nroot@host01:~# \nlsns\n        NS TYPE   NPROCS   PID USER            COMMAND\n...\n4026532183 uts         2 23867 root            /pause\n4026532184 ipc         2 23867 root            /pause\n4026532185 mnt         1 23867 root            /pause\n4026532186 pid         2 23867 root            /pause\n4026532187 mnt         1 23906 root            /bin/sleep 36000\n...\nHere, we see only four types of namespaces. Because we told CRI-O to\ngive the container access to the host’s network namespace, it didn’t need to\ncreate a \nnet\n namespace. Also, with CRI-O, most namespaces are associated\nwith the \npause\n command (although some are shared by multiple processes, as\nwe can see via the \nNPROCS\n column). There are two \nmnt\n namespaces because\neach separate container in a Pod gets a different set of mount points for\nreasons that we cover in \nChapter 5\n.\nRunning Processes in Namespaces Directly\nOne of the trickier jobs when running a process in a container is handling the\nresponsibility that comes with being PID 1. To better understand this, we\nwon’t have our container runtime create a namespace for us. Instead, we’ll\ntalk directly to the Linux kernel to run a process in a namespace \nmanually.\nWe’ll use the command line, although container runtimes use the Linux\nkernel API, but the result will be the same.\nBecause namespaces are a Linux kernel feature, nothing else needs to be\ninstalled or configured. We just use the \nunshare\n command when launching the\nprocess:\nroot@host01:~# \nunshare -f -p --mount-proc -- /bin/sh -c /bin/bash\nThe \nunshare\n command runs a program with different namespaces from the\nparent. By adding \n-p\n, we specify that a new PID namespace is needed. The\noption \n--mount-proc\n goes along with that, adding a new mount namespace and\nensuring \n/proc\n is remounted correctly, so that the process sees the correct\nprocess information. Otherwise, the process would still be able to see\ninformation about other processes in the system. Finally, the content after \n--\nindicates the command to run.\nBecause this is an isolated process namespace, it cannot see a list of\nprocesses outside this namespace:\nroot@host01:~# \nps -ef\nUID          PID    PPID  C STIME TTY          TIME CMD\nroot           1       0  0 22:21 pts/0    00:00:00 /bin/sh -c /bin/bash\nroot           2       1  0 22:21 pts/0    00:00:00 /bin/bash\nroot           9       2  0 22:22 pts/0    00:00:00 ps -ef\nLet’s get the ID of this namespace so that we can recognize it in a list:\nroot@host01:~# \nls -l /proc/self/ns/pid\nlrwxrwxrwx 1 root root 0 Mar  6 22:22 /proc/self/ns/pid -> 'pid:[4026532190]'\nNow, from another terminal window, list all of the namespaces and look\nfor those related to our isolated shell:\nroot@host01:~# \nlsns\n        NS TYPE NPROCS PID   USER COMMAND\n...\n4026532189 mnt  3      12110 root unshare -f -p ...\n4026532190 pid  2      12111 root /bin/sh -c /bin/bash\n...\nroot@host01:~# \nexit\nWe see a \npid\n namespace matching what we saw. In addition, we see a \nmnt\nnamespace. This namespace ensures that our shell sees the proper\ninformation in \n/proc\n.\nBecause the \npid\n namespace is owned by the \nsh\n command, that command is\nPID 1 when we run \nps\n within the namespace. This means that \nsh\n has the",4086
14-3 RESOURCE LIMITING.pdf,14-3 RESOURCE LIMITING,"responsibility to manage its children properly (such as \nbash\n). For example, \nsh\nis responsible for passing signals to its children to ensure that they terminate\ncorrectly. It’s important to keep this in mind as it is a common problem when\nrunning containers that can result in zombie processes or other issues\ncleaning up a stopped container.\nFortunately, \nsh\n handles its management duties well, as we can see by the\nfact that when we pass a \nkill\n signal to it, it passes that signal on to its children.\nRun this from the second terminal window, outside the namespace:\nroot@host01:~# \nkill -9 12111\nInside the first window you will see this output:\nroot@host01:~# Killed\nThis indicates that \nbash\n received the \nkill\n signal and terminated correctly.\nFinal Thoughts\nAlthough containers create the appearance of a completely separate system,\nit’s done in a way that has nothing in common with virtual machines. Instead,\nthe process is similar to traditional means of process isolation, such as user\npermissions and separate filesystems. Container runtimes use namespaces,\nwhich are built in to the Linux kernel and enable various types of process\nisolation. In this chapter, we examined how the \ncontainerd\n and CRI-O container\nruntimes use multiple types of Linux namespaces to give each container an\nindependent view of other processes, network devices, and the filesystem.\nThe use of namespaces prevents processes running in a container from seeing\nand interfering with other processes.\nAt the same time, processes in a container are still sharing the same CPU,\nmemory, and network. A process that uses too many of those resources will\nprevent other processes from running properly. Namespaces can’t solve that\nproblem, however. To prevent this issue, we’ll need to look at resource\nlimiting—the topic of our next chapter.",1869
15-CPU Priorities.pdf,15-CPU Priorities,"3\nRESOURCE LIMITING\nThe process isolation work we did in \nChapter 2\n was very important, as a\nprocess cannot generally affect what it cannot “see.” However, our process\ncan see the host’s CPU, memory, and networking, so it is possible for a\nprocess to prevent other processes from running correctly by using too much\nof these resources, not leaving enough room for others. In this chapter, we\nwill see how to guarantee that a process uses only its allocated CPU,\nmemory, and network resources, ensuring that we can divide up our resources\naccurately. This will help when we move on to container orchestration\nbecause it will provide Kubernetes with certainty about the resources\navailable on each host when it schedules a container.\nCPU, memory, and network are important, but there’s one more really\nimportant shared resource: storage. However, in a container orchestration\nenvironment like Kubernetes, storage is distributed, and limits need to be\napplied at the level of the whole cluster. For this reason, our discussion of\nstorage must wait until we introduce distributed storage in \nChapter 15\n.\nCPU Priorities\nWe’ll need to look at CPU, memory, and network separately, as the effect of\napplying limits is different in each case. Let’s begin by looking at how to\ncontrol CPU usage. To understand CPU limits, we first need to look at how\nthe Linux kernel decides which process to run and for how long. In the Linux\nkernel, the \nscheduler\n keeps a list of all of the processes. It also tracks which\nprocesses are ready to run and how much time each process has received\nlately. This allows it to create a prioritized list so that it can choose the\nprocess that will run next. The scheduler is designed to be as fair as possible\n(it’s even known as the Completely Fair Scheduler); thus, it tries to give all\nprocesses a chance to run. However, it does accept outside input on which of\nthese processes are more important than others. This prioritization is made up\nof two parts: the scheduling policy, and the priority of each process within\nthat policy.\nReal-Time and Non-Real-Time Policies\nThe scheduler supports several different policies, but for our purposes we can\ngroup them into real-time policies and non-real-time policies. The term \nreal-\ntime\n means that some real-world event is critical to the process that creates a\ndeadline. The process needs to complete its processing before this deadline\nexpires, or something bad will happen. For example, the process might be\ncollecting data from an embedded hardware device. In that case, the process\nmust read the data before the hardware buffer overflows. A real-time process\nis typically not extremely CPU intensive, but when it needs the CPU, it\ncannot wait, so all processes under a real-time policy are higher priority than\nany process under a non-real-time policy. Let’s explore this on an example\nLinux system.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nThe Linux \nps\n command tells us the specific policy that applies to each\nprocess. Run this command on \nhost01\n from this chapter’s examples:\nroot@host01:~# \nps -e -o pid,class,rtprio,ni,comm\n PID CLS RTPRIO  NI COMMAND\n   1 TS       -   0 systemd\n...\n   6 TS       - -20 kworker/0:0H-kblockd\n...\n  11 FF      99   - migration/0\n  12 FF      50   - idle_inject/0\n...\n  85 FF      99   - watchdogd\n...\n \n484 RR      99   - multipathd\n...\n7967 TS       -   0 ps\nThe \n-o\n flag provides \nps\n with a custom list of output fields, including the\nscheduling policy \nclass\n (\nCLS\n) and two numeric priority fields: \nRTPRIO\n and \nNI\n.\nLooking at the \nCLS\n field first, lots of processes are listed as \nTS\n, which\nstands for “time-sharing” and is the default non-real-time policy. This\nincludes commands we run ourselves (like the \nps\n command we ran) as well as\nimportant Linux system processes like \nsystemd\n. However, we also see\nprocesses with policy \nFF\n for first in–first out (FIFO) and policy \nRR\n for round-\nrobin. These are real-time processes, and as such, they have priority over all\nnon-real-time policies in the system. Real-time processes in the list include\nwatchdog\n, which detects system lockups and thus might need to preempt other\nprocesses, and \nmultipathd\n, which watches for device changes and must be able\nto configure those devices before other processes get a chance to talk to them.\nIn addition to the class, the two numeric priority fields tell us how\nprocesses are prioritized within the policy. Not surprisingly, the \nRTPRIO\n field\nmeans “real-time priority” and applies only to real-time processes. The \nNI\nfield is the “nice” level of the process and applies only to non-real-time\nprocesses. For historical reasons, the nice level runs from –20 (least nice, or\nhighest priority) to 19 (nicest, lowest priority).\nSetting Process Priorities\nLinux allows us to set the priority for processes we start. Let’s try to use\npriorities to control CPU usage. We’ll run a program called \nstress\n that is\ndesigned to exercise our system. Let’s use a containerized version of \nstress\nusing CRI-O.\nAs before, we need to define YAML files for the Pod and container to tell\ncrictl\n what to run. The Pod YAML shown in \nListing 3-1\n is almost the same as\nthe BusyBox example in \nChapter 2\n; only the name is different:\npo-nolim.yaml\n---\nmetadata:\n  name: stress\n  namespace: crio\nlinux:\n  security_context:\n    namespace_options:\n      network: 2\nListing 3-1: BusyBox Pod\nThe container YAML has more changes compared to the BusyBox\nexample. In addition to using a different container image, one that already has\nstress\n installed, we also need to provide arguments to \nstress\n to tell it to exercise\na single CPU:\nco-nolim.yaml\n---\nmetadata:\n  name: stress\nimage:\n  image: docker.io/bookofkubernetes/stress:stable\nargs:\n  - ""--cpu""\n  - ""1""\n  - ""-v""\nCRI-O is already installed on \nhost01\n, so it just takes a few commands to\nstart this container. First, we’ll pull the image:\nroot@host01:/opt# \ncrictl pull docker.io/bookofkubernetes/stress:stable\nImage is up to date for docker.io/bookofkubernetes/stress...\nThen, we can run a container from the image:\nroot@host01:~# \ncd /opt\nroot@host01:/opt# \nPUL_ID=$(crictl runp po-nolim.yaml)\nroot@host01:/opt# \nCUL_ID=$(crictl create $PUL_ID co-nolim.yaml po-nolim.yaml)\nroot@host01:/opt# \ncrictl start $CUL_ID\n...\nroot@host01:/opt# \ncrictl ps\nCONTAINER      IMAGE                                    ...\n971e83927329e  docker.io/bookofkubernetes/stress:stable ...\nThe \ncrictl ps\n command is just to check that our container is running as\nexpected.\nThe \nstress\n program is now running on our system, and we can see the\ncurrent priority and CPU usage. We want the current CPU usage, so we’ll use\ntop\n:\nroot@host01:/opt# \ntop -b -n 1 -p $(pgrep -d , stress)\ntop - 18:01:58 up  1:39,  1 user,  load average: 1.01, 0.40, 0.16\nTasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 34.8 us, 0.0 sy, 0.0 ni, 65.2 id, 0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   1987.5 total,   1024.5 free,    195.8 used,    767.3 buff/cache\nMiB Swap:      0.0 total,      0.0 free,      0.0 used.   1643.7 avail Mem \n  PID   USER  PR  NI  ...  %CPU  %MEM    TIME+ COMMAND\n  13459 root  20   0  ... 100.0   0.2  0:29.78 stress-ng\n  13435 root  20   0  ...   0.0   0.2  0:00.01 stress-ng\nThe \npgrep\n command looks up the process IDs (PIDs) for \nstress\n; there are two\nbecause \nstress\n forked a separate process for the CPU exercise we requested.\nThis CPU worker is using up 100 percent of one CPU; fortunately, our VM\nhas two CPUs, so it’s not overloaded.\nWe started this process with default priority, so it has a nice value of \n0\n, as\nshown in the \nNI\n column. What happens if we change that priority? Let’s find\nout using \nrenice\n:\nroot@host01:/opt# \nrenice -n 19 -p $(pgrep -d ' ' stress)\n13435 (process ID) old priority 0, new priority 19\n13459 (process ID) old priority 0, new priority 19\nThe \nps\n command used previously expected the PIDs to be separated with a\ncomma, whereas the \nrenice\n command expects the PIDs to be separated with a\nspace; fortunately, \npgrep\n can handle both.\nWe have successfully changed the priority of the process:\nroot@host01:/opt# \ntop -b -n 1 -p $(pgrep -d , stress)\ntop - 18:11:04 up  1:48,  1 user,  load average: 1.07, 0.95, 0.57\nTasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 0.0 us, 0.0 sy, 28.6 ni, 71.4 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   1987.5 total,   1035.6 free,    182.2 used,    769.7 buff/cache\nMiB Swap:      0.0 total,      0.0 free,      0.0 used.   1657.2 avail Mem \n  PID   USER  PR  NI  ...  %CPU  %MEM     TIME+ COMMAND\n  13459 root  39  19  ... 100.0   0.2   9:35.50 stress-ng\n  13435 root  39  19  ...   0.0   0.2   0:00.01 stress-ng",9082
16-Linux Control Groups.pdf,16-Linux Control Groups,"The new nice value is \n19\n, meaning that our process is lower priority than\nbefore. However, the \nstress\n program is still using 100 percent of one CPU!\nWhat’s going on here? The problem is that priority is only a relative\nmeasurement. If nothing else needs the CPU, as is true in this case, even a\nlower-priority process can use as much as it wants.\nThis arrangement may seem to be what we want. After all, if the CPU is\navailable, shouldn’t we want our application components to be able to use it?\nUnfortunately, even though that sounds reasonable, it’s not suitable for our\ncontainerized applications for two main reasons. First, a container\norchestration environment like Kubernetes works best when a container can\nbe allocated to any host with enough resources to run it. It’s not reasonable\nfor us to know the relative priority of every single container in our\nKubernetes cluster, especially when we consider that a single Kubernetes\ncluster can be \nmultitenant\n, meaning multiple separate applications or teams\nmight be using a single cluster. Second, without some idea of how much CPU\na particular container will use, Kubernetes cannot know which hosts are full\nand which ones have more room available. We don’t want to get into a\nsituation in which multiple containers on the same host all become busy at\nthe same time, because they will fight for the available CPU cores, and the\nwhole host will slow down.\nLinux Control Groups\nAs we saw in the last section, process prioritization will not help a container\norchestration environment like Kubernetes know what host to use when\nscheduling a new container, because even low-priority processes can get a lot\nof CPU time when the CPU is idle. And because our Kubernetes cluster\nmight be multitenant, the cluster can’t just trust each container to promise to\nuse only a certain amount of CPU. First, that would allow one process to\naffect another negatively, either maliciously or accidentally. Second,\nprocesses don’t really control their own scheduling; they get CPU time when\nthe Linux kernel decides to give them CPU time. We need a different\nsolution for controlling CPU utilization.\nTo find the answer, we can take an approach used by real-time processing.\nAs we mentioned in the previous section, a real-time process is typically not\ncompute intensive, but when it needs the CPU, it needs it immediately. To\nensure that all real-time processes get the CPU they need, it is common to\nreserve a slice of the CPU time for each process. Even though our container\nprocesses are non-real-time, we can use the same strategy. If we can\nconfigure our containers so that they can use no more than their allocated\nslice of the CPU time, Kubernetes will be able to calculate how much space\nis available on each host and will be able to schedule containers onto hosts\nwith sufficient space.\nTo manage container use of CPU cores, we will use \ncontrol groups\n.\nControl groups (cgroups) are a feature of the Linux kernel that manage\nprocess resource utilization. Each resource type, such as CPU, memory, or a\nblock device, can have an entire hierarchy of cgroups associated with it. After\na process is in a cgroup, the kernel automatically applies the controls from\nthat group.\nThe creation and configuration of cgroups is handled through a specific\nkind of filesystem, similar to the way that Linux reports information on the\nsystem through the \n/proc\n filesystem. By default, the filesystem for cgroups is\nlocated at \n/sys/fs/cgroup\n:\nroot@host01:~# \nls /sys/fs/cgroup\nblkio        cpuacct  freezer  net_cls           perf_event  systemd\ncpu          cpuset   hugetlb  net_cls,net_prio  pids        unified\ncpu,cpuacct  devices  memory   net_prio          rdma\nEach of the entries in \n/sys/fs/cgroup\n is a different resource that can be\nlimited. If we look in one of those directories, we can begin to see what\ncontrols can be applied. For example, for \ncpu\n:\nroot@host01:~# \ncd /sys/fs/cgroup/cpu\nroot@host01:/sys/fs/cgroup/cpu# \nls -F\ncgroup.clone_children  cpuacct.stat               cpuacct.usage_user\ncgroup.procs           cpuacct.usage              init.scope/\ncgroup.sane_behavior   cpuacct.usage_all          notify_on_release\ncpu.cfs_period_us      cpuacct.usage_percpu       release_agent\ncpu.cfs_quota_us       cpuacct.usage_percpu_sys   system.slice/\ncpu.shares             cpuacct.usage_percpu_user  tasks\ncpu.stat               cpuacct.usage_sys          user.slice/\nThe \n-F\n flag on \nls\n adds a slash character to directories, which enables us to\nbegin to see the hierarchy. Each of those subdirectories (\ninit.scope\n,\nsystem.slice\n, and \nuser.slice\n) is a separate CPU cgroup, and each has its own\nset of configuration files that apply to processes in that cgroup.\nCPU Quotas with cgroups\nTo understand the contents of this directory, let’s see how we can use\ncgroups to limit the CPU usage of our \nstress\n container. We’ll begin by\nchecking its CPU usage again:\nroot@host01:/sys/fs/cgroup/cpu# \ntop -b -n 1 -p $(pgrep -d , stress)\ntop - 22:40:12 up 12 min,  1 user,  load average: 0.81, 0.35, 0.21\nTasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 37.0 us, 0.0 sy, 0.0 ni, 63.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   1987.5 total,   1075.1 free,    179.4 used,    733.0 buff/cache\nMiB Swap:      0.0 total,      0.0 free,      0.0 used.   1646.3 avail Mem \n  PID USER   PR  NI ...  %CPU  %MEM     TIME+ COMMAND\n  5964 root  20  19 ...  100.0  0.2   1:19.72 stress-ng\n  5932 root  20  19 ...  0.0    0.2   0:00.02 stress-ng\nIf you don’t still see \nstress\n running, start it up again using the commands\nfrom earlier in this chapter. Next, let’s explore what CPU cgroup our \nstress\nCPU process is in. We can do this by finding its PID inside a file within the\n/sys/fs/cgroup/cpu\n hierarchy:\nroot@host01:/sys/fs/cgroup/cpu# \ngrep -R $(pgrep stress-ng-cpu)\nsystem.slice/runc-050c.../cgroup.procs:5964\nsystem.slice/runc-050c.../tasks:5964\nThe \nstress\n process is part of the \nsystem.slice\n hierarchy, and is in a\nsubdirectory created by \nrunc\n, which is one of the internal components of CRI-\nO. This is really convenient, as it means we don’t need to create our own\ncgroup and move this process into it. It is also no accident; as we’ll see in a\nmoment, CRI-O supports CPU limits on containers, so it naturally needs to\ncreate a cgroup for each container it runs. In fact, the cgroup is named after\nthe container ID.\nLet’s move into the directory for our container’s cgroup:\nroot@host01:/sys/fs/cgroup/cpu# \ncd system.slice/runc-${CUL_ID}.scope\nWe use the container ID variable we saved earlier to change into the\nappropriate directory. As soon as we’re in this directory, we can see that it\nhas the same configuration files as the root of the hierarchy\n/sys/fs/cgroup/cpu\n:\nroot@host01:/sys/fs/...07.scope# \nls\ncgroup.clone_children  cpu.uclamp.max        cpuacct.usage_percpu_sys\ncgroup.procs           cpu.uclamp.min        cpuacct.usage_percpu_user\ncpu.cfs_period_us      cpuacct.stat          cpuacct.usage_sys\ncpu.cfs_quota_us       cpuacct.usage         cpuacct.usage_user\ncpu.shares             cpuacct.usage_all     notify_on_release\ncpu.stat               cpuacct.usage_percpu  tasks\nThe \ncgroup.procs\n file lists the processes in this control group:\nroot@host01:/sys/fs/...07.scope# \ncat cgroup.procs\n5932\n5964\nThis directory has many other files, but we are mostly interested in three:\ncpu.shares\n Slice of the CPU relative to this cgroup’s peers\ncpu.cfs_period_us\n Length of a period, in microseconds\ncpu.cfs_quota_us\n CPU time during a period, in microseconds\nWe’ll look at how Kubernetes uses \ncpu.shares\n in \nChapter 14\n. For now, we\nneed a way to get our instance under control so that it doesn’t overwhelm our\nsystem. To do that, we’ll set an absolute quota on this container. First, let’s\nsee the value of \ncpu.cfs_period_us\n:\nroot@host01:/sys/fs/...07.scope# \ncat cpu.cfs_period_us\n100000\nThe period is set to 100,000 μs, or 0.1 seconds. We can use this number to\nfigure out what quota to set in order to limit the amount of CPU the \nstress\ncontainer can use. At the moment, there is no quota:\nroot@host01:/sys/fs/...07.scope# \ncat cpu.cfs_quota_us\n-1\nWe can set a quota by just updating the \ncpu.cfs_quota_us\n file:\nroot@host01:/sys/fs/...07.scope# \necho ""50000"" > cpu.cfs_quota_us\nThis provides the processes in this cgroup with 50,000 μs of CPU time per\n100,000 μs, which averages out to 50 percent of a CPU. The processes are\nimmediately affected, as we can confirm:\nroot@host01:/sys/fs/...07.scope# \ntop -b -n 1 -p $(pgrep -d , stress)\ntop - 23:53:05 up  1:24,  1 user,  load average: 0.71, 0.93, 0.98\nTasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.0 us, 3.6 sy, 7.1 ni, 89.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   1987.5 total,   1064.9 free,    174.6 used,    748.0 buff/cache\nMiB Swap:      0.0 total,      0.0 free,      0.0 used.   1663.9 avail Mem \n  PID USER   PR  NI  ...  %CPU  %MEM     TIME+ COMMAND\n  5964 root  39  19  ...  50.0   0.2  73:45.68 stress-ng-cpu\n  5932 root  39  19  ...   0.0   0.2   0:00.02 stress-ng\nYour listing might not show exactly 50 percent CPU usage, because the\nperiod during which the \ntop\n command measures CPU usage might not align\nperfectly with the kernel’s scheduling period. But on average, our \nstress\ncontainer now cannot use more than 50 percent of one CPU.\nBefore we move on, let’s stop the \nstress\n container:\nroot@host01:/sys/fs/...07.scope# \ncd\nroot@host01:/opt# \ncrictl stop $CUL_ID\n...\nroot@host01:/opt# \ncrictl rm $CUL_ID\n...\nroot@host01:/opt# \ncrictl stopp $PUL_ID\nStopped sandbox ...\nroot@host01:/opt# \ncrictl rmp $PUL_ID\nRemoved sandbox ...\nCPU Quota with CRI-O and crictl\nIt would be tiresome to have to go through the process of finding the cgroup\nlocation in the filesystem and updating the CPU quota for every container in\norder to control CPU usage. Fortunately, we can specify the quota in our \ncrictl\nYAML files, and CRI-O will enforce it for us. Let’s look at an example that\nwas installed into \n/opt\n when we set up this example virtual machine.\nThe Pod configuration is only slightly different from \nListing 3-1\n. We add\na \ncgroup_parent\n setting so that we can control where CRI-O creates the cgroup,\nwhich will make it easier to find the cgroup to see the configuration:\npo-clim.yaml\n---\nmetadata:\n  name: stress-clim\n  namespace: crio\nlinux:\n  cgroup_parent: pod.slice\n  security_context:\n    namespace_options:\n      network: 2\nThe container configuration is where we include the CPU limits. Our\nstress1\n container will be allotted only 10 percent of a CPU:\nco-clim.yaml\n---\n---\nmetadata:\n  name: stress-clim\nimage:\n  image: docker.io/bookofkubernetes/stress:stable\nargs:\n  - ""--cpu""\n  - ""1""\n  - ""-v""\nlinux:\n  resources:\n    cpu_period: 100000\n    cpu_quota: 10000\nThe value for \ncpu_period\n corresponds with the file \ncpu.cfs_period_us\n and\nprovides the length of the period during which the quota applies. The value\nfor \ncpu_quota\n corresponds with the file \ncpu.cfs_quota_us\n. Dividing the quota by\nthe period, we can determine that this will set a CPU limit of 10 percent.\nLet’s go ahead and launch this \nstress\n container with its CPU limit:\nroot@host01:~# \ncd /opt\nroot@host01:/opt# \nPCL_ID=$(crictl runp po-clim.yaml)\nroot@host01:/opt# \nCCL_ID=$(crictl create $PCL_ID co-clim.yaml po-clim.yaml)\nroot@host01:/opt# \ncrictl start $CCL_ID\n...\nroot@host01:/opt# \ncrictl ps\nCONTAINER      IMAGE                                    ...\nea8bccd711b86  docker.io/bookofkubernetes/stress:stable ...\nOur container is immediately restricted to 10 percent of a CPU:\nroot@host01:/opt# \ntop -b -n 1 -p $(pgrep -d , stress)\ntop - 17:26:55 up 19 min,  1 user,  load average: 0.27, 0.16, 0.13",12029
17-Memory Limits.pdf,17-Memory Limits,"Tasks:   4 total,   2 running,   2 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 10.3 us, 0.0 sy, 0.0 ni, 89.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   1987.5 total,   1053.4 free,    189.3 used,    744.9 buff/cache\nMiB Swap:      0.0 total,      0.0 free,      0.0 used.   1640.4 avail Mem \n  PID USER   PR  NI ... %CPU  %MEM     TIME+ COMMAND\n  8349 root  20   0 ... 10.0   0.2   0:22.67 stress-ng\n  8202 root  20   0 ...  0.0   0.2   0:00.02 stress-ng\nAs in our earlier example, the CPU usage shown is a snapshot during the\ntime that \ntop\n was running, so it might not match the limit exactly, but over the\nlong term, this process will use no more than its allocated CPU.\nWe can inspect the cgroup to confirm that CRI-O put it in the place we\nspecified and automatically configured the CPU quota:\nroot@host01:/opt# \ncd /sys/fs/cgroup/cpu/pod.slice\nroot@host01:...pod.slice# \ncat crio-$CCL_ID.scope/cpu.cfs_quota_us\n10000\nCRI-O created a new cgroup parent \npod.slice\n for our container, created a\ncgroup within it specific to the container, and configured its CPU quota\nwithout us having to lift a finger.\nWe don’t need this container any longer, so let’s remove it:\nroot@host01:/sys/fs/cgroupcpu/pod.slice# \ncd\nroot@host01:~# \ncrictl stop $CCL_ID\n...\nroot@host01:~# \ncrictl rm $CCL_ID\n...\nroot@host01:~# \ncrictl stopp $PCL_ID\nStopped sandbox ...\nroot@host01:~# \ncrictl rmp $PCL_ID\nRemoved sandbox ...\nWith these commands we stop and then delete first the container, then the\nPod.\nMemory Limits\nMemory is another important resource for a process. If a system doesn’t have\nsufficient memory to meet a request, the allocation of memory will fail. This\nusually causes the process to behave badly or to fail entirely. Of course, most\nLinux systems use \nswap space\n to write memory contents to disk temporarily,\nwhich allows the system memory to appear larger than it is but also reduces\nsystem performance. It’s a big enough concern that the Kubernetes team\ndiscourages having swap enabled in a cluster.\nAlso, even if we could use swap, we don’t want one process grabbing all\nthe resident memory and making other processes very slow. As a result, we\nneed to limit the memory usage of our processes so that they cooperate with\none another. We also need to have a clear maximum for memory usage so\nthat Kubernetes can reliably ensure that a host has enough available memory\nbefore scheduling a new container onto a host.\nLinux systems, like other variants of Unix, have traditionally had to deal\nwith multiple users who are sharing scarce resources. For this reason, the\nkernel supports limits on system resources, including CPU, memory, number\nof child processes, and number of open files. We can set these limits from the\ncommand line using the \nulimit\n command. For example, one type of limit is a\nlimit on “virtual memory.” This includes not only the amount of \nRAM a\nprocess has in resident memory but also any swap space it is using. Here’s an\nexample of a \nulimit\n command limiting virtual memory:\nroot@host01:~# \nulimit -v 262144\nThe \n-v\n switch specifies a limit on virtual memory. The parameter is in\nbytes, so 262144 places a virtual memory limit of 256MiB on each additional\nprocess we start from this shell session. Setting a virtual memory limit is a\ntotal limit; it allows us to ensure that a process can’t use swap to get around\nthe limit. We can verify the limit was applied by pulling some data into\nmemory:\nroot@host01:~# \ncat /dev/zero | head -c 500m | tail\ntail: memory exhausted\nThis command reads from \n/dev/zero\n and tries to keep the first 500MiB of\nzeros it finds in memory. However, at some point, when the \ntail\n command\ntries to allocate more space to hold the zeros it is getting from \nhead\n, it fails\nbecause of the limit.\nThus, Unix limits give us the ability to control memory usage for our\nprocesses, but they won’t provide everything we need for containers, for a\ncouple of reasons. First, Unix limits can be applied only to individual\nprocesses or to an entire user. Neither of those provide what we need, as a\ncontainer is really a \ngroup\n of processes. A container’s initial process might\ncreate many child processes, and all processes in a container need to live\nwithin the same limit. At the same time, applying limits to an entire user\ndoesn’t really help us in a container orchestration environment like\nKubernetes, because from the perspective of the operating system, all of the\ncontainers belong to the same user. Second, when it comes to CPU limits, the\nonly thing that regular Unix limits can do is limit the maximum CPU time\nour process gets before it is terminated. That isn’t the kind of limit we need\nfor sharing the CPU between long-running processes.\nInstead of using traditional Unix limits, we’ll use cgroups again, this time\nto limit the memory available to a process. We’ll use the same \nstress\n container\nimage, this time with a child process that tries to allocate lots of memory.\nIf we were to try to apply a memory limit to this \nstress\n container after\nstarting it, we would find that the kernel won’t let us, because it will have\nalready grabbed too much memory. So instead we’ll apply it immediately in\nthe YAML configuration. As before, we need a Pod:\npo-mlim.yaml\n---\nmetadata:\n  name: stress2\n  namespace: crio\nlinux:\n  cgroup_parent: pod.slice\n  security_context:\n    namespace_options:\n      network: 2\nThis is identical to the Pod we used for CPU limit, but the name is\ndifferent to avoid a collision. As we did earlier, we are asking CRI-O to put\nthe cgroup into \npod.slice\n so that we can find it easily.\nWe also need a container definition:\nco-mlim.yaml\n ---\n ---\n metadata:\n   name: stress2\n image:\n   image: docker.io/bookofkubernetes/stress:stable\n args:\n   - ""--vm""\n   - ""1""\n   - ""--vm-bytes""\n➊\n - ""512M""\n   - ""-v""\n linux:\n   resources:\n  \n➋\n memory_limit_in_bytes: 268435456\n     cpu_period: 100000 \n  \n➌\n cpu_quota: 10000\nThe new resource limit is \nmemory_limit_in_bytes\n, which we set to 256MiB \n➋\n.\nWe keep the CPU quota in there \n➌\n because continuously trying to allocate\nmemory is going to use a lot of CPU. Finally, in the \nargs\n section, we tell \nstress\nto try to allocate 512MB of memory \n➊\n.\nWe can run this using similar \ncrictl\n commands to what we’ve previously\nused:\nroot@host01:~# \ncd /opt\n \nroot@host01:/opt# \nPML_ID=$(crictl runp po-mlim.yaml)\nroot@host01:/opt# \nCML_ID=$(crictl create $PML_ID co-mlim.yaml po-mlim.yaml)\nroot@host01:/opt# \ncrictl start $CML_ID\n...\nIf we tell \ncrictl\n to list containers, everything seems okay:\nroot@host01:/opt# \ncrictl ps\nCONTAINER     IMAGE                                    ... STATE   ...\n31025f098a6c9 docker.io/bookofkubernetes/stress:stable ... Running ...\nThis reports that the container is in a \nRunning\n state. However, behind the\nscenes, \nstress\n is struggling to allocate memory. We can see this if we print out\nthe log messages coming from the \nstress\n container:\nroot@host01:/opt# \ncrictl logs $CML_ID\n...\nstress-ng: info:  [6] dispatching hogs: 1 vm\n...\nstress-ng: debug: [11] stress-ng-vm: started [11] (instance 0)\nstress-ng: debug: [11] stress-ng-vm using method 'all'\nstress-ng: debug: [11] stress-ng-vm: child died: signal 9 'SIGKILL' (instance 0)\nstress-ng: debug: [11] stress-ng-vm: assuming killed by OOM killer, restarting again...\nstress-ng: debug: [11] stress-ng-vm: child died: signal 9 'SIGKILL' (instance 0)\nstress-ng: debug: [11] stress-ng-vm: assuming killed by OOM killer, restarting again...\nStress is reporting that its memory allocation process is being\ncontinuously killed by the “out of memory.”\nAnd we can see the kernel reporting that the \noom_reaper\n is indeed the reason\nthat the processes are being killed:\nroot@host01:/opt# \ndmesg | grep -i oom_reaper | tail -n 1\n[  696.651056] oom_reaper: reaped process 8756 (stress-ng-vm)...\nThe \nOOM killer\n is the same feature Linux uses when the whole system is\nlow on memory and it needs to kill one or more processes to protect the\nsystem. In this case, it is sending \nSIGKILL\n to the process to keep the cgroup\nunder its memory limit. \nSIGKILL\n is a message to the process that it should\nimmediately terminate without any cleanup.\nWHY USE THE OOM KILLER?\nWhen we used regular limits to control memory, an attempt to exceed\nour limits caused the memory allocation to fail, but the kernel didn’t use\nthe OOM killer to kill our process. Why the difference? The answer is\nthat this is the nature of containers. As we look at architecting reliable\nsystems using containerized microservices, we’ll see that a container is\nsupposed to be quick to start and quick to scale. This means that each\nindividual container in our application is intentionally just not very\nimportant. This further means that the idea that one of our containers\ncould be killed unexpectedly is not really a concern. Add to that the fact\nthat not checking for memory allocation errors is one of the most\ncommon bugs, so it’s considered safer simply to kill the process.\nThat said, it’s worth noting that it is possible to turn off the OOM killer\nfor a cgroup. However, rather than having the memory allocation fail,\nthe effect is to just pause the process until other processes in the group\nfree up memory. That’s actually worse, as now we have a process that\nisn’t officially killed but isn’t doing anything useful either.\nBefore we move on, let’s put this continuously failing \nstress\n container out\nof its misery:",9655
18-Network Bandwidth Limits.pdf,18-Network Bandwidth Limits,"root@host01:/opt# \ncrictl stop $CML_ID\n...\nroot@host01:/opt# \ncrictl rm $CML_ID\n...\nroot@host01:/opt# \ncrictl stopp $PML_ID\nStopped sandbox ...\nroot@host01:/opt# \ncrictl rmp $PML_ID\nRemoved sandbox ...\nroot@host01:/opt# \ncd\nStopping and removing the container and Pod prevents the \nstress\n container\nfrom wasting CPU by continually trying to restart the memory allocation\nprocess.\nNetwork Bandwidth Limits\nIn this chapter, we’ve moved from resources that are easy to limit to\nresources that are more difficult to limit. We started with CPU, where the\nkernel is wholly in charge of which process gets CPU time and how much\ntime it gets before being preempted. Then we looked at memory, where the\nkernel doesn’t have the ability to force a process to give up memory, but at\nleast the kernel can control whether a memory allocation is successful, or it\ncan kill a process that requests too much memory.\nNow we’re moving on to network bandwidth, for which control is even\nmore difficult to exert for two important reasons. First, network devices don’t\nreally “sum up” like CPU or memory, so we’ll need to limit usage at the level\nof each individual network device. Second, our system can’t really control\nwhat is sent to it across the network; we can only completely control \negress\nbandwidth, the traffic that is sent on a given network device.\nPROPER NETWORK MANAGEMENT\nTo have a completely reliable cluster, merely controlling egress traffic\nis clearly insufficient. A process that downloads a large file is going to\nsaturate the available bandwidth just as much as one that uploads lots of\ndata. However, we really can’t control what comes into our host via a\ngiven network interface, at least not at the host level. If we really want\nto manage network bandwidth, we need to handle that kind of thing at a\nswitch or a router. For example, it is very common to divide up the\nphysical network into virtual local area networks (VLANs). One VLAN\nmight be an administration network used for auditing, logging, and for\nadministrators to ensure that they can log in. We might also reserve\nanother VLAN for important container traffic, or use traffic shaping to\nensure that important packets get through. As long as we perform this\nkind of configuration at the switch, we can typically allow the\nremaining bandwidth to be “best effort.”\nAlthough Linux does provide some cgroup capability for network\ninterfaces, these would only help us prioritize and classify network traffic.\nFor this reason, rather than using cgroups to control egress traffic, we’re\ngoing to directly configure the Linux kernel’s \ntraffic control\n capabilities.\nWe’ll test network performance using \niperf3\n, apply a limit to outgoing traffic,\nand then test again. In this chapter’s examples, \nhost02\n with IP address\n192.168.61.12\n was set up automatically with an \niperf3\n server running so that we\ncan send data to it from \nhost01\n.\nLet’s begin by seeing the egress bandwidth we can get on an unlimited\ninterface:\nroot@host01:~# \niperf3 -c 192.168.61.12\nConnecting to host 192.168.61.12, port 5201\n[  5] local 192.168.61.11 port 49044 connected to 192.168.61.12 port 5201\n...\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec  2.18 GBytes  1.87 Gbits/sec  13184             sender\n[  5]   0.00-10.00  sec  2.18 GBytes  1.87 Gbits/sec                  receiver\n...\nThis example shows gigabit network speeds. Depending on how you’re\nrunning the examples, you might see lower or higher figures. Now that we\nhave a baseline, we can use \ntc\n to set a quota going out. You’ll want to choose\na quota that makes sense given your bandwidth; most likely enforcing a\n100Mb cap will work:\nroot@host01:~# \nIFACE=$(ip -o addr | grep 192.168.61.11 | awk '{print $2}')\nroot@host01:~# \ntc qdisc add dev $IFACE root tbf rate 100mbit \\n  \nburst 256kbit latency 400ms",3937
19-Final Thoughts.pdf,19-Final Thoughts,"The name of the network interface may be different on different systems,\nso we use \nip addr\n to identify which interface we want to control. Then, we use\ntc\n to actually apply the limit. The token \ntbf\n in the command stands for \ntoken\nbucket filter\n. With a token bucket filter, every packet consumes tokens. The\nbucket refills with tokens over time, but if at any point the bucket is empty,\npackets are queued until tokens are available. By controlling the size of the\nbucket and the rate at which it refills, it is very easy for the kernel to place a\nbandwidth limit.\nNow that we’ve applied a limit to this interface, let’s see it in action by\nrunning the exact same \niperf3\n command again:\nroot@host01:~# \niperf3 -c 192.168.61.12\nConnecting to host 192.168.61.12, port 5201\n[  5] local 192.168.61.11 port 49048 connected to 192.168.61.12 port 5201\n...\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec   114 MBytes  95.7 Mbits/sec    0             sender\n[  5]   0.00-10.01  sec   113 MBytes  94.5 Mbits/sec                  receiver\n...\nAs expected, we are now limited to 100Mbps on this interface.\nOf course, in this case, we limited the bandwidth available on this network\ninterface for everyone on the system. To use this ability properly to control\nbandwidth usage, we need to target the limits more precisely. However, in\norder to do that, we need to isolate a process to its own set of network\ninterfaces, which is the subject of the next chapter.\nFinal Thoughts\nEnsuring that a process doesn’t cause problems for other processes on the\nsystem includes making sure that it fairly shares system resources such as\nCPU, memory, and network bandwidth. In this chapter, we looked at how\nLinux provides control groups (cgroups) that manage CPU and memory\nlimits and traffic control capabilities that manage network interfaces. As we\ncreate a Kubernetes cluster and deploy containers to it, we’ll see how\nKubernetes uses these underlying Linux kernel features to ensure that\ncontainers are scheduled on hosts with sufficient resources and that\ncontainers are well behaved on those hosts.\nWe’ve now moved through some of the most important elements of\nprocess isolation provided by a container runtime, but there are two types of\nisolation that we haven’t explored yet: network isolation and storage\nisolation. In the next chapter, we’ll look at how Linux network namespaces\nare used to make each container appear to have its own set of network\ninterfaces, complete with separate IP addresses and ports. We’ll also look at\nhow traffic from those separate container interfaces flows through our system\nso that containers can talk to one another and to the rest of the network.",2762
20-4 NETWORK NAMESPACES.pdf,20-4 NETWORK NAMESPACES,,0
21-Network Isolation.pdf,21-Network Isolation,"4\nNETWORK NAMESPACES\nUnderstanding container networking is the biggest challenge in building\nmodern applications based on containerized microservices. First, networking\nis complicated even without introducing containers. Multiple levels of\nabstraction are involved just in sending a simple \nping\n from one physical server\nto another. Second, containers introduce additional complexity because each\nhas its own set of virtual network devices to make it look like a separate\nmachine. Not only that, but a container orchestration framework like\nKubernetes then adds another layer of complexity by adding an “overlay”\nnetwork through which containers can communicate even when they are\nrunning on different hosts.\nIn this chapter, we will look in detail at how container networking\noperates. We will look at a container’s virtual network devices, including\nhow each network device is assigned a separate IP address that can reach the\nhost. We’ll see how containers on the same host are connected to one another\nthrough a bridge device and how container devices are configured to \nroute\ntraffic. Finally, we’ll examine how address translation is used to enable\ncontainers to connect to other hosts without exposing container networking\ninternals on the host’s network.\nNetwork Isolation\nIn \nChapter 2\n, we discussed how isolation is important to system reliability\nbecause processes generally can’t affect something they cannot see. This is\none important reason for network isolation in containers. Another reason is\nease of configuration. To run a process that acts as a server, such as a web\nserver, we need to choose one or more network interfaces on which that\nserver will listen, and we need to choose a port number on which it will\nlisten. We can’t have two processes listening on the same port on the same\ninterface.\nAs a result, it’s common for a process that acts as a server to provide a\nway to configure which port it should use to listen for connections. However,\nthat still requires us to know what other servers are out there and what ports\nthey are using so that we can ensure there are no conflicts. That would be\nimpossible with a container orchestration framework like Kubernetes because\nnew processes can show up at any time, from different users, with a need to\nlisten on any port number.\nThe way to get around this is to provide separate virtual network\ninterfaces for each container. That way, a process in a container can choose\nany port it wants—it will be listening on a different network interface from a\nprocess in a different container. Let’s see a quick example.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nWe’ll run two instances of an NGINX web server; each instance will\nlisten on port 80. As before, we’ll use CRI-O and \ncrictl\n, but we’ll use a script\nto cut down on the typing:\nroot@host01:~# \ncd /opt\nroot@host01:/opt# \nsource nginx.sh\n...\nThe \nsource\n before \nnginx.sh\n is important; it ensures that the script is run in a\nway that makes the environment variables it sets available in our shell for\nfuture commands. Inside \nnginx.sh\n are the usual \ncrictl runp\n, \ncrictl create\n, and \ncrictl\nstart\n commands we’ve used in previous chapters. The YAML files are also\nvery similar to examples we’ve seen before; the only difference is that we use\na container image that has NGINX installed.\nLet’s verify that we have two NGINX servers running:\nroot@host01:/opt# \ncrictl ps\nCONTAINER      IMAGE            ... NAME    ...\nae341010886ae  .../nginx:latest ... nginx2  ...\n6a95800b16f15  .../nginx:latest ... nginx1  ...\nWe can also verify that both NGINX servers are listening on port 80, the\nstandard port for web servers:\nroot@host01:/opt# \ncrictl exec $N1C_ID cat /proc/net/tcp\n  sl  local_address ...\n   0: 00000000:0050 ...\nroot@host01:/opt# \ncrictl exec $N2C_ID cat /proc/net/tcp\n  sl  local_address ...\n   0: 00000000:0050 ...\nWe look at the open port by printing \n/proc/net/tcp\n because we need to run\nthis command inside the NGINX container, where we don’t have standard\nLinux commands like \nnetstat\n or \nss\n. As we saw in \nChapter 2\n, in a container we\nhave a separate \nmnt\n namespace providing a separate filesystem for each\ncontainer, so only the executables available in that separate filesystem can be\nrun in that namespace.\nThe port shown in both cases is \n0050\n in hexadecimal, which is port 80 in\ndecimal. If these two processes were running together on the same system\nwithout network isolation, they wouldn’t both be able to listen on port 80, but\nin this case, the two NGINX instances have separate network interfaces. To\nexplore this further, let’s start up a new BusyBox container:\nroot@host01:/opt# \nsource busybox.sh\n...\nBusyBox is now running in addition to our two NGINX containers:\nroot@host01:/opt# \ncrictl ps\nCONTAINER      IMAGE              ... NAME    ...\n189dd26766d26  .../busybox:latest ... busybox ...\nae341010886ae  .../nginx:latest   ... nginx2  ...\n6a95800b16f15  .../nginx:latest   ... nginx1  ...\nLet’s start a shell inside the container:\nroot@host01:/opt# \ncrictl exec -ti $B1C_ID /bin/sh\n/ #\nListing 4-1\n shows the container’s network devices and addresses.\n/ # \nip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue ...\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n        valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n        valid_lft forever preferred_lft forever\n3: eth0@if7: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue \n    link/ether 9a:7c:73:2f:f7:1a brd ff:ff:ff:ff:ff:ff\n    inet 10.85.0.4/16 brd 10.85.255.255 scope global eth0\n        valid_lft forever preferred_lft forever\n    inet6 fe80::987c:73ff:fe2f:f71a/64 scope link \n        valid_lft forever preferred_lft forever\nListing 4-1: BusyBox network\nIgnoring the standard loopback device, we see a network device with\n10.85.0.4\n for an IP address. This does not correspond at all with the IP address\nof the host, which is \n192.168.61.11\n; it is on a different network entirely. Because\nour container is on a separate network, we might not expect to be able to \nping\nthe underlying host system from inside the container, but it works, as \nListing\n4-2\n demonstrates.\n/ # \nping -c 1 192.168.61.11\nPING 192.168.61.11 (192.168.61.11): 56 data bytes\n64 bytes from 192.168.61.11: seq=0 ttl=64 time=7.471 ms\n--- 192.168.61.11 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 7.471/7.471/7.471 ms\nListing 4-2: BusyBox ping test\nFor traffic to get from our container to the host network, there must be an\nentry in the routing table to make that happen. As \nListing 4-3\n illustrates, we\ncan verify this by using the \nip\n command.\n/ # \nip route",7025
22-Network Namespaces.pdf,22-Network Namespaces,"default via 10.85.0.1 dev eth0 \n10.85.0.0/16 dev eth0 scope link  src 10.85.0.4\nListing 4-3: BusyBox routes\nAs expected, there is a default route. When we sent the \nping\n, our BusyBox\ncontainer reached out to \n10.85.0.1\n, which then had the ability to send the \nping\nonward until it reached \n192.168.61.11\n.\nWe’ll leave all three containers running to explore them further, but let’s\nexit our BusyBox shell to get back to the host:\n/ # \nexit\nThe view of the network from inside the container shows why our two\nNGINX servers are both able to listen on port 80. As mentioned earlier, only\none process can listen on a port for a particular interface, but of course, if\neach NGINX server has a separate network interface, there is no conflict.\nNetwork Namespaces\nCRI-O is using Linux network namespaces to create this isolation. We\nexplored network namespaces briefly in \nChapter 2\n; in this chapter, we’ll look\nat them in more detail.\nFirst, let’s use the \nlsns\n command to list the network namespaces that CRI-\nO has created for our containers:\nroot@host01:/opt# lsns -t net\n        NS TYPE NPROCS   PID USER    NETNSID NSFS                   COMMAND\n4026531992 net     114     1 root unassigned                        /sbin/init\n4026532196 net       4  5801 root          0 /run/netns/ab8be6e6... /pause\n4026532272 net       4  5937 root          1 /run/netns/8ffe0394... /pause\n4026532334 net       2  6122 root          2 /run/netns/686d71d9... /pause\nIn addition to the root network namespace that is used for all the processes\nthat aren’t in a container, we see three network namespaces, one for each Pod\nwe’ve created.\nWhen we use CRI-O with \ncrictl\n, the network namespace actually belongs to\nthe Pod. The \npause\n process that is listed here exists so that the namespaces can\ncontinue to exist even as containers come and go inside the Pod.\nIn the previous example, there are four network namespaces. The first one\nis the root namespace that was created when our host booted. The other three\nwere created for each of the containers we have started so far: two NGINX\ncontainers and one BusyBox container.\nInspecting Network Namespaces\nTo learn about how network namespaces work and manipulate them, we’ll\nuse the \nip netns\n command to list network namespaces:\nroot@host01:/opt# \nip netns list\n7c185da0-04e2-4321-b2eb-da18ceb5fcf6 (id: 2)\nd26ca6c6-d524-4ae2-b9b7-5489c3db92ce (id: 1)\n38bbb724-3420-46f0-bb50-9a150a9f0889 (id: 0)\nThis command looks in a different configuration location to find network\nnamespaces, so only the three container namespaces are listed.\nWe want to capture the network namespace for our BusyBox container.\nIt’s one of the three listed, and we can guess that it is the one labeled \n(id: 2)\nbecause we created it last, but we can also use \ncrictl\n and \njq\n to extract the\ninformation we need:\nroot@host01:/opt# \nNETNS_PATH=$(crictl inspectp $B1P_ID |\n  \njq -r '.info.runtimeSpec.linux.namespaces[]|select(.type==""network"").path')\nroot@host01:/opt# \necho $NETNS_PATH\n/var/run/netns/7c185da0-04e2-4321-b2eb-da18ceb5fcf6\nroot@host01:/opt# \nNETNS=$(basename $NETNS_PATH)\nroot@host01:/opt# \necho $NETNS\n7c185da0-04e2-4321-b2eb-da18ceb5fcf6\nIf you run \ncrictl inspectp $B1P_ID\n by itself, you’ll see a wealth of information\nabout the BusyBox Pod. Out of all that information, we want only the\ninformation about the network namespace, so we use \njq\n to extract that\ninformation in three steps. First, it reaches down into the JSON data to pull\nout all of the namespaces associated with this Pod. It then selects only the\nnamespace that has a \ntype\n field of \nnetwork\n. Finally, it extracts the \npath\n field for\nthat namespace and stores it in the environment variable \nNETNS_PATH\n.\nThe value that \ncrictl\n returns is the full path to the network namespace under\n/var/run\n. For our upcoming commands, we want only the value of the\nnamespace, so we use \nbasename\n to strip off the path. Also, because this\ninformation will be a lot more usable if we assign it to an environment\nvariable, we do that, and then we use \necho\n to print the value so that we can\nconfirm it all worked.\nOf course, for interactive debugging, you can often just scroll through the\nentire contents of \ncrictl inspectp\n (for Pods) and \ncrictl inspect\n (for containers) and\npick out the values you want. But this approach of extracting data with \njq\n is\nvery useful in scripting or in reducing the amount of output to scan through\nmanually.\nNow that we’ve extracted the network namespace for BusyBox from \ncrictl\n,\nlet’s see what processes are assigned to that namespace:\nroot@host01:/opt# \nps --pid $(ip netns pids $NETNS)\nPID TTY      STAT   TIME COMMAND\n5800 ?        Ss     0:00 /pause\n5839 ?        Ss     0:00 /bin/sleep 36000\nIf we just ran \nip netns pids $NETNS\n, we would get a list of the process IDs\n(PIDs), but no extra information. We take that output and send it to \nps --pid\n,\nwhich makes it possible for us to see the name of the commands. As\nexpected, we see a \npause\n process and the \nsleep\n process that we specified when\nwe ran the BusyBox container.\nIn the previous section, we used \ncrictl exec\n to run a shell inside the\ncontainer, which enabled us to see what network interfaces were available in\nthat network namespace. Now that we know the ID of the network\nnamespace, we can use \nip netns exec\n to run commands individually from within\na network namespace. Running \nip netns exec\n is very powerful in that it is not\nlimited to just networking commands, but could be any process such as a web\nserver. However, note that this is not the same as fully running inside the\ncontainer, because we are not entering any of the container’s other\nnamespaces (for example, the \npid\n namespace used for process isolation).\nNext, let’s try the \nip addr\n command from within the BusyBox network\nnamespace:\nroot@host01:/opt# \nip netns exec $NETNS ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue ...\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n        valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n        valid_lft forever preferred_lft forever\n3: eth0@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue ...\n    link/ether 9a:7c:73:2f:f7:1a brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.85.0.4/16 brd 10.85.255.255 scope global eth0\n        valid_lft forever preferred_lft forever\n    inet6 fe80::987c:73ff:fe2f:f71a/64 scope link \n        valid_lft forever preferred_lft forever\nThe list of network devices and IP addresses that we see here matches\nwhat we saw when we ran commands inside our BusyBox container in\nListing 4-1\n. CRI-O is creating these network devices and placing them in the\nnetwork namespace. (We will see how CRI-O was configured to perform\ncontainer networking when we look at Kubernetes networking in \nChapter 8\n.)\nFor now, let’s look at how we can create our own devices and namespaces for\nnetwork isolation. This will also show us how to debug container networking\nwhen something isn’t working correctly.\nCreating Network Namespaces\nWe can create a network namespace with a single command:\nroot@host01:/opt# \nip netns add myns\nThis new namespace immediately shows up in the list:\nroot@host01:/opt# \nip netns list\nmyns\n7c185da0-04e2-4321-b2eb-da18ceb5fcf6 (id: 2)\nd26ca6c6-d524-4ae2-b9b7-5489c3db92ce (id: 1)\n38bbb724-3420-46f0-bb50-9a150a9f0889 (id: 0)\nThis namespace isn’t very useful yet; it has a loopback interface but\nnothing else:\nroot@host01:/opt# \nip netns exec myns ip addr\n1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\nIn addition, even the loopback interface is down, so it couldn’t be used.\nLet’s quickly fix that:\nroot@host01:/opt# \nip netns exec myns ip link set dev lo up\nroot@host01:/opt# \nip netns exec myns ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue ...\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\nThe loopback interface is now up, and it has the typical IP address of\n127.0.0.1\n. A basic loopback \nping\n will now work in this network namespace:\nroot@host01:/opt# \nip netns exec myns ping -c 1 127.0.0.1\nPING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.\n64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.035 ms\n--- 127.0.0.1 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.035/0.035/0.035/0.000 ms\nThe ability to \nping\n the loopback network interface is a useful first test for\nany networking stack, as it shows the ability to send and receive packets. So,\nwe now have a basic working network stack in our new network namespace,\nbut it still isn’t terribly useful because a loopback interface by itself can’t talk\nto anything else on our system. We need to add another network device in\nthis network namespace in order to establish connectivity to the host and the\nrest of the network.\nTo do this, we’ll create a \nvirtual Ethernet\n (veth) device. You can think of\na veth as a virtual network cable. Like a network cable, it has two ends, and\nwhatever goes in one end comes out the other end. For this reason, the term\nveth pair\n is often used.\nWe start with a command that creates the veth pair:\nroot@host01:/opt# \nip link add myveth-host type veth \\n                  \npeer myveth-myns netns myns\nThis command does three things:\n1\n. \nCreates a veth device called \nmyveth-host\n2\n. \nCreates a veth device called \nmyveth-myns\n3\n. \nPlaces the device \nmyveth-myns\n in the network namespace \nmyns\nThe host side of the veth pair appears in the regular list of network devices\non the host:\nroot@host01:/opt# \nip addr\n...\n8: myveth-host@if2: <BROADCAST,MULTICAST> mtu 1500 ... state DOWN ...\n    link/ether fe:7a:5d:86:00:d9 brd ff:ff:ff:ff:ff:ff link-netns myns\nThis output shows \nmyveth-host\n and also that it is connected to a device in the\nnetwork namespace \nmyns\n.\nIf you run this command for yourself and look at the complete list of host\nnetwork devices, you will notice additional \nveth\n devices connected to each of\nthe container network namespaces. These were created by CRI-O when we\ndeployed NGINX and BusyBox.\nSimilarly, we can see that our \nmyns\n network namespace has a new network\ninterface:\nroot@host01:/opt# \nip netns exec myns ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue ...\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: myveth-myns@if8: <BROADCAST,MULTICAST> mtu 1500 ... state DOWN ...\n    link/ether 26:0f:64:a8:37:1f brd ff:ff:ff:ff:ff:ff link-netnsid 0\nAs before, this interface is currently down. We need to bring up both sides\nof the veth pair before we can start communicating. We also need to assign\nan IP address to the \nmyveth-myns\n side to enable it to communicate:\nroot@host01:/opt# \nip netns exec myns ip addr add 10.85.0.254/16 \\n                  \ndev myveth-myns\nroot@host01:/opt# \nip netns exec myns ip link set dev myveth-myns up\nroot@host01:/opt# \nip link set dev myveth-host up\nA quick check confirms that we’ve successfully configured an IP address\nand brought up the network:\nroot@host01:/opt# \nip netns exec myns ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue ...\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: myveth-myns@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> ... state UP ...",12100
23-Bridge Interfaces.pdf,23-Bridge Interfaces,"link/ether 26:0f:64:a8:37:1f brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.85.0.254/16 scope global myveth-myns\n       valid_lft forever preferred_lft forever\n    inet6 fe80::240f:64ff:fea8:371f/64 scope link \n       valid_lft forever preferred_lft forever\nIn addition to the loopback interface, we now see an additional interface\nwith the IP address \n10.85.0.254\n. What happens if we try to \nping\n this new IP\naddress? It turns out we can indeed \nping\n it, but only from within the network\nnamespace:\n   root@host01:/opt# \nip netns exec myns ping -c 1 10.85.0.254\n   PING 10.85.0.254 (10.85.0.254) 56(84) bytes of data.\n   64 bytes from 10.85.0.254: icmp_seq=1 ttl=64 time=0.030 ms\n   --- 10.85.0.254 ping statistics ---\n➊\n 1 packets transmitted, 1 received, 0% packet loss, time 0ms\n   \nrtt min/avg/max/mdev = 0.030/0.030/0.030/0.000 ms\n   root@host01:/opt# \nping -c 1 10.85.0.254\n   PING 10.85.0.254 (10.85.0.254) 56(84) bytes of data.\n   From 10.85.0.1 icmp_seq=1 Destination Host Unreachable\n   --- 10.85.0.254 ping statistics ---\n➋\n 1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms\nThe first \nping\n command, run using \nip netns exec\n so that it runs within the\nnetwork namespace, shows a successful response \n➊\n. However, the second\nping\n command, run without \nip netns exec\n, shows that no packets were received\n➋\n. The problem is that we have successfully created a network interface\ninside our network namespace, and we have the other end of the veth pair on\nour host network, but we haven’t connected up a corresponding network\ndevice on the host, so there’s no host network interface that can talk to the\ninterface in the network namespace.\nAt the same time, when we ran a \nping\n test from our BusyBox container in\nListing 4-2\n, we were able to \nping\n the host with no trouble. Clearly, there must\nbe more configuration that CRI-O did for us when it created our containers.\nLet’s explore that in the next section.\nBridge Interfaces\nThe host side of the veth pair currently isn’t connected to anything, so it isn’t\nsurprising that our network namespace can’t talk to the outside world yet. To\nfix that, let’s look at one of the veth pairs that CRI-O created:\nroot@host01:/opt# \nip addr\n...\n7: veth062abfa6@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> ... \nmaster cni0\n ...\n    link/ether fe:6b:21:9b:d0:d2 brd ff:ff:ff:ff:ff:ff link-netns ...\n    inet6 fe80::fc6b:21ff:fe9b:d0d2/64 scope link \n       valid_lft forever preferred_lft forever\n...\nUnlike the interface we created, this interface specifies \nmaster cni0\n, which\nshows that it belongs to a \nnetwork bridge\n. A network bridge exists to connect\nmultiple interfaces together. You can think of it as an Ethernet switch\nbecause it routes traffic from one network interface to another based on the\nmedia access control (MAC) address of the interfaces.\nWe can see the bridge \ncni0\n in the list of network devices on the host:\nroot@host01:/opt# \nip addr\n...\n4: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue ...\n    link/ether 8e:0c:1c:7d:94:75 brd ff:ff:ff:ff:ff:ff\n    inet 10.85.0.1/16 brd 10.85.255.255 scope global cni0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::8c0c:1cff:fe7d:9475/64 scope link \n       valid_lft forever preferred_lft forever\n...\nThe bridge is a little smarter than a typical Ethernet switch in that it\nprovides some firewall and routing capabilities. It also has an IP address of\n10.85.0.1\n. This IP address is the same as we saw with the default route for our\nBusyBox container in \nListing 4-3\n, so we’ve started to solve the mystery of\nhow our BusyBox container is able to talk to hosts outside of its own\nnetwork.\nAdding Interfaces to a Bridge\nTo inspect the bridge and add devices to it, we’ll use the \nbrctl\n command. Let’s\ninspect the bridge first:\nroot@host01:/opt# \nbrctl show\nbridge name     bridge id               STP enabled     interfaces\ncni0            8000.8e0c1c7d9475       no              veth062abfa6\n                                                        veth43ab68cd\n                                                        vetha251c619\nThe bridge \ncni0\n has three interfaces on it, corresponding to the host side of\nthe veth pair for each of the three containers we have running (two NGINX\nand one BusyBox). Let’s take advantage of this existing bridge to set up\nnetwork connectivity to the namespace we created:\nroot@host01:/opt# \nbrctl addif cni0 myveth-host\nroot@host01:/opt# \nbrctl show\nbridge name     bridge id               STP enabled     interfaces\ncni0            8000.8e0c1c7d9475       no              myveth-host\n                                                        veth062abfa6\n                                                        veth43ab68cd\n                                                        vetha251c619\nThe host side of our veth pair is now connected to the bridge, which\nmeans that we can now \nping\n into the namespace from the host:\n   root@host01:/opt# \nping -c 1 10.85.0.254\n   PING 10.85.0.254 (10.85.0.254) 56(84) bytes of data.\n   64 bytes from 10.85.0.254: icmp_seq=1 ttl=64 time=0.194 ms\n   --- 10.85.0.254 ping statistics ---\n➊\n 1 packets transmitted, 1 received, 0% packet loss, time 0ms\n   rtt min/avg/max/mdev = 0.194/0.194/0.194/0.000 ms\nThe fact that a packet was received \n➊\n shows that we set up a working\nconnection. We should be pleased that it worked, but if we want to really\nunderstand this, we can’t be satisfied with saying that we can \nping\n this\ninterface “from the host.” We need to be more specific as to exactly how\ntraffic is flowing.\nTracing Traffic\nLet’s actually trace this traffic to see what’s happening when we run the \nping\ncommand. We will use \ntcpdump\n to print out the traffic. First, let’s start a \nping\ncommand in the background to create some traffic to trace:\nroot@host01:/opt# \nping 10.85.0.254 >/dev/null 2>&1 &\n...\nWe send the output to \n/dev/null\n so that it doesn’t clutter up our session.\nNow, let’s use \ntcpdump\n to see the traffic:\nroot@host01:/opt# \ntimeout 1s tcpdump -i any -n icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on any, link-type LINUX_SLL (Linux cooked v1), ...\n17:37:33.204863 IP 10.85.0.1 > 10.85.0.254: ICMP echo request, ...\n17:37:33.204894 IP 10.85.0.1 > 10.85.0.254: ICMP echo request, ...\n17:37:33.204936 IP 10.85.0.254 > 10.85.0.1: ICMP echo reply, ...\n17:37:33.204936 IP 10.85.0.254 > 10.85.0.1: ICMP echo reply, ...\n4 packets captured\n4 packets received by filter\n0 packets dropped by kernel\nroot@host01:/opt# \nkillall ping\nWe use \ntimeout\n to prevent \ntcpdump\n from running indefinitely, and we also use\nkillall\n afterward to stop the \nping\n command and discontinue it running in the\nbackground.\nThe output shows that the \nping\n is originating from the bridge interface,\nwhich has IP address \n10.85.0.1\n. This is because of the host’s routing table:\nroot@host01:/opt# \nip route\n...\n10.85.0.0/16 dev cni0 proto kernel scope link src 10.85.0.1 \n192.168.61.0/24 dev enp0s8 proto kernel scope link src 192.168.61.11\nWhen CRI-O created the bridge and configured its IP address, it also set\nup a route so that all traffic destined for the \n10.85.0.0/16\n network (that is, all\ntraffic from \n10.85.0.0\n through \n10.85.255.255\n) would use \ncni0\n. This is enough\ninformation for the \nping\n command to know where to send its packet, and the\nbridge handles the rest.\nThe fact that the \nping\n is coming from the bridge interface of \n10.85.0.1\n rather\nthan the host interface of \n192.168.61.11\n actually makes a big difference, as we\ncan see if we try to run the \nping\n the other way around. Let’s try to \nping\n from\nwithin the namespace to the host network:\nroot@host01:/opt# \nip netns exec myns ping -c 1 192.168.61.11\nping: connect: Network is unreachable\nThe issue here is that the interface in our network namespace doesn’t\nknow how to reach the host network. The bridge is available and willing to\nroute traffic onto the host network, but we haven’t configured the necessary\nroute to use it. Let’s do that now:\nroot@host01:/opt# \nip netns exec myns ip route add default via 10.85.0.1\nAnd now the \nping\n works:\nroot@host01:/opt# \nip netns exec myns ping -c 1 192.168.61.11\nPING 192.168.61.11 (192.168.61.11) 56(84) bytes of data.\n64 bytes from 192.168.61.11: icmp_seq=1 ttl=64 time=0.097 ms\n--- 192.168.61.11 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.097/0.097/0.097/0.000 ms\nThis illustrates an important rule to remember when debugging network\nproblems: it’s very easy to jump to conclusions about what network traffic is\nreally being sent and received. There is often no substitute for using tracing\nto see what the traffic really looks like.\nIP ADDRESSES ON THE HOST\nThis approach is not the only one that results in connectivity from the\nhost into the network namespace. We also could have assigned an IP\naddress directly to the host side of the veth pair. However, even though\nthat would have enabled communication from the host into our network\nnamespace, it wouldn’t provide a way for multiple network namespaces\nto communicate with one another. Using a bridge interface, as CRI-O\ndoes, enables the interconnection of all of the containers on a host,\nmaking them all appear to be on the same network.\nThis also explains why we didn’t assign an IP address to the host side of\nthe veth pair. When working with bridges, only the bridge interface gets\nan IP address. Interfaces added to the bridge do not.\nWith that last change, it would seem like we’ve matched the network\nconfiguration of our containers, but we are still missing the ability to\ncommunicate with the broader network outside of \nhost01\n. We can demonstrate\nthis by trying to \nping\n from our network namespace to \nhost02\n, which is on the",10056
24-Masquerade.pdf,24-Masquerade,"same internal network as \nhost01\n and has the IP address \n192.168.61.12\n. If we try a\nping\n from our BusyBox container, it works:\nroot@host01:/opt# \ncrictl exec $B1C_ID ping -c 1 192.168.61.12\nPING 192.168.61.12 (192.168.61.12): 56 data bytes\n64 bytes from 192.168.61.12: seq=0 ttl=63 time=0.816 ms\n--- 192.168.61.12 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.816/0.816/0.816 ms\nThe \nping\n output reports that a packet was received. However, if we try the\nsame command using the network namespace we created, it doesn’t work:\nroot@host01:/opt# \nip netns exec myns ping -c 1 192.168.61.12\nPING 192.168.61.12 (192.168.61.12) 56(84) bytes of data.\n--- 192.168.61.12 ping statistics ---\n1 packets transmitted, 0 received, 100% packet loss, time 0ms\nThis command reports that no packets were received.\nReally, we ought to be surprised that the \nping\n from our BusyBox container\nworked. After all, \nhost02\n doesn’t know anything about the BusyBox container,\nor the \ncni0\n bridge interface, or the \n10.85.0.0/16\n network that the containers are in.\nHow is it possible for \nhost02\n to exchange a ping with our BusyBox container?\nTo understand that, we need to look at network masquerade.\nMasquerade\nMasquerade\n, also known as Network Address Translation (NAT), is used\nevery day in networking. For example, most home connections to the internet\nare provided with only a single IP address that is addressable from the\ninternet, but many devices within the home network need an internet\nconnection. It is the job of a router to make it appear that all traffic from that\nnetwork is originating from a single IP address. It does this by rewriting the\nsource\n IP address of outgoing traffic while tracking all outgoing connections\nso that it can rewrite the \ndestination\n IP address of any replies.\nNOTE\nThe kind of NAT that we are talking about here is technically known as\nSource NAT (SNAT). Don’t get hung up on the name, though; for it to\nwork correctly, any reply packets must have their destination rewritten.\nThe term Source in this case just means that the source address is what’s\nrewritten when a new connection is initiated.\nMasquerading sounds like just what we need to connect our containers\nrunning in the \n10.85.0.0/16\n network to the host network, \n192.168.61.0/24\n, and in\nfact it is exactly how it worked. When we sent a ping from our BusyBox\ncontainer, the source IP address was rewritten such that the ping appeared to\ncome from the \nhost01\n IP \n192.168.61.11\n. When \nhost02\n responded, it sent its reply to\n192.168.61.11\n, but the destination was rewritten so that it was actually sent to the\nBusyBox container.\nLet’s trace the \nping\n traffic all the way through to demonstrate:\nroot@host01:/opt# \ncrictl exec $B1C_ID ping 192.168.61.12 >/dev/null 2>&1 &\n[1] 6335\nroot@host01:/opt# \ntimeout 1s tcpdump -i any -n icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on any, link-type LINUX_SLL (Linux cooked v1)...\n18:53:44.310789 IP 10.85.0.4 \n➊\n > 192.168.61.12: ICMP echo request, id 12, seq 17...\n18:53:44.310789 IP 10.85.0.4 > 192.168.61.12: ICMP echo request, id 12, seq 17...\n18:53:44.310876 IP 192.168.61.11 \n➋\n > 192.168.61.12: ICMP echo request, id 12, seq 17...\n18:53:44.311619 IP 192.168.61.12 > 192.168.61.11: ICMP echo reply, \n➌\n id 12, seq 17...\n18:53:44.311648 IP 192.168.61.12 > 10.85.0.4: \n➍\n ICMP echo reply, id 12, seq 17...\n18:53:44.311656 IP 192.168.61.12 > 10.85.0.4: ICMP echo reply, id 12, seq 17...\n6 packets captured\n6 packets received by filter\n0 packets dropped by kernel\nroot@host01:/opt# \nkillall ping\nWhen the \nping\n originates from within our BusyBox container, it has a\nsource IP address of \n10.85.0.4\n \n➊\n. This address is rewritten, making the \nping\nappear to be coming from the host IP \n192.168.61.11\n \n➋\n. Of course, \nhost02\n knows\nhow to respond to a \nping\n coming from that address, so the \nping\n is answered \n➌\n.\nAt this point, the other half of the masquerade takes effect, and the\ndestination is rewritten to \n10.85.0.4\n \n➍\n. The result is that the BusyBox container\nis able to send a packet to a separate host and get a reply.\nTo complete the setup for our network namespace, we need a similar rule\nto masquerade traffic coming from \n10.85.0.254\n. We can start by using \niptables\n to\nlook at the rules that CRI-O created when it configured the containers:\nroot@host01:/opt# \niptables -t nat -n -L\n...\nChain POSTROUTING (policy ACCEPT)\ntarget                        prot opt source    destination ...\nCNI-f82910b3a7e28baf6aedc0d3  all  --  10.85.0.2 anywhere    ...\nCNI-7f8aa3d8a4f621b186149f43  all  --  10.85.0.3 anywhere    ...\nCNI-48ad69d30fe932fda9ea71d2  all  --  10.85.0.4 anywhere    ...\nChain CNI-48ad69d30fe932fda9ea71d2 (1 references)\ntarget     prot opt source               destination         \nACCEPT     all  --  anywhere             10.85.0.0/16 ...\nMASQUERADE all  --  anywhere             !224.0.0.0/4 ...\n...\nMasquerading starts when the connection is initiated; in this case, when\ntraffic has a source address in the \n10.85.0.0/16\n network. For this reason, the\nPOSTROUTING\n chain is used, because it sees all outgoing traffic. There is a rule\nin the \nPOSTROUTING\n chain for each container; each rule invokes a \nCNI\n chain for\nthat container.\nFor brevity, only one of the three \nCNI\n chains is shown. The other two are\nidentical. The \nCNI\n chain first does an \nACCEPT\n for all traffic that is local to the\ncontainer network, so this traffic won’t be masqueraded. It then sets up\nmasquerade for all traffic (except \n224.0.0.0/4\n, which is multicast traffic that\ncannot be masqueraded because there is no way to properly route replies).\nWhat’s missing from this configuration is a matching setup for traffic\nfrom \n10.85.0.254\n, the IP address we assigned to the interface in our network\nnamespace. Let’s add that. First, create a new chain in the \nnat\n table:\nroot@host01:/opt# \niptables -t nat -N chain-myns\nNext, add a rule to accept all traffic for the local network:\nroot@host01:/opt# \niptables -t nat -A chain-myns -d 10.85.0.0/16 -j ACCEPT\nNow all remaining traffic (except multicast) should be masqueraded:\nroot@host01:/opt# \niptables -t nat -A chain-myns \\n                  \n! -d 224.0.0.0/4 -j MASQUERADE\nAnd finally, tell \niptables\n to use this chain for any traffic coming from",6555
25-Final Thoughts.pdf,25-Final Thoughts,"10.85.0.254\n:\nroot@host01:/opt# \niptables -t nat -A POSTROUTING -s 10.85.0.254 -j chain-myns\nWe can verify that we did all that correctly by listing the rules again:\nroot@host01:/opt# \niptables -t nat -n -L\n...\nChain POSTROUTING (policy ACCEPT)\ntarget      prot opt source               destination\nchain-myns  all  --  10.85.0.254          anywhere            \n...\nChain chain-myns (1 references)\ntarget      prot opt source               destination         \nACCEPT      all  --  anywhere             10.85.0.0/16        \nMASQUERADE  all  --  anywhere             !224.0.0.0/4\nIt looks like we have the configuration we need, as this configuration\nmatches the way the virtual network devices were configured for the\nBusyBox container. To make sure, let’s try a \nping\n to \nhost02\n again:\nroot@host01:/opt# \nip netns exec myns ping -c 1 192.168.61.12\nPING 192.168.61.12 (192.168.61.12) 56(84) bytes of data.\n64 bytes from 192.168.61.12: icmp_seq=1 ttl=63 time=0.843 ms\n--- 192.168.61.12 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.843/0.843/0.843/0.000 ms\nSuccess! We’ve fully replicated the network isolation and connectivity\nthat CRI-O is providing our containers.\nFinal Thoughts\nContainer networking looks deceptively simple when running containers.\nEach container is provided with its own set of network devices, avoiding the\nneed to worry about port conflicts and reducing the effect that one container\ncan have on another. However, as we’ve seen in this chapter, this “simple”\nnetwork isolation requires some complex configuration to enable not just\nisolation, but also connectivity to other containers and other networks. In \nPart\nII\n, after we properly introduce Kubernetes, we’ll return to container\nnetworking and show how the complexity only increases when we need to\nconnect containers running on different hosts and load balance traffic across\nmultiple container instances.\nFor now, we have one more key topic to address with containers before\nwe can move on to Kubernetes. We need to understand how container storage\nworks, including the container image that is used as the base filesystem when\na new container is started as well as the temporary storage that a running\ncontainer uses. In the next chapter, we’ll investigate how container storage\nmakes application deployment easier and how a layered filesystem is used to\nsave on storage and improve efficiency.",2488
26-5 CONTAINER IMAGES AND RUNTIME LAYERS.pdf,26-5 CONTAINER IMAGES AND RUNTIME LAYERS,,0
27-Filesystem Isolation.pdf,27-Filesystem Isolation,"5\nCONTAINER IMAGES AND RUNTIME LAYERS\nTo run a process, we need storage. One of the great advantages of\ncontainerized software is the ability to bundle an application for delivery\ntogether with its dependencies. As a result, we need to store the executable\nfor the program and any shared libraries it uses. We also need to store\nconfiguration files, logs, and any data managed by the program. All of this\nstorage must be isolated so that a container can’t interfere with the host\nsystem or with other containers. Altogether, this represents a large need for\nstorage, and it means container engines must provide some unique features to\nbe efficient in the use of disk space and bandwidth. In this chapter, we’ll\nexplore how the use of a layered filesystem makes container images efficient\nto download and containers efficient to start.\nFilesystem Isolation\nIn \nChapter 2\n, we saw how we could use a \nchroot\n environment to create a\nseparate, isolated part of the filesystem that contained only the binaries and\nlibraries we needed to run a process. Even to run a simple \nls\n command, we\nneeded the binary and several libraries. A more fully featured container, such\nas one running the NGINX web server, needs quite a bit more—a complete\nset of files for a Linux distribution.\nIn the chroot example, we built the isolated filesystem from the host\nsystem when we were ready to use it. That approach would be impractical for\ncontainers. Instead, the isolated filesystem is packaged in a \ncontainer image\n,\nwhich is a ready-to-use bundle that includes all files and metadata, such as\nenvironment variables and the default executable.\nContainer Image Contents\nLet’s take a quick look inside an NGINX container image. For this chapter,\nwe’ll be running commands using Docker because it’s still the most common\ntool for building container images.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nRun the following command on \nhost01\n from this chapter’s examples to\ndownload the image:\nroot@host01:~# \ndocker pull nginx\nUsing default tag: latest\nlatest: Pulling from library/nginx\n...\nStatus: Downloaded newer image for nginx:latest\ndocker.io/library/nginx:latest\nThe \ndocker pull\n command downloads an image from an \nimage registry\n. An\nimage registry is a web server that implements an API for downloading and\npublishing container images. We can see the image we’ve downloaded by\nlisting images with \ndocker images\n:\nroot@host01:~# \ndocker images\nREPOSITORY   TAG       IMAGE ID       CREATED       SIZE\nnginx        latest    f0b8a9a54136   7 days ago    133MB\nThis image is 133MB and has a unique identifier of \nf0b8a9a54136\n. (Your\nidentifier will be different, as new NGINX container images are built every\nday.) This image includes not only the NGINX executables and required\nlibraries but also a Linux distribution based on Debian. We saw this briefly \nin\nChapter 1\n when we demonstrated a Rocky Linux container on an Ubuntu host\nand kernel, but let’s look at it in a little more detail. Start by running an\nNGINX container:\nroot@host01:~# \ndocker run --name nginx -d nginx\n516d13e912a55cfc6f73f0dd473661d6b7d3b868d5a07a2bc7253971015b6799\nThe \n--name\n flag gives the container a friendly name that we can use for\nfuture commands, whereas the \n-d\n flag sends it to the background.\nNow, let’s explore the filesystem of our running container:\nroot@host01:~# \ndocker exec -ti nginx /bin/bash\nroot@516d13e912a5:/#\nFrom here, we can see the various libraries needed for NGINX to work:\nroot@516d13e912a5:/# \nldd $(which nginx)\n        linux-vdso.so.1 (0x00007ffe2a1fa000)\n...\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fe0d6531000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007fe0d6ed4000)\nAll of these libraries are part of the container image we downloaded, so\nour NGINX container does not need (and cannot see) any files from the host\nsystem.\nNot only do we have a healthy number of libraries present, but we have\ntypical configuration files in \n/etc\n that we would expect for a Debian system:\nroot@516d13e912a5:/# \nls -1 /etc\n...\ndebian_version\ndeluser.conf\ndpkg\n...\nsystemd/\n...\nThis listing shows that the filesystem even includes directories that aren’t\nreally needed for a container, like the \n/etc/systemd\n directory. (Remember, a\ncontainer is just a set of related processes run under isolation, so a container\nalmost never runs a system service manager like systemd.) This full\nfilesystem is included for a couple reasons. First, many processes were\nwritten to expect the usual set of files to be present. Second, it’s just easier to\nbuild container images starting from a typical Linux distribution.\nThe separate filesystem for our container is writable as well. While we\nhave this shell open, let’s send some random data to a file in the container so\nthat we can inspect that storage from the host. We can then exit the shell:\nroot@516d13e912a5:/# \ndd if=/dev/urandom of=/tmp/data bs=1M count=10\n...\n10485760 bytes (10 MB, 10 MiB) copied, 0.0913977 s, 115 MB/s\nroot@516d13e912a5:/# \nexit\nThe \ndd\n command wrote a 10MB file into the \n/tmp\n directory. Even though\nwe exited the shell, the container is still running, so we can use \ndocker inspect\n to\nsee the amount of disk space this container is using:\nroot@host01:~# \ndocker inspect -s nginx | jq '.[0].SizeRw'\n10487109\nThe \n-s\n flag tells \ndocker inspect\n to report the size of the container. Because\ndocker inspect\n produces a huge JSON output, we use the JSON query tool \njq\n to\nchoose the field we want.\nThe reported size is just about 10MB, suggesting that the container is\nconsuming only the amount of read-write storage required for the file we\nwrote, plus any files written by NGINX. We’ll explore this in more detail as\nwe continue in this chapter.\nImage Versions and Layers\nThe ability to quickly download a prepackaged filesystem to run a process is\nonly one of the advantages of container images. Another is the ability to tag\ndifferent versions of an image to allow for rapid upgrading. Let’s explore this\nby pulling and running two different versions of Redis, the popular in-\nmemory key–value database:\n   root@host01:~# \ndocker pull redis:6.0.13-alpine\n   6.0.13-alpine: Pulling from library/redis\n➊\n 540db60ca938: Pull complete \n   29712d301e8c: Pull complete \n   8173c12df40f: Pull complete \n   ...\n   docker.io/library/redis:6.0.13-alpine\n   root@host01:~# \ndocker pull redis:6.2.3-alpine\n   6.2.3-alpine: Pulling from library/redis\n➋\n 540db60ca938: Already exists \n   29712d301e8c: Already exists \n   8173c12df40f: Already exists \n   ...\n   docker.io/library/redis:6.2.3-alpine\nThe data after the colon is the \nimage tag\n and acts as a version identifier.\nPreviously, when we left this off, Docker defaulted to \nlatest\n, which is a tag\nlike any other, but it is used by convention to refer to the latest published\nimage. By specifying the version, we can ensure that even as newer versions\nof Redis are released, we will continue to run the same version until we are\nready to upgrade. The tag can contain any characters, and it is common to\nadd extra information after a hyphen. In this case, the \n-alpine\n at the end of the\ntag indicates that this image is based on Alpine Linux, a lightweight \nLinux\ndistribution that is popular for making container images because of its small\nsize.\nOne other interesting item of note is the fact that when we downloaded the\nsecond version of Redis, some of the content \n➋\n was flagged as \nAlready exists\n.\nLooking at the first Redis download, we see the same unique identifiers are\npresent there \n➊\n. This is because a container image is made up of layers, and\nthese identifiers uniquely describe a layer. If a layer we’ve already\ndownloaded is used by another image, we don’t need to download it again,\nsaving download time. Additionally, each layer needs to be stored only once\non disk, saving disk space.\nWe now have two different versions of Redis downloaded:\nroot@host01:~# \ndocker images | grep redis\nredis        6.0.13-alpine   a556c77d3dce   2 weeks ago   31.3MB\nredis        6.2.3-alpine    efb4fa30f1cf   2 weeks ago   32.3MB\nAlthough Docker is reporting that each image has a size of about 30MB,\nthat is the total size of all the layers and doesn’t account for the storage\nsavings that come from shared layers. The actual storage on disk is less, as\nwe can see by examining Docker’s use of disk space:\nroot@host01:~# \ndocker system df -v\nImages space usage:\nREPOSITORY TAG           ... SIZE      SHARED SIZE   UNIQUE SIZE ...",8818
28-Building Container Images.pdf,28-Building Container Images,"redis      6.0.13-alpine ... 31.33MB   6.905MB       24.42MB     ...\nredis      6.2.3-alpine  ... 32.31MB   6.905MB       25.4MB      ...\nThe two Redis images are sharing almost 7MB of base layers.\nThese two versions of Redis can be run separately:\nroot@host01:~# \ndocker run -d --name redis1 redis:6.0.13-alpine\n66dbf56ec0e8db24ca78afc07c68b7d0699d68b4749e0c03310857cfce926366\nroot@host01:~# \ndocker run -d --name redis2 redis:6.2.3-alpine\n9dd3f86a1284171e5ca60f7f8a6a13dc517237826a92b3cb256f5ac64a5f5c31\nNow that both images are running, we can confirm that our containers\nhave exactly the version of Redis we want, independent of what version\nmight be the latest release and independent of the versions available for our\nhost server:\nroot@host01:~# \ndocker logs redis1 | grep version\n1:C 21 May 2021 14:18:24.952 # Redis version=6.0.13, ...\nroot@host01:~# \ndocker logs redis2 | grep version\n1:C 21 May 2021 14:18:36.387 # Redis version=6.2.3, ...\nThis is a big advantage for building reliable systems. We can test our\napplication thoroughly with one version of the software and be sure that\nversion will continue to be used until we choose to upgrade. We can also\neasily test our software against a new version without having to upgrade a\nhost system.\nBuilding Container Images\nIn the preceding example, we saw how we could reduce the download and\ndisk requirements for container images by sharing layers. This layer sharing\ncan be used with any container image, not just two different versions of the\nsame software.\nThe layers in a container image come from the way it is built. A container\nimage build starts with a \nbase image\n. For example, both of our two Redis\nversions started with the same exact Alpine Linux base image, which is why\nthose layers were shared in that image. Starting from the base image, each\nstep in the build process can produce a new layer. This new layer contains\nonly the changes to the filesystem that came from that build step.\nA base image must also come from somewhere, and, ultimately, there\nmust be an initial layer, which is typically a minimal Linux filesystem created\nfrom some Linux distribution, transferred into an empty container image, and\nthen expanded to become an initial layer.\nUsing a Dockerfile\nThere are many different ways to build container images, but the most\npopular is to create a file known as a \nDockerfile\n or \nContainerfile\n that\nspecifies the commands and configuration for the image. Here’s a simple\nDockerfile\n that adds web content to an NGINX image:\nDockerfile\n---\nFROM nginx\n# Add index.html\nRUN echo ""<html><body><h1>Hello World!</h1></body></html>"" \\n    >/usr/share/nginx/html/index.html\nEach line in a \nDockerfile\n starts with a command that is followed by\nparameters. Blank lines and content after a \n#\n are ignored, and a backslash at\nthe end of a line continues that command onto the next line. There are many\npossible commands; here are the most common:\nFROM\n Specify the base image for this build.\nRUN\n Run a command inside the container.\nCOPY\n Copy files into the container.\nENV\n Specify an environment variable.\nENTRYPOINT\n Configure the initial process for the container.\nCMD\n Set default parameters for the initial process.\nDocker provides the \ndocker build\n command to build an image from a\nDockerfile\n. The \ndocker build\n command creates a new image by running each\ncommand in the \nDockerfile\n, one at a time. \nListing 5-1\n illustrates how to run\ndocker build\n.\n   root@host01:~# \ncd /opt/hello\n   root@host01:/opt/hello# \ndocker build -t hello .\n➊\n Sending build context to Docker daemon  2.048kB\n   Step 1/2 : FROM nginx\n \n➋\n ---> f0b8a9a54136\n   Step 2/2 : RUN echo ""<html><body><h1>Hello World!</h1></body></html>"" ...\n \n➌\n ---> Running in 77ba9163d0a5\n   Removing intermediate container 77ba9163d0a5\n    ---> e9ca31d590f9\n   Successfully built e9ca31d590f9\n➍\n Successfully tagged hello:latest\nListing 5-1: Docker build\nThe \n-t\n switch tells \ndocker build\n to store the image from the build process\nunder the name \nhello\n.\nExamining the steps in this build process will help clarify how container\nimages are made. First, Docker sends the \nbuild context\n to the Docker daemon\n➊\n. The build context is a directory and all of its files and subdirectories. In\nthis case, we specified the build context as the current directory when we\nadded \n.\n to the end of the \ndocker build\n command. The actual container image\nbuild happens inside the daemon, so the only files that would be available for\na \nCOPY\n command are those that are in the build context.\nSecond, Docker identifies our base image, in this case \nnginx\n. The unique\nidentifier it displays \n➋\n matches the one displayed earlier for our NGINX\nimage when we ran \ndocker images\n. Third, Docker executes the command we\nspecified in the \nRUN\n step. This command is actually run inside a container\nbased on our NGINX base image \n➌\n, which means that only the commands\ninstalled in the container image are available to run. If we need other\ncommands to be available, we might need to create a \nRUN\n step that installs\nthem before we can use them.\nAfter all of the build steps are complete, Docker “tags” the new container\nimage with the name we provided using the \n-t\n flag. As before, we didn’t\nspecify a version, so \nlatest\n is used as a default. We now can see this image in\nthe list of available images:\nroot@host01:/opt/hello# \ndocker images | grep hello\nhello        latest          e9ca31d590f9   9 minutes ago   133MB\nThe unique identifier for this image matches the output from the end of\nListing 5-1\n. This image is shown as 133MB because it has all of the layers\nfrom the NGINX image in addition to the new small HTML file we added.\nAs before, the shared layers are stored only once, so the extra storage\nrequired to build this image was very small.\nNOTE\nWhen you try this example yourself, the unique identifier displayed for\nyour “hello” image will be different, even though the Dockerfile has the\nsame content for the HTML file. The identifier for each layer is based not\nonly on the layer’s file content but also on the identifier for the layer\nabove it. As a result, if two images have the same identifier, we can be\nconfident that the contents are exactly the same, even if they were built\nseparately.\nWe can run a container based on this new image just as we would any\nother image:\nroot@host01:/opt/hello# \ndocker run -d -p 8080:80 hello\n83a23cf2921bb37474bfcefb0da45f9953940febfefd01ebadf35405d88c4396\nroot@host01:/opt/hello# \ncurl http://localhost:8080/\n<html><body><h1>Hello World!</h1></body></html>\nAs described in \nChapter 1\n, the \n-p\n flag forwards a host port into the\ncontainer, enabling us to access the NGINX server from the host even though\nit is running in a separate network namespace. We then can use \ncurl\n to see that\nour container has the content we provided.\nTagging and Publishing Images\nThe image is ready to run locally, but we’re not ready yet to publish it to a\nregistry. To publish to a registry, we need to give it a name that includes the\nfull host and path for the registry location to ensure that when we refer to an\nimage, we are getting exactly what we expect.\nTo demonstrate, let’s pull multiple BusyBox images from different\nregistries. We’ll start with a BusyBox image from \nquay.io\n, an alternative\ncontainer image registry:\nroot@host01:/opt/hello# \ndocker pull quay.io/quay/busybox\n...\nquay.io/quay/busybox:latest\nThis image name specifies both the host \nquay.io\n and the location of the\nimage within that host, \nquay/busybox\n. As before, because we didn’t specify a\nversion, \nlatest\n is used as a default. We are able to pull a version called \nlatest\nbecause someone has explicitly published a \nlatest\n version of the image to this\nregistry.\nThe BusyBox image we get using this command is different from the one\nwe get if we just pull \nbusybox\n:\nroot@host01:/opt/hello# \ndocker pull busybox\n...\ndocker.io/library/busybox:latest\nroot@host01:/opt/hello# \ndocker images | grep busybox\nbusybox                latest          d3cd072556c2   3 days ago       1.24MB\nquay.io/quay/busybox   latest          e3121c769e39   8 months ago     1.22MB\nWhen we use the plain name \nbusybox\n, Docker defaults to pulling the image\nfrom \ndocker.io/library\n. This registry is known as \nDocker Hub\n, which you can\nbrowse at \nhttps://hub.docker.com\n.\nSimilarly, when we used the plain name \nhello\n to build our image, Docker\nsees it as belonging to \ndocker.io/library\n. That path is for official Docker images,\nand, of course, we don’t have the right to publish images there.\nThe automated setup for this chapter includes running a local container\nregistry, which means that we can publish this image to that local registry if\nwe name it correctly:\nroot@host01:/opt/hello# \ndocker tag hello registry.local/hello\nroot@host01:/opt/hello# \ndocker images | grep hello\nhello                  latest          e9ca31d590f9   52 minutes ago   133MB\nregistry.local/hello   latest          e9ca31d590f9   52 minutes ago   133MB\nThe same image now exists under two different names, providing an extra\nadvantage of the way images are stored by layer. It’s cheap to add an extra\nname for an image. Of course, we could also have used the full name in the\nfirst place when we ran \ndocker build\n, but it is convenient to use shorter names\nwhen building and using images locally.\nNow that we have named the image correctly, we can publish it using\ndocker push\n:\nroot@host01:/opt/hello# \ndocker push registry.local/hello\nUsing default tag: latest",9775
29-Image and Container Storage.pdf,29-Image and Container Storage,"The push refers to repository [registry.local/hello]\n...\nOur local registry starts out empty, so this command uploads all of the\nlayers, but if we push any future images that include some of the same layers,\nthey won’t be uploaded again. Similarly, if we were to delete an image tag\nfrom the registry, that would not remove the layer data.\nThis ability to publish images is not limited to images that we build\nourselves. We can tag and push the BusyBox image we just downloaded from\nDocker Hub:\nroot@host01:/opt/hello# \ndocker tag busybox registry.local/busybox\nroot@host01:/opt/hello# \ndocker push registry.local/busybox\nUsing default tag: latest\nThe push refers to repository [registry.local/busybox]\n...\nroot@host01:/opt/hello# \ncd\nRetagging an image so that we can upload it to a private registry is a\ncommon practice that can help an application start faster and avoid being\ndependent on an internet registry.\nThe last command (\ncd\n) takes us back to our home directory, given that\nwe’re finished in \n/opt/hello\n.\nImage and Container Storage\nAs mentioned previously, using individual layers to build up a container\nimage has multiple advantages, including reduced download size, reduced\ndisk space, and the ability to re-tag an image with a new name without using\nany additional space. The additional disk space needed by a running\ncontainer is limited to just the files that we write while the container is\nrunning. Finally, all of the examples have shown how fast a new container\nstarts up. All of these features together demonstrate why layers must be\nshared, not only for images but also for new containers. To make the best use\nof this layered approach in building efficient images, it helps to understand\nhow this layered filesystem works.\nOverlay Filesystems\nWhen we run a container, we are presented with what looks like a single\nfilesystem, with all the layers merged together and with the ability to make\nchanges to any file. If we run multiple containers from the same image, we\nsee an independent filesystem in each one, so that changes in one do not\naffect the other. How does this work without having to copy the entire\nfilesystem every time we start a container? The answer is an \noverlay\nfilesystem\n.\nAn overlay filesystem has three main parts. The \nlower directory\n is where\nthe “base” layer exists. (There may be multiple lower directories.) The \nupper\ndirectory has the “overlay” layer, and the \nmount\n directory is where the\nunified filesystem is made available for use. A directory listing in the mount\ndirectory reflects all of the files from all of the layers, in priority order. Any\nchanges made to the mount directory are really written to the upper directory\nby copying the changed file to the upper directory from a lower one, and then\nupdating it—a process known as \ncopy on write\n. Deletions are also written to\nthe upper directory as metadata, so the lower directory can remain\nunmodified. This means that multiple users can share the lower directory\nwithout conflict because it is only read from, never written to.\nAn overlay filesystem is useful for more than just container images and\ncontainers. It is also useful for embedded systems, such as a network router,\nfor which a read-only filesystem is written in firmware, making it possible for\nthe device to be safely rebooted to a known state every time. It is also useful\nfor virtual machines, enabling multiple virtual machines to be started from\nthe same image.\nOverlay filesystems are provided by a Linux kernel module, enabling very\nhigh performance. We can easily create an overlay filesystem. The first step\nis to create the necessary directories:\nroot@host01:~# \nmkdir /tmp/{lower,upper,work,mount}\nThe \nmkdir\n command creates four separate directories in \n/tmp\n. We’ve\nalready discussed the \nlower\n directory, \nupper\n directory, and \nmount\n directory.\nThe \nwork\n directory is an extra empty directory that the overlay filesystem\nuses as temporary space to ensure that changes in the mount directory appear\natomic—that is, to ensure that they appear all at once.\nLet’s put some content into the lower and upper directories:\nroot@host01:~# \necho ""hello1"" > /tmp/lower/hello1\nroot@host01:~# \necho ""hello2"" > /tmp/upper/hello2\nNext, we just mount the overlay filesystem:\nroot@host01:~# \nmount -t overlay \\n  \n-o rw,lowerdir=/tmp/lower,upperdir=/tmp/upper,workdir=/tmp/work \\n  \noverlay /tmp/mount\nThe \n/tmp/mount\n directory now contains the merged content of both the\nupper and lower directories:\nroot@host01:~# \nls -l /tmp/mount\ntotal 8\n-rw-r--r-- 1 root root 7 May 24 23:05 hello1\n-rw-r--r-- 1 root root 7 May 24 23:05 hello2\nroot@host01:/opt/hello# \ncat /tmp/mount/hello1\nhello1\nroot@host01:/opt/hello# \ncat /tmp/mount/hello2\nhello2\nAny changes that we make are shown in the mount location but are\nactually made in the upper directory:\nroot@host01:~# \necho ""hello3"" > /tmp/mount/hello3\nroot@host01:~# \nls -l /tmp/mount\ntotal 8\n-rw-r--r-- 1 root root 7 May 24 23:05 hello1\n-rw-r--r-- 1 root root 7 May 24 23:10 hello2\n-rw-r--r-- 1 root root 7 May 24 23:09 hello3\nroot@host01:~# \nls -l /tmp/lower\ntotal 4\n-rw-r--r-- 1 root root 7 May 24 23:05 hello1\nroot@host01:~# \nls -l /tmp/upper\ntotal 8\n-rw-r--r-- 1 root root 7 May 24 23:10 hello2\n-rw-r--r-- 1 root root 7 May 24 23:09 hello3\nAdditionally, even deleting files does not affect the lower directory:\n   root@host01:~# \nrm /tmp/mount/hello1\n   root@host01:~# \nls -l /tmp/mount\n   total 8\n   -rw-r--r-- 1 root root 7 May 24 23:10 hello2\n   -rw-r--r-- 1 root root 7 May 24 23:09 hello3\n   root@host01:~# \nls -l /tmp/lower\n   total 4\n   \n-rw-r--r-- 1 root root 7 May 24 23:05 hello1\n   root@host01:~# \nls -l /tmp/upper\n   total 8\n➊\n c--------- 1 root root 0, 0 May 24 23:11 hello1\n   -rw-r--r-- 1 root root    7 May 24 23:10 hello2\n   -rw-r--r-- 1 root root    7 May 24 23:09 hello3\nThe \nc\n next to the listing for \nhello1\n in the upper directory \n➊\n indicates that\nthis is a \ncharacter special file\n. Its purpose is to indicate that this file was\ndeleted in the upper directory. As a result, it does not show up in the mounted\nfilesystem, even though it still exists in the lower directory.\nThanks to this approach, we can reuse the lower directory with an\nindependent overlay, similar to how we can run multiple independent\ncontainers from the same image:\nroot@host01:~# \nmkdir /tmp/{upper2,work2,mount2}\nroot@host01:~# \nmount -t overlay \\n  \n-o rw,lowerdir=/tmp/lower,upperdir=/tmp/upper2,workdir=/tmp/work2 \\n  \noverlay /tmp/mount2\nroot@host01:~# \nls -l /tmp/mount2\ntotal 4\n-rw-r--r-- 1 root root 7 May 24 23:05 hello1\nNot only does the “deleted” file from the lower directory appear, but none\nof the content from the first upper directory shows up because it’s not part of\nthis new overlay.\nUnderstanding Container Layers\nArmed with this information about overlay filesystems, we can explore the\nfilesystem of our running NGINX container:\nroot@host01:~# \nROOT=$(docker inspect nginx \\n  \n| jq -r '.[0].GraphDriver.Data.MergedDir')\nroot@host01:~# \necho $ROOT\n/var/lib/docker/overlay2/433751e2378f9b11.../merged\nAs before, we use \njq\n to choose just the field we want; in this case, it’s the\npath to the \nmerged\n directory for the container’s filesystem. This merged\ndirectory is the mount point for an overlay filesystem:\nroot@host01:~# \nmount | grep $ROOT | tr [:,] '\n'\noverlay on /var/lib/docker/overlay2/433751e2378f9b11.../merged ...\nlowerdir=/var/lib/docker/overlay2/l/ERVEI5TCULK4PCNO2HSWB4MFDB\n/var/lib/docker/overlay2/l/RQDO2PYQ3OKMKDY3DAYPAJTZHF\n/var/lib/docker/overlay2/l/LFSBVPYPODQJXDL5WQTI7ISYNC\n/var/lib/docker/overlay2/l/TLZUYV2BFQNPFGU3AZFUHOH27V\n/var/lib/docker/overlay2/l/4M66FKSHDBNUWE7UAF2REQHSB2\n/var/lib/docker/overlay2/l/LCTKPRHP6LG7KC7JQHETKIL6TZ\n/var/lib/docker/overlay2/l/JOECSCSAQ5CPNHGEURVRT4JRQQ\nupperdir=/var/lib/docker/overlay2/433751e2378f9b11.../diff\nworkdir=/var/lib/docker/overlay2/433751e2378f9b11.../work,xino=off)\nThe \ntr\n command transforms colons and commas to newlines to make the\noutput more readable.\nThe \nmount\n command shows seven separate entries for \nlowerdir\n, one for each\nof the layers in the NGINX container image. All seven of these directories,\nplus the \nupperdir\n, are merged together in the overlay filesystem.\nWe can see the 10MB data file we created earlier in both the mount\ndirectory and the upper directory:\nroot@host01:~# \nls -l $ROOT/tmp/data\n-rw-r--r-- 1 root root 10485760 May 25 00:27 /var/lib/.../merged/tmp/data\nroot@host01:~# \nls -l $ROOT/../diff/tmp/data\n-rw-r--r-- 1 root root 10485760 May 25 00:27 /var/lib/.../diff/tmp/data\nThe actual file is stored in the upper directory \ndiff\n, whereas the mount\ndirectory \nmerged\n is just a view generated by the overlay filesystem.\nUsually, we don’t need to delve into the container filesystem from the\nhost, because we can just run commands from within the container to explore\nits files. However, this technique can be useful for pulling files from a\ncontainer for cases in which the container engine is not behaving correctly.\nPractical Image Building Advice\nSome important practical implications result from the way that overlay\nfilesystems are used with container images. First, because an overlay\nfilesystem can have multiple lower directories, and merging is performant,\nbreaking our container image into multiple layers causes very little\nperformance penalty. It allows us to be very modular when building container\nimages, enabling reuse of layers. For example, we might start with a base\nimage and then build an image on top that installs some common\ndependencies, and then another image that adds specialized dependencies for",9851
30-Open Container Initiative.pdf,30-Open Container Initiative,"some of our application components, and finally yet another image that adds\na specific application. Assembling application container images using a\nlayered approach can result in very efficient image transfer and storage, as\nthe base layers are shared between components where possible.\nSecond, because a deletion in an upper layer does not actually remove the\nfile from a lower layer, we need to be careful with how we handle large\ntemporary files and also in how we store secrets while building images. In\nboth cases, if we finish a layer while the file is still present, it will be there\nforever, causing us to waste bandwidth and space, or worse, leak secret\ninformation to anyone who downloads the image. In general, you should\nassume that every line of a \nDockerfile\n makes a new layer, and you should\nalso make \nthe assumption that all of the information associated with each\ncommand is stored in the image metadata. As a result:\nPerform multiple steps in a single \nRUN\n line, and make sure every \nRUN\ncommand cleans up after itself.\nDon’t use \nCOPY\n to transfer large files or secrets into the image, even if\nyou clean them up in a later \nRUN\n step.\nDon’t use \nENV\n to store secrets, because the resulting values become part\nof the image metadata.\nOpen Container Initiative\nA container image is more than just the set of layers that make up the overlay\nfilesystem. It also includes important metadata, such as the initial command\nfor the container and any environment variables for that command. The Open\nContainer Initiative (OCI) provides a standard format for storing image\ninformation. It ensures that container images built by one tool can be used by\nany other tool and provides a standard way to transfer images layer by layer\nor in a complete package.\nTo demonstrate the OCI format, let’s extract a BusyBox container image\nfrom Docker and store it in OCI format using Skopeo, a program designed to\nmove container images around between repositories and formats. The first\nstep is to extract the image:\nroot@host01:~# \nskopeo copy docker-daemon:busybox:latest oci:busybox:latest\n...\nThis command tells Skopeo to fetch the image from the Docker engine’s\nstorage and write it out in OCI format. We now have a \nbusybox\n directory that\ncontains the image:\nroot@host01:~# \nls -l busybox\ntotal 12\ndrwxr-xr-x 3 root root 4096 May 24 23:59 blobs\n-rw-r--r-- 1 root root  247 May 24 23:59 index.json\n-rw-r--r-- 1 root root   31 May 24 23:59 oci-layout\nThe \noci-layout\n file specifies the OCI version used for this image:\nroot@host01:~# \njq . busybox/oci-layout\n{\n  ""imageLayoutVersion"": ""1.0.0""\n}\nThe \nindex.json\n file tells us about the image:\nroot@host01:~# \njq . busybox/index.json\n{\n  ""schemaVersion"": 2,\n  ""manifests"": [\n    {\n      ""mediaType"": ""application/vnd.oci.image.manifest.v1+json"",\n      ""digest"": ""sha256:9c3c5aeeaa7e1629871808339..."",\n      ""size"": 347,\n      ""annotations"": {\n        ""org.opencontainers.image.ref.name"": ""latest""\n      }\n    }\n  ]\n}\nThe \nmanifests\n property is an array that allows us to store multiple images in\na single OCI directory or package. The actual filesystem content is stored by\nlayer in the \nblobs\n directory, with each layer as a separate \n.tar\n file, so any\nshared layers are stored only once.\nThis BusyBox image has only a single layer. To look at its contents, we’ll\nneed to work through the \nindex.json\n and image manifest to find the path to its\n.tar\n file:",3512
31-Final Thoughts.pdf,31-Final Thoughts,"root@host01:~# \nMANIFEST=$(jq -r \\n  \n.manifests[0].digest busybox/index.json | sed -e 's/sha256://')\nroot@host01:~# \nLAYER=$(jq -r \\n  \n.layers[0].digest busybox/blobs/sha256/$MANIFEST | sed -e 's/sha256://')\nroot@host01:~# \necho $LAYER\n197dfd3345530fd558a64f2a550e8af75a9cb812df5623daf0392aa39e0ce767\nThe files in the \nblobs\n directory are named using the SHA-256 digest\ncalculated from the file contents. We start by using \njq\n to get the digest for the\nBusyBox image’s manifest, stripping off the \nsha256:\n part at the front to get the\nname of the manifest file. We then read the manifest to find the first (and\nonly) layer. We now can see the content of this layer:\nroot@host01:~# \ntar tvf busybox/blobs/sha256/$LAYER\ndrwxr-xr-x 0/0               0 2021-05-17 19:07 bin/\n-rwxr-xr-x 0/0         1149184 2021-05-17 19:07 bin/[\nhrwxr-xr-x 0/0               0 2021-05-17 19:07 bin/[[ link to bin/[\n...\ndrwxr-xr-x 0/0               0 2021-05-17 19:07 dev/\ndrwxr-xr-x 0/0               0 2021-05-17 19:07 etc/\n...\nPassing \ntvf\n to the \ntar\n command tells it to list a table of contents from the file\nwe specify, which is the BusyBox image layer in this case. This layer\ncontains a complete Linux filesystem, with BusyBox acting as the single\nexecutable for most of the standard Linux commands.\nUsing this \nbusybox\n directory, we can also package up the container image,\nmove it to a separate system, and then pull it into another container engine.\nFinal Thoughts\nWhen we run a container, we get what appears to be a separate, isolated\nfilesystem that we can modify as desired. Underneath, the container engine is\nusing the overlay filesystem to merge together multiple container image\nlayers and a writeable directory that stores all the changes we make. Not only\ndoes the use of an overlay filesystem make a new container fast to start, but it\nalso means that we can run multiple containers from the same image without\nwaiting for file copy to complete, and we can reduce the required disk space\nby sharing image layers.\nNow that we’ve looked at process isolation, resource limits, network\nisolation, and container storage, we’ve covered the main features of\ncontainers that make them so valuable for packaging, distributing, updating,\nand running application components. It’s time to move on to the critical\nfeatures that we can get only from a container orchestration environment like\nKubernetes. We’ll do that in \nPart II\n.",2485
32-6 WHY KUBERNETES MATTERS.pdf,32-6 WHY KUBERNETES MATTERS,"PART II\nCONTAINERS IN KUBERNETES\nComputers have finite processing, storage, and memory, and are built of parts\nthat fail, especially at the wrong time. To build a scalable, reliable\napplication, we can’t be limited by the resources of a single host or dependent\non a single point of failure. At the same time, we don’t want to give up the\nmodularity and flexibility that containers provide. In \nPart II\n, we’ll see how\nKubernetes meets the essential requirements to run containers across a cluster\nof machines, with cross-host container networking, scalability, automated\nfailover, and distributed storage.",617
33-Running Containers in a Cluster.pdf,33-Running Containers in a Cluster,"6\nWHY KUBERNETES MATTERS\nContainers enable us to transform the way we package and deploy application\ncomponents, but orchestration of containers in a cluster enables the real\nadvantage of a containerized microservice architecture. As described in\nChapter 1\n, the main benefits of modern application architecture are\nscalability, reliability, and resiliency, and all three of those benefits require a\ncontainer orchestration environment like Kubernetes in order to run many\ninstances of containerized application components across many different\nservers and networks.\nIn this chapter, we’ll begin by looking at some cross-cutting concerns that\nexist when running containers across multiple servers in a cluster. We’ll then\ndescribe the core Kubernetes concepts designed to address those concerns.\nWith that introduction complete, we’ll spend the bulk of the chapter actually\ninstalling a Kubernetes cluster, including important add-on components like\nnetworking and storage.\nRunning Containers in a Cluster\nThe need to distribute our application components across multiple servers is\nnot new to modern application architecture. To build a scalable and reliable\napplication, we have always needed to take advantage of multiple servers to\nhandle the application’s load and preclude a single point of failure. The fact\nthat we are now running these components in containers does not change the\nneed for multiple servers; we are still ultimately using CPUs and we are still\nultimately dependent on hardware.\nAt the same time, a container orchestration environment brings challenges\nthat may not have existed with other kinds of application infrastructure.\nWhen the container is the smallest individual module around which we build\nour system, we end up with application components that are much more self-\ncontained and “opaque” from the perspective of our infrastructure. This\nmeans that instead of having a static application architecture through which\nwe choose in advance what application components are assigned to specific\nservers, with Kubernetes, we try to make it possible for any container to run\nanywhere.\nCross-Cutting Concerns\nThe ability to run any container anywhere maximizes our flexibility, but it\nadds complexity to Kubernetes itself. Kubernetes does not know in advance\nwhat containers it will be asked to run, and the container workload is\ncontinuously changing as new applications are deployed or applications\nexperience changes in load. To rise to this challenge, Kubernetes needs to\naccount for the following design parameters that apply to all container\norchestration software, no matter what containers are running:\nDynamic scheduling\n New containers must be allocated to a server, and\nallocations can change due to configuration changes or failures.\nDistributed state\n The entire cluster must keep information about what\ncontainers are running and where, even during hardware or network failures.\nMultitenancy\n It should be possible to run multiple applications in a single\ncluster, with isolation for security and reliability.\nHardware isolation\n Clusters must run in cloud environments and on regular\nservers of various types, isolating containers from the differences in these\nenvironments.\nThe best term to use to refer to these design parameters is \ncross-cutting\nconcern\n, because they apply to any kind of containerized software that we\nmight need to deploy, and even to the Kubernetes infrastructure itself. These\nparameters work together with the container orchestration requirements we\nsaw in \nChapter 1\n and ultimately drive the Kubernetes architecture and key\ndesign decisions.\nKubernetes Concepts\nTo address these cross-cutting concerns, the Kubernetes architecture allows\nanything to come and go at any time. This includes not only the containerized\napplications deployed to Kubernetes, but also the fundamental software\ncomponents of Kubernetes itself, and even the underlying hardware such as\nservers, network connections, and storage.\nSeparate Control Plane\nObviously, for Kubernetes to be a container orchestration environment, it\nrequires the ability to run containers. This ability is provided by a set of\nworker machines called \nnodes\n. Each node runs a \nkubelet\n service that\ninterfaces with the underlying container runtime to start and monitor\ncontainers.\nKubernetes also has a set of core software components that manage the\nworker nodes and their containers, but these software components are\ndeployed separately from the worker nodes. These core Kubernetes software\ncomponents are together referred to as the \ncontrol plane\n. Because the control\nplane is separate from the worker nodes, the worker nodes can run the control\nplane, gaining the benefits of containerization for the Kubernetes core\nsoftware components. A separate control plane also means that Kubernetes\nitself has a microservice architecture, which allows customization of each\nKubernetes cluster. For example, one control plane component, the \ncloud\ncontroller manager\n, is used only when deploying Kubernetes to a cloud\nprovider, and it’s customized based on the cloud provider used. This design\nprovides hardware isolation for application containers and the rest of the\nKubernetes control plane, while still allowing us to take advantage of the\nspecific features of each cloud provider.\nDeclarative API\nOne critical component of the Kubernetes control plane is the \nAPI server\n.\nThe API server provides an interface for cluster control and monitoring that\nother cluster users and control plane components use. In defining the API,\nKubernetes could have chosen an \nimperative\n style, in which each API\nendpoint is a command such as “run a container” or “allocate storage.”\nInstead, the API is \ndeclarative\n, providing endpoints such as \ncreate\n, \npatch\n,\nget\n, and \ndelete\n. The effect of these commands is to create, read, update, and\ndelete \nresources\n from the cluster configuration—the specific configuration of\neach resource tells Kubernetes what we want the cluster to do.\nThis declarative API is essential to meet the cross-cutting concerns of\ndynamic scheduling and distributed state. Because a declarative API simply\nreports or updates cluster configuration, reacting to server or network failures\nthat might cause a command to be missed is very easy. Consider an example\nin which the API server connection is lost just after an \napply\n command is\nissued to change the cluster configuration. When the connection is restored,\nthe client can simply query the cluster configuration and determine whether\nthe command was received successfully. Or, even easier, the client can just\nissue the same \napply\n command again, knowing that as long as the cluster\nconfiguration ends up as desired, Kubernetes will be trying to do \nthe “right\nthing” to the actual cluster. This core principle is known as \nidempotence\n,\nmeaning it is safe to issue the same command multiple times because it will\nbe applied at most once.\nSelf-Healing\nBuilding on the declarative API, Kubernetes is designed to be \nself-healing\n.\nThis means that the control plane components continually monitor both the\ncluster configuration and the actual cluster state and try to bring them into\nalignment. Every resource in the cluster configuration has an associated\nstatus and event log reflecting how the configuration has actually caused a\nchange in the cluster state.\nThe separation of configuration and state makes Kubernetes very resilient.\nFor example, a resource representing containers may be in a \nRunning\n state if\nthe containers have been scheduled and are actually running. If the\nKubernetes control plane loses connection to the server on which the\ncontainers are running, it can immediately set the status to \nUnknown\n and then\nwork to either reestablish connection or treat the node as failed and\nreschedule the containers.\nAt the same time, using a declarative API and self-healing approach has\nimportant implications. Because the Kubernetes API is declarative, a",8127
34-Cluster Deployment.pdf,34-Cluster Deployment,"“success” response to a command means only that the cluster configuration\nwas updated. It does not mean that the actual state of the cluster was updated,\nas it might take time to achieve the requested state, or there might be issues\nthat prevent the cluster from achieving that state. As a result, we cannot\nassume that just because we created the appropriate resources, the cluster is\nrunning the containers we expect. Instead, we must watch the status of the\nresources and explore the event log to diagnose any issues that the\nKubernetes control plane had in making the actual cluster state match the\nconfiguration we specified.\nCluster Deployment\nWith some core Kubernetes concepts under our belts, we’ll use the \nkubeadm\nKubernetes administration tool to deploy a highly available Kubernetes\ncluster across multiple virtual machines.\nCHOOSING A KUBERNETES\nDISTRIBUTION\nRather than using a particular Kubernetes distribution as we did in\nChapter 1\n, we’ll deploy a “vanilla” Kubernetes cluster using the generic\nupstream repository. This approach gives us the best opportunity to\nfollow along with the cluster deployment and will make it easier to\nexplore the cluster in-depth in the next several chapters. However, when\nyou’re ready to deploy a Kubernetes cluster of your own, especially for\nproduction work, consider using a prebuilt Kubernetes distribution for\nease of management and built-in security. The Cloud \nNative Computing\nFoundation (CNCF) publishes a set of conformance tests that you can\nuse to ensure that the Kubernetes distribution you choose is conformant\nto the Kubernetes specification.\nOur Kubernetes cluster will be split across four virtual machines, labeled\nhost01\n through \nhost04\n. Three of these, \nhost01\n through \nhost03\n, will run control plane\ncomponents, whereas the fourth will act solely as a worker node. We’ll have\nthree control plane nodes because that is the smallest number required to run\na highly available cluster. Kubernetes uses a voting scheme to provide\nfailover, and at least three control plane nodes are required; this allows the\ncluster to detect which side should keep running in the event of a network\nfailure. Also, to keep the cluster as small as possible for our examples, we’ll\nconfigure Kubernetes to run regular containers on the control plane nodes\neven though we would avoid doing that for a production cluster.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nStart by following the instructions for this chapter to get all four virtual\nmachines up and running, either in Vagrant or AWS. The automated\nprovisioning will set up all four machines with \ncontainerd\n and \ncrictl\n, so we don’t\nneed to do it manually. The automated provisioning script will also set up\neither \nkube-vip\n or an AWS network load balancer to provide required high-\navailability functionality, as discussed below.\nNOTE\nYou can install Kubernetes automatically using the\n \nextra\n \nprovisioning\nscript provided with this chapter’s examples. See the README file for\nthis chapter for instructions.\nYou’ll need to run commands on each of the four virtual machines, so you\nmight want to open terminal tabs for each one. However, the first series of\ncommands needs to be run on all of the hosts, so the automation sets up a\ncommand called \nk8s-all\n to do that from \nhost01\n. You can explore the content of\nthis script in \n/usr/local/bin/k8s-all\n or by looking at the \nk8s\n Ansible role in the\nsetup\n directory of the examples.\nPrerequisite Packages\nThe first step is to make sure the \nbr_netfilter\n kernel module is enabled and set to\nload on boot. Kubernetes uses advanced features of the Linux firewall to\nhandle networking across the cluster, so we need this module. Run these two\ncommands:\nroot@host01:~# \nk8s-all modprobe br_netfilter\n...\nroot@host01:~# \nk8s-all ""echo 'br_netfilter' > /etc/modules-load.d/k8s.conf""\nThe first command ensures that the module is installed for the currently\nrunning kernel, and the second command adds it to the list of modules to run\non boot. The slightly odd quoting in the second command ensures that the\nshell redirection happens on the remote hosts.\nNext, in \nListing 6-1\n, we’ll set some Linux kernel parameters to enable\nadvanced network features that are also needed for networking across the\ncluster by using the \nsysctl\n command:\nroot@host01:~# \nk8s-all sysctl -w net.ipv4.ip_forward=1 \\n  \nnet.bridge.bridge-nf-call-ip6tables=1 \\n  \nnet.bridge.bridge-nf-call-iptables=1\nListing 6-1: Kernel settings\nThis command enables the following Linux kernel network features:\nnet.ipv4.ip_forward\n Transfer packets from one network interface to another (for\nexample, from an interface inside a container’s network namespace to a host\nnetwork).\nnet.bridge.bridge-nf-call-ip6tables\n Run IPv6 bridge traffic through the \niptables\nfirewall.\nnet.bridge.bridge-nf-call-iptables\n Run IPv4 bridge traffic through the \niptables\n firewall.\nThe need for the last two items will become clear in \nChapter 9\n when we\ndiscuss how Kubernetes provides networking for Services.\nThese \nsysctl\n changes in \nListing 6-1\n do not persist after a reboot. The\nautomated scripts do handle making the changes persistent, so if you reboot\nyour virtual machines, either run the \nextra\n provisioning script, or run these\ncommands again.\nWe’ve now finished configuring the Linux kernel to support our\nKubernetes deployment and are almost ready for the actual install. First we\nneed to install some prerequisite packages:\nroot@host01:~# \nk8s-all apt install -y apt-transport-https \\n  \nopen-iscsi nfs-common\nThe \napt-transport-https\n package ensures that \napt\n can support connecting to\nrepositories via secure HTTP. The other two packages are needed for one of\nthe cluster add-ons that we’ll install after our cluster is up and running.\nKubernetes Packages\nWe can now add the Kubernetes repository to install the \nkubeadm\n tool that will\nset up our cluster. First, add the GPG key used to check the package\nsignatures:\nroot@host01:~# \nk8s-all ""curl -fsSL \\n  \nhttps://packages.cloud.google.com/apt/doc/apt-key.gpg | \\n  \ngpg --dearmor -o /usr/share/keyrings/google-cloud-keyring.gpg""\nThis command uses \ncurl\n to download the GPG key. It then uses \ngpg\n to\nreformat it, and then it writes the result to \n/usr/share/keyrings\n. The command\nline flags \nfsSL\n put \ncurl\n in a mode that behaves better for chained commands,\nincluding avoiding unnecessary output, following server redirects, and\nterminating with an error if there is a problem.\nNext, we add the repository configuration:\nroot@host01:~# \nk8s-all ""echo 'deb [arch=amd64' \\n  \n'signed-by=/usr/share/keyrings/google-cloud-keyring.gpg]' \\n  \n'https://apt.kubernetes.io/ kubernetes-xenial main' > \\n  \n/etc/apt/sources.list.d/kubernetes.list""\nAs before, the quoting is essential to ensure that the command is passed\ncorrectly via SSH to all the other hosts in the cluster. The command\nconfigures \nkubernetes-xenial\n as the distribution; this distribution is used for any\nversion of Ubuntu, starting with the older Ubuntu Xenial.\nAfter we have created this new repository, we then need to run \napt update\n on\nall hosts to download the list of packages:\nroot@host01:~# \nk8s-all apt update\n...\nNow we can install the packages we need using \napt\n:\nroot@host01:~# \nsource /opt/k8sver\nroot@host01:~# \nk8s-all apt install -y kubelet=$K8SV kubeadm=$K8SV kubectl=$K8SV\nThe \nsource\n command loads a file with a variable to install a specific\nKubernetes version. This file is created by the automated scripts and ensures\nthat we use a consistent Kubernetes version for all chapters. You can update\nthe automated scripts to choose which Kubernetes version to install.\nThe \napt\n command installs the following three packages along with some\ndependencies:\nkubelet\n Service for all worker nodes that interfaces with the container engine\nto run containers as scheduled by the control plane\nkubeadm\n Administration tool that we’ll use to install Kubernetes and maintain\nour cluster\nkubectl\n Command line client that we’ll use to inspect our Kubernetes cluster\nand to create and delete resources\nThe \nkubelet\n package starts its service immediately, but because we haven’t\ninstalled the control plane yet, the service will be in a failed state at first:\nroot@host01:~# \nsystemctl status kubelet\n  kubelet.service - kubelet: The Kubernetes Node Agent\n...\n   Main PID: 75368 (code=exited, status=1/FAILURE)\nWe need to control the version of the packages we just installed because\nwe want to upgrade all of the components of our cluster together. To protect\nourselves from accidentally updating these packages, we’ll hold them at their\ncurrent version:\nroot@host01:~# \nk8s-all apt-mark hold kubelet kubeadm kubectl\nThis command prevents the standard \napt full-upgrade\n command from\nupdating these packages. Instead, if we upgrade our cluster, we’ll need to\nspecify the exact version that we want by using \napt install\n.\nCluster Initialization\nThe next command, \nkubeadm init\n, initializes the control plane and provides the\nkubelet\n worker node service configuration for all the nodes. We’ll run \nkubeadm\ninit\n on one node in our cluster and then use \nkubeadm join\n on each of the other\nnodes so that they join the existing cluster.\nTo run \nkubeadm init\n, we first create a YAML configuration file. This\napproach has a few advantages. It greatly shortens the number of command\nline flags that we need to remember, and it lets us keep the cluster\nconfiguration in a repository, giving us configuration control over the cluster.\nWe then can update the YAML file and rerun \nkubeadm\n to make cluster\nconfiguration changes.\nThe automation scripts for this chapter have populated a YAML\nconfiguration file in \n/etc/kubernetes\n, so it’s ready to use. The following\nshows the contents of that file:\nkubeadm-init.yaml\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: InitConfiguration\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: 1d8fb1.2875d52d62a3282d\n  ttl: 2h0m0s\n  usages:\n  - signing\n  - authentication\nnodeRegistration:\n  kubeletExtraArgs:\n    node-ip: 192.168.61.11\n  taints: []\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.61.11\ncertificateKey: ""5a7e07816958efb97635e9a66256adb1""\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: ClusterConfiguration\nkubernetesVersion: 1.21.4\napiServer:\n  extraArgs:\n    service-node-port-range: 80-32767\nnetworking:\n  podSubnet: ""172.31.0.0/16""\ncontrolPlaneEndpoint: ""192.168.61.10:6443""\n---\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nserverTLSBootstrap: true\nThis YAML file has three documents, separated by dashes (\n---\n). The first\ndocument is specific to initializing the cluster, the second has more generic\nconfiguration, and the third is used to provide settings for \nkubelet\n across all the\nnodes. Let’s look at the purpose of each of these configuration items:\napiVersion / kind\n Tells Kubernetes about the purpose of each YAML document,\nso it can validate the contents.\nbootstrapTokens\n Configures a secret that other nodes can use to join the cluster.\nThe \ntoken\n should be kept secret in a production cluster. It is set to expire\nautomatically after two hours, so if we want to join more nodes later, we’ll\nneed to make another one.\nnodeRegistration\n Configuration to pass to the \nkubelet\n service running on \nhost01\n.\nThe \nnode-ip\n field ensures that \nkubelet\n registers the correct IP address with the\nAPI server so that the API server can communicate with it. The \ntaints\n field\nensures that regular containers can be scheduled onto control plane nodes.\nlocalAPIEndpoint\n The local IP address that the API server should use. Our\nvirtual machine has multiple IP addresses, and we want the API server\nlistening on the correct network.\ncertificateKey\n Configures a secret that other nodes will use to gain access to the\ncertificates for the API server. It’s needed so that all of the API server\ninstances in our highly available cluster can use the same certificate. Keep it\nsecret in a production cluster.\nnetworking\n All containers in the cluster will get an IP address from the \npodSubnet\n,\nno matter what host they run on. Later, we’ll install a network driver that will\nensure that every container on all hosts in the cluster can communicate.\ncontrolPlaneEndpoint\n The API server’s external address. For a highly available\ncluster, this IP address needs to reach any API server instance, not just the\nfirst one.\nserverTLSBootstrap\n Instructs \nkubelet\n to use the controller manager’s certificate\nauthority to request server certificates.\nThe \napiVersion\n and \nkind\n fields will appear in every Kubernetes YAML file.\nThe \napiVersion\n field defines a group of related Kubernetes resources, including\na version number. The \nkind\n field then selects the specific resource type within\nthat group. This not only allows the Kubernetes project and other vendors to\nadd new groups of resources over time, but it also allows updates to existing\nresource specifications while maintaining backward compatibility.\nHIGHLY AVAILABLE CLUSTERS\nThe \ncontrolPlaneEndpoint\n field is used to configure the most important\nrequirement for a highly available cluster: an IP address that reaches all\nof the API servers. We need to establish this IP address immediately\nwhen we initialize the cluster because it is used to generate certificates\nwith which clients will verify the API server’s identity. The best way to\nprovide a cluster-wide IP address depends on where the cluster is\nrunning; for example, in a cloud environment, using the provider’s\nbuilt-in capability, such as an Elastic Load Balancer (ELB) in Amazon\nWeb Services or an Azure Load Balancer, is best.\nBecause of the nature of the two different environments, the examples\nfor this book use \nkube-vip\n when running with Vagrant, and ELB when\nrunning in Amazon Web Services. The top-level \nREADME.md\n file in\nthe example documentation has more details. The installation and\nconfiguration is done automatically so there’s nothing more to\nconfigure. We can just use \n192.168.61.10:6443\n and expect traffic to get to\nany of the API server instances running on \nhost01\n through \nhost03\n.\nBecause we have the cluster configuration ready to go in a YAML file, the\nkubeadm init\n command to initialize the cluster is simple. We run this command\nsolely on \nhost01\n:\nroot@host01:~# \n/usr/bin/kubeadm init \\n  \n--config /etc/kubernetes/kubeadm-init.yaml --upload-certs\nThe \n--config\n option points to the YAML configuration file (\nkubeadm-\ninit.yaml\n) that we looked at earlier, and the \n--upload-certs\n option tells \nkubeadm\n that\nit should upload the API server’s certificates to the cluster’s distributed\nstorage. The other control plane nodes then can download those certificates\nwhen they join the cluster, allowing all API server instances to use the same\ncertificates so that clients will trust them. The certificates are encrypted using\nthe \ncertificateKey\n we provided, which means that the other nodes will need this\nkey to decrypt them.\nThe \nkubeadm init\n command initializes the control plane’s components on\nhost01\n. These components are run in containers and managed by the \nkubelet\nservice, which makes them easy to upgrade. Several container images will be\ndownloaded, so the command might take a while, depending on the speed of\nyour virtual machines and your internet connection.\nJoining Nodes to the Cluster\nThe \nkubeadm init\n command prints out a \nkubeadm join\n command that we can use to\njoin other nodes to the cluster. However, the automation scripts have already\nprestaged a configuration file to each of the other nodes to ensure that they\njoin as the correct type of node. The servers \nhost02\n and \nhost03\n will join as\nadditional control plane nodes, whereas \nhost04\n will join solely as a worker\nnode.\nHere’s the YAML configuration file for \nhost02\n with its specific settings:\nkubeadm-join.yaml (host02)\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: JoinConfiguration\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: 192.168.61.10:6443\n    token: 1d8fb1.2875d52d62a3282d\n    unsafeSkipCAVerification: true\n  timeout: 5m0s\nnodeRegistration:\n  kubeletExtraArgs:\n    cgroup-driver: containerd\n    node-ip: 192.168.61.12\n  taints: []\n  ignorePreflightErrors:\n    - DirAvailable--etc-kubernetes-manifests\ncontrolPlane:\n  localAPIEndpoint:\n    advertiseAddress: 192.168.61.12\n  certificateKey: ""5a7e07816958efb97635e9a66256adb1""\nThis resource has a type of \nJoinConfiguration\n, but most of the fields are the\nsame as the \nInitConfiguration\n in the \nkubeadm-init.yaml\n file. Most important, the\ntoken\n and \ncertificateKey\n match the secret we set up earlier, so this node will be\nable to validate itself with the cluster and decrypt the API server certificates.\nOne difference is the addition of \nignorePreflightErrors\n. This section appears\nonly when we are installing \nkube-vip\n, as in that case we need to prestage the\nconfiguration file for \nkube-vip\n to the \n/etc/kubernetes/manifests\n directory, and\nwe need to tell \nkubeadm\n that it is okay for that directory to already exist.\nBecause we have this YAML configuration file, the \nkubeadm join\n command\nis simple. Run it on \nhost02\n:\nroot@host02:~# \n/usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml\nAs before, this command runs the control plane components as containers\nusing the \nkubelet\n service on this node, so it will take some time to download\nthe container images and start the containers.\nWhen it finishes, run the exact same command on \nhost03\n:\nroot@host03:~# \n/usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml\nThe automation script set up the YAML file with the correct IP address\nfor each host, so the differences in configuration between each of the hosts is\nalready accounted for.\nWhen this command completes, we’ll have created a highly available\nKubernetes cluster, with the control plane components running on three\nseparate hosts. However, we do not yet have any regular worker nodes. Let’s\nfix that issue.\nWe’ll begin by joining \nhost04\n as a regular worker node and running exactly\nthe same \nkubeadm join\n command on \nhost04\n, but the YAML configuration file will\nbe a little different. Here’s that file:\nkubeadm-join.yaml (host04)\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: JoinConfiguration\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: 192.168.61.10:6443\n    token: 1d8fb1.2875d52d62a3282d\n    unsafeSkipCAVerification: true\n  timeout: 5m0s\nnodeRegistration:\n  kubeletExtraArgs:\n    cgroup-driver: containerd\n    node-ip: 192.168.61.14\n  taints: []\nThis YAML file is missing the \ncontrolPlane\n field, so \nkubeadm\n configures it as a\nregular worker node rather than a control plane node.\nNow let’s join \nhost04\n to the cluster:\nroot@host04:~# \n/usr/bin/kubeadm join --config /etc/kubernetes/kubeadm-join.yaml\nThis command completes a little faster because it doesn’t need to\ndownload the control plane container images and run them. We now have\nfour nodes in the cluster, which we can verify by running \nkubectl\n back on\nhost01\n:\nroot@host01:~# \nexport KUBECONFIG=/etc/kubernetes/admin.conf\nroot@host01:~# \nkubectl get nodes\nNAME     STATUS     ROLES        ...\nhost01   NotReady   control-plane...\nhost02   NotReady   control-plane...\nhost03   NotReady   control-plane...\nhost04   NotReady   <none>       ...\nThe first command sets an environment variable to tell \nkubectl\n what\nconfiguration file to use. The \n/etc/kubernetes/admin.conf\n file was created\nautomatically by \nkubeadm\n when it initialized \nhost01\n as a control plane node. That\nfile tells \nkubectl\n what address to use for the API server, what certificate to use\nto verify the secure connection, and how to authenticate.\nThe four nodes currently should be reporting a status of \nNotReady\n. Let’s run\nthe \nkubectl describe\n command to get the node details:\nroot@host01:~# \nkubectl describe node host04\nName:               host04\n...\nConditions:\n  Type   Status ... Message\n  ----   ------ ... -------\n  Ready  False  ... container runtime network not ready...\n...\nWe haven’t yet installed a network driver for our Kubernetes cluster, and\nas a result, all of the nodes are reporting a status of \nNotReady\n, which means that\nthey won’t accept regular application workloads. Kubernetes communicates\nthis by placing a \ntaint\n in the node’s configuration. A taint restricts what can\nbe scheduled on a node. We can list the taints on the nodes using \nkubectl\n:\nroot@host01:~# \nkubectl get node -o json | \\n  \njq '.items[]|.metadata.name,.spec.taints[]'\n""host01""\n{\n  ""effect"": ""NoSchedule"",\n  ""key"": ""node.kubernetes.io/not-ready""\n}\n""host02""\n{\n  ""effect"": ""NoSchedule"",\n  ""key"": ""node.kubernetes.io/not-ready""\n}\n""host03""\n{\n  ""effect"": ""NoSchedule"",\n  ""key"": ""node.kubernetes.io/not-ready""\n}\n""host04""\n{\n  ""effect"": ""NoSchedule"",\n  ""key"": ""node.kubernetes.io/not-ready""\n}\nWe select an output format of \njson\n so that we can use \njq\n to print just the\ninformation we need. Because all the nodes have a status of \nNotReady\n, they\nhave a \nnot-ready\n taint set to \nNoSchedule\n, which prevents the Kubernetes scheduler\nfrom scheduling containers onto them.\nBy specifying \ntaints\n as an empty array in the \nkubeadm\n configuration, we\nprevented the three control plane nodes from having an additional control\nplane taint. In a production cluster, this taint keeps application containers\nseparate from the control plane containers for security reasons, so we would\nleave it in place. For our example cluster, though, it would mean that we need\nmultiple extra virtual machines to act as worker nodes, which we don’t want.\nThe command \nkubectl taint\n would allow us to remove the \nnot-ready\n taint\nmanually, but the correct approach is to install a network driver as a cluster\nadd-on so that the nodes will properly report \nReady\n, enabling us to run\ncontainers on them.",22583
35-Installing Cluster Add-ons.pdf,35-Installing Cluster Add-ons,"Installing Cluster Add-ons\nWe’ve installed \nkubelet\n on four separate nodes and installed the control plane\non three of those nodes and joined them to our cluster. For the rest, we’ll use\nthe control plane to install cluster add-ons. These add-ons are similar to\nregular applications that we would deploy. They consist of Kubernetes\nresources and run in containers, but they provide essential services to the\ncluster that our applications will use.\nTo get a basic cluster up and running, we need to install three types of\nadd-ons: a \nnetwork driver\n, a \nstorage driver\n, and an \ningress controller\n. We\nwill also install a fourth optional add-on, a \nmetrics server\n.\nNetwork Driver\nKubernetes networking is based on the Container Network Interface (CNI)\nstandard. Anyone can build a new network driver for Kubernetes by\nimplementing this standard, and as a result, several choices are available for\nKubernetes network drivers. We’ll demonstrate different network plug-ins in\nChapter 8\n, but most of the clusters in this book use the Calico network driver\nbecause it is the default choice for many Kubernetes platforms.\nFirst, download the primary YAML configuration file for Calico:\nroot@host01:~# \ncd /etc/kubernetes/components\nroot@host01:/etc/kubernetes/components# \ncurl -L -O $calico_url\n...\nThe \n-L\n option tells \ncurl\n to follow any HTTP redirects, whereas the \n-O\n option\ntells \ncurl\n to save the content in a file using the same filename as in the URL.\nThe value of the \ncalico_url\n environment variable is set in the \nk8s-ver\n script that\nalso specified the Kubernetes version. This is essential, as Calico is sensitive\nto the specific version of Kubernetes we’re running, so it’s important to\nchoose values that are compatible.\nThe primary YAML configuration is written to the local file \ntigera-\noperator.yaml\n. This refers to the fact that the initial installation is a\nKubernetes Operator, which then creates all of the other cluster resources to\ninstall Calico. We’ll explore operators in \nChapter 17\n.\nIn addition to this primary YAML configuration, the automated scripts for\nthis chapter have added a file called \ncustom-resources.yaml\n that provides\nnecessary configuration for our example cluster. We now can tell the\nKubernetes API server to apply all the resources in these files to the cluster:\nroot@host01:/etc/kubernetes/components# \nkubectl apply -f tigera-operator.yaml\n...\nroot@host01:/etc/kubernetes/components# \nkubectl apply -f custom-resources.yaml\nKubernetes takes a few minutes to download container images and start\ncontainers, and then Calico will be running in our cluster and our nodes\nshould report a status of \nReady\n:\nroot@host01:/etc/kubernetes/components# \nkubectl get nodes\nNAME     STATUS   ROLES                ...\nhost01   Ready    control-plane,master ...\nhost02   Ready    control-plane,master ...\nhost03   Ready    control-plane,master ...\nhost04   Ready    <none>               ...\nCalico works by installing a \nDaemonSet\n, a Kubernetes resource that tells\nthe cluster to run a specific container or set of containers on every node. The\nCalico containers then provide network services for any containers running\non that node. However, that raises an important question. When we installed\nCalico in our cluster, all of our nodes had a taint that told Kubernetes not to\nschedule containers on them. How was Calico able to run its containers on all\nthe nodes? The answer is \ntolerations\n.\nA toleration is a configuration setting applied to a resource that instructs\nKubernetes it can be scheduled on a node despite a taint possibly being\npresent. Calico specifies a toleration when it adds its DaemonSet to the\ncluster, as we can see with \nkubectl\n:\nroot@host01:/etc/kubernetes/components# \nkubectl -n calico-system \\n  \nget daemonsets -o json | \\n  \njq '.items[].spec.template.spec.tolerations[]'\n{\n  ""key"": ""CriticalAddonsOnly"",\n  ""operator"": ""Exists""\n}\n{\n  ""effect"": ""NoSchedule"",\n  ""operator"": ""Exists""\n}\n{\n  ""effect"": ""NoExecute"",\n  ""operator"": ""Exists""\n}\nThe \n-n\n option selects the \ncalico-system\n \nNamespace\n. Namespaces are a way to\nkeep Kubernetes resources separate from one another on a cluster, for\nsecurity reasons as well as to avoid naming collisions. Also, as before, we\nrequest JSON output and use \njq\n to select only the field we’re interested in. If\nyou want to see the entire configuration for the resource, use \n-o=json\n without \njq\nor use \n-o=yaml\n.\nThis DaemonSet has three tolerations, and the second one provides the\nbehavior we need. It tells the Kubernetes scheduler to go ahead and schedule\nit even on nodes that have a \nNoSchedule\n taint. Calico then can get itself started\nbefore the node is ready, and once it’s running, the node changes its status to\nReady\n so that normal application containers can be scheduled. The control\nplane components needed a similar toleration in order to run on nodes before\nthey show \nReady\n.\nInstalling Storage\nThe cluster nodes are ready, so if we deployed a regular application, its\ncontainers would run. However, applications that require persistent storage\nwould fail to start because the cluster doesn’t yet have a storage driver. Like\nnetwork drivers, several storage drivers are available for Kubernetes. The\nContainer Storage Interface (CSI) provides the standard that storage drivers\nneed to meet to work with Kubernetes. We’ll use Longhorn, a storage driver\nfrom Rancher; it’s easy to install and doesn’t require any underlying\nhardware like extra block devices or access to cloud-based storage.\nLonghorn makes use of the iSCSI and NFS software we installed earlier.\nIt expects all of our nodes to have the \niscsid\n service enabled and running, so\nlet’s make sure that’s true on all our nodes:\nroot@host01:/etc/kubernetes/components# \nk8s-all systemctl enable --now iscsid\nWe now can install Longhorn on the cluster. The process for installing\nLonghorn looks a lot like Calico. Start by downloading the Longhorn YAML\nconfiguration:\nroot@host01:/etc/kubernetes/components# \ncurl -LO $longhorn_url\nThe \nlonghorn_url\n environment variable is also set by the \nk8s-ver\n script, which\nallows us to ensure compatibility.\nInstall Longhorn using \nkubectl\n:\nroot@host01:/etc/kubernetes/components# \nkubectl apply -f longhorn.yaml\nAs before, \nkubectl apply\n ensures that the resources in the YAML file are\napplied to the cluster, creating or updating them as necessary. The \nkubectl apply\ncommand supports URLs as the source of the resource it applies to the\ncluster, but for these three installs, we run a separate \ncurl\n command because\nit’s convenient to have a local copy of what was applied to the cluster.\nLonghorn is now installed on the cluster, which we’ll verify as we explore\nthe cluster in the rest of this chapter.\nIngress Controller\nWe now have networking and storage, but the networking allows access to\ncontainers only from within our cluster. We need another service that exposes\nour containerized applications outside the cluster. The easiest way to do that\nis to use an ingress controller. As we’ll describe in \nChapter 9\n, an ingress\ncontroller watches the Kubernetes cluster for \nIngress\n resources and routes\nnetwork traffic.\nWe begin by downloading the ingress controller YAML configuration:\nroot@host01:/etc/kubernetes/components# \ncurl -Lo ingress-controller.yaml\n  \n$ingress_url\nAs in our earlier example, the \ningress_url\n environment variable is set by the\nk8s-ver\n script so that we can ensure compatibility. In this case, the URL ends in\nthe generic path of \ndeploy.yaml\n, so we use \n-o\n to provide a filename to \ncurl\n to\nmake clear the purpose of the downloaded YAML file.\nInstall the ingress controller using \nkubectl\n:\nroot@host01:/etc/kubernetes/components# \nkubectl apply -f ingress-controller.yaml\nThis creates a lot of resources, but there are two main parts: an NGINX\nweb server that actually performs routing of HTTP traffic, and a component\nthat watches for changes in Ingress resources in the cluster and configures\nNGINX accordingly.\nThere’s one more step we need. As installed, the ingress controller tries to\nrequest an external IP address to allow traffic to reach it from outside the\ncluster. Because we’re running a sample cluster with no access to external IP\naddresses, this won’t work. Instead, we’ll be accessing our ingress controller\nusing port forwarding from our cluster hosts. At the moment, our ingress\ncontroller is set up for this port forwarding, but it’s using a random port. We\nwould like to select the port to be sure that we know where to find the ingress\ncontroller. At the same time, we’ll also add an annotation so that this ingress\ncontroller will be the default for this cluster.\nTo apply the port changes, we’re going to provide our Kubernetes cluster\nan with extra YAML configuration with just the changes we need. Here’s that\nYAML:\ningress-patch.yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\nspec:\n  ports:\n    - port: 80\n      nodePort: 80\n    - port: 443\n      nodePort: 443\nThis file specifies the name and Namespace of the Service to ensure that\nKubernetes knows where to apply these changes. It also specifies the \nport\nconfiguration we’re updating, along with the \nnodePort\n, which is the port on our\ncluster nodes that will be used for port forwarding. We’ll look at NodePort\nservice types and port forwarding in more detail in \nChapter 9\n.\nTo patch the service, we use the \nkubectl patch\n command:\nroot@host01:/etc/kubernetes/components# \nkubectl patch -n ingress-nginx \\n  \nservice/ingress-nginx-controller --patch-file ingress-patch.yaml\nservice/ingress-nginx-controller patched\nTo apply the annotation, use the \nkubectl annotate\n command:\nroot@host01:/etc/kubernetes/components# \nkubectl annotate -n ingress-nginx \\n  \ningressclass/nginx ingressclass.kubernetes.io/is-default-class=""true""\ningressclass.networking.k8s.io/nginx annotated\nKubernetes reports the change to each resource as we make it, so we know\nthat our changes have been applied.\nMetrics Server\nOur final add-on is a \nmetrics server\n that collects utilization metrics from our\nnodes, enabling the use of autoscaling. To do this, it needs to connect to the\nkubelet\n instances in our cluster. For security, it needs to verify the HTTP/S\ncertificate when it connects to a \nkubelet\n. This is why we configured \nkubelet\n to\nrequest a certificate signed by the controller manager rather than allowing the\nkubelet\n to generate self-signed certificates.\nDuring setup, \nkubelet\n created a certificate request on each node, but the\nrequests were not automatically approved. Let’s find these requests:\nroot@host01:/etc/kubernetes/components# \nkubectl get csr\nNAME      ... SIGNERNAME                                  ... CONDITION\ncsr-sgrwz ... kubernetes.io/kubelet-serving               ... Pending\ncsr-agwb6 ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued\ncsr-2kwwk ... kubernetes.io/kubelet-serving               ... Pending\ncsr-5496d ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued\ncsr-hm6lj ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued\ncsr-jbfmx ... kubernetes.io/kubelet-serving               ... Pending\ncsr-njjr7 ... kubernetes.io/kube-apiserver-client-kubelet ... Approved,Issued\ncsr-v7tcs ... kubernetes.io/kubelet-serving               ... Pending\ncsr-vr27n ... kubernetes.io/kubelet-serving               ... Pending\nEach \nkubelet\n has a client certificate that it uses to authenticate to the API\nserver; these were automatically approved during bootstrap. The requests we\nneed to approve are for \nkubelet-serving\n certificates, which are used when clients\nsuch as our metrics server connect to \nkubelet\n. As soon as the request is\napproved, the controller manager signs the certificate. The \nkubelet\n then collects\nthe certificate and starts using it.\nWe can approve all of these requests at once by querying for the name of\nall of the \nkubelet-serving\n requests and then passing those names to \nkubectl certificate\napprove\n:\nroot@host01:/etc/kubernetes/components# \nkubectl certificate approve \$(kubectl\n  \nget csr --field-selector spec.signerName=kubernetes.io/kubelet-serving -o name)\ncertificatesigningrequest.certificates.k8s.io/csr-sgrwz approved\n...",12584
36-Exploring a Cluster.pdf,36-Exploring a Cluster,"We now can install our metrics server by downloading and applying its\nYAML configuration:\nroot@host01:/etc/kubernetes/components# \ncurl -Lo metrics-server.yaml \$metrics_url\nroot@host01:/etc/kubernetes/components# \nkubectl apply -f metrics-server.yaml\n...\nroot@host01:/etc/kubernetes/components# \ncd\nroot@host01:~#\nThis component is the last one we need to install, so we can leave this\ndirectory. With these cluster add-ons, we now have a complete, highly\navailable Kubernetes cluster.\nExploring a Cluster\nBefore deploying our first application onto this brand-new Kubernetes\ncluster, let’s explore what’s running on it. The commands we use here will\ncome in handy later as we debug our own applications and a cluster that isn’t\nworking correctly.\nWe’ll use \ncrictl\n, the same command we used to explore running containers\nin \nPart I\n, to see what containers are running on \nhost01\n:\nroot@host01:~# \ncrictl ps\nCONTAINER       ... STATE    NAME                       ...\n25c63f29c1442   ... Running  longhorn-csi-plugin        ...\n2ffdd044a81d8   ... Running  node-driver-registrar      ...\n94468050de89c   ... Running  csi-provisioner            ...\n119fbf417f1db   ... Running  csi-attacher               ...\ne74c1a2a0c422   ... Running  kube-scheduler             ...\nd1ad93cdbc686   ... Running  kube-controller-manager    ...\n76266a522cc3d   ... Running  engine-image-ei-611d1496   ...\nfc3cd1679e33e   ... Running  replica-manager            ...\n48e792a973105   ... Running  engine-manager             ...\ne658baebbc295   ... Running  longhorn-manager           ...\neb51d9ec0f2fc   ... Running  calico-kube-controllers    ...\n53e7e3e4a3148   ... Running  calico-node                ...\n772ac45ceb94e   ... Running  calico-typha               ...\n4005370021f5f   ... Running  kube-proxy                 ...\n26929cde3a264   ... Running  kube-apiserver             ...\n9ea4c2f5af794   ... Running  etcd                       ...\nThe control plane node is very busy, as this list includes Kubernetes\ncontrol plane components, Calico components, and Longhorn components.\nRunning this command on all the nodes and sorting out what containers are\nrunning where and for what purpose would be confusing. Fortunately, \nkubectl\nprovides a clearer picture, although knowing that we can get down to these\nlower-level details and see exactly what containers are running on a given\nnode is nice.\nTo explore the cluster with \nkubectl\n, we need to know how the cluster\nresources are organized into Namespaces. As mentioned previously,\nKubernetes Namespaces provide security and avoid name collisions. To\nensure idempotence, Kubernetes needs each resource to have a unique name.\nBy dividing resources into Namespaces, we allow multiple resources to have\nthe same name while still enabling the API server to know exactly which\nresource we mean, which also supports multitenancy, one of our cross-cutting\nconcerns.\nEven though we just set up the cluster, it’s already populated with several\nNamespaces:\nroot@host01:~# \nkubectl get namespaces\nNAME              STATUS   AGE\ncalico-system     Active   50m\ndefault           Active   150m\nkube-node-lease   Active   150m\nkube-public       Active   150m\nkube-system       Active   150m\nlonghorn-system   Active   16m\ntigera-operator   Active   50m\nAs we run \nkubectl\n commands, they will apply to the \ndefault\n Namespace\nunless we use the \n-n\n option to specify a different Namespace.\nTo see what containers are running, we ask \nkubectl\n to get the list of Pods.\nWe look at Kubernetes Pods in much more detail in \nChapter 7\n. For now, just\nknow that a Pod is a group of one or more containers, much like the Pods that\nwe created with \ncrictl\n in \nPart I\n.\nIf we try to list Pods in the \ndefault\n Namespace, we can see that there aren’t\nany yet:\nroot@host01:~# \nkubectl get pods\nNo resources found in default namespace.\nSo far, as we installed cluster infrastructure components, they’ve been\ncreated in other Namespaces. That way, when we configure normal user\naccounts, we can prevent those users from viewing or editing the cluster\ninfrastructure. The Kubernetes infrastructure components were all installed\ninto the \nkube-system\n Namespace:\nroot@host01:~# \nkubectl -n kube-system get pods\nNAME                             READY   STATUS    ...\ncoredns-558bd4d5db-7krwr         1/1     Running   ...\n...\nkube-apiserver-host01            1/1     Running   ...\n...\nWe cover the control plane components in \nChapter 11\n. For now, let’s\nexplore just one of the control plane Pods, the API server running on \nhost01\n.\nWe can get all of the details for this Pod using \nkubectl describe\n:\nroot@host01:~# \nkubectl -n kube-system describe pod kube-apiserver-host01\nName:                 kube-apiserver-host01\nNamespace:            kube-system\n...\nNode:                 host01/192.168.61.11\n...\nStatus:               Running\nContainers:\n  kube-apiserver:\n    Container ID:  containerd://26929cde3a264e...\n...\nThe Namespace and name together uniquely identify this Pod. We also\nsee the node on which the Pod is scheduled, its status, and details about the\nactual containers, including a container ID that we can use with \ncrictl\n to find\nthe container in the underlying \ncontainerd\n runtime.\nLet’s also verify that Calico deployed into our cluster as expected:\nroot@host01:~# \nkubectl -n calico-system get pods\nNAME                                       READY   STATUS    ...\ncalico-kube-controllers-7f58dbcbbd-ch7zt   1/1     Running   ...\ncalico-node-cp88k                          1/1     Running   ...\ncalico-node-dn4rj                          1/1     Running   ...\ncalico-node-xnkmg                          1/1     Running   ...\ncalico-node-zfscp                          1/1     Running   ...\ncalico-typha-68b99cd4bf-7lwss              1/1     Running   ...\ncalico-typha-68b99cd4bf-jjdts              1/1     Running   ...\ncalico-typha-68b99cd4bf-pjr6q              1/1     Running   ...\nEarlier we saw that Calico installed a DaemonSet resource. Kubernetes\nhas used the configuration in this DaemonSet to automatically create a \ncalico-\nnode\n Pod for each node. Like Kubernetes itself, Calico also uses a separate\ncontrol plane to handle overall configuration of the network, and the other\nPods provide that control plane.\nFinally, we’ll see the containers that are running for Longhorn:\nroot@host01:~# \nkubectl -n longhorn-system get pods\nNAME                                       READY   STATUS    RESTARTS   AGE\nengine-image-ei-611d1496-8q58f             1/1     Running   0          31m\n...\nlonghorn-csi-plugin-8vkr6                  2/2     Running   0          31m\n...\nlonghorn-manager-dl9sb                     1/1     Running   1          32m\n...\nLike Calico, Longhorn uses DaemonSets so that it can run containers on\nevery node. These containers provide storage services to the other containers\non the node. Longhorn also includes a number of other containers that serve\nas a control plane, including providing the CSI implementation that\nKubernetes uses to tell Longhorn to create storage when needed.\nWe put a lot of effort into setting up this cluster, so it would be a shame to\nend this chapter without running at least one application on it. In the next\nchapter, we will look at many different ways to run containers, but let’s\nquickly run a simple NGINX web server in our Kubernetes cluster:\nroot@host01:~# \nkubectl run nginx --image=nginx\npod/nginx created\nThat may look like an imperative command, but under the hood, \nkubectl\n is\ncreating a Pod resource using the name and container image we specified,\nand then it’s applying that resource on the cluster. Let’s inspect the default\nNamespace again:\nroot@host01:~# \nkubectl get pods -o wide\nNAME    READY   STATUS    ... IP               NODE  ...\nnginx   1/1     Running   ... 172.31.89.203   host02 ...\nWe used \n-o wide\n to see extra information about the Pod, including its IP\naddress and where it was scheduled, which can be different each time the Pod\nis created. In this case, the Pod was scheduled to \nhost02\n, showing that we were\nsuccessful in allowing regular application containers to be deployed to our",8315
37-Final Thoughts.pdf,37-Final Thoughts,"control plane nodes. The IP address comes from the Pod CIDR we\nconfigured, and Calico automatically assigns it.\nCalico also handles routing traffic so that we can reach the Pod from any\ncontainer in the cluster as well as from the host network. Let’s verify that,\nstarting with a regular \nping\n:\nroot@host01:~# \nping -c 1 \n172.31.89.203\nPING 172.31.89.203 (172.31.89.203) 56(84) bytes of data.\n64 bytes from 172.31.89.203: icmp_seq=1 ttl=63 time=0.848 ms\n--- 172.31.89.203 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.848/0.848/0.848/0.000 ms\nUse your Pod’s IP address in the place of the one shown here.\nWe can also use \ncurl\n to verify that the NGINX web server is working:\nroot@host01:~# \ncurl http://\n172.31.89.203\n...\n<title>Welcome to nginx!</title>\n...\nThe Kubernetes cluster is working and ready for us to deploy applications.\nKubernetes will take advantage of all of the nodes in the cluster to load\nbalance our applications and provide resiliency in the event of any failures.\nFinal Thoughts\nIn this chapter, we’ve explored how Kubernetes is architected with the\nflexibility to allow cluster components to come and go at any time. This\napplies not only to containerized applications but also to the cluster\ncomponents, including control plane microservices and the underlying\nservers and networks the cluster uses. We were able to bootstrap a cluster and\nthen dynamically add nodes to it, configure those nodes to accept certain\ntypes of containers, and then dynamically add networking and storage drivers\nusing the Kubernetes cluster itself to run and monitor them. Finally, we\ndeployed our first container to a Kubernetes cluster, allowing it to\nautomatically schedule the container onto an available node, using our\nnetwork driver to access the container from the host network.\nNow that we have a highly available cluster, we can look at how to deploy\nan application to Kubernetes. We’ll explore some key Kubernetes resources\nthat we need to create a scalable, reliable application. This process will\nprovide a foundation for exploring Kubernetes in detail, including\nunderstanding what happens when our applications don’t run as expected and\nhow to debug issues with our application or the Kubernetes cluster.",2328
38-7 DEPLOYING CONTAINERS TO KUBERNETES.pdf,38-7 DEPLOYING CONTAINERS TO KUBERNETES,,0
39-Pods.pdf,39-Pods,"7\nDEPLOYING CONTAINERS TO KUBERNETES\nWe’re now ready to begin running containers on our working Kubernetes\ncluster. Because Kubernetes has a declarative API, we’ll create various kinds\nof resources to run them, and we’ll monitor the cluster to see what\nKubernetes does for each type of resource.\nDifferent containers have different use cases. Some might require multiple\nidentical instances with autoscaling to perform well under load. Other\ncontainers might exist solely to run a one-time command. Still others may\nrequire a fixed ordering to enable selecting a single primary instance and\nproviding controlled failover to a secondary instance. Kubernetes provides\ndifferent \ncontroller\n resource types for each of those use cases. We’ll look at\neach in turn, but we’ll begin with the most fundamental of them, the \nPod\n,\nwhich is utilized by all of those use cases.\nPods\nA Pod is the most basic resource in Kubernetes and is how we run containers.\nEach Pod can have one or more containers within it. The Pod is used to\nprovide the process isolation we saw in \nChapter 2\n. Linux kernel namespaces\nare used at the Pod and the container level:\nmnt\n Mount points: each container has its own root filesystem; other mounts\nare available to all containers in the Pod.\nuts\n Unix time sharing: isolated at the Pod level.\nipc\n Interprocess communication: isolated at the Pod level.\npid\n Process identifiers: isolated at the container level.\nnet\n Network: isolated at the Pod level.\nThe biggest advantage of this approach is that multiple containers can act\nlike processes on the same virtual host, using the \nlocalhost\n address to\ncommunicate, while still being based on separate container images.\nDeploying a Pod\nTo get started, let’s create a Pod directly. Unlike the previous chapter, in\nwhich we used \nkubectl run\n to have the Pod specification created for us, we’ll\nspecify it directly using YAML so that we have complete control over the\nPod and to prepare us for using controllers to create Pods, providing\nscalability and failover.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nThe automation script for this chapter does a full cluster install with three\nnodes that run the control plane and regular applications, providing the\nsmallest possible highly available cluster for testing. The automation also\ncreates some YAML files for Kubernetes resources. Here’s a basic YAML\nresource to create a Pod running NGINX:\nnginx-pod.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\nPods are part of the \ncore\n Kubernetes API, so we just specify a version\nnumber of \nv1\n for the \napiVersion\n. Specifying \nPod\n as the \nkind\n tells Kubernetes\nexactly what resource we’re creating in the API group. We will see these\nfields in all of our Kubernetes resources.\nThe \nmetadata\n field has many uses. For the Pod, we just need to provide the\none required field of \nname\n. We don’t specify the \nnamespace\n in the metadata, so\nby default this Pod will end up in the \ndefault\n Namespace.\nThe remaining field, \nspec\n, tells Kubernetes everything it needs to know to\nrun this Pod. For now we are providing the minimal information, which is a\nlist of containers to run, but many other options are available. In this case, we\nhave only one container, so we provide just the name and container image\nKubernetes should use.\nLet’s add this Pod to the cluster. The automation added files to \n/opt\n, so we\ncan do it from \nhost01\n as follows:\nroot@host01:~# \nkubectl apply -f /opt/nginx-pod.yaml\npod/nginx created\nIn \nListing 7-1\n, we can check the Pod’s status.\nroot@host01:~# \nkubectl get pods -o wide\nNAME    READY   STATUS    RESTARTS   AGE     IP               NODE   ...\nnginx   1/1     Running   0          2m26s   172.31.25.202   host03 ...\nListing 7-1: Status of NGINX\nIt can take some time before the Pod shows \nRunning\n, especially if you just\nset up your Kubernetes cluster and it’s still busy deploying core components.\nKeep trying this \nkubectl\n command to check the status.\nInstead of typing the \nkubectl\n command multiple times, you can also use\nwatch\n. The \nwatch\n command is a great way to observe changes in your cluster\nover time. Just add \nwatch\n in front of your command, and it will be run for you\nevery two seconds.\nWe added \n-o wide\n to the command to see the IP address and node\nassignment for this Pod. Kubernetes manages that for us. In this case, the Pod\nwas scheduled on \nhost03\n, so we need to go there to see the running container:\nroot@host03:~# \ncrictl pods --name nginx\nPOD ID         CREATED         STATE  NAME   NAMESPACE  ...\n9f1d6e0207d7e  19 minutes ago  Ready  nginx  default    ...\nRun this command on whatever host your NGINX Pod is on.\nIf we collect the Pod ID, we can see the container as well:\nroot@host03:~# \nPOD_ID=$(crictl pods -q --name nginx)\nroot@host03:~# \ncrictl ps --pod $POD_ID\nCONTAINER      IMAGE          CREATED         STATE    NAME   ...\n9da09b3671418  4cdc5dd7eaadf  20 minutes ago  Running  nginx  ...\nThis output looks very similar to the output from \nkubectl get\n in \nListing 7-1\n,\nwhich is not surprising given that our cluster gets that information from the\nkubelet\n service running on this node, which in turn uses the same Container\nRuntime Interface (CRI) API that \ncrictl\n is also using to talk to the container\nengine.\nPod Details and Logging\nThe ability to use \ncrictl\n with the underlying container engine to explore a\ncontainer running in the cluster is valuable, but it does require us to connect\nto the specific host running the container. Much of the time, we can avoid\nthat by using \nkubectl\n commands to inspect Pods from anywhere by connecting\nto our cluster’s API server. Let’s move back to \nhost01\n and explore the NGINX\nPod further.\nIn \nChapter 6\n, we saw how we could use \nkubectl describe\n to see the status and\nevent log for a cluster node. We can use the same command to see the status\nand configuration details of other Kubernetes resources. Here’s the event log\nfor our NGINX Pod:\n root@host01:~# \nkubectl describe pod nginx\n Name:         nginx\n Namespace: \n➊\n default \n ...\n Containers:\n   nginx:\n     Container ID:   containerd://9da09b3671418...\n ...\n➋\n Type    Reason     Age   From               Message\n   ----    ------     ----  ----               -------\n   Normal  Scheduled  22m   default-scheduler  Successfully assigned ...\n   Normal  Pulling    22m   kubelet            Pulling image ""nginx""\n   Normal  Pulled     21m   kubelet            Successfully pulled image ...\n   Normal  Created    21m   kubelet            Created container nginx\n   Normal  Started    21m   kubelet            Started container nginx\nWe can use \nkubectl describe\n with many different Kubernetes resources, so we\nfirst tell \nkubectl\n that we are interested in a Pod and provide the name. Because\nwe didn’t specify a Namespace, Kubernetes will look for this Pod in the \ndefault\nNamespace \n➊\n.\nNOTE\nWe use the \ndefault\n Namespace for most of the examples in this book to save\ntyping, but it’s a good practice to use multiple Namespaces to keep\napplications separate, both to avoid naming conflicts and to manage\naccess control. We look at Namespaces in more detail in \nChapter 11\n.\nThe \nkubectl describe\n command output provides an event log \n➋\n, which is the\nfirst place to look for issues when we have problems starting a container.\nKubernetes takes a few steps when deploying a container. First, it needs to\nschedule it onto a node, which requires that node to be available with\nsufficient resources. Then, control passes to \nkubelet\n on that node, which has to\ninteract with the container engine to pull the image, create a container, and\nstart it.\nAfter the container is started, \nkubelet\n collects the standard out and standard\nerror. We can view this output by using the \nkubectl logs\n command:\nroot@host01:~# \nkubectl logs nginx\n...\n2021/07/13 22:37:03 [notice] 1#1: start worker processes\n2021/07/13 22:37:03 [notice] 1#1: start worker process 33\n2021/07/13 22:37:03 [notice] 1#1: start worker process 34\nThe \nkubectl logs\n command always refers to a Pod because Pods are the basic\nresource used to run containers, and our Pod has only one container, so we\ncan just specify the name of the Pod as a single parameter to \nkubectl logs\n. As\nbefore, Kubernetes will look in the \ndefault\n Namespace because we didn’t\nspecify the Namespace.\nThe container output is available even if the container has exited, so the",8821
40-Deployments.pdf,40-Deployments,"kubectl logs\n command is the place to look if a container is pulled and started\nsuccessfully but then crashes. Of course, we have to hope that the container\nprinted a log message explaining why it crashed. In \nChapter 10\n, we look at\nwhat to do if we can’t get a container going and don’t have any log messages.\nWe’re done with the NGINX Pod, so let’s clean it up:\nroot@host01:~# \nkubectl delete -f /opt/nginx-pod.yaml\npod ""nginx"" deleted\nWe can use the same YAML configuration file to delete the Pod, which is\nconvenient when we have multiple Kubernetes resources defined in a single\nfile, as a single command will delete all of them. The \nkubectl\n command uses\nthe name of each resource defined in the file to perform the delete.\nDeployments\nTo run a container, we need a Pod, but that doesn’t mean we generally want\nto create the Pod directly. When we create a Pod directly, we don’t get all of\nthe scalability and failover that Kubernetes offers, because Kubernetes will\nrun only one instance of the Pod. This Pod will be allocated to a node only on\ncreation, with no re-allocation even if the node fails.\nTo get scalability and failover, we instead need to create a controller to\nmanage the Pod for us. We’ll look at multiple controllers that can run Pods,\nbut let’s start with the most common: the \nDeployment\n.\nCreating a Deployment\nA Deployment manages one or more \nidentical\n Kubernetes Pods. When we\ncreate a Deployment, we provide a Pod template. The Deployment then\ncreates Pods matching that template with the help of a \nReplicaSet\n.\nDEPLOYMENTS AND REPLICASETS\nKubernetes has evolved its controller resources over time. The first type\nof controller, the \nReplicationController\n, provided only basic\nfunctionality. It was replaced by the ReplicaSet, which has\nimprovements in how it identifies which Pods to manage.\nPart of the reason to replace ReplicationControllers with ReplicaSets is\nthat ReplicationControllers were becoming more and more complicated,\nmaking the code difficult to maintain. The new approach splits up\ncontroller responsibility between ReplicaSets and Deployments.\nReplicaSets are responsible for basic Pod management, including\nmonitoring Pod status and performing failover. Deployments are\nresponsible for tracking changes to the Pod template caused by\nconfiguration changes or container image updates. Deployments and\nReplicaSets work together, but the Deployment creates its own\nReplicaSet, so we usually need to interact only with Deployments. For\nthis reason, I use the term \nDeployment\n generically to refer to features\nprovided by the ReplicaSet, such as monitoring Pods to provide the\nrequested number of replicas.\nHere’s the YAML file we’ll use to create an NGINX Deployment:\nnginx-deploy.yaml\n---\n kind: Deployment\n apiVersion: apps/v1 \n metadata:\n➊\n name: nginx \n spec:\n   replicas: 3 \n   selector: \n     matchLabels:\n       app: nginx\n   template:\n     metadata:\n    \n➋\n labels:\n         app: nginx\n  \n➌\n spec:   \n       containers:\n       - name: nginx\n         image: nginx\n      \n➍\n resources:\n           requests:\n             cpu: ""100m""\nDeployments are in the \napps\n API group, so we specify \napps/v1\n for \napiVersion\n.\nLike every Kubernetes resource, we need to provide a unique name \n➊\n to\nkeep this Deployment separate from any others we might create.\nThe Deployment specification has a few important fields, so let’s look at\nthem in detail. The \nreplicas\n field tells Kubernetes how many identical instances\nof the Pod we want. Kubernetes will work to keep this many Pods \nrunning.\nThe next field, \nselector\n, is used to enable the Deployment to find its Pods. The\ncontent of \nmatchLabels\n must exactly match the content in the \ntemplate.metadata.labels\nfield \n➋\n, or Kubernetes will reject the Deployment.\nFinally, the content of \ntemplate.spec\n \n➌\n will be used as the \nspec\n for any Pods\ncreated by this Deployment. The fields here can include any configuration we\ncan provide for a Pod. This configuration matches \nnginx-pod.yaml\n that we\nlooked at earlier except that we add a CPU resource request \n➍\n so that we can\nconfigure autoscaling later on.\nLet’s create our Deployment from this YAML resource file:\nroot@host01:~# \nkubectl apply -f /opt/nginx-deploy.yaml\ndeployment.apps/nginx created\nWe can track the status of the Deployment with \nkubectl get\n:\nroot@host01:~# \nkubectl get deployment nginx\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   3/3     3            3           4s\nWhen the Deployment is fully up, it will report that it has three replicas\nready and available, which means that we now have three separate NGINX\nPods managed by this Deployment:\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-6799fc88d8-6vn44   1/1     Running   0          18s\nnginx-6799fc88d8-dcwx5   1/1     Running   0          18s\nnginx-6799fc88d8-sh8qs   1/1     Running   0          18s\nThe name of each Pod begins with the name of the Deployment.\nKubernetes adds some random characters to build the name of the\nReplicaSet, followed by more random characters so that each Pod has a\nunique name. We don’t need to create or manage the ReplicaSet directly, but\nwe can use \nkubectl get\n to see it:\nroot@host01:~# \nkubectl get replicasets\nNAME               DESIRED   CURRENT   READY   AGE\nnginx-6799fc88d8   3         3         3       30s\nAlthough we generally interact only with Deployments, it is important to\nknow about the ReplicaSet, as some specific errors encountered when\ncreating Pods are only reported in the ReplicaSet event log.\nThe \nnginx\n prefix on the ReplicaSet and Pod names are purely for\nconvenience. The Deployment does not use names to match itself to Pods.\nInstead, it uses its selector to match the labels on the Pod. We can see these\nlabels if we run \nkubectl describe\n on one of the three Pods:\nroot@host01:~# \nkubectl describe pod \nnginx-6799fc88d8-6vn44\nName:         nginx-6799fc88d8-6vn44\nNamespace:    default\n...\nLabels:       app=nginx\n...\nThis matches the Deployment’s selector:\nroot@host01:~# \nkubectl describe deployment nginx\nName:                   nginx\nNamespace:              default\n...\nSelector:               app=nginx\n...\nThe Deployment queries the API server to identify Pods matching its\nselector. Whereas the Deployment uses the programmatic API, the \nkubectl get\ncommand in the following example generates a similar API server query,\ngiving us an opportunity to see how that works:\nroot@host01:~# \nkubectl get all -l app=nginx\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-6799fc88d8-6vn44   1/1     Running   0          69s\nnginx-6799fc88d8-dcwx5   1/1     Running   0          69s\nnginx-6799fc88d8-sh8qs   1/1     Running   0          69s\nNAME                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/nginx-6799fc88d8   3         3         3       69s\nUsing \nkubectl get all\n in this case allows us to list multiple different kinds of\nresources as long as they match the selector. As a result, we see not only the\nthree Pods but also the ReplicaSet that was created by the Deployment to\nmanage those Pods.\nIt may seem strange that the Deployment uses a selector rather than just\ntracking the Pods it created. However, this design makes it easier for\nKubernetes to be self-healing. At any time, a Kubernetes node might go\noffline, or we might have a network split, during which some control nodes\nlose their connection to the cluster. If a node comes back online, or the\ncluster needs to recombine after a network split, Kubernetes must be able to\nlook at the current state of all of the running Pods and figure out what\nchanges are required to achieve the desired state. This might mean that a\nDeployment that started an additional Pod as the result of a node\ndisconnection would need to shut down a Pod when that node reconnects so\nthat the cluster can maintain the appropriate number of replicas. Using a\nselector avoids the need for the Deployment to remember all the Pods it has\never created, even Pods on failed nodes.\nMonitoring and Scaling\nBecause the Deployment is monitoring its Pods to make sure we have the\ncorrect number of replicas, we can delete a Pod, and it will be automatically\nre-created:\nroot@host01:~# \nkubectl delete pod\n \nnginx-6799fc88d8-6vn44\npod ""nginx-6799fc88d8-6vn44"" deleted\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-6799fc88d8-dcwx5   1/1     Running   0          3m52s\nnginx-6799fc88d8-dtddk   1/1     Running   0        \n➊\n 14s\nnginx-6799fc88d8-sh8qs   1/1     Running   0          3m52s\nAs soon as the old Pod is deleted, the Deployment created a new Pod \n➊\n.\nSimilarly, if we change the number of replicas for the Deployment, Pods are\nautomatically updated. Let’s add another replica:\nroot@host01:~# \nkubectl scale --replicas=4 deployment nginx\ndeployment.apps/nginx scaled\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-6799fc88d8-dcwx5   1/1     Running   0          8m22s\nnginx-6799fc88d8-dtddk   1/1     Running   0          4m44s\nnginx-6799fc88d8-kk7r6   1/1     Running   0        \n➊\n 5s \nnginx-6799fc88d8-sh8qs   1/1     Running   0          8m22s\nThe first command sets the number of replicas to four. As a result,\nKubernetes needs to start a new identical Pod to meet the number we\nrequested \n➊\n. We can scale the Deployment by updating the YAML file and\nre-running \nkubectl apply\n, or we can use the \nkubectl scale\n command to edit the\nDeployment directly. Either way, this is a declarative approach; we are\nupdating the Deployment’s resource declaration; Kubernetes then updates the\nactual state of the cluster to match.\nSimilarly, scaling the Deployment down causes Pods to be automatically\ndeleted:\nroot@host01:~# \nkubectl scale --replicas=2 deployment nginx\ndeployment.apps/nginx scaled\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-6799fc88d8-dcwx5   1/1     Running   0          10m\nnginx-6799fc88d8-sh8qs   1/1     Running   0          10m\nWhen we scale down, Kubernetes selects two Pods to terminate. These\nPods take a moment to finish shutting down, at which point we have only two\nNGINX Pods running.\nAutoscaling\nFor an application that is receiving real requests from users, we would choose\nthe number of replicas necessary to provide a quality application, while\nscaling down when possible to reduce the amount of resources used by our\napplication. Of course, the load on our application is constantly changing,\nand it would be tedious to monitor each component of our application\ncontinually to scale it independently. Instead, we can have the cluster perform\nthe monitoring and scaling for us using a \nHorizontalPodAutoscaler\n. The term\nhorizontal\n in this case just refers to the fact that the autoscaler can update the\nnumber of replicas of the same Pod managed by a controller.\nTo configure autoscaling, we create a new resource with a reference to our\nDeployment. The cluster then monitors resources used by the Pods and\nreconfigures the Deployment as needed. We could add a\nHorizontalPodAutoscaler to our Deployment using the \nkubectl autoscale\ncommand, but using a YAML resource file so that we can keep the autoscale\nconfiguration under version control is better. Here’s the YAML file:\nnginx-scaler.yaml\n   ---\n➊\n apiVersion: autoscaling/v2\n   kind: HorizontalPodAutoscaler\n   metadata:\n     name: nginx\n     labels:\n       app: nginx\n   spec:\n  \n➋\n scaleTargetRef:\n       apiVersion: apps/v1\n       kind: Deployment\n       name: nginx\n  \n➌\n minReplicas: 1\n     maxReplicas: 10\n     metrics:\n       - type: Resource\n         resource:\n           name: cpu\n           target:\n             type: Utilization\n             averageUtilization: \n➍\n 50\nIn the \nmetadata\n field, we add the label \napp: nginx\n. This does not change the\nbehavior of the resource; its only purpose is to ensure that this resource\nshows up if we use an \napp=nginx\n label selector in a \nkubectl get\n command. This\nstyle of tagging the components of an application with consistent metadata is\na good practice to help others understand what resources go together and to\nmake debugging easier.\nThis YAML configuration uses version 2 of the autoscaler configuration\n➊\n. Providing new versions of API resource groups is how Kubernetes\naccommodates future capability without losing any of its backward\ncompatibility. Generally, alpha and beta versions are released for new\nresource groups before the final configuration is released, and there is at least\none version of overlap between the beta version and the final release to\nenable seamless upgrades.\nVersion 2 of the autoscaler supports multiple resources. Each resource is\nused to calculate a vote on the desired number of Pods, and the largest\nnumber wins. Adding support for multiple resources requires a change in the\nYAML layout, which is a common reason for the Kubernetes maintainers to\ncreate a new resource version.\nWe specify our NGINX Deployment \n➋\n as the target for the autoscaler\nusing its API resource group, kind, and name, which is enough to uniquely\nidentify any resource in a Kubernetes cluster. We then tell the autoscaler to\nmonitor the CPU utilization of the Pods that belong to the Deployment \n➍\n.\nThe autoscaler will work to keep average CPU utilization by the Pods close\nto 50 percent over the long run, scaling up or down as necessary. However,\nthe number of replicas will never go beyond the range we specify \n➌\n.\nLet’s create our autoscaler using this configuration:\nroot@host01:~# \nkubectl apply -f /opt/nginx-scaler.yaml\nhorizontalpodautoscaler.autoscaling/nginx created\nWe can query the cluster to see that it was created:\nroot@host01:~# \nkubectl get hpa\nNAME    REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nnginx   Deployment/nginx   0%/50%    1         10        3          96s\nThe output shows the autoscaler’s target reference, the current and desired\nresource utilization, and the maximum, minimum, and current number of\nreplicas.\nWe use \nhpa\n as an abbreviation for \nhorizontalpodautoscaler\n. Kubernetes allows us\nto use either singular or plural names and provides abbreviations for most of\nits resources to save typing. For example, we can type \ndeploy\n for \ndeployment\n and\neven \npo\n for \npods\n. Every extra keystroke counts!\nThe autoscaler uses CPU utilization data that the \nkubelet\n is already\ncollecting from the container engine. This data is centralized by the metrics\nserver we installed as a cluster add-on. Without that cluster add-on, there\nwould be no utilization data, and the autoscaler would not make any changes\nto the Deployment. In this case, because we’re not really using our NGINX\nserver instances, they aren’t consuming any CPU, and the Deployment is\nscaled down to a single Pod, the minimum we specified:\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-6799fc88d8-dcwx5   1/1     Running   0          15m\nThe autoscaler has calculated that only one Pod is needed and has scaled",15452
41-Other Controllers.pdf,41-Other Controllers,"the Deployment to match. The Deployment then selected a Pod to terminate\nto reach the desired scale.\nFor accuracy, the autoscaler will not use CPU data from the Pod if it\nrecently started running, and it has logic to prevent it from scaling up or\ndown too often, so if you ran through these examples very quickly you might\nneed to wait a few minutes before you see it scale.\nWe explore Kubernetes resource utilization metrics in more detail when\nwe look at limiting resource usage in \nChapter 14\n.\nOther Controllers\nDeployments are the most generic and commonly used controller, but\nKubernetes has some other useful options. In this section, we explore \nJob\ns\nand \nCronJob\ns, \nStatefulSets\n, and \nDaemonSets\n.\nJobs and CronJobs\nDeployments are great for application components because we usually want\none or more instances to stay running indefinitely. However, for cases for\nwhich we need to run a command, either once or on a schedule, we can use a\nJob. The primary difference is a Deployment ensures that any container that\nstops running is restarted, whereas a Job can check the exit code of the main\nprocess and restart only if the exit code is non-zero, indicating failure.\nA Job definition looks very similar to a Deployment:\nsleep-job.yaml\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: sleep\nspec:\n  template:\n    spec:\n      containers:\n      - name: sleep\n        image: busybox\n        command: \n          - ""/bin/sleep""\n          - ""30""\n      restartPolicy: OnFailure\nThe \nrestartPolicy\n can be set to \nOnFailure\n, in which case the container will be\nrestarted for a non-zero exit code, or to \nNever\n, in which case the Job will be\ncompleted when the container exits regardless of the exit code.\nWe can create and view the Job and the Pod it has created:\nroot@host01:~# \nkubectl apply -f /opt/sleep-job.yaml\njob.batch/sleep created\nroot@host01:~# \nkubectl get job\nNAME    COMPLETIONS   DURATION   AGE\nsleep   0/1           3s         3s\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\n...\nsleep-fgcnz              1/1     Running   0          10s\nThe Job has created a Pod per the specification provided in the YAML\nfile. The Job reflects \n0/1\n completions because it is waiting for its Pod to exit\nsuccessfully.\nWhen the Pod has been running for 30 seconds, it exits with a code of\nzero, indicating success, and the Job and Pod status are updated accordingly:\nroot@host01:~# \nkubectl get jobs\nNAME    COMPLETIONS   DURATION   AGE\nsleep   1/1           31s        40s\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS      RESTARTS   AGE\nnginx-65db7cf9c9-2wcng   1/1     Running     0          31m\nsleep-fgcnz              0/1     Completed   0          43s\nThe Pod is still available, which means that we could review its logs if\ndesired, but it shows a status of \nCompleted\n, so Kubernetes will not try to restart\nthe exited container.\nA CronJob is a controller that creates Jobs on a schedule. For example, we\ncould set up our sleep Job to run once per day:\nsleep-cronjob.yaml\n ---\n apiVersion: batch/v1\n kind: CronJob\n metadata:\n   name: sleep\n spec:\n➊\n schedule: ""0 3 * * *""\n➋\n jobTemplate: \n   spec:\n     template:\n       spec:\n         containers:\n           - name: sleep\n             image: busybox\n             command: \n               - ""/bin/sleep""\n               - ""30""\n         restartPolicy: OnFailure\nThe entire contents of the Job specification are embedded inside the\njobTemplate\n field \n➋\n. To this, we add a \nschedule\n \n➊\n that follows the standard\nformat for the Unix \ncron\n command. In this case, \n0 3 * * *\n indicates that a Job\nshould be created at 3:00 AM every day.\nOne of Kubernetes’ design principles is that anything could go down at\nany time. For a CronJob, if the cluster has an issue during the time the Job\nwould be scheduled, the Job might not be scheduled, or it might be scheduled\ntwice, this means that you should take care to write Jobs in an idempotent\nway so that they can handle missing or duplicated scheduling.\nIf we create this CronJob\nroot@host01:~# \nkubectl apply -f /opt/sleep-cronjob.yaml\n \ncronjob.batch/sleep created\nit now exists in the cluster, but it does not immediately create a Job or a Pod:\nroot@host01:~# \nkubectl get jobs\nNAME    COMPLETIONS   DURATION   AGE\nsleep   1/1           31s        2m32s\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS      RESTARTS   AGE\nnginx-65db7cf9c9-2wcng   1/1     Running     0          33m\nsleep-fgcnz              0/1     Completed   0          2m23s\nInstead, the CronJob will create a new Job each time its schedule is\ntriggered.\nStatefulSets\nSo far, we’ve been looking at controllers that create identical Pods. With both\nDeployments and Jobs, we don’t really care which Pod is which, or where it\nis deployed, as long as we run enough instances at the right time. However,\nthat doesn’t always match the behavior we want. For example, even though a\nDeployment can create Pods with persistent storage, the storage must either\nbe brand new for each new Pod, or the same storage must be shared across all\nPods. That doesn’t align well with a “primary and secondary” architecture\nsuch as a database. For those cases, we want specific storage to be attached to\nspecific Pods.\nAt the same time, because Pods can come and go due to hardware failures\nor upgrades, we need a way to manage the replacement of a Pod so that each\nPod is attached to the right storage. This is the purpose of a \nStatefulSet\n. A\nStatefulSet identifies each Pod with a number, starting at zero, and each Pod\nreceives matching persistent storage. When a Pod must be replaced, the new\nPod is assigned the same numeric identifier and is attached to the same\nstorage. Pods can look at their hostname to determine their identifier, so a\nStatefulSet is useful both for cases with a fixed primary instance as well as\ncases for which a primary instance is dynamically chosen.\nWe’ll explore a lot more details related to Kubernetes StatefulSets in the\nnext several chapters, including persistent storage and Services. For this\nchapter, we’ll look at a basic example of a StatefulSet and then build on it as\nwe introduce other important concepts.\nFor this simple example, let’s create two Pods and show how they each\nget unique storage that stays in place even if the Pod is replaced. We’ll use\nthis YAML resource:\nsleep-set.yaml\n ---\n apiVersion: apps/v1\n kind: StatefulSet\n metadata:\n    name: sleep\n spec:\n➊\n serviceName: sleep \n   replicas: 2\n   selector:\n     matchLabels:\n       app: sleep\n   template:\n     metadata:\n       labels:\n         app: sleep\n     spec:\n       containers:\n         - name: sleep\n           image: busybox\n           command: \n             - ""/bin/sleep""\n             - ""3600""\n        \n➋\n volumeMounts: \n             - name: sleep-volume\n               mountPath: /storagedir\n➌\n volumeClaimTemplates: \n     - metadata:\n         name: sleep-volume\n       spec:\n         storageClassName: longhorn\n         accessModes:\n           - ReadWriteOnce\n         resources:\n           requests:\n             storage: 10Mi\nThere are a few important differences here compared to a Deployment or\na Job. First, we must declare a \nserviceName\n to tie this StatefulSet to a\nKubernetes Service \n➊\n. This connection is used to create a Domain Name\nService (DNS) entry for each Pod. We must also provide a template for the\nStatefulSet to use to request persistent storage \n➌\n and then tell Kubernetes\nwhere to mount that storage in our container \n➋\n.\nThe actual \nsleep-set.yaml\n file that the automation scripts install includes\nthe \nsleep\n Service definition. We cover Services in detail in \nChapter 9\n.\nLet’s create the \nsleep\n StatefulSet:\nroot@host01:~# \nkubectl apply -f /opt/sleep-set.yaml\nThe StatefulSet creates two Pods:\nroot@host01:~# \nkubectl get statefulsets\nNAME    READY   AGE\nsleep   2/2     1m14s\nroot@host01:~# \nkubectl get pods\nNAME      READY   STATUS    RESTARTS   AGE\n...\nsleep-0   1/1     Running   0          57s\nsleep-1   1/1     Running   0          32s\nThe persistent storage for each Pod is brand new, so it starts empty. Let’s\ncreate some content. The easiest way to do that is from within the container\nitself, using \nkubectl exec\n, which allows us to run commands inside a container,\nsimilar to \ncrictl\n. The \nkubectl exec\n command works no matter what host the\ncontainer is on, even if we’re connecting to our Kubernetes API server from\noutside the cluster.\nLet’s write each container’s hostname to a file and print it out so that we\ncan verify it worked:\nroot@host01:~# \nkubectl exec sleep-0 -- /bin/sh -c \\n  \n'hostname > /storagedir/myhost'\nroot@host01:~# \nkubectl exec sleep-0 -- /bin/cat /storagedir/myhost\nsleep-0\nroot@host01:~# \nkubectl exec sleep-1 -- /bin/sh -c \\n  \n'hostname > /storagedir/myhost'\nroot@host01:~# \nkubectl exec sleep-1 -- /bin/cat /storagedir/myhost\nsleep-1\nEach of our Pods now has unique content in its persistent storage. Let’s\ndelete one of the Pods and verify that its replacement inherits its\npredecessor’s storage:\nroot@host01:~# \nkubectl delete pod sleep-0\npod ""sleep-0"" deleted\nroot@host01:~# \nkubectl get pods\nNAME      READY   STATUS    RESTARTS   AGE\n...\nsleep-0   1/1     Running   0          28s\nsleep-1   1/1     Running   0          8m18s\nroot@host01:~# \nkubectl exec sleep-0 -- /bin/cat /storagedir/myhost\nsleep-0\nAfter deleting \nsleep-0\n, we see a new Pod created with the same name, which\nis different from the Deployment for which a random name was generated for\nevery new Pod. Additionally, for this new Pod, the file we created previously\nis still present because the StatefulSet attached the same persistent storage to\nthe new Pod it created when the old one was deleted.\nDaemon Sets\nThe \nDaemonSet\n controller is like a StatefulSet in that the DaemonSet also\nruns a specific number of Pods, each with a unique identity. However, the\nDaemonSet runs exactly one Pod per node, which is useful primarily for\ncontrol plane and add-on components for a cluster, such as a network or\nstorage plug-in.\nOur cluster already has multiple DaemonSets installed, so let’s look at the\ncalico-node\n DaemonSet that’s already running, which runs on each node to\nprovide network configuration for all containers on that node.\nThe \ncalico-node\n DaemonSet is in the \ncalico-system\n Namespace, so we’ll specify\nthat Namespace to request information about the DaemonSet:\nroot@host01:~# \nkubectl -n calico-system get daemonsets\nNAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   ...\ncalico-node   3         3         3       3            3           ...\nOur cluster has three nodes, so the \ncalico-node\n DaemonSet has created three\ninstances. Here’s the configuration of this DaemonSet in YAML format:\nroot@host01:~# \nkubectl -n calico-system get daemonset calico-node -o yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n...\n  name: calico-node\n  namespace: calico-system\n...\nspec:\n...\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n...\nThe \n-o yaml\n parameter to \nkubectl get\n prints out the configuration and status of\none or more resources in YAML format, allowing us to inspect Kubernetes\nresources in detail.\nThe selector for this DaemonSet expects a label called \nk8s-app\n to be set to\ncalico-node\n. We can use this to show just the Pods that this DaemonSet creates:\nroot@host01:~# \nkubectl -n calico-system get pods \\n  \n-l k8s-app=calico-node -o wide\nNAME                READY   STATUS   ... NODE   ...\ncalico-node-h9kjh   1/1     Running  ... host01 ...\ncalico-node-rcfk7   1/1     Running  ... host03 ...\ncalico-node-wj876   1/1     Running  ... host02 ...",11993
42-8 OVERLAY NETWORKS.pdf,42-8 OVERLAY NETWORKS,"The DaemonSet has created three Pods, each of which is assigned to one\nof the nodes in our cluster. If we add additional nodes to our cluster, the\nDaemonSet will schedule a Pod on the new nodes as well.\nFinal Thoughts\nThis chapter explored Kubernetes from the perspective of a regular cluster\nuser, creating controllers that in turn create Pods with containers. Having this\ncore knowledge of controller resource types is essential for building our\napplications. At the same time, it’s important to remember that Kubernetes is\nusing the container technology we explored in \nPart I\n.\nOne key aspect of container technology is the ability to isolate containers\nin separate network namespaces. Running containers in a Kubernetes cluster\nadds additional requirements for networking because we now need to connect\ncontainers running on different cluster nodes. In the next chapter, we consider\nmultiple approaches to make this work as we look at overlay networks.",972
43-Cluster Networking.pdf,43-Cluster Networking,"8\nOVERLAY NETWORKS\nContainer networking is complex enough when all of the containers are on a\nsingle host, as we saw in \nChapter 4\n. When we scale up to a cluster of nodes,\nall of which run containers, the complexity increases substantially. Not only\nmust we provide each container with its own virtual network devices and\nmanage IP addresses, dynamically creating new network namespaces and\ndevices when containers are created, but we also need to ensure that\ncontainers on one node can communicate with containers on all the other\nnodes.\nIn this chapter, we’ll describe how \noverlay networks\n are used to provide\nthe appearance of a single container network across all nodes in a Kubernetes\ncluster. We’ll consider two different approaches for routing container traffic\nacross a host network, examining the network configuration and traffic flows\nfor each. Finally, we’ll explore how Kubernetes uses the Container Network\nInterface (CNI) standard to configure networking as a separate plug-in,\nmaking it easy to shift to new technology as it becomes available and\nallowing for custom solutions where needed.\nCluster Networking\nThe fundamental goal of a Kubernetes cluster is to treat a set of hosts\n(physical or virtual machines) as a single computing resource that can be\nallocated as needed to run containers. From a networking standpoint, this\nmeans Kubernetes should be able to schedule a Pod onto any node without\nworrying about connectivity to Pods on other nodes. It also means that\nKubernetes should have a way to dynamically allocate IP addresses to Pods\nin a way that supports that cluster-wide network connectivity.\nAs we’ll see in this chapter, Kubernetes uses a plug-in design to allow any\ncompatible network software to allocate IP addresses and provide cross-node\nnetwork connectivity. All plug-ins must follow a couple of important rules.\nFirst, Pod IP addresses should come from a single pool of IP addresses,\nalthough this pool can be subdivided by node. This means that we can treat\nall Pods as part of a single flat network, no matter where the Pods run.\nSecond, traffic should be routable such that all Pods can see all other Pods\nand the control plane.\nCNI Plug-ins\nPlug-ins communicate with the Kubernetes cluster, specifically with \nkubelet\n,\nusing the CNI standard. CNI specifies how \nkubelet\n finds and invokes CNI\nplug-ins. When a new Pod is created, \nkubelet\n first allocates the network\nnamespace. It then invokes the CNI plug-in, providing it a reference to the\nnetwork namespace. The CNI plug-in adds network devices to the\nnamespace, assigns an IP address, and passes that IP address back to \nkubelet\n.\nLet’s see that process in action. To do so, our examples for this chapter\ninclude two different environments with two different CNI plug-ins: Calico\nand WeaveNet. Both of these plug-ins provide networking for Pods but with\ndifferent cross-node networking. We’ll begin with the Calico environment.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nBy default, CNI plug-in information is kept in \n/etc/cni/net.d\n. We can see\nthe Calico configuration in that directory:\nroot@host01:~# \nls /etc/cni/net.d\n10-calico.conflist  calico-kubeconfig\nThe file \n10-calico.conflist\n contains the actual Calico configuration. The\nfile \ncalico-kubeconfig\n is used by Calico components to authenticate with the\ncontrol plane; it was created based on a service account created during Calico\ninstallation. The configuration filename has the \n10-\n prefix because \nkubelet\n sorts\nany configuration files it finds and uses the first one.\nListing 8-1\n shows the configuration file, which is in JSON format and\nidentifies the network plug-ins to use.\nroot@host01:~# \ncat /etc/cni/net.d/10-calico.conflist\n \n{\n  ""name"": ""k8s-pod-network"",\n  ""cniVersion"": ""0.3.1"",\n  ""plugins"": [\n    {\n      ""type"": ""calico"",\n...\n    },\n    {\n      ""type"": ""bandwidth"",\n      ""capabilities"": {""bandwidth"": true}\n    },\n    {""type"": ""portmap"", ""snat"": true, ""capabilities"": {""portMappings"": true}}\n  ]\n}\nListing 8-1: Calico configuration\nThe most important field is \ntype\n; it specifies which plug-in to run. In this\ncase, we’re running three plug-ins: \ncalico\n, which handles Pod networking;\nbandwidth\n, which we can use to configure network limits; and \nportmap\n, which is\nused to expose container ports to the host network. These two plug-ins inform\nkubelet\n of their purposes using the \ncapabilities\n field; as a result, when \nkubelet\ninvokes them, it passes in the relevant bandwidth and port mapping\nconfiguration so that the plug-in can make the necessary network\nconfiguration changes.\nTo run these plug-ins, \nkubelet\n needs to know where they are located. The\ndefault location for the actual plug-in executables is \n/opt/cni/bin\n, and the\nname of the plug-in matches the \ntype\n field:\nroot@host01:~# \nls /opt/cni/bin\nbandwidth  calico-ipam  flannel      install   macvlan  sbr     vlan\nbridge     dhcp         host-device  ipvlan    portmap  static\ncalico     firewall     host-local   loopback  ptp      tuning\nHere, we see a common set of network plug-ins that were installed by\nkubeadm\n along with our Kubernetes cluster. We also see \ncalico\n, which was\nadded to this directory by the Calico DaemonSet we installed after cluster\ninitialization.\nPod Networking\nLet’s look at an example Pod to get a glimpse of how the CNI plug-ins\nconfigure the Pod’s network namespace. The behavior is very similar to the\nwork we did in \nChapter 4\n, adding virtual network devices into network\nnamespaces to enable communication between containers and with the host\nnetwork.\nLet’s create a basic Pod:\npod.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod\nspec:\n  containers:\n  - name: pod\n    image: busybox\n    command: \n      - ""sleep""\n      - ""infinity""\n  nodeName: host01\nWe’ve added the extra field \nnodeName\n to force this Pod to run on \nhost01\n,\nwhich will make it easier to find and examine how its networking is\nconfigured.\nWe start the Pod via the usual command:\nroot@host01:~# \nkubectl apply -f /opt/pod.yaml\npod/pod created\nNext, check to see that it’s running:\nroot@host01:~# \nkubectl get pods\nNAME   READY   STATUS    RESTARTS   AGE\npod    1/1     Running   0          2m32s\nAfter it’s running, we can use \ncrictl\n to capture its unique ID:\nroot@host01:~# \nPOD_ID=$(crictl pods --name pod -q)\nroot@host01:~# \necho $POD_ID\nb7d2391320e07f97add7ccad2ad1a664393348f1dcb6f803f701318999ed0295\nAt this point, using the Pod ID, we can find its network namespace. In\nListing 8-2\n, we use \njq\n to extract only the data we want, just as we did in\nChapter 4\n. We’ll then assign it to a variable.\nroot@host01:~# \nNETNS_PATH=$(crictl inspectp $POD_ID |\n  \njq -r '.info.runtimeSpec.linux.namespaces[]|select(.type==""network"").path')\nroot@host01:~# \necho $NETNS_PATH\n/var/run/netns/cni-7cffed61-fb56-9be1-0548-4813d4a8f996\nroot@host01:~# \nNETNS=$(basename $NETNS_PATH)\nroot@host01:~# \necho $NETNS\ncni-7cffed61-fb56-9be1-0548-4813d4a8f996\nListing 8-2: Network namespace\nWe now can explore the network namespace to see how Calico set up the\nIP address and network routing for this Pod. First, as expected, this network\nnamespace is being used for our Pod:\nroot@host01:~# \nps $(ip netns pids $NETNS)\n    PID TTY      STAT   TIME COMMAND\n  35574 ?        Ss     0:00 /pause\n  35638 ?        Ss     0:00 sleep infinity\nWe see the two processes that we should expect. The first is a pause\ncontainer that is always created whenever we create a Pod. This is a\npermanent container to hold the network namespace. The second is our\nBusyBox container running \nsleep\n, as we configured in the Pod YAML file.\nNow, let’s see the configured network interfaces:\nroot@host03:~# \nip netns exec $NETNS ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN ...\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n3: \n➊\n eth0@if16: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 ... state UP ...\n    link/ether 7a:9e:6c:e2:30:47 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet \n➋\n 172.31.239.205/32 brd 172.31.25.202 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::789e:6cff:fee2:3047/64 scope link \n       valid_lft forever preferred_lft forever\nCalico has created the network device \neth0@if16\n in the network namespace\n➊\n and given it an IP address of \n172.31.239.205\n \n➋\n. Note that the network length\nfor that IP address is \n/32\n, which indicates that any traffic must go through a\nconfigured router. This is different from how our bridged container\nnetworking worked in \nChapter 4\n. It is necessary so that Calico can provide\nfirewall capabilities via network policies.\nThe choice of IP address for this Pod was ultimately up to Calico. Calico\nis configured with \n172.31.0.0/16\n for use as the IP address space for Pods. Calico\ndecides how to divide this address space up between nodes and then allocates\nIP addresses to each Pod from the range allocated to the node. Calico then\npasses this IP address back to \nkubelet\n so that it can update the Pod’s status:\nroot@host01:~# \nkubectl get pods -o wide\nNAME   READY   STATUS    RESTARTS   AGE   IP                NODE    ...\npod    1/1     Running   0          16m   172.31.239.205   host01   ...\nWhen Calico created the network interface in the Pod, it created it as part\nof a virtual Ethernet (veth) pair. The veth pair acts as a virtual network wire\nthat creates a connection to a network interface in the root namespace,\nallowing connections outside the Pod. \nListing 8-3\n lets us have a look at both\nhalves of the veth pair.\nroot@host01:~# \nip netns exec $NETNS ip link\n...\n3: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue ... \n    link/ether 6e:4c:3a:41:d0:54 brd ff:ff:ff:ff:ff:ff link-netnsid 0\nroot@host01:~# \nip link | grep -B 1 $NETNS\n13: cali9381c30abed@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 ... \n    link/ether ee:ee:ee:ee:ee:ee ... link-netns cni-7cffed61-fb56-9be1-0548-4813d4a8f996\nListing 8-3: Calico veth pair\nThe first command prints the network interfaces inside the namespace,\nwhereas the second prints the interfaces on the host. Each contains the field\nlink-netns\n pointing to the corresponding network namespace of the other\ninterface, showing that these two interfaces create a link between our Pod’s\nnamespace and the root namespace.",10850
44-Cross-Node Networking.pdf,44-Cross-Node Networking,"Cross-Node Networking\nSo far, the configuration of the virtual network devices in the container looks\nvery similar to the container networking in \nChapter 4\n, where there was no\nKubernetes cluster installed. The difference in this case is that the network\nplug-in is configured not just to connect containers on a single node, but to\nconnect containers running anywhere in the cluster.\nWHY NOT NAT?\nRegular container networking does, of course, provide connectivity to\nthe host network. However, as we’ve discussed, it accomplishes this\nusing Network Address Translation (NAT). This is fine for containers\nrunning individual client applications, as connection tracking enables\nLinux to route server responses all the way into the originating\ncontainer. It does not work for containers that need to act as servers,\nwhich is a key use case for a Kubernetes cluster.\nFor most private networks that use NAT to connect to a broader\nnetwork, port forwarding is used to expose specific services from within\nthe private network. That isn’t a good solution for every container in\nevery Pod, as we would quickly run out of ports to allocate. The\nnetwork plug-ins do end up using NAT, but only to connect containers\nacting as clients to make connections to networks outside the cluster. In\naddition, we will see port forwarding behavior in \nChapter 9\n, where it\nwill be one possible way to expose Services outside the cluster.\nThe challenge in cross-node networking is that the Pod network has a\ndifferent range of IP addresses from the host network, so the host network\ndoes not know how to route this traffic. There are a couple of different ways\nthat network plug-ins work around this. We’ll begin by continuing with our\ncluster running Calico. Then, we’ll show a different cross-node networking\ntechnology using WeaveNet.\nCalico Networking\nCalico performs cross-node networking using Layer 3 routing. This means\nthat it routes based on IP addresses, configuring IP routing tables on each\nhost and in the Pod to ensure that traffic is sent to the correct host and then to\nthe correct Pod. Thus, at the host level, we see the Pod IP addresses as the\nsource and destination. Because Calico relies on the built-in routing\ncapabilities of Linux, we don’t need to configure our host network switch to\nroute the traffic, but we do need to configure any security controls on the host\nnetwork switch to allow Pod IP addresses to travel across the network.\nTo explore Calico cross-node networking, it helps to have two Pods: one\non \nhost01\n and the other on \nhost02\n. We’ll use this resource file:\ntwo-pods.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\nspec:\n  containers:\n  - name: pod1\n    image: busybox\n    command: \n      - ""sleep""\n      - ""infinity""\n  nodeName: host01\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\nspec:\n  containers:\n  - name: pod2\n    image: busybox\n    command: \n      - ""sleep""\n      - ""infinity""\n  nodeName: host02\nAs always, these files have been loaded into the \n/opt\n directory by the\nautomated scripts for this chapter.\nThe \n---\n separator allows us to put two different Kubernetes resources in the\nsame file so that we can manage them together. The only difference in\nconfiguration with these two Pods is that they each have a \nnodeName\n field to\nensure that they are assigned to the correct node.\nLet’s delete our existing Pod and replace it with the two that we need:\nroot@host01:~# \nkubectl delete -f /opt/pod.yaml\npod ""pod"" deleted\nroot@host01:~# \nkubectl apply -f /opt/two-pods.yaml\n \npod/pod1 created\npod/pod2 created\nAfter these Pods are running, we’ll need to collect their IP addresses:\nroot@host01:~# \nIP1=$(kubectl get po pod1 -o json | jq -r '.status.podIP')\nroot@host01:~# \nIP2=$(kubectl get po pod2 -o json | jq -r '.status.podIP')\nroot@host01:~# \necho $IP1\n172.31.239.216\nroot@host01:~# \necho $IP2\n172.31.89.197\nWe’re able to extract the Pod IP using a simple \njq\n filter because our \nkubectl\nget\n command is guaranteed to return only one item. If we were running \nkubectl\nget\n without a filter, or with a filter that might match multiple Pods, the JSON\noutput would be a list and we would need to change the \njq\n filter accordingly.\nLet’s quickly verify that we have connectivity between these two Pods:\nroot@host01:~# \nkubectl exec -ti pod1 -- ping -c 3 $IP2\nPING 172.31.89.197 (172.31.89.197): 56 data bytes\n64 bytes from 172.31.89.197: seq=0 ttl=62 time=2.867 ms\n64 bytes from 172.31.89.197: seq=1 ttl=62 time=0.916 ms\n64 bytes from 172.31.89.197: seq=2 ttl=62 time=1.463 ms\n--- 172.31.89.197 ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 0.916/1.748/2.867 ms\nThe \nping\n command shows that all three packets arrived successfully, so we\nknow the Pods can communicate across nodes.\nAs in our earlier example, each of these Pods has a network interface with\na network length of \n/32\n, meaning that all traffic must go through a router. For\nexample, here is the IP configuration and route table for \npod1\n:\nroot@host01:~# \nkubectl exec -ti pod1 -- ip addr\n...\n3: eth0@if17: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1450 qdisc noqueue \n    link/ether f2:ed:e8:04:00:cc brd ff:ff:ff:ff:ff:ff\n    inet 172.31.239.216/32 brd 172.31.239.216 scope global eth0\n...\nroot@host01:~# \nkubectl exec -ti pod1 -- ip route\ndefault via 169.254.1.1 dev eth0 \n169.254.1.1 dev eth0 scope link\nBased on this configuration, when we run our \nping\n command, the\nnetworking stack recognizes that the destination IP is not local to any\ninterface. It therefore looks up \n169.254.1.1\n in its Address Resolution Protocol\n(ARP) table to determine where to send the “next hop.” If we try to find an\ninterface either in the container or on the host that has the address \n169.254.1.1\n,\nwe won’t be successful. Rather than actually assign that address to an\ninterface, Calico just configures “proxy ARP” so that the packet will be sent\nthrough the \neth0\n end of the veth pair. As a result, there is an entry for \n169.254.1.1\nin the ARP table inside the container:\nroot@host01:~# \nkubectl exec -ti pod1 -- arp -n\n? (169.254.1.1) at ee:ee:ee:ee:ee:ee [ether]  on eth0\n...\nAs shown in \nListing 8-3\n, the hardware address \nee:ee:ee:ee:ee:ee\n belongs to the\nhost side of the veth pair, so this is sufficient to get the packet out of the\ncontainer and into the root network namespace. From there, IP routing takes\nover.\nCalico has already configured the routing table to send packets to other\ncluster nodes based on the destination IP address range for that node and to\nsend packets to local containers based on their individual IP addresses. We\ncan see the result of this in the IP routing table on the host:\nroot@host01:~# \nip route\n...\n172.31.25.192/26 via 192.168.61.13 dev enp0s8 proto 80 onlink \n172.31.89.192/26 via 192.168.61.12 dev enp0s8 proto 80 onlink \n172.31.239.216 dev calice0906292e2 scope link \n...\nBecause the destination address for the ping is within the \n172.31.89.192/26\nnetwork, the packet now is routed to \n192.168.61.12\n, which is \nhost02\n.\nLet’s look at the routing table on \nhost02\n so that we can follow along with\nthe next step:\nroot@host02:~# \nip route\n...\n172.31.239.192/26 via 192.168.61.11 dev enp0s8 proto 80 onlink \n172.31.25.192/26 via 192.168.61.13 dev enp0s8 proto 80 onlink \n172.31.89.197 dev calibd2348b4f67 scope link \n...\nIf you want to run this command for yourself, make sure you run it from\nhost02\n. When our packet arrives at \nhost02\n, it has a route for the specific IP\naddress that is the destination of the \nping\n. This route sends the packet into the\nveth pair that is attached to the \npod2\n network namespace.\nNow that the ping has arrived, the network stack inside \npod2\n sends back a\nreply. The reply goes through the same process to reach the root network\nnamespace of \nhost02\n. Based on the \nhost02\n routing table, it is sent to \nhost01\n, \nwhere\na routing table entry for \n172.31.239.216\n is used to send it to the appropriate\ncontainer.\nBecause Calico is using Layer 3 routing, the host network sees the actual\ncontainer IP addresses. We can confirm that using \ntcpdump\n. We’ll switch back\nto \nhost01\n for this.\nFirst, let’s kick off \ntcpdump\n in the background:\nroot@host01:~# \ntcpdump -n -w pings.pcap -i any icmp &\n[1] 70949\ntcpdump: listening on any ...\nThe \n-n\n flag tells \ntcpdump\n to avoid trying to lookup hostnames in DNS for\nany IP addresses; this saves time. The \n-w pings.pcap\n flag tells \ntcpdump\n to write its\ndata to the file \npings.pcap\n; the \n-i any\n flag tells it to listen on all network\ninterfaces; the \nicmp\n filter tells it to listen only to ICMP traffic; and finally, \n&\n at\nthe end puts it in the background.\nThe \npcap\n filename extension is important because our Ubuntu host system\nwill only allow \ntcpdump\n to read files with that extension.\nNow, let’s run \nping\n again:\nroot@host01:~# \nkubectl exec -ti pod1 -- ping -c 3 $IP2\n...\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 0.928/0.991/1.115 ms\nThe ICMP requests and replies have been collected, but they are being\nbuffered in memory.\nTo get them dumped to the file, we’ll shut down \ntcpdump\n:\nroot@host01:~# \nkillall tcpdump\n12 packets captured\n12 packets received by filter\n0 packets dropped by kernel\nThere were three pings, and each ping consists of a request and a reply.\nThus, we might have expected six packets, but in fact we captured 12. To see\nwhy, let’s print the details of the packets that \ntcpdump\n collected:\nroot@host01:~# \ntcpdump -enr pings.pcap\nreading from file pings.pcap, link-type LINUX_SLL (Linux cooked v1)\n00:16:23...  In f2:ed:e8:04:00:cc \n➊\n ... 172.31.239.216 > 172.31.89.197: ICMP echo request ...\n00:16:23... Out 08:00:27:b7:ef:ef \n➋\n ... 172.31.239.216 > 172.31.89.197: ICMP echo request ...\n00:16:23...  In 08:00:27:fc:d2:36 \n➌\n ... 172.31.89.197 > 172.31.239.216: ICMP echo reply ...\n00:16:23... Out ee:ee:ee:ee:ee:ee \n➍\n ... 172.31.89.197 > 172.31.239.216: ICMP echo reply ...\n...\nThe \n-e\n flag to \ntcpdump\n prints the hardware addresses; otherwise, we\nwouldn’t be able to tell some of the packets apart. The first hardware address\n➊\n is the hardware address of \neth0\n inside the Pod. Next is the same packet\nagain, but this time the hardware address is the host interface \n➋\n. We \nthen see\nthe reply, first arriving at the host interface and labeled with the hardware\naddress for \nhost02\n \n➌\n. Finally, the packet is routed into the Calico network\ninterface corresponding to our Pod \n➍\n, and our \nping\n has made its round trip.\nWe’re now done with these two Pods, so let’s delete them:\nroot@host01:~# \nkubectl delete -f /opt/two-pods.yaml\npod ""pod1"" deleted\npod ""pod2"" deleted\nUsing Layer 3 routing is an elegant solution to cross-node networking for\na Kubernetes cluster, as it takes advantage of the routing and traffic\nforwarding capabilities that are native to Linux. However, it does mean that\nthe host network sees the Pods’ IP addresses, which may require security rule\nchanges. For example, the automated scripts that set up virtual machines in\nAmazon Web Services (AWS) for use with this book not only configure a\nsecurity group to allow all traffic in the Pod IP address space, but they also\nturn off the “source/destination check” for the virtual machine instances.\nOtherwise, the underlying AWS network infrastructure would refuse to pass\ntraffic with unexpected IP addresses to our cluster’s nodes.\nWeaveNet\nLayer 3 routing is not the only solution for cross-node networking. Another\noption is to “encapsulate” the container packets into a packet that is sent\nexplicitly host to host. This is the approach taken by popular network plug-\nins such as Flannel and WeaveNet. We’ll look at a WeaveNet example, but\nthe traffic using Flannel looks very similar.\nNOTE\nLarger clusters based on Calico also use encapsulation for some traffic\nbetween networks. For example, a cluster that spans multiple regions, or\nAvailability Zones, in AWS would likely need to configure Calico to use\nencapsulation, given that it may not be possible or practical to configure\nall of the routers between the regions or Availability Zones with the\nnecessary Pod IP routes for the cluster.\nBecause everything you might want to do in networking has some defined\nstandard, it’s not surprising that there is a standard for encapsulation: Virtual\nExtensible LAN (VXLAN). In VXLAN, each packet is wrapped in a UDP\ndatagram and sent to the destination.\nWe’ll use the same \ntwo-pods.yaml\n configuration file to create two Pods in\nour Kubernetes cluster, this time using a cluster built from the \nweavenet\ndirectory from this chapter’s examples. As before, we end up with one Pod\non \nhost01\n and the other on \nhost02\n:\nroot@host01:~# \nkubectl apply -f /opt/two-pods.yaml\npod/pod1 created\npod/pod2 created\nLet’s check that these Pods are running and allocated correctly to their\ndifferent hosts:\nroot@host01:~# \nkubectl get po -o wide\nNAME   READY   STATUS    ... IP           NODE     ...\npod1   1/1     Running   ... 10.46.0.8    host01   ...\npod2   1/1     Running   ... 10.40.0.21   host02   ...\nAfter these Pods are running, we can collect their IP addresses using the\nsame commands shown earlier:\nroot@host01:~# \nIP1=$(kubectl get po pod1 -o json | jq -r '.status.podIP')\nroot@host01:~# \nIP2=$(kubectl get po pod2 -o json | jq -r '.status.podIP')\nroot@host01:~# \necho $IP1\n10.46.0.8\nroot@host01:~# \necho $IP2\n10.40.0.21\nNote that the IP addresses assigned look nothing like the Calico example.\nFurther exploration shows that the address and routing configuration is also\ndifferent, as demonstrated in \nListing 8-4\n.\nroot@host01:~# \nkubectl exec -ti pod1 -- ip addr\n...\n25: eth0@if26: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1376 qdisc noqueue\n  \n    link/ether e6:78:69:44:3d:a4 brd ff:ff:ff:ff:ff:ff\n    inet 10.46.0.8/12 brd 10.47.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\n...\nroot@host01:~# \nkubectl exec -ti pod1 -- ip route\ndefault via 10.46.0.0 dev eth0 \n10.32.0.0/12 dev eth0 scope link  src 10.46.0.8\nListing 8-4: WeaveNet networking\nThis time, our Pods are getting IP addresses in a massive \n/12\n network,\ncorresponding to more than one million possible addresses on a single\nnetwork. In this case, our Pod’s networking stack is going to expect to be\nable to use ARP to directly identify the hardware address of any other Pod on\nthe network rather than routing traffic to a gateway as we saw with Calico.\nAs before, we do have connectivity between these two Pods:\nroot@host01:~# \nkubectl exec -ti pod1 -- ping -c 3 $IP2\nPING 10.40.0.21 (10.40.0.21): 56 data bytes\n64 bytes from 10.40.0.21: seq=0 ttl=64 time=0.981 ms\n64 bytes from 10.40.0.21: seq=1 ttl=64 time=0.963 ms\n64 bytes from 10.40.0.21: seq=2 ttl=64 time=0.871 ms\n--- 10.40.0.21 ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 0.871/0.938/0.981 ms\nAnd now that we’ve run this \nping\n command, we should expect that the\nARP table in the \npod1\n networking stack is populated with the hardware\naddress of the \npod2\n network interface:\nroot@host01:~# \nkubectl exec -ti pod1 -- arp -n\n? (10.40.0.21) at ba:75:e6:db:7c:c6 [ether]  on eth0\n? (10.46.0.0) at 1a:72:78:64:36:c6 [ether]  on eth0\nAs expected, \npod1\n has an ARP table entry for \npod2\n’s IP address,\ncorresponding to the virtual network interface inside \npod2\n:\nroot@host01:~# \nkubectl exec -ti pod2 -- ip addr\n...\n53: eth0@if54: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1376 qdisc noqueue\n  \n    link/ether \n➊\n ba:75:e6:db:7c:c6 brd ff:ff:ff:ff:ff:ff\n    inet 10.40.0.21/12 brd 10.47.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\n...\nThe hardware address in the \npod1\n ARP table matches the hardware address\nof the virtual network device in \npod2\n \n➊\n. To make this happen, WeaveNet is\nrouting the ARP request over the network so that the network stack in \npod2\ncan respond.\nLet’s look at how the cross-node routing of ARP and ICMP traffic is\nhappening. First, although the IP address management may be different, one\nimportant similarity between Calico and WeaveNet is that both are using veth\npairs to connect containers to the host. If you want to explore that, use the\ncommands shown in \nListing 8-2\n and \nListing 8-3\n to determine the network\nnamespace for \npod1\n, and then use \nip addr\n on \nhost01\n to verify that there is a \nveth\ndevice with a \nlink-netns\n field that corresponds to that network namespace.\nFor our purposes, because we’ve seen that before, we’ll take it as a given\nthat the traffic goes through the virtual network wire created by the veth pair\nand gets to the host. Let’s start there and trace the ICMP traffic between the\ntwo Pods.\nIf we use the same \ntcpdump\n capture as we did with Calico, we’ll be able to\ncapture the ICMP traffic, but that will get us only so far. Let’s go ahead and\nlook at that:\nroot@host01:~# \ntcpdump -w pings.pcap -i any icmp &\n[1] 55999\ntcpdump: listening on any, link-type LINUX_SLL (Linux cooked v1) ...\nroot@host01:~# \nkubectl exec -ti pod1 -- ping -c 3 $IP2\n...\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 0.824/1.691/3.053 ms\nroot@host01:~# \nkillall tcpdump\n24 packets captured\n24 packets received by filter\n0 packets dropped by kernel\nAs before, we ran \ntcpdump\n in the background to capture ICMP on all\nnetwork interfaces, ran our \nping\n, and then stopped \ntcpdump\n so that it would\nwrite out the packets it captured. This time we have 24 packets to look at, but\nthey still don’t tell the whole story:\nroot@host01:~# \ntcpdump -enr pings.pcap\nreading from file pings.pcap, link-type LINUX_SLL (Linux cooked v1)\n16:22:08.211499   P e6:78:69:44:3d:a4 ... 10.46.0.8 > 10.40.0.21: ICMP echo request ...\n16:22:08.211551 Out e6:78:69:44:3d:a4 ... 10.46.0.8 > 10.40.0.21: ICMP echo request ...\n16:22:08.211553   P e6:78:69:44:3d:a4 ... 10.46.0.8 > 10.40.0.21: ICMP echo request ...\n16:22:08.211745 Out e6:78:69:44:3d:a4 ... 10.46.0.8 > 10.40.0.21: ICMP echo request ...\n16:22:08.212917   P ba:75:e6:db:7c:c6 ... 10.40.0.21 > 10.46.0.8: ICMP echo reply ...\n16:22:08.213704 Out ba:75:e6:db:7c:c6 ... 10.40.0.21 > 10.46.0.8: ICMP echo reply ...\n16:22:08.213708   P ba:75:e6:db:7c:c6 ... 10.40.0.21 > 10.46.0.8: ICMP echo reply ...\n16:22:08.213724 Out ba:75:e6:db:7c:c6 ... 10.40.0.21 > 10.46.0.8: ICMP echo reply ...\n...\nThese lines show four packets for a single \nping\n request and reply, but the\nhardware addresses aren’t changing. What’s happening is that these ICMP\npackets are being handed between network interfaces unmodified. However,\nwe’re still not seeing the actual traffic that’s going between \nhost01\n and \nhost02\n,\nbecause we never see any hardware addresses that correspond to host\ninterfaces.\nTo see the host-level traffic, we need to tell \ntcpdump\n to capture UDP and\nthen treat it as VXLAN, which enables \ntcpdump\n to identify the fact that an\nICMP packet is inside.\nLet’s start the capture again, this time looking for UDP traffic:\nroot@host01:~# \ntcpdump -w vxlan.pcap -i any udp &\n...\nroot@host01:~# \nkubectl exec -ti pod1 -- ping -c 3 $IP2\n...\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 1.139/1.364/1.545 ms\nroot@host01:~# \nkillall tcpdump\n22 packets captured\n24 packets received by filter\n0 packets dropped by kernel\nThis time we saved the packet data in \nvxlan.pcap\n. In this example, \ntcpdump\ncaptured 22 packets. Because there is lots of cross-Pod traffic in our cluster,\nnot just ICMP traffic, you might see a different number.\nThe packets we captured cover all of the UDP traffic on \nhost01\n, not just our\nICMP, so in printing out the packets shown in \nListing 8-5\n, we’ll need to be\nselective.\nroot@host01:~# \ntcpdump -enr vxlan.pcap -T vxlan | grep -B 1 ICMP\nreading from file vxlan.pcap, link-type LINUX_SLL (Linux cooked v1)\n16:45:47.307949 Out 08:00:27:32:a0:28 ... \n  length 150: 192.168.61.11.50200 > 192.168.61.12.6784: VXLAN ...\ne6:78:69:44:3d:a4 > ba:75:e6:db:7c:c6 ... \n  length 98: 10.46.0.8 > 10.40.0.21: ICMP echo request ...\n16:45:47.308699  In 08:00:27:67:b9:da ... \n  length 150: 192.168.61.12.43489 > 192.168.61.11.6784: VXLAN ... \nba:75:e6:db:7c:c6 > e6:78:69:44:3d:a4 ... \n  length 98: 10.40.0.21 > 10.46.0.8: ICMP echo reply ...\n16:45:48.308240 Out 08:00:27:32:a0:28 ... \n  length 150: 192.168.61.11.50200 > 192.168.61.12.6784: VXLAN ... \n...\nListing 8-5: VXLAN capture\nThe \n-T vxlan\n flag tells \ntcpdump\n to treat the packet data it sees as VXLAN\ndata. This causes \ntcpdump\n to look inside and pull out data from the\nencapsulated packets, enabling it to identify ICMP packets when those are\nhidden inside. We then use \ngrep\n with a \n-B 1\n flag to find those ICMP packets\nand also print the line immediately previous so that we can see the VXLAN\nwrapper.\nThis capture shows the host’s hardware address, which informs us that\nwe’ve managed to capture the traffic moving between hosts. Each ICMP\npacket is wrapped in a UDP datagram and sent across the host network. The\nIP source and destination for these datagrams are the host network IP\naddresses \n192.168.61.11\n and \n192.168.61.12\n, so the host network never sees the Pod\nIP addresses. However, that information is still there, in the encapsulated\nICMP packet, thus when the datagram arrives at its destination, WeaveNet\ncan send the ICMP packet to the correct destination.\nThe advantage of encapsulation is that all of our cross-node traffic looks",22021
45-Network Customization.pdf,45-Network Customization,"like ordinary UDP datagrams between hosts. Typically, we don’t need to do\nany additional network configuration to allow this traffic. However, we do\npay a price. As you can see in \nListing 8-5\n, each ICMP packet is 98 bytes, but\nthe encapsulated packet is 150 bytes. The wrapper needed for encapsulation\ncreates network overhead that we have to pay with each packet we send.\nLook back at \nListing 8-4\n for another consequence. The virtual network\ninterface inside the Pod has a maximum transmission unit (MTU) of 1,376.\nThis represents the largest packet that can be sent; anything bigger must to be\nfragmented into multiple packets and reassembled at the destination. This\nMTU of 1,376 is considerably smaller than the standard MTU of 1,500 on\nour host network. The smaller MTU on the Pod interface ensures that the\nPod’s network stack will do any required fragmenting. This way, we can\nguarantee that we don’t exceed 1,500 at the host layer, even after the wrapper\nis added. For this reason, if you are using a network plug-in that uses\nencapsulation, it might be worth exploring how to configure jumbo frames to\nenable an MTU larger than 1,500 on the host network.\nChoosing a Network Plug-in\nNetwork plug-ins can use different approaches to cross-node networking. As\nis universal in engineering, though, there are trade-offs with each approach.\nLayer 3 routing uses native capabilities of Linux and is efficient in its use of\nthe network bandwidth, but it may require customization of the underlying\nhost network. Encapsulation with VXLAN works in any network where we\ncan send UDP datagrams between hosts, but it adds overhead with each\npacket.\nEither way, however, our Pods are getting what they need, which is the\nability to communicate with other Pods, wherever in the cluster they may be.\nAnd in practice, the configuration effort and performance difference tends to\nbe small. For this reason, the best way to choose a network plug-in is to start\nwith the plug-in that is recommended for or installed by default with your\nparticular Kubernetes distribution. If you find specific use cases for which the\nperformance doesn’t meet your requirements, you’ll then be able to test an\nalternative plug-in based on real network traffic rather than guesswork.\nNetwork Customization\nSome scenarios may require cluster networking that is more complex than a\nsingle Pod network connected across all cluster nodes. For example, some\nregulated industries require certain data, such as security audit logs, to travel\nacross a separated network. Other systems may have specialized hardware so\nthat application components that interface with that hardware must be placed\non a specific network or virtual LAN (VLAN).\nOne of the advantages of a plug-in architecture for networking is that a\nKubernetes cluster can accommodate these specialized networking scenarios.\nAs long as Pods have an interface that can reach (and is reachable from) the\nrest of the cluster, Pods can have additional network interfaces that provide\nspecialized connectivity.\nLet’s look at an example. We’ll configure two Pods on the same node so\nthey have a local host-only network they can use for intercommunication.\nBeing a host-only network, it doesn’t provide connectivity to the rest of the\ncluster, so we’ll also use Calico to provide cluster networking for Pods.\nBecause of the need to configure both Calico and our host-only network,\nwe’ll be invoking two separate CNI plug-ins that will create virtual network\ninterfaces in our Pods’ network namespaces. As we saw in \nListing 8-1\n, it’s\npossible to configure multiple CNI plug-ins in a single configuration file.\nHowever, \nkubelet\n expects only one of these CNI plug-ins to actually assign a\nnetwork interface and IP address. To work around this, we’ll use Multus, a\nCNI plug-in that is designed to invoke multiple plug-ins but will treat one as\nprimary for purposes of reporting IP address information back to \nkubelet\n.\nMultus also allows us to be selective as to what CNI plug-ins are applied to\neach Pod.\nWe’ll begin by installing Multus into the \ncalico\n example cluster for this\nchapter:\nroot@host01:~# \nkubectl apply -f /opt/multus-daemonset.yaml\ncustomresourcedefinition.../network-attachment-definitions... created\nclusterrole.rbac.authorization.k8s.io/multus created\nclusterrolebinding.rbac.authorization.k8s.io/multus created\nserviceaccount/multus created\nconfigmap/multus-cni-config created\ndaemonset.apps/kube-multus-ds created\nAs the filename implies, the primary resource in this YAML file is a\nDaemonSet that runs a Multus container on every host. However, this file\ninstalls several other resources, including a \nCustomResourceDefinition\n. This\nCustomResourceDefinition will allow us to configure network attachment\nresources to tell Multus what CNI plug-ins to use for a given Pod.\nWe’ll look at CustomResourceDefinitions in detail in \nChapter 17\n. For\nnow, in \nListing 8-6\n we’ll just see the NetworkAttachmentDefinition that\nwe’ll use to configure Multus.\nnetattach.yaml\n---\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: macvlan-conf\nspec:\n  config: '{\n      ""cniVersion"": ""0.3.0"",\n      ""type"": ""macvlan"",\n      ""mode"": ""bridge"",\n      ""ipam"": {\n        ""type"": ""host-local"",\n        ""subnet"": ""10.244.0.0/24"",\n        ""rangeStart"": ""10.244.0.1"",\n        ""rangeEnd"": ""10.244.0.254""\n      }\n    }'\nListing 8-6: Network attachment\nThe \nconfig\n field in the \nspec\n looks a lot like a CNI configuration file, which\nisn’t surprising, as Multus needs to use this information to invoke the \nmacvlan\nCNI plug-in when we ask for it to be added to a Pod.\nWe need to add this NetworkAttachmentDefinition to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/netattach.yaml\n \nnetworkattachmentdefinition.k8s.cni.cncf.io/macvlan-conf created\nThis definition doesn’t immediately affect any of our Pods; it just provides\na Multus configuration for future use.\nOf course, to use this configuration, Multus must be invoked. How does\nthat happen when we’ve already installed Calico into this cluster? The answer\nis in the \n/etc/cni/net.d\n directory, which the Multus DaemonSet modified on\nall of our cluster nodes as part of its initialization:\nroot@host01:~# \nls /etc/cni/net.d\n00-multus.conf  10-calico.conflist  calico-kubeconfig  multus.d\nMultus left the existing Calico configuration files in place, but added its\nown \n00-multus.conf\n configuration file and a \nmultus.d\n directory. Because the\n00-multus.conf\n file is ahead of \n10-calico.conflist\n in an alphabetic sort, \nkubelet\nwill start to use it the next time it creates a new Pod.\nHere’s \n00-multus.conf\n:\n00-multus.conf\n{\n  ""cniVersion"": ""0.3.1"",\n  ""name"": ""multus-cni-network"",\n  ""type"": ""multus"",\n  ""capabilities"": {\n    ""portMappings"": true,\n    ""bandwidth"": true\n  },\n  ""kubeconfig"": ""/etc/cni/net.d/multus.d/multus.kubeconfig"",\n  ""delegates"": [\n    {\n      ""name"": ""k8s-pod-network"",\n      ""cniVersion"": ""0.3.1"",\n      ""plugins"": [\n        {\n          ""type"": ""calico"",\n...\n          }\n        },\n        {\n          ""type"": ""bandwidth"",\n...\n        },\n        {\n          ""type"": ""portmap"",\n...\n        }\n      ]\n    }\n  ]\n}\nThe \ndelegates\n field is pulled from the Calico configuration that Multus\nfound. This field is used to determine the default CNI plug-ins that Multus\nalways uses when it is invoked. The top-level \ncapabilities\n field is needed to\nensure that Multus will get all the correct configuration data from \nkubelet\n to be\nable to invoke the \nportmap\n and \nbandwidth\n plug-ins.\nNow that Multus is fully set up, let’s use it to add a host-only network to\ntwo Pods. The Pods are defined as follows:\nlocal-pods.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\n  annotations:\n    k8s.v1.cni.cncf.io/networks: macvlan-conf\nspec:\n  containers:\n  - name: pod1\n    image: busybox\n    command: \n      - ""sleep""\n      - ""infinity""\n  nodeName: host01\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\n  annotations:\n    k8s.v1.cni.cncf.io/networks: macvlan-conf\nspec:\n  containers:\n  - name: pod2\n    image: busybox\n    command: \n      - ""sleep""\n      - ""infinity""\n  nodeName: host01\nThis time we need both Pods to wind up on \nhost01\n so that the host-only\nnetworking functions. In addition, we add the \nk8s.v1.cni.cncf.io/networks\nannotation to each Pod. Multus uses this annotation to identify what\nadditional CNI plug-ins it should run. The name \nmacvlan-conf\n matches the name\nwe provided in the NetworkAttachmentDefinition in \nListing 8-6\n.\nLet’s create these two Pods:\nroot@host01:~# \nkubectl apply -f /opt/local-pods.yaml\npod/pod1 created\npod/pod2 created\nAfter these Pods are running, we can check that they each have an extra\nnetwork interface:\nroot@host01:~# \nkubectl exec -ti pod1 -- ip addr\n...\n3: eth0@if12: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1450 qdisc noqueue \n    link/ether 9a:a1:db:ec:c7:91 brd ff:ff:ff:ff:ff:ff\n    inet 172.31.239.198/32 brd 172.31.239.198 scope global eth0\n       valid_lft forever preferred_lft forever\n...\n4: net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue \n    link/ether 9e:4f:c4:47:40:07 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.0.2/24 brd 10.244.0.255 scope global net1\n       valid_lft forever preferred_lft forever\n...\nroot@host01:~# \nkubectl exec -ti pod2 -- ip addr\n...\n3: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1450 qdisc noqueue \n    link/ether 52:08:99:a7:d2:bc brd ff:ff:ff:ff:ff:ff\n    inet 172.31.239.199/32 brd 172.31.239.199 scope global eth0\n       valid_lft forever preferred_lft forever\n...\n4: net1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue \n    link/ether a6:e5:01:82:81:82 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.0.3/24 brd 10.244.0.255 scope global net1\n       valid_lft forever preferred_lft forever\n...\nThe \nmacvlan\n CNI plug-in has added the additional \nnet1\n network interface,\nusing the IP address management configuration we provided in the\nNetworkAttachmentDefinition.\nThese two Pods are now able to communicate with each other using these\ninterfaces:\nroot@host01:~# \nkubectl exec -ti pod1 -- ping -c 3 10.244.0.3\nPING 10.244.0.3 (10.244.0.3): 56 data bytes\n64 bytes from 10.244.0.3: seq=0 ttl=64 time=3.125 ms\n64 bytes from 10.244.0.3: seq=1 ttl=64 time=0.192 ms\n64 bytes from 10.244.0.3: seq=2 ttl=64 time=0.085 ms\n--- 10.244.0.3 ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 0.085/1.134/3.125 ms",10737
46-9 SERVICE AND INGRESS NETWORKS.pdf,46-9 SERVICE AND INGRESS NETWORKS,"This communication goes over the bridge created by the \nmacvlan\n CNI plug-\nin, as opposed to travelling via Calico.\nKeep in mind that our purpose here is solely to demonstrate custom\nnetworking without requiring any particular VLAN or complex setup outside\nour cluster hosts. For a real cluster, this kind of host-only network is of\nlimited value because it constrains where Pods can be deployed. In this kind\nof situation, it might be preferable to place the two containers into the same\nPod so that they will always be scheduled together and can use \nlocalhost\n to\ncommunicate.\nFinal Thoughts\nWe’ve looked at a lot of network interfaces and traffic flows in this chapter.\nMost of the time, it’s enough to know that every Pod in the cluster is\nallocated an IP address from a Pod network, and also that any Pod in the\ncluster can reach and is reachable from any other Pod. Any of the Kubernetes\nnetwork plug-ins provide this capability, whether they use Layer 3 routing or\nVXLAN encapsulation, or possibly both.\nAt the same time, networking issues do occur in a cluster, and it’s\nessential for cluster administrators and cluster users to understand how the\ntraffic is flowing between hosts and what that traffic looks like to the host\nnetwork in order to debug issues with switch and host configuration, or\nsimply to build applications that make best use of the cluster.\nWe’re not yet done with the networking layers that are needed to have a\nfully functioning Kubernetes cluster. In the next chapter, we’ll look at how\nKubernetes provides a Service layer on top of Pod networking to provide\nload balancing and automated failover, and then uses the Service networking\nlayer together with Ingress networking to make container services accessible\noutside the cluster.",1795
47-Services.pdf,47-Services,"9\nSERVICE AND INGRESS NETWORKS\nA decent amount of complexity was involved in creating a cluster-wide\nnetwork so that all of our Pods could communicate with one another. At the\nsame time, we still don’t have all of the networking functionality we need to\nbuild scalable, resilient applications. We need networking that supports load\nbalancing our application components across multiple instances and provides\nthe ability to send traffic to new Pod instances as existing instances fail or\nneed to be upgraded. Additionally, the Pod network is designed to be private,\nmeaning that it is directly reachable only from within the cluster. We need\nadditional traffic routing so that external users can reach our application\ncomponents running in containers.\nIn this chapter, we’ll look at Service and Ingress networking. Kubernetes\nService networking provides an entire additional networking layer on top \nof\nPod networking, including dynamic discovery and load balancing. We’ll see\nhow this networking layer works and how we can use it to expose our\napplication components to the rest of the cluster as scalable, resilient services.\nWe’ll then look at how Ingress configuration provides traffic routing for\nthese Services to expose them to external users.\nServices\nPutting together Deployments and overlay networking, we have the ability to\ncreate multiple identical container instances with a unique IP address for\neach. Let’s create an NGINX Deployment to illustrate:\nnginx-deploy.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: nginx\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\nThis is similar to Deployments we’ve seen previously. In this case we’re\nasking Kubernetes to maintain five Pods for us, each running an NGINX web\nserver.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nThe automated scripts have already placed this file in \n/opt\n, so we can\napply it to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/nginx-deploy.yaml\ndeployment.apps/nginx created\nAfter these Pods are running, we can check that they’ve been distributed\nacross the cluster and each one has an IP address:\nroot@host01:~# \nkubectl get pods -o wide\nNAME                     READY   STATUS    ... IP                NODE    ...\nnginx-6799fc88d8-2wqc7   1/1     Running   ... 172.31.239.231   host01   ...\nnginx-6799fc88d8-78bwx   1/1     Running   ... 172.31.239.229   host01   ...\nnginx-6799fc88d8-dtx7s   1/1     Running   ... 172.31.89.240    host02   ...\nnginx-6799fc88d8-wh479   1/1     Running   ... 172.31.239.230   host01   ...\nnginx-6799fc88d8-zwx27   1/1     Running   ... 172.31.239.228   host01   ...\nIf these containers were merely clients of some server, that might be all\nwe need to do. For example, if our application architecture was driven by\nsending and receiving messages, as long as these containers could connect to\nthe messaging server, they’d be able to function as required. However,\nbecause these containers act as servers, clients need to be able to find them\nand connect.\nAs it is, our separate NGINX instances aren’t very practical for clients to\nuse. Sure, it’s possible to connect to any one of these NGINX server Pods\ndirectly. For example, we can communicate with the first one in the list using\nits IP address:\nroot@host01:~# \ncurl -v http://\n172.31.239.231\n*   Trying 172.31.239.231:80...\n* Connected to 172.31.239.231 (172.31.239.231) port 80 (#0)\n> GET / HTTP/1.1\n...\n< HTTP/1.1 200 OK\n< Server: nginx/1.21.3\n...\nUnfortunately, just choosing one instance is not going to provide load\nbalancing or failover. Additionally, we don’t have any way of knowing ahead\nof time what the Pod IP address is going to be, and every time we make any\nchanges to the Deployment, the Pods will be re-created and get new IP\naddresses.\nThe solution to this situation needs to have two main features. First, we\nneed to have a well-known name that clients can use to find a server. Second,\nwe need a consistent IP address so that when a client has identified a server,\nit can continue to use the same address for connections even as Pod instances\ncome and go. This is exactly what Kubernetes provides with a \nService\n.\nCreating a Service\nLet’s create a Service for our NGINX Deployment and see what that gets us.\nListing 9-1\n presents the resource YAML file.\nnginx-service.yaml\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: nginx\nspec:\n  selector:\n    app: nginx\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\nListing 9-1: NGINX Service\nFirst, a Service has a \nselector\n much like a Deployment. This selector is used\nin the same way: to identify the Pods that will be associated with the Service.\nHowever, unlike a Deployment, a Service does not manage its Pods in any\nway; it simply routes traffic to them.\nThe traffic routing is based on the ports we identify in the \nports\n field.\nBecause the NGINX server is listening on port 80, we need to specify that as\nthe \ntargetPort\n. We can use any \nport\n we want, but it’s simplest to keep it the same,\nespecially as 80 is the default port for HTTP.\nLet’s apply this Service to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/nginx-service.yaml\n \nservice/nginx created\nWe can now see that the Service was created:\nroot@host01:~# \nkubectl get services\nNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   14d\nnginx        ClusterIP   10.100.221.220   <none>        80/TCP    25s\nThis \nnginx\n Service has the default type of \nClusterIP\n. Kubernetes has\nautomatically assigned a cluster IP address for this Service. The IP address is\nin an entirely different address space from that of our Pods.\nUsing the selector, this Service will identify our NGINX server Pods and\nautomatically start load balancing traffic to them. As Pods matching the\nselector come and go, the Service will automatically update its load balancing\naccordingly. As long as the Service exists, it will keep the same IP address,\nso clients have a consistent way of finding our NGINX server instances.\nLet’s verify that we can reach an NGINX server through the Service:\nroot@host01:~# \ncurl -v http://\n10.100.221.220\n*   Trying 10.100.221.220:80...\n* Connected to 10.100.221.220 (10.100.221.220) port 80 (#0)\n> GET / HTTP/1.1\n...\n< HTTP/1.1 200 OK\n< Server: nginx/1.21.3\n...\nWe can see that the Service has correctly identified all five NGINX Pods:\nroot@host01:~# \nkubectl describe service nginx\nName:              nginx\nNamespace:         default\n...\nSelector:          app=nginx\n...\nEndpoints:         172.31.239.228:80,172.31.239.229:80,172.31.239.230:80 \n+ 2 more...\n...\nThe \nEndpoints\n field shows that the Service is currently routing traffic to all\nfive NGINX Pods. As a client, we don’t need to know which Pod was used to\nhandle our request. We interact solely with the Service IP address and allow\nthe Service to choose an instance for us.\nOf course, for this example, we had to look up the IP address of the\nService. To make it easier on clients, we still should provide a well-known\nname.\nService DNS\nKubernetes provides a well-known name for each Service through a DNS\n(Domain Name System) server that is dynamically updated with the name\nand IP address of every Service in the cluster. Each Pod is configured with\nthis DNS server such that a Pod can use the name of the Service to connect to\nan instance.\nLet’s create a Pod that we can use to try this out:\npod.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod\nspec:\n  containers:\n  - name: pod\n    image: alpine\n    command: \n      - ""sleep""\n      - ""infinity""\nWe’re using \nalpine\n rather than \nbusybox\n as the image for this Pod because\nwe’ll want to use some DNS commands that require us to install a more full-\nfeatured DNS client.\nNOTE\nBusyBox makes a great debug image for Kubernetes clusters because it’s\ntiny and has many useful commands. However, in the interest of keeping\nBusyBox tiny, it’s typical for the commands to include only the most\npopular options. Alpine makes a great alternative for debugging. The\ndefault Alpine image uses BusyBox to provide many of its initial\ncommands, but it’s possible to replace them with a full-featured\nalternative by just installing the appropriate package.\nNext, create the Pod:\nroot@host01:~# \nkubectl apply -f /opt/pod.yaml\n \npod/pod created\nAfter it’s running, let’s use it to connect to our NGINX Service, as\ndemonstrated in \nListing 9-2\n.\nroot@host01:~# \nkubectl exec -ti pod -- wget -O - http://nginx\nConnecting to nginx (10.100.221.220:80)\n...\n<title>Welcome to nginx!</title>\n...\nListing 9-2: Connect to NGINX Service\nWe were able to use the name of the Service, \nnginx\n, and that name resolved\nto the Service IP address. This worked because our Pod is configured to talk\nto the DNS server that’s built in to the cluster:\nroot@host01:~# \nkubectl exec -ti pod -- cat /etc/resolv.conf\n \nsearch default.svc.cluster.local svc.cluster.local cluster.local \nnameserver 10.96.0.10\noptions ndots:5\nWe print out the file \n/etc/resolv.conf\n inside the container because this is\nthe file that is used to configure DNS.\nThe name server \n10.96.0.10\n referenced is itself a Kubernetes Service, but it’s\nin the \nkube-system\n Namespace, so we need to look there for it:\nroot@host01:~# \nkubectl -n kube-system get services\nNAME            TYPE       CLUSTER-IP      ... PORT(S)                  AGE\nkube-dns        ClusterIP  10.96.0.10      ... 53/UDP,53/TCP,9153/TCP   14d\nmetrics-server  ClusterIP  10.105.140.176  ... 443/TCP                  14d\nThe \nkube-dns\n Service connects to a DNS server Deployment called\nCoreDNS that listens for changes to Services in the Kubernetes cluster.\nCoreDNS updates the DNS server configuration as required to stay up to date\nwith the current cluster configuration.\nName Resolution and Namespaces\nDNS names in a Kubernetes cluster are based on the Namespace as well as\nthe cluster domain. Because our Pod is in the \ndefault\n Namespace, it has \nbeen\nconfigured with a search path of \ndefault.svc.cluster.local\n as the first entry in the list,\nso it will search the \ndefault\n Namespace first when looking for Services. This is\nwhy we were able to use the bare Service name \nnginx\n to find the \nnginx\n Service\n—that Service is also in the \ndefault\n Namespace.\nWe could have also found the same Service using the fully qualified\nname:\nroot@host01:~# \nkubectl exec -ti pod -- wget -O - http://nginx.default.svc\nConnecting to nginx.default.svc (10.100.221.220:80)\n...\n<title>Welcome to nginx!</title>\n...\nUnderstanding this interaction between Namespaces and Service lookup is\nimportant. One common deployment pattern for a Kubernetes cluster is to\ndeploy the same application multiple times to different Namespaces and use\nsimple hostnames for application components to communicate with one\nanother. This pattern is often used to deploy a “development” and\n“production” version of an application to the same cluster. If we’re planning\nto use this pattern, we need to be sure that we stick to bare hostnames when\nour application components try to find one another; otherwise, we could end\nup communicating with the wrong version of our application.\nAnother important configuration item in \n/etc/resolv.conf\n is the \nndots\n entry.\nThe \nndots\n entry tells the hostname resolver that when it sees a hostname with\nfour or fewer dots, it should try appending the various search domains \nprior\nto performing an absolute search without any domain appended. This is\ncritical to make sure that we try to find services inside the cluster before\nreaching outside the cluster.\nAs a result, when we used the name \nnginx\n in \nListing 9-2\n, the DNS resolver\nwithin our container immediately tried \nnginx.default.svc.cluster.local\n and found the\ncorrect Service.\nTo make sure this is clear, let’s try one more example: looking up a\nService in another Namespace. The \nkube-system\n Namespace has a \nmetrics-server\nService. To find it, let’s use the standard host lookup \ndig\n command in our Pod.\nOur Pod is using Alpine Linux, so we need to install the \nbind-tools\n package\nto get access to \ndig\n:\nroot@host01:~# \nkubectl exec -ti pod -- apk add bind-tools\n...\nOK: 13 MiB in 27 packages\nNow, let’s try looking up \nmetrics-server\n using the bare name first:\nroot@host01:~# \nkubectl exec -ti pod -- dig +search metrics-server\n...\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 38423\n...\nWe add the \n+search\n flag onto the command to tell \ndig\n to use the search path\ninformation from \n/etc/resolv.conf\n. However, even with that flag, we don’t\nfind the Service, because our Pod is in the \ndefault\n Namespace, so the search\npath doesn’t lead \ndig\n to look in the \nkube-system\n Namespace.\nLet’s try again, this time specifying the correct Namespace:\nroot@host01:~# \nkubectl exec -ti pod -- dig +search metrics-server.kube-system\n...\n;; ANSWER SECTION:\nmetrics-server.kube-system.svc.cluster.local. 30 IN A 10.105.140.176\n...\nThis lookup works, and we are able to get the IP address for the \nmetrics-\nserver\n Service. It works because the search path includes \nsvc.cluster.local\n as its\nsecond entry. After initially trying \nmetrics-server.kube-system.default.svc.cluster.local\n,\nwhich doesn’t work, \ndig\n then tries \nmetrics-server.kube-system.svc.cluster.local\n, which\ndoes.\nTraffic Routing\nWe’ve seen how to create and use Services, but we haven’t yet looked at how\nthe actual traffic routing works. It turns out that Service network traffic works\nin a way that’s completely different from the overlay networks we saw in\nChapter 8\n, which can lead to some confusion.\nFor example, because we can use \nwget\n to reach an NGINX server instance\nusing the \nnginx\n Service name, we might expect to be able to use \nping\n, as well,\nbut that doesn’t work:\nroot@host01:~# \nkubectl exec -ti pod -- ping -c 3 nginx\nPING nginx (10.100.221.220): 56 data bytes\n--- nginx ping statistics ---\n3 packets transmitted, 0 packets received, 100% packet loss\ncommand terminated with exit code 1\nName resolution worked as expected, so \nping\n knew what destination IP\naddress to use for its ICMP packets. But there was no reply from that IP\naddress. We could look at every host and container network interface in our\ncluster and never find an interface that carries the Service IP address of\n10.100.221.220\n. So how is our HTTP traffic getting through to an NGINX\nService instance?\nOn every node in our cluster, there is a component called \nkube-proxy\n that\nconfigures traffic routing for Services. \nkube-proxy\n is run as a DaemonSet in the\nkube-system\n Namespace. Each \nkube-proxy\n instance watches for changes to\nServices in the cluster and configures the Linux firewall to route traffic.\nWe can use \niptables\n commands to look at the firewall configuration to see\nhow \nkube-proxy\n has configured traffic routing for our \nnginx\n Service:\n   \nroot@host01:~# \niptables-save | grep 'default/nginx cluster IP'\n➊\n -A KUBE-SERVICES ! -s 172.31.0.0/16 -d 10.100.221.220/32 -p tcp -m comment \n    --comment ""default/nginx cluster IP"" -m tcp --dport 80 -j KUBE-MARK-MASQ\n➋\n -A KUBE-SERVICES -d 10.100.221.220/32 -p tcp -m comment --comment \n   ""default/nginx cluster IP"" -m tcp --dport 80 -j KUBE-SVC-2CMXP7HKUVJN7L6M\nThe \niptables-save\n command backs up all of the current Linux firewall rules,\nso it’s useful for printing out all rules. The \ngrep\n command searches for the\ncomment string that \nkube-proxy\n applies to the Service rules it creates. In this\nexample, \nkube-proxy\n has created two rules for the Service as a whole. The first\nrule \n➊\n looks for traffic destined for our Service that is \nnot\n coming from the\nPod network. This traffic must be marked for Network Address Translation\n(NAT) masquerade so that the source of any reply traffic will be rewritten to\nbe the Service IP address rather than the actual Pod that handles the request.\nThe second rule \n➋\n sends all traffic destined for the Service to a separate rule\nchain that will send it to a Pod instance. Note that in both cases, the rules\nonly match for TCP traffic that is destined for port 80.\nWe can examine this separate rule chain to see how the actual routing to\nindividual Pods works. Be sure to replace the name of the rule chain in this\ncommand with the one shown in the previous output:\nroot@host01:~# \niptables-save | grep\n \nKUBE-SVC-2CMXP7HKUVJN7L6M\n...\n-A KUBE-SVC-2CMXP7HKUVJN7L6M ... -m statistic --mode random \n  --probability 0.20000000019 -j KUBE-SEP-PIVU7ZHMCSOWIZ2Z\n-A KUBE-SVC-2CMXP7HKUVJN7L6M ... -m statistic --mode random \n  --probability 0.25000000000 -j KUBE-SEP-CFQXKE74QEHFB7VJ\n-A KUBE-SVC-2CMXP7HKUVJN7L6M ... -m statistic --mode random \n  --probability 0.33333333349 -j KUBE-SEP-DHDWEJZ7MGGIR5XF\n-A KUBE-SVC-2CMXP7HKUVJN7L6M ... -m statistic --mode random \n  --probability 0.50000000000 -j KUBE-SEP-3S3S2VJCXSAISE2Z\n-A KUBE-SVC-2CMXP7HKUVJN7L6M ... -j KUBE-SEP-AQWD2Y25T24EHSNI\nThe output shows five rules, corresponding to each of the five NGINX\nPod instances the Service’s selector matched. The five rules together provide\nrandom load balancing across all the instances so that each one has an equal\nchance of being selected for new connections.\nIt may seem strange that the \nprobability\n figure increases for each rule. This is\nnecessary because the rules are evaluated sequentially. For the first rule, we\nwant a 20 percent chance of choosing the first instance. However, if we don’t\nselect the first instance, only four instances are left, so we want a 25 percent\nchance of choosing the second one. The same logic applies until we get to the\nlast instance, which we always want to choose if we’ve skipped all the others.\nLet’s quickly verify that these rules go to the expected destination (again,\nbe sure to replace the name of the rule chain in this command):\nroot@host01:~# \niptables-save | grep\n \nKUBE-SEP-PIVU7ZHMCSOWIZ2Z\n...\n-A KUBE-SEP-PIVU7ZHMC ... -s 172.31.239.235/32 ... --comment ""default/nginx"" -j KUBE-MARK-\nMASQ\n-A KUBE-SEP-PIVU7ZHMCSOWIZ2Z -p tcp ... -m tcp -j DNAT --to-destination 172.31.239.235:80\nThis output shows two rules. The first is the other half of the NAT\nmasquerade configuration, as we mark all packets that leave our Pod instance\nso that they can have their source address rewritten to appear to come from\nthe Service. The second rule is the one that actually routes Service traffic to a\nspecific Pod as it performs a rewrite of the destination address so that a\npacket originally destined for the Service IP is now destined for a Pod. From\nthere, the overlay networking takes over to actually send the packet to the\ncorrect container.\nWith this understanding of how Service traffic is actually routed, it makes\nsense that our ICMP packets didn’t make it through. The firewall rule that\nkube-proxy\n created applies only to TCP traffic destined for port 80. As a result,\nthere was no firewall rule to rewrite our ICMP packets and therefore no way\nfor them to make it to a networking stack that could reply to them. Similarly,\nif we have a container that’s listening on multiple ports, we will be able to\nconnect to any of those ports by directly using the Pod IP address, but the\nService IP address will route traffic only if we explicitly declare that port in\nthe Service specification. It can be a significant source of confusion when\ndeploying an application where the Pod starts up as expected and listens for\ntraffic, but a misconfiguration of the Service means that the traffic is not\nbeing routed to all of the correct destination ports.",20225
48-External Networking.pdf,48-External Networking,"External Networking\nWe now have enough layers of networking to meet all of our internal cluster\ncommunication needs. Each Pod has its own IP address and has connectivity\nto other Pods as well as the control plane, and with Service networking we\nhave automatic load balancing and failover based on running multiple Pod\ninstances with a Service. However, we’re still missing the ability for external\nusers to access services running in our cluster.\nTo provide access for external users, we can no longer rely solely on the\ncluster-specific IP address ranges that we use for Pods and Services, given\nthat external networks don’t recognize those address ranges. Instead, we’ll\nneed a way to allocate externally routable IP addresses to our Services, either\nby explicitly associating an IP address with a Service or by using an \ningress\ncontroller\n that listens to external traffic and routes it to Services.\nExternal Services\nThe \nnginx\n Service we created earlier was a \nClusterIP\n Service, the default Service\ntype. Kubernetes supports multiple Service types, including Service types\nthat are made for Services that need to be exposed externally:\nNone\n Also known as a \nheadless\n Service, it’s used to enable tracking of\nselected Pods but without an IP address or any network routing behavior.\nClusterIP\n The default Service type that provides tracking of selected Pods, a\ncluster IP address that is routed internally, and a well-known name in the\ncluster DNS.\nNodePort\n Extends \nClusterIP\n and also provides a port on all nodes in the cluster\nthat is routed to the Service.\nLoadBalancer\n Extends \nNodePort\n and also uses an underlying cloud provider to\nobtain an IP address that is externally reachable.\nExternalName\n Aliases a well-known Service name in the cluster DNS to some\nexternal DNS name. Used to make external resources appear to be in-cluster\nServices.\nOf these Service types, the \nNodePort\n and \nLoadBalancer\n types are most useful\nfor exposing Services outside the cluster. The \nLoadBalancer\n type seems the most\nstraightforward, as it simply adds an external IP to the Service. However, it\nrequires integration with an underlying cloud environment to create the\nexternal IP address when the Service is created, to route traffic from that IP\naddress to the cluster’s nodes, and to create a DNS entry outside the cluster\nthat enables external users to find the Service as a host on some pre-\nregistered domain that we already own, rather than a \ncluster.local\n domain that\nworks only within the cluster.\nFor this reason, a \nLoadBalancer\n Service is most useful for cases in which we\nknow what cloud environment we’re using and we’re creating Services that\nwill live for a long time. For HTTP traffic, we can get most of the benefit of a\nLoadBalancer\n Service by using a \nNodePort\n Service together with an ingress\ncontroller, with the added feature of better support for dynamically deploying\nnew applications with new Services.\nBefore moving on to an ingress controller, let’s turn our existing \nnginx\nService into a \nNodePort\n Service so that we can look at the effect. We can do\nthis using a patch file:\nnginx-nodeport.yaml\n---\nspec:\n  type: NodePort\nA patch file allows us to update only the specific fields we care about. In\nthis case, we are updating only the type of the Service. For this to work, we\njust need to specify that one changed field in its correct position in the\nhierarchy, which allows Kubernetes to know what field to modify. We don’t\nneed to change the selector or ports for our Service, only the type, so the\npatch is very simple.\nLet’s use the patch:\nroot@host01:~# \nkubectl patch svc nginx --patch-file /opt/nginx-nodeport.yaml\n \nservice/nginx patched\nFor this command, we must specify the resource to be patched and a patch\nfile to be used. The result is identical to if we had edited the YAML resource\nfile for the Service and then used \nkubectl apply\n again.\nThe Service now looks a little different:\nroot@host01:~# \nkubectl get service nginx\nNAME    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nnginx   NodePort   10.100.221.220   <none>        80:31326/TCP   2h\nA \nNodePort\n Service provides all the behavior of a \nClusterIP\n Service, so we\nstill have a cluster IP associated with our \nnginx\n Service. The Service even kept\nthe same cluster IP. The only change is the \nPORT\n field now shows that the\nService port 80 is attached to node port 31326.\nThe \nkube-proxy\n Service on every cluster node is listening on this port (be\nsure to use the correct node port for your Service):\nroot@host01:~# \nss -nlp | grep\n \n31326\ntcp   LISTEN 0  4096  .0.0.0:31326 ... users:((""kube-proxy"",pid=3339,fd=15))\nAs a result, we can still use the \nnginx\n Service name inside our Pod, but we\ncan also use the NodePort from the host:\nroot@host01:~# \nkubectl exec -ti pod -- wget -O - http://nginx\nConnecting to nginx (10.100.221.220:80)\n...\n<title>Welcome to nginx!</title>\n...\nroot@host01:~# \nwget -O - http://host01:\n31326\n...\nConnecting to host01 (host01)|127.0.2.1|:31326... connected.\n...\n<h1>Welcome to nginx!</h1>\n...\nBecause \nkube-proxy\n is listening on all network interfaces, we’ve successfully\nexposed this Service to external users.\nIngress Services\nAlthough we’ve successfully exposed our NGINX Service outside the\ncluster, we still don’t provide a great user experience for external users. To\nuse the \nNodePort\n Service, external users will need to know the IP address of at\nleast one of our cluster nodes, and they’ll need to know the exact port on\nwhich \neach Service is listening. That port could change if the Service is\ndeleted and re-created. We could partially address this by telling Kubernetes\nwhich port to use for the \nNodePort\n, but we don’t want to do this with any\narbitrary Service because multiple Services may choose the same port.\nWhat we really need is a single external entry point to our cluster that\nkeeps track of multiple services that are available and uses rules to route\ntraffic to them. This way, we can do all of our routing configuration inside\nthe cluster so that Services can come and go dynamically. At the same time,\nwe can have a single well-known entry point for our cluster that all external\nusers can use.\nFor HTTP traffic, Kubernetes provides exactly this capability, calling it an\nIngress\n. To configure our cluster to route external HTTP traffic to Services,\nwe need to define the set of Ingress resources that specify the routing and to\ndeploy the ingress controller that receives and routes the traffic. We already\ninstalled our ingress controller when we set up our cluster:\nroot@host01:~# \nkubectl -n ingress-nginx get deploy\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ningress-nginx-controller   1/1     1            1           15d\nroot@host01:~# \nkubectl -n ingress-nginx get svc\nNAME                      TYPE        ... PORT(S)               ...\ningress-nginx-controller  NodePort    ... 80:80/TCP,443:443/TCP ...\n...\nOur ingress controller includes a Deployment and a Service. As the\nService is of type \nNodePort\n, we know that \nkube-proxy\n is listening to ports 80 and\n443 on all of our cluster’s nodes, ready to route traffic to the associated Pod.\nAs the name implies, our ingress controller is actually an instance of an\nNGINX web server; however, in this case NGINX is solely acting as an\nHTTP reverse proxy rather than serving any web content of its own. The\ningress controller listens for changes to Ingress resources in the cluster and\nreconfigures NGINX to connect to backend servers based on the rules that\nare defined. These rules use host or path information from the HTTP request\nto select a Service for the request.\nLet’s create an Ingress resource to route traffic to the \nnginx\n Service we\ndefined in \nListing 9-1\n. Here’s the resource we’ll create:\nnginx-ingress.yaml\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web01\nspec:\n  rules:\n    - host: web01\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: nginx\n                port:\n                  number: 80\nThis resource instructs the ingress controller to look at the HTTP \nHost\nheader. If it sees \nweb01\n as the \nHost\n header, it then tries to match against a path\nin the \npaths\n we specified. In this case, all paths will match the path \n/\n prefix, so\nall traffic will be routed to the \nnginx\n Service.\nBefore we apply this to the cluster, let’s confirm what happens if we try to\nuse a hostname that the ingress controller doesn’t recognize. We’ll use the\nhigh-availability IP address that’s associated with our cluster, as the cluster’s\nload balancer will forward that to one of the instances:\nroot@host01:~# \ncurl -vH ""Host:web01"" http://192.168.61.10\n...\n> Host:web01\n...\n<head><title>404 Not Found</title></head>\n...\nThe \n-H ""Host:web01""\n flag in the \ncurl\n command tells \ncurl\n to use the value \nhost01\nas the \nHost\n header in the HTTP request. This is necessary given that we don’t\nhave a DNS server in our example cluster that can turn \nweb01\n into our\ncluster’s IP address.\nAs we can see, the NGINX server that’s acting as the ingress controller is\nconfigured to reply with a \n404 Not Found\n error message whenever it gets a\nrequest that doesn’t match any configured Ingress resource. In this case,\nbecause we haven’t created any Ingress resources yet, any request will get\nthis response.\nLet’s apply the \nweb01\n Ingress resource to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/nginx-ingress.yaml\n \ningress.networking.k8s.io/web01 created\nNow that the Ingress resource exists, as \nListing 9-3\n illustrates, HTTP port\n80 requests on both the cluster high-availability IP and individual hosts are\nrouted to the \nnginx\n Service:\nroot@host01:~# \ncurl -vH ""Host:web01"" http://host01\n...\n> Host:web01\n...\n<title>Welcome to nginx!</title>\n...\nroot@host01:~# \ncurl -vH ""Host:web01"" http://192.168.61.10\n...\n> Host:web01\n...\n<title>Welcome to nginx!</title>\n...\nListing 9-3: NGINX via Ingress\nThe output in both cases is the same, showing that traffic is being routed\nto the \nnginx\n Service.\nIn the \nweb01-ingress\n resource, we were able to use the bare name of the \nnginx\nService. The Service name lookup is based on where the Ingress resource is\nlocated. Because we created the Ingress resource in the default Namespace,\nthat is where it looks first for Services.\nPutting this all together, we now have a high-availability solution to route\ntraffic from external users to HTTP servers in our cluster. This combines our\ncluster’s high-availability IP address \n192.168.61.10\n with an ingress controller\nexposed as a \nNodePort\n Service on port 80 of all our cluster’s nodes. The ingress\ncontroller can be dynamically configured to expose additional Services by\ncreating new Ingress resources.\nIngress in Production\nThe \ncurl\n command in \nListing 9-3\n still looks a little strange, as we’re required\nto override the HTTP \nHost\n header manually. We need to perform a few\nadditional steps to use Ingress resources to expose services in a production\ncluster.\nFirst, we need our cluster to have an externally routable IP address\ntogether with a well-known name that is registered in DNS. The best way to\ndo that is with a wildcard DNS scheme so that all hosts in a given domain are",11622
49-Final Thoughts.pdf,49-Final Thoughts,"all routed to the cluster’s external IP. For example, if we own the domain\ncluster.example.com\n, we could create a DNS entry so that \n*.cluster.example.com\n routes\nto the cluster’s external IP address.\nThis approach still works with larger clusters that span multiple networks.\nWe just need to have multiple IP addresses associated with the DNS entry,\npossibly using location-aware DNS servers that route clients to the closest\nservice.\nNext, we need to create an SSL certificate for our ingress controller that\nincludes our wildcard DNS as a Subject Alternative Name (SAN). This will\nallow our ingress controller to provide a secure HTTP connection for external\nusers no matter what specific service hostname they are using.\nFinally, when we define our Services, we need to specify the fully\nqualified domain name for the \nhost\n field. For the preceding example, we\nwould specify \nweb01.cluster.example.com\n rather than just \nweb01\n.\nAfter we’ve performed these additional steps, any external user would be\nable to connect via HTTPS to the fully qualified hostname of our Service,\nsuch as \nhttps://web01.cluster.example.com\n. This hostname would resolve to our\ncluster’s external IP address, and the load balancer would route it to one of\nthe cluster’s nodes. At that point, our ingress controller, listening on the\nstandard port of 443, would offer its wildcard certificate, which would match\nwhat the client expects. As soon as the secure connection is established, the\ningress controller would inspect the HTTP \nHost\n header and proxy a\nconnection to the correct Service, sending back the HTTP response to the\nclient.\nThe advantage of this approach is that after we have it set up, we can\ndeploy a new Ingress resource at any time to expose a Service externally, and\nas long as we choose a unique hostname, it won’t collide with any other\nexposed Service. After the initial setup, all of the configuration is maintained\nwithin the cluster itself, and we still have a highly available configuration for\nall of our Services.\nFinal Thoughts\nRouting network traffic in a Kubernetes cluster might involve a great deal of\ncomplexity, but the end result is straightforward: we can deploy our\napplication components to a cluster, with automatic scaling and failover, and\nexternal users can access our application using a well-known name without\nhaving to know how the application is deployed or how many container\ninstances we’re using to meet demand. If we build our application to be\nresilient, our application containers can upgrade to new versions or restart in\nresponse to failure without users even being aware of the change.\nOf course, if we’re going to build application components that are\nresilient, it’s important to know what can go wrong in deploying containers.\nIn the next chapter, we’ll look at some common issues with deploying\ncontainers to a Kubernetes cluster and how to debug them.",2953
50-10 WHEN THINGS GO WRONG.pdf,50-10 WHEN THINGS GO WRONG,,0
51-Scheduling.pdf,51-Scheduling,"10\nWHEN THINGS GO WRONG\nSo far our installation and configuration of Kubernetes has gone as planned,\nand our controllers have had no problem creating Pods and starting\ncontainers. Of course, in the real world, it’s rarely that easy. Although\nshowing everything that might go wrong with a complex application\ndeployment isn’t possible, we can look at some of the most common\nproblems. Most important, we can explore debugging tools that will help us\ndiagnose any issue.\nIn this chapter, we’ll look at how to diagnose problems with application\ncontainers that we deploy on top of Kubernetes. We’ll work our way through\nthe life cycle for scheduling and running containers, examining potential\nproblems at each step as well as how to diagnose and fix them.\nScheduling\nScheduling is the first activity Kubernetes performs on a Pod and its\ncontainers. When a Pod is first created, the Kubernetes scheduler assigns it to\na \nnode. Normally, this happens quickly and automatically, but some issues\ncan prevent scheduling from happening successfully.\nNo Available Nodes\nOne possibility is that the scheduler simply doesn’t have any nodes available.\nThis situation might occur because our cluster doesn’t have any nodes\nconfigured for regular application containers or because all nodes have failed.\nTo illustrate the case in which no nodes are available for assignment, let’s\ncreate a Pod with a \nnode selector\n. A node selector specifies one or more node\nlabels that are required for a Pod to be scheduled on that node. Node selectors\nare useful when some nodes in our cluster are different from others (for\nexample, when some nodes have newer CPUs with support for more\nadvanced instruction sets needed by some of our containers).\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nWe’ll begin with a Pod definition that has a node selector that doesn’t\nmatch any of our nodes:\nnginx-selector.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  nodeSelector:\n \n➊\n purpose: special\nThe node selector \n➊\n tells Kubernetes to assign this Pod only to a node\nwith a label called \npurpose\n whose value is equal to \nspecial\n. Even though none of\nour nodes currently match, we can still create this Pod:\nroot@host01:~# \nkubectl apply -f /opt/nginx-selector.yaml\npod/nginx created\nHowever, Kubernetes is stuck trying to schedule the Pod, because it can’t\nfind a matching node:\nroot@host01:~# \nkubectl get pods -o wide\nNAME    READY   STATUS    RESTARTS   AGE    IP       NODE     ...\nnginx   0/1     Pending   0          113s   <none>   <none>   ...\nWe see a status of \nPending\n and a node assignment of \n<none>\n. This is because\nKubernetes has not yet scheduled this Pod onto a node.\nThe \nkubectl get\n command is typically the first command we should run to\nsee whether there are issues with a resource we’ve deployed to our cluster. If\nwe have an issue, as we do in this case, the next step is to view the detailed\nstatus and event log using \nkubectl describe\n:\nroot@host01:~# \nkubectl describe pod nginx\nName:         nginx\nNamespace:    default\n...\nStatus:       Pending\n...\nNode-Selectors:              purpose=special\nEvents:\n  Type     Reason            Age    From               Message\n  ----     ------            ----   ----               -------\n  Warning  FailedScheduling  4m36s  default-scheduler  0/3 nodes are \n    available: 3 node(s) didn't match Pod's node affinity/selector.\n  Warning  FailedScheduling  3m16s  default-scheduler  0/3 nodes are \n    available: 3 node(s) didn't match Pod's node affinity/selector.\nThe event log informs us as to exactly what the issue is: the Pod can’t be\nscheduled because none of the nodes matched the selector.\nLet’s add the necessary label to one of our nodes:\nroot@host01:~# \nkubectl get nodes\nNAME     STATUS   ROLES        ...\nhost01   Ready    control-plane...\nhost02   Ready    control-plane...\nhost03   Ready    control-plane...\nroot@host01:~# \nkubectl label nodes host02 purpose=special\nnode/host02 labeled\nWe first list the three nodes we have available and then apply the\nnecessary label to one of them. As soon as we apply this label, Kubernetes\ncan now schedule the Pod:\nroot@host01:~# \nkubectl get pods -o wide\nNAME    READY   STATUS    RESTARTS   AGE   IP               NODE     ...\nnginx   1/1     Running   0          10m   172.31.89.196   host02   ...\nroot@host01:~# \nkubectl describe pod nginx\nName:         nginx\nNamespace:    default\n...\nEvents:\n  Type     Reason            Age    From               Message\n  ----     ------            ----   ----               -------\n  Warning  FailedScheduling  10m    default-scheduler  0/3 nodes are \n    available: 3 node(s) didn't match Pod's node affinity/selector.\n  Warning  FailedScheduling  9m17s  default-scheduler  0/3 nodes are \n    available: 3 node(s) didn't match Pod's node affinity/selector.\n  Normal   Scheduled         2m22s  default-scheduler  Successfully assigned \n    default/nginx to host02\n...\nAs expected, the Pod was scheduled onto the node where we applied the\nlabel.\nThis example, like the others we’ll see in this chapter, illustrates\ndebugging in Kubernetes. After we’ve created the resources that we need, we\nquery the cluster state to make sure the actual deployment of those resources\nwas successful. When we find issues, we can correct those issues and our\nresources will be started as desired without having to reinstall our application\ncomponents.\nLet’s clean up this NGINX Pod:\nroot@host01:~# \nkubectl delete -f /opt/nginx-selector.yaml\npod ""nginx"" deleted\nLet’s also remove the label from the node. We remove the label by\nappending a minus sign to it to identify it:\nroot@host01:~# \nkubectl label nodes host02 purpose-\nnode/host02 unlabeled\nWe’ve covered one issue with the scheduler, but there’s still another we\nneed to look at.\nInsufficient Resources\nWhen choosing a node to host a Pod, the scheduler also considers the\nresources that are available on each node and the resources the Pod requires.\nWe explore resource limits in detail in \nChapter 14\n; for now it’s enough to\nknow that each container in a Pod can request the resources it needs, and the\nscheduler will ensure that it is scheduled onto a node that has those resources\navailable. Of course, if there aren’t any nodes with enough room, the\nscheduler won’t be able to schedule the Pod. Instead the Pod will wait in a\nPending\n state.\nLet’s look at an example Pod definition to illustrate this:\nsleep-multiple.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sleep\nspec:\n  containers:\n  - name: sleep\n    image: busybox\n    command: \n      - ""/bin/sleep""\n      - ""3600""\n    resources:\n      requests:\n        cpu: ""2""\n  - name: sleep2\n    image: busybox\n    command: \n      - ""/bin/sleep""\n      - ""3600""\n    resources:\n      requests:\n        cpu: ""2""\nIn this YAML definition, we create two containers in the same Pod. Each\ncontainer requests two CPUs. Because all of the containers in a Pod must be\non the same host in order to share some Linux namespace types (especially\nthe network namespace so that they can use \nlocalhost\n for communication), the\nscheduler needs to find a single node with four CPUs available. In our small\ncluster, that can’t happen, as we can see if we try to deploy the Pod:\nroot@host01:~# \nkubectl apply -f /opt/sleep-multiple.yaml\npod/sleep created\nroot@host01:~# \nkubectl get pods -o wide\nNAME    READY   STATUS    RESTARTS   AGE   IP       NODE   ...\nsleep   0/2     Pending   0          7s    <none>   <none> ...\nAs before, \nkubectl describe\n gives us the event log that reveals the issue:\nroot@host01:~# \nkubectl describe pod sleep\nName:         sleep\nNamespace:    default\n...\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  Warning  FailedScheduling  71s   default-scheduler  0/3 nodes are \n    available: 3 Insufficient cpu.\nNotice that it doesn’t matter how heavily loaded our nodes actually are:\nroot@host01:~# \nkubectl top node\nNAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nhost01   429m         21%    1307Mi          69%\nhost02   396m         19%    1252Mi          66%\nhost03   458m         22%    1277Mi          67%\nNor does it matter how much CPU our containers will actually use. The\nscheduler allocates Pods purely based on what it requested; this way, we\ndon’t suddenly overwhelm a CPU when load increases.\nWe can’t magically provide our nodes with more CPUs, so to get this Pod\nscheduled, we’re going to need to specify a lower CPU usage for our two\ncontainers. Let’s use a more sensible figure of 0.1 CPU:\nsleep-sensible.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sleep\nspec:\n  containers:\n  - name: sleep\n    image: busybox\n    command: \n      - ""/bin/sleep""\n      - ""3600""\n    resources:\n      requests:\n     \n➊\n cpu: ""100m""\n  - name: sleep2\n    image: busybox\n    command: \n      - ""/bin/sleep""\n      - ""3600""\n    resources:\n      requests:\n        cpu: ""100m""\nThe value \n100m\n \n➊\n equates to “one hundred millicpu” or one-tenth (0.1) of\na CPU.\nEven though this is a separate file, it declares the same resource, so\nKubernetes will treat it as an update. However, if we try to apply this as a\nchange to the existing Pod, it will fail:\nroot@host01:~# \nkubectl apply -f /opt/sleep-sensible.yaml\nThe Pod ""sleep"" is invalid: spec: Forbidden: pod updates may not change \n  fields other than ...\nWe are not allowed to change the resource request of an existing Pod,\nwhich makes sense given that a Pod is allocated to a node only once on\ncreation, and a resource usage change might cause the node to be overly full.\nIf we were using a controller such as a Deployment, the controller could\nhandle replacing the Pods for us. Because we created a Pod directly, we need\nto manually delete and then re-create it:\nroot@host01:~# \nkubectl delete pod sleep\npod ""sleep"" deleted\nroot@host01:~# \nkubectl apply -f /opt/sleep-sensible.yaml\npod/sleep created\nOur new Pod has no trouble with node allocation:\nroot@host01:~# \nkubectl get pods -o wide\nNAME    READY   STATUS    RESTARTS   AGE   IP               NODE  ...\nsleep   2/2     Running   0          51s   172.31.89.199   host02 ...\nAnd if we run \nkubectl describe\n on the node, we can see how our new Pod has\nbeen allocated some of the node’s CPU:\nroot@host01:~# \nkubectl describe node\n \nhost02\nName:               host02\n...\nCapacity:",10811
52-Pulling Images.pdf,52-Pulling Images,"cpu:                2\n...\nNon-terminated Pods:          (10 in total)\n  Namespace  Name     CPU Requests  CPU Limits  ...\n  ---------  ----     ------------  ----------  ...\n...\n  default    sleep \n➊\n 200m (10%)    0 (0%)      ... \n...\nBe sure to use the correct node name for the node where your Pod was\ndeployed. Because our Pod has two containers, each requesting \n100m\n, its total\nrequest is \n200m\n \n➊\n.\nLet’s finish by cleaning up this Pod:\nroot@host01:~# \nkubectl delete pod sleep\npod ""sleep"" deleted\nOther errors can prevent a Pod from being scheduled, but these are the\nmost common. Most important, the commands we used here apply in all\ncases. First, use \nkubectl get\n to determine the Pod’s current status, followed by\nkubectl describe\n to view the event log. These two commands are always a good\nfirst step when something doesn’t seem to be working properly.\nPulling Images\nAfter a Pod is scheduled onto a node, the local \nkubelet\n service interacts with\nthe underlying container runtime to create an isolated environment and start\ncontainers. However, there’s still one application misconfiguration that can\ncause our Pod to become stuck in the \nPending\n phase: inability to pull the\ncontainer image.\nThree main issues can prevent the container runtime from pulling an\nimage:\nFailure to connect to the container image registry\nAuthorization issue with the requested image\nImage is missing from the registry\nAs we described in \nChapter 5\n, an image registry is a web server. Often,\nthe image registry is outside the cluster, and the nodes need to be able to\nconnect to an external network or the internet to reach the registry.\nAdditionally, most registries support publishing private images that require\nauthentication and authorization to access. And, of course, if there is no\nimage published under the name we specify, the container runtime is not\ngoing to be able to pull it from the registry.\nAll of these errors behave the same way in our Kubernetes cluster, with\ndifferences only in the message in the event log, so we’ll need to explore only\none of them. We’ll look at what is probably the most common issue: a\nmissing image caused by a typo in the image name.\nLet’s try to create a Pod using this YAML file:\nnginx-typo.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginz\nBecause there is no image in Docker Hub called \nnginz\n, it won’t be possible\nto pull this image. Let’s explore what happens when we add this resource to\nthe cluster:\nroot@host01:~# \nkubectl apply -f /opt/nginx-typo.yaml\npod/nginx created\nroot@host01:~# \nkubectl get pods\nNAME    READY   STATUS             RESTARTS   AGE\nnginx   0/1     ImagePullBackOff   0          20s\nOur Pod has status \nImagePullBackOff\n, which immediately signals two things.\nFirst, this Pod is not yet getting to the point at which the containers are\nrunning, because it has not yet pulled the container images. Second, as with\nall errors, Kubernetes will continue attempting the action, but it will use a\nback-off\n algorithm to avoid overwhelming our cluster’s resources. \nPulling an\nimage involves reaching out over the network to communicate with the image\nregistry, and it would be rude and a waste of network bandwidth to flood the\nregistry with many requests in a short amount of time. Moreover, the cause of\nthe failure may be transient, so the cluster will keep trying in hopes that the\nproblem will be resolved.\nThe fact that Kubernetes uses a back-off algorithm for retrying errors is\nimportant for debugging. In this case, we obviously are not going to publish\nan \nnginz\n image to Docker Hub to fix the problem. But for cases in which we\ndo fix the issue by publishing an image, or by changing the permissions for\nthe image, it’s important to know that Kubernetes will not pick up that\nchange immediately, because the amount of delay between tries increases\nwith each failure.\nLet’s explore the event log so that we can see this back-off in action:\nroot@host01:~# \nkubectl describe pod nginx\nName:         nginx\nNamespace:    default\n...\nStatus:     \n➊\n Pending \n...\nEvents:\n  Type     Reason     Age                 From               Message\n  ----     ------     ----                ----               -------\n  Normal   Scheduled  114s                default-scheduler  Successfully \n    assigned default/nginx to host03\n...\n  Warning  Failed     25s (x4 over 112s)  kubelet            Failed to pull \n    image ""nginz"": ... \n➋\n pull access denied, repository does not exist or may \n    require authorization  ...\n...\n  Normal   BackOff    1s \n➌\n (x7 over 111s)   kubelet            ...\nAs before, our Pod is stuck in a \nPending\n status \n➊\n. In this case, however, the\nPod has gotten past the scheduling activity and has moved on to pulling the\nimage. For security reasons, the registry does not distinguish between a\nprivate image for which we don’t have permission to access and a missing\nimage, so Kubernetes can tell us only that the issue is one or the other \n➋\n.\nFinally, we can see that Kubernetes has tried to pull the image seven times\nduring the two minutes since we created this Pod \n➌\n, and it last tried to pull\nthe image one second ago.\nIf we wait a few minutes and then run the same \nkubectl describe\n command\nagain, focusing on the back-off behavior, we can see that a long amount of\ntime elapses between tries:\nroot@host01:~# \nkubectl describe pod nginx\nName:         nginx\nNamespace:    default\n...\nEvents:\n  Type     Reason     Age                   From               Message\n  ----     ------     ----                  ----               -------\n...\n  Normal   BackOff    4m38s (x65 over 19m)  kubelet            ...\nKubernetes has now tried to pull the image 65 times over the course of 19\nminutes. However, the amount of delay has grown over time and has reached\nthe maximum of five minutes between each attempt. This means that as we\ndebug this issue, we will need to wait up to five minutes each time to see\nwhether the problem has been resolved.\nLet’s go ahead and fix the issue so that we can see this in action. We could\nfix the YAML file and run \nkubectl apply\n again, but we can also fix it using \nkubectl\nset\n:\nroot@host01:~# \nkubectl set image pod nginx nginx=nginx\npod/nginx image updated\nroot@host01:~# \nkubectl get pods\nNAME    READY   STATUS             RESTARTS   AGE\nnginx   0/1     ImagePullBackOff   0          28m\nThe \nkubectl set\n command requires us to specify the resource type and name;\nin this case \npod nginx\n. We then specify \nnginx=nginx\n to provide the name of the\ncontainer to modify (because a Pod can have multiple containers) along with\nthe new image.\nWe fixed the image name, but the Pod is still showing \nImagePullBackOff\nbecause we must wait for the five-minute timer to elapse before Kubernetes\ntries again. Upon the next try, the pull is successful and the Pod starts\nrunning:\nroot@host01:~# \nkubectl get pods\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          32m\nLet’s clean up the Pod before moving on:\nroot@host01:~# \nkubectl delete pod nginx\npod ""nginx"" deleted\nAgain, we were able to solve this using \nkubectl get\n and \nkubectl describe\n.",7376
53-Running Containers.pdf,53-Running Containers,"However, when we get to the point that the container is running, that won’t\nbe sufficient.\nRunning Containers\nAfter instructing the container runtime to pull any images needed, \nkubelet\n then\ntells the runtime to start the containers. For the rest of the examples in this\nchapter, we’ll assume that the container runtime is working as expected. At\nthis point, then, the main problem we’ll run into is the case in which the\ncontainer does not start as expected. Let’s begin with a simpler example of\ndebugging a container that fails to run, and then we’ll look at a more complex\nexample.\nDebugging Using Logs\nFor our simple example, we first need a Pod definition with a container that\nfails on startup. Here’s a Pod definition for PostgreSQL that will do what we\nwant:\npostgres-misconfig.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: postgres\nspec:\n  containers:\n  - name: postgres\n    image: postgres\nIt might not seem like there are any issues with this definition, but\nPostgreSQL has some required configuration when running in a container.\nWe can create the Pod using \nkubectl apply\n:\nroot@host01:~# \nkubectl apply -f /opt/postgres-misconfig.yaml\npod/postgres created\nAfter a minute or so to allow time to pull the image, we can check the\nstatus with \nkubectl get\n, and we’ll notice a status we haven’t seen before:\nroot@host01:~# \nkubectl get pods\nNAME       READY   STATUS             RESTARTS     AGE\npostgres   0/1     CrashLoopBackOff   1 (8s ago)   25s\nThe \nCrashLoopBackOff\n status indicates that a container in the Pod has exited.\nAs this is not a Kubernetes Job, it doesn’t expect the container to exit, so it’s\nconsidered a crash.\nIf you catch the Pod at the right time, you might see an \nError\n status rather\nthan \nCrashLoopBackOff\n. This is temporary: the Pod transitions through that status\nimmediately after crashing.\nLike the \nImagePullBackOff\n status, a \nCrashLoopBackOff\n uses an algorithm to retry\nthe failure, increasing the time between retries with every failure, to avoid\noverwhelming the cluster. We can see this back-off if we wait a few minutes\nand then print the status again:\nroot@host01:~# \nkubectl get pods\nNAME       READY   STATUS             RESTARTS       AGE\npostgres   0/1     CrashLoopBackOff   5 (117s ago)   5m3s\nAfter five restarts, we’re already up to more than a minute of wait time\nbetween retries. The wait time will continue to increase until we reach five\nminutes, and then Kubernetes will continue to retry every five minutes\nthereafter indefinitely.\nLet’s use \nkubectl describe\n, as usual, to try to get more information about this\nfailure:\nroot@host01:~# \nkubectl describe pod postgres\nName:         postgres\nNamespace:    default\n...\nContainers:\n  postgres:\n...\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n...\nEvents:\n  Type     Reason     Age                    From               Message\n  ----     ------     ----                   ----               -------\n...\n  Warning  BackOff    3m13s (x24 over 8m1s)  kubelet            Back-off \n    restarting failed container\nThe \nkubectl describe\n command does give us one piece of useful information:\nthe exit code for the container. However, that really just tells us there was an\nerror of some kind; it isn’t enough to fully debug the failure. To establish\nwhy the container is failing, we’ll look at the container logs using the \nkubectl\nlogs\n command:\nroot@host01:~# \nkubectl logs postgres\nError: Database is uninitialized and superuser password is not specified.\n  You must specify POSTGRES_PASSWORD to a non-empty value for the\n  superuser. For example, ""-e POSTGRES_PASSWORD=password"" on ""docker run"".\n...\nWe can see the logs even though the container has already stopped,\nbecause the container runtime has captured them.\nThis message comes directly from PostgreSQL itself. Fortunately, it tells\nus exactly what the issue is: we are missing a required environment variable.\nWe can quickly fix this with an update to the YAML resource file:\npostgres-fixed.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: postgres\nspec:\n  containers:\n  - name: postgres\n    image: postgres\n \n➊\n env:\n    - name: POSTGRES_PASSWORD\n      value: ""supersecret""\nThe \nenv\n field \n➊\n adds a configuration to pass in the required environment\nvariable. Of course, in a real system we would not put this directly in a\nYAML file in plaintext. We look at how to secure this kind of information in\nChapter 16\n.\nTo apply this change, we first need to delete the Pod definition and then\napply the new resource configuration to the cluster:\nroot@host01:~# \nkubectl delete pod postgres\npod ""postgres"" deleted\nroot@host01:~# \nkubectl apply -f /opt/postgres-fixed.yaml\npod/postgres created\nAs before, if we were using a controller such as a Deployment, we could\njust update the Deployment, and it would handle deleting the old Pod and\ncreating a new one for us.\nNow that we’ve fixed the configuration, our PostgreSQL container starts\nas expected:\nroot@host01:~# \nkubectl get pods\nNAME       READY   STATUS    RESTARTS   AGE\npostgres   1/1     Running   0          77s\nLet’s clean up this Pod before we continue to our next example:\nroot@host01:~# \nkubectl delete pod postgres\npod ""postgres"" deleted\nMost well-written applications will print log messages before terminating,\nbut we need to be prepared for more difficult cases. Let’s look at one more\nexample that includes two new debugging approaches.\nDebugging Using Exec\nFor this example, we’ll need an application that behaves badly. We’ll use a C\nprogram that does some very naughty memory access. This program is\npackaged into an Alpine Linux container so that we can run it as a container\nin Kubernetes. Here’s the C source code:\ncrasher.c\nint main() {\n  char *s = ""12"";\n  s[2] = '3';\n  return 0;\n}\nThe first line of code creates a pointer to a string that is two characters\nlong; the second line then tries to write to the non-existent third character,\ncausing the program to terminate immediately.\nThis C program can be compiled on any system by using \ngcc\n to create a\ncrasher\n executable. If you build it on a host Linux system, use this \ngcc\ncommand:\n$ \ngcc -g -static -o crasher crasher.c\nThe \n-g\n argument ensures that debugging symbols are available. We’ll use\nthose in a moment. The \n-static\n argument is the most important; we want to\npackage this as a standalone application inside an Alpine container image. If\nwe are building on a different Linux distribution, such as Ubuntu, the\nstandard libraries are based on a different toolchain, and dynamic linking will\nfail. For this reason, we want our executable to have all of its dependencies\nstatically linked. Finally, we use \n-o\n to specify the output executable name and\nthen provide the name of our C source file.\nAlternatively, you can just use the container image that’s already been\nbuilt and published to Docker Hub under the name \nbookofkubernetes/crasher: stable\n.\nThis image is built and published automatically using GitHub Actions based\non the code in the repository \nhttps://github.com/book-of-kubernetes/crasher\n.\nHere’s the \nDockerfile\n from that repository:\nDockerfile\nFROM alpine AS builder\nCOPY ./crasher.c /\nRUN apk --update add gcc musl-dev && \\n    gcc -g -o crasher crasher.c\nFROM alpine\nCOPY --from=builder /crasher /crasher\nCMD [ ""/crasher"" ]\nThis \nDockerfile\n takes advantage of Docker’s multistage builds capability\nto reduce the final image size. To compile inside an Alpine container, we\nneed \ngcc\n and the core C include files and libraries. However, these have the\neffect of making the container image significantly larger. We only need them\nat compile time, so we want to avoid having that extra content in the final\nimage.\nWhen we run this build using the \ndocker build\n command that we saw in\nChapter 5\n, Docker will create one container based on Alpine Linux, copy our\nsource code into it, install the developer tools, and compile the application.\nDocker will then start over with a fresh Alpine Linux container and will copy\nthe resulting executable from the first container. The final container image is\ncaptured from this second container, so we avoid adding the developer tools\nto the final image.\nLet’s run this image in our Kubernetes cluster. We’ll use a Deployment\nresource this time so that we can illustrate editing it to work around the\ncrashing container:\ncrasher-deploy.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: crasher\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crasher\n  template:\n    metadata:\n      labels:\n        app: crasher\n    spec:\n      containers:\n      - name: crasher\n        image: bookofkubernetes/crasher:stable\nThis basic Deployment is very similar to what we saw when we\nintroduced Deployments in \nChapter 7\n. We specify the \nimage\n field to match the\nlocation where the image is published.\nWe can add this Deployment to the cluster in the usual way:\nroot@host01:~# \nkubectl apply -f /opt/crasher-deploy.yaml\ndeployment.apps/crasher created\nAs soon as Kubernetes has had a chance to schedule the Pod and pull the\nimage, it starts crashing, as expected:\nroot@host01:~# \nkubectl get pods\nNAME                       READY   STATUS             RESTARTS      AGE\ncrasher-76cdd9f769-5blbn   0/1     CrashLoopBackOff   3 (24s ago)   73s\nAs before, using \nkubectl describe\n tells us only the exit code of the container.\nThere’s another way to get this exit code; we can use the JSON output format\nof \nkubectl get\n and the \njq\n tool to capture just the exit code:\nroot@host01:~# \nkubectl get pod \ncrasher-7978d9bcfb-wvx6q\n -o json | \\n  \njq '.status.containerStatuses[].lastState.terminated.exitCode'\n139\nBe sure to use the correct name for your Pod based on the output of \nkubectl\nget pods\n. The path to the specific field we need is based on how Kubernetes\ntracks this resource internally; with some practice it becomes easier \nto craft a\npath to \njq\n to capture a specific field, which is a very handy trick in scripting.\nThe exit code of 139 tells us that the container terminated with a\nsegmentation fault. However, the logs are unhelpful in diagnosing the\nproblem, because our program didn’t print anything before it crashed:\nroot@host01:~# \nkubectl logs\n \ncrasher-76cdd9f769-5blbn\n[ no output ]\nWe have quite a problem. The logs aren’t helpful, so the next step would\nbe to use \nkubectl exec\n to get inside the container. However, the container stops\nimmediately when our application crashes and is not around long enough for\nus to do any debugging work.\nTo fix this, we need a way to start this container without running the\ncrashing program. We can do that by overriding the default command to have\nour container remain running. Because we built on an Alpine Linux image,\nthe \nsleep\n command is available to us for this purpose.\nWe could edit our YAML file and update the Deployment that way, but\nwe can also edit the Deployment directly using \nkubectl edit\n, which will bring up\nthe current definition in an editor, and any changes we make will be saved to\nthe cluster:\nroot@host01:~# \nkubectl edit deployment crasher\nThis will bring up vi in an editor window with the Deployment resource in\nYAML format. The resource will include a lot more fields than we provided\nwhen we created it because Kubernetes will show us the status of the\nresource as well as some fields with default values.\nIf you don’t like vi, you can preface the \nkubectl edit\n command with\nKUBE_EDITOR=nano\n to use the Nano editor, instead.\nWithin the file, find these lines:\n    spec:\n      containers:\n      - image: bookofkubernetes/crasher:stable\n        imagePullPolicy: IfNotPresent\nYou will see the \nimagePullPolicy\n line even though it wasn’t in the YAML\nresource, as Kubernetes has added the default policy to the resource\nautomatically. Add a new line between \nimage\n and \nimagePullPolicy\n so that the\nresult looks like this:\n    spec:\n      containers:\n      - image: bookofkubernetes/crasher:stable\n        \nargs: [""/bin/sleep"", ""infinity""]\n        imagePullPolicy: IfNotPresent\nThis added line overrides the default command for the container so that it\nruns \nsleep\n instead of running our crashing program. Save and exit the editor,\nand \nkubectl\n will pick up the new definition:\ndeployment.apps/crasher edited\nAfter \nkubectl\n applies this change to the cluster, the Deployment must delete\nthe old Pod and create a new one. This is done automatically, so the only\ndifference we’ll notice is the automatically generated part of the Pod name.\nOf course, we’ll also see the Pod running:\nroot@host01:~# \nkubectl get pods\nNAME                       READY   STATUS    RESTARTS   AGE\ncrasher-58d56fc5df-vghbt   1/1     Running   0          3m29s\nOur Pod is now running, but it’s only running \nsleep\n. We still need to debug\nour actual application. To do that, we can now get a shell prompt inside our\ncontainer:\nroot@host01:~# \nkubectl exec -ti crasher-\n58d56fc5df-vghbt\n -- /bin/sh\n/ #\nThe Deployment replaced the Pod when we changed the definition, so the\nname has changed. As before, use the correct name for your Pod. At this\npoint we can try out our crashing program manually:\n/ # \n/crasher\nSegmentation fault (core dumped)\nIn many cases, the ability to run a program this way, playing with\ndifferent environment variables and command line options, may be enough to\nfind and fix the problem. Alternatively, we could try running the program\nwith \nstrace\n, which would tell us what system calls the program is trying to\nmake and what files it is trying to open prior to crashing. In this case, we\nknow that the program is crashing with a segmentation fault, meaning that the\nproblem is likely a programming error, so our best approach is to connect a\ndebugging tool to the application using port forwarding.\nDebugging Using Port Forwarding\nWe’ll illustrate port forwarding using the text-based debugger \ngdb\n, but any\ndebugger that can connect via a network port will work. First, we need to get\nour application created inside the container using a debugger that will listen\non a network port and wait before it runs the code. To do that, we’ll need to\ninstall \ngdb\n inside our container. Because this is an Alpine container, we’ll use\napk\n:\n/ # \napk add gdb\n...\n(13/13) Installing gdb (10.1-r0)\nExecuting busybox-1.32.1-r3.trigger\nOK: 63 MiB in 27 packages\nThe version of \ngdb\n we installed includes \ngdbserver\n, which enables us to start\na networked debug session.\nBecause \ngdb\n is a text-based debugger, we could obviously just start it\ndirectly to debug our application, but it is often nicer to use a debugger with a\nGUI, making it easier for us to step through source, set breakpoints, and\nwatch variables. For this reason, I’m showing the process for connecting a\ndebugger over the network.\nLet’s start \ngdbserver\n and set it up to listen on port \n2345\n:\n/ # \ngdbserver localhost:2345 /crasher\nProcess /crasher created; pid = 25\nListening on port 2345\nNote that we told \ngdbserver\n to listen to the \nlocalhost\n interface. We’ll still be\nable to connect to the debugger because we’ll have Kubernetes provide us\nwith port forwarding with the \nkubectl port-forward\n command. This command\ncauses \nkubectl\n to connect to the API server and request it to forward traffic to a\nspecific port on a specific Pod. The advantage is that we can use this port\nforwarding capability from anywhere we can connect to the API server, even\noutside the cluster.\nUsing port forwarding specifically to run a remote debugger may not be\nan everyday occurrence for either a Kubernetes cluster administrator or the\ndeveloper of a containerized application, but it’s a valuable skill to have\nwhen there’s no other way to find the bug. It’s also a great way to illustrate\nthe power of port forwarding to reach a Pod.\nBecause we have our debugger running in our first terminal, we’ll need\nanother terminal tab or window for the port forwarding, which can be done\nfrom any of the hosts in our cluster. Let’s use \nhost01\n:\nroot@host01:~# \nkubectl port-forward \npods/crasher-58d56fc5df-vghbt\n 2345:2345\nForwarding from 127.0.0.1:2345 -> 2345\nForwarding from [::1]:2345 -> 2345\nThis \nkubectl\n command starts listening on port \n2345\n and forwards all traffic\nthrough the API server to the Pod we specified. Because this command keeps\nrunning, we need yet another terminal window or tab for our final step, which\nis to run the debugger we’ll use to connect to our debug server running in the\ncontainer. This must be done from the same host as our \nkubectl port-forward\ncommand because that program is listening only on local interfaces.\nAt this point, we could run any debugger that knows how to talk to the\ndebug server. For simplicity, we’ll use \ngdb\n again. We’ll begin by changing to\nthe \n/opt\n directory because our C source file is there:\nroot@host01:~# \ncd /opt\nNow we can kick off \ngdb\n and use it to connect to the debug server:\nroot@host01:/opt# \ngdb -q\n(gdb) \ntarget remote localhost:2345\nRemote debugging using localhost:2345\n...",17496
54-API Server.pdf,54-API Server,"Reading /crasher from remote target...\nReading symbols from target:/crasher...\n0x0000000000401bc0 in _start ()\nOur debug session connects successfully and is waiting for us to start the\nprogram, which we’ll do by using the \ncontinue\n command:\n(gdb) \ncontinue\nContinuing.\nProgram received signal SIGSEGV, Segmentation fault.\nmain () at crasher.c:3\n3         s[2] = '3';\nWith the debugger, we’re able to see exactly which line of our source\ncode is causing the segmentation fault, and now we can figure out how to fix\nit.\nFinal Thoughts\nWhen we move our application components into container images and run\nthem in a Kubernetes cluster, we gain substantial benefits in scalability and\nautomated failover, but we introduce a number of new possibilities that can\ngo wrong when getting our application running, and we introduce new\nchallenges in debugging those problems. In this chapter, we’ve looked at how\nto use Kubernetes commands to systematically track our application startup\nand operation to determine what is preventing it from working correctly.\nWith these commands, we can debug any kind of issue happening at the\napplication level, even if an application component won’t start correctly in its\ncontainerized environment.\nNow that we have a clear picture of running containers using Kubernetes,\nwe can begin to look in depth into the capabilities of the cluster itself. As we\ndo this, we’ll be sure to explore how each component works so as to have the\ntools needed to diagnose problems. We’ll start in the next chapter by looking\nin detail at the Kubernetes control plane.\n11\nCONTROL PLANE AND ACCESS CONTROL\nThe control plane manages the Kubernetes cluster, storing the desired state of\napplications, monitoring the current state to detect and recover from any\nissues, scheduling new containers, and configuring network routing. In this\nchapter, we’ll look closely at the API server, the primary interface for the\ncontrol plane and the entry point for any status retrieval and changes made to\nthe entire cluster.\nAlthough we will focus on the API server, the control plane includes\nmultiple other services, each with a role to play. The other control plane\nservices act as clients to the API server, watching cluster changes and taking\nappropriate action to update the state of the cluster. The following list\ndescribes the other control plane components:\nScheduler\n Assigns each new Pod to a node.\nController manager\n Has multiple responsibilities, including creating Pods\nfor Deployments, monitoring nodes, and reacting to outages.\nCloud controller manager\n This optional component interfaces with an\nunderlying cloud provider to check on nodes and configure network traffic\nrouting.\nAs we demonstrate the workings of the API server, we’ll also see how\nKubernetes manages security to ensure that only authorized users and\nservices can query the cluster and make changes. The purpose of a container\norchestration environment like Kubernetes is to provide a platform for any\nkind of containerized application we might need to run, so this security is\ncritically important to ensure that the cluster is used only as intended.\nAPI Server\nDespite its centrality to the Kubernetes architecture, the API server’s purpose\nis simple. It exposes an interface using HTTP and representational state\ntransfer (REST) to perform basic creation, retrieval, update, and deletion of\nresources in the cluster. It performs authentication to identify clients,\nauthorization to ensure that clients have permission for the specific request,\nand validation to ensure that any created or updated resources match the\ncorresponding specification. It also reads from and writes to a data store\nbased on the commands it receives from clients.\nHowever, the API server is not responsible for actually updating the\ncurrent state of the cluster to match the desired state. That is the responsibility\nof other control plane and node components. For example, if a client creates a\nnew Kubernetes Deployment, the API server’s job is solely to update the data\nstore with the resource information. It is then the job of the scheduler to\ndecide where the Pods will run, and the job of the \nkubelet\n service on the\nassigned nodes to create and monitor the containers and to configure\nnetworking to route traffic to the containers.\nFor this chapter, we have a three-node Kubernetes cluster configured by\nour automation scripts. Each of the three nodes acts as a control plane node,\nso three copies of the API server are running. We can communicate with any\nof these three because they all share the same backend database. The API\nserver is listening for secure HTTP connections on port 6443, the default\nport.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nWe’ve been using \nkubectl\n to communicate with the API server to create and\ndelete resources and retrieve status, and \nkubectl\n has been using secure HTTP\non port 6443 to talk to the cluster. It knows to do this because of a\nKubernetes configuration file that was installed into \n/etc/kubernetes\n by\nkubeadm\n when the cluster was initialized. This configuration file also contains\nauthentication information that gives us permission to read cluster status and\nmake changes.\nBecause the API server is expecting secure HTTP, we can use \ncurl\n to\ncommunicate directly with the Kubernetes API. This will give us a better feel\nfor how the communication actually works. Let’s begin with a simple \ncurl\ncommand:\nroot@host01:~# \ncurl https://192.168.61.11:6443/\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\nMore details here: https://curl.se/docs/sslcerts.html\n...\nThis error message shows that \ncurl\n does not trust the certificate that the\nAPI server is offering. We can use \ncurl\n to see this certificate:\nroot@host01:~# \ncurl -kv https://192.168.61.11:6443/\n...\n* Server certificate:\n*  subject: CN=kube-apiserver\n...\n*  issuer: CN=kubernetes\n...\nThe \n-k\n option tells \ncurl\n to ignore any certificate issues, whereas \n-v\n tells \ncurl\nto provide us with extra logging information about the connection.\nFor \ncurl\n to trust this certificate, it will need to trust the \nissuer\n, as the issuer is\nthe signer of the certificate. Let’s fetch the certificate from our Kubernetes\ninstallation so that we can point \ncurl\n to it:\nroot@host01:~# \ncp /etc/kubernetes/pki/ca.crt .\nBe sure to add the \n.\n at the end to copy this file to the current directory.\nWe’re doing this solely to make the following commands easier to type.\nLet’s examine this certificate before we use it:\nroot@host01:~# \nopenssl x509 -in ca.crt -text\nCertificate:",6860
55-API Server Authentication.pdf,55-API Server Authentication,"...\n        Issuer: CN = kubernetes\n...\n        Subject: CN = kubernetes\nThe \nIssuer\n and the \nSubject\n are the same, so this is a \nself-signed\n certificate. It\nwas created by \nkubeadm\n when we initialized this cluster. Using a generated\ncertificate allows \nkubeadm\n to adapt to our particular cluster networking\nconfiguration and allows our cluster to have a unique certificate and key\nwithout requiring an external certificate authority (CA). However, it does\nmean that we need to configure \nkubectl\n to trust this certificate on any system\nfor which we need to communicate with this API server.\nWe can now tell \ncurl\n to use this certificate to verify the API server:\nroot@host01:~# \ncurl --cacert ca.crt https://192.168.61.11:6443/\n{\n...\n  ""status"": ""Failure"",\n  ""message"": ""forbidden: User \""system:anonymous\"" cannot get path \""/\"""",\n...\n  ""code"": 403\n}\nNow that we’re providing \ncurl\n with the correct root certificate, \ncurl\n can\nvalidate the API server certificate and we can successfully connect to the API\nserver. However, the API server responds with a 403 error, indicating that we\nare not authorized. This is because at the moment we are not providing any\nauthentication information for \ncurl\n to pass to the API server, so the API server\nsees us as an anonymous user.\nOne final note: for this \ncurl\n command to work, we need to be selective in\nthe hostname or IP address we use. The API server is listening on all network\ninterfaces, so we could connect to it using \nlocalhost\n or \n127.0.0.1\n. However, those\nare not listed in the \nkube-apiserver\n certificate and cannot be used for secure\nHTTP because \ncurl\n will not trust the connection.\nAPI Server Authentication\nWe need to provide authentication information before the API server will\naccept our requests, so let’s understand the API server’s process for\nauthentication. Authentication is handled through a set of plug-ins, each of\nwhich looks at the request to determine whether it can identify the client. The\nfirst plug-in that successfully identifies the client provides identity\ninformation to the API server. This identity is then used with authorization to\ndetermine what the client is allowed to do.\nBecause authentication is based on plug-ins, it’s possible to have as many\ndifferent ways of authenticating clients as needed. It’s even possible to add a\nproxy in front of the API server that performs custom authentication logic\nand passes the user’s identity to the API server in an HTTP header.\nFor our purposes, we’ll focus on three authentication primary plug-ins that\nare used within the cluster itself or as part of the cluster setup process: \nclient\ncertificates\n, \nbootstrap tokens\n, and \nservice accounts\n.\nClient Certificates\nAs mentioned previously, an HTTP client like \ncurl\n validates the server’s\nidentity by comparing the server’s hostname to its certificate and also by\nchecking the certificate’s signature against a list of trusted CAs. In addition to\nchecking the server identity, secure HTTP also allows a client to submit a\ncertificate to the server. The server checks the signature against its list of\ntrusted authorities and then uses the subject of the certificate as the client’s\nidentity.\nKubernetes uses HTTP client certificate authentication extensively to\nenable cluster services to authenticate with the API server. This includes\ncontrol plane components as well as the \nkubelet\n service running on each node.\nWe can use \nkubeadm\n to list the certificates used by the control plane:\nroot@host01:~# \nkubeadm certs check-expiration\n...\nCERTIFICATE                ...  RESIDUAL TIME   CERTIFICATE AUTHORITY ...\nadmin.conf                 ...  363d                                  ...\napiserver                  ...  363d            ca                    ...\napiserver-etcd-client      ...  363d            etcd-ca               ...\napiserver-kubelet-client   ...  363d            ca                    ...\ncontroller-manager.conf    ...  363d                                  ...\netcd-healthcheck-client    ...  363d            etcd-ca               ...\netcd-peer                  ...  363d            etcd-ca               ...\netcd-server                ...  363d            etcd-ca               ...\nfront-proxy-client         ...  363d            front-proxy-ca        ...\nscheduler.conf             ...  363d                                  ...\n...\nThe \nRESIDUAL TIME\n column shows how much time is left before these\ncertificates expire; by default, they expire after one year. Use \nkubeadm certs renew\nto renew them, passing the name of the certificate as a parameter.\nThe first item in the list, \nadmin.conf\n, is how we’ve been authenticating\nourselves to the cluster in the past few chapters. During initialization, \nkubeadm\ncreated this certificate and stored its information in the\n/etc/kubernetes/admin.conf\n file. Every \nkubectl\n command we’ve run has been\nusing this file because our automation scripts are setting the \nKUBECONFIG\nenvironment variable:\nroot@host01:~# \necho $KUBECONFIG\n/etc/kubernetes/admin.conf\nIf we had not set \nKUBECONFIG\n, \nkubectl\n would be using the default, which is a\nfile called \n.kube/config\n in the user’s home directory.\nThe \nadmin.conf\n credentials are designed to provide emergency access to\nthe cluster, bypassing authorization. In a production cluster, we would avoid\nusing these credentials directly for everyday operations. Instead, the best\npractice for a production cluster is to integrate a separate identity manager for\nadministrators and normal users. For our example, because we don’t have a\nseparate identity manager, we’ll instead create an additional certificate for a\nregular user. This kind of certificate may be useful for an automated process\nthat runs outside the cluster, but it can’t integrate with the identity manager.\nWe can create a new client certificate using \nkubeadm\n:\nroot@host01:~# \nkubeadm kubeconfig user --client-name=me \\n  \n--config /etc/kubernetes/kubeadm-init.yaml > kubeconfig\nThe \nkubeadm kubeconfig user\n command asks the API server to generate a new\nclient certificate. Because this certificate is signed by the cluster’s CA, it is\nvalid for authentication. The certificate is saved into the \nkubeconfig\n file along\nwith the necessary configuration to connect to the API server:\nroot@host01:~# \ncat kubeconfig\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: ...\n    server: https://192.168.61.10:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: me\n  name: me@kubernetes\ncurrent-context: me@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: me\n  user:\n    client-certificate-data: ...\n    client-key-data: ...\nThe \nclusters\n section defines the information needed to connect to the API\nserver, including the load-balanced address shared by all three API servers in\nour highly available configuration. The \nusers\n section defines the new user we\ncreated along with its client certificate.\nThus far, we’ve successfully created a new user, but we haven’t given that\nuser any permissions yet, so we won’t be very successful using these\ncredentials:\nroot@host01:~# \nKUBECONFIG=kubeconfig kubectl get pods\nError from server (Forbidden): pods is forbidden: User ""me"" cannot list \n  resource ""pods"" in API group """" in the namespace ""default""\nLater in the chapter, we’ll see how to give permissions to this user.\nBootstrap Tokens\nInitializing a distributed system like a Kubernetes cluster is challenging. The\nkubelet\n service running on each node must be added to the cluster. To do this,\nkubelet\n must connect to the API server and obtain a client certificate signed by\nthe cluster’s CA. The \nkubelet\n service then uses this client certificate to\nauthenticate to the cluster.\nThis certificate generation must be done securely so that we eliminate \nthe\npossibility of adding rogue nodes to the cluster and eliminate the possibility\nof a rogue process being able to impersonate a real node. For this reason, the\nAPI server cannot provide a certificate for just any node that asks to be added\nto the cluster. Instead, the node must generate its own private key, submit a\ncertificate signing request (CSR) to the API server, and receive a signed\ncertificate.\nTo keep this process secure, we need to ensure that a node is authorized to\nsubmit a certificate signing request. But this submission must happen before\nthe node has the client certificate that it uses for more permanent\nauthentication—we have a chicken-or-egg problem! Kubernetes solves this\nvia time-limited tokens, known as \nBootstrap Tokens\n. The bootstrap token\nbecomes a preshared secret that is known to the API server and the new\nnodes. Making this token time limited reduces the risk to the cluster if it is\nexposed. The Kubernetes controller manager has the task of automatically\ncleaning up bootstrap tokens when they expire.\nWhen we initialized our cluster, \nkubeadm\n created a bootstrap token, but it\nwas configured to expire after two hours. If we need to join additional nodes\nto the cluster after that, we can use \nkubeadm\n to generate a new bootstrap token:\nroot@host01:~# \nTOKEN=$(kubeadm token create)\nroot@host01:~# \necho $TOKEN\npqcnd6.4wawyqgkfaet06zm\nThis token is added as a Kubernetes \nSecret\n in the \nkube-system\n Namespace.\nWe look at secrets in more detail in \nChapter 16\n. For now, let’s just verify that\nit exists:\nroot@host01:~# \nkubectl -n kube-system get secret\nNAME                    TYPE                           DATA   AGE\n...\nbootstrap-token-pqcnd6  bootstrap.kubernetes.io/token  6      64s\n...\nWe can use this token to make requests of the API server by using HTTP\nBearer authentication. This means that we provide the token in an HTTP\nheader called \nAuthorization\n, prefaced with the word \nBearer\n. When the bootstrap\ntoken authentication plug-in sees that header and matches the provided token\nagainst the corresponding secret, it authenticates us to the API server and\nallows us access to the API.\nFor security reasons, bootstrap tokens have access only to the certificate\nsigning request functionality of the API server, so that’s all our token will be\nallowed to do.\nLet’s use our bootstrap token to list all of the certificate signing requests:\nroot@host01:~# \ncurl --cacert ca.crt \\n  \n-H ""Authorization: Bearer $TOKEN"" \\n  \nhttps://192.168.61.11:6443/apis/certificates.k8s.io/v1/certificatesigningrequests\n{\n  ""kind"": ""CertificateSigningRequestList"",\n  ""apiVersion"": ""certificates.k8s.io/v1"",\n  ""metadata"": {\n    ""resourceVersion"": ""21241""\n  },\n  ""items"": [\n...\n  ]\n}\nIt’s important to know how bootstrap tokens work, given that they’re\nessential to adding nodes to the cluster. However, as the name implies, that’s\nreally the only purpose for a bootstrap token; it’s not typical to use them for\nnormal API server access. For normal API server access, especially from\ninside the cluster, we need a \nServiceAccount\n.\nService Accounts\nContainers running in the Kubernetes cluster often need to communicate with\nthe API server. For example, all of the various components we deployed on\ntop of our cluster in \nChapter 6\n, including the Calico network plug-in, the\nLonghorn storage driver, and the metrics server, communicate with the API\nserver to watch and modify the cluster state. To support this, Kubernetes\nautomatically injects credentials into every running container.\nOf course, for security reasons, giving each container only the API server\npermissions it requires is important, so we should create a separate\nServiceAccount for each application or cluster component to do that. The\ninformation for these ServiceAccounts is then added to the Deployment or\nother controller so that Kubernetes will inject the correct credentials. In some\ncases, we may use multiple ServiceAccount with a single application,\nrestricting each application component to only the access it needs.\nIn addition to using a separate ServiceAccount per application or\ncomponent, it’s also good practice to use a separate Namespace per\napplication. As we’ll see in a moment, permissions can be limited to a single\nNamespace. Let’s start by creating the Namespace:\nroot@host01:~# \nkubectl create namespace sample\nnamespace/sample created\nA ServiceAccount uses a bearer token, which is stored in a secret\nautomatically generated by Kubernetes when the ServiceAccount is created.\nLet’s make a ServiceAccount for a Deployment that we’ll create in this\nchapter:\nread-pods-sa.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: read-pods\n  namespace: sample\nNote that we use the metadata to place this ServiceAccount in the \nsample\nNamespace we just created. We could also use the \n-n\n flag with \nkubectl\n to\nspecify the Namespace. We’ll use the usual \nkubectl apply\n to create this\nServiceAccount:\nroot@host01:~# \nkubectl apply -f /opt/read-pods-sa.yaml\nserviceaccount/read-pods created\nWhen the ServiceAccount is created, the controller manager detects this\nand automatically creates a Secret with the credentials:\nroot@host01:~# \nkubectl -n sample get serviceaccounts\nNAME        SECRETS   AGE\ndefault     1         27s\nread-pods   1         8s\nroot@host01:~# \nkubectl -n sample get secrets\nNAME                    TYPE                                  DATA   AGE\ndefault-token-mzwpt     kubernetes.io/service-account-token   3      43s\nread-pods-token-m4scq   kubernetes.io/service-account-token   3      25s\nNote that in addition to the \nread-pods\n ServiceAccount we just created, there\nis already a \ndefault\n ServiceAccount. This account was created automatically\nwhen the Namespace was created; it will be used if we don’t specify to\nKubernetes which ServiceAccount to use for a Pod.\nThe newly created ServiceAccount does not have any permissions yet. To\nstart adding permissions, we need to take a look at \nrole-based access control\n(RBAC).",14144
56-Role-Based Access Controls.pdf,56-Role-Based Access Controls,"Role-Based Access Controls\nAfter the API server has found an authentication plug-in that can identify the\nclient, it uses the identity to determine whether the client has permissions to\nperform the desired action, which is done by assembling a list of roles that\nbelong to the user. Roles can be associated directly with a user or with a\ngroup in which the user is a member. Group membership is part of the\nidentity. For example, client certificates can specify a user’s groups by\nincluding organization fields as part of the certificate’s subject.\nRoles and Cluster Roles\nEach role has a set of permissions. A permission allows a client to perform\none or more actions on one or more types of resources.\nAs an example, let’s define a role that will give a client permission to read\nPod status. We have two choices: we can create a \nRole\n or a \nClusterRole\n. A\nRole is visible and usable within a single Namespace, whereas a ClusterRole\nis visible and usable across all Namespaces. This difference allows\nadministrators to define common roles across the cluster that are immediately\navailable when new Namespaces are created, while also allowing the\ndelegation of access control for a specific Namespace.\nHere’s an example definition of a ClusterRole. This role only has the\nability to read data about Pods; it cannot change Pods or access any other\ncluster information:\npod-reader.yaml\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-reader\nrules:\n- apiGroups: [""""]\n  resources: [""pods""]\n  verbs: [""get"", ""watch"", ""list""]\nBecause this is a cluster-wide role, it doesn’t make sense to assign it to a\nNamespace, so we don’t specify one.\nThe critical part of this definition is the list of rules. Each ClusterRole or\nRole can have as many rules as necessary. Each rule has a list of \nverbs\n that\ndefine what actions are allowed. In this case, we identified \nget\n, \nwatch\n, and \nlist\n as\nthe verbs, with the effect that the role allows reading Pods but not any actions\nthat would modify them.\nEach rule applies to one or more resource types, based on the combination\nof \napiGroups\n and \nresources\n identified. Each rule gives permissions for the actions\nlisted as \nverbs\n. In this case, the empty string \n""""\n is used to refer to the default\nAPI group, which is where Pods are located. If we wanted to also include\nDeployments and StatefulSets, we would need to define our rule as follows:\n- apiGroups: ["""", ""apps""]\n  resources: [""pods"", ""deployments"", ""statefulsets""]\n  verbs: [""get"", ""watch"", ""list""]\nWe need to add \n""apps""\n to the \napiGroups\n field because Deployment and\nStatefulSet are part of that group (as identified in the \napiVersion\n when we\ndeclare the resource). When we declare a Role or ClusterRole, the API server\nwill accept any strings in the \napiGroups\n and \nresources\n fields, regardless of whether\nthe combination actually identifies any resource types, so it’s important to\npay attention to which group a resource is in.\nLet’s define our \npod-reader\n ClusterRole:\nroot@host01:~# \nkubectl apply -f /opt/pod-reader.yaml\nclusterrole.rbac.authorization.k8s.io/pod-reader created\nNow that the ClusterRole exists, we can apply it. To do that, we need to\ncreate a role binding.\nRole Bindings and Cluster Role Bindings\nLet’s apply this \npod-reader\n ClusterRole to the \nread-pods\n ServiceAccount we\ncreated earlier. We have two options: we can create a \nRoleBinding\n, which\nwill assign the permissions in a specific Namespace, or a\nClusterRoleBinding\n, which will assign the permissions across all\nNamespaces. This feature is beneficial because it means we can create a\nClusterRole such as \npod-reader\n once and have it visible across the cluster, but\ncreate the binding in an individual Namespace so that users and\nServiceAccount are restricted to only the Namespaces they should be allowed\nto access. This helps us apply the pattern we saw earlier of having a\nNamespace per application, while at the same time it keeps non-\nadministrators away from key infrastructure components such as the\ncomponents running in the \nkube-system\n Namespace.\nIn keeping with this practice, we’ll create a RoleBinding so that our\nServiceAccount has permissions to read Pods only in the \nsample\n Namespace:\nread-pods-bind.yaml\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: sample\nsubjects:\n- kind: ServiceAccount\n  name: read-pods\n  namespace: sample\nroleRef:\n  kind: ClusterRole\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\nNot surprisingly, a RoleBinding ties together a Role or a ClusterRole and\na subject. The RoleBinding can contain multiple subjects, so we can bind the\nsame role to multiple users or groups with a single binding.\nWe define a Namespace in both the metadata and where we identify the\nsubject. In this case, these are both \nsample\n, as we want to grant the\nServiceAccount the ability to read Pod status in its own Namespace.\nHowever, these could be different to allow a ServiceAccount in one\nNamespace to have specific permissions in another Namespace. And of\ncourse we could also use a ClusterRoleBinding to give out permissions across\nall Namespaces.\nWe can now create the RoleBinding:\nroot@host01:~# \nkubectl apply -f /opt/read-pods-bind.yaml\nrolebinding.rbac.authorization.k8s.io/read-pods created\nWe’ve now given permission for the \nread-pods\n ServiceAccount to read Pods\nin the \nsample\n Namespace. To demonstrate how it works, we need to create a\nPod that is assigned to the \nread-pods\n ServiceAccount.\nAssigning a Service Account to Pods\nTo assign a ServiceAccount to a Pod, just add the \nserviceAccountName\n field to the\nPod spec:\nread-pods-deploy.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: read-pods\n  namespace: sample\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: read-pods\n  template:\n    metadata:\n      labels:\n        app: read-pods\n    spec:\n      containers:\n      - name: read-pods\n        image: alpine\n        command: [""/bin/sleep"", ""infinity""]\n      serviceAccountName: read-pods\nThe ServiceAccount identified must exist in the Namespace that the Pod is\ncreated in. Kubernetes will inject the Pod’s containers with the Service-\nAccount token so that the containers can authenticate to the API server.\nLet’s walk through an example to show how this works and how the\nauthorization is applied. Start by creating this Deployment:\nroot@host01:~# \nkubectl apply -f /opt/read-pods-deploy.yaml\ndeployment.apps/read-pods created\nThis creates an Alpine container running \nsleep\n that we can use as a base for\nshell commands.\nTo get to a shell prompt, we’ll first get the generated name of the Pod and\nthen use \nkubectl exec\n to create the shell:\nroot@host01:~# \nkubectl -n sample get pods\nNAME                        READY   STATUS    RESTARTS   AGE\nread-pods-9d5565548-fbwjb   1/1     Running   0          6s\nroot@host01:~# \nkubectl -n sample exec -ti read-pods-\n9d5565548-fbwjb\n -- /bin/sh\n/ #\nThe ServiceAccount token is mounted in the directory\n/run/secrets/kubernetes.io/serviceaccount\n, so change to that directory and list\nits contents:\n/ # \ncd /run/secrets/kubernetes.io/serviceaccount\n/run/secrets/kubernetes.io/serviceaccount # \nls -l\ntotal 0\nlrwxrwxrwx    1 root     root  ...  ca.crt -> ..data/ca.crt\nlrwxrwxrwx    1 root     root  ...  namespace -> ..data/namespace\nlrwxrwxrwx    1 root     root  ...  token -> ..data/token\nThese files show up as odd looking symbolic links, but the contents are\nthere as expected. The \nca.crt\n file is the root certificate for the cluster, which\nis needed to trust the connection to the API server.\nLet’s save the token in a variable so that we can use it:\n/run/secrets/kubernetes.io/serviceaccount # \nTOKEN=$(cat token)\nWe can now use this token with \ncurl\n to connect to the API server. First,\nthough, we need to install \ncurl\n into our Alpine container:\ndefault/run/secrets/kubernetes.io/serviceaccount # \napk add curl\n...\nOK: 8 MiB in 19 packages\nOur ServiceAccount is allowed to perform \nget\n, \nlist\n, and \nwatch\n operations on\nPods. Let’s list all Pods in the \nsample\n Namespace:\n/run/secrets/kubernetes.io/serviceaccount # \ncurl --cacert ca.crt \\n  \n-H ""Authorization: Bearer $TOKEN"" \\n  \nhttps://kubernetes.default.svc/api/v1/namespaces/sample/pods\n  ""kind"": ""PodList"",\n  ""apiVersion"": ""v1"",\n  ""metadata"": {\n    ""resourceVersion"": ""566610""\n  },\n  ""items"": [\n    {\n      ""metadata"": {\n        ""name"": ""read-pods-9d5565548-fbwjb"",\n...\n  ]\n}\nAs with the bootstrap token, we use HTTP Bearer authentication to pass\nthe ServiceAccount token to the API server. Because we’re operating from\ninside a container, we can use the standard address \nkubernetes.default.svc\n to find\nthe API server. This works because a Kubernetes cluster always has a service\nin the \ndefault\n Namespace that routes traffic to API server instances using the\nService networking we saw in \nChapter 9\n.\nThe \ncurl\n command is successful because our ServiceAccount is bound to\nthe \npod-reader\n Role we created. However, the RoleBinding is limited to the\nsample\n Namespace, and as a result, we aren’t allowed to list Pods in a different\nNamespace:\n/run/secrets/kubernetes.io/serviceaccount # \ncurl --cacert ca.crt \\n  \n-H ""Authorization: Bearer $TOKEN"" \\n  \nhttps://kubernetes.default.svc/api/v1/namespaces/kube-system/pods\n{\n  ""kind"": ""Status"",\n  ""apiVersion"": ""v1"",\n  ""metadata"": {\n  },\n  ""status"": ""Failure"",\n  ""message"": ""pods is forbidden: User \n    \""system:serviceaccount:default:read-pods\"" cannot list resource \n    \""pods\"" in API group \""\"" in the namespace \""kube-system\"""",\n  ""reason"": ""Forbidden"",\n  ""details"": {\n    ""kind"": ""pods""\n  },\n  ""code"": 403\n}\nWe can use the error message to be certain that our ServiceAccount\nassignment and authentication worked as expected because the API server\nrecognizes us as the \nread-pods\n ServiceAccount. However, we don’t have a\nRoleBinding with the right permissions to read Pods in the \nkube-system\nNamespace, so the request is rejected.\nSimilarly, because we have permission only for Pods, we can’t list our\nDeployment, even though it is also in the \nsample\n Namespace:\n/run/secrets/kubernetes.io/serviceaccount # \ncurl --cacert ca.crt \\n  \n-H ""Authorization: Bearer $TOKEN"" \\n  \nhttps://kubernetes.default.svc/apis/apps/v1/namespaces/sample/deploy\nments\n{\n  ""kind"": ""Status"",\n  ""apiVersion"": ""v1"",\n  ""metadata"": {\n  },\n  ""status"": ""Failure"",\n  ""message"": ""deploy.apps is forbidden: User \n    \""system:serviceaccount:default:read-pods\"" cannot list resource \n    \""deploy\"" in API group \""apps\"" in the namespace \""sample\"""",\n  ""reason"": ""Forbidden"",\n  ""details"": {\n    ""group"": ""apps"",\n    ""kind"": ""deploy""\n  },\n  ""code"": 403\n}\nThe slightly different path scheme for the URL, starting with\n/apis/apps/v1\n instead of \n/api/v1\n, is needed because Deployments are in the\napps\n API group rather than the default group. This command fails in a similar\nway because we don’t have the necessary permissions to list Deployments.\nWe’re finished with this shell session, so let’s exit it:\n/run/secrets/kubernetes.io/serviceaccount # \nexit\nBefore we leave the RBAC topic, though, let’s illustrate an easy way to\ngrant normal user permissions for a Namespace without allowing any\nadministrator functions.\nBinding Roles to Users\nTo grant normal user permissions, we’ll leverage an existing ClusterRole\ncalled \nedit\n that’s already set up to grant view and edit permissions for most of\nthe resource types users need.\nLet’s take a quick look at the \nedit\n ClusterRole to see what permissions it\nhas:\nroot@host01:~# \nkubectl get clusterrole edit -o yaml\n...\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\n...\nrules:\n...\n- apiGroups:\n  - """"\n  resources:\n  - pods\n  - pods/attach\n  - pods/exec\n  - pods/portforward\n  - pods/proxy\n  verbs:\n  - create\n  - delete\n  - deletecollection\n  - patch\n  - update\n...\nThe full list has a large number of different rules, each with its own set of\npermissions. The subset in this example shows just one rule, used to provide\nedit permission for Pods.\nSome commands related to Pods, such as \nexec\n, are listed separately to\nallow for more granular control. For example, for a production system, it can\nbe useful to allow some individuals the ability to create and delete Pods and\nsee logs, but not provide the ability to use \nexec\n, because that might be used to\naccess sensitive production data.\nPreviously, we created a user called \nme\n and saved the client certificate to a\nfile called \nkubeconfig\n. However, we didn’t bind any roles to that user yet, so\nthe user has only the very limited permissions that come with automatic\nmembership in the \nsystem:authenticated\n group.\nAs a result, as we saw earlier, our normal user can’t even list Pods in the\ndefault\n Namespace. Let’s bind this user to the edit role. As before, we’ll use a\nregular RoleBinding, scoped to the \nsample\n Namespace, so this user won’t be\nable to access our cluster infrastructure components in the \nkube-system\nNamespace.\nListing 11-1\n presents the RoleBinding we need.\nedit-bind.yaml\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: editor\n  namespace: sample\nsubjects:",13577
57-Final Thoughts.pdf,57-Final Thoughts,"- kind: User\n  name: me\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole \n  name: edit\n  apiGroup: rbac.authorization.k8s.io\nListing 11-1: Bind the edit role to a user\nNow we apply this RoleBinding to add permissions to our user:\nroot@host01:~# \nkubectl apply -f /opt/edit-bind.yaml\nrolebinding.rbac.authorization.k8s.io/editor created\nWe’re now able to use this user to view and modify Pods, Deployments,\nand many other resources:\nroot@host01:~# \nKUBECONFIG=kubeconfig kubectl -n sample get pods\nNAME                        READY   STATUS    RESTARTS   AGE\nread-pods-9d5565548-fbwjb   1/1     Running   0          54m\nroot@host01:~# \nKUBECONFIG=kubeconfig kubectl delete -f /opt/read-pods-deploy.yaml\ndeployment.apps ""read-pods"" deleted\nHowever, because we used a RoleBinding and not a ClusterRoleBinding,\nthis user has no visibility into other Namespaces:\nroot@host01:~# \nKUBECONFIG=kubeconfig kubectl get -n kube-system pods\nError from server (Forbidden): pods is forbidden: User ""me"" cannot list \n  resource ""pods"" in API group """" in the namespace ""kube-system""\nThe error message displayed by \nkubectl\n is identical in form to the \nmessage\nfield that is part of the API server’s JSON response. This is not a\ncoincidence; \nkubectl\n is a friendly command line interface in front of the API\nserver’s REST API.\nFinal Thoughts\nThe API server is an essential component in the Kubernetes control plane.\nEvery other service in the cluster is continuously connected to the API server,\nwatching the cluster for changes, so it can take appropriate action. Users also\nuse the API server to deploy and configure applications and to monitor state.\nIn this chapter, we saw the underlying REST API that the API server\nprovides to create, retrieve, update, and delete resources. We also saw the\nextensive authentication and authorization capabilities built in to the API\nserver to ensure that only authorized users and services can access and\nmodify the cluster state.\nIn the next chapter, we’ll examine the other side of our cluster’s\ninfrastructure: the node components. We’ll see how the \nkubelet\n Service hides\nany differences between container engines and how it uses the container\ncapabilities we saw in \nPart I\n to create, start, and configure containers in the\ncluster.",2336
58-12 CONTAINER RUNTIME.pdf,58-12 CONTAINER RUNTIME,,0
59-Node Service.pdf,59-Node Service,"12\nCONTAINER RUNTIME\nIn the previous chapter, we saw how the control plane manages and monitors\nthe state of the cluster. However, it is the container runtime, especially the\nkubelet\n service, that creates, starts, stops, and deletes containers to actually\nbring the cluster to the desired state. In this chapter, we’ll explore how \nkubelet\nis configured in our cluster and how it operates.\nAs part of this exploration, we’ll address how \nkubelet\n manages to host the\ncontrol plane while also being dependent on it. Finally, we’ll look at node\nmaintenance in a Kubernetes cluster, including how to shut down a node for\nmaintenance, issues that can prevent a node from working correctly, how the\ncluster behaves if a node suddenly becomes unavailable, and how the node\nbehaves when it loses its cluster connection.\nNode Service\nThe primary service that turns a regular host into a Kubernetes node is \nkubelet\n.\nBecause of its criticality to a Kubernetes cluster, we’ll look in detail at how it\nis configured and how it behaves.\nCONTAINERD AND CRI-O\nThe examples for this chapter provide automated scripts to launch a\ncluster using either of two container runtimes: \ncontainerd\n and CRI-O.\nWe’ll primarily use the \ncontainerd\n installation, though we’ll briefly look at\nthe configuration difference. The CRI-O cluster is there to allow you to\nexperiment with a separate container runtime. It also illustrates the fact\nthat \nkubelet\n hides this difference from the rest of the cluster, as the rest of\nthe cluster configuration is unaffected by a container runtime change.\nWe installed \nkubelet\n as a package on all of our nodes when we set up our\ncluster in \nChapter 6\n, and the automation has been setting it up similarly for\neach chapter thereafter.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nThe \nkubelet\n package also includes a system service. Our operating system is\nusing \nsystemd\n to run services, so we can get service information using \nsystemctl\n:\nroot@host01:~# \nsystemctl status kubelet\n  kubelet.service - kubelet: The Kubernetes Node Agent\n     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; ...\n    Drop-In: /etc/systemd/system/kubelet.service.d\n               10-kubeadm.conf\n     Active: active (running) since ...\nThe first time \nkubelet\n started, it didn’t have the configuration needed to join\nthe cluster. When we ran \nkubeadm\n, it created the file \n10-kubeadm.conf\n shown\nin the preceding output. This file configures the \nkubelet\n service for the cluster\nby setting command line parameters.\nListing 12-1\n gives us a look at the command line parameters that are\npassed to the \nkubelet\n service.\nroot@host01:~# \nstrings /proc/$(pgrep kubelet)/cmdline\n/usr/bin/kubelet\n--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf\n--kubeconfig=/etc/kubernetes/kubelet.conf\n--config=/var/lib/kubelet/config.yaml\n--container-runtime=remote\n--container-runtime-endpoint=/run/containerd/containerd.sock\n--node-ip=192.168.61.11\n--pod-infra-container-image=k8s.gcr.io/pause:3.4.1\nListing 12-1: Kubelet command line\nThe \npgrep kubelet\n embedded command outputs the process ID of the \nkubelet\nservice. We then use this to print the command line of the process using the\n/proc\n Linux virtual filesystem. We use \nstrings\n to print this file rather than \ncat\nbecause each separate command line parameter is null-terminated and \nstrings\nturns this into a nice multiline display.\nThe \nkubelet\n service needs three main groups of configuration options:\ncluster configuration\n, \ncontainer runtime configuration\n, and \nnetwork\nconfiguration\n.\nKubelet Cluster Configuration\nThe cluster configuration options tell \nkubelet\n how to communicate with the\ncluster and how to authenticate. When \nkubelet\n starts for the first time, it uses\nthe \nbootstrap-kubeconfig\n shown in \nListing 12-1\n to find the cluster, verify the\nserver certificate, and authenticate using the bootstrap token we discussed in\nChapter 11\n. This bootstrap token is used to submit a Certificate Signing\nRequest (CSR) for this new node. The \nkubelet\n then downloads the signed client\ncertificate from the API server and stores it in \n/etc/kubernetes/kubelet.conf\n,\nthe location specified by the \nkubeconfig\n option. This \nkubelet.conf\n file follows\nthe same format that is used to configure \nkubectl\n to talk to the API server, as\nwe saw in \nChapter 11\n. After \nkubelet.conf\n has been written, the bootstrap file\nis deleted.\nThe \n/var/lib/kubelet/config.yaml\n file specified in \nListing 12-1\n also\ncontains important configuration information. To pull metrics from \nkubelet\n, we\nneed to set it up with its own server certificate, not just a client certificate,\nand we need to configure how it authenticates its own clients. Here is the\nrelevant content from the configuration file, created by \nkubeadm\n:\nroot@host01:~# \ncat /var/lib/kubelet/config.yaml\n...\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    cacheTTL: 0s\n    enabled: true\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.crt\n...\nThe \nauthentication\n section tells \nkubelet\n not to allow anonymous requests, but to\nallow both webhook bearer tokens as well as any client certificates signed by\nthe cluster certificate authority. The YAML resource file we installed for the\nmetrics server includes a ServiceAccount that is used in its Deployment, so it\nis automatically injected with credentials that it can use to authenticate to\nkubelet\n instances, as we saw in \nChapter 11\n.\nKubelet Container Runtime Configuration\nThe container runtime configuration options tell \nkubelet\n how to connect to the\ncontainer runtime so that \nkubelet\n can manage containers on the local machine.\nBecause \nkubelet\n expects the runtime to support the Container Runtime\nInterface (CRI) standard, only a couple of settings are needed, as shown in\nListing 12-1\n.\nThe first key setting is \ncontainer-runtime\n, which can be set to either \nremote\n or\ndocker\n. Kubernetes predates the separation of the Docker engine from the\ncontainerd\n runtime, so it had legacy support for Docker that used a \nshim\n to\nemulate the standard CRI interface. Because we are using \ncontainerd\n directly\nand not via the Docker shim or Docker engine, we set this to \nremote\n.\nNext, we specify the path to the container runtime using the \ncontainer-runtime-\nendpoint\n setting. The value in this case is \n/run/containerd/containerd.sock\n. The\nkubelet\n connects to this Unix socket to send CRI requests and receive status.\nThe \ncontainer-runtime-endpoint\n command line setting is the only difference\nneeded to switch the cluster between \ncontainerd\n and CRI-O. Additionally, it is\nautomatically detected by \nkubeadm\n when the node is initialized, so the only\ndifference in the automated scripts is to install CRI-O rather than \ncontainerd\nprior to installing Kubernetes. If we look at the command line for \nkubelet\n in\nour CRI-O cluster, we see only one change in the command line options:\nroot@host01:~# \nstrings /proc/$(pgrep kubelet)/cmdline\n...\n--container-runtime-endpoint=/var/run/crio/crio.sock\n...\nThe rest of the command line options are identical to our \ncontainerd\n cluster.\nFinally, we have one more setting that is relevant to the container runtime:\npod-infra-container-image\n. This specifies the Pod infrastructure image. We saw this\nimage in \nChapter 2\n in the form of a \npause\n process that was the owner of Linux\nnamespaces created for our containers. In this case, this \npause\n process will\ncome from the container image \nk8s.gcr.io/pause:3.4.1\n.\nIt’s highly convenient to have a separate container to own the namespaces\nthat are shared between the containers in a Pod. Because the \npause\n process\ndoesn’t really do anything, it is very reliable and isn’t likely to crash, so it can\ncontinue to own these shared namespaces even if the other containers in the\nPod terminate unexpectedly.\nThe \npause\n image clocks in at around 300kb, as we can see by running \ncrictl\non one of our nodes:\nroot@host01:~# \ncrictl images\nIMAGE             TAG                 IMAGE ID            SIZE\n,,,\nk8s.gcr.io/pause  3.4.1               0f8457a4c2eca       301kB\n...\nAdditionally, the \npause\n process uses practically no CPU, so the effect on\nour nodes of having an extra process for every Pod is minimal.\nKubelet Network Configuration\nNetwork configuration helps \nkubelet\n integrate itself into the cluster and to\nintegrate Pods into the overall cluster network. As we saw in \nChapter 8\n, the\nactual Pod network setup is performed by a network plug-in, but the \nkubelet\nhas a couple of important roles as well.\nOur \nkubelet\n command line includes one option relevant to the network\nconfiguration: \nnode-ip\n. It’s an optional flag, and if it is not present, \nkubelet\n will\ntry to determine the IP address it should use to communicate with the API\nserver. However, specifying the flag directly is useful because it guarantees\nthat our cluster works in cases for which nodes have multiple network\ninterfaces (such as the Vagrant configuration in this book’s examples, where\na separate internal network is used for cluster communication).\nIn addition to this one command line option, \nkubeadm\n places two important\nnetwork settings in \n/var/lib/kubelet/config.yaml\n:\nroot@host01:~# \ncat /var/lib/kubelet/config.yaml\n...\nclusterDNS:\n- 10.96.0.10\nclusterDomain: cluster.local\n...\nThese settings are used to provide the \n/etc/resolv.conf\n file to all containers.\nThe \nclusterDNS\n entry provides the IP address of this DNS server, whereas the\nclusterDomain\n entry provides a default domain for searches so that we can\ndistinguish between hostnames inside the cluster and hostnames on external\nnetworks.\nLet’s take a quick look at how these values are provided to the Pod. We’ll\nbegin by creating a Pod:\nroot@host01:~# \nkubectl apply -f /opt/pod.yaml\n \npod/debug created\nAfter a few seconds, when the Pod is running, we can get a shell:\nroot@host01:~# \nkubectl exec -ti debug -- /bin/sh\n/ #\nNotice that \n/etc/resolv.conf\n is a separately mounted file in our container:\n/ # \nmount | grep resolv\n/dev/sda1 on /etc/resolv.conf type ext4 ...\nIts contents reflect the \nkubelet\n configuration:\n/ # \ncat /etc/resolv.conf\n \nsearch default.svc.cluster.local svc.cluster.local cluster.local \nnameserver 10.96.0.10\noptions ndots:5\nThis DNS configuration points to the DNS server that is part of the\nKubernetes cluster core components, enabling the Service lookup we saw in\nChapter 9\n. Depending on the DNS configuration in your network, you might\nsee other items in the \nsearch\n list beyond what is shown here.\nWhile we’re here, note also that \n/run/secrets/kubernetes.io/serviceaccount\nis also a separately mounted directory in our container. This directory\ncontains the ServiceAccount information we saw in \nChapter 11\n to enable",11220
60-Static Pods.pdf,60-Static Pods,"authentication with the API server from within a container:\n/ # \nmount | grep run\ntmpfs on /run/secrets/kubernetes.io/serviceaccount type tmpfs (ro,relatime)\nIn this case, the mounted directory is of type \ntmpfs\n because \nkubelet\n has\ncreated an in-memory filesystem to hold the authentication information.\nLet’s finish by exiting the shell session and deleting the Pod (we no longer\nneed it):\n/ # \nexit\nroot@host01:~# \nkubectl delete pod debug\nThis cleanup will make upcoming Pod listings clearer as we look at how\nthe cluster reacts when a node stops working. Before we do that, we have one\nmore key mystery to solve: how \nkubelet\n can host the control plane and also\ndepend on it.\nStatic Pods\nWe have something of a chicken-or-egg problem with creating our cluster.\nWe want \nkubelet\n to manage the control plane components as Pods because that\nmakes it easier to monitor, maintain, and update the control plane\ncomponents. However, \nkubelet\n is dependent on the control plane to determine\nwhat containers to run. The solution is for \nkubelet\n to support static Pod\ndefinitions that it pulls from the filesystem and runs automatically prior to\nhaving its control plane connection.\nThis static Pod configuration is handled in \n/var/lib/kubelet/config.yaml\n:\nroot@host01:~# \ncat /var/lib/kubelet/config.yaml\n \n...\nstaticPodPath: /etc/kubernetes/manifests\n...\nIf we look in \n/etc/kubernetes/manifests\n, we see a number of YAML files.\nThese files were placed by \nkubeadm\n and define the Pods necessary to run the\ncontrol plane components for this node:\nroot@host01:~# \nls -1 /etc/kubernetes/manifests\netcd.yaml\nkube-apiserver.yaml\nkube-controller-manager.yaml\nkube-scheduler.yaml\nAs expected, we see a YAML file for each of the three essential control\nplane services we discussed in \nChapter 11\n. We also see a Pod definition for\netcd\n, the component that stores the cluster’s state and helps elect a leader for\nour highly available cluster. We’ll look at \netcd\n in more detail in \nChapter 16\n.\nEach of these files contains a Pod definition just like the ones we’ve\nalready seen:\nroot@host01:~# \ncat /etc/kubernetes/manifests/kube-apiserver.yaml\n \napiVersion: v1\nkind: Pod\nmetadata:\n...\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n...\nThe \nkubelet\n service continually monitors this directory for any changes, and\nupdates the corresponding static Pod accordingly, which makes it possible for\nkubeadm\n to upgrade the cluster’s control plane on a rolling basis without any\ndowntime.\nCluster add-ons like Calico and Longhorn could also be run using this\ndirectory, but they instead use a DaemonSet to have the cluster run a Pod on\neach node. This makes sense, as a DaemonSet can be managed once for the\nwhole cluster, guaranteeing a consistent configuration across all nodes.\nThis static Pod directory is different on our three control plane nodes,\nhost01\n through \nhost03\n, compared to our “normal” node, \nhost04\n. To make\nhost04\n a normal node, \nkubeadm\n omits the control plane static Pod files from\n/etc/kubernetes/manifests\n:\nroot@host04:~# \nls -1 /etc/kubernetes/manifests\nroot@host04:~#\nNote that this command is run from \nhost04\n, our sole normal node in this",3334
61-Node Maintenance.pdf,61-Node Maintenance,"cluster.\nNode Maintenance\nThe controller manager component of the control plane continuously\nmonitors nodes to ensure that they are still connected and healthy. The \nkubelet\nservice has the responsibility of reporting node information, including node\nmemory consumption, disk consumption, and connection to the underlying\ncontainer runtime. If a node becomes unhealthy, the control plane will shift\nPods to other nodes to maintain the requested scale for Deployments, and will\nnot schedule any new Pods to the node until it is healthy again.\nNode Draining and Cordoning\nIf we know that we need to perform maintenance on a node, such as a reboot,\nwe can tell the cluster to transfer Pods off of the node and mark the node as\nunscheduleable. We do this using the \nkubectl drain\n command.\nTo see an example, let’s create a Deployment with eight Pods, making it\nlikely that each of our nodes will get a Pod:\nroot@host01:~# \nkubectl apply -f /opt/deploy.yaml\n \ndeployment.apps/debug created\nIf we allow enough time for startup, we can see that the Pods are\ndistributed across the nodes:\nroot@host01:~# \nkubectl get pods -o wide\nNAME                     READY   STATUS    ... NODE   ...\ndebug-8677494fdd-7znxn   1/1     Running   ... host02 ...  \ndebug-8677494fdd-9dgvd   1/1     Running   ... host03 ...  \ndebug-8677494fdd-hv6mt   1/1     Running   ... host04 ...  \ndebug-8677494fdd-ntqjp   1/1     Running   ... host02 ...  \ndebug-8677494fdd-pfw5n   1/1     Running   ... host03 ...  \ndebug-8677494fdd-qbhmn   1/1     Running   ... host02 ...  \ndebug-8677494fdd-qp9zv   1/1     Running   ... host03 ...  \ndebug-8677494fdd-xt8dm   1/1     Running   ... host03 ...\nTo minimize the size of our test cluster, our normal node \nhost04\n is small in\nterms of resources, so in this example it gets only one of the Pods. But that’s\nsufficient to see what happens when we shut down the node. This process is\nsomewhat random, so if you don’t see any Pods allocated to \nhost04\n, you can\ndelete the Deployment and try again or scale it down and then back up, as we\ndo in the next example.\nTo shut down the node, we use the \nkubectl drain\n command:\nroot@host01:~# \nkubectl drain --ignore-daemonsets host04\nnode/host04 cordoned\nWARNING: ignoring DaemonSet-managed Pods: ...\n...\npod/debug-8677494fdd-hv6mt evicted\nnode/host04 evicted\nWe need to provide the \n--ignore-daemonsets\n option because all of our nodes\nhave Calico and Longhorn DaemonSets, and of course, those Pods cannot be\ntransferred to another node.\nThe eviction will take a little time. When it’s complete, we can see that\nthe Deployment has created a Pod on another node, which keeps our Pod\ncount at eight:\nroot@host01:~# \nkubectl get pods -o wide\nNAME                     READY   STATUS    ... NODE     ...\ndebug-8677494fdd-7znxn   1/1     Running   ... host02   ...\ndebug-8677494fdd-9dgvd   1/1     Running   ... host03   ...\ndebug-8677494fdd-ntqjp   1/1     Running   ... host02   ...\ndebug-8677494fdd-pfw5n   1/1     Running   ... host03   ...\ndebug-8677494fdd-qbhmn   1/1     Running   ... host02   ...\ndebug-8677494fdd-qfnml   1/1     Running   ... host01   ...\ndebug-8677494fdd-qp9zv   1/1     Running   ... host03   ...\ndebug-8677494fdd-xt8dm   1/1     Running   ... host03   ...\nAdditionally, the node has been \ncordoned\n, thus no more Pods will be\nscheduled on it:\nroot@host01:~# \nkubectl get nodes\nNAME     STATUS                     ROLES        ...\nhost01   Ready                      control-plane...\nhost02   Ready                      control-plane...\nhost03   Ready                      control-plane...\nhost04   Ready,SchedulingDisabled   <none>       ...\nAt this point, it is safe to stop \nkubelet\n or the container runtime, to reboot the\nnode, or even to delete it from Kubernetes entirely:\nroot@host01:~# \nkubectl delete node host04\nnode ""host04"" deleted\nThis deletion removes the node information from the cluster’s storage, but\nbecause the node still has a valid client certificate and all its configuration, a\nsimple restart of the \nkubelet\n service on \nhost04\n will add it back to the cluster. First\nlet’s restart \nkubelet\n:\nroot@host04:~# \nsystemctl restart kubelet\nBe sure to do this on \nhost04\n. Next, back on \nhost01\n, if we wait for \nkubelet\n on\nhost04\n to finish cleaning up from its previous run and to reinitialize, we can see\nit return in the list of nodes:\nroot@host01:~# \nkubectl get nodes\nNAME     STATUS   ROLES        ...\nhost01   Ready    control-plane...\nhost02   Ready    control-plane...\nhost03   Ready    control-plane...\nhost04   Ready    <none>       ...\nNote that the cordon has been removed and \nhost04\n no longer shows a status\nthat includes \nSchedulingDisabled\n. This is one way to remove the cordon. The\nother is to do it directly using \nkubectl uncordon\n.\nUnhealthy Nodes\nKubernetes will also shift Pods on a node automatically if the node becomes\nunhealthy as a result of resource constraints such as insufficient memory or\ndisk space. Let’s simulate a low-memory condition on \nhost04\n so that we can\nsee this in action.\nFirst, we’ll need to reset the scale of our \ndebug\n Deployment to ensure that\nnew Pods are allocated onto \nhost04\n:\nroot@host01:~# \nkubectl scale deployment debug --replicas=1\ndeployment.apps/debug scaled\nroot@host01:~# \nkubectl scale deployment debug --replicas=12\ndeployment.apps/debug scaled\nWe first scale the Deployment all the way down, and then we scale it back\nup. This way, we get more chances to schedule at least one Pod on \nhost04\n. As\nsoon as the Pods have had a chance to settle, we see Pods on \nhost04\n again:\nroot@host01:~# \nkubectl get pods -o wide\nNAME                     READY   STATUS    ... NODE     ...\n...\ndebug-8677494fdd-j7cth   1/1     Running   ... host04   ...\ndebug-8677494fdd-jlj4v   1/1     Running   ... host04   ...\n...\nWe can check the current statistics for our nodes using \nkubectl top\n:\nroot@host01:~# \nkubectl top nodes\nNAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \nhost01   503m         25%    1239Mi          65%       \nhost02   518m         25%    1346Mi          71%       \nhost03   534m         26%    1382Mi          73%       \nhost04   288m         14%    542Mi           29%\nWe have 2GB total on \nhost04\n, and currently we’re using more than\n500MiB. By default, \nkubelet\n will evict Pods when there is less than 100MiB of\nmemory remaining. We could try to use up memory on the node to get below\nthat default threshold, but it’s chancy because using up so much memory\ncould make our node behave badly. Instead, let’s update the eviction limit. To\ndo this, we’ll add lines to \n/var/lib/kubelet/config.yaml\n and then restart \nkubelet\n.\nHere’s the additional configuration we’ll add to our \nkubelet\n config file:\nnode-evict.yaml\nevictionHard:\n  memory.available: ""1900Mi""\nThis tells \nkubelet\n to start evicting Pods if it has less than 1,900MiB\navailable. For nodes in our example cluster, that will happen right away.\nLet’s apply this change:\nroot@host04:~# \ncat /opt/node-evict.yaml >> /var/lib/kubelet/config.yaml\nroot@host04:~# \nsystemctl restart kubelet\nBe sure to run these commands on \nhost04\n. The first command adds\nadditional lines to the \nkubelet\n config file. The second command restarts \nkubelet\nso that it picks up the change.\nIf we check on the node status for \nhost04\n, it will appear to still be ready:\nroot@host01:~# \nkubectl get nodes\nNAME     STATUS   ROLES        ...\nhost01   Ready    control-plane...\nhost02   Ready    control-plane...\nhost03   Ready    control-plane...\nhost04   Ready    <none>       ...\nHowever, the node’s event log makes clear what is happening:\nroot@host01:~# \nkubectl describe node host04\nName:               host04\n...\n  Normal   NodeHasInsufficientMemory  6m31s                ...\n  Warning  EvictionThresholdMet       7s (x14 over 6m39s)  ...\nThe node starts evicting Pods, and the cluster automatically creates new\nPods on other nodes as needed to stay at the desired scale:\nroot@host01:~# \nkubectl get pods -o wide\nNAME                     READY   STATUS        ... NODE     ...\ndebug-8677494fdd-4274k   1/1     Running       ... host01   ...\ndebug-8677494fdd-4pnzb   1/1     Running       ... host01   ...\ndebug-8677494fdd-5nw6n   1/1     Running       ... host01   ...\ndebug-8677494fdd-7kbp8   1/1     Running       ... host03   ...\ndebug-8677494fdd-dsnp5   1/1     Running       ... host03   ...\ndebug-8677494fdd-hgdbc   1/1     Running       ... host01   ...\ndebug-8677494fdd-j7cth   1/1     Running       ... host04   ...\ndebug-8677494fdd-jlj4v   0/1     OutOfmemory   ... host04   ...\ndebug-8677494fdd-lft7h   1/1     Running       ... host01   ...\ndebug-8677494fdd-mnk6r   1/1     Running       ... host01   ...\ndebug-8677494fdd-pc8q8   1/1     Running       ... host01   ...\ndebug-8677494fdd-sr2kw   0/1     OutOfmemory   ... host04   ...\ndebug-8677494fdd-tgpb2   1/1     Running       ... host03   ...\ndebug-8677494fdd-vnjks   0/1     OutOfmemory   ... host04   ...\ndebug-8677494fdd-xn8t8   1/1     Running       ... host02   ...\nPods allocated to \nhost04\n show \nOutOfMemory\n, and they have been replaced\nwith Pods on other nodes. The Pods are stopped on the node, but unlike the\nprevious case for which we drained the node, the Pods are not automatically\nterminated. Even if the node recovers from its low-memory situation, the\nPods will continue to show up in the list of Pods, stuck in the \nOutOfMemory\nstate, until \nkubelet\n is restarted.\nNode Unreachable\nWe have one more case to look at. In our previous two examples, \nkubelet\n could\ncommunicate with the control plane to update its status, allowing the control\nplane to act accordingly. But what happens if there is a network issue or\nsudden power failure and the node loses its connection to the cluster without\nbeing able to report that it is shutting down? In that case, the cluster will\nrecord the node status as unknown, and after a timeout, it will start shifting\nPods onto other nodes.\nLet’s simulate this. We’ll begin by restoring \nhost04\n to its proper working\norder:\nroot@host04:~# \nsed -i '/^evictionHard/,+2d' /var/lib/kubelet/config.yaml\n \nroot@host04:~# \nsystemctl restart kubelet\nBe sure to run these commands on \nhost04\n. The first command removes the\ntwo lines we added to the \nkubelet\n config, whereas the second restarts \nkubelet\n to\npick up the change. We now can rescale our Deployment again so that it is\nredistributed:\nroot@host01:~# \nkubectl scale deployment debug --replicas=1\nroot@host01:~# \nkubectl scale deployment debug --replicas=12\nAs before, after you’ve run these commands, allow a few minutes for the\nPods to settle. Then, use \nkubectl get pods -o wide\n to verify that at least one Pod\nwas allocated to \nhost04\n.\nWe’re now ready to forcibly disconnect \nhost04\n from the cluster. We’ll do\nthis by adding a firewall rule:\nroot@host04:~# \niptables -I INPUT -s 192.168.61.10 -j DROP\nroot@host04:~# \niptables -I OUTPUT -d 192.168.61.10 -j DROP\nBe sure to run this on \nhost04\n. The first command tells the firewall to drop\nall traffic coming from the IP address \n192.168.61.10\n, which is the highly\navailable IP that is shared by all three control plane nodes. The second\ncommand tells the firewall to drop all traffic going to that same IP address.\nAfter a minute or so, \nhost04\n will show a state of \nNotReady\n:\nroot@host01:~# \nkubectl get nodes\nNAME     STATUS     ROLES        ...\nhost01   Ready      control-plane...\nhost02   Ready      control-plane...\nhost03   Ready      control-plane...\nhost04   NotReady   <none>       ...\nAnd if we wait a few minutes, the Pods on \nhost04\n will be shown as\nTerminating\n because the cluster gives up on those Pods and shifts them to other\nnodes:\nroot@host01:~# \nkubectl get pods -o wide\nNAME                     READY   STATUS        ... NODE     ...\ndebug-8677494fdd-2wrn2   1/1     Running       ... host01   ...\ndebug-8677494fdd-4lz48   1/1     Running       ... host02   ...\ndebug-8677494fdd-78874   1/1     Running       ... host01   ...\ndebug-8677494fdd-7f8fw   1/1     Running       ... host01   ...\ndebug-8677494fdd-9vb5m   1/1     Running       ... host03   ...\ndebug-8677494fdd-b7vj6   1/1     Running       ... host03   ...\ndebug-8677494fdd-c2c4v   1/1     Terminating   ... host04   ...\ndebug-8677494fdd-c8tzv   1/1     Running       ... host03   ...\ndebug-8677494fdd-d2r6b   1/1     Terminating   ... host04   ...\ndebug-8677494fdd-d5t6b   1/1     Running       ... host01   ...\ndebug-8677494fdd-j7cth   1/1     Terminating   ... host04   ...\ndebug-8677494fdd-jjfsl   1/1     Terminating   ... host04   ...\ndebug-8677494fdd-nqb8z   1/1     Running       ... host03   ...\ndebug-8677494fdd-sskd5   1/1     Running       ... host02   ...\ndebug-8677494fdd-wz6c6   1/1     Terminating   ... host04   ...\ndebug-8677494fdd-x5b4w   1/1     Running       ... host02   ...\ndebug-8677494fdd-zfbml   1/1     Running       ... host01   ...\nHowever, because \nkubelet\n on \nhost04\n can’t connect to the control plane, it is\nunaware that it should be shutting down its Pods. If we check to see what\ncontainers are running on \nhost04\n, we still see multiple containers:\nroot@host04:~# \ncrictl ps\nCONTAINER           IMAGE          ...  STATE      NAME  ...\n2129a1cb00607       16ea53ea7c652  ...  Running    debug ...\ncfd7fd6142321       16ea53ea7c652  ...  Running    debug ...\n0289ffa5c816d       16ea53ea7c652  ...  Running    debug ...\nfb2d297d11efb       16ea53ea7c652  ...  Running    debug ...\n...\nNot only are the Pods still running, but because of the way we cut off the\nconnection, they are still able to communicate with the rest of the cluster.\nThis is very important. Kubernetes will do its best to run the number of\ninstances requested and to respond to errors, but it can only do that based on\nthe information it has available. In this case, because \nkubelet\n on \nhost04\n can’t talk\nto the control plane, Kubernetes has no way of knowing that the Pods are still",14210
62-13 HEALTH PROBES.pdf,62-13 HEALTH PROBES,"running. When building applications for a distributed system like a\nKubernetes cluster, you should recognize that some types of errors can have\nsurprising results, like partial network connectivity or a different number of\ninstances compared to what is specified. In more advanced application\narchitectures that include rolling updates, this can even lead to cases in which\nold versions of application components are still running unexpectedly. Be\nsure to build applications that are resilient in the face of these kinds of\nsurprising behaviors.\nFinal Thoughts\nUltimately, to have a Kubernetes cluster, we need nodes that can run\ncontainers, and that means instances of \nkubelet\n connected to the control plane\nand a container runtime. In this chapter, we’ve inspected how to configure\nkubelet\n and how the cluster behaves when nodes leave or enter the cluster,\neither intentionally or through an outage.\nOne of the key themes of this chapter is the way that Kubernetes acts to\nkeep the specified number of Pods running, even in the face of node issues.\nIn the next chapter, we’ll see how that monitoring extends inside the\ncontainer to its processes, ensuring that the processes run as expected. We’ll\nsee how to specify probes that allow Kubernetes to monitor containers, and\nhow the cluster responds when a container is unhealthy.",1355
63-Liveness Probes.pdf,63-Liveness Probes,"13\nHEALTH PROBES\nHaving a reliable application is about more than just keeping application\ncomponents running. Application components also need to be able to respond\nto requests in a timely way and get data from and make requests of\ndependencies. This means that the definition of a “healthy” application\ncomponent is different for each individual component.\nAt the same time, Kubernetes needs to know when a Pod and its\ncontainers are healthy so that it can route traffic to only healthy containers\nand replace failed ones. For this reason, Kubernetes allows configuration of\ncustom health checks for containers and integrates those health checks into\nmanagement of workload resources such as Deployment.\nIn this chapter, we’ll look at how to define health probes for our\napplications. We’ll look at both network-based health probes and probes that\nare internal to a container. We’ll see how Kubernetes runs these health probes\nand how it responds when a container becomes unhealthy.\nAbout Probes\nKubernetes supports three different types of probes:\nExec\n Run a command or script to check on a container.\nTCP\n Determine whether a socket is open.\nHTTP\n Verify that an HTTP GET succeeds.\nIn addition, we can use any of these three types of probes for any of three\ndifferent purposes:\nLiveness\n Detect and restart failed containers.\nStartup\n Give extra time before starting liveness probes.\nReadiness\n Avoid sending traffic to containers when they are not prepared for\nit.\nOf these three purposes, the most important is the liveness probe because\nit runs during the primary life cycle of the container and can result in\ncontainer restarts. We’ll look closely at liveness probes and use that\nknowledge to understand how to use startup and readiness probes.\nLiveness Probes\nA \nliveness\n probe runs continuously as soon as the container has started\nrunning. Liveness probes are created as part of the container definition, and a\ncontainer that fails its liveness probe will be restarted automatically.\nExec Probes\nLet’s begin with a simple liveness probe that runs a command inside the\ncontainer. Kubernetes expects the command to finish before a timeout and\nreturn zero to indicate success, or a non-zero code to indicate a problem.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nLet’s illustrate this with an NGINX web server container. We’ll use this\nDeployment definition:\nnginx-exec.yaml\n------\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        livenessProbe:\n          exec:\n            command: [""/usr/bin/curl"", ""-fq"", ""http://localhost""]\n          initialDelaySeconds: 10\n          periodSeconds: 5\nThe \nexec\n section of the \nlivenessProbe\n tells Kubernetes to run a command\ninside the container. In this case, \ncurl\n is used with a \n-q\n flag so that it doesn’t\nprint the page contents but just returns a zero exit code on success.\nAdditionally, the \n-f\n flag causes \ncurl\n to return a non-zero exit code for any\nHTTP error response (that is, any response code of 300 or above).\nThe \ncurl\n command runs every 5 seconds based on the \nperiodSeconds\n; it starts\n10 seconds after the container is started, based on \ninitialDelaySeconds\n.\nThe automated scripts for this chapter add the \nnginx-exec.yaml\n file to \n/opt\n.\nCreate this Deployment as usual:\nroot@host01:~# \nkubectl apply -f /opt/nginx-exec.yaml\n \ndeployment.apps/nginx created\nThe resulting Pod status doesn’t look any different from a Pod without a\nliveness probe:\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-68dc5f984f-jq5xl   1/1     Running   0          25s\nHowever, in addition to the regular NGINX server process, \ncurl\n is being\nrun inside the container every 5 seconds, verifying that it is possible to\nconnect to the server. The detailed output from \nkubectl describe\n shows this\nconfiguration:\nroot@host01:~# \nkubectl describe deployment nginx\nName:                   nginx\nNamespace:              default\n...\nPod Template:\n  Labels:  app=nginx\n  Containers:\n   nginx:\n...\n    Liveness:     exec [/usr/bin/curl -q http://localhost] delay=10s \n    timeout=1s period=5s #success=1 #failure=3\n...\nBecause a liveness probe is defined, the fact that the Pod continues to\nshow a \nRunning\n status and no restarts indicates that the check is successful. The\n#success\n field shows that one successful run is sufficient for the container to be\nconsidered live, whereas the \n#failure\n value shows that three consecutive\nfailures will cause the Pod to be restarted.\nWe used \n-q\n to discard the logs from \ncurl\n, but even without that flag, any\nlogs from a successful liveness probe are discarded. If we want to save the\nongoing log information from a probe, we need to send it to a file or use a\nlogging library to ship it across the network.\nBefore moving on to another type of probe, let’s see what happens if a\nliveness probe fails. We’ll patch the \ncurl\n command to try to retrieve a\nnonexistent path on the server, which will cause \ncurl\n to return a non-zero exit\ncode, so our probe will fail.\nWe used a patch file in \nChapter 9\n when we edited a Service type. Let’s do\nthat again here to make the change:\nnginx-404.yaml\n---\nspec:\n  template:\n    spec:\n      containers:\n     \n➊\n - name: nginx\n          livenessProbe:\n            exec:\n              command: [""/usr/bin/curl"", ""-fq"", ""http://localhost/missing""]\nAlthough a patch file allows us to update only the specific fields we care\nabout, in this case the patch file has several lines because we need to specify\nthe full hierarchy, and we also must specify the name of the container we\nwant to modify \n➊\n, so Kubernetes will merge this content into the existing\ndefinition for that container.\nTo patch the Deployment, use the \nkubectl patch\n command:\nroot@host01:~# \nkubectl patch deploy nginx --patch-file /opt/nginx-404.yaml\n \ndeployment.apps/nginx patched\nBecause we changed the Pod specification within the Deployment,\nKubernetes needs to terminate the old Pod and create a new one:\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS        RESTARTS   AGE\nnginx-679f866f5b-7lzsb   1/1     Terminating   0          2m28s\nnginx-6cb4b995cd-6jpd7   1/1     Running       0          3s\nInitially, the new Pod shows a \nRunning\n status. However, if we check back\nagain in about 30 seconds, we get an indication that the Pod has an issue:\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-6cb4b995cd-6jpd7   1/1     Running   1          28s\nWe didn’t change the initial delay or the period for our liveness probe, so\nthe first probe started after 10 seconds and the probe runs every 5 seconds. It\ntakes three failures to trigger a restart, so it’s not surprising that we see one\nrestart after 25 seconds have elapsed.\nThe Pod’s event log indicates the reason for the restart:\nroot@host01:~# \nkubectl describe pod\nName:         nginx-6cb4b995cd-6jpd7\n...\nContainers:\n  nginx:\n...\n    Last State:     Terminated\n...\nEvents:\n  Type     Reason     Age                From     Message\n  ----     ------     ----               ----     -------\n...\n  Warning  Unhealthy  20s (x9 over 80s)  kubelet  Liveness probe failed: ...\ncurl: (22) The requested URL returned error: 404 Not Found\n...\nThe event log helpfully provides the output from \ncurl\n telling us the reason\nfor the failed liveness probe. Kubernetes will continue to restart the container\nevery 25 seconds as each new container starts running and then fails three\nconsecutive liveness probes.\nHTTP Probes\nThe ability to run a command within a container to check health allows us to\nperform custom probes. However, for a web server like this one, we can take\nadvantage of the HTTP probe capability within Kubernetes, avoiding the\nneed for \ncurl\n inside our container image and also verifying connectivity from\noutside the Pod.\nLet’s replace our NGINX Deployment with a new configuration that uses\nan HTTP probe:\nnginx-http.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\nWith this configuration, we tell Kubernetes to connect to port 80 of our\nPod and do an HTTP GET at the root path of \n/\n. Because our NGINX server is\nlistening on port 80 and will serve a welcome file for the root path, we can\nexpect this to work.\nWe’ve specified the entire Deployment rather than using a patch, so we’ll\nuse \nkubectl apply\n to update the Deployment:\nroot@host01:~# \nkubectl apply -f /opt/nginx-http.yaml\n \ndeployment.apps/nginx configured\nWe could use a patch to make this change as well, but it would be more\ncomplex this time, because a patch file is merged into the existing\nconfiguration. As a result, we would require two commands: one to remove\nthe existing liveness probe and one to add the new HTTP liveness probe.\nBetter to just replace the resource entirely.\nNOTE\nThe \nkubectl patch\n command is a valuable command for debugging, but\nproduction applications should have YAML resource files under version\ncontrol to allow for change tracking and peer review, and the entire file\nshould always be applied every time to ensure that the cluster reflects the\ncurrent content of the repository.\nNow that we’ve applied the new Deployment configuration, Kubernetes\nwill make a new Pod:\nroot@host01:~# \nkubectl get pods\nNAME                    READY   STATUS    RESTARTS   AGE\nnginx-d75d4d675-wvhxl   1/1     Running   0          2m38s\nFor an HTTP probe, \nkubelet\n has the responsibility of running an HTTP GET\nrequest on the appropriate schedule and confirming the result. By default, any\nHTTP return code in the 200 or 300 series is considered a successful\nresponse.\nThe NGINX server is logging all of its requests, so we can use the\ncontainer logs to see the probes taking place:\nroot@host01:~# \nkubectl logs\n \nnginx-d75d4d675-wvhxl\n...\n... 22:23:31 ... ""GET / HTTP/1.1"" 200 615 ""-"" ""kube-probe/1.21"" ""-""\n... 22:23:41 ... ""GET / HTTP/1.1"" 200 615 ""-"" ""kube-probe/1.21"" ""-""\n... 22:23:51 ... ""GET / HTTP/1.1"" 200 615 ""-"" ""kube-probe/1.21"" ""-""\nWe didn’t specify \nperiodSeconds\n this time, so \nkubelet\n is probing the server at\nthe default rate of once every 10 seconds.\nLet’s clean up the NGINX Deployment before moving on:\nroot@host01:~# \nkubectl delete deployment nginx\ndeployment.apps ""nginx"" deleted\nWe’ve looked at two of the three types of probes; let’s finish by looking at\nTCP.\nTCP Probes\nA database server such as PostgreSQL listens for network connections, but it\ndoes not use HTTP for communication. We can still create a probe for these\nkinds of containers using a TCP probe. It won’t provide the configuration\nflexibility of an HTTP or exec probe, but it will verify that a container in the\nPod is listening for connections on the specified port.\nHere’s a PostgreSQL Deployment with a TCP probe:\npostgres-tcp.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres\n        env:",11987
64-Startup Probes.pdf,64-Startup Probes,"- name: POSTGRES_PASSWORD\n          value: ""supersecret""\n        livenessProbe:\n          tcpSocket:\n            port: 5432\nWe saw the requirement for the \nPOSTGRES_PASSWORD\n environment variable\nin \nChapter 10\n. The only configuration that’s changed for this example is the\nlivenessProbe\n. We specify a TCP socket of 5432, as this is the standard port for\nPostgreSQL.\nAs usual, we can create this Deployment and, after a while, observe that\nit’s running:\nroot@host01:~# \nkubectl apply -f /opt/postgres-tcp.yaml\n \ndeployment.apps/postgres created\n...\nroot@host01:~# \nkubectl get pods\nNAME                       READY   STATUS    RESTARTS   AGE\npostgres-5566ff748-jqp5d   1/1     Running   0          29s\nAgain, it is the job of \nkubelet\n to perform the probe. It does this solely by\nmaking a TCP connection to the port and then disconnecting. PostgreSQL\ndoesn’t emit any logging when this happens, so the only way we know that\nthe probe is working is to check that the container continues to run and\ndoesn’t show any restarts:\nroot@host01:~# \nkubectl get pods\nNAME                       READY   STATUS    RESTARTS   AGE\npostgres-5566ff748-jqp5d   1/1     Running   0          2m7s\nBefore we move on, let’s clean up the Deployment:\nroot@host01:~# \nkubectl delete deploy postgres\ndeployment.apps ""postgres"" deleted\nWe’ve now looked at all three types of probes. And although we used\nthese three types to create liveness probes, the same three types will work\nwith both startup and readiness probes as well. The only difference is the\nchange in the behavior of our cluster when a probe fails.\nStartup Probes\nUnhealthy containers can create all kinds of difficulties for an application,\nincluding lack of responsiveness, errors responding to requests, or bad data,\nso we want Kubernetes to respond quickly when a container becomes\nunhealthy. However, when a container is first started, it can take time before\nit is fully initialized. During that time, it might not be able to respond to\nliveness probes.\nBecause of that delay, we’re left with a need to have a long timeout before\na container fails a probe, so we can give our container enough time for\ninitialization. However, at the same time, we need to have a short timeout in\norder to detect a failed container quickly and restart it. The solution is \nto\nconfigure a separate \nstartup probe\n. Kubernetes will use the startup probe\nconfiguration until the probe is successful; then it will switch over to the\nliveness probe.\nFor example, we might configure our NGINX server Deployment as\nfollows:\n...\nspec:\n...\n  template:\n...\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n        startupProbe:\n          httpGet:\n            path: /\n            port: 80\n          periodSeconds: \n          initialDelaySeconds: 30\n          periodSeconds: 10\n          failureThreshold: 60\nGiven this configuration, Kubernetes would start checking the container\n30 seconds after startup. It would continue checking every 10 seconds until\nthe probe is successful or until there are 60 failed attempts. The effect is that\nthe container has 10 minutes to finish initialization and respond to a probe",3323
65-Readiness Probes.pdf,65-Readiness Probes,"successfully. If the container does not have a successful probe in that time, it\nwill be restarted.\nAs soon as the container has one successful probe, Kubernetes will switch\nto the configuration for \nlivenessProbe\n. Because we didn’t override any timing\nparameters, this will transition to a probe every 10 seconds, with three\nconsecutive failed probes leading to a restart. We give the container 10\nminutes to be live initially, but after that we will allow no more than 30\nseconds before restarting it.\nThe fact that the \nstartupProbe\n is defined completely separately means that it\nis possible to create a different check for startup from the one used for\nliveness. Of course, it’s important to choose wisely so that the container\ndoesn’t pass its startup probe before the liveness probe would also pass,\nbecause that would result in inappropriate restarts.\nReadiness Probes\nThe third probe purpose is to check the \nreadiness\n of the Pod. The term\nreadiness\n might seem redundant with the startup probe. However, even\nthough completing initialization is an important part of readiness for a piece\nof software, an application component might not be ready to do work for\nmany reasons, especially in a highly available microservice architecture\nwhere components can come and go at any time.\nRather than being used for initialization, readiness probes should be used\nfor any case in which the container cannot perform any work because of a\nfailure outside its control. It may be a temporary situation, as retry logic\nsomewhere else could fix the failure. For example, an API that relies on an\nexternal database might fail its readiness probe if the database is unreachable,\nbut that database might return to service at any time.\nThis also creates a valuable contrast with startup and liveness probes. As\nwe examined earlier, Kubernetes will restart a container if it fails the\nconfigured number of startup or liveness probes. But it makes no sense to do\nthat if the issue is a failed or missing external dependency, given that\nrestarting the container won’t fix whatever is wrong externally.\nAt the same time, if a container is missing a required external dependency,\nit can’t do work, so we don’t want to send any work to it. In that situation, the\nbest thing to do is to leave the container running and give it an opportunity to\nreestablish the connections it needs, but avoid sending any requests to it. In\nthe meantime, we can hope that somewhere in the cluster another Pod for the\nsame Deployment is working as expected, making our application as a whole\nresilient to a localized failure.\nThis is exactly how readiness probes work in Kubernetes. As we saw in\nChapter 9\n, a Kubernetes Service continually watches for Pods that match its\nselector and configures load balancing for its cluster IP that routes traffic to\nthose Pods. If a Pod reports itself as not ready, the Service will stop routing\ntraffic to it, but \nkubelet\n will not trigger any other action such as a container\nrestart.\nLet’s illustrate this situation. We want to have individual control over Pod\nreadiness, so we’ll use a somewhat contrived example rather than a real\nexternal dependency to determine readiness. We’ll deploy a set of NGINX\nPods, this time with a corresponding Service:\nnginx-ready.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 80\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: nginx\nspec:\n  selector:\n    app: nginx\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\nThis Deployment keeps its \nlivenessProbe\n as an indicator that NGINX is\nworking correctly and adds a \nreadinessProbe\n. The Service definition is identical\nto what we saw in \nChapter 9\n and will route traffic to our NGINX Pods.\nThis file has already been written to \n/opt\n, so we can apply it to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/nginx-ready.yaml\n \ndeployment.apps/nginx created\nservice/nginx created\nAfter these Pods are up and running, they stay running because the\nliveness probe is successful:\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-67fb6485f5-2k2nz   0/1     Running   0          38s\nnginx-67fb6485f5-vph44   0/1     Running   0          38s\nnginx-67fb6485f5-xzmj5   0/1     Running   0          38s\nIn addition, the Service we created has been allocated a cluster IP:\nroot@host01:~# \nkubectl get services\nNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\n...\nnginx        ClusterIP   10.101.98.80   <none>        80/TCP    3m1s\nHowever, we aren’t able to use that IP address to reach any Pods:\nroot@host01:~# \ncurl http://\n10.101.98.80\ncurl: (7) Failed to connect to 10.101.98.80 port 80: Connection refused\nThis is because, at the moment, there is nothing for NGINX to serve on\nthe \n/ready\n path, so it’s returning \n404\n, and the readiness probe is failing. A\ndetailed inspection of the Pod shows that it is not ready:\nroot@host01:~# \nkubectl describe pod\nName:         nginx-67fb6485f5-2k2nz\n...\nContainers:\n  nginx:\n...\n    Ready:          False\n...\nAs a result, the Service does not have any Endpoints to which to route\ntraffic:\nroot@host01:~# \nkubectl describe service nginx\nName:              nginx\n...\nEndpoints:         \n...\nBecause the Service has no Endpoints, it has configured \niptables\n to reject all\ntraffic:\nroot@host01:~# \niptables-save | grep default/nginx\n-A KUBE-SERVICES -d 10.101.98.80/32 -p tcp -m comment --comment ""default/nginx has no\n \nendpoints""  \n  -m tcp --dport 80 -j REJECT --reject-with icmp-port-unreachable\nTo fix this, we’ll need at least one Pod to become ready to ensure that\nNGINX has something to serve on the \n/ready\n path. We’ll use the container’s\nhostname to keep track of which Pod is serving our request.\nTo make one of our Pods ready, let’s first get the list of Pods again, just to\nhave the Pod names handy:\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-67fb6485f5-2k2nz   0/1     Running   0          10m\nnginx-67fb6485f5-vph44   0/1     Running   0          10m\nnginx-67fb6485f5-xzmj5   0/1     Running   0          10m\nNow, we’ll choose one and make it report that it is ready:\nroot@host01:~# \nkubectl exec -ti \nnginx-67fb6485f5-2k2nz\n -- \\n  \ncp -v /etc/hostname /usr/share/nginx/html/ready\n'/etc/hostname' -> '/usr/share/nginx/html/ready'\nOur Service will start to show a valid Endpoint:\nroot@host01:~# \nkubectl describe svc nginx\nName:              nginx\n...\nEndpoints:         172.31.239.199:80\n...\nEven better, we can now reach an NGINX instance via the cluster IP, and\nthe content corresponds to the hostname:\nroot@host01:~# \ncurl http://\n10.101.98.80\n/ready\nnginx-67fb6485f5-2k2nz\nNote the \n/ready\n at the end of the URL so the response is the hostname. If we\nrun this command many times, we’ll see that the hostname is the same every\ntime. This is because the one Pod that is passing its readiness probe is\nhandling all of the Service traffic.\nLet’s make the other two Pods ready as well:\nroot@host01:~# \nkubectl exec -ti \nnginx-67fb6485f5-vph44\n -- \\n  \ncp -v /etc/hostname /usr/share/nginx/html/ready\n'/etc/hostname' -> '/usr/share/nginx/html/ready'\nroot@host01:~# \nkubectl exec -ti \nnginx-67fb6485f5-xzmj5\n -- \\n  \ncp -v /etc/hostname /usr/share/nginx/html/ready\n'/etc/hostname' -> '/usr/share/nginx/html/ready'\nOur Service now shows all three Endpoints:\nroot@host01:~# \nkubectl describe service nginx\nName:              nginx\n...\nEndpoints:         172.31.239.199:80,172.31.239.200:80,172.31.89.210:80\n...\nRunning the \ncurl\n command multiple times shows that the traffic is now\nbeing distributed across multiple Pods:\nroot@host01:~# \nfor i in $(seq 1 5); do curl http://\n10.101.98.80\n/ready; done\nnginx-67fb6485f5-xzmj5\nnginx-67fb6485f5-2k2nz\nnginx-67fb6485f5-xzmj5",8373
66-14 LIMITS AND QUOTAS.pdf,66-14 LIMITS AND QUOTAS,"nginx-67fb6485f5-vph44\nnginx-67fb6485f5-vph44\nThe embedded command \n$(seq 1 5)\n returns the numbers one through five,\ncausing the \nfor\n loop to run \ncurl\n five times. If you run this same \nfor\n loop several\ntimes, you will see a different distribution of hostnames. As described in\nChapter 9\n, load balancing is based on a random uniform distribution wherein\neach endpoint has an equal chance of being selected for each new connection.\nA good practice is to offer an HTTP readiness endpoint for each\napplication that checks the current state of the application and its\ndependencies and returns an HTTP success code (such as \n200\n) if the\ncomponent is healthy, and an HTTP error code (such as \n500\n) if not. Some\napplication frameworks such as Spring Boot provide application state\nmanagement that automatically exposes liveness and readiness endpoints.\nFinal Thoughts\nKubernetes offers the ability to check on our containers and make sure they\nare working as expected, not just that the process is running. These probes\ncan include any arbitrary command run inside the container, verifying that a\nport is open for TCP connections, or that the container responds correctly to\nan HTTP request. To build resilient applications, we should define both a\nliveness probe and a readiness probe for each application component. The\nliveness probe is used to restart an unhealthy container; the readiness probe\ndetermines whether the Pod can handle Service traffic. Additionally, if a\ncomponent needs extra time for initialization, we should also define a startup\nprobe to make sure that give it the required initialization time while\nresponding quickly to failure as soon as initialization is complete.\nOf course, for our containers to run as expected, other containers in the\ncluster must also be well behaved, not using too many of the cluster’s\nresources. In the next chapter, we’ll look at how we can limit our containers\nin their use of CPU, memory, disk space, and network bandwidth, as well as\nhow we can control the maximum amount of total resources available to a\nuser. This ability to specify limits and quotas is important to ensure that our\ncluster can support multiple applications with reliable performance.",2255
67-Requests and Limits.pdf,67-Requests and Limits,"14\nLIMITS AND QUOTAS\nFor our cluster to provide a predictable environment for applications, we\nneed some control over what resources each individual application\ncomponent uses. If an application component can use all of the CPU or\nmemory on a given node, the Kubernetes scheduler will not be able to\nallocate a new Pod to a node confidently, as it won’t know how much\navailable space each node has.\nIn this chapter, we’ll explore how to specify requested resources and\nlimits to ensure that containers get the resources they need without impacting\nother containers. We’ll inspect individual containers at the runtime level so\nthat we can see how Kubernetes configures the container technology we saw\nin \nPart I\n to adequately meet the resource requirements of a container without\nallowing the container to exceed its limits.\nFinally, we’ll look at how role-based access control is used to manage\nquotas, limiting the amount of resources a given user or application can\ndemand, which will help us understand how to administer a cluster in a\nmanner that allows it to reliably support multiple separate applications or\ndevelopment teams.\nRequests and Limits\nKubernetes supports many different types of resources, including processing,\nmemory, storage, network bandwidth, and use of special devices such as\ngraphics processing units (GPUs). We’ll look at network limits later in this\nchapter, but let’s start with the most commonly specified resource types:\nprocessing and memory.\nProcessing and Memory Limits\nThe specifications for processing and memory resources serve two purposes:\nscheduling and preventing conflicts. Kubernetes provides a different kind of\nresource specification for each purpose. The Pod’s containers consume\nprocessing and memory resources in Kubernetes, so that’s where resource\nspecifications are applied.\nWhen scheduling Pods, Kubernetes uses the \nrequests\n field in the container\nspecification, summing this field across all containers in the Pod and finding\na node with sufficient margin in both processing and memory. Generally, the\nrequests\n field is set to the expected average resource requirements for each\ncontainer in the Pod.\nThe second purpose of resource specification is preventing denial-of-\nservice issues in which one container takes all of a node’s resources,\nnegatively affecting other containers. This requires runtime enforcement of\ncontainer resources. Kubernetes uses the \nlimits\n field of the container\nspecification for this purpose, thus we need to be sure to set the \nlimits\n field\nhigh enough that a container is able to run correctly without reaching the\nlimit.\nTUNING FOR PERFORMANCE\nThe idea that requests should match the expected average resource\nrequirements is based on an assumption that any load spikes in the\nvarious containers in the cluster are unpredictable and uncorrelated, and\nload spikes can therefore be assumed to happen at different times. Even\nwith that assumption, there is a risk that simultaneous load spikes in\nmultiple containers on a node will result in that node being overloaded.\nAnd if the load spikes between different Pods are correlated, this risk of\noverload increases. At the same time, if we configure \nrequests\n for the\nworst case scenario, we can end up with a very large cluster that is idle\nmost of the time. In \nChapter 19\n, we explore the different Quality of\nService (QoS) classes that Kubernetes offers for Pods and discuss how\nto find a balance between performance guarantees and cluster\nefficiency.\nListing 14-1\n kicks off our examination with an example of using requests\nand limits with a Deployment.\nnginx-limit.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          requests:\n            memory: ""64Mi""\n            cpu: ""250m""\n          limits:\n            memory: ""128Mi""\n            cpu: ""500m""\n      nodeName: host01\nListing 14-1: Deployment with limits\nWe’ll use this Deployment to explore how limits are configured at the\nlevel of the container runtime, so we use the \nnodeName\n field to make sure the\ncontainer ends up on \nhost01\n. This constrains where the scheduler can place\nthe Pod, but the scheduler still uses the \nrequests\n field to ensure that there are\nsufficient resources. If \nhost01\n becomes too busy, the scheduler will just\nrefuse to schedule the Pod, similar to what we saw in \nChapter 10\n.\nThe \nresources\n field is defined at the level of the individual container,\nallowing us to specify separate resource requirements for each container in a\nPod. For this container, we specify a memory request of \n64Mi\n and a memory\nlimit of \n128Mi\n. The suffix \nMi\n means that we are using the power-of-2 unit\nmebibytes\n, which is 2 to the 20th power, rather than the power-of-10 unit\nmegabytes\n, which would be the slightly smaller value of 10 to the 6th power.\nMeanwhile, the processing request and limit specified using the \ncpu\n fields\nis not based on any absolute unit of processing. Rather, it is based on a\nsynthetic \ncpu unit\n for our cluster. Each cpu unit roughly corresponds to one\nvirtual CPU or core. The \nm\n suffix specifies a \nmillicpu\n so that our \nrequests\n value\nof \n250m\n equates to one quarter of a core, whereas the \nlimit\n of \n500m\n equates to\nhalf of a core.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nLet’s create this Deployment:\nroot@host01:~# \nkubectl apply -f /opt/nginx-limit.yaml\n \ndeployment.apps/nginx created\nThe Pod will be allocated to \nhost01\n and started:\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-56dbd744d9-vg5rj   1/1     Running   0          22m\nAnd \nhost01\n will show that resources have been allocated for the Pod:\nroot@host01:~# \nkubectl describe node host01\nName:               host01\n...\nNon-terminated Pods:          (15 in total)\n  Namespace Name                   CPU Requests CPU Limits Memory Requests Memory Limits Age\n  --------- ----                   ------------ ---------- --------------- ------------- ---\n...\n  default   nginx-56dbd744d9-vg5rj 250m (12%)   500m (25%) 64M (3%)        128M (6%)     61s\n...\nThis is true even though our NGINX web server is idle and is not using a\nlot of processing or memory resources:\nroot@host01:~# \nkubectl top pods\n...\nNAME                     CPU(cores)   MEMORY(bytes)   \nnginx-56dbd744d9-vg5rj   0m           5Mi\nSimilar to what we saw in \nChapter 12\n, this command queries the metrics\nadd-on that is collecting data from \nkubelet\n running on each cluster node.\nCgroup Enforcement\nThe processing and memory limits we specified are enforced using the Linux\ncontrol group (cgroup) functionality we described in \nChapter 3\n. Kubernetes\nmanages its own space within each hierarchy inside the \n/sys/fs/cgroup\nfilesystem. For example, memory limits are configured in the memory\ncgroup:\nroot@host01:~# \nls -1F /sys/fs/cgroup/memory\n...\nkubepods.slice/\n...\nEach Pod on a given host has a directory within the \nkubepods.slice\n tree.\nHowever, finding the specific directory for a given Pod takes some work\nbecause Kubernetes divides Pods into different classes of service, and\nbecause the name of the cgroup directory does not match the ID of the Pod or\nits containers.\nTo save us from searching around inside \n/sys/fs/cgroup\n, we’ll use a script\ninstalled by this chapter’s automated scripts: \n/opt/cgroup-info\n. This script\nuses \ncrictl\n to query the container runtime for the cgroup path and then collects\nCPU and memory limit data from that path. The most important part of the\nscript is this section that collects the path:\ncgroup-info\n#!/bin/bash\n...\nPOD_ID=$(crictl pods --name ${POD} -q)\n...\ncgp_field='.info.config.linux.cgroup_parent'\nCGP=$(crictl inspectp $POD_ID | jq -r ""$cgp_field"")\nCPU=/sys/fs/cgroup/cpu/$CGP\nMEM=/sys/fs/cgroup/memory/$CGP\n...\nThe \ncrictl pods\n command collects the Pod’s ID, which is then used with \ncrictl\ninspectp\n and \njq\n to collect one specific field, called \ncgroup_parent\n. This field is the\ncgroup subdirectory created for that pod within each resource type.\nLet’s run this script with our NGINX web server to see how the CPU and\nmemory limits have been configured:\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-56dbd744d9-vg5rj   1/1     Running   0          59m\nroot@host01:~# \n/opt/cgroup-info\n \nnginx-56dbd744d9-vg5rj\nContainer Runtime\n-----------------\nPod ID: 54602befbd141a74316323b010fb38dae0c2b433cdbe12b5c4d626e6465c7315\nCgroup path: /kubepods.slice/...9f8f3dcf_6cca_49b8_a3df_d696ece01f59.slice\nCPU Settings\n------------\nCPU Shares: 256\nCPU Quota (us): 50000 per 100000\nMemory Settings\n---------------\nLimit (bytes): 134217728\nWe first collect the name of the Pod and then use it to collect cgroup\ninformation. Note that this works only because the Pod is running on \nhost01\n;\nthe script will work for any Pod, but it must be run from the host on which\nthat Pod is running.\nThere are two key pieces of data for the CPU configuration. The quota is\nthe hard limit; it means that in any given 100,000 microsecond period, \nthis\nPod can use only 50,000 microseconds of processor time. This value\ncorresponds to the \n500m\n CPU limit specified in \nListing 14-1\n (recall that the\n500m\n limit equates to half a core).\nIn addition to this hard limit, the CPU request field we specified in \nListing\n14-1\n has been used to configure the CPU shares. As we saw in \nChapter 3\n, this\nfield configures the CPU usage on a relative basis. Because it is relative to\nthe values in neighboring directories, it is unitless, so Kubernetes computes\nthe CPU share on the basis of one core equaling 1,024. We specified a CPU\nrequest of \n250m\n, so this equates to 256.\nThe CPU share does not set any kind of limit on CPU usage, so if the\nsystem is idle, a Pod can use processing up to its hard limit. However, as the\nsystem becomes busy, the CPU share determines how much processing each\nPod is allotted relative to others in the same class of service. This helps to\nensure that if the system becomes overloaded, all Pods will be degraded fairly\nbased on their CPU request.\nFinally, for memory, there is a single relevant value. We specified a\nmemory limit of \n128Mi\n, which equates to 128MiB. As we saw in \nChapter 3\n, if\nour container tries to exceed this limit, it will be terminated. For this reason,\nit is critical to either configure the application such that it does not exceed\nthis value, or to understand how the application acts under load to choose the\noptimum limit.\nThe amount of memory actually used by a process is ultimately up to that\nprocess, meaning that the memory request value has no purpose beyond its\ninitial use in ensuring sufficient memory to schedule the Pod. For this reason,\nwe don’t see the memory request value of \n64Mi\n being used anywhere in the\ncgroup configuration.\nThe way that resource allocations are reflected in cgroups shows us\nsomething important about cluster performance. Because \nrequests\n is used for\nscheduling and \nlimits\n is used for runtime enforcement, it is possible for a node\nto overcommit processing and memory. For the case in which containers\nhave higher \nlimit\n than \nrequests\n, and containers consistently operate above their\nrequests\n, this can cause performance issues with the containers on a node.\nWe’ll discuss this in more detail in \nChapter 19\n.\nWe’re finished with our NGINX Deployment, so let’s delete it:\nroot@host01:~# \nkubectl delete -f /opt/nginx-limit.yaml\n \ndeployment.apps ""nginx"" deleted\nSo far, the container runtime can enforce the limits we’ve seen. However,\nthe cluster must enforce other types of limits, such as networking.\nNetwork Limits\nIdeally, our application will be architected so that required bandwidth for\nintercommunication is moderate, and our cluster will have sufficient\nbandwidth to meet the demand of all the containers. However, if we do have\na container that tries to take more than its share of the network bandwidth, we\nneed a way to limit it.\nBecause the network devices are configured by plug-ins, we need a plug-\nin to manage bandwidth. Fortunately, the \nbandwidth\n plug-in is part of the\nstandard set of CNI plug-ins installed with our Kubernetes cluster.\nAdditionally, as we saw in \nChapter 8\n, the default CNI configuration enables\nthe \nbandwidth\n plug-in:\nroot@host01:~# \ncat /etc/cni/net.d/10-calico.conflist\n \n{\n  ""name"": ""k8s-pod-network"",\n  ""cniVersion"": ""0.3.1"",\n  ""plugins"": [\n...\n    {\n      ""type"": ""bandwidth"",\n      ""capabilities"": {""bandwidth"": true}\n    },\n...\n  ]\nAs a result, \nkubelet\n is already calling the \nbandwidth\n plug-in every time a new\nPod is created. If a Pod is configured with bandwidth limits, the plug-in uses\nthe Linux kernel’s traffic control capabilities that we saw in \nChapter 3\n to\nensure the Pod’s virtual network devices don’t exceed the specified limit.\nLet’s look at an example. First, let’s deploy an \niperf3\n server that will listen\nfor client connections:\niperf-server.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: iperf-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: iperf-server\n  template:\n    metadata:\n      labels:\n        app: iperf-server\n    spec:\n      containers:\n      - name: iperf\n        image: bookofkubernetes/iperf3:stable\n        env:\n        - name: IPERF_SERVER\n          value: ""1""\n        resources: ...\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: iperf-server\nspec:\n  selector:\n    app: iperf-server\n  ports:\n  - protocol: TCP\n    port: 5201\n    targetPort: 5201\nIn addition to a Deployment, we also create a Service. This way, our \niperf3\nclients can find the server under its well-known name of \niperf-server\n. We\nspecify port 5201, which is the default port for \niperf3\n.\nLet’s deploy this server:\nroot@host01:~# \nkubectl apply -f /opt/iperf-server.yaml\n \ndeployment.apps/iperf-server created\nservice/iperf-server created\nLet’s run an \niperf3\n client without applying any bandwidth limits. This will\ngive us a picture of how fast our cluster’s network is without any traffic\ncontrol. Here’s the client definition:\niperf.yaml\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: iperf\nspec:\n  containers:\n  - name: iperf\n    image: bookofkubernetes/iperf3:stable\n    resources: ...\nNormally, \niperf3\n in client mode would run once and then terminate. This\nimage has a script that runs \niperf3\n repeatedly, sleeping for one minute between\neach run. Let’s start a client Pod:\nroot@host01:~# \nkubectl apply -f /opt/iperf.yaml\n \npod/iperf created\nIt will take a few seconds for the Pod to start running, after which it will\ntake 10 seconds for the initial run. After 30 seconds or so, the Pod log will\nshow the results:\nroot@host01:~# \nkubectl logs iperf\nConnecting to host iperf-server, port 5201\n[  5] local 172.31.89.200 port 54346 connected to 10.96.0.192 port 5201\n[ ID] Interval           Transfer     Bitrate         Retr  Cwnd\n[  5]   0.00-1.00   sec   152 MBytes  1.28 Gbits/sec  225    281 KBytes       \n[  5]   1.00-2.00   sec   154 MBytes  1.29 Gbits/sec  153    268 KBytes       \n[  5]   2.00-3.00   sec   163 MBytes  1.37 Gbits/sec  230    325 KBytes       \n[  5]   3.00-4.00   sec   171 MBytes  1.44 Gbits/sec  254    243 KBytes       \n[  5]   4.00-5.00   sec   171 MBytes  1.44 Gbits/sec  191    319 KBytes       \n[  5]   5.00-6.00   sec   174 MBytes  1.46 Gbits/sec  230    302 KBytes       \n[  5]   6.00-7.00   sec   180 MBytes  1.51 Gbits/sec  199    221 KBytes       \n[  5]   7.00-8.01   sec   151 MBytes  1.26 Gbits/sec  159    270 KBytes       \n[  5]   8.01-9.00   sec   160 MBytes  1.36 Gbits/sec  145    298 KBytes       \n[  5]   9.00-10.00  sec   147 MBytes  1.23 Gbits/sec  230    276 KBytes       \n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec  1.59 GBytes  1.36 Gbits/sec  2016             sender\n[  5]   0.00-10.00  sec  1.59 GBytes  1.36 Gbits/sec                  receiver\niperf Done.\nIn this case, we see a transfer rate of \n1.36 GBits/sec\n between our client and\nserver. Your results will be different depending on how your cluster is\ndeployed and whether the client and server end up on the same host.\nBefore moving on, we’ll shut down the existing client to prevent it from\ninterfering with our next test:\nroot@host01:~# \nkubectl delete pod iperf\npod ""iperf"" deleted\nObviously, while it’s running, \niperf3\n is trying to use as much network\nbandwidth as possible. That’s fine for a test application, but it isn’t polite\nbehavior for an application component in a Kubernetes cluster. To limit its\nbandwidth, we’ll add an annotation to the Pod definition:\niperf-limit.yaml\n ---\n kind: Pod\n apiVersion: v1\n metadata:\n   name: iperf-limit\n➊\n annotations:\n     kubernetes.io/ingress-bandwidth: 1M\n     kubernetes.io/egress-bandwidth: 1M\n spec:\n   containers:\n   - name: iperf\n     image: bookofkubernetes/iperf3:stable\n     resources: ...\n   nodeName: host01\nWe’ll want to inspect how the limits are being applied to the network\ndevices, which will be easier if this Pod ends up on \nhost01\n, so we set \nnodeName\naccordingly. Otherwise, the only change in this Pod definition is the \nannotations\nsection in the Pod metadata \n➊\n. We set a value of \n1M\n for ingress and egress,\ncorresponding to a 1Mb bandwidth limit on the Pod. When this Pod is\nscheduled, \nkubelet\n will pick up these annotations and send the specified\nbandwidth limits to the bandwidth plug-in so that it can configure Linux\ntraffic shaping accordingly.\nLet’s create this Pod and get a look at this in action:\nroot@host01:~# \nkubectl apply -f /opt/iperf-limit.yaml\n \npod/iperf-limit created\nAs before, we wait long enough for the client to complete one test with the\nserver and then print the logs:\nroot@host01:~# \nkubectl logs iperf-limit\nConnecting to host iperf-server, port 5201\n[  5] local 172.31.239.224 port 45680 connected to 10.96.0.192 port 5201\n[ ID] Interval           Transfer     Bitrate         Retr  Cwnd\n[  5]   0.00-1.01   sec  22.7 MBytes 190 Mbits/sec    0   1.37 KBytes       \n[  5]   1.01-2.01   sec  0.00 Bytes  0.00 bits/sec    0    633 KBytes       \n[  5]   2.01-3.00   sec  0.00 Bytes  0.00 bits/sec    0    639 KBytes       \n[  5]   3.00-4.00   sec  0.00 Bytes  0.00 bits/sec    0    646 KBytes       \n[  5]   4.00-5.00   sec  0.00 Bytes  0.00 bits/sec    0    653 KBytes       \n[  5]   5.00-6.00   sec  1.25 MBytes 10.5 Mbits/sec   0    658 KBytes       \n[  5]   6.00-7.00   sec  0.00 Bytes  0.00 bits/sec    0    658 KBytes       \n[  5]   7.00-8.00   sec  0.00 Bytes  0.00 bits/sec    0    658 KBytes       \n[  5]   8.00-9.00   sec  0.00 Bytes  0.00 bits/sec    0    658 KBytes       \n[  5]   9.00-10.00  sec  0.00 Bytes  0.00 bits/sec    0    658 KBytes       \n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec  24.0 MBytes  20.1 Mbits/sec    0             sender\n[  5]   0.00-10.10  sec  20.7 MBytes  17.2 Mbits/sec                  receiver\niperf Done.\nThe change is significant, as the Pod is limited to a fraction of the speed\nwe saw with an unlimited client. However, because the traffic shaping is\nbased on a token bucket filter, the traffic control is inexact over shorter\nintervals, so we see a bitrate of around 20Mb rather than 1Mb. To see why,\nlet’s look at the actual traffic shaping configuration.\nThe \nbandwidth\n plug-in is applying this token bucket filter to the host side of\nthe virtual Ethernet (veth) pair that was created for the Pod, so we can see it\nby showing traffic control configuration for the host interfaces:\nroot@host01:~# \ntc qdisc show\n...\nqdisc tbf 1: dev calid43b03f2e06 ... rate 1Mbit burst 21474835b lat 4123.2s \n...\nThe combination of \nrate\n and \nburst\n shows why our Pod was able to achieve\n20Mb over the 10-second test run. Because of the \nburst\n value, the Pod was\nable to send a large quantity of data immediately, at the cost of spending\nseveral seconds without any ability to send or receive. Over a much longer\ninterval, we would see an average of 1Mbps, but we would still see this\nbursting behavior.\nBefore moving on, let’s clean up our client and server:\nroot@host01:~# \nkubectl delete -f /opt/iperf-server.yaml\n \ndeployment.apps ""iperf-server"" deleted\nservice ""iperf-server"" deleted\nroot@host01:~# \nkubectl delete -f /opt/iperf-limit.yaml\npod ""iperf-limit"" deleted\nManaging the bandwidth of a Pod can be useful, but as we’ve seen, the\nbandwidth limit can behave like an intermittent connection from the Pod’s\nperspective. For that reason, this kind of traffic shaping should be considered",21447
68-Quotas.pdf,68-Quotas,"a last resort for containers that cannot be configured to moderate their own\nbandwidth usage.\nQuotas\nLimits allow our Kubernetes cluster to ensure that each node has sufficient\nresources for its assigned Pods. However, if we want our cluster to host\nmultiple applications reliably, we need a way to control the amount of\nresources that any one application can request.\nTo do this, we’ll use quotas. Quotas are allocated based on Namespaces;\nthey specify the maximum amount of resources that can be allocated within\nthat Namespace. This includes not only the primary resources of CPU and\nmemory but also specialized cluster resources such as GPUs. We can even\nuse quotas to specify the maximum number of a specific object type, such as\na Deployment, Service, or CronJob, that can be created within a given\nNamespace.\nBecause quotas are allocated based on Namespaces, they need to be used\nin conjunction with the access controls we described in \nChapter 11\n to ensure\nthat a given user is bound by the quotas we create. This means that creating\nNamespaces and applying quotas is typically handled by the cluster\nadministrator.\nLet’s create a sample Namespace for our Deployment:\nroot@host01:~# \nkubectl create namespace sample\nnamespace/sample created\nNow, let’s create a \nResourceQuota\n resource type to apply a quota to the\nNamespace:\nquota.yaml\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: sample-quota\n  namespace: sample\nspec:\n  hard:\n    requests.cpu: ""1""\n    requests.memory: 256Mi\n    limits.cpu: ""2""\n    limits.memory: 512Mi\nThis resource defines a quota for CPU and memory for both requests and\nlimits. The units are the same as those used for limits in the Deployment\nspecification in \nListing 14-1\n.\nLet’s apply this quota to the \nsample\n Namespace:\nroot@host01:~# \nkubectl apply -f /opt/quota.yaml\nresourcequota/sample-quota created\nWe can see that this quota has been applied successfully:\nroot@host01:~# \nkubectl describe namespace sample\nName:         sample\nLabels:       kubernetes.io/metadata.name=sample\nAnnotations:  <none>\nStatus:       Active\nResource Quotas\n  Name:            sample-quota\n  Resource         Used  Hard\n  --------         ---   ---\n  limits.cpu       0     2\n  limits.memory    0     512Mi\n  requests.cpu     0     1\n  requests.memory  0     256Mi\n...\nEven though this quota will apply to all users that try to create Pods in the\nNamespace, even cluster administrators, it’s more realistic to use a \nnormal\nuser, given that an administrator can always create new Namespaces to get\naround a quota. Thus, we’ll also create a user:\nroot@host01:~# \nkubeadm kubeconfig user --client-name=me \\n  \n--config /etc/kubernetes/kubeadm-init.yaml > kubeconfig\nAs we did in \nChapter 11\n, we’ll bind the \nedit\n role to this user to provide the\nright to create and edit resources in the \nsample\n Namespace. We’ll use the same\nRoleBinding that we saw in \nListing 11-1\n:\nroot@host01:~# \nkubectl apply -f /opt/edit-bind.yaml\nrolebinding.rbac.authorization.k8s.io/editor created\nNow that our user is set up, let’s set the \nKUBECONFIG\n environment variable\nso that future \nkubectl\n commands will operate as our normal user:\nroot@host01:~# \nexport KUBECONFIG=kubeconfig\nFirst, we can verify that the \nedit\n role possessed by our normal user does not\nenable making changes to quotas in a Namespace, which makes sense—\nquotas are an administrator function:\nroot@host01:~# \nkubectl delete -n sample resourcequota sample-quota\nError from server (Forbidden): resourcequotas ""sample-quota"" is forbidden: \nUser ""me"" cannot delete resource ""resourcequotas"" in API group """" in the \nnamespace ""sample""\nWe can now create some Pods in the \nsample\n Namespace to test the quota.\nFirst, let’s try to create a Pod with no limits:\nroot@host01:~# \nkubectl run -n sample nginx --image=nginx\nError from server (Forbidden): pods ""nginx"" is forbidden: failed quota: \nsample-quota: must specify limits.cpu,limits.memory...\nBecause our Namespace has a quota, we are no longer allowed to create\nPods without specifying limits.\nIn \nListing 14-2\n, we try it again, this time using a Deployment that\nspecifies resource limits for the Pods it creates.\nsleep.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: sleep\n  namespace: sample\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n      - name: sleep\n        image: busybox\n        command:\n          - ""/bin/sleep""\n          - ""3600""\n        resources:\n          requests:\n            memory: ""64Mi""\n            cpu: ""250m""\n          limits:\n            memory: ""128Mi""\n            cpu: ""512m""\nListing 14-2: Deployment with Limit\nNow we can apply this to the cluster:\nroot@host01:~# \nkubectl apply -n sample -f /opt/sleep.yaml\n \ndeployment.apps/sleep created\nThis is successful because we specified the necessary request and limit\nfields and we didn’t exceed our quota. Additionally, a Pod is started with the\nlimits we specified:\nroot@host01:~# \nkubectl get -n sample pods\nNAME                     READY   STATUS    RESTARTS   AGE\nsleep-688dc46d95-wtppg   1/1     Running   0          72s\nHowever, we can see that we’re now using resources out of our quota:\nroot@host01:~# \nkubectl describe namespace sample\nName:         sample\nLabels:       kubernetes.io/metadata.name=sample\nAnnotations:  <none>\nStatus:       Active\nResource Quotas\n  Name:            sample-quota\n  Resource         Used   Hard\n  --------         ---    ---\n  limits.cpu       512m   2\n  limits.memory    128Mi  512Mi\n  requests.cpu     250m   1\n  requests.memory  64Mi   256Mi\n...\nThis will limit our ability to scale this Deployment. Let’s illustrate:\nroot@host01:~# \nkubectl scale -n sample deployment sleep --replicas=12\ndeployment.apps/sleep scaled\nroot@host01:~# \nkubectl get -n sample pods\nNAME                     READY   STATUS    RESTARTS   AGE\nsleep-688dc46d95-trnbl   1/1     Running   0          6s\nsleep-688dc46d95-vzfsx   1/1     Running   0          6s\nsleep-688dc46d95-wtppg   1/1     Running   0          3m13s\nWe’ve asked for 12 replicas, but we see only three running. If we describe\nthe Deployment we can see an issue:\nroot@host01:~# \nkubectl describe -n sample deployment sleep\nName:      sleep\nNamespace: sample\n...\nReplicas:   12 desired | 3 updated | 3 total | 3 available | 9 unavailable\n...\nConditions:\n  Type             Status  Reason\n  ----             ------  ------\n  Progressing      True    NewReplicaSetAvailable\n  Available        False   MinimumReplicasUnavailable\n  ReplicaFailure   True    FailedCreate\nOldReplicaSets:    <none>\nNewReplicaSet:     sleep-688dc46d95 (3/12 replicas created)\n...\nAnd the Namespace now reports that we have used up enough of our\nquota that there is no room to allocate the resources needed for another Pod:\nroot@host01:~# \nkubectl describe namespace sample\nName:         sample\n...\nResource Quotas\n  Name:            sample-quota\n  Resource         Used   Hard\n  --------         ---    ---\n  limits.cpu       1536m  2\n  limits.memory    384Mi  512Mi\n  requests.cpu     750m   1\n  requests.memory  192Mi  256Mi\n...\nOur Pods are running \nsleep\n, so we know they’re barely using any CPU or\nmemory. However, Kubernetes bases the quota utilization on what we\nspecified, not what the Pod is actually using. This is critical because",7567
69-15 PERSISTENT STORAGE.pdf,69-15 PERSISTENT STORAGE,"processes \nmay use more CPU or allocate more memory as they get busy, and\nKubernetes needs to make sure it leaves enough resources for the rest of the\ncluster to operate correctly.\nFinal Thoughts\nFor our containerized applications to be reliable, we need to know that one\napplication component can’t take too many resources and effectively starve\nthe other containers running in a cluster. Kubernetes is able to use the\nresource limit functionality of the underlying container runtime and the Linux\nkernel to limit each container to only the resources it has been allocated. This\npractice ensures more reliable scheduling of containers onto nodes in the\ncluster and ensures that the available cluster resources are shared in a fair\nway even as the cluster becomes heavily loaded.\nIn this chapter, we’ve seen how to specify resource requirements for our\nDeployments and how to apply quotas to Namespaces, effectively enabling\nus to treat all of the nodes in our cluster as one large pool of available\nresources. In the next chapter, we’ll examine how that same principle extends\nto storage as we look at dynamically allocating storage to Pods, no matter\nwhere they are scheduled.",1196
70-Storage Classes.pdf,70-Storage Classes,"15\nPERSISTENT STORAGE\nScalability and rapid failover are big advantages of containerized\napplications, and it’s a lot easier to scale, update, and replace stateless\ncontainers that don’t have any persistent storage. As a result, we’ve mostly\nused Deployments to create one or more instances of Pods with only\ntemporary storage.\nHowever, even if we have an application architecture in which most of the\ncomponents are stateless, we still need some amount of persistent storage for\nour application. At the same time, we don’t want to lose the ability to deploy\na Pod to any node in the cluster, and we don’t want to lose the contents of our\npersistent storage if a container or a node fails.\nIn this chapter, we’ll see how Kubernetes offers persistent storage on\ndemand to Pods by using a plug-in architecture that allows any supported\ndistributed storage engine to act as the backing store.\nStorage Classes\nThe Kubernetes storage plug-in architecture is highly flexible; it recognizes\nthat some clusters may not need storage at all, whereas others need multiple\nstorage plug-ins to handle large amounts of data or low-latency storage. \nFor\nthis reason, \nkubeadm\n doesn’t set up storage immediately during cluster\ninstallation; it’s configured afterward by adding \nStorageClass\n resources to\nthe cluster.\nEach StorageClass identifies a particular storage plug-in that will provide\nthe actual storage along with any additional required parameters. We can use\nmultiple storage classes to define different plug-ins or parameters, or even\nmultiple storage classes with the same plug-in but different parameters,\nallowing for separate classes of service for different purposes. For example, a\ncluster may provide in-memory, solid-state, and traditional spinning-disk\nmedia to give applications the opportunity to select the storage type that is\nmost applicable for a given purpose. The cluster may offer smaller quotas for\nmore expensive and lower-latency storage, while offering large quotas for\nslower storage that is more suitable for infrequently accessed data.\nKubernetes has a set of internal storage provisioners built in. This includes\nstorage drivers for popular cloud providers such as Amazon Web Services,\nMicrosoft Azure, and Google Container Engine. However, using any storage\nplug-in is easy as long as it has support for the Container Storage Interface\n(CSI), a published standard for interfacing with a storage provider.\nOf course, to be compatible with CSI, the storage provider must include a\nminimum set of features that are essential for storage in a Kubernetes cluster.\nThe most important of these are dynamic storage management (provisioning\nand deprovisioning) and dynamic storage attachment (mounting storage on\nany node in the cluster). Together, these two key features allow the cluster to\nallocate storage for any Pod that requests it, schedule that Pod on any node,\nand start a new Pod with the same storage on any node if the existing node\nfails or the Pod is replaced.\nStorage Class Definition\nOur Kubernetes cluster deployment in \nChapter 6\n included the Longhorn\nstorage plug-in (see “Installing Storage” on \npage 102\n). The automation\nscripts have installed it in the cluster for each following chapter. Part of this\ninstallation created a DaemonSet so that Longhorn components exist on\nevery node. That DaemonSet kicked off a number of Longhorn components\nand then created a StorageClass resource to tell Kubernetes how to use\nLonghorn to provision storage for a Pod.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nListing 15-1\n shows the StorageClass that Longhorn created.\nroot@host01:~# \nkubectl get storageclass\nNAME      PROVISIONER         RECLAIMPOLICY  VOLUMEBINDINGMODE\n  \nALLOWVOLUMEEXPANSION ...\nlonghorn  driver.longhorn.io  Delete         Immediate          true                 ...\nListing 15-1: Longhorn StorageClass\nThe two most important fields show the name of the StorageClass and the\nprovisioner. The name is used in resource specifications to identify that the\nLonghorn StorageClass should be used to provision the requested volume,\nwhereas the provisioner is used internally by \nkubelet\n to communicate with the\nLonghorn CSI plug-in.\nCSI Plug-in Internals\nLet’s look quickly at how \nkubelet\n finds and communicates with the Longhorn\nCSI plug-in before moving on to provisioning volumes and attaching them to\nPods. Note that \nkubelet\n runs as a service directly on the cluster nodes; on the\nother hand, all of the Longhorn components are containerized. This means\nthat the two need a little help to communicate in the form of a Unix socket\nthat is created on the host filesystem and then mounted into the filesystem of\nthe Longhorn containers. A Unix socket allows two processes to\ncommunicate by streaming data, similar to a network connection but without\nthe network overhead.\nTo explore how this communication works, first we’ll list the Longhorn\ncontainers that are running on \nhost01\n:\nroot@host01:~# \ncrictl ps --name 'longhorn.*|csi.*'\nCONTAINER     ... STATE    NAME ...\nc8347a513f71e ... Running  csi-provisioner ...\n47f950a3e8dbf ... Running  csi-provisioner ...\n3aad0fef7454e ... Running  longhorn-csi-plugin ...\n9bfb61f786afa ... Running  csi-snapshotter ...\n24a2994a264a1 ... Running  csi-snapshotter ...\n7ee4c748b4c02 ... Running  csi-snapshotter ...\n8d92886fdacda ... Running  csi-resizer ...\n9868014407fe0 ... Running  csi-resizer ...\n408d16181af51 ... Running  csi-attacher ...\n0c6c341debb0c ... Running  longhorn-driver-deployer ...\nba328a9d0aaf2 ... Running  longhorn-manager ...\nc39e5c4fee3bb ... Running  longhorn-ui ...\nLonghorn creates containers with names that start with either \nlonghorn\n or \ncsi\n,\nso we use a regular expression with \ncrictl\n to show only those containers.\nLet’s capture the container ID of the \ncsi-attacher\n container and then inspect it\nto see what volume mounts it has:\nroot@host01:~# \nCID=$(crictl ps -q --name csi-attacher)\nroot@host01:~# \ncrictl inspect $CID\n{\n...\n    ""mounts"": [\n      {\n        ""containerPath"": ""/csi/"",\n     \n➊\n ""hostPath"": ""/var/lib/kubelet/plugins/driver.longhorn.io"",\n        ""propagation"": ""PROPAGATION_PRIVATE"",\n        ""readonly"": false,\n        ""selinuxRelabel"": false\n      }\n...\n      ""envs"": [\n        {\n          ""key"": ""ADDRESS"",\n       \n➋\n ""value"": ""/csi/csi.sock""\n        },\n...\n}\nThe \ncrictl inspect\n command returns a lot of data from the container, but we\nshow only the relevant data in this example. We can see that this Longhorn\ncomponent is instructed to connect to \n/csi/csi.sock\n \n➋\n, which is the mount\npoint inside the container for the Unix socket that \nkubelet\n uses to communicate\nwith the storage driver. We can also see that \n/csi\n inside the container is\n/var/lib/kubelet/plugins/driver.longhorn.io\n \n➊\n. The location\n/var/lib/kubelet/plugins\n is a standard location for \nkubelet\n to look for storage\nplug-ins, and of course, \ndriver.longhorn.io\n is the value of the \nprovisioner\n field,\nas defined in the Longhorn StorageClass in \nListing 15-1\n.\nIf we look on the host, we can confirm that this Unix socket exists:\nroot@host01:~# \nls -l /var/lib/kubelet/plugins/driver.longhorn.io\ntotal 0",7452
71-Persistent Volumes.pdf,71-Persistent Volumes,"srwxr-xr-x 1 root root 0 Feb 18 20:17 csi.sock\nThe \ns\n as the first character indicates that this is a Unix socket.\nPersistent Volumes\nNow that we’ve seen how \nkubelet\n communicates with an external storage\ndriver, let’s look at how to request allocation of storage and then attach that\nstorage to a Pod.\nStateful Sets\nThe easiest way to get storage in a Pod is to use a StatefulSet (a resource\ndescribed in \nChapter 7\n). Like a Deployment, a StatefulSet creates multiple\nPods, which can be allocated to any node. However, a StatefulSet also creates\npersistent storage as well as a mapping between each Pod and its storage. If a\nPod needs to be replaced, it is replaced with a new Pod with the same\nidentifier and the same persistent storage.\nListing 15-2\n presents an example StatefulSet that creates two PostgreSQL\nPods with persistent storage.\npgsql-set.yaml\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\nspec:\n  serviceName: postgres\n  replicas: 2\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres\n        env:\n        - name: POSTGRES_PASSWORD\n       \n➊\n value: ""supersecret""\n        - name: PGDATA\n       \n➋\n value: /data/pgdata\n        volumeMounts:\n        - name: postgres-volume\n       \n➌\n mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: postgres-volume\n    spec:\n      storageClassName: longhorn\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\nListing 15-2: PostgreSQL StatefulSet\nIn addition to setting the password using an environment variable \n➊\n, we\nalso set \nPGDATA\n to \n/data/pgdata\n \n➋\n, which tells PostgreSQL where to store\nthe files for the database. It aligns with the volume mount we also declare as\npart of the StatefulSet, as that persistent volume will be mounted at \n/data\n \n➌\n.\nThe PostgreSQL container image documentation recommends configuring\nthe database files to reside in a subdirectory beneath the mount point to avoid\na potential issue with ownership of the data directory.\nSeparate from the configuration for the PostgreSQL Pods, we supply the\nStatefulSet with the \nvolumeClaimTemplates\n field. This field tells the StatefulSet\nhow we want the persistent storage to be configured. It includes the name of\nthe StorageClass and the requested size, and it also includes an \naccessMode\n of\nReadWriteOnce\n, which we’ll explore later. The StatefulSet will use this\nspecification to allocate independent storage for each Pod.\nAs mentioned in \nChapter 7\n, this StatefulSet references a Service using the\nserviceName\n field, and this Service is used to create the domain name for the\nPods. The Service is defined in the same file as follows:\npgsql-set.yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\nspec:\n  clusterIP: None\n  selector:\n    app: postgres\nSetting the \nclusterIP\n field to \nNone\n makes this a \nHeadless Service\n, which\nmeans that no IP address is allocated from the service IP range and none of\nthe load balancing described in \nChapter 9\n is configured for this Service. This\napproach is typical for a StatefulSet. With a StatefulSet, each Pod has its own\nunique identity and unique storage. Because service load balancing just\nrandomly chooses a destination, it is typically not useful with a StatefulSet.\nInstead, clients explicitly select a Pod instance as a destination.\nLet’s create the Service and StatefulSet:\nroot@host01:~# \nkubectl apply -f /opt/pgsql-set.yaml\n \nservice/postgres created\nstatefulset.apps/postgres created\nIt will take some time to get the Pods up and running because they are\ncreated sequentially, one at a time. After they are running, we can see how\nthey’ve been named:\nroot@host01:~# \nkubectl get pods\nNAME         READY   STATUS    RESTARTS   AGE\npostgres-0   1/1     Running   0          97s\npostgres-1   1/1     Running   0          51s\nLet’s examine the persistent storage from within the container:\nroot@host01:~# \nkubectl exec -ti postgres-0 -- /bin/sh\n# \nfindmnt /data\nTARGET SOURCE                         FSTYPE OPTIONS\n/data  /dev/longhorn/pvc-83becdac-... ext4   rw,relatime\n# \nexit\nAs requested, we see a Longhorn device that has been mounted at \n/data\n.\nKubernetes will keep this persistent storage even if the node fails or the Pod\nis upgraded.\nThis StatefulSet has two more important resources to explore. First is the\nheadless Service that we created:\nroot@host01:~# \nkubectl get svc\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   54m\npostgres     ClusterIP   None         <none>        <none>    19m\nThe \npostgres\n Service exists, but no cluster IP address is shown because we\ncreated it as a headless Service. However, it has created DNS entries for the\nassociated Pods, so we can use it to connect to specific PostgreSQL Pods\nwithout knowing the Pod IP address.\nWe need to use the cluster DNS to do the lookup. The easiest way to do\nthat is from within a container:\nroot@host01:~# \nkubectl run -ti --image=alpine --restart=Never alpine\nIf you don't see a command prompt, try pressing enter.\n/ #\nThis form of the \nrun\n command stays in the foreground and gives us an\ninteractive terminal. It also tells Kubernetes not to try to restart the container\nwhen we exit the shell.\nFrom inside this container, we can refer to either of our PostgreSQL Pods\nby a well-known name:\n/ # \nping -c 1 postgres-0.postgres.default.svc\nPING postgres-0.postgres.default.svc (172.31.239.198): 56 data bytes\n64 bytes from 172.31.239.198: seq=0 ttl=63 time=0.093 ms\n...\n/# \nping -c 1 postgres-1.postgres.default.svc\nPING postgres-1.postgres.default.svc (172.31.239.199): 56 data bytes\n64 bytes from 172.31.239.199: seq=0 ttl=63 time=0.300 ms\n...\n# \nexit\nThe naming convention is identical to what we saw for Services in\nChapter 9\n, but with an extra hostname prefix for the name of the Pod; in this\ncase, either \npostgres-0\n or \npostgres-1\n.\nThe other important resource is the \nPersistentVolumeClaim\n that the\nStatefulSet created automatically. The PersistentVolumeClaim is what\nactually allocates storage using the Longhorn StorageClass:\nroot@host01:~# \nkubectl get pvc\nNAME                         STATUS   VOLUME      ...   CAPACITY   ...\npostgres-volume-postgres-0   Bound    pvc-83becdac...   1Gi        ...\npostgres-volume-postgres-1   Bound    pvc-0d850889...   1Gi        ...\nWe use the abbreviation \npvc\n in lieu of its full name, \npersistentvolumeclaim\n.\nThe StatefulSet used the data in the \nvolumeClaimTemplates\n field in \nListing 15-2\nto create these two PersistentVolumeClaims. However, if we delete the\nStatefulSet, the PersistentVolumeClaims continue to exist:\nroot@host01:~# \nkubectl delete -f /opt/pgsql-set.yaml\n \nservice ""postgres"" deleted\nstatefulset.apps ""postgres"" deleted\nroot@host01:~# \nkubectl get pvc\nNAME                         STATUS   VOLUME      ...   CAPACITY   ...\npostgres-volume-postgres-0   Bound    pvc-83becdac...   1Gi        ...\npostgres-volume-postgres-1   Bound    pvc-0d850889...   1Gi        ...\nThis protects us from accidentally deleting our persistent storage. If we\ncreate the StatefulSet again and keep the same name in the volume claim\ntemplate, our new Pods will get the same storage back.\nHIGHLY AVAILABLE POSTGRESQL\nWe’ve deployed two separate instances of PostgreSQL, each with its\nown independent persistent storage. However, that’s only the first step\nin deploying a highly available database. We would also need to\nconfigure one instance as primary and the other as backup, configure\nreplication from the primary to the backup, and configure failover. We\nwould also need to configure clients to talk to the primary and switch to\na new primary when there’s a failure. Fortunately, we don’t need to do\nthis configuration ourselves. In \nChapter 17\n, we’ll see how to take\nadvantage of the power of custom resources to deploy a Kubernetes\nOperator for PostgreSQL that automatically will handle all of this.\nThe StatefulSet is the best way to handle the case in which we need\nmultiple instances of a container, each with its own independent storage.\nHowever, we can also use persistent volumes more directly, which gives us\nmore control over how they’re mounted into our Pods.\nVolumes and Claims\nKubernetes has both a \nPersistentVolume\n and a PersistentVolumeClaim\nresource type. The PersistentVolumeClaim represents a request for allocated\nstorage, whereas the PersistentVolume holds information on the allocated\nstorage. For the most part, this distinction doesn’t matter, and we can just\nfocus on the PersistentVolumeClaim. However, the difference is important in\ntwo cases:\nAdministrators can create a PersistentVolume manually, and this\nPersistentVolume can be directly mounted into a Pod.\nIf there is an issue allocating storage as specified in the\nPersistentVolumeClaim, the PersistentVolume will not be created.\nTo illustrate, first we’ll start with a PersistentVolumeClaim that\nautomatically allocates storage:\npvc.yaml\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nginx-storage\nspec:\n  storageClassName: longhorn\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Mi\nWe named this PersistentVolumeClaim \nnginx-storage\n because that’s how\nwe’ll use it in a moment. The PersistentVolumeClaim requests 100MiB of\nstorage from the \nlonghorn\n StorageClass. When we apply this\nPersistentVolumeClaim to the cluster, Kubernetes invokes the Longhorn\nstorage driver and allocates the storage, creating a PersistentVolume in the\nprocess:\nroot@host01:~# \nkubectl apply -f /opt/pvc.yaml\n \npersistentvolumeclaim/nginx-storage created\nroot@host01:~# \nkubectl get pv\nNAME         ...  CAPACITY ... STATUS  CLAIM                               STORAGECLASS ...\npvc-0b50e5b4-...  1Gi      ... Bound   default/postgres-volume-postgres-1  longhorn     ...\npvc-ad092ba9-...  1Gi      ... Bound   default/postgres-volume-postgres-0  longhorn     ...\npvc-cb671684-...  100Mi    ... Bound   default/nginx-storage               longhorn     ...\nThe abbreviation \npv\n is short for \npersistentvolumes\n.\nEven though no Pod is using the storage, it still shows a status of \nBound\nbecause there is an active PersistentVolumeClaim for the storage.\nIf we try to create a PersistentVolumeClaim without a matching storage\nclass, the cluster won’t be able to create the corresponding PersistentVolume:\npvc-man.yaml\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: manual\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Mi\nBecause there is no StorageClass called \nmanual\n, Kubernetes can’t create\nthis storage automatically:\nroot@host01:~# \nkubectl apply -f /opt/pvc-man.yaml\n \npersistentvolumeclaim/manual created\nroot@host01:~# \nkubectl get pvc\nNAME                         STATUS    ... STORAGECLASS   AGE\nmanual                       Pending   ... manual         6s\n...\nroot@host01:~# \nkubectl get pv\nNAME                                       ...\npvc-0b50e5b4-9889-4c8d-a651-df78fa2bc764   ...\npvc-ad092ba9-cf30-4b7d-af01-ff02a5924db7   ...\npvc-cb671684-1719-4c33-9dd8-bcbbf24523b4   ...\nOur PersistentVolumeClaim has a status of \nPending\n and there is no\ncorresponding PersistentVolume. However, as a cluster administrator, we can\ncreate this PersistentVolume manually:\npv.yaml\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: manual\nspec:\n  claimRef:\n    name: manual\n    namespace: default\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 100Mi\n  csi:\n    driver: driver.longhorn.io\n    volumeHandle: manual\nWhen creating a PersistentVolume in this way, we need to specify the\ntype of volume we want. In this case, by including the \ncsi\n field, we identify\nthis as a volume created by a CSI plug-in. We then specify the \ndriver\n to use\nand provide a unique value for \nvolumeHandle\n. After the PersistentVolume is\ncreated, Kubernetes directly invokes the Longhorn storage driver to allocate\nstorage.\nWe create the PersistentVolume with the following:\nroot@host01:~# \nkubectl apply -f /opt/pv.yaml\n \npersistentvolume/manual created\nBecause we specified a \nclaimRef\n for this PersistentVolume, it will\nautomatically move into the \nBound\n state:\nroot@host01:~# \nkubectl get pv manual\nNAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   ...\nmanual   100Mi      RWO            Retain           Bound    ...\nIt will take a few seconds, so the PersistentVolume may show up as\nAvailable\n briefly.\nThe PersistentVolumeClaim also moves into the \nBound\n state:\nroot@host01:~# \nkubectl get pvc manual\nNAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nmanual   Bound    manual   100Mi      RWO            manual         2m20s\nIt is useful for an administrator to create a PersistentVolume manually for\nthose rare cases when specialized storage is needed for an application.\nHowever, for most persistent storage, it is much better to automate storage\nallocation through a StorageClass and either a PersistentVolumeClaim or a\nStatefulSet.\nDeployments\nNow that we’ve directly created a PersistentVolumeClaim and we have the\nassociated volume, we can use it in a Deployment. To demonstrate this, we’ll\nshow how we can use persistent storage to hold HTML files served by an\nNGINX web server:\nnginx.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n       \n➊\n - name: html\n            mountPath: /usr/share/nginx/html\n      volumes:\n     \n➋\n - name: html\n          persistentVolumeClaim:\n            claimName: nginx-storage\nIt takes two steps to get the persistent storage mounted into our container.\nFirst, we declare a \nvolume\n named \nhtml\n \n➋\n that references the\nPersistentVolumeClaim we created. This makes the storage available in the\nPod. Next, we declare a \nvolumeMount\n \n➊\n to specify where in the container’s\nfilesystem this particular volume should appear. The advantage of having\nthese two separate steps is that we can mount the same volume in multiple\ncontainers within the same Pod, which enables us to share data between\nprocesses using files even for cases in which the processes come from\nseparate container images.\nThis capability allows for some interesting use cases. For example,\nsuppose that we’re building a web application that includes some static\ncontent. We might deploy an NGINX web server to serve that content, as\nwe’re doing here. At the same time, we also need a way to update the\ncontent. We might do that by having an additional container in the Pod that\nperiodically checks for new content and updates a persistent volume that is\nshared with the NGINX container.\nLet’s create the NGINX Deployment so that we can demonstrate that\nHTML files can be served from the persistent storage. The persistent storage\nwill start empty, so at first there won’t be any web content to serve. Let’s see\nhow NGINX behaves in that case:\nroot@host01:~# \nkubectl apply -f /opt/nginx.yaml\n \ndeployment.apps/nginx created\nAs soon as the NGINX server is up and running, we need to grab its IP\naddress so that we can make an HTTP request using \ncurl\n:\nroot@host01:~# \nIP=$(kubectl get po -l app=nginx -o jsonpath='{..podIP}')\nroot@host01:~# \ncurl -v http://$IP\n...\n* Connected to 172.31.25.200 (172.31.25.200) port 80 (#0)\n> GET / HTTP/1.1\n...\n< HTTP/1.1 403 Forbidden\nTo grab the IP address in this case, we use the \njsonpath\n output format for\nkubectl\n rather than use \njq\n to filter JSON output; \njsonpath\n has a very useful syntax\nfor searching into a JSON object and pulling out a single uniquely named\nfield (in this example, \npodIP\n). We could use a \njq\n filter similar to what we did in\nChapter 8\n, but the \njq\n syntax for recursion is more complex.\nAfter we have the IP, we use \ncurl\n to contact NGINX. As expected, we\ndon’t see an HTML response, because our persistent storage is empty.\nHowever, we know that our volume mounted correctly because in this case\nwe don’t even see the default NGINX welcome page.\nLet’s copy in an \nindex.html\n file to give our NGINX server something to\nserve:\nroot@host01:~# \nPOD=$(kubectl get po -l app=nginx -o jsonpath='{..metadata.name}')\nroot@host01:~# \nkubectl cp /opt/index.html $POD:/usr/share/nginx/html\nFirst, we capture the name of the Pod as randomly generated by the\nDeployment and then we use \nkubectl cp\n to copy in an HTML file. If we try\nrunning \ncurl\n again, we’ll see a much better response:\nroot@host01:~# curl -v http://$IP\n...\n* Connected to 172.31.239.210 (172.31.239.210) port 80 (#0)\n> GET / HTTP/1.1\n...\n< HTTP/1.1 200 OK\n...\n<html>\n  <head>\n    <title>Hello, World</title>\n  </head>\n  <body>\n    <h1>Hello, World!</h1>\n  </body>\n</html>\n...\nBecause this is persistent storage, this HTML content will remain\navailable even if we delete the Deployment and create it again.\nHowever, we still have one significant problem to overcome. One of the\nprimary reasons to have a Deployment is to be able to scale to multiple Pod\ninstances. Scaling this Deployment makes a lot of sense, as we could have\nmultiple Pod instances serving the same HTML content. Unfortunately,\nscaling won’t currently work:\nroot@host01:~# \nkubectl scale --replicas=3 deployment/nginx\ndeployment.apps/nginx scaled\nThe Deployment appears to scale, but if we look at the Pods, we will see\nthat we don’t really have multiple running instances:\nroot@host01:~# \nkubectl get pods\nNAME                    READY   STATUS              RESTARTS   AGE\n...\nnginx-db4f4d5d9-7q7rd   0/1     ContainerCreating   0          46s\nnginx-db4f4d5d9-gbqxm   0/1     ContainerCreating   0          46s\nnginx-db4f4d5d9-vrzr4   1/1     Running             0          10m\nThe two new instances are stuck in \nContainerCreating\n. Let’s examine one of\nthose two Pods to see why:\nroot@host01:~# \nkubectl describe\n \npod/nginx-db4f4d5d9-7q7rd\nName:           nginx-db4f4d5d9-7q7rd\n...\nStatus:         Pending\nEvents:\n  Type     Reason              Age   From                     Message\n  ----     ------              ----  ----                     -------\n...\n  Warning  FailedAttachVolume  110s  attachdetach-controller  Multi-Attach \n    error for volume ""pvc-cb671684-1719-4c33-9dd8-bcbbf24523b4"" Volume is \n    already used by pod(s) nginx-db4f4d5d9-vrzr4\nThe first Pod we created has claimed the volume, and no other Pods can\nattach to it, so they are stuck in a \nPending\n state. Even worse, this doesn’t just\nprevent scaling, it also prevents upgrading or making other configuration\nchanges to the Deployment. If we update the Deployment configuration,\nKubernetes will try to start a Pod using the new configuration before shutting\ndown any old Pods. The new Pods can’t attach to the volume and therefore\ncan’t start, so the old Pod will never be cleaned up and the configuration\nchange will never take place.\nWe could force a Pod update in a couple ways. First, we could manually\ndelete and re-create the Deployment anytime we made changes. Second, we\ncould configure Kubernetes to delete the old Pod first by using a \nRecreate\nupdate strategy. We explore update strategy options in greater detail in\nChapter 20\n. For now, it’s worth noting that this still would not allow us to\nscale the Deployment.\nIf we want to fix this so that we can scale the Deployment, we’ll need to\nallow multiple Pods to attach to the volume at the same time. We can do this\nby changing the access mode for the persistent volume.\nAccess Modes\nKubernetes is refusing to attach multiple Pods to the same persistent volume\nbecause we configured the PersistentVolumeClaim with an access mode of\nReadWriteOnce\n. An alternate access mode, \nReadWriteMany\n, will allow all of the\nNGINX server Pods to mount the storage simultaneously. Only some storage\ndrivers support the \nReadWriteMany\n access mode, because it requires the ability\nto manage simultaneous changes to files, including communicating changes\ndynamically to all of the nodes in the cluster.\nLonghorn does support \nReadWriteMany\n, so creating a PersistentVolumeClaim\nwith \nReadWriteMany\n access mode is an easy change:\npvc-rwx.yaml\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: storage\nspec:\n  storageClassName: longhorn\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 100Mi\nUnfortunately, we can’t modify our existing PersistentVolumeClaim to\nchange the access mode. And we can’t delete the PersistentVolumeClaim\nwhile the storage is in use by our Deployment. So we need to clean up\neverything and then deploy again:\nroot@host01:~# \nkubectl delete deploy/nginx pvc/storage\ndeployment.apps ""nginx"" deleted\npersistentvolumeclaim ""storage"" deleted\nroot@host01:~# \nkubectl apply -f /opt/pvc-rwx.yaml\n \npersistentvolumeclaim/storage created\nroot@host01:~# \nkubectl apply -f /opt/nginx.yaml\n \ndeployment.apps/nginx created\nWe specify \ndeploy/nginx\n and \npvc/storage\n as the resources to delete. This style of\nidentifying the resources allows us to operate on two resources in the same\ncommand.\nAfter a minute or so, the new NGINX Pod will be running:\nroot@host01:~# \nkubectl get pods\nNAME                    READY   STATUS      RESTARTS   AGE\n...\nnginx-db4f4d5d9-6thzs   1/1     Running     0          44s",22093
72-Final Thoughts.pdf,72-Final Thoughts,"At this point, we need to copy our HTML content over again because\ndeleting the PersistentVolumeClaim deleted the previous storage:\nroot@host01:~# \nPOD=$(kubectl get po -l app=nginx -o jsonpath='{..metadata.name}')\nroot@host01:~# \nkubectl cp /opt/index.html $POD:/usr/share/nginx/html\n... no output ...\nThis time, when we scale our NGINX Deployment, the additional two\nPods are able to mount the storage and start running:\nroot@host01:~# \nkubectl scale --replicas=3 deploy nginx\ndeployment.apps/nginx scaled\nroot@host01:~# \nkubectl get po\nNAME                    READY   STATUS      RESTARTS   AGE\n...\nnginx-db4f4d5d9-2j629   1/1     Running     0          23s\nnginx-db4f4d5d9-6thzs   1/1     Running     0          5m19s\nnginx-db4f4d5d9-7r5qj   1/1     Running     0          23s\nAll three NGINX Pods are serving the same content, as we can see if we\nfetch the IP address for one of the new Pods and connect to it:\nroot@host01:~# \nIP=$(kubectl get po \nnginx-db4f4d5d9-2j629\n -o jsonpath='{..podIP}')\nroot@host01:~# \ncurl http://$IP\n<html>\n  <head>\n    <title>Hello, World</title>\n  </head>\n  <body>\n    <h1>Hello, World!</h1>\n  </body>\n</html>\nAt this point, we could use any NGINX Pod to update the HTML content\nand all Pods would serve the new content. We could even use a separate\nCronJob with an application component that updates the content dynamically,\nand NGINX would happily serve whatever files are in place.\nFinal Thoughts\nPersistent storage is an essential requirement for building a fully functioning\napplication. After a cluster administrator has configured one or more storage\nclasses, it’s easy for application developers to dynamically request persistent\nstorage as part of their application deployment. In most cases, the best way to\ndo this is with a StatefulSet, as Kubernetes will automatically handle\nallocating independent storage for each Pod and will maintain a one-to-one\nrelationship between Pod and storage during failover and upgrades.\nAt the same time, there are other storage use cases, such as having\nmultiple Pods access the same storage. We can easily handle those use cases\nby directly creating a PersistentVolumeClaim resource and then declaring it\nas a volume in a controller such as a Deployment or Job.\nAlthough persistent storage is an effective way to make file content\navailable to containers, Kubernetes has other powerful resource types that can\nstore configuration data and pass it to containers as either environment\nvariables or file content. In the next chapter, we’ll explore how to manage\napplication configuration and secrets.",2633
73-16 CONFIGURATION AND SECRETS.pdf,73-16 CONFIGURATION AND SECRETS,,0
74-Injecting Configuration.pdf,74-Injecting Configuration,"16\nCONFIGURATION AND SECRETS\nAny high-quality application is designed so that key configuration items can\nbe injected at runtime rather than being embedded in the source code. When\nwe move our application components to containers, we need a way to tell the\ncontainer runtime what configuration information to inject to ensure that our\napplication components behave the way they should.\nKubernetes provides two primary resource types for injecting this\nconfiguration information: ConfigMap and Secret. These two resources are\nvery similar in capability but have slightly different use cases.\nInjecting Configuration\nWhen we looked at container runtimes in \nPart I\n, we saw that we could pass\nenvironment variables to our containers. Of course, as Kubernetes manages\nthe container runtime for us, we’ll first need to pass that information to\nKubernetes, which will then pass it to the container runtime for us.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nFor simple configuration injection, we can provide environment variables\ndirectly from the Pod specification. We saw an example of this in Pod form\nwhen we created a PostgreSQL server in \nChapter 10\n. Here’s a PostgreSQL\nDeployment with a similar configuration in its embedded Pod specification:\npgsql.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres\n        env:\n        - name: POSTGRES_PASSWORD\n          value: ""supersecret""\nWhen we provide environment variables directly in the Deployment, those\nenvironment variables are stored directly in the YAML file and in the\ncluster’s configuration for that Deployment. There are two important\nproblems with embedding environment variables in this manner. First, we’re\nreducing flexibility because we can’t specify a new value for the environment\nvariable without changing the Deployment YAML file. Second, the password\nis visible in plaintext directly in the Deployment YAML file. YAML files are\noften checked in to source control, so we’re going to have a hard time\nadequately protecting the password.\nGITOPS\nThe reason that the YAML files that define Kubernetes resources are\noften checked in to source control is that this is by far the best way to\nmanage an application deployment. GitOps is a best practice by which\nall configuration is kept in a Git repository. This includes the cluster\nconfiguration, additional infrastructure components including load\nbalancers, ingress controller, and storage plug-ins, as well as all of the\ninformation to build, assemble, and deploy applications. GitOps\nprovides a log of changes to the cluster configuration, avoids\nconfiguration drift that can occur over time, and ensures consistency\nbetween development, test, and production environments. Not only that,\nbut GitOps tools like FluxCD and ArgoCD can be used to watch\nchanges to a Git repository and automatically pull the latest\nconfiguration to update a cluster.\nLet’s first look at moving the configuration out of the Deployment; then\nwe’ll consider how best to protect the password.\nExternalizing Configuration\nEmbedding configuration in the Deployment makes the resource definition\nless reusable. If, for example, we wanted to deploy a PostgreSQL server for\nboth test and production versions of our application, it would be useful to\nreuse the same Deployment to avoid duplication and to avoid configuration\ndrift between the two versions. However, for security, we would not want to\nuse the same password in both environments.\nIt’s better if we externalize the configuration by storing it in a separate\nresource and referring to it from the Deployment. To enable this, Kubernetes\noffers the \nConfigMap\n resource. A ConfigMap specifies a set of key–value\npairs that can be referenced when specifying a Pod. For example, we can\ndefine our PostgreSQL configuration this way:\npgsql-cm.yaml\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: pgsql\ndata:\n  POSTGRES_PASSWORD: ""supersecret""\nBy storing this configuration information in a ConfigMap, it is no longer\ndirectly part of the Deployment YAML file or the cluster configuration for\nthe Deployment.\nAfter we’ve defined our ConfigMap, we can reference it in our\nDeployment, as demonstrated in \nListing 16-1\n.\npgsql-ext-cfg.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres\n        envFrom:\n        - configMapRef:\n            name: pgsql\nListing 16-1: PostgreSQL with ConfigMap\nIn place of the \nenv\n field, we have an \nenvFrom\n field that specifies one or more\nConfigMaps to serve as environment variables for the container. All of the\nkey–value pairs in the ConfigMap will become environment variables.\nThis has the same effect as specifying one or more environment variables\ndirectly in the Deployment, but our Deployment specification is now\nreusable. The Deployment will look for the identified ConfigMap in its own\nNamespace, so we can have multiple Deployments from the same\nspecification in separate Namespaces, and each can be configured differently.\nThis use of Namespace isolation to prevent naming conflicts, together\nwith the Namespace-scoped security controls we saw in \nChapter 11\n and the\nNamespace-scoped quotas we saw in \nChapter 14\n, allows a single cluster to be\nused for many different purposes, by many different groups, a concept known\nas \nmultitenancy\n.\nLet’s create this Deployment and see how Kubernetes injects the\nconfiguration. First, let’s create the actual Deployment:\nroot@host01:~# \nkubectl apply -f /opt/pgsql-ext-cfg.yaml\n \ndeployment.apps/postgres created\nThis command completes successfully because the Deployment has been\ncreated in the cluster, but Kubernetes will not be able to start any Pods\nbecause the ConfigMap is missing:\nroot@host01:~# \nkubectl get pods\nNAME                       READY  STATUS                      RESTARTS  AGE\npostgres-6bf595fcbc-s8dqz  0/1    CreateContainerConfigError  0         53s\nIf we now create the ConfigMap, we see that the Pod is then created:\nroot@host01:~# \nkubectl apply -f /opt/pgsql-cm.yaml\n \nconfigmap/pgsql created\nroot@host01:~# \nkubectl get pods\nNAME                        READY   STATUS    RESTARTS   AGE\npostgres-6bf595fcbc-s8dqz   1/1     Running   0          2m41s\nIt can take a minute or so for Kubernetes to determine that the ConfigMap\nis available and start the Pod. As soon as the Pod is running, we can verify\nthat the environment variables were injected based on the data in the\nConfigMap:\nroot@host01:~# \nkubectl exec -ti \npostgres-6bf595fcbc-s8dqz\n -- /bin/sh -c env\n...\nPOSTGRES_PASSWORD=supersecret\n...\nThe command \nenv\n prints out all of the environment variables associated\nwith a process. Because Kubernetes provides the same environment variables\nto our \n/bin/sh\n process as it provided to our main PostgreSQL process, we know\nthat the environment variable was set as expected. It’s important to note,\nhowever, that even though we can change the ConfigMap at any time, doing\nso will not cause the Deployment to update its Pods; the application will not\nautomatically pick up any environment variable changes. Instead, we need to\napply some configuration change to the Deployment to cause it to create new\nPods.\nAlthough the configuration has been externalized, we still are not\nprotecting it. Let’s do that next.\nProtecting Secrets\nWhen protecting secrets, thinking through the nature of the protection that\nmakes sense is important. For example, we might need to protect\nauthentication information that our application uses to connect to a database.\nHowever, given that the application itself needs that information to make the\nconnection, anyone who can inspect the inner details of the application is\ngoing to be able to extract those credentials.\nAs we saw in \nChapter 11\n, Kubernetes provides fine-grained access control\nover each individual resource type in a given Namespace. To enable\nprotection of secrets, Kubernetes provides a separate resource type, \nSecret\n.\nThis way, access to secrets can be limited to only those users who require\naccess, a principle known as \nleast privilege\n.\nOne more advantage to the Secret resource type is that it uses base64\nencoding for all of its data, with automatic decoding when the data is\nprovided to the Pod, which simplifies the storage of binary data.\nENCRYPTING SECRET DATA\nBy default, data stored in a Secret is base64 encoded but is not\nencrypted. It is possible to encrypt secret data, and doing so is good\npractice for a production cluster, but remember that the data must be\ndecrypted so that it can be provided to the Pod. For this reason, anyone\nwho can control what Pods exist in a namespace can access secret data,\nas can any cluster administrators who can access the underlying\ncontainer runtime. This is true even if the secret data is encrypted when\nstored. Proper access controls are essential to keep a cluster secure.\nA Secret definition looks almost identical to a ConfigMap definition:\npgsql-secret.yaml\n---\nkind: Secret\napiVersion: v1\nmetadata:\n  name: pgsql\nstringData:\n  POSTGRES_PASSWORD: ""supersecret""\nThe one obvious difference is the resource type of Secret rather than\nConfigMap. However, there is a subtle difference as well. When we define\nthis Secret, we place the key–value pairs in a field called \nstringData\n rather than\njust \ndata\n. This tells Kubernetes that we are providing unencoded strings. When\nit creates the Secret, Kubernetes will encode the strings for us:\nroot@host01:~# \nkubectl apply -f /opt/pgsql-secret.yaml\n \nsecret/pgsql created\nroot@host01:~# \nkubectl get secret pgsql -o json | jq .data\n{\n  ""POSTGRES_PASSWORD"": ""c3VwZXJzZWNyZXQ=""\n}\nEven though we specified the data using the field \nstringData\n and an\nunencoded string, the actual Secret uses the field \ndata\n and stores the value\nusing base64 encoding. We can also do the base64 encoding ourselves. In\nthat case, we place the value directly into the \ndata\n field:\npgsql-secret-2.yaml\n---\nkind: Secret\napiVersion: v1\nmetadata:\n  name: pgsql\ndata:\n  POSTGRES_PASSWORD: c3VwZXJzZWNyZXQ=\nThis approach is necessary to define binary content for the Secret in order\nfor us to be able to supply that binary content as part of a YAML resource\ndefinition.\nWe use a Secret in a Deployment definition in exactly the same way we\nuse a ConfigMap:\npgsql-ext-sec.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres\n        envFrom:\n        - secretRef:\n            name: pgsql\nThe only change is the use of \nsecretRef\n in place of \nconfigMapRef\n.\nTo test this, let’s apply this new Deployment configuration:\nroot@host01:~# \nkubectl apply -f /opt/pgsql-ext-sec.yaml\n \ndeployment.apps/postgres configured\nFrom the perspective of our Pod, the behavior is exactly the same.\nKubernetes handles the base64 decoding, making the decoded value visible to\nour Pod:\nroot@host01:~# \nkubectl get pods\nNAME                        READY   STATUS        RESTARTS   AGE\npostgres-6bf595fcbc-s8dqz   1/1     Terminating   0          12m\npostgres-794ff85bbf-xzz49   1/1     Running       0          26s\nroot@host01:~# \nkubectl exec -ti \npostgres-794ff85bbf-xzz49\n -- /bin/sh -c env\n...\nPOSTGRES_PASSWORD=supersecret\n...\nAs before, we use the \nenv\n command to show that the \nPOSTGRES_PASSWORD\nenvironment variable was set as expected. The Pod sees the same behavior\nwhether we specify the environment variable directly or use a ConfigMap or\nSecret.\nBefore we move on, let’s delete this Deployment:\nroot@host01:~# \nkubectl delete deploy postgres",12471
75-Injecting Files.pdf,75-Injecting Files,"deployment.apps ""postgres"" deleted\nUsing ConfigMaps and Secrets, we have the ability to externalize\nenvironment variable configuration for our application so that our\nDeployment specification can be reusable and to facilitate fine-grained access\ncontrol over secret data.\nInjecting Files\nOf course, environment variables are not the only way we commonly\nconfigure applications. We also need a way to provide configuration files.\nWe can do that using the same ConfigMap and Secret resources we’ve seen\nalready.\nAny files we inject in this way override files that exist in the container\nimage, which means that we can supply the container image with a sensible\ndefault configuration and then override that configuration with each container\nwe run. This makes it much easier to reuse container images.\nThe ability to specify file content in a ConfigMap and then mount it in a\ncontainer is immediately useful for configuration files, but we can also use it\nto update the NGINX web server example we showed in \nChapter 15\n. As\nwe’ll see, with this version we can declare our HTML content solely using\nKubernetes resource YAML files, with no need for console commands to\ncopy content into a PersistentVolume.\nThe first step is to define a ConfigMap with the HTML content we want\nto serve:\nnginx-cm.yaml\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: nginx\ndata:\n  index.html: |\n    <html>\n      <head>\n        <title>Hello, World</title>\n      </head>\n      <body>\n        <h1>Hello, World from a ConfigMap!</h1>\n      </body>\n    </html>\nThe key part of the key–value pair is used to specify the desired filename,\nin this case \nindex.html\n. For ease of reading, we use a pipe character (\n|\n) to start\na YAML multiline string. This string continues as long as the following lines\nare indented, or until the end of the YAML file. We can define multiple files\nin this way by just adding more keys to the ConfigMap.\nIn the Deployment we saw in \nListing 16-1\n, we specified the ConfigMap as\nthe source of environment variables. Here, we specify it as the source of a\nvolume mount:\nnginx-deploy.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - name: nginx-files\n          mountPath: /usr/share/nginx/html\n      volumes:\n        - name: nginx-files\n          configMap:\n            name: nginx\nThis volume definition looks similar to the one we saw in \nChapter 15\n. As\nbefore, the volume specification comes in two parts. The \nvolume\n field specifies\nwhere the volume comes from, in this case the ConfigMap. The \nvolumeMounts\nallows us to specify the path in the container where the files should be made\navailable. In addition to making it possible to use the same volume in\nmultiple containers in a Pod, this also means that we can share the same\nsyntax when mounting persistent volumes and when mounting the\nconfiguration as files in the container filesystem.\nLet’s create the ConfigMap and then get this Deployment started:\nroot@host01:~# \nkubectl apply -f /opt/nginx-cm.yaml\n \nconfigmap/nginx created\nroot@host01:~# \nkubectl apply -f /opt/nginx-deploy.yaml\ndeployment.apps/nginx created\nAfter the Pod is running, we can see that the file content is as expected,\nand NGINX is serving our HTML file:\nroot@host01:~# \nIP=$(kubectl get po -l app=nginx -o jsonpath='{..podIP}')\nroot@host01:~# \ncurl http://$IP\n<html>\n  <head>\n    <title>Hello, World</title>\n  </head>\n  <body>\n    <h1>Hello, World from a ConfigMap!</h1>\n  </body>\n</html>\nThe output looks similar to what we saw in \nChapter 15\n when we provided\nthe HTML content as a PersistentVolume, but we were able to avoid the\neffort of attaching the PersistentVolume and then copying content into it. In\npractice, both approaches have value, as maintaining a ConfigMap with a\nlarge amount of data would be unwieldy.\nTo make the contents of the ConfigMap appear as files in a directory,\nKubernetes is writing out the contents of the ConfigMap to the host\nfilesystem and then mounting the directory from the host into the container.\nThis means that the specific directory shows up as part of the output for the\nmount\n command inside the container:\nroot@host01:~# \nkubectl exec -ti \nnginx-58bc54b5cd-4lbkq\n -- /bin/mount\n...\n/dev/sda1 on /usr/share/nginx/html type ext4 (ro,relatime)\n...\nThe \nmount\n command reports that the directory \n/usr/share/nginx/html\n is a\nseparately mounted path coming from the host’s primary disk \n/dev/sda1\n.",4770
76-Cluster Configuration Repository.pdf,76-Cluster Configuration Repository,"We’re finished with the NGINX Deployment, so go ahead and delete it:\nroot@host01:~# \nkubectl delete deploy nginx\ndeployment.apps ""nginx"" deleted\nNext, let’s look at how ConfigMap and Secret information is stored in a\ntypical Kubernetes cluster so that we can see where \nkubelet\n is getting this\ncontent.\nCluster Configuration Repository\nAlthough it’s possible to run a Kubernetes cluster with different choices of\nconfiguration repository, most Kubernetes clusters use \netcd\n as the backing\nstore for all cluster configuration data. This includes not only the ConfigMap\nand Secret storage but also all of the other cluster resources and the \ncurrent\ncluster state. Kubernetes also uses \netcd\n to elect a leader when running in a\nhighly available configuration with multiple API servers.\nAlthough \netcd\n is generally stable and reliable, node failures can lead to\ncases in which the \netcd\n cluster can’t reestablish itself and elect a leader. Our\npurpose in demonstrating \netcd\n is not just to see how configuration data is\nstored, but also to provide some valuable background into an essential cluster\ncomponent that an administrator might need to debug.\nFor all of our example clusters, \netcd\n is installed on the same nodes as the\nAPI server, which is common in smaller clusters. In large clusters, running\netcd\n on separate nodes to allow it to scale separately from the Kubernetes\ncontrol plane is common.\nTo explore the contents of the \netcd\n backing store, we’ll use \netcdctl\n, a\ncommand line client designed for controlling and troubleshooting \netcd\n.\nUsing etcdctl\nWe need to tell \netcdctl\n where our \netcd\n server instance is located and how to\nauthenticate to it. For authentication, we’ll use the same client certificate that\nthe API server uses.\nFor convenience, we can set environment variables that \netcdctl\n will read, so\nwe don’t need to pass in those values via the command line with every\ncommand.\nHere are the environment variables we need:\netcd-env\nexport ETCDCTL_API=3\nexport ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt\nexport ETCDCTL_CERT=/etc/kubernetes/pki/apiserver-etcd-client.crt\nexport ETCDCTL_KEY=/etc/kubernetes/pki/apiserver-etcd-client.key\nexport ETCDCTL_ENDPOINTS=https://192.168.61.11:2379\nThese variables configure \netcdctl\n as follows:\nETCDCTL_API\n Use version 3 of the \netcd\n API. With recent versions of \netcd\n, only\nversion 3 is supported.\nETCDCTL_CACERT\n Verify the \netcd\n host using the provided certificate authority.\nETCDCTL_CERT\n Authenticate to \netcd\n using this certificate.\nETCDCTL_KEY\n Authenticate to \netcd\n using this private key.\nETCDCTL_ENDPOINTS\n Connect to \netcd\n at this URL. While \netcd\n is running on all\nthree nodes, we only need one node to talk to it.\nIn our example, these environment variables are conveniently stored in a\nscript in \n/opt\n so that we can load them for use with upcoming commands:\nroot@host01:~# \nsource /opt/etcd-env\nWe can now use \netcdctl\n commands to inspect the cluster and the\nconfiguration data it’s storing. Let’s begin by listing only the cluster\nmembers:\nroot@host01:~# \netcdctl member list\n45a2b6125030fdde, started, host02, https://192.168.61.12:2380, https://192.168.61.12:2379\n91007aab9448ce27, started, host03, https://192.168.61.13:2380, https://192.168.61.13:2379\nbf7b9991d532ba78, started, host01, https://192.168.61.11:2380, https://192.168.61.11:2379\nAs expected, each of the control plane nodes has an instance of \netcd\n. For a\nhighly available configuration, we need to run at least three instances, and we\nneed a majority of those instances to be running for the cluster to be healthy.\nThis \netcdctl\n command is a good first step to determine whether the cluster has\nany failed nodes.\nAs long as the cluster is healthy, we can store and retrieve data. Within\netcd\n, information is stored in key–value pairs. Keys are specified as paths in a\nhierarchy. We can list the paths that have content:\nroot@host01:~# \netcdctl get / --prefix --keys-only\n...\n/registry/configmaps/default/nginx\n/registry/configmaps/default/pgsql\n...\n/registry/secrets/default/pgsql\n...\nThe \n--prefix\n flag tells \netcdctl\n to get all keys that start with \n/\n, whereas \n--keys-only\nensures that we print only the keys to prevent being overwhelmed with data.\nStill, a lot of information is returned, including all of the various Kubernetes\nresource types that we’ve described in this book. Also included are the\nConfigMaps and Secrets we just created.\nDeciphering Data in etcd\nWe can generally rely on Kubernetes to store the correct configuration data in\netcd\n, and we can rely on \nkubectl\n to see the current cluster configuration.\nHowever, it is useful to know how the underlying data store works in case we\nneed to inspect the configuration when the cluster is down or in an anomalous\nstate.\nTo save storage space and bandwidth, both \netcd\n and Kubernetes use the\nprotobuf\n library, a language-neutral binary data format. Because we’re using\netcdctl\n to retrieve data from \netcd\n, we can ask it to return data in JSON format,\ninstead; however, that JSON data will include an embedded \nprotobuf\n structure\nwith the data from Kubernetes, so we’ll need to decode that as well.\nLet’s begin by examining the JSON format for a Kubernetes Secret in \netcd\n.\nWe’ll send the output through \njq\n for formatting:\nroot@host01:~# \netcdctl -w json get /registry/secrets/default/pgsql | jq\n{\n  ""header"": {\n...\n  },\n  ""kvs"": [\n    {\n      ""key"": ""L3JlZ2lzdHJ5L3NlY3JldHMvZGVmYXVsdC9wZ3NxbA=="",\n      ""create_revision"": 14585,\n      ""mod_revision"": 14585,\n      ""version"": 1,\n      ""value"": ""azhzAAoMCgJ2MRIGU2...""\n    }\n  ],\n  ""count"": 1\n}\nThe \nkvs\n field has the key–value pair that Kubernetes stored for this Secret.\nThe value for the key is a simple base64-encoded string:\nroot@host01:~# \necho $(etcdctl -w json get /registry/secrets/default/pgsql \\n| jq -r '.kvs[0].key' | base64 -d)\n/registry/secrets/default/pgsql\nWe use \njq\n to extract just the key’s value and return it in raw format\n(without quotes), and then we use \nbase64\n to decode the string.\nOf course, the interesting part of this key–value pair is the value because it\ncontains the actual Kubernetes Secret. Although the value is also base64\nencoded, we need to do a bit more detangling to access its information.\nAfter we decode the base 64 value, we’ll have a \nprotobuf\n message.\nHowever, it has a magic prefix that Kubernetes uses to allow for future\nchanges in the storage format. We can see that prefix if we look at the first\nfew bytes of the decoded value:\nroot@host01:~# \netcdctl -w json get /registry/secrets/default/pgsql \\n| jq -r '.kvs[0].value' | base64 -d | head --bytes=10 | xxd\n00000000: 6b38 7300 0a0c 0a02 7631                 k8s.....v1\nWe use \nhead\n to retrieve the first 10 bytes of the decoded value and then use\nxxd\n to see a hex dump. The first few bytes are \nk8s\n followed by an ASCII null\ncharacter. The rest of the data, starting with byte 5, is the actual \nprotobuf\nmessage.\nLet’s run one more command to actually decode the \nprotobuf\n message using\nthe \nprotoc\n tool:\nroot@host01:~# \netcdctl -w json get /registry/secrets/default/pgsql \\n| jq -r '.kvs[0].value' | base64 -d | tail --bytes=+5 | protoc --decode_raw\n1 {\n  1: ""v1""\n  2: ""Secret""\n}\n2 {",7443
77-Final Thoughts.pdf,77-Final Thoughts,"1 {\n    1: ""pgsql""\n    2: """"\n    3: ""default""\n    4: """"\n...\n  }\n  2 {\n    1: ""POSTGRES_PASSWORD""\n    2: ""supersecret""\n  }\n  3: ""Opaque""\n}\n...\nThe \nprotoc\n tool is mostly used for generating source code to read and write\nprotobuf\n messages, but it’s also handy for message decoding. As we can see,\nwithin the \nprotobuf\n message is all of the data Kubernetes stores for this Secret,\nincluding the resource version and type, the resource name and namespace,\nand the data. This illustrates, as mentioned earlier, that access to the hosts on\nwhich Kubernetes runs provides access to all of the secret data in the cluster.\nEven if we configured Kubernetes to encrypt data before storing it in \netcd\n, the\nencryption keys themselves need to be stored unencrypted in \netcd\n so that the\nAPI server can use them.\nFinal Thoughts\nWith the ability to provide either environment variables or files to Pods,\nConfigMaps and Secrets allow us to externalize the configuration of our\ncontainers, which makes it possible to reuse both Kubernetes resource\ndefinitions such as Deployments and container images in a variety of\napplications.\nAt the same time, we need to be aware of how Kubernetes stores this\nconfiguration data and how it provides it to containers. Anyone with the right\nrole can access configuration data using \nkubectl\n; anyone with access to the host\nrunning the container can access it from the container runtime; and anyone\nwith the right authentication information can access it directly from \netcd\n. For a\nproduction cluster, it’s critical that all of these mechanisms are correctly\nsecured.\nSo far, we’ve seen how Kubernetes stores built-in cluster resource data in\netcd\n, but Kubernetes can also store any kind of custom resource data we might\nchoose to declare. In the next chapter, we’ll explore how custom resource\ndefinitions enable us to add new behavior to a Kubernetes cluster in the form\nof operators.",1966
78-17 CUSTOM RESOURCES AND OPERATORS.pdf,78-17 CUSTOM RESOURCES AND OPERATORS,,0
79-Custom Resources.pdf,79-Custom Resources,"17\nCUSTOM RESOURCES AND OPERATORS\nWe’ve seen many different resource types used in a Kubernetes cluster to run\ncontainer workloads, scale them, configure them, route network traffic to\nthem, and provide storage for them. One of the most powerful features of a\nKubernetes cluster, however, is the ability to define custom resource types\nand integrate these into the cluster alongside all of the built-in resource types\nwe’ve already seen.\nCustom resource definitions enable us to define any new resource type\nand have the cluster track corresponding resources. We can use this\ncapability to add complex new behavior to our cluster, such as automating the\ndeployment of a highly available database engine, while taking advantage of\nall of the existing capabilities of the built-in resource types and the resource\nand status management of the cluster’s control plane.\nIn this chapter, we’ll see how custom resource definitions work and how\nwe can use them to deploy Kubernetes operators, extending our cluster to\ntake on any additional behavior we desire.\nCustom Resources\nIn \nChapter 6\n, we discussed how the Kubernetes API server provides a\ndeclarative API, where the primary actions are to create, read, update, and\ndelete resources in the cluster. A declarative API has advantages for\nresiliency, as the cluster can track the desired state of resources and work to\nensure that the cluster stays in that desired state. However, a declarative API\nalso has a significant advantage in extensibility. The actions provided by the\nAPI server are generic enough that extending them to any kind of resource is\neasy.\nWe’ve already seen how Kubernetes takes advantage of this extensibility\nto update its API over time. Not only can Kubernetes support new versions of\na resource over time, but brand-new resources with new capabilities can be\nadded to the cluster while backward compatibility is maintained through the\nold resources. We saw this in \nChapter 7\n in our discussion on the new\ncapabilities of version 2 of the HorizontalPodAutoscaler as well as the way\nthat the Deployment replaced the ReplicationController.\nWe really see the power of this extensibility in the use of\nCustomResourceDefinitions\n. A CustomResourceDefinition, or CRD, allows\nus to add any new resource type to a cluster dynamically. We simply provide\nthe API server with the name of the new resource type and a specification\nthat’s used for validation, and immediately the API server will allow us to\ncreate, read, update, and delete resources of that new type.\nCRDs are extremely useful and in widespread use. For example, the\ninfrastructure components that are already deployed to our cluster include\nCRDs.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details\non/linebreak getting set up.\nLet’s see the CRDs that are already registered with our cluster:\nroot@host01:~# \nkubectl get crds\nNAME                                                  CREATED AT\n...\nclusterinformations.crd.projectcalico.org             ...\n...\ninstallations.operator.tigera.io                      ...\n...\nvolumes.longhorn.io                                   ...\nTo avoid naming conflicts, the CRD name must include a group, which is\ncommonly based on a domain name to ensure uniqueness. This group is also\nused to establish the path to that resource for the REST API provided by the\nAPI server. In this example, we see CRDs in the \ncrd.projectcalico.org\n group and\nthe \noperator.tigera.io\n group, both of which are used by Calico. We also see a\nCRD in the \nlonghorn.io\n group, used by Longhorn.\nThese CRDs allow Calico and Longhorn to use the Kubernetes API to\nrecord configuration and status information in \netcd\n. CRDs also simplify\ncustom configuration. For example, as part of deploying Calico to the cluster,\nthe automation created an Installation resource that corresponds to the\ninstallations.operator.tigera.io\n CRD:\ncustom-resources.yaml\n---\napiVersion: operator.tigera.io/v1\nkind: Installation\nmetadata:\n  name: default\nspec:\n  calicoNetwork:\n    ipPools:\n    - blockSize: 26\n      cidr: 172.31.0.0/16\n...\nThis configuration is the reason why we see Pods getting IP addresses in\nthe \n172.31.0.0/16\n network block. This YAML file was automatically placed in\n/etc/kubernetes/components\n and automatically applied to the cluster as part of\nCalico installation. On deployment, Calico queries the API server for\ninstances of this Installation resource and configures networking accordingly.\nCreating CRDs\nLet’s explore CRDs further by creating our own. We’ll use the definition\nprovided in \nListing 17-1\n.\ncrd.yaml\n ---\n apiVersion: apiextensions.k8s.io/v1\n kind: CustomResourceDefinition\n metadata:\n➊\n name: samples.bookofkubernetes.com\n spec:\n➋\n group: bookofkubernetes.com\n   versions:\n  \n➌\n - name: v1\n       served: true\n       storage: true\n       schema:\n         openAPIV3Schema:\n           type: object\n           properties:\n             spec:\n               type: object\n               properties:\n                 value:\n                   type: integer\n➍\n scope: Namespaced\n   names:\n➎\n plural: samples\n➏\n singular: sample\n➐\n kind: Sample\n     shortNames:\n    \n➑\n - sam\nListing 17-1: Sample CRD\nThere are multiple important parts to this definition. First, several types of\nnames are defined. The metadata \nname\n field \n➊\n must combine the plural name\nof the resource \n➎\n and the group \n➋\n. These naming components will also be\ncritical for access via the API.\nNaming also includes the \nkind\n \n➐\n, which is used in YAML files. This\nmeans that when we create specific resources based on this CRD, we will\nidentify them with \nkind: Sample\n. Finally, we need to define how to refer to\ninstances of this CRD on the command line. This includes the full name of\nthe resource, specified in the \nsingular\n \n➏\n field, as well as any \nshortNames\n \n➑\n that\nwe want the command line to recognize.\nNow that we’ve provided Kubernetes with all of the necessary names for\ninstances based on this CRD, we can move on to how the CRD is tracked and\nwhat data it contains. The \nscope\n \n➍\n field tells Kubernetes whether this resource\nshould be tracked at the Namespace level or whether resources are cluster\nwide. Namespaced resources receive an API path that includes the\nNamespace they’re in, and authorization to access and modify Namespaced\nresources can be controlled on a Namespace-by-Namespace basis using Roles\nand RoleBindings, as we saw in \nChapter 11\n.\nThird, the \nversions\n section allows us to define the actual content that is valid\nwhen we create resources based on this CRD. To enable updates over time,\nthere can be multiple versions. Each version has a \nschema\n that declares what\nfields are valid. In this case, we define a \nspec\n field that contains one field\ncalled \nvalue\n, and we declare this one field to be an integer.\nThere was a lot of required configuration here, so let’s review the result.\nThis CRD enables us to tell the Kubernetes cluster to track a brand new kind\nof resource for us, a \nSample\n. Each instance of this resource (each Sample)\nwill belong to a Namespace and will contain an integer in a \nvalue\n field.\nLet’s create this CRD in our cluster:\nroot@host01:~# \nkubectl apply -f /opt/crd.yaml\ncustomresourcedefinition...k8s.io/samples.bookofkubernetes.com created\nWe can now create objects of this type and retrieve them from our cluster.\nHere’s an example YAML definition to create a new Sample using the CRD\nwe defined:\nsample.yaml\n---\napiVersion: bookofkubernetes.com/v1\nkind: Sample\nmetadata:\n  namespace: default\n  name: somedata\nspec:\n  value: 123\nWe match the \napiVersion\n and \nkind\n to our CRD and ensure that the \nspec\n is in\nalignment with the schema. This means that we’re required to supply a field\ncalled \nvalue\n with an integer value.\nWe can now create this resource in the cluster just like any other resource:\nroot@host01:~# \nkubectl apply -f /opt/somedata.yaml\n \nsample.bookofkubernetes.com/somedata created\nThere is now a Sample called \nsomedata\n that is part of the \ndefault\n Namespace.\nWhen we defined the CRD in \nListing 17-1\n, we specified a plural, singular,\nand short name for Sample resources. We can use any of these names to\nretrieve the new resource:\nroot@host01:~# \nkubectl get samples\nNAME       AGE\nsomedata   56s\nroot@host01:~# \nkubectl get sample\nNAME       AGE\nsomedata   59s\nroot@host01:~# \nkubectl get sam\nNAME       AGE\nsomedata   62s\nJust by declaring our CRD, we’ve extended the behavior of our\nKubernetes cluster so that it understands what \nsamples\n are, and we can use that\nnot only in the API but also in the command line tools.\nThis means that \nkubectl describe\n also works for Samples. We can see that\nKubernetes tracks other data related to our new resource, beyond just the data\nwe specified:\nroot@host01:~# \nkubectl describe sample somedata\nName:         somedata\nNamespace:    default\n...\nAPI Version:  bookofkubernetes.com/v1\nKind:         Sample\nMetadata:\n  Creation Timestamp:  ...\n...\n  Resource Version:  9386\n  UID:               37cc58db-179f-40e6-a9bf-fbf6540aa689\nSpec:\n  Value:  123\nEvents:   <none>\nThis additional data, including timestamps and resource versioning, is\nessential if we want to use the data from our CRD. To use our new resource\neffectively, we’re going to need a software component that continually\nmonitors for new or updated instances of our resource and takes action\naccordingly. We’ll run this component using a regular Kubernetes\nDeployment that interacts with the Kubernetes API server.\nWatching CRDs\nWith core Kubernetes resources, the control plane components communicate\nwith the API server to take the correct action when a resource is created,\nupdated, or deleted. For example, the controller manager includes a\ncomponent that watches for changes to Services and Pods, enabling it to\nupdate the list of endpoints for each Service. The \nkube-proxy\n instance on each\nnode then makes the necessary network routing changes to send traffic to\nPods based on those endpoints.\nWith CRDs, the API server merely tracks the resources as they are\ncreated, updated, and deleted. It is the responsibility of some other software\nto monitor instances of the resource and take the correct action. To make it\neasy to monitor resources, the API server offers a \nwatch\n action, using \nlong\npolling\n to keep a connection open and continually feed events as they occur.\nBecause a long-polling connection could be cut off at any time, the\ntimestamp and resource version data that Kubernetes tracks for us will enable\nus to detect what cluster changes we’ve already processed when we\nreconnect.\nWe could use the API server’s \nwatch\n capability directly from a \ncurl\ncommand or directly in an HTTP client, but it’s much easier to use a\nKubernetes client library. For this example, we’ll use the Python client\nlibrary to illustrate how to watch our custom resource. Here’s the Python\nscript we’ll use:\nwatch.py\n   #!/usr/bin/env python3\n   from kubernetes import client, config, watch\n   import json, os, sys\n   try:\n  \n➊\n config.load_incluster_config()\n   except:\n     print(""In cluster config failed, falling back to file"", file=sys.stderr)\n  \n➋\n config.load_kube_config()\n➌\n group = os.environ.get('WATCH_GROUP', 'bookofkubernetes.com')\n   version = os.environ.get('WATCH_VERSION', 'v1')\n   namespace = os.environ.get('WATCH_NAMESPACE', 'default')\n   resource = os.environ.get('WATCH_RESOURCE', 'samples')\n   \napi = client.CustomObjectsApi()\n   w = watch.Watch()\n➍\n for event in w.stream(api.list_namespaced_custom_object,\n          group=group, version=version, namespace=namespace, plural=resource):\n➎\n json.dump(event, sys.stdout, indent=2)\n    sys.stdout.flush()\nTo connect to the API server, we need to load cluster configuration. This\nincludes the location of the API server as well as the authentication\ninformation we saw in \nChapter 11\n. If we’re running in a container within a\nKubernetes Pod, we’ll automatically have that information available to us, so\nwe first try to load an in-cluster config \n➊\n. However, if we’re outside a\nKubernetes cluster, the convention is to use a Kubernetes config file, so we\ntry that as a secondary option \n➋\n.\nAfter we’ve established how to talk to the API server, we use the custom\nobjects API and a watch object to stream events related to our custom\nresource \n➍\n. The \nstream()\n method takes the name of a function and the\nassociated parameters, which we’ve loaded from the environment or from\ndefault values \n➌\n. We use the \nlist_namespaced_custom_object\n function because\nwe’re interested in our custom resource. All of the various \nlist_*\n methods in\nthe Python library are designed to work with \nwatch\n to return a stream of add,\nupdate, and remove events rather than simply retrieving the current list of\nobjects. As events occur, we then print them to the console in an easy-to-read\nformat \n➎\n.\nWe’ll use this Python script within a Kubernetes Deployment. I’ve built\nand published a container image to run it, so this is an easy task. Here’s the\nDeployment definition:\nwatch.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: watch\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: watch\n  template:\n    metadata:\n      labels:\n        app: watch\n    spec:\n      containers:\n      - name: watch\n        image: bookofkubernetes/crdwatcher:stable\n      serviceAccountName: watcher\nThis Deployment will run the Python script that watches for events on\ninstances of the Sample CRD. However, before we can create this\nDeployment, we need to ensure that our watcher script will have permissions\nto read our custom resource. The default ServiceAccount has minimal\npermissions, so we need to create a ServiceAccount for this Deployment and\nensure that it has the rights to see our Sample custom resources.\nWe could bind a custom Role to our ServiceAccount to do this, but it’s\nmore convenient to take advantage of role aggregation to add our Sample\ncustom resource to the \nview\n ClusterRole that already exists. This way, any\nuser in the cluster with the \nview\n ClusterRole will acquire rights to our Sample\ncustom resource.\nWe start by defining a new ClusterRole for our custom resource:\nsample-reader.yaml\n ---\n apiVersion: rbac.authorization.k8s.io/v1\n kind: ClusterRole\n metadata:\n   name: sample-reader\n   labels:\n  \n➊\n rbac.authorization.k8s.io/aggregate-to-view: ""true""\n rules:\n➋\n - apiGroups: [""bookofkubernetes.com""]\n    resources: [""samples""]\n    verbs: [""get"", ""watch"", ""list""]\nThis ClusterRole gives permission to \nget\n, \nwatch\n, and \nlist\n our Sample custom\nresources \n➋\n. We also add a label to the metadata \n➊\n to signal the cluster that\nwe want these permissions to be aggregated into the \nview\n ClusterRole. Thus,\nrather than bind our ServiceAccount into the \nsample-reader\n ClusterRole we’re\ndefining here, we can bind our ServiceAccount into the generic \nview\nClusterRole, giving it read-only access to all kinds of resources.\nWe also need to declare the ServiceAccount and bind it to the \nview\nClusterRole:\nsa.yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: watcher\n  namespace: default\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: viewer\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: watcher\n  namespace: default\nroleRef:\n  kind: ClusterRole\n  name: view\n  apiGroup: rbac.authorization.k8s.io\nWe use a RoleBinding to limit this ServiceAccount to read-only access\nsolely within the \ndefault\n Namespace. The RoleBinding binds the \nwatcher\nServiceAccount to the generic \nview\n ClusterRole. This ClusterRole will have\naccess to our Sample custom resources thanks to the role aggregation we\nspecified.\nWe’re now ready to apply all of these resources, including our\nDeployment:\nroot@host01:~# \nkubectl apply -f /opt/sample-reader.yaml\n \nclusterrole.rbac.authorization.k8s.io/sample-reader created\nroot@host01:~# \nkubectl apply -f /opt/sa.yaml\nserviceaccount/watcher created\nrolebinding.rbac.authorization.k8s.io/viewer created\nroot@host01:~# \nkubectl apply -f /opt/watch.yaml\n \ndeployment.apps/watch created\nAfter a little while, our watcher Pod will be running:\nroot@host01:~# \nkubectl get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nwatch-69876b586b-jp25m   1/1     Running   0          47s\nWe can print the watcher’s logs to see the events it has received from the\nAPI server:\nroot@host01:~# \nkubectl logs\n \nwatch-69876b586b-jp25m\n{\n  ""type"": ""ADDED"",\n  ""object"": {",17016
80-Operators.pdf,80-Operators,"""apiVersion"": ""bookofkubernetes.com/v1"",\n    ""kind"": ""Sample"",\n    ""metadata"": {\n...\n      ""creationTimestamp"": ""..."",\n...\n      ""name"": ""somedata"",\n      ""namespace"": ""default"",\n      ""resourceVersion"": ""9386"",\n      ""uid"": ""37cc58db-179f-40e6-a9bf-fbf6540aa689""\n    },\n    ""spec"": {\n      ""value"": 123\n    }\n  },\n...\nNote that the watcher Pod receives an \nADDED\n event for the \nsomedata\n Sample\nwe created, even though we created that Sample before we deployed our\nwatcher. The API server is able to determine that our watcher has not yet\nretrieved this object, so it sends us an event immediately on connection as if\nthe object were newly created, which avoids a race condition that we would\notherwise be forced to handle. However, note that if the client is restarted, it\nwill appear as a new client to the API server and will see the same \nADDED\nevent again for the same Sample. For this reason, when we implement the\nlogic to handle our custom resources, it’s essential to make the logic\nidempotent so that we can handle processing the same event multiple times.\nOperators\nWhat kinds of actions would we take in response to the creation, update, or\ndeletion of custom resources, other than just logging the events to the\nconsole? As we saw when we examined the way that custom resources are\nused to configure Calico networking in our cluster, one use for custom\nresources is to configure for cluster infrastructure components such as\nnetworking and storage. But another pattern that really makes the best use of\ncustom resources is the Kubernetes \nOperator\n.\nThe Kubernetes Operator pattern extends the behavior of the cluster to\nmake it easier to deploy and manage specific application components. Rather\nthan using the standard set of Kubernetes resources such as Deployments and\nServices directly, we simply create custom resources that are specific to the\napplication component, and the operator manages the underlying Kubernetes\nresources for us.\nLet’s look at an example to illustrate the power of the Kubernetes\nOperator pattern. We’ll add a Postgres Operator to our cluster that will enable\nus to deploy a highly available PostgreSQL database to our cluster by just\nadding a single custom resource.\nOur automation has staged the files that we need into\n/etc/kubernetes/components\n and has performed some initial setup, so the only\nstep remaining is to add the operator. The operator is a normal Deployment\nthat will run in whatever Namespace we choose. It then will watch for custom\npostgresql\n resources and will create PostgreSQL instances accordingly.\nLet’s deploy the operator:\nroot@host01:~# \nkubectl apply -f /etc/kubernetes/components/postgres-operator.yaml\n \ndeployment.apps/postgres-operator created\nThis creates a Deployment for the operator itself, which creates a single\nPod:\nroot@host01:~# \nkubectl get pods\nNAME                                 READY   STATUS    RESTARTS   AGE\npostgres-operator-5cdbff85d6-cclxf   1/1     Running   0          27s\n...\nThe Pod communicates with the API server to create the CRD needed to\ndefine a PostgreSQL database:\nroot@host01:~# \nkubectl get crd postgresqls.acid.zalan.do\nNAME                        CREATED AT\npostgresqls.acid.zalan.do   ...\nNo instances of PostgreSQL are running in the cluster yet, but we can\neasily deploy PostgreSQL by creating a custom resource based on that CRD:\npgsql.yaml\n---\napiVersion: ""acid.zalan.do/v1""\nkind: postgresql\nmetadata:\n  name: pgsql-cluster\n  namespace: default\nspec:\n  teamId: ""pgsql""\n  volume:\n    size: 1Gi\n    storageClass: longhorn\n  numberOfInstances: 3\n  users:\n    dbuser:\n    - superuser\n    - createdb\n  databases:\n    defaultdb: dbuser\n  postgresql:\n    version: ""14""\nThis custom resource tells the Postgres Operator to spawn a PostgreSQL\ndatabase using server version 14, with three instances (a primary and two\nbackups). Each instance will have persistent storage. The primary instance\nwill be configured with the specified user and database.\nThe real value of the Kubernetes Operator pattern is that the YAML\nresource file we declare is short, simple, and clearly relates to the\nPostgreSQL configuration we want to see. The operator’s job is to convert\nthis information into a StatefulSet, Services, and other cluster resources as\nneeded to operate this database.\nWe apply this custom resource to the cluster like any other resource:\nroot@host01:~# \nkubectl apply -f /opt/pgsql.yaml\n \npostgresql.acid.zalan.do/pgsql-cluster created\nAfter we apply it, the Postgres Operator will receive the add event and\nwill create the necessary cluster resources for PostgreSQL:\nroot@host01:~# \nkubectl logs\n \npostgres-operator-5cdbff85d6-cclxf\n... level=info msg=""Spilo operator...""\n...\n... level=info msg=""ADD event has been queued"" \n  cluster-name=default/pgsql-cluster pkg=controller worker=0\n... level=info msg=""creating a new Postgres cluster"" \n  cluster-name=default/pgsql-cluster pkg=controller worker=0\n...\n... level=info msg=""statefulset \n  \""default/pgsql-cluster\"" has been successfully created"" \n  cluster-name=default/pgsql-cluster pkg=cluster worker=0\n...\nUltimately, there will be a StatefulSet and three Pods running (in addition\nto the Pod for the operator itself, which is still running):\nroot@host01:~# \nkubectl get sts\nNAME            READY   AGE\npgsql-cluster   3/3     2m39s\nroot@host01:~# \nkubectl get po\nNAME                                 READY   STATUS    RESTARTS   AGE\npgsql-cluster-0                      1/1     Running   0          2m40s\npgsql-cluster-1                      1/1     Running   0          2m18s\npgsql-cluster-2                      1/1     Running   0          111s\npostgres-operator-5cdbff85d6-cclxf   1/1     Running   0          4m6s\n...\nIt can take several minutes for all of these resources to be fully running on\nthe cluster.\nUnlike the PostgreSQL StatefulSet we created in \nChapter 15\n, all instances\nin this StatefulSet are configured for high availability, as we can demonstrate\nby inspecting the logs for each Pod:\nroot@host01:~# \nkubectl logs pgsql-cluster-0\n...\n... INFO: Lock owner: None; I am pgsql-cluster-0\n... INFO: trying to bootstrap a new cluster\n...\n... INFO: initialized a new cluster\n...\n... INFO: no action. I am (pgsql-cluster-0) the leader with the lock\nroot@host01:~# \nkubectl logs pgsql-cluster-1\n...\n... INFO: Lock owner: None; I am pgsql-cluster-1\n... INFO: waiting for leader to bootstrap\n... INFO: Lock owner: pgsql-cluster-0; I am pgsql-cluster-1\n...\n... INFO: no action. I am a secondary (pgsql-cluster-1) and following \n    a leader (pgsql-cluster-0)\nAs we can see, the first instance, \npgsql-cluster-0\n, has identified itself as the\nleader, whereas \npgsql-cluster-1\n has configured itself as a follower that will\nreplicate any updates to the leader’s databases.\nTo manage the PostgreSQL leaders and followers and enable database\nclients to reach the leader, the operator has created multiple Services:\nroot@host01:~# \nkubectl get svc",7100
81-Final Thoughts.pdf,81-Final Thoughts,"NAME                   TYPE        CLUSTER-IP      ... PORT(S)    AGE\n...\npgsql-cluster          ClusterIP   10.101.80.163   ... 5432/TCP   6m52s\npgsql-cluster-config   ClusterIP   None            ... <none>     6m21s\npgsql-cluster-repl     ClusterIP   10.96.13.186    ... 5432/TCP   6m52s\nThe \npgsql-cluster\n Service routes traffic to the primary only; the other\nServices are used to manage replication to the backup instances. The operator\nhandles the task of updating the Service if the primary instance changes due\nto failover.\nTo remove the PostgreSQL database, we need to remove only the custom\nresource, and the Postgres Operator handles the rest:\nroot@host01:~# \nkubectl delete -f /opt/pgsql.yaml\n \npostgresql.acid.zalan.do ""pgsql-cluster"" deleted\nThe operator detects the removal and cleans up the associated Kubernetes\ncluster resources:\nroot@host01:~# \nkubectl logs\n \npostgres-operator-5cdbff85d6-cclxf\n...\n... level=info msg=""deletion of the cluster started"" \n  cluster-name=default/pgsql-cluster pkg=controller worker=0\n... level=info msg=""DELETE event has been queued"" \n  cluster-name=default/pgsql-cluster pkg=controller worker=0\n...\n... level=info msg=""cluster has been deleted"" \n  cluster-name=default/pgsql-cluster pkg=controller worker=0\nThe Postgres Operator has now removed the StatefulSet, persistent\nstorage, and other resources associated with this database cluster.\nThe ease with which we were able to deploy and remove a PostgreSQL\ndatabase server, including multiple instances automatically configured in a\nhighly available configuration, demonstrates the power of the Kubernetes\nOperator pattern. By defining a CRD, a regular Deployment can act to extend\nthe behavior of our Kubernetes cluster. The result is a seamless addition of\nnew cluster capability that is fully integrated with the built-in features of the\nKubernetes cluster.\nFinal Thoughts\nCustomResourceDefinitions and Kubernetes Operators bring advanced\nfeatures to a cluster, but they do so by building on the basic Kubernetes\ncluster functionality we’ve seen throughout this book. The Kubernetes API\nserver has the extensibility to handle storage and retrieval of any type of\ncluster resource. As a result, we’re able to define new resource types\ndynamically and have the cluster manage them for us.\nWe’ve seen this pattern across many of the features we’ve examined in\nPart II\n of this book. Kubernetes itself is built on the fundamental features of\ncontainers that we saw in \nPart I\n, and it is built so that its more advanced\nfeatures are implemented by bringing together its more basic features. By\nunderstanding how those basic features work, we’re better able to understand\nthe more advanced features, even if the behavior looks a bit magical at first.\nWe’ve now worked our way through the key capabilities of Kubernetes\nthat we need to understand to build high-quality, performant applications.\nNext, we’ll turn our attention to ways to improve the performance and\nresiliency of our applications when running them in a Kubernetes cluster.",3095
82-Affinity and Anti-affinity.pdf,82-Affinity and Anti-affinity,"PART III\nPERFORMANT KUBERNETES\nEven though containers are designed to hide some of the complexity of the\nindividual hosts in a cluster and their underlying hardware, real-world\napplications need tuning to get the most out of the available computing\npower. This tuning must be done in a way that works with the scalability and\nresiliency of our Kubernetes cluster so that we don’t lose the advantages of\ndynamic scheduling and horizontal scaling. In other words, we need to\nprovide hints to a cluster to help it schedule containers in the most efficient\nway.\n18\nAFFINITY AND DEVICES\nThe ideal application exhibits complete simplicity. It is simple to design. It is\nsimple to develop. It is simple to deploy. Its individual components are\nstateless, so it’s easy to scale to serve as many users as needed. The\nindividual service endpoints act as pure functions where the output is\ndetermined solely by the input. The application operates on a reasonable\namount of data, with modest CPU and memory requirements, and requests\nand responses easily fit into a JSON structure that is at most a couple of\nkilobytes.\nOf course, outside of tutorials, ideal applications don’t exist. Real-world\napplications store state, both in long-term persistent storage and in caches\nthat can be accessed quickly. Real-world applications have data security and\nauthorization concerns, so they need to authenticate users, remember who\nthose users are, and limit access accordingly. And many real-world\napplications need to access specialized hardware rather than just using\nidealized CPU, memory, storage, and network resources.\nWe want to deploy real-world applications on our Kubernetes cluster, not\njust idealized applications. This means that we need to make smart decisions\nabout how to deploy the application components that move us away from an\nideal world in which the cluster decides how many container instances to run\nand where to schedule them. However, we don’t want to create an application\narchitecture that is so rigid that we lose our cluster’s scalability and\nresiliency. Instead, we want to work within the cluster to give it hints about\nhow to deploy our application components while still maintaining as much\nflexibility as possible. In this chapter, we’ll explore how our application\ncomponents can enforce a little bit of coupling to other components or to\nspecialized hardware without losing the benefits of Kubernetes.\nAffinity and Anti-affinity\nWe’ll begin by looking at the case in which we want to manage the\nscheduling of Pods so that we can prefer or avoid co-locating multiple\ncontainers on the same node. For example, if we have two containers that\nconsume significant network bandwidth communicating with each other, we\nmight want those two containers to run together on a node to reduce latency\nand avoid slowing down the rest of the cluster. Or, if we want to ensure that a\nhighly available component can survive the loss of a node in the cluster, we\nmay want to split Pod instances so they run on as many different cluster\nnodes as possible.\nOne way to co-locate containers is to combine multiple separate\ncontainers into a single Pod specification. That is a great solution for cases in\nwhich two processes are completely dependent on each other. However, it\nremoves the ability to scale the instances separately. For example, in a web\napplication backed by distributed storage, we might need many more\ninstances of the web server process than we would need of the storage\nprocess. We need to place those application components in different Pods to\nbe able to scale them separately.\nIn \nChapter 8\n, when we wanted to guarantee that a Pod ran on a specified\nnode, we added the \nnodeName\n field to the Pod specification to override the\nscheduler. That was fine for an example, but for a real application it would\neliminate the scaling and failover that are essential for performance and\nreliability. Instead, we’ll use the Kubernetes concept of \naffinity\n to give the\nscheduler hints about how to allocate Pods without forcing any Pod to run on\na specific node.\nAffinity allows us to restrict where a Pod should be scheduled based on\nthe presence of other Pods. Let’s look at an example using the \niperf3\n network\ntesting application.\nCLUSTER ZONES\nPod affinity is most valuable for large clusters that span multiple\nnetworks. For example, we might deploy a Kubernetes cluster to\nmultiple different data centers to eliminate single points of failure. In\nthose cases, we would configure affinity based on a zone, which might\ncontain many nodes. Here, we have only a small example cluster, so\nwe’ll treat each node in our cluster as a separate zone.\nAnti-affinity\nLet’s start with the opposite of affinity: \nanti-affinity\n. Anti-affinity causes the\nKubernetes scheduler to avoid co-locating Pods. In this case, we’ll create a\nDeployment with three separate \niperf3\n server Pods, but we’ll use anti-affinity\nto distribute those three Pods across our nodes so that each node gets a Pod.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nHere’s the YAML definition we need:\nipf-server.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: iperf-server\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: iperf-server\n  template:\n    metadata:\n      labels:\n        app: iperf-server\n    spec:\n   \n➊\n affinity:\n        podAntiAffinity:\n       \n➋\n requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - iperf-server\n         \n➌\n topologyKey: ""kubernetes.io/hostname""\n      containers:\n      - name: iperf\n        image: bookofkubernetes/iperf3:stable\n        env:\n        - name: IPERF_SERVER\n          value: ""1""\nThis Deployment resource is typical except for the new \naffinity\n section \n➊\n.\nWe specify an anti-affinity rule that is based on the same label that the\nDeployment uses to manage its Pods. With this rule, we specify that we don’t\nwant a Pod to be scheduled into a zone that already has a Pod with the\napp=iperf-server\n label.\nThe \ntopologyKey\n \n➌\n specifies the size of the zone. In this case, each node in\nthe cluster has a different \nhostname\n label, so each node is considered to be a\ndifferent zone. The anti-affinity rule therefore prevents \nkube-scheduler\n from\nplacing a second Pod onto a node after the first Pod has already been\nscheduled there.\nFinally, because we specified the rule using \nrequiredDuringScheduling\n \n➋\n, it’s a\nhard\n anti-affinity rule, which means that the scheduler won’t schedule the\nPod unless it can satisfy the rule. It is also possible to use\npreferredDuringScheduling\n and assign a weight to give the scheduler a hint without\npreventing Pod scheduling if the rule can’t be satisfied.\nNOTE\nThe \ntopologyKey\n can be based on any label that’s applied on the node.\nCloud-based Kubernetes distributions typically automatically apply\nlabels to each node based on the availability zone for that node, making it\neasy to use anti-affinity to spread Pods across availability zones for\nredundancy.\nLet’s apply this Deployment and see the result:\nroot@host01:~# \nkubectl apply -f /opt/ipf-server.yaml\n \ndeployment.apps/iperf-server created\nAs soon as our Pods are running, we see that a Pod has been allocated to\neach node in the cluster:\nroot@host01:~# \nkubectl get po -o wide\nNAME                            READY   STATUS    ... NODE     ...\niperf-server-7666fb76d8-7rz8j   1/1     Running   ... host01   ...\niperf-server-7666fb76d8-cljkh   1/1     Running   ... host02   ...\niperf-server-7666fb76d8-ktk92   1/1     Running   ... host03   ...\nBecause we have three nodes and three instances, it’s essentially identical\nto using a DaemonSet, but this approach is more flexible because it doesn’t\nrequire an instance on every node. In a large cluster, we still might need only\na few Pod instances to meet demand for this service. Using anti-affinity with\nzones based on hostnames allows us to specify the correct scale for our\nDeployment while still distributing each Pod to a distinct node for higher\navailability. And anti-affinity can be used to distribute Pods across other\ntypes of zones as well.\nBefore we continue, let’s create a Service with which our \niperf3\n clients will\nbe able to find a server instance. Here’s the YAML:\nipf-svc.yaml\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: iperf-server\nspec:\n  selector:\n    app: iperf-server\n  ports:\n  - protocol: TCP\n    port: 5201\n    targetPort: 5201\nLet’s apply this to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/ipf-svc.yaml\n \nservice/iperf-server created\nThe Service picks up all three Pods:\nroot@host01:~# \nkubectl get ep iperf-server\nNAME           ENDPOINTS                                                 ...\niperf-server   172.31.239.207:5201,172.31.25.214:5201,172.31.89.206:5201 ...\nThe \nep\n is short for \nendpoints\n. Each Service has an associated Endpoint object\nthat records the current Pods that are receiving traffic for the Service.\nAffinity\nWe’re now ready to deploy our \niperf3\n client to use these server instances. We\nwould like to distribute the clients to each node in the same way, but we want\nto make sure that each client is deployed to a node that has a server instance.\nTo do this, we’ll use both an affinity and an anti-affinity rule:\nipf-client.yaml\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: iperf\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: iperf\n  template:\n    metadata:\n      labels:\n        app: iperf\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - iperf\n            topologyKey: ""kubernetes.io/hostname""\n        \n➊\n podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - iperf-server\n            topologyKey: ""kubernetes.io/hostname""\n      containers:\n      - name: iperf\n        image: bookofkubernetes/iperf3:stable\nThe additional \npodAffinity\n rule \n➊\n ensures that each client instance is\ndeployed to a node only if a server instance is already present. The fields in\nan affinity rule work the same way as an anti-affinity rule.\nLet’s deploy the client instances:\nroot@host01:~# \nkubectl apply -f /opt/ipf-client.yaml\n \ndeployment.apps/iperf created\nAfter these Pods are running, we can see that they have also been\ndistributed across all three nodes in the cluster:\nroot@host01:~# \nkubectl get po -o wide\nNAME                            READY   STATUS    ... NODE     ... \niperf-c8d4566f-btppf            1/1     Running   ... host02   ... \niperf-c8d4566f-s6rpn            1/1     Running   ... host03   ... \niperf-c8d4566f-v9v8m            1/1     Running   ... host01   ... \n...\nIt may seem like we’ve deployed our \niperf3\n client and server in a way that\nenables each client to talk to its local server instance, maximizing the\nbandwidth between client and server. However, that’s not actually the case.\nBecause the \niperf-server\n Service is configured with all three Pods, each client\nPod is connecting to a random server. As a result, our clients may not behave\ncorrectly. You might see logs indicating that a client is able to connect \nto a\nserver, but you might also see client Pods in the \nError\n or \nCrashLoopBackOff\n state,\nwith log output like this:\nroot@host01:~# \nkubectl logs\n \niperf-c8d4566f-v9v8m\niperf3: error - the server is busy running a test. try again later\niperf3 error - exiting\nThis indicates that a client is connecting to a server that already has a\nclient connected, which means that we must have at least two clients using\nthe same server.",12355
83-Service Traffic Routing.pdf,83-Service Traffic Routing,"Service Traffic Routing\nWe would like to configure our client Pods with the ability to access the local\nserver Pod we deployed rather than a server Pod on a different node. Let’s\nstart by confirming that traffic is being routed randomly across all three\nserver Pods. We can examine the \niptables\n rules created by \nkube-proxy\n for this\nService:\nroot@host01:~# \niptables-save | grep iperf-server\n...\n-A KUBE-SVC-KN2SIRYEH2IFQNHK -m comment --comment ""default/iperf-server"" \n  -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-IGBNNG5F5VCPRRWI\n-A KUBE-SVC-KN2SIRYEH2IFQNHK -m comment --comment ""default/iperf-server"" \n  -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-FDPADR4LUNHDJSPL\n-A KUBE-SVC-KN2SIRYEH2IFQNHK -m comment --comment ""default/iperf-server"" \n  -j KUBE-SEP-TZDPKVKUEZYBFM3V\nWe’re running this command on \nhost01\n, and we see that there are three\nseparate \niptables\n rules, with a random selection of the destination. This means\nthat the \niperf3\n client on \nhost01\n could potentially be routed to any server Pod.\nTo fix that, we need to change the internal traffic policy configuration of\nour Service. By default, the policy is \nCluster\n, indicating that all Pods in the\ncluster are valid destinations. We can change the policy to \nLocal\n, which\nrestricts the Service to route only to Pods on the same node.\nLet’s patch the Service to change this policy:\nroot@host01:~# \nkubectl patch svc iperf-server -p '{""spec"":{""internalTrafficPolicy"":""Local""}}'\nservice/iperf-server patched\nThe change takes effect immediately, as we can see by looking at the\niptables\n rules again:\nroot@host01:~# \niptables-save | grep iperf-server\n...\n-A KUBE-SVC-KN2SIRYEH2IFQNHK -m comment --comment ""default/iperf-server"" \\n  -j KUBE-SEP-IGBNNG5F5VCPRRWI\nThis time, only one possible destination is configured on \nhost01\n, as there\nis only one local Pod instance for this Service.\nAfter a few minutes, the \niperf3\n clients now show the kind of output we\nexpect to see:\nroot@host01:~# \nkubectl logs\n \niperf-c8d4566f-btppf\nConnecting to host iperf-server, port 5201\n...\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec  8.67 GBytes  7.45 Gbits/sec  1250             sender\n[  5]   0.00-10.00  sec  8.67 GBytes  7.45 Gbits/sec                  receiver\n...\nNot only are all of the clients able to connect to a unique server, but the\nperformance is consistently high as the network connection is local to each\nnode.\nBefore we go further, let’s clean up these resources:\nroot@host01:~# \nkubectl delete svc/iperf-server deploy/iperf deploy/iperf-server\nservice ""iperf-server"" deleted\ndeployment.apps ""iperf"" deleted\ndeployment.apps ""iperf-server"" deleted\nAlthough the \nLocal\n internal traffic policy is useful for maximizing\nbandwidth between client and server, it has a major limitation. If a node does\nnot contain a healthy Pod instance, clients on that node will not be able to\naccess the Service at all, even if there are healthy instances on other nodes. It\nis critical when using this design pattern to also configure a readiness probe,\nas described in \nChapter 13\n, that checks not only the Pod itself but also its\nService dependencies. This way, if a Service is inaccessible on a particular\nnode, the client on that node will also report itself to be unhealthy so that no\ntraffic will be routed to it.\nThe affinity and anti-affinity capabilities we’ve seen allows us to give\nhints to the scheduler without losing the scalability and resilience we want for\nour application components. However, even though it might be tempting to\nuse these features whenever we have closely connected components in our\napplication architecture, it’s probably best to allow the scheduler to work\nunhindered and add affinity only for cases in which real performance testing\nshows that it makes a significant difference.\nService routing for improved performance is an active area of\ndevelopment in Kubernetes. For clusters running across multiple zones, a\nnew feature called Topology Aware Hints can enable Kubernetes to route\nconnections to Services to the closest instances wherever possible, improving\nnetwork performance while still allowing cross-zone traffic where necessary.",4322
84-Hardware Resources.pdf,84-Hardware Resources,"Hardware Resources\nAffinity and anti-affinity allow us to control where Pods are scheduled but\nshould be used only if necessary. But what about cases for which a Pod needs\naccess to some specialized hardware that is available only on some nodes?\nFor example, we might have processing that would benefit from a graphics\nprocessing unit (GPU), but we might limit the number of GPU nodes in the\ncluster to reduce cost. In that case, it is absolutely necessary to ensure that the\nPod is scheduled in the right place.\nAs before, we could tie our Pod directly to a node using \nnodeName\n. But we\nmight have many nodes in our cluster with the right hardware, so what we\nreally want is to be able to tell Kubernetes about the requirement and then let\nthe scheduler decide how to satisfy it.\nKubernetes provides two related methods to address this need: device\nplug-ins and extended resources. A device plug-in provides the most\ncomplete functionality, but the plug-in itself must exist for the hardware\ndevice. Meanwhile, extended resources can be used for any hardware device,\nbut the Kubernetes cluster only tracks allocation of the resource; it doesn’t\nactually manage its availability in the container.\nImplementing a device plug-in requires close collaboration with \nkubelet\n.\nSimilar to the storage plug-in architecture we saw in \nChapter 15\n, a device\nplug-in registers itself with the \nkubelet\n instance running on a node, identifying\nany devices it manages. Pods identify any devices they require, and the\ndevice manager tells \nkubelet\n how to make the device available inside the\ncontainer (typically by mounting the device from the host into the container’s\nfilesystem).\nBecause we’re operating in a virtualized example cluster, we don’t have\nany specialized hardware to demonstrate a device plug-in, but an extended\nresource works identically from an allocation standpoint, so we can still get a\nfeel for the overall approach.\nLet’s begin by updating the cluster to indicate that one of the nodes has an\nexample extended resource. We do this by patching the \nstatus\n for the node.\nIdeally, we could do this with \nkubectl patch\n, but unfortunately it’s not possible to\nupdate the \nstatus\n of a resource with that command, so we’re reduced to using\ncurl\n to call the Kubernetes API directly. The \n/opt\n directory has a script to\nmake this easy. \nListing 18-1\n presents the relevant part.\nadd-hw.sh\n#!/bin/bash\n...\npatch='\n[\n  {\n    ""op"": ""add"", \n    ""path"": ""/status/capacity/bookofkubernetes.com~1special-hw"", \n    ""value"": ""3""\n  }\n]\n'\ncurl --cacert $ca --cert $cert --key $key \\n  -H ""Content-Type: application/json-patch+json"" \\n  -X PATCH -d ""$patch"" \\n  https://192.168.61.10:6443/api/v1/nodes/host02/status\n...\nListing 18-1: Special hardware script\nThis \ncurl\n command sends a JSON patch object to update the \nstatus\n field for\nthe node, adding an entry called \nbookofkubernetes.com/special-hw\n under \ncapacity\n. The\n~1\n acts as a slash character.\nRun the script to update the node:\nroot@host01:~# \n/opt/add-hw.sh\n \n...\nThe response from the API server includes the entire Node resource. Let’s\ndouble-check just the field we care about to make sure it applied:\nroot@host01:~# \nkubectl get node host02 -o json | jq .status.capacity\n{\n  ""bookofkubernetes.com/special-hw"": ""3"",\n  ""cpu"": ""2"",\n  ""ephemeral-storage"": ""40593612Ki"",\n  ""hugepages-2Mi"": ""0"",\n  ""memory"": ""2035228Ki"",\n  ""pods"": ""110""\n}\nThe extended resource shows up alongside the standard resources for the\nnode. We can now request this resource similar to how we request standard\nresources, as we saw in \nChapter 14\n.\nHere’s a Pod that requests the special hardware:\nhw.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sleep\nspec:\n  containers:\n  - name: sleep\n    image: busybox\n    command: [""/bin/sleep"", ""infinity""]\n    resources:\n      limits:\n        bookofkubernetes.com/special-hw: 1\nWe specify the requirement for the special hardware using the \nresources\nfield. The resource is either allocated or not allocated; thus, there’s no\ndistinction between requests and limits, so Kubernetes expects us to specify it\nusing \nlimits\n. When we apply this to the cluster, the Kubernetes scheduler will\nensure that this Pod runs on a node that can meet this requirement:\nroot@host01:~# \nkubectl apply -f /opt/hw.yaml\n \npod/sleep created\nAs a result, the Pod ends up on \nhost02\n:\nroot@host01:~# \nkubectl get po -o wide\nNAME    READY   STATUS    ... NODE     ...\nsleep   1/1     Running   ... host02   ...\nAdditionally, the node status now reflects an allocation for this extended\nresource:\nroot@host01:~# \nkubectl describe node host02\nName:               host02\n...\nAllocated resources:\n...\n  Resource                         Requests     Limits\n  --------                         --------     ------\n...\n  bookofkubernetes.com/special-hw  1            1\n...\nBoth the available quantity of three \nspecial-hw\n that we specified when we\nadded the extended resource in \nListing 18-1\n and the allocation of that\nresource to our Pod are arbitrary. The extended resource acts like a\nsemaphore in preventing too many users from using the same resource, but\nwe would need to add additional processing to deconflict multiple users if we\nreally had three separate special hardware devices on the same node.\nIf we do try to over-allocate based on what we specified is available, the\nPod won’t be scheduled. We can confirm this if we try to add another Pod\nthat needs all three of our special hardware devices:\nhw3.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sleep3\nspec:\n  containers:\n  - name: sleep\n    image: busybox\n    command: [""/bin/sleep"", ""infinity""]\n    resources:\n      limits:\n        bookofkubernetes.com/special-hw: 3\nLet’s try to add this Pod to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/hw3.yaml\n \npod/sleep created\nBecause there aren’t enough special hardware devices available, this Pod\nstays in the Pending state:\nroot@host01:~# \nkubectl get po -o wide\nNAME    READY   STATUS    ... NODE     ...\nsleep   1/1     Running   ... host02   ...\nsleep3  0/1     Pending   ... <none>   ...\nThe Pod will wait for the hardware to be available. Let’s delete our\noriginal Pod to free up room:\nroot@host01:~# \nkubectl delete pod sleep\n \npod/sleep deleted",6431
85-19 TUNING QUALITY OF SERVICE.pdf,85-19 TUNING QUALITY OF SERVICE,"Our new Pod will now start running:\nroot@host01:~# \nkubectl get po -o wide\nNAME    READY   STATUS    ... NODE     ...\nsleep3  1/1     Running   ... host02   ...\nAs before, the Pod was scheduled onto \nhost02\n because of the special\nhardware requirement.\nDevice drivers work identically from an allocation standpoint. In both\ncases, we use the \nlimits\n field to identify our hardware requirements. The only\ndifference is that we don’t need to patch the node manually to record the\nresource, because \nkubelet\n updates the node’s status automatically when the\ndevice driver registers. Additionally, \nkubelet\n invokes the device driver to\nperform any necessary allocation and configuration of the hardware when a\ncontainer is created.\nFinal Thoughts\nUnlike ideal applications, in the real world we often must deal with closely\ncoupled application components and the need for specialized hardware. It’s\ncritical that we account for those application requirements without losing the\nflexibility and resiliency that we gain from deploying our application to a\nKubernetes cluster. In this chapter, we’ve seen how affinity and device\ndrivers allow us to provide hints and resource requirements to the scheduler\nwhile still allowing it the flexibility to manage the application at scale\ndynamically.\nScheduling is not the only concern we might have as we consider how to\nobtain the desired behavior and performance from real-world applications. In\nthe next chapter, we’ll see how we can shape the processing and memory\nallocation for our Pods through the use of quality-of-service classes.",1612
86-Quality of Service Classes.pdf,86-Quality of Service Classes,"19\nTUNING QUALITY OF SERVICE\nIdeally, our applications would use minimal or highly predictable processing,\nmemory, storage, and network resources. In the real world, though,\napplications are “bursty,” with changes in load driven by user demand, large\namounts of data, or complex processing. In a Kubernetes cluster, where\napplication components are deployed dynamically to various nodes in the\ncluster, uneven distribution of load across those nodes can cause performance\nbottlenecks.\nFrom an application architecture standpoint, the more we can make the\napplication components small and scalable, the more we can evenly distribute\nload across the cluster. Unfortunately, it’s not always possible to solve\nperformance issues with horizontal scaling. In this chapter, we’ll look at how\nwe can use resource specifications to provide hints to the cluster about how to\nschedule our Pods, with the goal of making application performance more\npredictable.\nAchieving Predictability\nIn normal, everyday language, the term \nreal time\n has the sense of something\nthat happens quickly and continuously. But in computer science, we make a\ndistinction between \nreal time\n and \nreal fast\n to such a degree that they are\nthought of as opposites. This is due to the importance of predictability.\nReal-time processing is simply processing that needs to keep up with\nsome activity that is happening in the real world. It could be anything from\nairplane cockpit software that needs to keep up with sensor data input and\nmaintain up-to-date electronic flight displays, to a video streaming\napplication that needs to receive and decode each frame of video in time to\ndisplay it. In real-time systems, it is critical that we can guarantee that\nprocessing will be “fast enough” to keep up with the real-world requirement.\nFast enough is all we need. It’s not necessary for the processing to go any\nfaster than the real world, as there isn’t anything else for the application to\ndo. But even a single time interval when the processing is slower than the real\nworld means we fall behind our inputs or outputs, leading to annoyed movie\nwatchers—or even to crashed airplanes.\nFor this reason, the main goal in real-time systems is predictability.\nResources are allocated based on the worst-case scenario the system will\nencounter, and we’re willing to provide significantly more processing than\nnecessary to have plenty of margin on that worst case. Indeed, it’s common to\nrequire these types of systems to stay under 50 percent utilization of the\navailable processing and memory, even at maximum expected load.\nBut whereas responsiveness is always important, most applications don’t\noperate in a real-time environment, and this additional resource margin is\nexpensive. For that reason, most systems try to find a balance between\npredictability and efficiency, which means that we are often willing to\ntolerate a bit of slower performance from our application components as long\nas it is temporary.\nQuality of Service Classes\nTo help us balance predictability and efficiency for the containers in a cluster,\nKubernetes allocates Pods to one of three different Quality of Service classes:\nBestEffort\n, \nBurstable\n, and \nGuaranteed\n. In a way, we can think of these as descriptive.\nBestEffort\n is used when we don’t provide Kubernetes with any resource\nrequirements, and it can only do its best to provide enough resources for the\nPod. \nBurstable\n is used when a Pod might exceed its resource request. \nGuaranteed\nis used when we provide consistent resource requirements and our Pod is\nexpected to stay within them. Because these classes are descriptive and are\nbased solely on how the containers in the Pod specify their resource\nrequirements, there is no way to specify the QoS for a Pod manually.\nThe QoS class is used in two ways. First, Pods in a QoS class are grouped\ntogether for Linux control groups (cgroups) configuration. As we saw in\nChapter 3\n, cgroups are used to control resource utilization, especially\nprocessing and memory, for a group of processes, so a Pod’s cgroup affects\nits \npriority in use of processing time when the system load is high. Second, if\nthe node needs to start evicting Pods due to lack of memory resources, the\nQoS class affects which Pods are evicted first.\nBestEffort\nThe simplest case is one in which we declare a Pod with no \nlimits\n. In that case,\nthe Pod is assigned to the \nBestEffort\n class. Let’s create an example Pod to\nexplore what that means.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up.\nHere’s the Pod definition:\nbest-effort.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: best-effort\nspec:\n  containers:\n  - name: best-effort\n    image: busybox\n    command: [""/bin/sleep"", ""infinity""]\n  nodeName: host01\nThis definition includes no \nresources\n field at all, but the QoS class would be\nthe same if we included a \nresources\n field with \nrequests\n but no \nlimits\n.\nWe use \nnodeName\n to force this Pod onto \nhost01\n so that we can observe how\nits resource use is configured. Let’s apply it to to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/best-effort.yaml\n \npod/best-effort created\nAfter the Pod is running, we can look at its details to see that it has been\nallocated to the \nBestEffort\n QoS class:\nroot@host01:~# \nkubectl get po best-effort -o json | jq .status.qosClass\n""BestEffort""\nWe can use the \ncgroup-info\n script we saw in \nChapter 14\n to see how the QoS\nclass affects the cgroup configuration for containers in the Pod:\nroot@host01:~# \n/opt/cgroup-info best-effort\nContainer Runtime\n-----------------\nPod ID: 205...\nCgroup path: /kubepods.slice/kubepods-besteffort.slice/kubepods-...\nCPU Settings\n------------\nCPU Shares: 2\nCPU Quota (us): -1 per 100000\nMemory Settings\n---------------\nLimit (bytes): 9223372036854771712\nThe Pod is effectively unlimited in CPU and memory usage. However, the\nPod’s cgroup is under the \nkubepods-besteffort.slice\n path, reflecting its\nallocation to the \nBestEffort\n QoS class. This allocation has an immediate effect\non its CPU priority, as we can see when we compare the \ncpu.shares\n allocated to\nthe \nBestEffort\n class compared to the \nBurstable\n class:\nroot@host01:~# \ncat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-besteffort.slice/cpu.shares\n \n2\nroot@host01:~# \ncat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-burstable.slice/cpu.shares\n \n1157\nAs we saw in \nChapter 14\n, these values are relative, so this configuration\nmeans that when our system’s processing load is high, containers in \nBurstable\nPods are going to be allocated more than 500 times the processor share that\ncontainers in \nBestEffort\n Pods receive. This value is based on the number of\nPods that are already in the \nBestEffort\n and \nBurstable\n QoS classes, including the\nvarious cluster infrastructure components already running on \nhost01\n, thus\nyou might see a slightly different value.\nThe \nkubepods.slice\n cgroup sits at the same level as cgroups for user and\nsystem processes, so when the system is loaded it gets an approximately\nequal share of processing time as those other cgroups. Based on the\ncpu.shares\n identified within the \nkubepods.slice\n cgroup, \nBestEffort\n Pods are\nreceiving less than 1 percent of the total share of processing compared to\nBurstable\n Pods, even without considering any processor time allocated to\nGuaranteed\n Pods. This means that \nBestEffort\n Pods receive almost no processor\ntime when the system is loaded, so they should be used only for background\nprocessing that can run when the cluster is idle. In addition, because Pods are\nplaced in the \nBestEffort\n class only if they have no \nlimits\n specified, they cannot be\ncreated in a Namespace with limit quotas. So most of our application Pods\nwill be in one of the other two QoS classes.\nBurstable\nPods are placed in the \nBurstable\n class if they specify both \nrequests\n and \nlimits\n and if\nthose two specifications are different. As we saw in \nChapter 14\n, the \nrequests\nspecification is used for scheduling purposes, whereas the \nlimits\n specification\nis used for runtime enforcement. In other words, Pods in this \nsituation can\nhave “bursts” of resource utilization above their \nrequests\n level, but they cannot\nexceed their \nlimits\n.\nLet’s look at an example:\nburstable.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: burstable\nspec:\n  containers:\n  - name: burstable\n    image: busybox\n    command: [""/bin/sleep"", ""infinity""]\n    resources:\n      requests:\n        memory: ""64Mi""\n        cpu: ""50m""\n      limits:\n        memory: ""128Mi""\n        cpu: ""100m""\n  nodeName: host01\nThis Pod definition supplies both \nrequests\n and \nlimits\n resource requirements,\nand they are different, so we should expect this Pod to be placed in the\nBurstable\n class.\nLet’s apply this Pod to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/burstable.yaml\n \npod/burstable created\nNext, let’s verify that it was assigned to the \nBurstable\n QoS class:\nroot@host01:~# \nkubectl get po burstable -o json | jq .status.qosClass\n""Burstable""\nIndeed, the cgroup configuration follows the QoS class and the \nlimits\n we\nspecified:\nroot@host01:~# /opt/cgroup-info burstable\nContainer Runtime\n-----------------\nPod ID: 8d0...\nCgroup path: /kubepods.slice/kubepods-burstable.slice/kubepods-...\nCPU Settings\n------------\nCPU Shares: 51\nCPU Quota (us): 10000 per 100000\nMemory Settings\n---------------\nLimit (bytes): 134217728\nThe \nlimits\n specified for this Pod were used to set both a CPU limit and a\nmemory limit. Also, as we expect, this Pod’s cgroup is placed within\nkubepods-burstable.slice\n.\nAdding another Pod to the \nBurstable\n QoS class has caused Kubernetes to\nrebalance the allocation of processor time:\nroot@host01:~# \ncat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-besteffort.slice/cpu.shares\n \n2\nroot@host01:~# \ncat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-burstable.slice/cpu.shares\n \n1413\nThe result is that Pods in the \nBurstable\n QoS class now show a value of 1413\nfor \ncpu.shares\n, whereas Pods in the \nBestEffort\n class still show 2. This means\nthat the relative processor share under load is now 700 to 1 in favor of Pods\nin the \nBurstable\n class. Again, you may see slightly different values based on\nhow many infrastructure Pods Kubernetes has allocated to \nhost01\n.\nBecause \nBurstable\n Pods are scheduled based on \nrequests\n but cgroup runtime\nenforcement is based on \nlimits\n, a node’s processor and memory resources can\nbe overcommitted. It works fine as long as the Pods on a node balance out\none another so that the average utilization matches the \nrequests\n. It becomes a\nproblem if the average utilization exceeds the \nrequests\n. In that case, Pods will\nsee their CPU throttled and may even be evicted if memory becomes scarce,\nas we saw in \nChapter 10\n.\nGuaranteed\nIf we want to increase predictability for the processing and memory available\nto a Pod, we can place it in the \nGuaranteed\n QoS class by giving the \nrequests\n and\nlimits\n equal settings. Here’s an example:\nguaranteed.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: guaranteed\nspec:\n  containers:\n  - name: guaranteed\n    image: busybox\n    command: [""/bin/sleep"", ""infinity""]\n    resources:\n      limits:\n        memory: ""64Mi""\n        cpu: ""50m""\n  nodeName: host01\nIn this example, only \nlimits\n is specified given that Kubernetes automatically\nsets the \nrequests\n to match the \nlimits\n if \nrequests\n is missing.\nLet’s apply this to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/guaranteed.yaml\n \npod/guaranteed created\nAfter the Pod is running, verify the QoS class:\nroot@host01:~# \nkubectl get po guaranteed -o json | jq .status.qosClass\n""Guaranteed""\nThe cgroups configuration looks a little different:\nroot@host01:~# \n/opt/cgroup-info guaranteed\nContainer Runtime\n-----------------\nPod ID: 146...\nCgroup path: /kubepods.slice/kubepods-...\nCPU Settings\n------------\nCPU Shares: 51\nCPU Quota (us): 5000 per 100000\nMemory Settings\n---------------\nLimit (bytes): 67108864\nRather than place these containers into a separate directory, containers in\nthe \nGuaranteed\n QoS class are placed directly in \nkubepods.slice\n. Putting them in\nthis location has the effect of privileging containers in \nGuaranteed\n Pods when\nthe system is loaded because those containers receive their CPU shares\nindividually rather than as a class.\nQoS Class Eviction\nThe privileged treatment of Pods in the \nGuaranteed\n QoS class extends to Pod\neviction as well. As described in \nChapter 3\n, cgroup enforcement of memory\nlimits is handled by the OOM killer. The OOM killer also runs when a node\nis completely out of memory. To help the OOM killer choose which\ncontainers to terminate, Kubernetes sets the \noom_score_adj\n parameter based on\nthe QoS class of the Pod. This parameter can have a value from –1000 to\n1000. The higher the number, the more likely the OOM killer will choose a\nprocess to be killed.\nThe \noom_score_adj\n value is recorded in \n/proc\n for each process. The\nautomation has added a script called \noom-info\n to retrieve it for a given Pod.\nLet’s check the values for the Pods in each QoS class:\nroot@host01:~# \n/opt/oom-info best-effort\nOOM Score Adjustment: 1000\nroot@host01:~# \n/opt/oom-info burstable\nOOM Score Adjustment: 968\nroot@host01:~# \n/opt/oom-info guaranteed\nOOM Score Adjustment: -997\nPods in the \nBestEffort\n QoS class have the maximum adjustment of 1000, so\nthey would be targeted first by the OOM killer. Pods in the \nBurstable\n QoS class\nhave a score calculated based on the amount of memory specified in the\nrequests\n field, as a percentage of the node’s total memory capacity. This value\nwill therefore be different for every Pod but will always be between 2 and\n999. Thus, Pods in the \nBurstable\n QoS class will always be second in priority for\nthe OOM killer. Meanwhile, Pods in the \nGuaranteed\n QoS class are set close to\nthe minimum value, in this case –997, so they are protected from the OOM\nkiller as much as possible.\nOf course, as mentioned in \nChapter 3\n, the OOM killer terminates a\nprocess immediately, so it is an extreme measure. When memory on a node is\nlow but not yet exhausted, Kubernetes attempts to evict Pods to reclaim\nmemory. This eviction is also prioritized based on the QoS class. Pods in the\nBestEffort\n class and Pods in the \nBurstable\n class that are using more than their\nrequests\n value (high-use \nBurstable\n) are the first to be evicted, followed by Pods in\nthe \nBurstable\n class that are using less than their \nrequests\n value (low-use \nBurstable\n)\nand Pods in the \nGuaranteed\n class.\nBefore moving on, let’s do some cleanup:\nroot@host01:~# \nkubectl delete po/best-effort po/burstable po/guaranteed\npod ""best-effort"" deleted\npod ""burstable"" deleted\npod ""guaranteed"" deleted\nNow we can have a fresh start when we look at Pod priorities later in this\nchapter.\nChoosing a QoS Class\nGiven this prioritization in processing time and eviction priority, it might be\ntempting to place all Pods in the \nGuaranteed\n QoS class. And there are\napplication components for which this is a viable strategy. As described in\nChapter 7\n, we can configure a HorizontalPodAutoscaler to make new Pod\ninstances automatically if the existing instances are consuming a significant\npercentage of their allocated resources. This means that we can request a\nreasonable \nlimits\n value for Pods in a Deployment and allow the cluster to\nautomatically scale the Deployment if we’re getting too close to the limit\nacross those Pods. If the cluster is running in a cloud environment, we can\neven \nextend autoscaling to the node level, dynamically creating new cluster\nnodes when load is high and reducing the number of nodes when the cluster\nis idle.\nUsing only \nGuaranteed\n Pods together with autoscaling sounds great, but it\nassumes that our application components are easily scalable. It also only\nworks well when our application load consists of many small requests, so that\nan increase in load primarily means we are handing similar-sized requests\nfrom more users. If we have application components that periodically handle\nlarge or complex requests, we must set the \nlimits\n for those components to\naccommodate the worst-case scenario. Given that Pods in the \nGuaranteed\n QoS\nclass have \nrequests\n equal to \nlimits\n, our cluster will need enough resources to\nhandle this worst-case scenario, or we won’t even be able to schedule our\nPods. This results in a cluster that is largely idle unless the system is under its\nmaximum load. Similarly, if we have scalability limitations such as\ndependency on specialized hardware, we might have a natural limit on the\nnumber of Pods we can create for a component, forcing each Pod to have\nmore resources to handle its share of the overall load.\nFor this reason, it makes sense to balance the use of the \nGuaranteed\n and\nBurstable\n QoS classes for our Pods. Any Pods that have consistent load, or that\ncan feasibly be scaled horizontally to meet additional demand, should be in\nthe \nGuaranteed\n class. Pods that are harder to scale, or need to handle a mix of\nlarge and small workloads, should be in the \nBurstable\n class. These Pods should\nspecify their \nrequests\n based on their average utilization, and specify \nlimits\n based\non their worst-case scenario. Specifying resource requirements in this way\nwill ensure that the cluster’s expected performance margin can be monitored\nby simply comparing the allocated resources to the cluster capacity. Finally,",18022
87-Pod Priority.pdf,87-Pod Priority,"if a large request causes multiple application components to run at their\nworst-case utilization simultaneously, it may be worth running performance\ntests and exploring anti-affinity, as described in \nChapter 18\n, to avoid\noverloading a single node.\nPod Priority\nIn addition to using hints to help the Kubernetes cluster understand how to\nmanage Pods when the system is highly loaded, it is possible to tell the\ncluster directly to give some Pods a higher priority than others. This higher\npriority applies during Pod eviction, as Pods will be evicted in priority order\nwithin their QoS class. It also applies during scheduling because the\nKubernetes scheduler will evict Pods if necessary to be able to schedule a\nhigher-priority Pod.\nPod priority is a simple numeric field; higher numbers are higher priority.\nNumbers greater than one billion are reserved for critical system Pods. To\nassign a priority to a Pod, we must create a \nPriorityClass\n resource first.\nHere’s an example:\nessential.yaml\n---\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: essential\nvalue: 999999\nLet’s apply this to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/essential.yaml\n \npriorityclass.scheduling.k8s.io/essential created\nNow that this PriorityClass has been defined, we can apply it to Pods.\nHowever, let’s first create a large number of low-priority Pods through which\nwe can see Pods being preempted. We’ll use this Deployment:\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: lots\nspec:\n  replicas: 1000\n  selector:\n    matchLabels:\n      app: lots\n  template:\n    metadata:\n      labels:\n        app: lots\n    spec:\n      containers:\n      - name: sleep\n        image: busybox\n        command: [""/bin/sleep"", ""infinity""]\n        resources:\n          limits:\n            memory: ""64Mi""\n            cpu: ""250m""\nThis is a basic Deployment that runs \nsleep\n and doesn’t request very much\nmemory or CPU, but it does set \nreplicas\n to \n1000\n, so we’re asking our Kubernetes\ncluster to create 1,000 Pods. The example cluster isn’t large enough to deploy\n1,000 Pods, both because we don’t have sufficient resources to meet the\nspecification and because a node is limited to 110 Pods by default. Still, let’s\napply it to the cluster, as shown in \nListing 19-1\n, and the scheduler will create\nas many Pods as it can:\nroot@host01:~# \nkubectl apply -f /opt/lots.yaml\n \ndeployment.apps/lots created\nListing 19-1: Deploy lots of Pods\nLet’s describe the Deployment to see how things are going:\nroot@host01:~# \nkubectl describe deploy lots\nName:                   lots\nNamespace:              default\n...\nReplicas:               1000 desired ... | 7 available | 993 unavailable\n...\nWe managed to get only seven Pods in our example cluster, given the\nnumber of Pods already running for cluster infrastructure components.\nUnfortunately, that’s all the Pods we’ll get:\nroot@host01:~# \nkubectl describe node host01\nName:               host01\n  (Total limits may be over 100 percent, i.e., overcommitted.)\nAllocated resources:\n...\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu             \n➊\n 1898m (94%)  768m (38%)\n  memory             292Mi (15%)  192Mi (10%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\n...\nThe data for \nhost01\n shows that we’ve allocated 94 percent of the available\nCPU \n➊\n. But each of our Pods is requesting 250 millicores, so there isn’t\nenough capacity remaining to schedule another Pod on this node. The other\ntwo nodes are in a similar situation, with insufficient CPU room to schedule\nany more Pods. Still, the cluster is performing just fine. We’ve theoretically\nallocated all of the processing power, but those containers are just running\nsleep\n, and as such, they aren’t actually using much CPU.\nAlso, it’s important to remember that the \nrequests\n field is used for\nscheduling, so even though we have a number of infrastructure \nBestEffort\n Pods\nthat specify \nrequests\n but no \nlimits\n and we have plenty of \nLimits\n capacity on this\nnode, we still don’t have any room for scheduling new Pods. Only \nLimits\n can\nbe overcommitted, not \nRequests\n.\nBecause we have no more CPU to allocate to Pods, the rest of the Pods in\nour Deployment are stuck in a Pending state:\nroot@host01:~# \nkubectl get po | grep -c Pending\n993\nAll 993 of these Pods have the default pod priority of 0. As a result, when\nwe create a new Pod using the \nessential\n PriorityClass, it will jump to the front\nof the scheduling queue. Not only that, but the cluster will evict Pods as\nnecessary to enable it to be scheduled.\nHere’s the Pod definition:\nneeded.yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: needed\nspec:\n  containers:\n  - name: needed\n    image: busybox\n    command: [""/bin/sleep"", ""infinity""]\n    resources:\n      limits:\n        memory: ""64Mi""\n        cpu: ""250m""\n  priorityClassName: essential\nThe key difference here is the specification of the \npriorityClassName\n,\nmatching the PriorityClass we created. Let’s apply this to the cluster:\nroot@host01:~# \nkubectl apply -f /opt/needed.yaml\n \npod/needed created\nIt will take the cluster a little time to evict another Pod so that this one can\nbe scheduled, but after a minute or so it will start running:\nroot@host01:~# \nkubectl get po needed\nNAME     READY   STATUS    RESTARTS   AGE\nneeded   1/1     Running   0          36s\nTo allow this to happen, one of the Pods from the \nlots\n Deployment we\ncreated in \nListing 19-1\n had to be evicted:\nroot@host01:~# \nkubectl describe deploy lots\nName:                   lots\nNamespace:              default\nCreationTimestamp:      Fri, 01 Apr 2022 19:20:52 +0000\nLabels:                 <none>\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               app=lots\nReplicas:               1000 desired ... | \n➊\n 6 available | 994 unavailable\nWe’re now down to only six Pods available in the Deployment \n➊\n, as one\nPod was evicted. It’s worth noting that being in the \nGuaranteed\n QoS class did\nnot prevent this Pod from being evicted. The \nGuaranteed\n QoS class gets priority\nfor evictions caused by node resource usage, but not for eviction caused by\nthe scheduler finding room for a higher-priority Pod.\nOf course, the ability to specify a higher priority for a Pod, resulting in the",6512
88-Example Application Stack.pdf,88-Example Application Stack,"eviction of other Pods, is powerful and should be used sparingly. Normal\nusers do not have the ability to create a new PriorityClass, and administrators\ncan apply a quota to limit the use of a PriorityClass in a given Namespace,\neffectively limiting normal users from creating high-priority Pods.\nFinal Thoughts\nDeploying an application to Kubernetes so that it is performant and reliable\nrequires an understanding of the application architecture and of the normal\nand worst-case load for each component. Kubernetes QoS classes allow us to\nshape the way that Pods are deployed to nodes to achieve a balance of\npredictability and efficiency in the use of resources. Additionally, both QoS\nclasses and Pod priorities allow us to provide hints to the Kubernetes cluster\nso the deployed applications degrade gracefully as the load on the cluster\nbecomes too high.\nIn the next chapter, we’ll bring together the ideas we’ve seen on how to\nbest use the features of a Kubernetes cluster to deploy performant, resilient\napplications. We’ll also explore how we can monitor those applications and\nrespond automatically to changes in behavior.\n20\nAPPLICATION RESILIENCY\nOver the course of this book, we’ve seen how containers and Kubernetes\nenable scalable, resilient applications. Using containers, we can encapsulate\napplication components so that processes are isolated from one another, have\nseparate virtualized network stacks, and a separate filesystem. Each container\ncan then be rapidly deployed without interfering with other containers. When\nwe add Kubernetes as a container orchestration layer on top of the container\nruntime, we are able to include many separate hosts into a single cluster,\ndynamically scheduling containers across available cluster nodes with\nautomatic scaling and failover, distributed networking, traffic routing,\nstorage, and configuration.\nAll of the container and Kubernetes features we’ve seen in this book work\ntogether to provide the necessary infrastructure to deploy scalable, resilient\napplications, but it’s up to us to configure our applications correctly to take\nadvantage of what the infrastructure provides. In this chapter, we’ll take\nanother look at the \ntodo\n application we deployed in \nChapter 1\n. This \ntime,\nhowever, we’ll deploy it across multiple nodes in a Kubernetes cluster,\neliminating single points of failure and taking advantage of the key features\nthat Kubernetes has to offer. We’ll also explore how to monitor the\nperformance of our Kubernetes cluster and our deployed application so that\nwe can identify performance issues before they lead to downtime for our\nusers.\nExample Application Stack\nIn \nChapter 1\n, we deployed \ntodo\n onto a Kubernetes cluster running \nk3s\n from\nRancher. We already had some amount of scalability and failover available.\nThe web layer was based on a Deployment, so we were able to scale the\nnumber of server instances with a single command. Our Kubernetes cluster\nwas monitoring those instances so failed instances could be replaced.\nHowever, we still had some single points of failure. We had not yet\nintroduced the idea of a highly available Kubernetes control plane, so we\nchose to run \nk3s\n only in a single-node configuration. Additionally, even\nthough we used a Deployment for our PostgreSQL database, it was lacking in\nany of the necessary configuration for high availability. In this chapter, we’ll\nsee the details necessary to correct those limitations, and we’ll also take\nadvantage of the many other Kubernetes features we’ve learned.\nDatabase\nLet’s begin by deploying a highly available PostgreSQL database. \nChapter 17\ndemonstrated how the Kubernetes Operator design pattern uses\nCustomResourceDefinitions to extend the behavior of a cluster, making it\neasy to package and deploy advanced functionality. We’ll use the Postgres\nOperator we introduced in that chapter to deploy our database.\nNOTE\nThe example repository for this book is at\n \nhttps://github.com/book-of-\nkubernetes/examples\n. \nSee “Running Examples” on \npage xx\n for details on\ngetting set up. This chapter uses a larger six-node cluster to provide room\nfor the application and all the monitoring components that we’ll be\ndeploying. See the \nREADME.md\n file for this chapter for more\ninformation.\nThe automation for this chapter has already deployed the Postgres\nOperator together with its configuration. You can inspect the Postgres\nOperator and its configuration by looking at the files in\n/etc/kubernetes/components\n. The operator is running in the \ntodo\n Namespace,\nwhere the \ntodo\n application is also deployed. Many operators prefer to run in\ntheir own Namespace and operate across the cluster, but the Postgres\nOperator is designed to be deployed directly into the Namespace where the\ndatabase will reside.\nBecause we’re using the Postgres Operator, we can create a highly\navailable PostgreSQL database by applying a custom resource to the cluster:\ndatabase.yaml\n ---\n apiVersion: ""acid.zalan.do/v1""\n kind: postgresql\n \nmetadata:\n➊\n name: todo-db\n spec:\n   teamId: todo\n   volume:\n     size: 1Gi\n     storageClass: longhorn\n➋\n numberOfInstances: 3\n   users:\n  \n➌\n todo:\n     - superuser\n     - createdb\n   databases:\n  \n➍\n todo: todo\n   postgresql:\n     version: ""14""\nAll of the files shown in this walkthrough have been staged to the\n/etc/kubernetes/todo\n directory so that you can explore them and experiment\nwith changes. The \ntodo\n application is automatically deployed, but it can take\nseveral minutes for all the components to reach a healthy state.\nThe Postgres Operator has the job of creating the Secrets, StatefulSets,\nServices, and other core Kubernetes resources needed to deploy PostgreSQL.\nWe’re only required to supply the configuration it should use. We start by\nidentifying the name for this database, \ntodo-db\n \n➊\n, which will be used as the\nname of the primary Service that we’ll use to connect to the primary database\ninstance, so we’ll see this name again in the application configuration.\nWe want a highly available database, so let’s specify three instances \n➋\n.\nWe also ask the Postgres Operator to create a \ntodo\n user \n➌\n and to create a \ntodo\ndatabase with the \ntodo\n user as the owner \n➍\n. This way, our database is already\nset up and we only need to populate the tables to store the application data.\nWe can verify that the database is running in the cluster:\nroot@host01:~# \nkubectl -n todo get sts\nNAME      READY   AGE\ntodo-db   3/3     6m1s\nThe \ntodo-db\n StatefulSet has three Pods, all of which are ready.\nBecause the Postgres Operator is using a StatefulSet, as we saw in \nChapter\n15\n, a PersistentVolumeClaim is allocated for the database instances as they\nare created:\nroot@host01:~# \nkubectl -n todo get pvc\nNAME               STATUS   ... CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npgdata-todo-db-0   Bound    ... 1Gi        RWO            longhorn       10m\npgdata-todo-db-1   Bound    ... 1Gi        RWO            longhorn       8m44s\npgdata-todo-db-2   Bound    ... 1Gi        RWO            longhorn       7m23s\nThese PersistentVolumeClaims will be reused if one of the database\ninstance Pods fails and must be re-created, and the Longhorn storage engine\nis distributing its storage across our entire cluster, so the database will retain\nthe application data even if we have a node failure.\nNote that when we requested the Postgres Operator to create a \ntodo\n user,\nwe didn’t specify a password. For security, the Postgres Operator\nautomatically generates a password. This password is placed into a Secret\nbased on the name of the user and the name of the database. We can see the\nSecret created for the \ntodo\n user:\nroot@host01:~# \nkubectl -n todo get secret\nNAME                                                    TYPE    DATA   AGE\n...\ntodo.todo-db.credentials.postgresql.acid.zalan.do       Opaque  2      8m30s\nWe’ll need to use this information to configure the application so that it\ncan authenticate to the database.\nBefore we look at the application configuration, let’s inspect the Service\nthat the Postgres Operator created:\nroot@host01:~# \nkubectl -n todo get svc todo-db\nNAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\ntodo-db   ClusterIP   10.110.227.34   <none>        5432/TCP   59m\nThis is a \nClusterIP\n Service, meaning that it is reachable from anywhere\ninside the cluster but is not externally exposed. That matches perfectly with\nwhat we want for our application, as our web service component is the only\nuser-facing component and thus the only one that will be exposed outside the\ncluster.\nApplication Deployment\nAll of our application’s data is in the PostgreSQL database, so the web server\nlayer is stateless. For this stateless component, we’ll use a Deployment and\nset up automatic scaling.\nThe Deployment has a lot of information, so let’s look at it step by step.\nTo see the entire Deployment configuration and get a sense of how it all fits\ntogether, you can look at the file \n/etc/kubernetes/todo/application.yaml\n on\nany of the cluster nodes.\nThe first section tells Kubernetes that we’re creating a Deployment:\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: todo\n  labels:\n    app: todo\nThis part is simple because we’re only specifying the metadata for the\nDeployment. Note that we don’t include the \nnamespace\n in the metadata. Instead,\nwe provide it to Kubernetes directly when we apply this Deployment to the\ncluster. This way, we can reuse the same Deployment YAML for\ndevelopment, test, and production versions of this application, keeping each\nin a separate Namespace to avoid conflict.\nThe \nlabel\n field is purely informational, though it also provides a way for us\nto query the cluster for all of the resources associated with this application by\nmatching on the label.\nThe next part of the Deployment YAML specifies how the cluster should\nhandle updates:\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 30%\n      maxSurge: 50%\nThe \nreplicas\n field tells Kubernetes how many instances to create initially.\nThe autoscaling configuration will automatically adjust this.\nThe \nstrategy\n field allows us to configure this Deployment for updates\nwithout any application downtime. We can choose either \nRollingUpdate\n or\nRecreate\n as a strategy. With \nRecreate\n, when the Deployment changes, all of the\nexisting Pods are terminated, and then the new Pods are created. With\nRollingUpdate\n, new Pods are immediately created, and old Pods are kept running\nto ensure that this application component can continue functioning while it is\nupdated.\nWe can control how the rolling update operates using the \nmaxUnavailable\n and\nmaxSurge\n fields, which we can specify either as integer numbers or as a\npercentage of the current number of replicas. In this case, we specified 30\npercent for \nmaxUnavailable\n, so the Deployment will throttle the rolling update\nprocess to prevent us from falling below 70 percent of the current number of\nreplicas. Additionally, because we set \nmaxSurge\n at 50 percent, the Deployment\nwill immediately start new Pods until the number of Pods that are running or\nin the creation process reaches 150 percent of the current number of replicas.\nThe \nRollingUpdate\n strategy is the default, and by default, both \nmaxSurge\n and\nmaxUnavailable\n are 25 percent. Most Deployments should use the \nRollingUpdate\nstrategy unless it is absolutely necessary to use \nRecreate\n.\nThe next part of the Deployment YAML links the Deployment to its Pods:\n  selector:\n    matchLabels:\n      app: todo\n  template:\n    metadata:\n      labels:\n        app: todo\nThe \nselector\n and the \nlabels\n in the Pod \nmetadata\n must match. As we saw in\nChapter 7\n, the Deployment uses the \nselector\n to track its Pods.\nWith this part, we’ve now begun defining the \ntemplate\n for the Pods this\nDeployment creates. The rest of the Deployment YAML completes the Pod\ntemplate, which consists entirely of configuration for the single container this\nPod runs:\n    spec:\n      containers:\n      - name: todo\n        image: bookofkubernetes/todo:stable\nThe container name is mostly informational, though it is essential for Pods\nwith multiple containers so that we can choose a container when we need to\nretrieve logs and use \nexec\n to run commands. The \nimage\n tells Kubernetes what\ncontainer image to retrieve in order to run this container.\nThe next section of the Pod template specifies the environment variables\nfor this container:\n        env:\n        - name: NODE_ENV\n          value: production\n        - name: PREFIX\n          value: /\n        - name: PGHOST\n          value: todo-db\n        - name: PGDATABASE\n          value: todo\n        - name: PGUSER\n          valueFrom:\n            secretKeyRef:\n              name: todo.todo-db.credentials.postgresql.acid.zalan.do\n              key: username\n              optional: false\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: todo.todo-db.credentials.postgresql.acid.zalan.do\n              key: password\n              optional: false\nSome of the environment variables have static values; they’re expected to\nremain the same for all uses of this Deployment. The \nPGHOST\n environment\nvariable matches the name of the PostgreSQL database. The Postgres\nOperator has created a Service with the name \ntodo-db\n in the \ntodo\n Namespace\nwhere these Pods will run, so the Pods are able to resolve this hostname to\nthe Service IP address. Traffic destined for the Service IP address is then\nrouted to the primary PostgreSQL instance using the \niptables\n configuration we\nsaw in \nChapter 9\n.\nThe final two variables provide the credentials for the application to\nauthenticate to the database. We’re using the ability to fetch configuration\nfrom a Secret and provide it as an environment variable to a container, similar\nto what we saw in \nChapter 16\n. However, in this case, we need the\nenvironment variable to have a different name from the key name in the\nSecret, so we use a slightly different syntax that allows us to specify each\nvariable name separately.\nFinally, we declare the resource requirements of this container and the\nport it exposes:\n        resources:\n          requests:\n            memory: ""128Mi""\n            cpu: ""50m""\n          limits:\n            memory: ""128Mi""\n            cpu: ""50m""\n        ports:\n        - name: web\n          containerPort: 5000\nThe \nports\n field in a Pod is purely informational; the actual traffic routing\nwill be configured in the Service.\nWithin the \nresources\n field, we set the \nrequests\n and \nlimits\n to be the same for this\ncontainer. As we saw in \nChapter 19\n, this means that Pod will be placed in the\nGuaranteed\n Quality of Service class. The web service component is stateless and\neasy to scale, so it makes sense to use a relatively low CPU limit, in this case,\n50 millicores, or 5 percent of a core, and rely on the autoscaling to create new\ninstances if the load becomes high.\nPod Autoscaling\nTo automatically scale the Deployment to match the current load, we use a\nHorizontalPodAutoscaler, as we saw in \nChapter 7\n. Here’s the configuration\nfor the autoscaler:\nscaler.yaml\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: todo\n  labels:\n    app: todo\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: todo\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\nAs we did in our earlier example, we apply a label to this resource purely\nfor informational purposes. Three key configuration items are necessary for\nthis autoscaler. First, the \nscaleTargetRef\n specifies that we want to scale the \ntodo\nDeployment. Because this autoscaler is deployed to the \ntodo\n Namespace, it\nfinds the correct Deployment to scale.\nSecond, we specify a range for \nminReplicas\n and \nmaxReplicas\n. We choose \n3\n as\nthe minimum number of replicas, as we want to make sure the application is\nresilient even if we have a Pod failure. For simplicity, we didn’t apply the\nanti-affinity configuration we saw in \nChapter 18\n, but this may also be a good\npractice to avoid having all of the instances on a single node. We choose a\nmaximum number of replicas based on the size of our cluster; for a\nproduction application, we would measure our application load and choose\nbased on the highest load we expect to handle.\nThird, we need to specify the metric that the autoscaler will use to decide\nhow many replicas are needed. We base this autoscaler on CPU utilization. If\nthe average utilization across the Pods is greater than 50 percent of the Pod’s\nrequests\n, the Deployment will be scaled up. We set the \nrequests\n at 50 millicores,\nso this means that an average utilization greater than 25 millicores will cause\nthe autoscaler to increase the number of replicas.\nTo retrieve the average CPU utilization, the autoscaler relies on a cluster\ninfrastructure component that retrieves metrics data from the \nkubelet\n service\nrunning on each node and exposes that metrics data via an API. For this\nchapter, we have some extra cluster monitoring functionality to demonstrate,\nso the automation has skipped the regular metrics server component we\ndescribed in \nChapter 6\n. We’ll deploy an alternative later in this chapter.\nApplication Service\nThe final cluster resource for our application is the Service. \nListing 20-1\npresents the definition we’re using for this chapter.\nservice.yaml\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: todo\n  labels:\n    app: todo\nspec:\n  type: NodePort\n  selector:\n    app: todo\n  ports:\n  - name: web\n    protocol: TCP\n    port: 5000\n    nodePort: 5000\nListing 20-1: Todo Service\nWe use the same \nselector\n that we saw in the Deployment to find the Pods\nthat will receive traffic sent to this Service. As we saw in \nChapter 9\n, the \nports\nfield of a Service is essential because \niptables\n traffic routing rules are\nconfigured only for the ports we identify. In this case, we declare the \nport\n to\nbe 5000 and don’t declare a \ntargetPort\n, so this Service will send to port 5000 on\nthe Pods, which matches the port on which our web server is listening. We\nalso configure a \nname\n on this port, which will be important later when we\nconfigure monitoring.\nFor this chapter, we’re exposing our application Service using \nNodePort\n,\nwhich means that all of our cluster’s nodes will be configured to route traffic\nto the Service that is sent to the \nnodePort\n for any host interface. Thus, we can\naccess port 5000 on any of our cluster’s nodes and we’ll be routed to our\napplication:\nroot@host01:~# \ncurl -v http://host01:5000/\n...\n< HTTP/1.1 200 OK\n< X-Powered-By: Express\n...\n<html lang=""en"" data-framework=""backbonejs"">\n    <head>\n        <meta charset=""utf-8"">\n        <title>Todo-Backend client</title>\n        <link rel=""stylesheet"" href=""css/vendor/todomvc-common.css"">\n        <link rel=""stylesheet"" href=""css/chooser.css"">\n    </head>\n...\n</html>\nThis Service traffic routing works on any host interface, so the \ntodo\napplication can be accessed from outside the cluster as well. The URL is\ndifferent depending on whether you’re using the Vagrant or Amazon Web\nServices configuration, so the automation for this chapter includes a message\nwith the URL to use.\nNODEPORT, NOT INGRESS\nWhen we deployed \ntodo\n in \nChapter 1\n, we exposed the Service using an\nIngress. The Ingress, as we saw in \nChapter 9\n, consolidates multiple\nServices such that they can all be exposed outside the cluster without\nrequiring each Service to have a separate externally routable IP address.\nWe’ll expose a monitoring service later in this chapter, so we have\nmultiple Services to expose outside the cluster. However, because we’re\nworking with an example cluster on a private network, we don’t have\nthe underlying network infrastructure available to use an Ingress to its\nfull potential. By using a \nNodePort\n instead, we’re able to expose multiple\nServices outside the cluster in a way that works well with both the\nVagrant and Amazon Web Services configurations.\nWe’ve now looked all of the components in the \ntodo\n application, using\nwhat we’ve learned in this book to eliminate single points of failure and\nmaximize scalability.\nYou can also explore the source code for the \ntodo\n application at\nhttps://github.com/book-of-kubernetes/todo\n, including the \nDockerfile\n that’s\nused to build the application’s container image and the GitHub Actions that\nautomatically build it and publish it to Docker Hub whenever the code",21138
89-Application and Cluster Monitoring.pdf,89-Application and Cluster Monitoring,"changes.\nHowever, although our Kubernetes cluster will now do its best to keep this\napplication running and performing well, we can do more to monitor both the\ntodo\n application and the Kubernetes cluster.\nApplication and Cluster Monitoring\nProper application and cluster monitoring is essential for applications, for\nmultiple reasons. First, our Kubernetes cluster will try to keep the\napplications running, but any hardware or cluster failures could leave an\napplication in a non-working or degraded state. Without monitoring, we\nwould be dependent on our users to tell us when the application is down or\nbehaving badly, which is poor user experience. Second, if we do see failures\nor performance issues with our application, we’re going to need data to\ndiagnose them or to try to identify a pattern in order to find a root cause. It’s\na lot easier to build in monitoring ahead of time than to try to apply it after\nwe’re already seeing problems. Finally, we may have problems with our\ncluster or application that occurs below the level at which users notice, but\nthat indicates potential performance or stability issues. Integrating proper\nmonitoring allows us to detect those kinds of issues before they become a\nbigger headache. It also allows us to measure an application over time to\nmake sure that added features aren’t degrading its performance.\nFortunately, although we do need to think about monitoring at the level of\neach of our application components, we don’t need to build a monitoring\nframework ourselves. Many mature monitoring tools are already designed to\nwork in a Kubernetes cluster, so we can get up and running quickly. In \nthis\nchapter, we’ll look at \nkube-prometheus\n, a complete stack of tools that we can\ndeploy to our cluster and use to monitor both the cluster and the \ntodo\napplication.\nPrometheus Monitoring\nThe core component of \nkube-prometheus\n is, as the name implies, the open source\nPrometheus monitoring software. Prometheus deploys as a server that\nperiodically queries various metrics sources and accumulates the data it\nreceives. It supports a query language that is optimized for “time series” data,\nwhich makes it easy to collect individual data points showing a system’s\nperformance at a moment in time. It then aggregates those data points to get a\npicture of the system’s load, resource utilization, and responsiveness.\nFor each component that exposes metrics, Prometheus expects to reach\nout to a URL and receive data in return in a standard format. It’s common to\nuse the path \n/metrics\n to expose metrics to Prometheus. Following this\nconvention, the Kubernetes control plane components already expose metrics\nin the format that Prometheus is expecting.\nTo illustrate, we can use \ncurl\n to visit the \n/metrics\n path on the API server to\nsee the metrics that it provides. To do this, we’ll need to authenticate to the\nAPI server, so let’s use a script that collects a client certificate for\nauthentication:\napi-metrics.sh\n#!/bin/bash\nconf=/etc/kubernetes/admin.conf\n...\ncurl --cacert $ca --cert $cert --key $key https://192.168.61.10:6443/metrics\n...\nRunning this script returns a wealth of API server metrics:\nroot@host01:~# \n/opt/api-server-metrics.sh\n...\n# TYPE rest_client_requests_total counter\nrest_client_requests_total{code=""200"",host=""[::1]:6443"",method=""GET""} 9051\nrest_client_requests_total{code=""200"",host=""[::1]:6443"",method=""PATCH""} 25\nrest_client_requests_total{code=""200"",host=""[::1]:6443"",method=""PUT""} 21\nrest_client_requests_total{code=""201"",host=""[::1]:6443"",method=""POST""} 179\nrest_client_requests_total{code=""404"",host=""[::1]:6443"",method=""GET""} 155\nrest_client_requests_total{code=""404"",host=""[::1]:6443"",method=""PUT""} 1\nrest_client_requests_total{code=""409"",host=""[::1]:6443"",method=""POST""} 5\nrest_client_requests_total{code=""409"",host=""[::1]:6443"",method=""PUT""} 62\nrest_client_requests_total{code=""500"",host=""[::1]:6443"",method=""GET""} 18\nrest_client_requests_total{code=""500"",host=""[::1]:6443"",method=""PUT""} 1\n...\nThis example illustrates only a few of the hundreds of metrics that are\ncollected and exposed. Each line of this response provides one data point to\nPrometheus. We can include additional parameters for the metric in curly\nbraces, allowing for more complex queries. For example, the API server data\nin the preceding example can be used to determine not only the total number\nof client requests served by the API server but also the raw number and\npercentage of requests that resulted in an error. Most systems are resilient to a\nfew HTTP error responses, but a sudden increase in error responses is often a\ngood indication of a more serious issue, so this is valuable in configuring a\nreporting threshold.\nIn addition to all of the data that the Kubernetes cluster is already\nproviding to Prometheus, we can also configure our application to expose\nmetrics. Our application is based on Node.js, so we do this using the \nprom-client\nlibrary. As demonstrated in \nListing 20-2\n, our \ntodo\n application is exposing\nmetrics at \n/metrics\n, like the API server.\nroot@host01:~# \ncurl http://host01:5000/metrics/\n# HELP api_success Successful responses\n# TYPE api_success counter\napi_success{app=""todo""} 0\n# HELP api_failure Failed responses\n# TYPE api_failure counter\napi_failure{app=""todo""} 0\n...\n# HELP process_cpu_seconds_total Total user and system CPU time ...\n# TYPE process_cpu_seconds_total counter\nprocess_cpu_seconds_total{app=""todo""} 0.106392\n...\nListing 20-2: Todo metrics\nThe response includes some default metrics that are relevant to all\napplications. It also includes some counters that are specific to the \ntodo\napplication and track API usage and responses over time.\nDeploying kube-prometheus\nAt this point, our Kubernetes cluster and our application are ready to provide\nthese metrics on demand, but we don’t yet have a Prometheus server running\nin the cluster to collect them. To fix this, we’ll deploy the complete \nkube-\nprometheus\n stack. This includes not only a Prometheus Operator that makes it\neasy to deploy and configure Prometheus but also other useful tools, such as\nAlertmanager, which can trigger notifications in response to cluster and\napplication alerts, and Grafana, a dashboard tool that we’ll use to see the\nmetrics we’re collecting.\nTo deploy \nkube-prometheus\n, we’ll use a script that’s been installed in \n/opt\n.\nThis script downloads a current \nkube-prometheus\n release from GitHub and\napplies the manifests.\nRun the script as follows:\nroot@host01:~# \n/opt/install-kube-prometheus.sh\n...\nThese manifests also include a Prometheus Adapter. The Prometheus\nAdapter implements the same Kubernetes metrics API as the \nmetrics-server\n we\ndeployed to the clusters throughout \nPart II\n, so it exposes CPU and memory\ndata obtained from \nkubelet\n, enabling our HorizontalPodAutoscaler to track\nCPU utilization of our \ntodo\n application. However, it also exposes that\nutilization data to Prometheus so that we can observe it in Grafana\ndashboards. For this reason, we use the Prometheus Adapter in this chapter in\nplace of the regular \nmetrics-server\n.\nWe can see the Prometheus Adapter and the other components by listing\nPods in the \nmonitoring\n Namespace:\nroot@host01:~# \nkubectl -n monitoring get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\nalertmanager-main-0                    2/2     Running   0          14m\nalertmanager-main-1                    2/2     Running   0          14m\nalertmanager-main-2                    2/2     Running   0          14m\nblackbox-exporter-6b79c4588b-pgp5r     3/3     Running   0          15m\ngrafana-7fd69887fb-swjpl               1/1     Running   0          15m\nkube-state-metrics-55f67795cd-mkxqv    3/3     Running   0          15m\nnode-exporter-4bhhp                    2/2     Running   0          15m\nnode-exporter-8mc5l                    2/2     Running   0          15m\nnode-exporter-ncfd2                    2/2     Running   0          15m\nnode-exporter-qp7mg                    2/2     Running   0          15m\nnode-exporter-rtn2t                    2/2     Running   0          15m\nnode-exporter-tpg97                    2/2     Running   0          15m\nprometheus-adapter-85664b6b74-mglp4    1/1     Running   0          15m\nprometheus-adapter-85664b6b74-nj7hp    1/1     Running   0          15m\nprometheus-k8s-0                       2/2     Running   0          14m\nprometheus-k8s-1                       2/2     Running   0          14m\nprometheus-operator-6dc9f66cb7-jtrqd   2/2     Running   0          15m\nIn addition to the Prometheus Adapter, we see Pods for Alertmanager,\nGrafana, and various \nexporter\n Pods, which collect metrics from the cluster\ninfrastructure and expose it to Prometheus. We also see Pods for Prometheus\nitself and for the Prometheus Operator. The Prometheus Operator\nautomatically updates Prometheus whenever we change the custom resources\nthat the Prometheus Operator is monitoring. The most important of those\ncustom resources is the Prometheus resource shown in \nListing 20-3\n.\nroot@host01:~# \nkubectl -n monitoring describe prometheus\nName:         k8s\nNamespace:    monitoring\n...\nAPI Version:  monitoring.coreos.com/v1\nKind:         Prometheus\n...\nSpec:\n...\n  Image:  quay.io/prometheus/prometheus:v2.32.1\n...\n  Service Account Name:  prometheus-k8s\n  Service Monitor Namespace Selector:\n  Service Monitor Selector:\n...\nListing 20-3: Prometheus configuration\nThe Prometheus custom resource allows us to configure which\nNamespaces will be watched for Services to monitor. The default\nconfiguration presented in \nListing 20-3\n does not specify a value for the\nService Monitor Namespace Selector or the Service Monitor Selector. For\nthis reason, by default the Prometheus Operator will be looking for\nmonitoring configuration in all Namespaces, with any metadata label.\nTo identify specific Services to monitor, the Prometheus Operator keeps\nan eye out for another custom resource, \nServiceMonitor\n, as demonstrated in\nListing 20-4\n.\nroot@host01:~# \nkubectl -n monitoring get servicemonitor\nNAME                      AGE\nalertmanager-main         20m\nblackbox-exporter         20m\ncoredns                   20m\ngrafana                   20m\nkube-apiserver            20m\nkube-controller-manager   20m\nkube-scheduler            20m\nkube-state-metrics        20m\nkubelet                   20m\nnode-exporter             20m\nprometheus-adapter        20m\nprometheus-k8s            20m\nprometheus-operator       20m\nListing 20-4: Default ServiceMonitors\nWhen we installed \nkube-prometheus\n, it configured multiple ServiceMonitor\nresources. As a result, our Prometheus instance is already watching the\nKubernetes control plane components and the \nkubelet\n services running on our\ncluster nodes. Let’s see the targets from which Prometheus is scraping\nmetrics and see how those metrics are used to populate dashboards in\nGrafana.\nCluster Metrics\nThe installation script patched the Grafana and Prometheus Services in the\nmonitoring\n Namespace to expose them as \nNodePort\n Services. The automation\nscripts print the URL you can use to access Prometheus. The initial page\nlooks like \nFigure 20-1\n.\nFigure 20-1: Prometheus initial page\nClick the \nTargets\n item underneath the \nStatus\n menu on the top menu bar\nto see which components in the cluster Prometheus is currently scraping.\nClick \nCollapse All\n to get a consolidated list, as shown in \nFigure 20-2\n.\nFigure 20-2: Prometheus targets\nThis list matches the list of ServiceMonitors we saw in \nListing 20-4\n,\nshowing us that Prometheus is scraping Services as configured by the\nPrometheus Operator.\nWe can use the Prometheus web interface to query data directly, but\nGrafana has already been configured with some useful dashboards, so we \ncan\nmore easily see the data there. The automation scripts print the URL you can\nuse to access Grafana. Log in using the default \nadmin\n as the username and\nadmin\n as the password. You will be prompted to change the password; you can\njust click \nSkip\n. At this point you should see the Grafana initial page, as shown\nin \nFigure 20-3\n.\nFigure 20-3: Grafana initial page\nFrom this page, choose the \nBrowse\n item under \nDashboards\n in the menu.\nThere are many dashboards in the \nDefault\n folder. For example, by selecting\nDefault\n and then selecting \nKubernetes\n \n▸\n \nCompute Resources\n \n▸\n \nPod\n, you\ncan see a dashboard, depicted in \nFigure 20-4\n, that shows CPU and memory\nusage over time for any Pod in the cluster.\nFigure 20-4: Pod compute resources\nAll of the \ntodo\n database and application Pods are selectable in this\ndashboard by first selecting the \ntodo\n Namespace, so we can already get\nvaluable information about our application by using nothing more than the\ndefault \nmonitoring configuration. This is possible because the Prometheus\nAdapter is pulling data from the \nkubelet\n services, which includes resource\nutilization for each of the running Pods. The Prometheus Adapter is then\nexposing a \n/metrics\n endpoint for Prometheus to scrape and store, and Grafana\nis querying Prometheus to build the chart showing usage over time.\nThere are numerous other Grafana dashboards to explore in the default\ninstallation of \nkube-prometheus\n. Choose the \nBrowse\n menu item again to select\nother dashboards and see what data is available.\nAdding Monitoring for Services\nAlthough we are already getting useful metrics for our \ntodo\n application,\nPrometheus is not yet scraping our application Pods to pull in the Node.js\nmetrics we saw in \nListing 20-2\n. To configure Prometheus to scrape our \ntodo\nmetrics, we’ll need to provide a new ServiceMonitor resource to the\nPrometheus Operator, informing it about our \ntodo\n Service.\nIn a production cluster, the team deploying an application like our \ntodo\napplication wouldn’t have the permissions to create or update resources in the\nmonitoring\n Namespace. However, the Prometheus Operator looks for\nServiceMonitor resources in all Namespaces by default, so we can create a\nServiceMonitor in the \ntodo\n Namespace instead.\nFirst, though, we need to give Prometheus permission to see the Pods and\nServices we’ve created in the \ntodo\n Namespace. As this access control\nconfiguration needs to apply only in a single Namespace, we’ll do this by\ncreating a Role and a RoleBinding. Here’s the Role configuration we’ll use:\nrbac.yaml\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n...\n  name: prometheus-k8s\nrules:\n  - apiGroups:\n    - """"\n    resources:\n    - services\n    - endpoints\n    - pods\n    verbs:\n    - get\n    - list\n    - watch\n...\nWe need to make sure we allow access to Services, Pods, and Endpoints,\nso we confirm that these are listed in the \nresources\n field. The Endpoint resource\nrecords the current Pods that are receiving traffic for a Service, which \nwill be\ncritical for Prometheus to identify all of the Pods it scrapes. Because\nPrometheus needs only read-only access, we specify only the \nget\n, \nlist\n, and \nwatch\nverbs.\nAfter we have this Role, we need to bind it to the ServiceAccount that\nPrometheus is using. We do that with this RoleBinding:\nrbac.yaml\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n...\n  name: prometheus-k8s\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: prometheus-k8s\nsubjects:\n  - kind: ServiceAccount\n    name: prometheus-k8s\n    namespace: monitoring\nThe \nroleRef\n matches the Role we just declared in the preceding example,\nwhereas the \nsubjects\n field lists the ServiceAccount Prometheus is using, based\non the information we saw in \nListing 20-3\n.\nBoth of these YAML resources are in the same file, so we can apply them\nboth to the cluster at once. We need to make sure we apply them to the \ntodo\nNamespace, as that’s the Namespace where we want to enable access by\nPrometheus:\nroot@host01:~# \nkubectl -n todo apply -f /opt/rbac.yaml\nrole.rbac.authorization.k8s.io/prometheus-k8s created\nrolebinding.rbac.authorization.k8s.io/prometheus-k8s created\nNow that we’ve granted permission to Prometheus to see our Pods and\nServices, we can create the ServiceMonitor. Here’s that definition:\nsvc-mon.yaml\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: todo\nspec:\n  selector:\n    matchLabels:\n      app: todo\n  endpoints:\n    - port: web\nA ServiceMonitor uses a selector, similar to a Service or a Deployment.\nWe previously applied the \napp: todo\n label to the Service, so the \nmatchLabels\n field\nwill cause Prometheus to pick up the Service. The \nendpoints\n field matches the\nname of the port we declared in the Service in \nListing 20-1\n. Prometheus\nrequires us to name the port in order to match it.\nLet’s apply this ServiceMonitor to the cluster:\nroot@host01:~# \nkubectl -n todo apply -f /opt/svc-mon.yaml\nservicemonitor.monitoring.coreos.com/todo created\nAs before, we need to make sure we deploy this to the \ntodo\n Namespace\nbecause Prometheus will be configured to look for Services with the\nappropriate label in the same Namespace as the ServiceMonitor.\nBecause the Prometheus Operator is watching for new ServiceMonitor\nresources, using the API we saw in \nChapter 17\n, it picks up this new resource\nand immediately reconfigures Prometheus to start scraping the Service.\nPrometheus then takes a few minutes to register the new targets and start\nscraping them. If we go back to the Prometheus Targets page after this is\ncomplete, the new Service shows up, as illustrated in \nFigure 20-5\n.\nFigure 20-5: Prometheus monitoring todo\nIf we click the \nshow more\n button next to the \ntodo\n Service, we see its three\nEndpoints, shown in \nFigure 20-6\n.\nFigure 20-6: Todo Endpoints\nIt may be surprising that we created a ServiceMonitor, specifying the \ntodo\nService as the target, and yet Prometheus is scraping Pods. However, it’s\nessential that Prometheus works this way. Because Prometheus is using a\nregular HTTP request to scrape metrics, and because Service traffic routing\nchooses a random Pod for every new connection, Prometheus would get\nmetrics from a random Pod each time it did scraping. By reaching behind the\nService to identify the Endpoints, Prometheus is able to scrape metrics from\nall the Service’s Pods, enabling aggregation of metrics for the entire\napplication.\nWe’ve successfully incorporated the Node.js and custom metrics for the\ntodo\n application into Prometheus, in addition to the default resource utilization\nmetrics already collected. Before we finish our look at application\nmonitoring, let’s run a Prometheus query to demonstrate that the data is being\npulled in. First, you should interact with the \ntodo\n application using the URL\nprinted out by the automation scripts. This will ensure that there are metrics\nto display and that enough time has passed for Prometheus to scrape that\ndata. Next, open the Prometheus web interface again, or click \nPrometheus\n at\nthe top of any Prometheus web page to go back to the main page. Then, type",19392
90-Final Thoughts.pdf,90-Final Thoughts,"api_success\n into the query box and press ENTER. Custom \ntodo\n metrics\nshould appear, as illustrated in \nFigure 20-7\n.\nFigure 20-7: Todo metric query\nWe’re now able to monitor both the Kubernetes cluster and the \ntodo\napplication.\nFinal Thoughts\nIn this chapter, we’ve explored how the various features of containers and\nKubernetes come together to enable us to deploy a scalable, resilient\napplication. We’ve used everything we learned about containers—\nDeployments, Services, networking, persistent storage, Kubernetes\nOperators, and role-based access control—to not only deploy the \ntodo\napplication but also configure Prometheus monitoring of our cluster and our\napplication.\nKubernetes is a complex platform with many different capabilities, and\nnew capabilities are being added all the time. The purpose of this book is not\nonly to show you the most important features you need to run an application\non Kubernetes, but also to give you the tools to explore a Kubernetes cluster\nfor troubleshooting and performance monitoring. As a result, you should be\nequipped to explore new features as they are added to Kubernetes and to\nconquer the challenges of deploying complex applications and getting them\nto perform well.",1248
91-INDEX.pdf,91-INDEX,"INDEX\nA\nabstraction, \nxix\n, \n4\n, \n51\naccess control, \n8\n, \n107\n, \n114\n, \n174\n, \n188\n, \n195\n, \n201\n, \n233\n, \n268–270\n, \n272\n, \n295\n, \n339\n,\n343\naccess mode, \n262\nReadWriteMany\n, \n262\nReadWriteOnce\n, \n262\nAddress Resolution Protocol, \n137\n, \n140\n, \n141\nproxy ARP, \n137\naffinity, \n296\n, \n297\n, \n299\n, \n300\n, \n302\n, \n303\n, \n306\nair-gapped systems, \n20\nAlertmanager, \n334\n, \n335\nAlpine Linux, \n12\n, \n72\n, \n74\n, \n155–157\n, \n179\n, \n180\n, \n183\n, \n198\n, \n199\nAmazon Web Services, \nxxi\n, \n16\n, \n91\n, \n96\n, \n139\n, \n250\n, \n332\nAnsible, \nxx\n, \nxxi\nanti-affinity, \n297–300\n, \n302\n, \n303\n, \n317\n, \n330\nAPI server. \nSee\n Kubernetes: API server\nAPI, Kubernetes. \nSee\n Kubernetes: REST API\napk\n, \n157\n, \n183\napplication server, \n5\napt\n, \n27\n, \n92–94\narchitecture, \n1\n, \n3–7\n, \n17\n, \n87–89\n, \n124\n, \n144\n, \n153\n, \n188\n, \n218\n, \n228\n, \n249\n, \n296\n, \n302\n,\n303\n, \n309\n, \n321\nArgoCD, \n267\nARP. \nSee\n Address Resolution Protocol\nauthentication, \n99\n, \n105\n, \n130\n, \n188\n, \n190–192\n, \n195\n, \n200\n, \n203\n, \n207\n, \n210\n, \n269\n,\n275\n, \n278\n, \n285\n, \n326\n, \n329\n, \n333\nauthorization, \n188\n, \n190\n, \n191\n, \n193\n, \n203\n, \n295\navailability zones, \n139\n, \n297\n, \n298\n, \n302\nAWS. \nSee\n Amazon Web Services\nB\nbackground processing, \n312\nbase64 encoding, \n277\nbind-tools\n, \n157\nbreakpoints, \n184\nbr_netfilter\n, \n91\nBrooks, Fred, \n5\nBuildah, \n26\nbursty, \n309\nBusyBox, \n24\n, \n28\n, \n29\n, \n35\n, \n53–58\n, \n60\n, \n61\n, \n63–66\n, \n76\n, \n82\n, \n83\n, \n133\n, \n155\n, \n156\nC\nC, \n179\n, \n180\ncaching, \n295\nCalico, \n100–102\n, \n106–109\n, \n127\n, \n128\n, \n130\n, \n131\n, \n133–135\n, \n137\n, \n139–141\n, \n144\n,\n146\n, \n147\n, \n149\n, \n194\n, \n211\n, \n213\n, \n280\n, \n281\n, \n288\nInstallation, \n281\ncat\n, \n207\ncgroups, \n313\n. \nSee also\n Linux: cgroups\nchange tracking, \n224\n, \n267\ncharacter special file, \n80\nchroot, \n22\n, \n70\ncloud native, \n4\n, \n6\n, \n7\n, \n17\n, \n88\n, \n89\n, \n298\n, \n316\nCloud Native Computing Foundation, \n91\nClusterRole, \n195–197\n, \n201\n, \n286\n, \n287\nClusterRoleBinding, \n197\n, \n202\nCNI. \nSee\n Container Network Interface\ncohesion, \n4\nco-location, \n296\n, \n297\nConfigMap, \n265\n, \n267–274\n, \n276\n, \n278\nconfiguration, \n8\n, \n264–267\n, \n274\n, \n279\n, \n280\n, \n285\n, \n323\n, \n324\n, \n330\ndrift, \n267\nexternalized, \n267\n, \n269\n, \n272\n, \n278\n, \n329\nfiles, \n272\n, \n273\n, \n278\n, \n281\nrepository, \n274–276\ncontainerd\n, \n23\n, \n25–27\n, \n29\n, \n32\n, \n91\n, \n107\n, \n206\n, \n208\ncontainer engine, \n23\n, \n83\n, \n203\ncontainer image, \n8\n, \n9\n, \n13\n, \n23\n, \n24\n, \n35\n, \n52\n, \n69–72\n, \n74\n, \n75\n, \n77\n, \n81–83\n, \n98\n, \n101\n,\n112–114\n, \n116\n, \n155\n, \n173\n, \n180\n, \n181\n, \n185\n, \n208\n, \n260\n, \n272\n, \n278\n, \n285\n, \n328\nContainer Network Interface, \n28\n, \n100\n, \n129\n, \n130\n, \n145\n, \n147\n, \n148\n, \n239\ncontainer orchestration, \n3\n, \n9\n, \n14\n, \n17\n, \n37\n, \n51\n, \n84\n, \n87\n, \n88\n, \n188\n, \n323\ncontainer platform, \n9\n, \n23\ncontainer runtime, \n9\n, \n19\n, \n23\n, \n24\n, \n27\n, \n30\n, \n32\n, \n173\n, \n174\n, \n176\n, \n178\n, \n205\n, \n206\n,\n208\n, \n212\n, \n213\n, \n218\n, \n233\n, \n235\n, \n237\n, \n238\n, \n248\n, \n265\n, \n270\n, \n278\n, \n323\nContainer Runtime Interface, \n27\n, \n113\n, \n208\ncontainers, \n3\n, \n7\n, \n8\n, \n12\n, \n17\n, \n22\n, \n32\n, \n107\n, \n108\n, \n111\n, \n128\n, \n167\n, \n219\n, \n233\n, \n249\n,\n279\n, \n292\n, \n310\n, \n323\n, \n343\nenvironment variables, \n12\n, \n70\n, \n74\n, \n82\n, \n178\n, \n179\n, \n183\n, \n253\n, \n264–266\n, \n268\n,\n269\n, \n272\n, \n273\n, \n278\n, \n328\n, \n329\nexit code, \n122\n, \n178\n, \n181\n, \n221\nfilesystems, \n11\n, \n21\n, \n22\n, \n32\n, \n53\n, \n67\n, \n70–72\n, \n74\n, \n78\n, \n80\n, \n81\n, \n83\n, \n84\n, \n112\n, \n323\nimage building, \n75\n, \n180\nimage layers, \n73–78\n, \n81\nlimits, \n33\n, \n49\n, \n122\n, \n170\n, \n214\n, \n232–235\n, \n238\n, \n244–246\n, \n248\n, \n310–317\n, \n319\n,\n329\nlogs, \n179\n, \n182\nnetworking, \n11\n, \n13\n, \n22\n, \n26\n, \n28\n, \n32\n, \n49\n, \n51\n, \n67\n, \n112\n, \n128\n, \n129\n, \n131\n, \n134\n,\n138\n, \n160\n, \n343\nnon-root user, \nxxi\noverhead, \n8\n, \n14\n, \n46\n, \n67\n, \n77\n, \n78\n, \n84\n, \n209\npackaging, \n8\n, \n17\n, \n23\n, \n69\n, \n84\nport forwarding, \n13\n, \n76\n, \n103\n, \n131\n, \n134\n, \n183\n, \n184\nprivate image, \n174\n, \n175\nregistry, \n70\n, \n76\n, \n77\n, \n174\n, \n175\nstorage, \n67\n, \n72\n, \n81\nversioning, \n8\n, \n10\n, \n72\n, \n74–76\nvolume mount, \n12\n, \n26\n, \n30\n, \n112\n, \n125\n, \n251–253\n, \n260\n, \n272–274\n, \n303\nContainer Storage Interface, \n102\n, \n108\n, \n250\n, \n251\n, \n258\nControl groups. \nSee\n Linux: cgroups\ncopy on write, \n78\nCoreDNS, \n156\ncoupling, \n4\n, \n296\n, \n306\nCPU instruction sets, \n168\nCRD. \nSee\n CustomResourceDefinition\nCRI (Container Runtime Interface), \n27\n, \n113\n, \n208\ncrictl\n, \n27\n, \n28\n, \n35\n, \n36\n, \n41\n, \n45\n, \n52\n, \n55\n, \n56\n, \n91\n, \n105\n, \n107\n, \n113\n, \n114\n, \n126\n, \n132\n, \n208\n,\n217\n, \n237\n, \n251\n, \n252\nCRI-O, \n26–29\n, \n32\n, \n36\n, \n39\n, \n41\n, \n43\n, \n45\n, \n52\n, \n55\n, \n57\n, \n58\n, \n60\n, \n62\n, \n63\n, \n65\n, \n66\n, \n206\n,\n208\ncron\n, \n124\nCronJob, \n122–124\n, \n243\n, \n264\ncross-cutting concerns, \n87–89\n, \n106\nCSI. \nSee\n Container Storage Interface\ncurl\n, \n76\n, \n93\n, \n101\n, \n103\n, \n109\n, \n154\n, \n165\n, \n189\n, \n190\n, \n199\n, \n221–223\n, \n231\n, \n232\n, \n260\n,\n261\n, \n284\n, \n303\n, \n304\n, \n333\ncustomization, \n89\n, \n279\n, \n280\n, \n285\n, \n288\n, \n292\nCustomResourceDefinition, \n145\n, \n280–284\n, \n286\n, \n288\n, \n289\n, \n291\n, \n292\n, \n324\nPostgresql, \n288–290\nSample, \n282\n, \n283\n, \n286–288\nD\nDaemonSet, \n101\n, \n108\n, \n122\n, \n127\n, \n128\n, \n131\n, \n145\n, \n146\n, \n158\n, \n211\n, \n213\n, \n250\n, \n298\ndata centers, \n297\ndd\n, \n72\nDebian, \n70\n, \n71\ndeclarative configuration, \n14\n, \n89\n, \n90\n, \n111\n, \n119\n, \n280\ndenial-of-service, \n234\nDeployment, \n15\n, \n115–122\n, \n124\n, \n125\n, \n127\n, \n152–154\n, \n156\n, \n163\n, \n172\n, \n179\n, \n181–\n183\n, \n188\n, \n194\n, \n196\n, \n198\n, \n200–202\n, \n207\n, \n212–214\n, \n216\n, \n219–222\n,\n224–229\n, \n234–236\n, \n238\n, \n240\n, \n243\n, \n244\n, \n248\n, \n249\n, \n252\n, \n259–264\n,\n266–269\n, \n271–274\n, \n278\n, \n280\n, \n284–289\n, \n292\n, \n297\n, \n298\n, \n316\n, \n318–320\n,\n324\n, \n326–328\n, \n330\n, \n341\n, \n343\nautoscaling, \n326\n, \n327\n, \n329\n, \n330\n. \nSee also\n HorizontalPodAutoscaler;\nKubernetes: autoscaling\nreplicas, \n116–121\n, \n247\n, \n327\n, \n330\nscaling, \n16\n, \n119\n, \n122\n, \n212\n, \n215\n, \n247\n, \n261\n, \n263\n, \n316\n, \n318\n, \n324\ndevice plug-in, \n303\n, \n306\ndig\n, \n157\n, \n158\ndiscovery, \n8\n, \n152\ndistributed application, \n6\n, \n8\n, \n88\n, \n218\n, \n296\n, \n301\n, \n309\ndistributed storage, \n249\n, \n250\n, \n296\n, \n326\nDNS. \nSee\n Domain Name System\nDocker, \n9\n, \n10\n, \n16\n, \n23\n, \n24\n, \n70\n, \n72\n, \n73\n, \n75\n, \n77\n, \n82\n, \n208\ndocker build\n, \n75\ndocker run\n, \n10\n, \n12\nDockerfile\n, \n74\n, \n75\n, \n81\n, \n180\nDocker Hub, \n77\n, \n174\n, \n175\n, \n180\nDocker multistage build, \n180\nDomain Name System, \n125\n, \n138\n, \n155–157\n, \n161\n, \n164\n, \n165\n, \n209\n, \n210\n, \n255\ndowntime, \n211\n, \n324\n, \n327\nE\nencryption, \n270\n, \n278\nEndpoint, \n17\n, \n155\n, \n230\n, \n231\n, \n284\n, \n299\n, \n339\n, \n341\n, \n342\nenvironment variables, \n12\n, \n70\n, \n74\n, \n82\n, \n178\n, \n179\n, \n183\n, \n253\n, \n264–266\n, \n268\n, \n269\n,\n272\n, \n273\n, \n275\n, \n278\n, \n328\n, \n329\netcd\n, \n211\n, \n274–276\n, \n278\n, \n280\netcdctl\n, \n275\n, \n276\nextended resource, \n303–305\nextensibility, \n280\n, \n292\n, \n324\nF\nfailover, \n7\n, \n111\n, \n112\n, \n115\n, \n116\n, \n124\n, \n149\n, \n151\n, \n153\n, \n160\n, \n166\n, \n185\n, \n249\n, \n256\n,\n264\n, \n291\n, \n296\n, \n323\n, \n324\n, \n326\n, \n330\n, \n332\nFlannel, \n139\nflexibility, \n296\nFluxCD, \n267\nG\ngcc\n, \n180\ngdb\n, \n183–185\ngdbserver\n, \n184\nGit, \n267\nGitHub, \n334\nGitHub Actions, \n180\nGitOps, \n267\nGoogle Container Engine, \n250\nGPG, \n92\ngraceful degradation, \n1\n, \n321\n, \n324\n, \n332\nGrafana, \n334–337\n, \n339\nGraphics Processing Unit (GPU), \n234\n, \n243\n, \n303\ngrep\n, \n159\nH\nhead\n, \n277\nhigh availability, \n15\n, \n91\n, \n96\n, \n98\n, \n105\n, \n109\n, \n112\n, \n164–166\n, \n192\n, \n216\n, \n228\n, \n256\n,\n275\n, \n276\n, \n279\n, \n288–291\n, \n296\n, \n298\n, \n324\n, \n325\nHorizontalPodAutoscaler, \n120\n, \n121\n, \n280\n, \n316\n, \n329\n, \n335\nhorizontal scaling, \n309\n, \n317\nHTTP Bearer authentication, \n193\n, \n199\n, \n207\nHTTP header, \n164–166\nHTTP path, \n164\nHTTP return code, \n221\n, \n224\n, \n232\n, \n334\nHTTP reverse proxy, \n163\n, \n166\n, \n190\nI\nICMP, \n138\n, \n141–143\n, \n158\n, \n160\nidempotence, \n90\n, \n124\n, \n224\n, \n288\nidentity management, \n191\nIngress, \n103\n, \n149\n, \n151\n, \n152\n, \n162–166\n, \n332\ningress controller, \n100\n, \n103\n, \n160\n, \n161\n, \n163–165\n, \n267\ninterdependency, \n5\niperf3\n, \n47\n, \n48\n, \n239–241\n, \n296\n, \n297\n, \n299–302\niptables\n, \n65\n, \n66\n, \n92\n, \n158–160\n, \n216\n, \n230\n, \n301\n, \n329\n, \n331\niSCSI, \n102\nisolation, \n8\n, \n10\n, \n17\n, \n19–23\n, \n27\n, \n31\n, \n32\n, \n48\n, \n52\n, \n56\n, \n71\n, \n84\n, \n173\n, \n323\nJ\nJava Enterprise, \n5\nJob, \n122–125\n, \n177\n, \n264\njq\n, \n30\n, \n55\n, \n56\n, \n72\n, \n80\n, \n82\n, \n83\n, \n100\n, \n102\n, \n132\n, \n136\n, \n181\n, \n182\n, \n237\n, \n260\n, \n276\n,\n277\n, \n304\nJSONPath, \n260\nK\nK3s, \n14\nk3s\n, \n324\nkubeadm\n, \n90\n, \n92–94\n, \n96–100\n, \n131\n, \n188\n, \n189\n, \n191\n, \n193\n, \n206\n, \n208–211\n, \n250\nkubectl\n, \n14\n, \n93\n, \n98\n, \n99\n, \n101\n, \n103\n, \n104\n, \n106\n, \n108\n, \n113\n, \n114\n, \n117\n, \n126\n, \n183\n, \n184\n,\n188\n, \n189\n, \n191\n, \n203\n, \n207\n, \n260\n, \n263\n, \n276\n, \n278\n, \n282\nannotate\n, \n104\napply\n, \n101\n, \n103\n, \n113\n, \n119\n, \n162\n, \n176\n, \n177\n, \n195\n, \n224\n, \n283\n, \n290\ncertificate\n, \n105\ncp\n, \n261\ndelete\n, \n119\n, \n126\n, \n213\ndescribe\n, \n99\n, \n107\n, \n114\n, \n117\n, \n169\n, \n171\n, \n173\n, \n175\n, \n176\n, \n178\n, \n181\n, \n221\n, \n236\n,\n262\n, \n283\ndrain\n, \n212\nedit\n, \n182\nexec\n, \n126\n, \n136\n, \n182\n, \n183\n, \n198\n, \n255\n, \n269\n, \n271\n, \n274\n, \n328\nget\n, \n98\n, \n108\n, \n113\n, \n118\n, \n127\n, \n136\n, \n169\n, \n173\n, \n176\n, \n177\n, \n181\n, \n183\n, \n280\nlabel\n, \n169\n, \n170\nlogs\n, \n115\n, \n178\n, \n182\n, \n224\n, \n301\npatch\n, \n162\n, \n301\n, \n303\nport-forward\n, \n184\nrun\n, \n108\n, \n255\nscale\n, \n119\n, \n216\nset\n, \n176\ntop\n, \n171\n, \n214\n, \n236\nuncordon\n, \n214\nkube-dns\n, \n156\nkubelet\n, \n89\n, \n93–96\n, \n98\n, \n100\n, \n104\n, \n105\n, \n113\n, \n114\n, \n121\n, \n130\n, \n131\n, \n133\n, \n144\n, \n146\n,\n173\n, \n176\n, \n188\n, \n191\n, \n192\n, \n203\n, \n205–218\n, \n224–226\n, \n228\n, \n239\n, \n242\n, \n251\n,\n252\n, \n274\n, \n303\n, \n306\n, \n330\n, \n335\n, \n336\n, \n339\nkube-prometheus\n, \n333\n, \n334\n, \n336\n, \n339\nkube-proxy\n, \n158–160\n, \n162\n, \n163\n, \n284\n, \n301\nkube-vip\n, \n91\n, \n96\n, \n97\nKUBECONFIG\n, \n99\n, \n191\n, \n202\n, \n245\nKubernetes, \nxix\n, \nxxi\n, \n4\n, \n9\n, \n14\n, \n16\n, \n17\n, \n27\n, \n37\n, \n43\n, \n44\n, \n49\n, \n51\n, \n52\n, \n67\n, \n84\n, \n85\n,\n87\n, \n101\n, \n105\n, \n111\n, \n128\n, \n134\n, \n135\n, \n152\n, \n156\n, \n157\n, \n167\n, \n176\n, \n177\n, \n179\n,\n185\n, \n187\n, \n194\n, \n199\n, \n205\n, \n214\n, \n218\n, \n219\n, \n233\n, \n250\n, \n264\n, \n265\n, \n269\n, \n280\n,\n292\n, \n296\n, \n302\n, \n303\n, \n309\n, \n310\n, \n315\n, \n323\n, \n334\n, \n343\nannotations, \n104\n, \n242\nAPI server, \n14\n, \n89\n, \n96\n, \n97\n, \n99\n, \n101\n, \n105–107\n, \n118\n, \n126\n, \n184\n, \n187–196\n,\n198–200\n, \n203\n, \n207\n, \n209\n, \n210\n, \n275\n, \n280\n, \n281\n, \n284\n, \n285\n, \n287–289\n, \n292\n,\n304\n, \n333\n, \n334\nautoscaling, \n14\n, \n104\n, \n111\n, \n117\n, \n120–122\n, \n317\nback-off algorithm, \n174–177\nbackward compatibility, \n121\nbootstrap, \n95\n, \n96\n, \n105\n, \n190\n, \n193\n, \n194\n, \n199\n, \n207\ncertificates, \n95–97\n, \n99\n, \n104\n, \n105\n, \n189–193\n, \n195\n, \n199\n, \n202\n, \n207\n, \n213\n, \n275\n,\n333\nclient API, \n284\ncustom object, \n285\nevents, \n284–288\nstream, \n285\nwatch object, \n285\ncloud controller, \n89\n, \n161\n, \n188\ncluster state, \n88\n, \n89\n, \n279\n, \n280\nconformance, \n91\ncontroller manager, \n96\n, \n104\n, \n105\n, \n187\n, \n193\n, \n195\n, \n212\n, \n284\ncontrol plane, \n89–91\n, \n96–98\n, \n100\n, \n106–109\n, \n112\n, \n118\n, \n127\n, \n130\n, \n160\n, \n185\n,\n187\n, \n188\n, \n191\n, \n205\n, \n210–212\n, \n216–218\n, \n275\n, \n276\n, \n279\n, \n284\n, \n324\n, \n333\n,\n336\ncordon, \n213\n, \n214\ndebugging, \n114\n, \n115\n, \n117\n, \n120\n, \n149\n, \n156\n, \n160\n, \n166\n, \n167\n, \n169\n, \n170\n, \n173\n,\n175–179\n, \n182–185\n, \n224\n, \n275\n, \n343\ndistributions, \n14\n, \n15\n, \n90\n, \n144\n, \n298\nenvironment variables, \n178\n, \n179\n, \n183\n, \n253\n, \n264–266\n, \n268\n, \n269\n, \n272\n, \n273\n,\n278\n, \n328\n, \n329\nevent log, \n90\n, \n114\n, \n117\n, \n169\n, \n171\n, \n173–175\n, \n215\n, \n223\ngroup, \n195\n, \n197\ningress controller, \n100\n, \n103\n, \n160\n, \n161\n, \n163–165\n, \n267\nIP address management, \n108\n, \n113\n, \n130\n, \n133\n, \n134\n, \n137\n, \n140\n, \n141\n, \n144\n, \n148\n,\n149\n, \n152–154\n, \n158\n, \n160\n, \n209\n, \n254\n, \n281\n, \n332\nlabels, \n117\n, \n118\n, \n120\n, \n168–170\n, \n286\n, \n298\n, \n327\n, \n328\n, \n330\n, \n336\n, \n341\nlarge clusters, \n297\nlimits, \n122\n, \n170\n, \n214\n, \n232–235\n, \n238\n, \n244–246\n, \n248\n, \n305\n, \n310–317\n, \n319\n,\n329\nlogs, \n115\n, \n123\n, \n178\n, \n182\n, \n202\n, \n222\n, \n224\n, \n287\n, \n290\n, \n301\n, \n328\nmetrics server, \n104\n, \n105\n, \n121\n, \n157\n, \n158\n, \n194\n, \n207\n, \n236\n, \n330\n, \n335\nnetworking, \n51\n, \n57\n, \n67\n, \n87\n, \n91\n, \n95\n, \n99\n, \n100\n, \n103\n, \n108\n, \n109\n, \n127–133\n, \n144\n,\n149\n, \n151\n, \n152\n, \n158\n, \n160\n, \n166\n, \n209\n, \n343\ncustomized, \n144\n, \n148\n, \n149\nnodes, \n89–91\n, \n97–102\n, \n109\n, \n112\n, \n118\n, \n128–130\n, \n133\n, \n139\n, \n162\n, \n165\n, \n168\n,\n169\n, \n188\n, \n192\n, \n194\n, \n203\n, \n205\n, \n211\n, \n213–216\n, \n218\n, \n233\n, \n249\n, \n251\n, \n255\n,\n275\n, \n276\n, \n296\n, \n298\n, \n299\n, \n303\n, \n305\n, \n309\n, \n317–321\n, \n323\n, \n324\n, \n330\n, \n331\n,\n336\noperators, \n101\n, \n256\n, \n278\n, \n288–292\n, \n324–326\n, \n343\nresource naming, \n121\n, \n196\n, \n201\n, \n256\n, \n257\n, \n263\n, \n280\n, \n282\n, \n283\n, \n327\nresource patching, \n104\n, \n161\n, \n162\n, \n222\n, \n224\n, \n301\n, \n303\n, \n306\nresource schema, \n282\nresource updates, \n172\n, \n176\n, \n182\n, \n183\n, \n262\n, \n264\n, \n327\nresource versioning, \n95\n, \n96\n, \n112\n, \n116\n, \n121\n, \n196\n, \n280\n, \n282\n, \n284\nresource watch, \n284\n, \n285\nresources. \nSee names of individual resources\nREST API, \n188\n, \n203\n, \n280\n, \n303\nroles, \n195\nscheduler, \n49\n, \n88\n, \n89\n, \n102\n, \n109\n, \n113–115\n, \n128\n, \n135\n, \n167–170\n, \n172\n, \n173\n,\n187\n, \n188\n, \n233\n, \n235\n, \n296–299\n, \n302\n, \n303\n, \n305\n, \n307\n, \n309\n, \n314\n, \n317–320\n,\n323\nsecurity, \n188\n, \n193\n, \n243\n, \n266–269\n, \n278\nselectors, \n117\n, \n118\n, \n120\n, \n128\n, \n154\n, \n162\n, \n168\n, \n169\n, \n228\n, \n328\n, \n331\n, \n341\nservice account, \n130\n, \n190\nstatic Pods, \n210\n, \n211\nstorage, \n87\n, \n100\n, \n102\n, \n108\n, \n109\n, \n124\n, \n126\n, \n127\n, \n248–250\n, \n267\n, \n323\n, \n343\ntraffic routing, \n109\n, \n129\n, \n130\n, \n133\n, \n134\n, \n136\n, \n137\n, \n139–142\n, \n149\n, \n151\n, \n152\n,\n154\n, \n155\n, \n158\n, \n160\n, \n163\n, \n165\n, \n166\n, \n184\n, \n187\n, \n188\n, \n199\n, \n219\n, \n228\n, \n279\n,\n284\n, \n300–302\n, \n323\n, \n329\n, \n331\n, \n342\nupdate strategy, \n262\nvolume access mode, \n262\nvolume mount, \n112\n, \n125\n, \n251–253\n, \n260\n, \n272–274\nKubic, \n27\nL\nleast privilege, \n269\nlimits, \n33\n, \n49\n, \n122\n, \n170\n, \n214\n, \n232–235\n, \n238\n, \n244–246\n, \n248\n, \n305\n, \n310–317\n,\n319\n, \n329\nCPU, \n34\n, \n38–45\n, \n47\n, \n173\n, \n235\n, \n237\n, \n314\n, \n329\nmemory, \n43–45\n, \n47\n, \n235–239\n, \n314\nnetwork, \n47\n, \n48\n, \n131\n, \n234\n, \n238\n, \n240–243\nLinux, \n180\ncgroups, \n37–41\n, \n43\n, \n44\n, \n46\n, \n47\n, \n49\n, \n236–238\n, \n310–312\n, \n314\n, \n315\nchroot, \n22\nCompletely Fair Scheduler, \n34\ndistribution, \n10\n, \n74\nkernel, \n10–12\n, \n22\n, \n27\n, \n31\n, \n32\n, \n34\n, \n78\n, \n91\n, \n92\n, \n248\nnamespaces, \n19\n, \n25\n, \n29–32\n, \n49\n, \n51\n, \n55–65\n, \n76\n, \n129–134\n, \n137\n, \n171\n, \n208\nOOM killer, \n46\npermissions, \n21\n, \n22\n, \n32\nPID, \n25\n, \n30\n, \n31\n, \n36\n, \n39\n, \n56\n, \n112\nscheduler, \n34\nsignal, \n31\n, \n32\n, \n46\nswap, \n43\ntraffic control, \n47–49\n, \n239\n, \n240\n, \n242\n, \n243\nusers, \nxxi\n, \n21\n, \n44\nload balancing, \n7\n, \n9\n, \n14\n, \n17\n, \n67\n, \n88\n, \n96\n, \n109\n, \n149\n, \n151–154\n, \n159\n, \n160\n, \n164\n,\n165\n, \n192\n, \n228\n, \n232\n, \n254\n, \n267\n, \n309\n, \n314\n, \n317\n, \n321\n, \n330\nLonghorn, \n102\n, \n103\n, \n106\n, \n108\n, \n194\n, \n211\n, \n213\n, \n250–252\n, \n255\n, \n257\n, \n258\n, \n280\n,\n326\nlong polling, \n284\nlsns\n, \n25\n, \n30\n, \n31\n, \n55\nM\nmacvlan\n, \n149\nmasquerade. \nSee\n Network Address Translation\nmemory access error, \n179\nmessage-driven architecture, \n153\nmetrics server, \n104\n, \n105\n, \n121\n, \n157\n, \n158\n, \n194\n, \n207\n, \n236\nmicroservice, \n5–7\n, \n17\n, \n51\n, \n87\n, \n89\n, \n109\n, \n228\nMicrosoft Azure, \n96\n, \n250\nmodularity, \n4\nmonitoring, \n8\n, \n9\n, \n14\n, \n16\n, \n17\n, \n116\n, \n119–121\n, \n187\n, \n188\n, \n205\n, \n218\n, \n321\n, \n324\n,\n330–333\n, \n336\n, \n339\n, \n342\n, \n343\nmount\n, \n79\n, \n80\n, \n209\n, \n210\nmultitasking, \n20\nmultitenancy, \n37\n, \n38\n, \n88\n, \n106\n, \n233\n, \n268\nMultus, \n144–147\nMythical Man-Month, The\n, \n5\nN\nNamespace, \n102\n, \n104\n, \n106–108\n, \n114\n, \n127\n, \n156–158\n, \n194–198\n, \n200–202\n, \n243–\n245\n, \n247\n, \n248\n, \n268\n, \n269\n, \n282\n, \n288\n, \n312\n, \n321\n, \n324\n, \n327\n, \n336\n, \n339–341\ncalico-system\n, \n102\n, \n107\n, \n127\ndefault\n, \n107\n, \n108\n, \n113–115\n, \n156\n, \n158\n, \n165\n, \n197\n, \n199\n, \n202\n, \n287\nkube-system\n, \n107\n, \n156–158\n, \n193\n, \n197\n, \n200\n, \n202\nlonghorn-system\n, \n108\nmonitoring\n, \n335\n, \n337\n, \n339\nsample\n, \n194\n, \n195\n, \n197\n, \n199\n, \n200\n, \n202\n, \n244\n, \n245\ntodo\n, \n324\n, \n328\n, \n330\n, \n338–341\nnamespaces, Linux. \nSee\n Linux: namespaces\nNano, \n182\nNAT. \nSee\n Network Address Translation\nndots\n, \n157\nnetwork\nbandwidth, \n6\n, \n296\n, \n300\n, \n302\nbridge, \n25\n, \n51\n, \n60–64\n, \n92\n, \n133\nburst, \n243\negress, \n47\n, \n48\n, \n242\nfirewall, \n26\n, \n61\n, \n91\n, \n92\n, \n133\n, \n135\n, \n139\n, \n158–160\n, \n216\nfragmentation, \n143\ningress, \n242\njumbo frames, \n144\nlatency, \n296\nMAC, \n60\n, \n137\n, \n138\n, \n140–143\nmaximum transmission unit, \n143\nMTU, \n143\nnamespaces. \nSee\n Linux: namespaces\nport conflicts, \n52–54\n, \n67\n, \n134\n, \n163\nrouting, \n52\n, \n54\n, \n60\n, \n61\n, \n64\n, \n65\n, \n92\n, \n134–137\n, \n139\n, \n141\n, \n142\n, \n144\n, \n149\ntoken bucket filter, \n48\n, \n242\n, \n243\ntracing, \n62\n, \n63\ntraffic control, \n47–49\nvirtual device, \n51\n, \n52\n, \n54\n, \n58\n, \n60\n, \n66\n, \n129–131\n, \n133\n, \n134\n, \n141\n, \n239\n, \n323\nvirtual Ethernet (veth) pair, \n58–61\n, \n63\n, \n133\n, \n134\n, \n137\n, \n141\n, \n243\nVLAN, \n47\n, \n144\n, \n149\nNetwork Address Translation, \n25\n, \n52\n, \n64–66\n, \n134\n, \n159\n, \n160\nNetworkAttachmentDefinition, \n145\n, \n148\nNFS, \n102\nNGINX, \n13\n, \n14\n, \n23\n, \n52–55\n, \n58\n, \n61\n, \n70–72\n, \n75\n, \n76\n, \n80\n, \n81\n, \n103\n, \n108\n, \n109\n,\n112–117\n, \n119\n, \n121\n, \n152–156\n, \n158\n, \n159\n, \n161–165\n, \n170\n, \n220\n, \n221\n, \n224\n,\n225\n, \n227–231\n, \n236–238\n, \n259\n, \n260\n, \n263\n, \n264\n, \n272\n, \n274\nNode, \n304\nNode.js, \n15\n, \n334\n, \n339\n, \n342\nO\nOOM killer, \n46\n, \n315\n, \n316\noom_score_adj\n, \n315\n, \n316\nOpen Container Initiative (OCI), \n82\n, \n83\nOpenShift, \n26\nOpenSUSE, \n27\noptimization, \n5\novercommitment, \n238\n, \n314\n, \n319\noverlay filesystem, \n78\n, \n80\n, \n81\n, \n84\noverlay network, \n128–130\n, \n134–137\n, \n142–144\n, \n149\n, \n151\n, \n152\n, \n158\n, \n160\n, \n209\nP\npause process, \n29\n, \n55\n, \n56\n, \n133\n, \n208\npeer review, \n224\nperformance, \n6\n, \n234\n, \n238\n, \n292\n, \n296\n, \n302\n, \n307\n, \n309\n, \n310\n, \n314\n, \n317\n, \n321\n, \n324\n,\n330\n, \n332\n, \n343\nperformance margin, \n310\n, \n317\nPersistentVolume, \n256–259\n, \n272\n, \n274\nPersistentVolumeClaim, \n255–260\n, \n262–264\n, \n325\n, \n326\npgrep\n, \n207\n, \n208\nping\n, \n54\n, \n58–64\n, \n66\n, \n109\n, \n136–139\n, \n141\n, \n142\n, \n158\nPod, \n15\n, \n16\n, \n28\n, \n35\n, \n44\n, \n56\n, \n107–109\n, \n112–115\n, \n117–119\n, \n121–128\n, \n130–136\n,\n138–141\n, \n143–149\n, \n151–160\n, \n162\n, \n163\n, \n167–173\n, \n176\n, \n177\n, \n179\n, \n181\n,\n183\n, \n184\n, \n188\n, \n196–202\n, \n208–214\n, \n217–219\n, \n221–225\n, \n228\n, \n230\n,\n233–240\n, \n242–245\n, \n249\n, \n250\n, \n252\n, \n254\n, \n256\n, \n257\n, \n260–264\n, \n266–271\n,\n274\n, \n278\n, \n281\n, \n284\n, \n285\n, \n287–290\n, \n296–303\n, \n305–307\n, \n309–321\n,\n327–329\n, \n331\n, \n335\n, \n339\n, \n340\n, \n342\neviction, \n213–215\n, \n311\n, \n314–317\n, \n319–321\nlogs, \n115\n, \n123\n, \n178\n, \n182\n, \n202\n, \n222\n, \n224\n, \n287\n, \n290\n, \n301\n, \n328\nmultiple containers, \n296\npriority, \n316\n, \n317\n, \n319\n, \n321\nrestartPolicy\n, \n122\nstatus\nContainerCreating\n, \n262\nCrashLoopBackOff\n, \n177\n, \n301\nCreateContainerConfigError\n, \n268\nError\n, \n177\n, \n301\nImagePullBackOff\n, \n174\n, \n176\n, \n177\nOutOfMemory\n, \n216\nPending\n, \n168\n, \n170\n, \n173\n, \n175\n, \n262\n, \n306\n, \n319\nRunning\n, \n113\n, \n222\nTerminating\n, \n217\n, \n222\nPodman, \n26\nPostgres Operator, \n288–291\n, \n324–326\n, \n328\nPostgreSQL, \n15\n, \n177–179\n, \n225\n, \n226\n, \n252–256\n, \n266\n, \n267\n, \n269\n, \n288–291\n, \n324–\n326\n, \n328\n, \n329\npredictability, \n309\n, \n310\n, \n314\n, \n321\nPriorityClass, \n317–321\nprivilege escalation, \n21\nprobes, \n218–220\n, \n223\n, \n232\n, \n302\nexec, \n220\n, \n221\n, \n223\n, \n225\nHTTP, \n223–225\nliveness, \n220–224\n, \n226–229\n, \n232\nreadiness, \n220\n, \n226\n, \n228–232\nstartup, \n220\n, \n226–228\n, \n232\nTCP, \n225\n, \n226\nprocess, \n11–14\n, \n17\n, \n19\n, \n20\n, \n33\n, \n46\nisolation, \n8\n, \n10\n, \n17\n, \n19–23\n, \n27\n, \n31\n, \n32\n, \n48\n, \n52\n, \n56\n, \n71\n, \n84\n, \n173\n, \n323\nnice level, \n35\n, \n37\npolicy, \n34\nFIFO, \n35\nround-robin, \n35\ntime-sharing, \n35\npriority, \n34–37\n, \n311\n, \n312\nreal-time, \n34\n, \n35\n, \n38\n, \n310\nzombie, \n31\nprom-client\n, \n334\nPrometheus, \n333–337\n, \n339–343\nscraping, \n336\n, \n337\n, \n339–342\nPrometheus Adapter, \n335\n, \n339\nPrometheus Operator, \n334\n, \n335\n, \n337\n, \n339\n, \n341\nPrometheus query, \n342\nprotobuf\n, \n276–278\nprotoc\n, \n277\n, \n278\nPython, \n284–286\nQ\nQuality of Service (QoS), \n234\n, \n236\n, \n238\n, \n307\n, \n310–313\n, \n315–317\n, \n321\nclasses\nBestEffort\n, \n310–312\n, \n314\n, \n316\n, \n319\nBurstable\n, \n310\n, \n312–314\n, \n316\n, \n317\nGuaranteed\n, \n310\n, \n312\n, \n314–317\n, \n320\n, \n329\nquay.io\n, \n76\nquotas, \n232\n, \n233\n, \n243–248\n, \n250\n, \n268\n, \n312\n, \n321\nCPU, \n243\n, \n244\nmemory, \n243\n, \n244\nR\nRancher, \n14\n, \n102\n, \n324\nRBAC. \nSee\n access control\nreal-time processing, \n34\n, \n35\n, \n38\n, \n310\nRedis, \n72–74\nreliability, \n6\n, \n7\n, \n17\n, \n73\n, \n87\n, \n88\n, \n109\n, \n219\n, \n233\n, \n248\n, \n296\n, \n302\n, \n321\nrenew\n, \n191\nReplicaSet, \n115–118\nreplication, \n291\nReplicationController, \n116\n, \n280\nrequests, \n117\n, \n170\n, \n233–235\n, \n244\n, \n246\n, \n248\n, \n304–306\n, \n310–317\n, \n319\n, \n329\n,\n330\nCPU, \n171\n, \n172\n, \n235\n, \n238\nmemory, \n235\n, \n238\nresilience, \n7\n, \n9\n, \n14\n, \n87\n, \n90\n, \n109\n, \n151\n, \n152\n, \n166\n, \n218\n, \n280\n, \n292\n, \n296\n, \n302\n, \n306\n,\n321\n, \n323\n, \n330\n, \n343\nResourceQuota, \n244\nresponsiveness, \n226\n, \n310\n, \n333\n, \n334\nrestartPolicy\n, \n122\nRocky Linux, \n10\n, \n71\nRole, \n195–197\n, \n282\n, \n286\n, \n339\n, \n340\nrole aggregation, \n286\n, \n287\nrole-based access control. \nSee\n access control\nRoleBinding, \n197\n, \n202\n, \n245\n, \n282\n, \n287\n, \n339\n, \n340\nroot cause, \n332\nrsyslogd\n, \n21\nS\nSAN (Subject Alternative Name), \n165\nscalability, \n4\n, \n6\n, \n16\n, \n17\n, \n87\n, \n88\n, \n109\n, \n112\n, \n115\n, \n151\n, \n152\n, \n166\n, \n185\n, \n249\n, \n279\n,\n295\n, \n296\n, \n302\n, \n306\n, \n309\n, \n317\n, \n323\n, \n324\n, \n332\n, \n343\nscheduler, \n171\n, \n317\nscreen\n, \nxxi\nSecret, \n193\n, \n195\n, \n265\n, \n269–272\n, \n274\n, \n276–278\n, \n325\n, \n326\n, \n329\nbase64 encoding, \n270\n, \n271\nbinary content, \n271\nsecurity, \n20\n, \n295\nsed\n, \n216\nsegmentation fault, \n182\n, \n183\n, \n185\nself-healing, \n90\n, \n118\nsemaphore, \n305\nseq\n, \n232\nService, \n17\n, \n92\n, \n104\n, \n124–126\n, \n134\n, \n149\n, \n151–166\n, \n210\n, \n222\n, \n228–231\n, \n240\n,\n243\n, \n254\n, \n255\n, \n284\n, \n288\n, \n289\n, \n291\n, \n299–302\n, \n325\n, \n326\n, \n328–332\n, \n336\n,\n337\n, \n339–343\nClusterIP\n, \n154\n, \n161\n, \n162\n, \n230\n, \n326\nendpoints, \n17\n, \n155\n, \n230\n, \n231\n, \n284\n, \n299\n, \n339\n, \n341\n, \n342\nExternalName\n, \n161\nheadless, \n161\n, \n254\n, \n255\nLoadBalancer\n, \n161\nNodePort\n, \n104\n, \n161–163\n, \n165\n, \n331\n, \n332\n, \n337\ntraffic policy, \n301\nCluster\n, \n301\nLocal\n, \n301\n, \n302\nServiceAccount, \n194\n, \n195\n, \n197–200\n, \n207\n, \n210\n, \n286\n, \n287\n, \n340\nServiceMonitor, \n336\n, \n337\n, \n339–342\nSHA-256, \n83\nSkopeo, \n26\n, \n82\nsocket, \n5\n, \n28\n, \n208\n, \n251\n, \n252\nspecialized hardware, \n295\n, \n296\n, \n303\n, \n306\n, \n317\nSpring Boot, \n232\nss\n, \n162\nSSL certificate, \n165\nstability, \n332\nstateful components, \n295\nStatefulSet, \n122\n, \n124\n, \n126\n, \n127\n, \n196\n, \n252–256\n, \n259\n, \n264\n, \n289–291\n, \n325\nstateless component, \n15\n, \n249\n, \n295\n, \n326\n, \n329\nStorageClass, \n250–252\n, \n255\n, \n257–259\nstrace\n, \n183\nstress\n, \n35\n, \n36\n, \n39\n, \n40\n, \n42\n, \n44–46\nstrings\n, \n207\nSubject Alternative Name, \n165\nsysctl\n, \n92\nsystemctl\n, \n206\n, \n215\n, \n216\nsystemd\n, \n206\nT\ntaints, \n95\n, \n99–102\ntar\n, \n83\ntc. \nSee\n Linux: traffic control\nTCP, \n159\n, \n160\ntcpdump\n, \n62\n, \n64\n, \n138\n, \n141–143\nthrottling, \n314\ntime–series data, \n333\ntmpfs\n, \n210\ntmux\n, \nxxi\ntodo application, \n15\n, \n324\n, \n325\n, \n330–335\n, \n339\n, \n342\n, \n343\ntolerations, \n101\n, \n102\ntop\n, \n36\n, \n41\n, \n42\nTopology Aware Hints, \n302\ntraffic policy. \nSee\n Service: traffic policy\nU\nUbuntu, \n10\n, \n23\n, \n71\n, \n93\n, \n138\n, \n180\nUDP, \n139\n, \n142–144\nulimit\n, \n43\nunshare\n, \n31\nupdate strategy\nRecreate\n, \n327\nRollingUpdate\n, \n327\nuser experience, \n332\nutilization, \n38\n, \n104\n, \n121\n, \n122\n, \n247\n, \n310\n, \n313\n, \n314\n, \n317\n, \n330\n, \n333\n, \n335\n, \n338\n,\n339\n, \n342\nV\nVagrant, \nxxi\n, \n16\n, \n91\n, \n96\n, \n209\n, \n331\n, \n332\nvi, \n182\nVirtualBox, \nxxi\nVirtual Extensible LAN, \n139\n, \n142–144\n, \n149\nvirtual machines, \nxx\n, \n11–13\n, \n20\n, \n32\n, \n78\n, \n90\n, \n91\n, \n130\n, \n139\n, \n303\nvolume mount, \n12\n, \n26\n, \n30\n, \n112\n, \n125\n, \n251–253\n, \n260\n, \n272–274\n, \n303\nVXLAN, \n139\n, \n142–144\n, \n149\nW\nwatch\n, \n113\nWeaveNet, \n130\n, \n139\n, \n141\n, \n143\nwell-known name, \n153\n, \n155–157\n, \n161\n, \n163\n, \n165\n, \n166\n, \n240\n, \n255\nwget\n, \n158\nWindows Subsystem for Linux, \nxx\nWSL, \nxx\nX\nxxd\n, \n277\nY\nYAML multiline string, \n273\nyum, \n11\nThe fonts used in \nThe Book of Kubernetes\n are New Baskerville, Futura, The\nSans Mono Condensed, and Dogma. The book was typeset with LATEX 2\nε\npackage \nnostarch\n by Boris Veytsman \n(2008/06/06 v1.3 Typesetting books for\nNo Starch Press).",29683
