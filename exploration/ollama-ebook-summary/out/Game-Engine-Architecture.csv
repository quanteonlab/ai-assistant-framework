filename,title,text,len
01-Half Title.pdf,01-Half Title,Game Engine Architecture\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com,97
02-Dedication.pdf,02-Dedication,"Game Engine Architecture\nThird Edition\nJason Gregory\nCRC Press\nTaylor & Francis Group\nBoca Raton London New York\nCRC Press is an imprint of the\nTaylor & Francis Group, an informa business\nAN A K PETERS BOOKCRC\nCRC Press\nTaylor & Francis Group\n6000 Broken Sound Parkway NW, Suite 300\nBoca Raton, FL 33487-2742\n© 2019 by Taylor & Francis Group, LLC\nCRC Press is an imprint of Taylor & Francis Group, an Informa business\nNo claim to original U.S. Government works\nPrinted on acid-free paper\nVersion Date: 20180529\nInternational Standard Book Number-13: 978-1-1380-3545-4 (Hardback)\nThis book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been made to publish \nreliable data and information, but the author and publisher cannot assume responsibility for the validity of all materials or the \nconsequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in \nthis publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright \nmaterial has not been acknowledged please write and let us know so we may rectify in any future reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized in any \nform by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and \nrecording, or in any information storage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, please access www.copyright.com (http://www.\ncopyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. \nCCC is a not-for-profit organization that provides licenses and registration for a variety of users. For organizations that have been \ngranted a photocopy license by the CCC, a separate system of payment has been arranged.\nTrademark Notice:  Product or corporate names may be trademarks or registered trademarks, and are used only for identifica -\ntion and explanation without intent to infringe.\nVisit the Taylor & Francis Web site at\nhttp://www.taylorandfrancis.com\nand the CRC Press Web site at\nhttp://www.crcpress.comLibrary of Congress Cataloging-in-Publication Data\nNames: Gregory, Jason, 1970- author.\nTitle: Game engine architecture / Jason Gregory.\nDescription: Third edition. | Boca Raton : Taylor & Francis, CRC Press, 2018. \n| Includes bibliographical references and index.\nIdentifiers: LCCN 2018004893 | ISBN 9781138035454 (hardback : alk. paper)\nSubjects: LCSH: Computer games--Programming--Computer programs. | Software \narchitecture. | Computer games--Design.\nClassification: LCC QA76.76.C672 G77 2018 | DDC 794.8/1525--dc23\nLC record available at https://lccn.loc.gov/2018004893Cover image: 3D model of SpaceX Merlin rocket engine created by Brian Hauger (www.bionic3d.com).\nDedicated to\nTrina, Evan and Quinn Gregory,\nin memory of our heroes,\nJoyce Osterhus, Kenneth Gregory and Erica Gregory.\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",3253
03-Table of Contents.pdf,03-Table of Contents,"Contents\nPreface xiii\nI Foundations 1\n1 Introduction 3\n1.1 Structure of a Typical Game Team 5\n1.2 What Is a Game? 8\n1.3 What Is a Game Engine? 11\n1.4 Engine Differences across Genres 13\n1.5 Game Engine Survey 31\n1.6 Runtime Engine Architecture 38\n1.7 Tools and the Asset Pipeline 59\n2 Tools of the Trade 69\n2.1 Version Control 69\n2.2 Compilers, Linkers and IDEs 78\n2.3 Proﬁling Tools 99\nvii\nviii CONTENTS\n2.4 Memory Leak and Corruption Detection 101\n2.5 Other Tools 102\n3 Fundamentals of Software Engineering for Games 105\n3.1 C++ Review and Best Practices 105\n3.2 Catching and Handling Errors 119\n3.3 Data, Code and Memory Layout 131\n3.4 Computer Hardware Fundamentals 164\n3.5 Memory Architectures 181\n4 Parallelism and Concurrent Programming 203\n4.1 Deﬁning Concurrency and Parallelism 204\n4.2 Implicit Parallelism 211\n4.3 Explicit Parallelism 225\n4.4 Operating System Fundamentals 230\n4.5 Introduction to Concurrent Programming 256\n4.6 Thread Synchronization Primitives 267\n4.7 Problems with Lock-Based Concurrency 281\n4.8 Some Rules of Thumb for Concurrency 286\n4.9 Lock-Free Concurrency 289\n4.10 SIMD/Vector Processing 331\n4.11 Introduction to GPGPU Programming 348\n5 3D Math for Games 359\n5.1 Solving 3D Problems in 2D 359\n5.2 Points and Vectors 360\n5.3 Matrices 375\n5.4 Quaternions 394\n5.5 Comparison of Rotational Representations 403\n5.6 Other Useful Mathematical Objects 407\n5.7 Random Number Generation 412\nCONTENTS ix\nII Low-Level Engine Systems 415\n6 Engine Support Systems 417\n6.1 Subsystem Start-Up and Shut-Down 417\n6.2 Memory Management 426\n6.3 Containers 441\n6.4 Strings 456\n6.5 Engine Conﬁguration 470\n7 Resources and the File System 481\n7.1 File System 482\n7.2 The Resource Manager 493\n8 The Game Loop and Real-Time Simulation 525\n8.1 The Rendering Loop 525\n8.2 The Game Loop 526\n8.3 Game Loop Architectural Styles 529\n8.4 Abstract Timelines 532\n8.5 Measuring and Dealing with Time 534\n8.6 Multiprocessor Game Loops 544\n9 Human Interface Devices 559\n9.1 Types of Human Interface Devices 559\n9.2 Interfacing with a HID 561\n9.3 Types of Inputs 563\n9.4 Types of Outputs 569\n9.5 Game Engine HID Systems 570\n9.6 Human Interface Devices in Practice 587\n10 Tools for Debugging and Development 589\n10.1 Logging and Tracing 589\n10.2 Debug Drawing Facilities 594\n10.3 In-Game Menus 601\n10.4 In-Game Console 604\n10.5 Debug Cameras and Pausing the Game 605\n10.6 Cheats 606\nx CONTENTS\n10.7 Screenshots and Movie Capture 606\n10.8 In-Game Proﬁling 608\n10.9 In-Game Memory Stats and Leak Detection 615\nIII Graphics, Motion and Sound 619\n11 The Rendering Engine 621\n11.1 Foundations of Depth-Buffered Triangle Rasterization 622\n11.2 The Rendering Pipeline 667\n11.3 Advanced Lighting and Global Illumination 697\n11.4 Visual Effects and Overlays 710\n11.5 Further Reading 719\n12 Animation Systems 721\n12.1 Types of Character Animation 721\n12.2 Skeletons 727\n12.3 Poses 729\n12.4 Clips 734\n12.5 Skinning and Matrix Palette Generation 750\n12.6 Animation Blending 755\n12.7 Post-Processing 774\n12.8 Compression Techniques 777\n12.9 The Animation Pipeline 784\n12.10 Action State Machines 786\n12.11 Constraints 806\n13 Collision and Rigid Body Dynamics 817\n13.1 Do You Want Physics in Your Game? 818\n13.2 Collision/Physics Middleware 823\n13.3 The Collision Detection System 825\n13.4 Rigid Body Dynamics 854\n13.5 Integrating a Physics Engine into Your Game 892\n13.6 Advanced Physics Features 909\nCONTENTS xi\n14 Audio 911\n14.1 The Physics of Sound 912\n14.2 The Mathematics of Sound 924\n14.3 The Technology of Sound 941\n14.4 Rendering Audio in 3D 955\n14.5 Audio Engine Architecture 974\n14.6 Game-Speciﬁc Audio Features 995\nIV Gameplay 1013\n15 Introduction to Gameplay Systems 1015\n15.1 Anatomy of a Game World 1016\n15.2 Implementing Dynamic Elements: Game Objects 1021\n15.3 Data-Driven Game Engines 1024\n15.4 The Game World Editor 1025\n16 Runtime Gameplay Foundation Systems 1039\n16.1 Components of the Gameplay Foundation System 1039\n16.2 Runtime Object Model Architectures 1043\n16.3 World Chunk Data Formats 1062\n16.4 Loading and Streaming Game Worlds 1069\n16.5 Object References and World Queries 1079\n16.6 Updating Game Objects in Real Time 1086\n16.7 Applying Concurrency to Game Object Updates 1101\n16.8 Events and Message-Passing 1114\n16.9 Scripting 1134\n16.10 High-Level Game Flow 1157\nV Conclusion 1159\n17 You Mean There’s More? 1161\n17.1 Some Engine Systems We Didn’t Cover 1161\n17.2 Gameplay Systems 1162\nBibliography 1167\nIndex 1171\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",4640
04-Preface.pdf,04-Preface,"Preface\nWelcome to Game Engine Architecture. This book aims to present a com-\nplete discussion of the major components that make up a typical com-\nmercial game engine. Game programming is an immense topic, so we have\na lot of ground to cover. Nevertheless, I trust you’ll find that the depth of\nour discussions is sufficient to give you a solid understanding of both the the-\noryandthecommonpracticesemployedwithineachoftheengineeringdisci-\nplineswe’llcover. Thatsaid,thisbookisreallyjustthebeginningofafascinat-\ningandpotentiallylifelongjourney. Awealthofinformationisavailableonall\naspects of game technology, and this text serves both as a foundation-laying\ndevice and as a jumping-off point for further learning.\nOur focus in this book will be on game engine technologies and architec-\nture. This means we’ll cover the theory underlying the various subsystems\nthat comprise a commercial game engine, the data structures, algorithms and\nsoftware interfaces that are typically used to implement them, and how these\nsubsystems function together within a game engine as a whole. The line be-\ntween the game engine and the game is rather blurry. We’ll focus primarily\non the engine itself, including a host of low-level foundation systems, the ren-\ndering engine, the collision system, the physics simulation, character anima-\ntion,audio,andanin-depthdiscussionofwhatIcallthegameplayfoundation\nlayer. This layer includes the game’s object model, world editor, event system\nandscriptingsystem. We’llalsotouchonsomeaspectsofgameplayprogram-\nxiii\nxiv Preface\nming, including player mechanics, cameras and AI. However, by necessity,\nthe scope of these discussions will be limited mainly to the ways in which\ngameplay systems interface with the engine.\nThis book is intended to be used as a course text for a two- or three-course\ncollege-level series in intermediate game programming. It can also be used\nbyamateursoftwareengineers,hobbyists,self-taughtgameprogrammersand\nexistingmembersofthegameindustryalike. Juniorengineerscanusethistext\nto solidify their understanding of game mathematics, engine architecture and\ngame technology. And some senior engineers who have devoted their careers\nto one particular specialty may benefit from the bigger picture presented in\nthese pages as well.\nTo get the most out of this book, you should have a working knowledge\nof basic object-oriented programming concepts and at least some experience\nprogramminginC++. Thegameindustryroutinelymakesuseofawiderange\nof programming languages, but industrial-strength 3D game engines are still\nwritten primarily in C++. As such, any serious game programmer needs to be\nable to code in C++. We’ll review the basic tenets of object-oriented program-\nminginChapter3,andyouwillnodoubtpickupafewnewC++tricksasyou\nread this book, but a solid foundation in the C++ language is best obtained\nfrom [46], [36] and [37]. If your C++ is a bit rusty, I recommend you refer to\nthese or similar books to refresh your knowledge as you read this text. If you\nhave no prior C++ experience, you may want to consider reading at least the\nfirst few chapters of [46] and/or working through a few C++ tutorials online,\nbefore diving into this book.\nThe best way to learn computer programming of any kind is to actually\nwrite some code. As you read through this book, I strongly encourage you\nto select a few topic areas that are of particular interest to you and come up\nwith some projects for yourself in those areas. For example, if you find char-\nacter animation interesting, you could start by installing OGRE and explor-\ning its skinned animation demo. Then you could try to implement some of\nthe animation blending techniques described in this book, using OGRE. Next\nyou might decide to implement a simple joypad-controlled animated charac-\nter that can run around on a flat plane. Once you have something relatively\nsimple working, expand upon it! Then move on to another area of game tech-\nnology. Rinse and repeat. It doesn’t particularly matter what the projects are,\nas long as you’re practicing the art of game programming, not just reading\nabout it.\nGame technology is a living, breathing thing that can never be entirely\ncaptured within the pages of a book. As such, additional resources, errata,\nupdates, sample code and project ideas will be posted from time to time on\nPreface xv\nthisbook’swebsiteathttp://www.gameenginebook.com. Youcanalsofollow\nme on Twitter @jqgregory.\nNew to the Third Edition\nThe computing hardware that lies at the heart of today’s game consoles, mo-\nbile devices and personal computers makes heavy use of parallelism. Deep\nwithin the CPUs and GPUs in these devices, multiple functional units operate\nsimultaneously, employing a “divide and conquer” approach to high-speed\ncomputation. Whileparallelcomputinghardwarecanmaketraditionalsingle-\nthreaded programs run faster, programmers need to write concurrent software\nto truly take advantage of the hardware parallelism that has become ubiqui-\ntous in modern computing platforms.\nIn prior editions of Game Engine Architecture, the topics of parallelism and\nconcurrencyweretouchedoninthecontextofgameenginedesign. However,\nthey weren’t given the in-depth treatment they deserved. In this, the third\neditionofthebook,thisproblemhasbeenremediedviatheadditionofabrand\nnewchapteronconcurrencyandparallelism. Chapters8and16havealsobeen\naugmented to include detailed discussions of how concurrent programming\ntechniques are typically applied to game engine subsystem and game object\nmodel updates, and how a general-purpose job system can be used to unlock\nthe power of concurrency within a game engine.\nI’ve already mentioned that every good game programmer must have a\nstrong working knowledge of C++ (in addition to the wide variety of other\nuseful languages used regularly in the game industry). In my view, a pro-\ngrammer’sknowledgeofhigh-levellanguagesshouldrestuponasolidunder-\nstanding of the software and hardware systems that underlie them. As such,\nin this edition I’ve expanded Chapter 3 to include a treatment of the funda-\nmentals of computer hardware, assembly language, and the operating system\nkernel.\nThisthirdeditionof GameEngineArchitecture alsoimprovesuponthetreat-\nment of various topics covered in prior editions. A discussion of local and\nglobal compiler optimizations has been added. Fuller coverage of the vari-\nous C++ language standards is included. The section on memory caching and\ncache coherency has been expanded. The animation chapter has been stream-\nlined. And, as with the second edition, various errata have been repaired that\nwerebrought to my attention by you, my devoted readers. Thank you! I hope\nyou’ll find that the mistakes you found have all been fixed. (Although no\ndoubt they have been replaced by a slew of newmistakes, about which you\ncan feel free to inform me, so that I may correct them in the fourth edition of\nthe book!)\nxvi Preface\nOf course, as I’ve said before, the field of game engine programming is al-\nmost unimaginably broad and deep. There’s no way to cover every topic in\none book. As such, the primary purpose of this book remains to serve as an\nawareness-building tool and a jumping-off point for further learning. I hope\nyoufindthiseditionhelpfulonyourjourneythroughthefascinatingandmul-\ntifaceted landscape of game engine architecture.\nAcknowledgments\nNo book is created in a vacuum, and this one is certainly no exception. This\nbook—and its third edition, which you now hold in your hands—would not\nhavebeenpossiblewithoutthehelpofmyfamily,friendsandcolleaguesinthe\ngameindustry,andI’dliketoextendwarmthankstoeveryonewhohelpedme\nto bring this project to fruition.\nOf course, the ones most impacted by a project like this are invariably the\nauthor’s family. So I’d like to start by offering, for a thirdtime, a special thank-\nyou to my wife Trina. She was a pillar of strength during the writing of the\noriginal book, and she was as supportive and invaluably helpful as ever dur-\ningmyworkonthesecondandthirdeditions. WhileIwasbusytappingaway\non my keyboard, Trina was always there to take care of our two boys, Evan\n(now age 15) and Quinn (age 12), day after day and night after night, often\nforgoing her own plans, doing my chores as well as her own (more often than\nI’d like to admit), and always giving me kind words of encouragement when\nI needed them the most.\nIwouldalsoliketoextendspecialthankstomyeditorsforthefirstedition,\nMatt Whiting and Jeff Lander. Their insightful, targeted and timely feedback\nwasalwaysrightonthemoney,andtheirvastexperienceinthegameindustry\nhelped to give me confidence that the information presented in these pages\nis as accurate and up-to-date as humanly possible. Matt and Jeff were both\na pleasure to work with, and I am honored to have had the opportunity to\ncollaborate with such consummate professionals on this project. I’d like to\nextendaspecialthank-youtoJeffforputtingmeintouchwithAlicePetersand\nhelping me to get this projectoffthe groundin the first place. Matt, thank you\nalso for stepping up to the plate once again and providing me with valuable\nfeedback on the new concurrency chapter in the third edition.\nA number of my colleagues at Naughty Dog also contributed to this\nbook, either by providing feedback or by helping me with the structure and\ntopic content of one of the chapters. I’d like to thank Marshall Robin and\nCarlos Gonzalez-Ochoa for their guidance and tutelage as I wrote the render-\ningchapter, andPål-KristianEngstadforhisexcellentandinsightfulfeedback\nPreface xvii\non the content of that chapter. My thanks go to Christian Gyrling for his feed-\nback on various sections of the book, including the chapter on animation, and\nthe new chapter on parallelism and concurrency. I want to extend a special\nthank-you to Jonathan Lanier, Naughty Dog’s resident senior audio program-\nmer extraordinaire, for providing me with a great deal of the raw informa-\ntion you’ll find in the audio chapter, for always being available to chat when I\nhad questions, and for providing laser-focused and invaluable feedback after\nreading the initial draft. I’d also like to thank one of the newest members of\nourNaughtyDogprogrammingteam,KareemOmar,forhisvaluableinsights\nandfeedbackonthenewconcurrencychapter. Mythanksalsogototheentire\nNaughty Dog engineering team for creating all of the incredible game engine\nsystems that I highlight in this book.\nAdditional thanks go to Keith Schaeffer of Electronic Arts for providing\nme with much of the raw content regarding the impact of physics on a game,\nfound in Section 13.1. I’d also like to extend a warm thank-you to Christophe\nBalestra (who was co-president of Naughty Dog during my first ten years\nthere), Paul Keet (who was a lead engineer on the Medal of Honor franchise\nduring my time at Electronic Arts), and Steve Ranck (the lead engineer on the\nHydro Thunder project at Midway San Diego), for their mentorship and guid-\nance over the years. While they did not contribute to the book directly, they\ndid help to make me the engineer that I am today, and their influences are\nechoed on virtually every page in one way or another.\nThis book arose out of the notes I developed for a course entitled ITP-485:\nProgramming Game Engines, which I taught under the auspices of the Infor-\nmation Technology Program at the University of Southern California for ap-\nproximately four years. I would like to thank Dr. Anthony Borquez, the di-\nrector of the ITP department at the time, for hiring me to develop the ITP-485\ncourse curriculum in the first place.\nMyextendedfamilyandfriendsalsodeservethanks,inpartfortheirunwa-\nveringencouragement, andinpartforentertainingmywifeandour twoboys\non so many occasions while I was working. I’d like to thank my sister- and\nbrother-in-law, Tracy Lee and Doug Provins, my cousin-in-law Matt Glenn,\nand all of our incredible friends, including Kim and Drew Clark, Sherilyn and\nJim Kritzer, Anne and Michael Scherer, Kim and Mike Warner, and Kendra\nand Andy Walther. When I was a teenager, my father Kenneth Gregory\nwroteExtraordinary Stock Profits —a book on investing in the stock market—\nand in doing so, he inspired me to write this book. For this and so much\nmore, I am eternally grateful to him. I’d also like to thank my mother Er-\nica Gregory, in part for her insistence that I embark on this project, and in\npart for spending countless hours with me when I was a child, beating the art\nxviii Preface\nof writing into my cranium. I owe my writing skills, my work ethic, and my\nrather twisted sense of humor entirely to her!\nI’dliketothankAlicePetersandKevinJackson-Mead, aswellastheentire\nA K Peters staff, for their Herculean efforts in publishing the first edition of\nthis book. Since that time, A K Peters has been acquired by the CRC Press,\nthe principal science and technology book division of the Taylor & Francis\nGroup. I’d like to wish Alice and Klaus Peters all the best in their future en-\ndeavors. I’d also like to thank Rick Adams, Jennifer Ahringer, Jessica Vega\nand Cynthia Klivecka of Taylor & Francis for their patient support and help\nthroughout the process of creating the second and third editions of Game En-\ngine Architecture , Jonathan Pennell for his work on the cover for the second\nedition, Scott Shamblin for his work on the third edition’s cover art, and Brian\nHaeger (http://www.bionic3d.com) for graciously permitting me to use his\nbeautiful 3D model of the Space X Merlin rocket engine on the cover of the\nthird edition.\nI am thrilled to be able to say that both the first and second editions of\nGameEngineArchitecture have been or are being translated into Japanese, Chi-\nneseandKorean! IwouldliketoextendmysincerethankstoKazuhisaMinato\nand his team at Namco Bandai Games for taking on the incredibly daunting\ntask of the Japanese translation, and for doing such a great job with both edi-\ntions. I’d also like to thank the folks at Softbank Creative, Inc. for publishing\nthe Japanese version of the book. I would also like to extend my warmest\nthankstoMiloYipforhishardworkanddedicationtotheChinesetranslation\nproject. MysincereappreciationgoestothePublishingHouseoftheElectron-\nics Industry for publishing the Chinese translation of the book, and to both\nthe Acorn Publishing Company and Hongreung Science Publishing Co. for\ntheir publication of the Korean translations of the first and second editions,\nrespectively.\nMany of my readers took the time to send me feedback and alert me to er-\nrors in the first and second editions, and for that I’d like to extend my sincere\nthanks to all of you who contributed. I’d like to give a special thank-you to\nMilo Yip, Joe Conley and Zachary Turner for going above and beyond the call\nof duty in this regard. All three of you provided me with many-page docu-\nments, chockfulloferrataandincrediblyvaluableandinsightfulsuggestions.\nI’ve tried my best to incorporate all of this feedback into the third edition—\nplease keep it coming!\nJason Gregory\nApril 2018",15189
05-I Foundations.pdf,05-I Foundations,Part I\nFoundations\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com,92
06-1 Introduction.pdf,06-1 Introduction,"1\nIntroduction\nWhen I got my first game console in 1979—a way-cool Intellivision sys-\ntembyMattel—theterm“gameengine”didnotexist. Backthen, video\nand arcade games were considered by most adults to be nothing more than\ntoys, and the software that made them tick was highly specialized to both\nthe game in question and the hardware on which it ran. Today, games are a\nmulti-billion-dollarmainstreamindustryrivalingHollywoodinsizeandpop-\nularity. Andthesoftwarethatdrivesthesenow-ubiquitousthree-dimensional\nworlds—gameengines likeEpicGames’UnrealEngine4,Valve’sSourceengine\nand,Crytek’sCRYENGINE®3,ElectronicArtsDICE’sFrostbite™engine,and\nthe Unity game engine—have become fully featured reusable software devel-\nopment kits that can be licensed and used to build almost any game imagin-\nable.\nWhilegameenginesvarywidelyinthedetailsoftheirarchitectureandim-\nplementation,recognizablecoarse-grainedpatternshaveemergedacrossboth\npublicly licensed game engines and their proprietary in-house counterparts.\nVirtuallyallgameenginescontainafamiliarsetofcorecomponents,including\nthe rendering engine, the collision and physics engine, the animation system,\nthe audio system, the game world object model, the artificial intelligence sys-\ntem and so on. Within each of these components, a relatively small number of\nsemi-standard design alternatives are also beginning to emerge.\n3\n4 1. Introduction\nThere are a great many books that cover individual game engine subsys-\ntems, such as three-dimensional graphics, in exhaustive detail. Other books\ncobble together valuable tips and tricks across a wide variety of game tech-\nnology areas. However, I have been unable to find a book that provides its\nreader with a reasonably complete picture of the entire gamut of components\nthatmakeupamoderngameengine. Thegoalofthisbook,then,istotakethe\nreader on a guided hands-on tour of the vast and complex landscape of game\nengine architecture.\nIn this book you will learn:\n• how real industrial-strength production game engines are architected;\n• howgamedevelopmentteamsareorganizedandworkintherealworld;\n• which major subsystems and design patterns appear again and again in\nvirtually every game engine;\n• the typical requirements for each major subsystem;\n• which subsystems are genre- or game-agnostic, and which ones are typ-\nically designed explicitly for a specific genre or game; and\n• where the engine normally ends and the game begins.\nWe’llalsogetafirst-handglimpseintotheinnerworkingsofsomepopular\ngame engines, such as Quake, Unreal and Unity, and some well-known mid-\ndlewarepackages, suchastheHavokPhysicslibrary, theOGRErenderingen-\ngine and Rad Game Tools’ Granny 3D animation and geometry management\ntoolkit. Andwe’llexploreanumberofproprietarygameenginesthatI’vehad\nthe pleasure to work with, including the engine Naughty Dog developed for\nitsUncharted andThe Last of Us game series.\nBefore we get started, we’ll review some techniques and tools for large-\nscale software engineering in a game engine context, including:\n• the difference between logical and physical software architecture;\n• configuration management, revision control and build systems; and\n• some tips and tricks for dealing with one of the common development\nenvironments for C and C++, Microsoft Visual Studio.\nIn this book I assume that you have a solid understanding of C++ (the lan-\nguage of choice among most modern game developers) and that you under-\nstand basic software engineering principles. I also assume you have some",3553
07-1.1 Structure of a Typical Game Team.pdf,07-1.1 Structure of a Typical Game Team,"1.1. Structure of a Typical Game Team 5\nexposure to linear algebra, three-dimensional vector and matrix math and\ntrigonometry (although we’ll review the core concepts in Chapter 5). Ideally,\nyou should have some prior exposure to the basic concepts of real time and\nevent-driven programming. But never fear—I will review these topics briefly,\nand I’ll also point you in the right direction if you feel you need to hone your\nskills further before we embark.\n1.1 Structure of a Typical Game Team\nBefore we delve into the structure of a typical game engine, let’s first take a\nbrief look at the structure of a typical game development team. Game stu-\ndios are usually composed of five basic disciplines: engineers, artists, game\ndesigners, producers and other management and support staff (marketing,\nlegal, information technology/technical support, administrative, etc.). Each\ndisciplinecanbedividedintovarioussubdisciplines. We’lltakeabrieflookat\neach below.\n1.1.1 Engineers\nThe engineers design and implement the software that makes the game, and\nthetools,work. Engineersareoftencategorizedintotwobasicgroups: runtime\nprogrammers(whoworkontheengineandthegameitself)and toolsprogram-\nmers (who work on the offline tools that allow the rest of the development\nteam to work effectively). On both sides of the runtime/tools line, engineers\nhave various specialties. Some engineers focus their careers on a single en-\ngine system, such as rendering, artificial intelligence, audio or collision and\nphysics. Some focus on gameplay programming and scripting, while others\nprefer to work at the systems level and not get too involved in how the game\nactually plays. Some engineers are generalists—jacks of all trades who can\njump around and tackle whatever problems might arise during development.\nSeniorengineersaresometimesaskedtotakeonatechnicalleadershiprole.\nLead engineers usually still design and write code, but they also help to man-\nage the team’s schedule, make decisions regarding the overall technical direc-\ntion of the project, and sometimes also directly manage people from a human\nresources perspective.\nSomecompaniesalsohaveoneormoretechnicaldirectors(TD),whosejob\nit is to oversee one or more projects from a high level, ensuring that the teams\nareawareofpotentialtechnicalchallenges,upcomingindustrydevelopments,\nnew technologies and so on. The highest engineering-related position at a\ngame studio is the chief technical officer (CTO), if the studio has one. The\n6 1. Introduction\nCTO’sjob is to serve as a sort of technical directorfor the entirestudio, as well\nas serving a key executive role in the company.\n1.1.2 Artists\nAs we say in the game industry, “Content is king.” The artists produce all of\nthe visual and audio content in the game, and the quality of their work can\nliterally make or break a game. Artists come in all sorts of flavors:\n•Conceptartists producesketchesandpaintingsthatprovidetheteamwith\navisionofwhatthefinalgamewilllooklike. Theystarttheirworkearly\nintheconceptphaseofdevelopment,butusuallycontinuetoprovidevi-\nsual direction throughout a project’s life cycle. It is common for screen-\nshotstakenfromashippinggametobearanuncannyresemblancetothe\nconcept art.\n•3D modelers produce the three-dimensional geometry for everything in\nthevirtualgameworld. Thisdisciplineistypicallydividedintotwosub-\ndisciplines: foreground modelers and background modelers. The for-\nmer create objects, characters, vehicles, weapons and the other objects\nthat populate the game world, while the latter build the world’s static\nbackground geometry (terrain, buildings, bridges, etc.).\n•Texture artists create the two-dimensional images known as textures,\nwhichareappliedtothesurfacesof3Dmodelsinordertoprovidedetail\nand realism.\n•Lighting artists lay out all of the light sources in the game world, both\nstatic and dynamic, and work with color, intensity and light direction to\nmaximize the artfulness and emotional impact of each scene.\n•Animators imbue the characters and objects in the game with motion.\nTheanimatorsservequiteliterallyasactorsinagameproduction,justas\ntheydoinaCGfilmproduction. However,agameanimatormusthavea\nunique set of skills in order to produce animations that mesh seamlessly\nwith the technological underpinnings of the game engine.\n•Motioncaptureactors areoftenusedtoprovidearoughsetofmotiondata,\nwhich are then cleaned up and tweaked by the animators before being\nintegrated into the game.\n•Sounddesigners work closely with the engineers in order to produce and\nmix the sound effects and music in the game.\n1.1. Structure of a Typical Game Team 7\n•Voiceactors provide the voices of the characters in many games.\n• Manygameshaveoneormore composers,whocomposeanoriginalscore\nfor the game.\nAs with engineers, senior artists are often called upon to be team leaders.\nSomegameteamshaveoneormore artdirectors —veryseniorartistswhoman-\nage the look of the entire game and ensure consistency across the work of all\nteam members.\n1.1.3 Game Designers\nThe game designers’ job is to design the interactive portion of the player’s ex-\nperience, typically known as gameplay. Different kinds of designers work at\ndifferent levels of detail. Some (usually senior) game designers work at the\nmacrolevel,determiningthestoryarc,theoverallsequenceofchaptersorlev-\nels,andthehigh-levelgoalsandobjectivesoftheplayer. Otherdesignerswork\non individual levels or geographical areas within the virtual game world, lay-\ning out the static background geometry, determining where and when ene-\nmies will emerge, placing supplies like weapons and health packs, designing\npuzzle elements and so on. Still other designers operate at a highly technical\nlevel, workingcloselywithgameplayengineersand/orwritingcode(oftenin\na high-level scripting language). Some game designers are ex-engineers, who\ndecided they wanted to play a more active role in determining how the game\nwill play.\nSome game teams employ one or more writers. A game writer’s job can\nrangefromcollaboratingwiththeseniorgamedesignerstoconstructthestory\narc of the entire game, to writing individual lines of dialogue.\nAs with other disciplines, some senior designers play management roles.\nMany game teams have a game director, whose job it is to oversee all aspects\nof a game’s design, help manage schedules, and ensure that the work of indi-\nvidual designers is consistent across the entire product. Senior designers also\nsometimes evolve into producers.\n1.1.4 Producers\nThe role of producer is defined differently by different studios. In some game\ncompanies,theproducer’sjobistomanagethescheduleandserveasahuman\nresources manager. In other companies, producers serve in a senior game de-\nsign capacity. Still other studios ask their producers to serve as liaisons be-\ntween the development team and the business unit of the company (finance,\nlegal, marketing, etc.). Some smaller studios don’t have producers at all. For",7012
08-1.2 What Is a Game.pdf,08-1.2 What Is a Game,"8 1. Introduction\nexample, at Naughty Dog, literally everyone in the company, including the\ntwo co-presidents, plays a direct role in constructing the game; team man-\nagement and business duties are shared between the senior members of the\nstudio.\n1.1.5 Other Staff\nThe team of people who directly construct the game is typically supported by\na crucial team of support staff. This includes the studio’s executive manage-\nment team, the marketing department (or a team that liaises with an external\nmarketing group), administrative staff and the IT department, whose job is\nto purchase, install and configure hardware and software for the team and to\nprovide technical support.\n1.1.6 Publishers and Studios\nThe marketing, manufacture and distribution of a game title are usually han-\ndled by a publisher, not by the game studio itself. A publisher is typically a\nlarge corporation, like Electronic Arts, THQ, Vivendi, Sony, Nintendo, etc.\nMany game studios are not affiliated with a particular publisher. They sell\neachgamethattheyproducetowhicheverpublisherstrikesthebestdealwith\nthem. Other studios work exclusively with a single publisher, either via a\nlong-termpublishingcontractorasafullyownedsubsidiaryofthepublishing\ncompany. Forexample, THQ’sgamestudiosareindependentlymanaged, but\nthey are owned and ultimately controlled by THQ. Electronic Arts takes this\nrelationship one step further, by directly managing its studios. First-party de-\nvelopers are game studios owned directly by the console manufacturers (Sony,\nNintendo and Microsoft). For example, Naughty Dog is a first-party Sony de-\nveloper. These studios produce games exclusively for the gaming hardware\nmanufactured by their parent company.\n1.2 What Is a Game?\nWe probably all have a pretty good intuitive notion of what a game is. The\ngeneralterm“game”encompassesboardgameslikechessand Monopoly ,card\ngames like poker and blackjack, casino games like roulette and slot machines,\nmilitary war games, computer games, various kinds of play among children,\nand the list goes on. In academia we sometimes speak of game theory , in\nwhich multiple agents select strategies and tactics in order to maximize their\ngains within the framework of a well-defined set of game rules. When used\nin the context of console or computer-based entertainment, the word “game”\n1.2. What Is a Game? 9\nusually conjures images of a three-dimensional virtual world featuring a hu-\nmanoid, animal or vehicle as the main character under player control. (Or\nfor the old geezers among us, perhaps it brings to mind images of two-\ndimensionalclassicslike Pong,Pac-Man,or DonkeyKong .) Inhisexcellentbook,\nATheoryofFunforGameDesign,RaphKosterdefinesagametobeaninteractive\nexperiencethatprovidestheplayerwithanincreasinglychallengingsequence\nofpatternswhichheorshelearnsand eventuallymasters[30]. Koster’sasser-\ntion is that the activities of learning and mastering are at the heart of what we\ncall “fun,” just as a joke becomes funny at the moment we “get it” by recog-\nnizing the pattern.\nFor the purposes of this book, we’ll focus on the subset of games that com-\nprise two- and three-dimensional virtual worlds with a small number of play-\ners (between one and 16 or thereabouts). Much of what we’ll learn can also\nbe applied to HTML5/JavaScript games on the Internet, pure puzzle games\nlikeTetris, or massively multiplayer online games (MMOG). But our primary\nfocus will be on game engines capable of producing first-person shooters,\nthird-person action/platform games, racing games, fighting games and the\nlike.\n1.2.1 Video Games as Soft Real-Time Simulations\nMosttwo-andthree-dimensionalvideogamesareexamplesofwhatcomputer\nscientists would call soft real-time interactive agent-based computer simulations.\nLet’s break this phrase down in order to better understand what it means.\nIn most video games, some subset of the real world—or an imaginary\nworld—is modeled mathematicallysothatitcanbemanipulatedbyacomputer.\nThe model is an approximation to and a simplification of reality (even if it’s\nanimaginary reality), because it is clearly impractical to include every detail\ndowntothelevelofatomsorquarks. Hence,themathematicalmodelisa sim-\nulationoftherealorimaginedgameworld. Approximationandsimplification\nare two of the game developer’s most powerful tools. When used skillfully,\neven a greatly simplified model can sometimes be almost indistinguishable\nfrom reality—and a lot more fun.\nAnagent-based simulation is one in which a number of distinct entities\nknownas“agents”interact. Thisfitsthedescriptionofmostthree-dimensional\ncomputergamesverywell,wheretheagentsarevehicles,characters,fireballs,\npower dots and so on. Given the agent-based nature of most games, it should\ncomeasnosurprisethatmostgamesnowadaysareimplementedinanobject-\noriented, or at least loosely object-based, programming language.\n10 1. Introduction\nAll interactive video games are temporal simulations, meaning that the vir-\ntual game world model is dynamic—the state of the game world changes over\ntimeasthegame’seventsandstoryunfold. Avideogamemustalsorespondto\nunpredictableinputsfromitshumanplayer(s)—thus interactivetemporalsimu-\nlations. Finally, most video games present their stories and respond to player\ninput in real time, making them interactive real-time simulations. One notable\nexception is in the category of turn-based games like computerized chess or\nturn-basedstrategygames. Buteventhesetypesofgamesusuallyprovidethe\nuser with some form of real-time graphical user interface. So for the purposes\nof this book, we’ll assume that all video games have at least somereal-time\nconstraints.\nAt the core of every real-time system is the concept of a deadline. An ob-\nvious example in video games is the requirement that the screen be updated\nat least 24 times per second in order to provide the illusion of motion. (Most\ngamesrenderthescreenat30or60framespersecondbecausethesearemulti-\nplesofanNTSCmonitor’srefreshrate.) Ofcourse,therearemanyotherkinds\nof deadlines in video games as well. A physics simulation may need to be up-\ndated 120 times per second in order to remain stable. A character’s artificial\nintelligence system may need to “think” at least once every second to prevent\nthe appearance of stupidity. The audio library may need to be called at least\nonce every 1/60 second in order to keep the audio buffers filled and prevent\naudible glitches.\nA “soft” real-time system is one in which missed deadlines are not catas-\ntrophic. Hence,allvideogamesare softreal-timesystems —iftheframeratedies,\nthe human player generally doesn’t! Contrast this with a hardreal-timesystem,\nin which a missed deadline could mean severe injury to or even the death of a\nhumanoperator. Theavionicssysteminahelicopterorthecontrol-rodsystem\nin a nuclear power plant are examples of hard real-time systems.\nMathematicalmodelscanbe analytic ornumerical. Forexample,theanalytic\n(closed-form) mathematical model of a rigid body falling under the influence\nof constant acceleration due to gravity is typically written as follows:\ny(t) =1\n2gt2+v0t+y0. (1.1)\nAnanalyticmodelcanbeevaluatedforanyvalueofitsindependentvariables,\nsuch as the time tin the above equation, given only the initial conditions v0\nandy0and the constant g. Such models are very convenient when they can be\nfound. However, many problems in mathematics have no closed-form solu-\ntion. And in video games, where the user’s input is unpredictable, we cannot\nhope to model the entire game analytically.\nA numerical model of the same rigid body under gravity can be expressed",7671
09-1.3 What Is a Game Engine.pdf,09-1.3 What Is a Game Engine,"1.3. What Is a Game Engine? 11\nas follows:\ny(t+∆t) =F(y(t),˙y(t),¨y(t), . . .). (1.2)\nThat is, the height of the rigid body at some future time ( t+∆t) can be found\nasafunctionoftheheightanditsfirst, second, andpossiblyhigher-ordertime\nderivatives at the current time t. Numerical simulations are typically imple-\nmented by running calculations repeatedly, in order to determine the state of\nthe system at each discrete time step. Games work in the same way. A main\n“game loop” runs repeatedly, and during each iteration of the loop, various\ngame systems such as artificial intelligence, game logic, physics simulations\nand so on are given a chance to calculate or update their state for the next\ndiscrete time step. The results are then “rendered” by displaying graphics,\nemitting sound and possibly producing other outputs such as force-feedback\non the joypad.\n1.3 What Is a Game Engine?\nThe term “game engine” arose in the mid-1990s in reference to first-person\nshooter (FPS) games like the insanely popular Doomby id Software. Doom\nwas architected with a reasonably well-defined separation between its core\nsoftware components (such as the three-dimensional graphics rendering sys-\ntem, the collision detection system or the audio system) and the art assets,\ngameworldsandrulesofplaythatcomprisedtheplayer’sgamingexperience.\nThe value of this separation became evident as developers began licensing\ngames and retooling them into new products by creating new art, world lay-\nouts,weapons,characters,vehiclesandgameruleswithonlyminimalchanges\nto the “engine” software. This marked the birth of the “mod community”—\na group of individual gamers and small independent studios that built new\ngames by modifying existing games, using free toolkits provided by the orig-\ninal developers.\nTowards the end of the 1990s, some games like Quake III Arena andUnreal\nweredesignedwithreuseand“modding”inmind. Enginesweremadehighly\ncustomizable via scripting languages like id’s Quake C, and engine licensing\nbegantobeaviablesecondaryrevenuestreamforthedeveloperswhocreated\nthem. Today,gamedeveloperscanlicenseagameengineandreusesignificant\nportions of its key software components in order to build games. While this\npractice still involves considerable investment in custom software engineer-\ning, it can be much more economical than developing all of the core engine\ncomponents in-house.\n12 1. Introduction\nCan be “modded” to \nbuild any game in a \nspecific genreCan be used to build any \ngame imaginableCannot be used to bu ild \nmore than one gameCan be customized to \nmake very similar games\nUnity,\nUnre al Engine 4,\nSource Engine, ...Hydro Thunder \nEngineProbably \nimpossiblePacManQuake III \nEngine\nFigure 1.1. Game engine reusability gamut.\nThelinebetweenagameanditsengineisoftenblurry. Someenginesmake\na reasonably clear distinction, while others make almost no attempt to sepa-\nrate the two. In one game, the rendering code might “know” specifically how\ntodrawanorc. Inanothergame, therenderingenginemightprovidegeneral-\npurpose material and shading facilities, and “orc-ness” might be defined en-\ntirely in data. No studio makes a perfectly clear separation between the game\nand the engine, which is understandable considering that the definitions of\nthese two components often shift as the game’s design solidifies.\nArguably a data-driven architecture is what differentiates a game engine\nfrom a piece of software that is a game but not an engine. When a game\ncontains hard-coded logic or game rules, or employs special-case code to ren-\nder specific types of game objects, it becomes difficult or impossible to reuse\nthat software to make a different game. We should probably reserve the term\n“game engine” for software that is extensible and can be used as the founda-\ntion for many different games without major modification.\nClearly this is not a black-and-white distinction. We can think of a gamut\nof reusability onto which every engine falls. Figure 1.1 takes a stab at the lo-\ncations of some well-known games/engines along this gamut.\nOne would think that a game engine could be something akin to Apple\nQuickTime or Microsoft Windows Media Player—a general-purpose piece of\nsoftware capable of playing virtually anygame content imaginable. However,\nthis ideal has not yet been achieved (and may never be). Most game engines\nare carefully crafted and fine-tuned to run a particular game on a particular\nhardware platform. And even the most general-purpose multiplatform en-\ngines are really only suitable for building games in one particular genre, such\nas first-person shooters or racing games. It’s safe to say that the more general-\npurpose a game engine or middleware component is, the less optimal it is for\nrunning a particular game on a particular platform.\nThis phenomenon occurs because designing any efficient piece of soft-\nware invariably entails making trade-offs, and those trade-offs are based on\nassumptions about how the software will be used and/or about the target",5069
10-1.4 Engine Differences across Genres.pdf,10-1.4 Engine Differences across Genres,"1.4. Engine Differences across Genres 13\nhardware on which it will run. For example, a rendering engine that was de-\nsigned to handle intimate indoor environments probably won’t be very good\nat rendering vast outdoor environments. The indoor engine might use a bi-\nnaryspacepartitioning(BSP) treeorportalsystem toensurethatno geometry\nisdrawnthatisbeingoccludedbywallsorobjectsthatareclosertothecamera.\nTheoutdoorengine,ontheotherhand,mightusealess-exactocclusionmech-\nanism, or none at all, but it probably makes aggressive use of level-of-detail\n(LOD) techniques to ensure that distant objects are rendered with a minimum\nnumberoftriangles,whileusinghigh-resolutiontrianglemeshesforgeometry\nthat is close to the camera.\nThe advent of ever-faster computer hardware and specialized graphics\ncards, along with ever-more-efficient rendering algorithms and data struc-\ntures, is beginning to soften the differences between the graphics engines of\ndifferentgenres. Itisnowpossibletouseafirst-personshooterenginetobuild\na strategy game, for example. However, the trade-off between generality and\noptimality still exists. A game can always be made more impressive by fine-\ntuning the engine to the specific requirements and constraints of a particular\ngame and/or hardware platform.\n1.4 Engine Differences across Genres\nGame engines are typically somewhat genre specific. An engine designed for\natwo-personfightinggameinaboxingringwillbeverydifferentfromamas-\nsivelymultiplayeronlinegame(MMOG)engineorafirst-personshooter(FPS)\nengineorareal-timestrategy(RTS)engine. However,thereisalsoagreatdeal\nof overlap—all 3D games, regardless of genre, require some form of low-level\nuser input from the joypad, keyboard and/or mouse, some form of 3D mesh\nrendering, some form of heads-up display (HUD) including text rendering in\na variety of fonts, a powerful audio system, and the list goes on. So while\nthe Unreal Engine, for example, was designed for first-person shooter games,\nit has been used successfully to construct games in a number of other genres\nas well, including the wildly popular third-person shooter franchise Gears of\nWarby Epic Games, the hit action-adventure games in the Batman: Arkham se-\nries by Rocksteady Studios, the well-known fighting game Tekken7 by Bandai\nNamco Studios, and the first three role-playing third-person shooter games in\ntheMassEffect series by BioWare.\nLet’s take a look at some of the most common game genres and explore\nsome examples of the technology requirements particular to each.\n14 1. Introduction\nFigure 1.2. Overwatch by Blizzard Entertainment (Xbox One, PlayStation 4, Windows). (See Color\nPlate I.)\n1.4.1 First-Person Shooters (FPS)\nThe first-person shooter (FPS) genre is typified by games like Quake,Un-\nreal Tournament, Half-Life ,Battlefield, Destiny,Titanfall andOverwatch (see Fig-\nure1.2). Thesegameshavehistoricallyinvolvedrelativelyslowon-footroam-\ning of a potentially large but primarily corridor-based world. However, mod-\nern first-person shooters can take place in a wide variety of virtual environ-\nments including vast open outdoor areas and confined indoor areas. Modern\nFPStraversal mechanicscaninclude on-footlocomotion, rail-confinedor free-\nroaming ground vehicles, hovercraft, boats and aircraft. For an overview of\nthis genre, see http://en.wikipedia.org/wiki/First-person_shooter.\nFirst-persongamesaretypicallysomeofthemosttechnologicallychalleng-\ning to build, probably rivaled in complexity only by third-person shooters,\naction-platformer games, and massively multiplayer games. This is because\nfirst-person shooters aim to provide their players with the illusion of being\nimmersed in a detailed, hyperrealistic world. It is not surprising that many of\nthe game industry’s big technological innovations arose out of the games in\nthis genre.\nFirst-person shooters typically focus on technologies such as:\n• efficient rendering of large 3D virtual worlds;\n1.4. Engine Differences across Genres 15\n• a responsive camera control/aiming mechanic;\n• high-fidelity animations of the player’s virtual arms and weapons;\n• a wide range of powerful handheld weaponry;\n• a forgiving player character motion and collision model, which often\ngives these games a “floaty” feel;\n• high-fidelity animations and artificial intelligence for the non-player\ncharacters (NPCs)—the player’s enemies and allies; and\n• small-scale online multiplayer capabilities (typically supporting be-\ntween 10 and 100 simultaneous players), and the ubiquitous “death\nmatch” gameplay mode.\nThe rendering technology employed by first-person shooters is almost al-\nways highly optimized and carefully tuned to the particular type of environ-\nment being rendered. For example, indoor “dungeon crawl” games often em-\nploy binary space partitioning trees or portal-based rendering systems. Out-\ndoor FPS games use other kinds of rendering optimizations such as occlusion\nculling, or an offline sectorization of the game world with manual or auto-\nmated specification of which target sectors are visible from each source sector.\nOf course, immersing a player in a hyperrealistic game world requires\nmuchmorethanjustoptimizedhigh-qualitygraphicstechnology. Thecharac-\nter animations, audio and music, rigid body physics, in-game cinematics and\nmyriad other technologies must all be cutting-edge in a first-person shooter.\nSo this genre has some of the most stringent and broad technology require-\nments in the industry.\n1.4.2 Platformers and Other Third-Person Games\n“Platformer”isthetermappliedtothird-personcharacter-basedactiongames\nwhere jumping from platform to platform is the primary gameplay mechanic.\nTypical games from the 2D era include Space Panic, Donkey Kong ,Pitfall!and\nSuper Mario Brothers. The 3D era includes platformers like Super Mario 64 ,\nCrash Bandicoot, Rayman 2 ,Sonic the Hedgehog, the Jak and Daxter series (Fig-\nure 1.3), the Ratchet & Clank series and Super Mario Galaxy . See http://en.\nwikipedia.org/wiki/Platformer for an in-depth discussion of this genre.\nIn terms of their technological requirements, platformers can usually be\nlumped together with third-person shooters and third-person action/adven-\nture games like Just Cause 2, Gears of War 4 (Figure 1.4), the Uncharted series,\ntheResidentEvil series,the TheLastofUs series,RedDeadRedemption2,andthe\nlist goes on.\n16 1. Introduction\nThird-person character-based games have a lot in common with first-per-\nsonshooters,butagreatdealmoreemphasisisplacedonthemaincharacter’s\nabilities and locomotion modes. In addition, high-fidelity full-body charac-\nter animations are required for the player’s avatar, as opposed to the some-\nwhat less-taxing animation requirements of the “floating arms” in a typical\nFPSgame. It’simportanttonoteherethatalmostallfirst-personshootershave\nan online multiplayer component, so a full-body player avatar must be ren-\ndered in addition to the first-person arms. However, the fidelity of these FPS\nplayeravatarsisusuallynotcomparabletothefidelityofthenon-playerchar-\nactersinthesesamegames; norcanitbecomparedtothefidelity oftheplayer\navatar in a third-person game.\nInaplatformer,themaincharacterisoftencartoon-likeandnotparticularly\nrealistic or high-resolution. However, third-person shooters often feature a\nhighly realistic humanoid player character. In both cases, the player character\ntypically has a very rich set of actions and animations.\nSome of the technologies specifically focused on by games in this genre\ninclude:\nFigure 1.3. Jak II by Naughty Dog (Jak, Daxter, Jak and Daxter, and Jak II © 2003, 2013/™ SIE. Created\nand developed by Naughty Dog, PlayStation 2.) (See Color Plate II.)\n1.4. Engine Differences across Genres 17\nFigure 1.4. Gears of War 4 by The Coalition (Xbox One). (See Color Plate III.)\n• movingplatforms,ladders,ropes,trellisesandotherinterestinglocomo-\ntion modes;\n• puzzle-like environmental elements;\n• a third-person “follow camera” which stays focused on the player char-\nacter and whose rotation is typically controlled by the human player via\nthe right joypad stick (on a console) or the mouse (on a PC—note that\nwhile there are a number of popular third-person shooters on a PC, the\nplatformer genre exists almost exclusively on consoles); and\n• acomplexcameracollisionsystemforensuringthattheviewpointnever\n“clips” through background geometry or dynamic foreground objects.\n1.4.3 Fighting Games\nFighting games are typically two-player games involving humanoid charac-\nters pummeling each other in a ring of some sort. The genre is typified by\ngames like Soul Calibur andTekken 3 (see Figure 1.5). The Wikipedia page\nhttp://en.wikipedia.org/wiki/Fighting_game provides an overview of this\ngenre.\nTraditionally games in the fighting genre have focused their technology\nefforts on:\n18 1. Introduction\n• a rich set of fighting animations;\n• accurate hit detection;\n• a user input system capable of detecting complex button and joystick\ncombinations; and\n• crowds, but otherwise relatively static backgrounds.\nSince the 3D world in these games is small and the camera is centered\non the action at all times, historically these games have had little or no need\nfor world subdivision or occlusion culling. They would likewise not be ex-\npected to employ advanced three-dimensional audio propagation models, for\nexample.\nModern fighting games like EA’s Fight Night Round 4 and NetherRealm\nStudios’ Injustice2 (Figure1.6)haveuppedthetechnologicalantewithfeatures\nlike:\n• high-definition character graphics;\n• realistic skin shaders with subsurface scattering and sweat effects;\n• photo-realistic lighting and particle effects;\n• high-fidelity character animations; and\nFigure 1.5. Tekken 3 by Namco (PlayStation). (See Color Plate IV.)\n1.4. Engine Differences across Genres 19\n• physics-based cloth and hair simulations for the characters.\nIt’s important to note that some fighting games like Ninja Theory’s Heav-\nenlySword andForHonor byUbisoftMontrealtakeplaceinalarge-scalevirtual\nworld,notaconfinedarena. Infact,manypeopleconsiderthistobeaseparate\ngenre, sometimes called a brawler. This kind of fighting game can have tech-\nnical requirements more akin to those of a third-person shooter or a strategy\ngame.\n1.4.4 Racing Games\nThe racing genre encompasses all games whose primary task is driving a\ncar or other vehicle on some kind of track. The genre has many subcat-\negories. Simulation-focused racing games (“sims”) aim to provide a driv-\ning experience that is as realistic as possible (e.g., Gran Turismo ). Arcade\nracers favor over-the-top fun over realism (e.g., San Francisco Rush ,Cruis’n\nUSA,Hydro Thunder). One subgenre explores the subculture of street rac-\ning with tricked out consumer vehicles (e.g., Need for Speed ,Juiced). Kart\nracing is a subcategory in which popular characters from platformer games\nor cartoon characters from TV are re-cast as the drivers of whacky vehicles\n(e.g.,Mario Kart ,Jak X,Freaky Flyers ). Racing games need not always in-\nvolve time-based competition. Some kart racing games, for example, offer\nFigure 1.6. Injustice 2 by NetherRealm Studios (PlayStation 4, Xbox One, Android, iOS, Microsoft\nWindows). (See Color Plate V.)\n20 1. Introduction\nFigure 1.7. Gran Turismo Sport by Polyphony Digital (PlayStation 4). (See Color Plate VI.)\nmodes in which players shoot at one another, collect loot or engage in a va-\nriety of other timed and untimed tasks. For a discussion of this genre, see\nhttp://en.wikipedia.org/wiki/Racing_game.\nA racing game is often very linear, much like older FPS games. However,\ntravel speed is generally much faster than in an FPS. Therefore, more focus is\nplaced on very long corridor-based tracks, or looped tracks, sometimes with\nvarious alternate routes and secret short-cuts. Racing games usually focus all\ntheir graphic detail on the vehicles, track and immediate surroundings. As an\nexampleofthis,Figure1.7showsascreenshotfromthelatestinstallmentinthe\nwell-known Gran Turismo racing game series, Gran Turismo Sport, developed\nbyPolyphonyDigitalandpublishedbySonyInteractiveEntertainment. How-\never, kart racers also devote significant rendering and animation bandwidth\nto the characters driving the vehicles.\nSome of the technological properties of a typical racing game include the\nfollowing techniques:\n• Various “tricks” are used when rendering distant background elements,\nsuchasemployingtwo-dimensionalcardsfortrees,hillsandmountains.\n• The track is often broken down into relatively simple two-dimensional\nregions called “sectors.” These data structures are used to optimize ren-\ndering and visibility determination, to aid in artificial intelligence and\npathfindingfornon-human-controlledvehicles,andtosolvemanyother\ntechnical problems.\n• The camera typically follows behind the vehicle for a third-person per-\n1.4. Engine Differences across Genres 21\nFigure 1.8. Age of Empires by Ensemble Studios (Windows). (See Color Plate VII.)\nspective, or is sometimes situated inside the cockpit first-person style.\n• When the track involves tunnels and other “tight” spaces, a good deal\nof effort is often put into ensuring that the camera does not collide with\nbackground geometry.\n1.4.5 Strategy Games\nThemodernstrategygamegenrewasarguablydefinedby DuneII:TheBuilding\nof a Dynasty (1992). Other games in this genre include Warcraft ,Command &\nConquer, AgeofEmpires andStarcraft . Inthisgenre,theplayerdeploysthebattle\nunitsinhisorherarsenalstrategicallyacrossalargeplayingfieldinanattempt\nto overwhelm his or her opponent. The game world is typically displayed\nat an oblique top-down viewing angle. A distinction is often made between\nturn-based strategy games and real-time strategy (RTS). For a discussion of\nthis genre, see https://en.wikipedia.org/wiki/Strategy_video_game.\nThestrategygameplayerisusuallypreventedfromsignificantlychanging\nthe viewing angle in order to see across large distances. This restriction per-\nmits developers to employ various optimizations in the rendering engine of a\nstrategy game.\n22 1. Introduction\nFigure 1.9. Total War: Warhammer 2 by Creative Assembly (Windows). (See Color Plate VIII.)\nOlder games in the genre employed a grid-based (cell-based) world con-\nstruction,andanorthographicprojectionwasusedtogreatlysimplifytheren-\nderer. For example, Figure 1.8 shows a screenshot from the classic strategy\ngameAge of Empires .\nModern strategy games sometimes use perspective projection and a true\n3D world, but they may still employ a grid layout system to ensure that units\nandbackgroundelements, suchasbuildings, alignwithoneanotherproperly.\nA popular example, TotalWar: Warhammer2 , is shown in Figure 1.9.\nSome other common practices in strategy games include the following\ntechniques:\n• Each unit is relatively low-res, so that the game can support large num-\nbers of them on-screen at once.\n• Height-field terrain is usually the canvas upon which the game is de-\nsigned and played.\n• The player is often allowed to build new structures on the terrain in ad-\ndition to deploying his or her forces.\n• User interaction is typically via single-click and area-based selection of\nunits, plus menus or toolbars containing commands, equipment, unit\ntypes, building types, etc.\n1.4. Engine Differences across Genres 23\nFigure 1.10. World of Warcraft by Blizzard Entertainment (Windows, MacOS). (See Color Plate IX.)\n1.4.6 Massively Multiplayer Online Games (MMOG)\nThe massively multiplayer online game (MMOG or just MMO) genre is typ-\nified by games like Guild Wars 2 (AreaNet/NCsoft), EverQuest (989 Studios/\nSOE),WorldofWarcraft (Blizzard) and StarWarsGalaxies (SOE/Lucas Arts), to\nname a few. An MMO is defined as any game that supports huge numbers\nof simultaneous players (from thousands to hundreds of thousands), usually\nall playing in one very large, persistent virtual world (i.e., a world whose in-\nternal state persists for very long periods of time, far beyond that of any one\nplayer’s gameplay session). Otherwise, the gameplay experience of an MMO\nis often similar to that of their small-scale multiplayer counterparts. Subcate-\ngoriesofthisgenreincludeMMOrole-playinggames(MMORPG),MMOreal-\ntime strategy games (MMORTS) and MMO first-person shooters (MMOFPS).\nForadiscussionofthisgenre,seehttp://en.wikipedia.org/wiki/MMOG. Fig-\nure 1.10 shows a screenshot from the hugely popular MMORPG WorldofWar-\ncraft.\nAt the heart of all MMOGs is a very powerful battery of servers. These\nserversmaintaintheauthoritativestateofthegameworld,manageuserssign-\ninginandoutofthegame, provideinter-userchatorvoice-over-IP(VoIP)ser-\nvicesand more. Almost all MMOGsrequireusersto paysome kindof regular\n24 1. Introduction\nsubscriptionfeeinordertoplay,andtheymayoffermicro-transactionswithin\nthe game world or out-of-game as well. Hence, perhaps the most important\nrole of the central server is to handle the billing and micro-transactions which\nserve as the game developer’s primary source of revenue.\nGraphicsfidelityinanMMOisalmostalwayslowerthanitsnon-massively\nmultiplayer counterparts, as a result of the huge world sizes and extremely\nlarge numbers of users supported by these kinds of games.\nFigure 1.11 shows a screen from Bungie’s latest FPS game, Destiny 2 . This\ngame has been called an MMOFPS because it incorporates some aspects of\nthe MMO genre. However, Bungie prefers to call it a “shared world” game\nbecauseunlikeatraditionalMMO,inwhichaplayercanseeandinteractwith\nliterally any other player on a particular server, Destiny provides “on-the-fly\nmatch-making.” Thispermitstheplayertointeractonlywiththeotherplayers\nwith whom they have been matched by the server; this matchmaking system\nhasbeensignificantlyimprovedfor Destiny2 . AlsounlikeatraditionalMMO,\nthegraphicsfidelityin Destiny2 isonparwithfirst-andthird-personshooters.\nWe should note here that the game PlayerUnknown’sBattlegrounds (PUBG)\nhas recently popularized a subgenre known as battleroyale . This type of game\nblursthelinebetweenregularmultiplayershootersandmassivelymultiplayer\nonline games, because they typically pit on the order of 100 players against\neachotherinanonlineworld,employingasurvival-based“lastmanstanding”\ngameplay style.\nFigure 1.11. Destiny 2 by Bungie, © 2018 Bungie Inc. (Xbox One, PlayStation 4, PC) (See Color Plate X.)\n1.4. Engine Differences across Genres 25\n1.4.7 Player-Authored Content\nAs social media takes off, games are becoming more and more collaborative\nin nature. A recent trend in game design is toward player-authoredcontent. For\nexample, Media Molecule’s LittleBigPlanet,™ LittleBigPlanet™ 2 (Figure 1.12)\nandLittleBigPlanet™3: TheJourneyHome aretechnically puzzleplatformers , but\ntheir most notable and unique feature is that they encourage players to create,\npublishandsharetheirowngameworlds. MediaMolecule’slatestinstallment\nin this engaging genre is Dreams for the PlayStation 4 (Figure 1.13).\nPerhaps the most popular game today in the player-created content genre\nisMinecraft (Figure 1.14). The brilliance of this game lies in its simplicity:\nMinecraft game worlds are constructed from simple cubic voxel-like elements\nmapped with low-resolution textures to mimic various materials. Blocks can\nbe solid, or they can contain items such as torches, anvils, signs, fences and\npanes of glass. The game world is populated with one or more player charac-\nters, animals such as chickens and pigs, and various “mobs”—good guys like\nvillagers and bad guys like zombies and the ubiquitous creepers who sneak up\non unsuspecting players and explode (only scant moments after warning the\nplayer with the “hiss” of a burning fuse).\nPlayers can create a randomized world in Minecraft and then dig into the\ngenerated terrain to create tunnels and caverns. They can also construct their\nown structures, ranging from simple terrain and foliage to vast and complex\nFigure 1.12. LittleBigPlanet™ 2 by Media Molecule, © 2014 Sony Interactive Entertainment (PlaySta-\ntion 3). (See Color Plate XI.)\n26 1. Introduction\nFigure 1.13. Dreams by Media Molecule, © 2017 Sony Computer Computer Europe (PlayStation 4).\n(See Color Plate XII.)\nbuildings and machinery. Perhaps the biggest stroke of genius in Minecraft\nisredstone . This material serves as “wiring,” allowing players to lay down\ncircuitry that controls pistons, hoppers, mine carts and other dynamic ele-\nments in the game. As a result, players can create virtually anything they can\nimagine,andthensharetheirworldswiththeirfriendsbyhostingaserverand\ninviting them to play online.\nFigure 1.14. Minecraft by Markus “Notch” Persson / Mojang AB (Windows, MacOS, Xbox 360, PlaySta-\ntion 3, PlayStation Vita, iOS). (See Color Plate XIII.)\n1.4. Engine Differences across Genres 27\n1.4.8 Virtual, Augmented and Mixed Reality\nVirtual, augmented and mixed reality are exciting new technologies that aim\ntoimmersetheviewerina3Dworldthatiseitherentirelygeneratedbyacom-\nputer, or is augmented by computer-generated imagery. These technologies\nhavemanyapplicationsoutsidethegameindustry,buttheyhavealsobecome\nviable platforms for a wide range of gaming content.\n1.4.8.1 Virtual Reality\nVirtual reality (VR) can be defined as an immersive multimedia or computer-\nsimulated reality that simulates the user’s presence in an environment that is\neitheraplaceintherealworldorinanimaginaryworld. Computer-generated\nVR (CG VR) is a subset of this technology in which the virtual world is exclu-\nsively generated via computer graphics. The user views this virtual environ-\nment by donning a headset such as HTC Vive, Oculus Rift, Sony PlayStation\nVR, Samsung Gear VR or Google Daydream View. The headset displays the\ncontent directly in front of the user’s eyes; the system also tracks the move-\nment of the headset in the real world, so that the virtual camera’s movements\ncan be perfectly matched to those of the person wearing the headset. The user\ntypicallyholdsdevicesinhisorherhandswhichallowthesystemtotrackthe\nmovements of each hand. This allows the user to interact in the virtual world:\nObjects can be pushed, picked up or thrown, for example.\n1.4.8.2 Augmented and Mixed Reality\nThe terms augmented reality (AR) and mixed reality (MR) are often confused\nor used interchangeably. Both technologies present the user with a view of\nthe real world, but with computer graphics used to enhance the experience.\nIn both technologies, a viewing device like a smart phone, tablet or tech-\nenhanced pair of glasses displays a real-time or static view of a real-world\nscene, and computer graphics are overlaid on top of this image. In real-time\nAR and MR systems, accelerometers in the viewing device permit the virtual\ncamera’s movements to track the movements of the device, producing the il-\nlusion that the device is simply a window through which we are viewing the\nactual world, and hence giving the overlaid computer graphics a strong sense\nof realism.\nSome people make a distinction between these two technologies by us-\ning the term “augmented reality” to describe technologies in which computer\ngraphicsareoverlaidonalive,directorindirectviewoftherealworld,butare\nnot anchored to it. The term “mixed reality,” on the other hand, is more often\n28 1. Introduction\napplied to the use of computer graphics to render imaginary objects which\nare anchored to the real world and appear to exist within it. However, this\ndistinction is by no means universally accepted.\nHere are a few examples of AR technology in action:\n• The U.S. Army provides its soldiers with improved tactical awareness\nusing a system dubbed “tactical augmented reality” (TAR)—it overlays\na video-game-like heads-up display (HUD) complete with a mini-map\nand object markers onto the soldier’s view of the real world (https://\nyoutu.be/x8p19j8C6VI).\n• In 2015, Disney demonstrated some cool AR technology that renders a\n3D cartoon character on top of a sheet of real-world paper on which a\n2D version of the character is colored with a crayon (https://youtu.be/\nSWzurBQ81CM).\n• PepsiCo also pranked commuting Londoners with an AR-enabled bus\nstop. People sitting in the bus stop enclosure were treated to AR images\nof a prowling tiger, a meteor crashing, and an alien tentacle grabbing\nunwitting passers by off the street (https://youtu.be/Go9rf9GmYpM).\nAnd here are a few examples of MR:\n• Starting with Android 8.1, the camera app on the Pixel 1 and Pixel 2\nsupports AR Stickers, a fun feature that allows users to place animated\n3D objects and characters into videos and photos.\n• Microsoft’s HoloLens is another example of mixed reality. It overlays\nworld-anchored graphics onto a live video image, and can be used for a\nwide range of applications including education and training, engineer-\ning, health care, and entertainment.\n1.4.8.3 VR/AR/MR Games\nThegameindustryiscurrentlyexperimentingwithVRandAR/MRtechnolo-\ngies,andistryingtofinditsfootingwithinthesenewmedia. Sometraditional\n3D games have been “ported” to VR, yielding very interesting, if not particu-\nlarly innovative, experiences. But perhaps more exciting, entirely new game\ngenresarestartingtoemerge,offeringgameplayexperiencesthatcouldnotbe\nachieved without VR or AR/MR.\nFor example, Job Simulator by Owlchemy Labs plunges the user into a vir-\ntual job museum run by robots, and asks them to perform tongue-in-cheek\napproximations of various real-world jobs, making use of game mechanics\n1.4. Engine Differences across Genres 29\nthat simply wouldn’t work on a non-VR platform. Owlchemy’s next install-\nment,VacationSimulator, applies the same whimsical sense of humour and art\nstyle to a world in which the robots of Job Simulator invite the player to relax\nand perform various tasks. Figure 1.15 shows a screenshot from another in-\nnovative (and somewhat disturbing!) game for HTC Vive called Accounting ,\nfrom the creators of “Rick & Morty” and The Stanley Parable.\n1.4.8.4 VR Game Engines\nVR game engines are technologically similar in many respects to first-person\nshooter engines, and in fact many FPS-capable engines such as Unity and Un-\nreal Engine support VR “out of the box.” However, VR games differ from FPS\ngames in a number of significant ways:\n•Stereoscopic rendering. A VR game needs to render the scene twice, once\nfor each eye. This doubles the number of graphics primitives that must\nbe rendered, although other aspects of the graphics pipeline such as vis-\nibility culling can be performed only once per frame, since the eyes are\nreasonablyclosetogether. Assuch,aVRgameisn’tquiteasexpensiveto\nrender as the same game would be to render in split-screen multiplayer\nmode,buttheprincipleofrenderingeachframetwicefromtwo(slightly)\ndifferent virtual cameras is the same.\n•Very high frame rate. Studies have shown that VR running at below 90\nFigure 1.15. Accounting by Squanchtendo and Crows Crows Crows (HTC Vive). (See Color Plate XIV.)\n30 1. Introduction\nframes per second is likely to induce disorientation, nausea, and other\nnegative user effects. This means that not only do VR systems need to\nrender the scene twice per frame, they need to do so at 90+ FPS. This is\nwhy VR games and applications are generally required to run on high-\npowered CPU and GPU hardware.\n•Navigation issues. In an FPS game, the player can simply walk around\nthe game world with the joypad or the WASD keys. In a VR game, a\nsmall amount of movement can be realized by the user physically walk-\ning around in the real world, but the safe physical play area is typically\nquite small (the size of a small bathroom or closet). Travelling by “fly-\ning” tends to induce nausea as well, so most games opt for a point-and-\nclickteleportationmechanismtomovethevirtualplayer/cameraacross\nlarger distances. Various real-world devices have also been conceived\nthat allow a VR user to “walk” in place with their feet in order to move\naround in a VR world.\nOf course, VR makes up for these limitations somewhat by enabling new user\ninteraction paradigms that aren’t possible in traditional video games. For ex-\nample,\n• users can reach in the real world to touch, pick up and throw objects in\nthe virtual world;\n• a player can dodge an attack in the virtual world by dodging physically\nin the real world;\n• new user interface opportunities are possible, such as having floating\nmenusattachedtoone’svirtualhands,orseeingagame’screditswritten\non a whiteboard in the virtual world;\n• a player can even pick up a pair of virtual VR goggles and place them\nonto his or her head, thereby transporting them into a “nested” VR\nworld—an effect that might best be called “VR-ception.”\n1.4.8.5 Location-Based Entertainment\nGames like Pokémon Go neither overlay graphics onto an image of the real\nworld, nor do they generate a completely immersive virtual world. However,\nthe user’s view of the computer-generated world of Pokémon Go does react\nto movements of the user’s phone or tablet, much like a 360-degree video.\nAnd the game is aware of your actual location in the real world, prompting\nyou to go searching for Pokémon in nearby parks, malls and restaurants. This\nkindof game can’treallybe calledAR/MR, but neitherdoes it fallinto the VR\ncategory. Such a game might be better described as a form of location-based",29511
11-1.5 Game Engine Survey.pdf,11-1.5 Game Engine Survey,"1.5. Game Engine Survey 31\nentertainment, althoughsomepeopledousetheARmonikerforthesekindsof\ngames.\n1.4.9 Other Genres\nThere are of course many other game genres which we won’t cover in depth\nhere. Some examples include:\n• sports, with subgenres for each major sport (football, baseball, soccer,\ngolf, etc.);\n• role-playing games (RPG);\n• God games, like Populous andBlack& White ;\n• environmental/social simulation games, like SimCity orTheSims ;\n• puzzle games like Tetris;\n• conversions of non-electronic games, like chess, card games, go, etc.;\n• web-based games, such as those offered at Electronic Arts’ Pogo site;\nand the list goes on.\nWehaveseenthateachgamegenrehasitsownparticulartechnologicalre-\nquirements. Thisexplainswhygameengineshavetraditionallydifferedquite\na bit from genre to genre. However, there is also a great deal of technologi-\ncal overlap between genres, especially within the context of a single hardware\nplatform. With the advent of more and more powerful hardware, differences\nbetween genres that arose because of optimization concerns are beginning to\nevaporate. Itisthereforebecomingincreasinglypossibletoreusethesameen-\ngine technology across disparate genres, and even across disparate hardware\nplatforms.\n1.5 Game Engine Survey\n1.5.1 The Quake Family of Engines\nThe first 3D first-person shooter (FPS) game is generally accepted to be Castle\nWolfenstein3D (1992). WrittenbyidSoftwareofTexasforthePCplatform,this\ngame led the game industry in a new and exciting direction. id Software went\non to create Doom,Quake,Quake II andQuake III . All of these engines are very\nsimilarinarchitecture, andIwillrefertothemastheQuakefamily ofengines.\nQuake technology has been used to create many other games and even other\nengines. For example, the lineage of Medal of Honor for the PC platform goes\nsomething like this:\n32 1. Introduction\n•QuakeIII (id Software);\n•Sin(Ritual);\n•F.A.K.K.2 (Ritual);\n•Medalof Honor: Allied Assault (2015 & Dreamworks Interactive); and\n•Medalof Honor: Pacific Assault (Electronic Arts, Los Angeles).\nMany other games based on Quake technology follow equally circuitous\npaths through many different games and studios. In fact, Valve’s Source en-\ngine (used to create the Half-Life games) also has distant roots in Quake tech-\nnology.\nTheQuakeandQuake II source code is freely available, and the original\nQuake engines are reasonably well architected and “clean” (although they are\nof course a bit outdated and written entirely in C). These code bases serve\nas great examples of how industrial-strength game engines are built. The\nfull source code to QuakeandQuake II is available at https://github.com/\nid-Software/Quake-2.\nIf you own the Quake and/or Quake II games, you can actually build the\ncode using Microsoft Visual Studio and run the game under the debugger us-\ning the real game assets from the disk. This can be incredibly instructive. You\ncan set breakpoints, run the game and then analyze how the engine actually\nworks by stepping through the code. I highly recommend downloading one\nor both of these engines and analyzing the source code in this manner.\n1.5.2 Unreal Engine\nEpicGames,Inc.burstontotheFPSscenein1998withitslegendarygame Un-\nreal. Since then, the Unreal Engine has become a major competitor to Quake\ntechnology in the FPS space. Unreal Engine 2 (UE2) is the basis for Unreal\nTournament 2004 (UT2004) and has been used for countless “mods,” univer-\nsityprojectsandcommercialgames. UnrealEngine4(UE4)isthelatestevolu-\ntionary step, boasting some of the best tools and richest engine feature sets in\ntheindustry, includingaconvenientandpowerfulgraphicaluserinterfacefor\ncreating shaders and a graphical user interface for game logic programming\ncalledBlueprints (previously known as Kismet).\nThe Unreal Engine has become known for its extensive feature set and co-\nhesive, easy-to-use tools. The Unreal Engine is not perfect, and most devel-\nopers modify it in various ways to run their game optimally on a particular\nhardware platform. However, Unreal is an incredibly powerful prototyping\ntool and commercial game development platform, and it can be used to build\nvirtually any 3D first-person or third-person game (not to mention games in\n1.5. Game Engine Survey 33\nother genres as well). Many exciting games in all sorts of genres have been\ndeveloped with UE4, including Rimeby Tequila Works, Genesis: AlphaOne by\nRadiationBlue, AWayOut byHazelightStudios,and Crackdown3 byMicrosoft\nStudios.\nThe Unreal Developer Network (UDN) provides a rich set of documenta-\ntion and other information about all released versions of the Unreal Engine\n(see http://udn.epicgames.com/Main/WebHome.html). Some documenta-\ntionisfreelyavailable. However,accesstothefulldocumentationforthelatest\nversion of the Unreal Engine is generally restricted to licensees of the engine.\nThere are plenty of other useful websites and wikis that cover the Unreal En-\ngine. One popular one is http://www.beyondunreal.com.\nThankfully,EpicnowoffersfullaccesstoUnrealEngine4,sourcecodeand\nall, for a low monthly subscription fee plus a cut of your game’s profits if it\nships. This makes UE4 a viable choice for small independent game studios.\n1.5.3 The Half-Life Source Engine\nSourceisthegameenginethatdrivesthewell-known Half-Life2 anditssequels\nHL2: EpisodeOne andHL2: EpisodeTwo ,TeamFortress2 andPortal(shipped to-\ngetherunderthetitle TheOrangeBox). Sourceisahigh-qualityengine,rivaling\nUnreal Engine 4 in terms of graphics capabilities and tool set.\n1.5.4 DICE’s Frostbite\nTheFrostbiteenginegrewoutofDICE’seffortstocreateagameenginefor Bat-\ntlefield Bad Company in 2006. Since then, the Frostbite engine has become the\nmostwidelyadoptedenginewithinElectronicArts(EA);itisusedbymanyof\nEA’skeyfranchisesincluding MassEffect, Battlefield ,NeedforSpeed ,DragonAge,\nandStar Wars Battlefront II. Frostbite boasts a powerful unified asset creation\ntoolcalledFrostEd,apowerfultoolspipelineknownasBackendServices, and\na powerful runtime game engine. It is a proprietary engine, so it’s unfortu-\nnately unavailable for use by developers outside EA.\n1.5.5 Rockstar Advanced Game Engine (RAGE)\nRAGE is the engine that drives the insanely popular Grand Theft Auto V. De-\nveloped by RAGE Technology Group, a division of Rockstar Games’ Rockstar\nSanDiegostudio,RAGEhasbeenusedbyRockstarGames’internalstudiosto\ndevelopgamesforPlayStation4,XboxOne,PlayStation3,Xbox360,Wii,Win-\ndows,andMacOS.Othergamesdevelopedonthisproprietaryengineinclude\nGrand Theft Auto IV, Red Dead Redemption andMax Payne 3.\n34 1. Introduction\n1.5.6 CRYENGINE\nCrytek originally developed their powerful game engine known as CRYEN-\nGINE as a tech demo for NVIDIA. When the potential of the technology was\nrecognized, Crytek turned the demo into a complete game and Far Cry was\nborn. Since then, many games have been made with CRYENGINE including\nCrysis,Codename Kingdoms, Ryse: Son of Rome, and Everyone’s Gone to the Rap-\nture. Over the years the engine has evolved into what is now Crytek’s latest\noffering, CRYENGINE V. This powerful game development platform offers a\npowerfulsuiteofasset-creationtoolsandafeature-richruntimeenginefeatur-\ning high-quality real-time graphics. CRYENGINE can be used to make games\ntargeting a wide range of platforms including Xbox One, Xbox 360, PlaySta-\ntion 4, PlayStation 3, Wii U, Linux, iOS and Android.\n1.5.7 Sony’s PhyreEngine\nInan efforttomake developing gamesfor Sony’s PlayStation3 platform more\naccessible,SonyintroducedPhyreEngineattheGameDeveloper’sConference\n(GDC) in 2008. As of 2013, PhyreEngine has evolved into a powerful and full-\nfeatured game engine, supporting an impressive array of features including\nadvancedlightinganddeferredrendering. Ithasbeenusedbymanystudiosto\nbuild over 90 published titles, including thatgamecompany’s hits flOw,Flower\nandJourney, and Coldwood Interactive’s Unravel. PhyreEngine now supports\nSony’s PlayStation 4, PlayStation 3, PlayStation 2, PlayStation Vita and PSP\nplatforms. PhyreEngine gives developers access to the power of the highly\nparallel Cell architecture on PS3 and the advanced compute capabilities of the\nPS4, along with a streamlined new world editor and other powerful game de-\nvelopment tools. It is available free of charge to any licensed Sony developer\nas part of the PlayStation SDK.\n1.5.8 Microsoft’s XNA Game Studio\nMicrosoft’s XNA Game Studio is an easy-to-use and highly accessible game\ndevelopment platform based on the C# language and the Common Language\nRuntime (CLR), and aimed at encouraging players to create their own games\nandsharethemwiththeonlinegamingcommunity,muchasYouTubeencour-\nages the creation and sharing of home-made videos.\nFor better or worse, Microsoft officially retired XNA in 2014. However,\ndevelopers can port their XNA games to iOS, Android, Mac OS X, Linux\nand Windows 8 Metro via an open-source implementation of XNA called\nMonoGame. For more details, see https://www.windowscentral.com/xna-\ndead-long-live-xna.\n1.5. Game Engine Survey 35\n1.5.9 Unity\nUnity is a powerful cross-platform game development environment and run-\ntime engine supporting a wide range of platforms. Using Unity, developers\ncan deploy their games on mobile platforms (e.g., Apple iOS, Google An-\ndroid), consoles (Microsoft Xbox 360 and Xbox One, Sony PlayStation 3 and\nPlayStation 4, and Nintendo Wii, Wii U), handheld gaming platforms (e.g.,\nPlaystation Vita, Nintendo Switch), desktop computers (Microsoft Windows,\nApple Macintosh and Linux), TV boxes (e.g., Android TV and tvOS) and vir-\ntual reality (VR) systems (e.g., Oculus Rift, Steam VR, Gear VR).\nUnity’s primary design goals are ease of development and cross-platform\ngame deployment. As such, Unity provides an easy-to-use integrated editor\nenvironment, in which you can create and manipulate the assets and entities\nthatmakeupyourgameworldandquicklypreviewyourgameinactionright\nthere in the editor, or directly on your target hardware. Unity also provides a\npowerful suite of tools for analyzing and optimizing your game on each tar-\nget platform, a comprehensive asset conditioning pipeline, and the ability to\nmanagetheperformance-qualitytrade-offuniquelyoneachdeploymentplat-\nform. UnitysupportsscriptinginJavaScript, C#orBoo; apowerfulanimation\nsystem supporting animation retargeting (the ability to play an animation au-\nthored for one character on a totally different character); and support for net-\nworked multiplayer games.\nUnityhasbeenusedtocreateawidevarietyofpublishedgames,including\nDeusEx: TheFall byN-Fusion/EidosMontreal, HollowKnight byTeamCherry,\nand the subversive retro-style Cuphead by StudioMDHR. The Webby Award\nwinning short film Adamwas rendered in real time using Unity.\n1.5.10 Other Commercial Game Engines\nTherearelotsofothercommercialgameenginesoutthere. Althoughindiede-\nvelopers may not have the budget to purchase an engine, many of these prod-\nucts have great online documentation and/or wikis that can serve as a great\nsourceofinformationaboutgameenginesandgameprogrammingingeneral.\nForexample,checkouttheTombstoneengine(http://tombstoneengine.com/)\nbyTerathonSoftware,theLeadWerksengine(https://www.leadwerks.com/),\nand HeroEngine by Idea Fabrik, PLC (http://www.heroengine.com/).\n1.5.11 Proprietary In-House Engines\nMany companies build and maintain proprietary in-house game engines.\nElectronic Arts built many of its RTS games on a proprietary engine called\nSage, developed at Westwood Studios. Naughty Dog’s Crash Bandicoot and\n36 1. Introduction\nJak and Daxter franchises were built on a proprietary engine custom tailored\nto the PlayStation and PlayStation 2. For the Uncharted series, Naughty Dog\ndeveloped a brand new engine custom tailored to the PlayStation 3 hardware.\nThisengineevolvedandwasultimatelyusedtocreateNaughtyDog’s TheLast\nof Usseries on the PlayStation 3 and PlayStation 4, as well as its most recent\nreleases, Uncharted 4: A Thief’s End andUncharted: The Lost Legacy. And of\ncourse, most commercially licensed game engines like Quake, Source, Unreal\nEngine 4 and CRYENGINE all started out as proprietary in-house engines.\n1.5.12 Open Source Engines\nOpen source 3D game engines are engines built by amateur and professional\ngame developers and provided online for free. The term “open source” typi-\ncallyimpliesthatsourcecodeisfreelyavailableandthatasomewhatopende-\nvelopment model is employed, meaning almost anyone can contribute code.\nLicensing, if it exists at all, is often provided under the Gnu Public License\n(GPL) or Lesser Gnu Public License (LGPL). The former permits code to be\nfreely used by anyone, as long as their code is also freely available; the latter\nallows the code to be used even in proprietary for-profit applications. Lots of\nother free and semi-free licensing schemes are also available for open source\nprojects.\nThere are a staggering number of open source engines available on the\nweb. Some are quite good, some are mediocre and some are just plain aw-\nful! The list of game engines provided online at http://en.wikipedia.org/\nwiki/List_of_game_engines will give you a feel for the sheer number of en-\ngines that are out there. (The list at http://www.worldofleveldesign.com/\ncategories/level_design_tutorials/recommended-game-engines.php is a bit\nmore digestible.) Both of these lists include both open-source and commer-\ncial game engines.\nOGRE is a well-architected, easy-to-learn and easy-to-use 3D rendering\nengine. It boasts a fully featured 3D renderer including advanced lighting\nand shadows, a good skeletal character animation system, a two-dimensional\noverlay system for heads-up displays and graphical user interfaces, and a\npost-processing system for full-screen effects like bloom. OGRE is, by its\nauthors’ own admission, not a full game engine, but it does provide\nmany of the foundational components required by pretty much any game\nengine.\nSome other well-known open source engines are listed here:\n• Panda3D is a script-based engine. The engine’s primary interface is the\nPython custom scripting language. It is designed to make prototyping\n1.5. Game Engine Survey 37\n3D games and virtual worlds convenient and fast.\n• Yake is a game engine built on top of OGRE.\n• Crystal Space is a game engine with an extensible modular architecture.\n• Torque and Irrlicht are also well-known open-source game engines.\n• Whilenottechnicallyopen-source,theLumberyardenginedoesprovide\nsourcecodetoitsdevelopers. Itisafreecross-platformenginedeveloped\nby Amazon, and based on the CRYENGINE architecture.\n1.5.13 2D Game Engines for Non-programmers\nTwo-dimensional games have become incredibly popular with the recent ex-\nplosion of casual web gaming and mobile gaming on platforms like Apple\niPhone/iPad and Google Android. A number of popular game/multimedia\nauthoring toolkits have become available, enabling small game studios and\nindependent developers to create 2D games for these platforms. These\ntoolkits emphasize ease of use and allow users to employ a graphical user\ninterface to create a game rather than requiring the use of a programming lan-\nguage. Check out this YouTube video to get a feel for the kinds of games\nyou can create with these toolkits: https://www.youtube.com/watch?v=\n3Zq1yo0lxOU\n•MultimediaFusion2 (http://www.clickteam.com/website/worldisa2D\ngame/multimedia authoring toolkit developed by Clickteam. Fusion is\nused by industry professionals to create games, screen savers and other\nmultimediaapplications. Fusionanditssimplercounterpart,TheGames\nFactory 2, are also used by educational camps like PlanetBravo (http:\n//www.planetbravo.com) to teach kids about game development and\nprogramming/logic concepts. Fusion supports the iOS, Android, Flash,\nand Java platforms.\n•GameSaladCreator (http://gamesalad.com/creator) is another graphical\ngame/multimediaauthoringtoolkitaimedatnon-programmers,similar\nin many respects to Fusion.\n•Scratch (http://scratch.mit.edu) is an authoring toolkit and graphical\nprogramminglanguagethatcanbeusedtocreateinteractivedemosand\nsimple games. It is a great way for young people to learn about pro-\ngramming concepts such as conditionals, loops and event-driven pro-\ngramming. Scratchwasdevelopedin2003bytheLifelongKindergarten\ngroup, led by Mitchel Resnick at the MIT Media Lab.",16474
12-1.6 Runtime Engine Architecture.pdf,12-1.6 Runtime Engine Architecture,"38 1. Introduction\n1.6 Runtime Engine Architecture\nA game engine generally consists of a tool suite and a runtime component.\nWe’ll explore the architecture of the runtime piece first and then get into tool\narchitecture in the following section.\nFigure 1.16 shows all of the major runtime components that make up a\ntypical3Dgameengine. Yeah,it’s big!Andthisdiagramdoesn’tevenaccount\nfor all the tools. Game engines are definitely large software systems.\nLikeallsoftwaresystems,gameenginesarebuiltin layers. Normallyupper\nlayersdependonlowerlayers,butnotviceversa. Whenalowerlayerdepends\nuponahigherlayer,wecallthisa circulardependency . Dependencycyclesareto\nbe avoided in any software system, because they lead to undesirable coupling\nbetween systems, make the software untestable and inhibit code reuse. This\nis especially true for a large-scale system like a game engine.\nWhat follows is a brief overview of the components shown in the diagram\nin Figure 1.16. The rest of this book will be spent investigating each of these\ncomponents in a great deal more depth and learning how these components\nare usually integrated into a functional whole.\n1.6.1 Target Hardware\nThetargethardwarelayerrepresentsthecomputersystemorconsoleonwhich\nthe game will run. Typical platforms include Microsoft Windows, Linux and\nMacOS-based PCs; mobile platforms like the Apple iPhone and iPad, An-\ndroidsmartphonesandtablets,Sony’sPlayStationVitaandAmazon’sKindle\nFire (among others); and game consoles like Microsoft’s Xbox, Xbox 360 and\nXbox One, Sony’s PlayStation, PlayStation 2, PlayStation 3 and PlayStation 4,\nand Nintendo’s DS, GameCube, Wii, Wii U and Switch. Most of the topics in\nthis book are platform-agnostic, but we’ll also touch on some of the design\nconsiderations peculiar to PC or console development, where the distinctions\nare relevant.\n1.6.2 Device Drivers\nDevice drivers are low-level software components provided by the operating\nsystem or hardware vendor. Drivers manage hardware resources and shield\nthe operating system and upper engine layers from the details of communi-\ncating with the myriad variants of hardware devices available.\n1.6. Runtime Engine Architecture 39\nFigure 1.16. Runtime game engine architecture.\n40 1. Introduction\n1.6.3 Operating System\nOn a PC, the operating system (OS) is running all the time. It orchestrates the\nexecution of multiple programs on a single computer, one of which is your\ngame. Operating systems like Microsoft Windows employ a time-sliced ap-\nproach to sharing the hardware with multiple running programs, known as\npreemptive multitasking. This means that a PC game can never assume it has\nfull control of the hardware—it must “play nice” with other programs in the\nsystem.\nOnearlyconsoles,theoperatingsystem,ifoneexistedatall,wasjustathin\nlibrary layer that was compiled directly into your game executable. On those\nearly systems, the game “owned” the entire machine while it was running.\nHowever, on modern consoles this is no longer the case. The operating sys-\ntem on the Xbox 360, PlayStation 3, Xbox One and PlayStation 4 can interrupt\nthe execution of your game, or take over certain system resources, in order to\ndisplay online messages, or to allow the player to pause the game and bring\nup the PS4’s “XMB” user interface or the Xbox One’s dashboard, for example.\nOn the PS4 and Xbox One, the OS is continually running background tasks,\nsuch as recording video of your playthrough in case you decide to share it via\nthe PS4’s Share button, or downloading games, patches and DLC, so you can\nhave fun playing a game while you wait. So the gap between console and PC\ndevelopment is gradually closing (for better or for worse).\n1.6.4 Third-Party SDKs and Middleware\nMost game engines leverage a number of third-party software development\nkits (SDKs) and middleware, as shown in Figure 1.17. The functional or class-\nbased interface provided by an SDK is often called an application program-\nming interface (API). We will look at a few examples.\nFigure 1.17. Third-party SDK layer.\n1.6.4.1 Data Structures and Algorithms\nLike any software system, games depend heavily on container data structures\nand algorithms to manipulate them. Here are a few examples of third-party\nlibraries that provide these kinds of services:\n1.6. Runtime Engine Architecture 41\n•Boost. Boost is a powerful data structures and algorithms library, de-\nsigned in the style of the standard C++ library and its predecessor, the\nstandard template library (STL). (The online documentation for Boost is\nalso a great place to learn about computer science in general!)\n•Folly. Folly is a library used at Facebook whose goal is to extend the\nstandard C++ library and Boost with all sorts of useful facilities, with an\nemphasis on maximizing code performance.\n•Loki. Loki is a powerful generic programming template library which is\nexceedingly good at making your brain hurt!\nThe C++ Standard Library and STL\nThe C++ standard library also provides many of the same kinds of facil-\nities found in third-party libraries like Boost. The subset of the standard li-\nbrary that implements generic container classes such as std::vector and\nstd::list is often referred to as the standardtemplatelibrary (STL), although\nthisistechnicallyabitofamisnomer: Thestandardtemplatelibrarywaswrit-\nten by Alexander Stepanov and David Musser in the days before the C++ lan-\nguage was standardized. Much of this library’s functionality was absorbed\ninto what is now the C++ standard library. When we use the term STL in this\nbook, it’s usually in the context of the subset of the C++ standard library that\nprovides generic container classes, not the original STL.\n1.6.4.2 Graphics\nMost game rendering engines are built on top of a hardware interface library,\nsuch as the following:\n•Glideis the 3D graphics SDK for the old Voodoo graphics cards. This\nSDK was popular prior to the era of hardware transform and lighting\n(hardware T&L) which began with DirectX 7.\n•OpenGL is a widely used portable 3D graphics SDK.\n•DirectX is Microsoft’s 3D graphics SDK and primary rival to OpenGL.\n•libgcmis a low-level direct interface to the PlayStation 3’s RSX graphics\nhardware,whichwasprovidedbySonyasamoreefficientalternativeto\nOpenGL.\n•Edgeis a powerful and highly efficient rendering and animation engine\nproduced by Naughty Dog and Sony for the PlayStation 3 and used by\na number of first- and third-party game studios.\n42 1. Introduction\n•Vulkanis a low-level library created by the Khronos™ Group which en-\nablesgameprogrammerstosubmitrenderingbatchesandGPGPUcom-\npute jobs directly to the GPU as command lists, and provides them with\nfine-grained control over memory and other resources that are shared\nbetween the CPU and GPU. (See Section 4.11 for more on GPGPU pro-\ngramming.)\n1.6.4.3 Collision and Physics\nCollision detection and rigid body dynamics (known simply as “physics”\nin the game development community) are provided by the following well-\nknown SDKs:\n•Havokis a popular industrial-strength physics and collision engine.\n•PhysXis another popular industrial-strength physics and collision en-\ngine, available for free download from NVIDIA.\n•Open Dynamics Engine (ODE) is a well-known open source physics/col-\nlision package.\n1.6.4.4 Character Animation\nAnumberofcommercialanimationpackagesexist,includingbutcertainlynot\nlimited to the following:\n•Granny. Rad Game Tools’ popular Granny toolkit includes robust 3D\nmodel and animation exporters for all the major 3D modeling and ani-\nmation packages like Maya, 3D Studio MAX, etc., a runtime library for\nreading and manipulating the exported model and animation data, and\na powerful runtime animation system. In my opinion, the Granny SDK\nhas the best-designed and most logical animation API of any I’ve seen,\ncommercial or proprietary, especially its excellent handling of time.\n•Havok Animation . The line between physics and animation is becoming\nincreasingly blurred as characters become more and more realistic. The\ncompany that makes the popular Havok physics SDK decided to cre-\nateacomplimentaryanimationSDK,whichmakesbridgingthephysics-\nanimation gap much easier than it ever has been.\n•OrbisAnim. The OrbisAnim library produced for the PS4 by SN Systems\nin conjunction with the ICE and game teams at Naughty Dog, the Tools\nand Technology group of Sony Interactive Entertainment, and Sony’s\nAdvanced Technology Group in Europe includes a powerful and effi-\ncient animation engine and an efficient geometry-processing engine for\nrendering.\n1.6. Runtime Engine Architecture 43\n1.6.4.5 Biomechanical Character Models\n•Endorphin and Euphoria. These are animation packages that produce\ncharacter motion using advanced biomechanical models of realistic hu-\nman movement.\nAs we mentioned previously, the line between character animation and\nphysics is beginning to blur. Packages like Havok Animation try to marry\nphysics and animation in a traditional manner, with a human animator pro-\nviding the majority of the motion through a tool like Maya and with physics\naugmentingthatmotionatruntime. ButafirmcalledNaturalMotionLtd. has\nproducedaproductthatattemptstoredefinehowcharactermotionishandled\nin games and other forms of digital media.\nIts first product, Endorphin, is a Maya plug-in that permits animators to\nrun full biomechanical simulations on characters and export the resulting an-\nimations as if they had been hand animated. The biomechanical model ac-\ncounts for center of gravity, the character’s weight distribution, and detailed\nknowledge of how a real human balances and moves under the influence of\ngravity and other forces.\nItssecondproduct, Euphoria, isareal-timeversionofEndorphinintended\nto produce physically and biomechanically accurate character motion at run-\ntime under the influence of unpredictable forces.\n1.6.5 Platform Independence Layer\nMost game engines are required to be capable of running on more than one\nhardware platform. Companies like Electronic Arts and ActivisionBlizzard\nInc., for example, always target their games at a wide variety of platforms be-\ncause it exposes their games to the largest possible market. Typically, the only\ngame studios that do not target at least two different platforms per game are\nfirst-party studios, like Sony’s Naughty Dog and Insomniac studios. There-\nfore, most game engines are architected with a platform independence layer,\nlike the one shown in Figure 1.18. This layer sits atop the hardware, drivers,\noperating system and other third-party software and shields the rest of the\nengine from the majority of knowledge of the underlying platform by “wrap-\nping”certaininterfacefunctionsincustomfunctionsoverwhichyou,thegame\ndeveloper, will have control on every target platform.\nThere are two primary reasons to “wrap” functions as part of your game\nengine’s platform independence layer like this: First, some application pro-\ngramming interfaces (APIs), like those provided by the operating system, or\neven some functions in older “standard” libraries like the C standard library,\n44 1. Introduction\ndiffer significantly from platform to platform; wrapping these functions pro-\nvides the rest of your engine with a consistent API across all of your targeted\nplatforms. Second,evenwhenusingafullycross-platformlibrarysuchasHa-\nvok, you might want to insulate yourself from future changes, such as transi-\ntioning your engine to a different collision/physics library in the future.\nFigure 1.18. Platform independence layer.\n1.6.6 Core Systems\nEverygameengine,andreallyeverylarge,complexC++softwareapplication,\nrequires a grab bag of useful software utilities. We’ll categorize these under\nthe label “core systems.” A typical core systems layer is shown in Figure 1.19.\nHere are a few examples of the facilities the core layer usually provides:\n•Assertions are lines of error-checking code that are inserted to catch log-\nical mistakes and violations of the programmer’s original assumptions.\nAssertion checks are usually stripped out of the final production build\nof the game. (Assertions are covered in Section 3.2.3.3.)\n•Memory management. Virtually every game engine implements its own\ncustom memory allocation system(s) to ensure high-speed allocations\nand deallocations and to limit the negative effects of memory fragmen-\ntation (see Section 6.2.1).\n•Math library. Games are by their nature highly mathematics-intensive.\nAs such, every game engine has at least one, if not many, math libraries.\nThese libraries provide facilities for vector and matrix math, quaternion\nrotations, trigonometry, geometric operations with lines, rays, spheres,\nfrusta, etc., spline manipulation, numerical integration, solving systems\nof equations and whatever other facilities the game programmers re-\nquire.\n•Custom data structures and algorithms. Unless an engine’s designers de-\ncided to rely entirely on third-party packages such as Boost and Folly,\na suite of tools for managing fundamental data structures (linked lists,\ndynamic arrays, binary trees, hash maps, etc.) and algorithms (search,\nsort, etc.) is usually required. These are often hand coded to minimize\nor eliminate dynamic memory allocation and to ensure optimal runtime\nperformance on the target platform(s).\n1.6. Runtime Engine Architecture 45\nFigure 1.19. Core engine systems.\nA detailed discussion of the most common core engine systems can be\nfound in Part II.\n1.6.7 Resource Manager\nPresent in every game engine in some form, the resource manager provides\na unified interface (or suite of interfaces) for accessing any and all types of\ngameassetsandotherengineinputdata. Someenginesdothisinahighlycen-\ntralized and consistent manner (e.g., Unreal’s packages, OGRE’s Resource-\nManager class). Other engines take an ad hoc approach, often leaving it up\nto the game programmer to directly access raw files on disk or within com-\npressed archives such as Quake’s PAK files. A typical resource manager layer\nis depicted in Figure 1.20.\nResources (Game Assets)\nResource ManagerTexture \nResourceMaterial \nResource3D Model \nResourceFont\nResourceCollision \nResourcePhysics\nParametersGame\nWorld/Mapetc.Skeleton \nResource\nFigure 1.20. Resource manager.\n1.6.8 Rendering Engine\nThe rendering engine is one of the largest and most complex components of\nanygameengine. Rendererscanbearchitectedinmanydifferentways. There\nis no one accepted way to do it, although as we’ll see, most modern rendering\nengines share some fundamental design philosophies, driven in large part by\nthe design of the 3D graphics hardware upon which they depend.\nOne common and effective approach to rendering engine design is to em-\nploy a layered architecture as follows.\n46 1. Introduction\n1.6.8.1 Low-Level Renderer\nThelow-level renderer , shown in Figure 1.21, encompasses all of the raw ren-\nderingfacilitiesoftheengine. Atthislevel, thedesignisfocusedonrendering\na collection of geometric primitives as quickly and richly as possible, without\nmuch regard for which portions of a scene may be visible. This component is\nbroken into various subcomponents, which are discussed below.\nLow-Leve l Renderer\nPrimitive\nSubmissionViewports & \nVirtual ScreensMaterial s & \nShaders\nTexture and \nSurface Mgmt.\nGraphics Device InterfaceStatic & Dynamic \nLightingCam eras Text & Fonts\nDebug Drawin g\n(Line s etc.)Skeletal Mesh \nRendering\nFigure 1.21. Low-level rendering engine.\nGraphics Device Interface\nGraphics SDKs, such as DirectX, OpenGL or Vulkan, require a reasonable\namount of code to be written just to enumerate the available graphics devices,\ninitialize them, set up render surfaces (back-buffer, stencil buffer, etc.) and so\non. This is typically handled by a component that I’ll call the graphics device\ninterface (although every engine uses its own terminology).\nFor a PC game engine, you also need code to integrate your renderer with\nthe Windows message loop. You typically write a “message pump” that ser-\nvicesWindowsmessageswhentheyarependingandotherwiserunsyourren-\nder loop over and over as fast as it can. This ties the game’s keyboard polling\nloop to the renderer’s screen update loop. This coupling is undesirable, but\nwith some effort it is possible to minimize the dependencies. We’ll explore\nthis topic in more depth later.\nOther Renderer Components\nThe other components in the low-level renderer cooperate in order to collect\nsubmissions of geometric primitives (sometimes called render packets ), such as\nmeshes, line lists, point lists, particles, terrain patches, text strings and what-\never else you want to draw, and render them as quickly as possible.\n1.6. Runtime Engine Architecture 47\nk\nFigure 1.22. A typical scene graph/spatial subdivision layer, for culling optimization.\nThelow-levelrendererusuallyprovidesaviewportabstractionwithanas-\nsociated camera-to-world matrix and 3D projection parameters, such as field\nofviewandthelocationofthenearandfarclipplanes. Thelow-levelrenderer\nalso manages the state of the graphics hardware and the game’s shaders via\nitsmaterial system and itsdynamic lighting system. Each submitted primitive is\nassociated with a material and is affected by ndynamic lights. The material\ndescribes the texture(s) used by the primitive, what device state settings need\nto be in force, and which vertex and pixel shader to use when rendering the\nprimitive. Thelights determine how dynamiclighting calculations will be ap-\nplied to the primitive. Lighting and shading is a complex topic. We’ll discuss\nthefundamentalsinChapter11,butthesetopicsarecoveredindepthinmany\nexcellent books on computer graphics, including [16], [49] and [2].\n1.6.8.2 Scene Graph/Culling Optimizations\nThe low-level renderer draws all of the geometry submitted to it, without\nmuch regard for whether or not that geometry is actually visible (other than\nback-facecullingandclippingtrianglestothecamerafrustum). Ahigher-level\ncomponent is usually needed in order to limit the number of primitives sub-\nmitted for rendering, based on some form of visibility determination. This\nlayer is shown in Figure 1.22.\nFor very small game worlds, a simple frustum cull (i.e., removing objects\nthat the camera cannot “see”) is probably all that is required. For larger game\nworlds, a more advanced spatial subdivision data structure might be used to\nimprove rendering efficiency by allowing the potentially visible set (PVS) of\nobjects to be determined very quickly. Spatial subdivisions can take many\nforms, including a binary space partitioning tree, a quadtree, an octree, a kd-\ntree or a sphere hierarchy. A spatial subdivision is sometimes called a scene\ngraph, althoughtechnicallythelatterisaparticularkindofdatastructureand\ndoesnotsubsumetheformer. Portalsorocclusioncullingmethodsmightalso\nbe applied in this layer of the rendering engine.\nIdeally, the low-level renderer should be completely agnostic to the type\nof spatial subdivision or scene graph being used. This permits different game\n48 1. Introduction\nFigure 1.23. Visual effects.\nteamstoreusetheprimitivesubmissioncodebuttocraftaPVSdetermination\nsystem that is specific to the needs of each team’s game. The design of the\nOGRE open source rendering engine (http://www.ogre3d.org) is a great ex-\nampleofthisprincipleinaction. OGREprovidesaplug-and-playscenegraph\narchitecture. Game developers can either select from a number of preimple-\nmentedscenegraphdesigns,ortheycanprovideacustomscenegraphimple-\nmentation.\n1.6.8.3 Visual Effects\nModerngameengines supportawide rangeofvisual effects, asshownin Fig-\nure 1.23, including:\n• particle systems (for smoke, fire, water splashes, etc.);\n• decal systems (for bullet holes, foot prints, etc.);\n• light mapping and environment mapping;\n• dynamic shadows; and\n• full-screen post effects, applied after the 3D scene has been rendered to\nan off-screen buffer.\nSome examples of full-screen post effects include:\n• high dynamic range (HDR) tone mapping and bloom;\n• full-screen anti-aliasing (FSAA); and\n• color correction and color-shift effects, including bleach bypass, satura-\ntion and desaturation effects, etc.\nIt is common for a game engine to have an effects system component that\nmanages the specialized rendering needs of particles, decals and other visual\neffects. The particle and decal systems are usually distinct components of the\nrendering engine and act as inputs to the low-level renderer. On the other\n1.6. Runtime Engine Architecture 49\nFront End\nHeads-Up Display \n(HUD)Full-Motion Video \n(FMV)\nIn-Game Menus In-Game GUIWrappers / Attract \nModeIn-Game Cinematics \n(IGC)\nFigure 1.24. Front end graphics.\nhand, light mapping, environment mapping and shadows are usually han-\ndledinternallywithintherenderingengineproper. Full-screenposteffectsare\neither implemented as an integral part of the renderer or as a separate compo-\nnent that operates on the renderer’s output buffers.\n1.6.8.4 Front End\nMost games employ some kind of 2D graphics overlaid on the 3D scene for\nvarious purposes. These include:\n• the game’s heads-up display (HUD);\n• in-game menus, a console and/or other development tools , which may or\nmay not be shipped with the final product; and\n• possibly an in-game graphical userinterface (GUI), allowing the player to\nmanipulate his or her character’s inventory, configure units for battle or\nperform other complex in-game tasks.\nThislayerisshowninFigure1.24. Two-dimensionalgraphicsliketheseare\nusually implemented by drawing textured quads (pairs of triangles) with an\northographic projection. Or they may be rendered in full 3D, with the quads\nbill-boarded so they always face the camera.\nWe’ve also included the full-motion video (FMV) system in this layer. This\nsystem is responsible for playing full-screen movies that have been recorded\nearlier (either rendered with the game’s rendering engine or using another\nrendering package).\nA related system is the in-game cinematics (IGC) system. This component\ntypically allows cinematic sequences to be choreographed within the game\nitself,infull3D.Forexample,astheplayerwalksthroughacity,aconversation\nbetween two key characters might be implemented as an in-game cinematic.\nIGCs may or may not include the player character(s). They may be done as a\ndeliberate cut-away during which the player has no control, or they may be\nsubtly integrated into the game without the human player even realizing that\n50 1. Introduction\nan IGC is taking place. Some games, such as Naughty Dog’s Uncharted 4: A\nThief’sEnd, have moved away from pre-rendered movies entirely, and display\nallcinematic moments in the game as real-time IGCs.\n1.6.9 Proﬁling and Debugging Tools\nFigure 1.25. Proﬁling\nand debugging tools.Games are real-time systems and, as such, game engineers often need to pro-\nfiletheperformanceoftheirgamesinordertooptimizeperformance. Inaddi-\ntion, memory resources are usually scarce, so developers make heavy use of\nmemory analysis tools as well. The profiling and debugging layer, shown\nin Figure 1.25, encompasses these tools and also includes in-game debug-\nging facilities, such as debug drawing, an in-game menu system or console\nand the ability to record and play back gameplay for testing and debugging\npurposes.\nThere are plenty of good general-purpose software profiling tools avail-\nable, including:\n• Intel’s VTune,\n• IBM’s Quantify andPurify(part of the PurifyPlus tool suite),\n•Insure++ by Parasoft, and\n•Valgrind by Julian Seward and the Valgrind development team.\nHowever, most game engines also incorporate a suite of custom profiling\nand debugging tools. For example, they might include one or more of the\nfollowing:\n• a mechanism for manually instrumenting the code, so that specific sec-\ntions of code can be timed;\n• a facility for displaying the profiling statistics on-screen while the game\nis running;\n• a facility for dumping performance stats to a text file or to an Excel\nspreadsheet;\n• afacilityfordetermininghowmuchmemoryisbeingusedbytheengine,\nand by each subsystem, including various on-screen displays;\n• the ability to dump memory usage, high water mark and leakage stats\nwhen the game terminates and/or during gameplay;\n• tools that allow debug print statements to be peppered throughout the\ncode, along with an ability to turn on or offdifferent categories of debug\noutput and control the level of verbosity of the output; and\n1.6. Runtime Engine Architecture 51\nFigure 1.26. Collision and physics subsystem.\n• theability to recordgame events and thenplay them back. This istough\nto get right, but when done properly it can be a very valuable tool for\ntracking down bugs.\nThe PlayStation 4 provides a powerful core dump facility to aid program-\nmers in debugging crashes. The PlayStation 4 is always recording the last 15\nseconds of gameplay video, to allow players to share their experiences via the\nShare button on the controller. Because of this, the PS4’s core dump facility\nautomatically provides programmers not only with a complete call stack of\nwhat the program was doing when it crashed, but also with a screenshot of\nthe moment of the crash and 15 seconds of video footage showing what was\nhappening just prior to the crash. Coredumps can be automatically uploaded\nto the game developer’s servers whenever the game crashes, even after the\ngame has shipped. These facilities revolutionize the tasks of crash analysis\nand repair.\n1.6.10 Collision and Physics\nCollision detection is important for every game. Without it, objects would\ninterpenetrate, and it would be impossible to interact with the virtual world\nin any reasonable way. Some games also include a realistic or semi-realistic\ndynamics simulation. We call this the “physics system” in the game industry,\nalthough the term rigid body dynamics is really more appropriate, because we\nare usually only concerned with the motion (kinematics) of rigid bodies and\nthe forces and torques (dynamics) that cause this motion to occur. This layer\nis depicted in Figure 1.26.\n52 1. Introduction\nCollision and physics are usually quite tightly coupled. This is because\nwhen collisions are detected, they are almost always resolved as part of the\nphysics integration and constraint satisfaction logic. Nowadays, very few\ngame companies write their own collision/physics engine. Instead, a third-\nparty SDK is typically integrated into the engine.\n•Havokis the gold standard in the industry today. It is feature-rich and\nperforms well across the boards.\n•PhysXby NVIDIA is another excellent collision and dynamics engine.\nIt was integrated into Unreal Engine 4 and is also available for free as\na stand-alone product for PC game development. PhysX was originally\ndesignedastheinterfacetoAgeia’sphysicsacceleratorchip. TheSDKis\nnow owned and distributed by NVIDIA, and the company has adapted\nPhysX to run on its latest GPUs.\nOpen source physics and collision engines are also available. Perhaps the\nbest-known of these is the Open Dynamics Engine (ODE). For more informa-\ntion, see http://www.ode.org. I-Collide, V-Collide and RAPID are other pop-\nular non-commercial collision detection engines. All three were developed at\nthe University of North Carolina (UNC). For more information, see http://\nwww.cs.unc.edu/~geom/I_COLLIDE/index.html and http://www.cs.unc.\nedu/geom/V_COLLIDE/index.html.\n1.6.11 Animation\nAny game that has organic or semi-organic characters (humans, animals, car-\ntooncharactersorevenrobots)needsananimationsystem. Therearefivebasic\ntypes of animation used in games:\n• sprite/texture animation,\n• rigid body hierarchy animation,\n• skeletal animation,\n• vertex animation, and\n• morph targets.\nSkeletal animation permits a detailed 3D character mesh to be posed by an\nanimator using a relatively simple system of bones. As the bones move, the\nvertices of the 3D mesh move with them. Although morph targets and vertex\nanimation are used in some engines, skeletal animation is the most prevalent\nanimationmethodingamestoday; assuch, itwillbeourprimaryfocusinthis\nbook. A typical skeletal animation system is shown in Figure 1.27.\n1.6. Runtime Engine Architecture 53\nYou’ll notice in Figure 1.16 that the skeletal mesh rendering component\nbridges the gap between the renderer and the animation system. There is a\ntight cooperation happening here, but the interface is very well defined. The\nanimation system produces a pose for every bone in the skeleton, and then\nthese poses are passed to the rendering engine as a palette of matrices. The\nrenderer transforms each vertex by the matrix or matrices in the palette, in\norder to generate a final blended vertex position. This process is known as\nskinning.\nThere is also a tight coupling between the animation and physics systems\nwhenragdolls are employed. A rag doll is a limp (often dead) animated char-\nacter, whose bodily motion is simulated by the physics system. The physics\nsystem determines the positions and orientations of the various parts of the\nbody by treating them as a constrained system of rigid bodies. The animation\nsystem calculates the palette of matrices required by the rendering engine in\norder to draw the character on-screen.\n1.6.12 Human Interface Devices (HID)\nFigure 1.28. The\nplayer input/output\nsystem, also known\nas the human in-\nterface device (HID)\nlayer.Every game needs to process input from the player, obtained from various\nhuman interface devices (HIDs) including:\n• the keyboard and mouse,\n• a joypad, or\n• other specialized game controllers, like steering wheels, fishing rods,\ndance pads, the Wiimote, etc.\nWe sometimes call this component the player I/O component, because\nSkeletal Animation\nAnimation\nDecompressionInverse\nKinematics (IK)Game-Specific\nPost-Processing\nSub-skeletal\nAnimationLERP and \nAdditive BlendingAnimation\nPlaybackAnimation State \nTree & Layers\nFigure 1.27. Skeletal animation subsystem.\n54 1. Introduction\nwe may also provide outputto the player through the HID, such as force-\nfeedback/ rumble on a joypad or the audio produced by the Wiimote. A typ-\nical HID layer is shown in Figure 1.28.\nThe HID engine component is sometimes architected to divorce the low-\nlevel details of the game controller(s) on a particular hardware platform from\nthe high-level game controls. It massages the raw data coming from the hard-\nware,introducingadeadzonearoundthecenterpointofeachjoypadstick,de-\nbouncing button-press inputs, detecting button-down and button-up events,\ninterpreting and smoothing accelerometer inputs (e.g., from the PlayStation\nDualshock controller) and more. It often provides a mechanism allowing the\nplayer to customize the mapping between physical controls and logical game\nfunctions. It sometimes also includes a system for detecting chords (multiple\nbuttons pressed together), sequences (buttons pressed in sequence within a\ncertain time limit) and gestures (sequences of inputs from the buttons, sticks,\naccelerometers, etc.).\n1.6.13 Audio\nFigure 1.29. Audio\nsubsystem.Audio is just as important as graphics in any game engine. Unfortunately, au-\ndio often gets less attention than rendering, physics, animation, AI and game-\nplay. Case in point: Programmers often develop their code with their speak-\nersturnedoff! (Infact, I’veknownquiteafewgameprogrammerswhodidn’t\nevenhavespeakers or headphones.) Nonetheless, no great game is complete\nwithout a stunning audio engine. The audio layer is depicted in Figure 1.29.\nAudio engines vary greatly in sophistication. Quake’s audio engine is\npretty basic, and game teams usually augment it with custom functionality\nor replace it with an in-house solution. Unreal Engine 4 provides a reason-\nably robust 3D audio rendering engine (discussed in detail in [45]), although\nits feature set is limited and many game teams will probably want to aug-\nment and customize it to provide advanced game-specific features. For Di-\nrectX platforms (PC, Xbox 360, Xbox One), Microsoft provides an excellent\nruntime audio engine called XAudio2. Electronic Arts has developed an ad-\nvanced, high-powered audio engine internally called SoundR!OT. In conjunc-\ntionwithfirst-partystudioslikeNaughtyDog,SonyInteractiveEntertainment\n(SIE)providesapowerful3DaudioenginecalledScream,whichhasbeenused\non a number of PS3 and PS4 titles including Naughty Dog’s Uncharted 4: A\nThief’sEnd andTheLastofUs: Remastered . However,evenifagameteamusesa\npreexistingaudioengine,everygamerequiresagreatdealofcustomsoftware\ndevelopment, integration work, fine-tuning and attention to detail in order to\nproduce high-quality audio in the final product.\n1.6. Runtime Engine Architecture 55\n1.6.14 Online Multiplayer/Networking\nMany games permit multiple human players to play within a single virtual\nworld. Multiplayer games come in at least four basic flavors:\n•Single-screenmultiplayer. Twoormorehumaninterfacedevices(joypads,\nkeyboards, mice, etc.) are connected to a single arcade machine, PC or\nconsole. Multiple player characters inhabit a single virtual world, and a\nsingle camera keeps all player characters in frame simultaneously. Ex-\namples of this style of multiplayer gaming include Smash Brothers, Lego\nStarWars andGauntlet.\n•Split-screen multiplayer. Multiple player characters inhabit a single vir-\ntual world, with multiple HIDs attached to a single game machine, but\neach with its own camera, and the screen is divided into sections so that\neach player can view his or her character.\n•Networked multiplayer. Multiple computers or consoles are networked\ntogether, with each machine hosting one of the players.\n•Massively multiplayer online games (MMOG) . Literally hundreds of thou-\nsands of users can be playing simultaneously within a giant, persistent,\nonline virtual world hosted by a powerful battery of central servers.\nFigure 1.30. Online\nmultiplayer net-\nworking subsystem.The multiplayer networking layer is shown in Figure 1.30.\nMultiplayer games are quite similar in many ways to their single-player\ncounterparts. However, support for multiple players can have a profound\nimpact on the design of certain game engine components. The game world\nobjectmodel,renderer,humaninputdevicesystem,playercontrolsystemand\nanimationsystemsareallaffected. Retrofittingmultiplayerfeaturesintoapre-\nexisting single-player engine is certainly not impossible, although it can be a\ndaunting task. Still, many game teams have done it successfully. That said, it\nis usually better to design multiplayer features from day one, if you have that\nluxury.\nItisinterestingtonotethatgoingtheotherway—convertingamultiplayer\ngame into a single-player game—is typically trivial. In fact, many game en-\nginestreatsingle-playermodeasaspecialcaseofamultiplayergame,inwhich\nthere happens to be only one player. The Quake engine is well known for its\nclient-on-top-of-server mode, in which a single executable, running on a single\nPC, acts both as the client and the server in single-player campaigns.\n1.6.15 Gameplay Foundation Systems\nThe term gameplay refers to the action that takes place in the game, the rules\nthatgovernthevirtualworldinwhichthegametakesplace,theabilitiesofthe\n56 1. Introduction\nGameplay  Found ations\nEvent/Messaging \nSyste mDynamic Game \nObject ModelScripting  Syste m\nWorld Loading  / \nStreamin gStatic World \nElementsReal-Time  Agent-\nBased  SimulationHigh-Level  Game  Flow  System/FS M\nHiera rchical  \nObject Att achmen t\nFigure 1.31. Gameplay foundation systems.\nplayercharacter(s)(knownas playermechanics)andoftheothercharactersand\nobjects in the world, and the goals and objectives of the player(s). Gameplay\nis typically implemented either in the native language in which the rest of the\nengineiswrittenorinahigh-levelscriptinglanguage—orsometimesboth. To\nbridge the gap between the gameplay code and the low-level engine systems\nthat we’ve discussed thus far, most game engines introduce a layer that I’ll\ncall thegameplay foundations layer (for lack of a standardized name). Shown\nin Figure 1.31, this layer provides a suite of core facilities, upon which game-\nspecific logic can be implemented conveniently.\n1.6.15.1 Game Worlds and Object Models\nThe gameplay foundations layer introduces the notion of a game world, con-\ntaining both static and dynamic elements. The contents of the world are usu-\nally modeled in an object-oriented manner (often, but not always, using an\nobject-oriented programming language). In this book, the collection of object\ntypes that make up a game is called the game object model . The game object\nmodel providesa real-timesimulation of aheterogeneouscollection of objects\nin the virtual game world.\nTypical types of game objects include:\n• staticbackgroundgeometry,likebuildings,roads,terrain(oftenaspecial\ncase), etc.;\n• dynamic rigid bodies, such as rocks, soda cans, chairs, etc.;\n• player characters (PC);\n• non-player characters (NPC);\n1.6. Runtime Engine Architecture 57\n• weapons;\n• projectiles;\n• vehicles;\n• lights (which may be present in the dynamic scene at runtime, or only\nused for static lighting offline);\n• cameras;\nand the list goes on.\nThe game world model is intimately tied to a softwareobjectmodel , and this\nmodelcanenduppervadingtheentireengine. Thetermsoftwareobjectmodel\nrefers to the set of language features, policies and conventions used to imple-\nment a piece of object-oriented software. In the context of game engines, the\nsoftware object model answers questions, such as:\n• Is your game engine designed in an object-oriented manner?\n• What language will you use? C? C++? Java? OCaml?\n• How will the static class hierarchy be organized? One giant monolithic\nhierarchy? Lots of loosely coupled components?\n• Willyouusetemplatesandpolicy-baseddesign, ortraditionalpolymor-\nphism?\n• How are objects referenced? Straight old pointers? Smart pointers?\nHandles?\n• How will objects be uniquely identified? By address in memory only?\nBy name? By a global unique identifier (GUID)?\n• How are the lifetimes of game objects managed?\n• How are the states of the game objects simulated over time?\nWe’ll explore software object models and game object models in consider-\nable depth in Section 16.2.\n1.6.15.2 Event System\nGame objects invariably need to communicate with one another. This can be\naccomplished in all sorts of ways. For example, the object sending the mes-\nsage might simply call a member function of the receiver object. An event-\ndrivenarchitecture,muchlikewhatonewouldfindinatypicalgraphicaluser\ninterface, is also a common approach to inter-object communication. In an\nevent-driven system, the sender creates a little data structure called an event\normessage, containing the message’s type and any argument data that are to\nbe sent. The event is passed to the receiver object by calling its event handler\nfunction. Eventscanalsobestoredinaqueueforhandlingatsomefuturetime.\n58 1. Introduction\n1.6.15.3 Scripting System\nMany game engines employ a scripting language in order to make devel-\nopment of game-specific gameplay rules and content easier and more rapid.\nWithout a scripting language, you must recompile and relink your game exe-\ncutable every time a change is made to the logic or data structures used in the\nengine. Butwhenascriptinglanguageisintegratedintoyourengine, changes\nto game logic and data can be made by modifying and reloading the script\ncode. Some engines allow script to be reloaded while the game continues to\nrun. Other engines require the game to be shut down prior to script recompi-\nlation. But either way, the turnaround time is still much faster than it would\nbe if you had to recompile and relink the game’s executable.\n1.6.15.4 Artiﬁcial Intelligence Foundations\nTraditionally, artificial intelligence has fallen squarely into the realm of game-\nspecific software—it was usually not considered part of the game engine per\nse. More recently, however, game companies have recognized patterns that\narise in almost every AI system, and these foundations are slowly starting to\nfall under the purview of the engine proper.\nFor example, a company called Kynogon developed a middleware SDK\nnamed Kynapse, which provides much of the low-level technology required\ntobuildcommerciallyviablegameAI.ThistechnologywaspurchasedbyAu-\ntodesk and has been superseded by a totally redesigned AI middleware pack-\nagecalledGamewareNavigation,designedbythesameengineeringteamthat\ninvented Kynapse. This SDK provides low-level AI building blocks such as\nnavmeshgeneration,pathfinding,staticanddynamicobjectavoidance,iden-\ntification of vulnerabilities within a play space (e.g., an open window from\nwhich an ambush could come) and a well-defined interface between AI and\nanimation.\n1.6.16 Game-Speciﬁc Subsystems\nOn top of the gameplay foundation layer and the other low-level engine com-\nponents, gameplay programmers and designers cooperate to implement the\nfeatures of the game itself. Gameplay systems are usually numerous, highly\nvaried and specific to the game being developed. As shown in Figure 1.32,\nthese systems include, but are certainly not limited to the mechanics of the\nplayer character, various in-game camera systems, artificial intelligence for\nthe control of non-player characters, weapon systems, vehicles and the list\ngoes on. If a clear line could be drawn between the engine and the game,",41731
13-1.7 Tools and the Asset Pipeline.pdf,13-1.7 Tools and the Asset Pipeline,"1.7. Tools and the Asset Pipeline 59\nit would lie between the game-specific subsystems and the gameplay foun-\ndations layer. Practically speaking, this line is never perfectly distinct. At\nleastsomegame-specificknowledgeinvariablyseepsdownthroughthegame-\nplayfoundationslayerandsometimesevenextendsintothecoreoftheengine\nitself.\n1.7 Tools and the Asset Pipeline\nAny game engine must be fed a great deal of data, in the form of game assets,\nconfiguration files, scripts and so on. Figure 1.33 depicts some of the types of\ngame assets typically found in modern game engines. The thicker dark-grey\narrows show how data flows from the tools used to create the original source\nassets all the way through to the game engine itself. The thinner light-grey\narrows show how the various types of assets refer to or use other assets.\n1.7.1 Digital Content Creation Tools\nGames are multimedia applications by nature. A game engine’s input data\ncomesin a wide variety of forms, from3D mesh datato texturebitmaps to an-\nimation data to audio files. All of this source data must be createdand manip-\nulated by artists. The tools that the artists use are called digitalcontentcreation\n(DCC) applications.\nADCCapplicationisusuallytargetedatthecreationofoneparticulartype\nof data—although some tools can produce multiple data types. For example,\nAutodesk’s Maya and 3ds Max and Pixologic’s ZBrush are prevalent in the\ncreation of both 3D meshes and animation data. Adobe’s Photoshop and its\nilk are aimed at creating and editing bitmaps (textures). SoundForge is a pop-\nular tool for creating audio clips. Some types of game data cannot be created\nusing an off-the-shelf DCC app. For example, most game engines provide a\ncustom editor for laying out game worlds. Still, some engines do make use of\nGAME- SPECIFIC SUB SYSTEMS\nGame-Specific Rendering\nTerrain RenderingWater Simulation \n& Renderingetc.Player Mechanics\nCollision Manifol d MovementState Machine & \nAnimationGame Cameras\nPlayer-Follow \nCameraDebug Fly-\nThrough CamFixed CamerasScripted/Animated \nCamerasAI\nSight Traces  & \nPerceptionPath Finding (A*)Goals & Decision-\nMakin gActions\n(Engine Interface)Camera- Relative \nContro ls (HID)Weapons Powe r-Ups etc. Vehicles Puzzles\nFigure 1.32. Game-speciﬁc subsystems.\n60 1. Introduction\nDigital Content Creation (DCC) To ols\nGame World\nGame \nObjectMesh\nSkeletal Hierarchy \nExporterSkel. \nHierarchy\nAnimation \nExporterAnimation \nCurves\nTGA\nTextureDXT Compression DXT \nTextur eWorld EditorGame Object \nDefinition T ool\nMaterialGame Obj. \nTemplate\nAnimation \nSet\nAnim ation Tree \nEditorAnim ation \nTree\nGame \nObject\nGame \nObject\nAsset \nConditioning \nPipeline\nGAM EWAV\nsoundAudio Manager \nTool\nSound \nBankMesh Exporter\nPhotoshopPhotoshop\nSound Forge or Audio ToolSound Forge or Audio Tool\nGame \nObjectMaya, 3DSMAX, etc.Maya, 3DSMA X, etc.\nCustom Material \nPlug-In\nHoud ini/Other Par ticle Tool Houdini/Other Particle Tool\nPartic le \nSystemParticle Exporter\nFigure 1.33. Tools and the asset pipeline.\npreexisting tools for game world layout. I’ve seen game teams use 3ds Max\nor Maya as a world layout tool, with or without custom plug-ins to aid the\nuser. Ask most game developers, and they’ll tell you they can remember a\ntime when they laid out terrain height fields using a simple bitmap editor,\nor typed world layouts directly into a text file by hand. Tools don’t have to be\npretty—gameteamswillusewhatevertoolsareavailableandgetthejobdone.\nThat said, tools must be relatively easy to use, and they absolutely must be re-\nliable, if a game team is going to be able to develop a highly polished product\nin a timely manner.\n1.7. Tools and the Asset Pipeline 61\n1.7.2 The Asset Conditioning Pipeline\nThedataformatsusedbydigitalcontentcreation(DCC)applicationsarerarely\nsuitable for direct use in-game. There are two primary reasons for this.\n1. TheDCCapp’sin-memorymodelofthedataisusuallymuchmorecom-\nplexthanwhatthegameenginerequires. Forexample,Mayastoresadi-\nrected acyclic graph (DAG) of scene nodes, with a complex web of inter-\nconnections. It stores a history of all the edits that have been performed\non the file. It represents the position, orientation and scale of every ob-\nject in the scene as a full hierarchy of 3D transformations, decomposed\ninto translation, rotation, scale and shear components. A game engine\ntypicallyonlyneedsatinyfractionofthisinformationinordertorender\nthe model in-game.\n2. The DCC application’s file format is often too slow to read at runtime,\nand in some cases it is a closed proprietary format.\nTherefore, the data produced by a DCC app is usually exported to a more\naccessible standardized format, or a custom file format, for use in-game.\nOnce data has been exported from the DCC app, it often must be further\nprocessed before being sent to the game engine. And if a game studio is ship-\nping its game on more than one platform, the intermediate files might be pro-\ncessed differently for each target platform. For example, 3D mesh data might\nbe exported to an intermediate format, such as XML, JSON or a simple binary\nformat. Then it might be processed to combine meshes that use the same ma-\nterial, or split up meshes that are too large for the engine to digest. The mesh\ndata might then be organized and packed into a memory image suitable for\nloading on a specific hardware platform.\nThe pipeline from DCC app to game engine is sometimes called the asset\nconditioning pipeline (ACP). Every game engine has this in some form.\n1.7.2.1 3D Model/Mesh Data\nThe visible geometry you see in a game is typically constructed from triangle\nmeshes. Some older games also make use of volumetric geometry known as\nbrushes. We’ll discuss each type of geometric data briefly below. For an in-\ndepth discussion of the techniques used to describe and render 3D geometry,\nsee Chapter 11.\n3D Models (Meshes)\nA mesh is a complex shape composed of triangles and vertices. Renderable\ngeometry can also be constructed from quads or higher-order subdivision\n62 1. Introduction\nsurfaces. But on today’s graphics hardware, which is almost exclusively\ngeared toward rendering rasterized triangles, all shapes must eventually be\ntranslated into triangles prior to rendering.\nA mesh typically has one or more materials applied to it in order to define\nvisual surface properties (color, reflectivity, bumpiness, diffuse texture, etc.).\nIn this book, I will use the term “mesh” to refer to a single renderable shape,\nand “model” to refer to a composite object that may contain multiple meshes,\nplus animation data and other metadata for use by the game.\nMeshes are typically created in a 3D modeling package such as 3ds Max,\nMaya or SoftImage. A powerful and popular tool by Pixologic called ZBrush\nallowsultrahigh-resolutionmeshestobebuiltinaveryintuitivewayandthen\ndown-converted into a lower-resolution model with normal maps to approx-\nimate the high-frequency detail.\nExporters must be written to extract the data from the digital content cre-\nation (DCC) tool (Maya, Max, etc.) and store it on disk in a form that is di-\ngestible by the engine. The DCC apps provide a host of standard or semi-\nstandard export formats, although none are perfectly suited for game devel-\nopment (with the possible exception of COLLADA). Therefore, game teams\noften create custom file formats and custom exporters to go with them.\nBrush Geometry\nBrush geometry is defined as a collection of convex hulls, each of which is\ndefinedbymultipleplanes. Brushesaretypicallycreatedandediteddirectlyin\nthegameworldeditor. Thisisessentiallyan“oldschool”approachtocreating\nrenderable geometry, but it is still used in some engines.\nPros:\n• fast and easy to create;\n• accessibletogamedesigners—oftenusedto“blockout”agamelevelfor\nprototyping purposes;\n• can serve both as collision volumes and as renderable geometry.\nCons:\n• low-resolution;\n• difficult to create complex shapes;\n• cannot support articulated objects or animated characters.\n1.7.2.2 Skeletal Animation Data\nAskeletal mesh is a special kind of mesh that is bound to a skeletal hierarchy\nfor the purposes of articulated animation. Such a mesh is sometimes called a\n1.7. Tools and the Asset Pipeline 63\nskinbecauseitformstheskinthatsurroundstheinvisibleunderlyingskeleton.\nEach vertex of a skeletal mesh contains a list of indices indicating to which\njoint(s) in the skeleton it is bound. A vertex usually also includes a set of joint\nweights, specifying the amount of influence each joint has on the vertex.\nIn order to render a skeletal mesh, the game engine requires three distinct\nkinds of data:\n1. the mesh itself,\n2. the skeletal hierarchy (joint names, parent-child relationships and the\nbaseposetheskeletonwasinwhenitwasoriginallyboundtothemesh),\nand\n3. one or more animation clips, which specify how the joints should move\nover time.\nThe mesh and skeleton are often exported from the DCC application as a\nsingle data file. However, if multiple meshes are bound to a single skeleton,\nthen it is better to export the skeleton as a distinct file. The animations are\nusually exported individually, allowing only those animations which are in\nusetobeloadedintomemoryatanygiventime. However,somegameengines\nallowabankofanimationstobeexportedasasinglefile,andsomeevenlump\nthe mesh, skeleton and animations into one monolithic file.\nAn unoptimized skeletal animation is defined by a stream of 43matrix\nsamples, taken at a frequency of at least 30 frames per second, for each of the\njoints in a skeleton (of which there can be 500 or more for a realistic humanoid\ncharacter). Thus,animationdataisinherentlymemory-intensive. Forthisrea-\nson, animation data is almost always stored in a highly compressed format.\nCompression schemes vary from engine to engine, and some are proprietary.\nThere is no one standardized format for game-ready animation data.\n1.7.2.3 Audio Data\nAudio clips are usually exported from Sound Forge or some other audio pro-\nduction tool in a variety of formats and at a number of different data sam-\npling rates. Audio files may be in mono, stereo, 5.1, 7.1 or other multi-channel\nconfigurations. Wave files (.wav) are common, but other file formats such as\nPlayStation ADPCM files (.vag) are also commonplace. Audio clips are often\norganized into banks for the purposes of organization, easy loading into the\nengine, and streaming.\n64 1. Introduction\n1.7.2.4 Particle Systems Data\nModern games make use of complex particle effects. These are authored by\nartists who specialize in the creation of visual effects. Third-party tools, such\nas Houdini, permit film-quality effects to be authored; however, most game\nengines are not capable of rendering the full gamut of effects that can be cre-\nated with Houdini. For this reason, many game companies create a custom\nparticle effect editing tool, which exposes only the effects that the engine ac-\ntually supports. A custom tool might also let the artist see the effect exactly as\nit will appear in-game.\n1.7.3 The World Editor\nThe game world is where everything in a game engine comes together. To my\nknowledge, there are no commercially available game world editors (i.e., the\ngameworldequivalentofMayaorMax). However,anumberofcommercially\navailable game engines provide good world editors:\n• Some variant of the Radiant game editor is used by most game engines\nbased on Quake technology.\n• TheHalf-Life2 Source engine provides a world editor called Hammer .\n•UnrealEd is the Unreal Engine’s world editor. This powerful tool also\nserves as the asset manager for all data types that the engine can con-\nsume.\n•Sandbox is the world editor in CRYENGINE.\nWriting a good world editor is difficult, but it is an extremely important\npart of any good game engine.\n1.7.4 The Resource Database\nGameenginesdealwithawiderangeofassettypes,fromrenderablegeometry\nto materials and textures to animation data to audio. These assets are defined\ninpartbytherawdataproducedbytheartistswhentheyuseatoollikeMaya,\nPhotoshop or SoundForge. However, every asset also carries with it a great\ndeal ofmetadata. For example, when an animator authors an animation clip in\nMaya, the metadata provides the asset conditioning pipeline, and ultimately\nthe game engine, with the following information:\n• A unique id that identifies the animation clip at runtime.\n• The name and directory path of the source Maya (.ma or .mb) file.\n• Theframerange —on which frame the animation begins and ends.\n• Whether or not the animation is intended to loop.\n1.7. Tools and the Asset Pipeline 65\nOS\nDrivers\nHardware (PC, XBOX360, PS3, etc.)3rd Party SDKsPlatform Independence LayerCore SystemsRun-Time Engine\nTools and World Builder\nFigure 1.34. Stand-alone tools architecture.\n• The animator’s choice of compression technique and level. (Some assets\ncan be highly compressed without noticeably degrading their quality,\nwhile others require less or no compression in order to look right in-\ngame.)\nEvery game engine requires some kind of database to manage all of the\nmetadata associated with the game’s assets. This database might be imple-\nmented using an honest-to-goodness relational database such as MySQL or\nOracle, or it might be implemented as a collection of text files, managed by\na revision control system such as Subversion, Perforce or Git. We’ll call this\nmetadata the resourcedatabase in this book.\nNo matter in what format the resource database is stored and managed,\nsome kind of user interface must be provided to allow users to author and\nedit the data. At Naughty Dog, we wrote a custom GUI in C# called Builder\nfor this purpose. For more information on Builder and a few other resource\ndatabase user interfaces, see Section 7.2.1.3.\n1.7.5 Some Approaches to Tool Architecture\nA game engine’s tool suite may be architected in any number of ways. Some\ntools might be stand-alone pieces of software, as shown in Figure 1.34. Some\ntools may be built on top of some of the lower layers used by the runtime\nengine,asFigure1.35illustrates. Sometoolsmightbebuiltintothegameitself.\n66 1. Introduction\nOS\nDrivers\nHardware (PC, XBOX 360, PS3, et c.)3rd Party SDKsPlatform Independence LayerCore SystemsRun-Time Engine Tools and World Builder\nFigure 1.35. Tools built on a framework shared with the game.\nFor example, Quake- and Unreal-based games both boast an in-game console\nthat permits developers and “modders” to type debugging and configuration\ncommands while running the game. Finally, web-based user interfaces are\nbecoming more and more popular for certain kinds of tools.\nOS\nDrivers\nHardware(PC, XBOX 360, PS3, etc.)3rd Party SDKsPlatfor m Indep endence LayerCore SystemsRun-Time Engine\nOther ToolsWorld Builder\nFigure 1.36. Unreal Engine’s tool architecture.\n1.7. Tools and the Asset Pipeline 67\nAs an interesting and unique example, Unreal’s world editor and asset\nmanager, UnrealEd, is built right into the runtime game engine. To run the\neditor, you run your game with a command-line argument of “editor.” This\nunique architectural style is depicted in Figure 1.36. It permits the tools to\nhave total access to the full range of data structures used by the engine and\navoidsacommonproblemofhavingtohavetworepresentationsofeverydata\nstructure—onefortheruntimeengineandoneforthetools. Italsomeansthat\nrunning the game from within the editor is very fast (because the game is ac-\ntually already running). Live in-game editing, a feature that is normally very\ntricky to implement, can be developed relatively easily when the editor is a\npart of the game. However, an in-engine editor design like this does have its\nshareofproblems. Forexample,whentheengineiscrashing,thetoolsbecome\nunusable as well. Hence a tight coupling between engine and asset creation\ntools can tend to slow down production.\n1.7.5.1 Web-Based User Interfaces\nWeb-based user interfaces are quickly becoming the norm for certain kinds of\ngame development tools. At Naughty Dog, we use a number of web-based\nUIs. Naughty Dog’s localization tool serves as the front-end portal into our\nlocalization database. Taskeris the web-based interface used by all Naughty\nDog employees to create, manage, schedule, communicate and collaborate on\ngame development tasks during production. A web-based interface known\nasConnector also serves as our window into the various streams of debugging\ninformation that are emitted by the game engine at runtime. The game spits\nout its debug text into various named channels, each associated with a differ-\nent engine system (animation, rendering, AI, sound, etc.) These data streams\nare collected by a lightweight Redis database. The browser-based Connec-\ntor interface allows users to view and filter this information in a convenient\nway.\nWeb-based UIs offer a number of advantages over stand-alone GUI appli-\ncations. For one thing, web apps are typically easier and faster to develop\nand maintain than a stand-alone app written in a language like Java, C# or\nC++. Web apps require no special installation—all the user needs is a com-\npatible web browser. Updates to a web-based interface can be pushed out to\nthe users without the need for an installation step—they need only refresh or\nrestart their browser to receive the update. Web interfaces also force us to de-\nsign our tools using a client-server architecture. This opens up the possibility\nof distributing our tools to a wider audience. For example, Naughty Dog’s\nlocalization tool is available directly to outsourcing partners around the globe\n68 1. Introduction\nwho provide language translation services to us. Stand-alone tools still have\ntheir place of course, especially when specialized GUIs such as 3D visualiza-\ntion are required. But if your tool only needs to present the user with editable\nforms and tabular data, a web-based tool may be your best bet.",18034
14-2 Tools of the Trade.pdf,14-2 Tools of the Trade,,0
15-2.1 Version Control.pdf,15-2.1 Version Control,"2\nTools of the Trade\nBefore we embark on our journey across the fascinating landscape of game\nenginearchitecture, itisimportantthatweequipourselveswithsomeba-\nsic tools and provisions. In the next two chapters, we will review the software\nengineering concepts and practices that we will need during our voyage. In\nChapter 2, we’ll explore the tools used by the majority of professional game\nengineers. Then in Chapter 3, we’ll round out our preparations by reviewing\nsomekeytopicsintherealmsofobject-orientedprogramming,designpatterns\nand large-scale C++ programming.\nGame development is one of the most demanding and broad areas of soft-\nware engineering, so believe me, we’ll want to be well equipped if we are to\nsafelynavigatethesometimes-treacherousterrainwe’llbecovering. Forsome\nreaders, the contents of this chapter and the next will be very familiar. How-\never, I encourage you not to skip these chapters entirely. I hope that they will\nserve as a pleasant refresher; and who knows, you might even pick up a new\ntrick or two.\n2.1 Version Control\nAversion control system is a tool that permits multiple users to work on a\ngroup of files collectively. It maintains a history of each file so that changes\n69\n70 2. Tools of the Trade\ncan be tracked and reverted if necessary. It permits multiple users to mod-\nify files—even the same file—simultaneously, without everyone stomping on\neachother’swork. Versioncontrolgetsitsnamefromitsabilitytotrackthever-\nsion history of files. It is sometimes called source control, because it is primar-\nily used by computer programmers to manage their source code. However,\nversion control can be used for other kinds of files as well. Version control\nsystems are usually best at managing text files, for reasons we will discover\nbelow. However, many game studios use a single version control system to\nmanage both source code files (which are text) and game assets like textures,\n3D meshes, animations and audio files (which are usually binary).\n2.1.1 Why Use Version Control?\nVersion control is crucial whenever software is developed by a team of multi-\nple engineers. Version control\n• provides a central repository from which engineers can share source\ncode;\n• keeps a history of the changes made to each source file;\n• provides mechanisms allowing specific versions of the code base to be\ntagged and later retrieved; and\n• permits versions of the code to be branched off from the main devel-\nopment line, a feature often used to produce demos or make patches to\nolder versions of the software.\nA source control system can be useful even on a single-engineer project.\nAlthough its multiuser capabilities won’t be relevant, its other abilities, such\nas maintaining a history of changes, tagging versions, creating branches for\ndemos and patches, tracking bugs, etc., are still invaluable.\n2.1.2 Common Version Control Systems\nHere are the most common source control systems you’ll probably encounter\nduring your career as a game engineer.\n•SCCS and RCS. The Source Code Control System (SCCS) and the Revi-\nsionControl System(RCS)aretwoofthe oldestversioncontrolsystems.\nBothemployacommand-lineinterface. Theyareprevalentprimarilyon\nUNIX platforms.\n•CVS.TheConcurrentVersionSystem(CVS)isaheavy-dutyprofessional-\ngradecommand-line-basedsourcecontrolsystem,originallybuiltontop\n2.1. Version Control 71\nof RCS (but now implemented as a stand-alone tool). CVS is prevalent\non UNIX systems but is also available on other development platforms\nsuch as Microsoft Windows. It is open source and licensed under the\nGnuGeneralPublicLicense(GPL).CVSNT(alsoknownasWinCVS)isa\nnative Windows implementation that is based on, and compatible with,\nCVS.\n•Subversion. Subversionisanopensourceversioncontrolsystemaimedat\nreplacingandimprovinguponCVS.Becauseitisopensourceandhence\nfree,itisagreatchoiceforindividualprojects,studentprojectsandsmall\nstudios.\n•Git. Thisisanopensourcerevisioncontrolsystemthathasbeenusedfor\nmany venerable pr ojects, including the Linux kernel. In the git develop-\nment model, the programmer makes changes to files and commits the\nchanges to a branch. The programmer can then merge his changes into\nany other code branch quickly and easily, because git “knows” how to\nrewindasequenceofdiffsandreapplythemontoanewbaserevision—a\nprocess git calls rebasing. The net result is a revision control system that\nis highly efficient and fast when dealing with multiple code branches.\nGit is a distributed version control system; individual programmers can\nwork locally much of the time, yet they can merge their changes into a\nshared codebase easily. It’s also very easy to use on a one-person soft-\nware project, because there’s zero server set-up to worry about. More\ninformation on git can be found at http://git-scm.com/.\n•Perforce. Perforce is a professional-grade source control system, with\nboth text-based and GUI interfaces. One of Perforce’s claims to fame is\nits concept of changelists. A change list is a collection of source files that\nhave been modified as a logical unit. Change lists are checked into the\nrepositoryatomically—eithertheentirechangelistissubmitted,ornone\nof it is. Perforce is used by many game companies, including Naughty\nDogandElectronicArts.\n•NxNAlienbrain. Alienbrainisapowerfulandfeature-richsourcecontrol\nsystem designed explicitly for the game industry. Its biggest claim to\nfame is its support for very large databases containing both text source\ncode files and binary game art assets, with a customizable user interface\nthat can be targeted at specific disciplines such as artists, producers or\nprogrammers.\n•ClearCase. Rational ClearCase is a professional-gradesourcecontrolsys-\ntem aimed at very large-scale software projects. It is powerful and em-\nploys a unique user interface that extends the functionality of Windows\n72 2. Tools of the Trade\nExplorer. Ihaven’tseenClearCaseusedmuchinthegameindustry,per-\nhaps because it is one of the more expensive version control systems.\n•Microsoft Visual SourceSafe . SourceSafe is a lightweight source control\npackage that has been used successfully on some game projects.\n2.1.3 Overview of Subversion and TortoiseSVN\nI have chosen to highlight Subversion in this book for a few reasons. First off,\nit’s free, which is always nice. It works well and is reliable, in my experience.\nA Subversion central repository is quite easy to set up, and as we’ll see, there\nare already a number of free repository servers out there if you don’t want to\ngo to the trouble of setting one up yourself. There are also a number of good\nWindowsandMacSubversionclients,suchasthefreelyavailableTortoiseSVN\nfor Windows. So while Subversion may not be the best choice for a large com-\nmercial project (I personally prefer Perforce or git for that purpose), I find it\nperfectlysuitedtosmallpersonalandeducationalprojects. Let’stakealookat\nhow to set up and use Subversion on a Microsoft Windows PC development\nplatform. As we do so, we’ll review core concepts that apply to virtually any\nversion control system.\nSubversion, like most other version control systems, employs a client-\nserver architecture. The server manages a central repository, in which a\nversion-controlled directory hierarchy is stored. Clients connect to the server\nand request operations, such as checking out the latest version of the direc-\ntory tree, committing new changes to one or more files, tagging revisions,\nbranching the repository and so on. We won’t discuss setting up a server\nhere; we’ll assume you have a server, and instead we will focus on setting\nup and using the client. You can learn how to set up a Subversion server\nby reading Chapter 6 of [43]. However, you probably will never need to do\nso, because you can always find free Subversion servers. For example, He-\nlixTeamHub provides Subversion code hosting at http://info.perforce.com/\ntry-perforce-helix-teamhub-free.htmlthat’sfreeforprojectsof5usersorfewer\nand up to 1 GB of storage. Beanstalk is another good hosting service, but they\ndo charge a nominal monthly fee.\n2.1.4 Setting up a Code Repository\nThe easiest way to get started with Subversion is to visit the website of He-\nlixTeamHub or a similar SVN hosting service, and set up a Subversion repos-\nitory. Create an account and you’re off to the races. Most hosting sites offer\neasy-to-follow instructions.\n2.1. Version Control 73\nFigure 2.1. TortoiseSVN initial check-out dialog.\n Figure 2.2. TortoiseSVN user authentication dialog.\nOnceyou’vecreatedyourrepository,youcantypicallyadministeritonthe\nhosting service’s website. You can add and remove users, control options and\nperform a wealth of advanced tasks. But all you really need to do next is set\nup a Subversion client and start using your repository.\n2.1.5 Installing TortoiseSVN\nTortoiseSVN is a popular front end for Subversion. It extends the functional-\nity of the Microsoft Windows Explorer via a convenient right-click menu and\noverlay icons to show you the status of your version-controlled files and fold-\ners.\nTo get TortoiseSVN, visit http://tortoisesvn.tigris.org/. Download the lat-\nest version from the download page. Install it by double-clicking the .msi file\nthat you’ve downloaded and following the installation wizard’s instructions.\nOnce TortoiseSVN is installed, you can go to any folder in Windows Ex-\nplorerandright-click—TortoiseSVN’smenuextensionsshouldnowbevisible.\nTo connect to an existing code repository (such as one you created on Helix-\nTeamHub), create a folder on your local hard disk and then right-click and\nselect “SVN Checkout….” The dialog shown in Figure 2.1 will appear. In the\n“URLofrepository”field,enteryourrepository’sURL.IfyouareusingHelix-\nTeamHub, it would be https://helixteamhub.cloud/mr3/projects/ myproject-\nname/repositories/subversion/ myrepository,where myprojectname iswhatever\nyounamedyourprojectwhenyoufirstcreatedit,and myrepository isthename\nof your SVN code repository.\n74 2. Tools of the Trade\nFigure 2.3. File version histories.\nFigure 2.4. Editing the local copy of a version-controlled ﬁle.\nYou should now see the dialog shown in Figure 2.2. Enter your user name\nandpassword;checkingthe“Saveauthentication”optiononthisdialogallows\nyoutouseyourrepositorywithouteverhavingtologinagain. Onlyselectthis\noptionifyouareworkingonyourownpersonalmachine—neveronamachine\nthat is shared by many users.\nOnce you’ve authenticated your user name, TortoiseSVN will download\n(“check out”) the entire contents of your repository to your local disk. If you\nhavejustsetupyourrepository,thiswillbe…nothing! Thefolderyoucreated\nwill still be empty. But now it is connected to your Subversion repository on\nHelixTeamHub (or wherever your server is located). If you refresh your Win-\ndows Explorer window (hit F5), you should now see a little green and white\ncheckmark on your folder. This icon indicates that the folder is connected to\na Subversion repository via TortoiseSVN and that the local copy of the repos-\nitory is up to date.\n2.1.6 File Versions, Updating and Committing\nAs we’ve seen, one of the key purposes of any source control system like Sub-\nversion is to allow multiple programmers to work on a single software code\nbase by maintaining a central repository or “master” version of all the source\ncodeonaserver. Theservermaintainsaversionhistoryforeachfile,asshown\nin Figure 2.3. This feature is crucial to large-scale multiprogrammer software\ndevelopment. For example, if someone makes a mistake and checks in code\nthat “breaks the build,” you can easily go back in time to undo those changes\n(and check the log to see who the culprit was!). You can also grab a snap-\nshot of the code as it existed at any point in time, allowing you to work with,\ndemonstrate or patch previous versions of the software.\n2.1. Version Control 75\nFigure 2.5. TortoiseSVN Commit dialog.\nEachprogrammergetsalocalcopyofthecodeonhisorhermachine. Inthe\ncase of TortoiseSVN, you obtain your initial working copy by “checking out”\nthe repository, as described above. Periodically you should update your local\ncopy to reflect any changes that may have been made by other programmers.\nYoudothisbyright-clickingonafolderandselecting“SVNUpdate”fromthe\npop-up menu.\nYou can work on your local copy of the code base without affecting the\nother programmers on the team (Figure 2.4). When you are ready to share\nyour changes with everyone else, you commityour changes to the repository\n(also known as submitting orchecking in ). You do this by right-clicking on the\nfolder you want to commit and selecting “SVN Commit…” from the pop-up\nmenu. You will get a dialog like the one shown in Figure 2.5, asking you to\nconfirm the changes.\nDuring a commit operation, Subversion generates a diffbetween your lo-\ncal version of each file and the latest version of that same file in the repository.\nTheterm“diff”meansdifference,anditistypicallyproducedbyperforminga\nline-by-linecomparisonofthetwoversionsofthefile. Youcandouble-clickon\nanyfileintheTortoiseSVNCommitdialog(Figure2.5)toseethediffsbetween\nyour version and the latest version on the server (i.e., the changes you made).\nFiles that have changed (i.e., any files that “have diffs”) are committed. This\nreplaces the latest version in the repository with your local version, adding a\n76 2. Tools of the Trade\nnew entry to the file’s version history. Any files that have not changed (i.e.,\nyour local copy is identical to the latest version in the repository) are ignored\nby default during a commit. An example commit operation is shown in Fig-\nure 2.6.\nFigure 2.6. Com-\nmitting local edits to\nthe repository.Ifyoucreatedanynewfilespriortothecommit,theywillbelistedas“non-\nversioned” in the Commit dialog. You can check the little check boxes beside\nthem in order to add them to the repository. Any files that you deleted locally\nwill likewise show up as “missing”—if you check their check boxes, they will\nbe deleted from the repository. You can also type a comment in the Commit\ndialog. This comment is added to the repository’s history log, so that you and\nothers on your team will know why these files were checked in.\n2.1.7 Multiple Check-Out, Branching and Merging\nSome version control systems require exclusive check-out. This means that you\nmustfirstindicateyourintentionstomodifyafileby checkingitout andlocking\nit. The file(s) that are checked out to you are writable on your local disk and\ncannotbecheckedoutbyanyoneelse. Allotherfilesintherepositoryareread-\nonly on your local disk. Once you’re done editing the file, you can check it in,\nwhichreleasesthelockandcommitsthechangestotherepositoryforeveryone\nelse to see. The process of exclusively locking files for editing ensures that no\ntwo people can edit the same file simultaneously.\nSubversion, CVS, Perforce and many other high-quality version control\nsystems also permit multiple check-out, i.e., you can edit a file while someone\nelseiseditingthatsamefile. Whicheveruser’schangesarecommittedfirstbe-\ncome the latest version of the file in the repository. Any subsequent commits\nby other users require that programmer to merge his or her changes with the\nchanges made by the programmer(s) who committed previously.\nBecause more than one set of changes (diffs) have been made to the same\nfile, the version control system must mergethe changes in order to produce a\nfinal version of the file. This is often not a big deal, and in fact many conflicts\ncan be resolved automatically by the version control system. For example, if\nyou changed function f()and another programmer changed function g(),\nthen your edits would have been to a different range of lines in the file than\nthose of the other programmer. In this case, the merge between your changes\nandhisorherchangeswillusuallyresolveautomaticallywithoutanyconflicts.\nHowever,ifyouwerebothmakingchangestothesamefunction f(),thenthe\nsecond programmer to commit his or her changes will need to do a three-way\nmerge(see Figure 2.7).\nFor three-way merges to work, the version control server has to be smart\n2.1. Version Control 77\nFoo.cpp (joe_b) Foo.cpp (suzie_q) joe_b and suzie_q both \nstart editing Foo.cpp at \nthe same time\nFoo.cpp (version 4)Foo.cpp (joe_b) Foo.cpp (version 5) suzie_q commits her \nchanges first\njoe_b must now do a 3-way \nmerge, which involves 2 sets\nof diffs:Foo.cpp (v ersion 6)\nFoo.cpp (joe_b) Foo.cpp (v ersion 5)\nFoo.cpp (v ersion 4)Foo.cpp (v ersion 4)\nversi on 4 to his lo cal versionversion 4 to version 5\nFigure 2.7. Three-way merge due to local edits by two different users.\nenough to keep track of which version of each file you currently have on your\nlocal disk. That way, when you merge the files, the system will know which\nversion is the base version (the common ancestor, such as version 4 in Fig-\nure 2.7).\nSubversion permits multiple check-out, and in fact it doesn’t require you\nto check out files explicitly at all. You simply start editing the files locally—\nall files are writable on your local disk at all times. (By the way, this is one\nreason that Subversion doesn’t scale well to large projects, in my opinion. To\ndetermine which files you have changed, Subversion must search the entire\ntree of source files, which can be slow. Version control systems like Perforce,\nwhichexplicitlykeeptrackofwhichfilesyouhavemodified,areusuallyeasier\ntoworkwithwhendealingwithlargeamountsofcode. Butforsmallprojects,\nSubversion’s approach works just fine.)\nWhenyouperformacommitoperationbyright-clickingonanyfolderand\nselecting “SVN Commit…” from the pop-up menu, you may be prompted to\nmergeyour changes with changes made by someone else. But if no one has\nchanged the file since you last updated your local copy, then your changes",17907
16-2.2 Compilers Linkers and IDEs.pdf,16-2.2 Compilers Linkers and IDEs,"78 2. Tools of the Trade\nwill be committed without any further action on your part. This is a very\nconvenient feature, but it can also be dangerous. It’s a good idea to always\ncheck your commits carefully to be sure you aren’t committing any files that\nyou didn’t intend to modify. When TortoiseSVN displays its Commit Files\ndialog, you can double-click on an individual file in order to see the diffs you\nmade prior to hitting the “OK” button.\n2.1.8 Deleting Files\nWhen a file is deleted from the repository, it’s not really gone. The file still\nexistsintherepository,butitslatestversionissimplymarked“deleted”sothat\nusers will no longer see the file in their local directory trees. You can still see\nand access previous versions of a deleted file by right-clicking on the folder in\nwhich the file was contained and selecting “Show log” from the TortoiseSVN\nmenu.\nYou can undelete a deleted file by updating your local directory to the ver-\nsion immediately before the version in which the file was marked deleted.\nThen simply commit the file again. This replaces the latest deleted version\nof the file with the version just prior to the deletion, effectively undeleting the\nfile.\n2.2 Compilers, Linkers and IDEs\nCompiled languages, such as C++, require a compiler andlinkerin order to\ntransform source code into an executable program. There are many compil-\ners/linkers available for C++, but for the Microsoft Windows platform, the\nmost commonly used package is probably Microsoft Visual Studio. The fully-\nfeaturedProfessionalandEnterpriseEditionsoftheproductcanbepurchased\nonline from the Microsoft store, and its lighter-weight cousin Visual Studio\nCommunity Edition (previously known as Visual Studio Express) is available\nfor free download at https://www.visualstudio.com/downloads. Documen-\ntationonVisualStudioandthestandardCandC++librariesisavailableonline\nat the Microsoft Developer Network (MSDN) site (https://msdn.microsoft.\ncom/en-us).\nVisual Studio is more than just a compiler and linker. It is an integrated\ndevelopment environment (IDE), including a slick and fully featured text editor\nfor source code and a powerful source-level and machine-level debugger. In\nthis book, our primary focus is the Windows platform, so we’ll investigate\nVisualStudioinsomedepth. Muchofwhatyoulearnbelowwillbeapplicable\ntoothercompilers,linkersanddebuggerssuchasLLVM/Clang,gcc/gdb,and\n2.2. Compilers, Linkers and IDEs 79\ntheIntelC/C++ compiler. So evenif you’renotplanning oneverusing Visual\nStudio, I suggest you read this section carefully. You’ll find all sorts of useful\ntips on using compilers, linkers and debuggers in general.\n2.2.1 Source Files, Headers and Translation Units\nA program written in C++ is comprised of source files . These typically have\na .c, .cc, .cxx or .cpp extension, and they contain the bulk of your pro-\ngram’s source code. Source files are technically known as translationunits, be-\ncause the compiler translates one source file at a time from C++ into machine\ncode.\nA special kind of source file, known as a header file , is often used in\norder to share information, such as type declarations and function pro-\ntotypes, between translation units. Header files are not seen by the\ncompiler. Instead, the C++ preprocessor replaces each #include state-\nment with the contents of the corresponding header file prior to send-\ning the translation unit to the compiler. This is a subtle but very im-\nportant distinction to make. Header files exist as distinct files from\nthe point of view of the programmer—but thanks to the preproces-\nsor’s header file expansion, all the compiler ever sees are translation\nunits.\n2.2.2 Libraries, Executables and Dynamic Link Libraries\nWhenatranslationunitiscompiled,theresultingmachinecodeisplacedinan\nobject file (files with a .obj extension under Windows or .o under UNIX-based\noperating systems). The machine code in an object file is:\n•relocatable,meaningthatthememoryaddressesatwhichthecoderesides\nhave not yet been determined, and\n•unlinked , meaning that any external references to functions and global\ndata that are defined outside the translation unit have not yet been re-\nsolved.\nObject files can be collected into groups called libraries. A library is simply\nan archive, much like a ZIP or tar file, containing zero or more object files.\nLibraries exist merely as a convenience, permitting a large number of object\nfiles to be collected into a single easy-to-use file.\nObject files and libraries are linkedinto an executable by the linker. The\nexecutable file contains fully resolved machine code that can be loaded and\nrun by the operating system. The linker’s jobs are:\n80 2. Tools of the Trade\n• to calculate the final relative addresses of all the machine code, as it will\nappear in memory when the program is run, and\n• to ensure that all external references to functions and global data made\nby each translation unit (object file) are properly resolved.\nIt’s important to remember that the machine code in an executable file is\nstill relocatable, meaning that the addresses of all instructions and data in the\nfilearestill relativetoanarbitrarybaseaddress,notabsolute. Thefinalabsolute\nbaseaddressoftheprogramisnotknownuntiltheprogramisactuallyloaded\ninto memory, just prior to running it.\nAdynamiclinklibrary (DLL)isaspecialkindoflibrarythatactslikeahybrid\nbetween a regular static library and an executable. The DLL acts like a library,\nbecause it contains functions that can be called by any number of different\nexecutables. However, a DLL also acts like an executable, because it can be\nloaded by the operating system independently, and it contains some start-up\nand shut-down code that runs much the way the main() function in a C++\nexecutable does.\nThe executables that use a DLL contain partiallylinked machine code. Most\nof the function and data references are fully resolved within the final exe-\ncutable, but any references to external functions or data that exist in a DLL re-\nmain unlinked. When the executable is run, the operating system resolves the\naddresses of all unlinked functions by locating the appropriate DLLs, loading\nthemintomemoryiftheyarenotalreadyloaded,andpatchinginthenecessary\nmemory addresses. Dynamically linked libraries are a very useful operating\nsystemfeature,becauseindividualDLLscanbeupdatedwithoutchangingthe\nexecutable(s) that use them.\n2.2.3 Projects and Solutions\nNowthatweunderstandthedifferencebetweenlibraries,executablesanddy-\nnamic link libraries (DLLs), let’s see how to create them. In Visual Studio, a\nprojectis a collection of source files which, when compiled, produce a library,\nan executable or a DLL. In all versions of Visual Studio since VS 2013, projects\nare stored in project files with a .vcxproj extension. These files are in XML for-\nmat, so they are reasonably easy for a human to read and even edit by hand if\nnecessary.\nAllversionsofVisualStudiosinceversion7(VisualStudio2003)employ so-\nlution files (files with a .sln extension) as a means of containing and managing\ncollections of projects. A solution is a collection of dependent and/or inde-\npendent projects intended to build one or more libraries, executables and/or\nDLLs. In the Visual Studio graphical user interface, the Solution Explorer is\n2.2. Compilers, Linkers and IDEs 81\nusually displayed along the right or left side of the main window, as shown\nin Figure 2.8.\nThe Solution Explorer is a tree view. The solution itself is at the root, with\nthe projects as its immediate children. Source files and headers are shown as\nchildren of each project. A project can contain any number of user-defined\nfolders,nestedtoanydepth. Foldersarefororganizationalpurposesonlyand\nhave nothing to do with the folder structure in which the files may reside on-\ndisk. However, it is common practice to mimic the on-disk folder structure\nwhen setting up a project’s folders.\n2.2.4 Build Conﬁgurations\nThe C/C++ preprocessor, compiler and linker offer a wide variety of options\ntocontrolhowyourcodewillbebuilt. Theseoptionsarenormallyspecifiedon\nthe command line when the compiler is run. For example, a typical command\nto build a single translation unit with the Microsoft compiler might look like\nthis:\n> cl /c foo.cpp /Fo foo.obj /Wall /Od /Zi\nThis tells the compiler/linker to compile but not link ( /c) the translation\nunit named foo.cpp, output the result to an object file named foo.obj ( /Fo\nfoo.obj), turn on all warnings (/Wall ), turn off all optimizations (/Od) and\ngenerate debugging information ( /Zi). A roughly equivalent command line\nfor LLVM/Clang would look something like this:\n> clang --std=c++14 foo.cpp -o foo.o --Wall -O0 -g\nFigure 2.8. The VisualStudio Solution Explorer window.\n82 2. Tools of the Trade\nModern compilers provide so many options that it would be impractical\nand error prone to specify all of them every time you build your code. That’s\nwherebuildconfigurations comein. Abuildconfigurationisreallyjustacollec-\ntion of preprocessor, compiler and linker options associated with a particular\nproject in your solution. You can define any number of build configurations,\nname them whatever you want, and configure the preprocessor, compiler and\nlinker options differently in each configuration. By default, the same options\nare applied to every translation unit in the project, although you can override\nthe global project settings on an individual translation unit basis. (I recom-\nmend avoiding this if at all possible, because it becomes difficult to tell which\n.cpp files have custom settings and which do not.)\nWhen you create a new project/solution in Visual Studio, by default it cre-\nates two build configurations named “Debug” and “Release.” The release\nbuild is intended for the final version of your software that you’ll ship (re-\nlease) to customers, while the debug build is for development purposes. A\ndebug build runs more slowly than a non-debug build, but it provides the\nprogrammer with invaluable information for developing and debugging the\nprogram.\nProfessional software developers often set up more than two build config-\nurationsfortheirsoftware. Tounderstandwhy,we’llneedtounderstandhow\nlocal(compile-time)andglobal(link-time)optimizationswork—we’lldiscuss\nthese kinds of optimizations in Section 2.2.4.2. For now, let’s drop the some-\nwhat confusing term “release build” and stick with the terms “debug build”\n(meaninglocalandglobaloptimizationsaredisabled)and“non-debugbuild”\n(meaning local and/or global optimizations are enabled).\n2.2.4.1 Common Build Options\nThis section lists some of the most common options you’ll want to control via\nbuild configurations for a game engine project.\nPreprocessor Settings\nTheC++preprocessorhandlestheexpansionof #includedfilesandthedefi-\nnitionandsubstitutionof #definedmacros. Oneextremelypowerfulfeature\nof all modern C++ preprocessors is the ability to define preprocessor macros\nvia command-line options (and hence via build configurations). Macros de-\nfined in this way act as though they had been written into your source code\nwith a #define statement. For most compilers, the command line option for\nthis is -Dor/D, and any number of these directives can be used.\nThisfeatureallowsyoutocommunicatevariousbuildoptionstoyourcode,\n2.2. Compilers, Linkers and IDEs 83\nwithout having to modify the source code itself. As a ubiquitous example,\nthe symbol _DEBUG is always defined for a debug build, while in non-debug\nbuilds,thesymbol NDEBUG isdefinedinstead. Thesourcecodecancheckthese\nflags and in effect “know” whether it is being built in debug or non-debug\nmode. This is known as conditional compilation. For example,\nvoid f()\n{\n#ifdef _DEBUG\nprintf(""Calling function f()\n"");\n#endif\n// ...\n}\nThe compiler is also free to introduce “magic” preprocessor macros into\nyour code, based on its knowledge of the compilation environment and target\nplatform. For example, the macro __cplusplus is defined by most C/C++\ncompilers when compiling a C++ file. This allows code to be written that au-\ntomatically adapts to being compiled for C or C++.\nAs another example, every compiler identifies itself to the source code via\na “magic” preprocessor macro. When compiling code under the Microsoft\ncompiler, the macro _MSC_VER is defined; when compiling under the GNU\ncompiler (gcc), the macro __GNUC__ is defined instead and so on for the\nother compilers. The target platform on which the code will be run is like-\nwise identified via macros. For example, when building for a 32-bit Windows\nmachine, the symbol _WIN32 is always defined. These key features permit\ncross-platform code to be written, because they allow your code to “know”\nwhatcompileriscompilingitandonwhichtargetplatformitisdestinedtobe\nrun.\nCompiler Settings\nOne of the most common compiler options controls whether or not the com-\npilershouldinclude debugginginformation withtheobjectfilesitproduces. This\ninformation is used by debuggers to step through your code, display the val-\nues of variables and so on. Debugging information makes your executables\nlarger on disk and also opens the door for hackers to reverse-engineer your\ncode, so it is always stripped from the final shipping version of your exe-\ncutable. However, during development, debugging information is invaluable\nand should always be included in your builds.\nThe compiler can also be told whether or not to expand inline functions .\nWhen inline function expansion is turned off, every inline function appears\nonly once in memory, at a distinct address. This makes the task of tracing\n84 2. Tools of the Trade\nthrough the code in the debugger much simpler, but obviously comes at the\nexpense of the execution speed improvements normally achieved by inlining.\nInline function expansion is but one example of generalized code transfor-\nmations known as optimizations . The aggressiveness with which the compiler\nattempts to optimize your code, and the kinds of optimizations its uses, can\nbe controlled via compiler options. Optimizations have a tendency to reorder\nthe statements in your code, and they also cause variables to be stripped out\nof the code altogether, or moved around, and can cause CPU registers to be\nreused for new purposes later in the same function. Optimized code usually\nconfuses most debuggers, causing them to “lie” to you in various ways, and\nmaking it difficult or impossible to see what’s really going on. As a result,\nall optimizations are usually turned off in a debug build. This permits every\nvariable and every line of code to be scrutinized as it was originally coded.\nBut, of course, such code will run much more slowly than its fully optimized\ncounterpart.\nLinker Settings\nThe linker also exposes a number of options. You can control what type of\noutput file to produce—an executable or a DLL. You can also specify which\nexternal libraries should be linked into your executable, and which directory\npathstosearchinordertofindthem. Acommonpracticeistolinkwithdebug\nlibrarieswhenbuildingadebugexecutableandwithoptimizedlibrarieswhen\nbuilding in non-debug mode.\nLinkeroptionsalsocontrolthingslikestacksize,thepreferredbaseaddress\nof your program in memory, what type of machine the code will run on (for\nmachine-specific optimizations), whether or not global (link-time) optimiza-\ntions are enabled, and some other minutia with which we will not concern\nourselves here.\n2.2.4.2 Local and Global Optimizations\nAn optimizing compiler is one that can automatically optimize the machine\ncode it generates. All of the C/C++ compilers commonly used today are op-\ntimizing compilers. These include Microsoft Visual Studio, gcc, LLVM/Clang\nand the Intel C/C++ compiler.\nOptimizations can come in two basic forms:\n• local optimizations and\n• global optimizations,\n2.2. Compilers, Linkers and IDEs 85\nalthoughotherkindsofoptimizationsarepossible(e.g., peep-holeoptimizations ,\nwhichenabletheoptimizertomakeplatform-orCPU-specificoptimizations).\nLocal optimizations operate only on small chunks of code known as basic\nblocks. Roughly speaking, a basic block is a sequence of assembly language\ninstructions that doesn’t involve a branch. Local optimizations include things\nlike:\n•algebraicsimplification ,\n•operator strength reduction (e.g., converting x / 2 intox >> 1 be-\ncause the shift operator is “lower strength” and therefore less expensive\nthan the integer division operator),\n•codeinlining ,\n•constant folding (recognizing expressions that are constant at compile\ntime and replacing such expressions with their known value),\n•constant propagation (replacing all instances of a variable whose value\nturns out to be constant with the literal constant itself),\n•loop unrolling (e.g., converting a loop that always iterates exactly four\ntimes with four copies of the code within the loop, in order to eliminate\nconditional branching),\n•dead code elimination (removal of code that has no effect, such as remov-\ning the assignment expression x = 5; if it is immediately followed by\nanother assignment to xlikex = y + 1;), and\n•instructionreordering (in order to minimize CPU pipeline stalls).\nGlobal optimizations operate beyond the scope of basic code blocks—they\ntake the whole control flow graph of a program into account. An example of\nthis kind of optimization would be common sub-expression elimination . Global\noptimizations ideally operate across translation unit boundaries, and hence\nare performed by the linker rather than the compiler. Aptly, optimizations\nperformed by the linker are known as link-timeoptimizations (LTO).\nSome modern compilers like LLVM/Clang support profile-guidedoptimiza-\ntions(PGO). As the name implies, these optimizations use profiling informa-\ntion obtained from previous runs of your software to iteratively identify and\noptimize its most performance-critical code paths. PGO and LTO optimiza-\ntions can produce an impressive performance boost, but they come with a\ncost. LTO optimizations greatly increase the amount of time required to link\nan executable. And PGO optimizations, being iterative in nature, require the\nsoftware to be run (via the QA team or an automated test suite) in order to\ngenerate the profiling information that drives further optimizations.\n86 2. Tools of the Trade\nMost compilers offer various options controlling how aggressive its opti-\nmization efforts will be. Optimizations can be disabled entirely (useful for\ndebug builds, where debuggability is more important than performance), or\nincreasing“levels”ofoptimizationscanbeapplieduptosomepredetermined\nmaximum. Forexample,optimizationsforgcc,ClangandVisualC++allrange\nfrom -O0 (meaning no optimizations are performed) to -O3 (all optimizations\nenabled). Individual optimizations can be turned on or off as well, via other\ncommand line options.\n2.2.4.3 Typical Build Conﬁgurations\nGame projects often have more than just two build configurations. Here are a\nfew of the common configurations I’ve seen used in game development.\n•Debug. A debug build is a very slow version of your program, with all\noptimizations turned off, all function inlining disabled, and full debug-\nging information included. This build is used when testing brand new\ncodeandalsotodebugallbutthemosttrivialproblemsthatariseduring\ndevelopment.\n•Development. A development build (or “dev build”) is a faster version\nof your program, with most or all local optimizations enabled, but with\ndebugginginformationandassertionsstillturnedon. (SeeSection3.2.3.3\nforadiscussionofassertions.) Thisallowsyoutoseeyourgamerunning\nat a speed representative of the final product, but it still gives you some\nopportunity to debug problems.\n•Ship. A ship configuration is intended for building the final game that\nyou will ship to your customers. It is sometimes called a “final” build or\n“disk” build. Unlike a development build, all debugging information is\nstripped out of a ship build, most or all assertions are compiled out, and\noptimizationsarecrankedallthewayup,includingglobaloptimizations\n(LTO and PGO). A ship build is very tricky to debug, but it is the fastest\nand leanest of all build types.\nHybrid Builds\nAhybridbuildisabuildconfigurationinwhichthemajorityofthetranslation\nunits are built in development mode, but a small subset of them is built in\ndebugmode. Thispermitsthesegmentofcodethatiscurrentlyunderscrutiny\nto be easily debugged, while the rest of the code continues to run at close to\nfull speed.\nWith a text-based build system like make, it is quite easy to set up a hy-\nbrid build that permits users to specify the use of debug mode on a per-\n2.2. Compilers, Linkers and IDEs 87\ntranslation-unit basis. In a nutshell, we define a make variable called some-\nthing like $HYBRID_SOURCES, which lists the names of all translation units\n(.cpp files) that should be compiled in debug mode for our hybrid build. We\nset up build rules for compiling both debug and development versions of ev-\nery translation unit, and arrange for the resulting object files (.obj/.o) to be\nplacedintotwodifferentfolders,onefordebugandonefordevelopment. The\nfinal link rule is set up to link with the debug versions of the object files listed\nin$HYBRID_SOURCES and with the non-debug versions of all other object\nfiles. If we’ve set it up properly, make’s dependency rules will take care of the\nrest.\nUnfortunately, this is not so easy to do in Visual Studio, because its build\nconfigurationsaredesignedtobeappliedonaper-projectbasis,notper-trans-\nlation unit. The crux of the problem is that we cannot easily define a list of\nthe translation units that we want to build in debug mode. One solution to\nthis problem is to write a script (in Python or another suitable language) that\nautomaticallygeneratesVisualStudio.vcxprojfiles,givenalistofthe.cppfiles\nyou want to be built in debug mode in your hybrid configuration. Another\nalternativethatworksifyoursourcecodeisalreadyorganizedintolibrariesis\ntosetupa“Hybrid”buildconfigurationatthesolutionlevel,whichpicksand\nchooses between debug and development builds on a per-project (and hence\nper-library) basis. This isn’t as flexible as having control on a per-translation-\nunit basis, but it does work reasonably well if your libraries are sufficiently\ngranular.\nBuild Conﬁgurations and Testability\nThe more build configurations your project supports, the more difficult test-\ning becomes. Although the differences between the various configurations\nmay be slight, there’s a finite probability that a critical bug may exist in one\nof them but not in the others. Therefore, each build configuration must be\ntested equally thoroughly. Most game studios do not formally test their de-\nbugbuilds,becausethedebugconfigurationisprimarilyintendedforinternal\nuse during initial development of a feature and for the debugging of prob-\nlems found in one of the other configurations. However, if your testers spend\nmost of their time testing your development build configuration, then you\ncannot simply make a ship build of your game the night before Gold Mas-\nter and expect it to have an identical bug profile to that of the development\nbuild. Practically speaking, the test team must test both your development\nand ship builds equally throughout alpha and beta to ensure that there aren’t\nany nasty surprises lurking in your ship build. In terms of testability, there is\na clear advantage to keeping your build configurations to a minimum, and in\n88 2. Tools of the Trade\nfact some studios don’t have a separate ship build for this reason—they sim-\nply ship their development build once it has been thoroughly tested (but with\nthe debugging information stripped out).\n2.2.4.4 Project Conﬁguration Tutorial\nRight-clicking on any project in the Solution Explorer and selecting “Proper-\nties…” from the menu brings up the project’s “Property Pages” dialog. The\ntree view on the left shows various categories of settings. Of these, the four\nwe will use most are:\n• Configuration Properties/General,\n• Configuration Properties/Debugging,\n• Configuration Properties/C++, and\n• Configuration Properties/Linker.\nConﬁgurations Drop-Down Combo Box\nNoticethedrop-downcomboboxlabeled“Configuration:” atthetop-leftcor-\nner of the window. All of the properties displayed on these property pages\napply separately to each build configuration. If you set a property for the de-\nbug configuration, this does not necessarily mean that the same setting exists\nfor the other configuration(s).\nIf you click on the combo box to drop down the list, you’ll find that you\ncan select a single configuration or multiple configurations, including “All\nconfigurations.” As a rule of thumb, try to do most of your build configu-\nration editing with “All configurations” selected. That way, you won’t have\nto make the same edits multiple times, once for each configuration—and you\ndon’t risk setting things up incorrectly in one of the configurations by acci-\ndent. However, be aware that some settings needto be different between the\ndebug and development configurations. For example, function inlining and\ncode optimization settings should, of course, be different between debug and\ndevelopment builds.\nGeneral Property Page\nOn theGeneral property page, shown in Figure 2.9, the most useful fields are\nthe following:\n•Outputdirectory . Thisdefineswherethefinalproduct(s)ofthebuild will\ngo—namely, the executable, library or DLL that the compiler/linker ul-\ntimately outputs.\n2.2. Compilers, Linkers and IDEs 89\nFigure 2.9. Visual Studio project property pages—General page.\n•Intermediate directory . This defines where intermediate files, primarily\nobject files (.obj extension), are placed during a build. Intermediate files\narenevershippedwithyourfinalprogram—theyareonlyrequireddur-\ning the process of building your executable, library or DLL. Hence, it is\na good idea to place intermediate files in a different directory than the\nfinal products (.exe, .lib or .dll files).\nNotethatVisualStudioprovidesamacrofacility,whichmaybeusedwhen\nspecifyingdirectoriesandothersettingsinthe“ProjectPropertyPages”dialog.\nAmacrois essentially a named variable that contains a global value and that\ncan be referred to in your project configuration settings.\nMacros are invoked by writing the name of the macro enclosed in paren-\nthesesandprefixedwithadollarsign(e.g., $(ConfigurationName)). Some\ncommonly used macros are listed below.\n•$(TargetFileName). Thenameofthefinalexecutable,libraryorDLL\nfile being built by this project.\n•$(TargetPath). The full path of the folder containing the final exe-\ncutable, library or DLL.\n•$(ConfigurationName) . The name of the build config, which will\nbe “Debug” or “Release” by default in Visual Studio, although as we’ve\nsaid,arealgameprojectwillprobablyhavemultipleconfigurationssuch\nas “Debug,” “Hybrid,” “Development” and “Ship.”\n•$(OutDir). The value of the “Output Directory” field specified in this\ndialog.\n90 2. Tools of the Trade\n•$(IntDir). The value of the “Intermediate Directory” field in this dia-\nlog.\n•$(VCInstallDir) . The directory in which Visual Studio’s C++ stan-\ndard library is currently installed.\nThe benefit of using macros instead of hard-wiring your configuration set-\ntings is that a simple change of the global macro’s value will automatically af-\nfect all configuration settings in which the macro is used. Also, some macros\nlike$(ConfigurationName) automatically change their values depending\non the build configuration, so using them can permit you to use identical set-\ntings across all your configurations.\nToseeacompletelistofallavailablemacros,clickineitherthe“OutputDi-\nrectory” field or the “Intermediate Directory” field on the “General” property\npage,clickthelittlearrowtotherightofthetextfield,select“Edit…”andthen\nclick the “Macros” button in the dialog that comes up.\nDebugging Property Page\nThe “Debugging” property page is where the name and location of the exe-\ncutabletodebugisspecified. Onthispage,youcanalsospecifythecommand-\nline argument(s) that should be passed to the program when it runs. We’ll\ndiscuss debugging your program in more depth below.\nC/C++ Property Page\nThe C/C++ property page controls compile-time language settings—things\nthat affect how your source files will be compiled into object files (.obj exten-\nsion). The settings on this page do notaffect how your object files are linked\ninto a final executable or DLL.\nYou are encouraged to explore the various subpages of the C/C++ page\nto see what kinds of settings are available. Some of the most commonly used\nsettings include the following:\n•General Property Page/Additional Include Directories . This field lists the\non-disk directories that will be searched when looking for #included\nheader files.\nImportant : It is always best to specify these directories using relative\npathsand/orwithVisualStudiomacroslike $(OutDir) or$(IntDir).\nThatway,ifyoumoveyourbuildtreetoadifferentlocationondiskorto\nanother computer with a different root folder, everything will continue\nto work properly.\n2.2. Compilers, Linkers and IDEs 91\n•General Property Page/Debug Information Format. This field controls\nwhetherornotdebuginformationisgeneratedandinwhatformat. Typ-\nically both debug and development configurations include debugging\ninformation so that you can track down problems during the develop-\nment of your game. The ship build will have all the debug info stripped\nout to prevent hacking.\n•Preprocessor Property Page/Preprocessor Definitions . This very handy field\nlistsanynumberofC/C++preprocessorsymbolsthatshouldbedefined\ninthecodewhenitiscompiled. See PreprocessorSettings inSection2.2.4.1\nfor a discussion of preprocessor-defined symbols.\nLinker Property Page\nThe “Linker” property page lists properties that affect how your object code\nfiles will be linked into an executable or DLL. Again, you are encouraged to\nexplore the various subpages. Some commonly used settings follow:\n•GeneralPropertyPage/OutputFile. Thissettingliststhenameandlocation\nof the final product of the build, usually an executable or DLL.\n•GeneralPropertyPage/AdditionalLibraryDirectories . MuchliketheC/C++\nAdditional Include Directories field, this field lists zero or more directo-\nries that will be searched when looking for libraries and object files to\nlink into the final executable.\n•Input Property Page/Additional Dependencies. This field lists external li-\nbraries that you want linked into your executable or DLL. For example,\nthe OGRE libraries would be listed here if you are building an OGRE-\nenabled application.\nNotethatVisualStudioemploysvarious“magicspells”tospecifylibraries\nthat should be linked into an executable. For example, a special #pragma in-\nstruction in your source code can be used to instruct the linker to automat-\nically link with a particular library. For this reason, you may not see all of\nthelibrariesyou’reactuallylinkingtointhe“AdditionalDependencies”field.\n(In fact, that’s why they are called additional dependencies.) You may have\nnoticed, for example, that Direct X applications do not list all of the DirectX\nlibraries manually in their “Additional Dependencies” field. Now you know\nwhy.\n2.2.5 Debugging Your Code\nOne of the most important skills any programmer can learn is how to effec-\ntively debug code. This section provides some useful debugging tips and\n92 2. Tools of the Trade\ntricks. SomeareapplicabletoanydebuggerandsomearespecifictoMicrosoft\nVisualStudio. However, youcanusuallyfindanequivalenttoVisualStudio’s\ndebugging features in other debuggers, so this section should prove useful\neven if you don’t use Visual Studio to debug your code.\n2.2.5.1 The Start-Up Project\nA Visual Studio solution can contain more than one project. Some of these\nprojects build executables, while others build libraries or DLLs. It’s possible\nto have more than one project that builds an executable in a single solution.\nVisual Studio provides a setting known as the “Start-Up Project.” This is the\nproject that is considered “current” for the purposes of the debugger. Typi-\ncally a programmer will debug one project at a time by setting a single start-\nup project. However, it is possible to debug multiple projects simultaneously\n(see http://msdn.microsoft.com/en-us/library/0s590bew(v=vs.100).aspx for\ndetails).\nThe start-up project is highlighted in bold in the Solution Explorer. By de-\nfault, hitting F5 will run the .exe built by the start-up project, if the start-up\nproject builds an executable. (Technically speaking, F5 runs whatever com-\nmand you type into the Command field in the Debugging property page, so\nit’s not limited to running the .exe built by your project.)\n2.2.5.2 Breakpoints\nBreakpoints arethebreadandbutterofcodedebugging. Abreakpointinstructs\nthe program to stop at a particular line in your source code so that you can\ninspect what’s going on.\nIn Visual Studio, select a line and hit F9 to toggle a breakpoint. When you\nrunyourprogramandthelineofcodecontainingthebreakpointisabouttobe\nexecuted, the debugger will stop the program. We say that the breakpoint has\nbeen“hit.” AlittlearrowwillshowyouwhichlineofcodetheCPU’sprogram\ncounter is currently on. This is shown in Figure 2.10.\n2.2.5.3 Stepping through Your Code\nOnce a breakpoint has been hit, you can single-step your code by hitting the\nF10 key. The yellow program-counter arrow moves to show you the lines as\nthey execute. Hitting F11 steps intoa function call (i.e., the next line of code\nyou’ll see is the first line of the called function), while F10 steps overthat func-\ntion call (i.e., the debugger calls the function at full speed and then breaks\nagain on the line right after the call).\n2.2. Compilers, Linkers and IDEs 93\nFigure 2.10. Setting a breakpoint in Visual Studio.\nFigure 2.11. The call stack window.\n2.2.5.4 The Call Stack\nThecall stack window, shown in Figure 2.11, shows you the stack of functions\nthathavebeencalledatany givenmomentduringtheexecutionofyour code.\n(See Section 3.3.5.2 for more details on a program’s stack.) To display the call\nstack (if it is not already visible), go to the “Debug” menu on the main menu\nbar, select “Windows” and then “Call Stack.”\nOnce a breakpoint has been hit (or the program is manually paused), you\ncanmoveupanddownthecallstackbydouble-clickingonentriesinthe“Call\nStack” window. This is very useful for inspecting the chain of function calls\nthat were made between main() and the current line of code. For example,\nyou might trace back to the root cause of a bug in a parent function that has\nmanifested itself in a deeply nested child function.\n2.2.5.5 The Watch Window\nAsyoustepthroughyourcodeandmoveupanddownthecallstack,youwill\nwant to be able to inspect the values of the variables in your program. This\nis whatwatch windows are for. To open a watch window, go to the “Debug”\nmenu, select “Windows…,” then select “Watch…,” and finally select one of\n94 2. Tools of the Trade\nFigure 2.12. Visual Studio’s watch window.\n“Watch 1” through “Watch 4.” (Visual Studio allows you to open up to four\nwatchwindowssimultaneously.) Onceawatchwindowisopen,youcantype\nthe names of variables into the window or drag expressions in directly from\nyour source code.\nAs you can see in Figure 2.12, variables with simple data types are shown\nwiththeirvalueslistedimmediatelytotherightoftheirnames. Complexdata\ntypesareshownaslittletreeviewsthatcanbeeasilyexpandedto“drilldown”\nintovirtuallyanynestedstructure. Thebaseclassofaclassisalwaysshownas\nthe first child of an instance of a derived class. This allows you to inspect not\nonly the class’ data members, but also the data members of its base class(es).\nYou can type virtually any valid C/C++ expression into the watch window,\nand Visual Studio will evaluate that expression and attempt to display the re-\nsulting value. For example, you could type “5 + 3” and Visual Studio will\ndisplay“8.” YoucancastvariablesfromonetypetoanotherbyusingCorC++\ncastingsyntax. Forexample,typing“(float)intVar1/(float)intVar2 ”\nin the watch window will display the ratio of two integer variables as a\nfloating-point value.\nYou can even callfunctionsinyourprogram from within the watch window.\nVisual Studio reevaluates the expressions typed into the watch window(s)\nautomatically, so if you invoke a function in the watch window, it will be\ncalled every time you hit a breakpoint or single-step your code. This allows\nyou to leverage the functionality of your program in order to save yourself\nwork when trying to interpret the data that you’re inspecting in the debug-\nger. For example, let’s say that your game engine provides a function called\nquatToAngleDeg(), which converts a quaternion to an angle of rotation in\ndegrees. You can call this function in the watch window in order to easily\n2.2. Compilers, Linkers and IDEs 95\ninspect the rotation angle of any quaternion within the debugger.\nYou can also use various suffixes on the expressions in the watch window\nin order to change the way Visual Studio displays the data, as shown in Fig-\nure 2.13.\n• The “ ,d” suffix forces values to be displayed in decimal notation.\n• The “ ,x” suffix forces values to be displayed in hexadecimal notation.\n• The “ ,n” suffix (where nis any positive integer) forces Visual Studio to\ntreat the value as an array with nelements. This allows you to expand\narray data that is referenced through a pointer.\n• You can also write simple expressions in square brackets that calculate\nthevalueof nina“, n”suffix. Forexample,youcantypesomethinglike\nthis:\nmy_array,[my_array_count]\nto ask the debugger to show my_array_count elements of the array\nnamed my_array.\nBe careful when expanding very large data structures in the watch win-\ndow, because it can sometimes slow the debugger down to the point of being\nunusable.\n2.2.5.6 Data Breakpoints\nRegular breakpoints trip when the CPU’s program counter hits a particular\nmachine instruction or line of code. However, another incredibly useful fea-\nture of modern debuggers is the ability to set a breakpoint that trips when-\never a specific memory address is written to (i.e., changed). These are called\ndata breakpoints, because they are triggered by changes to data, or sometimes\nhardwarebreakpoints , because they are implemented via a special feature of the\nCPU’s hardware—namely, the ability to raise an interrupt when a predefined\nmemory address is written to.\nFigure 2.13. Comma sufﬁxes in the Visual Studio watch window.\n96 2. Tools of the Trade\nFigure 2.14. The Visual Studio breakpoints window.\n Figure 2.15. Deﬁning a data breakpoint.\nHere’s how data breakpoints are typically used. Let’s say you are tracking\ndown a bug that manifests itself as a zero ( 0.0f) value mysteriously appear-\ning inside a member variable of a particular object called m_angle thatshould\nalways contain a nonzero angle. You have no idea which function might be\nwriting that zero into your variable. However, you do know the address of\nthevariable. (Youcanjusttype“&object.m_angle”intothewatchwindow\nto find its address.) To track down the culprit, you can set a data breakpoint\non the address of object.m_angle, and then simply let the program run.\nWhen the value changes, the debugger will stop automatically. You can then\ninspect the call stack to catch the offending function red-handed.\nTo set a data breakpoint in Visual Studio, take the following steps.\n• Bringupthe“Breakpoints”windowfoundonthe“Debug”menuunder\n“Windows” and then “Breakpoints” (Figure 2.14).\n• Select the “New” drop-down button in the upper-left corner of the win-\ndow.\n• Select “New Data Breakpoint.”\n• Type in the raw address or an address-valued expression, such as\n“&myVariable ” (Figure 2.15).\n2.2.5.7 Conditional Breakpoints\nYou’llalsonoticeinthe“Breakpoints”windowthatyoucansetconditionsand\nhit counts on any type breakpoint—data breakpoints or regular line-of-code\nbreakpoints.\nAconditional breakpoint causes the debugger to evaluate the C/C++ ex-\npression you provide every time the breakpoint is hit. If the expression is\ntrue, the debugger stops your program and gives you a chance to see what’s\ngoing on. If the expression is false, the breakpoint is ignored and the pro-\ngram continues. This is very useful for setting breakpoints that only trip\nwhen a function is called on a particular instance of a class. For example,\n2.2. Compilers, Linkers and IDEs 97\nlet’s say you have a game level with 20 tanks on-screen, and you want to stop\nyour program when the third tank, whose memory address you know to be\n0x12345678, is running. By setting the breakpoint’s condition expression to\nsomething like “ (uintptr_t)this == 0x12345678”, you can restrict the\nbreakpoint only to the class instance whose memory address (this pointer)\nis 0x12345678.\nSpecifying a hitcount for a breakpoint causes the debugger to decrement a\ncounter every time the breakpoint is hit, and only actually stop the program\nwhen that counter reacheszero. This is reallyuseful for situations whereyour\nbreakpoint is inside a loop, and you need to inspect what’s happening during\nthe 376th iteration of the loop (e.g., the 376th element in an array). You can’t\nvery well sit there and hit the F5 key 375 times! But you canlet the hit count\nfeature of Visual Studio do it for you.\nOne note of caution: conditional breakpoints cause the debugger to evalu-\natetheconditionalexpressioneverytimethebreakpointishit,sotheycanbog\ndown the performance of the debugger and your game.\n2.2.5.8 Debugging Optimized Builds\nI mentioned above that it can be very tricky to debug problems using a de-\nvelopment or ship build, due primarily to the way the compiler optimizes the\ncode. Ideally, every programmer would prefer to do all of his or her debug-\nging in a debug build. However, this is often not possible. Sometimes a bug\noccurssorarelythatyou’lljumpatanychancetodebugtheproblem,evenifit\noccurs in a non-debug build on someone else’s machine. Other bugs only oc-\ncurinyournon-debugbuilds,buttheymagicallydisappearwheneveryourun\nthe debug build. These dreaded non-debug-onlybugs are sometimes caused by\nuninitialized variables, because variables and dynamically allocated memory\nblocks are often set to zero in debug mode but are left containing garbage in a\nnon-debugbuild. Othercommoncausesofnon-debug-onlybugsincludecode\nthathasbeenaccidentallyomittedfromthenon-debugbuild(s)(e.g.,whenim-\nportant code is erroneously placed inside an assertion statement), data struc-\ntures whose size or data member packing changes between debug and de-\nvelopment/ship builds, bugs that are only triggered by inlining or compiler-\nintroducedoptimizations, and(inrarecases)bugsinthecompiler’soptimizer\nitself, causing it to emit incorrect code in a fully optimized build.\nClearly, it behooves every programmer to be capable of debugging prob-\nlemsinanon-debugbuild,unpleasantasitmayseem. Thebestwaystoreduce\nthe pain of debugging optimized code is to practice doing it and to expand\nyour skill set in this area whenever you have the opportunity. Here are a few\ntips.\n98 2. Tools of the Trade\n•Learn to read and step through disassembly in the debugger . In a non-debug\nbuild, the debugger often has trouble keeping track of which line of\nsource code is currently being executed. Thanks to instruction reorder-\ning,you’lloftenseetheprogramcounterjumparounderraticallywithin\nthe function when viewed in source code mode. However, things be-\ncome sane again when you work with the code in disassembly mode\n(i.e., step through the assembly language instructions individually). Ev-\nery C/C++ programmer should be at least a little bit familiar with the\narchitecture and assembly language of their target CPU(s). That way,\neven if the debugger is confused, you won’t be. (For an introduction to\nassembly language, see Section 3.4.7.3.)\n•Use registers to deduce variables’ values or addresses . The debugger will\nsometimes be unable to display the value of a variable or the contents\nof an object in a non-debug build. However, if the program counter\nis not too far away from the initial use of the variable, there’s a good\nchance its address or value is still stored in one of the CPU’s registers. If\nyou can trace back through the disassembly to where the variable is first\nloaded into a register, you can often discover its value or its address by\ninspecting that register. Use the register window, or type the name of\nthe register into a watch window, to see its contents.\n•Inspectvariablesandobjectcontentsbyaddress . Giventheaddressofavari-\nable or data structure, you can usually see its contents by casting the\naddress to the appropriate type in a watch window. For example, if we\nknowthataninstanceofthe Fooclassresidesataddress0x1378A0C0,we\ncan type “(Foo*)0x1378A0C0” in a watch window, and the debugger\nwillinterpret thatmemoryaddressasifitwereapointertoa Fooobject.\n•Leveragestaticandglobalvariables. Eveninanoptimizedbuild,thedebug-\nger can usually inspect global and static variables. If you cannot deduce\nthe address of a variable or object, keep your eye open for a static or\nglobal that might contain its address, either directly or indirectly. For\nexample, if we want to find the address of an internal object within the\nphysics system, we might discover that it is in fact stored in a member\nvariable of the global PhysicsWorld object.\n•Modify the code. If you can reproduce a non-debug-only bug relatively\neasily, consider modifying the source code to help you debug the prob-\nlem. Add print statements so you can see what’s going on. Introduce\na global variable to make it easier to inspect a problematic variable or",46063
17-2.3 Profiling Tools.pdf,17-2.3 Profiling Tools,"2.3. Proﬁling Tools 99\nobject in the debugger. Add code to detect a problem condition or to\nisolate a particular instance of a class.\n2.3 Proﬁling Tools\nGamesaretypicallyhigh-performancereal-timeprograms. Assuch,gameen-\ngine programmers are always looking for ways to speed up their code. In this\nsection, we’ll investigate some of the tools we can use to measure the perfor-\nmance of our software. We’ll have more to say about how to use this profiling\ndata in order to optimize our software in Chapter 4.\nThere is a well-known, albeit rather unscientific, rule of thumb known as\nthePareto principle (see http://en.wikipedia.org/wiki/Pareto_principle). It is\nalso known as the 80/20 rule, because it states that in many situations, 80%\nof the effects of some event come from only 20% of the possible causes. In\ncomputer science, this principle has been applied both to bug-fixing (80% of\nthe perceived bugs in a piece of software can be eliminated by fixing bugs in\nonly 20% of the code), and to software optimization, where as a rule of thumb\n80% (or more) of the wall clock time spent running any piece of software is\naccountedforbyonly20%(orless)ofthecode. Inotherwords,ifyouoptimize\n20% of your code, you can potentially realize 80% of all the gains in execution\nspeed you’ll ever realize.\nSo, how do you know which20% of your code to optimize? For that, you\nneed aprofiler. A profiler is a tool that measures the execution time of your\ncode. Itcantellyouhowmuchtimeisspentineachfunction. Youcanthendi-\nrectyouroptimizationstowardonlythosefunctionsthataccountforthelion’s\nshare of the execution time.\nSome profilers also tell you how many timeseach function is called. This\nis an important dimension to understand. A function can eat up time for two\nreasons: (a) it takes a long time to execute on its own, or (b) it is called fre-\nquently. For example, a function that runs an A* algorithm to compute the\noptimal paths through the game world might only be called a few times each\nframe\nEven more information can be obtained if you use the right profiler. Some\nprofilers report the call graph, meaning that for any given function, you can\nsee which functions called it (these are known as parent functions ) and which\nfunctions it called (these are known as child functions ordescendants). You can\neven see what percentage of the function’s time was spent calling each of its\ndescendants and the percentage of the overall running time accounted for by\neach individual function.\n100 2. Tools of the Trade\nProfilers fall into two broad categories.\n1.Statistical profilers . This kind of profiler is designed to be unobtrusive,\nmeaning that the target code runs at almost the same speed, whether or\nnot profiling is enabled. These profilers work by sampling the CPU’s\nprogram counter register periodically and noting which function is cur-\nrently running. The number of samples taken within each function\nyields an approximate percentage of the total running time that is eaten\nup by that function. Intel’s VTuneis the gold standard in statistical pro-\nfilersforWindowsmachinesemployingIntelPentiumprocessors,andit\nis now also available for Linux. See https://software.intel.com/en-us/\nintel-vtune-amplifier-xe for details.\n2.Instrumenting profilers . This kind of profiler is aimed at providing the\nmost accurate and comprehensive timing data possible, but at the ex-\npense of real-time execution of the target program—when profiling is\nturned on, the target program usually slows to a crawl. These profil-\ners work by preprocessing your executable and inserting special pro-\nlogueandepiloguecodeintoeveryfunction. Theprologueandepilogue\ncode calls into a profiling library, which in turn inspects the program’s\ncall stack and records all sorts of details, including which parent func-\ntion called the function in question and how many times that parent\nhas called the child. This kind of profiler can even be set up to moni-\ntor every line of code in your source program, allowing it to report how\nlong each line is taking to execute. The results are stunningly accurate\nand comprehensive, but turning on profiling can make a game virtu-\nally unplayable. IBM’s Rational Quantify, available as part of the Ra-\ntional Purify Plus tool suite, is an excellent instrumenting profiler. See\nhttp://www.ibm.com/developerworks/rational/library/957.html for\nan introduction to profiling with Quantify.\nMicrosoft has also published a profiler that is a hybrid between the two\napproaches. It is called LOP, which stands for low-overhead profiler. LOP\nuses a statistical approach, sampling the state of the processor periodically,\nwhich means it has a low impact on the speed of the program’s execution.\nHowever, with each sample, it analyzes the call stack, thereby determining\nthe chain of parent functions that resulted in each sample. This allows LOP to\nprovide information normally not available with a statistical profiler, such as\nthe distribution of calls across parent functions.\nOn the PlayStation 4, SN Systems’ Razor CPU is the profiler of choice for\nmeasuring game software running on the PS4’s CPU. It supports both statis-",5204
18-2.5 Other Tools.pdf,18-2.5 Other Tools,"2.4. Memory Leak and Corruption Detection 101\ntical and instrumenting methods of profiling. (See https://www.snsystems.\ncom/tech-blog/2014/02/14/function-level-profiling/ for some more de-\ntails.) Its counterpart, Razor GPU, provides profiling and debugging facilities\nfor shaders and compute jobs running on the PS4’s GPU.\n2.3.1 List of Proﬁlers\nThereareagreatmanyprofilingtoolsavailable. Seehttp://en.wikipedia.org/\nwiki/List_of_performance_analysis_tool for a reasonably comprehensive list.\n2.4 Memory Leak and Corruption Detection\nTwo other problems that plague C and C++ programmers are memory leaks\nand memory corruption. A memory leak occurs when memory is allocated\nbut never freed. This wastes memory and eventually leads to a potentially fa-\ntal out-of-memory condition. Memory corruption occurs when the program\ninadvertently writes data to the wrong memory location, overwriting the im-\nportantdatathatwasthere—whilesimultaneouslyfailingtoupdatethemem-\nory location where that data shouldhave been written. Blame for both of these\nproblems falls squarely on the language feature known as the pointer.\nA pointer is a powerful tool. It can be an agent of good when used prop-\nerly—but it can also be all-too-easily transformed into an agent of evil. If a\npointer points to memory that has been freed, or if it is accidentally assigned\na nonzero integer or floating-point value, it becomes a dangerous tool for cor-\nrupting memory, because data written through it can quite literally end up\nanywhere. Likewise, when pointers are used to keep track of allocated mem-\nory, it is all too easy to forget to free the memory when it is no longer needed.\nThis leads to memory leaks.\nClearly, good coding practices are one approach to avoiding pointer-\nrelated memory problems. And it is certainly possible to write solid code\nthat essentially never corrupts or leaks memory. Nonetheless, having a tool\nto help you detect potential memory corruption and leak problems certainly\ncan’t hurt. Thankfully, many such tools exist.\nMy personal favorite is IBM’s Rational Purify, which comes as part of the\nPurify Plus toolkit. Purify instruments your code prior to running it, in order\nto hook into all pointer dereferences and all memory allocations and deallo-\ncations made by your code. When you run your code under Purify, you get\na live report of the problems—real and potential—encountered by your code.\nAnd when the program exits, you get a detailed memory leak report. Each\nproblem is linked directly to the source code that caused the problem, mak-\n102 2. Tools of the Trade\ning tracking down and fixing these kinds of problems relatively easy. You\ncan find more information on Purify at http://www-306.ibm.com/software/\nawdtools/purify.\nTwo other popular tools are Insure++ by Parasoft, and Valgrind by Julian\nSeward and the Valgrind development team. These tools provide both mem-\nory debugging and profiling facilities.\n2.5 Other Tools\nThere are a number of other commonly used tools in a game programmer’s\ntoolkit. We won’t cover them in any depth here, but the following list will\nmake you aware of their existence and point you in the right direction if you\nwant to learn more.\n•Difference tools. A difference tool, or diff tool, is a program that compares\ntwo versions of a text file and determines what has changed between\nthem. (See http://en.wikipedia.org/wiki/Diff for a discussion of diff\ntools.) Diffsareusuallycalculatedonaline-by-linebasis,althoughmod-\nern diff tools can also show you a range of characters on a changed line\nthat have been modified. Most version control systems come with a diff\ntool. Some programmers like a particular diff tool and configure their\nversion control software to use the tool of their choice. Popular tools in-\ncludeExamDiff(http://www.prestosoft.com/edp_examdiff.asp),Arax-\nisMerge (http://www.araxis.com), WinDiff (available in the Options\nPacks for most Windows versions and available from many indepen-\ndent websites as well), and the GNU diff tools package (http://www.\ngnu.org/software/diffutils/diffutils.html).\n•Three-way merge tools. When two people edit the same file, two inde-\npendent sets of diffs are generated. A tool that can merge two sets of\ndiffs into a final version of the file that contains both person’s changes is\ncalled a three-way merge tool. The name “three-way” refers to the fact\nthat three versions of the file are involved: the original, user A’s version\nand user B’s version. (See http://en.wikipedia.org/wiki/3-way_merge\n#Three-way_merge for a discussion of two-way and three-way merge\ntechnologies.) Manymergetoolscomewithanassociateddifftool. Some\npopularmergetoolsincludeAraxisMerge(http://www.araxis.com)and\nWinMerge(http://winmerge.org). Perforcealsocomeswithanexcellent\nthree-way merge tool (http://www.perforce.com/perforce/products/\nmerge.html).\n2.5. Other Tools 103\n•Hex editors . A hex editor is a program used for inspecting and mod-\nifying the contents of binary files. The data are usually displayed as\nintegers in hexadecimal format, hence the name. Most good hex ed-\nitors can display data as integers from one byte to 16 bytes each, in\n32- and 64-bit floating-point format and as ASCII text. Hex editors are\nparticularly useful when tracking down problems with binary file for-\nmats or when reverse-engineering an unknown binary format—both\nof which are relatively common endeavors in game engine develop-\nment circles. There are quite literally a million different hex editors\nout there; I’ve had good luck with HexEdit by Expert Commercial Soft-\nware (http://www.expertcomsoft.com/index.html), but your mileage\nmay vary.\nAs a game engine programmer you will undoubtedly come across other\ntools that make your life easier, but I hope this chapter has covered the main\ntools you’ll use on a day-to-day basis.\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",5970
19-3 Fundamentals of Software Engineering for Games.pdf,19-3 Fundamentals of Software Engineering for Games,,0
20-3.1 C Review and Best Practices.pdf,20-3.1 C Review and Best Practices,"3\nFundamentals of Software\nEngineering for Games\nIn this chapter, we’ll discuss the foundational knowledge needed by any\nprofessional game programmer. We’ll explore numeric bases and represen-\ntations, the components and architecture of a typical computer and its CPU,\nmachine and assembly language, and the C++ programming language. We’ll\nreview some key concepts in object-oriented programming (OOP), and then\ndelveintosomeadvancedtopicsthatshouldproveinvaluableinanysoftware\nengineeringendeavor(andespeciallywhencreatinggames). AswithChapter\n2, some of this material may already be familiar to some readers. However,\nI highly recommend that all readers at least skim this chapter, so that we all\nembark on our journey with the same set of tools and supplies.\n3.1 C++ Review and Best Practices\nBecause C++ is arguably the most commonly used language in the game\nindustry, we will focus primarily on C++ in this book. However, most of\nthe concepts we’ll cover apply equally well to anyobject-oriented program-\nming language. Certainly a great many other languages are used in the game\nindustry—imperative languages like C; object-oriented languages like C# and\n105\n106 3. Fundamentals of Software Engineering for Games\nJava; scripting languages like Python, Lua and Perl; functional languages like\nLisp, SchemeandF#, andthelistgoeson. Ihighlyrecommendthateverypro-\ngrammerlearnatleasttwohigh-levellanguages(themorethemerrier),aswell\nas learning at least some assembly language programming (see Section 3.4.7.3).\nEvery new language that you learn further expands your horizons and allows\nyoutothinkinamoreprofoundandproficientwayaboutprogrammingover-\nall. That being said, let’s turn our attention now to object-oriented program-\nming concepts in general, and C++ in particular.\n3.1.1 Brief Review of Object-Oriented Programming\nMuchofwhatwe’lldiscussinthisbookassumesyouhaveasolidunderstand-\ning of the principles of object-oriented design. If you’re a bit rusty, the follow-\ning section should serve as a pleasant and quick review. If you have no idea\nwhat I’m talking about in this section, I recommend you pick up a book or\ntwoonobject-orientedprogramming(e.g.,[7])andC++inparticular(e.g.,[46]\nand [36]) before continuing.\n3.1.1.1 Classes and Objects\nAclassis a collection of attributes (data) and behaviors (code) that together\nform a useful, meaningful whole. A class is a specification describing how in-\ndividual instances of the class, known as objects, should be constructed. For\nexample, your pet Rover is an instance of the class “dog.” Thus, there is a\none-to-many relationship between a class and its instances.\n3.1.1.2 Encapsulation\nEncapsulation means that an object presents only a limited interface to the out-\nsideworld; theobject’sinternalstateandimplementationdetailsarekepthid-\nden. Encapsulation simplifies life for the user of the class, because he or she\nneed only understand the class’ limited interface, not the potentially intricate\ndetails of its implementation. It also allows the programmer who wrote the\nclass to ensure that its instances are always in a logically consistent state.\n3.1.1.3 Inheritance\nInheritance allowsnewclassestobedefinedas extensions topreexistingclasses.\nThe new class modifies or extends the data, interface and/or behavior of the\nexisting class. If class Child extends class Parent, we say that Childin-\nherits from or isderived from Parent. In this relationship, the class Parent is\nknown as the base class orsuperclass , and the class Child is thederived class\n3.1. C++ Review and Best Practices 107\nFigure 3.1. UML static class diagram depicting a simple class hierarchy.\norsubclass. Clearly, inheritance leads to hierarchical (tree-structured) relation-\nships between classes.\nInheritance creates an “is-a” relationship between classes. For example,\na circleis atype of shape. So, if we were writing a 2D drawing application,\nit would probably make sense to derive our Circle class from a base class\ncalled Shape.\nWe can draw diagrams of class hierarchies using the conventions defined\nby the Unified Modeling Language (UML). In this notation, a rectangle rep-\nresents a class, and an arrow with a hollow triangular head represents inheri-\ntance. The inheritance arrow points from child class to parent. See Figure 3.1\nfor an example of a simple class hierarchy represented as a UML static class\ndiagram.\nMultiple Inheritance\nSome languages support multiple inheritance (MI), meaning that a class can\nhave more than one parent class. In theory MI can be quite elegant, but in\npractice this kind of design usually gives rise to a lot of confusion and techni-\ncaldifficulties(seehttp://en.wikipedia.org/wiki/Multiple_inheritance). This\nisbecausemultipleinheritancetransformsasimple treeofclassesintoapoten-\ntially complex graph. A class graph can have all sorts of problems that never\nplague a simple tree—for example, the deadly diamond (http://en.wikipedia.\norg/wiki/Diamond_problem), in which a derived class ends up containing\ntwo copies of a grandparent base class (see Figure 3.2). (In C++, virtual inher-\nitanceallows one to avoid this doubling of the grandparent’s data.) Multiple\ninheritance also complicates casting, because the actual address of a pointer\nmay change depending on which base class it is cast to. This happens because\nof the presence of multiple vtable pointers within the object.\nMost C++ software developers avoid multiple inheritance completely or\nonly permit it in a limited form. A common rule of thumb is to allow only\nsimple, parentless classes to be multiply inherited into an otherwise strictly\nsingle-inheritance hierarchy. Such classes are sometimes called mix-in classes\n108 3. Fundamentals of Software Engineering for Games\nClassA\nClassB ClassC\nClassDClassA\nClassA\nClassBClassB’s\nmemo ry layout:ClassA’s\nmemory layout:\nClassA\nClassCClassC’s\nmemory layout:\nClassA\nClassBClassD’s\nmemory layout:\nClassA\nClassC\nClassD\nFigure 3.2. “Deadly diamond” in a multiple inheritance hierarchy.\n+Draw()Shape\n+Draw()Circle\n+Draw()Rectangle\n+Draw()Triangle+Animate()AnimatorAnimator is a hypothetical mix-in\nclass that adds animation\nfunctionality to whatever class itis inherited by.\nFigure 3.3. Example of a mix-in class.\n3.1. C++ Review and Best Practices 109\nbecause they can be used to introduce new functionality at arbitrary points in\naclasstree. SeeFigure3.3forasomewhatcontrivedexampleofamix-inclass.\n3.1.1.4 Polymorphism\nPolymorphism is a language feature that allows a collection of objects of differ-\nenttypestobemanipulatedthroughasingle commoninterface. Thecommonin-\nterfacemakesa heterogeneouscollectionofobjects appeartobehomogeneous,\nfrom the point of view of the code using the interface.\nForexample,a2Dpaintingprogrammightbegivenalistofvariousshapes\nto draw on-screen. One way to draw this heterogeneous collection of shapes\nis to use a switch statement to perform different drawing commands for each\ndistinct type of shape.\nvoid drawShapes(std::list<Shape*>& shapes)\n{\nstd::list<Shape*>::iterator pShape = shapes.begin();\nstd::list<Shape*>::iterator pEnd = shapes.end();\nfor ( ; pShape != pEnd; pShape++)\n{\nswitch (pShape->mType)\n{\ncase CIRCLE:\n// draw shape as a circle\nbreak;\ncase RECTANGLE:\n// draw shape as a rectangle\nbreak;\ncase TRIANGLE:\n// draw shape as a triangle\nbreak;\n//...\n}\n}\n}\nTheproblemwiththisapproachisthatthe drawShapes() functionneeds\nto “know” about all of the kinds of shapes that can be drawn. This is fine in a\nsimple example, but as our code grows in size and complexity, it can become\ndifficult to add new types of shapes to the system. Whenever a new shape\n110 3. Fundamentals of Software Engineering for Games\ntype is added, one must find every place in the code base where knowledge\nof the set of shape types is embedded—like this switch statement—and add a\ncase to handle the new type.\nThe solution is to insulate the majority of our code from any knowledge of\nthetypesofobjectswithwhichitmightbedealing. Toaccomplishthis,wecan\ndefine classes for each of the types of shapes we wish to support. All of these\nclasses would inherit from the common base class Shape. A virtualfunction —\nthe C++ language’s primary polymorphism mechanism—would be defined\ncalled Draw() , and each distinct shape class would implement this function\nin a different way. Without “knowing” what specific types of shapes it has\nbeen given, the drawing function can now simply call each shape’s Draw()\nfunction in turn.\nstruct Shape\n{\nvirtual void Draw() = 0; // pure virtual function\nvirtual ~Shape() { } // ensure derived dtors are virtual\n};\nstruct Circle : public Shape\n{\nvirtual void Draw()\n{\n// draw shape as a circle\n}\n};\nstruct Rectangle : public Shape\n{\nvirtual void Draw()\n{\n// draw shape as a rectangle\n}\n};\nstruct Triangle : public Shape\n{\nvirtual void Draw()\n{\n// draw shape as a triangle\n}\n};\n3.1. C++ Review and Best Practices 111\nvoid drawShapes(std::list<Shape*>& shapes)\n{\nstd::list<Shape*>::iterator pShape = shapes.begin();\nstd::list<Shape*>::iterator pEnd = shapes.end();\nfor ( ; pShape != pEnd; pShape++)\n{\npShape->Draw (); // call virtual function\n}\n}\n3.1.1.5 Composition and Aggregation\nComposition is the practice of using a groupofinteracting objects to accomplish\na high-level task. Composition creates a “has-a” or “uses-a” relationship be-\ntween classes. (Technically speaking, the “has-a” relationship is called com-\nposition, while the “uses-a” relationship is called aggregation.) For example, a\nspaceship hasanengine, which in turn hasafuel tank. Composition/aggrega-\ntion usually results in the individual classes being simpler and more focused.\nInexperienced object-oriented programmers often rely too heavily on inheri-\ntance and tend to underutilize aggregation and composition.\nAsanexample,imaginethatwearedesigningagraphicaluserinterfacefor\nourgame’sfrontend. Wehaveaclass Window thatrepresentsanyrectangular\nGUI element. We also have a class called Rectangle that encapsulates the\nmathematical concept of a rectangle. A naïve programmer might derive the\nWindow class from the Rectangle class (using an “is-a” relationship). But in\na more flexible and well-encapsulated design, the Window class would referto\norcontainaRectangle (employing a “has-a” or “uses-a” relationship). This\nmakesbothclassessimplerandmorefocusedandallowstheclassestobemore\neasily tested, debugged and reused.\n3.1.1.6 Design Patterns\nWhen the same type of problem arises over and over, and many different pro-\ngrammersemployaverysimilarsolutiontothatproblem,wesaythata design\npatternhas arisen. In object-oriented programming, a number of common de-\nsignpatternshavebeenidentifiedanddescribedbyvariousauthors. Themost\nwell-known book on this topic is probably the “Gang of Four” book [19].\nHere are a few examples of common general-purpose design patterns.\n•Singleton . This pattern ensures that a particular class has only one in-\nstance (the singleton instance) and provides a global point of access to it.\n•Iterator. Aniteratorprovidesanefficientmeansofaccessingtheindivid-\nualelementsofacollection,withoutexposingthecollection’sunderlying\n112 3. Fundamentals of Software Engineering for Games\nimplementation. Theiterator“knows”theimplementationdetailsofthe\ncollection so that its users don’t have to.\n•Abstract factory . An abstract factory provides an interface for creating\nfamiliesofrelatedordependentclasseswithoutspecifyingtheirconcrete\nclasses.\nThe game industry has its own set of design patterns for addressing prob-\nlems in every realm from rendering to collision to animation to audio. In a\nsense, this book is all about the high-level design patterns prevalent in mod-\nern 3D game engine design.\nJanitors and RAII\nAs one very useful example of a design pattern, let’s have a brief look at the\n“resourceacquisitionisinitialization”pattern(RAII).Inthispattern,theacqui-\nsitionandreleaseofaresource(suchasafile, ablockofdynamicallyallocated\nmemory, or a mutex lock) are bound to the constructor and destructor of a\nclass, respectively. This prevents programmers from accidentally forgetting\nto release the resource—you simply construct a local instance of the class to\nacquire the resource, and let it fall out of scope to release it automatically. At\nNaughty Dog, we call such classes janitors because they “clean up” after you.\nForexample,wheneverweneedtoallocatememoryfromaparticulartype\nofallocator,we pushthatallocatorontoaglobal allocatorstack,andwhenwe’re\ndone allocating we must always remember to popthe allocator off the stack.\nTomakethismoreconvenientandlesserror-prone,weusean allocationjanitor.\nThis tiny class’s constructor pushes the allocator, and its destructor pops the\nallocator:\nclass AllocJanitor\n{\npublic:\nexplicit AllocJanitor(mem::Context context)\n{\nmem::PushAllocator (context);\n}\n~AllocJanitor()\n{\nmem::PopAllocator ();\n}\n};\nTo use the janitor class, we simply construct a local instance of it. When this\ninstance drops out of scope, the allocator will be popped automatically:\n3.1. C++ Review and Best Practices 113\nvoid f()\n{\n// do some work...\n// allocate temp buffers from single-frame allocator\n{\nAllocJanitor janitor (mem::Context::kSingleFrame);\nU8* pByteBuffer = new U8[SIZE];\nfloat* pFloatBuffer = new float[SIZE];\n// use buffers...\n// (NOTE: no need to free the memory because we\n// used a single-frame allocator)\n}//janitor pops allocator when it drops out of scope\n// do more work...\n}\nSee http://en.cppreference.com/w/cpp/language/raii for more informa-\ntion on the highly useful RAII pattern.\n3.1.2 C++ Language Standardization\nSince its inception in 1979, the C++ language has been continually evolv-\ning. Bjarne Stroustrup, its inventor, originally named the language “C with\nClasses”, but it was renamed “C++” in 1983. The International Organiza-\ntionforStandardization(ISO,www.iso.org)firststandardizedthelanguagein\n1998—this version is known today as C++98. Since then, the ISO has been pe-\nriodically publishing updated standards for the C++ language, with the goals\nof making the language more powerful, easier to use, and less ambiguous.\nThese goals are achieved by refining the semantics and rules of the language,\nby adding new, more-powerful language features, and by deprecating, or re-\nmovingentirely,thoseaspectsofthelanguagewhichhaveprovenproblematic\nor unpopular.\nThe most-recent variant of the C++ programming language standard is\ncalled C++17, which was published on July 31, 2017. The next iteration of\nthe standard, C++2a, was in development at the time of this publication. The\nvarious versions of the C++ standard are summarized in chronological order\nbelow.\n•C++98wasthefirstofficialC++standard,establishedbytheISOin1998.\n•C++03was introduced in 2003, to address various problems that had\nbeen identified in the C++98 standard.\n114 3. Fundamentals of Software Engineering for Games\n•C++11(also known as C++0xduring much of its development) was ap-\nproved by the ISO on August 12, 2011. C++11 added a large number of\npowerful new features to the language, including:\n◦a type-safe nullptr literal, to replace the bug-prone NULLmacro\nthat had been inherited from the C language;\n◦theautoanddecltype keywords for type inference;\n◦a“trailingreturntype”syntax,whichallowsa decltype ofafunc-\ntion’sinputargumentstobeusedtodescribethereturntypeofthat\nfunction;\n◦theoverride andfinalkeywords for improved expressiveness\nwhen defining and overriding virtual functions;\n◦defaulted and deleted functions (allowing the programmer to ex-\nplicitly request that a compiler-generated default implementation\nbe used, or that a function’s implementation should be undefined);\n◦delegating constructors—the ability of one constructor to invoke\nanother within the same class;\n◦strongly-typed enumerations;\n◦theconstexpr keywordfordefiningcompile-timeconstantvalues\nby evaluating expressions at compile time;\n◦a uniform initialization syntax that extends the original braces-\nbased POD initializers to cover non-POD types as well;\n◦support for lambda functions and variable capture (closures);\n◦the introduction of rvalue references and move semantics for more\nefficient handling of temporary objects; and\n◦standardized attribute specifiers, to replace compiler-specific spec-\nifiers such as __attribute__((...)) and__declspec().\nC++11 also introduced an improved and expanded standard library,\nincluding support for threading (concurrent programming), improved\nsmart pointer facilities and an expanded set of generic algorithms.\n•C++14was approved by the ISO on August 18, 2014 and released on\nDecember 15, 2014. Its additions and improvements to C++11 include:\n◦return type deduction, which in many situations allows function\nreturn types to be declared using a simple autokeyword, without\nthe need for the verbose trailing decltype expression required by\nC++11;\n3.1. C++ Review and Best Practices 115\n◦genericlambdas,allowingalambdatoactlikeatemplatedfunction\nby using autoto declare its input arguments;\n◦the ability to initialize “captured” variables in lambdas;\n◦binary literals prefaced with 0b(e.g., 0b10110110 );\n◦support for digit separators in numeric literals, for improved read-\nability (e.g., 1'000'000 instead of 1000000);\n◦variable templates, which allow template syntax to be used when\ndeclaring variables; and\n◦relaxationofsomerestrictionson constexpr,includingtheability\nto use if,switch and loops within constant expressions.\n•C++17was published by the ISO on July 31, 2017. It extends and im-\nproves C++14 in many ways, including but not limited to the following:\n◦removed a number of out-dated and/or dangerous language\nfeatures, including trigraphs, the register keyword, and the\nalready-deprecated auto_ptr smart pointer class;\n◦guaranteescopy elision,theomissionofunnecessaryobjectcopying;\n◦exception specifications are now part of the type system, mean-\ning that void f() noexcept(true); andvoid f() noex-\ncept(false); are now distinct types;\n◦addstwonewliteralstothelanguage: UTF-8characterliterals(e.g.,\nu8'x'), and floating-point literals with a hexadecimal base and\ndecimal exponent (e.g., 0xC.68p+2);\n◦introduces structured bindings to C++, allowing the values in a col-\nlection data type to be “unpacked” into individual variables (e.g.,\nauto [a, b] = func_that_returns_a_pair();)—asyntax\nthat is strikingly similar to that of returning multiple values from a\nfunction via a tuple in Python;\n◦adds some useful standardized attributes, including\n[[fallthrough]] which allows you to explicitly document\nthe fact that a missing break statement in a switch is inten-\ntional, thereby suppressing the warning that would otherwise be\ngenerated.\n3.1.2.1 Further Reading\nThere are plenty of great online resources and books that describe the features\nofC++11,C++14andC++17indetail,sowewon’tattempttocoverthemhere.\nHere are a few useful references:\n116 3. Fundamentals of Software Engineering for Games\n• The site http://en.cppreference.com/w/cpp provides an excellent C++\nreference manual, including call-outs such as “since C++11” or “until\nC++17“ to indicate when certain language features were added to or re-\nmoved from the standard, respectively.\n• Information about the ISO’s standardization efforts can be found at\nhttps://isocpp.org/std.\n• See the following sites for good summaries of C++11’s major new fea-\ntures:\n◦https://www.codeproject.com/Articles/570638/Ten-Cplusplus-\nFeatures-Every-Cplusplus-Developer, and\n◦https://blog.smartbear.com/development/the-biggest-changes-\nin-c11-and-why-you-should-care.\n• See http://thbecker.net/articles/auto_and_decltype/section_01.html\nfor a great treatment of autoanddecltype.\n• See http://www.drdobbs.com/cpp/the-c14-standard-what-you-need-\nto-know/240169034 for a good treatment of C++14’s changes to the\nC++11 standard.\n• See https://isocpp.org/files/papers/p0636r0.html for a complete list of\nhow the C++17 standard differs from C++14.\n3.1.2.2 Which Language Features to Use?\nAs you read about all the cool new features being added to C++, it’s tempt-\ning to think that you need to use allof these features in your engine or game.\nHowever, just because a feature exists doesn’t mean your team needs to im-\nmediately start making use of it.\nAtNaughtyDog,wetendtotakeaconservativeapproachtoadoptingnew\nlanguagefeaturesintoourcodebase. Aspartofourstudio’scodingstandards,\nwe have a list of those C++ language features that are approved for use in our\nruntime code, and another somewhat more liberal list of language features\nthat are allowed in our offline tools code. There are a number of reasons for\nthis cautious approach, which I’ll outline in the following sections.\nLack of Full Feature Support\nThe “bleeding edge” features may not be fully supported by your compiler.\nFor example, LLVM/Clang, the C++ compiler used on the Sony Playstation\n3.1. C++ Review and Best Practices 117\n4, currently supports the entire C++11 standard in versions 3.3 and later, and\nall of C++14 in versions 3.4 and later. But its support for C++17 is spread\nacross Clang versions 3.5 through 4.0, and it currently has no support for the\ndraftC++2astandard. Also,ClangcompilescodeinC++98modebydefault—\nsupport for some of the more-advanced standards are accepted as extensions,\nbut in order to enable full support one must pass specific command-line argu-\nmentstothecompiler. Seehttps://clang.llvm.org/cxx_status.htmlfordetails.\nCost of Switching between Standards\nThere’s a non-zero cost to switching your codebase from one standard to an-\nother. Because of this, it’s important for a game studio to decide on the most-\nadvanced C++ standard to support, and then stick with it for a reasonable\nlength of time (e.g., for the duration of one project). At Naughty Dog, we\nadopted the C++11 standard only relatively recently, and we only allowed its\nuse in the code branch in which The Last of Us Part II is being developed. The\ncode in the branch used for Uncharted 4: A Thief’sEnd andUncharted: The Lost\nLegacyhad been written originally using the C++98 standard, and we decided\nthat the relatively minor benefits of adopting C++11 features in that codebase\ndid not outweigh the cost and risks of doing so.\nRisk versus Reward\nNoteveryC++languagefeatureiscreatedequal. Somefeaturesareusefuland\npretty much universally acceptable, like nullptr. Others may have benefits,\nbut also associated negatives. Still other language features may be deemed\ninappropriate for use in runtime engine code altogether.\nAs an example of a language feature with both benefits and downsides,\nconsider the new C++11 interpretation of the autokeyword. This keyword\ncertainly makes variables and functions more convenient to write. But the\nNaughty Dog programmers recognized that over-use of autocan lead to ob-\nfuscated code: As an extreme example, imagine trying to read a .cpp file writ-\nten by somebody else, in which virtually every variable, function argument\nand return value is declared auto. It would be like reading a typeless lan-\nguage such as Python or Lisp. One of the benefits of a strongly-typed lan-\nguage like C++ is the programmer’s ability to quickly and easily determine\nthe types of all variables. As such, we decided to adopt a simple rule: auto\nmay only be used when declaring iterators, in situations where no other ap-\nproach works (such as within template definitions), or in special cases when\ncode clarity, readability and maintainability is significantly improved by its\nuse. In all other cases, we require the use of explicit type declarations.\n118 3. Fundamentals of Software Engineering for Games\nAs an example of a language feature that could be deemed inappropriate\nfor use in a commercial product like a game, consider template metaprogram-\nming. Andrei Alexandrescu’s Loki library [3] makes heavy use of template\nmetaprogramming to do some pretty interesting and amazing things. How-\never, the resulting code is tough to read, is sometimes non-portable, and\npresents programmers with an extremely high barrier to understanding. The\nprogramming leads at Naughty Dog believe that any programmer should be\nabletojumpinanddebug aproblemonshortnotice, evenincode withwhich\nhe or she may not be very familiar. As such, Naughty Dog prohibits complex\ntemplate metaprogramming in runtime engine code, with exceptions made\nonly on a case-by-case basis where the benefits are deemed to outweigh the\ncosts.\nIn summary, remember that when you have a hammer, everything can\ntend to look like a nail. Don’t be tempted to use features of your language\njust because they’re there (or because they’re new). A judicious and carefully\nconsidered approach will result in a stable codebase that’s as easy as possible\nto understand, reason about, debug and maintain.\n3.1.3 Coding Standards: Why and How Much?\nDiscussions of coding conventions among engineers can often lead to heated\n“religious” debates. I do not wish to spark any such debate here, but I will go\nso far as to suggest that following at least a minimal set of coding standards is\na good idea. Coding standards exist for two primary reasons.\n1. Some standards make the code more readable, understandable and\nmaintainable.\n2. Other conventions help to prevent programmers from shooting them-\nselves in the foot. For example, a coding standard might encourage the\nprogrammer to use only a smaller, more testable and less error-prone\nsubset of the whole language. The C++ language is rife with possibili-\nties for abuse, so this kind of coding standard is particularly important\nwhen using C++.\nIn my opinion, the most important things to achieve in your coding conven-\ntions are the following.\n•Interfaces are king. Keep your interfaces (.h files) clean, simple, minimal,\neasy to understand and well-commented.\n•Goodnamesencourageunderstandingandavoidconfusion. Stick to intuitive\nnames that map directly to the purpose of the class, function or vari-\nable in question. Spend time up-front identifying a good name. Avoid",26282
21-3.2 Catching and Handling Errors.pdf,21-3.2 Catching and Handling Errors,"3.2. Catching and Handling Errors 119\na naming scheme that requires programmers to use a look-up table in\norder to decipher the meaning of your code. Remember that high-level\nprogramming languages like C++ are intended for humans to read. (If\nyou disagree, just ask yourself why you don’t write all your software\ndirectly in machine language.)\n•Don’t clutter the global namespace . Use C++ namespaces or a common\nnaming prefix to ensure that your symbols don’t collide with symbols\nin other libraries. (But be careful not to overuse namespaces, or nest\nthem too deeply.) Name #define d symbols with extra care; remember\nthat C++ preprocessor macros are really just text substitutions, so they\ncut across all C/C++ scope and namespace boundaries.\n•FollowC++bestpractices. Bookslikethe EffectiveC++ seriesbyScottMey-\ners[36,37],Meyers’ EffectiveSTL [38]and Large-ScaleC++SoftwareDesign\nby John Lakos [31] provide excellent guidelines that will help keep you\nout of trouble.\n•Beconsistent. TheruleItrytouseisasfollows: Ifyou’rewritingabodyof\ncodefromscratch,feelfreetoinventanyconventionyoulike—thenstick\ntoit. Wheneditingpreexistingcode, trytofollowwhateverconventions\nhave already been established.\n•Make errors stick out . Joel Spolsky wrote an excellent article on coding\nconventions, which can be found at http://www.joelonsoftware.com/\narticles/Wrong.html. Joelsuggeststhatthe“cleanest”codeisnotneces-\nsarily code that looks neat and tidy on a superficial level, but rather the\ncode that is written in a way that makes common programming errors\neasier to see . Joel’s articles are always fun and educational, and I highly\nrecommend this one.\n3.2 Catching and Handling Errors\nThere are a number of ways to catch and handle error conditions in a game\nengine. As a game programmer, it’s important to understand these different\nmechanisms, their pros and cons and when to use each one.\n3.2.1 Types of Errors\nInanysoftwareprojecttherearetwobasickindsoferrorconditions: usererrors\nandprogrammererrors . A user error occurs when the user of the program does\nsomething incorrect, such as typing an invalid input, attempting to open a\nfile that does not exist, etc. A programmer error is the result of a bugin the\ncode itself. Although it may be triggered by something the user has done, the\n120 3. Fundamentals of Software Engineering for Games\nessence of a programmer error is that the problem could have been avoided\nif the programmer had not made a mistake, and the user has a reasonable\nexpectation that the program shouldhave handled the situation gracefully.\nOf course, the definition of “user” changes depending on context. In the\ncontext of a game project, user errors can be roughly divided into two cate-\ngories: errors caused by the person playing the game and errors caused by\nthe people who are making the game during development. It is important to\nkeep track of which type of user is affected by a particular error and handle\nthe error appropriately.\nThere’sactuallyathirdkindofuser—theotherprogrammersonyourteam.\n(And if you are writing a piece of game middleware software, like Havok or\nOpenGL, this third category extends to other programmers all over the world\nwhoareusingyourlibrary.) Thisiswherethelinebetween usererrors andpro-\ngrammererrors getsblurry. Let’s imagine thatprogrammerA writes a function\nf(), and programmer B tries to call it. If B calls f()with invalid arguments\n(e.g., a null pointer, or an out-of-range array index), then this could be seen as\na user error by programmer A, but it would be a programmer error from B’s\npoint of view. (Of course, one can also argue that programmer A should have\nanticipated the passing of invalid arguments and should have handled them\ngracefully, so the problem really is a programmer error, on A’s part.) The key\nthingtorememberhereisthatthelinebetweenuserandprogrammercanshift\ndepending on context—it is rarely a black-and-white distinction.\n3.2.2 Handling Errors\nWhen handling errors, the requirements differ significantly between the two\ntypes. It is best to handle user errors as gracefully as possible, displaying\nsome helpful information to the user and then allowing him or her to con-\ntinue working—or in the case of a game, to continue playing. Programmer\nerrors, on the other hand, should notbe handled with a graceful “inform and\ncontinue” policy. Instead, it is usually best to halt the program and provide\ndetailed low-level debugging information, so that a programmer can quickly\nidentify and fix the problem. In an ideal world, allprogrammer errors would\nbe caught and fixed before the software ships to the public.\n3.2.2.1 Handling Player Errors\nWhen the “user” is the person playing your game, errors should obviously be\nhandled within the context of gameplay. For example, if the player attempts\nto reload a weapon when no ammo is available, an audio cue and/or an ani-\nmation can indicate this problem to the player without taking him or her “out\nof the game.”\n3.2. Catching and Handling Errors 121\n3.2.2.2 Handling Developer Errors\nWhen the “user” is someone who is making the game, such as an artist, ani-\nmatororgamedesigner,errorsmaybecausedbyaninvalidassetofsomesort.\nFor example, an animation might be associated with the wrong skeleton, or a\ntexture might be the wrong size, or an audio file might have been sampled at\nan unsupported sample rate. For these kinds of developer errors , there are two\ncompeting camps of thought.\nOn the one hand, it seems important to prevent bad game assets from per-\nsisting for too long. A game typically contains literally thousands of assets,\nand a problem asset might get “lost,” in which case one risks the possibility of\nthe bad asset surviving all the way into the final shipping game. If one takes\nthis point of view to an extreme, then the best way to handle bad game assets\nis to prevent the entire game from running whenever even a single problem-\natic asset is encountered. This is certainly a strong incentive for the developer\nwho created the invalid asset to remove or fix it immediately.\nOntheotherhand,gamedevelopmentisamessyanditerativeprocess,and\ngenerating “perfect” assets the first time around is rare indeed. By this line of\nthought, agameengineshouldberobusttoalmostanykindofproblemimag-\ninable, so that work can continue even in the face of totally invalid game asset\ndata. Butthistooisnotideal,becausethegameenginewouldbecomebloated\nwith error-catching and error-handling code that won’t be needed once the\ndevelopment pace settles down and the game ships. And the probability of\nshipping the product with “bad” assets becomes too high.\nIn my experience, the best approach is to find a middle ground between\nthese two extremes. When a developer error occurs, I like to make the error\nobvious and then allow the team to continue to work in the presence of the\nproblem. It is extremely costly to prevent all the other developers on the team\nfrom working, just because one developer tried to add an invalid asset to the\ngame. Agamestudiopaysitsemployeeswell, andwhenmultipleteammem-\nbers experience downtime, the costs are multiplied by the number of people\nwho are prevented from working. Of course, we should only handle errors in\nthis way when it is practical to do so, without spending inordinate amounts\nof engineering time, or bloating the code.\nAs an example, let’s suppose that a particular mesh cannot be loaded. In\nmy view, it’s best to draw a big red box in the game world at the places that\nmesh would have been located, perhaps with a text string hovering over each\none that reads, “Mesh blah-dee-blah failed to load.” This is superior to printing\nan easy-to-miss message to an error log. And it’s farbetter than just crash-\ning the game, because then no one will be able to work until that one mesh\n122 3. Fundamentals of Software Engineering for Games\nreference has been repaired. Of course, for particularly egregious problems\nit’s fine to just spew an error message and crash. There’s no silver bullet for\nall kinds of problems, and your judgment about what type of error handling\napproach to apply to a given situation will improve with experience.\n3.2.2.3 Handling Programmer Errors\nThe best way to detect and handle programmer errors (a.k.a. bugs) is often\nto embed error-checking code into your source code and arrange for failed\nerror checks to halt the program. Such a mechanism is known as an assertion\nsystem; we’ll investigate assertions in detail in Section 3.2.3.3. Of course, as\nwe said above, one programmer’s user error is another programmer’s bug;\nhence, assertions are not always the right way to handle every programmer\nerror. Making a judicious choice between an assertion and a more graceful\nerror-handling technique is a skill that one develops over time.\n3.2.3 Implementation of Error Detection and Handling\nWe’ve looked at some philosophical approaches to handling errors. Now let’s\nturn our attention to the choices we have as programmers when it comes to\nimplementing error detection and handling code.\n3.2.3.1 Error Return Codes\nA common approach to handling errors is to return some kind of failure code\nfrom the function in which the problem is first detected. This could be a\nBoolean value indicating success or failure, or it could be an “impossible”\nvalue, one that is outside the range of normally returned results. For exam-\nple, a function that returns a positive integer or floating-point value could re-\nturn a negative value to indicate that an error occurred. Even better than a\nBoolean or an “impossible” return value, the function could be designed to\nreturn an enumerated value to indicate success or failure. This clearly sepa-\nrates the error code from the output(s) of the function, and the exact nature\nof the problem can be indicated on failure (e.g., enum Error { kSuccess,\nkAssetNotFound, kInvalidRange, ... } ).\nThe calling function should intercept error return codes and act appropri-\nately. It might handle the error immediately. Or, it might work around the\nproblem, complete its own execution and then pass the error code on to what-\never function called it.\n3.2. Catching and Handling Errors 123\n3.2.3.2 Exceptions\nErrorreturncodesareasimpleandreliablewaytocommunicateand respond\nto error conditions. However, error return codes have their drawbacks. Per-\nhaps the biggest problem with error return codes is that the function that de-\ntects an error may be totally unrelated to the function that is capable of han-\ndlingtheproblem. Intheworst-casescenario,afunctionthatis40callsdeepin\nthecallstackmightdetectaproblemthatcanonlybehandledbythetop-level\ngame loop, or by main() . In this scenario, every one of the 40 functions on\nthe call stack would need to be written so that it can pass an appropriate error\ncode all the way back up to the top-level error-handling function.\nOne way to solve this problem is to throw an exception. Exception han-\ndlingis a very powerful feature of C++. It allows the function that detects a\nproblem to communicate the error to the rest of the code without knowing\nanything about which function might handle the error. When an exception is\nthrown, relevant information about the error is placed into a data object of the\nprogrammer’s choice known as an exception object . The call stack is then au-\ntomatically unwound, in search of a calling function that has wrapped its call\nin atry-catch block. If a try-catch block is found, the exception object is\nmatched against all possible catchclauses, and if a match is found, the cor-\nresponding catch’s code block is executed. The destructors of any automatic\nvariables are called as needed during the stack unwinding process.\nThe ability to separate error detection from error handling in such a clean\nway is certainly attractive, and exception handling is an excellent choice for\nsome software projects. However, exception handling does add some over-\nhead to the program. The stack frame of any function that contains a try-\ncatchblock must be augmented to contain additional information required\nby the stack unwinding process. Also, if even one function in your program\n(or a library that your program links with) uses exception handling, your en-\ntire program must use exception handling—the compiler can’t know which\nfunctions might be above you on the call stack when you throw an exception.\nThat said, it is possible to “sandbox” a library or libraries that make use of\nexception handling, in order to avoid your entire game engine having to be\nwritten with exceptions enabled. To do this, you would wrap all the API calls\nintothelibrariesinquestioninfunctionsthatareimplementedinatranslation\nunitthathasexceptionhandlingenabled. Eachofthesefunctionswouldcatch\nall possible exceptions in a try/catch block and convert them into error return\ncodes. Any code that links with your wrapper library can therefore safely\ndisable exception handling.\nArguably more important than the overhead issue is the fact that excep-\n124 3. Fundamentals of Software Engineering for Games\ntions are in some ways no better than gotostatements. Joel Spolsky of Mi-\ncrosoft and Fog Creek Software fame argues that exceptions are in fact worse\nthangotos because they aren’t easily seen in the source code. A function that\nneither throws nor catches exceptions may nevertheless become involved in\nthe stack-unwinding process, if it finds itself sandwiched between such func-\ntions in the call stack. And the unwinding process is itself imperfect: Your\nsoftwarecaneasilybeleftinaninvalidstateunlesstheprogrammerconsiders\nevery possible way that an exception can be thrown, and handles it appropri-\nately. This can make writing robust software difficult. When the possibility\nfor exception throwing exists, pretty much every function in your codebase\nneeds to be robust to the carpet being pulled out from under it and all its local\nobjects destroyed whenever it makes a function call.\nAnotherissuewithexceptionhandlingisitscost. Althoughintheorymod-\nern exception handling frameworks don’t introduce additional runtime over-\nheadintheerror-freecase,thisisnotnecessarilytrueinpractice. Forexample,\nthe code that the compiler adds to your functions for unwinding the call stack\nwhen an exception occurs tends to produce an overall increase in code size.\nThis might degrade I-cache performance, or cause the compiler to decide not\nto inline a function that it otherwise would have.\nClearly there are some pretty strong arguments for turning offexception\nhandling in your game engine altogether. This is the approach employed at\nNaughty Dog and also on most of the projects I’ve worked on at Electronic\nArts and Midway. In his capacity as Engine Director at Insomniac Games,\nMike Acton has clearly stated his objection to the use of exception handling\nin runtime game code on numerous occasions. JPL and NASA also disallow\nexception handling in their mission-critical embedded software, presumably\nfor the same reasons we tend to avoid it in the game industry. That said, your\nmileage certainly may vary. There is no perfect tool and no one right way to\ndo anything. When used judiciously, exceptions canmake your code easier to\nwrite and work with; just be careful out there!\nThere are many interesting articles on this topic on the web. Here’s one\ngood thread that covers most of the key issues on both sides of the debate:\n• http://www.joelonsoftware.com/items/2003/10/13.html\n• http://www.nedbatchelder.com/text/exceptions-vs-status.html\n• http://www.joelonsoftware.com/items/2003/10/15.html\nExceptions and RAII\nThe“resourceacquisitionisinitialization”pattern(RAII,see Section3.1.1.6)is\noften used in conjuction with exception handling: The constructor attempts to\n3.2. Catching and Handling Errors 125\nacquire the desired resource, and throws an exception if it fails to do so. This\nis done to avoid the need for an if check to test the status of the object after it\nhas been created—if the constructor returns without throwing an exception,\nwe know for certain that the resource was successfully acquired.\nHowever, the RAII pattern can be used even without exceptions. All it re-\nquiresisalittledisciplinetocheckthestatusofeachnewresourceobjectwhen\nitisfirstcreated. Afterthat,alloftheotherbenefitsofRAIIcanbereaped. (Ex-\nceptions can also be replaced by assertion failures to signal the failure of some\nkinds of resource acquisitions.)\n3.2.3.3 Assertions\nAnassertion is a line of code that checks an expression. If the expression eval-\nuates to true, nothing happens. But if the expression evaluates to false, the\nprogram is stopped, a message is printed and the debugger is invoked if pos-\nsible.\nAssertions check a programmer’s assumptions. They act like land mines\nfor bugs. They check the code when it is first written to ensure that it is func-\ntioning properly. They also ensure that the original assumptions continue to\nhold for long periods of time, even when the code around them is constantly\nchangingandevolving. Forexample,ifaprogrammerchangescodethatused\nto work, but accidentally violates its original assumptions, they’ll hit the land\nmine. This immediately informs the programmer of the problem and per-\nmits him or her to rectify the situation with minimum fuss. Without asser-\ntions, bugs have a tendency to “hide out” and manifest themselves later in\nways that are difficult and time-consuming to track down. But with asser-\ntions embedded in the code, bugs announce themselves the moment they are\nintroduced—which is usually the best moment to fix the problem, while the\ncode changes that caused the problem are fresh in the programmer’s mind.\nSteve Maguire provides an in-depth discussion of assertions in his must-read\nbook,WritingSolid Code [35].\nThe cost of the assertion checks can usually be tolerated during develop-\nment,butstrippingouttheassertionspriortoshippingthegamecanbuyback\nthat little bit of crucial performance if necessary. For this reason assertions are\ngenerally implemented in such a way as to allow the checks to be stripped\nout of the executable in non-debug build configurations. In C, an assert()\nmacro is provided by the standard library header file <assert.h>; in C++,\nit’s provided by the <cassert> header.\nThe standard library’s definition of assert() causes it to be defined\nin debug builds (builds with the DEBUG preprocessor symbol defined) and\n126 3. Fundamentals of Software Engineering for Games\nstripped in non-debug builds (builds with the NDEBUG preprocessor sym-\nbol defined). In a game engine, you may want finer-grained control over\nwhich build configurations retain assertions, and which configurations strip\nthem out. For example, your game might support more than just a debug\nand development build configuration—you might also have a shipping build\nwith global optimizations enabled, and perhaps even a PGO build for use by\nprofile-guided optimization tools (see Section 2.2.4). Or you might also want\ntodefinedifferent“flavors”ofassertions—somethatarealwaysretainedeven\nin the shipping version of your game, and others that are stripped out of the\nnon-shipping build. For these reasons, let’s take a look at how you can imple-\nment your own ASSERT() macro using the C/C++ preprocessor.\nAssertion Implementation\nAssertions are usually implemented via a combination of a #defined macro\nthat evaluates to an if/elseclause, a function that is called when the asser-\ntion fails (the expression evaluates to false), and a bit of assembly code that\nhalts the program and breaks into the debugger when one is attached. Here’s\na typical implementation:\n#if ASSERTIONS_ENABLED\n// define some inline assembly that causes a break\n// into the debugger -- this will be different on each\n// target CPU\n#define debugBreak() asm { int 3 }\n// check the expression and fail if it is false\n#define ASSERT(expr) \\nif (expr) { } \\nelse \\n{ \\nreportAssertionFailure (#expr, \\n__FILE__, __LINE__); \\ndebugBreak(); \\n}\n#else\n#define ASSERT(expr) // evaluates to nothing\n#endif\nLet’s break down this definition so we can see how it works:\n3.2. Catching and Handling Errors 127\n• The outer #if/#else/ #endif is used to strip assertions from the\ncode base. When ASSERTIONS_ENABLED is nonzero, the ASSERT()\nmacro is defined in its full glory, and all assertion checks in the code\nwill be included in the program. But when assertions are turned off,\nASSERT(expr) evaluates to nothing, and all instances of it in the code\nare effectively removed.\n• The debugBreak() macro evaluates to whatever assembly-language\ninstructions are required in order to cause the program to halt and the\ndebugger to take charge (if one is connected). This differs from CPU to\nCPU, but it is usually a single assembly instruction.\n• The ASSERT() macro itself is defined using a full if/else statement\n(as opposed to a lone if). This is done so that the macro can be used in\nany context, even within otherunbracketed if/else statements.\nHere’s an example of what would happen if ASSERT() were defined us-\ning a solitary if:\n// WARNING: NOT A GOOD IDEA!\n#define ASSERT(expr) if (!(expr)) debugBreak()\nvoid f()\n{\nif (a < 5)\nASSERT(a >= 0);\nelse\ndoSomething(a);\n}\nThis expands to the following incorrect code:\nvoid f()\n{\nif (a < 5)\nif (!(a >= 0))\ndebugBreak();\nelse // oops! bound to the wrong if()!\ndoSomething(a);\n}\n• The elseclause of an ASSERT() macro does two things. It displays\nsome kind of message to the programmer indicating what went wrong,\nandthenitbreaksintothedebugger. Noticetheuseof #exprasthefirst\nargument to the message display function. The pound ( #) preprocessor\noperator causes the expression exprto be turned into a string, thereby\nallowing it to be printed out as part of the assertion failure message.\n128 3. Fundamentals of Software Engineering for Games\n• Notice also the use of __FILE__ and__LINE__. These compiler-defin-\ned macros magically contain the .cpp file name and line number of the\nline of code on which they appear. By passing them into our message\ndisplay function, we can print the exact location of the problem.\nI highly recommend the use of assertions in your code. How-\never, it’s important to be aware of their performance cost. You may\nwant to consider defining two kinds of assertion macros. The regular\nASSERT() macro can be left active in allbuilds, so that errors are eas-\nily caught even when not running in debug mode. A second assertion\nmacro, perhaps called SLOW_ASSERT(), could be activated only in debug\nbuilds. This macro could then be used in places where the cost of as-\nsertion checking is too high to permit inclusion in release builds. Ob-\nviously SLOW_ASSERT() is of lower utility, because it is stripped out of\nthe version of the game that your testers play every day. But at least\nthese assertions become active when programmers are debugging their\ncode.\nIt’s also extremely important to use assertions properly. They should be\nused to catch bugs in the program itself—never to catch user errors. Also, as-\nsertionsshouldalwayscausetheentiregametohaltwhentheyfail. It’susually\na bad idea to allow assertions to be skipped by testers, artists, designers and\nother non-engineers. (This is a bit like the boy who cried wolf: if assertions\ncan be skipped, then they cease to have any significance, rendering them inef-\nfective.) Inotherwords, assertionsshouldonlybeusedtocatchfatalerrors. If\nit’sOKtocontinuepastanassertion,thenit’sprobablybettertonotifytheuser\nof the error in some other way, such as with an on-screen message, or some\nugly bright-orange 3D graphics.\nCompile-Time Assertions\nOne weakness of assertions, as we’ve discussed them thus far, is that the con-\nditions encoded within them are only checked at runtime. We have to run the\nprogram, andthe code path in question must actually execute, in order for an\nassertion’s condition to be checked.\nSometimes the condition we’re checking within an assertion involves in-\nformation that is entirely known at compile time. For example, let’s say we’re\ndefining a struct that for some reason needs to be exactly 128 bytes in size. We\nwant to add an assertion so that if another programmer (or a future version of\nyourself) decides to change the size of the struct, the compiler will give us an\nerror message. In other words, we’d like to write something like this:\n3.2. Catching and Handling Errors 129\nstruct NeedsToBe128Bytes\n{\nU32 m_a;\nF32 m_b;\n// etc.\n};\n// sadly this doesn't work ...\nASSERT(sizeof(NeedsToBe128Bytes) == 128);\nThe problem of course is that the ASSERT() (orassert()) macro needs\nto be executable at runtime, and one can’t even put executable code at global\nscopeina.cppfileoutsideofafunctiondefinition. Thesolutiontothisproblem\nis acompile-timeassertion, also known as a staticassertion.\nStarting with C++1 1, the standard library defines a macro named\nstatic_assert() for us. So we can re-write the example above as follows:\nstruct NeedsToBe128Bytes\n{\nU32 m_a;\nF32 m_b;\n// etc.\n};\nstatic_assert (sizeof(NeedsToBe128Bytes) == 128,\n""wrong size"");\nIf you’re not using C++11, you can always roll your own STATIC_\nASSERT() macro. It can be implemented in a number of different ways, but\nthe basic idea is always the same: The macro places a declaration into your\ncode that (a) is legal at file scope, (b) evaluates the desired expression at com-\npile time rather than runtime, and (c) produces a compile error if and only if\ntheexpressionisfalse. Somemethodsofdefining STATIC_ASSERT() relyon\ncompiler-specific details, but here’s one reasonably portable way to define it:\n#define _ASSERT_GLUE(a, b) a ## b\n#define ASSERT_GLUE(a, b) _ASSERT_GLUE(a, b)\n#define STATIC_ASSERT (expr) \\nenum \\n{ \\nASSERT_GLUE(g_assert_fail_, __LINE__) \\n=1 / (int)(!!(expr)) \\n}\nSTATIC_ASSERT(sizeof(int) == 4); // should pass\nSTATIC_ASSERT(sizeof(float) == 1); // should fail\n130 3. Fundamentals of Software Engineering for Games\nThis works by defining an anonymous enumeration containing a single\nenumerator. The name of the enumerator is made unique (within the trans-\nlation unit) by “gluing” a fixed prefix such as g_assert_fail_ to a unique\nsuffix—in this case, the line number on which the STATIC_ASSERT() macro\nisinvoked. Thevalueoftheenumeratorissetto 1 / (!!(expr)) . Thedou-\nble negation !!ensures that exprhas a Boolean value. This value is then cast\ntoan int,yieldingeither 1or0dependingonwhether theexpressionis true\norfalse, respectively. If the expression is true, the enumerator will be set to\nthe value 1/1which is one. But if the expression is false, we’ll be asking the\ncompiler to set the enumerator to the value 1/0which is illegal, and will trig-\nger a compile error.\nWhenour STATIC_ASSERT() macroasdefinedabovefails, VisualStudio\n2015 produces a compile-time error message like this:\n1>test.cpp(48): error C2131: expression did not evaluate to\na constant\n1> test.cpp(48): note: failure was caused by an undefined\narithmetic operation\nHere’s another way to define STATIC_ASSERT() using template special-\nization. In this example, we first check to see if we’re using C++11 or beyond.\nIfso, weusethestandardlibrary’simplementationof static_assert() for\nmaximum portability. Otherwise we fall back to our custom implementation.\n#ifdef __cplusplus\n#if __cplusplus >= 201103L\n#define STATIC_ASSERT (expr) \\nstatic_assert (expr, \\n""static assert failed:"" \\n#expr)\n#else\n// declare a template but only define the\n// true case (via specialization)\ntemplate<bool> class TStaticAssert;\ntemplate<> class TStaticAssert<true> {};\n#define STATIC_ASSERT (expr) \\nenum \\n{ \\nASSERT_GLUE(g_assert_fail_, __LINE__) \\n= sizeof( TStaticAssert<!!(expr)>) \\n}\n#endif\n#endif",27954
22-3.3 Data Code and Memory Layout.pdf,22-3.3 Data Code and Memory Layout,"3.3. Data, Code and Memory Layout 131\nThis implementation, using template specialization, may be preferable to\nthe previous one using division by zero, because it produces a slightly better\nerror message in Visual Studio 2015:\n1>test.cpp(48): error C2027: use of undefined type\n'TStaticAssert<false>'\n1>test.cpp(48): note: see declaration of\n'TStaticAssert<false>'\nHowever, each compiler handles error reporting differently, so your mileage\nmay vary. For more implementation ideas for compile-time assertions,\none good reference is http://www.pixelbeat.org/programming/gcc/static_\nassert.html.\n3.3 Data, Code and Memory Layout\n3.3.1 Numeric Representations\nNumbers are at the heart of everything that we do in game engine develop-\nment(andsoftwaredevelopmentingeneral). Everysoftwareengineershould\nunderstand how numbers are represented and stored by a computer. This\nsection will provide you with the basics you’ll need throughout the rest of the\nbook.\n3.3.1.1 Numeric Bases\nPeople think most naturally in base ten, also known as decimal notation. In this\nnotation, tendistinctdigitsareused(0through9), andeachdigitfromrightto\nleft represents the next highest power of 10. For example, the number 7803 =\n(7103) + ( 8102) + ( 0101) + ( 3100) = 7000 +800+0+3.\nIn computer science, mathematical quantities such as integers and real-\nvalued numbers need to be stored in the computer’s memory. And as we\nknow, computers store numbers in binaryformat, meaning that only the two\ndigits 0 and 1 are available. We call this a base-two representation, because\neach digit from right to left represents the next highest power of 2. Com-\nputer scientists sometimes use a prefix of “0b” to represent binary numbers.\nFor example, the binary number 0b1101 is equivalent to decimal 13, because\n0b1101 = (123) + ( 122) + ( 021) + ( 120) = 8+4+0+1=13.\nAnother common notation popular in computing circles is hexadecimal, or\nbase16. In this notation, the 10 digits 0 through 9 and the six letters A through\nF are used; the letters A through F replace the decimal values 10 through 15,\nrespectively. A prefix of “0x” is used to denote hex numbers in the C and C++\n132 3. Fundamentals of Software Engineering for Games\nprogramming languages. This notation is popular because computers gener-\nally store data in groups of 8 bits known as bytes, and since a single hexadec-\nimal digit r epresents 4 bits exactly, a pairof hex digits represents a byte. For\nexample, the value 0xFF = 0b11111111 = 255 is the largest number that can be\nstored in 8 bits (1 byte). Each digit in a hexadecimal number, from right to\nleft, represents the next power of 16. So, for example, 0xB052 = (11163)\n+ (0162) + ( 5161) + ( 2160) = ( 114096 ) + ( 0256) + ( 516) +\n(21) = 45,138.\n3.3.1.2 Signed and Unsigned Integers\nIn computer science, we use both signed and unsigned integers. Of course,\nthe term “unsigned integer” is actually a bit of a misnomer—in mathematics,\nthewholenumbers ornaturalnumbers range from 0 (or 1) up to positive infinity,\nwhile the integers range from negative infinity to positive infinity. Neverthe-\nless, we’ll use computer science lingo in this book and stick with the terms\n“signed integer” and “unsigned integer.”\nMost modern personal computers and game consoles work most easily\nwithintegersthatare32bitsor64bitswide(although8-and16-bitintegersare\nalsousedagreatdealingameprogrammingaswell). Torepresenta32-bitun-\nsigned integer, we simply encode the value using binary notation (see above).\nThe range of possible values for a 32-bit unsigned integer is 0x00000000 ( 0) to\n0xFFFFFFFF (4,294,967,295 ).\nTo represent a signedinteger in 32 bits, we need a way to differentiate be-\ntween positive and negative vales. One simple approach called the sign and\nmagnitude encoding reserves the most significant bit as a sign bit. When this\nbit is zero, the value is positive, and when it is one, the value is negative. This\nleavesus31bitstorepresentthemagnitudeofthevalue,effectivelycuttingthe\nrange of possible magnitudes in half (but allowing both positive and negative\nforms of every distinct magnitude, including zero).\nMost microprocessors use a slightly more efficient technique for encoding\nnegativeintegers, called two’scomplement notation. Thisnotationhasonlyone\nrepresentation for the value zero, as opposed to the two representations pos-\nsible with simple sign bit (positive zero and negative zero). In 32-bit two’s\ncomplement notation, the value 0xFFFFFFFF is interpreted to mean  1, and\nnegative values count down from there. Any value with the most significant\nbit set is considered negative. So values from 0x00000000 (0) to 0x7FFFFFFF\n(2,147,483,647)representpositiveintegers,and0x80000000(  2,147,483,648 )to\n0xFFFFFFFF ( 1) represent negative integers.\n3.3. Data, Code and Memory Layout 133\n31 15 0magnitude (16 bits) fraction (15 bits)\n1 = –173.25sign\n0x80 0x56 0xA0 0x0010000000010101101010000000000000\nFigure 3.4. Fixed-point notation with 16-bit magnitude and 16-bit fraction.\n3.3.1.3 Fixed-Point Notation\nIntegers are great for representing whole numbers, but to represent fractions\nand irrational numbers we need a different format that expresses the concept\nof a decimal point.\nOne early approach taken by computer scientists was to use fixed-point no-\ntation. In this notation, one arbitrarily chooses how many bits will be used\nto represent the whole part of the number, and the rest of the bits are used\nto represent the fractional part. As we move from left to right (i.e., from the\nmost significant bit to the least significant bit), the magnitude bits represent\ndecreasing powers of two (…, 16, 8, 4, 2, 1), while the fractional bits represent\ndecreasing inversepowers of two (1\n2,1\n4,1\n8,1\n16, . . .). For example, to store the\nnumber 173.25in32-bitfixed-pointnotationwithonesignbit, 16bitsforthe\nmagnitudeand15bitsforthefraction,wefirstconvertthesign,thewholepart\nand the fractional part into their binary equivalents individually (negative =\n0b1, 173 = 0b0000000010101101 and 0.25 =1\n4= 0b010000000000000). Then we\npackthosevaluestogetherintoa32-bitinteger. Thefinalresultis0x8056A000.\nThis is illustrated in Figure 3.4.\nThe problem with fixed-point notation is that it constrains both the range\nof magnitudes that can be represented and the amount of precision we can\nachieve in the fractional part. Consider a 32-bit fixed-point value with 16 bits\nfor the magnitude, 15 bits for the fraction and a sign bit. This format can only\nrepresent magnitudes up to 65,535, which isn’t particularly large. To over-\ncome this problem, we employ a floating-point representation.\n3.3.1.4 Floating-Point Notation\nIn floating-point notation, the position of the decimal place is arbitrary and\nis specified with the help of an exponent. A floating-point number is broken\nintothreeparts: the mantissa ,whichcontainstherelevantdigitsofthenumber\non both sides of the decimal point, the exponent , which indicates where in that\nstring of digits the decimal point lies, and a signbit, which of course indicates\nwhether the value is positive or negative. There are all sorts of different ways\n134 3. Fundamentals of Software Engineering for Games\n0\n31 23 0exponent (8 bits)\n0111110001000000000000000000000mantissa (23 bits) sign\n= 0.1 5625\nFigure 3.5. IEEE-754 32-bit ﬂoating-point format.\nto lay out these three components in memory, but the most common standard\nis IEEE-754. It states that a 32-bit floating-point number will be represented\nwith the sign in the most significant bit, followed by 8 bits of exponent and\nfinally 23 bits of mantissa.\nThe value vrepresented by a sign bit s, an exponent eand a mantissa mis\nv=s2(e 127)(1+m).\nThe sign bit shas the value +1or 1. The exponent eis biased by 127 so\nthat negative exponents can be easily represented. The mantissa begins with\nan implicit 1 that is not actually stored in memory, and the rest of the bits are\ninterpreted as inverse powers of two. Hence the value represented is really\n1+m, where mis the fractional value stored in the mantissa.\nFor example, the bit pattern shown in Figure 3.5 represents the value\n0.15625, because s=0(indicating a positive number), e=0b01111100 =124\nandm=0b0100… =02 1+12 2=1\n4. Therefore,\nv=s2(e 127)(1+m)\n= (+ 1)2(124 127)(1+1\n4)\n=2 35\n4\n=1\n85\n4\n=0.1251.25 =0.15625.\nThe Trade-Off between Magnitude and Precision\nTheprecision of a floating-point number increases as the magnitude de-\ncreases, and vice versa. This is because there are a fixed number of bits in\nthe mantissa, and these bits must be shared between the whole part and the\nfractional part of the number. If a large percentage of the bits are spent repre-\nsentingalargemagnitude, thenasmallpercentageofbitsareavailabletopro-\nvide fractional precision. In physics the term significantdigits is typically used\nto describe this concept (http://en.wikipedia.org/wiki/Significant_digits).\nTo understand the trade-off between magnitude and precision, let’s look\nat the largest possible floating-point value, FLT_MAX3.4031038, whose\n3.3. Data, Code and Memory Layout 135\nrepresentation in 32-bit IEEE floating-point format is 0x7F7FFFFF. Let’s break\nthis down:\n• The largest absolute value that we can represent with a 23-bit mantissa\nis 0x00FFFFFF in hexadecimal, or 24 consecutive binary ones—that’s 23\nones in the mantissa, plus the implicit leading one.\n• An exponent of 255 has a special meaning in the IEEE-754 format—it is\nused for values like not-a-number (NaN) and infinity—so it cannot be\nused for regular numbers. Hence the maximum eight-bit exponent is\nactually 254, which translates into 127 after subtracting the implicit bias\nof 127.\nSoFLT_MAX is 0x00FFFFFF2127=0xFFFFFF00000000000000000000000000 .\nIn other words, our 24 binary ones were shifted up by 127 bit positions, leav-\ning127 23=104binary zeros (or 104/4 =26hexadecimal zeros) after the\nleastsignificantdigitofthemantissa. Thosetrailingzerosdon’tcorrespondto\nany actual bits in our 32-bit floating-point value—they just appear out of thin\nair because of the exponent. If we were to subtract a small number (where\n“small” means any number composed of fewer than 26 hexadecimal digits)\nfrom FLT_MAX, the result would still be FLT_MAX , because those 26 least sig-\nnificant hexadecimal digits don’t really exist!\nThe opposite effect occurs for floating-point values whose magnitudes are\nmuch less than one. In this case, the exponent is large but negative, and the\nsignificant digits are shifted in the opposite direction. We trade the ability to\nrepresent large magnitudes for high precision. In summary, we always have\nthe same number of significant digits (or really significant bits) in our floating-\npointnumbers,andtheexponentcanbeusedtoshiftthosesignificantbitsinto\nhigher or lower ranges of magnitude.\nSubnormal Values\nAnother subtlety to notice is that there is a finite gap between zero and the\nsmallestnonzerovaluewecanrepresentwithfloating-pointnotation(asithas\nbeen described thus far). The smallest nonzero magnitude we can represent\nisFLT_MIN =2 1261.17510 38, which has a binary representation of\n0x00800000 (i.e., the exponent is 0x01, or  126after subtracting the bias, and\nthe mantissa is all zeros except for the implicit leading one). The next small-\nest valid value is zero, so there is a finite gap between the values -FLT_MIN\nand+FLT_MIN.Thisunderscoresthefactthattherealnumberlineis quantized\nwhen using a floating-point representation. (Note that the C++ standard li-\nbrary exposes FLT_MIN as the rather more verbose std::numeric_limits\n136 3. Fundamentals of Software Engineering for Games\n<float>::min(). We’ll stick to FLT_MIN in this book for brevity.)\nThe gap around zero can be filled by employing an extension to the\nfloating-pointrepresentationknownas denormalizedvalues,alsoknownas sub-\nnormal values. With this extension, any floating-point value with a biased ex-\nponent of 0 is interpreted as a subnormal number. The exponent is treated as\nif it had been a 1 instead of a 0, and the implicit leading 1 that normally sits in\nfront of the bits of the mantissa is changed to a 0. This has the effect of filling\nthe gap between -FLT_MIN and+FLT_MIN with a linear sequence of evenly-\nspaced subnormal values. The positive subnormal float that is closest to\nzero is represented by the constant FLT_TRUE_MIN.\nThe benefit of using subnormal values is that it provides greater preci-\nsion near zero. For example, it ensures that the following two expressions\nare equivalent, even for values of aandbthat are very close to FLT_MIN:\nif (a == b) { ... }\nif (a - b == 0.0f ) { ... }\nWithout subnormal values, the expression a - bcould evaluate to zero even\nwhen a != b.\nMachine Epsilon\nFor a particular floating-point representation, the machine epsilon is defined to\nbe the smallest floating-point value #that satisfies the equation, 1+#̸=1. For\nanIEEE-754floating-point number,withits23bitsofprecision,thevalueof #is\n2 23, which is approximately 1.19210 7. The most significant digit of #falls\njust inside the range of significant digits in the value 1.0, so adding any value\nsmaller than #to 1.0 has no effect. In other words, any new bits contributed\nadding a value smaller than #will get “chopped off” when we try to fit the\nsum into a mantissa with only 23 bits.\nUnits in the Last Place (ULP)\nConsidertwofloating-pointnumberswhichareidenticalinallrespectsexcept\nfor the value of the least-significant bit in their mantissas. These two values\nare said to differ by one unitinthelastplace (1 ULP). The actual value of 1 ULP\nchanges depending on the exponent. For example, the floating-point value\n1.0fhas an unbiased exponent of zero, and a mantissa in which all bits are\nzero (except for the implicit leading 1). At this exponent, 1 ULP is equal to the\nmachine epsilon (2 23). If we change the exponent to a 1, yielding the value\n2.0f, the value of 1 ULP becomes equal to two times the machine epsilon.\nAnd if the exponent is 2, yielding the value 4.0f, the value of 1 ULP is four\n3.3. Data, Code and Memory Layout 137\ntimes the machine epsilon. In general, if a floating-point value’s unbiased ex-\nponent is x, then 1ULP =2x#.\nThe concept of units in the last place illustrates the idea that the precision\nof a floating-point number depends on its exponent, and is useful for quanti-\nfying the error inherent in any floating-point calculation. It can also be use-\nful for finding the floating-point value that is the next largest representable\nvalue relative to a known value, or conversely the next smallest representable\nvaluerelativetothatvalue. Thisinturncanbeusefulforconvertingagreater-\nthan-or-equal comparison into a greater-than comparison. Mathematically,\nthe condition abis equivalent to the condition a+1ULP>b. We use this\nlittle “trick” in the Naughty Dog engine to simplify some logic in our char-\nacter dialog system. In this system, simple comparisons can be used to select\ndifferent lines of dialog for the characters to say. Rather than supporting all\npossible comparison operators, we only support greater-than and less-than\nchecks, and we handle greater-than-or-equal-to and less-than-or-equal-to by\nadding or subtracting 1 ULP to or from the value being compared.\nImpact of Floating-Point Precision on Software\nTheconceptsoflimitedprecisionandthemachineepsilonhaverealimpactson\ngamesoftware. Forexample, let’ssayweuseafloating-pointvariabletotrack\nabsolute game time in seconds. How long can we run our game before the\nmagnitude of our clock variable gets so large that adding 1/30thof a second\nto it no longer changes its value? The answer is 12.14 days or 220seconds.\nThat’s longer than most games will be left running, so we can probably get\naway with using a 32-bit floating-point clock measured in seconds in a game.\nBut clearly it’s important to understand the limitations of the floating-point\nformatsothatwecanpredictpotentialproblemsandtakestepstoavoidthem\nwhen necessary.\nIEEE Floating-Point Bit Tricks\nSee [9, Section 2.1] for a few really useful IEEE floating-point “bit tricks” that\ncan make certain floating-point calculations lightning fast.\n3.3.2 Primitive Data Types\nC and C++ provide a number of primitive data types. The C and C++ stan-\ndards provide guidelines on the relative sizes and signedness of these data\ntypes, but each compiler is free to define the types slightly differently in order\nto provide maximum performance on the target hardware.\n138 3. Fundamentals of Software Engineering for Games\n•char. A charis usually 8 bits and is generally large enough to hold an\nASCII or UTF-8 character (see Section 6.4.4.1). Some compilers define\ncharto be signed, while others use unsigned chars by default.\n•int, short,long. An intissupposed to hold a signed integer value\nthat is the most efficient size for the target platform; it is usually defined\nto be 32 bits wide on a 32-bit CPU architecture, such as Pentium 4 or\nXeon, and 64 bits wide on a 64-bit architecture, such as Intel Core i7,\nalthough the size of an intis also dependent upon other factors such as\ncompiler options and the target operating system. A shortis intended\nto be smaller than an intand is 16 bits on many machines. A longis\nas large as or larger than an intand may be 32 or 64 bits wide, or even\nwider, again depending on CPU architecture, compiler options and the\ntarget OS.\n•float. On most modern compilers, a float is a 32-bit IEEE-754\nfloating-point value.\n•double . Adouble is a double-precision (i.e., 64-bit) IEEE-754 floating-\npoint value.\n•bool. A boolis a true/false value. The size of a boolvaries widely\nacross different compilers and hardware architectures. It is never imple-\nmented as a single bit, but some compilers define it to be 8 bits while\nothers use a full 32 bits.\nPortable Sized Types\nThe built-in primitive data types in C and C++ were designed to be portable\nandthereforenonspecific. However,inmanysoftwareengineeringendeavors,\nincluding game engine programming, it is often important to know exactly\nhow wide a particular variable is.\nBefore C++11, programmers had to rely on non-portable sized types pro-\nvided by their compiler. For example, the Visual Studio C/C++ compiler de-\nfined the following extended keywords for declaring variables that are an ex-\nplicit number of bits wide: __int8 ,__int16 ,__int32 and__int64. Most\nothercompilershavetheirown“sized”datatypes, withsimilarsemanticsbut\nslightly different syntax.\nBecause of these differences between compilers, most game engines\nachieved source code portability by defining their own custom sized types.\nFor example, at Naughty Dog we use the following sized types:\n•F32is a 32-bit IEEE-754 floating-point value.\n•U8,I8,U16,I16, U32, I32, U64andI64are unsigned and signed 8-,\n16-, 32- and 64-bit integers, respectively.\n3.3. Data, Code and Memory Layout 139\n•U32FandI32Fare “fast” unsigned and signed 32-bit values, respec-\ntively. Each of these data types contains a value that is at least 32 bits\nwide, but may be wider if that would result in faster code on the target\nCPU.\n<cstdint>\nThe C++11 standard library introduces a set of standardized sized inte-\nger types. They are declared in the <cstdint> header, and they include\nthe signed types std::int8_t, std::int16_t, std::int32_t and\nstd::int64_t and the unsigned types std::uint8_t, std::uint16_t,\nstd::uint32_t andstd::uint64_t, along with “fast” variants (like the\nI32FandU32Ftypes we defined at Naughty Dog). These types free the pro-\ngrammer from having to “wrap” compiler-specific types in order to achieve\nportability. Foracompletelistofthesesizedtypes,seehttp://en.cppreference.\ncom/w/cpp/types/integer.\nOGRE’s Primitive Data Types\nOGRE defines a number of sized types of its own. Ogre::uint8 ,Ogre::\nuint16 andOgre::uint32 are the basic unsigned sized integral types.\nOgre::Real defines a real floating-point value. It is usually defined to be\n32 bits wide (equivalent to a float), but it can be redefined globally to be 64\nbits wide (like a double) by defining the preprocessor macro OGRE_DOUBLE\n_PRECISION to1. This ability to change the meaning of Ogre::Real is\ngenerally only used if one’s game has a particular requirement for double-\nprecision math, which is rare. Graphics chips (GPUs) always perform their\nmath with 32-bit or 16-bit floats, the CPU/FPU is also usually faster when\nworking in single-precision, and SIMD vector instructions operate on 128-bit\nregisters that contain four 32-bit floats each. Hence, most games tend to stick\nto single-precision floating-point math.\nThe data types Ogre::uchar ,Ogre::ushort, Ogre::uint andOgre\n::ulong are just shorthand notations for C/C++’s unsigned char,\nunsigned short andunsigned long, respectively. As such, they are no\nmore or less useful than their native C/C++ counterparts.\nThe types Ogre::Radian andOgre::Degree are particularly interest-\ning. These classes are wrappers around a simple Ogre::Real value. The\nprimary role of these types is to permit the angular units of hard-coded literal\nconstantstobedocumentedandtoprovideautomaticconversionbetweenthe\ntwo unit systems. In addition, the type Ogre::Angle represents an angle in\nthe current “default” angle unit. The programmer can define whether the de-\nfault will be radians or degrees when the OGRE application first starts up.\n140 3. Fundamentals of Software Engineering for Games\nPerhaps surprisingly, OGRE does not provide a number of sized primi-\ntive data types that are commonplace in other game engines. For example, it\ndefines no signed 8-, 16- or 64-bit integral types. If you are writing a game\nengine on top of OGRE, you will probably find yourself defining these types\nmanually at some point.\n3.3.2.1 Multibyte Values and Endianness\nValues that are larger than eight bits (one byte) wide are called multibytequan-\ntities. They’recommonplaceonanysoftwareprojectthatmakesuseofintegers\nand floating-point values that are 16 bits or wider. For example, the integer\nvalue 4660 = 0x1234 is represented by the two bytes 0x12 and 0x34. We call\n0x12 the most significant byte and 0x34 the least significant byte. In a 32-bit\nvalue, such as 0xABCD1234, the most-significant byte is 0xAB and the least-\nsignificant is 0x34. The same concepts apply to 64-bit integers and to 32- and\n64-bit floating-point values as well.\nMultibyte integers can be stored into memory in one of two ways, and dif-\nferent microprocessors may differ in their choice of storage method (see Fig-\nure 3.6).\n•Little-endian . If a microprocessor stores the least significant byte of a\nmultibyte value at a lower memory address than the most significant\nbyte, we say that the processor is little-endian. On a little-endian ma-\nchine, the number 0xABCD1234 would be stored in memory using the\nconsecutive bytes 0x34, 0x12, 0xCD, 0xAB.\n•Big-endian . Ifamicroprocessorstoresthemostsignificantbyteofamulti-\nbyte value at a lower memory address than the least significant byte,\nU32 value = 0xABCD1234;\nU8* pBytes = (U8*)&value;\nFigure 3.6. Big- and little-endian representations of the value 0xABCD1234.\n3.3. Data, Code and Memory Layout 141\nwe say that the processor is big-endian. On a big-endian machine, the\nnumber0xABCD1234wouldbestoredinmemoryusingthebytes0xAB,\n0xCD, 0x12, 0x34.\nMostprogrammersdon’tneedtothinkmuchaboutendianness. However,\nwhen you’re a game programmer, endianness can become a bit of a thorn in\nyour side. This is because games are usually developed on a Windows or Linux\nmachine running an Intel Pentium processor (which is little-endian), but run\non a console such as the Wii, Xbox 360 or PlayStation 3—all three of which\nutilize a variant of the PowerPC processor (which can be configured to use\neither endianness, but is big-endian by default). Now imagine what happens\nwhen you generate a data file for consumption by your game engine on an\nIntel processor and then try to load that data file into your engine running on\na PowerPC processor. Any multibyte value that you wrote out into that data\nfile will be stored in little-endian format. But when the game engine reads\nthe file, it expects all of its data to be in big-endian format. The result? You’ll\nwrite 0xABCD1234, but you’ll read 0x3412CDAB, and that’s clearly not what\nyou intended!\nThere are at least two solutions to this problem.\n1. Youcouldwriteallyourdatafilesastextandstoreallmultibytenumbers\nas sequences of decimal or hexadecimal digits, one character (one byte)\nper digit. This would be an inefficient use of disk space, but it would\nwork.\n2. You can have your tools endian-swap the data prior to writing it into\na binary data file. In effect, you make sure that the data file uses the\nendianness of the target microprocessor (the game console), even if the\ntools are running on a machine that uses the opposite endianness.\nInteger Endian-Swapping\nEndian-swapping an integer is not conceptually difficult. You simply start at\nthe most significant byte of the value and swap it with the least significant\nbyte; you continue this process until you reach the halfway point in the value.\nFor example, 0xA7891023 would become 0x231089A7.\nThe only tricky part is knowing whichbytes to swap. Let’s say you’re writ-\ning the contents of a C struct or C++ classfrom memory out to a file. To\nproperly endian-swap this data, you need to keep track of the locations and\nsizes of each data member in the struct and swap each one appropriately\nbased on its size. For example, the structure\n142 3. Fundamentals of Software Engineering for Games\nstruct Example\n{\nU32 m_a;\nU16 m_b;\nU32 m_c;\n};\nmight be written out to a data file as follows:\nvoid writeExampleStruct(Example& ex, Stream& stream)\n{\nstream.writeU32(swapU32(ex.m_a));\nstream.writeU16(swapU16(ex.m_b));\nstream.writeU32(swapU32(ex.m_c));\n}\nand the swap functions might be defined like this:\ninline U16 swapU16(U16 value)\n{\nreturn ((value & 0x00FF) << 8)\n| ((value & 0xFF00) >> 8);\n}\ninline U32 swapU32(U32 value)\n{\nreturn ((value & 0x000000FF) << 24)\n| ((value & 0x0000FF00) << 8)\n| ((value & 0x00FF0000) >> 8)\n| ((value & 0xFF000000) >> 24);\n}\nYou cannot simply cast the Example object into an array of bytes and\nblindly swap the bytes using a single general-purpose function. We need to\nknowboth whichdatamembers toswapand howwide eachmemberis, andeach\ndata member must be swapped individually.\nSome compilers provide built-in endian-swapping macros, freeing you\nfrom having to write your own. For example, gcc offers a family of macros\nnamed __builtin_bswapXX() for performing 16-, 32- and 64-bit endian\nswaps. However, such compiler-specific facilities are of course non-portable.\nFloating-Point Endian-Swapping\nAs we’ve seen, an IEEE-754 floating-point value has a detailed internal struc-\nture involving some bits for the mantissa, some bits for the exponent and a\nsign bit. However, you can endian-swap it just as if it were an integer, be-\ncause bytes are bytes. You merely reinterpret the bit pattern of your float\n3.3. Data, Code and Memory Layout 143\nas if it were a std::int32_t, perform the endian swapping operation, and\nthen reinterpret the result as a floatagain.\nYoucanreinterpretfloatsasintegersbyusingC++’s reinterpret_cast\noperatoronapointertothefloat,andthendereferencingthetype-castpointer;\nthis is known as type punning. But punning can lead to optimization bugs\nwhen strict aliasing is enabled. (See http://www.cocoawithlove.com/2008/\n04/using-pointers-to-recast-in-c-is-bad.html for an excellent description of\nthis problem.) One alternative that’s guaranteed to be portable is to use a\nunion, as follows:\nunion U32F32\n{\nU32 m_asU32;\nF32 m_asF32;\n};\ninline F32 swapF32(F32 value)\n{\nU32F32 u;\nu.m_asF32 = value;\n// endian-swap as integer\nu.m_asU32 =swapU32(u.m_asU32);\nreturn u. m_asF32 ;\n}\n3.3.3 Kilobytes versus Kibibytes\nYou’veprobablyusedmetric(SI)unitslikekilobytes(kB)andmegabytes(MB)\nto describe quantities of memory. However, the use of these units to describe\nquantities of memory that are measured in powers of two isn’t strictly cor-\nrect. When a computer programmer speaks of a “kilobyte,” she or he usually\nmeans 1024 bytes. But SI units define the prefix “kilo” to mean 103or 1000,\nnot 1024.\nTo resolve this ambiguity, the International Electrotechnical Commission\n(IEC) in 1998 established a new set of SI-like prefixes for use in computer sci-\nence. These prefixes are defined in terms of powers of two rather than pow-\ners of ten, so that computer engineers can precisely and conveniently spec-\nify quantities that are powers of two. In this new system, instead of kilobyte\n(1000 bytes), we say kibibyte (1024 bytes, abbreviated KiB). And instead of\nmegabyte (1,000,000 bytes), we say mebibyte (1024 1024= 1,048,576 bytes,\nabbreviated MiB). Table 3.1 summarizes the sizes, prefixes and names of the\n144 3. Fundamentals of Software Engineering for Games\nMetric (SI) IEC\nValue Unit Name Value Unit Name\n1000 kBkilobyte 1024 KiBkibibyte\n10002MBmegabyte 10242MiBmebibyte\n10003GBgigabyte 10243GiBgibibyte\n10004TBterabyte 10244TiBtebibyte\n10005PBpetabyte 10245PiBpebibyte\n10006EBexabyte 10246EiBexbibyte\n10007ZBzettabyte 10247ZiBzebibyte\n10008YByottabyte 10248YiByobibyte\nTable 3.1. Comparison of metric (SI) units and IEC units for describing quantities of bytes.\nmostcommonlyusedbytequantityunitsinboththeSIandIECsystems. We’ll\nuse IEC units throughout this book.\n3.3.4 Declarations, Deﬁnitions and Linkage\n3.3.4.1 Translation Units Revisited\nAs we saw in Chapter 2, a C or C++ program is comprised of translationunits.\nThe compiler translates one .cpp file at a time, and for each one it generates\nan output file called an object file (.o or .obj). A .cpp file is the smallest unit of\ntranslation operated on by the compiler; hence, the name “translation unit.”\nAn object file contains not only the compiled machine code for all of the func-\ntions defined in the .cpp file, but also all of its global and static variables. In\naddition,anobjectfilemaycontain unresolvedreferences tofunctionsandglobal\nvariables defined in other.cpp files.\nThe compiler only operates on one translation unit at a time, so whenever\nit encounters a reference to an external global variable or function, it must\n“go on faith” and assume that the entity in question really exists, as shown\nin Figure 3.7. It is the linker’s job to combine all of the object files into a fi-\nnal executable image. In doing so, the linker reads all of the object files and\nattempts to resolve all of the unresolved cross-references between them. If it\nis successful, an executable image is generated containing all of the functions,\nglobal variables and static variables, with all cross-translation-unit references\nproperly resolved. This is depicted in Figure 3.8.\nThe linker’s primary job is to resolve external references, and in this\n3.3. Data, Code and Memory Layout 145\nfoo.cpp\nU32 gGlobalA;\nU32 gGlobalB;\nvoid f()\n{\n    // ...    gGlobalC = 5.3f;    // ...\n}extern U32 gGlobalC;bar.cpp\nF32 gGlobalC;\nvoid g()\n{\n    // ...\n    U32 a = gGlobalA;    // ...    f();\n    // ...\n    gGlobalB = 0;\n}extern U32 gGlobalA;\nextern U32 gGlobalB;\nextern void f();\nFigure 3.7. Unresolved external references in two translation units.\nfoo.cpp\nU32 gGlobalA;\nU32 gGlobalB;\nvoid f()\n{    // ...\n    gGlobalC = 5.3f;\n    // ...}extern U32 gGlobalC;bar.cpp\nF32 gGlobalC;\nvoid g(){\n    // ...\n    U32 a = gGlobalA;    // ...    f();\n    // ...\n    gGlobalB = 0;\n}extern U32 gGlobalA;\nextern U32 gGlobalB;\nextern void f();\nFigure 3.8. Fully resolved external references after successful linking.\n???\nUnresolved Reference???Multiply-D efined Symbol\n???foo.cpp\nU32 gGlobalA;\nU32 gGlobalB;\nvoid f()\n{    // ...    gGlobalC = 5.3f;\n    gGlobalD = -2;\n    // ...\n}extern U32 gG lobalC;bar.cpp\nF32 gGlobalC;\nvoid g()\n{\n    // ...\n    U32 a = gGlobalA;\n    // ...\n    f();\n    // ...\n    gGlobalB = 0;\n}extern U32 gGlo balA;\nextern U32 gGlo balB;\nextern void f();spam.cpp\nU32 gGlobalA;\nvoid h(){    // ...}\nFigure 3.9. The two most common linker errors.\n146 3. Fundamentals of Software Engineering for Games\ncapacity it can generate only two kinds of errors:\n1. Thetargetofan extern referencemightnotbefound, inwhichcasethe\nlinker generates an “unresolved symbol” error.\n2. The linker might find more than one variable or function with the same\nname, in which case it generates a “multiply defined symbol” error.\nThese two situations are shown in Figure 3.9.\n3.3.4.2 Declaration versus Deﬁnition\nIn the C and C++ languages, variables and functions must be declared andde-\nfinedbefore they can be used. It is important to understand the difference be-\ntween a declaration and a definition in C and C++.\n• Adeclaration is a description of a data object or function. It provides the\ncompiler with the nameof the entity and its datatype orfunctionsignature\n(i.e., return type and argument type(s)).\n• Adefinition, on the other hand, describes a unique region of memory in\nthe program. This memory might contain a variable, an instance of a\nstruct or class or the machine code of a function.\nIn other words, a declaration is a reference to an entity, while a definition\nis theentity itself. A definition is always a declaration, but the reverse is not\nalways the case—it is possible to write a pure declaration in C and C++ that is\nnot a definition.\nFunctionsare definedbywritingthebodyofthefunctionimmediatelyafter\nthe signature, enclosed in curly braces:\nfoo.cpp\n// definition of the max() function\nint max(int a, int b)\n{\nreturn (a > b) ? a : b;\n}\n// definition of the min() function\nint min(int a, int b)\n{\nreturn (a <= b) ? a : b;\n}\n3.3. Data, Code and Memory Layout 147\nA pure declaration can be provided for a function so that it can be used in\nother translation units (or later in the same translation unit). This is done by\nwriting a function signature followed by a semicolon, with an optional prefix\nofextern :\nfoo.h\nextern int max(int a, int b); // a function declaration\nint min(int a, int b); // also a declaration (the extern\n// is optional/assumed)\nVariables and instances of classes and structs are defined by writing the\ndata type followed by the name of the variable or instance and an optional\narray specifier in square brackets:\nfoo.cpp\n// All of these are variable definitions:\nU32 gGlobalInteger = 5;\nF32 gGlobalFloatArray[16];\nMyClass gGlobalInstance;\nA global variable defined in one translation unit can optionally be declared for\nuse in other translation units by using the extern keyword:\nfoo.h\n// These are all pure declarations:\nextern U32 gGlobalInteger;\nextern F32 gGlobalFloatArray[16];\nextern MyClass gGlobalInstance;\nMultiplicity of Declarations and Deﬁnitions\nNot surprisingly, any particular data object or function in a C/C++ program\ncanhavemultipleidentical declarations,buteachcanhaveonlyone definition . If\ntwoormoreidenticaldefinitionsexistinasingletranslationunit,thecompiler\nwill notice that multiple entities have the same name and flag an error. If two\nor more identical definitions exist in different translation units, the compiler\nwill not be able to identify the problem, because it operates on one translation\nunit at a time. But in this case, the linker will give us a “multiply defined\nsymbol” error when it tries to resolve the cross-references.\n148 3. Fundamentals of Software Engineering for Games\nDeﬁnitions in Header Files and Inlining\nIt is usually dangerous to place definitions in header files. The reason for this\nshould be pretty obvious: if a header file containing a definition is#included\ninto more than one .cpp file, it’s a sure-fire way of generating a “multiply de-\nfined symbol” linker error.\nInline function definitions are an exception to this rule, because each invo-\ncation of an inline function gives rise to a brand new copy of that function’s\nmachinecode,embeddeddirectlyintothecallingfunction. Infact,inlinefunc-\ntiondefinitions mustbeplacedinheaderfilesiftheyaretobeusedinmorethan\none translation unit. Note that it is notsufficient to tag a function declaration\nwith the inline keyword in a .h file and then place the body of that function\nin a .cpp file. The compiler must be able to “see” the body of the function in\norder to inline it. For example:\nfoo.h\n// This function definition will be inlined properly.\ninline int max(int a, int b)\n{\nreturn (a > b) ? a : b;\n}\n// This declaration cannot be inlined because the\n// compiler cannot ""see"" the body of the function.\ninline int min(int a, int b);\nfoo.cpp\n// The body of min() is effectively ""hidden"" from the\n// compiler, so it can ONLY be inlined within foo.cpp.\nint min(int a, int b)\n{\nreturn (a <= b) ? a : b;\n}\nTheinline keyword is really just a hint to the compiler. It does a cost/\nbenefitanalysisofeachinlinefunction,weighingthesizeofthefunction’scode\nversus the potential performance benefits of inling it, and the compiler gets\nthe final say as to whether the function will really be inlined or not. Some\ncompilers provide syntax like __forceinline, allowing the programmer\nto bypass the compiler’s cost/benefit analysis and control function inlining\ndirectly.\n3.3. Data, Code and Memory Layout 149\nTemplates and Header Files\nThe definition of a templated class or function must be visible to the compiler\nacross all translation units in which it is used. As such, if you want a template\nto be usable in more than one translation unit, the template must be placed\ninto a header file (just as inline function definitions must be). The declara-\ntionanddefinitionofatemplatearethereforeinseparable: Youcannotdeclare\ntemplated functions or classes in a header but “hide” their definitions inside\na .cpp file, because doing so would render those definitions invisible within\nany other .cpp file that includes that header.\n3.3.4.3 Linkage\nEvery definition in C and C++ has a property known as linkage. A definition\nwithexternal linkage is visible to and can be referenced by translation units\nother than the one in which it appears. A definition with internal linkage can\nonly be “seen” inside the translation unit in which it appears and thus cannot\nbe referenced by other translation units. We call this property linkagebecause\nit dictates whether or not the linker is permitted to cross-reference the entity\nin question. So, in a sense, linkage is the translation unit’s equivalent of the\npublic: andprivate: keywords in C++ class definitions.\nBydefault,definitionshaveexternallinkage. The static keywordisused\nto change a definition’s linkage to internal. Note that two or more identical\nstatic definitions in two or more different .cpp files are considered to be\ndistinct entities by the linker (just as if they had been given different names),\nso they will notgenerate a “multiply defined symbol” error. Here are some\nexamples:\nfoo.cpp\n// This variable can be used by other .cpp files\n// (external linkage).\nU32 gExternalVariable;\n// This variable is only usable within foo.cpp (internal\n// linkage).\nstatic U32 gInternalVariable;\n// This function can be called from other .cpp files\n// (external linkage).\nvoid externalFunction()\n{\n// ...\n}\n150 3. Fundamentals of Software Engineering for Games\n// This function can only be called from within foo.cpp\n// (internal linkage).\nstatic void internalFunction()\n{\n// ...\n}\nbar.cpp\n// This declaration grants access to foo.cpp's variable.\nextern U32 gExternalVariable;\n// This 'gInternalVariable' is distinct from the one\n// defined in foo.cpp -- no error. We could just as\n// well have named it gInternalVariableForBarCpp -- the\n// net effect is the same.\nstatic U32 gInternalVariable;\n// This function is distinct from foo.cpp's\n// version -- no error. It acts as if we had named it\n// internalFunctionForBarCpp().\nstatic void internalFunction()\n{\n// ...\n}\n// ERROR -- multiply defined symbol!\nvoid externalFunction()\n{\n// ...\n}\nTechnically speaking, declarations don’t have a linkage property at all, be-\ncausetheydonotallocateanystorageintheexecutableimage;therefore,there\nis no question as to whether or not the linker should be permitted to cross-\nreference that storage. A declaration is merely a reference to an entity defined\nelsewhere. However, it is sometimes convenient to speak about declarations\nashavinginternallinkage,becauseadeclarationonlyappliestothetranslation\nunit in which it appears. If we allow ourselves to loosen our terminology in\nthis manner, then declarations alwayshave internal linkage—there is no way\ntocross-referenceasingledeclarationinmultiple.cppfiles. (Ifweputadecla-\nration in a header file, then multiple .cpp files can “see” that declaration, but\nthey are in effect each getting a distinct copyof the declaration, and each copy\nhas internal linkage within that translation unit.)\n3.3. Data, Code and Memory Layout 151\nThis leads us to the real reason why inline function definitions are permit-\ntedinheaderfiles: itisbecauseinlinefunctionshave internallinkage bydefault,\njust as if they had been declared static. If multiple .cpp files #include a\nheader containing an inline function definition, each translation unit gets a\nprivate copy of that function’s body, and no “multiply defined symbol” errors\nare generated. The linker sees each copy as a distinct entity.\n3.3.5 Memory Layout of a C/C++ Program\nAprogramwritteninCorC++storesitsdatainanumberofdifferentplacesin\nmemory. Inordertounderstandhowstorageisallocatedandhowthevarious\ntypes of C/C++ variables work, we need to understand the memory layout of\na C/C++ program.\n3.3.5.1 Executable Image\nWhen a C/C++ program is built, the linker creates an executable file. Most\nUNIX-like operating systems, including many game consoles, employ a pop-\nular executable file format called the executable and linking format (ELF). Exe-\ncutable files on those systems therefore have a .elf extension. The Windows\nexecutable format is similar to the ELF format; executables under Windows\nhavea.exeextension. Whateveritsformat,theexecutablefilealwayscontains\na partial imageof the program as it will exist in memory when it runs. I say a\n“partial” image because the program generally allocates memory at runtime\nin addition to the memory laid out in its executable image.\nThe executable image is divided into contiguous blocks called segments or\nsections. Every operating system lays things out a little differently, and the\nlayout may also differ slightly from executable to executable on the same op-\nerating system. But the image is usually comprised of at least the following\nfour segments:\n1.Text segment . Sometimes called the code segment , this block contains exe-\ncutable machine code for all functions defined by the program.\n2.Data segment . This segment contains all initialized global and static vari-\nables. The memory needed for each global variable is laid out exactly\nas it will appear when the program is run, and the proper initial values\nare all filled in. So when the executable file is loaded into memory, the\ninitialized global and static variables are ready to go.\n3.BSSsegment . “BSS”is an outdated name which stands for “block started\nby symbol.” This segment contains all of the uninitialized global and\n152 3. Fundamentals of Software Engineering for Games\nstatic variables defined by the program. The C and C++ languages ex-\nplicitly define the initial value of any uninitialized global or static vari-\nable to be zero. But rather than storing a potentially very large block of\nzeros in the BSS section, the linker simply stores a countof how many\nzero bytes are required to account for all of the uninitialized globals and\nstatics in the segment. When the executable is loaded into memory, the\noperatingsystemreservestherequestednumberofbytesfortheBSSsec-\ntionandfillsitwithzerospriortocallingtheprogram’sentrypoint(e.g.,\nmain() orWinMain()).\n4.Read-only data segment. Sometimes called the rodatasegment, this seg-\nment contains any read-only (constant) global data defined by the pro-\ngram. For example, all floating-point constants (e.g., const float\nkPi = 3.141592f; ) and all global object instances that have been de-\nclared with the const keyword (e.g., const Foo gReadOnlyFoo;)\nreside in this segment. Note that integer constants (e.g., const int\nkMaxMonsters = 255;) are often used as manifest constants by the\ncompiler, meaning that they are inserted directly into the machine code\nwherever they are used. Such constants occupy storage in the text seg-\nment, but they are not present in the read-only data segment.\nGlobal variables (variables defined at file scope outside any function or\nclass declaration) are stored in either the data or BSS segments, depending on\nwhether or not they have been initialized. The following global will be stored\nin the data segment, because it has been initialized:\nfoo.cpp\nF32 gInitializedGlobal = -2.0f;\nand the following global will be allocated and initialized to zero by the oper-\nating system, based on the specifications given in the BSS segment, because it\nhas not been initialized by the programmer:\nfoo.cpp\nF32 gUninitializedGlobal;\nWe’ve seen that the static keyword can be used to give a global variable\nor function definition internal linkage, meaning that it will be “hidden” from\nother translation units. The static keyword can also be used to declare a\nglobal variable withinafunction. A function-static variable is lexicallyscoped to\nthefunctioninwhichitisdeclared(i.e.,thevariable’snamecanonlybe“seen”\ninside the function). It is initialized the first time the function is called (rather\nthan before main() is called, as with file-scope statics). But in terms of mem-\nory layout in the executable image, a function-static variable acts identically\n3.3. Data, Code and Memory Layout 153\nto a file-static global variable—it is stored in either the data or BSS segment\nbased on whether or not it has been initialized.\nvoid readHitchhikersGuide(U32 book)\n{\nstatic U32 sBooksInTheTrilogy = 5; // data segment\nstatic U32 sBooksRead; // BSS segment\n// ...\n}\n3.3.5.2 Program Stack\nWhen an executable program is loaded into memory and run, the operating\nsystemreservesanareaofmemoryforthe programstack. Wheneverafunction\niscalled, acontiguousareaofstackmemoryispushedontothestack—wecall\nthis block of memory a stackframe . If function a()calls another function b(),\nanewstackframefor b()ispushedontopof a()’sframe. When b()returns,\nits stack frame is popped, and execution continues wherever a()left off.\nA stack frame stores three kinds of data:\n1. It stores the return address of the calling function so that execution may\ncontinue in the calling function when the called function returns.\n2. The contents of all relevant CPU registers are saved in the stack frame.\nThis allows the new function to use the registers in any way it sees fit,\nwithout fear of overwriting data needed by the calling function. Upon\nreturn to the calling function, the state of the registers is restored so that\nexecution of the calling function may resume. The return value of the\ncalled function, if any, is usually left in a specific register so that the call-\ning function can retrieve it, but the other registers are restored to their\noriginal values.\n3. The stack frame also contains all local variables declared by the function;\nthese are also known as automatic variables . This allows each distinct\nfunction invocation to maintain its own private copy of every local vari-\nable,evenwhenafunctioncallsitselfrecursively. (Inpractice,somelocal\nvariablesareactuallyallocatedtoCPUregistersratherthanbeingstored\nin the stack frame, but for the most part such variables operate as if they\nwere allocated within the function’s stack frame.)\nPushing and popping stack frames is usually implemented by adjust-\ning the value of a single register in the CPU, known as the stack pointer.\nFigure 3.10 illustrates what happens when the functions shown below are\nexecuted.\n154 3. Fundamentals of Software Engineering for Games\nvoid c()\n{\nU32 localC1;\n// ...\n}\nF32 b()\n{\nF32 localB1;\nI32 localB2;\n// ...\nc();\n// ...\nreturn localB1;\n}\na()’s\nstack\nframesaved CPU registersreturn ad dress\naLocal sA1[5]\nlocalA2a()’s\nstack\nframesaved CPU registersreturn address\naLocalsA1[5]\nlocalA2a()’s\nstack\nframesaved CPU registersreturn address\naLocalsA1[5]\nlocalA2\nb()’s\nstack\nframesaved CPU registersreturn address\nlocalB1\nlocalB2b()’s\nstack\nframesaved CPU registersreturn address\nlocalB1\nlocalB2\nsaved CPU registersreturn address\nlocalC1c()’s\nstack\nframefunction a() is called function b() is called function c() is called\nFigure 3.10. Stack frames.\n3.3. Data, Code and Memory Layout 155\nvoid a()\n{\nU32 aLocalsA1[5];\n// ...\nF32 localA2 = b();\n// ...\n}\nWhen a function containing automatic variables returns, its stack frame is\nabandoned and all automatic variables in the function should be treated as if\nthey no longer exist. Technically, the memory occupied by those variables is\nstill there in the abandoned stack frame—but that memory will very likely be\noverwritten as soon as another function is called. A common error involves\nreturning the address of a local variable, like this:\nU32* getMeaningOfLife()\n{\nU32 anInteger = 42;\nreturn &anInteger;\n}\nYoumightget away with this if you use the returned pointer immediately\nand don’t call any other functions in the interim. But more often than not, this\nkind of code will crash—sometimes in ways that can be difficult to debug.\n3.3.5.3 Dynamic Allocation Heap\nThus far, we’ve seen that a program’s data can be stored as global or static\nvariables or as local variables. The globals and statics are allocated within the\nexecutable image, as defined by the data and BSS segments of the executable\nfile. The locals are allocated on the program stack. Both of these kinds of\nstorage are statically defined, meaning that the size and layout of the memory\nis known when the program is compiled and linked. However, a program’s\nmemory requirements are often not fully known at compile time. A program\nusually needs to allocate additional memory dynamically .\nTo allow for dynamic allocation, the operating system maintains a block\nof memory for each running process from which memory can be allocated by\ncalling malloc() (or an OS-specific function like HeapAlloc() under Win-\ndows)andlaterreturnedforreusebytheprocessatsomefuturetimebycalling\nfree() (or an OS-specific function like HeapFree()). This memory block is\n156 3. Fundamentals of Software Engineering for Games\nknown as heap memory , or thefree store. When we allocate memory dynami-\ncally, we sometimes say that this memory resides on the heap.\nIn C++, the global newanddelete operators are used to allocate and free\nmemorytoandfromthefreestore. Bewary,however—individualclassesmay\noverload these operators to allocate memory in custom ways, and even the\nglobal newanddelete operators can be overloaded, so you cannot simply\nassume that newis always allocating from the global heap.\nWe will discuss dynamic memory allocation in more depth in Chapter\n7. For additional information, see http://en.wikipedia.org/wiki/Dynamic_\nmemory_allocation.\n3.3.6 Member Variables\nCstruct sandC++ classesallowvariablestobegroupedintologicalunits.\nIt’s important to remember that a class orstruct declaration allocates no\nmemory. It is merely a description of the layout of the data—a cookie cutter\nwhich can be used to stamp out instances of that struct orclasslater on.\nFor example:\nstruct Foo // struct declaration\n{\nU32 mUnsignedValue;\nF32 mFloatValue;\nbool mBooleanValue;\n};\nOnceastructorclasshasbeendeclared,itcanbeallocated(defined)inany\nof the ways that a primitive data type can be allocated; for example,\n• as an automatic variable, on the program stack;\nvoid someFunction()\n{\nFoo localFoo;\n// ...\n}\n• as a global, file-static or function-static;\nFoo gFoo;\nstatic Foo sFoo;\nvoid someFunction()\n{\nstatic Foo sLocalFoo;\n// ...\n}\n3.3. Data, Code and Memory Layout 157\n• dynamically allocated from the heap. In this case, the pointer or refer-\nence variable used to hold the address of the data can itself be allocated\nas an automatic, global, static or even dynamically.\nFoo* gpFoo = nullptr; // global pointer to a Foo\nvoid someFunction()\n{\n// allocate a Foo instance from the heap\ngpFoo = new Foo;\n// ...\n// allocate another Foo, assign to local pointer\nFoo* pAnotherFoo = new Foo;\n// ...\n// allocate a POINTER to a Foo from the heap\nFoo** ppFoo = new Foo*;\n(*ppFoo) = pAnotherFoo;\n}\n3.3.6.1 Class-Static Members\nAswe’veseen, the static keywordhasmanydifferentmeaningsdepending\non context:\n• When used at file scope, static means “restrict the visibility of this\nvariable or function so it can only be seen inside this .cpp file.”\n• When used at function scope, static means “this variable is a global,\nnot an automatic, but it can only be seen inside this function.”\n• When used inside a struct orclassdeclaration, static means “this\nvariable is not a regular member variable, but instead acts just like a\nglobal.”\nNotice that when static is used inside a class declaration, it does not\ncontrolthe visibility ofthevariable(asitdoeswhenusedatfilescope)—rather,\nit differentiates between regular per-instance member variables and per-class\nvariables that act like globals. The visibility of a class-static variable is deter-\nmined by the use of public:, protected: orprivate: keywords in the\nclass declaration. Class-static variables are automatically included within the\nnamespace of the classorstruct in which they are declared. So the name\nofthe classorstruct mustbeusedtodisambiguatethevariablewhenever\nit is used outside that classorstruct (e.g., Foo::sVarName).\n158 3. Fundamentals of Software Engineering for Games\nLike an extern declaration for a regular global variable, the declaration\nof a class-static variable within a class allocates no memory. The memory for\nthe class-static variable must be defined in a .cpp file. For example:\nfoo.h\nclass Foo\n{\npublic:\nstatic F32 sClassStatic; // allocates no\n// memory!\n};\nfoo.cpp\nF32 Foo::sClassStatic = -1.0f; // define memory and\n// initialize\n3.3.7 Object Layout in Memory\nFigure 3.11. Memory\nlayout of a simple\nstruct.It’susefultobeabletovisualizethememorylayoutofyourclassesandstructs.\nThisisusuallyprettystraightforward—wecansimplydrawaboxforthestruct\nor class, with horizontal lines separating data members. An example of such\na diagram for the struct Foo listed below is shown in Figure 3.11.\nstruct Foo\n{\nU32 mUnsignedValue;\nF32 mFloatValue;\nI32 mSignedValue;\n};\nFigure 3.12. A mem-\nory layout using\nwidth to indicate\nmember sizes.The sizes of the data members are important and should be represented in\nyour diagrams. This is easily done by using the width of each data member to\nindicate its size in bits—i.e., a 32-bit integer should be roughly four times the\nwidth of an eight-bit integer (see Figure 3.12).\nstruct Bar\n{\nU32 mUnsignedValue;\nF32 mFloatValue;\nbool mBooleanValue; // diagram assumes this is 8 bits\n};\n3.3. Data, Code and Memory Layout 159\n3.3.7.1 Alignment and Packing\nAs we start to think more carefully about the layout of our structs and classes\nin memory, we may start to wonder what happens when small data members\nare interspersed with larger members. For example:\nstruct InefficientPacking\n{\nU32 mU1; // 32 bits\nF32 mF2; // 32 bits\nU8 mB3; // 8 bits\nI32 mI4; // 32 bits\nbool mB5; // 8 bits\nchar* mP6; // 32 bits\n};\nFigure 3.13. Inefﬁ-\ncient struct packing\ndue to mixed data\nmember sizes.You might imagine that the compiler simply packs the data members into\nmemory as tightly as it can. However, this is not usually the case. Instead, the\ncompiler will typically leave “holes” in the layout, as depicted in Figure 3.13.\n(Some compilers can be requested not to leave these holes by using a prepro-\ncessor directivelike #pragma pack, or via command-line options; but the de-\nfault behavior is to space out the members as shown in Figure 3.13.)\nWhy does the compiler leave these “holes”? The reason lies in the fact that\nevery data type has a natural alignment, which must be respected in order to\npermit the CPU to read and write memory effectively. The alignment of a data\nobject refers to whether its address in memory is a multiple of its size(which is\ngenerally a power of two):\n• An object with 1-byte alignment resides at any memory address.\n• An object with 2-byte alignment resides only at even addresses (i.e., ad-\ndresses whose least significant nibble is 0x0, 0x2, 0x4, 0x8, 0xA, 0xC or\n0xE).\n• Anobjectwith4-bytealignmentresidesonlyataddressesthatareamul-\ntipleoffour(i.e., addresseswhoseleastsignificantnibbleis0x0, 0x4, 0x8\nor 0xC).\n• A 16-byte aligned object resides only at addresses that are a multiple of\n16 (i.e., addresses whose least significant nibble is 0x0).\nAlignment is important because many modern processors can actually\nonly read and write properly aligned blocks of data. For example, if a pro-\ngram requests that a 32-bit (4-byte) integer be read from address 0x6A341174,\nthememorycontrollerwillloadthedatahappilybecausetheaddressis4-byte\naligned (in this case, its least significant nibble is 0x4). However, if a request\nis made to load a 32-bit integer from address 0x6A34117 3, the memory con-\ntroller now has to read two4-byte blocks: the one at 0x6A341170 and the one\n160 3. Fundamentals of Software Engineering for Games\nCPUalignedValue0x6A341170\n0x6A341174\n0x6A341178\nregister-alignedValue0x6A3411700x6A341174\n0x6A341178un-\n-alignedValueun- shift\nshift\n-alignedValue un-Aligned read from\n0x6A341174Unaligned read from\n0x6A341173\nCPU\nregister\nFigure 3.14. Aligned and unaligned reads of a 32-bit integer.\nat 0x6A341174. It must then mask and shift the two parts of the 32-bit integer\nand logically OR them together into the destination register on the CPU. This\nis shown in Figure 3.14.\nSome microprocessors don’t even go this far. If you request a read or write\nof unaligned data, you might just get garbage. Or your program might just\ncrash altogether! (The PlayStation 2 is a notable example of this kind of intol-\nerance for unaligned data.)\nDifferent data types have different alignment requirements. A good rule\nof thumb is that a data type should be aligned to a boundary equal to the\nwidth of the data type in bytes. For example, 32-bit values generally have a 4-\nbyte alignment requirement, 16-bit values should be 2-byte aligned, and 8-bit\nvalues can be stored at any address (1-byte aligned). On CPUs that support\nSIMD vector math, the vector registers each contain four 32-bit floats, for a\ntotalof128bitsor16bytes. Andasyouwouldguess, afour-floatSIMDvector\ntypically has a 16-byte alignment requirement.\nFigure 3.15. More\nefﬁcient packing by\ngrouping small mem-\nbers together.Thisbringsusbacktothose“holes”inthelayoutof struct Inefficient\nPacking shown in Figure 3.13. When smaller data types like 8-bit bools are\ninterspersed with larger types like 32-bit integers or floats in a structure or\nclass, the compiler introduces padding (holes) in order to ensure that every-\nthing is properly aligned. It’s a good idea to think about alignment and pack-\ning when declaring your data structures. By simply rearranging the members\nofstruct InefficientPacking fromtheexampleabove,wecaneliminate\nsome of the wasted padding space, as shown below and in Figure 3.15:\n3.3. Data, Code and Memory Layout 161\nstruct MoreEfficientPacking\n{\nU32 mU1; // 32 bits (4-byte aligned)\nF32 mF2; // 32 bits (4-byte aligned)\nI32 mI4; // 32 bits (4-byte aligned)\nchar* mP6; // 32 bits (4-byte aligned)\nU8 mB3; // 8 bits (1-byte aligned)\nbool mB5; // 8 bits (1-byte aligned)\n};\nYou’ll notice in Figure 3.15 that the size of the structure as a whole is now\n20 bytes, not 18 bytes as we might expect, because it has been padded by two\nbytes at the end. This padding is added by the compiler to ensure proper\nalignmentofthestructureinan arraycontext . Thatis,ifanarrayofthesestructs\nis defined and the first element of the array is aligned, then the padding at the\nend guarantees that all subsequent elements will also be aligned properly.\nThe alignment of a structure as a whole is equal to the largest alignment\nrequirement among its members. In the example above, the largest member\nalignment is 4-byte, so the structure as a whole should be 4-byte aligned. I\nusuallyliketoaddexplicitpaddingtotheendofmystructstomakethewasted\nspace visible and explicit, like this:\nstruct BestPacking\n{\nU32 mU1; // 32 bits (4-byte aligned)\nF32 mF2; // 32 bits (4-byte aligned)\nI32 mI4; // 32 bits (4-byte aligned)\nchar* mP6; // 32 bits (4-byte aligned)\nU8 mB3; // 8 bits (1-byte aligned)\nbool mB5; // 8 bits (1-byte aligned)\nU8 _pad [2]; // explicit padding\n};\n3.3.7.2 Memory Layout of C++ Classes\nTwo things make C++ classes a little different from C structures in terms of\nmemory layout: inheritance andvirtualfunctions.\nFigure 3.16. Effect of\ninheritance on class\nlayout.When class B inherits from class A, B’s data members simply appear im-\nmediately after A’s in memory, as shown in Figure 3.16. Each new derived\nclasssimplytacksitsdatamembersonattheend,althoughalignmentrequire-\nmentsmayintroducepaddingbetweentheclasses. (Multipleinheritancedoes\nsomewhackythings,likeincludingmultiplecopiesofasinglebaseclassinthe\nmemory layout of a derived class. We won’t cover the details here, because\ngame programmers usually prefer to avoid multiple inheritance altogether\nanyway.)\n162 3. Fundamentals of Software Engineering for Games\nIf a class contains or inherits one or more virtual functions, then four addi-\ntional bytes (or eight bytes if the target hardware uses 64-bit addresses) are\nadded to the class layout, typically at the very beginning of the class’ lay-\nout. These four or eight bytes are collectively called the virtual table pointer\norvpointer, because they contain a pointer to a data structure known as the\nvirtual function table orvtable. The vtable for a particular class contains point-\ners to all the virtual functions that it declares or inherits. Each concrete class\nhas its own virtual table, and every instance of that class has a pointer to it,\nstored in its vpointer.\nThe virtual function table is at the heart of polymorphism, because it al-\nlows code to be written that is ignorant of the specific concrete classes it is\ndealing with. Returning to the ubiquitous example of a Shape base class\nwith derived classes for Circle ,Rectangle andTriangle, let’s imagine\nthatShapedefines a virtual function called Draw(). The derived classes all\noverride this function, providing distinct implementations named Circle::\nDraw(), Rectangle::Draw() andTriangle::Draw(). The virtual ta-\nble for any class derived from Shape will contain an entry for the Draw()\nfunction, but that entry will point to different function implementations,\ndepending on the concrete class. Circle ’s vtable will contain a pointer\ntoCircle::Draw(), while Rectangle’s virtual table will point to Rect-\nangle::Draw(),and Triangle’svtablewillpointto Triangle::Draw().\nGiven an arbitrary pointer to a Shape(Shape* pShape ), the code can sim-\nply dereference the vtable pointer, look up the Draw() function’s entry in the\nvtable, and call it. The result will be to call Circle::Draw() when pShape\npoints to an instance of Circle ,Rectangle::Draw() when pShape\npoints to a Rectangle, and Triangle::Draw() when pShape points to a\nTriangle.\nThese ideas are illustrated by the following code excerpt. Notice that the\nbase class Shape defines two virtual functions, SetId() andDraw(), the\nlatter of which is declared to be pure virtual. (This means that Shape pro-\nvides no default implementation of the Draw() function, and derived classes\nmustoverride it if they want to be instantiable.) Class Circle derives from\nShape, adds some data members and functions to manage its center and ra-\ndius, and overrides the Draw() function; this is depicted in Figure 3.17. Class\nTriangle also derives from Shape. It adds an array of Vector3 objects to\nstore its three vertices and adds some functions to get and set the individual\nvertices. Class Triangle overrides Draw() as we’d expect, and for illustra-\ntivepurposesitalsooverrides SetId(). Thememoryimagegeneratedbythe\nTriangle class is shown in Figure 3.18.\n3.3. Data, Code and Memory Layout 163\nShape::m_id\nCircle::m_center\nCircle::m_r adiusvtable pointer pointer to SetId()\npointer to Draw()+0x00\n+0x04\n+0x08\n+0x14pShape1\nInstance of Circle Circle’s Virtual Table\nCircle::Draw()\n{\n    // code to draw a Circle\n}Shape::SetId(int id)\n{    m_id = id;\n}\nFigure 3.17. pShape1 points to an instance of class Circle.\nclass Shape\n{\npublic:\nvirtual void SetId(int id) { m_id = id; }\nint GetId() const { return m_id; }\nvirtual void Draw() = 0; // pure virtual -- no impl.\nprivate:\nint m_id;\n};\nclass Circle : public Shape\n{\npublic:\nvoid SetCenter(const Vector3& c) { m_center=c; }\nVector3 GetCenter() const { return m_center; }\nvoid SetRadius(float r) { m_radius = r; }\nfloat GetRadius() const { return m_radius; }\nvirtual void Draw()\n{\n// code to draw a circle\n}\nprivate:\nVector3 m_center;\nfloat m_radius;\n};",65866
23-3.4 Computer Hardware Fundamentals.pdf,23-3.4 Computer Hardware Fundamentals,"164 3. Fundamentals of Software Engineering for Games\nclass Triangle : public Shape\n{\npublic:\nvoid SetVertex(int i, const Vector3& v);\nVector3 GetVertex(int i) const { return m_vtx[i]; }\nvirtual void Draw()\n{\n// code to draw a triangle\n}\nvirtual void SetId(int id)\n{\n// call base class' implementation\nShape::SetId(id);\n// do additional work specific to Triangles...\n}\nprivate:\nVector3 m_vtx[3];\n};\n// -----------------------------\nvoid main(int, char**)\n{\nShape* pShape1 = new Circle;\nShape* pShape2 = new Triangle;\npShape1->Draw();\npShape2->Draw();\n// ...\n}\n3.4 Computer Hardware Fundamentals\nProgramming in a high-level language like C++, C# or Python is an efficient\nway to build software. But the higher-level your language is, the more it\nshields you from the underlying details of the hardware on which your code\nruns. To become a truly proficient programmer, it’s important to understand\nthe architecture of your target hardware. This knowledge can help you to op-\n3.4. Computer Hardware Fundamentals 165\nShape::m_id\nTriangle::m_vtx[0]\nTriangle::m_vtx[1]vtable pointer pointer to SetId()\npointer to Draw()+0x00\n+0x04\n+0x08\n+0x14pShape2\nInstance of Triangle Triangle’s Virtual Table\nTriangle::Draw()\n{    // code to draw a Triangle\n}Triangle::SetId(int id)\n{\n    Shape::SetId(id);\n    // do additional work\n    // specific to Triangles}\nTriangle::m_vtx[2] +0x20\nFigure 3.18. pShape2 points to an instance of class Triangle .\ntimize your code. It’s also crucial for concurrent programming—and all pro-\ngrammers must understand concurrency if they hope to take full advantage\nof the ever-increasing degree of parallelism in modern computing hardware.\n3.4.1 Learning from the Simpler Computers of Yesteryear\nIn the following pages, we’ll discuss the design of a simple, generic CPU,\nratherthandivingintothespecificsofanyoneparticularprocessor. However,\nsome readers may find it helpful to ground our somewhat theoretical discus-\nsion by reading about the specifics of a real CPU. I myself learned how com-\nputers work when I was a teenager, by programming my Apple II computer\nand the Commodore 64. Both of these machines had a simple CPU known\nas the 6502, which was designed and manufactured by MOS Technology Inc.\nI also learned some of what I know by reading about and working with the\ncommon ancestor to Intel’s entire x86 line of CPUs, the 8086 (and its cousin\nthe8088). Bothoftheseprocessorsaregreatforlearningpurposesduetotheir\nsimplicity. This is especially true of the 6502, which is the simplest CPU I’ve\never worked with. Once you understand how a 6502 and/or an 8086 works,\nmodern CPUs will be a lot easier to understand.\nTo that end, here are a few great resources on the details of 6502 and 8086\narchitecture and programming:\n• Chapters1and2ofthebook“InsidetheAppleIIe”byGaryB.Little[33]\nprovides a great overview of 6502 assembly language programming.\nThe book is available online at http://www.apple2scans.net/files/\nInsidetheIIe.pdf.\n166 3. Fundamentals of Software Engineering for Games\nFigure 3.19. A simple serial von Neumann computer architecture.\n• http://flint.cs.yale.edu/cs421/papers/x86-asm/asm.html gives a nice\noverview of the x86 instruction set architecture.\n• You should also definitely check out Michael Abrash’s Graphics Program-\nming Black Book [1] for loads of useful information about assembly pro-\ngrammingonthe8086,plusincredibletipsforsoftwareoptimizationand\ngraphics programming back in the early days of gaming.\n3.4.2 Anatomy of a Computer\nThesimplestcomputerconsistsofa centralprocessingunit (CPU)andabankof\nmemory, connected to one another on a circuit board called the motherboard via\none or mor ebuses, and connected to external peripheral devices by means of\na set ofI/O ports and/orexpansion slots . This basic design is referred to as the\nvon Neumann architecture because it was first described in 1945 by the mathe-\nmatician and physicist John von Neumann and his colleagues while working\nontheclassifiedENIACproject. Asimpleserialcomputerarchitectureisillus-\ntrated in Figure 3.19.\n3.4.3 CPU\nThe CPU is the “brains” of the computer. It consists of the following compo-\nnents:\n• anarithmetic/logic unit (ALU) for performing integer arithmetic and bit\nshifting,\n• afloating-point unit (FPU) for doing floating-point arithmetic (typically\nusing the IEEE 754 floating-point standard representation),\n• virtually all modern CPUs also contain a vector processing unit (VPU)\nwhich is capable of performing floating-point and integer operations on\nmultiple data items in parallel,\n3.4. Computer Hardware Fundamentals 167\nFigure 3.20. Typical components of a serial CPU.\n• amemorycontroller (MC)ormemorymanagementunit (MMU)forinterfac-\ning with on-chip and off-chip memory devices,\n• a bank of registers which act as temporary storage during calculations\n(among other things), and\n• acontrol unit (CU) for decoding and dispatching machine language in-\nstructionstotheothercomponentsonthechip,androutingdatabetween\nthem.\nAll of these components are driven by a periodic square wave signal known\nas theclock. The frequency of the clock determines the rate at which the CPU\nperforms operations such as executing instructions or performing arithmetic.\nThe typical components of a serial CPU are shown in Figure 3.20.\n3.4.3.1 ALU\nThe ALU performs unary and binary arithmetic operations such as negation,\naddition, subtraction, multiplication and division, and also performs logical\noperationssuchasAND,OR,exclusiveOR(abbreviatedXORorEOR),bitwise\ncomplementandbitshifting. InsomeCPUdesigns,theALUisphysicallysplit\ninto an arithmetic unit (AU) and a logic unit (LU).\nAn ALU typically performs integer operations only. Floating-point calcu-\nlations require very different circuitry, and are usually performed by a phys-\nically separate floating-point unit (FPU). Early CPUs like the Intel 8088/8086\nhad no on-chip FPU; if floating-point math support was desired, these CPUs\nhad to be augmented with a separate FPU co-processor chip such as the Intel\n8087. In later CPU designs, an FPU was typically included on the main CPU\ndie.\n168 3. Fundamentals of Software Engineering for Games\n3.4.3.2 VPU\nA vector processing unit (VPU) acts a bit like a combination ALU/FPU, in\nthat it can typically perform both integer and floating-point arithmetic. What\ndifferentiates a VPU is its ability to apply arithmetic operators to vectorsof\ninput data—each of which typically consists of between two and 16 floating-\npointvaluesorupto64integervaluesofvariouswidths—ratherthantoscalar\ninputs. Vector processing is also known as single instruction multiple data or\nSIMD, because a single arithmetic operator (e.g, multiplication) is applied to\nmultiple pairs of inputs simultaneously. See Section 4.10 for more details.\nToday’s CPUs don’t actually contain an FPU per se. Instead, all floating-\npoint calculations, even those involving scalar floatvalues, are performed\nby the VPU. Eliminating the FPU reduces the transistor count on the CPU\ndie, allowing those transistors to be used to implement larger caches, more-\ncomplex out-of-order execution logic, and so on. And as we’ll see in Section\n4.10.6, optimizing compilers will typically convert math performed on float\nvariables into vectorized code that uses the VPU anyway.\n3.4.3.3 Registers\nIn order to maximize performance, an ALU or FPU can usually only perform\ncalculations on data that exists in special high-speed memory cells called reg-\nisters. Registers are typically physically separate from the computer’s main\nmemory, located on-chip and in close proximity to the components that ac-\ncess them. They’re usually implemented using fast, high-cost multi-ported\nstatic RAM or SRAM. (See Section 3.4.5 for more information about memory\ntechnologies.) A bank of registers within a CPU is called a registerfile .\nBecause registers aren’t part of main memory,1they typically don’t have\naddresses but they do have names. These could be as simple as R0, R1, R2\netc., although early CPUs tended to use letters or short mnemonics as regis-\nter names. For example, the Intel 8088/8086 had four 16-bit general-purpose\nregisters named AX, BX, CX and DX. The 6502 by MOS Technology, Inc. per-\nformed all of its arithmetic operations using a register known as the accumula-\ntor2(A), and used two auxillary registers called X and Y for other operations\nsuch as indexing into an array.\n1Some early computers did use main RAM to implement registers. For example, the 32 regis-\nters in the IBM 7030 Stretch (IBM’s first transistor-based supercomputer) were “overlaid” on the\nfirst 32 addresses in main RAM. In some early ALU designs, one of its inputs would come from\na register while the other came from main RAM. These designs were practical because back then,\nRAM access latencies were low relative to the overall performance of the CPU.\n2The term “accumulator” arose because early ALUs used to work one bit at a time, and would\ntherefore accumulate theanswerbymaskingandshiftingtheindividualbitsintotheresultregister.\n3.4. Computer Hardware Fundamentals 169\nSome of the registers in a CPU are designed to be used for general calcula-\ntions. They’reappropriatelynamed general-purposeregisters (GPR).EveryCPU\nalso contains a number of special-purposeregisters (SPR). These include:\n• theinstructionpointer (IP),\n• thestackpointer (SP),\n• thebasepointer (BP) and\n• thestatusregister .\nInstruction Pointer\nTheinstruction pointer (IP) contains the address of the currently-executing in-\nstruction in a machine language program (more on machine language in Sec-\ntion 3.4.7.2).\nStack Pointer\nInSection3.3.5.2wesawhowaprogram’s callstack servesbothastheprimary\nmechanism by which functions call one another, and as the means by which\nmemory is allocated for local variables. The stack pointer (SP) contains the ad-\ndress of the top of the program’s call stack. The stack can grow up or down in\ntermsofmemoryaddresses,butforthepurposeofthisdiscussionlet’sassume\nit grows downward. In this case, a data item may be pushedonto the stack by\nsubtracting the size of the item from the value of the stack pointer, and then\nwriting the item at the new address pointed to by SP. Likewise, an item can be\npoppedoff the stack by reading it from the address pointed to by SP, and then\nadding its size to SP.\nBase Pointer\nThebase pointer (BP) contains the base address of the current function’s stack\nframeonthecallstack. Manyofafunction’slocalvariablesareallocatedwithin\nits stack frame, although the optimizer may assign others exclusively to a reg-\nister for the duration of the function. Stack-allocated variables occupy a range\nof memory addresses at a unique offset from the base pointer. Such a vari-\nable can be located in memory by simply subtracting its unique offset from\nthe address stored in BP (assuming the stack grows down).\nStatus Register\nA special register known as the status register ,condition code register orflags\n170 3. Fundamentals of Software Engineering for Games\nregistercontains bits that reflect the results of the most-recent ALU operation.\nFor instance, if the result of a subtraction is zero, the zero bit(typically named\n“Z”) is set within the status register, otherwise the bit is cleared. Likewise,\nif an add operation resulted in an overflow, meaning that a binary 1 must be\n“carried”tothenextwordofamulti-wordaddition,the carrybit (oftennamed\n“C”) is set, otherwise it is cleared.\nThe flags in the status register can be used to control program flow via\nconditional branching, or they can be used to perform subsequent calculations,\nsuch as adding the carry bit to the next word in a multi-word addition.\nRegister Formats\nIt’s important to understand that the FPU and VPU typically operate on their\nown private sets of registers, rather than making use of the ALU’s general-\npurposeintegerregisters. Onereasonforthisisspeed—theclosertheregisters\nare to the compute unit that uses them, the less time is needed to access the\ndata they contain. Another reason is that the FPU’s and VPU’s registers are\ntypically widerthan the ALU’s GPRs.\nFor example, a 32-bit CPU has GPRs that are 32 bits wide each, but an\nFPUmightoperateon64-bitdouble-precisionfloats,oreven80-bit“extended”\ndouble-precision values, meaning that its registers have to be 64 or 80 bits\nwide, respectively. Likewise, each register used by a VPU needs to contain\navectorof input data, meaning that these registers must be much wider than\na typical GPR. For example, Intel’s SSE2 (streaming SIMD extensions) vector\nprocessor can be configured to perform calculations on vectors containing ei-\nther four single-precision (32-bit) floating-point values each, or two double-\nprecision (64-bit) values each. Hence SSE2 vector registers are each 128 bits\nwide.\nThe physical separation of registers between ALU and FPU is one reason\nwhy conversions between intandfloatwere very expensive, back in the\ndays when FPUs were commonplace. Not only did the bit pattern of each\nvalue have to be converted back and forth between its two’s complement in-\nteger format and its IEEE 754 floating-point representation, but the data also\nhad to be transferred physically between the general-purpose integer regis-\nters and the FPU’s registers. However, today’s CPUs no longer contain an\nFPU per se—all floating-point math is typically performed by a vector pro-\ncessing unit. A VPU can handle both integer and floating-point math, and\nconversions between the two are much less expensive, even when moving\ndata from an integer GPU into a vector register or vice-versa. That said, it’s\nstill a good idea to avoid converting data between intandfloat formats\n3.4. Computer Hardware Fundamentals 171\nwhere possible, because even a low-cost conversion is still more expensive\nthan no conversion.\n3.4.3.4 Control Unit\nIf the CPU is the “brains” of the computer, then the control unit (CU) is the\n“brains” of the CPU. Its job is to manage the flow of data within the CPU, and\nto orchestrate the operation of all of the CPU’s other components.\nThe CU runs a program by reading a stream of machine language instruc-\ntions, decoding each instruction by breaking it into its opcode and operands,\nand then issuing work requests and/or routing data to the ALU, FPU, VPU,\nthe registers and/or the memory controller as dictated by the current instruc-\ntion’s opcode. In pipelined and superscalar CPUs, the CU also conains com-\nplex circuitry to handle branch prediction and the scheduling of instructions\nforout-of-orderexecution. We’lllookattheoperationoftheCUinmoredetail\nin Section 3.4.7.\n3.4.4 Clock\nEvery digital electronic circuit is essentially a state machine. In order for it to\nchange states, the circuit must be driven to do so by a digital signal. Such a\nsignal might be provided by changing the voltage on a line in the circuit from\n0volts to 3.3volts, or vice-versa.\nStatechangeswithinaCPUaretypicallydrivenbyaperiodicsquarewave\nsignal known as the system clock . Each rising or falling edge of this signal is\nknown as a clock cycle , and the CPU can perform at least one primitive opera-\ntion on every cycle. To a CPU, time therefore appears to be quantized.3\nThe rate at which a CPU can perform its operations is governed by the fre-\nquencyof the system clock. Clock speeds have increased significantly over the\npast few decades. Early CPUs developed in the 1970s, like the MOS Technol-\nogy’s 6502 and Intel’s 8086/8088 CPUs, had clocks that ran in the 1–2 MHz\nrange (millions of cycles per second). Today’s CPUs, like the Intel Core i7, are\ntypically clocked in the 2–4 GHz range ( billionsof cycles per second).\nIt’s important to realize that one CPU instruction doesn’t necessarily take\none clock cycle to execute. Not all instructions are created equal—some in-\nstructions are very simple, while others are more complex. Some instructions\nareimplementedunderthehoodasacombinationofsimplermicro-operations\n(m-ops) and therefore take many more cycles to execute than their simpler\ncounterparts.\n3Contrast this to an analogelectronic circuit, where time is treated as continuous. For example,\nan old-school signal generator can produce a true sine wave that smoothly varies between, say,\n 5volts and 5volts over time.\n172 3. Fundamentals of Software Engineering for Games\nAlso, while early CPUs could truly execute some instructions in a single\nclock cycle, today’s pipelined CPUs break even the simplest instruction down\nintomultiple stages. EachstageinapipelinedCPUtakesoneclockcycletoexe-\ncute, meaningthat aCPUwithan N-stagepipelinehasaminimum instruction\nlatencyofNclock cycles. A simple pipelined CPU can retire instructions at\narateof one instruction per clock cycle, because a new instruction is fed into\nthe pipeline each clock tick. But if you were to trace one particular instruc-\ntion through the pipeline, it would take Ncycles to move from start to finish\nthrough the pipeline. We’ll discuss pipelined CPUs in more depth in Section\n4.2.\n3.4.4.1 Clock Speed versus Processing Power\nThe“processingpower”ofaCPUorcomputercanbedefinedinvariousways.\nOne common measure is the throughput of the machine—the number of oper-\nationsitcanperformduringagivenintervaloftime. Throughputisexpressed\neither in units of millions of instructions per second (MIPS) or floating-point\noperations per second (FLOPS).\nBecause instructions or floating-point operations don’t generally complete\nin exactly one cycle, and because different instructions take differing num-\nbers of cycles to run, the MIPS or FLOPS metrics of a CPU are just averages.\nAs such, you cannot simply look at the clock frequency of a CPU and deter-\nmine its processing power in MIPS or FLOPS. For example, a serial CPU run-\nning at 3 GHz, in which one floating-point multiply takes six cycles to com-\nplete on average, could theoretically achieve 0.5 GFLOPS. But many factors\nincluding pipelining, superscalar designs, vector processing, multicore CPUs\nand other forms of parallelism conspire to obfuscate the relationship between\nclock speed and processing power. Thus the only way to determine the true\nprocessingpowerofaCPUorcomputeristomeasureit—typicallybyrunning\nstandardized benchmarks.\n3.4.5 Memory\nThememory inacomputeractslikeabankofmailboxesatthepostoffice, with\neachboxor“cell”typicallycontainingasingle byteofdata(aneight-bitvalue).4\nEach one-byte memory cell is identified by its address—a simple numbering\n4Actually, early computers often accessed memory in units of “words” that were larger than\n8 bits. For example, the IBM 701 (produced in 1952) addressed memory in units of 36-bit words,\nand the PDP-1 (made in 1959) could access up to 4096 18-bit memory words. Eight-bit bytes were\npopularizedbytheIntel8008in1972. Sevenbitsarerequiredtoencodebothlower-anduppercase\nEnglish letters. By extending this to eight bits, a wide range of special characters could also be\nsupported.\n3.4. Computer Hardware Fundamentals 173\nscheme ranging from 0 to N 1, where Nis the size of addressable memory\nin bytes.\nMemory comes in two basic flavors:\n•read-onlymemory (ROM), and\n• read/writememory,knownforhistoricalreasons5asrandomaccessmem-\nory(RAM).\nROM modules retain their data even when power is not applied to them.\nSometypesofROMcanbeprogrammedonlyonce. Others,knownas electron-\nicallyerasableprogrammableROM orEEPROM,canbereprogrammedoverand\nover again. (Flash drives are one example of EEPROM memory.)\nRAM can be further divided into staticRAM (SRAM) and dynamic RAM\n(DRAM). Both static and dynamic RAM retain their data as long as power\nis applied to them. But unlike static RAM, dynamic RAM also needs to be\n“refreshed”periodically(byreadingthedataandthenre-writingit)inorderto\nprevent its contents from disappearing. This is because DRAM memory cells\nare built from MOS capacitors that gradually lose their charge, and reading\nsuch memory cells is destructive to the data they contain.\nRAM can also be categorized by various other design characteristics, such\nas:\n• whether it is multi-ported, meaning that it be accessed simultaneously by\nmore than one component within the CPU;\n• whether it operates by being synchronized to a clock (SDRAM) or asyn-\nchronously;\n• and whether or not it supports double data rate access (DDR), meaning\nthe RAM can be read or written on both rising and falling clock edges.\n3.4.6 Buses\nData is transferred between the CPU and memory over connections known\nasbuses. A bus is just a bundle of parallel digital “wires” called lines, each\nof which can represent a single bit of data. When the line carries a voltage\nsignal6it represents a binary one, and when the line has no voltage applied\n5Random-accessmemorywassonamedbecauseearliermemorytechnologiesuseddelayloops\nto store data, meaning that it could only be read in the order it was written. RAM technology\nimproved on this situation by permitting data to be accessed at random, i.e., in any order.\n6Earlytransistor-transistor logic(TTL)devicesoperatedatasupplyvoltageof5volts,soa5-volt\nsignal would represent a binary 1. Most of today’s digital electronic devices utilize complementary\nmetal oxide semiconductor logic (CMOS) which can operate at a lower supply voltage, typically\nbetween 1.2 and 3.3 volts.\n174 3. Fundamentals of Software Engineering for Games\n(0 volts) it represents a binary zero. A bundle of nsingle-bit lines arranged in\nparallel can transmit an n-bit number (i.e., any number in the range 0through\n2n 1).\nA typical computer contains two buses: An addressbus and adatabus. The\nCPU loads data from a memory cell into one of its registers by supplying an\naddress to the memory controller via the address bus. The memory controller\nresponds by presenting the bits of the data item stored in the cell(s) in ques-\ntion onto the data bus, where it can be “seen” by the CPU. Likewise, the CPU\nwritesadataitemtomemorybybroadcastingthedestinationaddressoverthe\naddress bus and placing the bit pattern of the data item to write onto the data\nbus. Thememorycontrollerrespondsbywritingthegivendataintothecorre-\nsponding memory cell(s). We should note that the address and data buses are\nsometimes implemented as two physically separate sets of wires, and some-\ntimesasasinglesetofwireswhicharemultiplexedbetweenaddressanddata\nbus functions during different phases of the memory access cycle.\n3.4.6.1 Bus Widths\nThewidthof theaddress bus, measured in bits, controls the range of possible\naddressesthatcanbeaccessedbytheCPU(i.e.,thesizeof addressablememory in\nthemachine). Forexample, acomputerwitha16-bitaddressbuscanaccessat\nmost64KiBofmemory,usingaddressesintherange0x0000through0xFFFF.A\ncomputerwitha32-bitaddressbuscanaccessa4GiBmemory,usingaddresses\nin the range 0x00000000 through 0xFFFFFFFF. And a machine with a 64-bit\naddressbuscanaccessastaggering16EiB(exbibytes)ofmemory. That’s 264=\n16102461.81019bytes!\nThe width of the data bus determines how much data can be transferred\nbetween CPU registers and memory at a time. (The data bus is typically the\nsame width as the general-purpose registers in the CPU, although this isn’t\nalways the case.) An 8-bit data bus means that data can be transferred one\nbyte at a time—loading a 16-bit value from memory would require two sep-\narate memory cycles, one to fetch the least-significant byte and one to fetch\nthe most-significant byte. At the other end of the spectrum, a 64-bit data bus\ncan transfer data between memory and a 64-bit register as a single memory\noperation.\nIt’s possible to access data items that are narrower than the width of a ma-\nchine’s data bus, but it’s typically more costly than accessing items whose\nwidths match that of the data bus. For example, when reading a 16-bit value\non a 64-bit machine, a full 64 bits worth of data must still be read from mem-\nory. Thedesired16-bitfieldthenhastobemaskedoffandpossiblyshiftedinto\nplace within the destination register. This is one reason why the C language\n3.4. Computer Hardware Fundamentals 175\ndoes not define an intto be a specific number of bits wide—it was purpose-\nfullydefinedtomatchthe“natural”sizeofawordonthetargetmachine,inan\nattempttomakesourcecodemoreportable. (Ironicallythispolicyhasactually\nresulted in source code often being lessportable due to implicit assumptions\nabout the width of an int.)\n3.4.6.2 Words\nThe term “word” is often used to describe a multi-byte value. However, the\nnumber of bytes comprising a word is not universally defined. It depends to\nsome degree on context.\nSometimestheterm“word”referstothesmallestmulti-bytevalue,namely\n16 bits or two bytes. In that context, a double word would be 32 bits (four\nbytes) and a quad word would be 64 bits (eight bytes). This is the way the\nterm “word” is used in the Windows API.\nOn the other hand, the term “word” is also used to refer to the “natural”\nsizeofdataitemsonaparticularmachine. Forexample,amachinewith32-bit\nregisters and a 32-bit data bus operates most naturally with 32-bit (four byte)\nvalues, and programmers and hardware folks will sometimes say that such a\nmachine as a word size of 32 bits. The takeaway here is to be aware of context\nwhenever you hear the term “word” being used to refer to the size of a data\nitem.\n3.4.6.3 n-Bit Computers\nYou may have encountered the term “ n-bit computer.” This usually means\na machine with an n-bit data bus and/or registers. But the term is a bit am-\nbiguous, because it might also refer to a computer whose address bus is nbits\nwide. Also, on some CPUs the data bus and register widths don’t match. For\nexample, the8088had16-bitregistersanda16-bitaddressbus, butitonlyhad\nan 8-bit data bus. Hence it acted like a 16-bit machine internally, but its 8-bit\ndatabuscausedittobehavelikean8-bitmachineintermsofmemoryaccesses.\nAgain, be aware of context when talking about an n-bit machine.\n3.4.7 Machine and Assembly Language\nAs far as the CPU is concerned, a “program” is nothing more than a sequen-\ntial stream of relatively simple instructions. Each instruction tells the control\nunit (CU), and ultimately the other components within the CPU such as the\nmemory controller, ALU, FPU or VPU, to perform an operation. An instruc-\ntion might move data around within the computer or within the CPU itself,\nor it might transform that data in some manner (e.g., by performing an arith-\n176 3. Fundamentals of Software Engineering for Games\nmeticorlogicaloperationonthedata). Normallytheinstructionsinaprogram\nare executed sequentially, although some instructions can alter this sequential\nflow of control by “jumping” to a new spot within the program’s overall in-\nstruction stream.\n3.4.7.1 Instruction Set Architecture (ISA)\nCPU designs vary widely from one manufacturer to the next. The set of all\ninstructionssupportedbyagivenCPU,alongwithvariousotherdetailsofthe\nCPU’sdesignlikeitsaddressingmodesandthein-memoryinstructionformat,\nis called its instruction set architecture or ISA. (This is not to be confused with\na programming language’s application binary interface or ABI, which defines\nhigher-levelprotocolslikecallingconventions.) Wewon’tattempttocoverthe\ndetails of any one CPU’s ISA here, but the following categories of instruction\ntypes are common to pretty much every ISA:\n•Move. Theseinstructionsmovedatabetweenregisters,orbetweenmem-\noryandaregister. SomeISAsbreakthe“move”instructionintoseparate\n“load” and “store” instructions.\n•Arithmeticoperations. Theseofcourseincludeaddition,subtraction,mul-\ntiplicationanddivision,butmayalsoincludeotheroperationslikeunary\nnegation, inversion, square root, and so on.\n•Bitwise operators. These include AND, OR, exclusive OR (abbreviated\nXOR or EOR) and bitwise complement.\n•Shift/rotateoperators. Theseinstructionsallowthebitswithinadataword\nto be shifted left or right, with or without affecting the carry bit in the\nstatus register, or rotated (where the bits rolling off one end of the word\n“wrap around” to the other end).\n•Comparison. These instructions allow two values to be compared, in or-\nder to determine if one is less than, greater than or equal to the other. In\nmostCPUs,thecomparisoninstructionsusetheALUtosubtractthetwo\ninput values, thereby setting the appropriate bits in the status register,\nbut the result of the subtraction is simply discarded.\n•Jumpandbranch. Theseallowprogramflowtobealteredbystoringanew\naddress into the instruction pointer. This can be done either uncondi-\ntionally (in which case it is called a “jump” instruction) or conditionally\nbased on the state of various flags in the status register (in which case it\nis often called a “branch”). For example, a “branch if zero” instruction\naltersthecontentsoftheIPifandonlyifthe“Z”bitinthestatusregister\nis set.\n3.4. Computer Hardware Fundamentals 177\n•Push and pop. Most CPUs provide special instructions for pushing the\ncontentsofaregisterontotheprogramstackandforpoppingthecurrent\nvalue at the top of the stack into a register.\n•Functioncalland return . Some ISAs provide explicit instructions for call-\ning a function (also known as a procedure or subroutine) and returning\nfrom it. However, function call and return semantics can also be pro-\nvided by a combination of push, pop and jump instructions.\n•Interrupts . An “interrupt” instruction triggers a digital signal within the\nCPUthatcausesittojumptemporarilytoapre-installed interruptservice\nroutineroutine which is often not part of the program being run. Inter-\nruptsareusedtonotifytheoperatingsystemorauserprogramofevents\nsuch as an input becoming available on a peripheral device. Interrupts\ncan also be triggered by user programs in order to “call” into the oper-\nating system kernel’s routines. See section 4.4.2 for more details.\n•Other instruction types. Most ISAs support a variety of instruction types\nthat don’t fall into one of the categories listed above. For example, the\n“no-op”instruction(oftencalledNOP)isaninstructionthathasnoeffect\nother than to introduce a short delay. NOP instructions also consume\nmemory, and on some ISAs they are used to align subsequent instruc-\ntions properly in memory.\nWe can’t possibly list all instruction types here, but if you’re curious you\ncan always read through the ISA documentation of a real processor like the\nIntel x86 (which is available at http://intel.ly/2woVFQ8).\n3.4.7.2 Machine Language\nComputers can only deal with numbers. As such, each instruction in a pro-\ngram’s instruction stream must be encoded numerically. When a program is\nencoded in this way, we say it is written in machine language, or ML for short.\nOfcourse,machinelanguageisn’tasinglelanguage—it’sreallyamultitudeof\nlanguages, one for each distinct CPU/ISA.\nEvery machine language instruction is comprised of three basic parts:\n• anopcode,whichtellstheCPUwhichoperationtoperform(add,subtract,\nmove, jump, etc.),\n•zero or more operands which specify the inputs and/or outputs of the\ninstruction, and\n• somekindof optionsfield ,specifyingthingslikethe addressingmode ofthe\ninstruction and possibly other flags.\n178 3. Fundamentals of Software Engineering for Games\nFigure 3.21. Two hypothetical machine language instruction encoding schemes. Top: In a variable-\nwidth encoding scheme, different instructions may occupy different numbers of bytes in memory.\nBottom: A ﬁxed-width instruction encoding uses the same number of bytes for every instruction\nin an instruction stream.\nOperands come in many flavors. Some instructions might take the names\n(encoded as numeric ids) of one or more registers as operands. Others might\nexpectaliteralvalueasanoperand(e.g.,“loadthevalue5intoregisterR2,”or\n“jump to address 0x0102ED5C”). The way in which an instruction’s operands\nare interpreted and used by the CPU is known as the instruction’s addressing\nmode. We’ll discuss addressing modes in more detail in Section 3.4.7.4.\nThe opcode and operands (if any) of an ML instruction are packed into a\ncontiguous sequence of bits called an instruction word . A hypothetical CPU\nmight encode its instructions as shown in Figure 3.21 with perhaps the first\nbyte containing the opcode, addressing mode and various options flags, fol-\nlowed by some number of bytes for the operands. Each ISA defines the width\nof an instruction word (i.e., the number of bits occupied by each instruction)\ndifferently. InsomeISAs, allinstructionsoccupyafixednumberofbits; thisis\ntypicalof reducedinstructionsetcomputers (RISC).InotherISAs, differenttypes\nof instructions may be encoded into differently-sized instruction words; this\nis common in complexinstruction set computers (CISC).\nInstruction words can be as small as four bits in some microcontrollers, or\nthey may be many bytes in size. Instruction wordsare often multiples of 32 or\n64bits,becausethismatchesthewidthoftheCPU’sregistersand/ordatabus.\nInvery long instruction word (VLIW) CPU designs, parallelism is achieved by\nallowingmultipleoperationstobeencodedintoasingleverywideinstruction\nword, for the purpose of executing them in parallel. Instructions in a VLIW\nISA can therefore be upwards of hundreds of bytes in width.\n3.4. Computer Hardware Fundamentals 179\nFor a concrete example of machine language instruction encoding on\nthe Intel x86 ISA, see http://aturing.umcs.maine.edu/~meadow/courses/\ncos335/Asm07-MachineLanguage.pdf.\n3.4.7.3 Assembly Language\nWritingaprogramdirectlyinmachinelanguageistediousanderror-prone. To\nmake life easier for programmers, a simple text-based version of machine lan-\nguagewasdevelopedcalled assemblylanguage. Inassemblylanguage,eachin-\nstructionwithinagivenCPU’sISAisgivena mnemonic—ashortEnglishword\nor abbreviation that is much easier to remember than corresponding numeric\nopcode. Instruction operands can be specified conveniently as well: Regis-\nters can be referred to by name (e.g., R0orEAX), and memory addresses can\nbe written in hex, or assigned symbolic names much like the global variables\nof higher-level languages. Locations in the assembly program can be tagged\nwith human-readable labels, and jump/branch instructions refer to these la-\nbels rather than to raw memory addresses.\nAnassemblylanguageprogramconsistsofasequenceofinstructions,each\ncomprised of a mnemonic and zero or more operands, listed in a text file with\none instruction per line. A tool known as an assembler reads the program\nsource file and converts it into the numeric ML representation understood by\nthe CPU. For example, an assembly language program that implements the\nfollowing snippet of C code:\nif (a > b)\nreturn a + b;\nelse\nreturn 0;\ncould be implemented by an assembly language program that looks some-\nthing like this:\n; if (a > b)\ncmp eax, ebx ; compare the values\njle ReturnZero ; jump if less than or equal\n; return a + b;\nadd eax, ebx ; add & store result in EAX\nret ; (EAX is the return value)\n180 3. Fundamentals of Software Engineering for Games\nReturnZero:\n; else return 0;\nxor eax, eax ; set EAX to zero\nret ; (EAX is the return value)\nLet’sbreakthisdown. The cmpinstructioncomparesthevaluesinregisters\nEAX and EBX (which we’re assuming contain the values aandbfrom the C\nsnippet). Next, the jleinstruction branches to the label ReturnZero, but it\ndoes so if and onlyifthe value in EAX is less than or equal to EBX. Otherwise\nit falls through.\nIf EAX is greaterthan EBX (a > b), we fall through to the addinstruction,\nwhich calculates a + band stores the result back into EAX, which we’re as-\nsuming serves as the return value. We issue a retinstruction, and control is\nreturned to the calling function.\nIf EAX is less than or equal to EBX ( a <= b), the branch is taken and we\ncontinue execution immediately after the label ReturnZero. Here, we use a\nlittle trick to set EAX to zero by XORing it with itself. Then we issue a ret\ninstruction to return that zero to the caller.\nFor more details on Intel assembly language programming, see http://\nwww.cs.virginia.edu/~evans/cs216/guides/x86.html.\n3.4.7.4 Addressing Modes\nA seemingly simple instruction like “move” (which transfers data between\nregisters and memory) has many different variants. Are we moving a value\nfrom one register to another? Are we loading a literal value like 5 into a reg-\nister? Are we loading a value from memory into a register? Or are we storing\na value in a register out to memory? All of these variants are called addressing\nmodes. We won’t go over all the possible addressing modes here, but the fol-\nlowing list should give you a good feel for the kinds of addressing modes you\nwill encounter on a real CPU:\n•Register addressing . This mode allows values to be transferred from one\nregister to another. In this case, the operands of the instruction specify\nwhich registers are involved in the operation.\n•Immediateaddressing . This mode allows a literal or “immediate” value to\nbeloadedintoaregister. Inthiscase, theoperandsarethetargetregister\nand the immediate value to be loaded.\n•Directaddressing . Thismodeallowsdatatobemovedtoorfrommemory.\nIn this case, the operands specify the direction of the move (to or from\nmemory) and the memory address in question.",37409
24-3.5 Memory Architectures.pdf,24-3.5 Memory Architectures,"3.5. Memory Architectures 181\n•Register indirect addressing . In this mode, the target memory address is\ntaken from a register, rather than being encoded as a literal value in the\noperands of the instruction. This is how pointer dereferences are im-\nplemented in languages like C and C++. The value of the pointer (an\naddress)isloadedintoaregister, andthenaregister-indirect“move”in-\nstruction is used to dereference the pointer and either load the value it\npoints to into a target register, or store a value in a source register into\nthat memory address.\n•Relative addressing . In this mode, the target memory address is specified\nas an operand, and the value stored in a specified register is used as an\noffset from that target memory address. This is how indexed array ac-\ncesses are implemented in a language like C or C++.\n•Otheraddressingmodes. There are numerous other variations and combi-\nnations, some of which are common to virtually all CPUs, and some of\nwhich are specific to certain CPUs.\n3.4.7.5 Further Reading on Assembly Language\nIn the preceding sections, we’ve had just a tiny taste of assembly language.\nFor an easy-to-digest description of x86 assembly programming, see http://\nflint.cs.yale.edu/cs421/papers/x86-asm/asm.html. For more on calling con-\nventions and ABIs, see https://en.wikibooks.org/wiki/X86_Disassembly/\nFunctions_and_Stack_Frames.\n3.5 Memory Architectures\nInasimplevonNeumanncomputerarchitecture,memoryistreatedasasingle\nhomogeneous block, all of which is equally accessible to the CPU. In reality,\na computer’s memory is almost never architected in such a simplistic man-\nner. For one thing, the CPU’s registers are a form of memory, but registers\nare typically referenced by name in an assembly language program, rather\nthan being addressed like ordinary ROM or RAM. Moreover, even “regular”\nmemory is typically segregated into blocks with different characteristics and\ndifferent purposes. This segregation is done for a variety of reasons, includ-\ningcostmanagementandoptimizationofoverallsystemperformance. Inthis\nsection, we’ll investigate some of the memory architectures commonly found\nin today’s personal computers and gaming consoles, and explore some of the\nkey reasons why they are architected the way they are.\n182 3. Fundamentals of Software Engineering for Games\n3.5.1 Memory Mapping\nAnn-bit address bus gives the CPU access to a theoretical address space that\nis2nbytes in size. An individual memory device (ROM or RAM) is always\naddressed as a contiguous block of memory cells. So a computer’s address\nspace is typically divided into various contiguous segments. One or more of\nthese segments correspond to ROM memory modules, and others correspond\nto RAM modules. For example on the Apple II, the 16-bit addresses in the\nrange 0xC100 through 0xFFFF were assigned to ROM chips (containing the\ncomputer’sfirmware), whiletheaddressesfrom0x0000through0xBFFFwere\nassigned to RAM. Whenever a physical memory device is assigned to a range\nof addresses in a computer’s address space, we say that the address range has\nbeenmapped to the memory device.\nOf course a computer may not have as much memory installed as could\nbe theoretically addressed by its address bus. A 64-bit address bus can ac-\ncess 16 EiB of memory, so we’d be unlikely to ever fully populate such an\naddress space! (At 160 TiB, even HP’s prototype supercomputer called “The\nMachine” isn’t anywhere close to that amount of physical memory.) There-\nfore, it’s commonplace for some segments of a computer’s address space to\nremain unassigned.\n3.5.1.1 Memory-Mapped I/O\nAddress ranges needn’t all map to memory devices—an address range might\nalso be mapped to other peripheral devices, such as a joypad or a network in-\nteracecard(NIC).Thisapproachiscalled memory-mappedI/O becausetheCPU\ncanperformI/Ooperationsonaperipheraldevicebyreadingorwritingtoad-\ndresses, just as if they were oridinary RAM. Under the hood, special circuitry\ndetects that the CPU is reading from or writing to a range of addresses that\nhave been mapped to a non-memory device, and converts the read or write\nrequest into an I/O operation directed at the device in question. As a con-\ncreteexample,theAppleIImappedI/Odevicesintotheaddressrange0xC000\nthrough 0xC0FF, allowing programs to do things like control bank-switched\nRAM, read and control voltages on the pins of a game controller socket on the\nmotherboard, and perform other I/O operations by merely reading from and\nwriting to the addresses in this range.\nAlternatively, a CPU might communicate with non-memory devices via\nspecial registers known as ports. In this case, whenever the CPU requests that\ndata be r ead from or written to a port register, the hardware converts the re-\nquest into an I/O operation on the target device. This approach is called port-\nmapped I/O. In the Arduino line of microcontrollers, port-mapped I/O gives a\n3.5. Memory Architectures 183\nprogram direct control over the digital inputs and outputs at some of the pins\nof the chip.\n3.5.1.2 Video RAM\nRaster-based display devices typically read a hard-wired range of physical\nmemoryaddressesinordertodeterminethebrightness/colorofeachpixelon\nthe screen. Likewise, early charcter-based displays would determine which\ncharacter glyph to display at each location on the screen by reading ASCII\ncodes from a block of memory. A range of memory addresses assigned for\nuse by a video controller is known as video RAM (VRAM).\nInearlycomputersliketheAppleIIandearlyIBMPCs,videoRAMusedto\ncorrespond to memory chips on the motherboard, and memory addresses in\nVRAM could be read from and written to by the CPU in exactly the same way\nas any other memory location. This is also the case on game consoles like the\nPlayStation4andtheXboxOne, whereboththeCPUandGPUshareaccessto\na single, large block of unified memory.\nIn personal computers, the GPU often lives on a separate circuit board,\nplugged into an expansion slot on the motherboard. Video RAM is typically\nlocated on the video card so that the GPU can access it as quickly as possi-\nble. A bus protocol such as PCI, AGP or PCI Express (PCIe) is used to trans-\nfer data back and forth between “main RAM” and VRAM, via the expansion\nslot’s bus. This physical separation between main RAM and VRAM can be\na significant performance bottleneck, and is one of the primary contributors\nto the complexity of rendering engines and graphics APIs like OpenGL and\nDirectX 11.\n3.5.1.3 Case Study: The Apple II Memory Map\nToillustratetheconceptsofmemorymapping,let’slookatasimplereal-world\nexample. The Apple II had a 16-bit address bus, meaning that its address\nspacewasonly64KiBinsize. ThisaddressspacewasmappedtoROM,RAM,\nmemory-mapped I/O devices and video RAM regions as follows:\n0xC100 - 0xFFFF ROM (Firmware)\n0xC000 - 0xC0FF Memory-Mapped I/O\n0x6000 - 0xBFFF General-purpose RAM\n0x4000 - 0x5FFF High-res video RAM (page 2)\n0x2000 - 0x3FFF High-res video RAM (page 1)\n0x0C00 - 0x1FFF General-purpose RAM\n0x0800 - 0x0BFF Text/lo-res video RAM (page 2)\n0x0400 - 0x07FF Text/lo-res video RAM (page 1)\n184 3. Fundamentals of Software Engineering for Games\n0x0200 - 0x03FF General-purpose and reserved RAM\n0x0100 - 0x01FF Program stack\n0x0000 - 0x00FF Zero page (mostly reserved for DOS)\nWe should note that the addresses in the Apple II memory map corresponded\ndirectly to memory chips on the motherboard. In today’s operating systems,\nprograms work in terms of virtualaddresses rather than physical addresses.\nWe’ll explore virtual memory in the next section.\n3.5.2 Virtual Memory\nMostmodernCPUsandoperatingsystemssupportamemoryremappingfea-\ntureknownasa virtualmemorysystem. Inthesesystems,thememoryaddresses\nused by a program don’t map directly to the memory modules installed in the\ncomputer. Instead, whenever a program reads from or writes to an address,\nthat address is first remapped by the CPU via a look-up table that’s maintained\nby the OS. The remapped address might end up referring to an actual cell in\nmemory (with a totally different numerical address). It might also end up re-\nferring to a block of data on-disk. Or it might turn out not to be mapped to\nany physical storage at all. In a virtual memory system, the addresses used\nby programs are called virtual addresses , and the bit patterns that are actually\ntransmitted over the address bus by the memory controller in order to access\na RAM or ROM module are called physicaladdresses.\nVirtual memory is a powerful concept. It allows programs to make use\nof more memory than is actually installed in the computer, because data can\noverflow from physical RAM onto disk. Virtual memory also improves the\nstability and security of the operating system, because each program has its\nown private “view” of memory and is prevented from stomping on memory\nblocks that are owned by other programs or the operating system itself. We’ll\ntalkmoreabouthowtheoperatingsystemmanagesthevirtualmemoryspaces\nof running programs in Section 4.4.5.\n3.5.2.1 Virtual Memory Pages\nTo understand how this remapping takes place, we need to imagine that the\nentire addressable memory space (that’s 2nbyte-sized cells if the address bus\nisnbits wide) is conceptually divided into equally-sized contiguous chunks\nknown as pages. Page sizes differ from OS to OS, but are always a power of\ntwo—atypicalpagesizeis4KiBor8KiB.Assuminga4KiBpagesize,a32-bit\naddress space would be divided up into 1,048,576 distinct pages, numbered\nfrom 0x0 to 0xFFFFF, as shown in Table 3.2.\n3.5. Memory Architectures 185\nFrom Address To Address Page Index\n0x00000000 0x00000FFF Page 0x0\n0x00001000 0x00001FFF Page 0x1\n0x00002000 0x00002FFF Page 0x2\n...\n0x7FFF2000 0x7FFF2FFF Page 0x7FFF2\n0x7FFF3000 0x7FFF3FFF Page 0x7FFF3\n...\n0xFFFFE000 0xFFFFEFFF Page 0xFFFFE\n0xFFFFF000 0xFFFFFFFF Page 0xFFFFF\nTable 3.2. Division of a 32-bit address space into 4 KiB pages.\nThe mapping between virtual addresses and physical addresses is always\ndone at the granularity of pages. One hypothetical mapping between virtual\nand physical addresses is illustrated in Figure 3.22.\n3.5.2.2 Virtual to Physical Address Translation\nWhenever the CPU detects a memory read or write operation, the address is\nsplit into two parts: the pageindex and anoffsetwithin that page (measured in\nbytes). Forapagesizeof4KiB,theoffsetisjustthelower12bitsoftheaddress,\nand the page index is the upper 20 bits, masked off and shifted to the right by\n12 bits. For example, the virtual address 0x1A7C6310 corresponds to an offset\nof 0x310 and a page index of 0x1A7C6.\nThe page index is then looked up by the CPU’s memory management unit\n(MMU) in a page table that maps virtual page indices to physical ones. (The\npage table is stored in RAM and is managed by the operating system.) If the\npageinquestionhappenstobemappedtoaphysicalmemorypage,thevirtual\npage index is translated into the corresponding physical page index, the bits\nof this physical page index are shifted to the left as appropriate and ORed\ntogetherwiththebitsoftheoriginalpageoffset,andvoila! Wehaveaphysical\naddress. Sotocontinueourexample, ifvirtualpage0x1A7C6happenstomap\nto physical page 0x73BB9, then the translated physical address would end up\nbeing0x73BB9310. Thisistheaddressthatwouldactuallybetransmittedover\nthe address bus. Figure 3.23 illustrates the operation of the MMU.\nIf the page table indicates that a page is not mapped to physical RAM\n(either because it was never allocated, or because that page has since been\n186 3. Fundamentals of Software Engineering for Games\nFigure 3.22. Page-sized address ranges in a virtual memory space are remapped either to physical\nmemory pages, a swap ﬁle on disk, or they may remain unmapped.\nswapped out to a disk file), the MMU raises an interrupt, which tells the oper-\nating system that the memory request can’t be fulfilled. This is called a page\nfault. (See Section 4.4.2 for more on interrupts.)\n3.5.2.3 Handling Page Faults\nForaccessestounallocatedpages,theOSnormallyrespondstoapagefaultby\ncrashing the program and generating a core dump. For accesses to pages that\nhave been swapped out to disk, the OS temporarily suspends the currently-\nrunning program, reads the page from the swap file into a physical page of\nRAM, and then translates the virtual address to a physical address as usual.\nFinally it returns control back to the suspended program. From the point of\nview of the program, this operation is entirely seamless—it never “knows”\nwhether a page was already in memory or had to be swapped in from disk.\nNormally pages are swapped out to disk only when the load on the mem-\nory system is high, and physical pages are in short supply. The OS tries to\nswap out only the least-frequently-used pages of memory to avoid programs\ncontinually “thrashing” between memory-based and disk-based pages.\n3.5. Memory Architectures 187\nFigure 3.23. The MMU intercepts a memory read operation, and breaks the virtual address into\na virtual page index and an offset. The virtual page index is converted to a physical page index\nvia the page table, and the physical address is constructed from the physical page index and the\noriginal offset. Finally, the instruction is executed using the remapped physical address.\n3.5.2.4 The Translation Lookaside Buffer (TLB)\nBecause page sizes tend to be small relative to the total size of addressable\nmemory(typically4KiBor8KiB),thepagetablecanbecomeverylarge. Look-\ning up physical addresses in the page table would be time-consuming if the\nentire table had to be scanned for every memory access a program makes.\nTo speed up access, a caching mechanism is used, based on the assump-\ntion that an average program will tend to reuse addresses within a relatively\nsmall number of pages, rather than read and write randomly across the entire\naddress range. A small table known as the translation lookaside buffer (TLB) is\nmaintained within the MMU on the CPU die, in which the virtual-to-physical\naddress mappings of the most recently-used addresses are cached. Because\nthis buffer is located in close proximity to the MMU, accessing it is very fast.\nTheTLBactsalotlikeageneral-purpose memorycachehierarchy ,exceptthat\nit is only used for caching page table entries. See Section 3.5.4 for a discussion\nof how cache hierarchies work.\n188 3. Fundamentals of Software Engineering for Games\n3.5.2.5 Further Reading on Virtual Memory\nSeehttps://www.cs.umd.edu/class/sum2003/cmsc311/Notes/Memory/virtual.\nhtmlandhttps://gabrieletolomei.wordpress.com/miscellanea/operating-systems/\nvirtual-memory-paging-and-swapping for two good discussions of virtual\nmemory implementation details.\nThis paper by Ulrich Drepper entitled, “What Every Programmer Should\nKnowAboutMemory”isalsoamust-readforallprogrammers: https://www.\nakkadia.org/drepper/cpumemory.pdf.\n3.5.3 Memory Architectures for Latency Reduction\nThespeedwithwhichdatacanbeaccessedfromamemorydeviceisanimpor-\ntantcharacteristic. Weoftenspeakof memoryaccesslatency,whichisdefinedas\nthe length of time between the moment the CPU requests data from the mem-\nory system and the moment that that data is actually received by the CPU.\nMemory access latency is primarily dependent on three factors:\n1. the technology used to implement the individual memory cells,\n2. the number of read and/or write ports supported by the memory, and\n3. thephysicaldistancebetweenthosememorycellsandtheCPUcorethat\nuses them.\nThe access latency of staticRAM (SRAM) is generally much lower than that of\ndynamic RAM (DRAM). SRAM achieves its lower latency by utilizing a more\ncomplex design which requires more transistors per bit of memory than does\nDRAM. This in turn makes SRAM more expensive than DRAM, both in terms\nof financial cost per bit, and in terms of the amount of real estate per bit that it\nconsumes on the die.\nThe simplest memory cell has a single port, meaing only one read or write\noperation can be performed by it at any given time. Multi-ported RAM al-\nlows multiple read and/or write operations to be performed simultaneously,\nthereby reducing the latency caused by contention when multiple cores, or\nmultiplecomponentswithinasinglecore,attempttoaccessabankofmemory\nsimultaneously. As you’d expect, a multi-ported RAM requires more transis-\ntorsperbitthanasingle-porteddesign,andhenceitcostsmoreandusesmore\nreal estate on the die than a single-ported memory.\nThe physical distance between the CPU and a bank of RAM also plays a\nrole in determining the access latency of that memory. This is because elec-\ntronicsignalstravelatafinitespeedwithinthecomputer. Intheory, electronic\n3.5. Memory Architectures 189\nsignals are comprised of electromagnetic waves, and hence travel at close7to\nthe speed of light. Additional latencies are introduced by the various switch-\ning and logic circuits that a memory access signal encounters on its journey\nthrough the system. As such, the closer a memory cell is to the CPU core that\nuses it, the lower its access latency tends to be.\n3.5.3.1 The Memory Gap\nIn the early days of computing, memory access latency and instruction execu-\ntion latency were on roughly equal footing. For example, on the Intel 8086,\nregister-based instructions could execute in two to four cycles, and a main\nmemory access also took roughly four cycles. However, over the past few\ndecades both raw clock speeds and the effective instruction throughput of\nCPUs have been improving at a much faster rate than memory access speeds.\nToday, the access latency of main memory is extremely high relative to the\nlatency of executing a single instruction: Whereas a register-based instruc-\ntion still takes between one and 10 cycles to complete on an Intel Core i7, an\naccess to main RAM can take on the order of 500 cycles to complete! This\never-increasing discrepancy between CPU speeds and memory access laten-\ncies is often called the memorygap. Figure 3.24 illustrates the trend of an ever-\nincreasing memory gap over time.\nProgrammers and hardware designers have together developed a wide\nrange of techniques to work around the problems caused by high memory\naccess latency. These techniques usually focus on one or more of the follow-\ning:\n1.reducing average memory latency by placing smaller, faster memory\nbanks closer to the CPU core, so that frequently-used data can be ac-\ncessed more quickly;\n2. “hiding” memory access latency by arranging for the CPU to do other\nuseful work while waiting for a memory operation to complete; and/or\n3.minimizing accesses to main memory by arranging a program’s data in\nas efficient a manner as possible with respect to the work that needs to\nbe done with that data.\nInthissection,we’llhaveacloserlookatmemoryarchitecturesforaveragela-\ntency reduction. We’ll discuss the other two techniques (latency “hiding” and\n7Thespeedthatanelectronicsignaltravelswithinatransmissionmediumsuchascopperwire\noropticalfiberisalwayssomewhatlowerthanthespeedoflightinavaccuum. Eachinterconnect\nmaterialhasitsowncharacteristic velocityfactor (VF)rangingfromlessthan50%to99%thespeed\nof light in a vaccuum.\n190 3. Fundamentals of Software Engineering for Games\nthe minimization of memory accesses via judicious data layout) while inves-\ntigating parallel hardware design and concurrent programming techniques in\nChapter 4.\n3.5.3.2 Register Files\nA CPU’s register file is perhaps the most extreme example of a memory ar-\nchitecture designed to minimize access latency. Registers are typically imple-\nmented using multi-ported static RAM (SRAM), usually with dedicated ports\nfor read and write operations, allowing these operations to occur in parallel\nrather than serially. What’s more, the register file is typically located imme-\ndiately adjacent to the circuitry for the ALU that uses it. Furthermore, regis-\nters are accessed pretty much directly by the ALU, whereas accesses to main\nRAMtypicallyhavetopassthroughthevirtualaddresstranslationsystem,the\nmemory cache hierarchy and cache coherence protocol (see Section 3.5.4), the\naddress and data buses, and possibly also through crossbar switching logic.\nThese facts explain why registers can be accessed so quickly, and also why the\ncost of register RAM is relatively high as compared to general-purpose RAM.\nThis higher cost is justified because registers are by far the most frequently-\nused memory in any computer, and because the total size of the register file is\nvery small when compared to the size of main RAM.\nFigure 3.24. The ever-increasing difference between CPU performance and the performance of\nmemory is called the memory gap. (Adapted from [23] Computer Architecture: A Quantitative Ap-\nproach by John L. Hennessey and David A. Patterson.)\n3.5. Memory Architectures 191\n3.5.4 Memory Cache Hierarchies\nAmemory cache hierarchy is one of the primary mechanisms for mitigating the\nimpacts of high memory access latencies in today’s personal computers and\ngame consoles. In a cache hierarchy, a small but fast bank of RAM called the\nlevel 1(L1) cache is placed very near to the CPU core (on the same die). Its\naccess latency is almost as low as the CPU’s register file, because it is so close\nto the CPU core. Some systems provide a larger but somewhat slower level 2\n(L2)cache,locatedfurtherawayfromthecore(usuallyalsoon-chip,andoften\nshared between two or more cores on a multicore CPU). Some machines even\nhave larger but more-distant L3 or L4 caches. These caches work in concert to\nautomatically retain copies of the most frequently-used data, so that accesses\ntotheverylargebutveryslowbankofmainRAMlocatedonthesystemmoth-\nerboard are kept to a minimum.\nA caching system improves memory access performance by keeping local\ncopiesin the cache of those chunks of data that are most frequently accessed\nby the program. If the data requested by the CPU is already in the cache, it\ncan be provided to the CPU very quickly—on the order of tens of cycles. This\nis called a cache hit. If the data is not already present in the cache, it must be\nfetched into the cache from main RAM. This is called a cache miss. Reading\ndata from main RAM can take hundreds of cycles, so the cost of a cache miss\nis very high indeed.\n3.5.4.1 Cache Lines\nMemory caching takes advantage of the fact that memory access patterns in\nreal software tend to exhibit two kinds of locality of reference:\n1.Spatiallocality. If memory address Nis accessed by a program, it’s likely\nthat nearby addresses such as N+1,N+2and so on will also be ac-\ncessed. Iterating sequentially through the data stored in an array is an\nexampleofamemoryaccesspatternwithahighdegreeofspatiallocality.\n2.Temporal locality. If memory address Nis accessed by a program, it’s\nlikely that that same address will be accessed again in the near future.\nReading data from a variable or data structure, performing a transfor-\nmation on it, and then writing an updated result into the same variable\nor data structure is an example of a memory access pattern with a high\ndegree of temporal locality.\nTotakeadvantageoflocalityofreference,memorycachingsystemsmovedata\ninto the cache in contiguous blocks called cache lines rather than caching data\nitems individually.\n192 3. Fundamentals of Software Engineering for Games\nFor example, let’s say the program is accessing the data members of an\ninstance of a class or struct. When the first member is read, it might take\nhundreds of cycles for the memory controller to reach out into main RAM\nto retrieve the data. However, the cache controller doesn’t just read that one\nmember—it actually reads a larger contiguous block of RAM into the cache.\nThat way, subsequent reads of the other data members of the instance will\nlikely result in low-cost cache hits.\n3.5.4.2 Mapping Cache Lines to Main RAM Addresses\nThere is a simple one-to-many correspondence between memory addresses in\nthe cache and memory addresses in main RAM. We can think of the address\nspace of the cache as being “mapped” onto the main RAM address space in\na repeating pattern, starting at address 0 in main RAM and continuing on up\nuntil all main RAM addresses have been “covered” by the cache.\nAs a concrete example, let’s say that our cache is 32 KiB in size, and that\ncache lines are 128 bytes each. The cache can therefore hold 256 cache lines\n(256128 =32,768B=32KiB). Let’s further assume that main RAM is\n256 MiB in size. So main RAM is 8192 times as big as the cache, because\n(2561024 )/32 =8192. That means we need to overlay the address space\nof the cache onto the main RAM address space 8192 times in order to cover all\npossible physical memory locations. Or put another way, a single line in the\ncache maps to 8192 distinct line-sized chunks of main RAM.\nGiven any address in main RAM, we can find its address in the cache by\ntaking the main RAM address modulo the size of the cache. So for a 32 KiB\ncache and 256 MiB of main RAM, the cache addresses 0x0000 through 0x7FFF\n(that’s 32 KiB) map to main RAM addresses 0x0000 through 0x7FFF. But this\nrange of cache addresses alsomaps to main RAM addresses 0x8000 through\n0xFFFF, 0x10000 through 0x17FFF, 0x18000 through 0x1FFFF and so on, all\nthe way up to the last main RAM block at addresses 0xFFF8000 through\n0xFFFFFFF.Figure3.25illustratesthemappingbetweenmainRAMandcache\nRAM.\n3.5.4.3 Addressing the Cache\nLet’s consider what happens when the CPU reads a single byte from memory.\nTheaddressofthedesiredbyteinmainRAMisfirstconvertedintoanaddress\nwithin the cache. The cache controller then checks whether or not the cache\nline containing that byte already exists in the cache. If it does, it’s a cache hit,\nand the byte is read from the cache rather than from main RAM. If it doesn’t,\nit’s a cache miss, and the line-sized chunk of data is read from main RAM and\n3.5. Memory Architectures 193\nLines 0 -255\nLines 0 -255\nLines 0 -255\nLines 0 - 2550x18000\n0x100000x08000\n0x00000Lines 0 -255\nLines 0 -255\n0x1FFFF\n0x17FFF\n0x0FFFF\n0x07FFF0xFFF80000xFFF00000xFFFFFFF\n0xFFF7FFF\n0x000000x0007F\nLine 0Line 1Line 2Line 255\nLine 254\n0x000800x000FF0x001000x0017F......Main RAM Cache\n0x07F000x07F7F0x07F800x07FFF\nFigure 3.25. Direct mapping between main memory addresses and cache lines.\nloaded into the cache so that subsequent reads of nearby addresses will be\nfast.\nThecachecanonlydealwithmemoryaddressesthatare alignedtoamulti-\nple of the cache line size (see Section 3.3.7.1 for a discussion of memory align-\nment). Put another way, the cache can really only be addressed in units of\nlines, not bytes. Hence we need to convert our byte’s address into a cache line\nindex.\nConsideracachethatis 2Mbytesintotalsize,containinglinesthatare 2nin\nsize. The nleast-significant bits of the main RAM address represent the offset\nof the byte within the cache line. We strip off these nleast-significant bits to\nconvert from units of bytes to units of lines (i.e., we divide the address by the\ncache line size, which is 2n). Finally we split the resulting address into two\npieces: The (M n)least-significant bits become the cache line index, and all\ntheremainingbitstellusfrom whichcache-sizedblockinmainRAMthecache\nline came from. The block index is known as the tag.\nIn the case of a cache miss, the cache controller loads a line-sized chunk\nof data from main RAM into the corresponding line within the cache. The\ncache also maintains a small table of tags, one for each cache line. This al-\nlows the caching system to keep track of from whichmain RAM block each\nline in the cache came. This is necessary because of the many-to-one relation-\nship between memory addresses in the cache and memory addresses in main\nRAM. Figure 3.26 illustrates how the cache associates a tag with each active\nline within it.\nReturning to our example of reading a byte from main RAM, the complete\n194 3. Fundamentals of Software Engineering for Games\nFigure 3.26. A tag is associated with each line in the cache, indicating from which cache-sized block\nof main RAM the corresponding line came.\nsequence of events is as follows: The CPU issues a read operation. The main\nRAM address is converted into an offset, line index and tag. The correspond-\ning tag in the cache is checked, using the line index to find it. If the tag in the\ncache matches the requested tag, it’s a cache hit. In this case, the line index is\nused to retrieve the line-sized chunk of data from the cache, and the offset is\nused to locate the desired byte within the line. If the tags do not match, it’s\na cache miss. In this case, the appropriate line-sized chunk of main RAM is\nread into the cache, and the corresponding tag is stored in the cache’s tag ta-\nble. Subsequent reads of nearby addresses (those that reside within the same\ncache line) will therefore result in much faster cache hits.\n3.5.4.4 Set Associativity and Replacement Policy\nThe simplemapping between cache lines andmain RAM addressesdescribed\nabove is known as a direct-mapped cache. It means that each address in main\nRAM maps to only oneline in the cache. Using our 32 KiB cache with 128-\nbyte lines as an example, the main RAM address 0x203 maps to cache line 4\n(because 0x203 is 515, and ⌊515/128⌋=4). However, in our example there\nare 8192 unique cache-line-sized blocks of main RAM that all map to cache\nline 4. Specifically, cache line 4 corresponds to main RAM addresses 0x200\nthrough 0x27F, but also to addresses 0x8200 through 0x827F, and 0x10200\nthrough 0x1027f, and 8189 othercache-line-sized address ranges as well.\nWhenacachemissoccurs,theCPUmustloadthecorrespondingcacheline\nfrom main memory into the cache. If the line in the cache contains no valid\n3.5. Memory Architectures 195\nLine 0, Way 0Line 0, Way 1Line 1, Way 0...\nLine 1, Way 1\n0x000000x0007F0x000800x000FF\n0x000800x000FF\n0x000000x0007F\nLine 0Line 1Line 2...\n0x000000x0007F0x001000x0017F\n0x000800x00 0FFMain RAMCache\n(2-Way Set Associat ive)\nFigure 3.27. A 2-way set associative cache.\ndata, we simply copy the data into it and we’re done. But if the line already\ncontains data (from a different main memory block), we must overwrite it.\nThis is known as evicting the cache line.\nTheproblemwithadirect-mappedcacheisthatitcanresultinpathological\ncases. For example, two unrelated main memory blocks might keep evicting\noneanotherinaping-pongfashion. Wecanobtainbetteraverageperformance\nif each main memory address can map to two or more distinct lines in the\ncache. In a 2-way set associative cache, each main RAM address maps to two\ncachelines. ThisisillustratedinFigure3.27. Obviouslya4-waysetassociative\ncache performs even better than a 2-way, and an 8-way or 16-way cache can\noutperform a 4-way cache and so on.\nOnce we have more than one “cache way,” the cache controller is faced\nwith a dilemma: When a cache miss occurs, which of the “ways” should\nwe evict and which ones should we allow to stay resident in the cache?\nThe answer to this question differs between CPU designs, and is known as\nthe CPU’s replacement policy. One popular policy is not most-recently used\n(NMRU). In this scheme, the most-recently used “way” is kept track of, and\neviction always affects the “way” or “ways” that are notthe most-recently\nused one. Other policies include first in first out (FIFO), which is the only\noption in a direct-mapped cache, least-recently used (LRU), least-frequently\nused (LFU), and pseudorandom. For more on cache replacement policies, see\nhttps://ece752.ece.wisc.edu/lect11-cache-replacement.pdf.\n3.5.4.5 Multilevel Caches\nThehit rateis a measure of how often a program hits the cache, as opposed to\nincurring the large cost of a cache miss. The higher the hit rate, the better the\nprogram will perform (all other things being equal). There is a fundamental\n196 3. Fundamentals of Software Engineering for Games\ntrade-off between cache latency and hit rate. The larger the cache, the higher\nthe hit rate—but larger caches cannot be located as close to the CPU, so they\ntend to be slower than smaller ones.\nMost game consoles employ at least two levelsof cache. The CPU first tries\nto find the data it’s looking for in the level 1(L1) cache. This cache is small\nbut has a very low access latency. If the data isn’t there, it tries the larger but\nhigher-latency level 2(L2) cache. Only if the data cannot be found in the L2\ncache do we incur the full cost of a main memory access. Because the latency\nof main RAM can be so high relative to the CPU’s clock rate, some PCs even\nincludelevel3(L3) and level4(L4) caches.\n3.5.4.6 Instruction Cache and Data Cache\nWhen writing high-performance code for a game engine or for any other\nperformance-critical system, it is important to realize that both data and code\nare cached. The instruction cache (I-cache, often denoted I$) is used to preload\nexecutablemachinecodebeforeitruns,whilethe datacache (D-cache,orD$)is\nused to speed up read and write operations performed by that machine code.\nInalevel1(L1)cache,thetwocachesarealwaysphysicallydistinct,becauseit\nis undesirable for an instruction read to cause valid data to be bumped out of\nthe cache or vice versa. So when optimizing our code, we must consider both\nD-cacheandI-cacheperformance(althoughaswe’llsee,optimizingonetends\nto have a positive effect on the other). Higher-level caches (L2, L3, L4) typi-\ncally do not make this distinction between code and data, because their larger\nsizetendstomitigatetheproblemsofcodeevictingdataordataevictingcode.\n3.5.4.7 Write Policy\nWehaven’ttalkedyetaboutwhathappenswhentheCPU writesdatatoRAM.\nHow the cache controller handles writes is known as its write policy . The sim-\nplestkindofcacheiscalleda write-throughcache;inthisrelativelysimplecache\ndesign, all writes to the cache are mirrored to main RAM immediately. In a\nwrite-back (orcopy-back ) cache design, data is first written into the cache and\nthe cache line is only flushed out to main RAM under certain circumstances,\nsuch as when a dirty cache line needs to be evicted in order to read in a new\ncache line from main RAM, or when the program explicitly requests a flush to\noccur.\n3.5.4.8 Cache Coherency: MESI, MOESI and MESIF\nWhen multiple CPU cores share a single main memory store, things get more\ncomplicated. It’s typical for each core to have its own L1 cache, but multiple\ncores might share an L2 cache, as well as sharing main RAM. See Figure 3.28\n3.5. Memory Architectures 197\nFigure 3.28. Level 1 and level 2 caches.\nforanillustrationofatwo-levelcachearchitecturewithtwoCPUcoressharing\none main memory store and an L2 cache.\nIn the presence of multiple cores, it’s important for the system to maintain\ncache coherency . This amounts to making sure that the data in the caches be-\nlonging to multiple cores match one another and the contents of main RAM.\nCoherency doesn’t have to be maintained at every moment—all that matters\nis that the running program can never tellthat the contents of the caches are\nout of sync.\nThe three most common cache coherency protocols are known as MESI\n(modified, exclusive, shared, invalid), MOESI (modified, owned, exclusive,\nshared, invalid) and MESIF (modified, exclusive, shared, invalid and for-\nward). We’ll discuss the MESI protocol in more depth when we talk about\nmulticore computing architectures in Section 4.9.4.2.\n3.5.4.9 Avoiding Cache Misses\nObviously cache misses cannot be totally avoided, since data has to move to\nand from main RAM eventually. The trick to writing high performance soft-\nware in the presence of a memory cache hierarchy is to arrange your data in\nRAM, and design your data manipulation algorithms, in such a way that the\nminimum number of cache misses occur.\nThebestwaytoavoidD-cachemissesistoorganizeyourdatain contiguous\nblocksthatareas smallaspossibleandthenaccessthem sequentially. Whenthe\ndata is contiguous (i.e., you don’t “jump around” in memory a lot), a single\ncache miss will load the maximum amount of relevant data in one go. When\n198 3. Fundamentals of Software Engineering for Games\nthedataissmall,itismorelikelytofitintoasinglecacheline(oratleastamin-\nimum number of cache lines). And when you access your data sequentially,\nyou avoid evicting and reloading cache lines multiple times.\nAvoiding I-cache misses follows the same basic principle as avoiding D-\ncache misses. However, the implementation requires a different approach.\nThe easiest thing to do is to keep your high-performance loops as small as\npossible in terms of code size, and avoid calling functions within your inner-\nmost loops. If you do opt to call functions, try to keep their code size small\ntoo. This helps to ensure that the entire body of the loop, including all called\nfunctions, will remain in the I-cache the entire time the loop is running.\nUse inline functions judiciously. Inlining a small function can be a big\nperformance boost. However, too much inlining bloats the size of the code,\nwhich can cause a performance-critical section of code to no longer fit within\nthe cache.\n3.5.5 Nonuniform Memory Access (NUMA)\nWhendesigningamultiprocessorgameconsoleorpersonalcomputer,system\narchitects must choose between two fundamentally different memory archi-\ntectures: uniformmemoryaccess (UMA)and nonuniformmemoryaccess (NUMA).\nIn a UMA design, the computer contains a single large bank of main RAM\nwhich is visible to all CPU cores in the system. The physical address space\nlooks the same to each core, and each can read from and write to all memory\nlocations in main RAM. A UMA architecture typically makes use of a cache\nhierarchy to mitigate memory access latency issues.\nOne problem with a UMA architecture is that the cores often contend for\naccess to main RAM and any shared caches. For example, the PS4 contains\neight cores arranged into two clusters. Each core has its own private L1 cache,\nbuteachclusteroffourcoressharesasingleL2cache,andallcoressharemain\nRAM. As such, the cores often contend with one another for access to the L2\ncache and main RAM.\nOne way to address core contention problems is to employ a non-uniform\nmemoryaccess (NUMA)design. InaNUMAsystem,eachcoreisprovidedwith\na relatively small bank of high-speed dedicated RAM called a local store . Like\nan L1 cache, a local store is typically located on the same die as the core itself,\nand is only accessible by that core. But unlike an L1 cache, access to the lo-\ncal store is explicit. A local store might be mapped to part of a core’s address\nspace, with main RAM mapped to a different range of addresses. Alterna-\ntively, certain cores might onlybe able to see the physical addresses within\nits local store, and might rely on a direct memory access controller (DMAC) to\n3.5. Memory Architectures 199\nSystem RAM\n256 MiB XDRDMA Ring BusL1 D$\n32 KiB\n4-wayL1 I$\n32 KiB\n2-wayLocal\nStore\n256 KiBLocalStore\n256 KiBLocalStore\n256 KiBSPU 0\n@ 3.2 GHzSPU 1\n@ 3.2 GHzSPU 6\n@ 3.2 GHz\nDMA\nControllerL2 Cache\n512 KiBPPU\n@ 3.2 GHz\nVideo RAM\n256 MiB GDDR3Nvidia RSX \nGPU\n@550 MHz\nFigure 3.29. Simpliﬁed view of the PS3’s cell broadband architecture.\ntransfer data between the local store and main RAM.\n3.5.5.1 SPU Local Stores on the PS3\nThe PlayStation 3 is a classic example of a NUMA architecture. The PS3 con-\ntains a single main CPU known as the Power processing unit (PPU), eight8co-\nprocessors known as synergistic processing units (SPUs), and an NVIDIA RSX\ngraphics processing unit (GPU). The PPU has exclusive access to 256 MiB of\nmain system RAM (with an L1 and L2 cache), the GPU has exclusive access\nto 256 MiB of video RAM (VRAM), and each SPU has its own private 256 KiB\nlocal store.\nThephysicaladdressspacesofmainRAM,videoRAMandtheSPUs’local\nstores are all totally isolated from one another. This means that, for example,\nthePPUcannotdirectlyaddressmemoryinVRAMorinanyoftheSPUs’local\nstores, and any given SPU cannot directly address main RAM, video RAM,\nor any of the other SPUs’ local stores, only its own local store. The memory\narchitecture of the PS3 is illustrated in Figure 3.29.\n3.5.5.2 PS2 Scratchpad (SPR)\nLooking even farther back to the PlayStation 2, we can learn about another\nmemory architecture that was designed to improve overall system perfor-\n8Only six of the SPUs are available for use by game applications—one SPU is reserved for\nuse by the operating system, and one is entirely off-limits to account for inevitable flaws in the\nfabrication process.\n200 3. Fundamentals of Software Engineering for Games\nFigure 3.30. Simpliﬁed view of the PS2’s memory architecture, illustrating the 16 KiB scratchpad\n(SPR), the L1 caches of the main CPU (EE) and the two vector units (VU0 and VU1), the 4 MiB bank\nof video RAM accessible to the graphics synthesizer (GS), the 32 MiB bank of main RAM, the DMA\ncontroller and the system buses.\nmance. ThemainCPUonthePS2,calledtheemotionengine(EE),hasaspecial\n16 KiB area of memory called the scratchpad (abbreviated SPR), in addition to\na 16 KiB L1 instruction cache (I-cache) and an 8 KiB L1 data cache (D-cache).\nThe PS2 also contains two vector coprocessors known as VU0 and VU1, each\nwiththeirownL1I-andD-caches,andaGPUknownasthe graphicssynthesizer\n(GS)connectedtoa4MiBbankofvideoRAM.ThePS2’smemoryarchitecture\nis illustrated in Figure 3.30.\nThe scratchpad is located on the CPU die, and therefore enjoys the same\nlow latency as L1 cache memory. But unlike the L1 cache, the scratchpad is\nmemory-mappedsothatitappearstotheprogrammertobearangeofregular\nmain RAM addresses. The scratchpad on the PS2 is itself uncached, meaning\nthat reads and writes from and to it are direct; they bypass the EE’s L1 cache\nentirely.\nThe scratchpad’s main benefit isn’t actually its low access latency—it’s the\nfact that the CPU can access scratchpad memory without making use of the\nsystem buses. As such, reads and writes from and to the scratchpad can hap-\npen while the system’s address and data bus are being used for other pur-\nposes. For example, a game might set up a chain of DMA requests to transfer\ndata between main RAM and the two vector processing units (VUs) in the\nPS2. While these DMAs are being processed by the DMAC, and/or while the\nVUs are busy performing calculations (both of which would make heavy use\nof the system buses), the EE can be performing calculations on data that re-\nsides within the scratchpad, without interfering with the DMAs or the oper-\nation of the VUs. Moving data into and out of the scratchpad can be done\nviaregularmemorymoveinstructions(or memcpy() inC/C++), butthistask\n3.5. Memory Architectures 201\ncan also be accomplished via DMA requests. The PS2’s scratchpad therefore\ngives the programmer a lot of flexibility and power in order to maximize a\ngame engine’s data throughput.\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",43107
25-4.1 Defining Concurrency and Parallelism.pdf,25-4.1 Defining Concurrency and Parallelism,"4\nParallelism and\nConcurrent Programming\nComputing performance—typically measured in millions of instructions\npersecond(MIPS)orfloating-pointoperationspersecond(FLOPS)—has\nbeen improving at a staggeringly rapid and consistent rate over the past four\ndecades. In the late 1970s, the Intel 8087 floating-point coprocessor could\nmuster only about 50 kFLOPS (5 104FLOPS), while at roughly the same\ntime a Cray-1 supercomputer the size of a large refrigerator could opeate at a\nrate of roughly 160 MFLOPS (1.6 108FLOPS). Today, the CPU in game con-\nsoles like the Playstation 4 or the Xbox One produces roughly 100 GFLOPS\n(1011FLOPS) of pr ocessing power, and the fastest supercomputer, currently\nChina’s Sunway TaihuLight, has a LINPACK benchmark score of 93 PFLOPS\n(peta-FLOPS, or a staggering 9.31016floating-point operations per second).\nThisisanimprovementofsevenordersofmagnitudeforpersonalcomputers,\nand eight orders of magnitude for supercomputers.\nMany factors contributed to this rapid rate of improvement. In the early\ndays, the move from vacuum tubes to solid-state transistors permitted the\nminiaturizationofcomputinghardware. Thenumberoftransistorsthatcould\nbeetchedontoasinglechiprosedramaticallyasnewkindsoftransistors,new\ntypes of digital logic, new substrate materials and new manufacturing pro-\ncesses were developed. These advances also contributed to improvements in\n203\n204 4. Parallelism and Concurrent Programming\nFigure 4.1. Two ﬂows of control operating on independent data are not considered concurrent\nbecause they are not prone to data races.\npower consumption and dramatic increases in CPU clock speeds. And start-\ning in the 1990s, computer hardware manufacturers have increasingly turned\ntoparallelism as a means of improving computing performance.\nWriting software that runs correctly andefficiently on parallel computing\nhardware is significantly more difficult than writing software for the serial\ncomputers of yesteryear. It requires a deep understanding of how the hard-\nware actually works. What’s more, taking full advantage of the multicore\nCPUs found in modern computing platforms requires an approach to soft-\nware design called concurrent programming . In a concurrent software system,\nmultiple flows of control cooperate to solve a common problem. These flows\nof control must be carefully coordinated. Many of the techniques that work\nwell in serial programs break down when applied to concurrent programs.\nAs such, it’s important for modern programmers (in all industries, including\ngames) to have a solid understanding of parallel computing hardware, and to\nbe well versed in concurrent programming techniques.\n4.1 Deﬁning Concurrency and Parallelism\n4.1.1 Concurrency\nA concurrent piece of software utilizes multipleflowsofcontrol to solve a prob-\nlem. Theseflowsofcontrolmightbeimplementedasmultiple threadsrunning\nwithin the context of a single process, or multiple cooperating processes run-\nning either on a single computer or multiple computers. Multiple flows of\ncontrolcan alsobe implementedwithina processusing othertechniques such\nasfibersorcoroutines .\nThe primary factor that distinguishes concurrentprogramming fromsequen-\ntial programming is the reading and/or writing of shared data. As shown in\n4.1. Deﬁning Concurrency and Parallelism 205\nFigure 4.2. Two ﬂows of control both reading from a shared data ﬁle and/or writing to a shared\ndata ﬁle are examples of concurrency .\nFigure 4.1, if we have two or more flows of control, each operating on a to-\ntally independent block of data, then this is not technically an example of\nconcurrency—it’s just “computing at the same time.”\nThecentralproblemofconcurrentprogrammingishowtocoordinatemul-\ntiple readers and/or multiple writers of a shared data file, in such a way as to\nensure predictable, correct results. At the heart of this problem is a special\nkind ofracecondition known as a datarace, in which two or more flows of con-\ntrol“compete”tobethefirsttoread,modifyandwriteachunkofshareddata.\nThe crux of the concurrency problem is to identify and eliminate data races.\nTwo examples of concurrency are illustrated in Figure 4.2.\nWe’ll explore the techiques programmers use to avoid data races and\nthereby write reliable concurrent programs starting in Section 4.5. But before\nwe do that, let’s take a look at how parallel computer hardware can both pro-\nvide an effective platform for running concurrent software, and improve the\nexecution speed of sequential programs as well.\n4.1.2 Parallelism\nIn computer engineering, the term parallelism refers to any situation in which\ntwo or more distinct hardware components are operating simultaneously. In\nother words, parallelcomputer hardware can perform more than one task at a\ntime. In contrast, serialcomputer hardware is capable of doing only one thing\nat a time.\nPrior to 1989, consumer-grade computing devices were exclusively serial\nmachines. Examples include the MOS Technology 6502 CPU, which was used\nin the Apple II and Commodore 64 personal computers, and the Intel 8086,\n80286 and 80386 CPUs which were at the heart of early IBM PCs and clones.\n206 4. Parallelism and Concurrent Programming\nToday, parallel computing hardware is ubiquitous. One obvious exam-\nple of hardware parallelism is a multicore CPU, such as the Intel Core™ i7\nor the AMD Ryzen™ 7. But parallelism can be employed at a wide range of\ngranularities. For example, a single CPU might contain multiple ALUs and\ntherefore be capable of performing multiple independent calculations in par-\nallel. And at the other end of the spectrum, a cluster of computers working\nin tandem to solve a common problem is also an example of hardware paral-\nlelism.\n4.1.2.1 Implicit versus Explicit Parallelism\nOne way to classify the various forms of parallelism in computer hardware\ndesign is to consider the purpose of parallelism in each. In other words, what\nproblem does parallelism solve in a given design? Thinking along these lines,\nwe can divide parallelism coarsely into two categories:\n• implicit parallelism, and\n• explicit parallelism.\nImplicitparallelism referstotheuseofparallelhardwarecomponentswithin\na CPU for the purpose of improving the performance of a singleinstruction\nstream. Thisisalsoknownas instructionlevelparallelism (ILP),becausetheCPU\nexecutesinstructionsfromasinglestream(asinglethread)buteachinstruction\nis executed with some degree of hardware parallelism. Examples of implicit\nparallelism include:\n• pipelining,\n• superscalar architectures, and\n• very long instruction word (VLIW) architectures.\nWe’llexploreimplicitparallelisminSection4.2. GPUsalsomakeextensiveuse\nofimplicitparallelism; we’lltakeacloserlookatthedesignandprogramming\nof GPUs in Section 4.11.\nExplicit parallelism refers to the use of duplicated hardware components\nwithin a CPU, computer or computer system, for the purpose of running more\nthan one instruction stream simultaneously. In other words, explicitly parallel\nhardware is designed to run concurrent software more efficiently than would\nbe possible on a serial computing platform. The most common examples of\nexplicit parallelism are:\n• hyperthreaded CPUs,\n• multicore CPUs,\n4.1. Deﬁning Concurrency and Parallelism 207\n• multiprocessor computers,\n• computer clusters,\n• grid computing, and\n• cloud computing.\nWe’ll investigate these explicitly parallel architectures further in Section 4.3.\n4.1.3 Task versus Data Parallelism\nAnotherwaytounderstandparallelismistodivideitintotwobroadcategories\nbased on the kind of work being done in parallel.\n•Taskparallelism. Whenmultipleheterogeneousoperationsareperformed\nin parallel, we call this task parallelism. Performing animation calcula-\ntions on one core while performing collision checks on another would\nbe an example of this form of parallelism.\n•Data parallelism. When a singleoperation is performed on multiple data\nitems in parallel, it is called data parallelism. Calculating 1000 skinning\nmatrices by running250 matrix calculations on each of four coreswould\nbe an example of data parallelism.\nMost real concurrent programs make use of both task and data parallelism to\nvarying degrees.\n4.1.4 Flynn’s Taxonomy\nYet another way to classify the varying degrees of parallelism we’ll encounter\nin computing hardware is to use Flynn’s Taxonomy . Proposed by Michael J.\nFlynn of Stanford University in 1966, this approach breaks down parallelism\nintoatwo-dimensionalspace. Alongoneaxis, wehavethenumberofparallel\nflows of control (which Flynn refers to as the number of instructions running\nin parallel at any given moment). On the other axis, we have the number of\ndistinct data streams being operated on by each instruction in the program.\nThe space is thus divided into four quadrants:\n•Singleinstruction,singledata(SISD):Asingleinstructionstreamoperating\non a single data stream.\n•Multiple instruction, multiple data (MIMD): Multiple instruction streams\noperating on multiple independent data streams.\n•Single instruction, multiple data (SIMD): A single instruction stream op-\nerating on multiple data streams (i.e., performing the same sequence of\noperations on multiple independent streams of data simultaneously).\n208 4. Parallelism and Concurrent Programming\nFigure 4.3. Example of SISD. A single ALU performs the multiply ﬁrst, followed by the divide.\n•Multiple instruction, single data (MISD): Multiple instruction streams all\noperating on a single data stream. (MISD is rarely used in games, but\none common application is to provide fault tolerance via redundancy.)\n4.1.4.1 Single versus Multiple Data\nIt’simportanttorealizeherethata“datastream”isn’tjustanarrayofnumbers.\nMost arithmetic operators are binary—they operate on two inputs to produce\na single output. When applied to binary arithmetic, the term “single data”\nreferstoasingle pairofinputs, withasingleoutput. Asanexample, let’shave\nalookathowtwobinaryarithmeticoperations,amultiply( ab)andadivide\n(c/d), might be accomplished under each of the four Flynn categories:\n• InaSISDarchitecture,asingleALUperformsthemultiplyfirst,followed\nby the divide. This is illustrated in Figure 4.3.\n• In a MIMD architecture, two ALUs perform operations in parallel, oper-\nating on two independent instruction streams. This is shown in Figure\n4.4.\n• The MIMD classification also applies to the case in which a single ALU\nprocessestwoindependentinstructionstreamsvia time-slicing,asshown\nin Figure 4.5.\n• In a SIMD architecture, a single “wide ALU” known as a vector pro-\ncessing unit (VPU) performs the multiply first, followed by the divide,\nbut each instruction operates on a pair of four-element input vectors\nand produces a four-element output vector. Figure 4.6 illustrates this\napproach.\n4.1. Deﬁning Concurrency and Parallelism 209\nFigure 4.4. Example of MIMD. Two ALUs perform operations in parallel.\nInstruction Stream\nmul i,jsub g,h\n...Instruction Stream\ndiv c,dmul a,b\n...\nALU\nFigure 4.5. Example of time-sliced MIMD. A single ALU performs operations on behalf of two inde-\npendent instruction streams, perhaps by alternating between them.\n• In a MISD architecture, two ALUs process the same instruction stream\n(multiplyfirst,followedbydivide)andideallyproduceidenticalresults.\nIllustrated in Figure 4.7, this architecture is primarily useful for imple-\nmenting fault tolerance via redundancy. ALU 1 acts as a “hot spare” for\nALU 0 and vice-versa, meaning that if one of the ALUs experiences a\nfailure, the system can seamlessly switch to the other.\n4.1.4.2 GPU Parallelism: SIMT\nIn recent years, a fifth classification has been added to Flynn’s taxonomy to\naccount for the design of graphics processing units (GPU). Single instruction\nmultiple thread (SIMT) is essentially a hybrid between SIMD and MIMD, used\nprimarily in the design of GPUs. It mixes SIMD processing (a single instruc-\n210 4. Parallelism and Concurrent Programming\nInstruction  Stream\nvdiv  c,dvmul  a,b\n...\nVPU\nFigure 4.6. Example of SIMD. A single vector processing unit (VPU) performs the multiply ﬁrst,\nfollowed by the divide, but each instruction operates on a pair of four-element input vectors and\nproduces a four-element output vector.\nCan Hot-Swap \nOn FailureALU0 ALU1Instruction Stream\ndiv c,dmul a,b\n...\nFigure 4.7. Example of MISD. Two ALUs process the same instruction stream (multiply ﬁrst, followed\nby divide) and ideally produce identical results.\ntion operating on multiple data streams simultaneously) with multithreading\n(more than one instruction stream sharing a processor via time-slicing).\nThe term “SIMT” was coined by NVIDIA, but it can be used to describe\nthe design of any GPU. The term manycore is also often used to refer to a SIMT\ndesign(i.e.,aGPUconsistingofarelativelylargenumberoflightweightSIMD\ncores) whereas the term multicore refers to MIMD designs (i.e., a CPU with a\nrelatively smaller number of heavyweight general purpose cores). We’ll take\na closer look at the SIMT design employed by GPUs in Section 4.11.",13131
26-4.2 Implicit Parallelism.pdf,26-4.2 Implicit Parallelism,"4.2. Implicit Parallelism 211\n4.1.5 Orthogonality of Concurrency and Parallelism\nWe should stress here that concurrent software doesn’t requireparallel hard-\nware, and parallel hardware isn’t onlyfor running concurrent software. For\nexample, a concurrent multithreaded program can run on a single, serial CPU\ncore via preemptive multitasking (see Section 4.4.4). Likewise, instruction\nlevel parallelism is intended to improve the performance of a single thread,\nand therefore benefits both concurrent and sequential software. So while they\nare closely related, concurrency and parallelism are really orthogonal con-\ncepts.\nAs long as our system involves multiplereaders and/ormultiplewriters of a\nshareddataobject,wehaveaconcurrentsystem. Concurrencycanbeachieved\nvia preemptive multitasking (on serial orparallel hardware) or via true paral-\nlelism(inwhicheachthreadexecutesonadistinctcore)—thetechniqueswe’ll\nlearn in this chapter will be applicable either way.\n4.1.6 Roadmap of the Chapter\nIn the following sections, we’ll first turn our attention to implicit parallelism,\nand how best to optimize our software to take advantage of it. Next, we’ll re-\nview the most common forms of explicit parallelism. Then we’ll explore the\nvarious concurrent programming techniques used to harness explicitly paral-\nlel computing platforms. Finally, we’ll round out the discussion of parallel\nprogramming by discussing SIMD vector processing, and how it applies to\nGPU design and general-purpose GPU programming (GPGPU) techniques.\n4.2 Implicit Parallelism\nIn Section 4.1.2.1 we said that implicit parallelism is the use of parallel com-\nputing hardware for the purpose of improving the execution speed of a sin-\ngle thread. CPU manufacturers began using implicit parallelism in their con-\nsumer products in the late 1980s, in an attempt to make existingcode run faster\non each new generation of CPUs within a given product line.\nThere are a number of ways to apply parallelism to the problem of im-\nprovingaCPU’ssingle-threadedperformance. Themostcommonare pipelined\nCPU architectures, superscalar designs, and very long instruction word (VLIW)\narchitectures. We’ll begin by having a look at how a pipelined CPU functions,\nand then address the other two variants of implicit parallelism.\n212 4. Parallelism and Concurrent Programming\nFetch Dec Exec Mem WB\nA\nA\nA\nA\nA0\n1\n23456Clock\nCycle\nB\nB\nFigure 4.8. In a non-pipelined CPU, instruction stages are idle much of the time.\n4.2.1 Pipelining\nIn order for a single machine language instruction to be executed by the CPU,\nit must pass through a number of distinct stages. Every CPU’s design is a bit\ndifferent—some CPUdesignsemployalargenumberofgranularstages,while\nothersutilizeasmallernumberofcoarse-grainedstages. HowevereveryCPU\nimplements the following basic stages in one way or another:\n•Fetch. The instruction to be executed is read from the memory location\npointed to by the instruction pointer register (IP).\n•Decode. The instruction word is decomposed into its opcode, addressing\nmode and optional operands.\n•Execute. Based on the opcode, the appropriatefunctional unit within the\nCPU is selected (ALU, FPU, memory controller, etc.). The instruction\nis dispatched to the selected component for processing, along with any\nrelevant operand data. The functional unit then performs its operation.\n•Memory access . If the instruction involves reading or writing memory,\nthe memory controller performs the appropriate operation during this\nstage.\n•Register write-back. The functional unit executing the instruction (ALU,\nFPU, etc.) writes its results back into the destination register.\nFigure 4.8 traces the path of two instructions named “A” and “B” through\nthe five execution phases of a serial CPU. You’ll notice right away that this\ndiagramcontainsalotofblankspace: Whileonestageisbusydoingitsthingto\nan instruction, all the other stages are twiddling their thumbs, doing nothing.\n4.2. Implicit Parallelism 213\nL1\nCacheGPRsB A C D ...\nFetch\nDecode ALUMem\nCtrlWB\nL2 &\nMain RA MInstruction Stream\nFigure 4.9. Components of a pipelined scalar CPU.\nFetch Dec Exec Mem WB\nA\nA\nA\nA\nAB\nB0\n1\n23456Clock\nCycle\nB\nB\nBC\nC\nC\nC\nCD\nD\nD\nDE\nE\nEF\nF G\nFigure 4.10. Ideal ﬂow of instructions through a pipelined CPU.\nEach of the stages of instruction execution is actually handled by differ-\nent hardware within the CPU, as shown in Figure 4.9. The control unit (CU)\nand memory controller handle the instruction fetch stage. A different circuit\nwithin the CU then handles the decode stage. The ALU, FPU or VPU handles\nthe lion’s share of the execute stage. The memory stage is performed by the\nmemory controller. And finally, the write-back stage primarily involves the\nregisters. This division of labor amongst different circuits within the CPU is\nthe key to making the CPU more efficient: We just need to keep all the stages’\nhardware busy all the time.\nThe solution is known as pipelining. Instead of waiting for each instruction\ntocompleteallfivestagesbeforestartingtoexecutethenextone, webeginthe\nexecution of a new instruction on everyclock cycle. Multiple instructions are\ntherefore “in flight” simultaneously. This process is illustrated in Figure 4.10.\nPipelining is a bit like doing laundry. If you have a large number of loads\ntodo,itwouldn’tbeveryefficienttowaituntileachloadhasbeenwashedand\n214 4. Parallelism and Concurrent Programming\ndried before starting the next one—while the washer is busy, the dryer would\nbe sitting idle, and vice versa. It’s much better to keep both machines busy at\nalltimes,bystartingthesecondloadwashingjustassoonasthefirstloadgoes\ninto the dryer, and so on.\nPipelining is a form of parallelism known as instruction-level parallelism\n(ILP). For the most part, ILP is designed to be transparent to the programmer.\nIdeally, at a given clock speed, a program that runs properly on a scalar CPU\nshouldbeabletoruncorrectly—butfaster—onapipelinedCPU,aslongasthe\ntwo processors support the same instruction set architecture (ISA) of course.\nIn theory, a CPU with a pipeline that is Nstages deep can execute a program\nNtimesfasterthanitsserialcounterpart. However, aswe’llexploreinSection\n4.2.4, pipelining doesn’t always perform as well as we would expect, thanks\ntovariouskindsof dependencies betweeninstructionsintheinstructionstream.\nProgrammers who are interested in writing high-performance code therefore\ncannot remain oblivious to ILP. We must embrace it, understand it, and some-\ntimes adjust the design of our code and/or data in order to get the most out\nof a pipelined CPU.\n4.2.2 Latency versus Throughput\nThelatencyof a pipeline is the amount of time required to completely process\na single instruction. This is just the sum of the latencies of all the stages in the\npipeline. Denoting latencies with the time variable T, we can write:\nTpipeline =N 1\nå\ni=0Ti (4.1)\nfor a pipeline with Nstages.\nThethroughput orbandwidth ofapipelineisameasureofhowmanyinstruc-\ntions it can process per unit time. The throughput of a pipeline is determined\nby the latency of its slowest stage—much as a chain is only as strong as its\nweakest link. The throughput can be thought of as a frequency f, measured\nin instructions per second. It can be written as follows:\nf=1\nmax (Ti). (4.2)\n4.2.3 Pipeline Depths\nWe said that each stage in a CPU can potentially have a different latency ( Ti),\nandthatthestagewiththelongestlatencydictatesthethroughputoftheentire\nprocessor. On each clock cycle, the other stages sit idle waiting for the longest\n4.2. Implicit Parallelism 215\nstage to complete. Ideally, then, we’d like all of the stages in our CPU to have\nroughly the same latency.\nThis goal can be achieved by increasing the total number of stages in the\npipeline: If one stage is taking much longer than the others, it can be broken\nintotwoormoreshorterstagesinanattempttomakeallthelatenciesroughly\nequal. However, we can’t just keep subdividing stages forever. The larger\nthe number of stages, the higher the overall instruction latency will be. This\nincreases the cost of pipeline stalls (see Section 4.2.4). Therefore CPU man-\nufacturers try to strike a balance between increasing throughput via deeper\npipelines, and keeping the overall instruction latency in check. As a result,\nreal CPU pipelines range from 4 or 5 stages at minimum, to something on the\norder of 30 stages at most.\n4.2.4 Stalls\nSometimes the CPU is unable to issue a new instruction on a particular clock\ncycles. Thisiscalleda stall. Onsuchaclockcycle,thefirststageinthepipeline\nlies idle. On the next clock cycle, the second stage will be idle, and so on. A\nstall can therefore be thought of as a “bubble” of idle time that propagates\nthrough the pipeline at a rate of one stage per clock cycle. These bubbles are\nsometimes called delayslots .\n4.2.5 Data Dependencies\nStallsarecausedby dependencies betweeninstructionsintheinstructionstream\nbeingexecuted. For example, considerthe following sequenceof instructions:\nmov ebx,5 ;; load the value 5 into register EBX\nimul eax,10 ;; multiply the contents of EAX by 10\n;; (result stored in EAX)\nadd eax,7 ;; add 7 to EAX (result stored in EAX)\nIdeally, we’d like to issue the mov,imulandaddinstructions on three con-\nsecutive clock cycles, to keep the pipeline as busy as possible. But in this case,\ntheresultsofthe imulinstructionareusedbythe addinstructionthatfollows\nit, so the CPU must wait until the imulhas made it all the way through the\npipelinebeforeissuingthe add. Ifthepipelinecontainsfivestages,thatmeans\nfourcyclesarewasted(seeFigure4.11). Thesekindsofdependenciesbetween\ninstructions are called datadependencies .\nThere are actually three kinds of dependencies between instructions that\ncan cause stalls:\n• data dependencies,\n216 4. Parallelism and Concurrent Programming\nFigure 4.11. A data dependency between instructions causes a pipeline stall.\n• control dependencies (also known as branch dependencies), and\n• structural dependencies (also known as resource dependencies).\nFirst we’ll discuss how to avoid data dependencies, then we’ll have a look at\nbranchdependenciesandhowtomitigatetheireffects. Finally,we’llintroduce\nsuperscalarCPUarchitecturesanddiscusshowtheycangiverisetostructural\ndependencies in a pipelined CPU.\n4.2.5.1 Instruction Reordering\nTo mitigate the effects of a data dependency, we need to find some other in-\nstructions for the CPU to execute while it waits for the dependent instruction\nto make its way through the pipeline. This can often be accomplished by re-\nordering the instructions in the program (while taking care not to change the\nbehavior of the program in the process). For any given pair of interdependent\ninstructions, we want to find some nearby instructions that are notdependent\nonthem,andmovethoseinstructionsupordownsothattheyenduprunning\nbetween the dependent instruction pair, thus filling the “bubble” with useful\nwork.\nInstruction reordering may of course be done by hand, by an adventurous\nprogrammerwhodoesn’tminddivingintoassemblylanguageprogramming.\nThankfully,however,thisisoftennotnecessary: Today’soptimizingcompilers\nare very good at reordering instructions automatically to reduce or eliminate\nthe impact of data dependencies.\nAs programmers, of course we shouldn’t blindly trust the compiler to op-\ntimize our code perfectly—when writing high-performance code, it’s always\na good idea to have a look at the disassembly and verify that the compiler\ndid a reasonable job. But that said, we should also remember the 80/20 rule\n4.2. Implicit Parallelism 217\n(Section 2.3) and only spend time optimizing the 20% or less of our code that\nactually has a noticable impact on overall performance.\n4.2.5.2 Out-of-Order Execution\nCompilers and programmers aren’t the only ones capable of reordering a se-\nquence of machine language instructions to prevent stalls. Many of today’s\nCPUs support a feature known as out-of-order execution , which enables them\nto dynamically detect data dependencies between instructions, and automat-\nically resolve them.\nTo accomplish this feat, the CPU looks ahead in the instruction stream\nand analyzes the instructions’ register usage in order to detect dependencies\nbetween them. When a dependency is found, the “look-ahead window” is\nsearched for another instruction that is notdependent on any of the currently-\nexecuting instructions. If one is found, it is issued (out of order!) to keep the\npipeline busy. The details of how out-of-order execution works is beyond our\nscope here. Suffice it to say that as programmers, we cannot rely on the CPU\nto execute the instructions in the same order we (or the compiler) wrote them.\nBoth the compiler’s optimizer and the CPU’s out-of-order execution logic\ntake great care to ensure that the behavior of the program doesn’t change as\na result of instruction reordering. However, as we’ll see in Section 4.9.3, com-\npiler optimizations and out-of-order execution cancause bugs in a concurrent\nprogram (i.e., a program consisting of multiple threads that share data). This\nis one of the many reasons why concurrent programming requires more care\nthan serial programming.\n4.2.6 Branch Dependencies\nWhathappenswhenapipelinedCPUencountersaconditionalbranchinstruc-\ntion (e.g., an ifstatement, or the conditional expression at the end of a for\norwhileloop)? To answer this question, let’s consider the following C/C++\ncode:\nint SafeIntegerDivide(int a, int b, int defaultVal)\n{\nreturn (b != 0) ? a / b :defaultVal;\n}\nIf we looked at the disassembly for this function, it might look something like\nthis on an Intel x86 CPU:\n218 4. Parallelism and Concurrent Programming\nFigure 4.12. A dependency between a comparison instruction and a conditional branch instruction\nis called a branch dependency.\n; function preamble omitted for clarity...\n; first, put the default into the return register\nmov eax,dword ptr [defaultVal]\nmov esi,dword ptr [b] ; check (b != 0)\ncmp esi,0\njz SkipDivision\nmov eax, dword ptr[a] ; divisor (a) must be in EDX:EAX\ncdq ; ... so sign-extend into EDX\nidiv esi ; quotient lands in EAX\nSkipDivision:\n; function postamble omitted for clarity...\nret ; EAX is the return value\nThedependencyhereisbetweenthe cmp(compare)instructionandthe jz\n(jumpifequaltozero)instruction. TheCPUcannotissuetheconditionaljump\nuntil it knows the results of the comparison. This is called a branchdependency\n(also known as a control dependency). See Figure 4.12 for an illustration of a\nbranch dependency.\n4.2.6.1 Speculative Execution\nOne way CPUs deal with branch dependencies is via a technique known as\nspeculative execution, also known as branch prediction . Whenever a branch in-\nstruction is encountered, the CPU tries to guessat which branch is going to\nbe taken. It continues to issue the instructions from the selected branch, in\nthe hopes that its guess was correct. Of course, the CPU won’t know for sure\nwhether it guessed correctly until the dependent instruction pops out at the\nend of the pipeline. If the guess ends up being wrong, the CPU has executed\n4.2. Implicit Parallelism 219\ninstructions that shouldn’t have been executed at all. So the pipeline must be\nflushedand restarted at the first instruction of the correct branch. This is called\nabranch penalty.\nThe simplest guess a CPU can make is to assume that branches are never\ntaken. The CPU just keeps executing instructions in sequential order, and\nonly jumps the instruction pointer to a new location when its guess is proven\nwrong. This approach is I-cache friendly, in the sense that the CPU always\nprefers the branch whose instructions are most likely to be in the cache.\nAnother slightly more advanced approach to branch prediction is to as-\nsume that backward branches are always taken and forward branches are\nnever taken. A backward branch is the kind found at the end of a whileor\nforloop, so such branches tend to be more prevalent than forward branches.\nMost high-quality CPUs include branch prediction hardware that can im-\nprove the quality these “static” guesses significantly. A branch predictor can\ntrack the results of a branch instruction over multiple iterations of a loop and\ndiscover patterns that help it make better guesses on subsequent iterations.\nPS3 game programmers had to deal with the poor performance of\n“branchy” code all the time, because the branch predictors on the Cell pro-\ncessor were frankly pretty terrible. But the AMD Jaguar CPU found in the\nPS4 and Xbox One has highly advanced branch prediction hardware, so game\nprogrammers can breathe a little easier when writing code for the PS4.\n4.2.6.2 Predication\nAnotherwaytomitigatetheeffectsofbranchdependenciesistosimplyavoid\nbranching altogether. Consider again the SafeIntegerDivide() function,\nalthough we’ll modify it slightly to work in terms of floating-point values in-\nstead of integers:\nfloat SafeFloatDivide(float a, float b, float d)\n{\nreturn (b != 0.0f) ? a / b :d;\n}\nThis simple function calculates one of two answers, depending on the re-\nsults of the conditional test b != 0. Instead of using a conditional branch\nstatement to return one of these two answers, we can instead arrange for our\nconditional test to generate a bit mask consisting of all zeros (0x0U) if the con-\ndition is false, and all ones (0xFFFFFFFFU) if it is true. We can then execute\nbothbranches, generating two alterate answers. Finally, we use the mask to\nproduce the final answer that we’ll return from the function.\n220 4. Parallelism and Concurrent Programming\nThe following pseudocode illustrates the idea behind predication. (Note\nthat this code won’t run as-is. In particular, you can’t mask a float with\nanunsigned int and obtain a floatresult—you’d need to use a union to\nreinterpret the bit patterns of the floats as if they were unsigned integers\nwhen applying the mask.)\nint SafeFloatDivide_pred(float a, float b, float d)\n{\n// convert Boolean (b != 0.0f) into either 1U or 0U\nconst unsigned condition = (unsigned)( b != 0.0f );\n// convert 1U -> 0xFFFFFFFFU\n// convert 0U -> 0x00000000U\nconst unsigned mask = 0U - condition;\n// calculate quotient (will be QNaN if b == 0.0f)\nconst float q=a / b;\n// select quotient when mask is all ones, or default\n// value d when mask is all zeros (NOTE: this won't\n// work as written -- you'd need to use a union to\n// interpret the floats as unsigned for masking)\nconst float result = ( q&mask) | (d &~mask);\nreturn result;\n}\nLet’s take a closer look at how this works:\n• The test b != 0.0f produces a boolresult. We convert this into an\nunsigned integer by simply casting it. This results in either the value\n1U(corresponding to true) or 0U(corresponding to false).\n• We convert this unsigned result into a bit mask by subtracting it from\n0U. Zero minus zero is still zero, and zero minus one is  1, which is\n0xFFFFFFFFU in 32-bit unsigned integer arithmetic.\n• Next, we go ahead and calculate the quotient. We run this code regard-\nlessof the result of the non-zero test, thereby side-stepping any branch\ndependency issues.\n• We now have our two answers ready to go: The quotient qand the\ndefault value d. We want to apply the mask in order to “select” one\nor the other value. But to do this, we need to reinterpret the floating-\npoint bit patterns of qanddasifthey were unsigned integers. The most\nportable way to accomplish this in C/C++ is to use a union containing\n4.2. Implicit Parallelism 221\ntwo members, one of which interprets a 32-bit value as a float, the\nother of which interprets it as an unsigned.\n• Themaskisappliedasfollows: WebitwiseANDthequotient qwiththe\nmask, producing a bit pattern that matches qif the mask is all ones, but\nis all zeros if the mask is all zeros. We bitwise AND the default value\ndwith the complement of the mask, yielding all zeros if the mask is all\nones, or the bit pattern of dif the mask is all zeros. Finally, we bitwise\nORthesetwovaluestogether,effectivelyselecting eitherthevalueof qor\nthe value of d.\nThe use of a mask to select one of two possible values like this is called pred-\nicationbecause we run both code paths (the one that returns a / b and\nthe one that returns d) but each code path is predicated on the results of the\ntest(a != 0), via the mask. Because we are selecting one of two possible\nvalues, this is also often called a selectoperation.\nGoing to all this trouble to avoid a branch may seem like overkill. And it\ncanbe—itsusefulnessdependsontherelativecostofabranchversusthepred-\nicated alternative on your target hardware. Predication really shines when\na CPU’s ISAs provide special machine language instructions for performing\na select operation. For example, the PowerPC ISA offers an integer select\ninstruction isel, a floating-point select instruction fsel, and even a SIMD\nvector select instruction vecsel , and their use can definitely result in perfor-\nmance improvements on PowerPC based platforms (like the PS3).\nIt’s important to realize that predication only works when both branches\ncanbeexecuted safely. Performingadividebyzerooperationinfloating-point\ngenerates a quiet not-a-number (QNaN), but an integer divide by zero throws\nan exception that will crash your game (unless it is caught). That’s why we\nconverted this example to floating-point before applying predication to it.\n4.2.7 Superscalar CPUs\nThe pipelined CPU we described in Section 4.2.1 is what is called a scalarpro-\ncessor. This means that it can start executing at most one instruction per clock\ncycle. Yes, multiple instructions are “in flight” at any given moment, but only\nonenewinstruction is sent down the pipeline every clock cycle.\nAt its core, parallelism is about making use of multiple hardware compo-\nnents simultaneously. So one way to double the throughput of a CPU (at least\nin theory!) would be to duplicate most of the components on the chip, in such\na way that twoinstructions could be launched each clock cycle. This is called\nasuperscalar architecture.\n222 4. Parallelism and Concurrent Programming\nFigure 4.13. A pipelined superscalar CPU contains multiple execution components (ALUs, FPUs\nand/or VPUs) fed by a single instruction scheduler which typically supports out-of-order and spec-\nulative execution.\nIn a superscalar CPU, two (or more) instances of the circuitry that man-\nages each stage of the pipeline1is present on-chip. The CPU still fetches in-\nstructions from a single instruction stream, but instead of issuing the one in-\nstructionpointedtobytheIPduringeachclockcycle,thenext twoinstructions\nare fetched and dispatched during each clock cycle. Figure 4.13 illustrates the\nhardware components found in a two-way superscalar CPU, and Figure 4.14\ntraces the path of ten instructions, “A” through “N,” as they move through\nthis CPU’s two parallel pipelines.\n4.2.7.1 Complexity of Superscalar Designs\nImplementing a superscalar CPU isn’t quite as simple as “copying and past-\ning” two identical CPU cores onto a die. Although it’s reasonable to envision\na superscalar CPU as two parallel instruction pipelines, these two pipelines\nare fed from a single instruction stream. Some some kind of control logic is\ntherefore required at the front end of these parallel pipes. Just as on a CPU\nthat supports out-of-order execution, a superscalar CPU’s control logic looks\nahead in the instruction stream in an attempt to identify dependencies between\ninstructions,andthenissuesinstructionsoutoforderinanattempttomitigate\ntheir effects.\nIn addition to data and branch dependencies, a superscalar CPU is prone\nto a third kind of dependency known as a resource dependency. This kind of\ndependency arises when two or more consecutive instructions all require the\n1Technically pipelining and superscalar designs are two independent forms of parallelism. A\npipelined CPU needn’t be superscalar. Likewise, a superscalar CPU needn’t be pipelined, al-\nthough the majority of them are.\n4.2. Implicit Parallelism 223\nE1M0M1W0W1 F0F1D0D1E0\nA\nB0\n1\n23456Clock\nCycle\nC D\nE FB\nA\nA\nB AB\nB AC D\nC D\nC D\nC DE F\nE F\nE F\nE FG\nH I J\nK LH\nG\nG\nH GH I J\nI J K L M N\nFigure 4.14. Best-case execution of 14 instructions “A” through “N” on a superscalar pipelined CPU\nover seven clock cycles.\nsame functional unit within the CPU. For example, let’s imagine we have a\nsuperscalar CPU with two integer ALUs but only one FPU. Such a processor\nis capable of issuing two integer arithmetic instructions on each clock. But if\ntwo floating-point arithmetic instructions are encountered in the instruction\nstream, they cannot both be issued on the same clock cycle because the re-\nsource required by the second (the FPU) will already be in use by the first. As\nsuch,thecontrollogicthatmanagesinstructiondispatchonasuperscalarCPU\nis even more complex than that found on a scalar CPU that supports out-of-\norder execution.\n4.2.7.2 Superscalar and RISC\nA two-way superscalar CPU requires roughly two times the silicon real-estate\nofacomparablescalarCPUdesign. Inordertofreeuptransistors,mostsuper-\nscalar CPUs are therefore reduced instruction set (RISC) processors. The ISA of\naRISCprocessorprovidesacomparativelysmallsetofinstructions,eachwith\na very focused purpose. More complex operations are peformed by building\nupsequencesofthesesimplerinstructions. Incontrast,theISAofa complexin-\nstruction set computer (CISC) offers a much wider variety of instructions, each\nof which may be capable of performing more complex operations.\n4.2.8 Very Long Instruction Word (VLIW)\nWe saw in Section 4.2.7.1 that a superscalar CPU contains highly complex in-\nstruction dispatch logic. This logic takes up valuable real-estate on the CPU\n224 4. Parallelism and Concurrent Programming\ndie. Also,CPUsareonlycapableoflookingaheadintheinstructionstreamby\na relatively small number of instructions when analyzing dependencies and\nlooking for opportunities for out-of-order and/or superscalar instruction dis-\npatch. This limits the effectiveness of the dynamic optimizations a CPU can\nperform.\nA somewhat simpler way to implement instruction level parallelism is\nto design a CPU that has multiple compute elements (ALUs, FPUs, VPUs)\non-chip, but leaves the task of dispatching instructions to those compute ele-\nmentsentirelytotheprogrammerand/orcompiler. Thatway, allthecomplex\ninstruction-dispatchlogiccanbeeliminated, andthosetransistorsdevotedin-\nstead to implementing more compute elements or a larger cache. As a side\nbenefit, programmers and compilers ought to be better at optimizing the dis-\npatching of the instructions in their programs than the CPU could ever be,\nbecause they can select instructions for dispatch from a much wider window\n(typically an entire function’s worth of instructions).\nToallowprogrammersand/orcompilerstodispatchinstructionstomulti-\nple compute elements on each clock cycle, the instruction word is extended so\nthatitcontainstwoormore“slots,” eachcorrespondingtoacomputeelement\nonthechip. Forexample,ifourhypotheticalCPUcontainedtwointegerALUs\nandtwoFPUs, aprogrammerorcompilerwouldneed tobeableto encodeup\ntotwointegerandtwofloating-pointoperationswithineachinstructionword.\nWe call this a verylonginstructionword (VLIW) design. The VLIW architecture\nis illustrated in Figure 4.15.\nWe can look to the Playstation 2 for a concrete example of VLIW architec-\nture: The PS2 contained two coprocessors called vector units (VU0 and VU1),\neach of which was capable of dispatching two instructions per clock cycle.\nEach instruction word was comprised of two slots called lowandhigh. It\nwas often a challenge to fill both slots effectively when hand-coding in as-\nsemblylanguage, althoughtoolsweredevelopedthathelpedprogrammersto\nconvertaone-instruction-per-clockprogramintoanefficienttwo-instructions-\nper-clock format.\nThere are trade-offs between the superscalar and VLIW approaches. Be-\ncause it lacks the complex scheduling, out-of-order execution and branch pre-\ndiction logic of a superscalar CPU, a VLIW processor is much simpler, and\ncan therefore potentially make heavier use of parallelism than its superscalar\ncounterparts. However,itcanbeverytoughtotransformaserialprograminto\na form that takes full advantage of the parallelism in a VLIW. This makes the\njob of the programmer and/or compiler more difficult. That said, a number\nof advances have been made to overcome some of these limitations, includ-\ningvariable-widthVLIWdesigns. Forexample,seehttp://researcher.watson.",28752
27-4.3 Explicit Parallelism.pdf,27-4.3 Explicit Parallelism,"4.3. Explicit Parallelism 225\nFigure 4.15. A pipelined VLIW CPU architecture consisting of two integer ALUs and two ﬂoating-\npoint FPUs. Each very long instruction word consists of two integer and two ﬂoating-point op-\nerations, which are dispatched to the corresponding functional units. Notice the absence of the\ncomplex instruction scheduling logic that would be present in a superscalar CPU.\nibm.com/researcher/view_group_subpage.php?id=2834.\n4.3 Explicit Parallelism\nExplicit parallelism is designed to make concurrent software run more effi-\nciently. Hence all explicitly parallel hardware designs permit more than one\ninstruction stream to be processed in parallel. We’ll list a few common explic-\nitlyparalleldesignsbelow,increasingingranularityfrom hyperthreading atthe\nmost fine-grained end of the spectrum to cloud computing at the most coarse-\ngrained end.\n4.3.1 Hyperthreading\nAs we saw in Section 4.2.5.2, some pipelined CPUs are capable of issuing\ninstructions out of order as a means of reducing pipeline stalls. Normally a\npipelinedCPUexecutesinstructionsinprogramorder;butsometimesthenext\ninstruction in the instruction stream cannot be issued due to a dependency on\nan in-flight instruction. This creates a delayslot into which another instruction\ncould theoretically be issued. An OOO CPU can “look ahead” in the instruc-\ntion stream and select an instruction to issue out-of-order into such a delay\nslot.\n226 4. Parallelism and Concurrent Programming\nHyperthread 0\nF/DGPRs FPRsB A C ...\nHyperthread 1\nF/DGPRs FPRsQ P R ...\nShared\nResources\nL1Mem\nCtrlALU ALU FPU FPUScheduler\nFigure 4.16. A hyperthreaded CPU containing two front ends (each consisting of a fetch/decode\nunit and a register ﬁle), but with a single back end containing ALUs, FPUs, a memory controller, an\nL1 cache, and an out-of-order instruction scheduler. The scheduler issues instructions from both\nfront-end threads to the shared back-end components.\nWith only a single instruction stream, the CPU’s options are somewhat\nlimited when selecting an instruction to issue into a delay slot. But what if\nthe CPU could select its instructions from two separate instruction streams at\nonce? This is the principle behind a hyperthreaded (HT) CPU core.\nTechnically speaking, an HT core consists of two register files and two in-\nstruction decode units, but with a single “back end” for executing instruc-\ntions, and a single shared L1 cache. This design enables an HT core to run\ntwo independent threads, while requiring fewer transistors than a dual core\nCPU, thanks to the shared back end and L1 cache. Of course, this sharing of\nhardware components also results in lower instruction throughput relative to\na comparable dual core CPU, because the threads contend for these shared re-\nsources. Figure4.16illustratesthekeycomponentsinatypicalhyperthreaded\nCPU design.\n4.3.2 Multicore CPUs\nACPUcorecanbedefinedasaself-containedunitcapableofexecutinginstruc-\ntions from at least one instruction stream. Every CPU design we’ve looked at\nuntil now could therefore qualify as a “core.” When more than one core is\nincluded on a single CPU die, we call it a multicore CPU.\n4.3. Explicit Parallelism 227\nCore 0B A C ...\nF/D\nL1Mem\nCtrl\nSchedGPRsALUFPRsFPU\nCore 1Q P R ...\nF/D\nL1Mem\nCtrl\nSchedGPRsALUFPRsFPUL2\nFigure 4.17. A simple multicore CPU design.\nThespecificdesignwithineachcorecanbeanyofthedesignswe’velooked\natthusfar—eachcoremightemployasimpleserialdesign,apipelineddesign,\na superscalar architecture, a VLIW design, or might be a hyperthreaded core.\nFigure 4.17 illustrates a simple example of a multicore CPU design.\nThe PlayStation 4 and Xbox One game consoles both contain multicore\nCPUs. Each contains an accelerated processing unit (APU) consisting of two\nquad-core AMD Jaguar modules, integrated onto a single die with a GPU,\nmemory controller and video codec. (Of these eight cores, seven are avail-\nable for use by game applications. However, roughly half of the bandwidth\nontheseventhcoreisreservedforoperatingsystemuse.) TheXboxOneXalso\ncontains an eight-core APU, but its cores are based on proprietary technology\ndevelopedinpartnershipwithAMD,ratherthanontheJaguarmicroarchitec-\nture like its predecessor. Figure 4.18 shows a block diagram of the PS4 hard-\nware architecture, and Figure 4.19 presents a block diagram of the Xbox One\nhardware architecture.\n4.3.3 Symmetric versus Asymmetric Multiprocessing\nThesymmetry of a parallel computing platform has to do with how the CPU\ncores in the machine are treated by the operating system. In symmetric mul-\ntiprocessing (SMP), the available CPU cores in the machine (provided by any\ncombination of hyperthreading, multicore CPUs or multiple CPUs on a single\n228 4. Parallelism and Concurrent Programming\nAMD Jaguar CPU @ 1.6 GHz\nCPC 0\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-wayCore 0\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-wayCore 1\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-way\nCore 3L1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-way\nCore 2CPC 0\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-wayCore 4\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-wayCore 5\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-way\nCore 7L1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-way\nCore 6\nAMD Radeon GPU\n(comparable to 7870)\n@ 800 MHz\n1152 stream processorssnoop snoop\n“Onion” Bus\n(10 GiB/s each way)\n“Garlic” Bus\n(176 GiB/s)\n(non cache-coherent)L2 Cache\n2 MiB / 16-wayL2 Cache\n2 MiB / 16-wayCPU Bus (20 GiB/s)\nMain RAM\n8 GiB GDDR5Cache Coherent\nMemory Controller\nFigure 4.18. Simpliﬁed view of the PS4’s architecture.\nmotherboard) are homogeneous in terms of design and ISA, and are treated\nequally by the operating system. Any thread can be scheduled to execute on\nany core. (Note, however, that it is possible in such systems to specify an\naffinityfor a thread, causing it to be more likely, or even guaranteed, to be\nscheduled on a particular core.)\nThe PlayStation 4 and the Xbox One are examples of SMP. Both of these\nconsoles contain eight cores, of which seven are available for use by the pro-\ngrammer, and the application is free to run threads on any of the available\ncores.\nInasymmetricmultiprocessing (AMP), the CPU cores are not necessarily ho-\nmogeneous, and the operating system does not treat them equally. In AMP,\none “master” CPU core typically runs the operating system, and the other\ncoresare treated as “slaves” to which workloads are distributed by the master\ncore.\n4.3. Explicit Parallelism 229\nAMD Jaguar CPU @ 1.75 GHz\nCPC 0\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-wayCore 0\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-wayCore 1\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-way\nCore 3L1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-way\nCore 2CPC 0\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-wayCore 4\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-wayCore 5\nL1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-way\nCore 7L1 D$\n32 KiB\n8-wayL1 I$\n32 KiB\n2-way\nCore 6\nAMD Radeon GPU\n(comparable to 7790)\n@ 853 MHz\n768 stre am pr ocessors30 GiB/s\nup to\n204 GiB/seSRAM\n32 MiBMain RAM\n8 GiB GDDR3Cache Coherent\nMemory AccessL2 Cache\n2 MiB / 16-wayL2 Cache\n2 MiB / 16-way\n68 GiB/s\n(non cache-coherent)30 GiB/s\n(cache-coherent)\nFigure 4.19. Simpliﬁed view of the Xbox One’s architecture.\nThecell broadband engine (CBE) used in the PlayStation 3 is an example of\nAMP; it employs a main CPU known as the “power processing unit” (PPU)\nwhich is based on the PowerPC ISA, along with eight coprocessors known as\n“synergystic processing units” (SPUs) which are based around a completely\ndifferent ISA. (See Section 3.5.5 for more on the PS3’s hardware architecture.)\n4.3.4 Distributed Computing\nYet another way to achieve parallelism in computing is to make use of mul-\ntiple stand-alone computers working in concert. This is known as distributed\ncomputing in the most general sense. There are various ways to architect a\ndistributed computing system, including:\n• computer clusters,\n• grid computing, and",7994
28-4.4 Operating System Fundamentals.pdf,28-4.4 Operating System Fundamentals,"230 4. Parallelism and Concurrent Programming\nKernel\nDriversUser Pro cesses\nOS Processes\nCPU Memory Devices\nFigure 4.20. The kernel and device drivers sit directly on top of the hardware, and run in privileged\nmode. All other operating system software and all user programs are implemented on top of the\nkernel and driver layer, and run in a somewhat restricted user mode.\n• cloud computing.\nWe’ll focus exclusively on parallelism within a single computer in this book,\nbutyoucanreadmoreaboutdistributedcomputingbysearchingfortheabove\nterms online.\n4.4 Operating System Fundamentals\nNow that we have a solid understanding of the basics of parallel computer\nhardware, let’s turn our attention to the services provided by the operating\nsystem that make concurrent programming possible.\n4.4.1 The Kernel\nModernoperatingsystemshandleawidevarietyoftasks,acrossawiderange\nofgranularities. Theserangefromthehandlingofkeyboardandmouseevents\northeschedulingofprogramsfor preemptivemultitasking atoneendofthespec-\ntrum, to managing a printer queue or the network stack at the other end. The\n“core” of the operating system—the part that handles all of the most funda-\nmental and lowest-level operations—is called the kernel. The rest of the oper-\nating system, and all user programs, are built atop the services provided by\nthe kernel. This architecture is illustrated in Figure 4.20.\n4.4.1.1 Kernel Mode versus User Mode\nThe kernel and its device drives run in a special mode called protected mode ,\nprivilegedmode orkernelmode ,whileallotherprogramsinthesystem(including\nall other parts of the operating system that aren’t part of the kernel) operate\ninuser mode . As the name suggests, software running in privileged mode has\n4.4. Operating System Fundamentals 231\nFigure 4.21. Example of CPU protection rings, showing four rings. The kernel runs in ring 0, de-\nvice drivers run in ring 1, trusted programs with I/O permissions run in ring 2, and all other user\nprograms run in ring 3.\nfullaccesstoallofthehardwareinthecomputer,whereasusermodesoftware\nisrestricted invariouswaysinordertoensurestabilityofthecomputersystem\nas a whole. Software running in user mode can access low-level services only\nby making a special kernelcall —a request for the kernel to perform a low-level\noperation on the user program’s behalf. This ensures that a program can’t\ninadvertently or maliciously destabilize the system.\nIn practice, operating systems may implement multiple protection rings.\nThe kernel runs in ring 0, which is the most trusted ring and has all possible\nprivileges within the system. Device drivers might run in ring 1, trusted pro-\ngrams with I/O permissions might run in ring 2, while all other “untrusted”\nuserprogramsruninring3. Butthisisjustoneexample—thenumberofrings\nvariesfromCPUtoCPUandfromOStoOS,asdoestheassignmentofsubsys-\ntems to the various rings. The protection ring concept is illustrated in Figure\n4.21.\n4.4.1.2 Kernel Mode Privileges\nKernel mode (ring 0) software has access to allof the machine language in-\nstructions defined by the CPU’s ISA. This includes a powerful subset of in-\nstructions called privileged instructions. These privileged instructions might\nallow certain registers to be modified that are normally off-limits (e.g., to con-\ntrol virtual memory mapping, or to mask and unmask interrupts). Or they\nmight allow certain regions of memory to be accessed, or allow other nor-\n232 4. Parallelism and Concurrent Programming\nmally restricted operations to be performed. Examples of privileged instruc-\ntions on the Intel x86 processor include wrmsr(write to model-specific regis-\nter)and cli(clearinterrupts). Byrestrictingtheuseofthesepowerfulinstruc-\ntionsonlyto“trusted”softwarelikethekernel,systemstabilityandsecurityis\nimproved.\nWith these privileged ML instructions, the kernel can implement security\nmeasures. For example, the kernel typically locks down certain pages of vir-\ntual memory so that they cannot be written to by a user program. Both the\nkernel’s software and all of its internal record-keeping data are kept in pro-\ntected memory pages. This ensures that a user program won’t stomp on the\nkernel, and thereby crash the entire system.\n4.4.2 Interrupts\nAninterrupt is a signal sent to the CPU in order to notify it of an important\nlow-levelevent,suchasakeypressonthekeyboard,asignalfromaperipheral\ndevice, or the expiration of a timer. When such an event occurs, an interrupt\nrequest(IRQ) is raised. If the operating system wishes to respond to the event,\nit pauses (interrupts) whatever processing had been going on, and calls a spe-\ncial kind of function called an interrupt service routine (ISR). The ISR function\nperforms some operation in response to the event (ideally doing so as quickly\nas possible) and then control is returned back to whatever program had been\nrunning prior to the interrupt having been raised.\nTherearetwokindsofinterrupt: hardwareinterrupts andsoftwareinterrupts.\nA hardware interrupt is requested by placing a non-zero voltage onto one of\nthe pins of the CPU. Hardware interrupts might be raised by devices such as\na keyboard or mouse, or by a periodic timer circuit on the motherboard or\nwithintheCPUitself. Becauseit’striggeredbyanexternaldevice, ahardware\ninterruptcanhappenatanytime—evenrightinthemiddleofexecutingaCPU\ninstruction. As such, there may be a tiny delay between the moment when a\nhardwareinterruptisphysicallyraisedandwhentheCPUisinasuitablestate\nto handle it.\nA software interrupt is triggered by software rather than by a voltage on a\nCPU pin. It has the same basic effect as a hardware interrupt, in that it causes\nthe operation of the CPU to be interrupted and a service routine to be called.\nA software interrupt can be triggered explicitly by executing an “interrupt”\nmachinelanguageinstruction. Oronemaybetriggeredinresponsetoanerro-\nneousconditiondetectedbytheCPUwhilerunningapieceofsoftware—these\narecalled trapsorsometimes exceptions (althoughthelattertermshouldnotbe\nconfused with language-level exception handling). For example, if an ALU is\n4.4. Operating System Fundamentals 233\ninstructed to perform a divide-by-zero operation, a software interrupt will be\nraised. The operating system normally handles such interrupts by crashing\nthe program in question and producing a core dump file. However, a debug-\nger attached to the program could catch this interrupt and instead cause the\nprogram to break into the debugger for inspection.\n4.4.3 Kernel Calls\nIn order for user software to perform a privileged operation, such as mapping\nor unmapping physical memory pages in the virtual memory system or ac-\ncessing a raw network socket, the user program must make a request to the\nkernel. The kernel responds by performing the operation in a safe manner on\nbehalf of the user program. Such a request is called a kernel call orsystemcall.\nOn most systems, calling into the kernel is accomplished via a software\ninterrupt.2In the case of an interrupt-triggered system call, the user program\nplacesanyinputargumentsinaspecificplace(eitherinmemoryorinregisters)\nand then issues a “software interrupt” instruction with an integer argument\nthat specifies which kernel operation is being requested. This causes the CPU\nto be put into a mode with elevated privileges, staves the state of the calling\nprogram, and then causes the appropriate kernel interrupt service routine to\nbecalled. Presumingthatthekernelallowstherequesttoproceed,itperforms\nthe requested operation (in privileged mode) and then control is returned to\nthecaller(afterfirstrestoringitsexecutionstate). Thisswitchfromausermode\nprogram into the kernel is an example of context switching . See Section 4.4.6.5\nfor more on context switching.\nOn most modern operating systems, the user program doesn’t execute a\nsoftware interrupt or system call instruction manually, for example via inline\nassemblycode. Thatwouldbemessyanderror-prone. Instead,auserprogram\ncallsakernelAPI function,whichinturnmarshallstheargumentsandtripsthe\nsoftwareinterrupt. Thisiswhysystemcallsappeartoberegularfunctioncalls\nfrom the point of view of the user program.\n4.4.4 Preemptive Multitasking\nThe earliest minicomputers and personal computers ran a single program\nat a time. They were inherently serialcomputers, capable of reading a pro-\ngramfromasingleinstructionstream,andexecutingoneinstructionfromthis\nstream at a time. The diskoperatingsystems (DOS) in those days weren’t much\n2On some systems, a special variant of the callinstruction is used to call into the kernel. For\nexample, on MIPS processors this instruction is named syscall .\n234 4. Parallelism and Concurrent Programming\nFigure 4.22. Three full-screen programs running on the Apple II computer. Programs on the Apple\nII were always full-screen because it could only run one program at a time. From left to right:\nCopy II Plus, the AppleWorks word processor, and The Locksmith.\nmore than glorified device drivers, allowing programs to interface with de-\nvices such as tape, floppy and hard disk drives. The entire computer would\nbe devoted to running a single program at a time. Figure 4.22 shows a few\nfull-screen programs running on an Apple II computer.\nAs operating systems and computer hardware became more advanced, it\nbecame possible to runmorethan one programon a serial computer at a time.\nOnsharedmainframe computersystems, a techniqueknownas multiprogram-\nmingwould allow one program to run while another was waiting for a time-\nconsuming request to be satisifed by a peripheral device. Classic Mac OS and\nversionsofWindowspriortoWindowsNTandWindows95usedatechnique\nknown as cooperative multitasking, in which only one program would be run-\nningonthemachineatatime,buteachprogramwouldregularly yieldtheCPU\nso that another program could get a chance to run. In this way, each program\nended up with a periodic “slice” of CPU time. Technically, this technique is\nknown as time division multiplexing (TDM) or temporal multithreading (TMT).\nInformally it’s called time-slicing.\nCooperative multitasking suffered from one big problem: Time-slicing re-\nquiredthecooperationofeachandeveryprograminthesystem. One“rogue”\nprogram could consume all of the CPU’s time if it failed to yield to other\nprograms periodically. The PDP-6 Monitor and Multics operating systems\nsolved this problem by introducing a technique known as preemptive multi-\ntasking. ThistechnologywaslateradoptedbytheUNIXoperatingsystemand\nall of its variants, along with later versions of Mac OS and Windows.\nIn preemptive multitasking, programs still share the CPU by time-slicing.\nHowever the scheduling of programs is controlled by the operating system,\nnot via cooperation between the programs themselves. As a result, each pro-\n4.4. Operating System Fundamentals 235\ngram gets a regular, consistent and reliable time slice on the CPU. The time\nslice during which one particular program is allowed to run on the CPU is\nsometimes called the program’s quantum. To implement preemptive multi-\ntasking, the operating system responds to a regularly-timed hardware inter-\nruptinordertoperiodically contextswitch betweenthedifferentprogramsrun-\nning on the system. We’ll see how context switching works in more depth in\nthe next section (4.4.6.5).\nWe should note here that preemptive multitasking is used even on mul-\nticore machines, because typically the number of threads is greater than the\nnumber of cores. For example, if we were to have 100 threads and only four\nCPU cores, then the kernel would use preemptive multitasking to time-slice\nbetween 25 threads on each core.\n4.4.5 Processes\nAprocessis the operating system’s way of managing a running instance of a\nprogram contained in an executable file (.exe on Windows, .elf on Linux). A\nprocess only exists while its program is actually running—when an instance\nof a program exits, is killed, or crashes, the OS destroys the process associated\nwith that instance. Multiple processes can be running on a computer system\nat any given time. This might include multiple instances of the sameprogram.\nProgrammers interact with processes via an API provided by the operat-\ning system. The details of this API differ from OS to OS, but the key concepts\nare roughly the same across all of them. A complete discussion of any one\noperating system’s process API is beyond the scope of this book, but for the\npurposes of illustrating the concepts we’ll focus primarily on the API style of\nUNIX-like operating systems such as Linux, BSD and MacOS. But we’ll make\nnote of situations in which Windows or game console operating systems de-\nviate significantly from the core ideas of a UNIX-like process API.\n4.4.5.1 Anatomy of a Process\nUnder the hood, a process consists of:\n• aprocessid(PID) thatuniquelyidentifiestheprocesswithintheoperating\nsystem;\n• asetof permissions,suchaswhichuser“owns”eachprocessandtowhich\nuser group it belongs;\n• a reference to the process’s parentprocess , if any,\n• avirtualmemoryspace containing the process’s “view” of physical mem-\nory (see Section 4.4.5.2 for more information);\n236 4. Parallelism and Concurrent Programming\n• the values of all defined environmentvariables;\n• the set of all open filehandles in use by the process;\n• the current workingdirectory for the process,\n• resources for managing synchronization andcommunication between pro-\ncesses in the system, such as message queues, pipes, and semaphores;\n• one or more threads.\nAthreadencapsulates a running instance of a single stream of machine lan-\nguage instructions. By default, a process contains a single thread. But as we’ll\ndiscuss in depth in Section 4.4.6, more than one thread can be created within\na process, allowing more than one instruction stream to run concurrently . The\nkernel schedules all of the threads in the system (from all currently-running\nprocesses) to run on the available cores. It uses preemptive multitasking to\ntime-slice between threads when there are more threads than there are cores.\nWe should stress here that threads are the fundamental unit of program exe-\ncutionwithin an operating system, not processes. A process merely provides\nanenvironment within which its thread(s) can run, including a virtual mem-\nory map and a set of resources that are used by and shared between all threads\nwithin that process. Whenever a thread is scheduled to run on a core, its pro-\ncess becomes active, and that process’s resources and environment become\navailable for use by the thread while it runs. So when we say that a threadis\nrunning on a core, remember that it is always doing so within the contextof\nexactly one process.\n4.4.5.2 Virtual Memory Map of a Process\nYou’ll recall from Section 3.5.2 that a program generally never works directly\nwith physical memory addresses.3Rather, the program accesses memory in\nterms of virtual addresses, and the CPU and operating system cooperate to\nremapthesevirtual addressesintophysicaladdresses. Wesaidthattheremap-\nping of virtual to physical addresses happens in terms of contiguous blocks of\naddresses called pages, and that a page table is used by the OS to map virtual\npage indices to physical page indices.\nEvery process has its own virtual page table. This means that every pro-\ncess has its own custom viewof memory. This is one of the primary ways in\nwhich the operating system provides a secure and stable execution environ-\nment. Two processes cannot corrupt each other’s memory, because the physi-\ncalpagesownedbyoneprocessaresimplynotmappedintotheotherprocess’s\naddress space (unless they explicitly share pages). Also, pages owned by the\n3User programs always work in terms of virtual memory addresses, but the kernel can work\ndirectly with physical addresses.\n4.4. Operating System Fundamentals 237\nkernel are protected from inadvertent or deliberate corruption by a user pro-\ncess because they are mapped to a special range of addresses known as kernel\nspacewhich can only be accessed by code running in kernel mode.\nAprocess’svirtualpagetableeffectivelydefinesits memorymap. Themem-\nory map typically contains:\n• the text, data and BSS sections as read in from the program’s executable\nfile;\n• a view of any shared libraries (DLLs, PRXs) used by the program;\n• acallstack for each thread;\n• a region of memory called the heapfor dynamic memory allocation;\n• possiblysomepagesofmemorythatare sharedwithotherprocesses;and\n• a range of kernel space addresses which are inaccessible to the process\n(but become accessible whenever a kernel call executes).\nText, Data and BSS Sections\nWhen a program is first run, the kernel creates a process internally and as-\nsigns it a unique PID. It then sets up a virtual page map for the process—in\notherwords,itcreatesthevirtualaddressspaceoftheprocess. Itthenallocates\nphysical pages as necessary and maps them into the virtual address space by\nadding entries to the process’s page table.\nThekernelreadstheexecutablefile(text, dataandBSSsections)intomem-\nory by allocating virtual pages and loading the data into them. This allows\ntheprogram’scodeandglobaldatatobe“visible”withintheprocess’svirtual\naddress space. The machine code in an executable file is actually relocatable,\nmeaning that its addresses are specified as relative offsets rather than abso-\nlute memory addresses. These relative addresses are fixedup by the operating\nsystem, meaningtheyareconvertedbackintoreal(virtual)addresses,priorto\nrunningtheprogram. (Formoreontheformatofanexecutablefile,seeSection\n3.3.5.1.)\nCall Stack\nEvery running thread needs a call stack (see Section 3.3.5.2). When a process\nisfirstrun,thekernelcreatesasingledefaultthreadforit. Thekernelallocates\nphysical memory pages for this thread’s call stack, and maps them into the\nprocess’ virtual address space so the stack can be “seen” by the thread. The\nvalues of the stack pointer (SP) and base pointer (BP) are initialized to point\nto the bottom of the empty stack. Finally, the thread starts executing at the\n238 4. Parallelism and Concurrent Programming\nentrypoint oftheprogram. (InC/C++thisistypically main() ,orWinMain()\nunder Windows.)\nHeap\nProcessescanallocatememory dynamically viamalloc() andfree() inC,or\nglobal newanddelete inC++. Theserequestscomefromaregionofmemory\ncalled the heap. Physical pages of memory are allocated on demand by the\nkernel in order to fulfill dynamic allocation requests; these pages are mapped\ninto the virtual address space of the process as memory is allocated by it, and\npageswhosecontentshavebeencompletelyfreedareunmappedandreturned\nto the system.\nShared Libraries\nAll non-trivial programs depend on external libraries. A library can be stati-\ncallylinked into a program, meaning that a copyof the libary’s code is placed\ninto the executable file itself. Most operating systems also support the con-\ncept ofshared libraries . In this case, the program contains only references to the\nlibrary’s API functions, not a copy of the library’s machine code. Shared li-\nbraries are called dynamiclinklibraries (DLL) under Windows. On the PlaySta-\ntion 4, the OS supports a kind of dynamically linked library called a PRX.\n(Interestingly, the name PRX comes from the PlayStation 3, where it stood\nforPPURelocatable E xecutable, in reference to the main processor in the PS3\nwhich was called the PPU.)\nShared libraries generally work as follows: The first time a shared library\nis needed by a process, the OS loads that library into physical memory, and\nmaps a view of it into the process’s virtual address space. The addresses of\nthe functions and global variables provided by the shared library are patched\ninto the program’s machine code, allowing it to call them as if they had been\nstatically linked into the executable.\nThe benefit of shared libraries only becomes evident when a second pro-\ncess is run that uses the same shared library. Rather than loading a copyof\nthe library’s code and global variables, the already-loaded physical pages are\nsimply mapped into the virtual address space of the new process. This saves\nmemoryandspeedsuptheprocessofrunningallbutthefirstprocessthatuses\na given shared library.\nShared libraries have other benefits, too. For example, a shared library can\nbe updated, say to fix some bugs, and in theory all programs that use that\nshared library will immediately benefit (without having to be relinked and\nredistributed to users). That said, in practice updating shared libraries can\n4.4. Operating System Fundamentals 239\ninadvertently cause compatibility problems amongst the programs that use\nthem. This leads to a proliferation of different versions of each shared library\nwithin the system—a situation affectionately known as “DLL hell” amongst\nWindows developers. To work around these problems, Windows moved to\na system of manifests that help to guarantee compatibility between shared li-\nbraries and the programs that use them.\nKernel Pages\nOn most operating systems, the address space of a process is actually divided\ninto two large contiguous blocks—user space and kernel space. For example,\non32-bitWindows, userspacecorrespondstotheaddressrangefromaddress\n0x0 through 0x7FFFFFFF (the lower 2 GiB of the address space), while ker-\nnel space corresponds to addresses between 0x80000000 and 0xFFFFFFFF (the\nupper 2 GiB of the space). On 64-bit Windows, user space corresponds to the\n8 TiB range of addresses from 0x0 through 0x7FF’FFFFFFFF, and the gigantic\n248 TiB range from 0xFFFF0800’00000000 through 0xFFFFFFFF’FFFFFFFF is\nreserved for use by the kernel (although not all of it is actually used).\nUser space is mapped through a virtual page table that is unique to each\nprocess. However,kernelspaceusesaseparatevirtualpagetablethatisshared\nbetween all processes. This is done so that all processes in the system have a\nconsistent “view” of the kernel’s internal data.\nNormally, user processes are prevented from accessing the kernel’s\npages—if they try to do so, a page fault will occur and the program will crash.\nHowever, when a user process makes a system call, a context switch (see Sec-\ntion 4.4.6.5) is performed into the kernel. This puts the CPU into privileged\nmode, allowing the kernel to access the kernel space address ranges (as well\nas the virtual pages of the current process). The kernel runs its code in privi-\nleged mode, updates its internal data structures as necessary, and finally puts\nthe CPU back into user mode and returns control to the user program. For\nmore details on how user and kernel space memory mapping works under\nWindows, search for “virtual address spaces” on https://docs.microsoft.com.\nIt’s interesting (and a bit frightening!) to note that the recently-discovered\n“Meltdown”and“Spectre”exploitsmakeuseofaCPU’s out-of-order andspec-\nulativeexecution logic (respectively) to trick it into accessing data located in\nmemory pages that are normally protected from a user-mode process. For\nmore on these exploits and how operating systems are protecting themselves\nagainst them, see https://meltdownattack.com/.\n240 4. Parallelism and Concurrent Programming\nFigure 4.23. A process’s memory map as it might look under 32-bit Windows.\nExample Process Memory Map\nFigure 4.23 depicts the memory map of a process as it might look under 32-bit\nWindows. All of the process’s virtual pages are mapped into user space—\nthe lower 2 GiB of the address space. The executable files’ text, data and BSS\nsegments are mapped at a low memory address, followed by the heap in a\nhigherrange,followedbyanysharedmemorypages. Thecallstackismapped\natthehighendoftheuseraddressspace. Finally,theoperatingsystem’skernel\npages are mapped into the upper 2 GiB of the address space.\nThe actual addresses for each of the regions in the memory map aren’t\npredictable. This is partly because each program’s segments are of different\nsizes and hence will map to different address ranges. Also, the numeric val-\nues of the addresses actually changebetween runs of the same executable pro-\ngram,thankstoasecuritymeasureknownas addressspacelayoutrandomization\n(ASLR).\n4.4. Operating System Fundamentals 241\n4.4.6 Threads\nAthreadencapsulates a running instance of a single stream of machine lan-\nguage instructions. Each thread within a process is comprised of:\n• athread id (TID) which is unique within its process, but may or may not\nbe unique across the entire operating system;\n• the thread’s call stack —a contiguous block of memory containing the\nstack frames of all currently-executing functions;\n• the values of all special- and general-purpose registers4including the\ninstruction pointer (IP), which points at the current instruction in the\nthread’s instruction stream, the base pointer (BP) and stack pointer (SP)\nwhich define the current function’s stack frame;\n• ablockofgeneral-purposememory associatedwitheachthread, known\nasthreadlocal storage (TLS).\nBy default, a process contains a single main thread and hence executes a\nsingle instruction stream. This thread begins executing at the entry point of\ntheprogram—typicallythe main() function. However, allmodernoperating\nsystems are capable of executing more than one concurrent instruction stream\nwithin the context of a single process.\nYou can think of a thread as the fundamentalunitofexecution within the op-\neratingsystem. Athreadprovidestheminimumresourcesrequiredtoexecute\nan instruction stream—a call stack and a set of registers. The process merely\nprovides the environment in which one or more threads execute. This is illus-\ntrated in Figure 4.24.\n4.4.6.1 Thread Libraries\nAlloperatingsystemsthatsupportmultithreadingprovideacollectionofsys-\ntem calls for creating and manipulating threads. A few portable thread li-\nbrariesarealsoavailable, thebestknownofwhicharetheIEEEPOSIX1003.1c\nstandard thread library (pthread) and the C11 and C++11 standard thread li-\nbraries. TheSonyPlayStation4SDKprovidesasetofthreadfunctionsprefixed\nwith scethat map pretty much directly to the POSIX thread API.\nThe various thread APIs differ in their details, but all of them support the\nfollowing basic operations:\n1.Create. A function or class constructor that spawns a new thread.\n4Technically a thread’s execution context only encompasses the values of registers that are vis-\nible inusermode; it excludes the values of certain privileged-mode registers.\n242 4. Parallelism and Concurrent Programming\nProcess\nThread\nExecution \nContext\nRegisters\nCall StackThread\nExecution \nContext\nRegisters\nCall StackFile \nDescriptors\nOther \nResourcesVirtual \nMemory\nHeap\nDLLs\nBSS\nData\nTextThread\nExecution \nContext\nRegisters\nCall Stack\nFigure 4.24. A process encapsulates the resources required to run one or more threads. Each\nthread encapsulates an execution context comprised of the contents of the CPU’s registers and a\ncall stack.\n2.Terminate. A function that terminates the calling thread.\n3.Request to exit. A function that allows one thread to request another\nthread to exit.\n4.Sleep. A function that puts the current thread to sleep for a specified\nlength of time.\n5.Yield. A function that yields the remainder of the thread’s time slice so\nother threads can get a chance to run.\n6.Join. A function that puts the calling thread to sleep until another thread\nor group of threads has terminated.\n4.4.6.2 Thread Creation and Termination\nWhen an executable file is run, the process created by the OS to encapsulate it\nautomatically contains a single thread, and execution of this thread begins at\ntheentrypoint of the program—in C/C++ this is the special function main().\nThis “main thread” can spawn new threads if desired, by making calls to\nan operating system specific function such as pthread_create() (POSIX\nthreads), CreateThread() (Windows), or by instantiating an instance of a\n4.4. Operating System Fundamentals 243\nthread class such as std::thread (C++11). The new thread begins execu-\ntion at an entry point function whose address is provided by the caller.\nOnce created, a thread will continue to exist until it terminates. The execu-\ntion of a thread can terminate in a number of ways:\n• It can end “naturally” by returning from its entry point function. (In the\nspecial case of the main thread, returning from main() not only ends\nthe thread, but also ends the entire process.)\n• Itcancallafunctionsuchas pthread_exit() toexplicitly terminateits\nexecution beforehaving returned from its entry point function.\n• It can be killedexternally by another thread. In this case, the external\nthread makes a requestto cancel the thread in question, but the thread\nmaynotrespondimmediatelytotherequest,oritmayignoretherequest\nentirely. The cancelability of a thread is determined when that thread is\ncreated.\n• It can be forcibly killed because its process has ended. (A process termi-\nnates when the main thread returns from the main() entry point func-\ntion,whenanythreadcalls exit() tokilltheprocessexplicitly,orwhen\nan external actor kills the process.)\n4.4.6.3 Joining Threads\nIt’scommonforonethreadtospawnoneormorechildthreads,dosomeuseful\nworkofitsown,andthenwaitforthechildthreadstobedonewiththeirwork\nbefore continuing. For example, let’s say the main thread wants to perform\n1000 computations, and let’s assume further that this program is running on a\nquad-core machine. The most efficient approach would be to divide the work\ninto four equally-sized chunks, and spawn four threads to do the processing\nin parallel. Once the computations are complete, let’s assume that the main\nthread wants to perform a checksum on the results. The resulting code might\nlook something like this:\nComputationResult g_aResult[1000];\nvoid Compute(void* arg)\n{\nuintptr_t startIndex = (uintptr_t)arg;\nuintptr_t endIndex = startIndex + 250;\nfor (uintptr_t i = startIndex; i < endIndex; ++i)\n{\ng_aResult[i] = ComputeOneResult(...);\n244 4. Parallelism and Concurrent Programming\n}\n}\nvoid main()\n{\npthread_t tid[4];\nfor (int i = 0; i < 4; ++i)\n{\nconst uintptr_t startIndex = i * 250;\npthread_create (&tid[i], nullptr,\nCompute, (void*)startIndex);\n}\n// perhaps do some other useful work...\n// wait for computations to be done\nfor (int i = 0; i < 4; ++i)\n{\npthread_join (&tid[i], nullptr);\n}\n// all threads are done, so we can do our checksum\nunsigned checksum = Sha1(g_aResult,\n1000*sizeof(ComputationResult));\n// ...\n}\n4.4.6.4 Polling, Blocking and Yielding\nNormallly a thread runs until it terminates. But sometimes a running thread\nneedstowaitforsomefutureeventtooccur. Forexample,athreadmightneed\nto wait for a time-consuming operation to complete, or for some resource to\nbecome available. In such a situation, we have three options:\n1. The thread can poll,\n2. it can block, or\n3. it can yieldwhile polling.\nPolling\nPollinginvolves a thread sitting in a tight loop, waiting for a condition to be-\n4.4. Operating System Fundamentals 245\ncome true. It’s a bit like kids in the back seat on a road trip, repeatedly asking,\n“Are we there yet? Are we there yet?” Here’s an example:\n// wait for condition to become true\nwhile (!CheckCondition())\n{\n// twiddle thumbs\n}\n// the condition is now true and we can continue...\nIt should be obvious that this approach, while simple, has the potential of\nburning CPU cycles unnecessarily. This approach is sometimes called a spin-\nwaitor abusy-wait.\nBlocking\nIf we expect our thread to wait for a relatively long period of time for a con-\ndition to become true, busy-waiting is not a good option. Ideally we’d like\nto put our thread to sleep so that it doesn’t waste CPU resources, and rely on\nthe kernel to wake it back up when the condition becomes true at some future\ntime. This is called blocking the thread.\nA thread blocks by making a special kind of operating system call known\nas ablockingfunction. If the condition is already true at the moment a blocking\nfunction is called, the function won’t actually block—it will simply return im-\nmediately. But if the condition is false, the kernel will put the thread to sleep,\nandaddthethreadandtheconditiononwhichitiswaitingintoatable. Later,\nwhentheconditionbecomestrue,thekernelusesthisinternaltabletoidentify\nand wake up any threads that are waiting on that condition.\nThere are all sorts of OS functions that block. Here are a few examples:\n•Openingafile. Mostfunctionsthatopenafilesuchas fopen() willblock\nthecallingthreaduntilthefilehasactuallybeenopened(whichmaytake\nhundredsoreventhousandsofcycles). Somefunctions,like open() un-\nder Linux, offer a non-blocking option (O_NONBLOCK) to support asyn-\nchronous file I/O.\n•Explicit sleeping . Some functions explicitly put the calling thread to\nsleep for a specified length of time. Variants include usleep() (Linux),\nSleep() (Windows) std::this_thread::sleep_until() (C++11\nstandard library) and pthread_sleep() (POSIX threads).\n•Joiningwithanotherthread . A function such as pthread_join() blocks\nthe calling thread until the thread being waited on has terminated.\n246 4. Parallelism and Concurrent Programming\n•Waiting for a mutex lock . Functions like pthread_mutex_wait() at-\ntempt to obtain an exclusive lock on a resource via an operating system\nobject known as a mutex(see Section 4.6). If no other thread holds the\nlock, the function grants the lock to the calling thread and returns im-\nmediately; otherwise, the calling thread is put to sleep until the lock can\nbe obtained.\nOperating system calls aren’t the only functions that can block. Any user-\nspace function that ultimately calls a blocking OS function is itself considered\ntobeablockingfunction. It’sagoodideatodocumentsuchafunction, sothat\nthe programmers who use it will know that it has the potential to block.\nYielding\nThis technique falls part-way between polling and blocking. The thread polls\nthe condition in a loop, but on each iteration it relinquishes the remain-\nder of its time slice by calling pthread_yield() (POSIX), Sleep(0) or\nSwitchToThread() (Windows), or an equivalent system call.\nHere’s an example:\n// wait for condition to become true\nwhile (!CheckCondition())\n{\n// yield the remainder of my time slice\npthread_yield (nullptr);\n}\n// the condition is now true and we can continue...\nThis approach tends to result in fewer wasted cycles and better power con-\nsumption than a pure busy-wait loop.\nYielding the CPU still involves a kernel call, and is therefore quite expen-\nsive. Some CPUsprovidealightweight “pause”instruction. (Forexample, on\nan Intel x86 ISA with SSE2, the _mm_pause() intrinsic emits such an instruc-\ntion.) This kind of instruction reduces the power consumption of a busy-wait\nloop by simply waiting for the CPU’s instruction pipeline to empty out before\nallowing execution to continue:\n// wait for condition to become true\nwhile (!CheckCondition())\n{\n// Intel SSE2 only:\n// reduce power consumption by pausing for ~40 cycles\n_mm_pause ();\n4.4. Operating System Fundamentals 247\n}\n// the condition is now true and we can continue...\nSeehttps://software.intel.com/en-us/comment/1134767andhttp://software.\nintel.com/en-us/forums/topic/309231foranin-depthdiscussionofhowand\nwhy to use a pause instruction in a busy-wait loop.\n4.4.6.5 Context Switching\nEvery thread maintained by the kernel exists in one of three states:5\n•Running. The thread is actively running on a core.\n•Runnable . Thethreadisabletorun,butitiswaitingtoreceiveatimeslice\non a core.\n•Blocked. Thethreadisasleep,waitingforsomeconditiontobecometrue.\nAcontextswitch occurs whenever the kernel causes a thread to transition from\none of these states to another.\nA context switch always happens in privileged mode on the CPU—in re-\nsponse to the hardware interrupt that drives preemptive multitasking (i.e.,\ntransitions between Running and Runnable), in response to an explicit block-\ning kernel call made by a running thread (i.e., a transition from Running\nor Runnable to Blocked), or in response to a waited-on condition becoming\ntrue, thus “waking” a sleeping thread (i.e., transitioning it from Blocked to\nRunnable). The kernel’s thread state machine is illustrated in Figure 4.25.\nWhen a thread is in the Running state, it is actively making use of a CPU\ncore. Thecore’sregisterscontaininformationpertinenttotheexecutionofthat\nthread,suchasitsinstructionpointer(IP),stackandbasepointers(SPandBP),\nand the contents of various general-purpose registers (GPRs). The thread also\nmaintains a call stack, which stores local variables and return addresses for\nthecurrently-runningfunctionandtheentirestackoffunctionsthatultimately\ncalled it. Together, this information is known as the thread’s executioncontext.\nWhenever a thread transitions away from the Running state to either\nRunnable or Blocked, the contents of the CPU’s registers are saved to a mem-\nory block that has been reserved for the thread by the kernel. Later, when a\nRunnable thread transitions back to the Running state, the kernel repopulates\nthe CPU’s registers with that thread’s saved register contents.\n5Some operating systems make use of additional states, but such states are implementation\ndetails that we can safely ignore for our purposes.\n248 4. Parallelism and Concurrent Programming\nSchedule\nEnd of \nQuantumSleep/BlockWakeBlocked \nThreadsRunnable \nThreads\nThread 3Thread 0Thread 1\nThread 7Thread 6Thread 4\nRunning \nThreads\nThread 2\nCore 0\nALU\nFPU\nVPUL1 I$L1 D$ F/D\nScheduler\nRegistersThread 5\nCore 1\nALU\nFPU\nVPUL1 I$L1 D$ F/D\nScheduler\nRegisters\nFigure 4.25. Every thread can be in one of three states: Running, Runnable or Blocked.\nWeshouldnoteherethatathread’scallstackneednotbesavedorrestored\nexplicitly during a context switch. This is because each thread’s call stack al-\nready resides in a distinct region within its process’ virtual memory map. The\nact of saving and restoring the contents of the CPU’s registers includes saving\nand restoring the stack and base pointers (SP and BP), and thereby effectively\nsaves and restores the thread’s call stack “for free.”\nDuring a context switch, if the incoming thread resides in a different pro-\ncessfromthatoftheoutgoingthread,thekernelalsoneedstosaveoffthestate\nof the outgoing process’ virtual memory map, and set up the virtual memory\nmap of the incoming process. You’ll recall from Section 3.5.2 that a virtual\nmemory map is defined by a virtual page table. The act of saving and restor-\ningavirtualmemorymapthereforeinvolvessavingandrestoringapointerto\nthis page table, which is usually maintained in a special privileged CPU reg-\nister. The translation lookaside buffer (TLB) must also be flushed whenever\nan inter-process context switch occurs (see Section 3.5.2.4). These additional\nsteps make context switching between processes more expensive than context\nswitching between threads within a single process.\n4.4.6.6 Thread Priorities and Afﬁnity\nFor the most part, the kernel handles the job of scheduling threads to run on\ntheavailablecoresinthemachine. However, programmersdohavetwoways\n4.4. Operating System Fundamentals 249\nto affect how threads are scheduled: priority andaffinity.\nA thread’s priority controls how it is scheduled relative to other Runnable\nthreadsinthesystem. Higher-prioritythreadsgenerallytakeprecedenceover\nlower-prioritythreads. Differentoperatingsystemsofferdifferentnumbersof\nprioritylevels. Forexample,Windowsthreadscanbelongtooneofsixpriority\nclasses,andtherearesevendistinctprioritylevelswithineachclass. Thesetwo\nvalues are combined to produce a total of 32 distinct “base priorities” which\nare used when scheduling threads.\nThe simplest thread scheduling rule is this: As long as at least one higher-\npriority Runnable thread exists, no lower-priority threads will be scheduled\nto run. The idea behind this approach is that most threads in the system will\nbe created at some default priority level, and hence will share the process-\ning resources fairly. But once in a while, a higher-priority thread can become\nRunnable. Whenitdoes, itrunsasclosetoimmediatelyaspossible, hopefully\nexits after a relatively short period of time, and thereby returns control to all\nof the lower-priority threads.\nSuch a simple priority-based scheduling algorithm can lead to a situation\ninwhichasmallnumberofhigh-prioritythreadsruncontinually,therebypre-\nventing any lower-priority threads from running. This is known as starvation.\nSome operating systems attempt to mitigate the ill effects of starvation by in-\ntroducing exceptions to the simple scheduling rule that aim to give at least\nsome CPU time to starving lower-priority threads.\nAnother way in which programmers can control thread scheduling is via\na thread’s affinity. This setting requests that the kernel either lock a thread to a\nparticularcore,orthatitshouldatleast preferoneormorecoresovertheothers\nwhen scheduling the thread.\n4.4.6.7 Thread Local Storage\nWe said that all threads within a process share the process’s resources, includ-\ning its virtual memory space. There is one exception to this rule—each thread\nis given a private memory block known as thread local storage (TLS). This al-\nlows threads to keep track of data that shouldn’t be shared with other pro-\ncesses. For example, each thread might maintain a private memory allocator.\nWecanthinkoftheTLSmemoryblockasbeingapartofthethread’sexecution\ncontext.\nIn practice, TLS memory blocks are usually visible to all threads within\na process. They’re typically not protected, the way operating system virtual\nmemory pages are. Instead, the OS grants each thread its own TLS block, all\nmapped into the process’s virtual address space at different numerical ad-\ndresses, and a system call is provided that allows any one thread to obtain\n250 4. Parallelism and Concurrent Programming\nFigure 4.26. The Threads window in Visual Studio is the primary interface for debugging multithreaded programs.\nthe address of its private TLS block.\n4.4.6.8 Thread Debugging\nAllgooddebuggersnowadaysprovidetoolsfordebuggingmultithreadedap-\nplications. In Microsoft Visual Studio, the Threads Window is the central tool\nfor this purpose. Whenever you break into the debugger, this window lists\nall the threads currently in existence within the application. Double-clicking\non a thread makes its execution context active within the debugger. Once a\nthread’s context has been activated, you can walk up and down its call stack\nvia the Call Stack window, and view local variables within each function’s\nscopeviatheWatchwindows. ThisworksevenifthethreadisinitsRunnable\nor Blocked state. The Visual Studio Threads window is shown in Figure 4.26.\n4.4.7 Fibers\nIn preemptive multitasking, thread scheduling is handled automatically by\nthe kernel. This is often convenient, but sometimes programmers find it de-\nsirable to have control over the scheduling of workloads in their programs.\nFor example, when implementing a jobsystem for a game engine (discussed in\nSection 8.6.4), we might want to allow jobs to explicitly yield the CPU to other\njobs, without worrying about the possibility of preemption “pulling the rug\nout” from under our jobs as they run. In other words, sometimes we want to\nusecooperative rather than preemptive multitasking.\n4.4. Operating System Fundamentals 251\nSome operating systems provide just such a cooperative multitasking\nmechanism: They are known as fibers. A fiber is a lot like a thread, in that it\nrepresents arunninginstanceofastreamofmachinelanguageinstructions. A\nfiberhasacallstackandregisterstate(anexecutioncontext),justlikeathread.\nHowever, the big difference is that a fiber is never scheduled directly by the\nkernel. Instead, fibers run within the context of a thread, and are scheduled\ncooperatively, by each other.\nInthissection,we’lltalkaboutWindowsfibersspecifically. Someotherop-\nerating systems, such as Sony’s PlayStation 4 SDK, provide very similar fiber\nAPIs.\n4.4.7.1 Fiber Creation and Destruction\nHow do we convert a thread-based process into a fiber-based one? Every\nprocess starts with a single thread when it first runs; hence processes are\nthread-basedbydefault. Whenathreadcallsthefunction ConvertThreadTo\nFiber(), a new fiber is created within the context of the calling thread. This\n“bootstraps” the process so that it can create and schedule more fibers. Other\nfibers are created by calling CreateFiber() and passing it the address of\na function that will serve as its entry point. Any running fiber can coopera-\ntively schedule a different fiber to run within its thread by calling SwitchTo\nFiber(). When a fiber is no longer needed, it can be destroyed by calling\nDeleteFiber().\n4.4.7.2 Fiber States\nA fiber can be in one of two states: Active or Inactive. When a fiber is in its\nActivestate, itisassignedtoathread, andexecutesonitsbehalf. Whenafiber\nisinitsInactivestate,itissittingonthesidelines,notconsumingtheresources\nof any thread, just waiting to be activated. Windows calls an Active fiber the\n“selected” fiber for a given thread.\nAn Active fiber can deactivate itself and make another fiber active by call-\ningSwitchToFiber() . This is the only way that fibers can switch between\nthe Active and Inactive states.\nWhether or not an Active fiber is actively executing on a CPU core is de-\ntermined by the state of its enclosing thread. When an Active fiber’s thread is\nin the Running state, that fiber’s machine language instructions are being ex-\necuted on a core. When an Active fiber’s thread is in the Runnable or Blocked\nstate, its instructions of course cannot execute, because the entire thread is sit-\nting on the sidelines, either waiting to be scheduled on a core or waiting for a\ncondition to become true.\n252 4. Parallelism and Concurrent Programming\nIt’s important to understand that fibers don’t themselves have a Blocked\nstate, the way threads do. In other words, it’s not possible to put a fiber to\nsleep waiting on a condition. Only its thread can be put to sleep. Because of\nthis restriction, whenever a fiber needs to wait for a condition to become true,\nit either busy-waits or it calls SwitchToFiber in order to yield control to an-\nother fiber while it waits. Making a blocking OS call from within a fiber is\nusually a pretty big no-no. Doing so would put the fiber’s enclosing thread to\nsleep, thereby preventing that fiber from doing anything—including schedul-\ning other fibers to run cooperatively.\n4.4.7.3 Fiber Migration\nA fiber can migrate from thread to thread, but only by passing through its\nInactive state. As an example, consider a fiber F that is running within the\ncontext of thread A. Fiber F calls SwitchToFiber(G) to activate a different\nfibernamedGinsidethreadA.ThisputsfiberFintoitsInactivestate(meaning\nit is no longer associated with any thread). Now let’s assume that another\nthread named B is running fiber H. If fiber H calls SwitchToFiber(F), then\nfiber F has effectively migrated from thread A to thread B.\n4.4.7.4 Debugging with Fibers\nBecause fibers are provided by the OS, debugging tools and profiling tools\nshouldbeableto“see”them,justthewaytheycan“see”threads. Forexample,\nwhen debugging on the PS4 using SN Systems’ Visual Studio debugger plug-\nin for Clang, fibers automatically show up in the Threads window as if they\nwere threads. You can double-click a fiber to activate it within the Watch and\nCall Stack windows, and then walk up and down its call stack just as you\nnormally would with a thread.\nIf you’re considering using fibers in your game engine, it’s a good idea\nto check out your debugger’s capabilities on your target platform before you\ncommit a lot of time and effort to a fiber-based design. If your debugger\nand/or target platform doesn’t provide good tools for debugging fibers, that\ncould be a deal breaker.\n4.4.7.5 Further Reading on Fibers\nYou can read more about Windows fibers here: https://msdn.microsoft.com/\nen-us/library/windows/desktop/ms682661(v=vs.85).aspx.\n4.4. Operating System Fundamentals 253\n4.4.8 User-Level Threads and Coroutines\nBoth threads and fibers tend to be rather “heavy weight” because these fa-\ncilities are provided by the kernel. This implies that most functions that\nyou’d call to manipulate threads or fibers involve a context switch into kernel\nspace—not a cheap operation. But there are lighter-weight alternatives to\nthreads and fibers. These mechanisms allow programmers to code in terms\nofmultipleindependentflowsofcontrol, eachwithitsownexecutioncontext,\nbut without the high cost of making kernel calls. Collectively, these facilities\nare known as user-level threads .\nUser-level threads are implemented entirely in user space. The kernel\nknows nothing about them. Each user-level thread is represented by an or-\ndinary data structure that keeps track of the thread’s id, possibly a human-\nreadable name, and execution context information (the contents of CPU reg-\nisters and a call stack). A user-level thread library provides API functions for\ncreating and destroying threads, and context switching between them. Each\nuser-level thread runs within the context of a “real” thread or fiber that has\nbeen provided by the operating system.\nThetricktoimplementingauser-levelthreadlibraryisfiguringouthowto\nimplementacontextswitch. Ifyouthinkaboutit,acontextswitchmostlyboils\ndown to swapping the contents of the CPU’s registers. After all, the registers\ncontainalloftheinformationneededtodescribeathread’sexecutioncontext—\nincluding the instruction pointer and the call stack. So by writing some clever\nassemblylanguagecode,it’spossibletoimplementacontextswitch. Andonce\nyou have a context switch, the rest of your user-level thread library is nothing\nmore than data management.\nUser-level threads aren’t supported very well in C and C++, but some\nportable and non-portable solutions do exist. POSIX provided a collec-\ntion of functions for managing lightweight thread execution contexts via its\nucontext.h headerfile(https://en.wikipedia.org/wiki/Setcontext),butthis\nAPI has since been deprecated. The C++ Boost library provides a portable\nuser-level thread library. (Search for “context” on http://www.boost.org/ for\ndocumentation on this library.)\n4.4.8.1 Coroutines\nCoroutinesareaparticulartypeofuser-levelthreadthatcanproveveryuseful\nfor writing inherently asynchronous programs, like web servers—and games!\nA coroutine is a generalization of the concept of a subroutine. Whereas a sub-\nroutine can only exit by returning control to its caller, a coroutine can also\nexit byyielding to another coroutine. When a coroutine yields, its execution\n254 4. Parallelism and Concurrent Programming\ncontext is maintained in memory. The next time the coroutine is called(by\nbeing yielded toby some other coroutine) it continues from where it left off.\nSubroutines call each other in a heirarchical fashion. Subroutine A calls\nB, which calls C, which returns to B, which returns to A. But coroutines call\neach other symmetrically. Coroutine A can yield to B, which can yield to A,\nad infinitum. This back and forth calling pattern doesn’t lead to an infinitely\ndeepening call stack, because each coroutine maintains its own private execu-\ntion context (call stack and register contents). Thus yielding from coroutine\nA to coroutine B acts more like a context switch between threads than like a\nfunction call. But because coroutines are implemented with user-level threads,\nthese context switches are very efficient.\nHere’s a pseudocode example of a system in which one coroutine continu-\nally produces data that is consumed by another coroutine:\nQueue g_queue;\ncoroutine void Produce()\n{\nwhile (true)\n{\nwhile (!g_queue.IsFull())\n{\nCreateItemAndAddToQueue(g_queue);\n}\nYieldToCoroutine (Consume);\n// continues from here on next yield...\n}\n}\ncoroutine void Consume()\n{\nwhile (true)\n{\nwhile (!g_queue.IsEmpty())\n{\nConsumeItemFromQueue(g_queue);\n}\nYieldToCoroutine (Produce);\n// continues from here on next yield...\n}\n}\nCoroutinesaremostoftenprovidedbyhigh-levellanguageslikeRuby,Lua\n4.4. Operating System Fundamentals 255\nand Google’s Go. It’s also possible to use coroutines in C or C++. The C++\nBoostlibraryprovidesasolidimplementationofcoroutines,butBoostrequires\nyoutocompileandlinkagainstaprettyhugecodebase. Ifyouwantsomething\nleaner, you may want to try rolling your own coroutine library. The following\nblogpostbyMalteSkarupkedemonstratesthatdoingsoisn’tquiteasonerous\nataskasyoumightatfirstimagine: https://probablydance.com/2013/02/20/\nhandmade-coroutines-for-windows/.\n4.4.8.2 Kernel Threads versus User Threads\nThe term “kernel thread” has two very different meanings, and this can be-\ncome a major source of confusion as you read more about multithreading. So\nlet’s demystify the term. The two definitions are as follows:\n1. OnLinux,a“kernelthread”isaspecialkindofthreadcreatedforinternal\nuse by the kernel itself, which runs only while the CPU is in privileged\nmode. The kernel also creates threads for use by processes (via an API\nsuch as pthread or C++11’s std::thread ). These threads run in user\nspacewithinthecontextofaprocess. Inthissenseoftheterm,anythread\nthatrunsinprivilegedmodeisakernelthread, andanythreadthatruns\nin user mode (in the context of a single-threaded or multithreaded pro-\ncess) is a “user thread.”\n2. The term “kernel thread” can also be used to refer to any thread that\nisknown to andscheduled by the kernel. Using this definition, a kernel\nthread can execute in either kernel space or user space, and the term\n“user thread” only applies to a flow of control that is managed entirely\nbyauser-spaceprogram without thekernelbeinginvolvedatall, suchas\na coroutine.\nUsing definition #2, a fiber blurs the line between “kernel thread” and “user\nthread.” On the one hand, the kernel is aware of fibers and maintains a sepa-\nrate call stack for each one. On the other hand, a fiber is not scheduled by the\nkernel—it can run only when another fiber or thread explicitly hands control\nto it via a call such as SwitchToFiber().\n4.4.9 Further Reading on Processes and Threads\nWe’ve covered the basics of processes, threads and fibers in the preceding sec-\ntions, but really we’ve only just scratched the surface. For more information,\ncheck out some of the following websites:",54672
29-4.5 Introduction to Concurrent Programming.pdf,29-4.5 Introduction to Concurrent Programming,"256 4. Parallelism and Concurrent Programming\n• For an introduction to threads, see https://www.cs.uic.edu/~jbell/\nCourseNotes/OperatingSystems/4_Threads.html.\n• The full pthread API docs are available online; just search for “pthread\ndocumentation.”\n• For documentation on the Windows thread API, seach for “Process and\nThread Functions” on https://msdn.microsoft.com/.\n• For more information on thread scheduling, search for Nikita Ishkov’s\n“A Complete Guide to Linux Process Scheduling” online.\n• ForagreatintroductiontoGo’simplementationofcoroutines(whichare\nknown as “goroutines”), watch this presentation by Rob Pike: https://\nwww.youtube.com/watch?v=f6kdp27TYZs.\n4.5 Introduction to Concurrent Programming\nThere’s a wide range of explicitly parallel computing hardware out there, but\nhow can we take advantage of it as programmers? The answer lies in con-\ncurrentprogramming techniques. In concurrent software, a workload is broken\ndown into two or more flows of control that can run semi-independently. As\nwe saw in Section 4.1, in order for a system to qualify as concurrent, it must\ninvolve multiple readers and/or multiple writers of shareddata.\nRob Pike, a Distinguished Engineer at Google Inc. who specializes in dis-\ntributed and concurrent systems and programming languages, defines con-\ncurrencyas“thecompositionofindependentlyexecutingcomputations.” This\ndefinition underscores the idea that the multiple flows of control in a concur-\nrent system normally operate semi-independently, but their computations are\ncomposed bysharingdata andbysynchronizing theiroperationsinvariousways.\nConcurrency can take many forms. Some examples include:\n• a piped chain of commands running under Linux or Windows, such as\ncat render.cpp | grep ""light"",\n• asingleprocesscomprisedofmultiplethreadsthatshareavirtualmem-\nory space and operate on a common dataset,\n• athreadgroupcomprisedofthousandsofthreadsrunningonaGPU,all\ncooperating to render a scene,\n• amultiplayervideogame,sharingacommongamestatebetweenclients\nrunning on multiple PCs or game consoles.\n4.5. Introduction to Concurrent Programming 257\n4.5.1 Why Write Concurrent Software?\nConcurrent programs are sometimes written because a model of multiple\nsemi-independent flows of control simply matches the problem better than\na single flow-of-control design. A concurrent design might also be chosen to\nbestmakeuseofamulticorecomputingplatform, eveniftheproblemathand\nmight be more naturally suited to a sequential design.\n4.5.2 Concurrent Programming Models\nIn order for the various threads within a concurrent program to cooperate,\nthey need to share data, and they need to synchronize their activities. In other\nwords, they need to communicate. There are two basic ways in which concur-\nrent threads can communicate:\n•Message passing. In this communication mode, concurrent threads pass\nmessages between one another in order to share data and synchronize\ntheir activities. The messages might be sent across a network, passed\nbetween processes using a pipe, or transmitted via a message queue in\nmemory that is accessible to both sender and receiver. This approach\nworks both for threads running on a single computer (either within a\nsingle process or across multiple processes), and for threads within pro-\ncessesrunningonphysicallydistinctcomputers(e.g.,acomputercluster\nor a grid of machines spread across the globe).\n•Shared memory . In this communication mode, two or more threads are\ngranted access to the same block of physical memory, and can therefore\noperate directly on any data objects residing in that memory area. Di-\nrect access to shared memory only works when all threads are running\non a single computer with a bank of physical RAM that can be “seen”\nby all CPU cores. Threads within a single process always share a vir-\ntualaddressspace, sotheycansharememory“forfree.” Threadswithin\ndifferent processes can also share memory by mapping certain physical\nmemory pages into all of the processes’ virtual address spaces.\nIt’s interesting to note that the illusion of shared memory between phys-\nically separate computers can be implemented on top of a message-passing\nsystem—this technique is known as distributed shared memory. Likewise, a\nmessage-passing mechanism can be implemented on top of a shared mem-\nory architecture, by implementing a message queue that resides in the shared\nmemory pool.\n258 4. Parallelism and Concurrent Programming\nEachapproachhasitsprosandcons. Physically-sharedmemoryisthemost\nefficient way to share a large amount of data, because that data doesn’t have\nto be copied for transmission between threads. On the other hand, as we’ll\nsee in Sections 4.5.3 and 4.7, the sharing of resources of any kind (memory or\nother resources) brings with it a host of synchronization problems that tend\nto be difficult to reason about, and are very tricky to account for in a manner\nthat guarantees correctness of the program. A message-passing design tends\nto lessen the impacts of (but not eliminate) these kinds of problems.\nIn this book, we’ll focus primarily on shared memory concurrency. We do\nthisfor tworeasons: First, this isthekind ofconcurrencyyou’remostlikely to\nencounter as a game progammer, because game engines are typically imple-\nmented as single-process multithreaded programs. (Networked multiplayer\ngames are a notable exception to this rule, since they make heavy use of mes-\nsagepassing.) Second,sharedmemoryconcurrencyisamoredifficulttopicto\nget your head around. Once you understand concurrency within a shared\nmemory environment, message passing techniques should prove relatively\neasy to learn.\n4.5.3 Race Conditions\nAracecondition is defined as any situation in which the behavior of a program\nis dependent on timing. In other words, in the presence of a race condition,\nthe behavior of the program can changewhen the relative sequence of events\noccurring across the system changes, due to variability in the lengths of time\ntaken by the various flows of control to perform their tasks.\n4.5.3.1 Critical Races\nSometimes race conditions are innocuous—the behavior of the program may\nchange somewhat depending on timing, but the race causes no ill effects. On\nthe other hand, a critical race is a race condition that has the potential to cause\nincorrect program behavior.\nThe kinds of bugs caused by critical races often seem “strange” or even\n“impossible” to programmers who aren’t experienced with them. Examples\ninclude:\n• intermittent or seemingly random bugs or crashes,\n• incorrect results,\n• data structures that get into corrupted states,\n• bugs that magically disappear when you switch to a debug build,\n4.5. Introduction to Concurrent Programming 259\n• bugs that are around for a while, and then go away for a few days, only\nto return again (usually the night before E3!),\n• bugs that go away when logging (a.k.a., “printf() debugging”) is\nadded to the program in an attempt to discover the source of the prob-\nlem.\nProgrammers often call these kinds of issues Heisenbugs.\n4.5.3.2 Data Races\nAdata race is a critical race condition in which two or more flows of control\ninteferewithoneanotherwhilereadingand/orwritingablockofshareddata,\nresulting in data corruption. Data races are thecentral problem of concurrent\nprogramming. Writingconcurrentprogramsalwaysboilsdowntoeliminating\ndataraces,eitherbycarefullycontrollingaccesstoshareddata,orbyreplacing\nshared data with private, independent copies of data (thereby transforming a\nconcurrency problem into a sequential programming problem).\nTo better understand data races, consider the following simple snippet of\nC/C++ code:\nint g_count = 0;\ninline void IncrementCount()\n{\n++g_count;\n}\nIf you compiled this code for an Intel x86 CPU and viewed the disassembly, it\nwould look something like this:\nmov eax,[g_count] ; read g_count into register EAX\ninc eax ; increment the value\nmov [g_count],eax ; write EAX back into g_count\nThis is an example of a read-modify-write (RMW) operation.\nNow imagine that two threads A and B were both to call the Increment\nCount() function concurrently (either in parallel or via preemptive multi-\nthreading). Undernormaloperation, ifeachthreadcalledthefunctionexactly\nonce, we’d expect the final value of g_count to be 2, because either thread A\nincrements g_count and then thread B increments it, or vice-versa. This is\nillustrated in Table 4.1.\nNext let’s consider the case of our two threads running on a single-core\nmachine with preemptive multitasking. Let’s say that thread A runs first, and\n260 4. Parallelism and Concurrent Programming\nThread A Thread B Value of\nAction EAX Action EAX g_count\n? ? 0\nRead 0 ? 0\nIncrement 1 ? 0\nWrite 1 ? 1\n1 Read 1 1\n1Increment 2 1\n1 Write 2 2\nTable 4.1. Example of correct operation of a simple two-threaded concurrent program. First thread\nA reads the contents of a shared variable, increments the value, and writes the results back into\nthe shared variable. At a later time, thread B does the same steps. The ﬁnal value of the shared\nvariable is 2 as expected.\nThread A Thread B Value of\nAction EAX Action EAX g_count\n? ? 0\nRead 0 ? 0\n0 Read 0 0\n0Increment 1 0\n0 Write 1 1\nIncrement 1 1 1\nWrite 1 1 1\nTable 4.2. Example of a race condition. Thread A reads the value of the shared variable, but then\nis preempted by thread B, which also reads the (same) value. By the time both threads have incre-\nmented the value and written their results back to the shared variable, the global variable contains\nthe incorrect value 1 instead of the exected value of 2.\nhas just finished executing the first movinstruction when a context switch to\nthread B occurs. Instead of thread A executing its incinstruction, thread B\nruns its first movinstruction. After some time thread B’s quantum expires,\nand the kernel context switches back to thread A, which continues where it\nleft off and executes the incinstruction. Table 4.2 illustrates what happens.\nHint: It’s not good! The final value of g_count is no longer 2 as it should be.\n4.5. Introduction to Concurrent Programming 261\nmodify read write modify read write ...\nTimeCore\nmodify read writemodify read write\n...... Core 0\nCore 1\nTime\nmodify read writemodify read write\n...... Core 0\nCore 1\nTimeA  d a e r h T B  d a e r h T A  d a e r h T\nThread A\nThread B\nThread A\nThread B\nFigure 4.27. Three ways in which a data race can occur within a read-modify-write operation. Top:\nTwo threads racing on a single CPU core. Middle: Two threads overlapping on two separate cores\nand offset by one instruction. Bottom: Two threads overlapping in perfect synchronization on\ntwo cores.\nIfwerunourtwothreadsonparallelhardware,asimilarbugcanoccur,al-\nthough for a slightly different reason. As in the single-core case, we might get\nlucky: Thetworead-modify-writeoperationsmightnotoverlapatall,andthe\nresult will be correct. However, if the two read-modify-write operations over-\nlap, either offset from one another or in perfect synchronization, both threads\ncan end up loading the same value of g_count into their respective EAX reg-\nisters. Both will increment the value, and both will write it to memory. One\nthread will overwrite the results of the other, but it doesn’t really matter—\nbecause they both loaded the same initial value, the final value of g_count\nwillendupbeingincorrect,justasitwasinthesingle-corescenario. Thethree\ndata race scenarios (preemption, offset overlap, and perfect synchronization)\nare illustrated in Figure 4.27.\n4.5.4 Critical Operations and Atomicity\nWhenever one operation is interrupted by another, we have the potential for\na data race bug. However, not all interruptions actually cause bugs. For ex-\nample, if a thread is performing an operation on a chunk of data that can only\nbe “seen” by that one thread, no data race can occur. Such an operation can\nbe interrupted at any moment by any other operation without consequence.\n262 4. Parallelism and Concurrent Programming\nB A Thread 0\nD C Thread 1\nF E Thread 2\nFigure 4.28. Because each step in an algorithm takes a ﬁnite amount of time to be performed, it\nbecomes difﬁcult to answer questions about the relative ordering of the steps in a multithreaded\nprogram. For example, does operation B happen before or after operation C?\nLikewise,ifanoperationononedataobjectisinterruptedbyanoperationona\ndifferent object, there’s no way for those two operations to interefere with one\nanother,6and hence no data race bugs are possible. Data race bugs only occur\nwhen an operation on a shared object is interrupted by another operation on\nthatsameobject. So we can only talk meaningfully about data races in relation\nto a particular shared data object.\nLet’s use the term critical operation to refer to any operation that can possi-\nbly read or mutate one particular shared object. To guarantee that the shared\nobject is free from data race bugs, we must ensure that none of its critical op-\nerations can interrupt one another. When a critical operation is made uninter-\nruptable in this manner, it is called an atomic operation. Alternatively, we can\nsay that such an operation has the property of atomicity .\n4.5.4.1 Invocation and Response\nWhen we first learn to program, we’re usually taught that the timerequired\nto perform each step in an algorithm is not relevant to the correctness of the\nalgorithm—all that matters is that the steps are performed in the proper order.\nThis simple model works well for sequential (single-threaded) programs. But\nin the presenceof multiple threads, it’s not possible to define the order of a set\nof operations when those operations each have a finite duration. This idea is\nillustrated in Figure 4.28.\nIn a concurrent system, the only way to define the notion of orderis to\nrestrict ourselves to talking about instantaneous events . Given any pair of in-\nstantaneous events, there are only three possibilities: event A happens before\nevent B, event A happens after event B, or the two events are simultaneous.\n(Perfectlysimultaneouseventsarerare,buttheycanoccurinamulticorecom-\nputer in which some or all cores share a synchronized clock.)\nAny operation with a finite duration can be broken down into two instan-\ntaneousevents—its invocation (themomentatwhichtheoperationbegins)and\n6This is only strictly true if the two objects reside on different cache lines.\n4.5. Introduction to Concurrent Programming 263\nFigure 4.29. Any code snippet that includes a critical operation can be partitioned into three sec-\ntions. The critical operation itself is bounded above by its invocation, and below by its response.\nFigure 4.30. Another example, this time in assembly language, of a code snippet partitioned into\nthree sections, bounded by the invocation and response of a critical operation.\nitsresponse (the moment at which it is considered to be complete). When we\nlookatanycodesnippetthatincludesacriticaloperationonsomeshareddata\nobject,wecanthusdivideitintothreesections,withtheinstantaneousinvoca-\ntion and response events demarking the boundaries between them. Note that\nwe’retalkinghereabouttheorderinwhichevents occur astheywerewritten in\nthe source code—this is known as programorder.\n•Preamblesection: All code that occurs before the critical operation’s invo-\ncation, in program order.\n•Criticalsection: The code that comprises the critical operation itself.\n•Postamble section: All code that occurs after the critical operation’s re-\nsponse, in program order.\nThis notion of partitioning a block of code into three sections is illustrated in\nFigures 4.29 and 4.30.\n264 4. Parallelism and Concurrent Programming\n4.5.4.2 Atomicity Deﬁned\nAswesawinSection4.5.3.2,adataracebugcanoccurwhenacriticaloperation\nisinterruptedbyanothercriticaloperationonthesamesharedobject. Thiscan\nhappen:\n• when one thread preempts another on a single core, or\n• when two or more critical operations overlap across multiple cores.\nThinkingintermsofinvocationandresponse,wecanpindownthegeneral\nnotion of interruption a bit more precisely: An interruption occurs whenever\ntheinvocationand/orresponseofoneoperationoccurs between theinvocation\nandresponseofanotheroperation. Butaswe’vesaid,notallinterruptionslead\ntodataracebugs. Acriticaloperationonaparticularsharedobjectcanonlybe\naffectedbyadataraceifitsinvocationandresponseareinterruptedbyanother\ncritical operation on that sameobject. Therefore, we can define the atomicity of\na critical operation as follows:\nAcriticaloperationcanbesaidtohaveexecuted atomically ifitsin-\nvocation and response are not interrupted by another critical op-\neration on that same object.\nWe should stress here that it’s perfectly fine for a critical operation to be\ninterrupted by other noncritical operations, or by critical operations affecting\nother unrelated data objects. Only when two critical operations on the same\nobject interrupt one another does a data race bug occur. Figure 4.31 illustrates\nvarious cases—one in which a critical operation succeeds in executing atomi-\ncally, and three in which it does not.\nWecanguaranteethatacriticaloperationwillbeexecutedatomicallyifwe\ncan make it appear, from the point of view of all other threads in the system,\ntohaveoccurred instantaneously. Inother words, itmustappear asthoughthe\ninvocation and response of the operation are simultaneous, or that the critical\noperation itself has a zero duration. That way, there can be no possibility of\nanother critical operation’s invocation or response “sneaking in” between the\ninvocation and response of the operation in question.\n4.5.4.3 Making an Operation Atomic\nBut how can we transform a critical operation into an atomic operation? The\neasiestandmostreliablewaytoaccomplishthisistouseaspecialobjectcalled\namutex. Amutexisanobjectprovidedbytheoperatingsystemthatactslikea\npadlock, in that it can be locked and unlocked by a thread. Given two critical\n4.5. Introduction to Concurrent Programming 265\nAtomic\nA\nRA IA\nB\nRB\n(before  IA)C\nIC\n(after  RA)\nNon-Atomic\nA\nRA IA\nB\nRB\n(after  IA)D\nID\n(before  RA)C\nRC\n(after  IA)IC\n(before  RA)\nFigure 4.31. Top: Critical operation A can be said to have executed atomically, because it was not\ninterrupted by any other invocations or responses from critical operations on the same shared\nobject. Bottom: Three scenarios in which critical operation A executed nonatomically because\nit was interrupted by the invocation and/or response of another critical operation on the same\nobject.\noperations on a particular shared data object, we guard the invocation of each\noperationwiththeacquistionofthemutex,andreleasethemutexateachone’s\nresponse. Because the OS guarantees that a mutex can only be acquired by\none thread at a time, we can thus be certain that the invocation or response\nof one operation can never happen in between the invocation and response\nof the other. From the point of view of the global ordering of the events in a\nconcurrent system, a critical operation guarded with a mutex lock appears to\nbe instantaneous.\nMutexes are part of a collection of concurrency tools provided by the op-\neratingsystemknownas threadsynchronizationprimitives. We’llexplorethread\nsynchronization primitives in Section 4.6.\n4.5.4.4 Atomicity as Serialization\nConsider a group of threads, all attempting to perform a single operation on a\nshared data object. Without atomicity, these operations might happen simul-\ntaneously, or they might overlap in all sorts of unpredictable ways over time.\nThis is illustrated in Figure 4.32.\nHowever, making the operation atomic guarantees that only one thread\nwill ever be performing it at any given moment in time. This has the effect\nofserializing the operations—what used to be a jumble of overlapping opera-\n266 4. Parallelism and Concurrent Programming\nB A 0\nC 1\nD 2\nE 3\nFigure 4.32. Without atomicity, operations performed by multiple threads can overlap in unpre-\ndictable ways over time.\nB A 0\nC 1\nD 2\nE 3\nFigure 4.33. By wrapping each critical operation in a mutex lock-unlock pair, we force the opera-\ntions to execute sequentially.\ntions is transformed into an orderly sequential sequence of atomic operations.\nMaking an operation atomic gives us no control over what the order will end\nup being; all we can say for certain is that the operations will be performed in\nsomesequential order. This idea is illustrated in Figure 4.33.\n4.5.4.5 Data-Centric Consistency Models\nThe concepts of atomicity and the serialization of operations within a concur-\nrent system are part of a larger topic known as data-centric consistency models.\nA consistency model is a contract between a data store, such as a shared data\nobject in a concurrent system or a database in a distributed system, and a col-\nlection of threads that share that data store. It makes reasoning about the be-\nhavior of the data store easier—as long as the threads follow the rules of the\ncontract, the programmer can be certain that the data store will behave in a\nconsistent and predictable manner, and its data will not become corrupted.\nA data store that provides a guarantee of atomicity can be said to be lin-\nearizable. The topic of data-centric consistency is a bit beyond our scope here,\nbut you can read more about it online. Here are a few good places to start:",21501
30-4.6 Thread Synchronization Primitives.pdf,30-4.6 Thread Synchronization Primitives,"4.6. Thread Synchronization Primitives 267\n• Search for “consistency model” and “linearizability” on Wikipedia;\n• https://www.cse.buffalo.edu/~stevko/courses/cse486/spring13/\nlectures/26-consistency2.pdf;\n• http://www.cs.cmu.edu/~srini/15-446/S09/lectures/\n10-consistency.pdf.\n4.6 Thread Synchronization Primitives\nEvery operating system that supports concurrency provides a suite of tools\nknown as threadsynchronizationprimitives. These tools provide two services to\nconcurrent programmers:\n1. The ability to share resources between threads by making critical opera-\ntionsatomic.\n2. The ability tosynchronize the operation of two or more threads:\na. by enabling a thread to go to sleep while it waits for a resource to\nbecome available or for one or more other threads to complete a\ntask, and\nb. byenablingarunningthreadto notifyoneormoresleepingthreads\nby waking them up.\nWe should note here that while these thread synchronization primitives are\nrobust and relatively easy to use, they are generally quite expensive. This is\nbecause these tools are provided by the kernel. Interacting with any of them\ntherefore requires a kernel call, which involves a context switch into protected\nmode. Such context switches can cost upwards of 1000 clock cycles. Because\nof their high cost, some concurrent programmers prefer to implement their\nownatomicityandsynchronizationtools, ortheyturnto lock-freeprogramming\nto improve the efficiency of their concurrent software. Nevertheless, a solid\nunderstandingofthesesynchronizationprimitivesisanimportantpartofany\nconcurrent programmer’s toolkit.\n4.6.1 Mutexes\nA mutex is an operating system object that allows critical operations to be\nmade atomic. A mutex can be in one of two states: unlocked orlocked.\n(These two states are sometimes called released andacquired, or signaled and\nnonsignaled, respectively.)\n268 4. Parallelism and Concurrent Programming\nThe most important property of a mutex is that it guarantees that only one\nthread will ever be holding a lock on it at any given time. So, if we wrap all\ncriticaloperationsforaparticularshareddataobjectinamutexlock,thoseop-\nerationsbecomeatomicrelativetooneanother. Inotherwords,theoperations\nbecomemutually exclusive. This is where the name “mutex” comes from—it’s\nshort for “mutual exclusion.”\nA mutex is represented either by a regular C++ object or by a handle to an\nopaquekernelobject. ItsAPIistypicallycomprisedofthefollowingfunctions:\n1.create() orinit() . A function call or class constructor that creates\nthe mutex.\n2.destroy(). A function call or destructor that destroys the mutex.\n3.lock() oracquire(). Ablockingfunctionthatlocksthemutexonbe-\nhalfofthecallingthread,butputsthethreadtosleep(seeSection4.4.6.4)\nif the lock is currently held by another thread.\n4.try_lock() ortry_acquire(). A non-blocking function that at-\ntempts to lock the mutex, but returns immediately if the lock cannot be\nacquired.\n5.unlock() orrelease() . A non-blocking function that releases the\nlock on the mutex. In most operating systems, only the thread that\nlocked a mutex is permitted to unlock it.\nWhen a mutex is locked by a thread in the system, we say it is in a non-\nsignaled state. When the thread releases the lock, the mutex becomes signaled.\nIf one or more other threads is asleep (blocked) waiting on the mutex, the act\nof signaling it causes the kernel to select one of these waiting threads and\nwake it up. In some operating systems, it’s possible for a thread to explic-\nitly wait for a kernel object such as a mutex to become signaled. Under Win-\ndows, the WaitForSingleObject() andWaitForMultipleObjects()\nOS calls serve this purpose.\n4.6.1.1 POSIX\nNowthatwehaveanunderstandingofhowmutexeswork, let’stakealookat\na few examples. The POSIX thread library exposes kernel mutex objects via a\nC-stylefunctionalinterface. Here’showwe’duseittoturnoursharedcounter\nexample from Section 4.5.3.2 into an atomic operation:\n#include <pthread.h>\nint g_count = 0;\npthread_mutex_t g_mutex ;\n4.6. Thread Synchronization Primitives 269\ninline void IncrementCount()\n{\npthread_mutex_lock (&g_mutex);\n++g_count;\npthread_mutex_unlock (&g_mutex);\n}\nNote that in the interest of clarity and brevity, we’ve omitted the code, nor-\nmally executed by the main thread, that calls pthread_mutex_init() to\ninitializethemutexpriortospawningthethreadsthatwilluseit,andthatcalls\npthread_mutex_destroy() to destroy the mutex once all other threads\nhave exited.\n4.6.1.2 C++ Standard Library\nStarting with C++11, the C++ standard library exposes kernel mutexes via the\nclass std::mutex. Here’s how we’d use it to make the increment of a shared\ncounter atomic:\n#include <mutex>\nint g_count = 0;\nstd::mutex g_mutex ;\ninline void IncrementCount()\n{\ng_mutex.lock ();\n++g_count;\ng_mutex.unlock ();\n}\nThe constructor and destructor of the std::mutex class handles the initial-\nizationanddestructionoftheunderlyingkernelmutexobject,makingitalittle\nbit easier to use than pthread_mutex_t.\n4.6.1.3 Windows\nUnderWindows, amutexisrepresentedbyanopaquekernelobjectandrefer-\nenced through a handle. A mutex is “locked” by waiting for it to become sig-\nnaled, using the general-purpose WaitForSingleObject() function. Un-\nlockingamutexisaccomplishedbycalling ReleaseMutex() . Rewritingour\nsimpleexampleusingWindowsmutexes,andagainomittingthedetailsofthe\ncreation and destruction of the mutex object, we arrive at the following code:\n270 4. Parallelism and Concurrent Programming\n#include <windows.h>\nint g_count = 0;\nHANDLE g_hMutex ;\ninline void IncrementCount()\n{\nif (WaitForSingleObject (g_hMutex, INFINITE)\n== WAIT_OBJECT_0)\n{\n++g_count;\nReleaseMutex (g_hMutex);\n}\nelse\n{\n// learn to deal with failure...\n}\n}\n4.6.2 Critical Sections\nInmostoperatingsystems,amutexcanbesharedbetweenprocesses. Assuch,\nit is a data structure that is managed internally by the kernel. This means\nthat all operations performed on a mutex involve a kernel call, and hence a\ncontextswitchintoprotectedmodeontheCPU.Thismakesmutexesrelatively\nexpensive, even when no other threads are contending for the lock.\nSome operating systems provide less-expensive alternatives to a mutex.\nFor example, Microsoft Windows provides a locking mechanism known as a\ncriticalsection . The terminology and API look a bit different to that of a mutex,\nbut a critical section under Windows is really just a low-cost mutex.\nThe API of a critical section looks like this:\n1.InitializeCriticalSection() . Constructs a critical section ob-\nject.\n2.DeleteCriticalSection() . Destroys an initialized critical section\nobject.\n3.EnterCriticalSection(). A blocking function that locks a critical\nsection on behalf of the calling thread, but busy-waits or puts the thread\nto sleep if the lock is currently held by another thread.\n4.TryEnterCriticalSection() . A non-blocking function that at-\ntempts to lock a critical section, but returns immediately if the lock can-\nnot be acquired.\n4.6. Thread Synchronization Primitives 271\n5.LeaveCriticalSection() . Anon-blockingfunctionthatreleasesthe\nlock on a critical section object.\nHere’s how we’d implement an atomic increment using the Windows crit-\nical section API:\n#include <windows.h>\nint g_count = 0;\nCRITICAL_SECTION g_critsec ;\ninline void IncrementCount()\n{\nEnterCriticalSection(&g_critsec) ;\n++g_count;\nLeaveCriticalSection(&g_critsec) ;\n}\nAs before, we’ve omitted some details. The main thread normally initializes\nthe critical section prior to spawning the threads that use it, and would of\ncourse clean it up once the threads have all exited.\nHow is the low cost of a critical section achieved? When a thread first at-\ntemptstoenter(lock)acriticalsectionthatisalreadylockedbyanotherthread,\naninexpensive spinlock isusedtowaituntiltheotherthreadhasleft(unlocked)\nthat critical section. A spin lock does not require a context switch into the ker-\nnel, making it a few thousand clock cycles cheaper than a mutex. Only if the\nthread busy-waits for too long is the thread put to sleep, as it would be with a\nregularmutex. Thisless-expensiveapproachworksbecause,unlikeamutex,a\ncritical section cannot be shared across process boundaries. We’ll discuss spin\nlocks in more depth in Section 4.9.7.\nSomeotheroperatingsystemsprovide“cheap”mutexvariantsaswell. For\nexample, Linux supports a thing called a “futex” that acts somewhat like a\ncritical section under Windows. Its use is beyond our scope here, but you can\nread more about futexes at https://www.akkadia.org/drepper/futex.pdf.\n4.6.3 Condition Variables\nIn concurrent programming, we often need to send signals between threads\nin order to synchronize their activities. One example of this is the ubiquitous\nproducer-consumer problem which we introduced in Section 4.4.8.1. In this\nproblem, we have two threads: A producer thread calculates or otherwise gen-\nerates some data, and that data is read and put to use by a consumer thread.\nObviously the consumer thread cannot consume the data until the producer\n272 4. Parallelism and Concurrent Programming\nhas produced it. Hence the producer thread needs a way to notify the con-\nsumer that its data is ready for consumption.\nWe could consider using a global Boolean variable as a signalling mech-\nanism. The following code snippet illustrates the idea, using POSIX threads.\n(Some details have been omitted for clarity.)\nQueue g_queue;\npthread_mutex_t g_mutex ;\nbool g_ready = false;\nvoid* ProducerThread(void*)\n{\n// keep on producing forever...\nwhile (true)\n{\npthread_mutex_lock (&g_mutex);\n// fill the queue with data\nProduceDataInto(&g_queue);\ng_ready = true ;\npthread_mutex_unlock (&g_mutex);\n// yield the remainder of my timeslice\n// to give the consumer a chance to run\npthread_yield();\n}\nreturn nullptr;\n}\nvoid* ConsumerThread(void*)\n{\n// keep on consuming forever...\nwhile (true)\n{\n// wait for the data to be ready\nwhile (true)\n{\n// read the value into a local,\n// making sure to lock the mutex\npthread_mutex_lock (&g_mutex);\nconst bool ready = g_ready;\npthread_mutex_unlock (&g_mutex);\nif (ready)\nbreak;\n4.6. Thread Synchronization Primitives 273\n}\n// consume the data\npthread_mutex_lock (&g_mutex);\nConsumeDataFrom(&g_queue);\ng_ready = false ;\npthread_mutex_unlock (&g_mutex);\n// yield the remainder of my timeslice\n// to give the producer a chance to run\npthread_yield();\n}\nreturn nullptr;\n}\nBesides the fact that this example is somewhat contrived, there’s one big\nproblem with it: The consumer thread spins in a tight loop, polling the value\nofg_ready. As we discussed in Section 4.4.6.4, busy-waiting like this wastes\nvaluable CPU cycles.\nIdeally, we’d like a way to blockthe consumer thread (put it to sleep) while\nthe producer does its work, and then wake it up when the data is ready to be\nconsumed. This can be accomplished by making use of a new kind of kernel\nobject called a condition variable (CV).\nAconditionvariableisn’tactuallyavariablethatstoresacondition. Rather,\nit’saqueueofwaiting(sleeping)threads,combinedwithamechanismthatal-\nlowsarunningthreadtowakeupthesleepingthreadsatatimeofitschoosing.\n(Perhaps “wait queue” would have been a better name for these things.) The\nsleep and wake operations are performed in an atomic way with the help of a\nmutex provided by the program, plus a little help from the kernel.\nThe API for a condition variable typically looks something like this:\n1.create() orinit() . A function call or class constructor that creates a\ncondition variable.\n2.destroy(). Afunctioncallordestructorthatdestroysaconditionvari-\nable.\n3.wait() . A blocking function that puts the calling thread to sleep.\n4.notify(). A non-blocking function that wakes up any threads that are\ncurrently asleep waiting on the condition variable.\nLet’s rewrite our simple producer-consumer example using a CV:\nQueue g_queue;\npthread_mutex_t g_mutex;\n274 4. Parallelism and Concurrent Programming\nbool g_ready = false;\npthread_cond_t g_cv ;\nvoid* ProducerThreadCV(void*)\n{\n// keep on producing forever...\nwhile (true)\n{\npthread_mutex_lock(&g_mutex);\n// fill the queue with data\nProduceDataInto(&g_queue);\n// notify and wake up the consumer thread\ng_ready = true;\npthread_cond_signal (&g_cv);\npthread_mutex_unlock(&g_mutex);\n}\nreturn nullptr;\n}\nvoid* ConsumerThreadCV(void*)\n{\n// keep on consuming forever...\nwhile (true)\n{\n// wait for the data to be ready\npthread_mutex_lock(&g_mutex);\nwhile (!g_ready)\n{\n// go to sleep until notified... the mutex\n// will be relased for us by the kernel\npthread_cond_wait (&g_cv, &g_mutex);\n// when it wakes up, the kernel makes sure\n// that this thread holds the mutex again\n}\n// consume the data\nConsumeDataFrom(&g_queue);\ng_ready = false;\npthread_mutex_unlock(&g_mutex);\n}\nreturn nullptr;\n}\nThe consumer thread calls pthread_cond_wait() to go to sleep until\n4.6. Thread Synchronization Primitives 275\ng_ready becomes true. The producer works for a while producing its\ndata. When the data is ready, the producer sets the global g_ready\nflag to true, and then wakes up the sleeping consumer by calling\npthread_cond_signal(). The consumer then consumes the data. In this\nexample, the consumer and producer ping-pong back and forth like this in-\ndefinitely.\nYou probably noticed that the consumer thread locks its mutex before en-\nteringthewhileloopthatchecksthe g_ready flag. Whenitwaitsonthecondi-\ntionvariable, itapparentlygoestosleep whileholdingthemutexlock! Normally\nthiswouldbeabigno-no: Ifathreadgoestosleepwhileholdingalock, itwill\nalmost certainly lead to a deadlock situation (see Section 4.7.1). However, this\nis not a problem when using a condition variable. That’s because the kernel\nactually does a little slight of hand, unlocking the mutex after the thread has\nbeen safely put to bed. Later, when the sleeping thread is woken back up, the\nkernel does some more slight of hand to ensure that the lock will once again\nbe held by the freshly awoken thread.\nYou may have noticed another oddity: The consumer thread still uses a\nwhilelooptocheckthevalueof g_ready, despitealsousingaconditionvari-\nable to wait for the flag to become true. The reason that this loop is necessary\nis that threads can sometimes be awoken spuriously by the kernel. As a re-\nsult, whenthecallto pthread_cond_wait() returns, thevalueof g_ready\nmightnotactually be true yet. So we must keep polling in a loop until the\ncondition really is true.\n4.6.4 Semaphores\nJustasamutexactslikeanatomicBooleanflag,a semaphore actslikeanatomic\ncounter whose value is never allowed to drop below zero. We can think of a\nsemaphoreasaspecialkindofmutexthatallows morethanonethread toacquire\nit simultaneously.\nA semaphore can be used to permit a group of threads to share a limited\nsetofresources. Forexample,let’ssupposethatwe’reimplementingarender-\ning system that allows text and 2D images to be rendered to offscreen buffers,\nforthepurposesofdrawingthegame’sheads-updisplay(HUD)andin-game\nmenus. Due to memory constraints, let’s further assume that we can only af-\nford to allocate four of these buffers. A semaphore can be used to ensure that\nno more than four threads are permitted to render into these buffers at any\ngiven moment.\nThe API of a semaphore is typically comprised of the following functions:\n1.init() . Initializesasemaphoreobject,andsetsitscountertoaspecified\n276 4. Parallelism and Concurrent Programming\ninitial value.\n2.destroy(). Destroys a semaphore object.\n3.take() orwait(). If the counter value encapsulated by a given\nsemaphore is greater than zero, this function decrements the counter\nand returns immediately. If its counter value is currently zero, this func-\ntion blocks (puts the thread to sleep) until the semaphore’s counter rises\nabove zero again.\n4.give() ,post() orsignal(). Increments the encapsulated counter\nvalue by one, thereby opening up a “slot” for another thread to take()\nthe semaphore. If a thread is currently asleep waiting on the semaphore\nwhen give() iscalled, thatthreadwillwakeupfromitscallto take()\norwait().7\nSo, to implement a resource pool that can be accessed by up to Nthreads\nat a time, we would simply create a semaphore and initialize its counter to N.\nA thread gains access to the resource pool by calling take(), and releases its\nhold on the resource pool when it is done by calling give() .\nWesaythatasemaphoreis signaled wheneveritscountisgreaterthanzero,\nand it is nonsignaled when its counter is equal to zero. This is why the func-\ntions that take and give a semaphore are named wait() andsignal(), re-\nspectively, in some APIs: If the semaphore isn’t signaled when a thread calls\nthis function, the thread will waitfor the semaphore to become signaled.\n4.6.4.1 Mutex versus Binary Semaphore\nA semaphore whose initial value is set to 1 is called a binary semaphore . One\nmight think that a binary semaphore is identical to a mutex. Certainly both\nobjects permit only one thread to acquire it at a time. However, these two\nsynchronization objects are not equivalent, and are typically used for quite\ndifferent purposes.\nThekeydifferencebetweenamutexandabinarysemaphoreisthatamutex\ncan only be unlocked by the thread that locked it. A semaphore’s counter, on\nthe other hand, can be incremented by one thread and later decremented by\nanother thread. This implies that a binary semaphore can be “unlocked” by a\ndifferent thread than the one that “locked” it. Or really, we should say that a\nbinary semaphore can be givenby a different thread than the one that takesit.\nThisseeminglysubtledifferencebetweenmutexesandbinarysemaphores\nleads to very different use cases for these two kinds of synchronization object.\n7Whenyou readabout semaphores, you maydiscoversome authorsusing the functionnames\np() andv() instead of wait() and signal(). These letters come from the Dutch names for these two\noperations.\n4.6. Thread Synchronization Primitives 277\nA mutex is used to make an operation atomic. But a binary semaphore is typi-\ncally used to send asignalfrom one thread to another.\nConsider again our producer-consumer example, in which the producer\nneeds to notify the consumer when the data it produces is ready for con-\nsumption. This notification mechanism can be implemented using two bi-\nnary semaphores, one that allows the producer to wake the consumer, and\none that allows the consumer to wake the producer. We can think of these\nsemaphores as representing the number of elements that are used and free\nwithin a buffer that’s shared between the two threads, although in this sim-\nple example the buffer can only hold a single item. As such, we’ll call the\nsemaphores g_semUsed andg_semFree , respectively. Here’s what the code\nwould look like, using POSIX semaphores:\nQueue g_queue;\nsem_t g_semUsed ; // initialized to 0\nsem_t g_semFree ; // initialized to 1\nvoid* ProducerThreadSem(void*)\n{\n// keep on producing forever...\nwhile (true)\n{\n// produce an item (can be done non-\n// atomically because it's local data)\nItem item = ProduceItem();\n// decrement the free count\n// (wait until there's room)\nsem_wait(&g_semFree);\nAddItemToQueue(&g_queue, item);\n// increment the used count\n// (notify consumer that there's data)\nsem_post(&g_semUsed);\n}\nreturn nullptr;\n}\nvoid* ConsumerThreadSem(void*)\n{\n// keep on consuming forever...\nwhile (true)\n{\n// decrement the used count\n278 4. Parallelism and Concurrent Programming\n// (wait for the data to be ready)\nsem_wait(&g_semUsed);\nItem item = RemoveItemFromQueue(&g_queue);\n// increment the free count\n// (notify producer that there's room)\nsem_post(&g_semFree);\n// consume the item (can be done non-\n// atomically because it's local data)\nConsumeItem(item);\n}\nreturn nullptr;\n}\n4.6.4.2 Implementing a Semaphore\nIt turns out that one can implement a semaphore in terms of a mutex, a con-\ndition variable and an integer. In that sense, a semaphore is a “higher-level”\nconstruct than either a mutex or a condition variable. Here’s what the imple-\nmentation looks like:\nclass Semaphore\n{\nprivate:\nint m_count;\npthread_mutex_t m_mutex;\npthread_cond_t m_cv;\npublic:\nexplicit Semaphore(int initialCount)\n{\nm_count = initialCount;\npthread_mutex_init(&m_mutex, nullptr);\npthread_cond_init(&m_cv, nullptr);\n}\nvoid Take()\n{\npthread_mutex_lock(&m_mutex);\n// put the thread to sleep as long as\n// the count is zero\nwhile (m_count == 0)\npthread_cond_wait(&m_cv, &m_mutex);\n4.6. Thread Synchronization Primitives 279\n--m_count;\npthread_mutex_unlock(&m_mutex);\n}\nvoid Give()\n{\npthread_mutex_lock(&m_mutex);\n++m_count;\n// if the count was zero before the\n// increment, wake up a waiting thread\nif (m_count == 1)\npthread_cond_signal(&m_cv);\npthread_mutex_unlock(&m_mutex);\n}\n// aliases for other commonly-used function names\nvoid Wait() { Take(); }\nvoid Post() { Give(); }\nvoid Signal() { Give(); }\nvoid Down() { Take(); }\nvoid Up() { Give(); }\nvoid P() { Take(); } // Dutch ""proberen"" = ""test""\nvoid V() { Give(); } // Dutch ""verhogen"" =\n// ""increment""\n};\n4.6.5 Windows Events\nWindowsprovidesamechanismcalledan eventobject thatissimilarinfunction\ntoaconditionvariable,butmuchsimplertouse. Onceaneventobjecthasbeen\ncreated, a thread can go to sleep by calling WaitForSingleObject(), and\nthatthreadcanbeawokenbyanotherthreadbycalling SetEvent(). Rewrit-\ning our producer-consumer example using event objects yields the following\nvery simple implementation:\n#include <windows.h>\nQueue g_queue;\nHandle g_hUsed ; // initialized to false (nonsignaled)\nHandle g_hFree ; // initialized to true (signaled)\nvoid* ProducerThreadEv(void*)\n{\n// keep on producing forever...\nwhile (true)\n{\n// produce an item (can be done non-\n280 4. Parallelism and Concurrent Programming\n// atomically because it's local data)\nItem item = ProduceItem();\n// wait until there's room\nWaitForSingleObject (&g_hFree);\nAddItemToQueue(&g_queue, item);\n// notify consumer that there's data\nSetEvent(&g_hUsed);\n}\nreturn nullptr;\n}\nvoid* ConsumerThreadEv(void*)\n{\n// keep on consuming forever...\nwhile (true)\n{\n// wait for the data to be ready\nWaitForSingleObject (&g_hUsed);\nItem item = RemoveItemFromQueue(&g_queue);\n// notify producer that there's room\nSetEvent(&g_hFree);\n// consume the item (can be done non-\n// atomically because it's local data)\nConsumeItem(item);\n}\nreturn nullptr;\n}\nvoid MainThread()\n{\n// create event in the nonsignalled state\ng_hUsed =CreateEvent(nullptr, false,\nfalse, nullptr);\ng_hFree =CreateEvent(nullptr, false,\ntrue, nullptr);\n// spawn our threads\nCreateThread(nullptr, 0x2000, ConsumerThreadEv,\n0, 0, nullptr);\nCreateThread(nullptr, 0x2000, ProducerThreadEv,\n0, 0, nullptr);",22889
31-4.7 Problems with Lock-Based Concurrency.pdf,31-4.7 Problems with Lock-Based Concurrency,"4.7. Problems with Lock-Based Concurrency 281\n// ...\n}\n4.7 Problems with Lock-Based Concurrency\nIn Section 4.5.3.2, we learned that data races can lead to incorrect program be-\nhavior in a concurrent system. We saw that the solution to this problem is\nto make operations on shareddata objects atomic. One way to achieve atom-\nicity is to wrap these operations in locks, which are often implemented us-\ningoperating-systemprovidedthreadsynchronizationprimitivessuchasmu-\ntexes.\nHowever, atomicity is only part of the concurrency story. There are other\nproblems that can plague a concurrent system, even when all shared-data op-\nerationshavebeencarefullyprotectedbylocks. Inthefollowingsections,we’ll\nbriefly explore the most common of these problems.\n4.7.1 Deadlock\nDeadlock is a situation in which no thread in the system can make progress,\nresulting in a hang. When a deadlock occurs, all threads are in their Blocked\nstates, waiting on some resource to become available. But because no threads\nare Runnable, none of those resources can ever become available, so the entire\nprogram hangs.\nFor a deadlock to occur, we need at least two threads and two resources.\nFor example, Thread 1 holds Resource A but is waiting for Resource B; and at\nthe same time, Thread 2 holds Resource B but is waiting for Resource A. This\nsituation is illustrated by the following code snippet:\nvoid Thread1()\n{\ng_mutexA.lock(); // holds lock for Resource A\ng_mutexB.lock(); // sleeps waiting for Resource B\n// ...\n}\nvoid Thread2()\n{\ng_mutexB.lock(); // holds lock for Resource B\ng_mutexA.lock(); // sleeps waiting for Resource A\n// ...\n}\n282 4. Parallelism and Concurrent Programming\nT0\nT1T2T0\nR0\nR3R1\nT1T2\nR2\nFigure 4.34. Left: Dark arrows show resources currently held by threads. Light dashed arrows\nshow threads waiting on resources to become available. Right: We can eliminate the resources\nand simply draw the dependencies (light dashed arrows) between threads. A cycle in such a thread\ndependency graph indicates a deadlock.\nOther more complex kinds of deadlock can of course occur as well, involving\nmorethreadsandmoreresources. Butthekeyfactorthatdefinesanydeadlock\nsituation is a circular dependency between threads and their resources.\nTo analyze a system for the possibility of deadlock, we can draw a graph\nof our threads, our resources, and the dependencies between them, as shown\nin Figure 4.34. In this graph, we’ve used squares to represent threads and\ncircles to represent resources (or more precisely, the mutex locks that protect\nthem). Solid arrows connect resources to the threads that currently hold locks\non them. Dashed arrows connect threads to the resources they are waiting\nfor. We can actually eliminate the resource nodes in the graph for simplicity\nif we like, leaving only the dashed lines connecting threads that are waiting\non other threads. If such a dependency graph ever contains a cycle, we have a\ndeadlock.\nActually a cycle in the dependency graph isn’t quite enough to produce a\ndeadlock. Strictly speaking, there are four necessary and sufficient conditions\nfor deadlock, known as the Coffmanconditions:\n1.Mutual exclusion. A single thread can be granted exclusive access to a\nsingle resource via a mutex lock.\n2.Hold and wait . A thread must be holding one lock when it goes to sleep\nwaiting on another lock.\n3.No lock preemption. No one (not even the kernel) is allowed to forcibly\n4.7. Problems with Lock-Based Concurrency 283\nbreak a lock held by a sleeping thread.\n4.Circularwait . There must exist a cycle in the thread dependency graph.\nAvoiding deadlock always boils down to preventing one or more of the\nCoffman conditions from holding true. Since violating conditions #1 and #3\nwould be “cheating,” solutions usually focus on avoiding conditions #2 and\n#4.\nHold and wait can be avoided by reducing the number of locks. In our\nsimpleexample,ifResourceAandResourceBwerebothprotectedbyasingle\nlock L, then deadlock could not occur. Either Thread 1 would obtain the lock,\nand gain exclusive access to both resources while Thread 2 waits, or Thread 2\nwould obtain the lock while Thread 1 waits.\nThecircularwaitconditioncanbeavoidedbyimposingaglobalordertoall\nlock-taking in the system. In our simple two-threaded example, if we were to\nensure that Resource A’s lock is always taken before Resource B’s lock, dead-\nlock would be avoided. This would work because one thread will always ob-\ntain a lock on Resource A before attempting to take any other locks. This ef-\nfectively puts all other contending threads to sleep, thereby ensuring that the\nattempt to take a lock on Resource B will always succeed.\n4.7.2 Livelock\nAnother solution to the deadlock problem is for threads to tryto take\nlocks without going to sleep (using a function such as pthread_mutex_\ntrylock()). If a lock cannot be obtained, the thread sleeps for some short\nperiod of time and then retriesthe lock.\nWhen threads use an explicit strategy, such as timed retries, in order to\navoid or resolve deadlocks, a new problem can arise: The threads can end\nup spending all of their time just trying to resolve the deadlock, rather than\nperforming any real work. This is known as livelock.\nAsasimple exampleoflivelock,consideragainourexampleoftwothreads\n1 and 2 contending over two resources A and B. Whenever a thread is unable\nto obtain a lock, it releases any locks it already holds and waits for a fixed\ntimeout before trying again. If both threads use the same timeout, we can get\ninto a situation in which the same degenerate situation simply repeats over\nand over. Our threads become “stuck” forever trying to resolve the conflict,\nand neither one ever gets a chance to do its real job. Livelock is akin to a\nstalemate in chess.\nLivelock can be avoided by using an asymmetric deadlock resolution al-\ngoirthm. For example, we could ensure that only one thread, either chosen at\n284 4. Parallelism and Concurrent Programming\nrandomorbasedonpriority, evertakesactiontoresolveadeadlockwhenone\nis detected.\n4.7.3 Starvation\nStarvation is defined as any situation in which one or more threads fail to re-\nceiveanyexecutiontimeontheCPU.Starvationcanhappenwhenoneormore\nhigher-priority threads fail to relinquish control of the CPU, thereby prevent-\ning lower-priority threads from running. Livelock is another kind of starva-\ntion, in which a deadlock-resolution algorithm effectively starves all threads\nof their ability to do “real” work.\nPriority-based starvation is normally avoided by ensuring that high-\npriority threads never run for very long. Ideally a multithreaded program\nwould consist of a pool of lower-priority threads that normally share the sys-\ntem’s CPU resources fairly. Once in a while, a higher-priority thread runs,\ntakes care of its business quickly, and then ends, returning the CPU resources\nto the lower-priority threads.\n4.7.4 Priority Inversion\nMutexlockscanleadtoasituationknownas priorityinversion,inwhichalow-\npriority thread acts as though it is the highest-priority thread in the system.\nConsider two threads, L and H, with a low and high priority, respectively.\nThreadLtakesamutexlockandthenispreemptedbyH.IfHattemptstotake\nthis same lock, then H will be put to sleep because L already holds the lock.\nThis permits L to run even though it is lower priority than H—in violation of\ntheprinciplethatlower-prioritythreadsshouldnotrunwhileahigher-priority\nthread is runnable.\nPriority inversion can also occur if a medium-priority thread M preempts\nthread L while it is holding a lock needed by H. In this case, L goes to sleep\nwhile M runs, preventing it from releasing the lock. When H runs, it is there-\nfore unable to obtain the lock; it goes to sleep, and now M’s priority has effec-\ntively been inverted with that of thread H.\nThe consequences of priority inversion may be neglible. For example, if\nthe lower-priority thread immediately relinquishes the lock, the duration of\nthe priority inversion may be short and it may go unnoticed or produce only\nminor ill effects. However, in extreme cases, priority inversion can lead to\ndeadlock or other kinds of system failure. For example, a priority inversion\ncan cause a high-priority thread to miss a critical deadline.\nSolutions to the priority inversion problem include:\n4.7. Problems with Lock-Based Concurrency 285\n• Avoidinglocksthatcanbetakenbybothlow-andhigh-prioritythreads.\n(This solution is often not feasible.)\n• Assigning a very high priority to the mutex itself. Any thread that takes\nthe mutex has its priority temporarily raised to that of the mutex, thus\nensuring it cannot be preempted while holding the lock.\n• Random priority boosting. In this approach, threads that are actively\nholding locks are randomly boosted in priority until they exit their crit-\nical sections. This approach is used in the Windows scheduling model.\n4.7.5 The Dining Philosophers\nThe famous dining philosophers problem is a great illustration of the problems\nof deadlock, livelock and starvation. It describes a situation in which five\nphilosophers sit around a circular table, each with a plate of spaghetti in front\nof her. Between each philosopher sits a single chopstick. The philosophers\nwish to alternate between thinking (which they can do without any chop-\nsticks), and eating (a task for which a philosopher requires two chopsticks).\nThe problem is to define a pattern of behavior for the philosophers that en-\nsures they can all alternate between thinking and eating without experienc-\ning deadlock, livelock or starvation. (Obviously the philosophers represent\nthreads, and the chopsticks represent mutex locks.)\nYou can read about this well-known problem online, so we won’t devote a\nlot of space to discussing it here. However, it will be instructive to consider a\nfew of the most common solutions to the problem:\n•Globalorder. Adependencycyclecanoccurifaphilosopherwerealways\nto pick up the chopstick to her left first (or always the one to her right).\nOne solution to this problem is to assign each chopstick a unique global\nindex. Whenever a philosopher wishes to eat, he or she always picks up\nthe chopstick with the lowest index first. This prevents the dependency\ncycle, and hence avoids deadlock.\n•Central arbitor. In this solution, a central arbitor (the “waiter”) either\ngrants a philosopher two chopsticks or none. This avoids the hold and\nwait problem by ensuring that no philosopher ever gets into a situation\nin which he or she holds only one chopstick, thereby preventing dead-\nlock.\n•Chandra-Misra . In this solution, chopsticks are marked either dirty or\nclean, and the philosophers send each other messages requesting chop-\nsticks. You can read more about this solution by searching for “chandy-\nmisra” online.",10887
32-4.8 Some Rules of Thumb for Concurrency.pdf,32-4.8 Some Rules of Thumb for Concurrency,"286 4. Parallelism and Concurrent Programming\n•N 1philosophers . For a table with Nphilosophers and Nchopsticks,\nan integer semaphore can be used to limit the number of philosophers\nwhoarepermittedtopickupchopsticksatanygiventimeto N 1. This\nsolves the deadlock and livelock problems, because even in degenerate\nsituations, at least one philosopher will succeed in obtaining two chop-\nsticks. It does, however, permit one of the philosophers to starve, unless\nan additional “fairness” criterion is introduced.\n4.8 Some Rules of Thumb for Concurrency\nThe solutions to the dining philosophers problem we described in the preced-\ning section hint at some general principles and rules of thumb which we can\napply to virtually any concurrent programming problem. Let’s take a brief\nlook at a few of them.\n4.8.1 Global Ordering Rules\nIn a concurrent program, the orderin which events occur is not dictated by the\norder of the instructions in the program, as it is in a single-threaded program.\nIf ordering is required in a concurrent system, that ordering must be imposed\nglobally, across all threads.\nThis is one reason why a doubly linked list is not a concurrent data struc-\nture. A doubly linked list is designed the way it is in order to provide fast\n(O(N)) insertion and deletion of elements at any location within the list. Un-\nderlying this design is an assumption that program order is equivalent to data\norder—that when an ordered sequence of operations is performed on a list,\ntheresulting listwillcontaincorrespondinglyorderedelements. Forexample,\nlet’s say we are given a linked list that contains the ordered elements { A, B, C\n}. If we execute the following two operations:\n1. insert D before C,\n2. insert E before C,\nthe assumption is that the resulting list will contain the ordered elements { A,\nB, D, E, C }.\nIn a single-threaded program, this assumption holds true. But in a multi-\nthreaded system, program order no longer dictates data order. If one thread\nperformstheinsertionofDbeforeC,andanotherthreadperformstheinsertion\nof E before C, we have a race condition that could lead to any of the following\nresults:\n4.8. Some Rules of Thumb for Concurrency 287\n• { A, B, D, E, C },\n• { A, B, E, D, C }, or\n• { A, B, corrupteddata }.\nA corrupted list can occur if the data structure’s operations aren’t properly\nprotected with critical sections. For example, both D’s and E’s “next” pointers\nmight end up pointing to C.\nA global ordering rule is the only viable solution to this problem. We first\nneed to ask ourselves why the order of the elements in the list matters, and\nif it even matters at all. If order is unimportant, we can use a singly-linked\nlist and always append the elements—an operation that can be implemented\nreliably either with locks or in a lock-free manner. And if a global ordering\nisrequired, we need to identify a stable and deterministic ordering criterion\nthat does not depend on the order in which program events happen to occur.\nForexample,wemightsortthelistalphabetically,orbypriority,orsomeother\nuseful criterion. The answers to these questions in turn dictate what kind of\ndata structure we’ll want to use. Attempting to use a doubly-linked list in a\nconcurrent way (i.e., giving multiple threads mutable access to the list) is like\ntrying to fit a square peg into a round hole.\n4.8.2 Transaction-Based Algorithms\nIn the central arbiter solution to the dining philosophers problem, the arbiter\nor “waiter” hands out chopsticks in pairs: Either a philosopher receives all of\nthe resources he needs (two chopsticks), or he receives none of them. This is\nknown as a transaction.\nA transaction can be more precisely defined as an indivisible bundle of re-\nsourcesand/oroperations. Threadsinaconcurrentsystemsubmittransaction\nrequests to a central arbiter of some kind. A transaction either succeeds in its\nentirety, or it fails in its entirety (because some other thread’s transaction is\nactively being processed when the request arrives). If the transaction fails, its\nthread keeps resubmitting the transaction request until it succeeds (possibly\nwaiting for a short time between retries).\nTransaction-based algorithms are common in concurrent and distributed\nsystems programming. And as we’ll see in Section 4.9, the concept of a trans-\naction underlies most lock-free data structures and algorithms.\n4.8.3 Minimizing Contention\nThe most efficient concurrent system would be one in which all threads run\nwithout ever having to wait for a lock. This ideal can never be fully achieved,\n288 4. Parallelism and Concurrent Programming\nof course, but concurrent systems programmers do attempt to minimize the\namount of contention between threads.\nAs an example, consider a group of threads that are producing data and\nstoring it into a central repository. Every time one of these threads attempts\nto store its data in the repository, it contends with all of the other threads for\nthis shared resource. A simple solution that can sometimes work is to give\neach thread its own private repository. The threads can now produce data\nindependently of one another, with no contention. When all of the threads are\ndone producing their output, a central thread can collate the results.\nThe analog to this approach in the case of the dining philosophers prob-\nlem would be to give each philosopher two chopsticks from the outset. Doing\nso would of course remove all concurrency from the problem—without any\nshared resources, there is no concurrency. In real-world concurrent systems\nwe can’t remove allsharing of resources, but we can certainly look for waysto\nminimize resource sharing in order to minimize lock contention.\n4.8.4 Thread Safety\nGenerallyspeaking,aclassorfunctionalAPIiscalled thread-safe whenitsfunc-\ntions can be safely called by any thread in a multithreaded process. For any\none function, thread safety is typically achieved by entering a critical section\nat the top of the function’s body, performing some work, and then leaving the\ncritical section just prior to returning. A class whose functions are all thread-\nsafe is sometimes called a monitor. (The term “monitor” is also used to refer to\na class that uses condition variables internally to permit clients to sleep while\nwaiting for its protected resources to become available.)\nThread-safety is a convenient feature for a class or interface to offer. But\nit also imposes overhead that is sometimes unnecessary. For example, this\noverhead would be wasteful if the interface is being used in a single-threaded\nprogram, or if it’s being used exclusively by one thread in a multithreaded\nprogram.\nThread-safetycanalsobecomeaproblemwhenaninterfacefunctionneeds\nto be called reentrantly. For example, if a class provides two thread-safe func-\ntions A()andB(), then these functions cannot call one another because each\none independently enters and leaves the critical section. One solution to this\nproblemistousereentrantlocks(seeSection4.9.7.3). Anotheristoimplement\n“unsafe” versions of the functions in an interface, and then “wrap” each of\nthem in thread-safe variant that simply enters a critical section, calls the “un-\nsafe” function, and then leaves the critical section. That way, we can call the\n“unsafe” functions internally to the system, but the external interface of the",7391
33-4.9 Lock-Free Concurrency.pdf,33-4.9 Lock-Free Concurrency,"4.9. Lock-Free Concurrency 289\nsystem remains thread-safe.\nIn my view, it’s a bad idea to attempt to handle concurrent programming\nby simply creating classes and APIs that are 100% thread-safe. Doing so leads\ntointerfacesthatareunnecessarilyheavy-weight, anditencouragesprogram-\nmers to ignore the fact that they are working in a concurrent environment.\nInstead, we should recognize and embrace the presence of concurrency in our\nsoftware, and aim to design data structures and algorithms that address this\nconcurrency explicitly. The goal should be to produce a software system that\nminimizes contention and dependencies between threads, and minimizes the\nuse of locks. Achieving a completely lock-free system is a lot of work and is\ntoughtogetright. (SeeSection4.9.) Butstrivinginthe direction oflock-freedom\n(i.e., minimizing the use of locks) is a far better strategy than over-using locks\nin a futile attempt to create “water-tight” interfaces that allow programmers\nto be oblivious of concurrency.\n4.9 Lock-Free Concurrency\nThus far, all of our solutions to the problem of race conditions in a concurrent\nsystem have revolved around using mutex locks to make critical operations\natomic, and possibly leveraging condition variables and the kernel’s ability to\nput threads to sleep to avoid them wasting valuable CPU cycles in a busy-wait\nloop. During that discussion, we mentioned that there’s another way to avoid\nraceconditionsthatcanpotentiallybemoreefficient. Thatapproachisknown\naslock-free concurrency.\nContrary to popular belief, the term “lock-free” doesn’t actually refer to\nthe elimination of mutex locks, although that is certainly a component of the\napproach. In fact, “lock-free” refers to the practice of preventing threads from\ngoingtosleepwhilewaitingonaresourcetobecomeavailable. Inotherwords,\nin lock-free programming we never allow a thread to block. So perhaps the\nterm “blocking-free” would have been more descriptive.\nLock-free programming is actually just one of a collection of non-blocking\nconcurrent programming techniques. When a thread is blocked, it ceases to\nmake progress. The goal of all of these techniques is to provide guarantees\nabout the progress that can be made by the threads in the system, and by the\nsystem as a whole. We can organize these techniques into the following cat-\negories, listed in order of increasing strength of the progress guarantees they\nprovide:\n•Blocking. A blocking algorithm is one in which a thread can be put to\nsleep while waiting for shared resources to become available. A block-\n290 4. Parallelism and Concurrent Programming\ning algorithm is prone to deadlock, livelock, starvation and priority in-\nversion.\n•Obstructionfreedom. Analgorithmisobstruction-freeifwecanguarantee\nthat a single thread will always complete its work in a bounded number\nof steps, when all of the other threads in the system are suddenly sus-\npended. The single thread that continues to run is said to be perform-\ning asolo execution in this scenario, and an obstruction-free algorithm is\nsometimescalled solo-terminating becausethesolothreadeventuallyter-\nminates while all other threads are suspended. No algorithm that uses\na mutex lock or spin lock can be obstruction-free, because if any thread\nwere to be suspended while it is holding a lock, the solo thread might\nbecome stuck forever waiting for that lock.\n•Lock freedom . The technical definition of lock freedom is that in any\ninfinitely-long run of the program, an infinite number of operations will\nbe completed. Intuitively, a lock-free algorithm guarantees that some\nthread in the system can always make progress; in other words, if one\nthread is arbitrarily suspended, all others can still make progress. This\nagain precludes the use of mutex or spin locks, because if a thread hold-\ning a lock is suspended, it can cause other threads to block. A lock-free\nalgorithm is typically transaction-based: A transaction may fail if an-\notherthreadinterruptsit,inwhichcasethetransactionisrolledbackand\nretrieduntilitsucceeds. Thisapproachavoidsdeadlock,butitcanallow\nsomethreadstostarve. Inotherwords,certainthreadsmightgetstuckin\na loop of failing and retrying their transactions indefinitely, while other\nthreads’ transactions always succeed.\n•Wait freedom . A wait-free algorithm provides all the guarantees of lock\nfreedom, but also guarantees starvation freedom. In other words, all\nthreads can make progress, and no thread is allowed to starve indefi-\nnitely.\nThe term “lock-free programming” is sometimes used loosely to refer to any\nalgorithm that avoids blocking, but technically speaking the correct term for\nobstruction-free,lock-freeandwait-freealgorithmsasawholeis“non-blocking\nalgorithm.”\nThe topic of non-blocking algorithms is vast, and is still an open area of\nresearch. A complete discussion of the topic would require an entire book of\nits own. In this chapter, we’ll introduce some of the basic principles of lock-\nfree programming. We’ll begin by exploring the true casues of data race bugs.\nThen we’ll have a look at how mutexes are actually implemented under the\n4.9. Lock-Free Concurrency 291\nhood, and learn how to implement our own inexpensive spin locks . Finally,\nwe’ll present an implementation of a simple lock-free linked list. This discus-\nsionoughttobesufficienttogiveyouaflavorforwhatlock-freedatastructures\nandalgorithmstendtolooklike,andshouldprovideasolidjumpingoffpoint\nfor further reading and experimentation with lock-free techniques.\n4.9.1 Causes of Data Race Bugs\nIn Section 4.5.3.2, we said that data race bugs occur when a critical operation\nis interrupted by another critical operation on the same shared data. As it\nturns out, there are two other ways in which data race bugs can arise, and if\nwe’re going to implement our own spin locks or write lock-free algorithms,\nwe’ll need to understand them all. Data race bugs can be introduced into a\nconcurrent program:\n• via the interruption of one critical operation by another,\n• by the instruction reordering optimizations performed by the compiler\nand CPU, and\n• as a result of hardware-specific memoryordering semantics.\nLet’s break these down a bit further:\n• Threadsinterruptoneanotherallthetimeasaresultofpreemptivemul-\ntitasking and/or by running them on multiple cores. However, when a\ncriticaloperation is interrupted by another critical operation on the same\nshared data object, a data race bug can occur.\n• Optimizing compilers often reorder instructions in an attempt to min-\nimize pipeline stalls. Likewise, the out-of-order execution logic within\nthe CPU can cause instructions to be executed in an order that differs\nfrom program order. Instruction reordering is guaranteed not to alter\nthe observable behavior of a single-threaded program. But it canalter\nthe way in which two or more threads cooperate to share data, thereby\nintroducing bugs into a concurrent program.\n• Thanks to aggressive optimizations within a computer’s memory con-\ntroller, the effectsof a read or write instruction can sometimes become\ndelayed relative to other reads and/or writes in the system. As with\ncompiler optimizations and OOO execution, these memory controller\noptimizations are designed never to alter the observable behavior of a\nsingle-threaded program. However, these optimizations canchange the\norder of a critical pair of reads and/or writes in a concurrent system,\n292 4. Parallelism and Concurrent Programming\nthereby preventing threads from sharing data in a predictable way. In\nthisbook,we’llrefertothesekindsofconcurrencybugsas memoryorder-\ningbugs .\nIn orderto guarantee that a critical operation is free fromdata races, and is\ntherefore atomic, we must ensure that none of these three things can happen.\n4.9.2 Implementing Atomicity\nFirst, let’s tackle the problem of making a critical operation atomic (i.e., unin-\nterruptable). Before, we waved our hands a bit and said that by wrapping our\ncritical operations in a mutexlock/unlock pair, we can magically transform\nthem into atomic operations. But how does a mutex actually work?\n4.9.2.1 Atomicity by Disabling Interrupts\nTo prevent other threads from interrupting our operation, we could try dis-\nabling interrupts just prior to performing the operation, making sure to reen-\nable them after the operation has been completed. That would certainly pre-\nvent the kernel from context-switching to another thread in the middle of our\natomic operation. However, this approach only works on a single-core ma-\nchine using preemptive multitasking.\nInterrupts are disabled by executing a machine language instruction (such\nascli, “clearinterruptenablebit,” onanIntelx86architecture). Butthiskind\nof instruction only affects the core that executed it. The other cores would\ncontinuetoruntheirthreads(withinterrupts,andthereforepreemptivemulti-\nthreading, still enabled), and those threads could still interrupt our operation.\nSo this approach has only limited applicability in the real world.\n4.9.2.2 Atomic Instructions\nThe term “atomic” suggests the notion of breaking an operation down into\nsmallerandsmallerpieces,untilwereachagranularitythatisindivisible. This\nidea raises the question: Are there any machine language instructions that are\nnaturally atomic? In other words, does the CPU guarantee that some instruc-\ntions are uninterruptable?\nThe answer to these questions is “yes,” but with a few caveats. There are\nmostcertainlysomemachine languageinstructionsthatcan neverbeassumed\nto execute atomically. Other instructions are atomic, but only when operating\non certain kinds of data. Some CPUs permit virtually any instruction to be\nforcedto execute atomically by specifying a prefix on the instruction in assem-\nbly language. (The Intel x86 ISA’s lockprefix is one example.)\n4.9. Lock-Free Concurrency 293\nThis is good news for concurrent programmers. In fact, it is the existence\nof theseatomic instructions that permits us to implement atomicity tools such\nas mutexes and spin locks, which in turn permit us to make larger-scale oper-\nations atomic.\nDifferentCPUsandISAsprovidedifferentsetsofatomicinstructions,gov-\nernedbydifferentrules. Butwecangeneralizeallatomicinstructionsasfalling\ninto two categories:\n• atomic reads and writes, and\n• atomic read-modify-write (RMW) instructions.\n4.9.2.3 Atomic Reads and Writes\nOn most CPUs, we can be reasonably certain that a read or write of a four-\nbyte-aligned 32-bit integer will be atomic. That being said, every processor is\ndifferent, so it’s important to always consult your CPU’s ISA documentation\nbefore relying on the atomicity of any particular instruction.\nSome CPUs also support atomic reads and writes of smaller or larger ob-\njects, such as single bytes or 64-bit integers, again assuming they are aligned\nto a multiple of their own size. This is true because on most CPUs, reading\nand writing an aligned integer whose width in bits is equal to or smaller than\nthe width of a register (or sometimes the width of a cache line) can be per-\nformed in a single memory access cycle. Because a CPU performs its operation\nin lock-step with a discrete clock, a memory cycle can’t be interrupted, even\nby another core. As a result, the read or write is effectively atomic.\nMisaligned reads and writes usually don’t have this atomicity property.\nThis is because in order to read or write a misaligned object, the CPU usually\ncomposes two aligned memory accesses. As such, the read or write might be\ninterrupted, and we cannot assume it will be atomic. (See Section 3.3.7.1 for\nmore details on alignment.)\n4.9.2.4 Atomic Read-Modify-Write\nAtomic reads and writes aren’t enough to implement atomic operations in a\ngeneral sense. In order to implement a locking mechanism like a mutex, we\nneed to be able to read a variable’s contents from memory, perform some op-\nerationonthatvariable,andthenwritetheresultsbacktomemory,allwithout\nbeing interrupted.\nAll modern CPUs support concurrency by providing at least one atomic\nread-modify-write (RMW) instruction. In the following sections, we’ll have a\nlook at a few different kinds of atomic RMW instructions. Each has its pros\nand cons. All of them can be used to implement a mutex or spin lock.\n294 4. Parallelism and Concurrent Programming\n4.9.2.5 Test and Set\nThesimplestRMWinstructionisknownas test-and-set (TAS).TheTASinstruc-\ntion doesn’t actually test and set a value. Rather, it atomically sets a Boolean\nvariable to 1 (true) and returns its previous value (so that the value can be\ntested).\n// pseudocode for the test-and-set instruction\nbool TAS(bool* pLock)\n{\n// atomically...\nconst bool old = *pLock;\n*pLock = true;\nreturn old;\n}\nThetest-and-setinstructioncanbeusedtoimplementasimplekindoflock\ncalled aspin lock . Here’s some pseudocode illustrating the idea. In this exam-\nple, we using a hypothetical compiler intrinsic called _tas() to emit a TAS\nmachine language instruction into our code. Different compilers will provide\ndifferentintrinsicsforthisinstruction,ifthetargetCPUsupportsit. Forexam-\nple, under Visual Studio the TAS intrinsic is named _interlockedbittest\nandset().\nvoid SpinLockTAS(bool* pLock)\n{\nwhile (_tas(pLock) == true)\n{\n// someone else has lock -- busy-wait...\nPAUSE();\n}\n// when we get here, we know that we successfully\n// stored a value of true into *pLock AND that it\n// previously contained false, so no one else has\n// the lock -- we're done\n}\nHere, the PAUSE() macro indicates the use of a compiler intrinsic, such as\nIntel’s SSE2 _mm_pause(), to reduce power consumption during the busy-\nwait loop. See Section 4.4.6.4 for details on why it’s advisable to use a pause\ninstruction inside a busy-wait loop where possible.\nIt’s important to stress here that the above example is intended for illus-\ntrative purposes only. It is not 100% correct because it lacks proper memory\n4.9. Lock-Free Concurrency 295\nfencing. We’ll present a complete working example of a spin lock in Section\n4.9.7.\n4.9.2.6 Exchange\nSome ISAs like Intel x86 offer an atomic exchange instruction. This instruction\nswapsthecontentsoftworegisters,oraregisterandalocationinmemory. On\nx86,itisatomicbydefaultwhenexchangingaregisterwithmemory(meaning\nit acts as if the instruction had been preceded with the lockprefix).\nHere’s how to implement a spin lock using an atomic exchange instruction.\nIn this example, we’re using Visual Studio’s _InterlockedExchange()\ncompiler intrinsic to emit the Intel x86 xchginstruction into our code. (Again,\nnote that without proper memory fencing this example is incomplete and\nwon’t work reliably. See Section 4.9.7 for a complete implementation.)\nvoid SpinLockXCHG(bool* pLock)\n{\nbool old = true;\nwhile (true)\n{\n// emit the xchg instruction for 8-bit words\n_InterlockedExchange8 (old, pLock);\nif (!old)\n{\n// if we get back false,\n// then the lock succeeded\nbreak;\n}\nPAUSE();\n}\n}\nUnder Microsoft Visual Studio, all of the “interlocked” functions named\nwith a leading underscore are compiler intrinsics —they simply emit the ap-\npropriate assembly language instruction directly into your code. The Win-\ndows SDK provides a set of similarly-named functions without the leading\nunderscore—those functions are implemented in terms of the intrinsic where\npossible,butthey’remuchmoreexpensivebecausetheyinvolvemakingaker-\nnel call.\n4.9.2.7 Compare and Swap\nSomeCPUsprovideaninstructionknownas compare-and-swap (CAS).Thisin-\nstructioncheckstheexistingvalueinamemorylocation,andatomicallyswaps\nit with a new value ifandonlyif the existing value matches an expected value\n296 4. Parallelism and Concurrent Programming\nprovidedbytheprogrammer. Itreturnstrueiftheoperationsucceeded,mean-\ning that the memory location contained the expected value. It returns false if\nthe operation failed because the location’s contents were not as expected.\nCAS can operate on values larger than a Boolean. Variants are typically\nprovided at least for 32- and 64-bit integers, although smaller word sizes may\nalso be supported.\nThe behavior of the CAS instruction is illustrated by the following pseu-\ndocode:\n// pseudocode for compare and swap\nbool CAS(int* pValue, int expectedValue, int newValue)\n{\n// atomically...\nif (*pValue == expectedValue)\n{\n*pValue = newValue;\nreturn true;\n}\nreturn false;\n}\nToimplementanyatomicread-modify-writeoperationusingCAS,wegen-\nerally apply the following strategy:\n1. Read the old value of the variable we’re attempting to update.\n2. Modify the value in whatever way we see fit.\n3. When writing the result, use the CAS instruction instead of a regular\nwrite.\n4. Iterate until the CAS succeeds.\nThe CAS instruction allows us to detect data races, by comparing the value\nthat’s actually in the memory location at the time of the write with its value\nbeforethe invocation of our read-modify-write operation. In the absence of a\nrace, the CAS instruction acts just like a write instruction. But if the value in\nmemory changes between the read and the write, we know that some other\nthread beat us to the punch. In that case, we back off and try again.\nImplementing a spin lock via compare-and-swap would look something\nlike this, again using a hypothetical compiler intrinsic called _cas() to emit\nthe CAS instruction. (This example once again omits the memory fences that\nwould be required to make it work reliably on all hardware—see Section 4.9.7\nfor a fully-functional spin lock.)\n4.9. Lock-Free Concurrency 297\nvoid SpinLockCAS(int* pValue)\n{\nconst int kLockValue = -1; // 0xFFFFFFFF\nwhile (!_cas(pValue, 0, kLockValue))\n{\n// must be locked by someone else -- retry\nPAUSE();\n}\n}\nAnd here’s how we’d implement an atomic increment using CAS.\nvoid AtomicIncrementCAS(int* pValue)\n{\nwhile (true)\n{\nconst int oldValue = *pValue; // atomic read\nconst int newValue = oldValue + 1;\nif (_cas(pValue, oldValue, newValue))\n{\nbreak; // success!\n}\nPAUSE();\n}\n}\nOn the Intel x86 ISA, the CAS instruction is called cmpxchg , and it can be\nemittedwithVisualStudio’s _InterlockedCompareExchange() compiler\nintrinsic.\n4.9.2.8 ABA Problem\nWeshouldmentionthattheCASinstructionsuffersfromaninabilitytodetect\none specific kind of data race. Consider an atomic RMW write operation in\nwhichthereadseesthevalueA.Beforewe’reabletoissuetheCASinstruction,\nanother thread preempts us, or runs on another core, and writes two values\ninto the location we’re trying to atomically update: First it writes the value B,\nand then it writes the value A again. When our CAS instruction does finally\nexecute, it won’t be able to tell the difference between the first A it read and\nthe A that was written by the other thread. Hence it will “think” that no data\nrace occurred, when in fact one did. This is known as the ABAproblem .\n4.9.2.9 Load Linked/Store Conditional\nSome CPUs break the compare-and-swap operation into a pairof instructions\nknown as load linked andstore conditional (LL/SC). The load linked instruction\n298 4. Parallelism and Concurrent Programming\nreadsthevalueofamemorylocationatomically,andalsostorestheaddressin\na special CPU register known as the link register. The store conditional instruc-\ntion writes a value into the given address, but only if the address matches the\ncontents of the link register. It returns true if the write succeeded, or false if it\nfailed.\nAnywrite operation on the bus (including a store conditional) clears the\nlink register to zero. This means that an LL/SC instruction pair is capable\nof detecting data races, because if any write occurs between the LL and SC\ninstructions, the SC will fail.\nAn LL/SC instruction pair is used in much the same way a regular read\nis paired with a CAS instruction. Specifically, an atomic read-modify-write\noperation would be implemented using the following strategy:\n1. Read the old value of the variable via an LL instruction.\n2. Modify the value in whatever way we see fit.\n3. Write the result using the SC instruction.\n4. Iterate until the SC succeeds.\nHere’s how we’d implement an atomic increment using LL/SC (using the\nhypothetical compiler intrinsics _ll()and_sc()to emit the LL and SC in-\nstructions, respectively):\nvoid AtomicIncrementLLSC(int* pValue)\n{\nwhile (true)\n{\nconst int oldValue = _ll(*pValue);\nconst int newValue = oldValue + 1;\nif (_sc(pValue, newValue))\n{\nbreak; // success!\n}\nPAUSE();\n}\n}\nBecause the link register is cleared by anybus write, the SC instruction\nmay fail spuriously. But that doesn’t affect the correctness of an atomic RMW\nimplemented with LL/SC—it just means that our busy-wait loop might end\nup executing a few more iterations than we’d expect.\n4.9. Lock-Free Concurrency 299\nLL\nSCCASloadcomparestore\nload & link\ncheck link & storeWBM2 E M1 D F\nWB\nWBM\nME\nED\nDF\nF\nFigure 4.35. The compare and swap (CAS) instruction reads a memory location, performs a com-\nparison, and then conditionally writes to that same location. It therefore requires two memory\naccess stages, making it more difﬁcult to implement within a simple pipelined CPU architecture\nthan the linked/store conditional (LL/SC) instruction pair, each of which requires only a single\nmemory access stage.\n4.9.2.10 Advantages of LL/SC over CAS\nTheLL/SCinstructionpairofferstwodistinctadvantagesoverthesingleCAS\ninstruction.\nFirst, because the SC instruction fails whenever anywrite is performed on\nthe bus, an LL/SC pair is not prone to the ABA problem.\nSecond, an LL/SC pair is more pipeline-friendly than a CAS instruction.\nThesimplestpipelineiscomprisedoffivestages: fetch,decode,execute,mem-\nory access and register write-back. But a CAS instruction requires two mem-\nory access cycles: One to read the memory location so it can be compared to\nthe expected value, and one to write the result if the comparison passes. This\nmeansthatapipelinethatsupportsCAShastoincludeanadditionalmemory\naccess stage that goes unused most of the time. On the other hand, the LL\nand SC instructions each only require a single memory access cycle, so they\nfit more naturally into a pipeline with only one memory access stage. A com-\nparison of CAS and LL/SC from a pipelining perspective is shown in Figure\n4.35.\n4.9.2.11 Strong and Weak Compare-Exchange\nTheC++11standardlibraryprovidesportablefunctionsforperformingatomic\ncompare-exchange operations. These may be implemented by a CAS instruc-\ntion on some target hardware, or by an LL/SC instruction pair on other hard-\nware. Because of the possibility of spurious failures of the store-conditional\ninstruction, C++11 provides two varieties of compare-exchange: strong and\nweak. Strong compare-exchange “hides” spurious SC failures from the pro-\n300 4. Parallelism and Concurrent Programming\ngrammer, while weak compare-exchange does not. Search for “Strong Com-\npare and Exchange Lawrence Crowl” online for a paper on the rationale be-\nhind strong and weak compare-exchange functions in C++11.\n4.9.2.12 Relative Strength of Atomic RMW Instructions\nIt’s interesting to note that the TAS instruction is weakerthan the CAS and\nLL/SC instructions in terms of achieving consensus between multiple threads\nin a concurrent system. Consensus in this context refers to an agreement be-\ntweenthreadsaboutthevalueofasharedvariable(evenifsomethreadsinthe\nsystem experience a failure).\nBecause the TAS instruction only operates on a Boolean value, it can only\nsolve a problem known as the wait-free consensus problem for two concurrent\nthreads. The CAS instruction operates on a 32-bit value, so it can solve this\nproblem for any number of threads.\nThe topic of wait-free consensus is well beyond our scope here; it’s mostly\nof interest when building fault-tolerant systems. If you’re interested in fault\ntolerance, you can read more about the consensus problem by searching for\n“consensus (computer science)” on Wikipedia.\n4.9.3 Barriers\nInterruptions aren’t the only cause of data race bugs. Compilers and CPUs\nalsoconspiretointroducesubtlebugsintoourconcurrentprogramsbymeans\noftheinstructionreordering optimizationstheyperform,asdescribedinSection\n4.2.5.1.\nThe cardinal rule of compiler optimizations and out-of-order execution is\nthat their optimizations shall have no visible effects on the behavior of asingle\nthread. However, neither the compiler nor the CPU’s control logic has any\nknowledge of what other threads might be running in the system, or what\nthey might be doing. As a result, the cardinal rule isn’t sufficient to prevent\ninstruction reordering optimizations from introducing bugs into concurrent\nprograms.\nThe thread synchronization primitives provided by the operating system\n(mutexes et al.) are carefully crafted to avoid the concurrency bugs that can\nbe caused by instruction reordering optimizations. But now that we’re inves-\ntigating how mutexes are implemented, let’s take a look at how to avoid these\nproblems manually.\n4.9. Lock-Free Concurrency 301\n4.9.3.1 How Instruction Reordering Causes Concurrency Bugs\nAs an example of the kinds of problems instruction reordering can cause in\nconcurrent software, consider again the producer-consumer problem from\nSection 4.6.3. We’ve simplified the example, and removed the mutexes so we\ncan expose the bugs that can be introduced by instruction reordering.\nint32_t g_data = 0;\nint32_t g_ready = 0;\nvoid ProducerThread()\n{\n// produce some data\ng_data = 42;\n// inform the consumer\ng_ready = 1;\n}\nvoid ConsumerThread()\n{\n// wait for the data to be ready\nwhile (!g_ready)\nPAUSE();\n// consume the data\nASSERT(g_data == 42);\n}\nOn a CPU on which aligned reads and writes of 32-bit integers are atomic,\nthis example doesn’t actually require mutexes. However, there’s nothing to\nprevent the compiler or the CPU’s out-of-order execution logic from reorder-\ning the producer’s write of 1 into g_ready so that it occurs beforethe write\nof 42 into g_data . Likewise, in theory the compiler could reorder the con-\nsumer’s check that g_data is equal to 42 so that it happens beforethe while\nloop. So even though all of our readsand writes areatomic, this code may not\nbehave reliably.\nInstruction reordering really happens at the assembly language level, so\nit may be a lot more subtle than a reordering of the statements in a C/C++\nprogram. For example, the following C/C++ code:\nA = B + 1;\nB = 0;\n302 4. Parallelism and Concurrent Programming\nwould produce the following Intel x86 assembly code:\nmov eax,[B]\nadd eax,1\nmov [A],eax\nmov [B],0\nThecompilercouldverywellreordertheinstructionsasfollows, withoutpro-\nducing any noticeable effect in a single-threaded execution:\nmov eax,[B]\nmov [B],0 ;; write to B before A!\nadd eax,1\nmov [A],eax\nIf a second thread were waiting for Bto become zero before reading the value\nofA, it would cease to function correctly if this compiler optimization were to\nbe applied.\nJeffPreshing wrote a great blog post on this topic, available at http://\npreshing.com/20120625/memory-ordering-at-compile-time/. (This is where\nthe assembly language example above comes from.) I highly recommend all\nof Jeff’s posts on concurrent programming, so do be sure to check them out.\n4.9.3.2 Volatile in C/C++ (and Why It Doesn’t Help Us)\nHow can we prevent the compiler from reordering critical sequences of reads\nand writes? In C and C++, the volatile type qualifier guarantees that con-\nsecutivereadsorwritesofavariablecannotbe“optimizedaway”bythecom-\npiler, so this sounds like a promising idea. However, it doesn’t work reliably\nfor a number of reasons.\nThevolatile qualifier in C/C++ was really designed to make memory-\nmappedI/Oandsignal handlersworkreliably. Assuch, theonly guaranteeit\nprovides is that the contents of a variable marked volatile won’t be cached\nin a register—the variable’s value will be re-read directly from memory every\ntime it’s accessed. Some compilers do guarantee that instructions will not be\nreordered across a read or write of a volatile variable, but not all of them\ndo, and some only provide this guarantee when targeting certain CPUs, or\nonly when a particular command-line option is passed to the compiler. The\nC and C++ standards don’t require this behavior, so we certainly cannot rely\non it when writing portable code. (See https://msdn.microsoft.com/en-us/\nmagazine/dn973015.aspx for an in-depth discussion of this topic.)\n4.9. Lock-Free Concurrency 303\nMoreover, the volatile keyword in C/C++ does nothing to prevent the\nCPU’s out-of-order execution logic from reordering the instructions at run-\ntime. And it cannot prevent cache coherency related issues either (see Section\n4.9.3). So,atleastinCandC++,the volatile keywordwon’thelpustowrite\nreliable concurrent software.8\n4.9.3.3 Compiler Barriers\nOne reliable way of preventing the compiler from reordering read and write\ninstructions across critical operation boundaries is to explicitly instruct it not\nto do so. This can be accomplished by inserting a special pseudoinstruction\ninto our code known as a compilerbarrier.\nDifferent compilers express barriers with different syntax. With GCC, a\ncompiler barrier can be inserted via some inline assembly syntax as shown\nbelow; under Microsoft Visual C++, the compiler intrinsic _ReadWrite\nBarrier() has the same effect.\nint32_t g_data = 0;\nint32_t g_ready = 0;\nvoid ProducerThread()\n{\n// produce some data\ng_data = 42;\n// dear compiler: please don't reorder\n// instructions across this barrier!\nasm volatile("""" ::: ""memory"")\n// inform the consumer\ng_ready = 1;\n}\nvoid ConsumerThread()\n{\n// wait for the data to be ready\nwhile (!g_ready)\nPAUSE();\n// dear compiler: please don't reorder\n// instructions across this barrier!\nasm volatile("""" ::: ""memory"")\n8In some languages including Java and C#, the volatile type qualifier doesguarantee atomicity,\nand can be used to implement concurrent data structures and algorithms. See Section 4.9.6 for\nmore on this topic.\n304 4. Parallelism and Concurrent Programming\n// consume the data\nASSERT(g_data == 42);\n}\nThereareotherwaystopreventthecompilerfromreorderinginstructions.\nFor example, most function calls serve as an implicit compiler barrier. This\nmakes sense, because the compiler doesn’t know anything9about the side ef-\nfects of a function call. As such, it can’t assume that the state of memory will\nbe the same before and after the call, which means most optimizations aren’t\npermitted across a function call. Some optimizing compilers do make an ex-\nception to this rule for inline functions.\nUnfortunately, compiler barriers don’t prevent the CPU’s out-of-order ex-\necution logic from reordering instructions at runtime. Some ISAs provide a\nspecial instruction for this purpose (e.g., PowerPC’s isync instruction). In\nSection 4.9.4.5, we’ll learn about a collection of machine language instructions\nknownas memoryfences whichserveasinstructionreorderingbarriersforboth\nthe compiler and the CPU, but more importantly also prevent memoryreorder-\ningbugs. So atomic instructions and fences are all we really need to write\nreliable mutexes, as well as spin locks and other lock-free algorithms.\n4.9.4 Memory Ordering Semantics\nIn Section 4.9.1, we said that in addition to the compiler or CPU actually re-\nordering the machine language instructions in our programs, it’s possible for\nread and write instructions to become effectively reordered in a concurrent sys-\ntem. Specifically,inamulticoremachinewithamultilevelmemorycache,two\nor more cores can sometimes disagree about the apparent order in which a se-\nquence of read and write instructions occurred, even when the instructions\nwereactually executed in the order we intended. Obviously, such disagree-\nments can cause subtle bugs in concurrent software.\nThese mysterious and vexing problems can only occur on a multicore ma-\nchine with a multilevel cache. A single CPU core always “sees” the effects\nof its own read and write instructions in the order they were executed; only\nwhen there are two or more cores can disagreements arise. What’s more, dif-\nferent CPUs have different memory ordering behavior, meaning that these\n9Function calls only serve as implicit barriers when the compiler is unable to “see” the defi-\nnition of the function, such as when the function is defined in a separate translation unit. Link\ntime optimizations (LTO) can introduce concurrency bugs by providing a way for the compiler’s\noptimizer to see the definitions of functions it otherwise could not have seen, thereby effectively\neliminating these implicit barriers.\n4.9. Lock-Free Concurrency 305\nstrange effects can differ from machine to machine when running the exact\nsame source program!\nThankfully, all is not lost. Every CPU is governed by a strict set of rules\nknown as its memory ordering semantics. These rules provide various guaran-\ntees about how reads and writes propagate between cores, and they provide\nprogrammers with the tools necessary to enforce a particular ordering, when\nthe default semantics are insufficient.\nSome CPUs offer only weak guarantees by default, while others provide\nstrongerguaranteesandhencerequirelessinterventiononthepartofthepro-\ngrammer. So, ifwecanunderstandhowtoovercomememoryorderingissues\nonaCPUwiththeweakestmemoryorderingsemantics,wecanbeprettysure\nthose techniques will also work on CPUs with stronger default semantics.\n4.9.4.1 Memory Caching Revisited\nIn order to understand how these mysterious memory reordering effects can\noccur, we need to look moreclosely at how a multilevel memory cache works.\nInSection3.5.4,wedescribedindetailhowamemorycacheavoidsthevery\nhighmemoryaccesslatencyofmainRAMbykeepingfrequently-useddatain\nthe cache. This means that as long as a data object is present in the cache, the\nCPU will always try to work with that cached copy, rather than reaching out\nto access the copy in main RAM.\nLet’s briefly review how this works by considering the following simple\n(and totally contrived) function:\nconstexpr int COUNT = 16;\nalignas(64) float g_values[COUNT];\nfloat g_sum = 0.0f;\nvoid CalculateSum()\n{\ng_sum = 0.0f;\nfor (int i = 0; i < COUNT; ++i)\n{\ng_sum += g_values[i];\n}\n}\nThe first statement sets g_sumto zero. Presuming that the contents of g_sum\naren’talreadyintheL1cache,thecachelinecontainingitwillbereadintoL1at\nthispoint. Likewise, onthefirstiterationoftheloop, thecachelinecontaining\nalloftheelementsofthe g_values arraywillbeloadedintoL1. (Theyshould\nallfitpresumingourcachelinesareatleast64bytes wide, because wealigned\n306 4. Parallelism and Concurrent Programming\nthearraytoa64-byteboundarywithC++11’s alignas specifier.) Subsequent\niterations will read the copy of g_values that resides in the L1 cache, rather\nthan reading them from main RAM.\nDuring each iteration, g_sum is updated. The compiler might optimize\nthis by keeping the sum in a register until the end of the loop. But whether\nor not this optimization is performed, we know that the g_sumvariable will\nbe written to at some point during this function. When that happens, again\nthe CPU will write to the copyofg_sumthat exists in the L1 cache, rather than\nwriting directly to main RAM.\nEventually, of course, the “master” copy of g_sumhas to be updated. The\nmemorycachehardwaredoesthisautomaticallybytriggeringa write-back op-\neration that copies the cache line from L1 back to main RAM. But the write-\nback doesn’t usually happen right away; it’s typically deferred until the mod-\nified variable is read again.10\n4.9.4.2 Multicore Cache Coherency Protocols\nIn a multicore machine, memory caching gets a lot more complicated. Figure\n4.36 illustrates a simple dual-core machine, in which each core has its own\nprivateL1cache,andthetwocoresshareanL2cacheandalargebankofmain\nRAM. To keep the following discussion as simple as possible, let’s ignore the\nL2 cache and treat it as being roughly equivalent to main RAM.\nSupposethatthesimplifiedproducer-consumerexampleshowninSection\n4.9.3.3 is running on this dual-core machine. The producer thread runs on\nCore 1, and the consumer thread runs on Core 2. Let’s further assume for the\npurposes of this discussion that none of the instructions in either thread have\nbeen reordered.\nint32_t g_data = 0;\nint32_t g_ready = 0;\nvoid ProducerThread() // running on Core 1\n{\ng_data = 42;\n// assume no instruction reordering across this line\ng_ready = 1;\n}\n10Some memory cache hardware does allow cached writes to write-through to main RAM im-\nmediately. We can safely ignore write-through caches for the purposes of the present discussion.\n4.9. Lock-Free Concurrency 307\nCore 0 Core 1\nMemory ControllerInterconnect Bus (ICB)L10 L11\nL2 & Main RAM\nFigure 4.36. A dual core machine with a local L1 cache per core, connected to a memory controller\nvia the interconnect bus (ICB). The memory controller implements a cache coherency protocol\nsuch as MESI to ensure that both cores have a consistent view of the contents of memory within\nthe CPU’s cache coherency domain.\nvoid ConsumerThread() // running on Core 2\n{\nwhile (!g_ready)\nPAUSE();\n// assume no instruction reordering across this line\nASSERT(g_data == 42);\n}\nNow consider what happens when the producer (on Core 1) writes to\ng_ready. In the name of efficiency, this write causes Core 1’s L1 cache to be\nupdated, but it won’t trigger a write-back to main RAM until some time later.\nThis means that for some finite amount of time after the write has occurred,\nthe most up-to-date value of g_ready don’t exist anywhere but inside Core\n1’s L1 cache.\nLet’ssaythattheconsumer(runningonCore2)attemptstoread g_ready\nat some time after the producer set it to 1. Like Core 1, Core 2 prefers to\nreadfromthecachewheneverpossible, toavoidthehighcostofreadingmain\nRAM.Core2’slocalL1cachedoesnotcontainacopyof g_ready ,butCore1’s\nL1 cache does. So ideally Core 2 would like to ask Core 1 for its copy, because\nthat would still be a lot less expensive than getting the data from main RAM.\nAnd in this particular case, doing so would also have the distinct advantage\nof returning the most up-to-date value.\nAcachecoherencyprotocol isacommunicationmechanismthatpermitscores\nto share data between their local L1 caches in this way. Most CPUs use either\nthe MESI or MOESI protocol.\n308 4. Parallelism and Concurrent Programming\n4.9.4.3 The MESI Protocol\nUnder the MESI protocol, each cache line can be in one of four states:\n•Modified. This cache line has been modified (written to) locally.\n•Exclusive. The main RAM memory block corresponding to this cache\nline exists onlyin thiscore’s L1 cache—no other core has a copy of it.\n•Shared. The main RAM memory block corresponding to this cache line\nexists in morethan one core’s L1 cache, and all cores have an identical\ncopy of it.\n•Invalid. Thiscachelinenolongercontainsvaliddata—thenextreadwill\nneed to obtain the line either from another core’s L1 cache, or from main\nRAM.\nThe MOESI protocol adds another state named Owned, which allows cores to\nshare modified data without writing it back to main RAM first. We’ll focus on\nMESI here in the name of simplicity.\nUnder the MESI protocol, all cores’ L1 caches are connected via a special\nbus called the interconnect bus (ICB). Collectively, the L1 caches, any higher-\nlevel caches, and main RAM form what is known as a cache coherency domain .\nThe protocol ensures that all cores have a consistent “view” of the data in this\ndomain.\nWe can get a feel for how the MESI state machine works by returning to\nour example.\n• Let’s assume that Core 1 (the producer) first tries to read the current\nvalue of g_ready forsome reason. Presuming that this variable doesn’t\nalready exist in any core’s L1 cache, the cache line that contains it is\nloaded into Core 1’s L1 cache. The cache line is put into the Exclusive\nstate, meaning that no other core has this line.\n• Now let’s say that Core 2 (the consumer) attempts to read g_ready. A\nReadmessage is sent over the ICB. Core 1 has this cache line, so it re-\nsponds with a copy of the data. At this point, the cache line is put into\ntheShared state on both cores, indicating that both have an identical\ncopy of the line.\n• Next, the producer on Core 1 writes a 1 into g_ready . This updates the\nvalueinCore1’sL1cache,andputsitscopyofthelineintothe Modified\nstate. An Invalidate messageissentacrosstheICB,causingCore2’scopy\nof the line to be put into the Invalid state. This indicates that Core 2’s\ncopy of the line containing g_ready is no longer up-to-date.\n4.9. Lock-Free Concurrency 309\n• The next time that Core 2 (the consumer) tries to read g_ready, it finds\nthatitslocally-cachedcopyis Invalid. Itsendsa Readmessageacrossthe\nICB and obtains the newly-modified line from Core 1’s L1 cache. This\ncauses both cores’ cache lines to be put into the Shared state once again.\nIt also triggers a write-back of the line to main RAM.\nA complete discussion of the MESI protocol is beyond our scope, but this ex-\nample should give you a good feel for how it works to allow multiple cores to\nshare data between their L1 caches while minimizing accesses to main RAM.\n4.9.4.4 How MESI Can Go Wrong\nBasedonthediscussionoftheMESIprotocolintheprecedingsection,itwould\nseem that the problem of data sharing between L1 caches in a multicore ma-\nchine has been solved in a watertight way. How, then, can the memory order-\ning bugs we’ve hinted at actually happen?\nThere’s a one-word answer to that question: Optimization. On most hard-\nware, the MESI protocol is highly optimized to minimize latency. This means\nthat some operations aren’t actually performed immediately when messages\nare received over the ICB. Instead, they are deferred to save time. As with\ncompiler optimizations and CPU out-of-order execution optimizations, MESI\noptimizations are carefully crafted so as to be undetectable by a single thread.\nBut, as you might expect, concurrent programs once again get the raw end of\nthis deal.\nFor example, our producer (running on Core 1) writes 42 into g_data and\nthen immediately writes 1 into g_ready. Under certain circumstances, op-\ntimizations in the MESI protocol can cause the new value of g_ready to be-\ncome visible to other cores within the cache coherency domain beforethe up-\ndatedvalueof g_data becomesvisible. Thiscanhappen,forexample,ifCore\n1 already has g_ready’s cache line in its local L1 cache, but does nothave\ng_data’s line yet. This means that the consumer (on Core 2) can potentially\nsee a value of 1 for g_ready beforeit sees a value of 42 in g_data, resulting in\na data race bug.\nThis state of affairs can be summarized as follows:\nOptimizations within a cache coherency protocol can make two\nreadand/orwriteinstructions appeartohappen, fromthepointof\nview of other cores in the system, in an order that is opposite to\nthe order in which the instructions were actually executed.\n310 4. Parallelism and Concurrent Programming\n4.9.4.5 Memory Fences\nWhen the apparent order of two instructions gets reversed by our cache co-\nherencyprotocol,wesaythatthefirstinstruction(inprogramorder)has passed\nthe second. There are four ways in which one instruction can pass another:\n1. A read can pass another read,\n2. a read can pass a write,\n3. a write can pass another write, or\n4. a write can pass a read.\nTo prevent the memory effects of a read or write instruction passing other\nreadsand/orwrites,modernCPUsprovidespecialmachinelanguageinstruc-\ntions known as memory fences, also known as memorybarriers .\nIn theory, a CPU could provide individual fence instructions to prevent\neachofthesefourcasesfromhappening. Forexample,a ReadRead fencewould\nonlypreventreadsfrompassingotherreads,butwouldnotpreventanyofthe\nother cases. Also, a fence instruction could be unidirectional or bidirectional.\nAone-wayfencewouldguaranteethatnoreadsorwritesthatprecedeitinpro-\ngram order can end up having an effect after it, but not vice-versa. A bidirec-\ntionalfence, ontheotherhand, wouldprevent“leakage”ofmemoryeffectsin\neither direction across the fence. Theoretically, then, we could imagine a CPU\nthatprovidestwelvedistinctfenceinstructions—abidirectional, forward, and\nreverse variant of each of the four basic fence types listed above.\nThankfully real CPUs don’t usually provide all twelve kinds of fences. In-\nstead, an ISA typically specifies a handful of fence instructions which serve as\ncombinations of these theoretical fence types.\nThe strongest kind of fence is called a full fence. It ensures that all reads\nand writes occurring before the fence in program order will never appear to\nhave occurred after it, and likewise that all reads and writes occurring after it\nwill never appear to have happened before it. In other words, a full fence is a\ntwo-way barrier that affects both reads and writes.\nA full fence is actually very expensive to realize in hardware. CPU de-\nsigners don’t like forcing programmers to use an expensive construct when a\ncheaper one will do, so most CPUs provide a variety of less-expensive fence\ninstructions which provide weaker guarantees than those provided by a full\nfence.\nAll fence instructions have two very useful side-effects:\n1. They serve as compiler barriers, and\n4.9. Lock-Free Concurrency 311\n2. they prevent the CPU’s out-of-order logic from reordering instructions\nacross the fence.\nThis means that when we use a fence to prevent the memory ordering bugs\nthatarecausedbyourCPU’scachecoherencyprotocol,italsoservestoprevent\ninstruction reordering. So atomic instructions and memory fences are all we\nreallyneedtowritereliablemutexesandspinlocks,andtowriteother lock-free\nalgorithms as well.\n4.9.4.6 Acquire and Release Semantics\nNomatterwhatthespecificfenceinstructionslooklikeunderaparticularISA,\nwe can reason about their effects by thinking in terms of the semantics they\nprovide—in other words, the guarantees they enforce about the behavior of\nreads and writes in the system.\nMemory ordering semantics are really properties of read or write instruc-\ntions, not properties of the fences themselves. The fences merely provide a\nway for programmers to ensure that a read or write instruction has a particu-\nlar memory ordering semantic. There are really only three memory ordering\nsemantics we typically need to worry about:\n•Releasesemantics. This semantic guarantees that a writeto shared mem-\nory can never be passed by any other read or write that precedes it in\nprogram order. When this semantic is applied to a shared write, we call\nit awrite-release . This semantic operates in the forward direction only—it\nsays nothing about preventing memory operations that occur after the\nwrite-release from appearing to happen before it.\n•Acquire semantics. This semantic guarantees that a readfrom shared\nmemory can never be passed by any other read or write that occurs af-\nterit in program order. When this semantic is applied to a shared read,\nwe call it a read-acquire . This semantic operates in the reversedirection\nonly—it does nothing to prevent memory operations that occur before\nthe read-acquire from having their effects seen after it.\n•Fullfence semantics. Thisbidirectionalsemanticensuresthatallmemory\noperationsappeartooccurinprogramorderacrosstheboundarycreated\nbyafenceinstructioninthecode. Noreadorwritethatoccursbeforethe\nfence in program order can appear to have occurred after the fence, and\nlikewise no read or write that is after the fence in program order can\nappear to have occurred before it.\n312 4. Parallelism and Concurrent Programming\nTheindividualfenceinstructionsprovidedbyanyparticularISAtypicallypro-\nvideatleastoneofthesethreememoryorderingsemantics. Thedetailsofhow\neach fence instruction actually provides these semantic guarantees are CPU-\nspecific, and for the most part as concurrent programmers, we don’t care. As\nlongaswecanexpresstheconceptofwrite-release,read-acquireandfullfence\ninoursourcecode,weshouldbeabletowriteareliablespinlockorcodeother\nlock-free algorithms.\n4.9.4.7 When to Use Acquire and Release Semantics\nAwrite-release is most often used in a producer scenario—in which a thread\nperformstwoconsecutivewrites(e.g.,writingto g_data andthen g_ready),\nand we need to ensure that all other threads will see the two writes in the cor-\nrect order. We can enforce this ordering by making the second of these two\nwrites a write-release. To implement this, a fence instruction that provides re-\nleasesemantics is placed beforethe write-release instruction. Technically speak-\ning, when a core executes a fence instruction with release semantics, it waits\nuntil all prior writes have been fully committed to memory within the cache\ncoherency domain before executing the second write (the write-release).\nAread-acquire is typically used in a consumer scenario—in which a thread\nperforms two consecutive readsin which the second is conditional on the first\n(e.g.,onlyreading g_data afterareadoftheflag g_ready comesback true).\nWe enforce this ordering by making sure that the first read is a read-acquire.\nTo implement this, a fence instruction that provides acquiresemantics is placed\nafterthe read-acquire instruction. Technically speaking, when a core executes\na fence instruction with acquire semantics, it waits until all writes from other\ncores have been fully flushed into the cache coherency domain before it con-\ntinues on to execute the second read. This ensures that the second read will\nnever appear to have occurred before the read-acquire.\nHere’sourproducer-consumerexampleagain,infulllock-freeglory,using\nacquire and release fences to impose the necessary memory ordering seman-\ntics:\nint32_t g_data = 0;\nint32_t g_ready = 0;\nvoid ProducerThread() // running on Core 1\n{\ng_data = 42;\n// make the write to g_ready into a write-release\n// by placing a release fence *before* it\nRELEASE_FENCE();\n4.9. Lock-Free Concurrency 313\ng_ready = 1;\n}\nvoid ConsumerThread() // running on Core 2\n{\n// make the read of g_ready into a read-acquire\n// by placing an acquire fence *after* it\nwhile (!g_ready)\nPAUSE();\nACQUIRE_FENCE() ;\n// we can now read g_data safely...\nASSERT(g_data == 42);\n}\nFor an excellent in-depth presentation of exactly why acquire and release\nfences are required under the MESI cache coherency protocol, see http://\nwww.swedishcoding.com/2017/11/10/multi-core-programming-and-cache-\ncoherency/.\n4.9.4.8 CPU Memory Models\nWementionedinSection4.9.4thatsomeCPUsprovidestrongermemoryorder\nsemantics by default than others. On a CPU with strong memory semantics,\nread and/or write instructions behave like a certain kind of fence by default,\nwithout the programmer having to specify a fence instruction explicitly. For\nexample,theDECAlphahasnotoriouslyweaksemanticsbydefault,requiring\ncareful fencing in almost all situations. At the other end of the spectrum, an\nIntel x86 CPU has quite strong memory ordering semantics by default. For a\ngreat discussion of weak and strong memory ordering, see http://preshing.\ncom/20120930/weak-vs-strong-memory-models/.\n4.9.4.9 Fence Instructions on Real CPUs\nNow that we understand the theory behind memory ordering semantics, let’s\ntake a very brief look at memory fence instructions on some real CPUs.\nThe Intel x86 ISA specifies three fence instructions: sfence provides re-\nlease semantics, lfence provides acquire semantics, and mfence acts as a\nfull fence. Certain x86 instructions may also be prefixed by a lockmodifier\nto make them behave atomically, and to provide a memory fence prior to ex-\necution of the instruction. The x86 ISA is strongly ordered by default, mean-\ning that fences aren’t actually required in many cases where they would be\non CPUs with weaker default ordering semantics. But there are some cases\nin which these fence instructions arerequired. For an example, see the post\n314 4. Parallelism and Concurrent Programming\nentitled, “Who ordered memory fences on an x86?” by Bartosz Milewski\n(https://bit.ly/2HuXpfo).\nThe PowerPC ISA is quite weakly ordered, so explicit fence instructions\nare usually required to ensure correct semantics. The PowerPC makes a dis-\ntinction between reads and writes to memory versus reads and writes to I/O\ndevices, and hence it offers a variety of fence instructions that differ primar-\nily in how they handle memory versus I/O. A full fence on PowerPC is pro-\nvided by the syncinstruction, but there’s also a “lightweight” fence called\nlwsync, a fence for I/O operations called eieio (ensure in-order execu-\ntion of I/O), and even a pure instruction reordering barrier isyncthat does\nnot provide any memory ordering semantics. You can read more about the\nPowerPC’s fence instructions here: https://www.ibm.com/developerworks/\nsystems/articles/powerpc.html.\nThe ARM ISA provides a pure instruction reordering barrier called isb,\ntwo full memory fence instructions dmbanddsb, a one-way read-acquire in-\nstruction ldarand aone-way write-release instruction stlr. Interestingly,\nthis ISA rolls acquire and release semantics into the read and write instruc-\ntions themselves, rather than as separate fence instructions. For more infor-\nmation, see http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.\nden0024a/CJAIAJFI.html.\n4.9.5 Atomic Variables\nUsing atomic instructions and memory fences directly can be tedious and\nerror-prone, not to mention being totally non-portable. Thankfully, as\nof C++11, the class template std::atomic<T> allows virtually any data\ntype to be converted into an atomic variable. (A specialized class called\nstd::atomic_flag encapsulates an atomic Boolean variable.) In addition\ntoatomicity,the std::atomic familyoftemplatesprovidesitsvariableswith\n“full fence” memory ordering semantics by default (although weaker seman-\ntics can be specified if desired). This frees us to write lock-free code without\nhaving to worry about any of the three causes of data race bugs.\nUsing these facilities, our producer-consumer example can be written as\nfollows:\nstd::atomic< float> g_data;\nstd::atomic_flag g_ready = false;\nvoid ProducerThread()\n{\n// produce some data\ng_data = 42;\n4.9. Lock-Free Concurrency 315\n// inform the consumer\ng_ready = true;\n}\nvoid ConsumerThread()\n{\n// wait for the data to be ready\nwhile (!g_ready)\nPAUSE();\n// consume the data\nASSERT(g_data == 42);\n}\nNotice that this code looks nearly identical to the erroneous code we first pre-\nsented when talking about race conditions in Section 4.5.3. By simply wrap-\nping our variables in std::atomic , we’ve converted a concurrent program\nthat is prone to data race bugs into one that is race-free.\nUnder the hood, the implementations of std::atomic<T> andstd::\natomic_flag are, of course, complex. The standard C++ library has to be\nportable, so its implementation makes use of whichever atomic and barrier\nmachine language instructions happen to be available on the target platform.\nWhat’s more, the std::atomic<T> template can wrap any type imagin-\nable, but of course CPUs don’t provide atomic instructions for manipulating\narbitrarily-sized data objects. As such, the std::atomic<T> template must\nbe specialized by size: When the template is applied to a 32- or 64-bit type, it\ncan be implemented without locks, using atomic machine language instruc-\ntions directly. But when it is applied to larger types, mutex locks are used\nto ensure correct atomic behavior. (You can call is_lock_free() on any\natomic variable to find out whether its implementation really is lock-free on\nyour target hardware.)\n4.9.5.1 C++ Memory Order\nBy default, C++ atomic variables make use of full memory barriers, to ensure\nthat they will work correctly in all possible use cases. However, it is possible\ntorelaxthese guarantees, by passing a memory order semantic (an optional ar-\ngument of type std::memory_order) to functions that manipulate atomic\nvariables. Thedocumentationregarding std::memory_order isprettycon-\nfusing, so let’s demystify it. Here are the possible memory order settings, and\nwhat they mean:\n1.Relaxed. An atomic operation performed with relaxed memory order\n316 4. Parallelism and Concurrent Programming\nsemantics guarantees onlyatomicity. No barrier or fence is used.\n2.Consume. A read performed with consume semantics guarantees that\nno other read or write withinthesamethread can be reordered before this\nread. In other words, this semantic only serves to prevent compiler op-\ntimizations andout-of-order execution from reordering the instructions—\nit does nothing to ensure any particular memory ordering semantics\nwithin the cache coherency domain. It is typically implemented with\nan instruction reordering barrier like the PowerPC’s isyncinstruction.\n3.Release. A write performed with releasesemantics guarantees that no\nother read or write in this thread can be reordered after it, and the write\nis guaranteed to be visible by other threads reading the same address. It\nemploys a release fence in the CPU’s cache coherency domain to accom-\nplish this.\n4.Acquire. A read performed with acquiresemantics guarantees consume\nsemantics, and in addition it guarantees that writes to the same address\nby other threads will be visible to this thread. It does this via an acquire\nfencein the CPU’s cache coherency domain.\n5.Acquire/Release. This semantic (the default) is the safest, because it ap-\nplies afullmemory fence.\nIt’s important to realize that using a memory ordering specifier doesn’t guar-\nantee that a particular semantic will actually be used on every platform. All it\ndoes is to guarantee that the semantics will be at leastthat strong—a stronger\nsemantic might be employed on some target hardware. For example, on Intel\nx86, the relaxed memory order isn’t possible because the CPU’s default mem-\nory ordering semantics are relatively strong. On an Intel CPU, any request for\na relaxed read operation will actually end up having acquiresemantics.\nUsing these memory order specifiers requires switching from std::\natomic’s overloaded assignment and cast operators to explicit calls to\nstore() andload(). Here’soursimpleproducer-consumerexampleagain,\nthis time using std::memory_order specifiers to provide our release and\nacquire barriers:\nstd::atomic< float> g_data;\nstd::atomic< bool> g_ready = false;\nvoid ProducerThread()\n{\n// produce some data\n4.9. Lock-Free Concurrency 317\ng_data.store(42, std::memory_order_relaxed );\n// inform the consumer\ng_ready.store(true, std::memory_order_release );\n}\nvoid ConsumerThread()\n{\n// wait for the data to be ready\nwhile (!g_ready. load(std::memory_order_acquire ))\nPAUSE();\n// consume the data\nASSERT(g_data. load(std::memory_order_relaxed ) == 42);\n}\nIt’s important to remember the 80/20 rule when using “relaxed” memory\nordering semantics like this. These semantics are easy to get wrong, so you\nshould probably only use non-default memory ordering with std::atomic\nwhen you can prove, via profiling, that the performance improvement is re-\nally necessary—and that changing your code to use explicit memory ordering\nsemantics does, in fact, produce the benefits you expect!\nAcompletediscussionofhowtousememoryorderingsemanticsinC++11\nis beyond our scope, but you can read more by searching online for the article\nentitled, “Implementing Scalable Atomic Locks for Multi-Core Intel® EM64T\nand IA32 Architectures” by Michael Chynoweth. This forum discussion of-\nfers some interesting insights as well, and illustrates just how complex this\nkind of programming can get: https://groups.google.com/forum/#!topic/\nboost-developers-archive/Qlrat5ASrnM.\n4.9.6 Concurrency in Interpreted Programming Languages\nThus far we’ve only discussed concurrency in the context of compiled lan-\nguages like C and C++, and in assembly language. These languages com-\npile or assemble down to raw machine code that is executed directly by the\nCPU.Assuch, atomicoperationsandlocksmustbeimplementedwiththeaid\nof special machine language instructions that provide atomic operations and\ncache coherent memory barriers, with additional help from the kernel (which\nmakes sure threads are put to sleep and awoken appropriately) and the com-\npiler (which respects barrier instructions when optimizing the code).\nThe story is somewhat different for interpreted programming languages\nlike Java and C#. Programs written in these languages execute in the context\nof avirtual machine (VM): Java programs run inside the Java Virtual Machine\n318 4. Parallelism and Concurrent Programming\n(JVM), and C# programs run within the context of the Common Language\nRuntime (CLR). A virtual machine is essentially a software emulation of a\nCPU, reading bytecoded instructions one by one and executing them. A VM\nalso acts a bit like an emulated operating system kernel: It provides its own\nnotion of “threads” (comprised of bytecoded instructions), and it handles all\nofthedetailsofschedulingthosethreadsitself. BecausetheoperationofaVM\nis implemented entirely in software, an interpreted language like Java or C#\ncanprovidepowerfulconcurrentsynchronizationfacilitiesthatarenotascon-\nstrained by the hardware as they would be in a compiled language like C or\nC++.\nOne example of this principle in action is the volatile type qualifier. We\nsaid in Section 4.6 that in C/C++ a volatile variable is not atomic. How-\never, in Java and C#, the volatile type qualifier doesguarantee atomicity.\nOperations on volatile variables in these languages cannot be optimized, and\ntheycannotbeinterruptedbyanotherthread. Moreover, allreadsofavolatile\nvariable in Java and C# are effectively performed directly from main memory\nrather than from the cache, and likewise all writes are effectively written to\nmain RAM rather than to the cache. All of these guarantees can be provided\nin part because the virtual machine has full control over the execution of the\nbytecoded instruction streams that comprise each app.\nAfulldiscussionoftheatomicityandthreadsynchronizationfacilitiespro-\nvided by interpreted languages like C# and Java is well beyond the scope of\nthis book. But now that you have a solid understanding of the principles\nbehind atomicity at the lowest levels, understanding these facilities in other\nhigher-level languages should be a snap. For further reading, the following\nwebsites are a good start:\n• C#: Searchfor“ParallelProcessingandConcurrencyinthe.NETFrame-\nwork” on https://docs.microsoft.com.\n• Java: https://docs.oracle.com/javase/tutorial/essential/concurrency/.\n4.9.7 Spin Locks\nWhen discussing atomic machine language instructions in Section 4.9.2.2, we\npresented some code snippets illustrating how a spin lock might be imple-\nmentedusingeachofthoseinstructions. Butbecauseofinstructionreordering\nandmemoryorderingsemantics,thoseexamplesweren’t100%correct. Inthis\nsection, we’ll present an industrial-strength spin lock, and then explore a few\nuseful variants.\n4.9. Lock-Free Concurrency 319\n4.9.7.1 Basic Spin Lock\nAspinlockcanbeimplementedusinga std::atomic_flag,eitherwrapped\ninaC++classoraccessedviaasimplefunctionalAPI.Thespinlockisacquired\nbyusingaTASinstructiontoatomicallysettheflagto true,retryinginawhile\nloop until the TAS succeeds. It is unlocked by atomically writing a value of\nfalseinto the flag.\nWhen acquiring a spin lock, it’s important to use read-acquire memory or-\ndering semantics to read the lock’s current contents as part of the TAS opera-\ntion. This fence guards against the rare scenario in which the lock is observed\nas having been released when in reality some other thread hasn’t completely\nexited its critical section yet. In C++11, this can be accomplished by pass-\ningstd::memory_order_acquire to the test_and_set() call. In raw\nassembly language, we’d place an acquire fence instruction after the TAS in-\nstruction.\nWhen releasing a spin lock, it’s likewise important to use write-release se-\nmantics to ensure that all writes performed afterthe call to Unlock() aren’t\nobserved by other threads as if they had happened beforethe lock was re-\nleased.\nHere’s a complete implementation of a TAS-based spin lock, using correct\nand minimal memory ordering semantics:\nclass SpinLock\n{\nstd::atomic_flag m_atomic;\npublic:\nSpinLock() : m_atomic(false) { }\nbool TryAcquire()\n{\n// use an acquire fence to ensure all subsequent\n// reads by this thread will be valid\nbool alreadyLocked = m_atomic. test_and_set (\nstd::memory_order_acquire );\nreturn !alreadyLocked;\n}\nvoid Acquire()\n{\n// spin until successful acquire\nwhile (!TryAcquire())\n{\n320 4. Parallelism and Concurrent Programming\n// reduce power consumption on Intel CPUs\n// (can substitute with std::this_thread::yield()\n// on platforms that don't support CPU pause, if\n// thread contention is expected to be high)\nPAUSE();\n}\n}\nvoid Release()\n{\n// use release semantics to ensure that all prior\n// writes have been fully committed before we unlock\nm_atomic.clear(std::memory_order_release );\n}\n};\n4.9.7.2 Scoped Locks\nIt’s often inconvenient and error-prone to manually unlock a mutex or spin\nlock, especially when the function using the lock has multiple return sites. In\nC++wecanuseasimplewrapperclasscalleda scopedlock toensurethatalock\nisautomaticallyreleasedwhenaparticularscopeisexited. Itworksbysimply\nacquiring the lock in the constructor, and releasing it in the destructor.\ntemplate<class LOCK>\nclass ScopedLock\n{\ntypedef LOCK lock_t;\nlock_t* m_pLock;\npublic:\nexplicit ScopedLock(lock_t& lock) : m_pLock(&lock)\n{\nm_pLock->Acquire();\n}\n~ScopedLock ()\n{\nm_pLock->Release();\n}\n};\nThis scoped lock class can be used with any spin lock or mutex that\nhas a conformant interface (i.e., any lock class that supports the functions\nAcquire() andRelease()). Here’s how it might be used:\n4.9. Lock-Free Concurrency 321\nSpinLock g_lock;\nint ThreadSafeFunction ()\n{\n// the scoped lock acts like a ""janitor""\n// because it cleans up for us!\nScopedLock<decltype(g_lock)> janitor(g_lock);\n// do some work...\nif (SomethingWentWrong())\n{\n// lock will be released here\nreturn -1;\n}\n// so some more work...\n// lock will also be released here\nreturn 0;\n}\n4.9.7.3 Reentrant Locks\nAvanilla spin lock will causea threadto deadlock if itever tries to reacquirea\nlock that it already holds. This can happen whenever two or more thread-safe\nfunctionsattempttocalloneanotherreentrantlyfromwithinthesamethread.\nFor example, given two functions:\nSpinLock g_lock;\nvoid A()\n{\nScopedLock<decltype(g_lock)> janitor(g_lock);\n//do some work...\n}\nvoid B()\n{\nScopedLock<decltype(g_lock)> janitor(g_lock);\n//do some work...\n// make a call to A() while holding the lock\nA(); // deadlock!\n322 4. Parallelism and Concurrent Programming\n// do some more work...\n}\nWe can relax this reentrancy restriction if we can arrange for our spin lock\nclass to cache the id of the thread that has locked it. That way, the lock can\n“know”thedifferencebetweenathreadtryingtoreacquireitsownlock(which\nwewishtoallow)versusathreadtryingtograbalockthat’salreadyheldbya\ndifferentthread(whichshouldcausethecallertowait). Tomakesurethatcalls\ntoAcquire() andRelease() aredoneinmatchingpairs, we’llalsowantto\nadd a reference count to the class. Here’s a functional implementation, using\nappropriate memory fencing:\nclass ReentrantLock32\n{\nstd::atomic <std::size_t> m_atomic;\nstd::int32_t m_refCount;\npublic:\nReentrantLock32() : m_atomic(0), m_refCount(0) { }\nvoid Acquire()\n{\nstd::hash<std::thread::id> hasher;\nstd::size_t tid = hasher(std::this_thread::get_id());\n// if this thread doesn't already hold the lock...\nif (m_atomic. load(std::memory_order_relaxed ) != tid)\n{\n// ... spin wait until we do hold it\nstd::size_t unlockValue = 0;\nwhile (!m_atomic. compare_exchange_weak (\nunlockValue,\ntid,\nstd::memory_order_relaxed ,// fence below!\nstd::memory_order_relaxed))\n{\nunlockValue = 0;\nPAUSE();\n}\n}\n// increment reference count so we can verify that\n// Acquire() and Release() are called in pairs\n++m_refCount;\n4.9. Lock-Free Concurrency 323\n// use an acquire fence to ensure all subsequent\n// reads by this thread will be valid\nstd::atomic_thread_fence (std::memory_order_acquire);\n}\nvoid Release()\n{\n// use release semantics to ensure that all prior\n// writes have been fully committed before we unlock\nstd::atomic_thread_fence (std::memory_order_release);\nstd::hash<std::thread::id> hasher;\nstd::size_t tid = hasher(std::this_thread::get_id());\nstd::size_t actual = m_atomic.load(std::memory_order_relaxed);\nassert(actual == tid);\n--m_refCount;\nif (m_refCount == 0)\n{\n// release lock, which is safe because we own it\nm_atomic.store(0, std::memory_order_relaxed );\n}\n}\nbool TryAcquire()\n{\nstd::hash<std::thread::id> hasher;\nstd::size_t tid = hasher(std::this_thread::get_id());\nbool acquired = false;\nif (m_atomic. load(std::memory_order_relaxed ) == tid)\n{\nacquired = true;\n}\nelse\n{\nstd::size_t unlockValue = 0;\nacquired = m_atomic. compare_exchange_strong (\nunlockValue,\ntid,\nstd::memory_order_relaxed ,// fence below!\nstd::memory_order_relaxed);\n}\nif (acquired)\n{\n++m_refCount;\n324 4. Parallelism and Concurrent Programming\nstd::atomic_thread_fence (\nstd::memory_order_acquire );\n}\nreturn acquired;\n}\n};\n4.9.7.4 Readers-Writer Locks\nIn a system in which multiple threads can read and write a shared data object,\nwe could control access to the object using a mutex or a spin lock. However,\nmultiple threads should be allowed to readthe shared object concurrently. It’s\nonly when the shared object is being mutated that we need to ensure mutual\nexclusivity. Whatwe’dlikeisakindoflockthatallowsanynumberofreaders\nto acquire it concurrently. Whenever a writer thread attempts to acquire the\nlock,itshouldwaituntilallreadersarefinished,andthenitshouldacquirethe\nlock in a special “exclusive” mode that prevents any other readers or writers\nfrom gaining access until it has completed its mutation of the shared object.\nThis kind of lock is called a readers-writerlock (also known as a shared-exclusive\nlockor apush lock ).\nWe can implement a readers-writer lock in a manner similar to how we\nimplemented our reentrant lock. However, instead of storing the thread id in\nthe atomic variable, we’ll store a reference count indicating how many read-\ners currently hold the lock. Each time a reader acquires the lock, the count is\nincremented; each time a reader releases the lock, the count is decremented.\nBut how, then, can we also provide an “exclusive” lock mode for the\nwriter? All we need to do is to reserve one (very high) reference count value\nand use it to denote that a writer currently holds the lock. If our reference\ncount is an unsigned 32-bit integer, the value 0xFFFFFFFFU could do nicely\nas the reserved value. Even easier, we can simply reserve the most-significant\nbit, meaning that reference counts from 0 to 0x7FFFFFFFU represent reader\nlocks, and the reserved value 0x80000000U represents a writer lock (with no\nother values being valid).\nAreaders-writerlocksuffersfromstarvationproblems: Awriterthatholds\nthelocktoolongcancauseallofthereaderstostarve,andlikewisealotofread-\ners can cause the writers to starve. A sequential lock is one possible alternative\nthat tackles the starvation issue (see https://en.wikipedia.org/wiki/Seqlock\nfor details). Check out https://lwn.net/Articles/262464 for a description of\nyet another interesting locking technique, used in the Linux kernel, that sup-\nports multiple concurrent readers and writers, called read-copy-update (RCU).\n4.9. Lock-Free Concurrency 325\nWe’ll leave the implementation of a readers-writer lock up to you as an ex-\nercise! However,ifyou’dliketocomparenotes,youcanfindafully-functional\nimplementation on this book’s website (www.gameenginebook.com).\n4.9.7.5 Lock-Not-Needed Assertions\nNo matter how you slice it, locks are expensive. Mutexes are expensive even\nin the absence of contention. In a low-contention scenario, spin locks are rela-\ntivelycheap,buttheystillintroduceanon-zerocostintoanypieceofsoftware.\nIt’s often the case that the programmer knows a priorithat a lock isn’t re-\nquired. In a game engine, for example, each iteration of the game loop is usu-\nally performed in distinct stages. If a shared data structure is accessed exclu-\nsively by a single thread early in the frame, and that same data is accessed\nagain by a single thread later in the frame, then we don’t actually need a lock.\nYes,intheorythosetwothreadscouldoverlap,andiftheyweretodosoalock\nwould definitely be needed. But in practice, given the way our game loop is\nstructured, we might know that such overlap can never occur.\nIn this kind of situation, we have a few choices. We could put the locks\nin, just in case. That way, if someone rearranges the order in which things are\ndone during the frame, and ends up making these threads overlap, we’d be\ncovered. Another option is to just ignore the possibility of overlap and not\nlock anything.\nThere is a third option which I find more appealing in such scenarios: We\ncanassertthat a lock isn’t required. This has two benefits. First, it can be done\nvery cheaply, and in fact the assertions can be stripped prior to shipping your\ngame. Second, it automatically detects problems if our assumptions about the\noverlapofthethreadsprovestobeincorrect—oriftheassumptionsarebroken\nlater on by a refactor of the code. There’s no standardized name for this kind\nof assertion, so we’ll call them lock-not-neededassertions in this book.\nSo how do we detect that a lock is needed? One way would be to use\nan atomic Boolean variable, complete with proper memory fencing, and use it\nlikeamutex. Exceptthatinsteadofactuallyacquiringamutexlock,wewould\nsimply assert that the Boolean is false, and then set it to true atomically. And\ninstead of releasing a lock, we assert that our Boolean is true, and then set it to\nfalse atomically. This approach would work, but it would be just as expensive\nas an uncontended spin lock. We can do better.\nThe “trick” is to realize that we only care about detecting overlaps between\ncritical operations on a shared object. And that detection needn’t be 100% re-\nliable. A 90% hit rate is probably just fine. If two critical operations ever do\noverlap, there may be times when we fail to detect it. But if your game is be-\ning run multiple times a day, every day, by a team of 100 or more developers,\n326 4. Parallelism and Concurrent Programming\nplus a QA department consisting of at least another 10 or 20 people, you can\nbe pretty sure someone will detect the problem if one exists.\nSo, instead of an atomicBoolean, we’ll just use a volatile Boolean. As\nwe’ve stated, the volatile keyword doesn’t do much to prevent concurrent\nrace bugs. But it does guarantee that reads and writes of the Boolean won’t\nbe optimized away, and that’s really all we need. We’ll get a reasonably good\ndetection rate, and the test is dirt cheap.\nclass UnnecessaryLock\n{\nvolatile bool m_locked;\npublic:\nvoid Acquire()\n{\n// assert no one already has the lock\nassert(!m_locked) ;\n// now lock (so we can detect overlapping\n// critical operations if they happen)\nm_locked = true;\n}\nvoid Release()\n{\n// assert correct usage (that Release()\n// is only called after Acquire())\nassert(m_locked) ;\n// unlock\nm_locked = false;\n}\n};\n#if ASSERTIONS_ENABLED\n#define BEGIN_ASSERT_LOCK_NOT_NECESSARY (L) (L).Acquire()\n#define END_ASSERT_LOCK_NOT_NECESSARY (L) (L).Release()\n#else\n#define BEGIN_ASSERT_LOCK_NOT_NECESSARY (L)\n#define END_ASSERT_LOCK_NOT_NECESSARY (L)\n#endif\n// Example usage...\nUnnecessaryLock g_lock;\n4.9. Lock-Free Concurrency 327\nvoid EveryCriticalOperation ()\n{\nBEGIN_ASSERT_LOCK_NOT_NECESSARY (g_lock);\nprintf(""perform critical op...\n"");\nEND_ASSERT_LOCK_NOT_NECESSARY (g_lock);\n}\nWe could also wrap the locks in a janitor (see Section 3.1.1.6), like this:\nclass UnnecessaryLockJanitor\n{\nUnnecessaryLock * m_pLock;\npublic:\nexplicit\nUnnecessaryLockJanitor(UnnecessaryLock& lock)\n: m_pLock(&lock) { m_pLock-> Acquire(); }\n~UnnecessaryLockJanitor() { m_pLock-> Release(); }\n};\n#if ASSERTIONS_ENABLED\n#define ASSERT_LOCK_NOT_NECESSARY (J,L) \\nUnnecessaryLockJanitor J(L)\n#else\n#define ASSERT_LOCK_NOT_NECESSARY (J,L)\n#endif\n// Example usage...\nUnnecessaryLock g_lock;\nvoid EveryCriticalOperation ()\n{\nASSERT_LOCK_NOT_NECESSARY (janitor, g_lock);\nprintf(""perform critical op...\n"");\n}\nWe implemented this at Naughty Dog and it successfully caught a num-\nber of cases of critical operations overlapping when the programmers had as-\nsumed they never could do so. So this little gem is tried and true.\n4.9.8 Lock-Free Transactions\nThis is supposed to be a section on lock-free programming, but thus far we’ve\nspent all of our time on writing spin locks! Perhaps counterintuitively, the\n328 4. Parallelism and Concurrent Programming\nact of writing a spin lock is an example of lock-free programming, from the\nperspective of the implementation of the spin lock itself. We’ve also learned a\nlot about atomic instructions, compiler barriers and memory fences along the\nway. So this has been a useful exercise. However, we haven’t really explored\nthe principles of lock-free programming per se; for that purpose it will be in-\nstructive to look at an example other than a spin lock. The topic of lock-free\nand non-blocking algorithms is huge. It really deserves its own book, so we\nwon’t attempt to cover the topic in depth here. But we can at least get a feel\nfor how lock-free approaches usually work.\nThe goal of lock-free programming is of course to avoid taking locks that\nwill either put a thread to sleep, or cause it to get caught up in a busy-wait\nloop inside a spin lock. To perform a critical operation in a lock-free manner,\nweneedtothinkofeachsuchoperationasa transaction thatcaneithersucceed\nin its entirety, or fail in its entirety. If it fails, the transaction is simply retried\nuntil it does succeed.\nTo implement any transaction, no matter how complex, we perform the\nmajority of the work locally(i.e., using data that’s visible only to the current\nthread, rather than operating directly on the shared data). When all of our\nducks are in a row and the transaction is ready to commit, we execute a single\natomic instruction, such as CAS or LL/SC. If this atomic instruction succeeds,\nwe have successfully “published” our transaction globally—it becomes a per-\nmanent part of the shared data structure on which we’re operating. But if the\natomic instruction fails, that means some otherthread was attempting to com-\nmit a transaction at the same time we were.\nThis fail-and-retry approach works because whenever one thread fails to\ncommit its transaction, we know that it was because some otherthread man-\nagedtosucceed. Asaresult,onethreadinthesystemisalwaysmakingforward\nprogress (just maybe not us). And that is the definition of lock-free .\n4.9.9 A Lock-Free Linked List\nAs a concrete example, let’s look at a simple lock-free singly-linked list. The\nonly operation we’ll support in this discussion is push_front().\nTo prepare the transaction, we allocate the new Nodeand populate it with\ndata. Wealsosetitsnextpointertopointtowhichevernodeiscurrentlyatthe\nhead of the linked list. The transaction is now ready to commit atomically.\nThe commit itself consists of a call to compare_exchange_weak() on\nthe head pointer, which we declared as an atomic pointer to a Node. If this\ncall succeeds, we’ve inserted our new node at the head of the linked list and\nwe’re done. But if it fails, we need to retry. This involves re-initializing our\n4.9. Lock-Free Concurrency 329\nStep 1: Prepare Tran saction\nStep 2: Attempt CAS with Head\n(Retry if Head no longer points to A)\nCASHead A B\nCHead A B\nC\nFigure 4.37. A lock-free implementation of insertion at the head of a singly-linked list. Top: The\ntransaction is prepared by setting the next pointer of the new node to point to the current head\nof the list. Bottom: The transaction is committed by using an atomic CAS operation to swap the\nhead pointer with a pointer to the new node. If the CAS fails, we return to the top and try again\nuntil it succeeds.\nnew node’s next pointer to point to what is now potentially a brand new head\nnode (presumably inserted by another thread—this is perhaps why we failed\nin the first place). This two-stage process is illustrated in Figure 4.37.\nIn the code below, you won’t see an explicit re-initialization of the node’s\nnext pointer. That’s because compare_exchange_weak() does the re-init-\nialization step for us. (How convenient!) Here’s what the code would look\nlike:\ntemplate< class T >\nclass SList\n{\nstruct Node\n{\nT m_data;\nNode* m_pNext;\n};\nstd::atomic< Node* > m_head { nullptr };\npublic:\nvoid push_front(T data)\n{\n// prep the transaction locally\nauto pNode = new Node();\npNode->m_data = data;\n330 4. Parallelism and Concurrent Programming\npNode->m_pNext = m_head;\n// commit the transaction atomically\n// (retrying until it succeeds)\nwhile (!m_head. compare_exchange_weak (\npNode->m_pNext, pNode))\n{ }\n}\n};\n4.9.10 Further Reading on Lock-Free Programming\nConcurrency is a vast and profound topic, and in this chapter we’ve only just\nscratchedthesurface. Asalways,thegoalofthisbookismerelytobuildaware-\nness and serve as a jumping-off point for further learning.\n• For a complete discussion of implementing a lock-free singly-linked list,\ncheckoutHerbSutter’stalkatCppCon2014,whichiswheretheexample\nabove came from. The talk is available on YouTube in two parts:\n◦https://www.youtube.com/watch?v=c1gO9aB9nbs, and\n◦https://www.youtube.com/watch?v=CmxkPChOcvw.\n• ThislecturebyGeoffLangdaleofCMUprovidesagreatoverview: https:\n//www.cs.cmu.edu/~410-s05/lectures/L31_LockFree.pdf.\n• Also be sure to check out this presentation by Samy Al Bahra for a\nclear and easily digestible overview of pretty much every topic under\nthesunrelatedtoconcurrentprogramming: http://concurrencykit.org/\npresentations/lockfree_introduction/#/.\n• Mike Acton’s excellent talk on concurrent thinking is a must-read; it\nis available at http://cellperformance.beyond3d.com/articles/public/\nconcurrency_rabit_hole.pdf.\n• Thesetwoonlinebooksaregreatresourcesforconcurrentprogramming:\nhttp://greenteapress.com/semaphores/LittleBookOfSemaphores.pdfand\nhttps://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/\nperfbook.2011.01.02a.pdf.\n• Someexcellentarticlesonlock-freeprogrammingandhowatomics,bar-\nriers and fences work can be found on Jeff Preshing’s blog: http://\npreshing.com/20120612/an-introduction-to-lock-free-programming.",83110
34-4.10 SIMDVector Processing.pdf,34-4.10 SIMDVector Processing,"4.10. SIMD/Vector Processing 331\n• This page has great information about memory barriers on Linux: https:\n//www.mjmwired.net/kernel/Documentation/memory-barriers.txt#305\n4.10 SIMD/Vector Processing\nIn Section 4.1.4, we introduced a form of parallelism known as single instruc-\ntion multiple data (SIMD). This refers to the ability of most modern micropro-\ncessors to perform a mathematical operation on multiple data items in par-\nallel, using a single machine instruction. In this section, we’ll explore SIMD\ntechniques in some detail, and conclude the chapter with a brief discussion of\nhowSIMDandmultithreadingarecombinedintoaformofparallelsmknown\nsingle instruction multiple thread (SIMT), which forms the basis of all modern\nGPUs.\nIntel first introduced its MMX11instruction set with their Pentium line\nof CPUs in 1994. These instructions permitted SIMD calculations to be per-\nformed on eight 8-bit integers, four 16-bit integers, or two 32-bit integers\npacked into special 64-bit MMX registers. Intel followed this up with vari-\nous revisions of an extended instruction set called streaming SIMD extensions,\nor SSE, the first version of which appeared in the Pentium III processor.\nThe SSE instruction set utilizes 128-bit registers that can contain integer or\nIEEE floating-point data. The SSE mode most commonly used by game en-\nginesiscalled packed32-bitfloating-pointmode. Inthismode, four32-bit float\nvalues are packed into a single 128-bit register. An operation such as addition\nor multiplication can thus be performed on four pairs of floats in parallel\nby taking two of these 128-bit registers as its inputs. Intel has since intro-\nduced various upgrades to the SSE instruction set, named SSE2, SSE3, SSSE3\nand SSE4. In 2007, AMD introduced its own variants named XOP, FMA4 and\nCVT16.\nIn 2011, Intel introduced a new, wider SIMD register file and accompany-\ning instruction set named advanced vector extensions (AVX). AVX registers are\n256 bits wide, permitting a single instruction to operate on pairs of up to eight\n32-bit floating-point operands in parallel. The AVX2 instruction set is an ex-\ntension to AVX. Some Intel CPUs now support AVX-512, an extension to AVX\npermitting 16 32-bit floats to be packed into a 512-bit register.\n11Officially, MMX is a meaningless initialism trademarked by Intel. Unofficially, developers\nconsider it to stand for “multimedia extensions” or “matrix math extensions.”\n332 4. Parallelism and Concurrent Programming\nx y z w32 bits 32 bits 32 bits 32 bits\nFigure 4.38. The four components of an SSE register in 32-bit ﬂoating-point mode.\n4.10.1 The SSE Instruction Set and Its Registers\nThe SSE instruction set includes a wide variety of operations, with many vari-\nants for operating on differently-sized data elements within SSE registers. For\nthe purposes of the present discussion, however, we’ll stick to the relatively\nsmall subset of instructions that deal with packed 32-bit floating-point data.\nThese instructions are denoted with a pssuffix, indicating that we’re deal-\ning with packeddata (p), and that each element is a single-precision float\n(s). However most of the upcoming discussions extend intuitively into AVX’s\n256- and 512-bit modes; see https://software.intel.com/en-us/articles/intro-\nduction-to-intel-advanced-vector-extensions for an overview of AVX.\nTheSSEregistersarenamedXMM i, where iisanintegerbetween0and15\n(e.g., XMM0, XMM1, and so on). In packed 32-bit floating-point mode, each\n128-bit XMM iregister contains four 32-bit floats. In AVX, the registers are 256\nbits wide and are named YMM i; in AVX-512, they are 512 bits in width and\nare named ZMM i.\nIn this chapter, we’ll often refer to the individual floats within an SSE reg-\nister as[x y z w]\n, just as they would be when doing vector/matrix math\nin homogeneous coordinates on paper (see Figure 4.38). It doesn’t usually\nmatter what you call the elements of an SSE register, as long as you’re con-\nsistent about how you interpret each element. The most general approach is\nto think of an SSE vector ras containing the elements[\nr0r1r2r3]\n. Most\nSSE documentation uses this convention, although some documentation uses\na[\nw x y z]\nconvention, so be careful out there!\n4.10.1.1 The __m128 Data Type\nIn order for SSE instructions to perform arithmetic on packed floating-point\ndata, that data must reside in one of the XMM iregisters. For long-term stor-\nage, packed floating-point data can of course reside in memory, but it must be\ntransferred from RAM into an SSE register prior to being used for any calcu-\nlations, and the results subsequently transferred back to RAM.\nTomakeworkingwithSSEandAVXdataeasier,CandC++compilerspro-\nvide special data types that represent packed arrays of floats. The __m128\ntype encapsulates a packed array of four floats for use with the SSE intrin-\nsics. (The __m256 and__m512 typeslikewiserepresentpackedarraysofeight\n4.10. SIMD/Vector Processing 333\nand 16 floats, respectively, for use with AVX intrinsics.)\nThe__m128 data type and its kin can be used to declare global variables,\nautomatic variables, function arguments and return types, and even class and\nstructure members. Declaring automatic variables and function arguments to\nbe of type __m128 often results in the compiler treating those values as di-\nrect proxies for SSE registers. But using the __m128 type to declare global\nvariables, structure/class members, and sometimes local variables results in\nthe data being stored as a 16-byte aligned array of floatin memory. Using\na memory-based __m128 variable in an SSE calculation will cause the com-\npiler to implicitly emit instructions for loading the data from memory into an\nSSE register prior to performing the calculation on it, and likewise to emit in-\nstructions for storing the results of the calculation back into the memory that\n“backs” each such variable. As such, it’s a good idea to check the disassem-\nbly to make sure that you’re not doing unnecessary loads and stores of SSE\nregisters when using the __m128 type (and its AVX relatives).\n4.10.1.2 Alignment of SSE Data\nWhenever data that is intended for use in an XMM iregister is stored in mem-\nory, it must be 16-byte (128-bit) aligned. (Likewise, data intended for use with\nAVX’s 256-bit YMM iregisters must be 32-byte (256-bit) aligned, and data for\nuse with the 512-bit ZMM iregisters must be 64-byte (512-bit) aligned.)\nThe compiler ensures that global and local variables of type __m128 are\naligned automatically. It also pads struct andclassmembers so that any\n__m128 members are aligned properly relative to the start of the object, and\nensures that the alignment of the entire struct or class is equal to the worst-\ncase alignment of its members. This means that declaring a global or local\nvariableinstanceofastructorclassthatincludesatleastone __m128 member\nwill be 16-byte aligned by the compiler automatically.\nHowever, all dynamically allocated instances of such a struct or class need\nto be aligned manually. Likewise, any array of floatthat you intend to use\nwith SSE instructions must be properly aligned; you can ensure this via the\nC++11 alignas specifier. See Section 6.2.1.3 for more information on aligned\nmemory allocations.\n4.10.1.3 SSE Compiler Intrinsics\nWe could work directly with the SSE and AVX assembly language instruc-\ntions,perhapsusingourcompiler’sinlineassemblysyntax. However,writing\ncode like this is not only non-portable, it’s also a big pain in the butt! To make\nlife easier, modern compilers provide intrinsics—special syntax that looks and\n334 4. Parallelism and Concurrent Programming\nbehaves like a regular C function, but is actually boiled down to inline as-\nsembly code by the compiler. Many intrinsics translate into a single assembly\nlanguage instruction, although some are macrosthat translate into a sequence\nof instructions.\nIn order to use SSE and AVX intrinsics, your .cpp file must #include\n<xmmintrin.h> inVisualStudio, or <x86intrin.h> whencompilingwith\nClang or gcc.\n4.10.1.4 Some Useful SSE Intrinsics\nTherearealotofSSEintrinsics,butforthepurposesofourdiscussionherewe\ncan start with only five of them:\n•__m128 _mm_set_ps(float w, float z, float y, float x);\nThis intrinsic initializes an __m128 variable with the four floating-point\nvalues provided.\n•__m128 _mm_load_ps(const float* pData);\nThis intrinsic loads four floats from a C-style array into an __m128\nvariable. The input array must be 16-byte aligned.\n•void _mm_store_ps(float* pData, __m128 v);\nThis intrinsic stores the contents of an __m128 variable into a C-style\narray of four floats, which must be 16-byte aligned.\n•__m128 _mm_add_ps(__m128 a, __m128 b);\nThis intrinsic adds the four pairs of floats contained in the variables\naandbin parallel and returns the result.\n•__m128 _mm_mul_ps(__m128 a, __m128 b);\nThis intrinsic multiplies the four pairs of floats contained in the vari-\nables aandbin parallel and returns the result.\nYou may have noticed that the arguments x,y,zandware passed to the\n_mm_set_ps() function in reverse order. This strange convention probably\narises from the fact that Intel CPUs are little-endian . Just as a single floating-\npoint value with the bit pattern 0x12345678 would be stored in memory as\nthe bytes 0x78, 0x56, 0x34, 0x12 in order of increasing addresses, so too are\nthe contents of an SSE register stored in memory in an order that’s opposite\nto the order those components actually appear within the register. In other\nwords, not only are the four bytes comprising each float in an SSE register\nstoredinlittle-endianorder,butsotooarethefourfloatsthemselves. Allofthis\nis just a question of naming convention: There’s really no “most-significant”\n4.10. SIMD/Vector Processing 335\nor “least-significant” float within an SSE register. So we can either treat the\nin-memory order as the “correct” order and consider _mm_set_ps() to be\n“backwards,” or we can treat the arguments of _mm_set_ps() as being in\nthe “correct” order and think of all in-memory vectors as being “backwards.”\nWe’ll stick with the former convention, since it means that we’ll be able to\nread off our vectors more naturally: A homogeneous vector vconsisting of\nmembers (vx,vy,vz,vw)would be stored in a C/C++ array as float v[] =\n{ vx, vy, vz, vw }, but would be passed to _mm_set_ps() asw,z,y,\nx.\nHere’sa smallcodesnippetthatloadstwofour-elementfloating-pointvec-\ntors, adds them, and prints the results.\n#include <xmmintrin.h>\nvoid TestAddSSE()\n{\nalignas(16) float A[4];\nalignas(16) float B[4] = { 2.0f, 4.0f, 6.0f, 8.0f };\n// set a = (1, 2, 3, 4) from literal values, and\n// load b = (2, 4, 6, 8) from a floating-point array\n// just to illustrate the two ways of doing this\n// (remember that _mm_set_ps() is backwards!)\n__m128 a = _mm_set_ps(4.0f, 3.0f, 2.0f, 1.0f);\n__m128 b = _mm_load_ps(&B[0]);\n// add the vectors\n__m128 r = _mm_add_ps(a, b);\n// store '__m128 a' into a float array for printing\n_mm_store_ps (&A[0], a);\n// store result into a float array for printing\nalignas(16) float R[4];\n_mm_store_ps (&R[0], r);\n// inspect results\nprintf(""a = %.1f %.1f %.1f %.1f\n"",\nA[0], A[1], A[2], A[3]);\nprintf(""b = %.1f %.1f %.1f %.1f\n"",\nB[0], B[1], B[2], B[3]);\nprintf(""r = %.1f %.1f %.1f %.1f\n"",\nR[0], R[1], R[2], R[3]);\n}\n336 4. Parallelism and Concurrent Programming\n4.10.1.5 AltiVec vector Types\nAs a quick aside, the GNU C/C++ compiler gcc(used to compile code for the\nPS3, for example) provides support for the PowerPC’s AltiVecinstruction set,\nwhich provides support for SIMD operations, much as SSE does on Intel pro-\ncessors. 128-bit vector types can be declared like regular C/C++ types, but\ntheyareprecededbythekeyword vector . Forexample,aSIMDvariablecon-\ntaining four floats would be declared vector float . gcc also provides a\nmeans of writing literalSIMD values into your source code. For example, you\ncan initialize a vector float with a value like this:\nvector float v = (vector float)(-1.0f, 2.0f, 0.5f, 1.0f);\nThe corresponding Visual Studio code is a tad more clunky:\n// use compiler intrinsic to load ""literal"" value\n// (remember _mm_set_ps() is backwards!)\n__m128 v = _mm_set_ps(1.0f, 0.5f, 2.0f, -1.0f);\nWewon’tcoverAltiVecexplicitlyinthischapter,butonceyouunderstandSSE\nit’ll be very easy to learn.\n4.10.2 Using SSE to Vectorize a Loop\nSIMDoffersthepotentialtospeedupcertainkindsofcalculationsbyafactorof\nfour,becauseasinglemachinelanguageinstructioncanperformanarithmetic\noperation on four floating-point values in parallel. Let’s see how this can be\ndone using SSE intrinsics.\nFirst,considerasimpleloopthataddstwoarraysof floatspairwise,stor-\ning each result into an output array:\nvoid AddArrays_ref (int count,\nfloat* results,\nconst float* dataA,\nconst float* dataB,\n{\nfor (int i = 0; i < count; ++i)\n{\nresults[i] = dataA[i] + dataB[i];\n}\n}\nWe can speed up this loop significantly using SSE intrinsics, like this:\n4.10. SIMD/Vector Processing 337\nvoid AddArrays_sse (int count,\nfloat* results,\nconst float* dataA,\nconst float* dataB)\n{\n// NOTE: the caller needs to ensure that the size of\n// all 3 arrays are equal, and a multiple of four!\nassert(count % 4 == 0);\nfor (int i = 0; i < count; i += 4 )\n{\n__m128 a = _mm_load_ps(&dataA[i]);\n__m128 b = _mm_load_ps(&dataB[i]);\n__m128 r = _mm_add_ps(a, b);\n_mm_store_ps (&results[i], r);\n}\n}\nIn this version, we loop over the values four at a time. We load blocks of four\nfloatsintoSSEregisters,addtheminparallel,andstoretheresultsintoacorre-\nsponding block of four floats within the result array. This is called vectorizing\nour loop. (In this example, we’re assuming that the sizes of the three arrays\nare equal, and that the size is a multiple of four. The caller is responsible for\npadding the arrays with one, two or three dummy values each, as necessary, in\norder to meet this requirement.)\nVectorization can lead to a significant speed improvement. This particular\nexample isn’t quite four times faster, due to the overhead of having to load\nthe values in groups of four and store the results on each iteration; but when\nI measured these functions running on very large arrays of floats, the non-\nvectorizedlooptookroughly3.8timesaslongtodoitsworkasthevectorized\nloop.\n4.10.3 Vectorized Dot Products\nLet’s apply vectorization to a somewhat more interesting task: Calculating\ndotproducts. Giventwoarraysoffour-elementvectors,thegoalistocalculate\ntheirdotproductspairwise,andstoretheresultsintoanoutputarrayoffloats.\nHere’s a reference implementation without using SSE. In this implemen-\ntation, each contiguous block of four floats within the a[]andb[]input\narrays is interpreted as one homogeneous four-element vector.\nvoid DotArrays_ref (int count,\nfloat r[],\nconst float a[],\n338 4. Parallelism and Concurrent Programming\nconst float b[])\n{\nfor (int i = 0; i < count; ++i)\n{\n// treat each block of four floats as a\n// single four-element vector\nconst int j = i * 4;\nr[i] = a[j+0]*b[j+0] // ax*bx\n+ a[j+1]*b[j+1] // ay*by\n+ a[j+2]*b[j+2] // az*bz\n+ a[j+3]*b[j+3]; // aw*bw\n}\n}\n4.10.3.1 A First Attempt (That’s Slow)\nHere’s a first attempt at using SSE intrinsics for this task:\nvoid DotArrays_sse_horizontal(int count,\nfloat r[],\nconst float a[],\nconst float b[])\n{\nfor (int i = 0; i < count; ++i)\n{\n// treat each block of four floats as a\n// single four-element vector\nconst int j = i * 4;\n__m128 va = _mm_load_ps(&a[j]); // ax,ay,az,aw\n__m128 vb = _mm_load_ps(&b[j]); // bx,by,bz,bw\n__m128 v0 = _mm_mul_ps(va, vb);\n// add across the register...\n__m128 v1 = _mm_hadd_ps(v0, v0);\n// (v0w+v0z, v0y+v0x, v0w+v0z, v0y+v0x)\n__m128 vr = _mm_hadd_ps(v1, v1);\n// (v0w+v0z+v0y+v0x, v0w+v0z+v0y+v0x,\n// v0w+v0z+v0y+v0x, v0w+v0z+v0y+v0x)\n_mm_store_ss (&r[i], vr); // extract vr.x as a float\n}\n}\nThisimplementationrequiredanewinstruction: _mm_hadd_ps() (horizontal\n4.10. SIMD/Vector Processing 339\nadd). Thisintrinsicoperatesonasingleinputregister (x,y,z,w)andcalculates\ntwosums: s=x+yandt=z+w.Itstoresthesetwosumsintothefourslots\nof the destination register as (t,s,t,s). Performing this operation twice allows\nus to calculate the sum d=x+y+z+w. This is called addingacrossaregister.\nAdding across a register is not usually a good idea because it’s a very slow\noperation. Profilingthe DotArrays_sse() implementationshowsthatitac-\ntuallytakesalittlebit moretimethanthereferenceimplementation. UsingSSE\nhere has actually slowed us down!12\n4.10.3.2 A Better Approach\nThekeytorealizingthepowerofSIMDparallelismfordotproductsistofigure\nout a way to avoid having to add across a register. This can be done, but we’ll\nhave totranspose our input vectors first. By storing them in transposed order,\nwe can calculate our dot product in just the same way that we calculated it\nwhen using floats: We multiply the xcomponents, then add that result to\nthe product of the ycomponents, then add that result to the product of the z\ncomponents, and finally add that result to the product of the wcomponents.\nHere’s what the code looks like:\nvoid DotArrays_sse (int count,\nfloat r[],\nconst float a[],\nconst float b[])\n{\nfor (int i = 0; i < count; i += 4 )\n{\n__m128 vaX = _mm_load_ps(&a[(i+0)*4]); // a[0,4,8,12]\n__m128 vaY = _mm_load_ps(&a[(i+1)*4]); // a[1,5,9,13]\n__m128 vaZ = _mm_load_ps(&a[(i+2)*4]); // a[2,6,10,14]\n__m128 vaW = _mm_load_ps(&a[(i+3)*4]); // a[3,7,11,15]\n__m128 vbX = _mm_load_ps(&b[(i+0)*4]); // b[0,4,8,12]\n__m128 vbY = _mm_load_ps(&b[(i+1)*4]); // b[1,5,9,13]\n__m128 vbZ = _mm_load_ps(&b[(i+2)*4]); // b[2,6,10,14]\n__m128 vbW = _mm_load_ps(&b[(i+3)*4]); // b[3,7,11,15]\n__m128 result;\nresult = _mm_mul_ps(vaX, vbX);\nresult = _mm_add_ps(result, _mm_mul_ps(vaY, vbY));\nresult = _mm_add_ps(result, _mm_mul_ps(vaZ, vbZ));\n12With SSE4, Intel introduced the intrinsic _mm_dp_ps() (and the corresponding dppsinstruc-\ntion) which calculates a dot product with somewhat lower latency than the version involving\ntwo invocations of _mm_hadd_ps (). But all horizontal adds are very expensive, and should be\navoided wherever possible.\n340 4. Parallelism and Concurrent Programming\nresult = _mm_add_ps(result, _mm_mul_ps(vaW, vbW));\n_mm_store_ps (&r[i], result);\n}\n}\nThe MADD Instruction\nIt’s interesting to note that a multiply followed by an add is such a common\noperation that it has its own name— madd. Some CPUs provide a single SIMD\ninstruction for performing a maddoperation. For example, the PowerPC Al-\ntiVecintrinsic vec_madd() performsthisoperation. SoinAltiVec, thegutsof\nourDotArrays() function could be made just a little bit simpler:\nvector float result = vec_mul(vaX, vbX);\nresult = vec_madd(vaY, vbY, result));\nresult = vec_madd(vaZ, vbZ, result));\nresult = vec_madd(vaW, vbW, result));\n4.10.3.3 Transpose as We Go\nTheaboveimplementation assumes thatthe inputdatahas alreadybeentrans-\nposed by the caller. In other words, the a[]array is assumed to contain the\ncomponentsfa0,a4,a8,a12,a1,a5,a9,a13, ...gand likewise for the b[]array. If\nwewanttooperateoninputdatathat’sinthesameformatasitwasfortheref-\nerenceimplementation,we’llhavetodothetranspositionwithinourfunction.\nHere’s how:\nvoid DotArrays_sse_transpose(int count,\nfloat r[],\nconst float a[],\nconst float b[])\n{\nfor (int i = 0; i < count; i += 4)\n{\n__m128 vaX = _mm_load_ps(&a[(i+0)*4]); // a[0,1,2,3]\n__m128 vaY = _mm_load_ps(&a[(i+1)*4]); // a[4,5,6,7]\n__m128 vaZ = _mm_load_ps(&a[(i+2)*4]); // a[8,9,10,11]\n__m128 vaW = _mm_load_ps(&a[(i+3)*4]); // a[12,13,14,15]\n__m128 vbX = _mm_load_ps(&b[(i+0)*4]); // b[0,1,2,3]\n__m128 vbY = _mm_load_ps(&b[(i+1)*4]); // b[4,5,6,7]\n__m128 vbZ = _mm_load_ps(&b[(i+2)*4]); // b[8,9,10,11]\n__m128 vbW = _mm_load_ps(&b[(i+3)*4]); // b[12,13,14,15]\n4.10. SIMD/Vector Processing 341\n_MM_TRANSPOSE4_PS (vaX, vaY, vaZ, vaW);\n// vaX = a[0,4,8,12]\n// vaY = a[1,5,9,13]\n// ...\n_MM_TRANSPOSE4_PS (vbX, vbY, vbZ, vbW);\n// vbX = b[0,4,8,12]\n// vbY = b[1,5,9,13]\n// ...\n__m128 result;\nresult = _mm_mul_ps(vaX, vbX);\nresult = _mm_add_ps(result, _mm_mul_ps(vaY, vbY));\nresult = _mm_add_ps(result, _mm_mul_ps(vaZ, vbZ));\nresult = _mm_add_ps(result, _mm_mul_ps(vaW, vbW));\n_mm_store_ps(&r[i], result);\n}\n}\nThosetwocallsto _MM_TRANSPOSE() areactuallyinvocationsofasomewhat\ncomplex macro that uses shuffleinstructions to move the components of the\nfour input registers around. Thankfully shuffling isn’t a particularly expen-\nsive operation, so transposing our vectors as we calculate the dot products\ndoesn’t introduce too much overhead. Profiling all three implementations of\nDotArrays() shows that our final version (the one that transposes the vec-\ntors as it goes) is roughly 3.5 times faster than the reference implementation.\n4.10.3.4 Shufﬂe and Transpose\nFor the curious reader, here’s what the _MM_TRANSPOSE() macro looks like:\n#define _MM_TRANSPOSE4_PS(row0, row1, row2, row3) \\n{ __m128 tmp3, tmp2, tmp1, tmp0; \\n\\ntmp0 = _mm_shuffle_ps ((row0), (row1), 0x44); \\ntmp2 = _mm_shuffle_ps ((row0), (row1), 0xEE); \\ntmp1 = _mm_shuffle_ps ((row2), (row3), 0x44); \\ntmp3 = _mm_shuffle_ps ((row2), (row3), 0xEE); \\n\\n(row0) = _mm_shuffle_ps (tmp0, tmp1, 0x88); \\n(row1) = _mm_shuffle_ps (tmp0, tmp1, 0xDD); \\n(row2) = _mm_shuffle_ps (tmp2, tmp3, 0x88); \\n(row3) = _mm_shuffle_ps (tmp2, tmp3, 0xDD); }\nThose crazy hexadecimal numbers are bit-packed four-element fields called\n342 4. Parallelism and Concurrent Programming\nshuffle masks . They tell the _mm_shuffle() intrinsic how exactly to shuffle\nthe components. These bit-packed fields are a common source of confusion,\npossiblybecauseofthenamingconventionsusedinmostdocumentation. But\nit’s actually pretty simple: A shuffle mask is constructed out of four integers,\neach of which represents one of the components of an SSE register (and hence\ncan have a value between 0 and 3).\n#define SHUFMASK(p,q,r,s) \\n(p | (q<<2) | (r<<4) | (s<<6))\nPassing two SSE registers aand balong with a shuffle mask to the\n_mm_shuffle_ps() intrinsic results in the specified components of aand\nbappearing in the output register ras follows:\n__m128 a = ...;\n__m128 b = ...;\n__m128 r = _mm_shuffle_ps (a, b,\nSHUFMASK(p,q,r,s) );\n// r == ( a[p], a[q], b[r], b[s] )\n4.10.4 Vector-Matrix Multiplication with SSE\nNow that we understand how to perform a dot product, we can multiply a\nfour-element vector with a 44matrix. To do it, we simply need to perform\nfour dot products between the input vector and each of the four rows of the\ninput matrix.\nWe’ll start by defining a Mat44class that encapsulates four SSE vectors,\nrepresenting its four rows. We’ll use a unionso that we can easily access the\nindividual members of the matrix as floats. (This works because instances\nof our Mat44class will always reside in memory, never directly in SSE regis-\nters.)\nunion Mat44\n{\nfloat c[4][4]; // components\n__m128 row[4]; // rows\n};\nThe function to multiply a vector and a matrix looks like this:\n__m128 MulVecMat_sse (const __m128& v, const Mat44& M)\n{\n// first transpose v\n__m128 vX = _mm_shuffle_ps (v, v, 0x00); // (vx,vx,vx,vx)\n4.10. SIMD/Vector Processing 343\n__m128 vY = _mm_shuffle_ps (v, v, 0x55); // (vy,vy,vy,vy)\n__m128 vZ = _mm_shuffle_ps (v, v, 0xAA); // (vz,vz,vz,vz)\n__m128 vW = _mm_shuffle_ps (v, v, 0xFF); // (vw,vw,vw,vw)\n__m128 r = _mm_mul_ps(vX, M.row[0]);\nr = _mm_add_ps(r, _mm_mul_ps(vY, M.row[1]));\nr = _mm_add_ps(r, _mm_mul_ps(vZ, M.row[2]));\nr = _mm_add_ps(r, _mm_mul_ps(vW, M.row[3]));\nreturn r;\n}\nThe shuffles are used to replicate onecomponent of v(either vx,vy,vzorvw)\nacrossall four lanes of an SSE register. This has the effect of transposing v\nprior to performing the dot product with the rows of M, which are already\ntransposed. (Remember that vector-matrix multiplication normally involves\ntaking dot products between an input vector and the columns of the matrix.\nHere, we’re transposing vinto four SSE registers, and then doing our math,\ncomponent-wise, with the rowsof the matrix.)\n4.10.5 Matrix-Matrix Multiplication with SSE\nMultiplying two 44matrices with SSE intrinsics is trivial once we have a\nfunction to multiply a vector and a matrix. Here’s what the code looks like:\nvoid MulMatMat_sse (Mat44& R, const Mat44& A, const Mat44& B)\n{\nR.row[0] = MulVecMat_sse (A.row[0], B);\nR.row[1] = MulVecMat_sse (A.row[1], B);\nR.row[2] = MulVecMat_sse (A.row[2], B);\nR.row[3] = MulVecMat_sse (A.row[3], B);\n}\n4.10.6 Generalized Vectorizaton\nBecause an SSE register contains four floating-point values, it’s tempting to\nthink of it as a natural “fit” for the components of a four-element homoge-\nneous vector v, and to think that the best use of SSE is for doing 3D vector\nmath. However, this is a very limiting way of thinking about SIMD paral-\nlelism.\nMost“batched”operations,inwhichasinglecomputationisperformedre-\npeatedly on a large dataset, can be vectorized using SIMD parallelism. If you\nthink about it, the components of a SIMD register really function like parallel\n“lanes”inwhicharbitraryprocessingcanbeperformed. Workingwith float\n344 4. Parallelism and Concurrent Programming\nvariablesgivesusasinglelane,butworkingwith128-bit(four-element)SIMD\nvariables allows us to do that same calculation in parallel across four lanes—\nin other words, we can perform our calculations four at a time. Working with\n256-bit AVX registers gives us eight lanes, allowing us to perform our calcula-\ntions eight at a time. And AVX-512 gives us 16 lanes, letting us do 16 calcula-\ntions at a time.\nThe easiest way to write vectorized code is to start out by writing it as a\nsingle-lane algorithm (just using floats). Once it works, we can convert it\nto operate Nelements at a time, using SIMD registers that have an N-lane\ncapacity. We’ve already seen this process in action: In Section 4.10.3, we first\nwrote a loop that performed a large batch of dot products one at a time, and\nthen we converted to use SSE so that we could perform those dot products\nfour at a time.\nOneniceside-effectofvectorizingyourcodeinthiswayisthatyoucanreap\nthebenefitsofwiderSIMDhardwarewithlittleadjustmenttoyourcode. Ona\nmachine with only SSE support, you perform four operations per iteration of\nyour loop; on a machine that supports AVX, you simply change it to do eight\noperations per iteration; and on an AVX-512 system, you can do 16 operations\nper iteration.\nInterestingly,mostoptimizingcompilerscan vectorize somekindsofsingle-\nlane loops automatically. In fact, when writing the above examples, it took\nsomedoingtoforcethecompiler nottovectorizemysingle-lanecode,sothatI\ncould compare its performance to my SIMD implementation! Once again, it’s\nalwaysagoodideatolookatthedisassemblywhenwritingoptimizedcode—\nyou may discover the compiler is doing more (or less) than you thought!\n4.10.7 Vector Predication\nLet’s take a look at another (totally contrived) example. This example will re-\ninforcetheideasofgeneralizedvectorization,butitwillalsoservetoillustrate\nanother useful technique: vector predication .\nImaginethatweneededtotakethesquarerootsofalargearrayof floats.\nWe’d start out by writing it as a single-lane loop, like this:\n#include <cmath>\nvoid SqrtArray_ref (float* __restrict__ r,\nconst float* __restrict__ a,\nint count)\n{\nfor (unsigned i = 0; i < count; ++i)\n{\n4.10. SIMD/Vector Processing 345\nif (a[i] >= 0.0f)\nr[i] = std::sqrt(a[i]);\nelse\nr[i] = 0.0f;\n}\n}\nNext, let’s convert this loop into SSE, performing four square roots at a time:\n#include <xmmintrin.h>\nvoid SqrtArray_sse_broken (float* __restrict__ r,\nconst float* __restrict__ a,\nint count)\n{\nassert(count % 4 == 0);\n__m128 vz = _mm_set1_ps(0.0f); // all zeros\nfor (int i = 0; i < count; i += 4 )\n{\n__m128 va = _mm_load_ps(a + i);\n__m128 vr;\nif (_mm_cmpge_ps (va, vz)) // ???\nvr = _mm_sqrt_ps(va);\nelse\nvr = vz;\n_mm_store_ps(r + i, vr);\n}\n}\nThis seems simple enough: We simply stride through the input array four\nfloats at a time, load groups of four floats into an SSE register, and then\nperform four parallel square roots with _mm_sqrt_ps() .\nHowever, there’s one small gotcha in this loop. We need to check whether\nthe input values are greater than or equal to zero, because the square root of a\nnegativenumberisimaginary(andwillthereforeproduceQNaN).Theintrin-\nsic_mm_cmpge_ps() compares the values of two SSE registers, component-\nwise, to see if they are greater than or equal to a vector of values supplied by\nthe caller. However, this function doesn’t return a bool. How can it? We’re\ncomparingfourvaluestofourothervalues, sosomeofthemmaypassthetest\nwhileothersfailit. Thatmeanswecan’tjustdoan ifcheckagainsttheresults\nof_mm_cmpge_ps().13\n13The if check in the single-lane reference implementation also prevents the compiler from au-\ntomatically vectorizing the loop.\n346 4. Parallelism and Concurrent Programming\nDoes this spell doom for our vectorized implementation? Thankfully\nnot. All SSE comparison intrinsics like _mm_cmpge_ps() produce a four-\ncomponent result, stored in an SSE register. But instead of containing four\nfloating-point values, the result consists of four 32-bit masks. Each mask con-\ntainsallbinary1s(0xFFFFFFFFU)ifthecorrespondingcomponentintheinput\nregister passed the test, and all binary 0s (0x0U) if that component failed the\ntest.\nWe can use the results of an SSE comparison intrinsic by applying it as a\nbitmaskinorderto selectbetweenoneoftwopossibleresults. Inourexample,\nwhentheinputvaluepassesthetest(isgreaterthanorequaltozero),wewant\nto select the square root of that input value; when it fails the test (is negative),\nwe want to select a value of zero. This is called predication , and because we’re\napplying it to SIMD vectors, it’s called vector predication .\nIn Section 4.2.6.2, we saw how to do predication with floating-point val-\nues, using bitwise AND, OR and NOT operators. Here’s an excerpt from that\nexample:\n// ...\n// select quotient when mask is all ones, or default\n// value d when mask is all zeros (NOTE: this won't\n// work as written -- you'd need to use a union to\n// interpret the floats as unsigned for masking)\nconst float result = ( q&mask) | (d &~mask);\nreturn result;\n}\nIt’s no different here, we just need to use SSE versions of these bitwise opera-\ntors:\n#include <xmmintrin.h>\nvoid SqrtArray_sse (float* __restrict__ r,\nconst float* __restrict__ a,\nint count)\n{\nassert(count % 4 == 0);\n__m128 vz = _mm_set1_ps(0.0f);\nfor (int i = 0; i < count; i += 4)\n{\n__m128 va = _mm_load_ps(a + i);\n4.10. SIMD/Vector Processing 347\n// always do the quotient, but it may end\n// up producing QNaN in some or all lanes\n__m128 vq = _mm_sqrt_ps(va);\n// now select between vq and vz, depending\n// on whether the input was greater than\n// or equal to zero or not\n__m128 mask = _mm_cmpge_ps(va, zero);\n// (vq & mask) | (vz & ~mask)\n__m128 qmask = _mm_and_ps(mask, vq);\n__m128 znotmask = _mm_andnot_ps (mask, vz);\n__m128 vr = _mm_or_ps(qmask, znotmask);\n_mm_store_ps(r + i, vr);\n}\n}\nIt’s convenient and commonplace to encapsulate this vector predication op-\neration in a function, which is typically called vectorselect . PowerPC’s AltiVec\nISA provides an intrinsic called vec_sel() for this purpose. It works like\nthis:\n// pseudocode illustrating how AltiVec's vec_sel() intrinsic\n// works\nvector float vec_sel(vector float falseVec,\nvector float trueVec,\nvector bool mask)\n{\nvector float r;\nfor (each lane i)\n{\nif (mask[i] == 0)\nr[i] = falseVec[i];\nelse\nr[i] = trueVec[i];\n}\nreturn r;\n}\nSSE2providednovectorselectinstruction,butthankfullyonewasintroduced\nin SSE4—it is emitted by the intrinsic _mm_blendv_ps() .\nLet’stakealookathowwemightimplementavectorselectoperationour-\nselves. We can write it like this:",31951
35-4.11 Introduction to GPGPU Programming.pdf,35-4.11 Introduction to GPGPU Programming,"348 4. Parallelism and Concurrent Programming\n__m128 _mm_select_ps (const __m128 a,\nconst __m128 b,\nconst __m128 mask)\n{\n// (b & mask) | (a & ~mask)\n__m128 bmask = _mm_and_ps(mask, b);\n__m128 anotmask = _mm_andnot_ps (mask, a);\nreturn _mm_or_ps(bmask, anotmask);\n}\nOr if we’re feeling brave, we can accomplish the same thing with exclusive\nOR:\n__m128 _mm_select_ps (const __m128 a,\nconst __m128 b,\nconst __m128 mask)\n{\n// (((a ^ b) & mask) ^ a)\n__m128 diff = _mm_xor_ps(a, b);\nreturn _mm_xor_ps(a, _mm_and_ps(mask, diff));\n}\nSee if you can figure out how this works. Here are two hints: The exclusive\nOR operator calculates the bitwisedifference between two values. Two XORs in\na row leave the input value unchanged ((a ^ b) ^ b == a).\n4.11 Introduction to GPGPU Programming\nWe said in the preceding section that most optimizing compilers can vectorize\nsome code automatically, if the target CPU includes a SIMD vector processing\nunit, and if the source code meets certain requirements (such as not involving\ncomplex branching). Vectorization is also one of the pillars of general-purpose\nGPUprogramming (GPGPU). In this section, we’ll take a brief introductory\nlook at how a GPU differs from a CPU in terms of its hardware architecture,\nand how the concepts of SIMD parallelism and vectorization enable program-\nmers to write compute shaders that are capable of processing large amounts of\ndata in parallel on a GPU.\n4.11.1 Data-Parallel Computations\nA GPU is a specialized coprocessor designed specifically to accelerate those\ncomputations that involve a high degree of dataparallelism. It does so by com-\nbining SIMD parallelism (vectorized ALUs) with MIMD parallelism (by em-\nployingaformofpreemptivemultithreading). NVIDIAcoinedtheterm single\ninstructionmultiplethread (SIMT) to describe this SIMD/MIMD hybrid design,\n4.11. Introduction to GPGPU Programming 349\nalthough the design is not unique to NVIDIA GPUs—although the specifics\nof GPU designs vary from vendor to vendor and from product line to prod-\nuct line in significant ways, all GPUs employ the general principles of SIMT\nparallelism in their designs.\nGPUs are designed specifically to perform data-parallel computations on\nvery large datasets. In order for a computational task to be well-suited to ex-\necution on a GPU, the computations performed on any one element of the\ndataset must be independent of the results of computations on other elements.\nIn other words, it must be possible to process the elements in any order.\nThe simple examples of SIMD vectorization that we presented starting in\nSection 4.10.3 are all examples of data-parallel computations. Recall this func-\ntion, which processes two potentially very large arrays of input vectors and\nproduces an output array containing the scalar dot products of those vectors:\nvoid DotArrays_ref (unsigned count,\nfloat r[],\nconst float a[],\nconst float b[])\n{\nfor (unsigned i = 0; i < count; ++i)\n{\n// treat each block of four floats as a\n// single four-element vector\nconst unsigned j = i * 4;\nr[i] = a[j+0]*b[j+0] // ax*bx\n+ a[j+1]*b[j+1] // ay*by\n+ a[j+2]*b[j+2] // az*bz\n+ a[j+3]*b[j+3]; // aw*bw\n}\n}\nThecomputationperformedbyeachiterationofthisloopisindependentofthe\ncomputations performed by the other iterations. That means that we are free\nto perform the computations in any order we see fit. Moreover, this property\niswhatallowsustoapplySSEorAVXintrinsicstovectorizetheloop—instead\nof performing the computations one-at-a-time, we can use SIMD parallelism\nto perform four, eight or 16 computations simultaneously, thereby reducing\nthe iteration count by a factor of four, eight or 16, respectively.\nNowimaginetakingSIMDparallelizationtoanextreme. Whatifwehada\nSIMD VPU with 1024 lanes? In that case, we could divide the total number of\niterationsby1024—andwhentheinputarrayscontain1024elementsorfewer,\nwe could literally execute the entire loop in a single iteration!\n350 4. Parallelism and Concurrent Programming\nThis is, roughly speaking, what a GPU does. However, it doesn’t use\nSIMDs that are actually 1024 lanes wide each. A GPU’s SIMD units are typ-\nically eight or 16 lanes wide, but they process workloads in batches of 32 or 64\nelements at a time. What’s more, a GPU contains manysuch SIMD units. So a\nlargeworkloadcanbedispatchedacrosstheseSIMDunitsinparallel,resulting\nintheGPUbeingcapableofprocessingliterallythousandsofdataelementsin\nparallel.\nData-parallel computations are just what the doctor ordered when apply-\ning a pixel shader (also known as a fragment shader) to millions of pixels, or a\nvertex shader to hundreds of thousands or even millions of 3D mesh vertices,\nevery frame at 30 or 60 FPS. But modern GPUs expose their computational\npower to programmers, allowing us to write general-purpose computeshaders .\nAs long as the computations we wish to perform on a large dataset have the\nproperty of being largely independent of one another, they can probably be\nexecuted by a GPU more efficiently than they could be on a CPU.\n4.11.2 Compute Kernels\nIn Section 4.10.3, we saw that in order to vectorize a loop like the one in the\nDotArrays_ref() function, we must rewrite the code. The vectorized ver-\nsion of the function makes use of SSE or AVX intrinsics; our scalardata types\narereplacedby vectortypessuchasSSE’s __m128 orAltiVec’s vector float;\nand the loop is hard-coded to iterate four, eight or 16 elements at a time.\nWhen writing a GPGPU compute shader, we take a different tack. Instead\nofhard-codingthe loopto operatein fixed-size batches, weleave the loopas a\n“single-lane” computation using scalar data types. Then, we extract the body\noftheloopintoaseparatefunctionknownasa kernel. Here’swhattheexample\nabove would look like as a kernel:\nvoid DotKernel (unsigned i,\nfloat r[],\nconst float a[],\nconst float b[])\n{\n// treat each block of four floats as a\n// single four-element vector\nconst unsigned j = i * 4;\nr[i] = a[j+0]*b[j+0] // ax*bx\n+ a[j+1]*b[j+1] // ay*by\n+ a[j+2]*b[j+2] // az*bz\n+ a[j+3]*b[j+3]; // aw*bw\n}\n4.11. Introduction to GPGPU Programming 351\nvoid DotArrays_gpgpu1(unsigned count,\nfloat r[],\nconst float a[],\nconst float b[])\n{\nfor (unsigned i = 0; i < count; ++i)\n{\nDotKernel(i, r, a, b);\n}\n}\nTheDotKernel() function is now in a form that’s suitable for conversion\ninto acompute shader. It processes just one element of the input data, and pro-\nduces a single output element. This is analogous to how a pixel/fragment\nshader receives a single input pixel/fragment color and transforms it into a\nsingle output color, or to how a vertex shader receives a single input vertex\nand produces a single output vertex. The GPU effectively runs the forloop\nfor us, calling our kernel function once for each element of our dataset.\nGPGPU compute kernels are typically written in a special shadinglanguage\nwhich can be compiled into machine code that’s understandable by the GPU.\nShading languages are usually very close to C in syntax, so converting a C\nor C++ loop into a GPU compute kernel isn’t usually a particularly difficult\nundertaking. Some examples of shading languages include DirectX’s HLSL\n(high-level shader language), OpenGL’s GLSL, NVIDIA’s Cg and CUDA C\nlanguages, and OpenCL.\nSomeshadinglanguagesrequireyoutomoveyourkernelcodeintospecial\nsource files, separate from your C++ application code. OpenCL and CUDA\nC, however, are extensions to the C++ language itself. As such, they permit\nprogrammerstowritecomputekernelsasregularC/C++functions,withonly\nminor syntactic adjustments, and to invoke those kernels on the GPU with\nrelatively simple syntax.\nAs a concrete example, here’s our DotKernel() function written in\nCUDA C:\n__global__ void DotKernel_CUDA(int count,\nfloat* r,\nconst float* a,\nconst float* b)\n{\n// CUDA provides a magic ""thread index"" to each\n// invocation of the kernel -- this serves as\n// our loop index i\n352 4. Parallelism and Concurrent Programming\nsize_t i=threadIdx.x;\n// make sure the index is valid\nif (i < count)\n{\n// treat each block of four floats as a\n// single four-element vector\nconst unsigned j = i * 4;\nr[i] = a[j+0]*b[j+0] // ax*bx\n+ a[j+1]*b[j+1] // ay*by\n+ a[j+2]*b[j+2] // az*bz\n+ a[j+3]*b[j+3]; // aw*bw\n}\n}\nYou’ll notice that the loop index iis taken from a variable called threadIdx\nwithinthekernelfunctionitself. Thethreadindexisa“magic”inputprovided\nby the compiler, in much the same way that the thispointer “magically”\npoints to the current instance within a C++ class member function. We’ll talk\nmore about the thread index in the next section.\n4.11.3 Executing a Kernel\nGiven that we’ve written a compute kernel, let’s see how to execute it on the\nGPU. The details differ from shading language to shading language, but the\nkey concepts are roughly equivalent. For example, here’s how we’d kick off\nour compute kernel in CUDA C:\nvoid DotArrays_gpgpu2(unsigned count,\nfloat r[],\nconst float a[],\nconst float b[])\n{\n// allocate ""managed"" buffers that are visible\n// to both CPU and GPU\nint *cr, *ca, *cb;\ncudaMallocManaged(&cr, count * sizeof(float));\ncudaMallocManaged(&ca, count * sizeof(float) * 4);\ncudaMallocManaged(&cb, count * sizeof(float) * 4);\n// transfer the data into GPU-visible memory\nmemcpy(ca, a, count * sizeof(float) * 4);\nmemcpy(cb, b, count * sizeof(float) * 4);\n4.11. Introduction to GPGPU Programming 353\n// run the kernel on the GPU\nDotKernel_CUDA <<<1, count>>> (cr, ca, cb, count);\n// wait for GPU to finish\ncudaDeviceSynchronize();\n// return results and clean up\nmemcpy(r, cr, count * sizeof(float));\ncudaFree(cr);\ncudaFree(ca);\ncudaFree(cb);\n}\nA bit of set-up code is required to allocate the input and output buffers as\n“managed” memory that is visible to both CPU and GPU, and to copy the\ninput data into them. The CUDA-specific triple angled brackets notation\n(<<<G,N>>>) then executes the compute kernel on the GPU, by submitting\na request to the driver. The call to cudaDeviceSynchronize() forces the\nCPU to wait until the GPU has done its work (in much the same way that\npthread_join() forces one thread to wait for the completion of another).\nFinally, we free the GPU-visible data buffers.\nLet’s have a closer look at the <<<G,N>>> angled bracket notation. The\nsecond argument Nwithin the angled brackets allows us to specify the dimen-\nsionsof our input data. This corresponds to the number of iterations of our\nloopthatwewanttheGPUtoperform. Itcanactuallybeaone-,two-orthree-\ndimensional quantity, allowing us to process one-, two- or three-dimensional\ninput arrays. However, just like in C/C++, a multidimensional array is re-\nally just a one-dimensional array that’s indexed in a special way. For exam-\nple, in C/C++ a two-dimensional array access written like [row][column]\nis really equivalent to a one-dimensional array access [row*numColumns +\ncolumn]. The same principle applies to multidimensional GPU buffers.\nThefirstargument Gintheangledbracketstellsthedriverhowmany thread\ngroups(known as thread blocks in NVIDIA terminology) to use when running\nthis compute kernel. A compute job with a single thread group is constrained\nto run on a single compute unit on the GPU. A compute unit is esssentially a\ncorewithin the GPU—a hardware component that is capable of executing an\ninstruction stream. Passing a larger number for Gallows the driver to divide\nthe workload up across more than one compute unit.\n4.11.4 GPU Threads and Thread Groups\nTheGargument tells the GPU driver into how many thread groups to divide\nour work. As you might expect, a thread group is comprised of some number\n354 4. Parallelism and Concurrent Programming\nGPU\nCU 2 CU 3CU 0 CU 1\nCU 4 CU 5\nCU 6 CU 7\n...Compute Unit\nLDSSIMD\nSIMD\nSIMD\nSIMD\nLarge Register File\n...Scalar\nALU\nL1F/D\nScheduler\nFigure 4.39. A typical GPU is comprised of multiple compute units (CU), each of which acts like a\nstripped down CPU core. A CU contains an instruction fetch/decode unit, an L1 cache, possibly a\nblock of local data storage RAM (LDS), a scalar ALU, and a number of SIMD units for executing\nvectorized code.\nofGPU threads. But what exactly does the term “thread” mean in the context\nof a GPU?\nEvery GPU kernel is compiled into an instruction stream consisting of a se-\nquence of GPU machine language instructions, in much the same way that a\nC/C++ function is compiled down into a stream of CPU instructions. So a\nGPU “thread” is equivalent to a CPU “thread,” in the sense that both kinds of\nthreads represent a stream of machine language instructions that can be exe-\ncuted by one or more cores. However, a GPU executes its threads in a manner\nthat is somewhat different from the way in which a CPU executes its threads.\nAs a result, the term “thread” has a subtly different meaning when applied\nto GPU compute kernels than it has when applied to running programs on a\nCPU.\nIn order to understand this difference in terminology, let’s take a brief\nglimpse at the architecture of a GPU. We said in Section 4.11.1 that a GPU is\ncomprised of multiple compute units, each of which contains some number of\nSIMD units. We can think of a compute unit like a stripped down CPU core:\nIt contains an instruction fetch/decode unit, possibly some memory caches, a\nregular “scalar” ALU, and usually somewhere in the neighborhood of four\nSIMD units, which serve much the same function as the vector processing\nunit (VPU) in a SIMD-enabled CPU. This architecture is illustrated in Figure\n4.39.\nThe SIMD units in a CU have different lane widths on different GPUs,\nbut for the sake of this discussion let’s assume we’re working on an AMD\n4.11. Introduction to GPGPU Programming 355\nRadeon™ Graphics Core Next (GCN) architecture, in which the SIMDs are 16\nlaneswide. TheCUisnotcapableofspeculativeorout-of-orderexecution—it\nsimply reads a stream of instructions and executes them one by one, using the\nSIMDunitstoapplyeachinstruction14to16elementsofinputdatainparallel.\nTo execute a compute kernel on a CU, the driver first subdivides the in-\nput data buffers into blocks consisting of 64 elements each. For each of these\n64-element blocks of data, the compute kernel is invoked on one CU. Such an\ninvocationiscalleda wavefront (alsoknownasa warpinNVIDIAspeak). When\nexecutingawavefront, theCUfetchesand decodestheinstructionsofthe ker-\nnel one by one. Each instruction is applied to 16 data elements in lock step\nusing a SIMD unit. Internally, the SIMD unit consists of a four- stagepipeline,\nso it takes four clock cycles to complete. So rather than allow the stages of this\npipeline to sit idle for three clock cycles out of every four, the CU applies the\nsame instruction three more times, to three more blocks of 16 data elements.\nThis is why a wavefront consists of 64 data elements, even though the SIMDs\nin the CU are only 16 lanes wide.\nBecauseof thissomewhatpeculiarway inwhichaCU executesaninstruc-\ntion stream, the term “GPU thread ” actually refers to a single SIMD lane. You\ncan think of a GPU thread, therefore, as a single iteration of the original non-\nvectorizedloopthatwestartedwith, beforeweconverteditsbodyintoacom-\npute kernel. Alternatively, you can think of a GPU thread as a single invoca-\ntion of the kernel function, operating on a single input datum and producing\na single output datum. The fact that the GPU actually runs multiple GPU\nthreads in parallel (i.e., the fact that it really runs the kernel once per wave-\nfront, but processes 64 data elements at a time) is just an implementation de-\ntail. By insulating the programmer from having to think about the details of\nhow the computation is vectorized on any particular GPU, compute kernels\n(and graphics shaders as well) can be written in a portable manner.\n4.11.4.1 From SIMD to SIMT\nTheterm singleinstructionmultiplethread (SIMT)wasintroducedtounderscore\nthe fact that a GPU doesn’t just use SIMD parallelism—it also uses a form of\npreemptivemultithreadingtotime-slicebetweenwavefronts. Let’stakeabrief\nlook at why this is done.\nA SIMD unit runs a wavefront by applying each instruction in the shader\nprogram to 64 data elements at a time, essentially in lock-step. (We can ignore\nthe fact that the wavefront is processed in subgroups of 16 elements each for\nthe purposes of the present discussion.) However, any one instruction in the\n14The compute units on a GPU do contain a scalar ALU and therefore can perform some in-\nstructions in “single lane” fashion, operating on a single input datum at a time.\n356 4. Parallelism and Concurrent Programming\nStall\nRunnableStall\nRunnableStall\nRunnableStall\nRunnableWavefront 0 Wavefront 1 Wa vefront 2 Wavefront 3\nDone\nDone\nDoneTimeDone\nFigure 4.40. Whenever one wavefront stalls, for example due to memory access latency, the SIMD\nunit context switches to another wavefront to ﬁll the delay slot.\nprogram might end up having to access memory, and that introduces a large\nstall while the SIMD unit waits for the memory controller to respond.\nTo fill these large delay slots, a SIMD unit time-slices between multiple\nwavefronts (taken from a single shader program, or potentially from many\nunrelated shader programs). Whenever one wavefront stalls, the SIMD unit\ncontext switches to another wavefront, thereby keeping the unit busy (as long\nas there are runnable wavefronts to switch to). This strategy is illustrated in\nFigure 4.40.\nAsyoumightwellimagine, theSIMDunitsinaGPUneedtoperformcon-\ntext switches at a very high frequency. During a context switch on a CPU, the\nstate of the outgoing thread’s registers are saved to memory so they won’t be\nlost, and then the state of the incoming thread’s registers are read from mem-\nory into the CPU’s registers so that it can continue executing where it left off.\nHowever, on a GPU it would take too much time to save the state of each\nwavefront’s SIMD registers every time a context switch occurred.\nTo eliminate the cost of saving registers during context switches, each\nSIMD unit contains a very large register file. The number of physical regis-\nters in this register file is many times larger than the number of logicalregis-\ntersavailabletoanyonewavefront(typicallyontheorderoftentimeslarger).\n4.11. Introduction to GPGPU Programming 357\nThis means that the contents of the logical registers for up to ten wavefronts\ncan be maintained at all times in these physical registers. And that in turn\nimplies that context switches can be performed between wavefronts without\nsaving or restoring any registers whatsoever.\n4.11.5 Further Reading\nObviously GPGPU programming is a huge topic, and as always we’ve only\njust scratched the surface in this book. For more information on GPGPU and\ngraphics shader programming, check out the following online tutorials and\nresources:\n• Introduction to CUDA programming: https://developer.nvidia.com/\nhow-to-cuda-c-cpp\n• OpenCL learning resources: https://developer.nvidia.com/opencl\n• HLSL programming guide and reference manual: https://msdn.micro\nsoft.com/en-us/library/bb509561(v=VS.85).aspx\n• Introduction to the OpenGL shading language: https://www.khronos.\norg/opengl/wiki/OpenGL_Shading_Language\n• AMD Radeon™ GCN architecture whitepaper: https://www.amd.\ncom/Documents/GCN_Architecture_whitepaper.pdf\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",19490
36-5 3D Math for Games.pdf,36-5 3D Math for Games,,0
37-5.2 Points and Vectors.pdf,37-5.2 Points and Vectors,"5\n3D Math for Games\nAgame is a mathematical model of a virtual world simulated in real time\non a computer of some kind. Therefore, mathematics pervades every-\nthing we do in the game industry. Game programmers make use of virtu-\nally all branches of mathematics, from trigonometry to algebra to statistics\nto calculus. However, by far the most prevalent kind of mathematics you’ll\nbe doing as a game programmer is 3D vector and matrix math (i.e., 3D linear\nalgebra).\nEven this one branch of mathematics is very broad and very deep, so we\ncannot hope to cover it in any great depth in a single chapter. Instead, I will\nattempt to provide an overview of the mathematical tools needed by a typi-\ncal game programmer. Along the way, I’ll offer some tips and tricks, which\nshouldhelpyoukeepalloftheratherconfusingconceptsandrulesstraightin\nyour head. For an excellent in-depth coverage of 3D math for games, I highly\nrecommend Eric Lengyel’s book [32] on the topic. Chapter 3 of Christer Eric-\nson’s book [14] on real-time collision detection is also an excellent resource.\n5.1 Solving 3D Problems in 2D\nManyofthemathematicaloperationswe’regoingtolearnaboutinthefollow-\ning chapter work equally well in 2D and 3D. This is very good news, because\n359\n360 5. 3D Math for Games\nit means you can sometimes solve a 3D vector problem by thinking and draw-\ning pictures in 2D (which is considerably easier to do!) Sadly, this equivalence\nbetween 2D and 3D does not hold all the time. Some operations, like the cross\nproduct,areonlydefinedin3D,andsomeproblemsonlymakesensewhenall\nthreedimensionsareconsidered. Nonetheless,italmostneverhurtstostartby\nthinking about a simplified two-dimensional version of the problem at hand.\nOnce you understand the solution in 2D, you can think about how the prob-\nlemextendsintothreedimensions. Insomecases,you’llhappilydiscoverthat\nyour 2D result works in 3D as well. In others, you’ll be able to find a coor-\ndinate system in which the problem really istwo-dimensional. In this book,\nwe’llemploytwo-dimensionaldiagramswhereverthedistinctionbetween2D\nand 3D is not relevant.\n5.2 Points and Vectors\ny\nzx\nFigure 5.1. A point\nrepresented in Car-\ntesian coordinates.\nh\nr\nFigure 5.2. A point\nrepresented in cylin-\ndrical coordinates.The majority of modern 3D games are made up of three-dimensional objects\nin a virtual world. A game engine needs to keep track of the positions, ori-\nentations and scales of all these objects, animate them in the game world, and\ntransformthemintoscreenspacesotheycanberenderedonscreen. Ingames,\n3D objects are almost always made up of triangles, the vertices of which are\nrepresented by points. So, before we learn how to represent whole objects in\na game engine, let’s first take a look at the point and its closely related cousin,\nthe vector.\n5.2.1 Points and Cartesian Coordinates\nTechnicallyspeaking,a pointisalocationin n-dimensionalspace. (Ingames, n\nis usually equal to 2 or 3.) The Cartesian coordinate system is by far the most\ncommon coordinate system employed by game programmers. It uses two or\nthreemutuallyperpendicularaxestospecifyapositionin2Dor3Dspace. So,a\npoint Pisrepresentedbyapairortripleofrealnumbers, (Px,Py)or(Px,Py,Pz)\n(see Figure 5.1).\nOf course, the Cartesian coordinate system is not our only choice. Some\nother common systems include:\n•Cylindrical coordinates. This system employs a vertical “height” axis h, a\nradialaxis remanatingoutfromthevertical,andayawangle theta(q). In\ncylindricalcoordinates, apoint Pisrepresentedbythetripleofnumbers\n(Ph,Pr,Pq). This is illustrated in Figure 5.2.\n5.2. Points and Vectors 361\n•Spherical coordinates. This system employs a pitch angle phi ( ϕ), a yaw\nangle theta ( q) and a radial measurement r. Points are therefore rep-\nresented by the triple of numbers (Pr,Pϕ,Pq). This is illustrated in\nFigure 5.3.\nCartesian coordinates are by far the most widely used coordinate system\nin game programming. However, always remember to select the coordinate\nsystemthatbestmapstotheproblemathand. Forexample,inthegame Crank\nthe Weasel by Midway Home Entertainment, the main character Crank runs\naround an art-deco city picking up loot. I wanted to make the items of loot\nswirl around Crank’s body in a spiral, getting closer and closer to him until\nthey disappeared. I represented the position of the loot in cylindrical coor-\ndinates relative to the Crank character’s current position. To implement the\nspiral animation, I simply gave the loot a constant angular speed in q, a small\nconstant linear speed inward along its radial axis rand a very slight constant\nlinear speed upward along the h-axis so the loot would gradually rise up to\nthe level of Crank’s pants pockets. This extremely simple animation looked\ngreat, and it was much easier to model using cylindrical coordinates than it\nwould have been using a Cartesian system.\n5.2.2 Left-Handed versus Right-Handed Coordinate Systems\nr\nFigure 5.3. A point\nrepresented in\nspherical coordi-\nnates.\nIn three-dimensional Cartesian coordinates, we have two choices when ar-\nranging our three mutually perpendicular axes: right-handed (RH) and left-\nhanded (LH). In a right-handed coordinate system, when you curl the fingers\nof your right hand around the z-axis with the thumb pointing toward posi-\ntivezcoordinates, your fingers point from the x-axis toward the y-axis. In a\nleft-handed coordinate system the same thing is true using your left hand.\nThe only difference between a left-handed coordinate system and a right-\nRight-Handedx\nzy\nLeft-Handedxy\nz\nFigure 5.4. Left- and right-handed Cartesian coordinate systems.\n362 5. 3D Math for Games\nhanded coordinate system is the direction in which one of the three axes is\npointing. For example, if the y-axis points upward and xpoints to the right,\nthen zcomes toward us (out of the page) in a right-handed system, and away\nfromus (into the page) in a left-handed system. Left- and right-handed Carte-\nsian coordinate systems are depicted in Figure 5.4.\nIt is easy to convert from left-handed to right-handed coordinatesand vice\nversa. We simply flip the direction of any one axis, leaving the other two axes\nalone. It’simportanttorememberthattherulesofmathematicsdonotchange\nbetween left-handed and right-handed coordinate systems. Only our interpre-\ntationof the numbers—our mental image of how the numbers map into 3D\nspace—changes. Left-handed and right-handed conventions apply to visual-\nization only, not to the underlying mathematics. (Actually, handedness does\nmatter when dealing with cross products in physical simulations, because a\ncrossproductisnotactuallyavector—it’saspecialmathematicalobjectknown\nas apseudovector . We’ll discuss pseudovectors in a little more depth in Section\n5.2.4.9.)\nThe mapping between the numerical representation and the visual repre-\nsentation is entirely up to us as mathematicians and programmers. We could\nchoose to have the y-axis pointing up, with zforward and xto the left (RH)\nor right (LH). Or we could choose to have the z-axis point up. Or the x-axis\ncould point up instead—or down. All that matters is that we decide upon a\nmapping, and then stick with it consistently.\nThat being said, some conventions do tend to work better than others for\ncertain applications. For example, 3D graphics programmers typically work\nwith a left-handed coordinate system, with the y-axis pointing up, xto the\nright and positive zpointing away from the viewer (i.e., in the direction the\nvirtual camera is pointing). When 3D graphics are rendered onto a 2D screen\nusing this particular coordinate system, increasing z-coordinates correspond\nto increasing depthinto the scene (i.e., increasing distance away from the vir-\ntual camera). As we will see in subsequent chapters, this is exactly what is\nrequired when using a z-buffering scheme for depth occlusion.\n5.2.3 Vectors\nAvectorisaquantitythathasbotha magnitude andadirection inn-dimensional\nspace. A vector can be visualized as a directed line segment extending from a\npoint called the tailto a point called the head. Contrast this to a scalar(i.e.,\nan ordinary real-valued number), which represents a magnitude but has no\ndirection. Usuallyscalarsarewritteninitalics(e.g., v)whilevectorsarewritten\nin boldface (e.g., v).\nA3Dvectorcanberepresentedbyatripleofscalars (x,y,z),justasapoint\n5.2. Points and Vectors 363\ncan be. The distinction between points and vectors is actually quite subtle.\nTechnically, a vector is just an offset relative to some known point. A vector\ncan be moved anywhere in 3D space—as long as its magnitude and direction\ndon’t change, it is the same vector.\nA vector can be used to represent a point, provided that we fix the tail of\nthe vector to the origin of our coordinate system. Such a vector is sometimes\ncalled aposition vector orradius vector . For our purposes, we can interpret any\ntripleofscalarsaseitherapointoravector, providedthatwerememberthata\npositionvector isconstrainedsuchthatitstailremainsattheoriginofthechosen\ncoordinate system. This implies that points and vectors are treated in subtly\ndifferent ways mathematically. One might say that points are absolute, while\nvectors are relative.\nThevastmajorityofgameprogrammersusetheterm“vector”toreferboth\nto points (position vectors) and to vectors in the strict linear algebra sense\n(purely directional vectors). Most 3D math libraries also use the term “vec-\ntor” in this way. In this book, we’ll use the term “direction vector” or just\n“direction” when the distinction is important. Be careful to always keep the\ndifferencebetweenpointsanddirectionsclearinyourmind(evenifyourmath\nlibrary doesn’t). As we’ll see in Section 5.3.6.1, directions need to be treated\ndifferently from points when converting them into homogeneous coordinates\nformanipulationwith 44matrices, sogettingthetwotypesofvectormixed\nup can and will lead to bugs in your code.\n5.2.3.1 Cartesian Basis Vectors\nIt is often useful to define three orthogonal unit vectors (i.e., vectors that are\nmutuallyperpendicularandeachwithalengthequaltoone),correspondingto\nthe three principal Cartesian axes. The unit vector along the x-axis is typically\ncalled i, the y-axis unit vector is called j, and the z-axis unit vector is called k.\nThe vectors i,jandkare sometimes called Cartesian basis vectors.\nAny point or vector can be expressed as a sum of scalars (real numbers)\nmultiplied by these unit basis vectors. For example,\n(5, 3, 2) = 5i+3j 2k.\n5.2.4 Vector Operations\nMost of the mathematical operations that you can perform on scalars can be\nappliedtovectorsaswell. Therearealsosomenewoperationsthatapplyonly\nto vectors.\n364 5. 3D Math for Games\nv2vv\nFigure 5.5. Multiplication of a vector by the scalar 2.\n5.2.4.1 Multiplication by a Scalar\nMultiplication of a vector aby a scalar sis accomplished by multiplying the\nindividual components of abys:\nsa= (sax,say,saz).\nMultiplication by a scalar has the effect of scaling the magnitude of the\nvector, while leaving its direction unchanged, as shown in Figure 5.5. Multi-\nplication by 1flips the direction of the vector (the head becomes the tail and\nvice versa).\nThe scale factor can be different along each axis. We call this nonuniform\nscale,anditcanberepresentedasthe component-wiseproduct ofascalingvector\nsand the vector in question, which we’ll denote with the \noperator. Techni-\ncally speaking, this special kind of product between two vectors is known as\ntheHadamard product . It is rarely used in the game industry—in fact, nonuni-\nform scaling is one of its onlycommonplace uses in games:\ns\na= (sxax,syay,szaz). (5.1)\nAs we’ll see in Section 5.3.7.3, a scaling vector sis really just a compact\nway to represent a 33diagonal scaling matrix S. So another way to write\nEquation (5.1) is as follows:\naS=[axayaz]2\n4sx0 0\n0sy0\n0 0 sz3\n5=[sxaxsyayszaz]\n.\nWe’ll explore matrices in more depth in Section 5.3.\n5.2.4.2 Addition and Subtraction\nTheadditionoftwovectors aandbisdefinedasthevectorwhosecomponents\nare the sums of the components ofaandb. This can be visualized by placing\n5.2. Points and Vectors 365\na+ b–b b\na\na–b\nFigure 5.6. Vector addition and subtraction.\nxy Figure 5.7. Magnitude of a vector (shown in\n2D for ease of illustration).\nthe head of vector aonto the tail of vector b—the sum is then the vector from\nthe tail of ato the head of b(see also Figure 5.6):\na+b=[(ax+bx),(ay+by),(az+bz)]\n.\nVector subtraction a bis nothing more than addition of aand b(i.e., the\nresultofscaling bby 1,whichflipsitaround). Thiscorrespondstothevector\nwhose components ar e the difference between the components of aand the\ncomponents of b:\na b=[(ax bx),(ay by),(az bz)]\n.\nVector addition and subtraction are depicted in Figure 5.6.\nAdding and Subtracting Points and Directions\nYoucanaddandsubtractdirectionvectorsfreely. However,technicallyspeak-\ning,pointscannotbeaddedtooneanother—youcanonlyaddadirectionvec-\ntor to a point, the result of which is another point. Likewise, you can take the\ndifference between two points, resulting in a direction vector. These opera-\ntions are summarized below:\n• direction + direction = direction\n• direction – direction = direction\n• point + direction = point\n• point – point = direction\n• point + point = nonsense\n5.2.4.3 Magnitude\nThe magnitude of a vector is a scalar representing the length of the vector as\nit would be measured in 2D or 3D space. It is denoted by placing vertical bars\n366 5. 3D Math for Games\naround the vector’s boldface symbol. We can use the Pythagorean theorem to\ncalculate a vector’s magnitude, as shown in Figure 5.7:\njaj=√\na2x+a2y+a2z.\n5.2.4.4 Vector Operations in Action\nBelieve it or not, we can already solve all sorts of real-world game problems\ngivenjustthevectoroperationswe’velearnedthusfar. Whentryingtosolvea\nproblem, we can use operations like addition, subtraction, scaling and magni-\ntude to generate new data out of the things we already know. For example, if\nwehavethecurrentpositionvectorofanAIcharacter P1,andavector vrepre-\nsenting her current velocity, we can find her position on the next frame P2by\nscaling the velocity vector by the frame time interval ∆t, and then adding it to\nthe current position. As shown in Figure 5.8, the resulting vector equation is\nP2=P1+v∆t. (This is known as explicit Euler integration —it’s actually only\nvalid when the velocity is constant, but you get the idea.)\n12Figure 5.8. Simple vec-\ntor addition can be\nused to ﬁnd a char-\nacter’s position in the\nnext frame, given her\nposition and velocity\nin the current frame. As another example, let’s say we have two spheres, and we want to know\nwhether they intersect. Given that we know the center points of the two\nspheres, C1andC2, we can find a direction vector between them by simply\nsubtracting the points, d=C2 C1. The magnitude of this vector d=jdj\ndetermines how far apart the spheres’ centers are. If this distance is less than\nthesumofthespheres’radii, theyareintersecting; otherwisethey’renot. This\nis shown in Figure 5.9.\nSquare roots are expensive to calculate on most computers, so game pro-\ngrammers should always use the squared magnitude whenever it is valid to do\n1\n2\ny\nx1r\n2r\nd\n2–1\nFigure 5.9. A sphere-sphere intersection test involves only vector subtraction, vector magnitude\nand ﬂoating-point comparison operations.\n5.2. Points and Vectors 367\nso:\njaj2= (a2\nx+a2\ny+a2\nz).\nUsing the squared magnitude is valid when comparing the relative lengths\nof two vectors (“is vector alonger than vector b?”), or when comparing a vec-\ntor’s magnitude to some other (squared) scalar quantity. So in our sphere-\nsphere intersection test, we should calculate d2=jdj2and compare this to\nthe squared sum of the radii, (r1+r2)2for maximum speed. When writing\nhigh-performance software, never take a square root when you don’t have to!\n5.2.4.5 Normalization and Unit Vectors\nAunit vector is a vector with a magnitude (length) of one. Unit vectors are\nvery useful in 3D mathematics and game programming, for reasons we’ll see\nbelow.\nGiven an arbitrary vector vof length v=jvj, we can convert it to a unit\nvector uthat points in the same direction as v, but has unit length. To do\nthis, we simply multiply vby the reciprocal of its magnitude. We call this\nnormalization:\nu=v\njvj=1\nvv.\n5.2.4.6 Normal Vectors\nA vector is said to be normalto a surface if it is perpendicular to that surface.\nNormal vectors are highly useful in games and computer graphics. For exam-\nple,aplanecanbedefinedbyapointandanormalvector. Andin3Dgraphics,\nlighting calculations make heavy use of normal vectors to define the direction\nof surfaces relative to the direction of the light rays impinging upon them.\nNormal vectors are usually of unit length, but they do not need to be. Be\ncarefulnottoconfusetheterm“normalization”withtheterm“normalvector.”\nAnormalizedvectorisanyvectorofunitlength. Anormalvectorisanyvector\nthat is perpendicular to a surface, whether or not it is of unit length.\n5.2.4.7 Dot Product and Projection\nVectors can be multiplied, but unlike scalars there are a number of different\nkinds of vector multiplication. In game programming, we most often work\nwith the following two kinds of multiplication:\n• thedotproduct (a.k.a. scalar product or inner product), and\n• thecrossproduct (a.k.a. vector product or outer product).\n368 5. 3D Math for Games\nThe dot product of two vectors yields a scalar; it is defined by adding the\nproducts of the individual components of the two vectors:\nab=axbx+ayby+azbz=d(a scalar).\nThe dot product can also be written as the product of the magnitudes of the\ntwo vectors and the cosine of the angle between them:\nab=jajjbjcosq.\nThe dot product is commutative (i.e., the order of the two vectors can be\nreversed) and distributive over addition:\nab=ba;\na(b+c) =ab+ac.\nAnd the dot product combines with scalar multiplication as follows:\nsab=asb=s(ab).\nVector Projection\nIfuisaunitvector (juj=1),thenthedotproduct (au)representsthelength\nof theprojection of vector aonto the infinite line defined by the direction of u,\nas shown in Figure 5.10. This projection concept works equally well in 2D or\n3D and is highly useful for solving a wide variety of three-dimensional prob-\nlems.\na\nu\na \nu\nFigure 5.10. Vector projection using the dot product.\nMagnitude as a Dot Product\nThe squared magnitude of a vector can be found by taking the dot product of\n5.2. Points and Vectors 369\nthat vector with itself. Its magnitude is then easily found by taking the square\nroot:\njaj2=aa;\njaj=paa.\nThis works because the cosine of zero degrees is 1, so jajjajcosq=jajjaj=\njaj2.\nDot Product Tests\nDotproductsaregreatfortestingiftwovectorsarecollinearorperpendicular,\norwhethertheypointinroughlythesameorroughlyoppositedirections. For\nanytwoarbitraryvectors aandb, gameprogrammersoftenusethefollowing\ntests, as shown in Figure 5.11:\n•Collinear. (ab) =jajjbj=ab(i.e., the angle between them is exactly 0\ndegrees—this dot product equals +1when aandbareunitvectors ).\n•Collinear but opposite. (ab) = ab(i.e., the angle between them is 180\ndegrees—this dot product equals  1when aandbare unit vectors).\n•Perpendicular. (ab) = 0(i.e., the angle between them is 90 degrees).\n•Same direction .(ab)>0(i.e., the angle between them is less than 90\ndegrees).\n•Opposite directions .(ab)<0(i.e., the angle between them is greater\nthan 90 degrees).\nSome Other Applications of the Dot Product\nDot products can be used for all sorts of things in game programming. For\nexample, let’s say we want to find out whether an enemy is in front of the\nplayercharacterorbehindhim. Wecanfindavectorfromtheplayer’sposition\nPto the enemy’s position Eby simple vector subtraction (v=E P). Let’s\nassume we have a vector fpointing in the direction that the player is facing.\n(As we’ll see in Section 5.3.10.3, the vector fcan be extracted directly from the\nplayer’s model-to-world matrix.) The dot product d=vfcan be used to test\nwhethertheenemyisinfrontoforbehindtheplayer—itwillbepositivewhen\nthe enemy is in front and negative when the enemy is behind.\nThe dot product can also be used to find the height of a point above or\nbelow a plane (which might be useful when writing a moon-landing game for\nexample). We can define a plane with two vector quantities: a point Qlying\nanywhere on the plane, and a unit vector nthat is perpendicular (i.e., normal)\n370 5. 3D Math for Games\n(a · b) = ab\n(a · b) = –ab\n(a · b) = 0\n(a · b) > 0(a · b) < 0a\nb\na\nb\na\nba\nbba\nFigure 5.11. Some common dot product tests.\nQnP\nh= (P–Q) \nn\nFigure 5.12. The dot product can be used to ﬁnd the height of a point above or below a plane.\ntotheplane. Tofindtheheight hofapoint Pabovetheplane,wefirstcalculate\navectorfromanypointontheplane( Qwilldonicely)tothepointinquestion\nP. So we have v=P Q. The dot product of vector vwith the unit-length\nnormal vector nis just the projection of vonto the line defined by n. But that\nis exactly the height we’re looking for. Therefore,\nh=vn= (P Q)n. (5.2)\nThis is illustrated in Figure 5.12.\n5.2.4.8 Cross Product\nThecrossproduct (also known as the outerproduct orvectorproduct ) of two vec-\ntors yields another vectorthat isperpendicular to the two vectors being multi-\nplied, as shown in Figure 5.13. The cross product operation is only defined in\n5.2. Points and Vectors 371\n2\n13\n2 1\n3 1\nFigure 5.14. Area of a parallelogram expressed as the magnitude of a cross product.\nthree dimensions:\nab=[(aybz azby),(azbx axbz),(axby aybx)]\n= (aybz azby)i+ (azbx axbz)j+ (axby aybx)k.\nFigure 5.13. The\ncross product of\nvectors a and b\n(right-handed).Magnitude of the Cross Product\nThemagnitudeofthecrossproductvectoristheproductofthemagnitudesof\nthe two vectors and the sine of the angle between them. (This is similar to the\ndefinition of the dot product, but it replaces the cosine with the sine.)\njabj=jajjbjsinq.\nThe magnitude of the cross product jabjis equal to the area of the par-\nallelogram whose sides are aandb, as shown in Figure 5.14. Since a triangle\nis one half of a parallelogram, the area of a triangle whose vertices are speci-\nfied by the position vectors V1,V2andV3can be calculated as one half of the\nmagnitude of the cross product of any two of its sides:\nAtriangle =1\n2(V2 V1)(V3 V1).\nDirection of the Cross Product\nWhen using a right-handed coordinate system, you can use the right-handrule\nto determine the direction of the cross product. Simply cup your fingers such\nthattheypointinthedirectionyou’drotatevector atomoveitontopofvector\nb, and the cross product (ab)will be in the direction of your thumb.\nNotethatthecrossproductisdefinedbythe left-handrule whenusingaleft-\nhanded coordinate system. This means that the direction of the cross product\nchanges depending on the choice of coordinate system. This might seem odd\n372 5. 3D Math for Games\nat first, but remember that the handedness of a coordinate system does not\naffect the mathematical calculations we carry out—it only changes our visu-\nalization of what the numbers look like in 3D space. When converting from\na right-handed system to a left-handed system or vice versa, the numerical\nrepresentations of all the points and vectors stay the same, but one axis flips.\nOur visualization of everything is therefore mirrored along that flipped axis.\nSo if a cross product just happens to align with the axis we’re flipping (e.g.,\nthez-axis), it needs to flip when the axis flips. If it didn’t, the mathemati-\ncal definition of the cross product itself would have to be changed so that the\nz-coordinate of the cross product comes out negative in the new coordinate\nsystem. I wouldn’t lose too much sleep over all of this. Just remember: when\nvisualizing a cross product, use the right-hand rule in a right-handed coordi-\nnate system and the left-hand rule in a left-handed coordinate system.\nProperties of the Cross Product\nThe cross product is not commutative (i.e., order matters):\nab̸=ba.\nHowever, it is anti-commutative:\nab= (ba).\nThe cross product is distributive over addition:\na(b+c) = (ab) + (ac).\nAnd it combines with scalar multiplication as follows:\n(sa)b=a(sb) =s(ab).\nThe Cartesian basis vectors are related by cross products as follows:\nij= (ji) =k\njk= (kj) =i\nki= (ik) =j\nThese three cross products define the direction of positive rotations about the\nCartesian axes. The positive rotations go from xtoy(about z), from ytoz\n(about x) and from ztox(about y). Notice how the rotation about the y-axis\n“reversed” alphabetically, in that it goes from ztox(not from xtoz). As we’ll\n5.2. Points and Vectors 373\nsee below, this gives us a hint as to why the matrix for rotation about the y-\naxislooks inverted whencomparedtothematricesforrotationaboutthe x-and\nz-axes.\nThe Cross Product in Action\nThe cross product has a number of applications in games. One of its most\ncommonusesisforfindingavectorthatisperpendiculartotwoothervectors.\nAs we’ll see in Section 5.3.10.2, if we know an object’s local unit basis vectors,\n(ilocal,jlocalandklocal), we can easily find a matrix representing the object’s\norientation. Let’sassumethatallweknowistheobject’s klocalvector—i.e.,the\ndirection in which the object is facing. If we assume that the object has no roll\nabout klocal, then we can find ilocalby taking the cross product between klocal\n(which we already know) and the world-space up vector jworld(which equals\n[0 1 0 ]). We do so as follows: ilocal =normalize (jworldklocal). We can then\nfindjlocalby simply crossing ilocalandklocalas follows: jlocal =klocalilocal.\nA very similar technique can be used to find a unit vector normal to the\nsurface of a triangle or some other plane. Given three points on the plane, P1,\nP2andP3, the normal vector is just n=normalize((P2 P1)(P3 P1))\n.\nCross products are also used in physics simulations. When a force is ap-\npliedtoanobject,itwillgiverisetorotationalmotionifandonlyifitisapplied\noff-center. This rotational force is known as a torque, and it is calculated as fol-\nlows. Given a force F, and a vector rfrom the center of mass to the point at\nwhich the force is applied, the torque N=rF.\n5.2.4.9 Pseudovectors and Exterior Algebra\nWe mentioned in Section 5.2.2 that the cross product doesn’t actually produce\na vector—it produces a special kind of mathematical object known as a pseu-\ndovector. The difference between a vector and a pseudovector is pretty sub-\ntle. In fact, you can’t tell the difference between them at all when performing\nthe kinds of transformations we normally encounter in game programming—\ntranslation, rotation and scaling. It’s only when you reflectthe coordinate sys-\ntem (as happens when you move from a left-handed coordinate system to a\nright-handed system) that the special nature of pseudovectors becomes ap-\nparent. Under reflection, a vector transforms into its mirror image, as you’d\nprobably expect. But when a pseudovector is reflected, it transforms into its\nmirror image and also changesdirection .\nPositions and all of the derivatives thereof (linear velocity, acceleration,\njerk) are represented by true vectors (also known as polarvectors orcontravari-\nant vectors ). Angular velocities and magnetic fields are represented by pseu-\n374 5. 3D Math for Games\nuv\nuvw\nFigure 5.15. In the exterior algebra (Grassman algebra), a single wedge product yields a pseudovec-\ntor or bivector, and two wedge products yield a pseudoscalar or trivector.\ndovectors (also known as axial vectors, covariant vectors ,bivectors or2-blades).\nThe surface normal of a triangle (which is calculated using a cross product) is\nalso a pseudovector.\nIt’sprettyinterestingtonotethatthecrossproduct( AB),thescalartriple\nproduct( A(BC))andthedeterminantofamatrixareallinter-related,and\npseudovectors lie at the heart of it all. Mathematicians have come up with\na set of algebraic rules, called an exterior algebra orGrassman algebra, which\ndescribe how vectors and pseudovectors work and allow us to calculate areas\nof parallelograms (in 2D), volumes of parallelepipeds (in 3D), and so on in\nhigher dimensions.\nWe won’t get into all the details here, but the basic idea of Grassman alge-\nbraistointroduceaspecialkindofvectorproductknownasthe wedgeproduct,\ndenoted A^B. Apairwisewedgeproductyieldsapseudovectorandisequiv-\nalent to a cross product, which also represents the signed area of the parallelo-\ngramformedbythetwovectors(wherethesigntellsuswhetherwe’rerotating\nfrom AtoBor vice versa). Doing two wedge products in a row, A^B^C,",28824
38-5.3 Matrices.pdf,38-5.3 Matrices,"5.3. Matrices 375\nis equivalent to the scalar triple product A(BC)and produces another\nstrange mathematical object known as a pseudoscalar (also known as a trivector\nora3-blade),whichcanbeinterpretedasthe signedvolume oftheparallelepiped\nformed by the three vectors (see Figure 5.15). This extends into higher dimen-\nsions as well.\nWhat does all this mean for us as game programmers? Not too much. All\nwe really need to keep in mind is that some vectors in our code are actually\npseudovectors, sothatwecantransformthemproperlywhenchanginghand-\nedness,forexample. Ofcourseifyoureallywanttogeekout,youcanimpress\nyour friends by talking about exterioralgebras andwedgeproducts and explain-\ninghowcrossproductsaren’treallyvectors. Whichmightmakeyoulookcool\nat your next social engagement …or not.\nFor more information, see http://en.wikipedia.org/wiki/Pseudovector,\nhttp://en.wikipedia.org/wiki/Exterior_algebra, and http://www.terathon\n.com/gdc12_lengyel.pdf.\n5.2.5 Linear Interpolation of Points and Vectors\nIn games, we often need to find a vector that is midway between two known\nvectors. For example, if we want to smoothly animate an object from point A\nto point Bover the course of two seconds at 30 frames per second, we would\nneed to find 60 intermediate positions between AandB.\nAlinearinterpolation isasimplemathematicaloperationthatfindsaninter-\nmediatepointbetweentwoknownpoints. Thenameofthisoperationisoften\nshortened to LERP. The operation is defined as follows, where branges from\n0 to 1 inclusive:\nL=LERP (A,B,b) = ( 1 b)A+bB\n=[(1 b)Ax+bBx,(1 b)Ay+bBy,(1 b)Az+bBz]\nGeometrically, L=LERP (A,B,b)is the position vector of a point that lies\nbpercentofthewayalongthelinesegmentfrompoint Atopoint B, asshown\nin Figure 5.16. Mathematically, the LERP function is just a weighted average of\nthe two input vectors, with weights (1 b)andb, respectively. Notice that\nthe weights always add to 1, which is a general requirement for any weighted\naverage.\n5.3 Matrices\nAmatrixisarectangulararrayof mnscalars. Matricesareaconvenientway\nof representing linear transformations such as translation, rotation and scale.\n376 5. 3D Math for Games\nFigure 5.16. Linear interpolation (LERP) between points A andB , with b=0.4.\nA matrix Mis usually written as a grid of scalars Mrcenclosed in square\nbrackets,wherethesubscripts randcrepresenttherowandcolumnindicesof\nthe entry, respectively. For example, if Mis a33matrix, it could be written\nas follows:\nM=2\n4M11M12M13\nM21M22M23\nM31M32M333\n5.\nWe can think of the rows and/or columns of a 33matrix as 3D vectors.\nWhenalloftherowandcolumnvectorsofa 33matrixareofunitmagnitude,\nwe call it a special orthogonal matrix. This is also known as an isotropic matrix,\nor anorthonormal matrix. Such matrices represent pure rotations.\nUndercertainconstraints,a 44matrixcanrepresentarbitrary3D transfor-\nmations, including translations, rotations , and changes in scale. These are called\ntransformationmatrices ,andtheyarethekindsofmatricesthatwillbemostuse-\nful to us as game engineers. The transformations represented by a matrix are\napplied to a point or vector via matrix multiplication. We’ll investigate how\nthis works below.\nAnaffinematrix is a 44transformation matrix that preserves parallelism\nof lines and relative distance ratios, but not necessarily absolute lengths and\nangles. An affine matrix is any combination of the following operations: rota-\ntion, translation, scale and/or shear.\n5.3.1 Matrix Multiplication\nThe product Pof two matrices AandBis written P=AB. IfAandBare\ntransformation matrices, then the product Pis another transformation matrix\nthat performs bothof the original transformations. For example, if Ais a scale\nmatrix and Bis a rotation, the matrix Pwould both scale androtate the points\nor vectors to which it is applied. This is particularly useful in game program-\nming, because we can precalculate a single matrix that performs a whole se-\nquence of transformations and then apply all of those transformations to a\nlarge number of vectors efficiently.\n5.3. Matrices 377\nTo calculate a matrix product, we simply take dot products between the\nrows of the nAmAmatrix Aand the columns of the nBmBmatrix B. Each\ndotproductbecomesonecomponentoftheresultingmatrix P. Thetwomatri-\nces can be multiplied as long as the inner dimensions are equal (i.e., mA=nB).\nFor example, if AandBare33matrices, then P=ABmay be expressed as\nfollows:\nP=2\n4P11P12P13\nP21P22P23\nP31P32P333\n5\n=2\n4Arow1Bcol1 Arow1Bcol2 Arow1Bcol3\nArow2Bcol1 Arow2Bcol2 Arow2Bcol3\nArow3Bcol1 Arow3Bcol2 Arow3Bcol33\n5.\nMatrix multiplication is not commutative. In other words, the order in\nwhich matrix multiplication is done matters:\nAB̸=BA\nWe’ll see exactly why this matters in Section 5.3.2.\nMatrix multiplication is often called concatenation, because the product of\nntransformationmatricesisamatrixthatconcatenates,orchainstogether,the\noriginalsequenceoftransformationsintheorderthematricesweremultiplied.\n5.3.2 Representing Points and Vectors as Matrices\nPointsandvectorscanberepresentedas rowmatrices (1n)orcolumnmatrices\n(n1), where nis the dimension of the space we’re working with (usually 2\nor 3). For example, the vector v= (3, 4, 1)can be written either as\nv1=[3 4 1]\nor as\nv2=2\n43\n4\n 13\n5=vT\n1.\nHere, the superscripted T represents matrix transposition (see Section 5.3.5).\nThe choice between column and row vectors is a completely arbitrary one,\nbut it does affect the order in which matrix multiplications are written. This\nhappens because when multiplying matrices, the inner dimensions of the two\nmatrices must be equal, so\n378 5. 3D Math for Games\n• tomultiplya 1nrowvectorbyan nnmatrix,thevectormustappear\nto theleftof the matrix ( v′\n1n=v1nMnn), whereas\n• to multiply an nnmatrix by an n1column vector, the vector must\nappear to the rightof the matrix ( v′\nn1=Mnnvn1).\nIf multiple transformation matrices A,BandCare applied in order to a\nvector v, the transformations “read” from lefttoright when using rowvectors,\nbut from righttoleft when using columnvectors. The easiest way to remember\nthis is to realize that the matrix closestto the vector is applied first. This is\nillustrated by the parentheses below:\nv′= ((( vA)B)C) Row vectors: read left-to-right ;\nv′T= (CT(BT(ATvT)))Column vectors: read right-to-left.\nIn this book we’ll adopt the row vector convention , because the left-to-right\norderoftransformationsismostintuitivetoreadforEnglish-speakingpeople.\nThatsaid, beverycarefultocheckwhichconventionisusedbyyourgameen-\ngine, andbyotherbooks, papersorwebpagesyoumayread. Youcanusually\ntell by seeing whether vector-matrix multiplications are written with the vec-\ntor on the left (for row vectors) or the right (for column vectors) of the matrix.\nWhen using column vectors, you’ll need to transpose all the matrices shown in\nthis book.\n5.3.3 The Identity Matrix\nTheidentitymatrix isamatrixthat,whenmultipliedbyanyothermatrix,yields\nthe very same matrix. It is usually represented by the symbol I. The identity\nmatrix is always a square matrix with 1’s along the diagonal and 0’s every-\nwhere else:\nI33=2\n41 0 0\n0 1 0\n0 0 13\n5;\nAI=IAA.\n5.3.4 Matrix Inversion\nTheinverseof a matrix Ais another matrix (denoted A 1) thatundoesthe ef-\nfects of matrix A. So, for example, if Arotates objects by 37degrees about\nthez-axis, then A 1will rotate by 37degrees about the z-axis. Likewise, if\nAscales objects to be twice their original size, then A 1scales objects to be\nhalf-sized. When a matrix is multiplied by its own inverse, the result is al-\nwaysthe identity matrix, so A(A 1)(A 1)AI. Not all matrices have\n5.3. Matrices 379\ninverses. However, all affinematrices (combinations of pure rotations, trans-\nlations, scales and shears) do have inverses. Gaussian elimination or lower-\nupper (LU) decomposition can be used to find the inverse, if one exists.\nSincewe’llbedealingwithmatrixmultiplicationalot,it’simportanttonote\nhere that the inverse of a sequence of concatenated matrices can be written as\nthereverseconcatenation of the individual matrices’ inverses. For example,\n(ABC ) 1=C 1B 1A 1.\n5.3.5 Transposition\nThetranspose of a matrix Mis denoted MT. It is obtained by reflecting the\nentries of the original matrix across its diagonal. In other words, the rows of\nthe original matrix become the columns of the transposed matrix, and vice\nversa:2\n4a b c\nd e f\ng h i3\n5T\n=2\n4a d g\nb e h\nc f i3\n5.\nThe transpose is useful for a number of reasons. For one thing, the inverse\nof an orthonormal (pure rotation) matrix is exactly equal to its transpose—\nwhich is good news, because it’s much cheaper to transpose a matrix than it is\nto find its inverse in general. Transposition can also be important when mov-\ning data from one math library to another, because some libraries use column\nvectors while others expect row vectors. The matrices used by a row-vector–\nbasedlibrarywillbe transposed relativetothoseusedbyalibrarythatemploys\nthe column vector convention.\nAs with the inverse, the transpose of a sequence of concatenated matrices\ncanberewrittenasthereverseconcatenationoftheindividualmatrices’trans-\nposes. For example,\n(ABC )T=CTBTAT.\nThiswillproveusefulwhenweconsiderhowtoapplytransformationmatrices\nto points and vectors.\n5.3.6 Homogeneous Coordinates\nYou may recall from high-school algebra that a 22matrix can represent a\nrotation in two dimensions. To rotate a vector rthrough an angle of ϕdegrees\n(where positive rotations are counterclockwise), we can write\n[r′\nxr′\ny]=[rxry][cosϕ sinϕ\n sinϕcosϕ]\n.\n380 5. 3D Math for Games\nIt’sprobablynosurprisethatrotationsinthreedimensionscanberepresented\nby a 33matrix. The two-dimensional example above is really just a three-\ndimensional rotation about the z-axis, so we can write\n[r′\nxr′\nyr′\nz]=[rxryrz]2\n4cosϕ sinϕ0\n sinϕcosϕ0\n0 0 13\n5.\nThequestionnaturallyarises: Cana 33matrixbeusedtorepresent trans-\nlations? Sadly, theanswerisno. Theresultoftranslatingapoint rbyatransla-\ntiontrequiresaddingthecomponentsof ttothecomponentsof rindividually:\nr+t=[(rx+tx) (ry+ty) (rz+tz)]\n.\nMatrixmultiplicationinvolvesmultiplicationandadditionofmatrixelements,\nsotheideaofusingmultiplicationfortranslationseemspromising. But,unfor-\ntunately, there is no way to arrange the components of twithin a 33matrix\nsuchthatthe resultofmultiplyingit withthecolumn vector ryieldssums like\n(rx+tx).\nThegoodnewsisthatwe canobtainsumslikethisifweusea 44matrix.\nWhat would such a matrix look like? Well, we know that we don’t want any\nrotational effects, so the upper 33should contain an identity matrix. If we\narrange the components of tacross the bottom-most row of the matrix and set\nthe fourth element of the rvector (usually called w) equal to 1, then taking the\ndot product of the vector rwith column 1 of the matrix will yield (1rx) + ( 0\nry) + ( 0rz) + (tx1),whichisexactlywhatwewant. Ifthebottomright-hand\ncorner of the matrix contains a 1 and the rest of the fourth column contains\nzeros, then the resulting vector will also have a 1 in its wcomponent. Here’s\nwhat the final 44translation matrix looks like:\nr+t=[\nrxryrz1]2\n6641 0 0 0\n0 1 0 0\n0 0 1 0\ntxtytz13\n775\n=[(rx+tx) (ry+ty) (rz+tz)1]\n.\nWhen a point or vector is extended from three dimensions to four in this\nmanner, we say that it has been written in homogeneous coordinates. A point\nin homogeneous coordinates always has w=1. Most of the 3D matrix math\ndone by game engines is performed using 44matrices with four-element\npoints and vectors written in homogeneous coordinates.\n5.3. Matrices 381\n5.3.6.1 Transforming Direction Vectors\nMathematically, points (position vectors) and direction vectors are treated in\nsubtlydifferentways. Whentransformingapointbyamatrix, thetranslation,\nrotation and scale of the matrix are all applied to the point. But when trans-\nforming a direction by a matrix, the translational effects of the matrix are ig-\nnored. This is because direction vectors have no translation per se—applying\na translation to a direction would alter its magnitude, which is usually not\nwhat we want.\nIn homogeneous coordinates, we achieve this by defining points to have\ntheir wcomponents equal to one, while direction vectors have their wcompo-\nnents equal to zero. In the example below, notice how the w=0component\nof the vector vmultiplies with the tvector in the matrix, thereby eliminating\ntranslation in the final result:\n[\nv0][\nU0\nt1]\n=[(vU+0t)0]=[\nvU 0]\n.\nTechnically, a point in homogeneous (four-dimensional) coordinates can\nbe converted into non-homogeneous (three-dimensional) coordinates by di-\nviding the x,yandzcomponents by the wcomponent:\n[x y z w][x\nwy\nwz\nw]\n.\nThis sheds some light on why we set a point’s wcomponent to one and a vec-\ntor’s wcomponenttozero. Dividingby w=1hasnoeffectonthecoordinates\nof a point, but dividing a pure direction vector’s components by w=0would\nyield infinity. A point at infinity in 4D can be rotated but not translated, be-\ncause no matter what translation we try to apply, the point will remain at in-\nfinity. So in effect, a pure direction vector in three-dimensional space acts like\na point at infinity in four-dimensional homogeneous space.\n5.3.7 Atomic Transformation Matrices\nAny affine transformation matrix can be created by simply concatenating a\nsequenceof 44matricesrepresentingpuretranslations,purerotations,pure\nscale operations and/or pure shears. These atomic transformation building\nblocks are presented below. (We’ll omit shear from these discussions, as it\ntends to be used only rarely in games.)\nNotice that all affine 44transformation matrices can be partitioned into\nfour components:\nMaffine =[U33031\nt13 1]\n.\n382 5. 3D Math for Games\n• the upper 33matrix U, which represents the rotation and/or scale,\n• a 13translation vector t,\n• a 31vector of zeros 0=[0 0 0]T, and\n• a scalar 1 in the bottom-right corner of the matrix.\nWhen a point is multiplied by a matrix that has been partitioned like this, the\nresult is as follows:\n[r′\n131]=[\nr131][U33031\nt13 1]\n=[(rU+t)1]\n.\n5.3.7.1 Translation\nThe following matrix translates a point by the vector t:\nr+t=[\nrxryrz1]2\n6641 0 0 0\n0 1 0 0\n0 0 1 0\ntxtytz13\n775(5.3)\n=[(rx+tx) (ry+ty) (rz+tz)1]\n,\nor in partitioned shorthand:\n[r1][\nI0\nt1]\n=[(r+t)1]\n.\nTo invert a pure translation matrix, simply negate the vector t(i.e., negate tx,\ntyandtz).\n5.3.7.2 Rotation\nAll44pure rotation matrices have the form\n[r1][R0\n0 1]\n=[rR 1]\n.\nThetvector is zero, and the upper 33matrix Rcontains cosines and sines\nof the rotation angle, measured in radians.\nThe following matrix represents rotation about the x-axis by an angle ϕ.\nrotate x(r,ϕ) =[\nrxryrz1]2\n6641 0 0 0\n0 cos ϕ sinϕ0\n0 sinϕcosϕ0\n0 0 0 13\n775.(5.4)\n5.3. Matrices 383\nThe matrix below represents rotation about the y-axis by an angle q. (Notice\nthat this one is transposed relative to the other two—the positive and negative\nsine terms have been reflected across the diagonal.)\nrotate y(r,q) =[rxryrz1]2\n664cosq0 sinq0\n0 1 0 0\nsinq0 cos q 0\n0 0 0 13\n775.(5.5)\nThe following matrix represents rotation about the z-axis by an angle g:\nrotate z(r,g) =[rxryrz1]2\n664cosg sing0 0\n singcosg0 0\n0 0 1 0\n0 0 0 13\n775.(5.6)\nHere are a few observations about these matrices:\n• The 1within the upper 33always appears on the axis we’re rotating\nabout, while the sine and cosine terms are off-axis.\n• Positiverotationsgofrom xtoy(about z), from ytoz(about x)andfrom\nztox(about y). The ztoxrotation “wraps around,” which is why the\nrotation matrix about the y-axis is transposed relative to the other two.\n(Use the right-hand or left-hand rule to remember this.)\n• The inverse of a pure rotation is just its transpose. This works because\ninverting a rotation is equivalent to rotating by the negative angle. You\nmay recall that cos( q) = cos(q)while sin( q) = sin(q), so negating\ntheanglecausesthetwosinetermstoeffectivelyswitchplaces,whilethe\ncosine terms stay put.\n5.3.7.3 Scale\nThe following matrix scales the point rby a factor of sxalong the x-axis, sy\nalong the y-axis and szalong the z-axis:\nrS=[rxryrz1]2\n664sx0 0 0\n0sy0 0\n0 0 sz0\n0 0 0 13\n775(5.7)\n=[\nsxrxsyryszrz1]\n,\nor in partitioned shorthand:\n[\nr1][S330\n0 1]\n=[\nrS331]\n.\nHere are some observations about this kind of matrix:\n384 5. 3D Math for Games\n• To invert a scaling matrix, simply substitute sx,syandszwith their re-\nciprocals (i.e., 1/sx,1/syand 1/sz).\n• When the scale factor along all three axes is the same ( sx=sy=sz),\nwe call this uniform scale. Spheres remain spheres under uniform scale,\nwhereas under nonuniform scale they become ellipsoids. To keep the\nmathematics of bounding sphere checks simple and fast, many game\nengines impose the restriction that only uniform scale may be applied\nto renderable geometry or collision primitives.\n• When a uniform scale matrix Suand a rotation matrix Rare concate-\nnated,theorderofmultiplicationisunimportant(i.e., SuR=RSu). This\nonly works for uniform scale!\n5.3.8 4 3 Matrices\nThe rightmost column of an affine 44matrix always contains the vector[0 0 0 1]T. As such, game programmers often omit the fourth column to\nsave memory. You’ll encounter 43affine matrices frequently in game math\nlibraries.\n5.3.9 Coordinate Spaces\nWe’ve seen how to apply transformations to points and direction vectors us-\ning44matrices. We can extend this idea to rigid objects by realizing that\nsuch an object can be thought of as an infinite collection of points. Applying\na transformation to a rigid object is like applying that same transformation to\nevery point within the object. For example, in computer graphics an object is\nusually represented by a mesh of triangles, each of which has three vertices\nrepresented by points. In this case, the object can be transformed by applying\na transformation matrix to all of its vertices in turn.\nAABB\nAB\nFigure 5.17. Position vectors for the point P relative to different coordinate axes.\n5.3. Matrices 385\nWe said above that a point is a vector whose tail is fixed to the origin of\nsome coordinate system. This is another way of saying that a point (position\nvector) is always expressed relativeto a set of coordinate axes. The triplet of\nnumbers representing a point changes numerically whenever we select a new\nset of coordinate axes. In Figure 5.17, we see a point P represented by two\ndifferent position vectors—the vector PAgives the position of Prelative to the\n“A” axes, while the vector PBgives the position of that same point relative to\na different set of axes “B.”\nIn physics, a set of coordinate axes represents a frame of reference, so we\nsometimes refer to a set of axes as a coordinateframe (or just a frame). People in\nthe game industry also use the term coordinate space (or simply space) to refer\nto a set of coordinate axes. In the following sections, we’ll look at a few of the\nmost common coordinate spaces used in games and computer graphics.\n5.3.9.1 Model Space\nWhen a triangle mesh is created in a tool such as Maya or 3DStudioMAX, the\npositions of the triangles’ vertices are specified relative to a Cartesian coordi-\nnatesystem,whichwecall modelspace (alsoknownas objectspace orlocalspace).\nThemodel-spaceoriginisusuallyplacedatacentrallocationwithintheobject,\nsuch as at its center of mass, or on the ground between the feet of a humanoid\nor animal character.\nMost game objects have an inherent directionality. For example, an air-\nplane has a nose, a tail fin and wings that correspond to the front, up and\nleft/right directions. The model-space axes are usually aligned to these natu-\nraldirectionsonthemodel, andthey’regivenintuitivenamestoindicatetheir\ndirectionality as illustrated in Figure 5.18.\n•Front. This name is given to the axis that points in the direction that the\nobject naturally travels or faces. In this book, we’ll use the symbol Fto\nleftfrontup\nFigure 5.18. One possible choice of the model-space front, left and up axis basis vectors for an\nairplane.\n386 5. 3D Math for Games\nrefer to a unit basis vector along the front axis.\n•Up. This name is given to the axis that points towards the top of the\nobject. The unit basis vector along this axis will be denoted U.\n•Left or right . The name “left” or “right” is given to the axis that points\ntoward the left or right side of the object. Which name is chosen de-\npends on whether your game engine uses left-handed or right-handed\ncoordinates. The unit basis vector along this axis will be denoted LorR,\nas appropriate.\nThe mapping between the (f ront ,up,le f t)labels and the (x,y,z)axes\nis completely arbitrary. A common choice when working with right-handed\naxesistoassignthelabel fronttothepositive z-axis,thelabel lefttothepositive\nx-axis and the label upto the positive y-axis (or in terms of unit basis vectors,\nF=k,L=iandU=j). However, it’sequallycommonfor +xtobefrontand\n+zto be right ( F=i,R=k,U=j). I’ve also worked with engines in which\nthez-axis is oriented vertically. The only real requirement is that you stick to\none convention consistently throughout your engine.\nAs an example of how intuitive axis names can reduce confusion, consider\nEuler angles (pitch ,yaw,roll), which are often used to describe an aircraft’s\norientation. It’s not possible to define pitch, yaw, and roll angles in terms of\nthe(i,j,k)basis vectors because their orientation is arbitrary. However, we\ncandefine pitch, yaw and roll in terms of the (L,U,F)basis vectors, because\ntheir orientations are clearly defined. Specifically,\n•pitchis rotation about LorR,\n•yawis rotation about U, and\n•rollis rotation about F.\n5.3.9.2 World Space\nWorldspace isafixedcoordinatespace,inwhichthepositions,orientationsand\nscalesofallobjectsinthegameworldareexpressed. Thiscoordinatespaceties\nall the individual objects together into a cohesive virtual world.\nThe location of the world-space origin is arbitrary, but it is often placed\nnear the center of the playable game space to minimize the reduction in\nfloating-point precision that can occur when (x,y,z)coordinates grow very\nlarge. Likewise, the orientation of the x-,y- and z-axes is arbitrary, although\nmost of the engines I’ve encountered use either a y-up or a z-up convention.\nThey-up convention was probably an extension of the two-dimensional con-\nvention found in most mathematics textbooks, where the y-axis is shown go-\ning up and the x-axis going to the right. The z-up convention is also common,\n5.3. Matrices 387\nzWxWxM\nzM(5,0,0)M(–25,50,3)W\n(–25,50,8)WAircraft:Left\nWingtip:\nFigure 5.19. A Lear jet whose left wingtip is at (5, 0, 0 ) in model space. If the jet is rotated by 90\ndegrees about the world-space y-axis, and its model-space origin translated to ( 25, 50, 8 ) in\nworld space, then its left wingtip would end up at ( 25, 50, 3 ) when expressed in world-space\ncoordinates.\nbecauseitallowsatop-downorthographicviewofthegameworldtolooklike\na traditional two-dimensional xy-plot.\nAsanexample,let’ssaythatouraircraft’sleftwingtipisat (5, 0, 0 )inmodel\nspace. (In our game, front vectors correspond to the positive z-axis in model\nspace with yup, as shown in Figure 5.18.) Now imagine that the jet is facing\ndown the positive x-axis in world space, with its model-space origin at some\narbitrary location, such as ( 25, 50, 8 ). Because the Fvector of the airplane,\nwhich corresponds to + zin model space, is facing down the +x-axis in world\nspace, we know that the jet has been rotated by 90 degrees about the world\ny-axis. So, if the aircraft were sitting at the world-space origin, its left wingtip\nwould be at (0, 0, 5)in world space. But because the aircraft’s origin has\nbeen translated to ( 25, 50, 8 ), the final position of the jet’s left wingtip in\nworld space is ( 25, 50, [8 5]) = ( 25, 50, 3 ). This is illustrated in Fig-\nure 5.19.\nWe could of course populate our friendly skies with more than one Lear\njet. In that case, all of their left wingtips would have coordinates of (5, 0, 0 )\nin model space. But in world space, the left wingtips would have all sorts of\ninteresting coordinates, depending on the orientation and translation of each\naircraft.\n5.3.9.3 View Space\nViewspace (also known as cameraspace) is a coordinate frame fixed to the cam-\nera. The view space origin is placed at the focal point of the camera. Again,\nany axis orientation scheme is possible. However, a y-up convention with z\nincreasinginthedirectionthecameraisfacing(left-handed)istypicalbecause\nit allows zcoordinates to represent depths into the screen. Other engines and\nAPIs,suchasOpenGL,defineviewspacetoberight-handed,inwhichcasethe\ncamera faces towards negative z, and zcoordinates represent negative depths.\n388 5. 3D Math for Games\nLeft-Handedxzy\nRight-Handedzxy Virtual\nScreenVirtua l \nScreen\nFigure 5.20. Left- and right-handed examples of view space, also known as camera space.\nTwo possible definitions of view space are illustrated in Figure 5.20.\n5.3.10 Change of Basis\nIn games and computer graphics, it is often quite useful to convert an object’s\nposition, orientation and scale from one coordinate system into another. We\ncall this operation a changeof basis .\n5.3.10.1 Coordinate Space Hierarchies\nCoordinate frames are relative. That is, if you want to quantify the position,\norientation and scale of a set of axes in three-dimensional space, you must\nspecify these quantities relative to some other set of axes (otherwise the num-\nbers would have no meaning). This implies that coordinate spaces form a hi-\nerarchy—every coordinate space is a childof some other coordinate space, and\nthe other space acts as its parent. World space has no parent; it is at the root\nof the coordinate-space tree, and all other coordinate systems are ultimately\nspecified relative to it, either as direct children or more-distant relatives.\n5.3.10.2 Building a Change of Basis Matrix\nThe matrix that transforms points and directions from any child coordinate\nsystem C to its parent coordinate system P can be written MC!P(pronounced\n“CtoP”).Thesubscriptindicatesthatthismatrixtransformspointsanddirec-\ntionsfromchildspacetoparentspace. Anychild-spacepositionvector PCcan\n5.3. Matrices 389\nbe transformed into a parent-space position vector PPas follows:\nPP=PCMC!P;\nMC!P=2\n664iC0\njC0\nkC0\ntC13\n775\n=2\n664iCx iCy iCz 0\njCx jCy jCz 0\nkCxkCykCz 0\ntCx tCy tCz 13\n775.\nIn this equation,\n•iCis the unit basis vector along the child space x-axis, expressed in\nparent-space coordinates;\n•jCis the unit basis vector along the child space y-axis, in parent space;\n•kCis the unit basis vector along the child space z-axis, in parent space;\nand\n•tCis the translation of the child coordinate system relative to parent\nspace.\nThisresultshouldnotbetoosurprising. The tCvectorisjustthetranslation\nofthechild-spaceaxesrelativetoparentspace, soiftherestofthematrixwere\nidentity, the point (0, 0, 0 )in child space would become tCin parent space,\njust as we’d expect. The iC,jCandkCunit vectors form the upper 33of\nthe matrix, which is a pure rotation matrix because these vectors are of unit\nlength. We can see this more clearly by considering a simple example, such as\na situation in which child space is rotated by an angle gabout the z-axis, with\nno translation. Recall from Equation (5.6) that the matrix for such a rotation is\ngiven by\nrotate z(r,g) =[\nrxryrz1]2\n664cosg sing0 0\n singcosg0 0\n0 0 1 0\n0 0 0 13\n775.\nBut in Figure 5.21, we can see that the coordinates of the iCandjCvectors, ex-\npressedinparentspace,are iC=[cosgsing0]\nandjC=[ singcosg0]\n.\nWhenweplugthesevectorsintoourformulafor MC!P,with kC=[0 0 1]\n,\nit exactly matches the matrix rotate z(r,g)from Equation (5.6).\n390 5. 3D Math for Games\nScaling the Child Axes\nScaling of the child coordinate system is accomplished by simply scaling the\nunit basis vectors appropriately. For example, if child space is scaled up by a\nfactor of two, then the basis vectors iC,jCandkCwill be of length 2instead of\nunit length.\n5.3.10.3 Extracting Unit Basis Vectors from a Matrix\nThe fact that we can build a change of basis matrix out of a translation and\nthree Cartesian basis vectors gives us another powerful tool: Given anyaffine\n44transformation matrix, we can go in the other direction and extract the\nchild-space basis vectors iC,jCandkCfrom it by simply isolating the ap-\npropriate rows of the matrix (or columns if your math library uses column\nvectors).\nThis can be incredibly useful. Let’s say we are given a vehicle’s model-\nto-world transform as an affine 44matrix (a very common representation).\nThisisreallyjustachangeofbasismatrix,transformingpointsinmodelspace\ninto their equivalents in world space. Let’s further assume that in our game,\nthe positive z-axis always points in the direction that an object is facing. So,\nto find a unit vector representing the vehicle’s facing direction, we can simply\nextract kCdirectlyfromthemodel-to-worldmatrix(bygrabbingitsthirdrow).\nThis vector will already be normalized and ready to go.\n5.3.10.4 Transforming Coordinate Systems versus Vectors\nWe’ve said that the matrix MC!Ptransforms points and directions from child\nspace into parent space. Recall that the fourth row of MC!Pcontains tC, the\ntranslationofthechildcoordinateaxesrelativetotheworld-spaceaxes. There-\nfore, another way to visualize the matrix MC!Pis to imagine it taking the\nparentcoordinate axes and transforming them intothe child axes. This is the\nxy\nγγ\nγγ\nγγCC\nFigure 5.21. Change of basis when child axes are rotated by an angle g relative to parent.\n5.3. Matrices 391\nxy\nx'y'y\nxP'\nP P\nFigure 5.22. Two ways to interpret a transformation matrix. On the left, the point moves against\na ﬁxed set of axes. On the right, the axes move in the opposite direction while the point remains\nﬁxed.\nreverse of what happens to points and direction vectors. In other words, if a\nmatrix transforms vectorsfrom child space to parent space, then it also trans-\nformscoordinateaxes fromparentspacetochildspace. Thismakessensewhen\nyou think about it—moving a point 20 units to the right with the coordinate\naxes fixed is the same as moving the coordinate axes 20 units to the left with\nthe point fixed. This concept is illustrated in Figure 5.22.\nOf course, this is just another point of potential confusion. If you’re think-\ning in terms of coordinate axes, then transformations go in one direction, but\nifyou’rethinkingintermsofpointsandvectors,theygointheotherdirection!\nAs with many confusing things in life, your best bet is probably to choose a\nsingle“canonical”wayofthinkingaboutthingsandstickwithit. Forexample,\nin this book we’ve chosen the following conventions:\n• Transformations apply to vectors (not coordinate axes).\n• Vectors are written as rows (not columns).\nTaken together, these two conventions allow us to read sequences of ma-\ntrix multiplications from left to right and have them make sense (e.g., in the\nexpression rD=rAMA!BMB!CMC!D, the B’s and C’s in effect “cancel out,”\nleaving only rD=rAMA!D). Obviously if you start thinking about the coor-\ndinateaxesmovingaroundratherthanthepointsandvectors,youeitherhave\nto read the transforms from right to left, or flip one of these two conventions\naround. It doesn’t really matter what conventions you choose as long as you\nfind them easy to remember and work with.\nThat said, it’s important to note that certain problems are easier to think\nabout in terms of vectors being transformed, while others are easier to work\nwith when you imagine the coordinate axes moving around. Once you get\ngood at thinking about 3D vector and matrix math, you’ll find it pretty easy\nto flip back and forth between conventions as needed to suit the problem at\nhand.\n392 5. 3D Math for Games\n5.3.11 Transforming Normal Vectors\nAnormal vector is a special kind of vector, because in addition to (usually!)\nbeing of unit length, it carries with it the additional requirementthat it should\nalways remain perpendicular to whatever surface or plane it is associated with.\nSpecial care must be taken when transforming a normal vector to ensure that\nboth its length and perpendicularity properties are maintained.\nIngeneral, ifapointor(non-normal)vectorcanberotatedfromspaceAto\nspaceBviathe 33matrix MA!B,thenanormalvector nwillbetransformed\nfrom space A to space B via the inversetranspose of that matrix, (M 1\nA!B)T. We\nwill not prove or derive this result here (see [32, Section 3.5] for an excellent\nderivation). However, we will observe that if the matrix MA!Bcontains only\nuniformscaleandnoshear,thentheanglesbetweenallsurfacesandvectorsin\nspaceBwillbethesameastheywereinspaceA.Inthiscase,thematrix MA!B\nwill actually work just fine for any vector, normal or non-normal. However, if\nMA!Bcontainsnonuniform scale orshear (i.e., is non-orthogonal), thenthe an-\ngles between surfaces and vectors are notpreserved when moving from space\nAtospaceB.AvectorthatwasnormaltoasurfaceinspaceAwillnotnecessar-\nilybeperpendiculartothatsurfaceinspaceB.Theinversetransposeoperation\naccounts for this distortion, bringing normal vectors back into perpendicu-\nlarity with their surfaces even when the transformation involves nonuniform\nscale or shear. Another way of looking at this is that the inverse transpose is\nrequiredbecauseasurfacenormalisreallya pseudovector ratherthanaregular\nvector (see Section 5.2.4.9).\n5.3.12 Storing Matrices in Memory\nIn the C and C++ languages, a two-dimensional array is often used to store a\nmatrix. RecallthatinC/C++two-dimensionalarraysyntax,thefirstsubscript\nis the row and the second is the column, and the column index varies fastest\nas you move through memory sequentially.\nfloat m[4][4]; // [row][col], col varies fastest\n// ""flatten"" the array to demonstrate ordering\nfloat* pm = &m[0][0];\nASSERT( &pm[0] == &m[0][0] );\nASSERT( &pm[1] == &m[0][1] );\nASSERT( &pm[2] == &m[0][2] );\n// etc.\nWe have two choices when storing a matrix in a two-dimensional C/C++\narray. We can either\n5.3. Matrices 393\n1. store the vectors (iC,jC,kC,tC)contiguously in memory (i.e., each row\ncontains a single vector), or\n2. storethevectors stridedinmemory(i.e.,eachcolumncontainsonevector).\nThe benefit of approach (1) is that we can address any one of the four vec-\ntors by simply indexing into the matrix and interpreting the four contiguous\nvalues we find there as a 4-element vector. This layout also has the benefit\nof matching up exactly with row vector matrix equations (which is another\nreason why I’ve selected row vector notation for this book). Approach (2) is\nsometimesnecessarywhendoingfastmatrix-vectormultipliesusingavector-\nenabled(SIMD)microprocessor,aswe’llseelaterinthischapter. Inmostgame\nengines I’ve personally encountered, matrices are stored using approach (1),\nwiththe vectorsintherowsofthetwo-dimensionalC/C++array. Thisisshown\nbelow:\nfloat M[4][4];\nM[0][0]=ix; M[0][1]=iy; M[0][2]=iz; M[0][3]=0.0f;\nM[1][0]=jx; M[1][1]=jy; M[1][2]=jz; M[1][3]=0.0f;\nM[2][0]=kx; M[2][1]=ky; M[2][2]=kz; M[2][3]=0.0f;\nM[3][0]=tx; M[3][1]=ty; M[3][2]=tz; M[3][3]=1.0f;\nThe matrix looks like this when viewed in a debugger:\nM[][]\n[0]\n[0] ix\n[1] iy\n[2] iz\n[3] 0.0000\n[1]\n[0] jx\n[1] jy\n[2] jz\n[3] 0.0000\n[2]\n[0] kx\n[1] ky\n[2] kz\n[3] 0.0000\n[3]\n[0] tx\n[1] ty\n[2] tz\n[3] 1.0000",35671
39-5.4 Quaternions.pdf,39-5.4 Quaternions,"394 5. 3D Math for Games\nOneeasywaytodeterminewhichlayoutyourengineusesistofindafunc-\ntion that builds a 44translation matrix. (Every good 3D math library pro-\nvides such a function.) You can then inspect the source code to see where the\nelementsofthe tvectorarebeingstored. Ifyoudon’thaveaccesstothesource\ncode of your math library (which is pretty rare in the game industry), you can\nalwayscallthefunctionwithaneasy-to-recognizetranslationlike (4, 3, 2 ),and\nthen inspect the resulting matrix. If row 3 contains the values 4.0f, 3.0f,\n2.0f, 1.0f, then the vectors are in the rows, otherwise the vectors are in the\ncolumns.\n5.4 Quaternions\nWe’ve seen that a 33matrix can be used to represent an arbitrary rotation\nin three dimensions. However, a matrix is not always an ideal representation\nof a rotation, for a number of reasons:\n1. We need nine floating-point values to represent a rotation, which seems\nexcessive considering that we only have three degrees of freedom—\npitch, yaw and roll.\n2. Rotatingavectorrequiresavector-matrixmultiplication,whichinvolves\nthree dot products, or a total of nine multiplications and six additions.\nWe would like to find a rotational representation that is less expensive\nto calculate, if possible.\n3. In games and computer graphics, it’s often important to be able to find\nrotations that are some percentage of the way between two known rota-\ntions. For example, if we are to smoothly animate a camera from some\nstartingorientationAtosomefinalorientationBoverthecourseofafew\nseconds,weneedtobeabletofindlotsofintermediaterotationsbetween\nA and B over the course of the animation. It turns out to be difficult to\ndo this when the A and B orientations are expressed as matrices.\nThankfully, there is a rotational representation that overcomes these three\nproblems. It is a mathematical object known as a quaternion . A quaternion\nlooks a lot like a four-dimensional vector, but it behaves quite differently.\nWe usually write quaternions using non-italic, non-boldface type, like this:\nq=[\nqxqyqzqw]\n.\nQuaternions were developed by Sir William Rowan Hamilton in 1843 as\nan extension to the complex numbers. (Specifically, a quaternion may be\ninterpreted as a four-dimensional complex number, with a single real axis\n5.4. Quaternions 395\nand three imaginary axes represented by the imaginary numbers i,jandk.\nAs such, a quaternion can be written in “complex form” as follows: q=\niqx+jqy+kqz+qw.) Quaternions were first used to solve problems in the\narea of mechanics. Technically speaking, a quaternion obeys a set of rules\nknown as a four-dimensional normed division algebra over the real numbers.\nThankfully, we won’t need to understand the details of these rather esoteric\nalgebraic rules. For our purposes, it will suffice to know that the unit-length\nquaternions(i.e., allquaternionsobeyingtheconstraint q2\nx+q2\ny+q2\nz+q2\nw=1)\nrepresent three-dimensional rotations.\nTherearealotofgreatpapers,webpagesandpresentationsonquaternions\navailable on the web for further reading. Here’s one of my favorites: http://\ngraphics.ucsd.edu/courses/cse169_w05/CSE169_04.ppt.\n5.4.1 Unit Quaternions as 3D Rotations\nAunitquaternioncanbevisualizedasathree-dimensionalvectorplusafourth\nscalar coordinate. The vector part qVis the unit axis of rotation, scaled by the\nsine of the half-angle of the rotation. The scalar part qSis the cosine of the\nhalf-angle. So the unit quaternion qcan be written as follows:\nq=[\nqVqS]\n=[\nasinq\n2cosq\n2]\n,\nwhere aisaunitvectoralongtheaxisofrotation,and qistheangleofrotation.\nThedirectionoftherotationfollowsthe right-handrule ,soifyourthumbpoints\nin the direction of a, positive rotations will be in the direction of your curved\nfingers.\nOf course, we can also write qas a simple four-element vector:\nq=[qxqyqzqw]\n,where\nqx=qVx=axsinq\n2,\nqy=qVy=aysinq\n2,\nqz=qVz=azsinq\n2,\nqw=qS=cosq\n2.\nA unit quaternion is very much like an axis+angle representation of a ro-\ntation (i.e., a four-element vector of the form[aq]\n). However, quaternions\naremoreconvenientmathematicallythantheiraxis+anglecounterparts,aswe\nshall see below.\n396 5. 3D Math for Games\n5.4.2 Quaternion Operations\nQuaternionssupportsomeofthefamiliaroperationsfromvectoralgebra,such\nas magnitude and vector addition. However, we must remember that the\nsum of two unit quaternions does not represent a 3D rotation, because such a\nquaternionwouldnotbeofunitlength. Asaresult, youwon’tseeanyquater-\nnion sums in a game engine, unless they are scaled in some way to preserve\nthe unit length requirement.\n5.4.2.1 Quaternion Multiplication\nOne of the most important operations we will perform on quaternions is that\nof multiplication. Given two quaternions pandqrepresenting two rotations\nPandQ, respectively, the product pqrepresents the composite rotation (i.e.,\nrotation Qfollowed by rotation P). There are actually quite a few different\nkinds of quaternion multiplication, but we’ll restrict this discussion to the va-\nriety used in conjunction with 3D rotations, namely the Grassman product.\nUsing this definition, the product pqis defined as follows:\npq=[(pSqV+qSpV+pVqV) (pSqS pVqV)]\n.\nNotice how the Grassman product is defined in terms of a vector part, which\nends up in the x,yandzcomponents of the resultant quaternion, and a scalar\npart, which ends up in the wcomponent.\n5.4.2.2 Conjugate and Inverse\nTheinverseof a quaternion qis denoted q 1and is defined as a quaternion\nthat, whenmultipliedbytheoriginal, yieldsthescalar1(i.e., qq 1=0i+0j+\n0k+1). Thequaternion[\n0 0 0 1]\nrepresentsazerorotation(whichmakes\nsense since sin(0) = 0for the first three components, and cos(0) = 1for the\nlast component).\nIn order to calculate the inverse of a quaternion, we must first define a\nquantity known as the conjugate. This is usually denoted qand it is defined\nas follows:\nq=[ qVqS]\n.\nInotherwords, wenegatethevectorpartbutleavethescalarpartunchanged.\nGiven this definition of the quaternion conjugate, the inverse quaternion\nq 1is defined as follows:\nq 1=q\njqj2.\n5.4. Quaternions 397\nOurquaternionsarealwaysofunitlength(i.e., jqj=1),becausetheyrepresent\n3D rotations. So, for our purposes, the inverse and the conjugate are identical:\nq 1=q=[ qVqS]\nwhenjqj=1.\nThis fact is incredibly useful, because it means we can always avoid doing\nthe(relativelyexpensive)divisionbythesquaredmagnitudewheninvertinga\nquaternion,aslongasweknowapriorithatthequaternionisnormalized. This\nalsomeansthatinvertingaquaternionisgenerallymuchfasterthaninverting\na33matrix—afactthatyoumaybeabletoleverageinsomesituationswhen\noptimizing your engine.\nConjugate and Inverse of a Product\nThe conjugate of a quaternion product (pq)is equal to the reverse product of\nthe conjugates of the individual quaternions:\n(pq)=qp.\nLikewise, the inverse of a quaternion product is equal to the reverse product\nof the inverses of the individual quaternions:\n(pq) 1=q 1p 1. (5.8)\nThis is analogous to the reversal that occurs when transposing or inverting\nmatrix products.\n5.4.3 Rotating Vectors with Quaternions\nHowcanweapplyaquaternionrotationtoavector? Thefirststepistorewrite\nthe vector in quaternion form. A vector is a sum involving the unit basis vec-\ntorsi,jandk. A quaternion is a sum involving i,jandk, but with a fourth\nscalar term as well. So it makes sense that a vector can be written as a quater-\nnion with its scalar term qSequal to zero. Given the vector v, we can write a\ncorresponding quaternion v=[v0]=[vxvyvz0]\n.\nIn order to rotate a vector vby a quaternion q, we premultiply the vector\n(written in its quaternion form v) byqand then post-multiply it by the inverse\nquaternion q 1. Therefore, the rotated vector v′can be found as follows:\nv′=rotate (q,v) =qvq 1.\nThis is equivalent to using the quaternion conjugate, because our quaternions\nare always unit length:\nv′=rotate (q,v) =qvq. (5.9)\n398 5. 3D Math for Games\nThe rotated vector v′is obtained by simply extracting it from its quaternion\nform v′.\nQuaternion multiplication can be useful in all sorts of situations in real\ngames. For example, let’s say that we want to find a unit vector describ-\ning the direction in which an aircraft is flying. We’ll further assume that in\nour game, the positive z-axis always points toward the front of an object by\nconvention. So the forward unit vector of any object in model space is always\nFM[\n0 0 1]\nby definition. To transform this vector into world space, we\ncan simply take our aircraft’s orientation quaternion qand use it with Equa-\ntion (5.9) to rotate our model-space vector FMinto its world-space equivalent\nFW(after converting these vectors into quaternion form, of course):\nFW=qFMq 1=q[0 0 1 0]\nq 1.\n5.4.3.1 Quaternion Concatenation\nRotations can be concatenated in exactly the same way that matrix-based trans-\nformations can, by multiplying the quaternions together. For example, con-\nsider three distinct rotations, represented by the quaternions q1,q2andq3,\nwith matrix equivalents R1,R2andR3. We want to apply rotation 1 first, fol-\nlowed by rotation 2 and finally rotation 3. The composite rotation matrix Rnet\ncan be found and applied to a vector vas follows:\nRnet=R1R2R3;\nv′=vR1R2R3\n=vRnet.\nLikewise, the composite rotation quaternion qnetcan be found and applied to\nvector v(in its quaternion form, v) as follows:\nqnet=q3q2q1;\nv′=q3q2q1v q 1\n1q 1\n2q 1\n3\n=qnetv q 1\nnet.\nNotice how the quaternion product must be performed in an order opposite\nto that in which the rotations are applied (q 3q2q1). This is because quater-\nnionrotationsalwaysmultiplyon bothsidesofthevector,withtheuninverted\nquaternions on the left and the inverted quaternions on the right. As we saw\nin Equation (5.8), the inverse of a quaternion product is the reverse product of\nthe individual inverses, so the uninverted quaternions read right-to-left while\nthe inverted quaternions read left-to-right.\n5.4. Quaternions 399\n5.4.4 Quaternion-Matrix Equivalence\nWecanconvertany3Drotationfreelybetweena 33matrixrepresentation R\nand a quaternion representation q. If we let q= [qVqS] = [qVxqVyqVzqS]\n=[x y z w]\n, then we can find Ras follows:\nR=2\n41 2y2 2z22xy+2zw 2xz 2yw\n2xy 2zw 1 2x2 2z22yz+2xw\n2xz+2yw 2yz 2xw 1 2x2 2y23\n5.\nLikewise, given R, we can find qas follows (where q[0]=qVx,q[1]\n=qVy,q[2]=qVzandq[3]=qS). This code assumes that we are using\nrow vectors in C/C++ (i.e., that the rows of the matrix correspond to the\nrows of the matrix Rshown above). The code was adapted from a Gamasu-\ntraarticle by Nick Bobic, published on July 5, 1998, which is available here:\nhttp://www.gamasutra.com/view/feature/3278/rotating_objects_using_\nquaternions.php. For a discussion of some even faster methods for convert-\ning a matrix to a quaternion, leveraging various assumptions about the na-\nture of the matrix, see http://www.euclideanspace.com/maths/geometry/\nrotations/conversions/matrixToQuaternion/index.htm.\nvoid matrixToQuaternion(\nconst float R[3][3],\nfloat q[/*4*/])\n{\nfloat trace = R[0][0] + R[1][1] + R[2][2];\n// check the diagonal\nif (trace > 0.0f)\n{\nfloat s = sqrt(trace + 1.0f);\nq[3] = s * 0.5f;\nfloat t = 0.5f / s;\nq[0] = (R[2][1] - R[1][2]) * t;\nq[1] = (R[0][2] - R[2][0]) * t;\nq[2] = (R[1][0] - R[0][1]) * t;\n}\nelse\n{\n// diagonal is negative\nint i = 0;\nif (R[1][1] > R[0][0]) i = 1;\nif (R[2][2] > R[i][i]) i = 2;\nstatic const int NEXT[3] = {1, 2, 0};\n400 5. 3D Math for Games\nint j = NEXT[i];\nint k = NEXT[j];\nfloat s = sqrt((R[i][j]\n- (R[j][j] + R[k][k]))\n+ 1.0f);\nq[i] = s * 0.5f;\nfloat t;\nif (s != 0.0) t = 0.5f / s;\nelse t = s;\nq[3] = (R[k][j] - R[j][k]) * t;\nq[j] = (R[j][i] + R[i][j]) * t;\nq[k] = (R[k][i] + R[i][k]) * t;\n}\n}\nLet’s pause for a moment to consider notational conventions. In this book,\nwewriteourquaternionslikethis: [x y z w ]. Thisdiffersfromthe [w x y z ]\nconventionfoundinmanyacademicpapersonquaternionsasanextensionof\nthe complex numbers. Our convention arises from an effort to be consistent\nwiththecommonpracticeofwritinghomogeneousvectorsas [x y z 1](with\nthew=1at the end). The academic convention arises from the parallels be-\ntween quaternions and complex numbers. Regular two-dimensional complex\nnumbers are typically written in the form c=a+jb, and the corresponding\nquaternion notation is q=w+ix+jy+kz. So be careful out there—make\nsure you know which convention is being used before you dive into a paper\nhead first!\n5.4.5 Rotational Linear Interpolation\nRotational interpolation has many applications in the animation, dynamics\nand camera systems of a game engine. With the help of quaternions, rotations\ncan be easily interpolated just as vectors and points can.\nThe easiest and least computationally intensive approach is to perform a\nfour-dimensional vector LERP on the quaternions you wish to interpolate.\nGiven two quaternions qAandqBrepresenting rotations A and B, we can find\nan intermediate rotation qLERPthat is bpercent of the way from A to B as fol-\n5.4. Quaternions 401\nqA\nqLERP= LERP(q A, qB, 0.4)qB\nFigure 5.23. Linear interpolation (LERP) between two quaternions qA andqB.\nlows:\nqLERP =LERP (qA,qB,b) =(1 b)qA+bqB\nj(1 b)qA+bqBj\n=normalize0\nBBB@2\n664(1 b)qAx+bqBx\n(1 b)qAy+bqBy\n(1 b)qAz+bqBz\n(1 b)qAw+bqBw3\n775T1\nCCCA.\nNotice that the resultant interpolated quaternion had to be renormalized.\nThis is necessary because the LERP operation does not preserve a vector’s\nlength in general.\nGeometrically, qLERP =LERP (qA,qB,b)is the quaternion whose orienta-\ntion lies bpercent of the way fromorientation A to orientation B, as shown (in\ntwo dimensions for clarity) in Figure 5.23. Mathematically, the LERP opera-\ntion results in a weighted average of the two quaternions, with weights (1 b)\nandb(notice that these two weights sum to 1).\n5.4.5.1 Spherical Linear Interpolation\nTheproblemwiththeLERPoperationisthatitdoesnottakeaccountofthefact\nthat quaternions are really points on a four-dimensional hypersphere. A LERP\neffectively interpolates along a chordof the hypersphere, rather than along the\nsurface of the hypersphere itself. This leads to rotation animations that do not\nhaveaconstantangularspeedwhentheparameter bischangingataconstant\nrate. Therotationwillappearslowerattheendpointsandfasterinthemiddle\nof the animation.\nTo solve this problem, we can use a variant of the LERP operation known\nasspherical linear interpolation, or SLERP for short. The SLERP operation uses\nsines and cosines to interpolate along a great circle of the 4D hypersphere,\n402 5. 3D Math for Games\nqA\nqLERP= LERP(q A, qB, 0.4)qB\nqSLER P= SLERP(q A, qB, 0.4)\n0.4 along chord0.4 along arc\nFigure 5.24. Spherical linear interpolation along a great circle arc of a 4D hypersphere.\nrather than along a chord, as shown in Figure 5.24. This results in a constant\nangular speed when bvaries at a constant rate.\nThe formula for SLERP is similar to the LERP formula, but the weights\n(1 b)andbarereplacedwithweights wpandwqinvolvingsinesoftheangle\nbetween the two quaternions.\nSLERP (p,q,b) =wpp+wqq,\nwhere\nwp=sin(1 b)q\nsinq,\nwq=sinbq\nsinq.\nThe cosine of the angle between any two unit-length quaternions can be\nfound by taking their four-dimensional dot product. Once we know cosq, we\ncan calculate the angle qand the various sines we need quite easily:\ncosq=pq=pxqx+pyqy+pzqz+pwqw;\nq=cos 1(pq).\n5.4.5.2 To SLERP or Not to SLERP (That’s Still the Question)\nThejuryisstilloutonwhetherornottouseSLERPinagameengine. Jonathan\nBlow wrote a great article positing that SLERP is too expensive, and LERP’s\nquality is not really that bad—therefore, he suggests, we should understand\nSLERP but avoid it in our game engines (see http://number-none.com/pro\nduct/Understanding%20Slerp,%20Then%20Not%20Using%20It/index.html ).",15886
40-5.5 Comparison of Rotational Representations.pdf,40-5.5 Comparison of Rotational Representations,"5.5. Comparison of Rotational Representations 403\nOn the other hand, some of my colleagues at Naughty Dog have found that\na good SLERP implementation performs nearly as well as LERP. (For exam-\nple, on the PS3’s SPUs, Naughty Dog’s Ice team’s implementation of SLERP\ntakes 20 cycles per joint, while its LERP implementation takes 16.25 cycles per\njoint.) Therefore, I’d personally recommend that you profile your SLERP and\nLERP implementations before making any decisions. If the performance hit\nfor SLERP isn’t unacceptable, I say go for it, because it may result in slightly\nbetter-looking animations. But if your SLERP is slow (and you cannot speed\nit up, or you just don’t have the time to do so), then LERP is usually good\nenough for most purposes.\n5.5 Comparison of Rotational Representations\nWe’veseenthatrotationscanberepresentedinquiteafewdifferentways. This\nsection summarizes the most common rotational representations and outlines\ntheir pros and cons. No one representation is ideal in all situations. Using the\ninformationinthissection,youshouldbeabletoselectthebestrepresentation\nfor a particular application.\n5.5.1 Euler Angles\nWe briefly explored Euler angles in Section 5.3.9.1. A rotation represented via\nEuler angles consists of three scalar values: yaw, pitch and roll. These quanti-\nties are sometimes represented by a 3D vector[\nqYqPqR]\n.\nThe benefits of this representation are its simplicity, its small size (three\nfloating-point numbers) and its intuitive nature—yaw, pitch and roll are easy\nto visualize. You can also easily interpolate simple rotations about a single\naxis. For example, it’s trivial to find intermediate rotations between two dis-\ntinctyawanglesbylinearlyinterpolatingthescalar qY. However,Eulerangles\ncannotbeinterpolatedeasilywhentherotationisaboutanarbitrarilyoriented\naxis.\nIn addition, Euler angles are prone to a condition known as gimbal lock.\nThis occurs when a 90-degree rotation causes one of the three principal axes\nto “collapse” onto another principal axis. For example, if you rotate by 90\ndegrees about the x-axis, the y-axis collapses onto the z-axis. This prevents\nany further rotations about the original y-axis, because rotations about yand\nzhave effectively become equivalent.\nAnother problem with Euler angles is that the order in which the rotations\nare performed around each axis matters. The order could be PYR, YPR, RYP\nand so on, and each ordering may produce a different composite rotation. No\n404 5. 3D Math for Games\none standard rotation order exists for Euler angles across all disciplines (al-\nthough certain disciplines do follow specific conventions). So the rotation an-\ngles[qYqPqR]\ndo not uniquely define a particular rotation—you need to\nknow the rotation order to interpret these numbers properly.\nA final problem with Euler angles is that they depend upon the mapping\nfrom the x-,y- and z-axes onto the natural front,left/right andupdirections for\ntheobjectbeingrotated. Forexample, yawisalwaysdefinedasrotationabout\ntheupaxis, but without additional information we cannot tell whether this\ncorresponds to a rotation about x,yorz.\n5.5.2 3 3 Matrices\nA33matrix is a convenient and effective rotational representation for a\nnumber of reasons. It does not suffer from gimbal lock, and it can repre-\nsent arbitrary rotations uniquely. Rotations can be applied to points and vec-\ntors in a straightforward manner via matrix multiplication (i.e., a series of dot\nproducts). Most CPUs and all GPUs now have built-in support for hardware-\naccelerated dot products and matrix multiplication. Rotations can also be re-\nversed by finding an inverse matrix, which for a pure rotation matrix is the\nsame thing as finding the transpose—a trivial operation. And 44matrices\noffer a way to represent arbitrary affine transformations—rotations, transla-\ntions and scaling—in a totally consistent way.\nHowever, rotation matrices are not particularly intuitive. Looking at a big\ntable of numbers doesn’t help one picture the corresponding transformation\ninthree-dimensionalspace. Also,rotationmatricesarenoteasilyinterpolated.\nFinally,arotationmatrixtakesupalotofstorage(ninefloating-pointnumbers)\nrelative to Euler angles (three floats).\n5.5.3 Axis + Angle\nWe can represent rotations as a unit vector, defining the axis of rotation plus\na scalar for the angle of rotation. This is known as an axis+angle representa-\ntion, and it is sometimes denoted by the four-dimensional vector[aq]=[\naxayazq]\n, where aistheaxisofrotationand qtheangleinradians. Ina\nright-handed coordinate system, the direction of a positive rotation is defined\nbytheright-handrule,whileinaleft-handedsystem,weusetheleft-handrule\ninstead.\nThe benefits of the axis+angle representation are that it is reasonably intu-\nitive and also compact. (It only requires four floating-point numbers, as op-\nposed to the nine required for a 33matrix.)\n5.5. Comparison of Rotational Representations 405\nOne important limitation of the axis+angle representation is that rotations\ncannot be easily interpolated. Also, rotations in this format cannot be ap-\nplied to points and vectors in a straightforward way—one needs to convert\nthe axis+angle representation into a matrix or quaternion first.\n5.5.4 Quaternions\nAswe’veseen,aunit-lengthquaternioncanrepresent3Drotationsinamanner\nanalogous to the axis+angle representation. The primary difference between\nthe two representations is that a quaternion’s axis of rotation is scaled by the\nsine of the half-angle of rotation, and instead of storing the angle in the fourth\ncomponent of the vector, we store the cosine of the half-angle.\nThe quaternion formulation provides two immense benefits over the axis\n+angle representation. First, it permits rotations to be concatenated and ap-\nplied directly to points and vectors via quaternion multiplication. Second, it\npermits rotations to be easily interpolated via simple LERP or SLERP oper-\nations. Its small size (four floating-point numbers) is also a benefit over the\nmatrix formulation.\n5.5.5 SRT Transformations\nByitself,aquaternioncanonlyrepresentarotation,whereasa 44matrixcan\nrepresent an arbitrary affine transformation (rotation, translation and scale).\nWhenaquaternioniscombinedwitha translationvector andascalefactor (either\nascalarforuniformscalingoravectorfornonuniformscaling),thenwehavea\nviable alternative to the 44matrix representation of affine transformations.\nWe sometimes call this an SRT transform , because it contains a scale factor, a\nrotation quaternion and a translation vector. (It’s also sometimes called an\nSQT, because the rotation is a quaternion.)\nSRT =[\nsqt](uniform scale s),\nor\nSRT =[sqt](nonuniform scale vector s).\nSRT transforms are widely used in computer animation because of their\nsmaller size (eight floats for uniform scale, or ten floats for nonuniform scale,\nas opposed to the 12 floating-point numbers needed for a 43matrix) and\ntheir ability to be easily interpolated. The translation vector and scale factor\nare interpolated via LERP, and the quaternion can be interpolated with either\nLERP or SLERP.\n406 5. 3D Math for Games\n5.5.6 Dual Quaternions\nArigid transformation is a transformation involving a rotation and a transla-\ntion—a“corkscrew”motion. Suchtransformationsareprevalentinanimation\nand robotics. A rigid transformation can be represented using a mathematical\nobject known as a dualquaternion . The dual quaternion representation offers a\nnumberofbenefitsoverthetypicalvector-quaternionrepresentation. Thekey\nbenefit is that linear interpolation blending can be performed in a constant-\nspeed, shortest-path, coordinate-invariant manner, similar to using LERP for\ntranslation vectors and SLERP for rotational quaternions (see Section 5.4.5.1),\nbutinawaythatiseasilygeneralizabletoblendsinvolvingthreeormoretrans-\nforms.\nA dual quaternion is like an ordinary quaternion, except that its four com-\nponentsare dualnumbers insteadofregularreal-valuednumbers. Adualnum-\nber can be written as the sum of a non-dual part and a dual part as follows:\nˆa=a+#b. Here #is a magical number called the dual unit , defined in such a\nwaythat #2=0(yetwithout #itselfbeingzero). Thisisanalogoustotheimag-\ninary number j=p 1used when writing a complex number as the sum of a\nreal and an imaginary part: c=a+jb.\nBecause each dual number can be represented by two real numbers (the\nnon-dual and dual parts, aandb), a dual quaternion can be represented by an\neight-element vector. It can also be represented as the sum of two ordinary\nquaternions, where the second one is multiplied by the dual unit, as follows:\nˆq=qa+#qb.\nA full discussion of dual numbers and dual quaternions is beyond our\nscope here. However, the excellent paper entitled, “Dual Quaternions for\nRigid Transformation Blending” by Kavan et al. outlines the theory and prac-\ntice of using dual quaternions to represent rigid transformations—it is avail-\nable online at https://bit.ly/2vjD5sz. Note that in this paper, a dual number\nis written in the form ˆa=a0+#a#, whereas I have used a+#babove to under-\nscore the similarity between dual numbers and complex numbers.1\n5.5.7 Rotations and Degrees of Freedom\nThe term “degrees of freedom” (orDOFfor short) refers to the number of mu-\ntually independent ways in which an object’s physical state (position and ori-\nentation) can change. You may have encountered the phrase “six degrees of\n1Personally I would have preferred the symbol a1over a0, so that a dual number would be\nwritten ˆa= (1)a1+ (#)a#. Just as when we plot a complex number in the complex plane, we can\nthink of the real unit as a “basis vector” along the real axis, and the dual unit #as a “basis vector”\nalong the dual axis.",9823
41-5.6 Other Useful Mathematical Objects.pdf,41-5.6 Other Useful Mathematical Objects,"5.6. Other Useful Mathematical Objects 407\nfreedom” in fields such as mechanics, robotics and aeronautics. This refers to\nthe fact that a three-dimensional object (whose motion is not artificially con-\nstrained) has three degrees of freedom in its translation (along the x-,y- and\nz-axes)andthreedegreesoffreedominitsrotation(aboutthe x-,y-and z-axes),\nfor a total of six degrees of freedom.\nThe DOF concept will help us to understand how different rotational rep-\nresentations can employ different numbers of floating-point parameters, yet\nall specify rotations with only three degrees of freedom. For example, Euler\nangles require three floats, but axis+angle and quaternion representations use\nfour floats, and a 33matrix takes up nine floats. How can these representa-\ntions all describe 3-DOF rotations?\nThe answer lies in constraints. All 3D rotational representations employ\nthree or more floating-point parameters, but some representations also have\noneormoreconstraintsonthoseparameters. Theconstraintsindicatethatthe\nparameters are not independent —a change to one parameter induces changes\nto the other parameters in order to maintain the validity of the constraint(s).\nIf we subtract the number of constraints from the number of floating-point\nparameters, we arrive at the number of degrees of freedom—and this number\nshould always be three for a 3D rotation:\nNDOF =Nparameters Nconstraints . (5.10)\nThe following list shows Equation (5.10) in action for each of the rotational\nrepresentations we’ve encountered in this book.\n•EulerAngles .3parameters 0constraints =3DOF.\n•Axis+Angle .4parameters 1constraint =3DOF.\nConstraint: Axis is constrained to be unit length.\n•Quaternion .4parameters 1constraint =3DOF.\nConstraint: Quaternion is constrained to be unit length.\n•33Matrix.9parameters 6constraints =3DOF.\nConstraints: All three rows and all three columns must be of unit length\n(when treated as three-element vectors).\n5.6 Other Useful Mathematical Objects\nAs game engineers, we will encounter a host of other mathematical objects\nin addition to points, vectors, matrices and quaternions. This section briefly\noutlines the most common of these.\n408 5. 3D Math for Games\nt = 0t = 1t = 2t = 3\nt = –10\nFigure 5.25. Parametric equation of a line.\n0 Figure 5.26. Parametric equation of a ray.\n5.6.1 Lines, Rays and Line Segments\nAn infinite line can be represented by a point P0plus a unit vector uin the\ndirection of the line. A parametric equation of a line traces out every possible\npoint Palongthelinebystartingattheinitialpoint P0andmovinganarbitrary\ndistance talong the direction of the unit vector v. The infinitely large set of\npoints Pbecomes a vectorfunction of the scalar parameter t:\nP(t) =P0+tu,where ¥<t<¥. (5.11)\nThis is depicted in Figure 5.25.\nArayis a line that extends to infinity in only one direction. This is easily\nexpressed as P(t)with the constraint t0, as shown in Figure 5.26.\nAline segment is bounded at both ends by P0andP1. It too can be repre-\nsented by P(t), in either one of the following two ways (where L=P1 P0,\nL=jLjis the length of the line segment, and u= (1/L)Lis a unit vector in\nthe direction of L):\n1.P(t) =P0+tu, where 0tL, or\n2.P(t) =P0+tL, where 0t1.\nThe latter format, depicted in Figure 5.27, is particularly convenient be-\ncause the parameter tis normalized; in other words, talways goes from zero\nto one, no matter which particular line segment we are dealing with. This\nmeans we do not have to store the constraint Lin a separate floating-point pa-\nrameter; it is already encoded in the vector L=Lu(which we have to store\nanyway).\n5.6.2 Spheres\nSpheres are ubiquitous in game engine programming. A sphere is typically\ndefinedasacenterpoint Cplusaradius r,asshowninFigure5.28. Thispacks\nnicely into a four-element vector,[CxCyCzr]\n. As we saw when we dis-\ncussedSIMDvectorprocessing,therearedistinctbenefitstobeingabletopack\ndata into a vector containing four 32-bit floats (i.e., a 128-bit package).\n5.6. Other Useful Mathematical Objects 409\n1 0\n01\nFigure 5.27. Parametric equation of a line segment, with normalized parameter t.\nFigure 5.28. Point-radius representation of a sphere.\n5.6.3 Planes\nAplaneis a 2D surface in 3D space. As you may recall from high-school alge-\nbra, the equation of a plane is often written as follows:\nAx+By+Cz+D=0.\nThis equation is satisfied only for the locus of points P=[x y z]\nthat lie on\nthe plane.\nPlanes can be represented by a point P0and a unit vector nthat is nor-\nmal to the plane. This is sometimes called point-normal form, as depicted in\nFigure 5.29.\nFigure 5.29. A plane in\npoint-normal form.It’s interesting to note that when the parameters A,BandCfrom the tra-\nditional plane equation are interpreted as a 3D vector, that vector lies in the\ndirection of the plane normal. If the vector[A B C]\nis normalized to unit\nlength,thenthenormalizedvector[a b c]=n,andthenormalizedparam-\neterd=D/p\nA2+B2+C2is just the distance from the plane to the origin.\nThe sign of dis positive if the plane’s normal vector nis pointing toward the\norigin (i.e., the origin is on the “front” side of the plane) and negative if the\nnormalispointingawayfromtheorigin(i.e.,theoriginis“behind”theplane).\n410 5. 3D Math for Games\nAnother way of looking at this is that the plane equation and the point-\nnormal form are really just two ways of writing the same equation. Imagine\ntesting whether or not an arbitrary point P=[x y z]\nlies on the plane. To\ndothis,wefindthesigneddistancefrompoint Ptotheoriginalongthenormal\nn=[a b c]\n, and if this signed distance is equal to the signed distance d=\n nP0from the plane from the origin, then Pmust lie on the plane. So let’s\nset them equal and expand some terms:\n(signed distance Pto origin ) = (signed distance plane to origin )\nnP=nP0\nnP nP0=0\nax+by+cz nP0=0\nax+by+cz+d=0. (5.12)\nEquation (5.12) only holds when the point Plies on the plane. But what\nhappens when the point Pdoesnotlie on the plane? In this case, the left-\nhandsideoftheplaneequation( ax+by+cz,whichisequalto nP)tellshow\nfar “off” the point is from being on the plane. This expression calculates the\ndifference between the distance from Pto the origin and the distance from the\nplane to the origin. In other words, the left-hand side of Equation (5.12) gives\nus the perpendicular distance hbetween the point and the plane! This is just\nanother way to write Equation (5.2) from Section 5.2.4.7.\nh= (P P0)n;\nh=ax+by+cz+d. (5.13)\nA plane can actually be packed into a four-element vector, much like a\nsphere can. To do so, we observe that to describe a plane uniquely, we need\nonly the normal vector n=[\na b c]\nand the distance from the origin d. The\nfour-element vector L=[\nnd]=[\na b c d]\nis a compact and convenient\nway to represent and store a plane in memory. Note that when Pis written in\nhomogeneous coordinates with w=1, the equation (LP) = 0is yetanother\nway of writing (nP) = d. These equations are satisfied for all points Pthat\nlie on the plane L.\nPlanesdefinedinfour-elementvectorformcanbeeasilytransformedfrom\nonecoordinatespacetoanother. Givenamatrix MA!Bthattransformspoints\nand (non-normal) vectors from space A to space B, we already know that to\ntransform a normal vector such as the plane’s nvector, we need to use the in-\nverse transpose of that matrix, (M 1\nA!B)T. So it shouldn’t be a big surprise to\nlearn that applying the inverse transpose of a matrix to a four-element plane\nvector Lwill,infact,correctlytransformthatplanefromspaceAtospaceB.We\nwon’t derive or prove this result any further here, but a thorough explanation\nof why this little “trick” works is provided in Section 4.2.3 of [32].\n5.6. Other Useful Mathematical Objects 411\n5.6.4 Axis-Aligned Bounding Boxes (AABB)\nAn axis-aligned bounding box (AABB) is a 3D cuboidwhose six rectangular\nfaces are aligned with a particular coordinate frame’s mutually orthogonal\naxes. Assuch,anAABBcanberepresentedbyasix-elementvectorcontaining\nthe minimum and maximum coordinates along each of the 3 principal axes,\n[xmin,ymin,zmin,xmax,ymax,zmax], or two points PminandPmax.\nThis simple representation allows for a particularly convenient and inex-\npensive method of testing whether a point Pis inside or outside any given\nAABB. We simply test if all of the following conditions are true:\nPxxminandPxxmaxand\nPyyminandPyymaxand\nPzzminandPzzmax.\nBecauseintersectiontestsaresospeedy, AABBsareoftenusedasan“early\nout” collision check; if the AABBs of two objects do not intersect, then there is\nno need to do a more detailed (and more expensive) collision test.\n5.6.5 Oriented Bounding Boxes (OBB)\nAn oriented bounding box (OBB) is a cuboidthat has been oriented so as to\nalign in some logical way with the object it bounds. Usually an OBB aligns\nwith the local-space axes of the object. Hence, it acts like an AABB in local\nspace, although it may not necessarily align with the world-space axes.\nVarious techniques exist for testing whether or not a point lies within an\nOBB, but one common approach is to transform the point into the OBB’s\n“aligned” coordinate system and then use an AABB intersection test as pre-\nsented above.\n5.6.6 Frusta\nRight\nBottom\nFigure 5.30. A frustum.As shown in Figure 5.30, a frustum is a group of six planes that define a trun-\ncated pyramid shape. Frusta are commonplace in 3D rendering because they\nconveniently define the viewable region of the 3D world when rendered via a\nperspective projection from the point of view of a virtual camera. Four of the\nplanes bound the edges of the screen space, while the other two planes repre-\nsent the the near and far clipping planes (i.e., they define the minimum and\nmaximum zcoordinates possible for any visible point).\nOneconvenientrepresentationofafrustumisasanarrayofsixplanes,each\nof which is represented in point-normal form (i.e., one point and one normal\nvector per plane).",10020
42-5.7 Random Number Generation.pdf,42-5.7 Random Number Generation,"412 5. 3D Math for Games\nTestingwhetherapointliesinsideafrustumisabitinvolved,butthebasic\nidea is to use dot products to determine whether the point lies on the front or\nback side of each plane. If it lies inside all six planes, it is inside the frustum.\nAhelpfultrickistotransformtheworld-spacepointbeingtestedbyapply-\ning the camera’s perspective projection to it. This takes the point from world\nspace into a space known as homogeneous clip space. In this space, the frustum\nis just an axis-aligned cuboid (AABB). This permits much simpler in/out tests\nto be performed.\n5.6.7 Convex Polyhedral Regions\nAconvexpolyhedralregion is defined by an arbitrary set of planes, all with nor-\nmals pointing inward (or outward). The test for whether a point lies inside\nor outside the volume defined by the planes is relatively straightforward; it\nis similar to a frustum test, but with possibly more planes. Convex regions\nare very useful for implementing arbitrarily shaped trigger regions in games.\nMany engines employ this technique; for example, the Quake engine’s ubiq-\nuitousbrushesare just volumes bounded by planes in exactly this way.\n5.7 Random Number Generation\nRandom numbers are ubiquitous in game engines, so it behooves us to have a\nbrieflookatthetwomostcommonrandomnumbergenerators(RNG),thelin-\near congruential generator and the Mersenne Twister. It’s important to realize\nthat random number generators don’t actually generate random numbers—\ntheymerelyproduceacomplex,buttotallydeterministic,predefinedsequence\nof values. For this reason, we call the sequences they produce pseudorandom,\nand technically speaking we should really call them “pseudorandom number\ngenerators” (PRNG). What differentiates a good generator from a bad one is\nhow long the sequence of numbers is before it repeats (its period), and how\nwell the sequences hold up under various well-known randomness tests.\n5.7.1 Linear Congruential Generators\nLinear congruential generators are a very fast and simple way to generate a\nsequence of pseudorandom numbers. Depending on the platform, this algo-\nrithm is sometimes used in the C standard library’s rand() function. How-\never, your mileage may vary, so don’t count on rand() being based on any\nparticular algorithm. If you want to be sure, you’ll be better off implementing\nyour own random number generator.\n5.7. Random Number Generation 413\nThelinearcongruentialalgorithmisexplainedindetailinthebook Numer-\nical Recipes in C, so I won’t go into the details of it here.\nWhatIwillsayisthatthisrandomnumbergeneratordoesnotproducepar-\nticularly high-quality pseudorandom sequences. Given the same initial seed\nvalue, the sequence is always exactly the same. The numbers produced do\nnot meet many of the criteria widely accepted as desirable, such as a long pe-\nriod,low-andhigh-orderbitsthathavesimilarlylongperiods,andabsenceof\nsequential or spatial correlation between the generated values.\n5.7.2 Mersenne Twister\nThe Mersenne Twister pseudorandom number generator algorithm was de-\nsignedspecificallytoimproveuponthevariousproblemsofthelinearcongru-\nential algorithm. Wikipedia provides the following description of the benefits\nof the algorithm:\n1. Itwasdesignedtohaveacolossalperiodof 219937 1(thecreatorsofthe\nalgorithm proved this property). In practice, there is little reason to use\nlarger ones, as most applications do not require 219937unique combina-\ntions (2199374.3106001).\n2. It has a very high order of dimensional equidistribution. Note that this\nmeans, bydefault, thatthereisnegligibleserialcorrelationbetweensuc-\ncessive values in the output sequence.\n3. It passes numerous tests for statistical randomness, including the strin-\ngent Diehard tests.\n4. It is fast.\nVarious implementations of the Twister are available on the web, includ-\ning a particularly cool one that uses SIMD vector instructions for an extra\nspeed boost, called SFMT(SIMD-oriented fast Mersenne Twister). SFMT can\nbe downloaded from http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/\nSFMT/index.html.\n5.7.3 Mother-of-All, Xorshift and KISS99\nIn 1994, George Marsaglia, a computer scientist and mathematician best\nknown for developing the Diehard battery of tests of randomness (http://\nwww.stat.fsu.edu/pub/diehard), published a pseudorandom number gener-\nation algorithm that is much simpler to implement and runs faster than the\nMersenne Twister algorithm. He claimed that it could produce a sequence\n414 5. 3D Math for Games\nof 32-bit pseudorandom numbers with a period of non-repetition of 2250. It\npassed all of the Diehard tests and still stands today as one of the best pseu-\ndorandom number generators for high-speed applications. He called his al-\ngorithm the Mother of All Pseudorandom Number Generators , because it seemed\nto him to be the only random number generator one would ever need.\nLater, Marsaglia published another generator called Xorshift, which is be-\ntween Mersenne and Mother-of-All in terms of randomness, but runs slightly\nfaster than Mother.\nMarsaglia also developed a series of random number generators that are\ncollectively called KISS (Keep It Simple Stupid). The KISS99 algorithm is a\npopular choice, because it has a large period ( 2123) and passes all tests in the\nTestU01 test suite (https://bit.ly/2r5FmSP).\nYou can read about George Marsaglia at http://en.wikipedia.org/wiki/\nGeorge_Marsaglia, and about the Mother-of-All generator at ftp://ftp.forth.\norg/pub/C/mother.candathttp://www.agner.org/random. Youcandown-\nload a PDF of George’s paper on Xorshift at http://www.jstatsoft.org/v08/\ni14/paper.\n5.7.4 PCG\nAnotherverypopularandhigh-qualityfamilyofpseudorandomnumbergen-\nerators is called PCG. It works by combining a congruential generator for its\nstate transitions (the “CG” in PCG) with permutation functions to generate its\noutput (the “P” in PCG). You can read more about this family of PRNGs at\nhttp://www.pcg-random.org/.",5980
43-II Low-Level Engine Systems.pdf,43-II Low-Level Engine Systems,Part II\nLow-Level\nEngine Systems\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com,107
44-6 Engine Support Systems.pdf,44-6 Engine Support Systems,,0
45-6.1 Subsystem Start-Up and Shut-Down.pdf,45-6.1 Subsystem Start-Up and Shut-Down,"6\nEngine Support Systems\nEvery game engine requires some low-level support systems that manage\nmundane but crucial tasks, such as starting up and shutting down the\nengine,configuringengineandgamefeatures,managingtheengine’smemory\nusage, handling access to file system(s), providing access to the wide range of\nheterogeneous asset types used by the game (meshes, textures, animations,\naudio, etc.), and providing debugging tools for use by the game development\nteam. This chapter will focus on the lowest-level support systems found in\nmost game engines. In the chapters that follow, we will explore some of the\nlargercoresystems,includingresourcemanagement,humaninterfacedevices\nand in-game debugging tools.\n6.1 Subsystem Start-Up and Shut-Down\nA game engine is a complex piece of software consisting of many interacting\nsubsystems. When the engine first starts up, each subsystem must be config-\nured and initialized in a specific order. Interdependencies between subsys-\ntems implicitly define the order in which they must be started—i.e., if subsys-\ntemBdependsonsubsystemA,thenAwillneedtobestartedupbeforeBcan\nbe initialized. Shut-down typically occurs in the reverse order, so B would\nshut down first, followed by A.\n417\n418 6. Engine Support Systems\n6.1.1 C++ Static Initialization Order (or Lack Thereof)\nSince the programming language used in most modern game engines is C++,\nwe should briefly consider whether C++’s native start-up and shut-down se-\nmanticscanbeleveragedinordertostartupandshutdownourengine’ssub-\nsystems. InC++,globalandstaticobjectsareconstructedbeforetheprogram’s\nentry point (main(), or WinMain() under Windows) is called. However,\nthese constructors are called in a totally unpredictable order. The destructors\nof global and static class instances are called after main() (orWinMain())\nreturns, and once again they are called in an unpredictable order. Clearly this\nbehavior is not desirable for initializing and shutting down the subsystems\nof a game engine, or indeed any software system that has interdependencies\nbetween its global objects.\nThis is somewhat unfortunate, because a common design pattern for im-\nplementing major subsystems such as the ones that make up a game engine is\nto define a singleton class (often called a manager) for each subsystem. If C++\ngave us more control over the order in which global and static class instances\nwere constructed and destroyed, we could define our singleton instances as\nglobals, without the need for dynamic memory allocation. For example, we\ncould write:\nclass RenderManager\n{\npublic:\nRenderManager()\n{\n// start up the manager...\n}\n~RenderManager()\n{\n// shut down the manager...\n}\n// ...\n};\n// singleton instance\nstatic RenderManager gRenderManager;\nAlas, with no way to directly control construction and destruction order, this\napproach won’t work.\n6.1. Subsystem Start-Up and Shut-Down 419\n6.1.1.1 Construct On Demand\nThere is one C++ “trick” we can leverage here. A static variable that is de-\nclared within a function will not be constructed before main() is called,\nbut rather on the first invocation of that function. So if our global single-\nton is function-static, we can control the order of construction for our global\nsingletons.\nclass RenderManager\n{\npublic:\n// Get the one and only instance.\nstatic RenderManager& get()\n{\n// This function-static will be constructed on the\n// first call to this function.\nstatic RenderManager sSingleton ;\nreturn sSingleton;\n}\nRenderManager()\n{\n// Start up other managers we depend on, by\n// calling their get() functions first...\nVideoManager::get() ;\nTextureManager::get() ;\n// Now start up the render manager.\n// ...\n}\n~RenderManager()\n{\n// Shut down the manager.\n// ...\n}\n};\nYou’ll find that many software engineering textbooks suggest this design\nor a variant that involves dynamic allocation of the singleton as shown below.\nstatic RenderManager& get()\n{\nstatic RenderManager* gpSingleton = nullptr;\nif (gpSingleton == nullptr)\n{\ngpSingleton = new RenderManager ;\n420 6. Engine Support Systems\n}\nASSERT(gpSingleton);\nreturn *gpSingleton;\n}\nUnfortunately, this still gives us no way to control destruction order. It\nis possible that C++ will destroy one of the managers upon which the\nRenderManager depends for its shut-down procedure, prior to the\nRenderManager’sdestructorbeingcalled. Inaddition,it’sdifficulttopredict\nexactly when the RenderManager singleton will be constructed, because the\nconstruction will happen on the first call to RenderManager::get() —and\nwho knows when that might be? Moreover, the programmers using the class\nmay not be expecting an innocuous-looking get()function to do something\nexpensive, like allocating and initializing a heavyweight singleton. This is an\nunpredictable and dangerous design. Therefore, we are prompted to resort to\na more direct approach that gives us greater control.\n6.1.2 A Simple Approach That Works\nLet’s presume that we want to stick with the idea of singleton managers for\nour subsystems. In this case, the simplest “brute-force” approach is to define\nexplicit start-up and shut-down functions for each singleton manager class.\nThese functions take the place of the constructor and destructor, and in fact\nwe should arrange for the constructor and destructor to do absolutely nothing .\nThat way, the start-up and shut-down functions can be explicitly called in the\nrequiredorder from within main() (or from some overarching singleton object\nthat manages the engine as a whole). For example:\nclass RenderManager\n{\npublic:\nRenderManager()\n{\n// do nothing\n}\n~RenderManager()\n{\n// do nothing\n}\nvoid startUp()\n{\n// start up the manager...\n6.1. Subsystem Start-Up and Shut-Down 421\n}\nvoid shutDown()\n{\n// shut down the manager...\n}\n// ...\n};\nclass PhysicsManager { /* similar... */ };\nclass AnimationManager { /* similar... */ };\nclass MemoryManager { /* similar... */ };\nclass FileSystemManager { /* similar... */ };\n// ...\nRenderManager gRenderManager;\nPhysicsManager gPhysicsManager;\nAnimationManager gAnimationManager;\nTextureManager gTextureManager;\nVideoManager gVideoManager;\nMemoryManager gMemoryManager;\nFileSystemManager gFileSystemManager;\n// ...\nint main(int argc, const char* argv)\n{\n// Start up engine systems in the correct order .\ngMemoryManager. startUp();\ngFileSystemManager. startUp();\ngVideoManager. startUp();\ngTextureManager. startUp();\ngRenderManager. startUp();\ngAnimationManager. startUp();\ngPhysicsManager. startUp();\n// ...\n// Run the game.\ngSimulationManager. run();\n// Shut everything down, in reverse order .\n// ...\ngPhysicsManager. shutDown();\n422 6. Engine Support Systems\ngAnimationManager. shutDown();\ngRenderManager. shutDown();\ngFileSystemManager. shutDown();\ngMemoryManager. shutDown();\nreturn 0;\n}\nThereare“moreelegant”waystoaccomplishthis. Forexample,youcould\nhave each manager register itself into a global priority queue and then walk\nthisqueuetostartupallthemanagersintheproperorder. Youcoulddefinethe\nmanager-to-manager dependency graph by having each manager explicitly\nlist the other managers upon which it depends and then write some code to\ncalculate the optimal start-up order given their interdependencies. You could\nuse the construct-on-demand approach outlined above. In my experience, the\nbrute-force approach always wins out, because of the following:\n• It’s simple and easy to implement.\n• It’s explicit. You can see and understand the start-up order immediately\nby just looking at the code.\n• It’seasytodebugandmaintain. Ifsomethingisn’tstartingearlyenough,\nor is starting too early, you can just move one line of code.\nOneminordisadvantagetothebrute-forcemanualstart-upandshut-down\nmethod is that you might accidentally shut things down in an order that isn’t\nstrictly the reverse of the start-up order. But I wouldn’t lose any sleep over it.\nAs long as you can start up and shut down your engine’s subsystems success-\nfully, you’re golden.\n6.1.3 Some Examples from Real Engines\nLet’s take a brief look at some examples of engine start-up and shut-down\ntaken from real game engines.\n6.1.3.1 OGRE\nOGRE is by its authors’ admission a rendering engine, not a game engine\nper se. But by necessity it provides many of the low-level features found in\nfull-fledged game engines, including a simple and elegant start-up and shut-\ndown mechanism. Everything in OGRE is controlled by the singleton object\nOgre::Root. ItcontainspointerstoeveryothersubsysteminOGREandman-\nages their creation and destruction. This makes it very easy for a programmer\nto start up OGRE—just newan instance of Ogre::Root and you’re done.\n6.1. Subsystem Start-Up and Shut-Down 423\nHereareafewexcerptsfromtheOGREsourcecodesowecanseewhatit’s\ndoing:\nOgreRoot.h\nclass _OgreExport Root : public Singleton<Root>\n{\n//<some code omitted...>\n// Singletons\nLogManager* mLogManager;\nControllerManager* mControllerManager;\nSceneManagerEnumerator* mSceneManagerEnum;\nSceneManager* mCurrentSceneManager;\nDynLibManager* mDynLibManager;\nArchiveManager* mArchiveManager;\nMaterialManager* mMaterialManager;\nMeshManager* mMeshManager;\nParticleSystemManager* mParticleManager;\nSkeletonManager* mSkeletonManager;\nOverlayElementFactory* mPanelFactory;\nOverlayElementFactory* mBorderPanelFactory;\nOverlayElementFactory* mTextAreaFactory;\nOverlayManager* mOverlayManager;\nFontManager* mFontManager;\nArchiveFactory *mZipArchiveFactory;\nArchiveFactory *mFileSystemArchiveFactory;\nResourceGroupManager* mResourceGroupManager;\nResourceBackgroundQueue* mResourceBackgroundQueue;\nShadowTextureManager* mShadowTextureManager;\n//etc.\n};\nOgreRoot.cpp\nRoot::Root(const String& pluginFileName,\nconst String& configFileName,\nconst String& logFileName) :\nmLogManager(0),\nmCurrentFrame(0),\nmFrameSmoothingTime(0.0f),\nmNextMovableObjectTypeFlag(1),\nmIsInitialised(false)\n{\n// superclass will do singleton checking\nString msg;\n// Init\nmActiveRenderer = 0;\n424 6. Engine Support Systems\nmVersion\n= StringConverter::toString(OGRE_VERSION_MAJOR)\n+ "".""\n+ StringConverter::toString(OGRE_VERSION_MINOR)\n+ "".""\n+ StringConverter::toString(OGRE_VERSION_PATCH)\n+ OGRE_VERSION_SUFFIX + "" ""\n+ ""("" + OGRE_VERSION_NAME + "")"";\nmConfigFileName = configFileName;\n// create log manager and default log file if there\n// is no log manager yet\nif(LogManager::getSingletonPtr() == 0)\n{\nmLogManager = new LogManager();\nmLogManager->createLog(logFileName, true, true);\n}\n// dynamic library manager\nmDynLibManager = new DynLibManager();\nmArchiveManager = new ArchiveManager();\n// ResourceGroupManager\nmResourceGroupManager = new ResourceGroupManager();\n// ResourceBackgroundQueue\nmResourceBackgroundQueue\n= new ResourceBackgroundQueue();\n//and so on...\n}\nOGRE provides a templated Ogre::Singleton base class from which all of\nitssingleton(manager)classesderive. Ifyoulookatitsimplementation,you’ll\nseethat Ogre::Singleton doesnotusedeferredconstructionbutinsteadre-\nlies on Ogre::Root to explicitly neweach singleton. As we discussed above,\nthis is done to ensure that the singletons are created and destroyed in a well-\ndefined order.\n6.1.3.2 Naughty Dog’s Uncharted andThe Last of Us Series\nThe engine created by Naughty Dog, Inc. for its Uncharted andThe Last of Us\nseriesofgamesusesasimilarexplicittechniqueforstartingupitssubsystems.\nYou’ll notice by looking at the following code that engine start-up is not al-\nways a simple sequence of allocating singleton instances. A wide range of\noperating system services, third-party libraries and so on must all be started\n6.1. Subsystem Start-Up and Shut-Down 425\nup during engine initialization. Also, dynamic memory allocation is avoided\nwherever possible, so many of the singletons are statically allocated objects\n(e.g., g_fileSystem, g_languageMgr , etc.) It’s not always pretty, but it\ngets the job done.\nErr BigInit()\n{\ninit_exception_handler();\nU8* pPhysicsHeap = new(kAllocGlobal, kAlign16)\nU8[ALLOCATION_GLOBAL_PHYS_HEAP];\nPhysicsAllocatorInit(pPhysicsHeap,\nALLOCATION_GLOBAL_PHYS_HEAP);\ng_textDb.Init();\ng_textSubDb.Init();\ng_spuMgr.Init();\ng_drawScript.InitPlatform();\nPlatformUpdate();\nthread_t init_thr;\nthread_create(&init_thr, threadInit, 0, 30,\n64*1024, 0, ""Init"");\nchar masterConfigFileName[256];\nsnprintf(masterConfigFileName,\nsizeof(masterConfigFileName),\nMASTER_CFG_PATH);\n{\nErr err = ReadConfigFromFile(\nmasterConfigFileName);\nif (err.Failed())\n{\nMsgErr(""Config file not found (%s).\n"",\nmasterConfigFileName);\n}\n}\nmemset(&g_discInfo, 0, sizeof(BootDiscInfo));\nint err1 = GetBootDiscInfo(&g_discInfo);\nMsg(""GetBootDiscInfo() : 0x%x\n"", err1);\nif(err1 == BOOTDISCINFO_RET_OK)\n{\nprintf(""titleId : [%s]\n"",\ng_discInfo.titleId);",12875
46-6.2 Memory Management.pdf,46-6.2 Memory Management,"426 6. Engine Support Systems\nprintf(""parentalLevel : [%d]\n"",\ng_discInfo.parentalLevel);\n}\ng_fileSystem.Init(g_gameInfo.m_onDisc);\ng_languageMgr.Init();\nif (g_shouldQuit) return Err::kOK;\n//and so on...\n6.2 Memory Management\nAsgamedevelopers,wearealwaystryingtomakeourcoderunmorequickly.\nTheperformanceofanypieceofsoftwareisdictatednotonlybythealgorithms\nit employs, or the efficiency with which those algorithms are coded, but also\nby how the program utilizes memory (RAM). Memory affects performance in\ntwo ways:\n1.Dynamicmemoryallocation viamalloc() orC++’sglobal operator new\nis a very slow operation. We can improve the performance of our code\nby either avoiding dynamic allocation altogether or by making use of\ncustom memory allocators that greatly reduce allocation costs.\n2. On modern CPUs, the performance of a piece of software is often dom-\ninated by its memory access patterns. As we’ll see, data that is located in\nsmall,contiguous blocks of memory can be operated on much more effi-\nciently by the CPU than if that same data were to be spread out across\na wide range of memory addresses. Even the most efficient algorithm,\ncoded with the utmost care, can be brought to its knees if the data upon\nwhich it operates is not laid out efficiently in memory.\nIn this section, we’ll learn how to optimize our code’s memory utilization\nalong these two axes.\n6.2.1 Optimizing Dynamic Memory Allocation\nDynamic memory allocation via malloc() andfree() or C++’s global new\nanddelete operators—also known as heapallocation—is typically very slow.\nThe high cost can be attributed to two main factors. First, a heap allocator\nis a general-purpose facility, so it must be written to handle any allocation\nsize, from one byte to one gigabyte. This requires a lot of management over-\nhead, making the malloc() andfree() functions inherently slow. Second,\n6.2. Memory Management 427\non most operating systems a call to malloc() orfree() must first context-\nswitchfromusermodeintokernelmode,processtherequestandthencontext-\nswitch back to the program. These context switches can be extraordinarily\nexpensive. One rule of thumb often followed in game development is:\nKeep heap allocations to a minimum, and never allocate from the\nheap within a tight loop.\nOfcourse, nogameenginecanentirelyavoiddynamicmemoryallocation,\nso most game engines implement one or more custom allocators. A custom\nallocator can have better performance characteristics than the operating sys-\ntem’s heap allocator for two reasons. First, a custom allocator can satisfy re-\nquests from a preallocated memory block (itself allocated using malloc() or\nnew, or declared as a global variable). This allows it to run in user mode and\nentirelyavoid thecostofcontext-switchingintotheoperatingsystem. Second,\nby making various assumptions about its usage patterns, a custom allocator\ncan be much more efficient than a general-purpose heap allocator.\nIn the following sections, we’ll take a look at some common kinds of\ncustom allocators. For additional information on this topic, see Christian\nGyrling’sexcellentblogpost,http://www.swedishcoding.com/2008/08/31/\nare-we-out-of-memory.\n6.2.1.1 Stack-Based Allocators\nMany games allocate memory in a stack-like fashion. Whenever a new game\nlevel is loaded, memory is allocated for it. Once the level has been loaded,\nlittle or no dynamic memory allocation takes place. At the conclusion of\nthe level, its data is unloaded and all of its memory can be freed. It makes\na lot of sense to use a stack-like data structure for these kinds of memory\nallocations.\nAstackallocator is very easy to implement. We simply allocate a large con-\ntiguous block of memory using malloc() or global new, or by declaring a\nglobal array of bytes (in which case the memory is effectively allocated out of\nthe executable’s BSS segment). A pointer to the top of the stack is maintained.\nAll memory addresses below this pointer are considered to be in use, and all\naddresses above it are considered to be free. The top pointer is initialized to\nthelowestmemoryaddressinthestack. Eachallocationrequestsimplymoves\nthe pointer up by the requested number of bytes. The most recently allocated\nblock can be freed by simply moving the top pointer back down by the size of\nthe block.\n428 6. Engine Support Systems\nObtain marker after a llocating blocks A and B.\nA B\nAllocate additional blocks C, D and E.\nA B C D E\nFree back to marker.\nA B\nFigure 6.1. Stack allocation and freeing back to a marker.\nIt is important to realize that with a stack allocator, memory cannot be\nfreed in an arbitrary order. All frees must be performed in an order oppo-\nsite to that in which they were allocated. One simple way to enforce these\nrestrictions is to disallow individual blocks from being freed at all. Instead,\nwe can provide a function that rolls the stack top back to a previously marked\nlocation, thereby freeing all blocks between the current top and the roll-back\npoint.\nIt’s important to always roll the top pointer back to a point that lies at the\nboundary between two allocated blocks, because otherwise new allocations\nwould overwrite the tail end of the top-most block. To ensure that this is done\nproperly, a stack allocator often provides a function that returns a markerrep-\nresenting the current top of the stack. The roll-back function then takes one of\nthese markers as its argument. This is depicted in Figure 6.1. The interface of\na stack allocator often looks something like this.\nclass StackAllocator\n{\npublic:\n// Stack marker: Represents the current top of the\n// stack. You can only roll back to a marker, not to\n// arbitrary locations within the stack.\ntypedef U32 Marker;\n6.2. Memory Management 429\n// Constructs a stack allocator with the given total\n// size.\nexplicit StackAllocator (U32 stackSize_bytes);\n// Allocates a new block of the given size from stack\n// top.\nvoid* alloc(U32 size_bytes);\n// Returns a marker to the current stack top.\nMarker getMarker();\n// Rolls the stack back to a previous marker.\nvoid freeToMarker (Marker marker);\n// Clears the entire stack (rolls the stack back to\n// zero).\nvoid clear();\nprivate:\n// ...\n};\nDouble-Ended Stack Allocators\nA single memory block can actually contain two stack allocators—one that\nallocates up from the bottom of the block and one that allocates down from\nthe top of the block. A double-ended stack allocator is useful because it uses\nmemorymoreefficientlybyallowingatrade-offtooccurbetweenthememory\nusage of the bottom stack and the memory usage of the top stack. In some\nsituations,bothstacksmayuseroughlythesameamountofmemoryandmeet\nin the middle of the block. In other situations, one of the two stacks may eat\nup a lot more memory than the other stack, but all allocation requests can still\nbesatisfiedaslongasthetotalamountofmemoryrequestedisnotlargerthan\nthe block shared by the two stacks. This is depicted in Figure 6.2.\nInMidway’s HydroThunder arcadegame, allmemoryallocationsaremade\nfrom a single large block of memory managed by a double-ended stack allo-\ncator. The bottom stack is used for loading and unloading levels (race tracks),\nwhilethetopstackisusedfortemporarymemoryblocksthatareallocatedand\nfreedeveryframe. Thisallocationschemeworkedextremelywellandensured\nthatHydroThunder never suffered from memory fragmentation problems (see\nSection 6.2.1.4). Steve Ranck, Hydro Thunder’ s lead engineer, describes this al-\nlocation technique in depth in [8, Section 1.9].\n430 6. Engine Support Systems\nLower Upper\nFigure 6.2. A double-ended stack allocator.\n6.2.1.2 Pool Allocators\nIt’squitecommoningameengineprogramming(andsoftwareengineeringin\ngeneral) to allocate lots of small blocks of memory, each of which are the same\nsize. For example, we might want to allocate and free matrices, or iterators, or\nlinks in a linked list, or renderable mesh instances. For this type of memory\nallocation pattern, a poolallocator is often the perfect choice.\nA pool allocator works by preallocating a large block of memory whose\nsize is an exact multiple of the size of the elements that will be allocated. For\nexample, a pool of 44matrices would be an exact multiple of 64 bytes—\nthat’s 16 elements per matrix times four bytes per element (assuming each\nelement is a 32-bit float). Each element within the pool is added to a linked\nlistoffreeelements;whenthepoolisfirstinitialized,thefreelistcontainsallof\ntheelements. Wheneveranallocationrequestismade,wesimplygrabthenext\nfreeelementoffthefreelistandreturnit. Whenanelementisfreed,wesimply\ntack it back onto the free list. Both allocations and frees are O(1)operations,\nsince each involves only a couple of pointer manipulations, no matter how\nmany elements are currently free. (The notation O(1)is an example of “big\nO” notation. In this case it means that the execution times of both allocations\nand frees are roughly constant and do not depend on things like the number\nof elements currently in the pool. See Section 6.3.3 for an explanation of “big\nO” notation.)\nThelinkedlistoffreeelementscanbeasingly-linkedlist, meaningthatwe\nneed a single pointer (four bytes on 32-bit machines or eight bytes on 64-bit\nmachines) for each free element. Where should we obtain the memory for all\nthesepointers? Certainlytheycouldbestoredinaseparatepreallocatedmem-\nory block, occupying (sizeof(void*) * numElementsInPool) bytes.\nHowever,thisisundulywasteful. Thekeyistorealizethatthememoryblocks\nresidingonthefreelistare,bydefinition,freememoryblocks. Sowhynotstore\neach free list “next” pointer within the free block itself ? This little “trick” works\nas long as elementSize >= sizeof(void*). We don’t waste any mem-\nory, because our free list pointers all reside inside the free memory blocks—in\nmemory that wasn’t being used for anything anyway!\nIf each element is smaller than a pointer, then we can use pool element in-\n6.2. Memory Management 431\ndicesinsteadofpointerstoimplementourlinkedlist. Forexample,ifourpool\ncontains 16-bit integers, then we can use 16-bit indices as the “next pointers”\nin our linked list. This works as long as the pool doesn’t contain more than\n216=65,536elements.\n6.2.1.3 Aligned Allocations\nAs we saw in Section 3.3.7.1, every variable and data object has an alignment\nrequirement. An 8-bit integer variable can be aligned to any address, but a\n32-bit integer or floating-point variable must be 4-byte aligned, meaning its\naddresscanonlyendinthenibbles0x0,0x4,0x8or0xC.A128-bitSIMDvector\nvaluegenerallyhasa16-bytealignmentrequirement,meaningthatitsmemory\naddress can end only in the nibble 0x0. On the PS3, memory blocks that are\nto be transferred to an SPU via the direct memory access (DMA) controller\nshouldbe128-byte alignedformaximumDMAthroughput,meaningtheycan\nonly end in the bytes 0x00 or 0x80.\nAll memory allocators must be capable of returning aligned memory\nblocks. This is relatively straightforward to implement. We simply allocate\na little bit more memory than was actually requested, shift the address of the\nmemory block upward slightly so that it is aligned properly, and then return\nthe shifted address. Because we allocated a bit more memory than was re-\nquested, the returned block will still be large enough, even with the slight\nupward shift.\nInmostimplementations,thenumberofadditionalbytesallocatedisequal\nto the alignment minus one, which is the worst-case alignment shift we can\nmake. For example, if we want a 16-byte aligned memory block, the worst\ncase would be to get back an unaligned pointer that ends in 0x1, because it\nwould require us to apply a shift of 15 bytes to bring it to a 16-byte boundary.\nHere’s one possible implementation of an aligned memory allocator:\n// Shift the given address upwards if/as necessary to\n// ensure it is aligned to the given number of bytes.\ninline uintptr_t AlignAddress (uintptr_t addr, size_t align)\n{\nconst size_t mask = align - 1;\nassert((align & mask) == 0); // pwr of 2\nreturn (addr + mask) & ~mask;\n}\n// Shift the given pointer upwards if/as necessary to\n// ensure it is aligned to the given number of bytes.\ntemplate<typename T>\ninline T* AlignPointer (T* ptr, size_t align)\n432 6. Engine Support Systems\n{\nconst uintptr_t addr = reinterpret_cast<uintptr_t>(ptr);\nconst uintptr_t addrAligned = AlignAddress (addr, align);\nreturn reinterpret_cast<T*>(addrAligned);\n}\n// Aligned allocation function. IMPORTANT: 'align'\n// must be a power of 2 (typically 4, 8 or 16).\nvoid* AllocAligned (size_t bytes, size_t align)\n{\n// Determine worst case number of bytes we'll need.\nsize_t worstCaseBytes = bytes + align - 1 ;\n// Allocate unaligned block.\nU8* pRawMem = new U8[worstCaseBytes ];\n// Align the block.\nreturn AlignPointer (pRawMem, align);\n}\nThe alignment “magic” is performed by the function AlignAddress().\nHere’s how it works: Given an address and a desired alignment L, we can\nalign that address to an L-byte boundary by first adding L 1to it, and\nthen stripping off the Nleast-significant bits of the resulting address, where\nN=log2(L). For example, to align any address to a 16-byte boundary, we\nshiftitupby15bytesandthenmaskoffthe N=log2(16) = 4least-significant\nbits.\nTostripoffthesebits,weneeda maskthatwecanapplytotheaddressusing\nthe bitwise AND operator. Because Lis always a power of two, L 1will be\na mask with binary 1s in the Nleast-significant bits and binary 0s everywhere\nelse. Soallweneedtodois invertthismaskandthenANDitwiththeaddress\n(addr & ~mask ).\nFreeing Aligned Blocks\nWhenan aligned block is later freed, we will be passed the shiftedaddress,\nnot the original address that we allocated. But in order to free the memory,\nwe need to free the address that was actually returned by new. How can we\nconvert an aligned address back into its original, unaligned address?\nOne simple approach is to store the shift (i.e., the difference between the\naligned address and the original address) some place where our free function\nwillbeabletofindit. Recallthatweactuallyallocate align - 1 extrabytesin\nAllocAligned(), in order to give us some room to align the pointer. Those\n6.2. Memory Management 433\nextra bytes are a perfect place to store the shift value. The smallest shift we’ll\never make is one byte, so that’s the minimum space we’ll have to store the\noffset. Therefore, given an aligned pointer p, we can simply store the shift as\na one-byte value at address p - 1.\nHowever, ther e’s one problem: It’s possible that the raw address returned\nbynewwill already be aligned. In that case, the code we presented above\nwould not shift the raw address at all, meaning there’d be no extra bytes into\nwhich to store the offset. To overcome this, we simply allocate Lextra bytes,\ninsteadof L 1,andthenwe alwaysshifttherawpointeruptothenext L-byte\nboundary, even if it was already aligned. Now the maximum shift will be L\nbytes, and the minimum shift will be 1 byte. So there will always be at least\none extra byte into which we can store our shift value.\nStoring the shift in a single byte works for alignments up to and including\n128. We never ever shift a pointer by zero bytes, so we can make this scheme\nwork for up to 256-byte alignment by interpreting the impossible shift value\nof zero as a 256-byte shift. (For larger alignments, we’d have to allocate even\nmore bytes, and shift the pointer even further up, to make room for a wider\n“header”.)\nHere’s how the modified AllocAligned() function and its correspond-\ningFreeAligned() function could be implemented. The process of allocat-\ning and freeing aligned blocks is illustrated in Figure 6.3.\n// Aligned allocation function. IMPORTANT: 'align'\n// must be a power of 2 (typically 4, 8 or 16).\nvoid* AllocAligned (size_t bytes, size_t align)\n{\n// Allocate 'align' more bytes than we need.\nsize_t actualBytes = bytes + align ;\n// Allocate unaligned block.\nU8* pRawMem = new U8[actualBytes];\n// Align the block. If no alignment occurred,\n// shift it up the full 'align' bytes so we\n// always have room to store the shift.\nU8* pAlignedMem = AlignPointer (pRawMem, align);\nif (pAlignedMem == pRawMem)\npAlignedMem += align;\n// Determine the shift, and store it.\n// (This works for up to 256-byte alignment.)\nptrdiff_t shift = pAlignedMem - pRawMem;\nassert(shift > 0 && shift <= 256);\n434 6. Engine Support Systems\npAlignedMem [-1] = static_cast<U8>(shift & 0xFF);\nreturn pAlignedMem;\n}\nvoid FreeAligned (void* pMem)\n{\nif (pMem)\n{\n// Convert to U8 pointer.\nU8* pAlignedMem = reinterpret_cast<U8*>(pMem);\n// Extract the shift.\nptrdiff_t shift = pAlignedMem [-1];\nif (shift == 0)\nshift = 256;\n// Back up to the actual allocated address,\n// and array-delete it.\nU8* pRawMem = pAlignedMem - shift;\ndelete[] pRawMem;\n}\n}\nFigure 6.3. Aligned memory allocation with a 16-byte alignment requirement. The difference be-\ntween the allocated memory address and the adjusted (aligned) address is stored in the byte im-\nmediately preceding the adjusted address, so that it may be retrieved during free.\n6.2.1.4 Single-Frame and Double-Buffered Memory Allocators\nVirtually all game engines allocate at least some temporary data during the\ngame loop. This data is either discarded at the end of each iteration of the\nlooporusedonthenextframeandthendiscarded. Thisallocationpatternisso\ncommonthatmanyenginessupport single-frame anddouble-buffered allocators.\n6.2. Memory Management 435\nSingle-Frame Allocators\nA single-frame allocator is implemented by reserving a block of memory and\nmanagingitwithasimplestackallocatorasdescribedabove. Atthebeginning\nofeachframe,thestack’s“top”pointerisclearedtothebottomofthememory\nblock. Allocations made during the frame grow toward the top of the block.\nRinse and repeat.\nStackAllocator g_singleFrameAllocator;\n// Main Game Loop\nwhile (true)\n{\n// Clear the single-frame allocator's buffer every\n// frame.\ng_singleFrameAllocator. clear ();\n// ...\n// Allocate from the single-frame buffer. We never\n// need to free this data! Just be sure to use it\n// only this frame .\nvoid* p = g_singleFrameAllocator. alloc(nBytes);\n// ...\n}\nOne of the primary benefits of a single-frame allocator is that allocated\nmemory needn’t ever be freed—we can rely on the fact that the allocator will\nbe cleared at the start of every frame. Single-frame allocators are also blind-\ningly fast. The one big negative is that using a single-frame allocator requires\na reasonable level of discipline on the part of the programmer. You need to\nrealize that a memory block allocated out of the single-frame buffer will only\nbe valid during the current frame. Programmers must nevercache a pointer to\na single-frame memory block across the frame boundary!\nDouble-Buffered Allocators\nA double-buffered allocator allows a block of memory allocated on frame ito\nbeusedonframe (i+1). Toaccomplishthis, wecreatetwosingle-framestack\nallocators of equal size and then ping-pong between them every frame.\nclass DoubleBufferedAllocator\n{\nU32 m_curStack;\n436 6. Engine Support Systems\nStackAllocator m_stack[2];\npublic:\nvoid swapBuffers()\n{\nm_curStack = (U32)!m_curStack;\n}\nvoid clearCurrentBuffer ()\n{\nm_stack[m_curStack]. clear();\n}\nvoid* alloc(U32 nBytes)\n{\nreturn m_stack[m_curStack]. alloc (nBytes);\n}\n// ...\n};\n// ...\nDoubleBufferedAllocator g_doubleBufAllocator;\n// Main Game Loop\nwhile (true)\n{\n// Clear the single-frame allocator every frame as\n// before.\ng_singleFrameAllocator.clear();\n// Swap the active and inactive buffers of the double-\n// buffered allocator.\ng_doubleBufAllocator. swapBuffers();\n// Now clear the newly active buffer, leaving last\n// frame's buffer intact.\ng_doubleBufAllocator. clearCurrentBuffer();\n// ...\n// Allocate out of the current buffer, without\n// disturbing last frame's data. Only use this data\n//this frame ornext frame . Again, this memory never\n6.2. Memory Management 437\n// needs to be freed.\nvoid* p = g_doubleBufAllocator. alloc(nBytes);\n// ...\n}\nThis kind of allocator is extremely useful for caching the results of asyn-\nchronousprocessingonamulticoregameconsoleliketheXbox360,XboxOne,\nPlayStation 3 or PlayStation 4. On frame i, we can kick off an asynchronous\njob on one of the PS4’s cores, for example, handing it the address of a destina-\ntionbufferthathasbeenallocatedfromourdouble-bufferedallocator. Thejob\nrunsandproducesitsresultssometimebeforetheendofframe i,storingthem\ninto the buffer we provided. On frame (i+1), the buffers are swapped. The\nresultsofthejobarenow intheinactivebuffer, sotheywillnotbe overwritten\nby any double-buffered allocations that might be made during this frame. As\nlong as we use the results of the job before frame (i+2), our data won’t be\noverwritten.\n6.2.2 Memory Fragmentation\nAnother problem with dynamic heap allocations is that memory can become\nfragmented over time. When a program first runs, its heap memory is entirely\nfree. When a block is allocated, a contiguous region of heap memory of the\nappropriatesizeismarkedas“inuse,” andtheremainderoftheheapremains\nfree. When a block is freed, it is marked as such, and adjacent free blocks are\nmerged into a single, larger free block. Over time, as allocations and deallo-\ncations of various sizes occur in random order, the heap memory begins to\nlooklikeapatchworkoffreeandusedblocks. Wecanthinkofthefreeregions\nas “holes” in the fabric of used memory. When the number of holes becomes\nlarge,and/ortheholesareallrelativelysmall,wesaythememoryhasbecome\nfragmented. This is illustrated in Figure 6.4.\nThe problem with memory fragmentation is that allocations may fail even\nwhen there are enough free bytes to satisfy the request. The crux of the prob-\nlem is that allocated memory blocks must always be contiguous. For example,\nin order to satisfy a request of 128 KiB, there must exist a free “hole” that is\n128 KiB or larger. If there are two holes, each of which is 64 KiB in size, then\nenoughbytesareavailablebuttheallocationfailsbecausetheyarenot contigu-\nous bytes.\nMemory fragmentation is not as much of a problem on operating sys-\ntems that support virtualmemory. A virtual memory system maps discontigu-\nous blocks of physical memory known as pagesinto avirtual address space, in\n438 6. Engine Support Systems\nfree\nAfter one allocation...\nAfter eight allocations...\nAfter eight allocations and three frees...\nAftern allocati ons and m frees...free used\nFigure 6.4. Memory fragmentation.\nwhich the pages appear to the application to be contiguous. Stale pages can\nbe swapped to the hard disk when physical memory is in short supply and\nreloaded from disk when they are needed. For a detailed discussion of how\nvirtual memory works, see http://en.wikipedia.org/wiki/Virtual_memory.\nMostembeddedsystemscannotaffordtoimplementavirtualmemorysystem.\nWhilesomemodernconsolesdotechnicallysupportit,mostconsolegameen-\nginesstilldonotmakeuseofvirtualmemoryduetotheinherentperformance\noverhead.\n6.2.2.1 Avoiding Fragmentation with Stack and Pool Allocators\nThe detrimental effects of memory fragmentation can be avoided by using\nstack and/or pool allocators.\n• A stack allocator is impervious to fragmentation because allocations are\nalwayscontiguous,andblocksmustbefreedinanorderoppositetothat\nin which they were allocated. This is illustrated in Figure 6.5.\n6.2. Memory Management 439\nSingle free block, al ways contiguous Allocated blocks, always contiguous\ndeallocationallocation\nFigure 6.5. A stack allocator is free from fragmentation problems.\nAllocated and free blocks all the same size\nFigure 6.6. A pool allocator is not degraded by fragmentation.\n• A pool allocator is also free from fragmentation problems. Pools dobe-\ncomefragmented,butthefragmentationnevercausesprematureout-of-\nmemoryconditionsasitdoesinageneral-purposeheap. Poolallocation\nrequests can never fail due to a lack of a large enough contiguous free\nblock, because all of the blocks are exactly the same size. This is shown\nin Figure 6.6.\n6.2.2.2 Defragmentation and Relocation\nWhendifferentlysizedobjectsarebeingallocatedandfreedinarandomorder,\nneither a stack-based allocator nor a pool-based allocator can be used. In such\ncases, fragmentation can be avoided by periodically defragmenting the heap.\nDefragmentationinvolvescoalescingallofthefree“holes”intheheapbyshift-\ning allocated blocks from higher memory addresses down to lower addresses\n(thereby shifting the holes up to higher addresses). One simple algorithm is\nto search for the first “hole” and then take the allocated block immediately\nabove the hole and shift it down to the start of the hole. This has the effect\nof “bubbling up” the hole to a higher memory address. If this process is re-\npeated, eventually all the allocated blocks will occupy a contiguous region of\nmemoryatthelowendoftheheap’saddressspace,andalltheholeswillhave\nbubbled up into one big hole at the high end of the heap. This is illustrated in\nFigure 6.7.\nTheshiftingofmemoryblocksdescribedaboveisnotparticularlytrickyto\nimplement. What istrickyisaccountingforthefactthatwe’removing allocated\nblocks of memory around. If anyone has a pointerinto one of these allocated\nblocks, then moving the block will invalidate the pointer.\n440 6. Engine Support Systems\nA B C D E\nA B C D E\nA B C D E\nA B C D E\nA B C D E\nFigure 6.7. Defragmentation by shifting allocated blocks to lower addresses.\nThe solution to this problem is to patch any and all pointers into a shifted\nmemoryblocksothattheypointtothecorrectnewaddressaftertheshift. This\nprocedure is known as pointer relocation. Unfortunately, there is no general-\npurposewayto findallthepointersthatpointintoaparticularregionofmem-\nory. So if we are going to support memory defragmentation in our game en-\ngine, programmers must either carefully keep track of all the pointers man-\nually so they can be relocated, or pointers must be abandoned in favor of\nsomething inherently more amenable to relocation, such as smart pointers or\nhandles.\nA smart pointer is a small class that contains a pointer and acts like a\npointer for most intents and purposes. But because a smart pointer is a class,\nit can be coded to handle memory relocation properly. One approach is to\narrange for all smart pointers to add themselves to a global linked list. When-\never a block of memory is shifted within the heap, the linked list of all smart\npointers can be scanned, and each pointer that points into the shifted block of\nmemory can be adjusted appropriately.\nA handle is usually implemented as an index into a non-relocatable table,\nwhich itself contains the pointers. When an allocated block is shifted in mem-\nory, the handle table can be scanned and all relevant pointers found and up-\ndatedautomatically. Becausethehandlesarejustindicesintothepointertable,\ntheirvaluesneverchangenomatterhowthememoryblocksareshifted,sothe\nobjects that use the handles are never affected by memory relocation.\nAnother problem with relocation arises when certain memory blocks can-\nnot be relocated. For example, if you are using a third-party library that does\nnot use smart pointers or handles, it’s possible that any pointers into its data\nstructures will not be relocatable. The best way around this problem is usu-\nally to arrange for the library in question to allocate its memory from a special\nbuffer outside of the relocatable memory area. The other option is to simply",27618
47-6.3 Containers.pdf,47-6.3 Containers,"6.3. Containers 441\naccept that some blocks will not be relocatable. If the number and size of the\nnon-relocatable blocks are both small, a relocation system will still perform\nquite well.\nIt is interesting to note that all of Naughty Dog’s engines have supported\ndefragmentation. Handles are used wherever possible to avoid the need to\nrelocate pointers. However, in some cases raw pointers cannot be avoided.\nThesepointersarecarefullytrackedandrelocatedmanuallywheneveramem-\nory block is shifted due to defragmentation. A few of Naughty Dog’s game\nobject classes are not relocatable for various reasons. However, as mentioned\nabove, this doesn’t pose any practical problems, because the number of such\nobjects is always very small, and their sizes are tiny when compared to the\noverall size of the relocatable memory area.\nAmortizing Defragmentation Costs\nDefragmentationcanbeaslowoperationbecauseitinvolvescopyingmemory\nblocks. However, we needn’t fully defragment the heap all at once. Instead,\nthe cost can be amortized over many frames. We can allow up to Nallocated\nblocks to be shifted each frame, for some small value of Nlike 8 or 16. If\nour game is running at 30 frames per second, then each frame lasts 1/30 of a\nsecond (33 ms). So, the heap can usually be completely defragmented in less\nthanonesecondwithouthavinganynoticeableeffectonthegame’sframerate.\nAs long as allocations and deallocations aren’t happening at a faster rate than\nthe defragmentation shifts, the heap will remain mostly defragmented at all\ntimes.\nThis approach is only valid when the size of each block is relatively small,\nso that the time required to move a single block does not exceed the time al-\nlotted to relocation each frame. If very large blocks need to be relocated, we\ncan often break them up into two or more subblocks, each of which can be re-\nlocated independently. This hasn’t proved to be a problem in Naughty Dog’s\nengine,becauserelocationisonlyusedfordynamicgameobjects,andtheyare\nnever larger than a few kibibytes—and usually much smaller.\n6.3 Containers\nGame programmers employ a wide variety of collection-oriented data struc-\ntures, also known as containers orcollections . The job of a container is always\nthesame—tohouseandmanagezeroormoredataelements; however,thede-\ntails of how they do this vary greatly, and each type of container has its pros\nand cons. Common container data types include, but are certainly not limited\n442 6. Engine Support Systems\nto, the following.\n•Array. An ordered, contiguous collection of elements accessed by in-\ndex. Thelength ofthearrayisusuallystaticallydefinedatcompiletime.\nIt may be multidimensional. C and C++ support these natively (e.g.,\nint a[5] ).\n•Dynamic array. An array whose length can change dynamically at run-\ntime (e.g., the C++ standard library’s std::vector).\n•Linked list. An ordered collection of elements not stored contiguously\nin memory but rather linked to one another via pointers (e.g., the C++\nstandard library’s std::list ).\n•Stack. A container that supports the last-in-first-out (LIFO) model\nfor adding and removing elements, also known as push/pop (e.g.,\nstd::stack ).\n•Queue. A container that supports the first-in-first-out (FIFO) model for\nadding and r emoving elements (e.g., std::queue).\n•Deque. Adouble-endedqueue—supportsefficientinsertionandremoval\nat both ends of the array (e.g., std::deque ).\n•Tree. A container in which elements are grouped hierarchically. Each\nelement (node) has zero or one parent and zero or more children. A tree\nis a special case of a DAG (see below).\n•Binary search tree (BST). A tree in which each node has at most two chil-\ndren, with an order property to keep the nodes sorted by some well-\ndefinedcriteria. Therearevariouskindsofbinarysearchtrees,including\nred-black trees, splay trees, AVL trees, etc.\n•Binaryheap . A binary treethat maintains itself in sorted order, much like\na binary search tree, via two rules: the shape property , which specifies\nthat the tree must be fully filled and that the last row of the tree is filled\nfrom left to right; and the heap property , which states that every node\nis, by some user-defined criterion, “greater than” or “equal to” all of its\nchildren.\n•Priority queue . A container that permits elements to be added in any or-\nder and then removed in an order defined by some property of the ele-\nmentsthemselves(i.e.,their priority). Apriorityqueueistypicallyimple-\nmented as a heap(e.g., std::priority_queue ), but other implemen-\ntations are possible. A priority queue is a bit like a list that stays sorted\nat all times, except that a priority queue only supports retrieval of the\nhighest-priority element, and it is rarely implemented as a list under the\nhood.\n6.3. Containers 443\n•Dictionary . A table of key-value pairs. A value can be “looked up” ef-\nficiently given the corresponding key. A dictionary is also known as a\nmaporhash table , although technically a hash table is just one possible\nimplementation of a dictionary (e.g., std::map, std::hash_map).\n•Set. A container that guarantees that all elements are unique accord-\ning to some criteria. A set acts like a dictionary with only keys, but no\nvalues.\n•Graph. A collection of nodes connected to one another by unidirectional\nor bidirectional pathways in an arbitrary pattern.\n•Directed acyclic graph (DAG). A collection of nodes with unidirectional\n(i.e.,directed) interconnections, with no cycles(i.e., there is no nonempty\npath that starts and ends on the same node).\n6.3.1 Container Operations\nGame engines that make use of container classes inevitably make use of vari-\nous commonplace algorithms as well. Some examples include:\n•Insert. Add a new element to the container. The new element might be\nplaced at the beginning of the list, or the end, or in some other location;\nor the container might not have a notion of ordering at all.\n•Remove. Remove an element from the container; this may require a find\noperation (see below). However, if an iterator is available that refers to\nthedesiredelement,itmaybemoreefficienttoremovetheelementusing\nthe iterator.\n•Sequential access (iteration). Accessing each element of the container in\nsome “natural” predefined order.\n•Randomaccess. Accessingelementsinthecontainerinanarbitraryorder.\n•Find. Search a container for an element that meets a given criterion.\nThere ar e all sorts of variants on the find operation, including finding\nin reverse, finding multiple elements, etc. In addition, different types of\ndata structures and different situations call for different algorithms (see\nhttp://en.wikipedia.org/wiki/Search_algorithm).\n•Sort. Sort the contents of a container according to some given criteria.\nThere are many different sorting algorithms, including bubble sort, se-\nlectionsort,insertionsort,quicksortandsoon. (Seehttp://en.wikipedia.\norg/wiki/Sorting_algorithm for details.)\n444 6. Engine Support Systems\n6.3.2 Iterators\nAn iterator is a little class that “knows” how to efficiently visit the elements in\naparticularkindofcontainer. Itactslikeanarrayindexorpointer—itrefersto\none element in the container at a time, it can be advanced to the next element,\nanditprovidessomesortofmechanismfortestingwhetherornotallelements\ninthecontainerhavebeenvisited. Asanexample,thefirstofthefollowingtwo\ncode snippets iterates over a C-style array using a pointer, while the second\niterates over a linked list using almost identical syntax.\nvoid processArray(int container[], int numElements)\n{\nint* pBegin = &container[0];\nint* pEnd = &container[numElements];\nfor (int* p = pBegin; p != pEnd; p++)\n{\nint element = *p;\n// process element...\n}\n}\nvoid processList(std::list<int>& container)\n{\nstd::list<int>:: iterator pBegin = container.begin();\nstd::list<int>:: iterator pEnd = container.end();\nfor (auto p = pBegin; p != pEnd; ++p)\n{\nint element = *p;\n// process element...\n}\n}\nThe key benefits to using an iterator over attempting to access the con-\ntainer’s elements directly are as follows:\n• Direct access would break the container class’ encapsulation. An itera-\ntor, on the other hand, is typically a friendof the container class, and as\nsuch it can iterate efficiently without exposing any implementation de-\ntailstotheoutsideworld. (Infact,mostgoodcontainerclasseshidetheir\ninternal details and cannot be iterated over without an iterator.)\n• An iterator can simplify the process of iterating. Most iterators act like\narray indices or pointers, so a simple loop can be written in which the\n6.3. Containers 445\niterator is incremented and compared against a terminating condition—\neven when the underlying data structure is arbitrarily complex. For ex-\nample,aniteratorcanmakeanin-orderdepth-firsttreetraversallookno\nmore complex than a simple array iteration.\n6.3.2.1 Preincrement versus Postincrement\nNotice in the processArray() example that we are using C++’s postincre-\nmentoperator, p++, rather than the preincrement operator, ++p. This is a sub-\ntle but sometimes important optimization. The preincrement operator incre-\nments the contents of the variable beforeits (now modified) value is used in\nthe expression. The postincrement operator increments the contents of the\nvariable afterit has been used. This means that writing ++pintroduces a data\ndependency into your code—the CPU must wait for the increment operation\nto be completed before its value can be used in the expression. On a deeply\npipelinedCPU,this introducesa stall. Onthe otherhand, with p++thereisno\ndata dependency. The value of the variable can be used immediately, and the\nincrement operation can happen later or in parallel with its use. Either way,\nno stall is introduced into the pipeline.\nOfcourse,withinthe“update”expressionofa forloop,thereshouldbeno\ndifferencebetweenpre-andpostincrement. Thisisbecauseanygoodcompiler\nwillrecognizethatthe valueofthevariableisn’tusedin update_expr . Butin\ncases where the value isused, postincrement is preferable because it doesn’t\nintroduce a stall in the CPU’s pipeline.\nIt can be wise to make an exception to this little rule of thumb for classes\nwithoverloaded increment operators, as is common practice in iteratorclasses.\nBy definition, the postincrement operator must return an unmodified copyof\nthe object on which it is called. Depending on the size and complexity of\nthe data members of the class, the added cost of copying the iterator can tip\nthe scales toward a preference for preincrement when using such classes in\nperformance-critical loops. (Preincrement isn’t necessarily better than postin-\ncrement in such a simple example as the function processList() shown\nabove, but I’ve implemented it with preincrement to highlight the difference.)\n6.3.3 Algorithmic Complexity\nThechoiceofwhichcontainertypetouseforagivenapplicationdependsupon\nthe performance and memory characteristics of the container being consid-\nered. For each container type, we can determine the theoretical performance\nof common operations such as insertion, removal, find and sort.\nWe usually indicate the amount of time Tthat an operation is expected to\n446 6. Engine Support Systems\ntake as a function of the number of elements nin the container:\nT=f(n).\nRather than try to find the exact function f, we concern ourselves only with\nfinding the overall orderof the function. For example, if the actual theoretical\nfunction were any of the following,\nT=5n2+17,\nT=102n2+50n+12,\nT=1\n2n2,\nwe would, in all cases, simplify the expression down to its most relevant\nterm—inthiscase n2. Toindicatethatweareonlystatingthe orderofthefunc-\ntion, not its exact equation, we use “big O” notation and write\nT=O(n2).\nThe order of an algorithm can usually be determined via an inspection of\nthe pseudocode. If the algorithm’s execution time is not dependent upon the\nnumber of elements in the container at all, we say it is O(1)(i.e., it completes\ninconstanttime). If the algorithm performs a loopover the elements in the con-\ntainer and visits each element once, such as in a linear search of an unsorted\nlist, we say the algorithm is O(n). If two loops are nested, each of which po-\ntentiallyvisitseachnodeonce,thenwesaythealgorithmis O(n2). Ifadivide-\nand-conquer approach is used, as in a binary search (where half of the list is\neliminated at each step), then we would expect that only ⌊log2(n) +1⌋ele-\nments will actually be visited by the algorithm in the worst case, and hence\nwe refer to it as an O(logn)operation. If an algorithm executes a subalgo-\nrithm ntimes, and the subalgorithm is O(logn), then the resulting algorithm\nwould be O(nlogn).\nTo select an appropriate container class, we should look at the operations\nthat we expect to be most common, then select the container whose perfor-\nmance characteristics for those operations are most favorable. The most com-\nmon orders you’ll encounter are listed here from fastest to slowest: O(1),\nO(logn),O(n),O(nlogn),O(n2),O(nk)fork>2.\nWe should also take the memory layout and usage characteristics of\nour containers into account. For example, an array (e.g., int a[5] or\nstd::vector) stores its elements contiguously in memory and requires no\noverhead storage for anything other than the elements themselves. (Note that\n6.3. Containers 447\nadynamic array does require a small fixed overhead.) On the other hand, a\nlinked list (e.g., std::list) wraps each element in a “link” data structure\nthat contains a pointer to the next element and possibly also a pointer to the\nprevious element, for a total of up to 16 bytes of overhead per element on a\n64-bit machine. Also, the elements in a linked list need not be contiguous in\nmemory and often aren’t. A contiguous block of memory is usually much\nmore cache-friendly than a set of disparate memory blocks. Hence, for high-\nspeed algorithms, arrays are usually better than linked lists in terms of cache\nperformance (unless the nodes of the linked list are themselves allocated from\na small, contiguous block of memory). But a linked list is better for situations\nin which speed of inserting and removing elements is of prime importance.\n6.3.4 Building Custom Container Classes\nMany game engines provide their own custom implementations of the com-\nmon container data structures. This practice is especially prevalent in console\ngame engines and games targeted at mobile phone and PDA platforms. The\nreasons for building these classes yourself include:\n•Total control . You control the data structure’s memory requirements, the\nalgorithms used, when and how memory is allocated, etc.\n•Opportunitiesforoptimization . Youcan optimizeyour datastructuresand\nalgorithms to take advantage of hardware features specific to the con-\nsole(s) you are targeting; or fine-tune them for a particular application\nwithin your engine.\n•Customizability. You can provide custom algorithms not prevalent in\nthe C++ standard library or third-party libraries like Boost (for exam-\nple, searching for the nmost relevant elements in a container, instead of\njust the single most relevant).\n•Eliminationofexternaldependencies . Sinceyoubuiltthesoftwareyourself,\nyou are not beholden to any other company or team to maintain it. If\nproblemsarise,theycanbedebuggedandfixedimmediatelyratherthan\nwaitinguntilthenextreleaseofthelibrary(whichmightnotbeuntilafter\nyou have shipped your game!)\n•Control over concurrent data structures . When you write your own con-\ntainer classes, you have full control over the means by which they are\nprotectedagainstconcurrentaccessonamultithreadedormulticoresys-\ntem. Forexample,onthePS4,NaughtyDoguseslightweight“spinlock”\nmutexes for the majority of our concurrent data structures, because they\nwork well with our fiber-based job scheduling system. A third-party\ncontainer library might not have given us this kind of flexibility.\n448 6. Engine Support Systems\nWe cannot cover all possible data structures here, but let’s look at a few\ncommon ways in which game engine programmers tend to tackle containers.\n6.3.4.1 To Build or Not to Build\nWewillnotdiscussthedetailsofhowtoimplementallofthesedatatypesand\nalgorithms here—a plethora of books and online resources are available for\nthat purpose. However, we willconcern ourselves with the question of where\ntoobtainimplementationsofthetypesandalgorithmsthatyouneed. Asgame\nengine designers, we have basically three choices:\n1. Build the needed data structures manually.\n2. Make use of the STL-style containers provided by the C++ standard li-\nbrary.\n3. Rely on a third-party library such as Boost (http://www.boost.org).\nThe C++ standard library and third-party libraries like Boost are attractive\noptions,becausetheyprovidearichandpowerfulsetofcontainerclassescov-\nering pretty much every type of data structure imaginable. In addition, these\npackagesprovideapowerfulsuiteoftemplate-based genericalgorithms —impl-\nementationsofcommonalgorithms,suchasfindinganelementinacontainer,\nwhich can be applied to virtually any type of data object. However, these im-\nplementations may not be appropriate for some kinds of game engines. Let’s\ntake a moment to investigate some of the pros and cons of each approach.\nThe C++ Standard Library\nThe benefits of the C++ standard library’s STL-style container classes include:\n• They offer a rich set of features.\n• Their implementations are robust and fully portable.\nHowever, these container classes also have some drawbacks, including:\n• The header files are cryptic and difficult to understand (although the\ndocumentation is quite good).\n• General-purposecontainerclassesareoften slowerthanadatastructure\nthat has been crafted specifically to solve a particular problem.\n• A generic container may consume more memory than a custom-\ndesigned data structure.\n• The C++ standard library does a lot of dynamic memory allocation, and\nit’s sometimes challenging to control its appetite for memory in a way\nthat is suitable for high-performance, memory-limited console games.\n6.3. Containers 449\n• The templated allocator system provided by the standard C++ library\nisn’t flexible enough to allow these containers to be used with certain\nkinds of memory allocators, such as stack-based allocators (see Section\n6.2.1.1).\nTheMedal of Honor: Pacific Assault engine for the PC made heavy use of\nwhatwasknownatthetimeasthestandardtemplatelibrary(STL).Andwhile\nMOHPA did have its share of frame rate problems, the team was able to work\naround the performance problems caused by STL (primarily by carefully lim-\niting and controlling its use). OGRE, the popular object-oriented rendering\nlibrary that we use for some of the examples in this book, also makes heavy\nuse of STL-style containers. However, at Naughty Dog we prohibit the use\nof STL containers in runtime game code (although we do permit their use in\noffline tools code). Your mileage may vary: Using the STL-style containers\nprovided by the C++ standard library on a game engine project is certainly\nfeasible, but it should be used with care.\nBoost\nThe Boost project was started by members of the C++ Standards Committee\nLibrary Working Group, but it is now an open source project with many con-\ntributors from across the globe. The aim of the project is to produce libraries\nthat extend and work together with the standard C++ library, for both com-\nmercial and non-commercial use. Many of the Boost libraries have already\nbeen included in the C++ standardlibrary as of C++11, and more components\nare included in the Standards Committee’s Library Technical Report (TR2),\nwhichisasteptowardbecomingpartofafutureC++standard. Hereisabrief\nsummary of what Boost brings to the table:\n• Boost provides a lot of useful facilities not available in the C++ standard\nlibrary.\n• Insomecases,Boostprovidesalternativesorwork-aroundsforproblems\nwith the design or implementation of some classes in the C++ standard\nlibrary.\n• Boost does a great job of handling some very complex problems, like\nsmart pointers. (Bear in mind that smart pointers are complex beasts,\nand they can be performance hogs. Handles are usually preferable; see\nSection 16.5 for details.)\n• The Boost libraries’ documentation is usually excellent. Not only does\nthedocumentationexplainwhateachlibrarydoesandhowtouseit,but\n450 6. Engine Support Systems\nin most cases it also provides an excellent in-depth discussion of the de-\nsigndecisions, constraints and requirementsthat went intoconstructing\nthe library. As such, reading the Boost documentation is a great way to\nlearn about the principles of software design.\nIf you are already using the C++ standard library, then Boost can serve as\nanexcellentextensionand/oralternativetomanyofitsfeatures. However, be\naware of the following caveats:\n• MostofthecoreBoostclassesaretemplates,soallthatoneneedsinorder\nto use them is the appropriate set of header files. However, some of the\nBoost libraries build into rather large .lib files and may not be feasible\nfor use in very small-scale game projects.\n• WhiletheworldwideBoostcommunityisanexcellentsupportnetwork,\nthe Boost libraries come with no guarantees. If you encounter a bug, it\nwill ultimately be your team’s responsibility to work around it or fix it.\n• The Boost libraries are distributed under the Boost Software License.\nRead the license information (http://www.boost.org/more/license_\ninfo.html) carefully to be sure it is right for your engine.\nFolly\nFolly is an open source library developed by Andrei Alexandrescu and the\nengineers at Facebook. Its goal is to extend the standard C++ library and the\nBoost library (rather than to compete with these libraries), with an emphasis\non ease of use and the development of high-performance software. You can\nread about it by searching online for the article entitled “Folly: The Facebook\nOpen Source Library” which is hosted at https://www.facebook.com/. The\nlibrary itself is available on GitHub here: https://github.com/facebook/folly.\nLoki\nThereisaratheresotericbranchofC++programmingknownas templatemeta-\nprogramming . The core idea is to use the compiler to do a lot of the work that\nwouldotherwisehavetobedoneatruntimebyexploitingthetemplatefeature\nof C++ and in effect “tricking” the compiler into doing things it wasn’t orig-\ninally designed to do. This can lead to some startlingly powerful and useful\nprogramming tools.\nBy far the most well-known and probably most powerful template meta-\nprogramminglibraryforC++isLoki,alibrarydesignedandwrittenbyAndrei\n6.3. Containers 451\nAlexandrescu (whose home page is at http://www.erdani.org). The library\ncan be obtained from SourceForge at http://loki-lib.sourceforge.net.\nLoki is extremely powerful; it is a fascinating body of code to study and\nlearn from. However, its two big weaknesses are of a practical nature: (a)\nits code can be daunting to read and use, much less truly understand, and (b)\nsomeofitscomponentsaredependentuponexploiting“side-effect”behaviors\nofthecompilerthatrequirecarefulcustomizationinordertobemadetowork\non new compilers. So Loki can be somewhat tough to use, and it is not as\nportable as some of its “less-extreme” counterparts. Loki is not for the faint of\nheart. That said, some of Loki’s concepts such as policy-basedprogramming can\nbe applied to any C++ project, even if you don’t use the Loki library per se. I\nhighlyrecommendthatallsoftwareengineersreadAndrei’sground-breaking\nbook,Modern C++ Design [3], from which the Loki library was born.\n6.3.5 Dynamic Arrays and Chunky Allocation\nFixed size C-style arrays are used quite a lot in game programming, because\nthey require no memory allocation, are contiguous and hence cache-friendly,\nandsupportmanycommonoperationssuchasappendingdataandsearching\nvery efficiently.\nWhenthesizeofanarraycannotbedeterminedapriori,programmerstend\nto turn either to linked lists ordynamic arrays. If we wish to maintain the per-\nformanceandmemorycharacteristicsoffixed-lengtharrays,thenthedynamic\narray is often the data structure of choice.\nThe easiest way to implement a dynamic array is to allocate an n-element\nbuffer initially and then growthe list only if an attempt is made to add more\nthan nelements to it. This gives us the favorable characteristics of a fixed\nsize array but with no upper bound. Growing is implemented by allocating\na new larger buffer, copying the data from the original buffer into the new\nbuffer, and then freeing the original buffer. The size of the buffer is increased\nin some orderly manner, such as adding nto it on each grow, or doubling it\non each grow. Most of the implementations I’ve encountered never shrink the\narray, only grow it (with the notable exception of clearing the array to zero\nsize, which might or might not free the buffer). Hence the size of the array\nbecomes a sort of “high water mark.” The std::vector class works in this\nmanner.\nOfcourse,ifyoucanestablishahighwatermarkforyourdata,thenyou’re\nprobably better off just allocating a single buffer of that size when the engine\nstartsup. Growingadynamicarraycanbeincrediblycostlyduetoreallocation\nanddatacopyingcosts. Theimpactofthesethingsdependsonthesizesofthe\n452 6. Engine Support Systems\nbuffers involved. Growing can also lead to fragmentation when discarded\nbuffers are freed. So, as with all data structures that allocate memory, caution\nmust be exercised when working with dynamic arrays. Dynamic arrays are\nprobably best used during development, when you are as yet unsure of the\nbuffersizesyou’llrequire. Theycanalwaysbeconvertedintofixedsizearrays\nonce suitable memory budgets have been established.\n6.3.6 Dictionaries and Hash Tables\nA dictionary is a table of key-value pairs. A value in the dictionary can be\nlooked up quickly, given its key. The keys and values can be of any data type.\nThis kind of data structure is usually implemented either as a binary search\ntree or as a hash table.\nInabinarytreeimplementation,thekey-valuepairsarestoredinthenodes\nof the binary tree, and the tree is maintained in key-sorted order. Looking up\na value by key involves performing an O(logn)binary search.\nIn a hash table implementation, the values are stored in a fixed size table,\nwhere each slot in the table represents one or more keys. To insert a key-value\npair into a hash table, the key is first converted into integer form via a process\nknown as hashing (if it is not already an integer). Then an indexinto the hash\ntableiscalculatedbytakingthehashedkey modulothesizeofthetable. Finally,\nthe key-value pair is stored in the slot corresponding to that index. Recall that\nthemodulooperator ( %in C/C++) finds the remainder of dividing the integer\nkey by the table size. So if the hash table has five slots, then a key of 3 would\nbe stored at index 3 (3 % 5 == 3), while a key of 6 would be stored at index\n1(6 % 5 == 1 ). Findingakey-valuepairisan O(1)operationintheabsence\nof collisions.\n6.3.6.1 Collisions: Open and Closed Hash Tables\nSometimestwoormorekeysendupoccupyingthesameslotinthehashtable.\nThis is known as a collision. There are two basic ways to resolvea collision,\ngiving rise to two different kinds of hash tables:\n•Open. In an open hash table (see Figure 6.8), collisions are resolved by\nsimplystoringmorethanonekey-valuepairateachindex,usuallyinthe\nform of a linked list. This approach is easy to implement and imposes\nno upper bound on the number of key-value pairs that can be stored.\nHowever,itdoesrequirememorytobeallocateddynamicallywhenever\na new key-value pair is added to the table.\n•Closed. In a closed hash table (see Figure 6.9), collisions are resolved via\na process of probing until a vacant slot is found. (“Probing” means ap-\n6.3. Containers 453\nplying a well-defined algorithm to search for a free slot.) This approach\nis a bit more difficult to implement, and it imposes an upper limit on\nthe number of key-value pairs that can reside in the table (because each\nslot can hold only one key-value pair). But the main benefit of this kind\nof hash table is that it uses up a fixed amount of memory and requires\nno dynamic memory allocation. Therefore, it is often a good choice in a\nconsole engine.\nConfusingly, closed hash tables are sometimes said to use open addressing,\nwhile open hash tables are said to use an addressing method known as chain-\ning, so named due to the linked lists at each slot in the table.\n6.3.6.2 Hashing\nHashing is the process of turning a key of some arbitrary data type into an\ninteger, which can be used modulo the table size as an index into the table.\nMathematically, given a key k, we want to generate an integer hash value h\nusing the hash function Hand then find the index iinto the table as follows:\nh=H(k),\ni=hmod N,\nwhere Nis the number of slots in the table, and the symbol mod represents\nthemodulooperation, i.e., finding the remainder of the quotient h/N.\nIf the keys are unique integers, the hash function can be the identity func-\ntion, H(k) =k. If the keys are unique 32-bit floating-point numbers, a hash\nfunction might simply reinterpret the bit pattern of the 32-bit float as if it were\na 32-bit integer.\nU32 hashFloat(float f)\n{\nSlot 0\nSlot 1\nSlot 2\nSlot 3\nSlot 4(55, apple) (0, orange)\n(26, grape)\n(33, plum)\nFigure 6.8. An open hash table.\n454 6. Engine Support Systems\n(55, apple) (0, orange)collision!\n(33, plum)(55, apple)\n(26, grape)\n(33, plum)\n(0, orange)(26, grape)\nprobe to\nfind new \nslot0\n1\n2\n3\n40\n1\n2\n3\n4\nFigure 6.9. A closed hash table.\nunion\n{\nfloat m_asFloat;\nU32 m_asU32;\n} u;\nu.m_asFloat = f;\nreturn u.m_asU32;\n}\nIf the key is a string, we can employ a stringhashing function, which combines\nthe ASCII or UTF codes of all the characters in the string into a single 32-bit\ninteger value.\nThequalityof the hashing function H(k)is crucial to the efficiency of the\nhashtable. A“good”hashingfunctionisonethatdistributesthesetofallvalid\nkeys evenly across the table, thereby minimizing the likelihood of collisions.\nA hash function must also be reasonably quick to calculate, and deterministic\nin the sense that it must produce the exact same output every time it is called\nwith an indentical input.\nStrings are probably the most prevalent type of key you’ll encounter, so\nit’s particularly helpful to know a “good” string hashing function. Table 6.1\nlists a number of well-known hashing algorithms, their throughput ratings\n(based on benchmark measurements and then converted into a rating of Low,\nMedium or High) and their score on the SMHasher test (https://github.com/\naappleby/smhasher). Please note that the relative throughputs listed in the\ntable are for rough comparison purposes only. Many factors contribute to\nthe throughput of a hash function, including the hardware on which it is run\nand the properties of the input data. Cryptographic hashes are deliberately\nslow, as their focus is on producing a hash that is extremely unlikely to collide\nwith the hashes of other input strings, and for which the task of determining\n6.3. Containers 455\nName Throughput Score Cryptographic?\nxxHash High 10 No\nMurmurHash 3a High 10 No\nSBox Medium 9 No‡\nLookup3 Medium 9 No\nCityHash64 Medium 10 No\nCRC32 Low 9 No\nMD5-32 Low 10 Yes\nSHA1-32 Low 10 Yes\nTable 6.1. Comparison of well-known hashing algorithms in terms of their relative throughput and\ntheir scores on the SMHasher test. ‡Note that SBox is not itself a cryptographic hash, but it is one\ncomponent of the symmetric key algorithms used in cryptography.\na string that would produce a given hash value is extremely computationally\ndifficult.\nFor more information on hash functions, see the excellent article by Paul\nHsieh available at http://www.azillionmonkeys.com/qed/hash.html.\n6.3.6.3 Implementing a Closed Hash Table\nInaclosedhashtable,thekey-valuepairsarestoreddirectlyinthetable,rather\nthan in a linked list at each table entry. This approach allows the programmer\nto define a priori the exact amount of memory that will be used by the hash\ntable. A problem arises when we encounter a collision—two keys that end up\nwanting to be stored in the same slot in the table. To address this, we use a\nprocess known as probing.\nThe simplest approach is linearprobing . Imagine that our hashing function\nhas yielded a table index of i, but that slot is already occupied; we simply try\nslots (i+1),(i+2)and so on until an empty slot is found (wrapping around\nto the start of the table when i=N). Another variation on linear probing is\nto alternate searching forwards and backwards, (i+1),(i 1),(i+2),(i 2)\nand so on, making sure to modulo the resulting indices into the valid range of\nthe table.\nLinear probing tends to cause key-value pairs to “clump up.” To avoid\nthese clusters, we can use an algorithm known as quadratic probing. We start\nat the occupied table index iand use the sequence of probes ij= (ij2)for\nj=1, 2, 3, . . . . In other words, we try ( i+12), (i 12), (i+22), (i 22) and so",33167
48-6.4 Strings.pdf,48-6.4 Strings,"456 6. Engine Support Systems\non, remembering to always modulo the resulting index into the valid range of\nthe table.\nWhen using closed hashing, it is a good idea to make your table size a\nprime number . Using a prime table size in conjunction with quadratic probing\ntends to yield the best coverage of the available table slots with minimal clus-\ntering. See http://stackoverflow.com/questions/1145217/why-should-hash\n-functions-use-a-prime-number-modulus for a good discussion of why prime\nhash table sizes are preferable.\n6.3.6.4 Robin Hood Hashing\nRobin Hood hashing is another probing method for closed hash tables that has\ngainedpopularityrecently. Thisprobingschemeimprovestheperformanceof\naclosedhashtable,evenwhenthetableisnearlyfull. Foragooddiscussionof\nhowRobinHoodhashingworks,seehttps://www.sebastiansylvan.com/post\n/robin-hood-hashing-should-be-your-default-hash-table-implementation/.\n6.4 Strings\nStrings are ubiquitous in almost every software project, and game engines are\nno exception. On the surface, the string may seem like a simple, fundamental\ndata type. But, when you start using strings in your projects, you will quickly\ndiscover a wide range of design issues and constraints, all of which must be\ncarefully accounted for.\n6.4.1 The Problem with Strings\nThemostfundamentalquestionaboutstringsishowtheyshouldbestoredand\nmanagedinyourprogram. InCandC++,stringsaren’tevenanatomictype—\nthey are implemented as arrays of characters. The variable length of strings\nmeans we either have to hard-code limitations on the sizes of our strings, or\nwe need to dynamically allocate our string buffers.\nAnother big string-related problem is that of localization —the process of\nadapting your software for release in other languages. This is also known as\ninternationalization, or I18N for short. Any string that you display to the user\nin English must be translated into whatever languages you plan to support.\n(Strings that are used internally to the program but are never displayed to the\nuser are exempt from localization, of course.) This not only involves making\nsure that you can represent all the character glyphs of all the languages you\nplan to support (via an appropriate set of fonts), but it also means ensuring\nthatyourgamecanhandledifferenttextorientations. Forexample,traditional\n6.4. Strings 457\nChinese text is oriented vertically instead of horizontally (although modern\nChinese and Japanese are commonly written horizontally and left-to-right),\nand some languages like Hebrew read right-to-left. Your game also needs to\ngracefully deal with the possibility that a translated string will be either much\nlonger or much shorter than its English counterpart.\nFinally, it’s important to realize that strings are used internally within a\ngame engine for things like resource file names and object ids. For example,\nwhen a game designer lays out a level, it’s highly convenient to permit him or\nher to identify the objects in the level using meaningful names, like “Player-\nCamera,” “enemy-tank-01” or “explosionTrigger.”\nHowourenginedealswiththeseinternalstringsoftenhaspervasiverami-\nficationsontheperformanceofthegame. Thisisbecausestringsareinherently\nexpensive to work with at runtime. Comparing or copying ints or floats\ncan be accomplished via simple machine language instructions. On the other\nhand, comparing strings requires an O(n)scan of the character arrays using a\nfunction like strcmp() (where nis the length of the string). Copying a string\nrequires an O(n)memory copy, not to mention the possibility of having to\ndynamically allocate the memory for the copy. During one project I worked\non, we profiled our game’s performance only to discover that strcmp() and\nstrcpy() were the top two most expensive functions! By eliminating un-\nnecessary string operations and using some of the techniques outlined in this\nsection, we were able to all but eliminate these functions from our profile and\nincrease the game’s frame rate significantly. (I’ve heard similar stories from\ndevelopers at a number of different studios.)\n6.4.2 String Classes\nMany C++ programmers prefer to use a stringclass, such as the C++ standard\nlibrary’s std::string , rather than deal directly with character arrays. Such\nclassescanmakeworkingwithstringsmuchmoreconvenientfortheprogram-\nmer. However, a string class can have hidden costs that are difficult to see\nuntil the game is profiled. For example, passing a string to a function using a\nC-style character array is fast because the address of the first character is typi-\ncally passed in a hardware register. On the other hand, passing a string object\nmight incur the overhead of one or more copy constructors, if the function is\nnot declared or used properly. Copying strings might involve dynamic mem-\nory allocation, causing what looks like an innocuous function call to end up\ncosting literally thousands of machine cycles.\nBecauseofthemyriadissueswithstringclasses,Igenerallyprefertoavoid\nthem in runtime game code. However, if you feel a strong urge to use a string\n458 6. Engine Support Systems\nclass, make sure you pick or implement one that has acceptable runtime per-\nformancecharacteristics—andbesureallprogrammersthatuseitareawareof\nits costs. Know your string class: Does it treat all string buffers as read-only?\nDoes it utilize the copy on write optimization? (See http://en.wikipedia.org\n/wiki/Copy-on-write.) In C++11, does it provide a move constructor? Does\nit own the memory associated with the string, or can it reference memory that\nit does not own? (See http://www.boost.org/doc/libs/1_57_0/libs/utility\n/doc/html/string_ref.html for more on the issue of memory ownership in\nstring classes.) As a rule of thumb, always pass string objects by reference,\nnever by value (as the latter often incurs string-copying costs). Profile your\ncode early and often to ensure that your string class isn’t becoming a major\nsource of lost frame rate!\nOnesituationinwhichaspecializedstringclassdoesseemjustifiabletome\nis when storing and managing file system paths. Here, a hypothetical Path\nclass could add significant value over a raw C-style character array. For ex-\nample, it might provide functions for extracting the filename, file extension\nor directory from the path. It might hide operating system differences by\nautomatically converting Windows-style backslashes to UNIX-style forward\nslashes or some other operating system’s path separator. Writing a Pathclass\nthatprovidesthiskindoffunctionalityinacross-platformwaycouldbehighly\nvaluablewithinagameenginecontext. (SeeSection7.1.1.4formoredetailson\nthis topic.)\n6.4.3 Unique Identiﬁers\nTheobjectsin any virtual game world need to be uniquely identified in some\nway. For example, in Pac Man we might encounter game objects named\n“pac_man,” “blinky,” “pinky,” “inky” and “clyde.” Unique object identifiers\nallow game designers to keep track of the myriad objects that make up their\ngame worlds and also permit those objects to be found and operated on at\nruntime by the engine. In addition, the assetsfrom which our game objects\nare constructed—meshes, materials, textures, audio clips, animations and so\non—all need unique identifiers as well.\nStrings seem like a natural choice for such identifiers. Assets are often\nstored in individual files on disk, so they can usually be identified uniquely\nby their file paths, which of course are strings. And game objects are created\nbygamedesigners, soitisnaturalforthemtoassigntheirobjectsunderstand-\nable string names, rather than have to remember integer object indices, or 64-\nor128-bitgloballyuniqueidentifiers(GUIDs). However,thespeedwithwhich\ncomparisons between unique identifiers can be made is of paramount impor-\n6.4. Strings 459\ntance in a game, so strcmp() simply doesn’t cut it. We need a way to have\nour cake and eat it too—a way to get all the descriptiveness and flexibility of\na string, but with the speed of an integer.\n6.4.3.1 Hashed String Ids\nOne good solution is to hashour strings. As we’ve seen, a hash function maps\na string onto a semi-unique integer. String hash codes can be compared just\nlike any other integers, so comparisons are fast. If we store the actual strings\nin a hash table, then the original string can always be recovered from the hash\ncode. This is useful for debugging purposes and to permit hashed strings to\nbe displayed on-screen or in log files. Game programmers sometimes use the\ntermstringid torefertosuchahashedstring. TheUnrealengineusestheterm\nnameinstead (implemented by class FName).\nAs with any hashing system, collisions are a possibility (i.e., two different\nstrings might end up with the same hash code). However, with a suitable\nhash function, we can all but guarantee that collisions will not occur for all\nreasonable input strings we might use in our game. After all, a 32-bit hash\ncoderepresentsmorethanfourbillionpossiblevalues. So,ifourhashfunction\ndoesagoodjobofdistributingstringsevenlythroughoutthisverylargerange,\nwe are unlikely to collide. At Naughty Dog, we started out using a variant of\ntheCRC-32algorithmto hashourstrings, and weencounteredonlya handful\nof collisions during many years of development on Uncharted andThe Last of\nUs. And when a collision did occur, fixing it was a simple matter of slightly\naltering one of the strings (e.g., append a “2” or a “b” to one of the strings,\nor use a totally different but synonymous string). That being said, Naughty\nDog has moved to a 64-bit hashing function for TheLastofUsPartII and all of\nour future game titles; this should essentially eliminate the possibility of hash\ncollisions, given the quantity and typical lengths of the strings we use in any\none game.\n6.4.3.2 Some Implementation Ideas\nConceptually, it’s easy enough to run a hash function on your strings in order\nto generate string ids. Practically speaking, however, it’s important to con-\nsiderwhenthe hash will be calculated. Most game engines that use string ids\ndo the hashing at runtime. At Naughty Dog, we permit runtime hashing of\nstrings, but we also use C++11’s user-defined literals feature to transform the\nsyntax ""any_string""_sid directly into a hashed integer value at compile\ntime. This permits string ids to be used anywhere that an integer manifest\nconstant can be used, including the constant caselabels of a switch state-\nment. (The result of a function call that generates a string id at runtime is not\n460 6. Engine Support Systems\na constant, so it cannot be used as a caselabel.)\nThe process of generating a string id from a string is sometimes called\ninterning the string, because in addition to hashing it, the string is typically\nalso added to a global string table. This allows the original string to be re-\ncovered from the hash code later. You may also want your tools to be ca-\npable of hashing strings into string ids. That way, when the tool gener-\nates data for consumption by your engine, the strings will already have been\nhashed.\nThe main problem with interning a string is that it is a slow operation.\nThe hashing function must be run on the string, which can be an expensive\nproposition, especially when a large number of strings are being interned. In\naddition, memory must be allocated for the string, and it must be copied into\nthe lookup table. As a result (if you are not generating string ids at compile-\ntime),itisusuallybesttointerneachstringonlyonceandsaveofftheresultfor\nlater use. For example, it would be preferable to write code like this because\nthe latter implementation causes the strings to be unnecessarily re-interned\nevery time the function f()is called.\nstatic StringId sid_foo =internString(""foo"");\nstatic StringId sid_bar =internString(""bar"");\n// ...\nvoid f(StringId id)\n{\nif (id == sid_foo)\n{\n// handle case of id == ""foo""\n}\nelse if (id == sid_bar)\n{\n// handle case of id == ""bar""\n}\n}\nThe following approach is less efficient:\nvoid f(StringId id)\n{\nif (id == internString(""foo"") )\n{\n// handle case of id == ""foo""\n}\n6.4. Strings 461\nelse if (id == internString(""bar""))\n{\n// handle case of id == ""bar""\n}\n}\nHere’s one possible implementation of internString().\nstringid.h\ntypedef U32 StringId;\nextern StringId internString (const char* str);\nstringid.cpp\nstatic HashTable<StringId, const char*> gStringIdTable;\nStringId internString (const char* str)\n{\nStringId sid = hashCrc32(str);\nHashTable<StringId, const char*>::iterator it\n= gStringIdTable. find(sid);\nif (it == gStringTable.end())\n{\n// This string has not yet been added to the\n// table. Add it, being sure to copy it in case\n// the original was dynamically allocated and\n// might later be freed.\ngStringTable[sid] = strdup(str);\n}\nreturn sid;\n}\nAnother idea employed by the Unreal Engine is to wrap the string id\nand a pointer to the corresponding C-style character array in a tiny class.\nIn the Unreal Engine, this class is called FName. At Naughty Dog we\ndo the same and wrap our string ids in a StringId class. We define\na macro so that SID(""any_string"") produces an instance of this class,\nwith its hashed value produced by our user-defined string literal syntax\n""any_string""_sid.\n462 6. Engine Support Systems\nUsing Debug Memory for Strings\nWhenusingstringids, thestringsthemselvesareonlykeptaroundforhuman\nconsumption. Whenyoushipyourgame,youalmostcertainlywon’tneedthe\nstrings—the game itself should only ever use the ids. As such, it’s a good idea\nto store your string table in a region of memory that won’t exist in the retail\ngame. Forexample, aPS3developmentkithas256MiBofretailmemory, plus\nanadditional256MiBof“debug”memorythatisnotavailableonaretailunit.\nIfwestoreourstringsindebugmemory, weneedn’tworryabouttheirimpact\nonthememoryfootprintofthefinalshippinggame. (Wejustneedtobecareful\nnever to write production code that depends on the strings being available!)\n6.4.4 Localization\nLocalization of a game (or any software project) is a big undertaking. It is a\ntaskbesthandledbyplanningforitfromdayoneandaccountingforitatevery\nstep of development. However, this is not done as often as we all would like.\nHere are some tips that should help you plan your game engine project for\nlocalization. For an in-depth treatment of software localization, see [34].\n6.4.4.1 Unicode\nThe problem for most English-speaking software developers is that they are\ntrained from birth (or thereabouts!) to think of strings as arrays of eight-bit\nASCII character codes (i.e., characters following the ANSI standard). ANSI\nstrings work great for a language with a simple alphabet, like English. But,\nthey just don’t cut it for languages with complex alphabets containing a great\nmany more characters, sometimes totally different glyphs than English’s 26\nletters. ToaddressthelimitationsoftheANSIstandard,theUnicodecharacter\nset system was devised.\nThe basic idea behind Unicode is to assign every character or glyph from\neverylanguageincommonusearoundtheglobetoauniquehexadecimalcode\nknown as a code point. When storing a string of characters in memory, we se-\nlect a particular encoding —a specific means of representing the Unicode code\npointsforeachcharacter—andfollowingthoserules, welaydownasequence\nof bits in memory that represent the string. UTF-8 and UTF-16 are two com-\nmon encodings. You should select the specific encoding standard that best\nsuits your needs.\nPlease set down this book right now and read the article entitled, “The\nAbsolute Minimum Every Software Developer Absolutely, Positively Must\nKnow About Unicode and Character Sets (No Excuses!)” by Joel Spolsky.\nYoucanfindithere: http://www.joelonsoftware.com/articles/Unicode.html.\n(Once you’ve done that, please pick up the book again!)\n6.4. Strings 463\nUTF-32\nThe simplest Unicode encoding is UTF-32. In this encoding, each Unicode\ncode point is encoded into a 32-bit (4-byte) value. This encoding wastes a lot\nof space, for two reasons: First, most strings in Western European languages\ndo not use any of the highest-valued code points, so an average of at least\n16 bits (2 bytes) is usually wasted per character. Second, the highest Unicode\ncode point is 0x10FFFF, so even if we wanted to create a string that uses every\npossible Unicode glyph, we’d still only need 21 bits per character, not 32.\nThatsaid,UTF-32doeshavesimplicityinitsfavor. Itisa fixed-length encod-\ning, meaning that every character occupies the same number of bits in mem-\nory(32bitstobeprecise). Assuch,wecandeterminethelengthofanyUTF-32\nstring by taking its length in bytes and dividing by four.\nUTF-8\nIn the UTF-8 encoding scheme, the code points for each character in a string\nare stored using eight-bit (one-byte) granularity, but some code points occupy\nmorethanonebyte. HencethenumberofbytesoccupiedbyaUTF-8character\nstring is not necessarily the length of the string in characters. This is known\nas avariable-length encoding, or a multibyte character set (MBCS), because each\ncharacter in a string may take one or more bytes of storage.\nOne of the big benefits of the UTF-8 encoding is that it is backwards-\ncompatiblewiththeANSIencoding. Thisworksbecausethefirst127Unicode\ncode points correspond numerically to the old ANSI character codes. This\nmeans that every ANSI character will be represented by exactly one byte in\nUTF-8, and a string of ANSI characters can be interpreted as a UTF-8 string\nwithout modification.\nTo represent higher-valued code points, the UTF-8 standard uses multi-\nbyte characters. Each multibyte character starts with a byte whose most-\nsignificant bit is 1 (i.e., its value lies in the range 128–255, inclusive). Such\nhigh-valued bytes will never appear in an ANSI string, so there is no am-\nbiguity when distinguishing between single-byte characters and multibyte\ncharacters.\nUTF-16\nTheUTF-16encodingemploysasomewhatsimpler,albeitmoreexpensiveap-\nproach. Each character in a UTF-16 string is represented by either one or two\n16-bit values. The UTF-16 encoding is known as a wide character set (WCS)\nbecause each character is at least 16 bits wide, instead of the eight bits used by\n464 6. Engine Support Systems\n“regular” ANSI chars and their UTF-8 counterparts.\nIn UTF-16, the set of all possible Unicode code points is divided into 17\nplanescontaining 216codepointseach. Thefirstplaneisknownasthe basicmul-\ntilingual plane (BMP). It contains the most commonly used code points across\na wide range of languages. As such, many UTF-16 strings can be represented\nentirely by code points within the first plane, meaning that each character in\nsuch a string is represented by only one16-bit value. However, if a character\nfrom one of the other planes (known as supplementary planes) is required in a\nstring, it is represented by two consecutive 16-bit values.\nThe UCS-2 (2-byte universal character set) encoding is a limited subset\nof the UTF-16 encoding, utilizing onlythe basic multilingual page. As such,\nit cannot represent characters whose Unicode code points are numerically\nhigher than 0xFFFF. This simplifies the format, because every character is\nguaranteed to occupy exactly 16 bits (two bytes). In other words, UCS-2\nis afixed-length character encoding, while in general UTF-8 and UTF-16 are\nvariable-length encodings.\nIf we know a priori that a UTF-16 string only utilizes code points from the\nBMP(orifwearedealingwithaUCS-2encodedstring), wecandeterminethe\nnumber of characters in the string by simply dividing the number of bytes by\ntwo. Ofcourse,ifsupplementalplanesareusedinaUTF-16string,thissimple\n“trick” no longer works.\nNotethataUTF-16encodingcanbelittle-endianorbig-endian(seeSection\n3.3.2.1),dependingonthenativeendiannessofyourtargetCPU.Whenstoring\nUTF-16texton-disc,it’scommontoprecedethetextdatawitha byteordermark\n(BOM) indicating whether the individual 16-bit characters are stored in little-\nor big-endian format. (This is true of UTF-32 encoded string data as well, of\ncourse.)\n6.4.4.2 char versus wchar_t\nThestandardC/C++librarydefinestwodatatypesfordealingwithcharacter\nstrings—char andwchar_t . The chartype is intended for use with legacy\nANSI strings and with multibyte character sets (MBCS), including (but not\nlimited to) UTF-8. The wchar_t type is a “wide” character type, intended to\nbe capable of representing any valid code point in a single integer. As such,\nits size is compiler- and system-specific. It could be eight bits on a system\nthat does not support Unicode at all. It could be 16 bits if the UCS-2 encoding\nis assumed for all wide characters, or if a multi-word encoding like UTF-16\nis being employed. Or it could be 32 bits if UTF-32 is the “wide” character\nencoding of choice.\nBecause of this inherent ambiguity in the definition of wchar_t, if you\n6.4. Strings 465\nneed to write truly portable string-handling code, you’ll need to define your\nown character data type(s) and provide a library of functions for dealing with\nwhatever Unicode encoding(s) you need to support. However, if you are tar-\ngeting a specific platform and compiler, you can write your code within the\nlimits of that particular implementation, at the loss of some portability.\nThe following article does a good job of outlining the pros and cons of\nusing the wchar_t data type: http://icu-project.org/docs/papers/unicode_\nwchar_t.html.\n6.4.4.3 Unicode under Windows\nUnder Windows, the wchar_t data type is used exclusively for UTF-16 en-\ncodedUnicodestrings, andthe chartypeisusedforANSIstringsandlegacy\nWindows code page string encodings. When reading the Windows API docs,\ntheterm“Unicode”isthereforealwayssynonymouswith“widecharacterset”\n(WCS) and UTF-16 encoding. This is a bit confusing, because of course Uni-\ncode strings can in general be encoded in the “non-wide” multibyte UTF-8\nformat.\nThe Windows API defines three sets of character/string manipulation\nfunctions: one set for single-byte character set ANSI strings (SBCS), one set\nfor multibyte character set (MBCS) strings, and one set for wide character set\nstrings. The ANSI functions are essentially the old-school “C-style” string\nfunctions we all grew up with. The MBCS string functions handle a variety\nof multibyte encodings and are primarily designed for dealing with legacy\nWindows code pages encodings. The WCS functions handle Unicode UTF-16\nstrings.\nThroughout the Windows API, a prefix or suffix of “w,” “wcs” or “W” in-\ndicatesawidecharacterset(UTF-16)encoding; aprefixorsuffixof“mb”indi-\ncates a multibyte encoding; and a prefix or suffix of “a” or “A,” or the lack of\nany prefix or suffix, indicates an ANSI or Windows code pages encoding. The\nC++ standard library uses a similar convention—for example, std::string\nisitsANSIstringclass,while std::wstring isitswidecharacterequivalent.\nUnfortunately, the names of the functions aren’t always 100% consistent. This\nallleadstosomeconfusionamongprogrammerswhoaren’tintheknow. (But\nyouaren’t one of those programmers!) Table 6.2 lists some examples.\nWindows also provides functions for translating between ANSI char-\nacter strings, multibyte strings and wide UTF-16 strings. For example,\nwcstombs() converts a wide UTF-16 string into a multibyte string accord-\ning to the currently active localesetting.\nTheWindowsAPIusesalittlepreprocessortricktoallowyoutowritecode\nthat is at least superficially portable between wide (Unicode) and non-wide\n466 6. Engine Support Systems\nANSI WCS MBCS\nstrcmp() wcscmp() _mbscmp()\nstrcpy() wcscpy() _mbscpy()\nstrlen() wcslen() _mbstrlen()\nTable 6.2. Variants of some common C standard library string functions for use with ANSI, wide\nand multibyte character sets.\n(ANSI/MBCS) string encodings. The generic character data type TCHAR is\ndefined to be a typedef tocharwhen building your application in “ANSI\nmode,” and it’s defined to be a typedef towchar_t when building your ap-\nplication in “Unicode mode.” The macro _T()is used to convert an eight-bit\nstring literal (e.g., char* s = ""this is a string""; ) into a wide string\nliteral (e.g., wchar_t* s = L""this is a string"";) when compiling in\n“Unicode mode.” Likewise, a suite of “fake” API functions are provided that\n“automagically” morph into their appropriate 8-bit or 16-bit variant, depend-\ning on whether you are building in “Unicode mode” or not. These magic\ncharacter-set-independent functions are either named with no prefix or suf-\nfix, or with a “t,” “tcs” or “T” prefix or suffix.\nComplete documentation for all of these functions can be found on Mi-\ncrosoft’s MSDN website. Here’s a link to the documentation for strcmp()\nanditsilk,fromwhichyoucanquiteeasilynavigatetotheotherrelatedstring-\nmanipulationfunctionsusingthetreeviewontheleft-handsideofthepage,or\nvia the search bar: http://msdn2.microsoft.com/en-us/library/kk6xf663(VS.\n80).aspx.\n6.4.4.4 Unicode on Consoles\nThe Xbox 360 software development kit (XDK) uses WCS strings pretty much\nexclusively, for all strings—even for internal strings like file paths. This is cer-\ntainly one valid approach to the localization problem, and it makes for very\nconsistent string handling throughout the XDK. However, the UTF-16 encod-\ningisabitwastefulonmemory, sodifferentgameenginesmayemploydiffer-\nent conventions. At Naughty Dog, we use eight-bit charstrings throughout\nourengine,andwehandleforeignlanguagesviaaUTF-8encoding. Thechoice\nof encoding is not particularly important, as long as you select one as early in\nthe project as possible and stick with it consistently.\n6.4.4.5 Other Localization Concerns\nEven once you have adapted your software to use Unicode characters, there\nis still a host of other localization problems to contend with. For one thing,\n6.4. Strings 467\nId English French\np1score “Player 1 Score” “Joueur 1 Score”\np2score “Player 2 Score” “Joueur 2 Score”\np1wins “Player one wins!” “Joueur un gagne!”\np2wins “Player two wins!” “Joueur deux gagne!”\nTable 6.3. Example of a string database used for localization.\nstrings aren’t the only place where localization issues arise. Audio clips in-\ncludingrecordedvoicesmustbetranslated. TexturesmayhaveEnglishwords\npainted into them that require translation. Many symbols have different\nmeaningsindifferentcultures. Evensomethingasinnocuousasano-smoking\nsign might be misinterpreted in another culture. In addition, some markets\ndraw the boundaries between the various game-rating levels differently. For\nexample, in Japan a Teen-rated game is not permitted to show blood of any\nkind, whereas in North America small red blood spatters are considered ac-\nceptable.\nFor strings, there are other details to worry about as well. You will need to\nmanage a database of all human-readable strings in your game, so that they\ncan all be reliably translated. The software must display the proper language\ngiven the user’s installation settings. The formatting of the strings may be to-\ntallydifferentindifferentlanguages—forexample,Chineseissometimeswrit-\nten vertically, and Hebrew reads right-to-left. The lengths of the strings will\nvarygreatlyfromlanguagetolanguage. You’llalsoneedtodecidewhetherto\nship a single DVD or Blu-ray disc that contains all languages or ship different\ndiscs for particular territories.\nThe most crucial components in your localization system will be the cen-\ntraldatabaseofhuman-readablestringsandanin-gamesystemforlookingup\nthose strings by id. For example, let’s say you want a heads-up display that\nliststhescoreofeachplayerwith“Player1Score:” and“Player2Score:” labels\nand that also displays the text “Player 1 Wins” or “Player 2 Wins” at the end\nofaround. Thesefourstringswouldbestoredinthelocalizationdatabaseun-\nder unique ids that are understandable to you, the developer of the game. So\nourdatabasemightusetheids“p1score,”“p2score,”“p1wins”and“p2wins,”\nrespectively. Once our game’s strings have been translated into French, our\ndatabase would look something like the simple example shown in Table 6.3.\nAdditionalcolumnscanbeaddedforeachnewlanguageyourgamesupports.\nThe exact format of this database is up to you. It might be as simple as\na Microsoft Excel worksheet that can be saved as a comma-separated values\n(CSV) file and parsed by the game engine or as complex as a full-fledged Or-\n468 6. Engine Support Systems\nacle database. The specifics of the string database are largely unimportant to\nthe game engine, as long as it can read in the string ids and the correspond-\ningUnicodestringsforwhateverlanguage(s)yourgamesupports. (However,\nthe specifics of the database may be veryimportant from a practical point of\nview, depending upon the organizational structure of your game studio. A\nsmall studio with in-house translators can probably get away with an Excel\nspreadsheetlocatedonanetworkdrive. Butalargestudiowithbranchoffices\nin Britain, Europe, South America and Japan would probably find some kind\nof distributed database a great deal more amenable.)\nAt runtime, you’ll need to provide a simple function that returns the Uni-\ncode string in the “current” language, given the unique id of that string. The\nfunction might be declared like this:\nwchar_t getLocalizedString (const char* id);\nand it might be used like this:\nvoid drawScoreHud(const Vector3& score1Pos,\nconst Vector3& score2Pos)\n{\nrenderer.displayTextOrtho(getLocalizedString(""p1score""),\nscore1Pos);\nrenderer.displayTextOrtho(getLocalizedString(""p2score""),\nscore2Pos);\n// ...\n}\nOf course, you’ll need some way to set the “current” language globally. This\nmight be done via a configuration setting, which is fixed during the installa-\ntion of the game. Or you might allow users to change the current language on\nthe fly via an in-game menu. Either way, the setting is not difficult to imple-\nment; it can be as simple as a global integer variable specifying the index of\nthe column in the string table from which to read (e.g., column one might be\nEnglish, column two French, column three Spanish and so on).\nOnce you have this infrastructure in place, your programmers must re-\nmember to neverdisplayarawstringtotheuser. They must always use the id of\na string in the database and call the look-up function in order to retrieve the\nstring in question.\n6.4.4.6 Case Study: Naughty Dog’s Localization Tool\nAt Naughty Dog, we use a localization database that we developed in-house.\nThe localization tool’s back end consists of a MySQL database located on a\n6.4. Strings 469\nFigure 6.10. Naughty Dog’s localization tool’s main window, showing a list of pure text assets used\nin the menus and HUD. The user has just performed a search for an asset called MENU_NEWGAME .\nFigure 6.11. Detailed asset view, showing the MENU_NEWGAME string.",30851
49-6.5 Engine Configuration.pdf,49-6.5 Engine Configuration,"470 6. Engine Support Systems\nserver that is accessible both to the developers within Naughty Dog and also\nto the various external companies with which we work to translate our text\nand speech audio clips into the various languages our games support. The\nfront end is a web interface that “speaks” to the database, allowing users to\nview all of the text and audio assets, edit their contents, provide translations\nfor each asset, search for assets by id or by content and so on.\nIn Naughty Dog’s localization tool, each asset is either a string (for use in\nthe menus or HUD) or a speech audio clip with optional subtitle text (for use\nas in-game dialog or within cinematics). Each asset has a unique identifier,\nwhich is represented as a hashed string id (see Section 6.4.3.1). If a string is\nrequired for use in the menus or HUD, we look it up by its id and get back a\nUnicode (UTF-8) string suitable for display on-screen. If a line of dialog must\nbe played, we likewise look up the audio clip by its id and use the data in-\nengine to look up its corresponding subtitle (if any). The subtitle is treated\njust like a menu or HUD string, in that it is returned by the localization tool’s\nAPI as a UTF-8 string suitable for display.\nFigure 6.10 shows the main interface of the localization tool, in this case\ndisplayed in the Chrome web browser. In this image, you can see that the\nuser has typed in the id MENU_NEWGAME in order to look up the string “NEW\nGAME” (used on the game’s main menu for launching a new game). Fig-\nure 6.11 shows the detailed view of the MENU_NEWGAME asset. If the user\nhits the “Text Translations” button in the upper-left corner of the asset de-\ntails window, the screen shown in Figure 6.12 comes up, allowing the user\nto enter or edit the various translations of the string. Figure 6.13 shows an-\nother tab on the localization tool’s main page, this time listing audio speech\nassets. Finally, Figure 6.14 depicts the detailed asset view for the speech as-\nsetBADA_GAM_MIL_ESCAPE_OVERPASS_001 (“We missed all the action”),\nshowing translations of this line of dialog into some of the supported lan-\nguages.\n6.5 Engine Conﬁguration\nGame engines are complex beasts, and they invariably end up having a large\nnumber of configurable options. Some of these options are exposed to the\nplayer via one or more options menus in-game. For example, a game might\nexpose options related to graphics quality, the volume of music and sound\neffects, or controller configuration. Other options are created for the benefit\nof the game development team only and are either hidden or stripped out\nof the game completely before it ships. For example, the player character’s\n6.5. Engine Conﬁguration 471\nFigure 6.12. Text translations of the string “NEW GAME” into all languages supported by Naughty\nDog’s The Last of Us .\nmaximum walk speed might be exposed as an option so that it can be fine-\ntuned during development, but it might be changed to a hard-coded value\nprior to ship.\n6.5.1 Loading and Saving Options\nA configurable option can be implemented trivially as a global variable or a\nmember variable of a singleton class. However, configurable options are not\nparticularlyusefulunlesstheirvaluescanbeconfigured,storedonaharddisk,\nmemorycardorotherstoragemedium,andlaterretrievedbythegame. There\nare a number of simple ways to load and save configuration options:\n•Text configuration files . By far the most common method of saving and\nloading configuration options is by placing them into one or more text\n472 6. Engine Support Systems\nFigure 6.13. Naughty Dog’s localization tool’s main window again, this time showing a list of speech\naudio assets with accompanying subtitle text.\nFigure 6.14. Detailed asset view showing recorded translations for the speech asset\nBADA_GAM_MIL_ESCAPE_OVERPASS_001 (“We missed all the action”).\n6.5. Engine Conﬁguration 473\nfiles. Theformatofthesefilesvarieswidelyfromenginetoengine,butit\nis usually very simple. For example, Windows INI files (which are used\nby the OGRE renderer) consist of flat lists of key-value pairs grouped\ninto logical sections. The JSON format is another common choice for\nconfigurablegameoptionsfiles. XMLisanotherviableoption, although\nmost developers these days find JSON to be less verbose and easier to\nread than XML.\n•Compressed binary files . Most modern consoles have hard disk drives in\nthem, but older consoles could not afford this luxury. As a result, all\ngame consoles since the Super Nintendo Entertainment System (SNES)\nhave come equipped with proprietary removable memory cards that\npermit both reading and writing of data. Game options are sometimes\nstored on these cards, along with saved games. Compressed binary files\nare the format of choice on a memory card, because the storage space\navailable on these cards is often very limited.\n•The Windows registry . The Microsoft Windows operating system pro-\nvides a global options database known as the registry. It is stored as\na tree, where the interior nodes (known as registry keys ) act like file\nfolders, and the leaf nodes store the individual options as key-value\npairs. That being said, I don’t recommend using the Windows registry\nfor storing engine configuration information. The registry is a mono-\nlithic database that can easily be corrupted, lost (when Windows is rein-\nstalled), or thrown out-of-sync with the files in the filesystem. For more\non the weaknesses of the Windows registry, see https://blog.coding\nhorror.com/was-the-windows-registry-a-good-idea/.\n•Command line options. The command line can be scanned for option set-\ntings. Theenginemightprovideamechanismforcontrollinganyoption\ninthegameviathecommandline,oritmightexposeonlyasmallsubset\nof the game’s options here.\n•Environmentvariables . On personal computers running Windows, Linux\nor MacOS, environment variables are sometimes used to store configu-\nration options as well.\n•Online user profiles . With the advent of online gaming communities like\nXboxLive,eachusercancreateaprofileanduseittosaveachievements,\npurchased and unlockable game features, game options and other infor-\nmation. The data are stored on a central server and can be accessed by\nthe player wherever an Internet connection is available.\n474 6. Engine Support Systems\n6.5.2 Per-User Options\nMostgameenginesdifferentiatebetweenglobaloptionsandper-useroptions.\nThisisnecessarybecausemostgamesalloweachplayertoconfigurethegame\ntohisorherliking. Itisalsoausefulconceptduringdevelopmentofthegame,\nbecauseitallowseachprogrammer,artistanddesignertocustomizehisorher\nwork environment without affecting other team members.\nObviously care must be taken to store per-user options in such a way that\neach player “sees” only his or her options and not the options of other play-\ners on the same computer or console. In a console game, the user is typically\nallowed to save his or her progress, along with per-user options such as con-\ntroller preferences, in “slots” on a memory card or hard disk. These slots are\nusually implemented as files on the media in question.\nOn a Windows machine, each user has a folder under C:\Users contain-\ning information such as the user’s desktop, his or her My Documents folder,\nhis or her Internet browsing history and temporary files and so on. A hid-\nden subfolder named AppData is used to store per-user information on a per-\napplicationbasis; eachapplicationcreatesafolderunder AppData andcanuse\nit to store whatever per-user information it requires.\nWindows games sometimes store per-user configuration data in the reg-\nistry. Theregistryisarrangedasatree, andoneofthetop-levelchildrenofthe\nroot node, called HKEY_CURRENT_USER, stores settings for whichever user\nhappenstobeloggedon. Everyuserhashisorherownsubtreeintheregistry\n(storedunderthetop-levelsubtree HKEY_USERS),and HKEY_CURRENT_USER\nis really just an alias to the current user’s subtree. So games and other ap-\nplications can manage per-user configuration options by simply reading and\nwriting them to keys under the HKEY_CURRENT_USER subtree.\n6.5.3 Conﬁguration Management in Some Real Engines\nIn this section, we’ll take a brief look at how some real game engines manage\ntheir configuration options.\n6.5.3.1 Example: Quake’s Cvars\nTheQuakefamilyofenginesusesaconfigurationmanagementsystemknown\nasconsole variables , orcvarsfor short. A cvar is just a floating-point or string\nglobal variable whose value can be inspected and modified from within\nQuake’s in-game console. The values of some cvars can be saved to disk and\nlater reloaded by the engine.\nAt runtime, cvars are stored in a global linked list. Each cvar is a dy-\nnamicallyallocatedinstanceof struct cvar_t,whichcontainsthevariable’s\n6.5. Engine Conﬁguration 475\nname, its value as a string or float, a set of flag bits, and a pointer to the next\ncvar in the linked list of all cvars. Cvars are accessed by calling Cvar_Get(),\nwhich creates the variable if it doesn’t already exist and modified by calling\nCvar_Set(). One of the bit flags, CVAR_ARCHIVE, controls whether or not\nthe cvar will be saved into a configuration file called config.cfg. If this flag is\nset, the value of the cvar will persist across multiple runs of the game.\n6.5.3.2 Example: OGRE\nThe OGRE rendering engine uses a collection of text files in Windows INI for-\nmat for its configuration options. By default, the options are stored in three\nfiles, each of which is located in the same folder as the executable program:\n•plugins.cfg contains options specifying which optional engine plug-ins\nare enabled and where to find them on disk.\n•resources.cfg contains a search path specifying where game assets (a.k.a.\nmedia, a.k.a. resources) can be found.\n•ogre.cfg contains a rich set of options specifying which renderer (DirectX\nor OpenGL) to use and the preferred video mode, screen size, etc.\nOut of the box, OGRE provides no mechanism for storing per-user con-\nfiguration options. However, the OGRE source code is freely available, so\nit would be quite easy to change it to search for its configuration files in the\nuser’s home directory instead of in the folder containing the executable. The\nOgre::ConfigFile class makes it easy to write code that reads and writes\nbrand new configuration files as well.\n6.5.3.3 Example: The Uncharted and The Last of Us Series\nNaughty Dog’s engine makes use of a number of configuration mechanisms.\nIn-Game Menu Settings\nThe Naughty Dog engine supports a powerful in-game menu system, allow-\ningdeveloperstocontrolglobalconfigurationoptionsandinvokecommands.\nThe data types of the configurable options must be relatively simple (primar-\nily Boolean, integer and floating-point variables), but this limitation has not\nprevented the developers at Naughty Dog from creating literally hundreds of\nuseful menu-driven options.\nEach configuration option is implemented as a global variable, or a mem-\nber of a singleton struct or class. When the menu option that controls an op-\ntion is created, the address of the variable is provided, and the menu item\ndirectly controls its value. As an example, the following function creates a\n476 6. Engine Support Systems\nsubmenu item containing some options for Naughty Dog’s rail vehicles (sim-\nple vehicles that ride on splines which have been used in pretty much every\nNaughty Dog game, from the “Out of the Frying Pan” jeep chase level in Un-\ncharted: Drake’sFortune tothetruckconvoy/jeepchasesequencein Uncharted\n4). It defines menu items controlling three global variables: two Booleans\nand one floating-point value. The items are collected onto a menu, and a\nspecial item is returned that will bring up the menu when selected. Presum-\nably the code calling this function adds this item to the parent menu that it is\nbuilding.\nDMENU::ItemSubmenu * CreateRailVehicleMenu()\n{\nextern bool g_railVehicleDebugDraw2D;\nextern bool g_railVehicleDebugDrawCameraGoals;\nextern float g_railVehicleFlameProbability;\nDMENU::Menu * pMenu\n= new DMENU::Menu(""RailVehicle"");\npMenu->PushBackItem(\nnew DMENU::ItemBool(""Draw 2D Spring Graphs"",\nDMENU::ToggleBool,\n&g_railVehicleDebugDraw2D ));\npMenu->PushBackItem(\nnew DMENU::ItemBool (""Draw Goals (Untracked)"",\nDMENU::ToggleBool,\n&g_railVehicleDebugDrawCameraGoals));\nDMENU::ItemFloat * pItemFloat;\npItemFloat = new DMENU::ItemFloat(\n""FlameProbability"",\nDMENU::EditFloat, 5, ""%5.2f"",\n&g_railVehicleFlameProbability);\npItemFloat->SetRangeAndStep(0.0f, 1.0f, 0.1f, 0.01f);\npMenu->PushBackItem(pItemFloat);\nDMENU::ItemSubmenu * pSubmenuItem;\npSubmenuItem = new DMENU::ItemSubmenu(\n""RailVehicle..."", pMenu);\nreturn pSubmenuItem;\n}\nThe value of any option can be saved by simply marking it with the cir-\ncle button on the Dualshock joypad when the corresponding menu item is se-\n6.5. Engine Conﬁguration 477\nlected. ThemenusettingsaresavedinanINI-styletextfile,allowingthesaved\nglobalvariablestoretainthevaluesacrossmultiplerunsofthegame. Theabil-\nitytocontrolwhichoptionsaresavedona per-menu-itembasis ishighlyuseful,\nbecauseanyoptionthatisnotsavedwilltakeonitsprogrammer-specifiedde-\nfault value. If a programmer changes a default, all users will “see” the new\nvalue, unless of course a user has saved a custom value for that particular op-\ntion.\nCommand Line Arguments\nThe Naughty Dog engine scans the command line for a predefined set of spe-\ncial options. The name of the level to load can be specified, along with a num-\nber of other commonly used arguments.\nScheme Data Deﬁnitions\nThe vast majority of engine and game configuration information in the\nNaughty Dog engine (used to produce the Uncharted andThe Last of Us se-\nries) is specified using a Lisp-like language called Scheme. Using a propri-\netary data compiler, data structuresdefined in the Scheme language aretrans-\nformed into binary files that can be loaded by the engine. The data compiler\nalso spits out header files containing C struct declarations for every data\ntype defined in Scheme. These header files allow the engine to properly in-\nterpret the data contained in the loaded binary files. The binary files can even\nbe recompiled and reloaded on the fly, allowing developers to alter the data\nin Scheme and see the effects of their changes immediately (as long as data\nmembers are not added or removed, as that would require a recompile of the\nengine).\nThefollowingexampleillustratesthecreationofadatastructurespecifying\nthe properties of an animation. It then exports three unique animations to\nthegame. YoumayhaveneverreadSchemecodebefore,butforthisrelatively\nsimple example it should be pretty self-explanatory. One oddity you’ll notice\nis that hyphens are permitted within Scheme symbols, so simple-animation\nisasinglesymbol(unlikeinC/C++where simple-animation wouldbethe\nsubtraction of two variables, simple andanimation ).\nsimple-animation.scm\n;; Define a new data type called simple-animation.\n(deftype simple-animation ()\n(\n(name string)\n(speed float :default 1.0)\n478 6. Engine Support Systems\n(fade-in-seconds float :default 0.25)\n(fade-out-seconds float :default 0.25)\n)\n)\n;; Now define three instances of this data structure...\n(define-export anim-walk\n(new simple-animation\n:name ""walk""\n:speed 1.0\n)\n)\n(define-export anim-walk-fast\n(new simple-animation\n:name ""walk""\n:speed 2.0\n)\n)\n(define-export anim-jump\n(new simple-animation\n:name ""jump""\n:fade-in-seconds 0.1\n:fade-out-seconds 0.1\n)\n)\nThis Scheme code would generate the following C/C++ header file:\nsimple-animation.h\n// WARNING: This file was automatically generated from\n// Scheme. Do not hand-edit.\nstruct SimpleAnimation\n{\nconst char* m_name;\nfloat m_speed;\nfloat m_fadeInSeconds;\nfloat m_fadeOutSeconds;\n};\nIn-game, the data can be read by calling the LookupSymbol() function,\nwhich is templated on the data type returned, as follows:\n6.5. Engine Conﬁguration 479\n#include ""simple-animation.h ""\nvoid someFunction()\n{\nSimpleAnimation * pWalkAnim\n=LookupSymbol <SimpleAnimation*>(\nSID(""anim-walk""));\nSimpleAnimation * pFastWalkAnim\n=LookupSymbol <SimpleAnimation*>(\nSID(""anim-walk-fast""));\nSimpleAnimation * pJumpAnim\n=LookupSymbol <SimpleAnimation*>(\nSID(""anim-jump""));\n// use the data here...\n}\nThissystemgivestheprogrammersagreatdealofflexibilityindefiningall\nsorts of configuration data—from simple Boolean, floating-point and string\noptions all the way to complex, nested, interconnected data structures. It is\nusedtospecifydetailedanimationtrees,physicsparameters,playermechanics\nand so on.\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",16741
50-7.1 File System.pdf,50-7.1 File System,"7\nResources and\nthe File System\nGames are by nature multimedia experiences. A game engine therefore\nneeds to be capable of loading and managing a wide variety of different\nkinds of media—texture bitmaps, 3D mesh data, animations, audio clips, col-\nlision and physics data, game world layouts, and the list goes on. Moreover,\nbecause memory is usually scarce, a game engine needs to ensure that only\none copy of each media file is loaded into memory at any given time. For ex-\nample, if five meshes share the same texture, then we would like to have only\nonecopyofthattextureinmemory,notfive. Mostgameenginesemploysome\nkind ofresourcemanager (a.k.a.assetmanager, a.k.a. mediamanager ) to load and\nmanage the myriad resources that make up a modern 3D game.\nEveryresourcemanagermakesheavyuseofthefilesystem. Onapersonal\ncomputer,thefilesystemisexposedtotheprogrammerviaalibraryofoperat-\ning system calls. However, game engines often “wrap” the native file system\nAPIinanengine-specificAPI,fortwoprimaryreasons. First,theenginemight\nbe cross-platform, in which case the game engine’s file system API can shield\nthe rest of the software from differences between different target hardware\nplatforms. Second, the operating system’s file system API might not provide\nall the tools needed by a game engine. For example, many engines support\nfilestreaming (i.e., the ability to load data “on the fly” while the game is run-\nning), yet most operating systems don’t provide a streaming file system API\n481\n482 7. Resources and the File System\nout of the box. Console game engines also need to provide access to a variety\nofremovableandnon-removablemedia,frommemorystickstooptionalhard\ndrivestoaDVD-ROMorBlu-rayfixeddisktonetworkfilesystems(e.g.,Xbox\nLiveorthePlayStationNetwork,PSN).Thedifferencesbetweenvariouskinds\nof media can likewise be “hidden” behind a game engine’s file system API.\nIn this chapter, we’ll first explore the kinds of file system APIs found in\nmodern 3D game engines. Then we’ll see how a typical resource manager\nworks.\n7.1 File System\nA game engine’s file system API typically addresses the following areas of\nfunctionality:\n• manipulating file names and paths,\n• opening, closing, reading and writing individual files,\n• scanning the contents of a directory, and\n• handling asynchronous file I/O requests (for streaming).\nWe’ll take a brief look at each of these in the following sections.\n7.1.1 File Names and Paths\nApathis a string describing the location of a file or directory within a file sys-\ntem hierarchy. Each operating system uses a slightly different path format,\nbut paths have essentially the same structure on every operating system. A\npath generally takes the following form:\nvolume/directory1/directory2/.../directory N/file-name\nor\nvolume/directory1/directory2/.../directory (N 1)/directory N\nIn other words, a path generally consists of an optional volume specifier fol-\nlowed by a sequence of path components separated by a reserved path separa-\ntor character such as the forward or backward slash (/ or\). Each component\nnames a dir ectory along the route from the root directory to the file or direc-\ntory in question. If the path specifies the location of a file, the last component\nin the path is the file name; otherwise it names the target directory. The root\ndirectoryisusuallyindicatedbyapathconsistingoftheoptionalvolumespec-\nifier followed by a single path separator character (e.g., /on UNIX, or C:\on\nWindows).\n7.1. File System 483\n7.1.1.1 Differences across Operating Systems\nEach operating system introduces slight variations on this general path struc-\nture. Here are some of the key differences between Microsoft DOS, Microsoft\nWindows, the UNIX family of operating systems and Apple Macintosh OS:\n• UNIX uses a forward slash (/) as its path component separator, while\nDOS and older versions of Windows used a backslash (\) as the path\nseparator. Recent versions of Windows allow either forward or back-\nward slashes to be used to separate path components, although some\napplications still fail to accept forward slashes.\n• Mac OS 8 and 9 use the colon (:) as the path separator character. Mac\nOS X is based on BSD UNIX, so it supports UNIX’s forward slash nota-\ntion.\n• Some filesystems consider paths and filenames to be case-sensitive (like\nUNIXanditsvariants),whileothersarecase-insensitive(likeWindows).\nThis can cause problems when dealing with files across multiple oper-\nating systems during development, or when writing a cross-platform\ngame. (For example, should an asset file named EnemyAnims.json\nbe considered equivalent to an asset file named enemyanims.json or\nnot?)\n• UNIX and its variants don’t support volumes as separate directory hi-\nerarchies. The entire file system is contained within a single monolithic\nhierarchy, and local disk drives, network drives and other resources are\nmounted sothattheyappeartobesubtreeswithinthemainhierarchy. As\na result, UNIX paths never have a volume specifier.\n• On Microsoft Windows, volumes can be specified in two ways. A local\ndisk drive is specified using a single letter followed by a colon (e.g., the\nubiquitous C:). A remote network share can either be mounted so that\nit looks like a local disk, or it can be referenced via a volume specifier\nconsisting of two backslashes followed by the remote computer name\nand the name of a shared directory or resource on that machine (e.g.,\n\\some-computer\some-share ). This double backslash notation is\nan example of the Universal Naming Convention (UNC).\n• Under DOS and early versions of Windows, a file name could be up to\neight characters in length, with a three-character extension which was\nseparated from the main file name by a dot. The extension described\nthe file’s type, for example .txtfor a text file or .exefor an executable\nfile. In recent Windows implementations, file names can contain any\nnumber of dots (as they can under UNIX), but the characters after the\n484 7. Resources and the File System\nfinal dot are still interpreted as the file’s extension by many applications\nincluding the Windows Explorer.\n• Each operating system disallows certain characters in the names of files\nand directories. For example, a colon cannot appear anywhere in a Win-\ndowsorDOSpathexceptaspartofadrivelettervolumespecifier. Some\noperating systems permit a subset of these reserved characters to ap-\npear in a path as long as the path is quoted in its entirety or the offend-\ning character is escaped by preceding it with a backslash or some other\nreserved escape character. For example, file and directory names may\ncontain spaces under Windows, but such a path must be surrounded\nby double quotes in certain contexts.\n• Both UNIX and Windows have the concept of a currentworkingdirectory\nor CWD (also known as the presentworkingdirectory or PWD). The CWD\ncan be set from a command shell via the cd(change directory ) command\non both operating systems, and it can be queried by typing cdwith no\nargumentsunderWindowsorbyexecutingthe pwdcommandonUNIX.\nUnder UNIX there is only one CWD. Under Windows, each volume has\nits own private CWD.\n• Operating systems that support multiple volumes, like Windows, also\nhavetheconceptofa currentworkingvolume. FromaWindowscommand\nshell,thecurrentvolumecanbesetbyenteringitsdriveletterandacolon\nfollowed by the Enter key (e.g., C:<Enter>).\n• Consoles often also employ a set of predefined path prefixes to rep-\nresent multiple volumes. For example, PlayStation 3 uses the prefix\n/dev_bdvd/ torefertotheBlu-raydiskdrive,while /dev_hddx /refers\nto one or more hard disks (where xis the index of the device). On a PS3\ndevelopmentkit, /app_home/ mapstoauser-definedpathonwhatever\nhost machine is being used for development. During development, the\ngame usually reads its assets from /app_home/ rather than from the\nBlu-ray or the hard disk.\n7.1.1.2 Absolute and Relative Paths\nAllpathsarespecifiedrelativetosomelocationwithinthefilesystem. Whena\npath is specified relative to the root directory, we call it an absolutepath. When\nit is relative to some otherdirectory in the file system hierarchy, we call it a\nrelativepath .\nUnder both UNIX and Windows, absolute paths start with a path sepa-\nrator character (/ or \), while relative paths have no leading path separator.\nOn Windows, both absolute and relative paths may have an optional volume\n7.1. File System 485\nspecifier—if the volume is omitted, then the path is assumed to refer to the\ncurrent working volume.\nThe following paths are all absolute:\nWindows\n•C:\Windows\System32\n•D:\(root directory on the D:volume)\n•\(root directory on the current working volume)\n•\game\assets\animation\walk.anim (current working volume)\n•\\joe-dell\Shared_Files\Images\foo.jpg (network path)\nUNIX\n•/usr/local/bin/grep\n•/game/src/audio/effects.cpp\n•/(root directory)\nThe following paths are all relative:\nWindows\n•System32 (relative to CWD \Windows on the current volume)\n•X:animation\walk.anim (relative to CWD \game\assets on the\nX:volume)\nUNIX\n•bin/grep (relative to CWD /usr/local)\n•src/audio/effects.cpp (relative to CWD /game)\n7.1.1.3 Search Paths\nThe term pathshould not be confused with the term search path . Apathis a\nstring representing the location of a single file or directory within the file sys-\ntemhierarchy. A searchpath isastringcontainingalistofpaths,eachseparated\nby a special character such as a colon or semicolon, which is searched when\nlooking for a file. For example, when you run any program from a command\nprompt, the operating system finds the executable file by searching each di-\nrectory on the search path contained in the shell’s environment variable.\nSome game engines also use search paths to locate resource files. For ex-\nample, the OGRE rendering engine uses a resource search path contained in a\ntext file named resources.cfg. The file provides a simple list of directories\nand ZIP archives that should be searched in order when trying to find an as-\nset. Thatsaid,searchingforassetsatruntimeisatime-consumingproposition.\nUsuallythere’snoreasonourassets’pathscannotbeknownapriori. Presum-\ning this is the case, we can avoid having to search for assets at all—which is\nclearly a superior approach.\n486 7. Resources and the File System\n7.1.1.4 Path APIs\nClearly, paths are much more complex than simple strings. There are many\nthings a programmer may need to do when dealing with paths, such as iso-\nlating the directory, filename and extension, canonicalizing a path, converting\nback and forth between absolute and relative paths and so on. It can be ex-\ntremely helpful to have a feature-rich API to help with these tasks.\nMicrosoft Windows provides an API for this purpose. It is implemented\nby the dynamic link library shlwapi.dll, and it is exposed via the header\nfileshlwapi.h. CompletedocumentationforthisAPIisprovidedontheMi-\ncrosoft Developer’s Network (MSDN) at the following URL: http://msdn2.\nmicrosoft.com/en-us/library/bb773559(VS.85).aspx.\nOf course, the shlwapi API is only available on Win32 platforms. Sony\nprovidessimilarAPIsforuseonthePlayStation3andPlayStation4. Butwhen\nwriting a cross-platform game engine, we cannot use platform-specific APIs\ndirectly. A game engine may not need all of the functions provided by an\nAPI like shlwapi anyway. For these reasons, game engines often implement\na stripped-down path-handling API that meets the engine’s particular needs\nandworksoneveryoperatingsystemtargetedbytheengine. SuchanAPIcan\nbe implemented as a thin wrapper around the native API on each platform or\nit can be written from scratch.\n7.1.2 Basic File I/O\nThe C standard library provides two APIs for opening, reading and writing\nthe contents of files—one buffered and the other unbuffered. Every file I/O\nAPI requires data blocks known as buffersto serve as the source or destination\nof the bytes passing between the program and the file on disk. We say a file\nI/O API is buffered when the API manages the necessary input and output\ndata buffers for you. With an unbuffered API, it is the responsibility of the\nprogrammer using the API to allocate and manage the data buffers. The C\nstandard library’s buffered file I/O routines are sometimes referred to as the\nstream I/O API, because they provide an abstraction which makes disk files\nlook like streams of bytes.\nThe C standard library functions for buffered and unbuffered file I/O are\nlisted in Table 7.1.\nThe C standard library I/O functions are well-documented, so we will\nnot repeat detailed documentation for them here. For more information,\nplease refer to http://msdn.microsoft.com/en-us/library/c565h7xx.aspx for\nMicrosoft’s implementation of the buffered (stream I/O) API, and to http:\n//msdn.microsoft.com/en-us/library/40bbyw78.aspx for Microsoft’s imple-\n7.1. File System 487\nOperation Buffered API Unbuffered API\nOpen a file fopen() open()\nClose a file fclose() close()\nRead from a file fread() read()\nWrite to a file fwrite() write()\nSeek to an offset fseek() seek()\nReturn current offset ftell() tell()\nRead a single line fgets() n/a\nWrite a single line fputs() n/a\nRead formatted string fscanf() n/a\nWrite formatted string fprintf() n/a\nQuery file status fstat() stat()\nTable 7.1. Buffered and unbuffered ﬁle operations in the C standard library.\nmentation of the unbuffered (low-level I/O) API.\nOn UNIX and its variants, the C standard library’s unbuffered I/O routes\nare native operating system calls. However, on Microsoft Windows these\nroutines are merely wrappers around an even lower-level API. The Win32\nfunction CreateFile() creates or opens a file for writing or reading,\nReadFile() andWriteFile() read and write data, respectively, and\nCloseFile() closes an open file handle. The advantage to using low-level\nsystem calls as opposed to C standard library functions is that they expose\nall of the details of the native file system. For example, you can query and\ncontrol the security attributes of files when using the Windows native API—\nsomething you cannot do with the C standard library.\nSomegameteamsfinditusefultomanagetheirownbuffers. Forexample,\ntheRedAlert3 team at Electronic Arts observed that writing data into log files\nwas causing significant performance degradation. They changed the logging\nsystem so that it accumulated its output into a memory buffer, writing the\nbuffer out to disk only when it was filled. Then they moved the buffer dump\nroutine out into a separate thread to avoid stalling the main game loop.\n7.1.2.1 To Wrap or Not to Wrap\nA game engine can be written to use the C standard library’s file I/O func-\ntions or the operating system’s native API. However, many game engines\nwrapthe file I/O API in a library of custom I/O functions. There are at least\nthree advantages to wrapping the operating system’s I/O API. First, the en-\ngineprogrammerscanguaranteeidenticalbehavioracrossalltargetplatforms,\neven when native libraries are inconsistent or buggy on a particular platform.\n488 7. Resources and the File System\nSecond, the API can be simplified down to only those functions actually re-\nquired by the engine, which keeps maintenance efforts to a minimum. Third,\nextended functionality can be provided. For example, the engine’s custom\nwrapper API might be capable of dealing with files on a hard disk, a DVD-\nROM or Blu-ray disk on a console, files on a network (e.g., remote files man-\nagedbyXboxLiveorPSN),andalsowithfilesonmemorysticksorotherkinds\nof removable media.\n7.1.2.2 Synchronous File I/O\nBoth of the standard C library’s file I/O libraries are synchronous , meaning\nthat the program making the I/O request must wait until the data has been\ncompletely transferred to or from the media device before continuing. The\nfollowingcodesnippetdemonstrateshowtheentirecontentsofafilemightbe\nread into an in-memory buffer using the synchronous I/O function fread().\nNotice how the function syncReadFile() does not return until all the data\nhas been read into the buffer provided.\nbool syncReadFile (const char* filePath,\nU8* buffer,\nsize_t bufferSize,\nsize_t& rBytesRead)\n{\nFILE* handle = fopen(filePath, ""rb"");\nif (handle)\n{\n// BLOCK here until all data has been read.\nsize_t bytesRead = fread(buffer, 1,\nbufferSize, handle);\nint err = ferror(handle); // get error if any\nfclose(handle);\nif (0 == err)\n{\nrBytesRead = bytesRead;\nreturn true;\n}\n}\nrBytesRead = 0;\nreturn false;\n}\nvoid main(int argc, const char* argv[])\n{\n7.1. File System 489\nU8 testBuffer[512];\nsize_t bytesRead = 0;\nif ( syncReadFile (""C:\\testfile.bin"",\ntestBuffer, sizeof(testBuffer),\nbytesRead))\n{\nprintf(""success: read %u bytes\n"", bytesRead);\n// contents of buffer can be used here...\n}\n}\n7.1.3 Asynchronous File I/O\nStreaming refers to the act of loading data in the background while the main\nprogram continues to run. Many games provide the player with a seam-\nless, load-screen-free playing experience by streaming data for upcoming lev-\nels from the DVD-ROM, Blu-ray disk or hard drive while the game is being\nplayed. Audio and texture data are probably the most commonly streamed\ntypes of data, but any type of data can be streamed, including geometry, level\nlayouts and animation clips.\nIn order to support streaming, we must utilize an asynchronous file I/O li-\nbrary, i.e., one which permits the program to continue to run while its I/O\nrequests are being satisfied. Some operating systems provide an asynchro-\nnous file I/O library out of the box. For example, the Windows Common\nLanguage Runtime (CLR, the virtual machine upon which languages like\nVisual BASIC, C#, managed C++ and J# are implemented) provides func-\ntionslike System.IO.BeginRead() andSystem.IO.BeginWrite(). An\nasynchronous API known as fiosis available for the PlayStation 3 and\nPlayStation 4. If an asynchronous file I/O library is not available for your tar-\nget platform, it is possible to write one yourself. And even if you don’t have\nto write it from scratch, it’s probably a good idea to wrap the system API for\nportability.\nThe following code snippet demonstrates how the entire contents of a file\nmight be read into an in-memory buffer using an asynchronous read oper-\nation. Notice that the asyncReadFile() function returns immediately—\nthe data is not present in the buffer until our callback function asyncRead-\nComplete() has been called by the I/O library.\nAsyncRequestHandle g_hRequest; // async I/O request handle\nU8 g_asyncBuffer[512]; // input buffer\nstatic void asyncReadComplete (AsyncRequestHandle hRequest);\n490 7. Resources and the File System\nvoid main(int argc, const char* argv[])\n{\n// NOTE: This call to asyncOpen() might itself be an\n// asynchronous call, but we'll ignore that detail\n// here and just assume it's a blocking function.\nAsyncFileHandle hFile = asyncOpen(\n""C:\\testfile.bin"");\nif (hFile)\n{\n// This function requests an I/O read, then\n// returns immediately (non-blocking).\ng_hRequest = asyncReadFile (\nhFile, // file handle\ng_asyncBuffer, // input buffer\nsizeof(g_asyncBuffer), // size of buffer\nasyncReadComplete ); // callback function\n}\n// Now go on our merry way...\n// (This loop simulates doing real work while we wait\n// for the I/O read to complete.)\nfor (;;)\n{\nOutputDebugString(""zzz...\n"");\nSleep(50);\n}\n}\n// This function will be called when the data has been read.\nstatic void asyncReadComplete (AsyncRequestHandle hRequest)\n{\nif (hRequest == g_hRequest\n&& asyncWasSuccessful(hRequest))\n{\n// The data is now present in g_asyncBuffer[] and\n// can be used. Query for the number of bytes\n// actually read:\nsize_t bytes = asyncGetBytesReadOrWritten(\nhRequest);\nchar msg[256];\nsnprintf(msg, sizeof(msg),\n""async success, read %u bytes\n"",\n7.1. File System 491\nbytes);\nOutputDebugString(msg);\n}\n}\nMost asynchronous I/O libraries permit the main program to wait for an\nI/O operation to complete some time after the request was made. This can\nbe useful in situations where only a limited amount of work can be done be-\nfore the results of a pending I/O request are needed. This is illustrated in the\nfollowing code snippet.\nU8 g_asyncBuffer[512]; // input buffer\nvoid main(int argc, const char* argv[])\n{\nAsyncRequestHandle hRequest = ASYNC_INVALID_HANDLE;\nAsyncFileHandle hFile = asyncOpen(\n""C:\\testfile.bin"");\nif (hFile)\n{\n// This function requests an I/O read, then\n// returns immediately (non-blocking).\nhRequest = asyncReadFile (\nhFile, // file handle\ng_asyncBuffer, // input buffer\nsizeof(g_asyncBuffer), // size of buffer\nnullptr); //no callback\n}\n// Now do some limited amount of work...\nfor (int i = 0; i < 10; i++)\n{\nOutputDebugString(""zzz...\n"");\nSleep(50);\n}\n// We can't do anything further until we have that\n// data, so wait for it here.\nasyncWait(hRequest);\nif (asyncWasSuccessful(hRequest))\n{\n// The data is now present in g_asyncBuffer[] and\n// can be used. Query for the number of bytes\n// actually read:\nsize_t bytes = asyncGetBytesReadOrWritten(\nhRequest);\n492 7. Resources and the File System\nchar msg[256];\nsnprintf(msg, sizeof(msg),\n""async success, read %u bytes\n"",\nbytes);\nOutputDebugString(msg);\n}\n}\nSome asynchronous I/O libraries allow the programmer to ask for an esti-\nmate of how long a particular asynchronous operation will take to complete.\nSome APIs also allow you to set deadlines on a request (which effectively pri-\noritizes the request relative to other pending requests), and to specify what\nhappens when a request misses its deadline (e.g., cancel the request, notify\nthe program and keep trying, etc.)\n7.1.3.1 Priorities\nIt’s important to remember that file I/O is a real-time system, subject to dead-\nlines just like the rest of the game. Therefore, asynchronous I/O operations\noften have varying priorities. For example, if we are streaming audio from\nthe hard disk or Blu-ray and playing it on the fly, loading the next buffer-full\nof audio data is clearly higher priority than, say, loading a texture or a chunk\nof a game level. Asynchronous I/O systems must be capable of suspending\nlower-priority requests, so that higher-priority I/O requests have a chance to\ncomplete within their deadlines.\n7.1.3.2 How Asynchronous File I/O Works\nAsynchronous file I/O works by handling I/O requests in a separate thread.\nThe main thread calls functions that simply place requests on a queue and\nthen return immediately. Meanwhile, the I/O thread picks up requests from\nthe queue and handles them sequentially using blocking I/O routines like\nread() orfread() . When a request is completed, a callback provided by\nthe main thread is called, thereby notifying it that the operation is done. If the\nmain thread chooses to wait for an I/O request to complete, this is handled\nvia asemaphore. (Each request has an associated semaphore, and the main\nthread can put itself to sleep waiting for that semaphore to be signaled by\nthe I/O thread upon completion of the request. See Section 4.6.4 for more on\nsemaphores.)\nVirtually anysynchronous operation you can imagine can be transformed\ninto an asynchronous operation by moving the code into a separate thread—\nor by running it on a physically separate processor, such as on one of the CPU\ncores on the PlayStation 4. See Section 8.6 for more details.",23485
51-7.2 The Resource Manager.pdf,51-7.2 The Resource Manager,"7.2. The Resource Manager 493\n7.2 The Resource Manager\nEvery game is constructed from a wide variety of resources (sometimes called\nassetsormedia). Examples include meshes, materials, textures, shader pro-\ngrams, animations, audio clips, level layouts, collision primitives, physics pa-\nrameters, and the list goes on. A game’s resources must be managed, both in\nterms of the offline tools used to create them, and in terms of loading, unload-\ning and manipulating them at runtime. Therefore, every game engine has a\nresourcemanager of some kind.\nEvery resource manager is comprised of two distinct but integrated com-\nponents. One component manages the chain of offline tools used to create the\nassetsandtransformthemintotheirengine-readyform. Theothercomponent\nmanages the resources at runtime, ensuring that they are loaded into memory\nin advance of being needed by the game and making sure they are unloaded\nfrom memory when no longer needed.\nIn some engines, the resource manager is a cleanly designed, unified, cen-\ntralized subsystem that manages all types of resources used by the game. In\notherengines,theresourcemanagerdoesn’texistasasinglesubsystemperse,\nbut rather is spread across a disparate collection of subsystems, perhaps writ-\nten by different individuals at various times over the engine’s long and some-\ntimes colorful history. But no matter how it is implemented, a resource man-\nager invariably takes on certain responsibilities and solves a well-understood\nsetofproblems. Inthissection,we’llexplorethefunctionalityandsomeofthe\nimplementation details of a typical game engine resource manager.\n7.2.1 Ofﬂine Resource Management and the Tool Chain\n7.2.1.1 Revision Control for Assets\nOn a small game project, the game’s assets can be managed by keeping loose\nfiles sitting around on a shared network drive with an ad hoc directory struc-\nture. This approach is not feasible for a modern commercial 3D game, com-\nprised of a massive number and variety of assets. For such a project, the team\nrequires a more formalized way to track and manage its assets.\nSome game teams use a source code revision control system to manage\ntheir resources. Art source files (Maya scenes, Photoshop PSD files, Illustrator\nfiles, etc.) are checked in to Perforce or a similar package by the artists. This\napproach works reasonably well, although some game teams build custom\nassetmanagementtoolstohelpflattenthelearningcurvefortheirartists. Such\ntools may be simple wrappers around a commercial revision control system,\nor they might be entirely custom.\n494 7. Resources and the File System\nDealing with Data Size\nOne of the biggest problems in the revision control of art assets is the sheer\namount of data. Whereas C++ and script source code files are small, relative\nto their impact on the project, art files tend to be much, much larger. Because\nmanysourcecontrolsystemsworkbycopyingfilesfromthecentralrepository\ndown to the user’s local machine, the sheer size of the asset files can render\nthese packages almost entirely useless.\nI’ve seen a number of different solutions to this problem employed at var-\nious studios. Some studios turn to commercial revision control systems like\nAlienbrainthathavebeenspecificallydesignedtohandleverylargedatasizes.\nSome teams simply “take their lumps” and allow their revision control tool\nto copy assets locally. This can work, as long as your disks are big enough\nand your network bandwidth sufficient, but it can also be inefficient and slow\nthe team down. Some teams build elaborate systems on top of their revision\ncontrol tool to ensure that a particular end user only gets local copies of the\nfiles he or she actually needs. In this model, the user either has no access\nto the rest of the repository or can access it on a shared network drive when\nneeded.\nAt Naughty Dog we use a proprietary tool that makes use of UNIX sym-\nbolic links to virtually eliminate data copying, while permitting each user to\nhave a complete local view of the asset repository. As long as a file is not\nchecked out for editing, it is a symlink to a master file on a shared network\ndrive. A symbolic link occupies very little space on the local disk, because it\nis nothing more than a directory entry. When the user checks out a file for\nediting, the symlink is removed, and a local copy of the file replaces it. When\nthe user is done editing and checks the file in, the local copy becomes the new\nmaster copy, its revision history is updated in a master database, and the local\nfile turns back into a symlink. This system works very well, but it requires\nthe team to build their own revision control system from scratch; I am un-\naware of any commercial tool that works like this. Also, symbolic links are a\nUNIX feature—such a tool could probably be built with Windows junctions\n(the Windows equivalent of a symbolic link), but I haven’t seen anyone try it\nas yet.\n7.2.1.2 The Resource Database\nAs we’ll explore in depth in the next section, most assets are not used in their\noriginal format by the game engine. They need to pass through some kind\nof asset conditioning pipeline, whose job it is to convert the assets into the\nbinary format needed by the engine. For every resource that passes through\n7.2. The Resource Manager 495\ntheassetconditioningpipeline,thereissomeamountof metadata thatdescribes\nhowthat resource should be processed. When compressing a texture bitmap,\nwe need to know what typeof compression best suits that particular image.\nWhenexportingananimation,weneedtoknowwhatrangeofframesinMaya\nshould be exported. When exporting character meshes out of a Maya scene\ncontaining multiple characters, we need to know which mesh corresponds to\nwhich character in the game.\nTo manage all of this metadata, we need some kind of database. If we are\nmaking a very small game, this database might be housed in the brains of\nthe developers themselves. I can hear them now: “Remember: the player’s\nanimations need to have the ‘flip X’ flag set, but the other characters must not\nhave it set…or…rats…is it the other way around?”\nClearly for any game of respectable size, we simply cannot rely on the\nmemoriesofourdevelopersinthismanner. Foronething,thesheervolumeof\nassets becomes overwhelming quite quickly. Processing individual resource\nfiles by hand is also far too time-consuming to be practical on a full-fledged\ncommercial game production. Therefore, every professional game team has\nsome kind of semiautomated resource pipeline, and the data that drive the\npipeline is stored in some kind of resourcedatabase.\nThe resour ce database takes on vastly different forms in different game\nengines. In one engine, the metadata describing how a resource should be\nbuilt might be embedded into the source assets themselves (e.g., it might be\nstored as so-called blind data within a Maya file). In another engine, each\nsource resource file might be accompanied by a small text file that describes\nhow it should be processed. Still other engines encode their resource build-\ning metadata in a set of XML files, perhaps wrapped in some kind of custom\ngraphicaluserinterface. Someenginesemployatruerelationaldatabase,such\nasMicrosoftAccess, MySQLorconceivablyevenaheavyweightdatabaselike\nOracle.\nWhatever its form, a resource database must provide the following basic\nfunctionality:\n• The ability to deal with multiple typesof resources, ideally (but certainly\nnot necessarily) in a somewhat consistent manner.\n• The ability to create new resources.\n• The ability to delete resources.\n• The ability to inspect and modify existing resources.\n• The ability to move a resource’s source file(s) from one location to an-\nother on-disk. (This is very helpful because artists and game designers\n496 7. Resources and the File System\noften need to rearrange assets to reflect changing project goals, rethink-\ning of game designs, feature additions and cuts, etc.)\n• The ability of a resource to cross-reference other resources (e.g., the ma-\nterialusedbyamesh,orthecollectionofanimationsneededbylevel17).\nThesecross-referencestypicallydriveboththeresourcebuildingprocess\nand the loading process at runtime.\n• The ability to maintain referential integrity of all cross-references within\nthe database and to do so in the face of all common operations such as\ndeleting or moving resources around.\n• The ability to maintain a revision history, complete with a log of who\nmade each change and why.\n• It is also very helpful if the resource database supports searching or\nqueryinginvariousways. Forexample,adevelopermightwanttoknow\nin which levels a particular animation is used or which textures are ref-\nerenced by a set of materials. Or they might simply be trying to find a\nresource whose name momentarily escapes them.\nIt should be pretty obvious from looking at the above list that creating a\nreliable and robust resource database is no small task. When designed well\nand implemented properly, the resource database can quite literally make the\ndifferencebetweenateamthatshipsahitgameandateamthatspinsitswheels\nfor 18 months before being forced by management to abandon the project (or\nworse). I know this to be true, because I’ve personally experienced both.\n7.2.1.3 Some Successful Resource Database Designs\nEvery game team will have different requirements and make different deci-\nsions when designing their resource database. However, for what it’s worth,\nhere are some designs that have worked well in my own experience.\nUnreal Engine 4\nUnreal’sresourcedatabaseismanagedbytheirüber-tool,UnrealEd. UnrealEd\nisresponsibleforliterallyeverything,fromresourcemetadatamanagementto\nasset creation to level layout and more. UnrealEd has its drawbacks, but its\nsingle biggest benefit is that UnrealEd is a part of the game engine itself. This\npermits assets to be created and then immediately viewed in their full glory,\nexactly as they will appear in-game. The game can even be run from within\nUnrealEdinordertovisualizetheassetsintheirnaturalsurroundingsandsee\nif and how they work in-game.\n7.2. The Resource Manager 497\nFigure 7.1. UnrealEd’s Generic Browser.\nAnotherbigbenefitofUnrealEdiswhatIwouldcall one-stopshopping. Un-\nrealEd’s Generic Browser (depicted in Figure 7.1) allows a developer to access\nliterally every resource that is consumed by the engine. Having a single, uni-\nfied and reasonably consistent interface for creating and managing all types\nof resources is a big win. This is especially true considering that the resource\ndata in most other game engines is fragmented across countless inconsistent\nand often cryptic tools. Just being able to findany resource easily in UnrealEd\nis a big plus.\nUnreal can be less error-prone than many other engines, because assets\nmust be explicitly imported into Unreal’s resource database. This allows re-\nsourcestobecheckedforvalidityveryearlyintheproductionprocess. Inmost\ngameengines,anyolddatacanbethrownintotheresourcedatabase,andyou\nonly know whether or not that data is valid when it is eventually built—or\nsometimes not until it is actually loaded into the game at runtime. But with\nUnreal, assets can be validated as soon as they are imported into UnrealEd.\nThis means that the person who created the asset gets immediate feedback as\nto whether his or her asset is configured properly.\nOf course, Unreal’s approach has some serious drawbacks. For one thing,\n498 7. Resources and the File System\nall resource data is stored in a small number of large package files. These files\nare binary, so they are not easily merged by a revision control package like\nCVS, Subversion or Perforce. This presents some major problems when more\nthan one user wants to modify resources that reside in a single package. Even\nif the users are trying to modify different resources, only one user can lock the\npackageatatime, sotheotherhastowait. Theseverityofthis problemcanbe\nreduced by dividing resources into relatively small, granular packages, but it\ncannot practically be eliminated.\nReferential integrity is quite good in UnrealEd, but there are still some\nproblems. When a resource is renamed or moved around, all references to\nit are maintained automatically using a dummy object that remaps the old re-\nsource to its new name/location. The problem with these dummy remapping\nobjects is that they hang around and accumulate and sometimes cause prob-\nlems, especially if a resource is deleted. Overall, Unreal’s referential integrity\nis quite good, but it is not perfect.\nDespite its problems, UnrealEd is by far the most user-friendly, well-in-\ntegrated and streamlined asset creation toolkit, resource database and asset\nconditioning pipeline that I have ever worked with.\nNaughty Dog’s Engine\nForUncharted: Drake’s Fortune, Naughty Dog stored its resource metadata in\na MySQL database. A custom graphical user interface was written to man-\nage the contents of the database. This tool allowed artists, game designers\nand programmers alike to create new resources, delete existing resources and\ninspect and modify resources as well. This GUI was a crucial component of\nthe system, because it allowed users to avoid having to learn the intricacies of\ninteracting with a relational database via SQL.\nThe original MySQL database used on Uncharted did not provide a use-\nful history of the changes made to the database, nor did it provide a good\nway to roll back “bad” changes. It also did not support multiple users edit-\ning the same resource, and it was difficult to administer. Naughty Dog has\nsince moved away from MySQL in favor of an XML file-based asset database,\nmanaged under Perforce.\nBuilder, Naughty Dog’s resource database GUI, is depicted in Figure 7.2.\nThe window is broken into two main sections: a tree view showing all re-\nsourcesinthegameontheleftandapropertieswindowontheright,allowing\nthe resource(s) that are selected in the tree view to be viewed and edited. The\nresource tree contains folders for organizational purposes, so that the artists\nand game designers can organize their resources in any way they see fit. Vari-\nous types of resources can be created and managed within any folder, includ-\n7.2. The Resource Manager 499\nFigure 7.2. The front-end GUI for Naughty Dog’s ofﬂine resource database, Builder.\ning actors and levels, and the various subresources that comprise them (pri-\nmarily meshes, skeletons and animations). Animations can also be grouped\ninto pseudo-folders known as bundles. This allows large groups of anima-\ntions to be created and then managed as a unit, and prevents a lot of wasted\ntime dragging individual animations around in the tree view.\nThe asset conditioning pipeline employed on the Uncharted andThe Last\nof Usseries consists of a set of resource exporters, compilers and linkers that\nare run from the command line. The engine is capable of dealing with a wide\nvariety of different kinds of data objects, but these are packaged into one of\ntwo types of resource files: actors and levels. An actor can contain skeletons,\nmeshes, materials, textures and/or animations. A level contains static back-\nground meshes, materials and textures, and also level-layout information. To\n500 7. Resources and the File System\nbuild an actor, one simply types baname-of-actor on the command line; to\nbuild a level, one types blname-of-level. These command-line tools query the\ndatabase to determine exactly howto build the actor or level in question. This\nincludes information on how to export the assets from DCC tools like Maya\nand Photoshop, how to process the data, and how to package it into binary\n.pakfiles that can be loaded by the game engine. This is much simpler than\ninmanyengines,whereresourceshavetobe exportedmanually bytheartists—a\ntime-consuming, tedious and error-prone task.\nThebenefitsoftheresourcepipelinedesignusedbyNaughtyDoginclude:\n•Granular resources. Resources can be manipulated in terms of logical en-\ntities in the game—meshes, materials, skeletons and animations. These\nresource types are granular enough that the team almost never has con-\nflicts in which two users want to edit the same resource simultaneously.\n•Thenecessaryfeatures(andnomore) . TheBuilder tool providesapowerful\nset of features that meet the needs of the team, but Naughty Dog didn’t\nwaste any resources creating features they didn’t need.\n•Obviousmappingtosourcefiles . A user can very quickly determine which\nsource assets (native DCC files, like Maya .ma files or photoshop .psd\nfiles) make up a particular resource.\n•Easy to change how DCC data is exported and processed . Just click on the\nresource in question and twiddle its processing properties within the re-\nsource database GUI.\n•Easytobuildassets. Just type baorblfollowed by the resource name on\nthe command line. The dependency system takes care of the rest.\nOf course, Naughty Dog’s tool chain does have some drawbacks as well, in-\ncluding:\n•Lack of visualization tools. The only way to preview an asset is to load\nit into the game or the model/animation viewer (which is really just a\nspecial mode of the game itself).\n•Thetoolsaren’tfullyintegrated. Naughty Dog uses one tool to lay out lev-\nels,anothertomanagethemajorityofresourcesintheresourcedatabase,\nandathirdtosetupmaterialsandshaders(thisisnotpartoftheresource\ndatabase front end). Building the assets is done on the command line. It\nmight be a bit more convenient if all of these functions were to be inte-\ngratedintoasingletool. However,NaughtyDoghasnoplanstodothis,\nbecause the benefit would probably not outweigh the costs involved.\n7.2. The Resource Manager 501\nOGRE’s Resource Manager System\nOGREisarenderingengine, notafull-fledgedgameengine. Thatsaid, OGRE\ndoes boast a reasonably complete and very well-designed runtime resource\nmanager. A simple, consistent interface is used to load virtually any kind of\nresource. And the system has been designed with extensibility in mind. Any\nprogrammer can quite easily implement a resource manager for a brand new\nkind of asset and integrate it easily into OGRE’s resource framework.\nOne of the drawbacks of OGRE’s resource manager is that it is a runtime-\nonly solution. OGRE lacks any kind of offline resource database. OGRE does\nprovide some exporters that are capable of converting a Maya file into a mesh\nthat can be used by OGRE (complete with materials, shaders, a skeleton and\noptional animations). However, the exporter must be run manually from\nwithin Maya itself. Worse, all of the metadata describing how a particular\nMaya file should be exported and processed must be entered by the user do-\ning the export.\nIn summary, OGRE’s runtime resource manager is powerful and well-\ndesigned. But,OGREwouldbenefitagreatdealfromanequallypowerfuland\nmodern resource database and asset conditioning pipeline on the tools side.\nMicrosoft’s XNA\nXNAisagamedevelopmenttoolkitbyMicrosoft,targetedatthePCandXbox\n360 platforms. Although it was retired by Microsoft in 2014, it’s still a good\nresource for learning about game engines. XNA’s resource management sys-\ntem is unique, in that it leverages the project management and build systems\nof the Visual Studio IDE to manage and build the assets in the game as well.\nXNA’s game development tool, Game Studio Express, is just a plug-in to Vi-\nsual Studio Express.\n7.2.1.4 The Asset Conditioning Pipeline\nIn Section 1.7, we learned that resource data is typically created using ad-\nvanced digital content creation (DCC) tools like Maya, ZBrush, Photoshop or\nHoudini. However, the data formats used by these tools are usually not suit-\nablefordirectconsumptionbyagameengine. Sothemajorityofresourcedata\nis passed through an asset conditioning pipeline (ACP) on its way to the game\nengine. The ACP is sometimes referred to as the resource conditioning pipeline\n(RCP), or simply the tool chain .\nEvery resource pipeline starts with a collection of source assets in native\nDCC formats (e.g., Maya .ma or .mb files, Photoshop .psd files, etc.). These\n502 7. Resources and the File System\nassetsaretypicallypassedthroughthreeprocessingstagesontheirway tothe\ngame engine:\n1.Exporters. We need some way of getting the data out of the DCC’s na-\ntive format and into a format that we can manipulate. This is usually\naccomplished by writing a custom plug-in for the DCC in question. It\nis the plug-in’s job to export the data into some kind of intermediate file\nformat that can be passed to later stages in the pipeline. Most DCC ap-\nplications provide a reasonably convenient mechanism for doing this.\nMaya actually provides three: a C++ SDK, a scripting language called\nMEL and most recently a Python interface as well.\nIn cases where a DCC application provides no customization hooks, we\ncan always save the data in one of the DCC tool’s native formats. With\nany luck, one of these will be an open format, a reasonably intuitive text\nformat, or some other format that we can reverse engineer. Presum-\ning this is the case, we can pass the file directly to the next stage of the\npipeline.\n2.Resource compilers. We often have to “massage” the raw data exported\nfrom a DCC application in various ways in order to make them game-\nready. For example, we might need to rearrange a mesh’s triangles into\nstrips, or compress a texture bitmap, or calculate the arc lengths of the\nsegments of a Catmull-Rom spline. Not all types of resources need to\nbe compiled—some might be game-ready immediately upon being ex-\nported.\n3.Resource linkers. Multiple resource files sometimes need to be combined\ninto a single useful package prior to being loaded by the game engine.\nThis mimics the process of linking together the object files of a compiled\nC++ program into an executable file, and so this process is sometimes\ncalledresource linking . For example, when building a complex compos-\nite resource like a 3D model, we might need to combine the data from\nmultiple exported mesh files, multiple material files, a skeleton file and\nmultipleanimationfilesintoasingleresource. Notalltypesofresources\nneed to be linked—some assets are game-ready after the export or com-\npile steps.\nResource Dependencies and Build Rules\nMuch like compiling the source files in a C or C++ project and then linking\nthem into an executable, the asset conditioning pipeline processes source as-\nsets (in the form of Maya geometry and animation files, Photoshop PSD files,\n7.2. The Resource Manager 503\nraw audio clips, text files, etc.), converts them into game-ready form and then\nlinks them together into a cohesive whole for use by the engine. And just like\nthe source files in a computer program, game assets often have interdepen-\ndencies. (For example, a mesh refers to one or more materials, which in turn\nrefer to various textures.) These interdependencies typically have an impact\non the order in which assets must be processed by the pipeline. (For example,\nwe might need to build a character’s skeleton before we can process any of\nthatcharacter’sanimations.) Inaddition,thedependenciesbetweenassetstell\nus which assets need to be rebuilt when a particular source asset changes.\nBuild dependencies revolve not only around changes to the assets them-\nselves, but also around changes to data formats. If the format of the files used\nto store triangle meshes changes, for instance, all meshes in the entire game\nmay need to be reexported and/or rebuilt. Some game engines employ data\nformats that are robust to version changes. For example, an asset may contain\na version number, and the game engine may include code that “knows” how\nto load and make use of legacy assets. The downside of such a policy is that\nasset files and engine code tend to become bulky. When data format changes\nare relatively rare, it may be better to just bite the bullet and reprocess all the\nfiles when format changes do occur.\nEvery asset conditioning pipeline requires a set of rules that describe the\ninterdependenciesbetweentheassets,andsomekindofbuildtoolthatcanuse\nthis information to ensure that the proper assets are built, in the proper order,\nwhenasourceassetismodified. Somegameteamsrolltheirownbuildsystem.\nOthers use an established tool, such as make. Whatever solution is selected,\nteams should treat their build dependency system with utmost care. If you\ndon’t,changestosources,assetsmaynottriggertheproperassetstoberebuilt.\nTheresultcanbeinconsistentgameassets,whichmayleadtovisualanomalies\nor even engine crashes. In my personal experience, I’ve witnessed countless\nhours wasted in tracking down problems that could have been avoided had\nthe asset interdependencies been properly specified and the build system im-\nplemented to use them reliably.\n7.2.2 Runtime Resource Management\nLet us turn our attention now to how the assets in our resource database are\nloaded, managed and unloaded within the engine at runtime.\n7.2.2.1 Responsibilities of the Runtime Resource Manager\nA game engine’s runtime resource manager takes on a wide range of respon-\nsibilities, all related to its primary mandate of loading resources into memory:\n504 7. Resources and the File System\n• Ensures that only one copy of each unique resource exists in memory at\nany given time.\n• Manages the lifetimeof each resource.\n•Loadsneeded resources and unloads resources that are no longer needed.\n• Handles loading of composite resources . A composite resource is a re-\nsource comprised of other resources. For example, a 3D model is a com-\nposite resource that consists of a mesh, one or more materials, one or\nmore textures and optionally a skeleton and multiple skeletal anima-\ntions.\n• Maintains referential integrity. This includes internal referential integrity\n(cross-references within a single resource) and external referential in-\ntegrity(cross-referencesbetweenresources). Forexample,amodelrefers\nto its mesh and skeleton; a mesh refers to its materials, which in turn re-\nfer to texture resources; animations refer to a skeleton, which ultimately\nties them to one or more models. When loading a composite resource,\nthe resource manager must ensure that all necessary subresources are\nloaded, and it must patch in all of the cross-references properly.\n• Manages the memory usage of loaded resources and ensures that re-\nsources are stored in the appropriate place(s) in memory.\n• Permits customprocessing to be performed on a resource after it has been\nloaded, on a per-resource-type basis. This process is sometimes known\nasloggingin orload-initializing the resource.\n• Usually(butnotalways)providesasingle unifiedinterface throughwhich\nawidevarietyofresourcetypescanbemanaged. Ideallyaresourceman-\nagerisalsoeasilyextensible,sothatitcanhandlenewtypesofresources\nas they are needed by the game development team.\n• Handles streaming (i.e., asynchronous resource loading), if the engine\nsupports this feature.\n7.2.2.2 Resource File and Directory Organization\nInsomegameengines(typicallyPCengines),eachindividualresourceisman-\naged in a separate “loose” file on-disk. These files are typically contained\nwithin a tree of directories whose internal organization is designed primar-\nily for the convenience of the people creating the assets; the engine typically\ndoesn’t care where resource files are located within the resource tree. Here’s a\ntypical resource directory tree for a hypothetical game called Space Evaders:\n7.2. The Resource Manager 505\nSpaceEvaders Root directory for entire game.\nResources Root of all resources.\nNPC Non-player character models and animations.\nPirate Models and animations for pirates.\nMarine Models and animations for marines.\n...\nPlayer Player character models and animations.\nWeapons Models and animations for weapons.\nPistol Models and animations for the pistol.\nRifle Models and animations for the rifle.\nBFG Models and animations for the big…uh…gun.\n...\nLevels Background geometry and level layouts.\nLevel1 First level’s resources.\nLevel2 Second level’s resources.\n...\nObjects Miscellaneous 3D objects.\nCrate The ubiquitous breakable crate.\nBarrel The ubiquitous exploding barrel.\nOther engines package multiple resources together in a single file, such as\na ZIP archive, or some other composite file (perhaps of a proprietary format).\nThe primary benefit of this approach is improved load times. When loading\ndatafromfiles, thethreebiggestcostsare seektimes (i.e., movingthereadhead\nto the correct place on the physical media), the time required to open each\nindividual file, and the time to read the data from the file into memory. Of\nthese, the seek times and file-open times can be nontrivial on many operating\nsystems. When a single large file is used, all of these costs are minimized. A\nsingle file can be organized sequentially on the disk, reducing seek times to\na minimum. And with only one file to open, the cost of opening individual\nresource files is eliminated.\nSolid-state drives (SSD) do not suffer from the seek time problems that\nplague spinning media like DVDs, Blu-ray discs and hard disc drives (HDD).\nHowever, no game console to date includes a solid-state drive as the primary\nfixed storage device (not even the PS4 and Xbox One). So designing your\ngame’s I/O patterns in order to minimize seek times is likely to be a neces-\nsity for some time to come.\nTheOGRErenderingengine’sresourcemanagerpermitsresourcestoexist\n506 7. Resources and the File System\nasloosefilesondisk,orasvirtualfileswithinalargeZIParchive. Theprimary\nbenefits of the ZIP format are the following:\n1.It is an open format . The zlibandzziplib libraries used to read and\nwrite ZIP archives are freely available. The zlib SDK is totally free (see\nhttp://www.zlib.net), while the zziplib SDK falls under the Lesser Gnu\nPublic License (LGPL) (see http://zziplib.sourceforge.net).\n2.The virtual files within a ZIP archive “remember” their relative paths. This\nmeans that a ZIP archive “looks like” a raw file system for most in-\ntentsandpurposes. TheOGREresourcemanageridentifiesallresources\nuniquely via strings that appear to be file system paths. However, these\npaths sometimes identify virtual files within a ZIP archive instead of\nloose files on disk, and a game programmer needn’t be aware of the dif-\nference in most situations.\n3.ZIP archives may be compressed . This reduces the amount of disk space\noccupied by resources. But, more importantly, it again speeds up load\ntimes, aslessdataneedbeloadedintomemoryfromthefixeddisk. This\nis especially helpful when reading data from a DVD-ROM or Blu-ray\ndisk, as the data transfer rates of these devices are much slower than a\nhard disk drive. Hence the cost of decompressing the data after it has\nbeen loaded into memory is often more than offset by the time saved in\nloading less data from the device.\n4.ZIP archives are modular. Resources can be grouped together into a ZIP\nfile and managed as a unit. One particularly elegant application of this\nidea is in product localization. All of the assets that need to be localized\n(suchasaudioclipscontainingdialogueandtexturesthatcontainwords\nor region-specific symbols) can be placed in a single ZIP file, and then\ndifferentversionsofthisZIPfilecanbegenerated,oneforeachlanguage\nor region. To run the game for a particular region, the engine simply\nloads the corresponding version of the ZIP archive.\nThe Unreal Engine takes a similar approach, with a few important differ-\nences. In Unreal, all resources must be contained within large composite files\nknown as packages (a.k.a. “pak files”). No loose disk files are permitted. The\nformat of a package file is proprietary. The Unreal Engine’s game editor, Un-\nrealEd, allows developers to create and manage packages and the resources\nthey contain.\n7.2. The Resource Manager 507\n7.2.2.3 Resource File Formats\nEach type of resource file potentially has a different format. For example, a\nmesh file is always stored in a different format than that of a texture bitmap.\nSome kinds of assets are stored in standardized, open formats. For example,\ntextures are typically stored as Targa files (TGA), Portable Network Graphics\nfiles(PNG),TaggedImageFileFormatfiles(TIFF),JointPhotographicExperts\nGroupfiles(JPEG)orWindowsBitmapfiles(BMP)—orinastandardizedcom-\npressed format such as DirectX’s S3 Texture Compression family of formats\n(S3TC, also known as DXT nor DXTC). Likewise, 3D mesh data is often ex-\nported out of a modeling tool like Maya or Lightwave into a standardized\nformat such as OBJ or COLLADA for consumption by the game engine.\nSometimes a single file format can be used to house many different types\nof assets. For example, the Granny SDK by Rad Game Tools (http://www.\nradgametools.com) implements a flexible open file format that can be used to\nstore 3D mesh data, skeletal hierarchies and skeletal animation data. (In fact\nthe Granny file format can be easily repurposed to store virtually any kind of\ndata imaginable.)\nMany game engine programmers roll their own file formats for various\nreasons. This might be necessary if no standardized format provides all of\nthe information needed by the engine. Also, many game engines endeavor to\ndo as much offline processing as possible in order to minimize the amount of\ntime needed to load and process resource data at runtime. If the data needs to\nconform to a particular layout in memory, for example, a raw binary format\nmight be chosen so that the data can be laid out by an offline tool (rather than\nattempting to format it at runtime after the resource has been loaded).\n7.2.2.4 Resource GUIDs\nEvery resource in a game must have some kind of globally unique identifier\n(GUID). The most common choice of GUID is the resource’s file system path\n(stored either as a string or a 32-bit hash). This kind of GUID is intuitive, be-\ncause it clearly maps each resource to a physical file on-disk. And it’s guaran-\nteedtobeuniqueacrosstheentiregame,becausetheoperatingsystemalready\nguarantees that no two files will have the same path.\nHowever, a file system path is by no means the only choice for a resource\nGUID. Some engines use a less-intuitive type of GUID, such as a 128-bit hash\ncode,perhapsassignedbyatoolthatguaranteesuniqueness. Inotherengines,\nusing a file system path as a resource identifier is infeasible. For example, Un-\nreal Engine stores many resources in a single large file known as a package,\nso the path to the package file does not uniquely identify any one resource.\nTo overcome this problem, an Unreal package file is organized into a folder\n508 7. Resources and the File System\nhierarchy containing individual resources. Unreal gives each individual re-\nsource within a package a unique name, which looks much like a file system\npath. So in Unreal, a resource GUID is formed by concatenating the (unique)\nname of the package file with the in-package path of the resource in question.\nFor example, the Gears of War resource GUID Locust_Boomer.Physical-\nMaterials.LocustBoomerLeather identifies a material called Locust -\nBoomerLeather within the PhysicalMaterials folder of the Locust_-\nBoomer package file.\n7.2.2.5 The Resource Registry\nIn order to ensure that only one copy of each unique resource is loaded into\nmemory at any given time, most resource managers maintain some kind of\nregistry of loaded resources. The simplest implementation is a dictionary —i.e.,\nacollectionof key-valuepairs. Thekeyscontaintheuniqueidsoftheresources,\nwhile the values are typically pointers to the resources in memory.\nWhenever a resource is loaded into memory, an entry for it is added to the\nresource registry dictionary, using its GUID as the key. Whenever a resource\nis unloaded, its registry entry is removed. When a resource is requested by\nthe game, the resource manager looks up the resource by its GUID within the\nresource registry. If the resource can be found, a pointer to it is simply re-\nturned. If the resource cannot be found, it can either be loaded automatically\nor a failure code can be returned.\nAt first blush, it might seem most intuitive to automatically load a re-\nquested resource if it cannot be found in the resource registry. And in fact,\nsome game engines do this. However, there are some serious problems with\nthis approach. Loading a resource is a slow operation, because it involves lo-\ncating and opening a file on disk, reading a potentially large amount of data\nintomemory(fromapotentiallyslowdevicelikeaDVD-ROMdrive),andalso\npossibly performing post-load initialization of the resource data once it has\nbeen loaded. If the request comes during active gameplay, the time it takes to\nloadtheresourcemightcauseaverynoticeablehitchinthegame’sframerate,\nor even a multi-second freeze. For this reason, engines tend to take one of two\nalternative approaches:\n1. Resource loading might be disallowed completely during active game-\nplay. In this model, all of the resources for a game level are loaded en\nmassejust prior to gameplay, usually while the player watches a loading\nscreen or progress bar of some kind.\n2. Resource loading might be done asynchronously (i.e., the data might be\nstreamed ). In this model, while the player is engaged in level X, the re-\n7.2. The Resource Manager 509\nsources for level Y are being loaded in the background. This approach\nis preferable because it provides the player with a load-screen-free play\nexperience. However, it is considerably more difficult to implement.\n7.2.2.6 Resource Lifetime\nThelifetimeof a resource is defined as the time period between when it is first\nloaded into memory and when its memory is reclaimed for other purposes.\nOne of the resource manager’s jobs is to manage resource lifetimes—either\nautomatically or by providing the necessary API functions to the game, so it\ncan manage resource lifetimes manually.\nEach resource has its own lifetime requirements:\n• Some resources must be loaded when the game first starts up and must\nstay resident in memory for the entire duration of the game. That is,\ntheir lifetimes are effectively infinite. These are sometimes called global\nresources orglobalassets. Typical examples include the player character’s\nmesh, materials, textures and core animations, textures and fonts used\non the heads-up display, and the resources for all of the standard-issue\nweapons used throughout the game. Any resource that is visible or au-\ndible to the player throughout the entire game (and cannot be loaded on\nthe fly when needed) should be treated as a global resource.\n• Other resources have a lifetime that matches that of a particular game\nlevel. These resources must be in memory by the time the level is first\nseen by the player and can be dumped once the player has permanently\nleft the level.\n• Someresourcesmighthavealifetimethatisshorterthanthedurationof\nthe level in which they are found. For example, the animations and au-\ndio clips that make up an in-gamecinematic (a mini-movie that advances\nthe story or provides the player with important information) might be\nloaded in advance of the player seeing the cinematic and then dumped\nonce the cinematic has played.\n• Some resources like background music, ambient sound effects or full-\nscreen movies are streamed “live” as they play. The lifetime of this kind\nof resource is difficult to define, because each byte only persists in mem-\nory for a tiny fraction of a second, but the entire piece of music sounds\nlike it lasts for a long period of time. Such assets are typically loaded in\nchunks of a size that matches the underlying hardware’s requirements.\nFor example, a music track might be read in 4 KiB chunks, because that\nmight be the buffer size used by the low-level sound system. Only two\n510 7. Resources and the File System\nEvent ABCDE\nInitial state 00000\nLevel X counts incremented 11100\nLevel X loads (1)(1)(1)00\nLevel X plays 11100\nLevel Y counts incremented 12211\nLevel X counts decremented 01111\nLevel X unloads, level Y loads (0)11(1)(1)\nLevel Y plays 01111\nTable 7.2. Resource usage as two levels load and unload.\nchunks are ever present in memory at any given moment—the chunk\nthat is currently playing and the chunk immediately following it that is\nbeing loaded into memory.\nThe question of when to load a resource is usually answered quite easily,\nbased on knowledge of when the asset is first seen by the player. However,\nthe question of when to unload a resource and reclaim its memory is not so\neasily answered. The problem is that many resources are shared across multi-\nple levels. We don’t want to unload a resource when level X is done, only to\nimmediately reload it because level Y needs the same resource.\nOnesolutiontothisproblemistoreference-counttheresources. Whenever\na new game level needs to be loaded, the list of all resources used by that\nlevel is traversed, and the reference count for each resource is incremented\nby one (but they are not loaded yet). Next, we traverse the resources of any\nunneeded levels and decrement their reference counts by one; any resource\nwhose reference count drops to zero is unloaded. Finally, we run through the\nlist of all resources whose reference count just went from zero to one and load\nthose assets into memory.\nForexample, imaginethatlevelXusesresourcesA,BandC,andthatlevel\nY uses resources B, C, D and E. (B and C are shared between both levels.) Ta-\nble 7.2 shows the reference counts of these five resources as the player plays\nthrough levels X and Y. In this table, reference counts are shown in boldface\ntype to indicate that the corresponding resource actually exists in memory,\nwhile a grey background indicates that the resource is not in memory. A ref-\nerence count in parentheses indicates that the corresponding resource data is\nbeing loaded or unloaded.\n7.2. The Resource Manager 511\n7.2.2.7 Memory Management for Resources\nResource management is closely related to memory management, because we\nmust inevitably decide wherethe resources should end up in memory once\nthey have been loaded. The destination of every resource is not always the\nsame. For one thing, certain types of resources must reside in video RAM\n(or, on the PlayStation 4, in a memory block that has been mapped for access\nvia the high-speed “garlic” bus). Typical examples include textures, vertex\nbuffers, index buffers and shader code. Most other resources can reside in\nmain RAM, but different kinds of resources might need to reside within dif-\nferent address ranges. For example, a resource that is loaded and stays resi-\ndent for the entire game (global resources) might be loaded into one region of\nmemory, while resources that are loaded and unloaded frequently might go\nsomewhere else.\nThe design of a game engine’s memory allocation subsystem is usually\nclosely tied to that of its resource manager. Sometimes we will design the re-\nsource manager to take best advantage of the types of memory allocators we\nhave available, or vice versa—we may design our memory allocators to suit\nthe needs of the resource manager.\nAs we saw in Section 6.2.1.4, one of the primary problems facing any re-\nsource management system is the need to avoid fragmenting memory as re-\nsources are loaded and unloaded. We’ll discuss a few of the more-common\nsolutions to this problem below.\nHeap-Based Resource Allocation\nOne approach is to simply ignore memory fragmentation issues and use a\ngeneral-purpose heap allocator to allocate your resources (like the one imple-\nmented by malloc() in C, or the global newoperator in C++). This works\nbest if your game is only intended to run on personal computers, on operat-\ning systems that support virtual memory allocation. On such a system, phys-\nical memory will become fragmented, but the operating system’s ability to\nmap noncontiguous pages of physical RAM into a contiguous virtual mem-\nory space helps to mitigate some of the effects of fragmentation.\nIf your game is running on a console with limited physical RAM and only\na rudimentary virtual memory manager (or none whatsoever), then fragmen-\ntation will become a problem. In this case, one alternative is to defragment\nyour memory periodically. We saw how to do this in Section 6.2.2.2.\n512 7. Resources and the File System\nStack-Based Resource Allocation\nA stack allocator does not suffer from fragmentation problems, because mem-\nory is allocated contiguously and freed in an order opposite to that in which it\nwas allocated. A stack allocator can be used to load resources if the following\ntwo conditions are met:\n• The game is linear and level-centric (i.e., the player watches a loading\nscreen, then plays a level, then watches another loading screen, then\nplays another level).\n• Each level fits into memory in its entirety.\nPresumingthattheserequirementsaresatisfied,wecanuseastackallocatorto\nload resources as follows: When the game first starts up, the global resources\nareallocatedfirst. Thetopofthestackisthenmarked,sothatwecanfreeback\nto this position later. To load a level, we simply allocate its resources on the\ntop of the stack. When the level is complete, we can simply set the stack top\nback to the marker we took earlier, thereby freeing all of the level’s resources\nin one fell swoop without disturbing the global resources. This process can be\nrepeated for any number of levels, without ever fragmenting memory. Fig-\nure 7.3 illustrates how this is accomplished.\nAdouble-endedstackallocatorcanbeusedtoaugmentthisapproach. Two\nstacksaredefinedwithinasinglelargememoryblock. Onegrowsupfromthe\nbottomofthememoryarea,whiletheothergrowsdownfromthetop. Aslong\nas the two stacks never overlap, the stacks can trade memory resources back\nandforthnaturally—somethingthatwouldn’tbepossibleifeachstackresided\nin its own fixed size block.\nOnHydro Thunder, Midway used a double-ended stack allocator. The\nlower stack was used for persistent data loads, while the upper was used for\ntemporary allocations that were freed every frame. Another way a double-\nended stack allocator can be used is to ping-pong level loads. Such an ap-\nproach was used at Bionic Games, Inc. for one of their projects. The basic idea\nis to load a compressed version of level B into the upper stack, while the cur-\nrently active level A resides (in uncompressed form) in the lower stack. To\nswitch from level A to level B, we simply free level A’s resources (by clearing\nthe lower stack) and then decompress level B from the upper stack into the\nlower stack. Decompression is generally much faster than loading data from\ndisk, so this approach effectively eliminates the load time that would other-\nwise be experienced by the player between levels.\n7.2. The Resource Manager 513\nLoad LSR data, then obtain marker.\nLoad-and-\nstay-resident\n(LSR) data\nLoad level A.\nLSR dataLevel A’s\nresources\nUnload level A, free back to marker.\nLSR data\nLoad level B.\nLSR dataLevel B’s\nresources\nFigure 7.3. Loading resources using a stack allocator.\nPool-Based Resource Allocation\nAnother resource allocation technique that is common in game engines that\nsupport streaming is to load resource data in equally sized chunks. Because\nthechunksareallthesamesize,theycanbeallocatedusinga poolallocator (see\nSection 6.2.1.2). When resources are later unloaded, the chunks can be freed\nwithout causing fragmentation.\nOf course, a chunk-based allocation approach requires that all resource\ndata be laid out in a manner that permits division into equally sized chunks.\nWe cannot simply load an arbitrary resource file in chunks, because the file\nmightcontainacontiguousdatastructurelikeanarrayoraverylarge struct\nthat is larger than a single chunk. For example, if the chunks that contain an\narray are not arranged sequentially in RAM, the continuity of the array will\nbe lost, and array indexing will cease to function properly. This means that all\nresource data must be designed with “chunkiness” in mind. Large contigu-\nous data structures must be avoided in favor of data structures that are either\nsmall enough to fit within a single chunk or do not require contiguous RAM\n514 7. Resources and the File System\nFile A\nChunk 1File A\nChunk 2File A\nChunk 3File B\nChunk 1File B\nChunk 2File C\nChunk 1\nFile C\nChunk 2File C\nChunk 3File C\nChunk 4File D\nChunk 1File D\nChunk 2File D\nChunk 3\nFile E\nChunk 1File E\nChunk 2File E\nChunk 3File E\nChunk 4File E\nChunk 5File E\nChunk 6Level X\n(files A, D)Level Y\n(files B, C, E)\nFigure 7.4. Chunky allocation of resources for levels X and Y.\nto function properly (e.g., linked lists).\nEachchunk in the pool is typically associated with a particular game level.\n(Onesimplewaytodothisistogiveeachlevelalinkedlistofitschunks.) This\nallows the engine to manage the lifetimes of each chunk appropriately, even\nwhenmultiplelevelswithdifferentlifespansareinmemoryconcurrently. For\nexample, when level X is loaded, it might allocate and make use of Nchunks.\nLater, level Y might allocate an additional Mchunks. When level X is even-\ntually unloaded, its Nchunks are returned to the free pool. If level Y is still\nactive, its Mchunks need to remain in memory. By associating each chunk\nwith a specific level, the lifetimes of the chunks can be managed easily and\nefficiently. This is illustrated in Figure 7.4.\nOne big trade-off inherent in a “chunky” resource allocation scheme is\nwasted space. Unless a resource file’s size is an exact multiple of the chunk\nsize, the last chunk in a file will not be fully utilized (see Figure 7.5). Choos-\ning a smaller chunk size can help to mitigate this problem, but the smaller the\nchunks, the more onerous the restrictions on the layout of the resource data.\n(Asanextremeexample,ifachunksizeofonebytewereselected,thennodata\nstructurecouldbelargerthanasinglebyte—clearlyanuntenablesituation.) A\ntypical chunk size is on the order of a few kibibytes. For example, at Naughty\nDog,weuseachunkyresourceallocatoraspartofourresourcestreamingsys-\ntem, and our chunks are 512 KiB in size on the PS3 and 1 MiB on the PS4. You\nmay also want to consider selecting a chunk size that is a multiple of the oper-\natingsystem’sI/Obuffersizetomaximizeefficiencywhenloadingindividual\nchunks.\nResource Chunk Allocators\nOne way to limit the effects of wasted chunk memory is to set up a special\nmemoryallocatorthatcanutilizetheunusedportionsofchunks. AsfarasI’m\n7.2. The Resource Manager 515\nFigure 7.5. The last chunk of a resource ﬁle is often not fully utilized.\naware,thereisnostandardizednameforthiskindofallocator,butwewillcall\nit aresourcechunk allocator for lack of a better name.\nA resource chunk allocator is not particularly difficult to implement. We\nneed only maintain a linked list of all chunks that contain unused memory,\nalong with the locations and sizes of each free block. We can then allocate\nfrom these free blocks in any way we see fit. For example, we might manage\nthe linked list of free blocks using a general-purpose heap allocator. Or we\nmightmapasmallstackallocatorontoeachfreeblock; wheneverarequestfor\nmemory comes in, we could then scan the free blocks for one whose stack has\nenough free RAM and then use that stack to satisfy the request.\nUnfortunately, there’s a rather grotesque-looking fly in our ointment here.\nIf we allocate memory in the unused regions of our resource chunks, what\nhappens when those chunks are freed? We cannot free part of a chunk—it’s\nan all or nothing proposition. So any memory we allocate within an unused\nportion of a resource chunk will magically disappear when that resource is\nunloaded.\nAsimplesolutiontothisproblemistoonlyuseourfree-chunkallocatorfor\nmemory requests whose lifetimes match the lifetime of the level with which a\nparticular chunk is associated. In other words, we should only allocate mem-\nory out of level A’s chunks for data that is associated exclusively with level\nA and only allocate from B’s chunks memory that is used exclusively by level\nB. This requires our resource chunk allocator to manage each level’s chunks\nseparately. And it requires the users of the chunk allocator to specify which\nlevel they are allocating for, so that the correct linked list of free blocks can be\nused to satisfy the request.\nThankfully, most game engines need to allocate memory dynamically\nwhenloadingresources,overandabovethememoryrequiredfortheresource\nfilesthemselves. Soaresourcechunkallocatorcanbeafruitfulwaytoreclaim\nchunk memory that would otherwise have been wasted.\n516 7. Resources and the File System\nSectioned Resource Files\nAnother useful idea that is related to “chunky” resource files is the concept\noffile sections . A typical resource file might contain between one and four\nsections, each of which is divided into one or more chunks for the purposes\nof pool allocation as described above. One section might contain data that is\ndestinedformainRAM,whileanothersectionmightcontainvideoRAMdata.\nAnother section could contain temporary data that is needed during the load-\ning process but is discarded once the resource has been completely loaded.\nYet another section might contain debugging information. This debug data\ncould be loaded when running the game in debug mode, but not loaded at\nall in the final production build of the game. The Granny SDK’s file system\n(http://www.radgametools.com)isanexcellentexampleofhowtoimplement\nfile sectioning in a simple and flexible manner.\n7.2.2.8 Composite Resources and Referential Integrity\nUsually a game’s resource database consists of multiple resource files, each file\ncontainingoneormore dataobjects . Thesedataobjectscanrefertoanddepend\nupononeanotherinarbitraryways. Forexample,ameshdatastructuremight\ncontain a reference to its material, which in turn contains a list of references to\ntextures. Usually cross-references imply dependency (i.e., if resource A refers\nto resource B, then both A and B must be in memory in order for the resources\nto be functional in the game.) In general, a game’s resource database can be\nrepresented by a directedgraph of interdependent data objects.\nCross-references between data objects can be internal (a reference between\ntwo objects within a single file) or external (a reference to an object in a dif-\nferent file). This distinction is important because internal and external cross-\nreferences are often implemented differently. When visualizing a game’s re-\nsource database, we can draw dotted lines surrounding individual resource\nfiles to make the internal/external distinction clear—any edge of the graph\nthat crosses a dotted line file boundary is an external reference, while edges\nthat do not cross file boundaries are internal. This is illustrated in Figure 7.6.\nWe sometimes use the term composite resource to describe a self-sufficient\ncluster of interdependent resources. For example, a modelis a composite re-\nsource consisting of one or more triangle meshes, an optional skeleton and an\noptional collection of animations. Each mesh is mapped with a material, and\neachmaterialreferstooneormore textures. Tofullyloadacompositeresource\nlike a 3D model into memory, all of its dependent resources must be loaded\nas well.\n7.2. The Resource Manager 517\nMesh 1 Material 1\nMesh 2 Material 2\nSkeleton 1\nAnim 1 Anim 2 Anim 3Texture 1File 1\nTexture 2File 2\nTexture 3File 3\nAnim 4 Anim 5 Anim 6File 4 File 5\nFile 6\n= internal cross- reference\n= exte rnal cross-re ferenceLegend\n= file boundary\nFigure 7.6. Example of a resource database dependency graph.\n7.2.2.9 Handling Cross-References between Resources\nOne of the more-challenging aspects of implementing a resource manager is\nmanagingthecross-referencesbetweenresourceobjectsandguaranteeingthat\nreferentialintegrityismaintained. Tounderstandhowaresourcemanagerac-\ncomplishes this, let’s look at how cross-references are represented in memory,\nand how they are represented on-disk.\nInC++,across-referencebetweentwodataobjectsisusuallyimplemented\nvia apointeror areference . For example, a mesh might contain the data mem-\nberMaterial* m_pMaterial (a pointer) or Material& m_material (a\nreference) in order to refer to its material. However, pointers are just memory\naddresses—they lose their meaning when taken out of the context of the run-\nning application. In fact, memory addresses can and do change even between\nsubsequent runs of the same application. Clearly when storing data to a disk\nfile, we cannot use pointers to describe inter-object dependencies.\nGUIDs as Cross-References\nOnegoodapproachistostoreeachcross-referenceasastringorhashcodecon-\ntainingtheuniqueidofthereferencedobject. Thisimpliesthateveryresource\nobject that might be cross-referenced must have a globally unique identifier or\nGUID.\n518 7. Resources and the File System\nAddresses: Offsets:\n0x0\n0x240\n0x4A0\n0x7F00x2A080\n0x2D750\n0x2F110\n0x32EE0\nFigure 7.7. In-memory object images become contiguous when saved into a binary ﬁle.\nTo make this kind of cross-reference work, the runtime resource manager\nmaintains a global resource look-up table. Whenever a resource object is\nloadedintomemory,apointertothatobjectisstoredinthetablewithitsGUID\nas the look-up key. After all resource objects have been loaded into memory\nand their entries added to the table, we can make a pass over all of the objects\nand convert all of their cross-references into pointers, by looking up the ad-\ndress of each cross-referenced object in the global resource look-up table via\nthat object’s GUID.\nPointer Fix-Up Tables\nAnother approach that is often used when storing data objects into a binary\nfile is to convert the pointers intofile offsets . Consider a group of C structs or\nC++ objects that cross-reference each other via pointers. To store this group\nof objects into a binary file, we need to visit each object once (and only once)\nin an arbitrary order and write each object’s memory image into the file se-\nquentially. This has the effect of serializing the objects into a contiguous image\nwithin the file, even when their memory images are not contiguous in RAM.\nThis is shown in Figure 7.7.\nBecause the objects’ memory images are now contiguous within the file,\nwe can determine the offsetof each object’s image relative to the beginning of\nthe file. During the process of writing the binary file image, we locate every\n7.2. The Resource Manager 519\npointer within every data object, convert each pointer into an offset and store\nthose offsets into the file in place of the pointers. We can simply overwrite the\npointers with their offsets, because the offsets never require more bits to store\nthan the original pointers. In effect, an offsetis the binary file equivalent of a\npointerinmemory. (Dobeawareofthedifferencesbetweenyourdevelopment\nplatform and your target platform. If you write out a memory image on a 64-\nbitWindowsmachine,itspointerswillallbe64bitswideandtheresultingfile\nwon’t be compatible with a 32-bit console.)\nOfcourse, we’llneedtoconverttheoffsetsbackintopointerswhenthefile\nisloadedintomemorysometimelater. Suchconversionsareknownas pointer\nfix-ups. When the file’s binary image is loaded, the objects contained in the\nimage retain their contiguous layout, so it is trivial to convert an offset into a\npointer. We merely add the offset to the address of the file image as a whole.\nThis is demonstrated by the code snippet below and illustrated in Figure 7.8.\nU8* ConvertOffsetToPointer(U32 objectOffset,\nU8* pAddressOfFileImage)\n{\nU8* pObject = pAddressOfFileImage + objectOffset;\nreturn pObject;\n}\nAddresses: Offsets:\n0x0\n0x240\n0x4A0\n0x7F00x30100\n0x30340\n0x305A0\n0x308F0\nFigure 7.8. Contiguous resource ﬁle image, after it has\nbeen loaded into RAM.\nAddresses:\nOffsets:\nObject 1\nObject 2\nObject 3\nObject 40x0\n0x240\n0x4A0\n0x7F0Object 1\nObject 4\nObject 2\nObject 30x2A080\n0x2D750\n0x2F110\n0x32EE00x32EE0\n0x2F110\n0x2A0800x4A0\n0x2400x0Pointers converted \nto offsets; locations \nof pointers stored in \nfix-up table.\nFix-Up Table\n0x200\n0x340\n0x810Pointers to various \nobjects are present.\n3 pointersFigure 7.9. A pointer ﬁx-up table.\n520 7. Resources and the File System\nThe problem we encounter when trying to convert pointers into offsets,\nand vice versa, is how to findall of the pointers that require conversion. This\nproblem is usually solved at the time the binary file is written. The code that\nwrites out the images of the data objects has knowledge of the data types and\nclasses being written, so it has knowledge of the locations of all the pointers\nwithin each object. The locations of the pointers are stored into a simple table\nknown as a pointer fix-up table. This table is written into the binary file along\nwith the binary images of all the objects. Later, when the file is loaded into\nRAMagain,thetablecanbeconsultedinordertofindandfixupeverypointer.\nThe table itself is just a list of offsets within the file—each offset represents a\nsingle pointer that requires fixing up. This is illustrated in Figure 7.9.\nStoring C++ Objects as Binary Images: Constructors\nOne important step that is easy to overlook when loading C++ objects from a\nbinary file is to ensure that the objects’ constructors are called. For example,\nif we load a binary image containing three objects—an instance of class A, an\ninstance of class B, and an instance of class C—then we must make sure that\nthe correct constructor is called on each of these three objects.\nThere are two common solutions to this problem. First, you can simply\ndecide not to support C++ objects in your binary files at all. In other words,\nrestrict yourself to plain old data structures (abbreviated PODS or POD)—i.e.,\nCstructsandC++structsandclassesthatcontain novirtualfunctions andtrivial\ndo-nothing constructors. (See http://en.wikipedia.org/wiki/Plain_Old_Data_\nStructures for a more complete discussion of PODS.)\nSecond, you can save off a table containing the offsets of all non-PODS\nobjects in your binary image along with some indication of which class each\nobject is an instance of. Then, once the binary image has been loaded, you can\niteratethroughthistable, visiteachobjectandcalltheappropriateconstructor\nusingplacementnew syntax(i.e.,callingtheconstructoronapreallocatedblock\nofmemory). Forexample,giventheoffsettoanobjectwithinthebinaryimage,\nwe might write:\nvoid* pObject = ConvertOffsetToPointer(objectOffset,\npAddressOfFileImage);\n::new(pObject) ClassName; // placement new syntax\nwhereClassName is the class of which the object is an instance.\nHandling External References\nThe two approaches described above work very well when applied to re-\nsources in which all of the cross-references are internal—i.e., they only refer-\n7.2. The Resource Manager 521\nence objects within a single resource file. In this simple case, you can load the\nbinaryimageintomemoryandthenapplythepointerfix-upstoresolveallthe\ncross-references. But when cross-references reachout into other resource files,\na slightly augmented approach is required.\nTo successfully represent an external cross-reference, we must specify not\nonly the offset or GUID of the data object in question, but also the path to the\nresource file in which the referenced object resides.\nThekeytoloadingamulti-filecompositeresourceistoload alloftheinter-\ndependent files first. This can be done by loading one resource file and then\nscanningthroughitstableofcross-referencesandloadinganyexternallyrefer-\nencedfilesthathavenotalreadybeenloaded. Asweloadeachdataobjectinto\nRAM, we can add the object’s address to the master look-up table. Once all of\nthe interdependent files have been loaded and all of the objects are present in\nRAM, we can make a final pass to fix up all of the pointers using the master\nlook-up table to convert GUIDs or file offsets into real addresses.\n7.2.2.10 Post-Load Initialization\nIdeally, each and every resource would be completely prepared by our offline\ntools, so that it is ready for use the moment it has been loaded into memory.\nPractically speaking, this is not always possible. Many types of resources re-\nquire at least some “massaging” after having been loaded in order to prepare\nthem for use by the engine. In this book, I will use the term post-loadinitializa-\ntionto refer to any processing of resource data after it has been loaded. Other\nenginesmayusedifferentterminology. (Forexample, atNaughtyDogwecall\nthisloggingin a resource.) Most resource managers also support some kind of\ntear-down step prior to a resource’s memory being freed. (At Naughty Dog,\nwe call this loggingout a resource.)\nPost-load initialization generally comes in one of two varieties:\n• In some cases, post-load initialization is an unavoidable step. For exam-\nple, on a PC, the vertices and indices that describe a 3D mesh are loaded\nintomainRAM,buttheymustbetransferredintovideoRAMbeforethey\ncan be rendered. This can only be accomplished at runtime, by creating\na Direct X vertex buffer or index buffer, locking it, copying or reading\nthe data into the buffer and then unlocking it.\n• In other cases, the processing done during post-load initialization is\navoidable (i.e., could be moved into the tools), but is done for conve-\nnienceorexpedience. Forexample,aprogrammermightwanttoaddthe\ncalculation of accurate arc lengths to our engine’s spline library. Rather\nthan spend the time to modify the tools to generate the arc length data,\n522 7. Resources and the File System\nthe programmer might simply calculate it at runtime during post-load\ninitialization. Later,whenthecalculationsareperfected,thiscodecanbe\nmoved into the tools, thereby avoiding the cost of doing the calculations\nat runtime.\nClearly, each type of resource has its own unique requirements for post-\nload initialization and tear-down. So, resource managers typically permit\nthese two steps to be configurable on a per-resource-type basis. In a non-\nobject-oriented language like C, we can envision a look-up table that maps\neach type of resource to a pair of function pointers, one for post-load initial-\nization and one for tear-down. In an object-oriented language like C++, life is\neveneasier—wecanmakeuseofpolymorphismtopermiteachclasstohandle\npost-load initialization and tear-down in a unique way.\nInC++,post-loadinitializationcouldbeimplementedasaspecialconstruc-\ntor, and tear-down could be done in the class’ destructor. However, there are\nsome problems with using constructors and destructors for this purpose. For\nexample, one typically needs to construct all loaded objects first, then apply\npointer fix-ups, and finally perform post-load initialization as a separate step.\nAssuch,mostdevelopersdeferpost-loadinitializationandtear-downtoplain\nold virtual functions. For example, we might choose to use a pair of virtual\nfunctions named something sensible like Init() andDestroy().\nPost-load initialization is closely related to a resource’s memory allocation\nstrategy, because new data is often generated by the initialization routine. In\nsomecases,thedatageneratedbythepost-loadinitializationstep augments the\ndata loaded from the file. (For example, if we are calculating the arc lengths\nof the segments of a Catmull-Rom spline curve after it has been loaded, we\nwould probably want to allocate some additional memory in which to store\nthe results.) In other cases, the data generated during post-load initialization\nreplaces the loaded data. (For example, we might allow mesh data in an older\nout-of-date format to be loaded and then automatically converted into the lat-\nest format for backwards compatibility reasons.) In this case, the loaded data\nmay need to be discarded, either partially or in its entirety, after the post-load\nstep has generated the new data.\nTheHydroThunder enginehadasimplebutpowerfulwayofhandlingthis.\nIt would permit resources to be loaded in one of two ways: (a) directly into its\nfinal resting place in memory or (b) into a temporary area of memory. In the\nlatter case, the post-load initialization routine was responsible for copying the\nfinalizeddataintoitsultimatedestination; thetemporarycopyoftheresource\nwouldbediscardedafterpost-loadinitializationwascomplete. Thiswasvery\nuseful for loading resource files that contained both relevant and irrelevant\n7.2. The Resource Manager 523\ndata. The relevant data would be copied into its final destination in mem-\nory, while the irrelevant data would be discarded. For example, mesh data in\nan out-of-date format could be loaded into temporary memory and then con-\nverted into the latest format by the post-load initialization routine, without\nhaving to waste any memory keeping the old-format data kicking around.\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",69178
52-8 The Game Loop and Real-Time Simulation.pdf,52-8 The Game Loop and Real-Time Simulation,,0
53-8.2 The Game Loop.pdf,53-8.2 The Game Loop,"8\nThe Game Loop and\nReal-Time Simulation\nGames are real-time, dynamic, interactive computer simulations. As such,\ntimeplays an incredibly important role in any electronic game. There are\nmany different kinds of time to deal with in a game engine—real time, game\ntime, the local timeline of an animation, the actual CPU cycles spent within\na particular function, and the list goes on. Every engine system might define\nand manipulate time differently. We must have a solid understanding of all\nthe ways time can be used in a game. In this chapter, we’ll take a look at how\nreal-time, dynamic simulation software works and explore the common ways\nin which time plays a role in such a simulation.\n8.1 The Rendering Loop\nIn a graphical user interface (GUI), of the sort found on a Windows PC or a\nMacintosh, the majority of the screen’s contents are static. Only a small part\nof any one window is actively changing appearance at any given moment.\nBecause of this, graphical user interfaces have traditionally been drawn on-\nscreen via a technique known as rectangleinvalidation, in which only the small\nportions of the screen whose contents have actually changed are redrawn.\nOlder 2D video games used similar techniques to minimize the number of\npixels that needed to be drawn.\n525\n526 8. The Game Loop and Real-Time Simulation\nReal-time 3D computer graphics are implemented in an entirely different\nway. Asthecameramovesaboutina3Dscene, the entirecontents ofthescreen\nor window change continually, so the concept of invalid rectangles no longer\napplies. Instead, an illusion of motion and interactivity is produced in much\nthesamewaythatamovieproducesit—bypresentingtheviewerwithaseries\nof still images in rapid succession.\nObviously, producing a rapid succession of still images on-screen requires\na loop. In a real-time rendering application, this is sometimes known as the\nrenderloop. At its simplest, a rendering loop is structured as follows:\nwhile (!quit)\n{\n// Update the camera transform based on interactive\n// inputs or by following a predefined path.\nupdateCamera ();\n// Update positions, orientations and any other\n// relevant visual state of any dynamic elements\n// in the scene.\nupdateSceneElements ();\n// Render a still frame into an off-screen frame\n// buffer known as the ""back buffer"".\nrenderScene();\n// Swap the back buffer with the front buffer, making\n// the most recently rendered image visible\n// on-screen. (Or, in windowed mode, copy (blit) the\n// back buffer's contents to the front buffer.\nswapBuffers();\n}\n8.2 The Game Loop\nA game is composed of many interacting subsystems, including device I/O,\nrendering, animation, collision detection and resolution, optional rigid body\ndynamics simulation, multiplayer networking, audio, and the list goes on.\nMostgameenginesubsystemsrequireperiodic servicing whilethegameisrun-\nning. However, the rateat which these subsystems need to be serviced varies\nfrom subsystem to subsystem. Animation typically needs to be updated at a\nrate of 30 or 60 Hz, in synchronization with the rendering subsystem. How-\never, a dynamics (physics) simulation may actually require more frequent up-\ndates (e.g., 120 Hz). Higher-level systems, like AI, might only need to be\n8.2. The Game Loop 527\nserviced once or twice per second, and they needn’t necessarily be synchro-\nnized with the rendering loop at all.\nThere are a number of ways to implement the periodic updating of our\ngame engine subsystems. We’ll explore some of the possible architectures in a\nmoment. Butforthetimebeing,let’sstickwiththesimplestwaytoupdateour\nengine’s subsystems—using a single loop to update everything. Such a loop\nis often called the game loop , because it is the master loop that services every\nsubsystem in the engine.\n8.2.1 A Simple Example: Pong\nPong is a well-known genre of table tennis video games that got its start in\n1958, in the form of an analog computer game called Tennis for Two, created\nby William A. Higinbotham at the Brookhaven National Laboratory and dis-\nplayed on an oscilloscope. The genre is best known by its later incarnations\nondigitalcomputers—theMagnavoxOddyseygame TableTennis andtheAtari\narcade game Pong.\nIn pong, a ball bounces back and forth between two movable vertical pad-\ndles and two fixed horizontal walls. The human players control the positions\nof the paddles via control wheels. (Modern re-implementations allow control\nvia a joystick, the keyboard or some other human interface device.) If the ball\npasses by a paddle without striking it, the other team wins the point and the\nball is reset for a new round of play.\nThe following pseudocode demonstrates what the game loop of a pong\ngame might look like at its core:\nvoid main() // Pong\n{\ninitGame();\nwhile (true) // game loop\n{\nreadHumanInterfaceDevices();\nif ( quitButtonPressed ())\n{\nbreak; // exit the game loop\n}\nmovePaddles();\nmoveBall();\ncollideAndBounceBall ();\n528 8. The Game Loop and Real-Time Simulation\nif (ballImpactedSide (LEFT_PLAYER))\n{\nincremenentScore (RIGHT_PLAYER);\nresetBall();\n}\nelse if (ballImpactedSide (RIGHT_PLAYER))\n{\nincrementScore (LEFT_PLAYER);\nresetBall();\n}\nrenderPlayfield ();\n}\n}\nClearly this example is somewhat contrived. The original pong games\nwere certainly not implemented by redrawing the entire screen at a rate of\n30 frames per second. Back then, CPUs were so slow that they could barely\nmuster the power to draw two lines for the paddles and a box for the ball in\nreal time. Specialized 2D sprite hardware was often used to draw moving ob-\njects on-screen. However, we’re only interested in the concepts here, not the\nimplementation details of the original Pong.\nAs you can see, when the game first runs, it calls initGame() to do\nwhatever set-up might be required by the graphics system, human I/O de-\nvices, audio system, etc. Then the main game loop is entered. The state-\nment while (true) tells us that the loop will continue forever, unless in-\nterrupted internally. The first thing we do inside the loop is to read the hu-\nmaninterfacedevice(s). Wechecktoseewhethereitherhumanplayerpressed\nthe “quit” button—if so, we exit the game via a break statement. Next,\nthe positions of the paddles are adjusted slightly upward or downward in\nmovePaddles(), based on the current deflection of the control wheels, joy-\nsticks or other I/O devices. The function moveBall() adds the ball’s cur-\nrent velocity vector to its position in order to find its new position next frame.\nIncollideAndBounceBall(), this position is then checked for collisions\nagainst both the fixed horizontal walls and the paddles. If collisions are de-\ntected, the ball’s position is recalculated to account for any bounce. We also\nnote whether the ball impacted either the left or right edge of the screen. This\nmeans that it missed one of the paddles, in which case we increment the other\nplayer’s score and reset the ball for the next round. Finally, renderPlay -\nfield() draws the entire contents of the screen.",7089
54-8.3 Game Loop Architectural Styles.pdf,54-8.3 Game Loop Architectural Styles,"8.3. Game Loop Architectural Styles 529\n8.3 Game Loop Architectural Styles\nGame loops can be implemented in a number of different ways—but at their\ncore,theyusuallyboildowntooneormoresimpleloops, withvariousembel-\nlishments. We’ll explore a few of the more common architectures below.\n8.3.1 Windows Message Pumps\nOn a Windows platform, games need to service messages from the Windows\noperating system in addition to servicing the various subsystems in the game\nengine itself. Windows games therefore contain a chunk of code known as a\nmessage pump . The basic idea is to service Windows messages whenever they\narrive and to service the game engine only when no Windows messages are\npending. A message pump typically looks something like this:\nwhile (true)\n{\n// Service any and all pending Windows messages.\nMSG msg;\nwhile (PeekMessage(&msg, nullptr, 0, 0) > 0)\n{\nTranslateMessage (&msg);\nDispatchMessage (&msg);\n}\n// No more Windows messages to process -- run one\n// iteration of our ""real"" game loop.\nRunOneIterationOfGameLoop ();\n}\nOne of the side-effects of implementing the game loop like this is that Win-\ndows messages take precedence over rendering and simulating the game. As\na result, the game will temporarily freeze whenever you resize or drag the\ngame’s window around on the desktop.\n8.3.2 Callback-Driven Frameworks\nMostgameenginesubsystemsandthird-partygamemiddlewarepackagesare\nstructured as libraries. A library is a suite of functions and/or classes that can\nbe called in any way the application programmer sees fit. Libraries provide\nmaximum flexibility to the programmer. But, libraries are sometimes difficult\nto use, because the programmer must understand how to properly use the\nfunctions and classes they provide.\n530 8. The Game Loop and Real-Time Simulation\nIn contrast, some game engines and game middleware packages are struc-\ntured as frameworks . A framework is a partially constructed application—the\nprogrammercompletestheapplicationbyprovidingcustomimplementations\nof missing functionality within the framework (or overriding its default be-\nhavior). But he or she has little or no control over the overall flow of control\nwithin the application, because it is controlled by the framework.\nIn a framework-based rendering engine or game engine, the main game\nloop has been written for us, but it is largely empty. The game program-\nmer can write callback functions in order to “fill in” the missing details. The\nOGRE rendering engine is an example of a library that has been wrapped\nin a framework. At the lowest level, OGRE provides functions that can be\ncalled directly by a game engine programmer. However, OGRE also pro-\nvides a framework that encapsulates knowledge of how to use the low-level\nOGRE library effectively. If the programmer chooses to use the OGRE frame-\nwork, he or she derives a class from Ogre::FrameListener and overrides\ntwovirtualfunctions: frameStarted() andframeEnded(). Asyoumight\nguess, these functions are called before and after the main 3D scene has been\nrendered by OGRE, respectively. The OGRE framework’s implementation of\nits internal game loop looks something like the following pseudocode. (See\nOgre::Root::renderOneFrame() inOgreRoot.cpp for the actual source\ncode.)\nwhile (true)\n{\nfor (each frameListener)\n{\nframeListener. frameStarted ();\n}\nrenderCurrentScene ();\nfor (each frameListener)\n{\nframeListener. frameEnded();\n}\nfinalizeSceneAndSwapBuffers ();\n}\nA particular game’s frame listener implementation might look something like\nthis.\n8.3. Game Loop Architectural Styles 531\nclass GameFrameListener : public Ogre::FrameListener\n{\npublic:\nvirtual void frameStarted (const FrameEvent& event)\n{\n// Do things that must happen before the 3D scene\n// is rendered (i.e., service all game engine\n// subsystems).\npollJoypad(event);\nupdatePlayerControls(event);\nupdateDynamicsSimulation(event);\nresolveCollisions(event);\nupdateCamera(event);\n// etc.\n}\nvirtual void frameEnded(const FrameEvent& event)\n{\n// Do things that must happen after the 3D scene\n// has been rendered.\ndrawHud(event);\n// etc.\n}\n};\n8.3.3 Event-Based Updating\nIngames,an eventisanyinterestingchangeinthestateofthegameoritsenvi-\nronment. Some examples include: the human player pressing a button on the\njoypad,anexplosiongoingoff,anenemycharacterspottingtheplayer,andthe\nlist goes on. Most game engines have an event system, which permits various\nengine subsystems to register interest in particular kinds of events and to re-\nspond to those events when they occur (see Section 16.8 for details). A game’s\nevent system is usually very similar to the event/messaging system underly-\ning virtually all graphical user interfaces (for example, Microsoft Windows’\nwindow messages, the event handling system in Java’s AWT or the services\nprovided by C#’s delegate andeventkeywords).\nSome game engines leverage their event system in order to implement the\nperiodic servicing of some or all of their subsystems. For this to work, the\nevent system must permit events to be posted into the future—that is, to be\nqueued for later delivery. A game engine can then implement periodic updat-\ning by simply posting an event. In the event handler, the code can perform",5298
55-8.4 Abstract Timelines.pdf,55-8.4 Abstract Timelines,"532 8. The Game Loop and Real-Time Simulation\nwhatever periodic servicing is required. It can then post a new event 1/30 or\n1/60 of a second into the future, thus continuing the periodic servicing for as\nlong as it is required.\n8.4 Abstract Timelines\nIn game programming, it can be extremely useful to think in terms of abstract\ntimelines . A timeline is a continuous, one-dimensional axis whose origin ( t=\n0) can lie at any arbitrary location relative to other timelines in the system. A\ntimeline can be implemented via a simple clock variable that stores absolute\ntime values in either integer or floating-point format.\n8.4.1 Real Time\nWe can think of times measured directly via the CPU’s high-resolution timer\nregister (see Section 8.5.3) as lying on what we’ll call the real timeline. The\norigin of this timeline is defined to coincide with the moment the CPU was\nlast powered on or reset. It measures times in units of CPU cycles (or some\nmultiplethereof),althoughthesetimevaluescanbeeasilyconvertedintounits\nof seconds by multiplying them by the frequency of the high-resolution timer\non the current CPU.\n8.4.2 Game Time\nWe needn’t limit ourselves to working with the real timeline exclusively. We\ncandefineasmanyothertimeline(s)asweneedinordertosolvetheproblems\nathand. Forexample,wecandefinea gametimeline thatistechnicallyindepen-\ndent of real time. Under normal circumstances, game time coincides with real\ntime. If we wish to pause the game, we can simply stop updating the game\ntimeline temporarily. If we want our game to go into slow motion, we can up-\ndate the game clock more slowly than the real-time clock. All sorts of effects\ncan be achieved by scaling and warping one timeline relative to another.\nPausing or slowing down the game clock is also a highly useful debug-\nging tool. To track down a visual anomaly, a developer can pause game time\nin order to freeze the action. Meanwhile, the rendering engine and debug fly-\nthroughcameracancontinuetorun,aslongastheyaregovernedbyadifferent\nclock (either the real-time clock or a separate camera clock ). This allows the de-\nveloper to fly the camera around the game world to inspect it from any angle\ndesired. We can even support single-stepping the game clock, by advancing\nthe game clock by one target frame interval (e.g., 1/30 of a second) each time\n8.4. Abstract Timelines 533\na “single-step” button is pressed on the joypad or keyboard while the game is\nin a paused state.\nWhenusingtheapproachdescribedabove,it’simportanttorealizethatthe\ngame loop is still running when the game is paused—only the game clock has\nstopped. Single-stepping the game by adding 1/30 of a second to a paused\ngame clock is not the same thing as setting a breakpoint in your main loop,\nandthenhittingtheF5keyrepeatedlytorunoneiterationoftheloopatatime.\nBoth kinds of single-stepping can be useful for tracking down different kinds\nof problems. We just need to keep the differences between these approaches\nin mind.\n8.4.3 Local and Global Timelines\nWe can envision all sorts of other timelines. For example, an animation clip\nor audio clip might have a local timeline, with its origin ( t=0) defined to\ncoincide with the start of the clip. The local timeline measures how time pro-\ngressed when the clip was originally authored or recorded. When the clip is\nplayed back in-game, we needn’t play it at the original rate. We might want\nto speed up an animation, or slow down an audio sample. We can even play\nan animation backwards by running its local clock in reverse.\nAny one of these effects can be visualized as a mapping between the local\ntimeline and a global timeline, such as real time or game time. To play an\nanimation clip back at its originally authored speed, we simply map the start\nof the animation’s local timeline ( t=0) onto the desired start time (t=tstart)\nalong the global timeline. This is shown in Figure 8.1.\nTo play an animation clip back at half speed, we can imagine scaling the\nlocaltimelinetotwiceitsoriginalsizepriortomappingitontotheglobaltime-\nline. Toaccomplishthis,wesimplykeeptrackofatimescalefactororplayback\nrateR, in addition to the clip’s global start time tstart. This is illustrated in Fig-\nure 8.2. A clip can even be played in reverse, by using a negative time scale\n(R<0)as shown in Figure 8.3.\nClip A\nt= 0 sec 5 sec\nstar t\n102 sec\n105 sec 110 sec\nFigure 8.1. Playing an animation clip can be visualized as mapping its local timeline onto the global\ngame timeline.",4529
56-8.5 Measuring and Dealing with Time.pdf,56-8.5 Measuring and Dealing with Time,"534 8. The Game Loop and Real-Time Simulation\nstart\nR\n(scale t by 1/ R= 0.5)t t\nt\nFigure 8.2. Animation playback speed can be controlled by simply scaling the local timeline prior\nto mapping it onto the global timeline.\nt= 5 sec 0 sec\nstart\n102 sec\n105 sec 110 sec\n Clip A\nClip A\nt= 0 sec 5 secR= –1\n(ﬂip t)\nFigure 8.3. Playing an animation in reverse is like mapping the clip to the global timeline with a time\nscale of R= 1.\n8.5 Measuring and Dealing with Time\nIn this section, we’ll investigate some of the subtle and not-so-subtle distinc-\ntions between different kinds of timelines and clocks and see how they are\nimplemented in real game engines.\n8.5.1 Frame Rate and Time Deltas\nTheframe rate of a real-time game describes how rapidly the sequence of still\n3D frames is presented to the viewer. The unit of Hertz(Hz), defined as the\nnumber of cycles per second, can be used to describe the rate of any periodic\nprocess. Ingamesandfilm,framerateistypicallymeasuredin framespersecond\n(FPS), which is the same thing as Hertz for all intents and purposes. Films\ntraditionally run at 24 FPS. Games in North America and Japan are typically\nrendered at 30 or 60 FPS, because this is the natural refresh rate of the NTSC\ncolor television standardused in these regions. In Europeand most of the rest\n8.5. Measuring and Dealing with Time 535\nof the world, games update at 50 FPS, because this is the natural refresh rate\nof a PAL or SECAM color television signal.\nTheamountoftimethatelapsesbetweenframesisknownasthe frametime ,\ntimedelta ordeltatime . Thislasttermiscommonplacebecausethedurationbe-\ntween frames is often represented mathematically by the symbol ∆t. (Techni-\ncallyspeaking, ∆tshouldreallybecalledthe frameperiod,sinceitistheinverse\nof theframe frequency :T=1/f. But, game programmers hardly ever use the\nterm “period” in this context.) If a game is being rendered at exactly 30 FPS,\nthenitsdeltatimeis1/30ofasecond,or33.3ms(milliseconds). At60FPS,the\ndeltatimeishalfasbig,1/60ofasecondor16.6ms. Toreallyknowhowmuch\ntime has elapsed during one iteration of the game loop, we need to measure\nit. We’ll see how this is done below.\nWeshouldnoteherethatmillisecondsareacommonunitoftimemeasure-\nment in games. For example, we might say that the animation system is tak-\ning 4 ms to run, which implies that it occupies about 12% of the entire frame\n(4/33.30.12). Other common units include seconds and machine cycles.\nWe’ll discuss time units and clock variables in more depth below.\n8.5.2 From Frame Rate to Speed\nLet’s imagine that we want to make a spaceship fly through our game world\nat a constant speed of 40 meters per second (or in a 2D game, we might\nspecify this as 40 pixelsper second). One simple way to accomplish this is\nto multiply the ship’s speed v(measured in meters per second) by the du-\nration of one frame ∆t(measured in seconds), yielding a change in position\n∆x=v∆t(which is measured in metersperframe ). This position delta can then\nbe added to the ship’s current position x1, in order to find its position next\nframe: x2=x1+∆x=x1+v∆t.\nThis is actually a simple form of numericalintegration known as the explicit\nEulermethod (see Section 13.4.4). It works well as long as the speeds of our\nobjects are roughly constant. To handle variable speeds, we need to resort to\nsomewhat more complex integration methods. But all numerical integration\ntechniques make use of the elapsed frame time ∆tin one way or another. So\nit is safe to say that the perceived speeds of the objects in a game are dependent\nupon the frame duration, ∆t. Hence a central problem in game programming\nistodetermineasuitablevaluefor ∆t. Inthesectionsthatfollow,we’lldiscuss\nvarious ways of doing this.\n8.5.2.1 Old-School CPU-Dependent Games\nIn many early video games, no attempt was made to measure how much real\ntime had elapsed during the game loop. The programmers would essentially\n536 8. The Game Loop and Real-Time Simulation\nignore ∆taltogetherandinsteadspecifythespeedsofobjectsdirectlyinterms\nof meters (or pixels, or some other distance unit) per frame. In other words,\ntheywere,perhapsunwittingly,specifyingobjectspeedsintermsof ∆x=v∆t,\ninstead of in terms of v.\nThe net effect of this simplistic approach was that the perceived speeds of\nthe objects in these games were entirely dependent upon the frame rate that\nthegamewasactuallyachievingonaparticularpieceofhardware. Ifthiskind\nof game were to be run on a computer with a faster CPU than the machine for\nwhich it was originally written, the game would appear to be running in fast\nforward. For this reason, I’ll call these games CPU-dependentgames.\nSome older PCs provided a “Turbo” button to support these kinds of\ngames. When the Turbo button was pressed, the PC would run at its fastest\nspeed,butCPU-dependentgameswouldruninfastforward. WhentheTurbo\nbutton was not pressed, the PC would mimic the processor speed of an older\ngeneration of PCs, allowing CPU-dependent games written for those PCs to\nrun properly.\n8.5.2.2 Updating Based on Elapsed Time\nTo make our games CPU-independent, we must measure ∆tin some way,\nrather than simply ignoring it. Doing this is quite straightforward. We simply\nread the value of the CPU’s high-resolution timer twice—once at the begin-\nning of the frame and once at the end. Then we subtract, producing an accu-\nrate measure of ∆tfor the frame that has just passed. This delta is then made\navailable to all engine subsystems that need it, either by passing it to every\nfunction that we call from within the game loop or by storing it in a global\nvariable or encapsulating it within a singleton class of some kind. (We’ll de-\nscribe the CPU’s high-resolution timer in more detail in Section 8.5.3.)\nThe approach outlined above is used by many game engines. In fact, I am\ntempted to go out on a limb and say that mostgame engines use it. However,\nthereisonebigproblemwiththistechnique: Weareusingthemeasuredvalue\nof∆ttakenduringframe kasanestimateofthedurationofthe upcoming frame\n(k+1). This isn’t necessarily very accurate. (As they say in investing, “past\nperformance is not a guarantee of future results.”) Something might happen\nnextframethatcausesittotakemuchmoretime(ormuchless)thanthecurrent\nframe. We call such an event a frame-ratespike .\nUsing last frame’s delta as an estimate of the upcoming frame can have\nsome very real detrimental effects. For example, if we’re not careful it can put\nthe game into a “viscious cycle” of poor frame times. Let’s assume that our\nphysics simulation is most stable when updated once every 33.3 ms (i.e., at\n30 Hz). If we get one bad frame, taking say 57 ms, then we might make the\n8.5. Measuring and Dealing with Time 537\nmistakeofsteppingthephysicssystem twiceonthenextframe,presumablyto\n“cover”the57msthathaspassed. Thosetwostepstakeroughlytwiceaslong\nto complete as a regular step, causing the nextframe to be at least as bad as\nthis one was, and possibly worse. This only serves to exacerbate and prolong\nthe problem.\n8.5.2.3 Using a Running Average\nItistruethatgameloopstendtohaveatleastsomeframe-to-framecoherency.\nIf the camera is pointed down a hallway containing lots of expensive-to-draw\nobjects on one frame, there’s a good chance it will still be pointed down that\nhallway on the next. Therefore, one reasonable approach is to average the\nframe-time measurements over a small number of frames and use that as the\nnext frame’s estimate of ∆t. This allows the game to adapt to a varying frame\nrate,whilesofteningtheeffectsofmomentaryperformancespikes. Thelonger\ntheaveraginginterval, thelessresponsivethegamewillbetoavaryingframe\nrate, but spikes will have less of an impact as well.\n8.5.2.4 Governing the Frame Rate\nWe can avoid the inaccuracy of using last frame’s ∆tas an estimate of this\nframe’s duration altogether, by flipping the problem on its head. Rather than\ntrying to guessat what next frame’s duration will be, we can instead attempt\ntoguarantee that every frame’s duration will be exactly 33.3 ms (or 16.6 ms if\nwe’re running at 60 FPS). To do this, we measure the duration of the current\nframe as before. If the measured duration is less than the ideal frame time, we\nsimply put the main thread to sleep until the target frame time has elapsed.\nIf the measured duration is more than the ideal frame time, we must “take\nour lumps” and wait for one more whole frame time to elapse. This is called\nframe-rate governing.\nClearly this approach only works when your game’s frame rate is reason-\nably close to your target frame rate on average. If your game is ping-ponging\nbetween 30 FPS and 15 FPS due to frequent “slow” frames, then the game’s\nquality can degrade significantly. As such, it’s still a good idea to design all\nengine systems so that they are capable of dealing with arbitrary frame dura-\ntions. During development, you can leave the engine in “variable frame rate”\nmode, and everything will work as expected. Later on, when the game is get-\nting closer to achieving its target frame rate consistently, we can switch on\nframe-rate governing and start to reap its benefits.\nKeepingtheframerateconsistentcanbeimportantforanumberofreasons.\nSomeenginesystems,suchasthenumericalintegratorsusedinaphysicssim-\nulation, operate best when updated at a constant rate. A consistent frame rate\n538 8. The Game Loop and Real-Time Simulation\nalsolooksbetter,andaswe’llseeinthenextsection,itcanbeusedtoavoidthe\ntearingthat can occur when the video buffer is updated at a rate that doesn’t\nmatch the refresh rate of the monitor (see Section 8.5.2.5).\nIn addition, when elapsed frame times are consistent, features like record\nand playback become a lot more reliable. As its name implies, the record-and-\nplayback feature allows a player’s gameplay experience to be recorded and\nlater played back in exactly the same way. This can be a fun game feature,\nand it’s also a valuable testing and debugging tool. For example, difficult-\nto-find bugs can be reproduced by simply playing back a recorded game that\ndemonstrates the bug.\nTo implement record and playback, we make note of every relevant event\nthat occurs during gameplay, saving each one in a list along with an accurate\ntime stamp. The list of events can then be replayed with exactly the same tim-\ning, using the same initial conditions and an identical initial random seed. In\ntheory,doingthisshouldproduceagameplayexperiencethatisindistinguish-\nable from the original playthrough. However, if the frame rate isn’t consis-\ntent, things may not happen in exactly the same order. This can cause “drift,”\nand pretty soon your AI characters are flanking when they should have fallen\nback.\n8.5.2.5 Screen Tearing and V-Sync\nA visual anomaly known as screen tearing occurs when the back buffer is\nswapped with the front buffer while the screen has only been partially\n“drawn” by the video hardware. When tearing occurs, a portion of the screen\nshows the new image, while the remainder shows the old one. To avoid tear-\ning, many rendering engines wait for the verticalblankinginterval of the moni-\ntor before swapping buffers.\nOlderCRTmonitorsandTVs“draw”thecontentsofthein-memoryframe\nbuffer by exciting phosphors on the screen via a beam of electrons that scans\nfrom left-to-right and top-to-bottom. On such displays, the v-blank interval\nis the time during which the electron gun is “blanked” (turned off) while it\nis being reset to the top-left corner of the screen. Modern LCD, plasma and\nLEDdisplaysnolongeruseanelectronbeam, andtheydon’trequireanytime\nbetween finishing the draw of the last scan line of one frame and the first scan\nlineofthenext. Butthev-blankintervalstillexists,inpartbecausevideostan-\ndards were established when CRTs were the norm, and in part because of the\nneed to support older displays.\nWaiting for the v-blank interval is called v-sync. It is really just another\nform of frame-rate governing, because it effectively clamps the frame rate of\nthe main game loop to a multiple of the screen’s refresh rate. For example, on\nan NTSC monitor that refreshes at a rate of 60 Hz, the game’s real update rate\n8.5. Measuring and Dealing with Time 539\nis effectively quantized to a multiple of 1/60 of a second. If more than 1/60 of\nasecondelapsesbetweenframes,wemustwaituntilthenextv-blankinterval,\nwhich means waiting 2/60 of a second (30 FPS). If we miss two v-blanks, then\nwe must wait a total of 3/60 of a second (20 FPS) and so on. Also, be careful\nnot to make assumptions about the frame rate of your game, even when it is\nsynchronized to the v-blank interval; if your game supports them, you must\nkeepinmindthatthePALandSECAMstandardsarebasedaroundanupdate\nrate of 50 Hz, not 60 Hz.\n8.5.3 Measuring Real Time with a High-Resolution Timer\nWe’ve talked a lot about measuring the amount of real “wall clock” time that\nelapses during each frame. In this section, we’ll investigate how such timing\nmeasurements are made in detail.\nMost operating systems provide a function for querying the system time,\nsuch as the C standard library function time() . However, such functions\nare not suitable for measuring elapsed times in a real-time game because they\ndo not provide sufficient resolution. For example, time() returns an integer\nrepresenting the number of secondssince midnight, January 1, 1970, so its res-\nolution is one second—far too coarse, considering that a frame takes only tens\nof milliseconds to execute.\nAll modern CPUs contain a high-resolution timer, which is usually imple-\nmented as a hardware register that counts the number of CPU cycles (or\nsome multiple thereof) that have elapsed since the last time the processor\nwas powered on or reset. This is the timer that we should use when mea-\nsuring elapsed time in a game, because its resolution is usually on the order\nof the duration of a few CPU cycles. For example, on a 3 GHz Pentium pro-\ncessor, the high-resolution timer increments once per CPU cycle, or 3 billion\ntimes per second. Hence the resolution of the high-res timer is 1/3billion =\n3.3310 10seconds =0.333ns(one-thirdofananosecond). Thisismorethan\nenough resolution for all of our time-measurement needs in a game.\nDifferent microprocessors and different operating systems provide differ-\nent ways to query the high-resolution timer. On a Pentium, a special instruc-\ntion called rdtsc(read time-stamp counter) can be used, although the Win32\nAPIwrapsthisfacilityinapairoffunctions: QueryPerformanceCounter()\nreads the 64-bit counter register and QueryPerformanceFrequency() re-\nturns the number of counter increments per second for the current CPU. On\na PowerPC architecture, such as the chips found in the Xbox 360 and PlaySta-\ntion3,theinstruction mftb(movefromtimebaseregister)canbeusedtoread\nthe two 32-bit time base registers, while on other PowerPC architectures, the\ninstruction mfspr(move from special-purpose register) is used instead.\n540 8. The Game Loop and Real-Time Simulation\nA CPU’s high-resolution timer register is 64 bits wide on most processors,\ntoensurethatitwon’twraptoooften. Thelargestpossiblevalueofa64-bitun-\nsignedintegeris0xFFFFFFFFFFFFFFFF 1.81019clockticks. So,ona3GHz\nPentiumprocessorthatupdatesitshigh-restimeronceperCPUcycle,thereg-\nister’s value will wrap back to zero once every 195 years or so—definitely not\na situation we need to lose too much sleep over. In contrast, a 32-bit integer\nclock will wrap after only about 1.4 seconds at 3 GHz.\n8.5.3.1 High-Resolution Clock Drift\nBe aware that even timing measurements taken via a high-resolution timer\ncan be inaccurate in certain circumstances. For example, on some multi-\ncore processors, the high-resolution timers are independent on each core, and\nthey can (and do) drift apart. If you try to compare absolute timer readings\ntaken on different cores to one another, you might end up with some strange\nresults—even negative time deltas. Be sure to keep an eye out for these kinds\nof problems.\n8.5.4 Time Units and Clock Variables\nWhenever we measure or specify time durations in a game, we have two\nchoices to make:\n1. What time units should be used? Do we want to store our times in sec-\nonds, or milliseconds, or machine cycles…or in some other unit?\n2. What data type should be used to store time measurements? Should we\nemploy a 64-bit integer, or a 32-bit integer, or a 32-bit floating point\nvariable?\nThe answers to these questions depend on the intended purpose of a given\nmeasurement. This gives rise to two more questions: How much precision do\nweneed? Andwhatrangeofmagnitudesdoweexpecttobeabletorepresent?\n8.5.4.1 64-Bit Integer Clocks\nWe’ve already seen that a 64-bit unsigned integer clock, measured in machine\ncycles, supports both an extremely high precision (a single cycle is 0.333 ns in\nduration on a 3 GHz CPU) and a broad range of magnitudes (a 64-bit clock\nwrapsonceroughlyevery195yearsat3GHz). Sothisisthemostflexibletime\nrepresentation, presuming you can afford 64 bits worth of storage.\n8.5. Measuring and Dealing with Time 541\n8.5.4.2 32-Bit Integer Clocks\nWhen measuring relatively short durations with high precision, we can turn\nto a 32-bit integer clock, measured in machine cycles. For eample, to profile\nthe performance of a block of code, we might do something like this:\n// Grab a time snapshot.\nU64 begin_ticks =readHiResTimer();\n// This is the block of code whose performance we wish\n// to measure.\ndoSomething();\ndoSomethingElse();\nnowReallyDoSomething();\n// Measure the duration.\nU64 end_ticks =readHiResTimer ();\nU32 dt_ticks = static_cast< U32>( end_ticks -begin_ticks);\n// Now use or cache the value of dt_ticks...\nNotice that we still store the raw time measurements in 64-bit integer vari-\nables. Onlythetimedelta dtisstoredina32-bitvariable. Thiscircumventspo-\ntential problems with wrapping at the 32-bit boundary. For example, if begin\n_ticks = 0x12345678FFFFFFB7 and end_ticks = 0x1234567 900000039,\nthen we would measure a negative time delta if we were to truncate the in-\ndividual time measurements to 32 bits each prior to subtracting them.\n8.5.4.3 32-Bit Floating-Point Clocks\nAnother common approach is to store relatively small time deltas in floating-\npoint format, measured in units of seconds. To do this, we simply multiply a\nduration measured in CPU cycles by the CPU’s clock frequency, which is in\ncycles per second. For example:\n// Start off assuming an ideal frame time (30 FPS).\nF32 dt_seconds = 1.0f / 30.0f;\n// Prime the pump by reading the current time.\nU64 begin_ticks =readHiResTimer();\nwhile (true) // main game loop\n{\nrunOneIterationOfGameLoop (dt_seconds );\n// Read the current time again, and calculate the delta.\n542 8. The Game Loop and Real-Time Simulation\nU64 end_ticks =readHiResTimer ();\n// Check our units: seconds = ticks / (ticks/second)\ndt_seconds = (F32)( end_ticks -begin_ticks )\n/ (F32) getHiResTimerFrequency();\n// Use end_ticks as the new begin_ticks for next frame.\nbegin_ticks =end_ticks;\n}\nNotice once again that we must be careful to subtract the two 64-bit time\nmeasurements beforeconverting them into floating-point format. This ensures\nthat we don’t store too large a magnitude into a 32-bit floating-point variable.\n8.5.4.4 Limitations of Floating-Point Clocks\nRecall that in a 32-bit IEEE float, the 23 bits of the mantissa are dynamically\ndistributed between the whole and fractional parts of the value, by way of\nthe exponent (see Section 3.3.1.4). Small magnitudes require only a few bits,\nleaving plenty of bits of precision for the fraction. But once the magnitude of\nour clock grows too large, its whole part eats up more bits, leaving fewer bits\nfor the fraction. Eventually, even the least-significant bits of the whole part\nbecome implicit zeros. This means that we must be cautious when storing\nlongdurations inafloating-pointclockvariable. Ifwekeeptrackoftheamount\nof time that has elapsed since the game was started, a floating-point clock will\neventually become inaccurate to the point of being unusable.\nFloating-point clocks are usually only used to store relatively short time\ndeltas, measuring at most a few minutes and more often just a single frame\nor less. If an absolute-valued floating-point clock is used in a game, you will\nneedtoresettheclocktozeroperiodicallytoavoidaccumulationoflargemag-\nnitudes.\n8.5.4.5 Other Time Units\nSomegameenginesallowtimingvaluestobespecifiedinagame-definedunit\nthatisfine-grainedenoughtopermitanintegerformattobeused(asopposed\nto requiring a floating-point format), precise enough to be useful for a wide\nrange of applications within the engine, and yet large enough that a 32-bit\nclock won’t wrap too often. One common choice is a 1/300 second time unit.\nThisworkswellbecause(a)itisfine-grainedenoughformanypurposes, (b)it\nonly wraps once every 165.7 days and (c) it is an even multiple of both NTSC\nandPALrefreshrates. A60FPSframewouldbe5suchunitsinduration,while\na 50 FPS frame would be 6 units in duration.\n8.5. Measuring and Dealing with Time 543\nObviously a 1/300 second time unit is not precise enough to handle subtle\neffects, like time-scaling an animation. (If we tried to slow a 30 FPS animation\ndown to less than 1/10 of its regular speed, we’d be out of precision!) So for\nmanypurposes,it’sstillbesttousefloating-pointtimeunitsormachinecycles.\nBut a 1/300 second time unit can be used effectively for things like specifying\nhow much time should elapse between the shots of an automatic weapon, or\nhow long an AI-controlled character should wait before starting his patrol, or\nthe amount of time the player can survive when standing in a pool of acid.\n8.5.5 Dealing with Breakpoints\nWhen your game hits a breakpoint, its loop stops running and the debugger\ntakesover. However, ifyourgameisrunningonthesamecomputerthatyour\ndebuggerisrunningon,thentheCPUcontinuestorun,andthereal-timeclock\ncontinues to accrue cycles. A large amount of wall clock time can pass while\nyou are inspecting your code at a breakpoint. When you allow the program\nto continue, this can lead to a measured frame time many seconds, or even\nminutes or hours in duration!\nClearlyifweallowsuchahugedeltatimetobepassedtothesubsystemsin\nour engine, bad things will happen. If we are lucky, the game might continue\nto function properly after lurching forward many seconds in a single frame.\nWorse, the game might just crash.\nA simple approach can be used to get around this problem. In the main\ngame loop, if we ever measure a frame time in excess of some predefined up-\nper limit (e.g., 1 second), we can assume that we have just resumed execution\nafter a breakpoint, and we set the delta-time artificially to 1/30 or 1/60 of a\nsecond (or whatever the target frame rate is). In effect, the game becomes\nframe-lockedforoneframe, inordertoavoidamassivespikeinthemeasured\nframe duration.\n// Start off assuming the ideal dt (30 FPS).\nF32 dt = 1.0f / 30.0f;\n// Prime the pump by reading the current time.\nU64 begin_ticks = readHiResTimer();\nwhile (true) // main game loop\n{\nupdateSubsystemA(dt);\nupdateSubsystemB(dt);\n// ...\nrenderScene();\nswapBuffers();",23318
57-8.6 Multiprocessor Game Loops.pdf,57-8.6 Multiprocessor Game Loops,"544 8. The Game Loop and Real-Time Simulation\n// Read the current time again, and calculate an\n// estimate of next frame's delta time.\nU64 end_ticks = readHiResTimer();\ndt = (F32)(end_ticks - begin_ticks)\n/ (F32)getHiResTimerFrequency();\n// If dt is too large, we must have resumed from a\n// breakpoint -- frame-lock to the target rate this\n// frame.\nif (dt > 1.0f)\n{\ndt = 1.0f/30.0f;\n}\n// Use end_ticks as the new begin_ticks for next frame.\nbegin_ticks = end_ticks;\n}\n8.6 Multiprocessor Game Loops\nIn Chapter 4, we explored the parallel computing hardware that is now ubiq-\nuitous in consumer-grade computers, mobile devices and game consoles, and\nwe learned the mechanics of how to write concurrent software that takes ad-\nvantage of these parallel computing resources. In this section, we’ll discuss\nvariouswaysinwhichthisknowledgecanbeappliedtoagameengine’s game\nloop.\n8.6.1 Task Decomposition\nTo take advantage of parallel computing hardware, we need to decompose the\nvarious tasks that are performed during each iteration of our game loop into\nmultiple subtasks, each of which can be executed in parallel. This act of de-\ncomposition transforms our software from a sequential program into a concur-\nrentone.\nTherearemanywaystodecomposeasoftwaresystemforconcurrency,but\nas we discussed in Section 4.1.3, we can place all of them roughly into one of\ntwo categories: task parallelism anddataparallelism .\nTask parallelism naturally fits situations in which a number of different\nthingsneedtobedone,andweopttodotheminparallelacrossmultiplecores.\nForexample,wemightattempttoperformanimationblendinginparallelwith\ncollisiondetectionduringeachiterationofourgameloop,orwemightsubmit\n8.6. Multiprocessor Game Loops 545\nprimitivesto the GPUfor renderingof frame Ninparallel withstarting to up-\ndate the state of the game world for frame N+1.\nData parallelism is best suited to situations in which a singlecomputation\nneeds to be performed repeatedly on a large number of data elements. The\nGPU is probably the best example of data parallelism in action—a GPU per-\nforms millions of per-pixel and per-vertex calculations every frame by dis-\ntributing the work across a large number of processing cores that operate in\nparallel. However, as we’ll see in the following sections, data parallelism isn’t\nonly found on the GPU—there are plenty of tasks performed by the CPU dur-\ning the game loop that can benefit from data parallelism as well.\nIn the following sections, we’ll take a look at a number of different ways\nto subdivide the work that’s done by the game loop, some of which employ a\ntask-parallel approach, others of which relyon data parallelism. We’llexplore\nthe pros and cons of each approach, and then see how a general-purpose job\nsystemcanbeausefultoolforconvertingliterallyanyworkloadintoaconcur-\nrent operation that can take advantage of hardware parallelism.\n8.6.2 One Thread per Subsystem\nOne simple way to decompose our game loop for concurrency would be to\nassign particular engine subsystems to run in separate threads. For exam-\nple, the rendering engine, the collision and physics simulation, the anima-\ntion pipeline, and the audio engine could each be assigned to its own thread.\nA master thread would control and synchronize the operations of these sec-\nondarysubsystemthreads, andwouldalsocontinuetohandlethelion’sshare\nof the game’s high-level logic (the main game loop). On a hardware platform\nwith multiple physical CPUs, this design would allow the threaded engine\nsubsystems to execute in parallel with one another and with the main game\nloop. This is a simple example of task parallelism; it is depicted in Figure 8.4.\nThereareanumberofproblemswiththesimpleapproachofassigningeach\nengine subsystem to its own thread. For one thing, the number of engine sub-\nsystems probably won’t match the number of cores on our gaming platform.\nAs a result, we’ll probably end up with more threads than we have cores, and\nsome subsystem threads will need to share a core via time-slicing.\nAnotherproblemisthateachenginesubsystemrequiresadifferentamount\nof processing each frame. This means that while some threads (and their cor-\nresponding CPU cores) are highly utilized each frame, others may sit idle for\nlarge portions of the frame.\nYet another issue is that some engine subsystems dependon the data pro-\nduced by others. For example, the rendering and audio subsystems cannot\n546 8. The Game Loop and Real-Time Simulation\nSkin Matrix \nPalette\nCalcFinalize Animatio nMain\nThread\nHID\nUpdate Game \nObjects\nKick Off Animation\nKick Dynamics Sim\nFinalize CollisionRagdoll Physics\nKick Rendering\n(for next frame)Pose\nBlending\nRagdoll\nSkinningGlobal \nPose CalcPost Animation \nGame Object Update\nSleepSleep\nSimulate\nand\nIntegrate\nSleepSleepVisibility\nDetermination\nSubmit\nPrimitives\nFull-Screen \nEffects\nWait for V-\nBlank\nSwap BuffersSort\nOther Processing\n(AI Planning, Audio \nWork, etc.)Broad\nPhase Coll.\nNarrow\nPhase Coll.\nReso lve \nCollisions,\nConstraintsWait for GPUSleepAnimation\nThreadDynamics\nThreadRendering\nThread\nSleep\nFigure 8.4. One thread per major engine subsystem.\nstart doing their work for frame Nuntil the animation, collision and physics\nsystems have completed their work for frame N. We cannot run two subsys-\ntems in parallel if they depend on one another like this.\nAs a result of all of these problems, attempting to assign each engine sub-\nsystem to its own thread is really not a practical design. We can do better.\n8.6.3 Scatter/Gather\nMany of the tasks performed during a single iteration of the game loop are\ndata-intensive. For example, we may need to process a large number of ray\ncast requests, blend together a large batch of animation poses, or calculate\nworld-spacematrices foreveryobject ina massiveinteractive scene. One way\nto take advantage of parallel computing hardware to perform these kinds of\ntasks is to use a divide-and-conquer approach. Instead of attempting to pro-\ncess 9000 ray casts one at a time on a single CPU core, we can divide the work\ninto,say,sixbatchesof1500raycastseach,andthenexecuteonebatchoneach\n8.6. Multiprocessor Game Loops 547\nMain \nThread\nHID\nUpdate Game \nObjects\nRagdoll PhysicsPost Animation \nGame Object UpdateScatter\nGather\nScatter\nGather\netc.Pose \nBlendingPose \nBlendingPose \nBlending\nSimulate/\nIntegrateSimulate/\nIntegrateSimulate/\nIntegrate\nFigure 8.5. Scatter/gather used to parallelize selected CPU-intensive parts of the game loop.\nof the six1CPU cores on our PS4 or Xbox One. This approach is a form of data\nparallelism .\nIn distributed systems terminology, this is known as a scatter/gather ap-\nproach, because a unit of work is divided into smaller subunits, distributed\nonto multiple processing cores for execution (scatter), and then the results are\ncombinedorfinalizedinanappropriatemanneronceallworkloadshavebeen\ncompleted (gather).\n8.6.3.1 Scatter/Gather in the Game Loop\nIn the context of our game loop, one or more scatter/gather operations might\nbe performed, at various times during one iteration of the game loop, by the\n“master” game loop thread. This architecture is illustrated in Figure 8.5.\nGivenadatasetcontaining Ndataitemsthatrequireprocessing,themaster\nthread would divide the work into mbatches, each containing roughly N/m\nelements. (Thevalueof mwouldprobablybedeterminedbasedonthenumber\nofavailablecoresinthesystem, althoughthismaynotbethecaseif,forexam-\nple,wewishtoleavesomecoresfreeforotherwork.) Themasterthreadwould\nthen spawn mworker threads and provide each with a start index and count,\n1Both the PS4 and the Xbox One allow developers to access some of the processing power of a\nseventhcore. Notallofitsbandwidthisavailabletodevelopersbecausethiscoreisalsoutilizedby\nthe operating system. The eighth core in these 8-core machines is entirely off-limits. This is done\nto allow for the fact that some faulty cores are inevitably produced during any CPU’s fabrication\nprocess.\n548 8. The Game Loop and Real-Time Simulation\nallowingittoprocessitsassignedsubsetofthedata. Eachworkerthreadmight\nupdatethedataitemsin-place,or(usuallybetter)itmightproduceoutputdata\ninto a separate preallocated buffer (one per worker thread).\nHavingsuccessfullyscatteredtheworkload,themasterthreadwouldthen\nbefreetoperformsomeotherusefulworkwhileitwaitsfortheworkerthreads\nto complete their tasks.\nAtsomepointlaterintheframe,themasterthreadwouldgathertheresults\nby waiting until all of the worker threads have terminated, perhaps using a\nfunction such as pthread_join() . If the worker threads have all exited,\nthis function will return immediately; however, if any worker threads are still\nrunning, this call would have the effect of putting the master thread to sleep.\nOnce the gather step has been completed, the master thread could poten-\ntially combine the results in whatever manner is appropriate. For example,\nafter blending animations together, the next step might be to calculate skin-\nningmatrices—thisstepwouldbekickedoffonlyafterallanimationblending\nthreads have completed their work. We presented an example very much like\nthis in Section 4.4.6 when we covered thread creation and joining.\n8.6.3.2 SIMD for Scatter/Gather\nIn Section 4.10, we explored loop vectorization as a means of leveraging SIMD\nparallelism to improve the performance of data-intensive work. This is really\njustanotherformofthescatter/gatherapproach,performedataveryfinelevel\nof granularity. SIMD could be used in lieu of thread-based scatter/gather, but\nit would likely be used in conjunction with it (with each worker thread utiliz-\ning vectorization internally to perform its work).\n8.6.3.3 Making Scatter/Gather More Efﬁcient\nThe scatter/gather approach is an intuitive way to distribute data-intensive\nwork across multiple cores. However, as we’ve described it above, this paral-\nlelization method does suffer from one big problem—spawning threads is ex-\npensive. Spawning a thread involves a kernel call, as does joining the master\nthread with its workers. The kernel itself does quite a lot of set-up and tear-\ndown work whenever threads come and go. So spawning a bunch of threads\nevery time we want to perform a scatter/gather is impractical.\nWe could mitigate the costs of spawning threads by using of a pool of pre-\nspawned threads. Some operating systems like Windows provide an API for\ncreating and managing a pool of threads. (See for example https://bit.ly/\n2H8ChIp.) In the absence of such an API, you can always implement a sim-\nple thread pool yourself, using condition variables, semaphores, atomic Boolean\n8.6. Multiprocessor Game Loops 549\nvariables, or some other mechanism in order to synchronize the activities of\nthe threads.\nWe’d like our thread pool to be capable of performing a wide variety of\nscatter/gather operations over the course of each frame. This means that we\ncannolongersimplyspawnabunchofthreadsforeachscatter/gather,whose\nentry point function does one particular computation. Instead, each thread in\nthe pool would have to be capable of performing any of the scatter/gather\noperations we might want to perform during any iteration of our game loop.\nWe could imagine using a giant switch statement to implement this, but that\nidea sounds clunky, ugly and hard to maintain. Really, what we want is a\ngeneral-purpose system for executing units of work concurrently, across the\navailable cores on our target hardware.\n8.6.4 Job Systems\nAjobsystem isageneral-purposesystemforexecutingarbitraryunitsofwork,\ntypically called jobs, across multiple cores. With a job system, a game pro-\ngrammer can subdivide each iteration of the game loop into a relatively large\nnumberofindependentjobs,andsubmitthemtothejobsystemforexecution.\nThe job system maintains a queue of submitted jobs, and it schedules those\njobs across the available cores, either by submitting them to be executed by\nworkerthreadsinathreadpool, orbysomeothermeans. Inasense, ajobsys-\ntemislikea custom, light-weightoperatingsystemkernel, exceptthat instead\nof scheduling threads to run on the available cores, it schedules jobs.\nJobs can be arbitrarily fine-grained, and in a real game engine many are\nindependent of one another. As shown in Figure 8.6, these facts help to maxi-\nmize processor utilization. This architecture also scales up or down naturally\nto hardware with any number of CPU cores.\n8.6.4.1 Typical Job System Interface\nAtypicaljobsystemprovidesasimple,easy-to-useAPIthatlooksverysimilar\nto the API for a threading library. There’s a function for spawning a job (the\nequivalent of pthread_create(), often called kickinga job), a function that\nallows one job to wait for one or more other jobs to terminate (the equivalent\nofpthread_join()), and perhaps a way for a job to self-terminate “early”\n(before returning from its entry point function). A job system must also pro-\nvide spin locks of mutexes of some kind for performing critical concurrent\noperations in an atomicmanner. It may also provide facilities for putting jobs\nto sleep and waking them back up, via condition variables or some similar\nmechanism.\n550 8. The Game Loop and Real-Time Simulation\nMatrix PaletteFinalize AnimationCPU0\nHID\nUpdate Game \nObjects\nKick Animation Jobs\nKick Dynamics Jobs\nFinalize CollisionRagdoll Physics\nKick Rendering\n(for next frame)Ragdoll \nSkinningPost Animation \nGame Object Update\nOther Processing\n(AI Planning, Audio \nWork, etc.)CPU1\nVisibility\nVisibility\nSort\nPose BlendSort\nVisibility\nSort\nPose Blend\nPhysics Sim\nSubmit Prims\nGobal Pose\nSubmit Prims\nGobal Pose\nCollisions/\nConstraints\nMatrix Palette\nRagdoll \nSkinningCPU2\nVisibility\nSort\nVisibility\nPose BlendVisibility\nVisibility\nPose Blend\nPose Blend\nSort\nPhysics Sim\nGobal Pose\nBroad Phase\nNarrow \nPhase\nNarrow \nPhase…\nFigure 8.6. In a job model, work is broken down into ﬁne-grained chunks that can be picked up\nby any available processor. This can help maximize processor utilization while providing the main\ngame loop with improved ﬂexibility.\nTokickajob, weneedtotellthejobsystem whatjobtoperform, and howto\nperform it. Normally this information is passed to the KickJob() function\nvia a small data structure which we’ll call a job declaration here.\nAt minimum, a job declaration must contain a pointer to the job’s entry\npointfunction. It’salsoimportanttobeabletopassarbitraryinputparameters\nto the job’s entry point function. This could be done in various ways, but the\nsimplest is to provide a single parameter of type uintptr_t, which will be\npassed to the entry point function when the job actually runs. This allows us\ntopasssimpleinformationlikeasingleBooleanorintegertothejobwithlittle\nfuss; but because a pointer can be safely cast to a uintptr_t, we can also use\nthis job parameter to pass a pointer to an arbitrary data structure, that itself\ncontains whatever input parameters the job might require.\nAjobsystemmayalsoprovideaprioritymechanism,muchasmostthread\nlibraries do. In this case, a priority might also be included within the job dec-\nlaration.\nHere’s an example of a simple job declaration, along with a simple job sys-\ntem API:\n8.6. Multiprocessor Game Loops 551\nnamespace job\n{\n// signature of all job entry points\ntypedef void EntryPoint(uintptr_t param);\n// allowable priorities\nenum class Priority\n{\nLOW, NORMAL, HIGH, CRITICAL\n};\n// counter (implementation not shown)\nstruct Counter ... ;\nCounter* AllocCounter ();\nvoid FreeCounter(Counter* pCounter);\n// simple job declaration\nstruct Declaration\n{\nEntryPoint* m_pEntryPoint;\nuintptr_t m_param;\nPriority m_priority;\nCounter* m_pCounter;\n};\n// kick a job\nvoid KickJob(const Declaration& decl);\nvoid KickJobs(int count, const Declaration aDecl[]);\n// wait for job to terminate (for its Counter to become zero)\nvoid WaitForCounter (Counter* pCounter);\n// kick jobs and wait for completion\nvoid KickJobAndWait (const Declaration& decl);\nvoid KickJobsAndWait (int count, const Declaration aDecl[]);\n}\nYou’ll notice there’s an opaque type here called job::Counter. Counters al-\nlowonejobtogotosleepandwaituntiloneormoreotherjobshavecompleted\nexecuting. We’ll discuss job counters in Section 8.6.4.6.\n8.6.4.2 A Simple Job System Based on a Thread Pool\nAs we hinted at earlier, it’s possible to build a job system around a pool of\nworker threads. It’s a good idea to spawn one thread for each CPU that’s\npresent in the target machine, and use affinity to lock each thread to one core.\nEach worker thread sits in an infinite loop processing job requests that are\n552 8. The Game Loop and Real-Time Simulation\nprovided to it by other threads (and/or other jobs). At the top of this infi-\nnite loop, the worker thread goes to sleep (possibly via a condition variable),\nwaiting for a job request to become available. When a request is received, the\nworker thread wakes up, calls the job’s entry point function and passes the\ninput parameter from the job declaration to it. When the entry point function\nreturns, that indicates that the work is complete, so the worker thread goes\nback to the top of its infinite loop to execute more jobs. If none are available,\nit goes back to sleep, waiting for another job request to come in.\nHere’s what a job worker thread might look like under the hood. Please\nbearinmindthatthisisnotacompleteimplementation;it’sjustforillustrative\npurposes.\nnamespace job\n{\nvoid* JobWorkerThread (void*)\n{\n// keep on running jobs forever...\nwhile (true)\n{\nDeclaration declCopy;\n// wait for a job to become available\npthread_mutex_lock(&g_mutex);\nwhile (!g_ready)\n{\npthread_cond_wait (&g_jobCv, &g_mutex);\n}\n// copy the JobDeclaration locally and\n// release our mutex lock\ndeclCopy = GetNextJobFromQueue();\npthread_mutex_unlock(&g_mutex);\n// run the job\ndeclCopy.m_pEntryPoint (declCopy.m_param);\n// job is done! rinse and repeat...\n}\n}\n}\n8.6.4.3 A Limitation of Thread-Based Jobs\nLet’s imagine writing a job that updates the state of an AI-driven non-player\ncharacter(NPC).Thejob’sentrypointfunctionmightlooksomethinglikethis:\n8.6. Multiprocessor Game Loops 553\nvoid NpcThinkJob(uintparam_t param)\n{\nNpc* pNpc = reinterpret_cast<Npc*>(param);\npNpc->StartThinking();\npNpc->DoSomeMoreUpdating();\n// ...\n// now let's cast a ray to see if we're aiming\n// at anything interesting -- this involves\n// kicking off another job that will run on\n// a different code (worker thread)\nRayCastHandle hRayCast = CastGunAimRay (pNpc);\n// the results of the ray cast aren't going to\n// be ready until later this frame, so let's\n// go to sleep until it's ready\nWaitForRayCast (hRayCast);\n// zzz...\n//wake up!\n// now fire my weapon, but only if the ray\n// cast indicates that we are aiming at an\n// enemy\npNpc->TryFireWeaponAtTarget(hRayCast);\n// ...\n}\nThis job seems simple enough: It performs some updates, kicks off a ray\ncast job (on another worker thread/core) to determine at what object the NPC\nis aiming his gun. The NPC then fires his weapon, but only if the ray cast\nreports that an enemy is in his sights.\nUnfortunatelythiskindofjobwouldn’tworkifwetriedtorunitwithinthe\nsimple job system that we described in the previous section. The problem is\nthe call to WaitForRayCast(). In our simple thread-pool-based job system,\nevery job must runtocompletion once it starts running. It cannot “go to sleep”\nwaitingfortheresultsoftheraycast, allowingotherjobstorunontheworker\nthread, and then “wake up” later, when the ray cast results are ready.\nThislimitationarisesbecauseinoursimplesystem,eachrunningjobshares\nthe same call stack as the worker thread that invoked it. To put a job to sleep,\nwewouldneedtoeffectively contextswitch toanotherjob. Thatwouldinvolve\nsaving off the outgoing job’s call stack and registers, and then overwriting the\n554 8. The Game Loop and Real-Time Simulation\nworkerthread’scallstackwiththeincomingjob’scallstack. There’snosimple\nway to do that when using such a simple job execution method.\n8.6.4.4 Jobs as Coroutines\nOne way to overcome this problem would be to change from a thread pool\nbased job system to one based on coroutines . Recall from Section 4.4.8 that a\ncoroutine possesses one important quality that ordinary threads do not: The\nabilityto yieldtoanothercoroutinepart-waythroughitsexecution,andtocon-\ntinue from where it left off at some later time when another coroutine yields\ncontrolbacktoit. Coroutinescanyieldtooneanotherlikethisbecausetheim-\nplementation actually does swap the callstacks of the outgoing and incoming\ncoroutines within the thread in which the coroutines are running. So unlike a\npurely thread-based job, a coroutine-based job could effectively “go to sleep”\nand allow other jobs to run while it waits for an operation such as a ray cast to\ncomplete.\n8.6.4.5 Jobs as Fibers\nAnother way to allow jobs to sleep and yield to one another is to implement\nthem with fibersinstead of threads. Recall from Section 4.4.7 that the key dif-\nference between fibers and threads is that context switches between fibers are\nalwayscooperative, never preemptive . A fiber-based system begins by convert-\ning one of its threads into a fiber. The thread will continue to run that fiber\nuntilitexplicitlycalls SwitchToFiber() toexplicitly yieldcontroltoanother\nfiber. Just as with coroutines, the entire call stack of a fiber is saved off when-\never a context switch to another fiber occurs. Fibers can even migrate from\none thread to another. This makes them well suited for use in implementing\na job system. Naughty Dog’s job system is based on fibers.\n8.6.4.6 Job Counters\nIf we implement our job system with coroutines or fibers, we have the ability\nto put a job to sleep (saving off its execution context), and to wake it back up\nat a future time (thus restoring its execution context). This in turn permits us\nto implement a joinfunction for our job system—a function that causes the\ncalling job to go to sleep, waiting until one or more other jobs have completed\nexecuting. Suchafunctionwouldberoughlyequivalentto pthread_join()\nin the POSIX thread library or WaitForSingleObject() under Windows.\nOne way to implement this would be to associate a handlewith each job,\njust as threads have handles in most thread libraries. Waiting for a job would\nthen amount to calling a job::Join() function of some kind, passing the\nhandle of the job for which you wish to wait.\n8.6. Multiprocessor Game Loops 555\nOne downside of the handle-based approach is that it doesn’t scale well\nto waiting for large numbers of jobs. Also, to wait for an individual job to\ncomplete, we’d need to poll periodically to check the status of all jobs in the\nsystem. SuchpollingwouldwastevaluableCPUcycles. Forthesereasons,the\njobsystemAPIpresentedaboveintroducestheconceptofa counter,whichacts\nabitlikeasemaphore,onlyinreverse. Wheneverajobiskicked,itcanoption-\nallybeassociatedwithacounter(providedtoitviathe job::Declaration).\nTheactofkickingthejob incrementsthecounter, andwhenthejob terminates\nthe counter is decremented. Waiting for a batch of jobs, then, involves simply\nkickingthemalloffwiththesamecounter,andthenwaitinguntilthatcounter\nreacheszero(whichindicatesthatalljobshavecompletedtheirwork.) Waiting\nuntil a counter reaches zero is much more efficient than polling the individual\njobs,becausethecheckcanbemadeatthemomentthecounterisdecremented.\nAs such, a counter based system can be a performance win. Counters like this\nare used in the Naughty Dog job system.\n8.6.4.7 Job Synchronization Primitives\nAny concurrent program requires a mechanism for performing atomic opera-\ntions, and a job system is no different. Just as a threading library provides a\nsetofthreadsynchronizationprimitives suchasmutexes,conditionvariablesand\nsemaphores,sotoomustajobsystemprovideacollectionof jobsynchronization\nprimitives.\nThe implementation of job synchronization primitives varies somewhat\ndependingonhowthejobsystemisactuallyimplemented. Butthey’reusually\nnotsimply wrappers around the kernel’s thread synchronization primitives.\nTo see why, consider what an OS mutex does: It puts a thread to sleep when-\never the lock it’s trying to acquire is already being held by another thread. If\nwe were to implement our job system as a thread pool, then waiting on a mu-\ntex within a job would put the entireworkerthread to sleep, not just the one job\nthat wants to wait for the lock. Clearly this would pose a serious problem,\nbecause no jobs would be able to run on that thread’s core until the thread\nwakes back up. Such a system would very likely be fraught with deadlock\nissues.\nTo overcome this problem, jobs could use spinlocks instead of OS mutexes.\nThis approach works well as long as there’s not very much lock contention\nbetweenthreads,becauseinthatcasenojobwilleverbusy-waitforeverylock\ntrying to obtain a lock. Naughty Dog’s job system uses spin locks for most of\nits locking needs.\nSometimes, however, a job may experience a high-contention situation. A\nwell-designed job system can handle this via a custom “mutex” mechanism\n556 8. The Game Loop and Real-Time Simulation\nthat can put a job to sleep while it waits for a resource to become available.\nSuch a mutex might start by busy-waiting when a lock can’t be obtained. If\nafter a brief timeout the lock is still unavailable, the mutex could yield the\ncoroutine or fiber to another waiting job, thereby putting the waiting job to\nsleep. Just as the kernel keeps track of all sleeping threads that are waiting on\na mutex, our job system would need to keep track of all sleeping jobs so that\nit can wake them up when their mutexes free up.\n8.6.4.8 Job Visualization and Proﬁling Tools\nOnce you start using a job system, the graph of running jobs and their depen-\ndencies becomes large and complex very quickly. It’s a good idea to provide\nvisualization and profiling tools with any job system.\nFor example, the Naughty Dog job system offers a visualization view like\ntheoneshowninFigure8.7. Inthisdisplay,youcanseeeachofthesevencores\n(plustheGPU)listedverticallyalongtheleft-handedge. Timeprogressesfrom\nleft to right, and each logical frame is demarked with vertical markers. Along\neach core’s timeline, thin blocks represent the various jobs that ran during the\ngiven frame. Beneath each job are additional rows of thin rectangles—these\nrepresentthecallstackofthejob(whichfunctionsitcalled,andhowlongeach\none took to run).\nFigure 8.7. The job system used in Uncharted: The Lost Legacy (© 2017/™ SIE. Created and devel-\noped by Naughty Dog, PlayStation 4) and Naughty Dog’s other PS4 games offers a visualization\ntool that shows which jobs ran on each core over the course of a given frame. Time increases\nfrom left to right. Jobs are represented by thin boxes along the timeline of each core. The func-\ntions called by each job are shown beneath it as additional thin boxes.\n8.6. Multiprocessor Game Loops 557\nJobs are color coded by their function, so users can quickly zero in on a\nparticular job. For example, let’s say we’re looking for a ray cast that’s taking\na particularly long time. If ray cast jobs are colored red, we can visually scan\nthe display for all the red jobs that are wider than we’d like. Clicking on a job\ncausesallotherjobsthatarenotofthesametypetobedrawningrey,allowing\nall jobs of the selected job’s type to be easily seen. Also, when a job is clicked,\nthinlinesconnectittothejobsitkicked,andtothejobthatkickedit. Hovering\nyour mouse over a job, or over one of the functions in its call stack beneath it,\npops up some text with the name of the job or function, and its execution time\nin milliseconds.\nOne other very useful job system feature is what I’ll call a profiletrap. Let’s\nsay an area of the game is running well at 30 FPS most of the time, but once in\na while it dips to 24 FPS. We can set a trap for any frame taking longer than,\nsay,35ms. Thenweplaythegamenormally. Assoonasaframetakinglonger\nthan 35 ms is detected, the game is automatically paused by the trap system,\nand the profile display is put up on-screen. It’s then possible to analyze the\njobs that ran that frame to find the culprit(s) causing the slowdown.\n8.6.4.9 The Naughty Dog Job System\nThe job system used by Naughty Dog on TheLastofUs: Remastered ,Uncharted\n4: AThief’sEnd andUncharted: TheLostLegacy largelyfollowsthedesignofthe\nhypothetical job systems we’ve discussed thus far. It is based on fibers (rather\nthan thread pools or coroutines). It makes use of spin locks and also provides\na special job mutex that can put a job to sleep while it waits for a lock. It uses\ncounters rather than job handles to implement a joinoperation.\nLet’s have a look at how the fiber-based job system used in the Naughty\nDog engine works. When the system first boots up, the main thread converts\nitself to a fiber in order to enable fibers within the process as a whole. Next,\njob worker threads are spawned, one for each of the seven cores available to\ndevelopersonthePS4. ThesethreadsareeachlockedtoonecoreviatheirCPU\naffinity settings, so we can think of these worker threads and their cores as\nbeing roughly synonymous (although in reality, other higher-priority threads\ndo sometimes interrupt the worker threads for very brief periods during the\nframe). Fiber creation is slow on the PS4, so a pool of fibers is pre-spawned,\nalong with memory blocks to serve as each fiber’s call stack.\nWhen jobs are kicked, their declarations are placed onto a queue. As\ncores/worker threads become free (as jobs terminate), new jobs are pulled\nfrom this queue and executed. A running job can also add more jobs to the\njob queue.\nTo execute a job, an unused fiber is pulled from the fiber pool, and the\n558 8. The Game Loop and Real-Time Simulation\nworker thread performs a SwitchToFiber() to start the job running. When\na job returns from its entry point function or otherwise self-terminates, the\njob’s final act is to perform a SwitchToFiber() back to the job system itself.\nItthenselectsanotherjobfromthequeue,andtheprocessrepeatsadinfinitum.\nWhen a job waits on a counter, the job is put to sleep and its fiber (exe-\ncution context) is placed on a wait list, along with the counter it is waiting\nfor. When this counter hits zero, the job is woken back up so it can continue\nwhere it left off. Sleeping and waking jobs is again implemented by calling\nSwitchToFiber() betweenthejob’sfiberandthejobsystem’smanagement\nfibers on each core/worker thread.\nFor an excellent in-depth discussion of how the Naughty Dog job sys-\ntem was built, and why it was built that way, check out Christian Gyrling’s\nexcellent GDC 2015 talk entitled, “Parallelizing the Naughty Dog Engine,”\nwhich is available at https://bit.ly/2H6v0J4. Christian’s slides are available\nat https://bit.ly/2ETr5x9.",31065
58-9 Human Interface Devices.pdf,58-9 Human Interface Devices,,0
59-9.1 Types of Human Interface Devices.pdf,59-9.1 Types of Human Interface Devices,"9\nHuman Interface Devices\nGames are interactive computer simulations, so the human player(s) need\nsome way of providing inputs to the game. All sorts of human interface\ndevices(HID) exist for gaming, including joysticks, joypads, keyboards and\nmice, track balls, the Wii remote and specialized input devices like steering\nwheels,fishingrods,dancepadsandevenelectricguitars. Inthischapter,we’ll\ninvestigate how game engines typically read, process and utilize the inputs\nfrom human interface devices. We’ll also have a look at how outputs from\nthese devices provide feedback to the human player.\n9.1 Types of Human Interface Devices\nA wide range of human interface devices are available for gaming purposes.\nConsoles like the Xbox 360 and PS3 come equipped with joypad controllers,\nas shown in Figures 9.1 and 9.2. Nintendo’s Wii console is well known for\nits unique and innovative Wii Remote controller (commonly referred to as the\n“Wiimote”), shown in Figure 9.3. And with the Wii U, Nintendo has created\naninnovativemixbetweenacontrollerandasemi-mobilegamingdevice(Fig-\nure9.4). PCgamesaregenerallyeithercontrolledviaakeyboardandmouseor\nvia a joypad. (Microsoft designed the Xbox 360 joypad so that it can be used\nboth on the Xbox 360 and on Windows/DirectX PC platforms.) As shown\nin Figure 9.5, arcade machines have one or more built-in controllers, such as\na joystick and various buttons, or a track ball, a steering wheel, etc. An ar-\ncade machine’s input device is usually somewhat customized to the game in\n559\n560 9. Human Interface Devices\nFigure 9.1. Standard joypads for the Xbox 360 and PlayStation 3\nconsoles.\nFigure 9.2. The DualShock 4 joypad for the PlayStation 4.\nFigure 9.3. The innovative Wii Remote for the Nintendo Wii.\n Figure 9.4. The Wii U controller by Nintendo.\nquestion, although input hardware is often reused among arcade machines\nproduced by the same manufacturer.\nOn console platforms, specialized input devices and adapters are usually\navailable, in addition to the “standard” input device such as the joypad. For\nexample, guitar and drum devices are available for the Guitar Hero series of\ngames, steering wheels can be purchased for driving games, and games like\nDance Dance Revolution use a special dance pad device. Some of these devices\nFigure 9.5. Buttons and joysticks for the arcade game Mortal Kombat II by Midway.",2393
60-9.2 Interfacing with a HID.pdf,60-9.2 Interfacing with a HID,"9.2. Interfacing with a HID 561\nFigure 9.6. Many specialized input devices are available for use with consoles.\nFigure 9.7. Steering wheel adapter for the Nintendo Wii.\nare shown in Figure 9.6.\nTheNintendoWiimoteisoneofthemostflexibleinputdevicesonthemar-\nket today. As such, it is often adapted to new purposes, rather than replaced\nwith an entirely new device. For example, Mario Kart Wii comes with a plas-\ntic steering wheel adapter into which the Wiimote can be inserted (see Fig-\nure 9.7).\n9.2 Interfacing with a HID\nAll human interface devices provide input to the game software, and some\nalso allow the software to provide feedback to the human player via various\nkinds of outputs as well. Game software reads and writes HID inputs and\noutputs in various ways, depending on the specific design of the device in\nquestion.\n9.2.1 Polling\nSome simple devices, like game pads and old-school joysticks, are read by\npollingthe hardware periodically (usually once per iteration of the main game\nloop). This means explicitly querying the state of the device, either by read-\n562 9. Human Interface Devices\ning hardware registers directly, reading a memory-mapped I/O port, or via\na higher-level software interface (which, in turn, reads the appropriate regis-\nters or memory-mapped I/O ports). Likewise, outputs might be sent to the\nHID by writing to special registers or memory-mapped I/O addresses, or via\na higher-level API that does our dirty work for us.\nMicrosoft’sXInputAPI,forusewithXbox360gamepadsonboththeXbox\n360 and Windows PC platforms, is a good example of a simple polling mech-\nanism. Every frame, the game calls the function XInputGetState(). This\nfunction communicates with the hardware and/or drivers, reads the data in\ntheappropriatewayandpackagesitallupforconvenientusebythesoftware.\nItreturnsapointertoan XINPUT_STATE struct,whichinturncontainsanem-\nbedded instance of a struct called XINPUT_GAMEPAD. This struct contains the\ncurrent states of all of the controls (buttons, thumb sticks and triggers) on the\ndevice.\n9.2.2 Interrupts\nSome HIDs only send data to the game engine when the state of the controller\nchanges in some way. For example, a mouse spends a lot of its time just sit-\nting still on the mouse pad. There’s no reason to send a continuous stream of\ndata between the mouse and the computer when the mouse isn’t moving—\nwe need only transmit information when it moves, or a button is pressed or\nreleased.\nThiskindofdeviceusuallycommunicateswiththehostcomputervia hard-\nwareinterrupts. Aninterruptisanelectronicsignalgeneratedbythehardware,\nwhich causes the CPU to temporarily suspend execution of the main program\nand run a small chunk of code called an interrupt service routine (ISR). Inter-\nrupts are used for all sorts of things, but in the case of a HID, the ISR code will\nprobably read the state of the device, store it off for later processing, and then\nrelinquish the CPU back to the main program. The game engine can pick up\nthe data the next time it is convenient to do so.\n9.2.3 Wireless Devices\nThe inputs and outputs of a Bluetooth device, like the Wiimote, the Dual-\nShock 3 and the Xbox 360 wireless controller, cannot be read and written by\nsimply accessing registers or memory-mapped I/O ports. Instead, the soft-\nware must “talk” to the device via the Bluetooth protocol. The software can\nrequesttheHIDtosendinputdata(suchasthestatesofitsbuttons)backtothe\nhost, or it can send output data (such as rumble settings or a stream of audio\ndata) to the device. This communication is often handled by a thread separate",3621
61-9.3 Types of Inputs.pdf,61-9.3 Types of Inputs,"9.3. Types of Inputs 563\nfromthegameengine’smainloop, oratleastencapsulatedbehindarelatively\nsimple interface that can be called from the main loop. So from the point of\nview of the game programmer, the state of a Bluetooth device can be made to\nlook pretty much indistinguishable from a traditional polled device.\n9.3 Types of Inputs\nAlthoughhumaninterfacedevicesforgamesvarywidelyintermsofformfac-\ntorand layout, most ofthe inputs they providefall intoone of a small number\nof categories. We’ll investigate each category in depth below.\n9.3.1 Digital Buttons\nAlmost every HID has at least a few digitalbuttons . These are buttons that can\nonly be in one of two states: pressedandnotpressed. Game programmers often\nrefer to a pressed button as being downand a non-pressed button as being up.\nElectrical engineers speak of a circuit containing a switch as being closed\n(meaning electricity is flowing through the circuit) or open(no electricity is\nflowing—the circuit has infinite resistance). Whether closedcorresponds to\npressedornot pressed depends on the hardware. If the switch is normally open ,\nthen when it is not pressed (up), the circuit is open, and when it is pressed\n(down),thecircuitis closed. Iftheswitchis normallyclosed,thereverseistrue—\nthe act of pressing the button opens the circuit.\nIn software, the state of a digital button (pressed or not pressed) is usually\nrepresented by a single bit. It’s common for 0 to represent not pressed (up)\nand 1 to represent pressed (down). But again, depending on the nature of the\ncircuitry and the decisions made by the programmers who wrote the device\ndriver, the sense of these values might be reversed.\nIt is quite common for the states of all of the buttons on a device to be\npacked into a single unsigned integer value. For example, in Microsoft’s XIn-\nput API, the state of the Xbox 360 joypad is returned in a struct called XINPUT\n_GAMEPAD, shown below.\ntypedef struct _XINPUT_GAMEPAD\n{\nWORD wButtons;\nBYTE bLeftTrigger;\nBYTE bRightTrigger;\nSHORT sThumbLX;\nSHORT sThumbLY;\nSHORT sThumbRX;\nSHORT sThumbRY;\n564 9. Human Interface Devices\n} XINPUT_GAMEPAD;\nThis struct contains a 16-bit unsigned integer (WORD) variable named\nwButtons that holds the state of all buttons. The following masks define\nwhich physical button corresponds to each bit in the word. (Note that bits\n10 and 11 are unused.)\n#define XINPUT_GAMEPAD_DPAD_UP 0x0001 // bit 0\n#define XINPUT_GAMEPAD_DPAD_DOWN 0x0002 // bit 1\n#define XINPUT_GAMEPAD_DPAD_LEFT 0x0004 // bit 2\n#define XINPUT_GAMEPAD_DPAD_RIGHT 0x0008 // bit 3\n#define XINPUT_GAMEPAD_START 0x0010 // bit 4\n#define XINPUT_GAMEPAD_BACK 0x0020 // bit 5\n#define XINPUT_GAMEPAD_LEFT_THUMB 0x0040 // bit 6\n#define XINPUT_GAMEPAD_RIGHT_THUMB 0x0080 // bit 7\n#define XINPUT_GAMEPAD_LEFT_SHOULDER 0x0100 // bit 8\n#define XINPUT_GAMEPAD_RIGHT_SHOULDER 0x0200 // bit 9\n#define XINPUT_GAMEPAD_A 0x1000 // bit 12\n#define XINPUT_GAMEPAD_B 0x2000 // bit 13\n#define XINPUT_GAMEPAD_X 0x4000 // bit 14\n#define XINPUT_GAMEPAD_Y 0x8000 // bit 15\nAn individual button’s state can be read by masking the wButtons word\nwiththeappropriatebitmaskviaC/C++’sbitwiseANDoperator(&)andthen\nchecking if the result is nonzero. For example, to determine if the A button is\npressed (down), we would write:\nbool IsButtonADown(const XINPUT_GAMEPAD& pad)\n{\n// Mask off all bits but bit 12 (the A button).\nreturn ((pad.wButtons & XINPUT_GAMEPAD_A) != 0);\n}\n9.3.2 Analog Axes and Buttons\nAnanaloginput isonethatcantakeonarangeofvalues(ratherthanjust0or1).\nThese kinds of inputs are often used to represent the degree to which a trigger\nis pressed, or the two-dimensional position of a joystick (which is represented\nusing two analog inputs, one for the x-axis and one for the y-axis, as shown\nin Figure 9.8). Because of this common usage, analog inputs are sometimes\ncalledanalogaxes, or just axes.\nOnsome devices,certainbuttonsareanalogaswell,meaningthatthegame\ncan actually detect how hard the player is pressing on them. However, the\nsignals produced by analog buttons are usually too noisy to be particularly\nusable. Games that use analog button inputs effectively are rare. One good\n9.3. Types of Inputs 565\nFigure 9.8. Two analog inputs can be used to represent the x andy deﬂection of a joystick.\nexample is Metal Gear Solid 2 on the PS2. It uses pressure-sensitive (analog)\nbutton data in aim mode to tell the difference between releasing the X but-\nton quickly (which fires the weapon) and releasing it slowly (which aborts the\nshot)—a useful feature in a stealth game, where you don’t want to alert the\nenemies unless you have to!\nStrictlyspeaking,analoginputsarenotreallyanalogbythetimetheymake\nit to the game engine. An analog input signal is usually digitized, meaning it\nis quantized and represented using an integer in software. For example, an\nanalog input might range from  32,768to32,767if represented by a 16-bit\nsigned integer. Sometimes analog inputs are converted to floating point—the\nvalues might range from  1to1, for instance. But as we know from Section\n3.3.1.3, floating-point numbers are really just quantized digital values as well.\nReviewing the definition of XINPUT_GAMEPAD (repeated below), we can\nseethatMicrosoftchosetorepresentthedeflectionsoftheleftandrightthumb\nsticks on the Xbox 360 gamepad using 16-bit signed integers (sThumbLX and\nsThumbLY for the left stick and sThumbRX andsThumbRY for the right).\nHence, these values range from  32,768(left or down) to 32,767(right or\nup). However, to represent the positions of the left and right shoulder trig-\ngers,Microsoftchosetouseeight-bitunsignedintegers( bLeftTrigger and\nbRightTrigger, respectively). These input values range from 0 (not\npressed) to 255 (fully pressed). Different game machines use different digi-\ntal representions for their analog axes.\ntypedef struct _XINPUT_GAMEPAD\n{\nWORD wButtons;\n// 8-bit unsigned\nBYTE bLeftTrigger ;\n566 9. Human Interface Devices\nBYTE bRightTrigger ;\n// 16-bit signed\nSHORT sThumbLX;\nSHORT sThumbLY;\nSHORT sThumbRX;\nSHORT sThumbRY;\n} XINPUT_GAMEPAD;\n9.3.3 Relative Axes\nThe position of an analog button, trigger, joystick or thumb stick is absolute,\nmeaning that there is a clear understanding of where zero lies. However, the\ninputs of some devices are relative. For these devices, there is no clear location\nat which the input value should be zero. Instead, a zero input indicates that\nthe position of the device has not changed, while nonzero values represent\na delta from the last time the input value was read. Examples include mice,\nmouse wheels and track balls.\n9.3.4 Accelerometers\nThe PlayStation’s DualShock joypads and the Nintendo Wiimote all contain\nacceleration sensors (accelerometers). These devices can detect acceleration\nalong the three principal axes ( x,yandz), as shown in Figure 9.9. These are\nrelativeanalog inputs, much like a mouse’s two-dimensional axes. When the\ncontroller is not accelerating these inputs are zero, but when the controller is\naccelerating, they measure the acceleration up to 3g along each axis, quan-\ntized into three signed eight-bit integers, one for each of x,yandz.\n9.3.5 3D Orientation with the Wiimote or DualShock\nSome Wii and PS3 games make use of the three accelerometers in the Wi-\nimote or DualShock joypad to estimate the orientation of the controller in the\nplayer’s hand. For example, in Super Mario Galaxy, Mario hops onto a large\nFigure 9.9. Accelerometer axes for the Wiimote.\n9.3. Types of Inputs 567\nball and rolls it around with his feet. To control Mario in this mode, the Wi-\nimote is held with the IR sensor facing the ceiling. Tilting the Wiimote left,\nright, forward or back causes the ball to accelerate in the corresponding direc-\ntion.\nAtrioofaccelerometerscanbeusedtodetecttheorientationoftheWiimote\nor DualShock joypad, because of the fact that we are playing these games on\nthe surface of the Earth where there is a constant downward acceleration due\nto gravity of 1g (9.8m/s2). If the controller is held perfectly level, with the\nIR sensor pointing toward your TV set, the vertical ( z) acceleration should be\napproximately 1g.\nIf the controller is held upright, with the IR sensor pointing toward the\nceiling, we would expect to see a 0g acceleration on the zsensor, and +1g\non the ysensor (because it is now experiencing the full gravitational effect).\nHoldingtheWiimoteata45-degreeangleshouldproduceroughly sin(45◦) =\ncos(45◦) = 0.707g on both the yandzinputs. Once we’ve calibrated the\naccelerometer inputs to find the zero points along each axis, we can calculate\npitch, yaw and roll easily, using inverse sine and cosine operations.\nTwo caveats here: First, if the person holding the Wiimote is not holding\nit still, the accelerometer inputs will include this acceleration in their values,\ninvalidating our math. Second, the z-axis of the accelerometer has been cal-\nibrated to account for gravity, but the other two axes have not. This means\nthat the z-axis has less precision available for detecting orientation. Many\nWii games request that the user hold the Wiimote in a nonstandard orienta-\ntion, such as with the buttons facing the player’s chest, or with the IR sen-\nsor pointing toward the ceiling. This maximizes the precision of the orien-\ntation reading by placing the x- ory-accelerometer axis in line with gravity,\ninstead of the gravity-calibrated z-axis. For more information on this topic,\nsee http://druid.caughq.org/presentations/turbo/Wiimote-Hacking.pdf.\n9.3.6 Cameras\nThe Wiimote has a unique feature not found on any other standard console\nHID—an infrared (IR) sensor. This sensor is essentially a low-resolution cam-\nera that records a two-dimension infrared image of whatever the Wiimote is\npointed at. The Wii comes with a “sensor bar” that sits on top of your televi-\nsion set and contains two infrared light emitting diodes (LEDs). In the image\nrecorded by the IR camera, these LEDs appear as two bright dots on an oth-\nerwise dark background. Image processing software in the Wiimote analyzes\nthe image and isolates the location and size of the two dots. (Actually, it can\ndetect and transmit the locations and sizes of up to four dots.) This position\n568 9. Human Interface Devices\nImage  Recorded  by \nInfrared CameraSensor  Bar\nFigure 9.10. The Wii sensor bar houses two infrared LEDs, which produce two bright spots on the\nimage recorded by the Wiimote’s IR camera.\nand size information can be read by the console via a Bluetooth wireless con-\nnection.\nThe position and orientation of the line segment formed by the two dots\ncan be used to determine the pitch, yaw and roll of the Wiimote (as long as it\nis being pointed toward the sensor bar). By looking at the separation between\nthe dots, software can also determine how close or far away the Wiimote is\nfrom the TV. Some software also makes use of the sizes of the dots. This is\nillustrated in Figure 9.10.\nFigure 9.11. Sony’s\nPlayStation Eye for\nthe PS3.Another popular camera device is Sony’s PlayStation Eye for the PS3,\nshown in Figure 9.11. This device is basically a high-quality color camera,\nwhich can be used for a wide range of applications. It can be used for sim-\nple video conferencing, like any web cam. It could also conceivably be used\nmuch like the Wiimote’s IR camera, for position, orientation and depth sens-\ning. The gamut of possibilities for these kinds of advanced input devices has\nonly begun to be tapped by the gaming community.\nWith the PlayStation 4, Sony has improved the Eye and re-dubbed it the\nPlayStation Camera. When combined with the PlayStation Move controller\n(see Figure 9.12) or the DualShock 4 controller, the PlayStation can detect ges-\ntures in basically the same way as Microsoft’s innovative Kinect system can\n(Figure 9.13).",11929
62-9.5 Game Engine HID Systems.pdf,62-9.5 Game Engine HID Systems,"9.4. Types of Outputs 569\nFigure 9.12. Sony’s PlayStation Camera, PlayStation Move controller and DualShock 4 joypad for the PS4.\nFigure 9.13. The Microsoft Kinect for Xbox 360 (top) and Xbox One (bottom).\n9.4 Types of Outputs\nHumaninterfacedevicesareprimarilyusedtotransmitinputsfromtheplayer\nto the game software. However, some HIDs can also provide feedback to the\nhuman player via various kinds of outputs.\n9.4.1 Rumble\nGame pads like the PlayStation’s DualShock line of controllers and the Xbox\nand Xbox 360 controllers have a rumblefeature. This allows the controller to\nvibrate in the player’s hands, simulating the turbulence or impacts that the\ncharacter in the game world might be experiencing. Vibrations are usually\n570 9. Human Interface Devices\nproduced by one or more motors, each of which rotates a slightly unbalanced\nweight at various speeds. The game can turn these motors on and off, and\ncontrol their speeds to produce different tactile effects in the player’s hands.\n9.4.2 Force-Feedback\nForce-feedback is a technique in which an actuator on the HID is driven by\na motor in order to slightly resist the motion the human operator is trying to\nimpart to it. It is common in arcade driving games, where the steering wheel\nresiststheplayer’sattempttoturnit,simulatingdifficultdrivingconditionsor\ntightturns. Aswithrumble,thegamesoftwarecantypicallyturnthemotor(s)\nonandoff,andcanalsocontrolthestrengthanddirectionoftheforcesapplied\nto the actuator.\n9.4.3 Audio\nAudio is usually a stand-alone engine system. However, some HIDs provide\noutputs that can be utilized by the audio system. For example, the Wiimote\ncontains a small, low-quality speaker. The Xbox 360, Xbox One and Dual-\nShock 4 controllers have a headphone jack and can be used just like any USB\naudio device for both output (speakers) and input (microphone). One com-\nmon use of USB headsets is for multiplayer games, in which human players\ncan communicate with one another via a voice over IP (VoIP) connection.\n9.4.4 Other Inputs and Outputs\nHuman interface devices may of course support many other kinds of inputs\nand outputs. On some older consoles like the Sega Dreamcast, the memory\ncard slots were located on the game pad. The Xbox 360 game pad, the Sixaxis\nand DualShock 3, and the Wiimote all have four LEDs which can be illumi-\nnated by game software if desired. The color of the light bar on the front of\ntheDualShock4controllercanbecontrolledbygamesoftware. Andofcourse\nspecialized devices like musical instruments, dance pads, etc. have their own\nparticular kinds of inputs and outputs.\nInnovation is actively taking place in the field of human interfaces. Some\nof the most interesting areas today are gestural interfaces and thought-con-\ntrolled devices. We can certainly expect more innovation from console and\nHID manufacturers in years to come.\n9.5 Game Engine HID Systems\nMost game engines don’t use “raw” HID inputs directly. The data is usu-\nally massaged in various ways to ensure that the inputs coming from the HID\n9.5. Game Engine HID Systems 571\ntranslateintosmooth,pleasing,intuitivebehaviorsin-game. Inaddition,most\nengines introduce at least one additional level of indirection between the HID\nand the game in order to abstract HID inputs in various ways. For example, a\nbutton-mapping table might be used to translate raw button inputs into log-\nical game actions, so that human players can reassign the buttons’ functions\nas they see fit. In this section, we’ll outline the typical requirements of a game\nengine HID system and then explore each one in some depth.\n9.5.1 Typical Requirements\nA game engine’s HID system usually provides some or all of the following\nfeatures:\n• dead zones,\n• analog signal filtering,\n• event detection (e.g., button up, button down),\n• detection of button sequences and multibutton combinations (known as\nchords),\n• gesture detection,\n• management of multiple HIDs for multiple players,\n• multiplatform HID support,\n• controller input remapping,\n• context-sensitive inputs, and\n• the ability to temporarily disable certain inputs.\n9.5.2 Dead Zone\nA joystick, thumb stick, shoulder trigger, or any other analog axis produces\ninputvaluesthatrangebetweenapredefinedminimumandmaximumvalue,\nwhich we’ll call Iminand Imax. When the control is not being touched, we\nwould expect it to produce a steady and clear “undisturbed” value, which\nwe’ll call I0. The undisturbed value is usually numerically equal to zero, and\niteitherlieshalfwaybetween IminandImaxforacentered,two-waycontrollike\na joystick axis, or it coincides with Iminfor a one-way control like a trigger.\nUnfortunately,becauseHIDsareanalogdevicesbynature,thevoltagepro-\nduced by the device is noisy, and the actual inputs we observe may fluctuate\nslightly around I0. The most common solution to this problem is to introduce\nasmalldeadzone around I0. Thedeadzonemightbedefinedas [I0 d,I0+d]\nforajoystick, or [I0,I0+d]foratrigger. Anyinputvaluesthatarewithinthe\ndead zone are simply clamped to I0. The dead zone must be wide enough to\n572 9. Human Interface Devices\naccount for the noisiest inputs generated by an undisturbed control, but small\nenough not to interfere with the player’s sense of the HID’s responsiveness.\n9.5.3 Analog Signal Filtering\nSignal noise is a problem even when the controls are not within their dead\nzones. This noise can sometimes cause the in-game behaviors controlled by\nthe HID to appear jerky or unnatural. For this reason, many games filterthe\nrawinputscomingfromtheHID.Anoisesignalisusuallyofahighfrequency\nrelative to the signal produced by the human player. Therefore, one solution\nis to pass the raw input data through a simple low-pass filter, prior to it being\nused by the game.\nAdiscretefirst-orderlow-passfiltercanbeimplementedbycombiningthe\ncurrent unfiltered input value with last frame’s filtered input. If we denote\nthe sequence of unfiltered inputs by the time-varying function u(t)and the\nfiltered inputs by f(t), where tdenotes time, then we can write\nf(t) = ( 1 a)f(t ∆t) +au(t), (9.1)\nwhere the parameter ais determined by the frame duration ∆tand a filtering\nconstant RC(which is just the product of the resistance and the capacitance in\na traditional analog RC low-pass filter circuit):\na=∆t\nRC+∆t. (9.2)\nThis can be implemented trivially in C or C++ as follows, where it is as-\nsumed the calling code will keep track of the last frame’s filtered input for use\nonthesubsequentframe. Formoreinformation, seehttp://en.wikipedia.org/\nwiki/Low-pass_filter.\nF32 lowPassFilter (F32 unfilteredInput,\nF32 lastFramesFilteredInput,\nF32 rc, F32 dt)\n{\nF32 a = dt / (rc + dt);\nreturn (1 - a) * lastFramesFilteredInput\n+ a * unfilteredInput;\n}\nAnother way to filter HID input data is to calculate a simple moving aver-\nage. For example, if we wish to average the input data over a 3/30 second (3\nframe) interval, we simply store the raw input values in a 3-element circular\n9.5. Game Engine HID Systems 573\nbuffer. The filtered input value is then the sum of the values in this array at\nany moment, divided by 3. There are a few minor details to account for when\nimplementing such a filter. For example, we need to properly handle the first\ntwo frames of input, during which the 3-element array has not yet been filled\nwithvaliddata. However,theimplementationisnotparticularlycomplicated.\nThe code below shows one way to properly implement an N-element moving\naverage.\ntemplate< typename TYPE, int SIZE >\nclass MovingAverage\n{\nTYPE m_samples[SIZE];\nTYPE m_sum;\nU32 m_curSample;\nU32 m_sampleCount;\npublic:\nMovingAverage() :\nm_sum(static_cast<TYPE>(0)),\nm_curSample(0),\nm_sampleCount(0)\n{\n}\nvoid addSample(TYPE data)\n{\nif (m_sampleCount == SIZE)\n{\nm_sum -= m_samples[m_curSample];\n}\nelse\n{\nm_sampleCount++;\n}\nm_samples[m_curSample] = data;\nm_sum += data;\nm_curSample++;\nif (m_curSample >= SIZE)\n{\nm_curSample = 0;\n}\n}\nF32 getCurrentAverage () const\n{\n574 9. Human Interface Devices\nif (m_sampleCount != 0)\n{\nreturn static_cast<F32>(m_sum)\n/ static_cast<F32>(m_sampleCount);\n}\nreturn 0.0f;\n}\n};\n9.5.4 Detecting Input Events\nThelow-levelHIDinterfacetypicallyprovidesthegamewiththecurrentstates\nof the device’s various inputs. However, games are often interested in de-\ntectingevents, such as changes in state, rather than just inspecting the current\nstate each frame. The most common HID events are probably button-down\n(pressed) and button-up (released), but of course we can detect other kinds of\nevents as well.\n9.5.4.1 Button Up and Button Down\nLet’s assume for the moment that our buttons’ input bits are 0 when not\npressed and 1 when pressed. The easiest way to detect a change in button\nstate is to keep track of the buttons’ state bits as observed last frame and com-\npare them to the state bits observed this frame. If they differ, we know an\nevent occurred. The current state of each button tells us whether the event is\na button-up or a button-down.\nWe can use simple bit-wise operators to detect button-down and button-\nup events. Given a 32-bit word buttonStates containing the current\nstate bits of up to 32 buttons, we want to generate two new 32-bit words:\none for button-down events, which we’ll call buttonDowns , and one for\nbutton-up events, which we’ll call buttonUps . In both cases, the bit cor-\nresponding to each button will be 0 if the event has not occurred this frame\nand 1 if it has. To implement this, we also need the last frame’s button states,\nprevButtonStates.\nThe exclusive OR (XOR) operator produces a 0 if its two inputs are identi-\ncal and a 1 if they differ. So if we apply the XOR operator to the previous and\ncurrent button state words, we’ll get 1’s only for buttons whose states have\nchanged between the last frame and this frame. To determine whether the\nevent is a button-up or a button-down, we need to look at the current state of\neach button. Any button whose state has changed that is currently down gen-\nerates a button-down event, and vice versa for button-up events. The follow-\ning code applies these ideas in order to generate our two button event words:\n9.5. Game Engine HID Systems 575\nclass ButtonState\n{\nU32 m_buttonStates; // current frame's button\n// states\nU32 m_prevButtonStates; // previous frame's states\nU32 m_buttonDowns; // 1 = button pressed this\n// frame\nU32 m_buttonUps; // 1 = button released this\n// frame\nvoid DetectButtonUpDownEvents()\n{\n// Assuming that m_buttonStates and\n// m_prevButtonStates are valid, generate\n// m_buttonDowns and m_buttonUps.\n// First determine which bits have changed via\n// XOR.\nU32 buttonChanges = m_buttonStates\n^m_prevButtonStates;\n// Now use AND to mask off only the bits that\n// are DOWN.\nm_buttonDowns = buttonChanges &m_buttonStates;\n// Use AND-NOT to mask off only the bits that\n// are UP.\nm_buttonUps = buttonChanges &(~m_buttonStates);\n}\n// ...\n};\n9.5.4.2 Chords\nAchordis a group of buttons that, when pressed at the same time, produce a\nunique behavior in the game. Here are a few examples:\n•Super Mario Galaxy’s start-up screen requires you to press the A and B\nbuttons on the Wiimote together in order to start a new game.\n• Pressingthe 1 and 2 buttons on the Wiimoteat the same time puts it into\nBluetooth discovery mode (no matter what game you’re playing).\n576 9. Human Interface Devices\n• The“grapple”moveinmanyfightinggamesistriggeredbyatwo-button\ncombination.\n• For development purposes, holding down both the left and right trig-\ngers on the DualShock 3 in Uncharted allows the player character to fly\nanywhere in the game world, with collisions turned off. (Sorry, this\ndoesn’t work in the shipping game!) Many games have a cheat like this\ntomakedevelopmenteasier. (Itmayormaynotbetriggeredbyachord,\nofcourse.) Itiscalled no-clipmode intheQuakeengine,becausethechar-\nacter’s collision volume is not clippedto the valid playable area of the\nworld. Other engines use different terminology.\nDetecting chords is quite simple in principle: We merely watch the states\nof two or more buttons and only perform the requested operation when allof\nthem are down.\nThere are some subtleties to account for, however. For one thing, if the\nchord includes a button or buttons that have other purposes in the game, we\nmusttakecarenottoperform boththeactionsoftheindividualbuttonsandthe\naction of chord when it is pressed. This is usually done by including a check\nthattheotherbuttonsinthechordare notdownwhendetectingtheindividual\nbutton presses.\nAnother fly in the ointment is that humans aren’t perfect, and they often\npress one or more of the buttons in the chord slightly earlier than the rest. So\nour chord-detection code must be robust to the possibility that we’ll observe\none or more individual buttons on frame iand the rest of the chord on frame\ni+1(or even multiple frames later). There are a number of ways to handle\nthis:\n• You can design your button inputs such that a chord always does the\nactions of the individual buttons plussome additional action. For ex-\nample, if pressing L1 fires the primary weapon and L2 lobs a grenade,\nperhapstheL1+L2chordcouldfiretheprimaryweapon,lobagrenade,\nandsend out an energy wave that doubles the damage done by these\nweapons. That way, whether or not the individual buttons are detected\nbefore the chord or not, the behavior will be identical from the point of\nview of the player.\n• You can introduce a delay between when an individual button-down\nevent is seen and when it “counts” as a valid game event. During the\ndelayperiod(say2or3frames),ifachordisdetected,thenittakesprece-\ndence over the individual button-down events. This gives the human\nplayer some leeway in performing the chord.\n• Youcandetectthechordwhenthebuttonsarepressed,butwaittotrigger\nthe effect until the buttons are released again.\n9.5. Game Engine HID Systems 577\n• You can begin the single-button move immediately and allow it to be\npreempted by the chord move.\n9.5.4.3 Sequences and Gesture Detection\nThe idea of introducing a delay between when a button actually goes down\nand when it really “counts” as down is a special case of gesture detection. A\ngesture is a sequence of actions performed via a HID by the human player\nover a period of time. For example, in a fighting game or brawler, we might\nwant to detect a sequence of button presses, such as A-B-A. We can extend this\nidea to non-button inputs as well. For example, A-B-A-Left-Right-Left, where\nthe latter three actions are side-to-side motions of one of the thumb sticks on\nthe game pad. Usually a sequence or gesture is only considered to be valid if\nit is performed within some maximum time frame. So a rapid A-B-A within a\nquarterofasecondmight“count,”butaslowA-B-Aperformedoverasecond\nor two might not.\nGesture detection is generally implemented by keeping a brief history of\nthe HID actions performed by the player. When the first component of the\ngesture is detected, it is stored in the history buffer, along with a time stamp\nindicating when it occurred. As each subsequent component is detected, the\ntime between it and the previous component is checked. If it is within the\nallowable time window, it too is added to the history buffer. If the entire se-\nquence is completed within the allotted time (i.e., the history buffer is filled),\nan event is generated telling the rest of the game engine that the gesture has\noccurred. However, ifanynon-validinterveninginputsaredetected, orifany\ncomponent of the gesture occurs outside of its valid time window, the entire\nhistory buffer is reset and the player must start the gesture over again.\nLet’slookatthreeconcreteexamples,sowecanreallyunderstandhowthis\nworks.\nRapid Button Tapping\nMany games require the user to tap a button rapidly in order to perform an\naction. Thefrequencyofthebuttonpressesmayormaynottranslateintosome\nquantity in the game, such as the speed with which the player character runs\norperformssomeotheraction. Thefrequencyisusuallyalsousedtodefinethe\nvalidity of the gesture—if the frequency drops below some minimum value,\nthe gesture is no longer considered valid.\nWe can detect the frequency of a button press by simply keeping track of\nthelasttimewesawabutton-downeventforthebuttoninquestion. We’llcall\nthisTlast. The frequency fis then just the inverse of the time interval between\n578 9. Human Interface Devices\npresses ∆T=Tcur Tlastandf=1/∆T. Every time we detect a new button-\ndown event, we calculate a new frequency f. To implement a minimum valid\nfrequency, we simply check fagainst the minimum frequency fmin(or we can\njust check ∆Tagainst the maximum period ∆Tmax=1/fmindirectly). If this\nthreshold is satisfied, we update the value of Tlast, and the gesture is consid-\nered to be on-going. If the threshold is not satisfied, we simply don’t update\nTlast. The gesture will be considered invalid until a new pair of rapid-enough\nbutton-down events occurs. This is illustrated by the following pseudocode:\nclass ButtonTapDetector\n{\nU32 m_buttonMask; // which button to observe (bit\n// mask)\nF32 m_dtMax; // max allowed time between\n// presses\nF32 m_tLast; // last button-down event, in\n// seconds\npublic:\n// Construct an object that detects rapid tapping of\n// the given button (identified by an index).\nButtonTapDetector (U32 buttonId, F32 dtMax) :\nm_buttonMask(1U << buttonId),\nm_dtMax(dtMax),\nm_tLast(CurrentTime() - dtMax) // start out\n// invalid\n{\n}\n// Call this at any time to query whether or not\n// the gesture is currently being performed.\nbool IsGestureValid () const\n{\nF32 t = CurrentTime();\nF32 dt = t - m_tLast;\nreturn (dt < m_dtMax);\n}\n// Call this once per frame.\nvoid Update()\n{\nif (ButtonsJustWentDown (m_buttonMask))\n{\nm_tLast = CurrentTime();\n}\n9.5. Game Engine HID Systems 579\n}\n};\nIn the above code excerpt, we assume that each button is identified by a\nunique id. The id is really just an index, ranging from 0 to N 1(where Nis\nthe number of buttons on the HID in question). We convert the button id to a\nbitmaskbyshiftinganunsigned1bittotheleftbyanamountequalingthebut-\nton’s index ( 1U << buttonId). The function ButtonsJustWentDown()\nreturnsanonzerovalueif anyoneofthebuttonsspecifiedbythegivenbitmask\njustwentdownthisframe. Here,we’reonlycheckingforasinglebutton-down\nevent, but we can and will use this same function later to check for multiple\nsimultaneous button-down events.\nMultibutton Sequence\nLet’s say we want to detect the sequence A-B-A, performed within at most\none second. We can detect this button sequence as follows: We maintain a\nvariable that tracks which button in the sequence we’re currently looking for.\nIf we define the sequence with an array of button ids (e.g., aButtons[3] =\n{A, B, A}), then our variable is just an index iinto this array. It starts out\ninitialized to the first button in the sequence, i=0. We also maintain a start\ntimeforthe entiresequence, Tstart,muchaswedidintherapidbutton-pressing\nexample.\nThelogicgoeslikethis: Wheneverweseeabutton-downeventthatmatch-\nes the button we’re currently looking for, we check its time stamp against the\nstart time of the entire sequence, Tstart. If it occurred within the valid time\nwindow,weadvancethecurrentbuttontothenextbuttoninthesequence; for\nthe first button in the sequence only ( i=0), we also update Tstart. If we see\na button-down event that doesn’t match the next button in the sequence, or\nif the time delta has grown too large, we reset the button index iback to the\nbeginningofthesequenceandset Tstarttosomeinvalidvalue(suchas0). This\nis illustrated by the code below.\nclass ButtonSequenceDetector\n{\nU32* m_aButtonIds; // sequence of buttons to watch for\nU32 m_buttonCount; // number of buttons in sequence\nF32 m_dtMax; // max time for entire sequence\nU32 m_iButton; // next button to watch for in seq.\nF32 m_tStart; // start time of sequence, in\n// seconds\n580 9. Human Interface Devices\npublic:\n// Construct an object that detects the given button\n// sequence. When the sequence is successfully\n// detected, the given event is broadcast so that the\n// rest of the game can respond in an appropriate way.\nButtonSequenceDetector (U32* aButtonIds,\nU32 buttonCount,\nF32 dtMax,\nEventId eventIdToSend) :\nm_aButtonIds(aButtonIds),\nm_buttonCount(buttonCount),\nm_dtMax(dtMax),\nm_eventId(eventIdToSend), // event to send when\n// complete\nm_iButton(0), // start of sequence\nm_tStart(0) // initial value\n// irrelevant\n{\n}\n// Call this once per frame.\nvoid Update()\n{\nASSERT(m_iButton < m_buttonCount);\n// Determine which button we're expecting next, as\n// a bitmask (shift a 1 up to the correct bit\n// index).\nU32 buttonMask = (1U << m_aButtonId[m_iButton]);\n// If any button OTHER than the expected button\n// just went down, invalidate the sequence. (Use\n// the bitwise NOT operator to check for all other\n// buttons.)\nif (ButtonsJustWentDown (~buttonMask ))\n{\nm_iButton = 0; // reset\n}\n// Otherwise, if the expected button just went\n// down, check dt and update our state appropriately.\nelse if (ButtonsJustWentDown (buttonMask ))\n{\n9.5. Game Engine HID Systems 581\nif (m_iButton == 0)\n{\n// This is the first button in the\n// sequence.\nm_tStart = CurrentTime();\nm_iButton++; // advance to next button\n}\nelse\n{\nF32 dt = CurrentTime() - m_tStart;\nif (dt < m_dtMax)\n{\n// Sequence is still valid.\nm_iButton++; // advance to next button\n// Is the sequence complete?\nif (m_iButton == m_buttonCount)\n{\nBroadcastEvent(m_eventId);\nm_iButton = 0; // reset\n}\n}\nelse\n{\n// Sorry, not fast enough.\nm_iButton = 0; // reset\n}\n}\n}\n}\n};\nFigure 9.14. Detecting circular rotations of the stick by dividing the 2D range of stick inputs into\nquadrants.\n582 9. Human Interface Devices\nThumb Stick Rotation\nAsanexampleofamorecomplexgesture,let’sseehowwemightdetectwhen\nthe player is rotating the left thumb stick in a clockwise circle. We can detect\nthis quite easily by dividing the two-dimensional range of possible stick po-\nsitions into quadrants, as shown in Figure 9.14. In a clockwise rotation, the\nstick passes through the upper-left quadrant, then the upper-right, then the\nlower-right and finally the lower-left. We can treat each of these cases like a\nbutton press and detect a full rotation with a slightly modified version of the\nsequence detection code shown above. We’ll leave this one as an exercise for\nthe reader. Try it!\n9.5.5 Managing Multiple HIDs for Multiple Players\nMost game machines allow two or more HIDs to be attached for multiplayer\ngames. The engine must keep track of which devices are currently attached\nandrouteeachone’sinputstotheappropriateplayerinthegame. Thisimplies\nthat we need some way of mapping controllers to players. This might be as\nsimple as a one-to-one mapping between controller index and player index,\nor it might be something more sophisticated, such as assigning controllers to\nplayers at the time the user hits the Start button.\nEven in a single-player game with only one HID, the engine needs to be\nrobust to various exceptional conditions, such as the controller being acciden-\ntally unplugged or running out of batteries. When a controller’s connection\nis lost, most games pause gameplay, display a message and wait for the con-\ntroller to be reconnected. Some multiplayer games suspend or temporarily\nremove the avatar corresponding to a removed controller, but allow the other\nplayers to continue playing the game; the removed/suspended avatar might\nreactivate when the controller is reconnected.\nOnsystemswithbattery-operatedHIDs,thegameortheoperatingsystem\nis responsible for detecting low-battery conditions. In response, the player is\nusually warned in some way, for example via an unobtrusive on-screen mes-\nsage and/or a sound effect.\n9.5.6 Cross-Platform HID Systems\nMany game engines are cross-platform. One way to handle HID inputs and\noutputs in such an engine would be to sprinkle conditional compilation di-\nrectives all over the code, wherever interactions with the HID take place, as\nshown below. This is clearly not an ideal solution, but it does work.\n#if TARGET_XBOX360\nif (ButtonsJustWentDown( XB360_BUTTONMASK_A))\n9.5. Game Engine HID Systems 583\n#elif TARGET_PS3\nif (ButtonsJustWentDown( PS3_BUTTONMASK_TRIANGLE))\n#elif TARGET_WII\nif (ButtonsJustWentDown( WII_BUTTONMASK_A))\n#endif\n{\n// do something...\n}\nA better solution is to provide some kind of hardware abstraction layer,\nthereby insulating the game code from hardware-specific details.\nIf we’re lucky, we can abstract most of the differences beween the HIDs\non the different platforms by a judicious choice of abstract button and axis\nids. For example, if our game is to ship on Xbox 360 and PS3, the layout\nof the controls (buttons, axes and triggers) on these two joypads are almost\nidentical. The controls have different ids on each platform, but we can come\nup with generic control ids that cover both types of joypad quite easily. For\nexample:\nenum AbstractControlIndex\n{\n// Start and back buttons\nAINDEX_START, // Xbox 360 Start, PS3 Start\nAINDEX_BACK_SELECT, // Xbox 360 Back, PS3 Select\n// Left D-pad\nAINDEX_LPAD_DOWN,\nAINDEX_LPAD_UP,\nAINDEX_LPAD_LEFT,\nAINDEX_LPAD_RIGHT,\n// Right ""pad"" of four buttons\nAINDEX_RPAD_DOWN, // Xbox 360 A, PS3 X\nAINDEX_RPAD_UP, // Xbox 360 Y, PS3 Triangle\nAINDEX_RPAD_LEFT, // Xbox 360 X, PS3 Square\nAINDEX_RPAD_RIGHT, // Xbox 360 B, PS3 Circle\n// Left and right thumb stick buttons\nAINDEX_LSTICK_BUTTON, // Xbox 360 LThumb, PS3 L3,\n// Xbox white\nAINDEX_RSTICK_BUTTON, // Xbox 360 RThumb, PS3 R3,\n// Xbox black\n// Left and right shoulder buttons\nAINDEX_LSHOULDER, // Xbox 360 L shoulder, PS3 L1\nAINDEX_RSHOULDER, // Xbox 360 R shoulder, PS3 R1\n584 9. Human Interface Devices\n// Left thumb stick axes\nAINDEX_LSTICK_X,\nAINDEX_LSTICK_Y,\n// Right thumb stick axes\nAINDEX_RSTICK_X,\nAINDEX_RSTICK_Y,\n// Left and right trigger axes\nAINDEX_LTRIGGER, // Xbox 360 -Z, PS3 L2\nAINDEX_RTRIGGER, // Xbox 360 +Z, PS3 R2\n};\nOur abstraction layer can translate between the raw control ids on the cur-\nrent target hardware into our abstract control indices. For example, when-\never we read the state of the buttons into a 32-bit word, we can perform a\nbit-swizzling operation that rearranges the bits into the proper order to corre-\nspond to our abstract indices. Analog inputs can likewise be shuffled around\ninto the proper order.\nIn performing the mapping between physical and abstract controls, we’ll\nsometimesneedtogetabitclever. Forexample,ontheXbox,theleftandright\ntriggers act as a single axis, producing negative values when the left trigger is\npressed, zero when neither is trigger is pressed, and positive values when the\nright trigger is pressed. To match the behavior of the PlayStation’s DualShock\ncontroller, we might want to separate this axis into two distinct axes on the\nXbox, scaling the values appropriatelyso the range of valid values is the same\non all platforms.\nThis is certainly not the only way to handle HID I/O in a multiplatform\nengine. We might want to take a more functional approach, for example, by\nnaming our abstract controls according to their function in the game, rather\nthan their physical locations on the joypad. We might introduce higher-level\nfunctions that detect abstract gestures, with custom detection code on each\nplatform, or we might just bite the bullet and write platform-specific versions\nof all of the game code that requires HID I/O. The possibilities are numerous,\nbutvirtuallyallcross-platformgameenginesinsulatethegamefromhardware\ndetails in somemanner.\n9.5.7 Input Remapping\nMany games allow the player some degree of choice with regard to the func-\ntionality of the various controls on the physical HID. A common option is the\nsense of the vertical axis of the right thumb stick for camera control in a con-\nsole game. Some folks like to push forward on the stick to angle the camera\n9.5. Game Engine HID Systems 585\nup, while others like an inverted control scheme, where pulling back on the\nstick angles the camera up (much like an airplane control stick). Other games\nallow the player to select between two or more predefined button mappings.\nSome PC games allow the user full control over the functions of individual\nkeys on the keyboard, the mouse buttons and the mouse wheel, plus a choice\nbetween various control schemes for the two mouse axes.\nToimplementthis,weturntoafavoritesayingofanoldprofessorofmine,\nProfessorJayBlackoftheUniversityofWaterloo,“Everyproblemincomputer\nscience can be solved with a level of indirection.” We assign each function in\nthe game aunique id and then providea simple table, which maps each phys-\nical or abstract control index to a logical function in the game. Whenever the\ngame wishes to determine whether a particular logical game function should\nbe activated, it looks up the corresponding abstract or physical control id in\nthe table and then reads the state of that control. To change the mapping, we\ncaneitherswapouttheentiretablewholesale, orwecanallowtheusertoedit\nindividual entries in the table.\nWe’reglossingoverafewdetailshere. Foronething,differentcontrolspro-\nducedifferentkindsofinputs. Analogaxesmayproducevaluesrangingfrom\n 32,768to32,767, or from 0 to 255, or some other range. The states of all the\ndigitalbuttonsonaHIDareusuallypackedintoasinglemachineword. There-\nfore, wemustbecarefultoonlypermitcontrolmappingsthatmakesense. We\ncannot use a button as the control for a logical game function that requires\nan axis, for example. One way around this problem is to normalize all of the\ninputs. For example, we could re-scale the inputs from all analog axes and\nbuttons into the range [0, 1]. This isn’t quite as helpful as you might at first\nthink, because some axes are inherently bidirectional (like a joy stick) while\nothers are unidirectional (like a trigger). But, if we group our controls into\na few classes, we can normalize the inputs within those classes and permit\nremapping only within compatible classes. A reasonable set of classes for a\nstandard console joypad and their normalized input values might be:\n•Digitalbuttons . States are packed into a 32-bit word, one bit per button.\n•Unidirectional absolute axes (e.g., triggers, analog buttons) . Produce float-\ning-point input values in the range [0, 1].\n•Bidirectional absolute axes (e.g., joy sticks). Produce floating-point input\nvalues in the range [ 1, 1].\n•Relative axes (e.g., mouse axes, wheels, track balls). Produce floating-point\ninput values in the range [ 1, 1], where1represents the maximum\nrelative offset possible within a single game frame (i.e., during a period\nof 1/30 or 1/60 of a second).\n586 9. Human Interface Devices\n9.5.8 Context-Sensitive Controls\nInmanygames,asinglephysicalcontrolcanhavedifferentfunctions,depend-\ning on context. A simple example is the ubiquitous “use” button. If pressed\nwhilestandinginfrontofadoor,the“use”buttonmightcausethecharacterto\nopen the door. If it is pressed while standing near an object, it might cause the\nplayer character to pick up the object and so on. Another common example is\na modal control scheme. When the player is walking around, the controls are\nused to navigate and control the camera. When the player is riding a vehicle,\nthe controls are used to steer the vehicle, and the camera controls might be\ndifferent as well.\nContext-sensitive controls are reasonably straightforward to implement\nvia a state machine. Depending on what state we’re in, a particular HID con-\ntrol may have a different purpose. The tricky part is deciding what state to be\nin. Forexample,whenthecontext-sensitive“use”buttonispressed,theplayer\nmightbestandingatapointequidistantbetweenaweaponandahealthpack,\nfacing the center point between them. Which object do we use in this case?\nSome games implement a priority system to break ties like this. Perhaps the\nweapon has a higher weight than the health pack, so it would “win” in this\nexample. Implementing context-sensitive controls isn’t rocket science, but it\ninvariably requires lots of trial and error to get it feeling and behaving just\nright. Plan on lots of iteration and focus testing!\nAnotherrelatedconceptisthatof controlownership . Certaincontrolsonthe\nHID might be “owned” by different parts of the game. For example, some in-\nputs are for player control, some for camera control and still others are for use\nbythegame’swrapperandmenusystem(pausingthegame,andsoon). Some\ngame engines introduce the concept of a logical device, which is composed of\nonly a subset of the inputs on the physical device. One logical device might\nbe used for player control, while another is used by the camera system, and\nanother by the menu system.\n9.5.9 Disabling Inputs\nIn most games, it is sometimes necessary to disallow the player from control-\nlinghisorhercharacter. Forexample,whentheplayercharacterisinvolvedin\nan in-game cinematic, we might want to disable all player controls temporar-\nily; or when the player is walking through a narrow doorway, we might want\nto temporarily disable free camera rotation.\nOne rather heavy-handed approach is to use a bitmask to disable individ-\nualcontrolsontheinputdeviceitself. Wheneverthecontrolisread,thedisable\nmask is checked, and if the corresponding bit is set, a neutral or zero value is",33631
63-9.6 Human Interface Devices in Practice.pdf,63-9.6 Human Interface Devices in Practice,"9.6. Human Interface Devices in Practice 587\nreturned instead of the actual value read from the device. We must be par-\nticularly cautious when disabling controls, however. If we forget to reset the\ndisable mask, the game can get itself into a state where the player loses all\ncontrol forever and must restart the game. It’s important to check our logic\ncarefully, and it’s also a good idea to put in some fail-safe mechanisms to en-\nsure that the disable mask is cleared at certain key times, such as whenever\nthe player dies and re-spawns.\nDisabling a HID input masks it for all possible clients, which can be overly\nlimiting. A better approach is probably to put the logic for disabling specific\nplayer actions or camera behaviors directly into the player or camera code\nitself. That way, if the camera decides to ignore the deflection of the right\nthumbstick, forexample, othergameenginesystemsstillhavethefreedomto\nread the state of that stick for other purposes.\n9.6 Human Interface Devices in Practice\nCorrect and smooth handling of human interface devices is an important part\nof any good game. Conceptually speaking, HIDs may seem quite straightfor-\nward. However, there can be quite a few “gotchas” to deal with, including\nvariations between different physical input devices, proper implementation\nof low-pass filtering, bug-free handling of control scheme mappings, achiev-\ningjusttheright“feel”inyourjoypadrumble,limitationsimposedbyconsole\nmanufacturers via their technical requirements checklists (TRCs), and the list\ngoes on. A game team should expect to devote a nontrivial amount of time\nand engineering bandwidth to a careful and complete implementation of the\nhuman interface device system. This is extremely important because the HID\nsystem forms the underpinnings of your game’s most precious resource—its\nplayer mechanics.\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",1943
64-10 Tools for Debugging and Development.pdf,64-10 Tools for Debugging and Development,,0
65-10.1 Logging and Tracing.pdf,65-10.1 Logging and Tracing,"10\nTools for Debugging\nand Development\nDeveloping game software is a complex, intricate, math-intensive and\nerror-pronebusiness. Soitshouldbenosurprisethatvirtuallyeverypro-\nfessional game team builds a suite of tools for themselves, in order to make\nthe game development process easier and less error-prone. In this chapter,\nwe’ll take a look at the development and debugging tools most often found in\nprofessional-grade game engines.\n10.1 Logging and Tracing\nRememberwhenyouwroteyourfirstprograminBASICorPascal? (OK,may-\nbe you don’t. If you’re significantly younger than me—and there’s a pretty\ngoodchanceofthat—youprobablywroteyourfirstprograminJava,ormaybe\nPythonorLua.) Inanycase,youprobablyrememberhowyoudebuggedyour\nprogramsbackthen. Youknow, backwhenyouthoughta debugger wasoneof\nthose glowing blue insect zapper things? You probably used print statements\ntodumpouttheinternalstateofyourprogram. C/C++programmerscallthis\nprintf debugging (after the C standard library function, printf()).\nIt turns out that printf debugging is still a perfectly valid thing to do—\nevenifyou know that a debugger isn’t a device for frying hapless insects at\n589\n590 10. Tools for Debugging and Development\nnight. Especially in real-time programming, it can be difficult to trace certain\nkinds of bugs using breakpoints and watch windows. Some bugs are timing-\ndependent: they only happen when the program is running at full speed.\nOther bugs are caused by a complex sequence of events too long and intricate\nto trace manually one-by-one. In these situations, the most powerful debug-\nging tool is often a sequence of print statements.\nEvery game platform has some kind of console or teletype (TTY) output\ndevice. Here are some examples:\n• In a console application written in C/C++, running under Linux or\nWin32, you can produce output in the console by printing to stdout\norstderr viaprintf(), fprintf() or the C++ standard library’s\niostream interface.\n• Unfortunately, printf() andiostream don’t work if your game is\nbuiltasawindowedapplicationunderWin32,becausethere’snoconsole\nin which to display the output. However, if you’re running under the\nVisual Studio debugger, it provides a debug console to which you can\nprint via the Win32 function OutputDebugString().\n• OnthePlayStation3andPlayStation4, anapplicationknownastheTar-\ngetManager(orPlayStationNeighborhoodonthePS4)runsonyourPC\nand allows you to launch programs on the console. The Target Manager\nincludesasetofTTYoutputwindowstowhichmessagescanbeprinted\nby the game engine.\nSo printing out information for debugging purposes is almost always as\neasyasaddingcallsto printf() throughoutyourcode. However,mostgame\nengines go a bit farther than this. In the following sections, we’ll investigate\nthe kinds of printing facilities most game engines provide.\n10.1.1 Formatted Output with OutputDebugString()\nTheWindowsSDKfunction OutputDebugString() isgreatforprintingde-\nbugging information to Visual Studio’s Debug Output window. However,\nunlike printf(), OutputDebugString() does not support formatted\noutput—it can only print raw strings in the form of arrays. For this reason,\nmost Windows game engines wrap it in a custom function, like this:\n#include <stdio.h> // for va_list et al\n#ifndef WIN32_LEAN_AND_MEAN\n#define WIN32_LEAN_AND_MEAN 1\n#endif\n#include <windows.h> // for OutputDebugString()\n10.1. Logging and Tracing 591\nint VDebugPrintF (const char* format, va_list argList)\n{\nconst U32 MAX_CHARS = 1024;\nstatic char s_buffer[MAX_CHARS];\nint charsWritten\n= vsnprintf(s_buffer, MAX_CHARS, format, argList);\n// Now that we have a formatted string, call the\n// Win32 API.\nOutputDebugString (s_buffer);\nreturn charsWritten;\n}\nint DebugPrintF(const char* format, ...)\n{\nva_list argList;\nva_start(argList, format);\nint charsWritten = VDebugPrintF (format, argList);\nva_end(argList);\nreturn charsWritten;\n}\nNotice that two functions are implemented: DebugPrintF() takes a\nvariable-length argument list (specified via the ellipsis, …), while VDebug-\nPrintF() takes a va_list argument. This is done so that programmers can\nbuild additional printing functions in terms of VDebugPrintF(). (It’s im-\npossible to pass ellipses from one function to another, but it ispossible to pass\nva_lists around.)\n10.1.2 Verbosity\nOnceyou’vegonetothetroubleofaddingabunchofprintstatementstoyour\ncode in strategically chosen locations, it’s nice to be able to leave them there,\nin case they’re needed again later. To permit this, most engines provide some\nkind of mechanism for controlling the level of verbosity via the command line,\nor dynamically at runtime. When the verbosity level is at its minimum value\n(usually zero), only critical error messages are printed. When the verbosity is\nhigher, more of the print statements embedded in the code start to contribute\nto the output.\n592 10. Tools for Debugging and Development\nThe simplest way to implement this is to store the current verbosity level\nin a global integer variable, perhaps called g_verbosity. We then provide\naVerboseDebugPrintF() function whose first argument is the verbosity\nlevel at or above which the message will be printed. This function could be\nimplemented as follows:\nint g_verbosity = 0;\nvoid VerboseDebugPrintF (int verbosity,\nconst char* format, ...)\n{\n// Only print when the global verbosity level is\n// high enough.\nif (g_verbosity >= verbosity )\n{\nva_list argList;\nva_start(argList, format);\nVDebugPrintF (format, argList);\nva_end(argList);\n}\n}\n10.1.3 Channels\nIt’salsoextremelyusefultobeabletocategorizeyourdebugoutputinto chan-\nnels. One channel might contain messages from the animation system, while\nanothermightbeusedtoprintmessagesfromthephysicssystem,forexample.\nOn some platforms, like the PlayStation 3, debug output can be directed\nto one of 14 distinct TTY windows. In addition, messages are mirrored to a\nspecialTTYwindowthatcontainstheoutputfromalloftheother14windows.\nThis makes it very easy for a developer to focus in on only the messages he or\nshe wants to see. When working on an animation problem, one can simply\nflip to the animation TTY and ignore all the other output. When working on a\ngeneral problem of unknown origin, the “all” TTY can be consulted for clues.\nOtherplatformslikeWindowsprovideonlyasingledebugoutputconsole.\nHowever, even on these systems it can be helpful to divide your output into\nchannels. The output from each channel might be assigned a different color.\nYou might also implement filters, which can be turned on and off at runtime,\nandrestrictoutputtoonlyaspecifiedchannelorsetofchannels. Inthismodel,\nif a developer is debugging an animation-related problem, for example, he or\nshe can simply filter out all of the channels except the animation channel.\n10.1. Logging and Tracing 593\nA channel-based debug output system can be implemented quite easily\nby adding an additional channel argument to our debug printing function.\nChannelsmightbenumbered,orbetter,assignedsymbolicvaluesviaaC/C++\nenumdeclaration. Orchannelsmightbenamedusingastringorhashedstring\nid. Theprintingfunctioncansimplyconsultthelistofactivechannelsandonly\nprint the message if the specified channel is among them.\nIf you don’t have more than 32 or 64 channels, it can be helpful to identify\nthe channels via a 32- or 64-bit mask. This makes implementing a channel\nfilter as easy as specifying a single integer. When a bit in the mask is 1, the\ncorresponding channel is active; when the bit is 0, the channel is muted.\n10.1.3.1 Using Redis to Manage TTY Channels\nThe developers at Naughty Dog use a web-based interface called Connector\nas their window into the various streams of debugging information that are\nemitted by the game engine at runtime. The game spits out its debug text\ninto various named channels, each associated with a different engine system\n(animation, rendering, AI, sound, etc.) These data streams are collected by a\nlightweightRediskey-valuestore(seehttp://redis.ioformoreinformationon\nRedis). The Connector interface allows users to view and filter this Redis data\neasily from any web browser.\n10.1.4 Mirroring Output to a File\nIt’sagoodideatomirroralldebugoutputtooneormorelogfiles(e.g.,onefile\nper channel). This permits problemsto be diagnosed after the fact. Ideally the\nlog file(s) should contain allof the debug output, independent of the current\nverbosity level and active channels mask. This allows unexpected problems to\nbe caught and tracked down by simply inspecting the most-recent log files.\nYou may want to consider flushing your log file(s) after every call to your\ndebug output function to ensure that if the game crashes, the log file(s) won’t\nbe missing the last buffer-full of output. The last data printed are usually the\nmost useful for determining the cause of a crash, so we want to be sure that\nthe log file always contains the most up-to-date output. Of course, flushing\nthe output buffer can be expensive. So you should only flush buffers after\nevery debug output call if either (a) you are not doing a lot of logging, or (b)\nyou discover that it is truly necessary on your particular platform. If flushing\nis deemed to be necessary, you can always provide an engine configuration\noption to turn it on and off.",9359
66-10.2 Debug Drawing Facilities.pdf,66-10.2 Debug Drawing Facilities,"594 10. Tools for Debugging and Development\n10.1.5 Crash Reports\nSome game engines produce special text output and/or log files when the\ngame crashes. In most operating systems, a top-level exception handler can\nbe installed that will catch most crashes. In this function, you could print out\nall sorts of useful information. You could even consider emailing the crash re-\nport to the entire programming team. This can be incredibly enlightening for\nthe programmers: When they see just how often the art and design teams are\ncrashing, they may discover a renewed sense of urgency in their debugging\ntasks!\nHere are just a few examples of the kinds of information you can include\nin a crash report:\n• Current level(s) being played at the time of the crash.\n• World-space location of the player character when the crash occurred.\n• Animation/action state of the player when the game crashed.\n• Gameplay script(s) that were running at the time of the crash. (This can\nbe especially helpful if the script is the cause of the crash!)\n• Stack trace. Most operating systems provide a mechanism for walking\nthe call stack (although they are nonstandard and highly platform spe-\ncific). With such a facility, you can print out the symbolic names of all\nnon-inline functions on the stack at the time the crash occurred.\n• State of all memory allocators in the engine (amount of memory free,\ndegree of fragmentation, etc.). This kind of data can be helpful when\nbugs are caused by low-memory conditions, for example.\n• Anyotherinformationyouthinkmightberelevantwhentrackingdown\nthe cause of a crash.\n• A screenshot of the game at the moment it crashed.\n10.2 Debug Drawing Facilities\nModerninteractivegamesaredrivenalmostentirelybymath. Weusemathto\nposition andorient objects in the game world, move them around, test for col-\nlisions, and cast rays to determine lines of sight, and of course we use matrix\nmultiplicationtotransformobjectsfromobjectspacetoworldspaceandeven-\ntually into screen space for rendering. Almost all modern games are three-\ndimensional, but even in a two-dimensional game it can be very difficult to\nmentally visualize the results of all these mathematical calculations. For this\nreason, most good game engines provide an API for drawing colored lines,\n10.2. Debug Drawing Facilities 595\nsimple shapes and 3D text. We call this a debug drawing facility, because the\nlines, shapes and text that are drawn with it are intended for visualization\nduring development and debugging and are removed prior to shipping the\ngame.\nAdebug drawing API can save you huge amounts of time. For example,\nif you are trying to figure out why your projectiles are not hitting the enemy\ncharacters,whichiseasier? Decipheringabunchofnumbersinthedebugger?\nOr drawing a line showing the trajectory of the projectile in three dimensions\nwithin your game? With a debug drawing API, logical and mathematical er-\nrors become immediately obvious. One might say that a picture is worth a\nthousand minutes of debugging.\nHerearesomeexamplesofdebugdrawinginactionwithinNaughtyDog’s\nengine. The following screenshots were all taken within our play-test level,\none of many special levels we use for testing out new features and debugging\nproblems in the game.\n• Figure 10.1 shows a visualization of an enemy NPC’s perception of the\nplayer. The little “stick man” figure represents the location of the player\nas perceived by the NPC. When the player has broken the line of sight\nbetweenhimselfandtheNPC,the“stickman”willremainattheplayer’s\nlast known location, even after the player sneaks away.\n• Figure 10.2 shows how a wireframe sphere can be used to visualize the\ndynamically expanding blast radius of an explosion.\nFigure 10.1. Visualizing the line of sight from an NPC to the player in The Last of Us: Remastered\n(© 2014/™ SIE. Created and developed by Naughty Dog, PlayStation 4).\n596 10. Tools for Debugging and Development\nFigure 10.2. Visualizing the expanding blast sphere of an explosion in the Naughty Dog engine.\n• Figure 10.3 shows how circles can be used to visualize the radii used by\nDrakewhensearchingforledgestohangfrominthegame. Alineshows\nthe ledge he is currently hanging from.\n• Figure 10.4 shows an AI character that has been placed in a special de-\nbugging mode. In this mode, the character’s brain is effectively turned\noff, and the developer is given full control over the character’s move-\nmentsandactionsviaasimpleheads-upmenu. Thedevelopercanpaint\nFigure 10.3. Spheres and vectors used in Drake’s ledge hang and shimmy system in the Uncharted\nseries (© 2014/™ SIE. Created and developed by Naughty Dog, PlayStation 3).\n10.2. Debug Drawing Facilities 597\nFigure 10.4. Manually controlling an NPC’s actions for debugging purposes in The Last of Us: Re-\nmastered (© 2014/™ SIE. Created and developed by Naughty Dog, PlayStation 4).\ntarget points in the game world by simply aiming the camera and can\nthen instruct the character to walk, run or sprint to the specified points.\nThe user can also tell the character to enter or leave nearby cover, fire its\nweapon and so on.\n10.2.1 Debug Drawing API\nA debug drawing API generally needs to satisfy the following requirements:\n• The API should be simple and easy to use.\n• It should support a useful set of primitives, including (but not limited\nto):\n◦lines,\n◦spheres,\n◦points (usually represented as small crosses or spheres, because a\nsingle pixel is very difficult to see),\n◦coordinateaxes(typically, the x-axisisdrawninred, yingreenand\nzin blue),\n◦bounding boxes, and\n◦formatted text.\n• It should provide a good deal of flexibility in controlling how primitives\nare drawn, including:\n598 10. Tools for Debugging and Development\n◦color,\n◦line width,\n◦sphere radii,\n◦the size of points, lengths of coordinate axes, and dimensions of\nother “canned” primitives.\n• Itshouldbepossibletodrawprimitivesinworldspace(full3D,usingthe\ngame camera’s perspective projection matrix) or in screen space (either\nusing an orthographic projection, or possibly a perspective projection).\nWorld-spaceprimitivesareusefulforannotatingobjectsinthe3Dscene.\nScreen-space primitives are helpful for displaying debugging informa-\ntion in the form of a heads-up display that is independent of camera\nposition or orientation.\n• It should be possible to draw primitives with or without depth testing\nenabled.\n◦When depth testing is enabled, the primitives will be occluded by\nreal objects in your scene. This makes their depth easy to visualize,\nbut it also means that the primitives may sometimes be difficult to\nsee or totally hidden by the geometry of your scene.\n◦With depth testing disabled, the primitives will “hover” over the\nreal objects in the scene. This makes it harder to gauge their real\ndepth,butitalsoensuresthatnoprimitiveiseverhiddenfromview.\n• ItshouldbepossibletomakecallstothedrawingAPIfromanywherein\nyour code. Most rendering engines require that geometry be submitted\nforrenderingduringaspecificphaseofthegameloop,usuallyattheend\nof each frame. This requirement implies that the system must queue up\nall incoming debug drawing requests, so that they may be submitted at\nthe proper time later on.\n• Ideally, every debug primitive should have a lifetimeassociated with it.\nThe lifetime controls how long the primitive will remain on-screen after\nhavingbeenrequested. Ifthecodethatisdrawingtheprimitiveiscalled\neveryframe,thelifetimecanbeoneframe—theprimitivewillremainon-\nscreenbecauseitwillberefreshedeveryframe. However,ifthecodethat\ndrawstheprimitiveiscalledrarelyorintermittently(e.g.,afunctionthat\ncalculates the initial velocity of a projectile), then you do not want the\nprimitive to flicker on-screen for just one frame and then disappear. In\nsuch situations, the programmer should be able to give his or her debug\nprimitives a longer lifetime, on the order of a few seconds.\n10.2. Debug Drawing Facilities 599\n• It’salsoimportantthatthedebugdrawingsystembecapableofhandling\na large number of debug primitives efficiently. When you’re drawing\ndebug information for 1,000 game objects, the number of primitives can\nreallyaddup,andyoudon’twantyourgametobeunusablewhendebug\ndrawing is turned on.\nThe debug drawing API in Naughty Dog’s engine looks something like\nthis:\nclass DebugDrawManager\n{\npublic:\n// Adds a line segment to the debug drawing queue.\nvoid AddLine(const Point& fromPosition,\nconst Point& toPosition,\nColor color,\nfloat lineWidth = 1.0f,\nfloat duration = 0.0f,\nbool depthEnabled = true);\n// Adds an axis-aligned cross (3 lines converging at\n// a point) to the debug drawing queue.\nvoid AddCross(const Point& position,\nColor color,\nfloat size,\nfloat duration = 0.0f,\nbool depthEnabled = true);\n// Adds a wireframe sphere to the debug drawing queue.\nvoid AddSphere(const Point& centerPosition,\nfloat radius,\nColor color,\nfloat duration = 0.0f,\nbool depthEnabled = true);\n// Adds a circle to the debug drawing queue.\nvoid AddCircle(const Point& centerPosition,\nconst Vector& planeNormal,\nfloat radius,\nColor color,\nfloat duration = 0.0f,\nbool depthEnabled = true);\n// Adds a set of coordinate axes depicting the\n// position and orientation of the given\n// transformation to the debug drawing queue.\nvoid AddAxes(const Transform& xfm,\nColor color,\nfloat size,\n600 10. Tools for Debugging and Development\nfloat duration = 0.0f,\nbool depthEnabled = true);\n// Adds a wireframe triangle to the debug drawing\n// queue.\nvoid AddTriangle(const Point& vertex0,\nconst Point& vertex1,\nconst Point& vertex2,\nColor color,\nfloat lineWidth = 1.0f,\nfloat duration = 0.0f,\nbool depthEnabled = true);\n// Adds an axis-aligned bounding box to the debug\n// queue.\nvoid AddAABB(const Point& minCoords,\nconst Point& maxCoords,\nColor color,\nfloat lineWidth = 1.0f,\nfloat duration = 0.0f,\nbool depthEnabled = true);\n// Adds an oriented bounding box to the debug queue.\nvoid AddOBB(const Mat44& centerTransform,\nconst Vector& scaleXYZ,\nColor color,\nfloat lineWidth = 1.0f,\nfloat duration = 0.0f,\nbool depthEnabled = true);\n// Adds a text string to the debug drawing queue.\nvoid AddString(const Point& pos,\nconst char* text,\nColor color,\nfloat duration = 0.0f,\nbool depthEnabled = true);\n};\n// This global debug drawing manager is configured for\n// drawing in full 3D with a perspective projection.\nextern DebugDrawManager g_debugDrawMgr;\n// This global debug drawing manager draws its\n// primitives in 2D screen space. The (x,y) coordinates\n// of a point specify a 2D location on-screen, and the\n// z coordinate contains a special code that indicates\n// whether the (x,y) coordidates are measured in absolute",10763
67-10.3 In-Game Menus.pdf,67-10.3 In-Game Menus,"10.3. In-Game Menus 601\n// pixels or in normalized coordinates that range from\n// 0.0 to 1.0. (The latter mode allows drawing to be\n// independent of the actual resolution of the screen.)\nextern DebugDrawManager g_debugDrawMgr2D;\nHere’s an example of this API being used within game code:\nvoid Vehicle::Update()\n{\n// Do some calculations...\n// Debug-draw my velocity vector.\nconst Point& start = GetWorldSpacePosition();\nPoint end = start + GetVelocity();\ng_debugDrawMgr.AddLine (start, end, kColorRed);\n// Do some other calculations...\n// Debug-draw my name and number of passengers.\n{\nchar buffer[128];\nsprintf(buffer, ""Vehicle %s: %d passengers"",\nGetName(), GetNumPassengers());\nconst Point& pos = GetWorldSpacePosition();\ng_debugDrawMgr.AddString(pos,\nbuffer, kColorWhite, 0.0f, false);\n}\n}\nYou’ll notice that the names of the drawing functions use the verb “add”\nrather than “draw.” This is because the debug primitives are typically not\ndrawn immediately when the drawing function is called. Instead, they are\naddedtoalistofvisualelementsthatwillbedrawnatalatertime. Mosthigh-\nspeed 3D rendering engines require that all visual elements be maintained in\nascenedata structure so that they can be drawn efficiently, usually at the end\nof the game loop. We’ll learn a lot more about how rendering engines work in\nChapter 11.\n10.3 In-Game Menus\nEvery game engine has a large number of configuration options and fea-\ntures. Infact,eachmajorsubsystem,includingrendering,animation,collision,\nphysics, audio, networking, player mechanics, AI and so on, exposes its own\nspecialized configuration options. It is highly useful to programmers, artists\n602 10. Tools for Debugging and Development\nand game designers alike to be able to configure these options while the game\nis running, without having to change the source code, recompile and relink\nthe game executable, and then rerun the game. This can greatly reduce the\namount of time the game development team spends on debugging problems\nand setting up new levels or game mechanics.\nOne simple and convenient way to permit this kind of thing is to provide\na system of in-game menus. Items on an in-game menu can do any number of\nthings, including (but certainly not limited to):\n• toggling global Boolean settings,\n• adjusting global integer and floating-point values,\n• calling arbitrary functions, which can perform literally any task within\nthe engine, and\n• bringing up submenus, allowing the menu system to be organized hier-\narchically for easy navigation.\nAn in-game menu should be easy and convenient to bring up, perhaps via\na simple button press on the joypad. (Of course, you’ll want to choose a but-\nton combination that doesn’t occur during normal gameplay.) Bringing up\nthe menus usually pauses the game. This allows the developer to play the\ngame until the moment just before a problem occurs, then pause the game\nby bringing up the menus, adjust engine settings in order to visualize the\nFigure 10.5. Main development menu in The Last of Us: Remastered (© 2014/™ SIE. Created and\ndeveloped by Naughty Dog, PlayStation 4).\n10.3. In-Game Menus 603\nFigure 10.6. Rendering submenu in The Last of Us: Remastered (© 2014/™ SIE. Created and devel-\noped by Naughty Dog, PlayStation 4).\nproblem more clearly, and then un-pause the game to inspect the problem in\ndepth.\nLet’s take a brief look at how the menu system works in the Naughty Dog\nengine. Figure 10.5 shows the top-level menu. It contains submenus for each\nmajor subsystem in the engine. In Figure 10.6, we’ve drilled down one level\nFigure 10.7. Mesh options subsubmenu in The Last of Us: Remastered (© 2014/™ SIE. Created and\ndeveloped by Naughty Dog, PlayStation 4).",3753
68-10.6 Cheats.pdf,68-10.6 Cheats,"604 10. Tools for Debugging and Development\nFigure 10.8. Background meshes turned off ( The Last of Us: Remastered © 2014/™ SIE. Created and\ndeveloped by Naughty Dog, PlayStation 4).\ninto the Rendering… submenu. Since the rendering engine is a highly com-\nplex system, its menu contains many submenus controlling various aspects\nof rendering. To control the way in which 3D meshes are rendered, we drill\ndownfurtherintothe MeshOptions… submenu,showninFigure10.7. Onthis\nmenu, wecanturnoffrenderingofallstaticbackgroundmeshes, leavingonly\nthe dynamic foreground meshes visible. This is shown in Figure 10.8. (Ah ha,\nthere’s that pesky deer!)\n10.4 In-Game Console\nSomeenginesprovideanin-gameconsole, eitherinlieuoforinadditiontoan\nin-game menu system. An in-game console provides a command-line inter-\nface to the game engine’s features, much as a DOS command prompt provides\nusers with access to various features of the Windows operating system, or a\ncsh, tcsh, ksh or bash shell prompt provides users with access to the features\nof UNIX-like operating systems. Much like a menu system, the game engine\nconsole can provide commands allowing a developer to view and manipulate\nglobal engine settings, as well as running arbitrary commands.\nA console is somewhat less convenient than a menu system, especially for\nthosewhoaren’tveryfasttypists. However,aconsolecanbemuchmorepow-\nerful than a menu. Some in-game consoles provide only a rudimentary set\nof hard-coded commands, making them about as flexible as a menu system.\n10.5. Debug Cameras and Pausing the Game 605\nFigure 10.9. The in-game console in Minecraft, overlaid on top of the main game screen and dis-\nplaying a list of valid commands.\nBut others provide a rich interface to virtually every feature of the engine. A\nscreenshot of the in-game console in Minecraft is shown in Figure 10.9.\nSomegameenginesprovideapowerfulscriptinglanguagethatcanbeused\nbyprogrammersandgamedesignerstoextendthefunctionalityoftheengine,\nor even build entirely new games. If the in-game console “speaks” this same\nscripting language, then anything you can do in script can also be done inter-\nactively via the console. We’ll explore scripting languages in depth in Section\n16.9.\n10.5 Debug Cameras and Pausing the Game\nAn in-game menu or console system is best accompanied by two other crucial\nfeatures: (a) the ability to detach the camera from the player character and fly\nit around the game world in order to scrutinize any aspect of the scene, and\n(b) the ability to pause, un-pause and single-step the game (see Section 8.5.5).\nWhen the game is paused, it is still important to be able to control the camera.\nTosupportthis,wecansimplykeeptherenderingengineandcameracontrols\nrunning, even when the game’s logical clock is paused.\nSlowmotionmodeisanotherincrediblyusefulfeatureforscrutinizingani-\nmations,particleeffects,physicsandcollisionbehaviors,AIbehaviors,andthe\nlist goes on. This feature is easy to implement. Presuming we’ve taken care\nto update all gameplay elements using a clock that is distinct from the real-\ntimeclock,wecanputthegameintoslo-mobysimplyupdatingthegameplay\nclock at a rate that is slower than usual. This approach can also be used to im-\nplement a fast-motion mode, which can be useful for moving rapidly through\ntime-consumingportionsofgameplayinordertogettoanareaofinterest(not\nto mention being a great source of laughs, especially when accompanied by a\nbad vocal rendition of Benny Hill music…).",3518
69-10.7 Screenshots and Movie Capture.pdf,69-10.7 Screenshots and Movie Capture,"606 10. Tools for Debugging and Development\n10.6 Cheats\nWhen developing or debugging a game, it’s important to allow the user to\nbreak the rules of the game in the name of expediency. Such features are aptly\nnamedcheats. For example, many engines allow you to “pick up” the player\ncharacterandflyhimorheraroundinthegameworld,withcollisionsdisabled\nso he or she can pass through all obstacles. This can be incredibly helpful for\ntesting out gameplay. Rather than taking the time to actually play the game\nin an attempt to get the player character into some desirable location, you can\nsimply pick him up, fly him over to where you want him to be, and then drop\nhim back into his regular gameplay mode.\nOther useful cheats include, but are certainly not limited to:\n•Invincible player . As a developer, you often don’t want to be bothered\nhaving to defend yourself from enemy characters, or worrying about\nfalling from too high a height, as you test out a feature or track down\na bug.\n•Give player weapon. It’s often useful to be able to give the player any\nweapon in the game for testing purposes.\n•Infiniteammo . Whenyou’retryingtokillbadguystotestouttheweapon\nsystem or AI hit reactions, you don’t want to be scrounging for clips!\n•Select player mesh. If the player character has more than one “costume,”\nit can be useful to be able to select any of them for testing purposes.\nObviously this list could go on for pages. The sky’s the limit—you can\nadd whatever cheats you need in order to develop or debug the game. You\nmight even want to expose some of your favorite cheats to the players of the\nfinal shipping game. Players can usually activate cheats by entering unpub-\nlishedcheatcodes on the joypad or keyboard and/or by accomplishing certain\nobjectives in the game.\n10.7 Screenshots and Movie Capture\nAnotherextremelyusefulfacilityistheabilitytocapturescreenshotsandwrite\nthem to disk in a suitable image format such as Windows Bitmap files (.bmp),\nJPEG(.jpg)orTarga(.tga). Thedetailsofhowtocaptureascreenshotvaryfrom\nplatform to platform, but they typically involve making a call to the graphics\nAPI that allows the contents of the frame buffer to be transferred from video\nRAMtomainRAM,whereitcanbescannedandconvertedintotheimagefile\nformat of your choice. The image files are typically written to a predefined\n10.7. Screenshots and Movie Capture 607\nfolder on disk and named using a date and time stamp to guarantee unique\nfile names.\nYoumay want to provideyourusers with various options controllinghow\nscreenshots are to be captured. Some common examples include:\n• Whether or not to include debug primitives and text in the screenshot.\n• Whether or not to include heads-up display (HUD) elements in the\nscreenshot.\n• The resolution at which to capture. Some engines allow high-resolution\nscreenshots to be captured, perhaps by modifying the projection matrix\nso that separate screenshots can be taken of the four quadrants of the\nscreen at normal resolution and then combined into the final high-res\nimage.\n• Simple camera animations. For example, you could allow the user to\nmark the starting and ending positions and orientations of the camera.\nA sequence of screenshots could then be taken while gradually interpo-\nlating the camera from the starting location to the ending location.\nSome engines also provide a full-fledged movie capture mode. Such a sys-\ntem captures a sequence of screenshots at the target frame rate of the game,\nwhich are processed either offline or at runtime to generate a movie file in a\nsuitable format such as MPEG-2 (H.262) or MPEG-4 Part 10 (H.264). But even\nifyourenginedoesn’tsupportreal-timevideocapture, externalhardwarelike\nRoxio Game Capture HD Pro can always be used to capture the output from\nyour game console or PC. And for PC and Mac games, a great many software\nvideo capture tools are available, including Frapsby Beepa, Camtasia by Cam-\ntasia Software, Dxtoryby ExKode, Debutby NCH Software and Action! by\nMirillis.\nThe PlayStation 4 has built-in support for sharing screenshots and video\nclips taken from within the game. During gameplay, the PS4 is continually\ncapturing video of the most-recent 15 minutes of the user’s in-game experi-\nence. At any moment, the user can hit the Share button on the controller and\nopt to share a screenshot or the recorded video in various ways—by saving it\nto the PS4’s HDD or a thumb drive, or by uploading it to one of a number of\nonline services. At Naughty Dog, we use these facilities to capture video and\na screenshot of the game any time it crashes, thereby allowing us to see what\nsituation led up to the crash.\nPlayStation 4 users can also live-stream video of their playthroughs. For\ndevelopment purposes, it’s possible to use a PC to connect to a remote PS4\n(perhapssittingonanotherdeveloper’sdeskwithinoroutsideyourstudio),to",4905
70-10.8 In-Game Profiling.pdf,70-10.8 In-Game Profiling,"608 10. Tools for Debugging and Development\nFigure 10.10. The Naughty Dog engine provides a proﬁle hierarchy display that allows the user to\ndrill down into particular function calls to inspect their costs.\nseethegamebeingplayedviastreamingvideo,andeventocontroltheremote\ngame by plugging a PS4 controller into a USB slot on the PC. This facility can\nbeincrediblyusefulforthose“butitworksonmymachine”moments,because\nyou can debug the problem directly on the other person’s PS4.\n10.8 In-Game Proﬁling\nGames are real-time systems, so achieving and maintaining a high frame rate\n(usually30FPSor60FPS)isimportant. Therefore, partofanygameprogram-\nmer’s job is ensuring that his or her code runs efficiently and within budget.\nAs we saw when we discussed the 80/20 rule in Chapter 2, a large percentage\nof your code probably doesn’t need to be optimized. The only way to know\nwhichbits require optimization is to measure your game’s performance. We dis-\ncussed various third-party profiling tools in Chapter 2. However, these tools\nhave various limitations and may not be available at all on a console. For this\nreason, and/or for convenience, many game engines provide an in-game pro-\nfiling tool of some sort.\nTypically an in-game profiler permits the programmer to annotate blocks\nof code that should be timed and give them human-readable names. The pro-\nfiler measures the execution time of each annotated block via the CPU’s hi-res\ntimerandstorestheresultsinmemory. Aheads-updisplayisprovided,which\nshows up-to-date execution times for each code block (examples are shown in\nFigures10.10and10.11). Thedisplayoftenprovidesthedatainvariousforms,\n10.8. In-Game Proﬁling 609\nincluding raw numbers of cycles, execution times in microseconds, and per-\ncentages relative to the execution time of the entire frame.\nFigure 10.11. The timeline mode in Uncharted: The Lost Legacy (© 2017/™ SIE. Created and developed\nby Naughty Dog, PlayStation 4) shows exactly when various operations are performed across a\nsingle frame on the PS4’s seven CPU cores.\n10.8.1 Hierarchical Proﬁling\nComputer programs written in an imperative language are inherently hierar-\nchical—a function calls other functions, which in turn call still more functions.\nForexample, let’simaginethatfunction a()callsfunctions b()andc(), and\nfunction b()in turn calls functions d(), e()andf(). The pseudocode for\nthis is shown below.\nvoid a()\n{\nb();\nc();\n}\nvoid b()\n{\nd();\ne();\nf();\n}\nvoid c() { ... }\n610 10. Tools for Debugging and Development\nFigure 10.13. Call stack resulting from setting a breakpoint in function e() .\nvoid d() { ... }\nvoid e() { ... }\nvoid f() { ... }\nFigure 10.12. A hypo-\nthetical function call\nhierarchy.Assuming function a()is called directly from main() , this function call\nhierarchy is shown in Figure 10.12.\nWhen debugging a program, the call stack shows only a snapshot of this\ntree. Specifically,itshowsusthepathfromwhicheverfunctioninthehierarchy\niscurrently executing all the way to the root function in the tree. In C/C++,\nthe root function is usually main() orWinMain(), although technically this\nfunction is called by a start-up function that is part of the standard C runtime\nlibrary (CRT), so that function is the true root of the hierarchy. If we set a\nbreakpoint in function e(), for example, the call stack would look something\nlike this:\ne()  The curr ently executing function.\nb()\na()\nmain()\n_crt_startup()  Root of the call hierarchy.\nThis call stack is depicted in Figure 10.13 as a pathway from function e()to\nthe root of the function call tree.\n10.8.1.1 Measuring Execution Times Hierarchically\nIf we measure the execution time of a single function, the time we measure\nincludes the execution time of any the child functions called and all of their\n10.8. In-Game Proﬁling 611\ngrandchildren, great-grandchildren and so on as well. To properly interpret\nany profiling data we might collect, we must be sure to take the function call\nhierarchy into account.\nMany commercial profilers can automatically instrument every single func-\ntion in your program. This permits them to measure both the inclusive and\nexclusive executiontimesofeveryfunctionthatiscalledduringaprofilingses-\nsion. As the name implies, inclusive times measure the execution time of the\nfunction including all of its children, while exclusive times measure only the\ntimespentinthefunctionitself. (Theexclusivetimeofafunctioncanbecalcu-\nlated by subtracting the inclusive times of all its immediate children from the\ninclusive time of the function in question.) In addition, some profilers record\nhowmanytimeseachfunctioniscalled. Thisisanimportantpieceofinforma-\ntiontohavewhenoptimizingaprogram,becauseitallowsyoutodifferentiate\nbetweenfunctionsthateatupalotoftimeinternallyandfunctionsthateatup\ntime because they are called a very large number of times.\nIn contrast, in-game profiling tools are not so sophisticated and usually\nrely onmanual instrumentation of the code. If our game engine’s main loop\nis structured simply enough, we may be able to obtain valid data at a coarse\nlevel without thinking much about the function call hierarchy. For example, a\ntypical game loop might look roughly like this:\nwhile (!quitGame)\n{\nPollJoypad();\nUpdateGameObjects();\nUpdateAllAnimations();\nPostProcessJoints();\nDetectCollisions();\nRunPhysics();\nGenerateFinalAnimationPoses();\nUpdateCameras();\nRenderScene();\nUpdateAudio();\n}\nWe could profile this game at a very coarse level by measuring the execu-\ntion times of each major phase of the game loop:\nwhile (!quitGame)\n{\n{\nPROFILE(SID(""Poll Joypad""));\nPollJoypad();\n}\n612 10. Tools for Debugging and Development\n{\nPROFILE(SID(""Game Object Update""));\nUpdateGameObjects();\n}\n{\nPROFILE(SID(""Animation""));\nUpdateAllAnimations();\n}\n{\nPROFILE(SID(""Joint Post-Processing""));\nPostProcessJoints();\n}\n{\nPROFILE(SID(""Collision""));\nDetectCollisions();\n}\n{\nPROFILE(SID(""Physics""));\nRunPhysics();\n}\n{\nPROFILE(SID(""Animation Finaling""));\nGenerateFinalAnimationPoses();\n}\n{\nPROFILE(SID(""Cameras""));\nUpdateCameras();\n}\n{\nPROFILE(SID(""Rendering""));\nRenderScene();\n}\n{\nPROFILE(SID(""Audio""));\nUpdateAudio();\n}\n}\nThePROFILE() macroshownabovewouldprobablybeimplementedasa\nclass whose constructor starts the timer and whose destructor stops the timer\nand records the execution time under the given name. Thus, it only times\nthe code within its containing block, by nature of the way C++ automatically\nconstructs and destroys objects as they go in and out of scope.\nstruct AutoProfile\n{\nAutoProfile(const char* name)\n10.8. In-Game Proﬁling 613\n{\nm_name = name;\nm_startTime = QueryPerformanceCounter();\n}\n~AutoProfile()\n{\nstd::int64_t endTime = QueryPerformanceCounter();\nstd::int64_t elapsedTime = endTime - m_startTime;\ng_profileManager.storeSample(m_name, elapsedTime);\n}\nconst char* m_name;\nstd::int64_t m_startTime;\n};\n#define PROFILE(name) AutoProfile p(name)\nThe problem with this simplistic approach is that it breaks down when\nused within deeper levels of function call nesting. For example, if we embed\nadditional PROFILE() annotations within the RenderScene() function, we\nneed to understand the function call hierarchy in order to properly interpret\nthose measurements.\nOne solution to this problem is to allow the programmer who is anno-\ntating the code to indicate the hierarchical interrelationships between\nprofiling samples. For example, any PROFILE(...) samples taken within\ntheRenderScene() function could be declared to be children of the\nPROFILE(SID(""Rendering"")) sample. These relationships are usually set\nupseparatelyfromtheannotationsthemselves,bypredeclaringallofthesam-\nple bins. For example, we might set up the in-game profiler during engine\ninitialization as follows:\n// This code declares various profile sample ""bins"",\n// listing the name of the bin and the name of its\n// parent bin, if any.\nProfilerDeclareSampleBin(SID(""Rendering""), nullptr);\nProfilerDeclareSampleBin(SID(""Visibility""), SID(""Rendering""));\nProfilerDeclareSampleBin(SID(""Shaders""), SID(""Rendering""));\nProfilerDeclareSampleBin(SID(""Materials""), SID(""Shaders""));\nProfilerDeclareSampleBin(SID(""SubmitGeo""), SID(""Rendering""));\nProfilerDeclareSampleBin(SID(""Audio""), nullptr);\n// ...\n614 10. Tools for Debugging and Development\nThis approach still has its problems. Specifically, it works well when every\nfunction in the call hierarchy has only one parent, but it breaks down when\nwe try to profile a function that is called by more than one parent function.\nThe reason for this should be pretty obvious. We’re statically declaring our\nsample bins as ifevery function can only appear once in the function call hi-\nerarchy, but actually the same function can reappear many times in the tree,\neach time with a different parent. The result can be misleading data, because\na function’s time will be included in one of the parent bins, but really should\nbe distributed across all of its parents’ bins. Most game engines don’t make\nan attempt to remedy this problem, since they are primarily interested in pro-\nfiling coarse-grained functions that are only called from one specific location\nin the function call hierarchy. But this limitation is something to be aware of\nwhen profiling your code with a simple in-engine profile of the sort found in\nmost game engines.\nOf course, it is also possible to write a much more sophisticated profiling\nsystem that handles nested instances of AutoProfile properly. This is an ex-\nample of the many trade-offs one makes when designing a game engine. Do\nwe invest the engineering time to create a fully hierarchical profiler? Or, do\nwemakedowithsomethingsimplerandinvestthoseprogrammingresources\nelsewhere? Ultimately, it’s up to you.\nWewouldalsoliketoaccountforhowmany timesagivenfunctioniscalled.\nIntheexampleabove, weknowthateachofthefunctionsweprofilediscalled\nexactly once per frame. But other functions, deeper in the function call hier-\narchy, may be called more than once per frame. If we measure function x()\nto take 2 ms to execute, it’s important to know whether it takes 2 ms to exe-\ncute on its own, or whether its execution time is 2 ms but it was called 1,000\ntimes during the frame. Keeping track of the number of times a function is\ncalled per frame is quite simple—the profiling system can simply increment\na counter each time a sample is received and reset the counters at the start of\neach frame.\n10.8.2 Exporting to Excel\nSome game engines permit the data captured by the in-game profiler to be\ndumped to a text file for subsequent analysis. I find that a comma-separated\nvalues (CSV) format is best, because such files can be loaded easily into a Mi-\ncrosoftExcelspreadsheet,wherethedatacanbemanipulatedandanalyzedin\nmyriad ways. I wrote such an exporter for the Medal of Honor: Pacific Assault\nengine. The columns corresponded to the various annotated blocks, and each\nrow represented the profiling sample taken during one frame of the game’s",11119
71-10.9 In-Game Memory Stats and Leak Detection.pdf,71-10.9 In-Game Memory Stats and Leak Detection,"10.9. In-Game Memory Stats and Leak Detection 615\nexecution. The first column contained frame numbers and the second actual\ngame time measured in seconds. This allowed the team to graph how the per-\nformance statistics varied over time and to determine how long each frame\nactually took to execute. By adding some simple formulae to the exported\nspreadsheet, we could calculate frame rates, execution time percentages and\nso on.\n10.9 In-Game Memory Stats and Leak Detection\nIn addition to runtime performance (i.e., frame rate), most game engines are\nalso constrained by the amount of memory available on the target hardware.\nPC games are least affected by such constraints, because modern PCs have\nsophisticated virtual memory managers. But even PC games are constrained\nby the memory limitations of their so-called “min spec” machine—the least-\npowerful machine on which the game is guaranteed to run, as promised by\nthe publisher and stated on the game’s packaging.\nFor this reason, most game engines implement custom memory-tracking\ntools. These tools allow the developers to see how much memory is being\nusedbyeachenginesubsystemandwhetherornotanymemoryisleaking(i.e.,\nmemory is allocated but never freed). It’s important to have this information\nso that you can make informed decisions when trying to cut back the memory\nusage of your game so that it will fit onto the console or type of PC you are\ntargeting.\nKeeping track of how much memory a game actually uses can be a sur-\nprisingly tricky job. You’d think you could simply wrap malloc()/free()\nornew/delete in a pair of functions or macros that keep track of the\namountofmemorythatisallocatedandfreed. However,it’sneverthatsimple\nfor a few reasons:\n1.You often can’t control the allocation behavior of other people’s code . Unless\nyou write the operating system, drivers and the game engine entirely\nfrom scratch, there’s a good chance you’re going to end up linking your\ngame with at least some third-party libraries. Most good libraries pro-\nvidememory allocation hooks so that you can replace their allocators with\nyour own. But some do not. It’s often difficult to keep track of the mem-\noryallocatedbyeachandeverythird-partylibraryyouuseinyourgame\nengine—but it usually canbe done if you’re thorough and selective in\nyour choice of third-party libraries.\n2.Memory comes in different flavors. For example, a PC has two kinds of\nRAM: main RAM and video RAM (the memory residing on your graph-\n616 10. Tools for Debugging and Development\nics card, which is used primarily for geometry and texture data). Even if\nyou manage to track all of the memory allocations and deallocations oc-\ncurringwithinmainRAM,itcanbewellneighimpossibletotrackvideo\nRAMusage. ThisisbecausegraphicsAPIslikeDirectXactuallyhidethe\ndetails of how video RAM is being allocated and used from the devel-\noper. On a console, life is a bit easier only because you often end up\nhaving to write a video RAM manager yourself. This is more difficult\nthan using DirectX, but at least you have complete knowledge of what’s\ngoing on.\n3.Allocators come in different flavors. Many games make use of specialized\nallocators for various purposes. For example, the Naughty Dog engine\nhas aglobalheap for general-purpose allocations, a special heap for man-\naging the memory created by game objects as they spawn into the game\nworld and are destroyed, a level-loadingheap for data that is streamed into\nmemory during gameplay, a stack allocator for single-frame allocations\n(the stack is cleared automatically every frame), an allocator for video\nRAM, and a debug memory heap used only for allocations that will not be\nneeded in the final shipping game. Each of these allocators grabs a large\nhunk of memory when the game starts up and then manages that mem-\nory block itself. If we were to track all the calls to new and delete, we’d\nseeonenewforeachofthesesixallocatorsandthat’sall. Togetanyuse-\nful information, we really need to track all of the allocations withineach\nof these allocators’ memory blocks.\nMost professional game teams expend a significant amount of effort on\ncreating in-engine memory-tracking tools that provide accurate and detailed\ninformation. The resulting tools usually provide their output in a variety of\nforms. Forexample,theenginemightproduceadetaileddumpofallmemory\nallocationsmadebythegameduringaspecificperiodoftime. Thedatamight\ninclude high water marks for each memory allocator or each game system,\nindicating the maximum amount of physical RAM required by each. Some\nengines also provide heads-up displays of memory usage while the game is\nrunning. This data might be tabular, as shown in Figure 10.14, or graphical as\nshown in Figure 10.15.\nInaddition, whenlow-memoryorout-of-memoryconditionsarise, agood\nengine will provide this information in as helpful a way as possible. When\nPC games are developed, the game team usually works on high-powered PCs\nwithmoreRAMthanthemin-specmachinebeingtargeted. Likewise, console\ngames are developed on special developmentkits that have more memory than\na retail console. So in both cases, the game can continue to run even when it\n10.9. In-Game Memory Stats and Leak Detection 617\nFigure 10.14. Tabular memory statistics from Naughty Dog’s engine.\nFigure 10.15. A graphical memory usage display, also from The Last of Us: Remastered (© 2014/™\nSIE. Created and developed by Naughty Dog, PlayStation 4).\ntechnically has run out of memory (i.e., would no longer fit on a retail console\nor min-spec PC). When this kind of out-of-memory condition arises, the game\nengine can display a message saying something like, “Out of memory—this\nlevel will not run on a retail system.”\nTherearelotsofotherwaysinwhichagameengine’smemorytrackingsys-\ntem can aid developers in pinpointing problems as early and as conveniently\n618 10. Tools for Debugging and Development\nas possible. Here are just a few examples:\n• If a model fails to load, a bright red text string could be displayed in 3D\nhovering in the game world where that object would have been.\n• If a texture fails to load, the object could be drawn with an ugly pink\ntexture that is very obviously not part of the final game.\n• If an animation fails to load, the character could assume a special (pos-\nsibly humorous) pose that indicates a missing animation, and the name\nof the missing asset could hover over the character’s head.\nThekeytoprovidinggoodmemoryanalysistoolsis(a)toprovideaccurate\ninformation, (b)topresentthedatainawaythatisconvenientandthatmakes\nproblemsobviousand(c)toprovidecontextualinformationtoaidtheteamin\ntracking down the root cause of problems when they occur.",6724
72-III Graphics Motion and Sound.pdf,72-III Graphics Motion and Sound,"Part III\nGraphics, Motion\nand Sound\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",110
73-11.1 Foundations of Depth-Buffered Triangle Rasterization.pdf,73-11.1 Foundations of Depth-Buffered Triangle Rasterization,"11\nThe Rendering Engine\nWhenmostpeoplethinkaboutcomputerandvideogames, thefirstthing\nthat comes to mind is the stunning three-dimensional graphics. Real-\ntime 3D rendering is an exceptionally broad and profound topic, so there’s\nsimply no way to cover all of the details in a single chapter. Thankfully there\nare a great many excellent books and other resources available on this topic.\nIn fact, real-time3D graphics is perhapsone of the best coveredof all the tech-\nnologies that make up a game engine. The goal of this chapter, then, is to pro-\nvide you with a broad understanding of real-time rendering technology and\ntoserveasajumping-offpointforfurtherlearning. Afteryou’vereadthrough\nthese pages, you should find that reading other books on 3D graphics seems\nlike a journey through familiar territory. You might even be able to impress\nyour friends at parties (…or alienate them…).\nWe’llbeginbylayingasolidfoundationintheconcepts,theoryandmathe-\nmaticsthatunderlieanyreal-time3Drenderingengine. Next,we’llhavealook\natthesoftwareandhardwarepipelinesusedtoturnthistheoreticalframework\ninto reality. We’ll discuss some of the most common optimization techniques\nand see how they drive the structure of the tools pipeline and the runtime\nrendering API in most engines. We’ll end with a survey of some of the ad-\nvanced rendering techniques and lighting models in use by game engines to-\nday. Throughout this chapter, I’ll point you to some of my favorite books and\n621\n622 11. The Rendering Engine\nother resources that should help you to gain an even deeper understanding of\nthe topics we’ll cover here.\n11.1 Foundations of Depth-Buffered\nTriangle Rasterization\nWhen you boil it down to its essence, rendering a three-dimensional scene\ninvolves the following basic steps:\n• Avirtual scene is described, usually in terms of 3D surfaces represented\nin some mathematical form.\n• Avirtual camera is positioned and oriented to produce the desired view\nofthescene. Typicallythecameraismodeledasanidealizedfocalpoint,\nwithanimagingsurfacehoveringsomesmalldistanceinfrontofit,com-\nposedof virtuallightsensors correspondingtothepictureelements(pixels)\nof the target display device.\n• Various lightsources are defined. These sources provide all the light rays\nthat will interact with and reflect off the objects in the environment and\neventually find their way onto the image-sensing surface of the virtual\ncamera.\n• Thevisual properties of the surfaces in the scene are described. This de-\nfines how light should interact with each surface.\n• For each pixel within the imaging rectangle, the rendering engine calcu-\nlates the color and intensity of the light ray(s) converging on the virtual\ncamera’s focal point through that pixel. This is known as solvingtheren-\nderingequation (also called the shadingequation).\nThis high-level rendering process is depicted in Figure 11.1.\nMany different technologies can be used to perform the basic rendering\nsteps described above. The primary goal is usually photorealism , although\nsome games aim for a more stylized look (e.g., cartoon, charcoal sketch, wa-\ntercolor and so on). As such, rendering engineers and artists usually attempt\nto describe the properties of their scenes as realistically as possible and to\nuse light transport models that match physical reality as closely as possible.\nWithin this context, the gamut of rendering technologies ranges from tech-\nniques designed for real-time performance at the expense of visual fidelity, to\nthose designed for photorealism but which are not intended to operate in real\ntime.\nReal-timerenderingenginesperformthestepslistedaboverepeatedly,dis-\nplaying rendered images at a rate of 30, 50 or 60 frames per second to provide\n11.1. Foundations of Depth-Buffered Triangle Rasterization 623\nVirtual \nScreen\n(Near Plane)xCzCyC\nRendered\nImageCamera\nFrustumCamera\nFigure 11.1. The high-level rendering approach used by virtually all 3D computer graphics technologies.\nthe illusion of motion. This means a real-time rendering engine has at most\n33.3 ms to generate each image (to achieve a frame rate of 30 FPS). Usually\nmuch less time is available, because bandwidth is also consumed by other en-\nginesystemslikeanimation,AI,collisiondetection,physicssimulation,audio,\nplayer mechanics and other gameplay logic. Considering that film rendering\nengines often take anywhere from many minutes to many hours to render a\nsingle frame, the quality of real-time computer graphics these days is truly\nastounding.\n11.1.1 Describing a Scene\nA real-world scene is composed of objects. Some objects are solid, like a brick,\nand some are amorphous, like a cloud of smoke, but every object occupies a\nvolumeof3Dspace. Anobjectmightbe opaque(inwhichcaselightcannotpass\nthroughitsvolume), transparent (inwhichcaselightpassesthroughitwithout\nbeing scattered, so that we can see a reasonably clear image of whatever is be-\nhind the object), or translucent (meaning that light can pass through the object\nbut is scattered in all directions in the process, yielding only a blur of colors\nthat hint at the objects behind it).\nOpaque objects can be rendered by considering only their surfaces. We\ndon’t need to know what’s inside an opaque object in order to render it, be-\ncause light cannot penetrate its surface. When rendering a transparent or\ntranslucent object, we really should model how light is reflected, refracted,\nscattered and absorbed as it passes through the object’s volume. This requires\nknowledge of the interior structure and properties of the object. However,\n624 11. The Rendering Engine\nmost game engines don’t go to all that trouble. They just render the surfaces\nof transparent and translucent objects in almost the same way opaque objects\nare rendered. A simple numeric opacity measure known as alphais used to\ndescribe how opaque or transparent a surface is. This approach can lead to\nvarious visual anomalies (for example, surface features on the far side of the\nobject may be rendered incorrectly), but the approximation can be made to\nlook reasonably realistic in many cases. Even amorphous objects like clouds\nof smoke are often represented using particle effects, which are typically com-\nposed of large numbers of semitransparent rectangular cards. Therefore, it’s\nsafe to say that most game rendering engines are primarily concerned with\nrendering surfaces.\n11.1.1.1 Representations Used by High-End Rendering Packages\nTheoretically, a surface is a two-dimensional sheet comprised of an infinite\nnumber of points in three-dimensional space. However, such a description is\nclearly not practical. In order for a computer to process and render arbitrary\nsurfaces, we need a compact way to represent them numerically.\nSome surfaces can be described exactly in analytical form using a paramet-\nric surface equation . For example, a sphere centered at the origin can be rep-\nresented by the equation x2+y2+z2=r2. However, parametric equations\naren’t particularly useful for modeling arbitrary shapes.\nIn the film industry, surfaces are often represented by a collection of rect-\nangularpatcheseachformedfromatwo-dimensionalsplinedefinedbyasmall\nnumber of control points. Various kinds of splines are used, including Bézier\nsurfaces (e.g., bicubic patches, which are third-order Béziers—see http://en.\nwikipedia.org/wiki/Bezier_surface for more information), nonuniform ra-\ntional B-splines (NURBS—see http://en.wikipedia.org/wiki/Nurbs), Bézier\ntriangles and N-patches (also known as normal patches —see http://ubm.io/\n1iGnvJ5formoredetails). Modelingwithpatchesisabitlikecoveringastatue\nwith little rectangles of cloth or paper maché.\nHigh-end film rendering engines like Pixar’s RenderMan use subdivision\nsurfaces to define geometric shapes. Each surface is represented by a mesh\nof control polygons (much like a spline), but the polygons can be subdivided\ninto smaller and smaller polygons using the Catmull-Clark algorithm. This\nsubdivision typically proceeds until the individual polygons are smaller than\na single pixel in size. The biggest benefit of this approach is that no matter\nhow close the camera gets to the surface, it can always be subdivided further\nsothatitssilhouetteedgeswon’tlookfaceted. Tolearnmoreaboutsubdivision\nsurfaces, check out the following great article: http://ubm.io/1lx6th5.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 625\n11.1.1.2 Triangle Meshes\nGame developers have traditionally modeled their surfaces using triangle\nmeshes. Trianglesserveasapiecewiselinearapproximationtoasurface,much\nas a chain of connected line segments acts as a piecewise approximation to a\nfunction or curve (see Figure 11.2).\nFigure 11.2. A mesh of\ntriangles is a linear ap-\nproximation to a sur-\nface, just as a series\nof connected line seg-\nments can serve as a lin-\near approximation to a\nfunction or curve.Triangles are the polygon of choice for real-time rendering because they\nhave the following desirable properties:\n•The triangle is the simplest type of polygon. Any fewer than three vertices,\nand we wouldn’t have a surface at all.\n•A triangle is always planar. Any polygon with four or more vertices need\nnot have this property because, while the first three vertices define a\nplane, the fourth vertex might lie above or below that plane.\n•Triangles remain triangles under most kinds of transformations, including\naffine transforms and perspective projections . At worst, a triangle viewed\nedge-on will degenerate into a line segment. At every other orientation,\nit remains triangular.\n•Virtuallyallcommercialgraphics-accelerationhardwareisdesignedaroundtri-\nangle rasterization . Starting with the earliest 3D graphics accelerators\nfor the PC, rendering hardware has been designed almost exclusively\naround triangle rasterization. This decision can be traced all the way\nback to the first software rasterizers used in the earliest 3D games like\nCastle Wolfenstein 3D andDoom. Like it or not, triangle-based technolo-\ngies are entrenched in our industry and probably will be for years to\ncome.\nTessellation\nThe term tessellation describes a process of dividing a surface up into a collec-\ntion of discrete polygons (which are usually either quadrilaterals, also known\nasquads, or triangles). Triangulation is tessellation of a surface into triangles.\nOne problem with the kind of triangle mesh used in games is that its level\nof tessellation is fixed by the artist when he or she creates it. Fixed tessellation\ncan cause an object’s silhouette edges to look blocky, as shown in Figure 11.3;\nthis is especially noticeable when the object is close to the camera.\nIdeally, we’d like a solution that can arbitrarily increase tessellation as an\nobject gets closer to the virtual camera. In other words, we’d like to have a\nuniformtriangle-to-pixeldensity,nomatterhowcloseorfarawaytheobjectis.\nSubdivision surfaces can achieve this ideal—surfaces can be tessellated based\non distance from the camera, so that every triangle is less than one pixel in\nsize.\n626 11. The Rendering Engine\nFigure 11.3. Fixed tessellation can cause an object’s silhouette edges to look blocky, especially when\nthe object is close to the camera.\nGame developers often attempt to approximate this ideal of uniform tri-\nangle-to-pixeldensitybycreatingachainofalternateversionsofeachtriangle\nmesh, each known as a levelofdetail (LOD). The first LOD, often called LOD 0,\nrepresents the highest level of tessellation; it is used when the object is very\nclose to the camera. Subsequent LODs are tessellated at lower and lower res-\nolutions (see Figure 11.4). As the object moves farther away from the camera,\ntheengineswitchesfromLOD0toLOD1toLOD2andsoon. Thisallowsthe\nrendering engine to spend the majority of its time transforming and lighting\nthe vertices of the objects that are closest to the camera (and therefore occupy\nthe largest number of pixels on-screen).\nSome game engines apply dynamic tessellation techniques to expansive\nmesheslikewaterorterrain. Inthistechnique,themeshisusuallyrepresented\nby a height field defined on some kind of regular grid pattern. The region of\nthe mesh that is closest to the camera is tessellated to the full resolution of\nthe grid. Regions that are farther away from the camera are tessellated using\nfewer and fewer grid points.\nProgressivemeshes areanothertechniquefordynamictessellationandLOD-\ning. With this technique, a single high-resolution mesh is created for dis-\nplay when the object is very close to the camera. (This is essentially the\nFigure 11.4. A chain of LOD meshes, each with a ﬁxed level of tessellation, can be used to approx-\nimate uniform triangle-to-pixel density. The leftmost torus is constructed from 5000 triangles,\nthe center torus from 450 triangles and the rightmost torus from 200 triangles.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 627\nLOD 0 mesh.) This mesh is automatically detessellated as the object gets far-\nther away by collapsing certain edges. In effect, this process automatically\ngenerates a semi-continuous chain of LODs. See http://research.microsoft.\ncom/en-us/um/people/hoppe/pm.pdf for a detailed discussion of progres-\nsive mesh technology.\n11.1.1.3 Constructing a Triangle Mesh\nNowthatweunderstandwhattrianglemeshesareandwhythey’reused,let’s\ntake a brief look at how they’re constructed.\nWinding Order\nA triangle is defined by the position vectors of its three vertices, which we\ncan denote p1,p2andp3. The edges of a triangle can be found by simply\nsubtracting the position vectors of adjacent vertices. For example,\ne12=p2 p1,\ne13=p3 p1,\ne23=p3 p2.\nThe normalized cross product of any two edges defines a unit face normal N:\nN=e12e13\nje12e13j.\nThese derivations are illustrated in Figure 11.5. To know the direction of the\nfacenormal(i.e.,thesenseoftheedgecrossproduct),weneedtodefinewhich\nside of the triangle should be considered the front (i.e., the outside surface of\nan object) and which should be the back (i.e., its inside surface). This can be\ndefinedeasilybyspecifyinga windingorder—clockwise(CW)orcounterclock-\nwise (CCW).\nMost low-level graphics APIs give us a way to cullback-facing triangles\nbased on winding order. For example, if we set the cull mode parameter in\np1p2p3\ne12N\ne13 e23\nFigure 11.5. Deriving the edges and plane of a triangle from its vertices.\n628 11. The Rendering Engine\nDirect3D (D3DRS_CULL) to D3DCULLMODE_CW, then any triangle whose ver-\nticeswindinaclockwisefashioninscreenspacewillbetreatedasaback-facing\ntriangle and will not be drawn.\nBack-face culling is important because we generally don’t want to waste\ntime drawing triangles that aren’t going to be visible anyway. Also, rendering\nthe back faces of transparent objects can actually cause visual anomalies. The\nchoice of winding order is an arbitrary one, but of course it must be consistent\nacross all assets in the entire game. Inconsistent winding order is a common\nerror among junior 3D modelers.\nTriangle Lists\nTheeasiestwaytodefineameshissimplytolisttheverticesingroupsofthree,\neach triple corresponding to a single triangle. This data structure is known as\natriangle list; it is illustrated in Figure 11.6.\nV0\nV1V2V3\nV4\nV5V6V7\n...V5V7V6 V0V5V1 V1V2V3 V0V1V3\nFigure 11.6. A triangle list.\nIndexed Triangle Lists\nYouprobablynoticedthatmanyoftheverticesinthetrianglelistshowninFig-\nure 11.6 were duplicated, often multiple times. As we’ll see in Section 11.1.2.1,\nwe often store quite a lot of metadata with each vertex, so repeating this data\nin a triangle list wastes memory. It also wastes GPU bandwidth, because a\nduplicated vertex will be transformed and lit multiple times.\nForthesereasons,mostrenderingenginesmakeuseofamoreefficientdata\nstructure known as an indexed triangle list. The basic idea is to list the vertices\nonce with no duplication and then to use lightweight vertex indices(usually\noccupying only 16 bits each) to define the triples of vertices that constitute the\n11.1. Foundations of Depth-Buffered Triangle Rasterization 629\nV0\nV1V2V3\nV4\nV5V6V7\nIndices 013 123 051 ... 576Vertices V0V1V2V3V4V5V6V7\nFigure 11.7. An indexed triangle list.\ntriangles. The vertices are stored in an array known as a vertexbuffer (DirectX)\norvertexarray (OpenGL). The indices are stored in a separate buffer known as\nanindex buffer orindexarray. This technique is shown in Figure 11.7.\nStrips and Fans\nSpecialized mesh data structures known as triangle strips andtriangle fans are\nsometimes used for game rendering. Both of these data structures eliminate\nthe need for an index buffer, while still reducing vertex duplication to some\ndegree. They accomplish this by predefining the order in which vertices must\nappear and how they are combined to form triangles.\nIn a strip, the first three vertices define the first triangle. Each subsequent\nvertex forms an entirely new triangle, along with its previous two neigh-\nbors. To keep the winding order of a triangle strip consistent, the previous\ntwo neighbor vertices swap places after each new triangle. A triangle strip is\nshown in Figure 11.8.\nInafan,thefirstthreeverticesdefinethefirsttriangleandeachsubsequent\nvertex defines a new triangle with the previous vertex and the first vertex in\nthe fan. This is illustrated in Figure 11.9.\nVertex Cache Optimization\nWhen a GPU processes an indexed triangle list, each triangle can refer to any\nvertex within the vertex buffer. The vertices must be processed in the order\ntheyappearwithinthetriangles,becausetheintegrityofeachtrianglemustbe\nmaintained for the rasterization stage. As vertices are processed by the vertex\nshader, they are cached for reuse. If a subsequent primitive refers to a vertex\n630 11. The Rendering Engine\nInterpreted\nas triangles:0 1 2 1322 3 4354V0V1V2V3V4V5 VerticesV0\nV1V2\nV3V4\nV5\nFigure 11.8. A triangle strip.\n0 1 2 0 2 30 3 4V0V1V2V3V4V0V4V3 V2\nV1 Figure 11.9. A triangle fan.\nthat already resides in the cache, its processed attributes are used instead of\nreprocessing the vertex.\nStrips and fans are used in part because they can potentially save mem-\nory (no index buffer required) and in part because they tend to improve the\ncache coherency of the memory accesses made by the GPU to video RAM.\nEven better, we can use an indexed strip orindexed fan to virtually eliminate\nvertex duplication (which can often save more memory than eliminating the\nindex buffer), while still reaping the cache coherency benefits of the strip or\nfan vertex ordering.\nIndexed triangle lists can also be cache-optimized without restricting our-\nselves to strip or fan vertex ordering. A vertex cache optimizer is an offline ge-\nometry processing tool that attempts to list the triangles in an order that op-\ntimizes vertex reuse within the cache. It generally takes into account factors\nsuch as the size of the vertex cache(s) present on a particular type of GPU and\nthe algorithms used by the GPU to decide when to cache vertices and when\nto discard them. For example, the vertex cache optimizer included in Sony’s\nEdgegeometryprocessinglibrarycanachieverenderingthroughputthatisup\nto 4% better than what is possible with triangle stripping.\n11.1.1.4 Model Space\nThe position vectors of a triangle mesh’s vertices are usually specified relative\nto a convenient local coordinate system called model space ,local space, or object\nspace. The originofmodelspaceisusuallyeitherinthecenteroftheobjectorat\n11.1. Foundations of Depth-Buffered Triangle Rasterization 631\nL= iF= kU= j\nFigure 11.10. One possible mapping of the model-space axes.\nsomeotherconvenientlocation,likeonthefloorbetweenthefeetofacharacter\nor on the ground at the horizontal centroid of the wheels of a vehicle.\nAs we learned in Section 5.3.9.1, the sense of the model-space axes is arbi-\ntrary, but the axes typically align with the natural “front,” “left,” “right” and\n“up” directions on the model. For a little mathematical rigor, we can define\nthree unit vectors F,L(orR) and Uand map them as desired onto the unit\nbasis vectors i,jandk(and hence to the x-,y- and z-axes, respectively) in\nmodel space. For example, a common mapping is L=i,U=jandF=k.\nThe mapping is completely arbitrary, but it’s important to be consistent for all\nmodels across the entire engine. Figure 11.10 shows one possible mapping of\nthe model-space axes for an aircraft model.\n11.1.1.5 World Space and Mesh Instancing\nMany individual meshes are composed into a complete scene by positioning\nandorientingthemwithinacommoncoordinatesystemknownas worldspace.\nAnyonemeshmightappearmanytimesinascene—examplesincludeastreet\nlinedwithidenticallampposts,afacelessmobofsoldiersoraswarmofspiders\nattacking the player. We call each such object a meshinstance.\nA mesh instance contains a reference to its shared mesh data and also in-\ncludes a transformation matrix that converts the mesh’s vertices from model\nspacetoworldspace,withinthecontextofthatparticularinstance. Thismatrix\nis called the model-to-world matrix, or sometimes just the world matrix. Using\nthe notation from Section 5.3.10.2, this matrix can be written as follows:\nMM!W=[(RS)M!W 0\ntM 1]\n,\n632 11. The Rendering Engine\nwhere the upper 33matrix (RS)M!Wrotates and scales model-space ver-\ntices into world space, and tMis the translation of the model-space axes ex-\npressed in world space. If we have the unit model-space basis vectors iM,jM\nandkM, expressed in world-space coordinates, this matrix can also be written\nas follows:\nMM!W=2\n664iM 0\njM 0\nkM 0\ntM 13\n775.\nGivenavertexexpressedinmodel-spacecoordinates,therenderingengine\ncalculates its world-space equivalent as follows:\nvW=vMMM!W.\nWe can think of the matrix MM!Was a description of the position and orien-\ntation of the model-space axes themselves, expressed in world-space coordi-\nnates. Or we can think of it as a matrix that transforms vertices from model\nspace to world space.\nWhen rendering a mesh, the model-to-world matrix is also applied to the\nsurface normals of the mesh (see Section 11.1.2.1). Recall from Section 5.3.11,\nthat in order to transform normal vectors properly, we must multiply them\nby the inverse transpose of the model-to-world matrix. If our matrix does\nnot contain any scale or shear, we can transform our normal vectors correctly\nby simply setting their wcomponents to zero prior to multiplication by the\nmodel-to-world matrix, as described in Section 5.3.6.1.\nSome meshes like buildings, terrain and other background elements are\nentirely static and unique. The vertices of these meshes are often expressed in\nworldspace,sotheirmodel-to-worldmatricesareidentityandcanbeignored.\n11.1.2 Describing the Visual Properties of a Surface\nIn order to properly render and light a surface, we need a description of its\nvisualproperties . Surfacepropertiesincludegeometricinformation,suchasthe\ndirection of the surface normal at various points on the surface. They also\nencompass a description of how light should interact with the surface. This\nincludes diffuse color, shininess/reflectivity, roughness or texture, degree of\nopacity or transparency, index of refraction and other optical properties. Sur-\nface properties might also include a specification of how the surface should\nchange over time (e.g., how an animated character’s skin should track the\njoints of its skeleton or how the surface of a body of water should move).\n11.1. Foundations of Depth-Buffered Triangle Rasterization 633\nThe key to rendering photorealistic images is properly accounting for\nlight’s behavior as it interacts with the objects in the scene. Hence render-\ning engineers need to have a good understanding of how light works, how it\nis transported through an environment and how the virtual camera “senses”\nit and translates it into the colors stored in the pixels on-screen.\n11.1.2.1 Introduction to Light and Color\nLight is electromagnetic radiation; it acts like both a wave and a particle in\ndifferent situations. The color of light is determined by its intensity Iand its\nwavelength l(or its frequency f, where f=1/l). The visible gamut ranges\nfrom a wavelength of 740 nm (or a frequency of 430 THz) to a wavelength\nof 380 nm (750 THz). A beam of light may contain a single pure wavelength\n(i.e., the colors of the rainbow, also known as the spectral colors ), or it may\ncontainamixtureofvariouswavelengths. Wecandrawagraphshowinghow\nmuch of each frequency a given beam of light contains, called a spectral plot.\nWhite light contains a little bit of all wavelengths, so its spectral plot would\nlook roughly like a box extending across the entire visible band. Pure green\nlightcontainsonlyonewavelength,soitsspectralplotwouldlooklikeasingle\ninfinitesimally narrow spike at about 570 THz.\nLight-Object Interactions\nLight can have many complex interactions with matter. Its behavior is gov-\nerned in part by the medium through which it is traveling and in part by the\nshape and properties of the interfaces between different types of media (air-\nsolid, air-water, water-glass, etc.). Technically speaking, a surface is really just\nan interface between two different types of media.\nDespite all of its complexity, light can really only do four things:\n• It can be absorbed.\n• It can be reflected.\n• It can be transmitted through an object, usually being refracted (bent) in\nthe process.\n• It can be diffracted when passing through very narrow openings.\nMost photorealistic rendering engines account for the first three of these be-\nhaviors; diffraction is not usually taken into account because its effects are\nrarely noticeable in most scenes.\nOnly certain wavelengths may be absorbed by a surface, while others are\nreflected. This is what gives rise to our perception of the color of an object.\nFor example, when white light falls on a red object, all wavelengths except\n634 11. The Rendering Engine\nred are absorbed, hence the object appears red. The same perceptual effect is\nachieved when red light is cast onto a white object—our eyes don’t know the\ndifference.\nReflectionscanbe diffuse,meaningthatanincomingrayisscatteredequally\ninalldir ections. Reflectionscanalsobe specular ,meaningthatanincidentlight\nray will reflect directly or be spread only into a narrow cone. Reflections can\nalso beanisotropic , meaning that the way in which light reflects from a surface\nchanges depending on the angle at which the surface is viewed.\nWhenlightistransmittedthroughavolume,itcanbe scattered (asisthecase\nfor translucent objects), partially absorbed (as with colored glass), or refracted\n(as happens when light travels through a prism). The refraction angles can be\ndifferent for different wavelengths, leading to spectral spreading. This is why\nweseerainbowswhenlightpassesthroughraindropsandglassprisms. Light\ncan also enter a semi-solid surface, bounce around and then exit the surface\nat a different point from the one at which it entered the surface. We call this\nsubsurfacescattering, anditisoneoftheeffectsthatgivesskin, waxandmarble\ntheir characteristic warm appearance.\nColor Spaces and Color Models\nAcolor model is a three-dimensional coordinate system that measures colors.\nAcolor space is a specific standard for how numerical colors in a particular\ncolor model should be mapped onto the colors perceived by human beings in\nthe real world. Color models are typically three-dimensional because of the\nthreetypesofcolorsensors(cones)inoureyes,whicharesensitivetodifferent\nwavelengths of light.\nThe most commonly used color model in computer graphics is the RGB\nmodel. In this model, color space is represented by a unit cube, with the rela-\ntive intensities of red, green and blue light measured along its axes. The red,\ngreen and blue components are called color channels . In the canonical RGB\ncolor model, each channel ranges from zero to one. So the color (0, 0, 0 )rep-\nresents black, while (1, 1, 1 )represents white.\nWhen colors are stored in a bitmapped image, various color formats can\nbe employed. A color format is defined in part by the number of bits per pixel\nit occupies and, more specifically, the number of bits used to represent each\ncolor channel. The RGB888 format uses eight bits per channel, for a total of\n24 bits per pixel. In this format, each channel ranges from 0 to 255 rather than\nfrom zero to one. RGB565 uses five bits for red and blue and six for green, for\na total of 16 bits per pixel. A paletted format might use eight bits per pixel\nto store indices into a 256-element color palette, each entry of which might be\nstored in RGB888 or some other suitable format.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 635\nA number of other color models are also used in 3D rendering. We’ll see\nhow the log-LUV color model is used for highdynamicrange (HDR) lighting in\nSection 11.3.1.5.\nOpacity and the Alpha Channel\nAfourth channel called alphais oftentacked on toRGB color vectors. Asmen-\ntioned in Section 11.1.1, alpha measures the opacity of an object. When stored\nin an image pixel, alpha represents the opacity of the pixel.\nRGB color formats can be extended to include an alpha channel, in which\ncase they are referred to as RGBA or ARGB color formats. For example,\nRGBA8888 is a 32 bit-per-pixel format with eight bits each for red, green, blue\nand alpha. RGBA5551 is a 16 bit-per-pixel format with one-bit alpha; in this\nformat, colors can either be fully opaque or fully transparent.\n11.1.2.2 Vertex Attributes\nThe simplest way to describe the visual properties of a surface is to specify\nthem at discrete points on the surface. The vertices of a mesh are a convenient\nplace to store surface properties, in which case they are called vertexattributes .\nA typical triangle mesh includes some or all of the following attributes at\neach vertex. As rendering engineers, we are of course free to define any ad-\nditional attributes that may be required in order to achieve a desired visual\neffect on-screen.\n•Position vector (pi=[pixpiypiz]). This is the 3D position of the ith\nvertex in the mesh. It is usually specified in a coordinate space local to\nthe object, known as model space.\n•Vertex normal (ni=[nixniyniz]). This vector defines the unit sur-\nface normal at the position of vertex i. It is used in per-vertex dynamic\nlighting calculations.\n•Vertex tangent (ti=[tixtiytiz])andbitangent (bi=[bixbiybiz]).\nThese two unit vectors lie perpendicular to one another and to the ver-\ntex normal ni. Together, the three vectors ni,tiandbidefine a set of\ncoordinate axes known as tangent space. This space is used for various\nper-pixel lighting calculations, such as normal mapping and environ-\nment mapping. (The bitangent biis sometimes confusingly called the\nbinormal , even though it is notnormal to the surface.)\n•Diffuse color (di=[diRdiGdiBdiA]). This four-element vector de-\nscribesthediffusecolorofthesurface, expressedintheRGBcolorspace.\nIt typically also includes a specification of the opacity or alpha(A) of the\n636 11. The Rendering Engine\nsurface at the position of the vertex. This color may be calculated offline\n(static lighting) or at runtime (dynamic lighting).\n•Specular color (si=[\nsiRsiGsiBsiA]). This quantity describes the\ncolor of the specular highlight that should appear when light reflects di-\nrectly from a shiny surface onto the virtual camera’s imaging plane.\n•Texture coordinates (uij=[uijvij]). Texture coordinates allow a two-\n(or sometimes three-) dimensional bitmap to be “shrink wrapped” onto\nthe surface of a mesh—a process known as texture mapping. A texture\ncoordinate (u,v)describes the location of a particular vertex within the\ntwo-dimensional normalized coordinate space of the texture. A triangle\ncan be mapped with more than one texture; hence it can have more than\none set of texture coordinates. We’ve denoted the distinct sets of texture\ncoordinates via the subscript jabove.\n•Skinning weights (kij=[kijwij]). In skeletal animation, the vertices of\nameshareattachedtoindividualjointsinanarticulatedskeleton. Inthis\ncase, each vertex must specify to which joint it is attached via an index,\nk. A vertex can be influenced by multiple joints, in which case the final\nvertex position becomes a weightedaverage of these influences. Thus, the\nweight of each joint’s influence is denoted by a weighting factor w. In\ngeneral, a vertex ican have multiple joint influences j, each denoted by\nthe pair of numbers (kij,wij).\n11.1.2.3 Vertex Formats\nVertex attributes are typically stored within a data structure such as a C\nstruct or a C++ class. The layout of such a data structure is known as a\nvertex format . Different meshes require different combinations of attributes\nand hence need different vertex formats. The following are some examples\nof common vertex formats:\n// Simplest possible vertex -- position only (useful for\n// shadow volume extrusion, silhouette edge detection\n// for cartoon rendering, z-prepass, etc.)\nstruct Vertex1P\n{\nVector3 m_p; // position\n};\n// A typical vertex format with position, vertex normal\n// and one set of texture coordinates.\nstruct Vertex1P1N1UV\n{\nVector3 m_p; // position\n11.1. Foundations of Depth-Buffered Triangle Rasterization 637\nVector3 m_n; // vertex normal\nF32 m_uv[2]; // (u, v) texture coordinate\n};\n// A skinned vertex with position, diffuse and specular\n// colors and four weighted joint influences.\nstruct Vertex1P1D1S2UV4J\n{\nVector3 m_p; // position\nColor4 m_d; // diffuse color and translucency\nColor4 m_S; // specular color\nF32 m_uv0[2]; // first set of tex coords\nF32 m_uv1[2]; // second set of tex coords\nU8 m_k[4]; // four joint indices, and...\nF32 m_w[3]; // three joint weights, for\n// skinning (fourth is calc'd\n// from the first three)\n};\nClearly the number of possible permutations of vertex attributes—and\nhence the number of distinct vertex formats—can grow to be extremely large.\n(In fact the number of formats is theoretically unbounded, if one were to per-\nmit any number of texture coordinates and/or joint weights.) Management\nof all these vertex formats is a common source of headaches for any graphics\nprogrammer.\nSome steps can be taken to reduce the number of vertex formats that an\nengine has to support. In practical graphics applications, many of the theoret-\nicallypossiblevertexformatsaresimplynotuseful,ortheycannotbehandled\nby the graphics hardware or the game’s shaders. Some game teams also limit\nthemselves to a subset of the useful/feasible vertex formats in order to keep\nthings more manageable. For example, they might only allow zero, two or\nfour joint weights per vertex, or they might decide to support no more than\ntwo sets of texture coordinates per vertex. Some GPUs are capable of extract-\ning a subset of attributes from a vertex data structure, so game teams can also\nchoose to use a single “überformat” for all meshes and let the hardware select\nthe relevant attributes based on the requirements of the shader.\n11.1.2.4 Attribute Interpolation\nThe attributes at a triangle’s vertices are just a coarse, discretized approxima-\ntion to the visual properties of the surface as a whole. When rendering a tri-\nangle, whatreallymattersarethevisualpropertiesattheinteriorpointsofthe\ntriangle as “seen” through each pixel on-screen. In other words, we need to\nknow the values of the attributes on a per-pixel basis, not a per-vertex basis.\n638 11. The Rendering Engine\nFigure 11.11. A Gouraud-shaded triangle with different shades of gray at the vertices.\nFigure 11.12. Gouraud shading can make faceted objects appear to be smooth.\nOne simple way to determine the per-pixel values of a mesh’s surface at-\ntributes is to linearly interpolate the per-vertex attribute data. When applied to\nvertex colors, attribute interpolation is known as Gouraud shading. An exam-\nple of Gouraud shading applied to a triangle is shown in Figure 11.11, and its\neffects on a simple triangle mesh are illustrated in Figure 11.12. Interpolation\nisroutinelyappliedtootherkindsofvertexattributeinformationaswell,such\nas vertex normals, texture coordinates and depth.\nVertex Normals and Smoothing\nAs we’ll see in Section 11.1.3, lighting is the process of calculating the color of\nan object at various points on its surface, based on the visual properties of the\nsurface and the properties of the light impinging upon it. The simplest way to\nlightameshistocalculatethecolorofthesurfaceona per-vertex basis. Inother\nwords, weusethepropertiesofthesurfaceandtheincominglighttocalculate\nthe diffuse color of each vertex ( di). These vertex colors are then interpolated\nacross the triangles of the mesh via Gouraud shading.\nIn order to determine how a ray of light will reflect from a point on a sur-\nface, most lighting models make use of a vector that is normalto the surface\nat the point of the light ray’s impact. Since we’re performing lighting calcula-\ntions on a per-vertex basis, we can use the vertex normal nifor this purpose.\nTherefore, the directions of a mesh’s vertex normals can have a significant im-\npact on the final appearance of a mesh.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 639\nFigure 11.13. The directions of a mesh’s vertex normals can have a profound effect on the colors\ncalculated during per-vertex lighting calculations.\nAs an example, consider a tall, thin, four-sided box. If we want the box\nto appear to be sharp-edged, we can specify the vertex normals to be perpen-\ndicular to the faces of the box. As we light each triangle, we will encounter\nthe same normal vector at all three vertices, so the resulting lighting will ap-\npearflat,anditwillabruptlychangeatthecornersoftheboxjustasthevertex\nnormals do.\nWe can also make the same box mesh look a bit like a smooth cylinder by\nspecifying vertex normals that point radially outward from the box’s center\nline. In this case, the vertices of each triangle will have different vertex nor-\nmals, causing us to calculate different colors at each vertex. Gouraud shading\nwillsmoothlyinterpolatethesevertexcolors,resultinginlightingthatappears\nto vary smoothly across the surface. This effect is illustrated in Figure 11.13.\n11.1.2.5 Textures\nWhen triangles are relatively large, specifying surface properties on a per-\nvertex basis can be too coarse-grained. Linear attribute interpolation isn’t al-\nways what we want, and it can lead to undesirable visual anomalies.\nAs an example, consider the problem of rendering the bright specularhigh-\nlightthat can occur when light shines on a glossy object. If the mesh is highly\ntessellated,per-vertexlightingcombinedwithGouraudshadingcanyieldrea-\nsonably good results. However, when the triangles are too large, the errors\nthat arise from linearly interpolating the specular highlight can become jar-\nringly obvious, as shown in Figure 11.14.\n640 11. The Rendering Engine\nFigure 11.14. Linear interpolation of vertex attributes does not always yield an adequate description\nof the visual properties of a surface, especially when tessellation is low.\nTo overcome the limitations of per-vertex surface attributes, rendering en-\ngineersusebitmappedimagesknownas texturemaps. Atextureoftencontains\ncolor information and is usually projected onto the triangles of a mesh. In this\ncase,itactsabitlikethosesillyfaketattoosweusedtoapplytoourarmswhen\nwewerekids. Butatexturecancontainotherkindsofvisualsurfaceproperties\nas well as colors. And a texture needn’t be projected onto a mesh—for exam-\nple,atexturemightbeusedasastand-alonedatatable. Theindividualpicture\nelements of a texture are called texelsto differentiate them from the pixels on\nthe screen.\nThe dimensions of a texture bitmap are constrained to be powers of two\non some graphics hardware. Typical texture dimensions include 256256,\n512512,10241024and 20482048, although textures can be any size on\nmost hardware, provided the texture fits into video memory. Some graph-\nics hardware imposes additional restrictions, such as requiring textures to be\nsquare, or lifts some restrictions, such as not constraining texture dimensions\nto be powers of two.\nTypes of Textures\nThe most common type of texture is known as a diffuse map, or albedo map . It\ndescribes the diffuse surface color at each texel on a surface and acts like a\ndecal or paint job on the surface.\nOther types of textures are used in computer graphics as well, including\nnormal maps (which store unit normal vectors at each texel, encoded as RGB\nvalues),glossmaps (whichencodehowshinyasurfaceshouldbeateachtexel),\nenvironment maps (which contain a picture of the surrounding environment\nfor rendering reflections) and many others. See Section 11.3.1 for a discussion\nof how various types of textures can be used for image-based lighting and\nother effects.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 641\nWe can actually use texture maps to store any information that we happen\nto need in our lighting calculations. For example, a one-dimensional texture\ncould be used to store sampled values of a complex math function, a color-to-\ncolor mapping table, or any other kind of look-up table (LUT).\nTexture Coordinates\nLet’s consider how to project a two-dimensional texture onto a mesh. To do\nthis, we define a two-dimensional coordinate system known as texture space .\nA texture coordinate is usually represented by a normalized pair of numbers\ndenoted (u,v). These coordinates always range from (0, 0)at the bottom left\ncorner of the texture to (1, 1)at the top right. Using normalized coordinates\nlikethisallowsthesamecoordinatesystemtobeusedregardlessofthedimen-\nsions of the texture.\nTo map a triangle onto a 2D texture, we simply specify a pair of texture\ncoordinates (ui,vi)at each vertex i. This effectively maps the triangle onto the\nimage plane in texture space. An example of texture mapping is depicted in\nFigure 11.15.\nFigure 11.15. An example of texture mapping. The triangles are shown both in three-dimensional\nspace and in texture space.\nTexture Addressing Modes\nTexture coordinates are permitted to extend beyond the [0, 1]range. The\ngraphics hardware can handle out-of-range texture coordinates in any one of\nthe following ways. These are known as textureaddressingmodes; which mode\nis used is under the control of the user.\n•Wrap. In this mode, the texture is repeated over and over in every di-\nrection. All texture coordinates of the form (ju,kv)are equivalent to the\ncoordinate (u,v), where jandkare arbitrary integers.\n642 11. The Rendering Engine\nFigure 11.16. Texture addressing modes.\n•Mirror. This mode acts like wrap mode, except that the texture is mir-\nroredaboutthe v-axisforoddintegermultiplesof u,andaboutthe u-axis\nfor odd integer multiples of v.\n•Clamp. Inthismode, thecolorsofthetexelsaroundtheouteredgeofthe\ntexture are simply extended when texture coordinates fall outside the\nnormal range.\n•Border color . In this mode, an arbitrary user-specified color is used for\nthe region outside the [0, 1]texture coordinate range.\nThese texture addressing modes are depicted in Figure 11.16.\nTexture Formats\nTexturebitmapscanbestoredondiskinvirtuallyanyimageformat,provided\nyour game engine includes the code necessary to read it into memory. Com-\nmonformatsincludeTarga(.tga),PortableNetworkGraphics(.png),Windows\nBitmap (.bmp) and Tagged Image File Format (.tif). In memory, textures are\nusually represented as two-dimensional (strided) arrays of pixels using vari-\nous color formats, including RGB888, RGBA8888, RGB565, RGBA5551 and so\non.\nMost modern graphics cards and graphics APIs support compressed tex-\ntures. DirectX supports a family of compressed formats known as DXT or\nS3 Texture Compression (S3TC). We won’t cover the details here, but the ba-\nsic idea is to break the texture into 44blocks of pixels and use a small\n11.1. Foundations of Depth-Buffered Triangle Rasterization 643\ncolor palette to store the colors for each block. You can read more about\nS3 compressed texture formats at http://en.wikipedia.org/wiki/S3_Texture_\nCompression.\nCompressed textures have the obvious benefit of using less memory than\ntheir uncompressed counterparts. An additional unexpected plus is that they\nare faster to render with as well. S3 compressed textures achieve this speed-\nup because of more cache-friendly memory access patterns—4 4blocks of\nadjacentpixelsarestoredinasingle64-or128-bitmachineword—andbecause\nmoreofthetexturecanfitintothecacheatonce. Compressedtexturesdosuffer\nfrom compression artifacts. While the anomalies are usually not noticeable,\nthere are situations in which uncompressed textures must be used.\nTexel Density and Mipmapping\nImagine rendering a full-screen quad (a rectangle composed of two triangles)\nthat has been mapped with a texture whose resolution exactly matches that\nof the screen. In this case, each texel maps exactly to a single pixel on-screen,\nand we say that the texel density (ratio of texels to pixels) is one. When this\nsame quad is viewed at a distance, its on-screen area becomes smaller. The\nresolution of the texture hasn’t changed, so the quad’s texel density is now\ngreater than one (meaning that more than one texel is contributing to each\npixel).\nClearly texel density is not a fixed quantity—it changes as a texture-map-\nped object moves relative to the camera. Texel density affects the memory\nconsumption and the visual quality of a three-dimensional scene. When the\ntexel density is much less than one, the texels become significantly larger than\na pixel on-screen, and you can start to see the edges of the texels. This de-\nstroys the illusion. When texel density is much greater than one, many texels\ncontributetoasinglepixelon-screen. Thiscancausea moirébandingpattern,as\nshowninFigur e11.17. Worse,apixel’scolorcanappeartoswimandflickeras\ndifferent texels within the boundaries of the pixel dominate its color depend-\ning on subtle changes in camera angle or position. Rendering a distant object\nwith a very high texel density can also be a waste of memory if the player can\nneverget close to it. After all, why keep such a high-res texture in memory if\nno one will ever see all that detail?\nIdeally we’d like to maintain a texel density that is close to one at all times,\nfor both nearby and distant objects. This is impossible to achieve exactly, but\nit can be approximated via a technique called mipmapping. For each texture,\nwe create a sequence of lower-resolution bitmaps, each of which is one-half\nthe width and one-half the height of its predecessor. We call each of these\nimages a mipmap, or mip level. For example, a 6464texture would have the\n644 11. The Rendering Engine\nFigure 11.17. A texel density greater than one can lead to a moiré pattern.\nfollowing mip levels: 6464,3232,1616,88,44,22and 11, as\nshown in Figure 11.18. Once we have mipmapped our textures, the graphics\nhardwareselectstheappropriatemiplevelbasedonatriangle’sdistanceaway\nfrom the camera, in an attempt to maintain a texel density that is close to one.\nForexample, ifatexturetakesupanareaof 4040on-screen, the 6464mip\nlevel might be selected; if that same texture takes up only a 1010area, the\n1616mip level might be used. As we’ll see below, trilinear filtering allows\nthe hardware to sample two adjacent mip levels and blend the results. In this\ncase, a 1010area might be mapped by blending the 1616and 88mip\nlevels together.\nFigure 11.18. Mip levels for a 6464 texture.\nWorld-Space Texel Density\nThe term “texel density” can also be used to describe the ratio of texels to\nworld-spaceareaonatexturedsurface. Forexample,a2mcubemappedwith\na256256texture would have a texel density of 2562/22=16,384. I will\ncall thisworld-space texel density to differentiate it from the screen-space texel\ndensity we’ve been discussing thus far.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 645\nWorld-space texel density need not be close to one, and in fact the specific\nvalue will usually be much greater than one and depends entirely upon your\nchoice of world units. Nonetheless, it is important for objects to be texture\nmapped with a reasonably consistent world-space texel density. For example,\nwe would expect all six sides of a cube to occupy the same texture area. If\nthis were not the case, the texture on one side of the cube would have a lower-\nresolutionappearancethananotherside,whichcanbenoticeabletotheplayer.\nManygamestudiosprovidetheirartteamswithguidelinesandin-enginetexel\ndensity visualization tools in an effort to ensure that all objects in the game\nhave a reasonably consistent world-space texel density.\nTexture Filtering\nWhen rendering a pixel of a textured triangle, the graphics hardware samples\nthe texture map by considering where the pixel center falls in texture space.\nThere is usually not a clean one-to-one mapping between texels and pixels,\nand pixel centers can fall at any place in texture space, including directly on\nthe boundary between two or more texels. Therefore, the graphics hardware\nusually has to sample more than one texel and blend the resulting colors to\narrive at the actual sampled texel color. We call this texturefiltering .\nMost graphics cards support the following kinds of texture filtering:\n•Nearestneighbor. In this crude approach, the texel whose center is closest\nto the pixel center is selected. When mipmapping is enabled, the mip\nlevel is selected whose resolution is nearest to but greater than the ideal\ntheoretical resolution needed to achieve a screen-space texel density of\none.\n•Bilinear. In this approach, the four texels surrounding the pixel center\naresampled,andtheresultingcolorisaweightedaverageoftheircolors\n(where the weights are based on the distances of the texel centers from\nthe pixel center). When mipmapping is enabled, the nearest mip level is\nselected.\n•Trilinear. In this approach, bilinear filtering is used on each of the two\nnearestmiplevels(onehigher-resthantheidealandtheotherlower-res),\nand these results are then linearly interpolated. This eliminates abrupt\nvisual boundaries between mip levels on-screen.\n•Anisotropic . Both bilinear and trilinear filtering sample 22square\nblocks of texels. This is the right thing to do when the textured sur-\nface is being viewed head-on, but it’s incorrect when the surface is at\nan oblique angle relative to the virtual screen plane. Anisotropic filter-\ningsamplestexelswithinatrapezoidalregioncorrespondingtotheview\n646 11. The Rendering Engine\nangle, thereby increasing the quality of textured surfaces when viewed\nat an angle.\n11.1.2.6 Materials\nAmaterial is a complete description of the visual properties of a mesh. This\nincludes a specification of the textures that are mapped to its surface and also\nvarious higher-level properties, such as which shader programs to use when\nrendering the mesh, the input parameters to those shaders and other parame-\nters that control the functionality of the graphics acceleration hardware itself.\nWhile technically part of the surface properties description, vertex attri-\nbutes are not considered to be part of the material. However, they come along\nfor the ride with the mesh, so a mesh-material pair contains all the informa-\ntion we need to render the object. Mesh-material pairs are sometimes called\nrender packets , and the term “geometric primitive” is sometimes extended to\nencompass mesh-material pairs as well.\nA 3D model typically uses more than one material. For example, a model\nof a human would have separate materials for the hair, skin, eyes, teeth and\nvarious kinds of clothing. For this reason, a mesh is usually divided into sub-\nmeshes, each mapped to a single material. The OGRE rendering engine imple-\nments this design via its Ogre::SubMesh class.\nFigure 11.19. A variation on the classic “Cornell box” scene illustrating how realistic lighting can\nmake even the simplest scene appear photorealistic.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 647\nFigure 11.20. A scene from The Last of Us: Remastered (© 2014/™ SIE. Created and developed by\nNaughty Dog, PlayStation 4) rendered without textures. (See Color Plate XV.)\n11.1.3 Lighting Basics\nLighting is at the heart of all CG rendering. Without good lighting, an other-\nwisebeautifully modeledscene willlook flatand artificial. Likewise, even the\nsimplest of scenes can be made to look extremely realistic when it is lit accu-\nrately. The classic “Cornell box” scene, shown in Figure 11.19, is an excellent\nexample of this.\nThesequenceofscreenshotsfromNaughtyDog’s TheLastofUs: Remastered\nis another good illustration of the importance of lighting. In Figure 11.20, the\nscene is rendered without textures. Figure 11.21 shows the same scene with\ndiffuse textures applied. The fully lit scene is shown in Figure 11.22. Notice\nthe marked jump in realism when lighting is applied to the scene.\nThe term shading is often used as a loose generalization of lighting plus\nothervisualeffects. Assuch, “shading”encompassesproceduraldeformation\nof vertices to simulate the motion of a water surface, generation of hair curves\nor fur shells, tessellation of high-order surfaces, and pretty much any other\ncalculation that’s required to render a scene.\nIn the following sections, we’ll lay the foundations of lighting that we’ll\nneed in order to understand graphics hardware and the rendering pipeline.\nWe’ll return to the topic of lighting in Section 11.3, where we’ll survey some\nadvanced lighting and shading techniques.\n11.1.3.1 Local and Global Illumination Models\nRendering engines use various mathematical models of light-surface and\nlight-volume interactions called light transport models. The simplest models\nonly account for direct lighting in which light is emitted, bounces off a single\n648 11. The Rendering Engine\nFigure 11.21. The same scene from The Last of Us: Remastered (© 2014/™ SIE. Created and developed\nby Naughty Dog, PlayStation 4) with only diffuse textures applied. (See Color Plate XVI.)\nFigure 11.22. Scene from The Last of Us: Remastered (© 2014/™ SIE. Created and developed by\nNaughty Dog, PlayStation 4) with full lighting. (See Color Plate XVII.)\nobject in the scene, and then proceeds directly to the imaging plane of the vir-\ntual camera. Such simple models are called local illumination models, because\nonlythelocaleffectsoflightonasingleobjectareconsidered;objectsdonotaf-\nfectoneanother’sappearanceinalocallightingmodel. Notsurprisingly,local\nmodelswerethefirsttobeusedingames,andtheyarestillinusetoday—local\nlighting can produce surprisingly realistic results in some circumstances.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 649\nTrue photorealism can only be achieved by accounting for indirect light-\ning, where light bounces multiple times off many surfaces before reaching the\nvirtual camera. Lighting models that account for indirect lighting are called\nglobalilluminationmodels. Someglobalilluminationmodelsaretargetedatsim-\nulating one specific visual phenomenon, such as producing realistic shadows,\nmodeling reflective surfaces, accounting for interreflection between objects\n(where the color of one object affects the colors of surrounding objects), and\nmodeling caustic effects (the intense reflections from water or a shiny metal\nsurface). Other global illumination models attempt to provide a holistic ac-\ncount of a wide range of optical phenomena. Ray tracing and radiosity meth-\nods are examples of such technologies.\nGlobal illumination is described completely by a mathematical formula-\ntion known as the rendering equation orshading equation . It was introduced in\n1986 by J. T. Kajiya as part of a seminal SIGGRAPH paper. In a sense, every\nrendering technique can be thought of as a full or partial solution to the ren-\ndering equation, although they differ in their fundamental approach to solv-\ning it and in the assumptions, simplifications and approximations they make.\nSeehttp://en.wikipedia.org/wiki/Rendering_equation,[10],[2]andvirtually\nanyothertextonadvancedrenderingandlightingformoredetailsontheren-\ndering equation.\n11.1.3.2 The Phong Lighting Model\nThe most common local lighting model employed by game rendering engines\nis thePhongreflection model. It models the light reflected from a surface as a\nsum of three distinct terms:\n• Theambient term models the overall lighting level of the scene. It is a\ngross approximation of the amount of indirect bounced light present in\nthe scene. Indirect bounces are what cause regions in shadow not to\nappear totally black.\n• Thediffuseterm accounts for light that is reflected uniformly in all direc-\ntions from each direct light source. This is a good approximation to the\nway in which real light bounces off a matte surface, such as a block of\nwood or a piece of cloth.\n• Thespecular term models the bright highlights we sometimes see when\nviewing a glossy surface. Specular highlights occur when the viewing\nangleiscloselyalignedwithapathofdirectreflectionfromalightsource.\nFigure 11.23 shows how the ambient, diffuse and specular terms add to-\ngether to produce the final intensity and color of a surface.\n650 11. The Rendering Engine\nFigure 11.23. Ambient, diffuse and specular terms are summed to calculate Phong reﬂection.\nTo calculate Phong reflection at a specific point on a surface, we require\na number of input parameters. The Phong model is normally applied to all\nthree color channels (R, G and B) independently, so all of the color parameters\ninthefollowingdiscussionarethree-elementvectors. TheinputstothePhong\nmodel are:\n• theviewingdirectionvector V=[VxVyVz]\n,whichextendsfromthe\nreflection point to the virtual camera’s focal point (i.e., the negation of\nthe camera’s world-space “front” vector);\n• the ambient light intensity for the three color channels, A =[\nARAGAB]\n;\n• the surface normal N=[NxNyNz]\nat the point the light ray im-\npinges on the surface;\n• the surface reflectance properties, which are\n◦the ambient reflectivity kA=[kAR kAG kAB]\n,\n◦the diffuse reflectivity kD=[kDR kDG kDB]\n,\n◦the specular reflectivity kS=[kSRkSGkSB]\n,\n◦a specular “glossiness” exponent a;\n• and, for each light source i,\n◦the light’s color and intensity Ci=[\nCiRCiGCiB]\n,\n◦the direction vector Lifrom the reflection point to the light source.\nIn the Phong model, the intensity Iof light reflected from a point can be ex-\npressed with the following vector equation:\nI= (kA\nA) +å\ni[kD(NLi) +kS(RiV)a]\nCi,\nwhere the sum is taken over all lights iaffecting the point in question. Recall\nthattheoperator\nrepresentsthe component-wise multiplicationoftwovectors\n11.1. Foundations of Depth-Buffered Triangle Rasterization 651\nN\nT T\nFigure 11.24. Calculation of the reﬂected lighting vector R from the original lighting vector L and\nthe surface normal N .\n(the so-called Hadamard product). This expression can be broken into three\nscalar equations, one for each color channel, as follows:\nIR=kARAR+å\ni[kDR(NLi) +kSR(RiV)a]\nCiR,\nIG=kAGAG+å\ni[kDG(NLi) +kSG(RiV)a]\nCiG,\nIB=kABAB+å\ni[kDB(NLi) +kSB(RiV)a]\nCiB.\nIn these equations, the vector Ri=[RixRiyRiz]\nis thereflection of the light\nray’s direction vector Liabout the surface normal N.\nThe vector Rican be easily calculated via a bit of vector math (see Fig-\nure 11.24). Any vector can be expressed as a sum of its normal and tangential\ncomponents. For example, we can break up the light direction vector Las\nfollows:\nL=LN+LT.\nWe know that the dot product (NL)represents the projection of Lnormal to\nthe surface (a scalar quantity). So the normal component LNis just the unit\nnormal vector Nscaled by this dot product:\nLN= (NL)N.\nThe reflected vector Rhas the same normal component as Lbut theopposite\ntangential component (  LT). So we can find Ras follows:\nR=LN LT\n=LN (L LN)\n=2LN L;\nR=2(NL)N LT\n652 11. The Rendering Engine\nThisequationcanbeusedtofindallofthe Rivaluescorrespondingtothelight\ndirections Li.\nBlinn-Phong\nTheBlinn-Phong lightingmodelisavariationonPhongshadingthatcalculates\nspecular reflection in a slightly different way. We define the vector Hto be the\nvectorthatlieshalfwaybetweentheviewvector Vandthelightdirectionvec-\ntorL. The Blinn-Phong specular component is then (NH)a, as opposed to\nPhong’s (RV)a. The exponent ais slightly different than the Phong expo-\nnent a, but its value is chosen in order to closely match the equivalent Phong\nspecular term.\nThe Blinn-Phong model offers increased runtime efficiency at the cost of\nsome accuracy, although it actually matches empirical results more closely\nthan Phong for some kinds of surfaces. The Blinn-Phong model was used al-\nmost exclusively in early computer games and was hard-wired into the fixed-\nfunction pipelines of early GPUs. See http://en.wikipedia.org/wiki/Blinn%\nE2%80%93Phong_shading_model for more details.\nBRDF Plots\nThethreetermsinthePhonglightingmodelarespecialcasesofagenerallocal\nreflection model known as a bidirectionalreflectiondistributionfunction (BRDF).\nA BRDF calculates the ratio of the outgoing (reflected) radiance along a given\nviewing direction Vto the incoming irradiance along the incident ray L.\nA BRDF can be visualized as a hemispherical plot, where the radial dis-\ntance from the origin represents the intensity of the light that would be seen if\nthe reflection point were viewed from that direction. The diffusePhong reflec-\ntion term is kD(NL). This term only accounts for the incoming illumination\nrayL, not the viewing angle V. Hence the value of this term is the same for\nall viewing angles. If we were to plot this term as a function of the viewing\nangle in three dimensions, it would look like a hemisphere centered on the\npoint at which we are calculating the Phong reflection. This is shown in two\ndimensions in Figure 11.25.\nThe specular term of the Phong model is kD(RV)a. This term is depen-\ndent on both the illumination direction Land the viewing direction V. It pro-\nduces a specular “hot spot” when the viewing angle aligns closely with the\nreflection Rof the illumination direction Labout the surface normal. How-\never, its contribution falls off very quickly as the viewing angle diverges from\nthe reflected illumination direction. This is shown in two dimensions in Fig-\nure 11.26.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 653\n1\n2\nFigure 11.25. The diffuse term of the Phong reﬂection model is dependent upon NL but is inde-\npendent of the viewing angle V .\nFigure 11.26. The specular term of the Phong reﬂection model is at its maximum when the view-\ning angle V coincides with the reﬂected light direction R and drops off quickly as V diverges\nfrom R .\n11.1.3.3 Modeling Light Sources\nIn addition to modeling the light’s interactions with surfaces, we need to de-\nscribe the sources of light in the scene. As with all things in real-time render-\ning,weapproximatereal-worldlightsourcesusingvarioussimplifiedmodels.\nStatic Lighting\nThefastestlightingcalculationistheoneyoudon’tdoatall. Lightingisthere-\nfore performed offline whenever possible. We can precalculate Phong reflec-\ntion at the vertices of a mesh and store the results as diffuse vertex color at-\ntributes. We can also precalculate lighting on a per-pixel basis and store the\nresults in a kind of texture map known as a light map . At runtime, the light\nmaptextureisprojectedontotheobjectsinthesceneinordertodeterminethe\nlight’s effects on them.\nYou might wonder why we don’t just bake lighting information directly\ninto the diffuse textures in the scene. There are a few reasons for this. For one\nthing,diffusetexturemapsareoftentiledand/orrepeatedthroughoutascene,\n654 11. The Rendering Engine\nso baking lighting into them wouldn’t be practical. Instead, a single light map\nisusuallygeneratedperlightsourceandappliedtoanyobjectsthatfallwithin\nthat light’s area of influence. This approach permits dynamic objects to move\npast a light source and be properly illuminated by it. It also means that our\nlightmapscanbeofadifferent(oftenlower)resolutionthanourdiffusetexture\nmaps. Finally, a “pure” light map usually compresses better than one that\nincludes diffuse color information.\nAmbient Lights\nAnambientlight correspondstotheambientterminthePhonglightingmodel.\nThis term is independent of the viewing angle and has no specific direction.\nAn ambient light is therefore represented by a single color, corresponding to\ntheAcolor term in the Phong equation (which is scaled by the surface’s am-\nbient reflectivity kAat runtime). The intensity and color of ambient light may\nvary from region to region within the game world.\nFigure 11.27. Model\nof a directional light\nsource.\nFigure 11.28. Model\nof a point light\nsource.Directional Lights\nAdirectional light models a light source that is effectively an infinite distance\naway from the surface being illuminated—like the sun. The rays emanating\nfrom a directional light are parallel, and the light itself does not have any par-\nticular location in the game world. A directional light is therefore modeled\nas a light color Cand a direction vector L. A directional light is depicted in\nFigure 11.27.\nPoint (Omnidirectional) Lights\nApoint light (omnidirectional light ) has a distinct position in the game world\nand radiates uniformly in all directions. The intensity of the light is usually\nconsidered to fall off with the square of the distance from the light source,\nand beyond a predefined maximum radius its effects are simply clamped to\nzero. A point light is modeled as a light position P, a source color/intensity C\nandamaximumradius rmax. Therenderingengineonlyappliestheeffectsofa\npointlighttothosesurfacesthatfallwithinitssphereofinfluence(asignificant\noptimization). Figure 11.28 illustrates a point light.\nSpot Lights\nAspot light acts like a point light whose rays are restricted to a cone-shaped\nregion, like a flashlight. Usually two cones are specified with an inner and an\nouterangle. Withintheinnercone,thelightisconsideredtobeatfullintensity.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 655\nThe light intensity falls off as the angle increases from the inner to the outer\nangle,andbeyondtheouterconeitisconsideredtobezero. Withinbothcones,\nthelight intensity also falls offwith radialdistance. A spot light is modeled as\na position P, a source color C, a central direction vector L, a maximum radius\nrmaxand inner and outer cone angles qminandqmax. Figure 11.29 illustrates a\nspot light source.\nFigure 11.29. Model\nof a spot light\nsource.Area Lights\nAll of the light sources we’ve discussed thus far radiate from an idealized\npoint, either at infinity or locally. A real light source almost always has a\nnonzero area—this is what gives rise to the umbra and penumbra in the shad-\nows it casts.\nRather than trying to model area lights explicitly, CG engineers often use\nvarious “tricks” to account for their behavior. For example to simulate a\npenumbra,wemightcastmultipleshadowsandblendtheresults,orwemight\nblur the edges of a sharp shadow in some manner.\nEmissive Objects\nSomesurfacesinascenearethemselveslightsources. Examplesincludeflash-\nlights, glowing crystal balls, flames from a rocket engine and so on. Glowing\nsurfacescanbemodeledusingan emissivetexturemap—atexturewhosecolors\nare always at full intensity, independent of the surrounding lighting environ-\nment. Such a texture could be used to define a neon sign, a car’s headlights\nand so on.\nSome kinds of emissive objects are rendered by combining multiple tech-\nniques. For example, a flashlight might be rendered using an emissive texture\nfor when you’re looking head-on into the beam, a colocated spot light that\ncasts light into the scene, a yellow translucent mesh to simulate the light cone,\nsome camera-facing transparent cards to simulate lens flare (or a bloomeffect\nif high dynamic range lighting is supported by the engine), and a projected\ntexture to produce the caustic effect that a flashlight has on the surfaces it il-\nluminates. The flashlight in Luigi’s Mansion is a great example of this kind of\neffect combination, as shown in Figure 11.30.\n11.1.4 The Virtual Camera\nIncomputergraphics,thevirtualcameraismuchsimplerthanarealcameraor\nthe human eye. We treat the camera as an ideal focal point with a rectangular\nvirtualsensingsurfacecalledthe imagingrectangle floatingsomesmalldistance\n656 11. The Rendering Engine\nFigure 11.30. The ﬂashlight in Luigi’s Mansion by Nintendo (Wii) is composed of numerous visual\neffects, including a cone of translucent geometry for the beam, a dynamic spot light to cast light\ninto the scene, an emissive texture on the lens and camera-facing cards for the lens ﬂare. (See\nColor Plate XVIII.)\nin front of it. The imaging rectangle consists of a grid of square or rectangular\nvirtual light sensors, each corresponding to a single pixel on-screen. Render-\ning can be thought of as the process of determining what color and intensity\nof light would be recorded by each of these virtual sensors.\n11.1.4.1 View Space\nThe focal point of the virtual camera is the origin of a 3D coordinate system\nknown as view space orcamera space . The camera usually “looks” down the\npositive or negative z-axis in view space, with yup and xto the left or right.\nTypical left- and right-handed view-space axes are illustrated in Figure 11.31.\nThe camera’s position and orientation can be specified using a view-to-\nworld matrix, just as a mesh instance is located in the scene with its model-to-\nworld matrix. If we know the position vector and three unit basis vectors of\ncameraspace,expressedinworld-spacecoordinates,theview-to-worldmatrix\ncan be written as follows, in a manner analogous to that used to construct a\nmodel-to-world matrix:\nMV!W=2\n664iV 0\njV 0\nkV0\ntV 13\n775.\nWhen rendering a triangle mesh, its vertices are transformed first from\nmodel space to world space, and then from world space to view space. To\nperform this latter transformation, we need the world-to-view matrix, which\nistheinverseoftheview-to-worldmatrix. Thismatrixissometimescalledthe\nview matrix:\nMW!V=M 1\nV!W=Mview.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 657\nLeft-Handed Right-HandedVirtual\nScreenVirtual\nScreenFrustum Frustum\nxCzCyC\nxC\nzCyC\nFigure 11.31. Left- and right-handed camera-space axes.\nBe careful here. The fact that the camera’s matrix is inverted relative to the\nmatrices of the objects in the scene is a common point of confusion and bugs\namong new game developers.\nTheworld-to-viewmatrixisoftenconcatenatedtothemodel-to-worldma-\ntrix prior to rendering a particular mesh instance. This combined matrix is\ncalled the model-view matrix in OpenGL. We precalculate this matrix so that\nthe rendering engine only needs to do a single matrix multiply when trans-\nforming vertices from model space into view space:\nMM!V=MM!WMW!V=Mmodelview .\n11.1.4.2 Projections\nIn order to render a 3D scene onto a 2D image plane, we use a special kind\nof transformation known as a projection. The perspective projection is the most\ncommon projection in computer graphics, because it mimics the kinds of im-\nagesproducedbyatypicalcamera. Withthisprojection,objectsappearsmaller\nthefartherawaytheyarefromthecamera—aneffectknownas perspectivefore-\nshortening.\nThe length-preserving orthographic projection is also used by some games,\nprimarily for rendering plan views (e.g., front, side and top) of 3D models or\ngame levels for editing purposes, and for overlaying 2D graphics onto the\nscreen for heads-up displays and the like. Figure 11.32 illustrates how a cube\nwould look when rendered with these two types of projections.\n658 11. The Rendering Engine\nFigure 11.32. A cube rendered using a perspective projection (on the left) and an orthographic\nprojection (on the right).\n11.1.4.3 The View Volume and the Frustum\nThe region of space that the camera can “see” is known as the view volume.\nA view volume is defined by six planes. The near plane corresponds to the\nvirtual image-sensing surface. The four side planes correspond to the edges\nofthevirtualscreen. The farplane isusedasarenderingoptimizationtoensure\nthat extremely distant objects are not drawn. It also provides an upper limit\nfor the depths that will be stored in the depth buffer (see Section 11.1.4.8).\nWhen rendering the scene with a perspective projection, the shape of the\nview volume is a truncated pyramid known as a frustum. When using an or-\nthographic projection, the view volume is a rectangular prism. Perspective\nandorthographicviewvolumesareillustratedinFigure11.33andFigure11.34,\nrespectively.\nThe six planes of the view volume can be represented compactly using\nsix four-element vectors (nix,niy,niz,di), where n= (nx,ny,nz)is the plane\nnormal and dis its perpendicular distance from the origin. If we prefer the\nFar\nPlane yV\nNear \nPlane\nxVzV(r, b, n)\n(r, b, f)(r, t, f)(l, t, f)\n(l, b, n)(l, t, n)\n(l, b, f)(r, t, n)\nFigure 11.33. A perspective view volume (frustum).\n11.1. Foundations of Depth-Buffered Triangle Rasterization 659\nFar\nPlane yV\nNear\nPlane\nxVzV\n(r, b, n)(r, b, f)(r, t, f)(l, t, f)\n(l, b, n)(l, t, n )\n(l, b, f)(r, t, n )\nFigure 11.34. An orthographic view volume.\npoint-normal plane representation, we can also describe the planes with six\npairs of vectors (Qi,ni), where Qis the arbitrary point on the plane and nis\nthe plane normal. (In both cases, iis an index representing the six planes.)\n11.1.4.4 Projection and Homogeneous Clip Space\nBothperspectiveandorthographicprojectionstransformpointsinviewspace\ninto a coordinate space called homogeneous clip space. This three-dimensional\nspace is r eally just a warped version of view space. The purpose of clip space\nistoconvertthecamera-spaceviewvolumeintoacanonicalviewvolumethat\nis independent both of the kind of projection used to convert the 3D scene into\n2D screen space, and of the resolution andaspectratio of the screen onto which\nthe scene is going to be rendered.\nIn clip space, the canonical view volume is a rectangular prism extending\nfrom 1to+1along the x- and y-axes. Along the z-axis, the view volume ex-\ntends either from  1to+1(OpenGL) or from 0 to 1 (DirectX). We call this co-\nordinatesystem“clipspace”becausetheviewvolumeplanesareaxis-aligned,\nmaking it convenient to cliptriangles to the view volume in this space (even\nwhen a perspective projection is being used). The canonical clip-space view\nvolume for OpenGL is depicted in Figure 11.35. Notice that the z-axis of clip\nspace goes into the screen, with yup and xto the right. In other words, ho-\nmogeneous clip space is usually left-handed. A left-handed convention is used\nhere because it causes increasing zvalues to correspond to increasing depth\ninto the screen, with yincreasing up and xincreasing to the right as usual.\n660 11. The Rendering Engine\nFar\nPlaneyH\nNear\nPlane\nxHzH\n(1, –1, –1)(1, –1, 1)(1, 1, 1)(–1, 1, 1)\n(–1, –1, –1)(–1, 1, –1)\nFigure 11.35. The canonical view volume in homogeneous clip space.\nPerspective Projection\nAn excellent explanation of perspective projection is given in Section 4.5.1 of\n[32], so we won’t repeat it here. Instead, we’ll simply present the perspective\nprojection matrix MV!Hbelow. (The subscript V!Hindicates that this\nmatrix transforms vertices from view space into homogeneous clip space.) If\nwetakeviewspacetoberight-handed,thenthenearplaneintersectsthe z-axis\natz= n, and the far plane intersects it at z= f. The virtual screen’s left,\nright, bottom, and top edges lie at x=l,x=r,y=bandy=ton the near\nplane, respectively. (Typically the virtual screen is centered on the camera-\nspace z-axis, in which case l= randb= t, but this isn’t always the case.)\nUsing these definitions, the perspective projection matrix for OpenGL is as\nfollows:\nMV!H=2\n66666666664(2n\nr l)\n0 0 0\n0(2n\nt b)\n0 0\n(r+l\nr l) (t+b\nt b) (\n f+n\nf n)\n 1\n0 0(\n 2n f\nf n)\n03\n77777777775.\nDirectX defines the z-axis extents of the clip-space view volume to lie in\n11.1. Foundations of Depth-Buffered Triangle Rasterization 661\ntherange [0, 1]ratherthan intherange [ 1, 1]asOpenGL does. Wecan easily\nadjust the perspective projection matrix to account for DirectX’s conventions\nas follows:\n(MV!H)DirectX =2\n66666666664(2n\nr l)\n0 0 0\n0(2n\nt b)\n0 0\n(r+l\nr l) (t+b\nt b) (\n f\nf n)\n 1\n0 0(\n n f\nf n)\n03\n77777777775.\nDivision by z\nPerspective projection results in each vertex’s x- and y-coordinates being di-\nvided by its z-coordinate. This is what produces perspective foreshortening.\nTounderstandwhythishappens, considermultiplyingaview-spacepoint pV\nexpressedinfour-elementhomogeneouscoordinatesbytheOpenGLperspec-\ntive projection matrix:\npH=pVMV!H\n=[pVx pVypVz 1]2\n66666666664(2n\nr l)\n0 0 0\n0(2n\nt b)\n0 0\n(r+l\nr l) (t+b\nt b) (\n f+n\nf n)\n 1\n0 0(\n 2n f\nf n)\n03\n77777777775.\nThe result of this multiplication takes the form\npH=[\na b c pVz]\n. (11.1)\nWhenweconvertanyhomogeneousvectorintothree-dimensionalcoordi-\nnates, the x-,y- and z-components are divided by the w-component:\n[\nx y z w][x\nwy\nwz\nw]\n.\nSo, after dividing Equation (11.1) by the homogeneous w-component, which\nis really just the negative view-space z-coordinate pVz, we have:\npH=[a\n pVzb\n pVzc\n pVz]\n=[pHx pHy pHz]\n.\n662 11. The Rendering Engine\nThus,thehomogeneousclip-spacecoordinateshavebeendividedbytheview-\nspace z-coordinate, which is what causes perspective foreshortening.\nPerspective-Correct Vertex Attribute Interpolation\nIn Section 11.1.2.4, we learned that vertex attributes are interpolated in order\nto determine appropriate values for them within the interior of a triangle. At-\ntribute interpolation is performed in screenspace . We iterate over each pixel of\nthe screen and attempt to determine the value of each attribute at the corre-\nsponding location on the surface of the triangle. When rendering a scene with\na perspective projection, we must do this very carefully so as to account for\nperspectiveforeshortening. This is known as perspective-correct attributeinter-\npolation.\nA derivation of perspective-correct interpolation is beyond our scope, but\nsuffice it to say that we must divide our interpolated attribute values by the\ncorresponding z-coordinates (depths) at each vertex. For any pair of vertex\nattributes A1andA2, we can write the interpolated attribute at a percentage t\nof the distance between them as follows:\nA\npz= (1 t)(A1\np1z)\n+t(A2\np2z)\n=LERP(A1\np1z,A2\np2z,t)\n.\nReferto[32]foranexcellentderivationofthemathbehindperspective-correct\nattribute interpolation.\nOrthographic Projection\nAn orthographic projection is performed by the following matrix:\n(MV!H)ortho =2\n66666666664(2\nr l)\n0 0 0\n0(2\nt b)\n0 0\n0 0(\n 2\nf n)\n0\n(\n r+l\nr l) (\n t+b\nt b) (\n f+n\nf n)\n13\n77777777775.\nThis is just an everyday scale-and-translate matrix. (The upper-left 33\ncontains a diagonal nonuniform scaling matrix, and the lower row contains\nthe translation.) Since the view volume is a rectangular prism in both view\nspace and clip space, we need only scale and translate our vertices to convert\nfrom one space to the other.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 663\nxS\n4:3 ySxS\n16:9 yS\nFigure 11.36. The two most prevalent screen-space aspect ratios are 4:3 and 16:9.\n11.1.4.5 Screen Space and Aspect Ratios\nScreenspaceisatwo-dimensionalcoordinatesystemwhoseaxesaremeasured\nintermsofscreenpixels. The x-axistypicallypointstotheright,withtheorigin\nat the top-left corner of the screen and ypointing down. (The reason for the\ninverted y-axis is that CRT monitors scan the screen from top to bottom.) The\nratio of screen width to screen height is known as the aspect ratio. The most\ncommonaspectratiosare4:3(theaspectratioofatraditionaltelevisionscreen)\nand 16:9 (the aspect ratio of a movie screen or HDTV). These aspect ratios are\nillustrated in Figure 11.36.\nWe can render triangles expressed in homogeneous clip space by simply\ndrawing their (x,y)coordinates and ignoring z. But before we do, we scale\nand shift the clip-space coordinates so that they lie in screen space rather than\nwithin the normalized unit square. This scale-and-shift operation is known as\nscreenmapping.\n11.1.4.6 The Frame Buffer\nThe final rendered image is stored in a bitmapped color buffer known as the\nframe buffer . Pixel colors are usually stored in RGBA8888 format, although\nother frame buffer formats are supported by most graphics cards as well.\nSome common formats include RGB565, RGB5551, and one or more paletted\nmodes.\nThedisplayhardware(CRT,flat-screenmonitor,HDTV,etc.)readsthecon-\ntents of the frame buffer at a periodic rate of 60 Hz for NTSC televisions used\ninNorthAmericaandJapan,or50HzforPAL/SECAMtelevisionsusedinEu-\nrope and many other places in the world. Rendering engines typically main-\ntainatleasttwoframebuffers. Whileoneisbeingscannedbythedisplayhard-\nware, the other one can be updated by the rendering engine. This is known as\ndouble buffering. By swapping or “flipping” the two buffers during the vertical\nblankinginterval (theperiodduringwhichtheCRT’selectrongunisbeingreset\nto the top-left corner of the screen), double buffering ensures that the display\nhardware always scans the complete frame buffer. This avoids a jarring effect\n664 11. The Rendering Engine\nknown as tearing, in which the upper portion of the screen displays the newly\nrendered image while the bottom shows the remnants of the previous frame’s\nimage.\nSome engines make use of three frame buffers—a technique aptly known\nastriple buffering . This is done so that the rendering engine can start work on\nthenextframe,evenwhilethepreviousframeisstillbeingscannedbythedis-\nplay hardware. For example, the hardware might still be scanning buffer A\nwhen the engine finishes drawing buffer B. With triple buffering, it can pro-\nceed to render a new frame into buffer C, rather than idling while it waits for\nthe display hardware to finish scanning buffer A.\nRender Targets\nAny buffer into which the rendering engine draws graphics is known as a\nrender target . As we’ll see later in this chapter, rendering engines make use\nof all sorts of other off-screen render targets in addition to the frame buffers.\nThese include the depth buffer, the stencil buffer and various other buffers\nused for storing intermediate rendering results.\n11.1.4.7 Triangle Rasterization and Fragments\nTo produce an image of a triangle on-screen, we need to fill in the pixels it\noverlaps. This process is known as rasterization. During rasterization, the\ntriangle’s surface is broken into pieces called fragments, each one representing\nasmallregionofthetriangle’ssurfacethatcorrespondstoasinglepixelonthe\nscreen. (In the case of multisample antialiasing, a fragment corresponds to a\nportionof a pixel—see below.)\nA fragment is like a pixel in training. Before it is written into the frame\nbuffer, it must pass a number of tests (described in more depth below). If\nit fails any of these tests, it will be discarded. Fragments that pass the tests\nare shaded (i.e., their colors are determined), and the fragment color is either\nwritten into the frame buffer or blended with the pixel color that’s already\nthere. Figure 11.37 illustrates how a fragment becomes a pixel.\n11.1.4.8 Occlusion and the Depth Buffer\nWhenrenderingtwotrianglesthatoverlapeachotherinscreenspace,weneed\nsome way of ensuring that the triangle that is closer to the camera will ap-\npear on top. We could accomplish this by always rendering our triangles in\nback-to-front order (the so-called painter’s algorithm). However, as shown in\nFigure 11.38, this doesn’t work if the triangles are intersecting one another.\n11.1. Foundations of Depth-Buffered Triangle Rasterization 665\nFragment\nPixel\nFigure 11.37. A fragment is a small region of a triangle corresponding to a pixel on the screen. It\npasses through the rendering pipeline and is either discarded or its color is written into the frame\nbuffer.\nTo implement triangle occlusion properly, independent of the order in\nwhich the triangles are rendered, rendering engines use a technique known\nasdepth buffering orz-buffering. The depth buffer is a full-screen buffer that\ntypicallycontains24-bitintegeror(morerarely)floating-pointdepthinforma-\ntion for each pixel in the frame buffer. (The depth buffer is usually stored in\na 32-bits-per-pixel format, with a 24-bit depth value and an 8-bit stencil value\npacked into each pixel’s 32-bit quadword.) Every fragment has a z-coordinate\nthatmeasuresitsdepth“into”thescreen. (Thedepthofafragmentisfoundby\ninterpolating the depths of the triangle’s vertices.) When a fragment’s color is\nwrittenintotheframebuffer,itsdepthisstoredintothecorrespondingpixelof\nthedepthbuffer. Whenanotherfragment(fromanothertriangle)isdrawninto\nthesamepixel,theenginecomparesthenewfragment’sdepthtothedepthal-\nready present in the depth buffer. If the fragment is closer to the camera (i.e.,\nFigure 11.38. The painter’s algorithm renders triangles in a back-to-front order to produce proper\ntriangle occlusion. However, the algorithm breaks down when triangles intersect one another.\n666 11. The Rendering Engine\nifithasasmallerdepth), itoverwritesthepixelintheframebuffer. Otherwise\nthe fragment is discarded.\nz-Fighting and the w-Buffer\nWhenrenderingparallelsurfacesthatareveryclosetooneanother,it’simpor-\ntant that the rendering engine can distinguish between the depths of the two\nplanes. If our depth buffer had infinite precision, this would never be a prob-\nlem. Unfortunately,arealdepthbufferonlyhaslimitedprecision,sothedepth\nvalues of two planes can collapse into a single discrete value when the planes\narecloseenoughtogether. Whenthishappens,themore-distantplane’spixels\nstart to “poke through” the nearer plane, resulting in a noisy effect known as\nz-fighting.\nTo reduce z-fighting to a minimum across the entire scene, we would like\nto have equal precision whether we’re rendering surfaces that are close to the\ncamera or far away. However, with z-buffering this is not the case. The pre-\ncision of clip-space z-depths ( pHz) are not evenly distributed across the entire\nrangefromthenearplanetothefarplane,becauseofthedivisionbytheview-\nspace z-coordinate. Because of the shape of the 1/ zcurve, most of the depth\nbuffer’s precision is concentrated near the camera.\nThe plot of the function pHz=1/pVzshown in Figure 11.39 demonstrates\nthis effect. Near the camera, the distance between two planes in view space\n∆pVzgets transformed into a reasonably large delta in clip space ∆pHz. But\nfar from the camera, this same separation gets transformed into a tiny delta in\nclip space. The result is z-fighting, and it becomes rapidly more prevalent as\nobjects get farther away from the camera.\nTocircumventthisproblem,wewouldliketostore view-space z-coordinates\n(pVz) in the depth buffer instead of clip-space z-coordinates ( pHz). View-space\nz-coordinatesvarylinearlywiththedistancefromthecamera,sousingthemas\nour depth measure achieves uniform precision across the entire depth range.\nΔpHz\nΔpVz ΔpVzΔpHzpHz= 1/pVz pHz= 1/pVz\nFigure 11.39. A plot of the function 1/pVz, showing how most of the precision lies close to the\ncamera.",87937
74-11.2 The Rendering Pipeline.pdf,74-11.2 The Rendering Pipeline,"11.2. The Rendering Pipeline 667\nThis technique is called w-buffering, because the view-space z-coordinate con-\nveniently appears in the w-component of our homogeneous clip-space coor-\ndinates. (Recall from Equation (11.1) that pHw= pVz.)\nThe terminology can be very confusing here. The z- and w-buffers store\ncoordinates that are expressed in clip space. But in terms of view-space coordi-\nnates, the z-buffer stores 1/z(i.e., 1/pVz) while the w-buffer stores z(i.e., pVz)!\nWe should note here that the w-buffering approach is a bit more expen-\nsive than its z-based counterpart. This is because with w-buffering, we cannot\nlinearly interpolate depths directly. Depths must be inverted prior to interpo-\nlation and then re-inverted prior to being stored in the w-buffer.\n11.2 The Rendering Pipeline\nNow that we’ve completed our whirlwind tour of the major theoretical and\npractical underpinnings of triangle rasterization, let’s turn our attention to\nhow it is typically implemented. In real-time game rendering engines, the\nhigh-level rendering steps described in Section 11.1 are implemented using a\nsoftware/hardware architecture known as a pipeline. A pipeline is just an or-\ndered chain of computational stages, each with a specific purpose, operating\non a stream of input data items and producing a stream of output data.\nEach stage of a pipeline can typically operate independently of the other\nstages. Hence,oneofthebiggestadvantagesofapipelinedarchitectureisthat\nitlendsitselfextremelywelltoparallelization. Whilethefirststageischewing\nononedataelement,thesecondstagecanbeprocessingtheresultspreviously\nproduced by the first stage, and so on down the chain.\nParallelization can also be achieved within an individual stage of the\npipeline. For example, if the computing hardware for a particular stage is du-\nplicated Ntimes on the die, Ndata elements can be processed in parallel by\nthat stage. A parallelized pipeline is shown in Figure 11.40. Ideally the stages\noperate in parallel (most of the time), and certain stages are capable of operat-\ning on multiple data items simultaneously as well.\nThethroughput of a pipeline measures how many data items are processed\npersecondoverall. Thepipeline’s latencymeasurestheamountoftimeittakes\nfor a single data element to make it through the entire pipeline. The latency\nof an individual stage measures how long that stage takes to process a sin-\ngle item. The slowest stage of a pipeline dictates the throughput of the entire\npipeline. Italsohasanimpactontheaveragelatencyofthepipelineasawhole.\nTherefore, when designing a rendering pipeline, we attempt to minimize and\nbalance latency across the entire pipeline and eliminate bottlenecks. In a well-\n668 11. The Rendering Engine\nFigure 11.40. A parallelized pipeline. The stages all operate in parallel, and some stages are capable\nof operating on multiple data items simultaneously as well.\ndesigned pipeline, all the stages operate simultaneously, and no stage is ever\nidle for very long waiting for another stage to become free.\n11.2.1 Overview of the Rendering Pipeline\nSome graphics texts divide the rendering pipeline into three coarse-grained\nstages. Inthisbook,we’llextendthispipelinebackevenfurther,toencompass\nthe offline tools used to create the scenes that are ultimately rendered by the\ngame engine. The high-level stages in our pipeline are:\n•Tools stage (offline) . Geometry and surface properties (materials) are de-\nfined.\n•Assetconditioningstage(offline). The geometry and material data are pro-\ncessedbytheassetconditioningpipeline(ACP)intoanengine-readyfor-\nmat.\n•Application stage (CPU). Potentially visible mesh instances are identified\nand submitted to the graphics hardware along with their materials for\nrendering.\n•Geometryprocessingstage(GPU) .Verticesaretransformedandlitandpro-\njected into homogeneous clip space. Triangles are processed by the op-\ntional geometry shader and then clipped to the frustum.\n•Rasterizationstage(GPU).Trianglesareconvertedintofragmentsthatare\nshaded, passed through various tests ( z-test, alpha test, stencil test, etc.)\nand finally blended into the frame buffer.\n11.2. The Rendering Pipeline 669\nTools ACP\nApplicationGeometry \nProces singVerticeVerticesMesh\nInstance\nSubmes hes\nTextures MaterialsTexturesMesh\nMaterials\nMaterials TexturesRasterizationVerticeFragments\nVerticePixelsVerticeTriangles\nFigure 11.41. The format of geometric data changes radically as it passes through the various stages\nof the rendering pipeline.\n11.2.1.1 How the Rendering Pipeline Transforms Data\nIt’s interesting to note how the format of geometry data changes as it passes\nthrough the rendering pipeline. The tools and asset conditioning stages deal\nwith meshes and materials. The application stage deals in terms of mesh in-\nstances and submeshes, each of which is associated with a single material.\nDuringthegeometrystage,eachsubmeshisbrokendownintoindividualver-\ntices, which are processed largely in parallel. At the conclusion of this stage,\nthe triangles are reconstructed from the fully transformed and shaded ver-\ntices. In the rasterization stage, each triangle is broken into fragments, and\nthese fragments are either discarded, or they are eventually written into the\nframe buffer as colors. This process is illustrated in Figure 11.41.\n11.2.1.2 Implementation of the Pipeline\nThefirsttwostagesoftherenderingpipelineareimplementedoffline, usually\nexecuted by a Windows or Linux machine. The application stage is typically\nrun on one or more CPU cores, whereas the geometry and rasterization stages\nare usually executed by the graphics processing unit (GPU). In the following\nsections,we’llexploresomeofthedetailsofhoweachofthesestagesisimple-\nmented.\n11.2.2 The Tools Stage\nIn the tools stage, meshes are authored by 3D modelers in a digital content\ncreation (DCC) application like Maya, 3ds Max, Lightwave, Softimage/XSI,\n670 11. The Rendering Engine\nSketchUp, etc. The models may be defined using any convenient surface\ndescription—NURBS, quads, triangles, etc. However, they are invariably tes-\nsellatedintotrianglespriortorenderingbytheruntimeportionofthepipeline.\nTheverticesofameshmayalsobeskinned. Thisinvolvesassociatingeach\nvertex with one or more joints in an articulated skeletal structure, along with\nweights describing each joint’s relative influence over the vertex. Skinning\ninformation and the skeleton are used by the animation system to drive the\nmovements of a model—see Chapter 12 for more details.\nMaterials are also defined by the artists during the tools stage. This in-\nvolves selecting a shader for each material, selecting textures as required by\nthe shader, and specifying the configuration parameters and options of each\nshader. Textures are mapped onto the surfaces, and other vertex attributes are\nalso defined, often by “painting” them with some kind of intuitive tool within\nthe DCC application.\nMaterialsareusuallyauthoredusingacommercialorcustomin-housema-\nterialeditor. ThematerialeditorissometimesintegrateddirectlyintotheDCC\napplication as a plug-in, or it may be a stand-alone program. Some material\neditors are live-linked to the game, so that material authors can see what the\nmaterials will look like in the real game. Other editors provide an offline 3D\nvisualization view. Some editors even allow shader programs to be written\nand debugged by the artist or a shader engineer. Such tools allow rapid pro-\ntotyping of visual effects by connecting various kinds of nodes together with\na mouse. These tools generally provide a WYSIWYG display of the resulting\nmaterial. NVIDIA’sFxComposerisanexampleofsuchatool. Sadly,NVIDIA\nis no longer updating Fx Composer, and it only supports shader models up to\nDirectX 10. But they do offer a new Visual Studio plugin called NVIDIA®\nNsight™ Visual Studio Edition. Depicted in Figure 11.42, Nsight provides\npowerful shader authoring and debugging facilities. The Unreal Engine also\nprovides a graphical shader editor called Material Editor; it is shown in Fig-\nure 11.43.\nMaterials may be stored and managed with the individual meshes. How-\never, this can lead to duplication of data—and effort. In many games, a rela-\ntively small number of materials can be used to define a wide range of objects\nin the game. For example, we might define some standard, reusable materials\nlike wood, rock, metal, plastic, cloth, skin and so on. There’s no reason to du-\nplicatethesematerialsinsideeverymesh. Instead,manygameteamsbuildup\na library of materials from which to choose, and the individual meshes refer\nto the materials in a loosely coupled manner.\n11.2. The Rendering Pipeline 671\nFigure 11.42. NVIDIA® Nsight™ Visual Studio Edition allows shader programs to be written, previ-\nsualized and debugged easily.\n11.2.3 The Asset Conditioning Stage\nThe asset conditioning stage is itself a pipeline, sometimes called the assetcon-\nditioningpipeline (ACP)orthe toolspipeline. AswesawinSection7.2.1.4,itsjob\nis to export, process and link together multiple types of assets into a cohesive\nwhole. For example, a 3D model is comprised of geometry (vertex and index\nbuffers), materials, textures and an optional skeleton. The ACP ensures that\nall of the individual assets referenced by a 3D model are available and ready\nto be loaded by the engine.\nGeometric and material data is extracted from the DCC application and\nis usually stored in a platform-independent intermediate format. The data is\nthen further processed into one or more platform-specific formats, depend-\ning on how many target platforms the engine supports. Ideally the platform-\nspecific assets produced by this stage are ready to load into memory and use\nwith little or no postprocessing at runtime. For example, mesh data targeted\nfor the Xbox One or PS4 might be output as index and vertex buffers that are\nreadytobeconsumedbytheGPU;onthePS3,geometrymightbeproducedin\ncompressed data streams that are ready to be DMA’d to the SPUs for decom-\npression. The ACP often takes the needs of the material/shader into account\n672 11. The Rendering Engine\nFigure 11.43. The Unreal Engine 4 Material Editor.\nwhen building assets. For example, a particular shader might require tangent\nandbitangentvectorsaswellasavertexnormal;theACPcouldgeneratethese\nvectors automatically.\nHigh-level scenegraph datastructuresmayalsobecomputedduringtheas-\nsetconditioningstage. Forexample,static-levelgeometrymaybeprocessedin\norder to build a BSP tree. (As we’ll investigate in Section 11.2.7.4, scene graph\ndata structures help the rendering engine to very quickly determine which\nobjects should be rendered, given a particular camera position and orienta-\ntion.)\nExpensive lighting calculations are often done offline as part of the asset\nconditioning stage. This is called static lighting; it may include calculation\nof light colors at the vertices of a mesh (this is called “baked” vertex light-\ning), construction of texture maps that encode per-pixel lighting information\nknown as light maps , calculation of precomputed radiance transfer (PRT) coeffi-\ncients (usually represented by spherical harmonic functions) and so on.\n11.2.4 The GPU Pipeline\nGraphics hardware has evolved around a specialized type of microprocessor\nknown as a graphics processing unit or GPU. As we discussed in Section 4.11,\n11.2. The Rendering Pipeline 673\nConfig urable\nFixed-FunctionProgra mmablePrimitive\nAssembly\nGeometry\nShaderClippingScreen\nMappingTriangle\nSetupTriang le \nTraversalEarly \nZ TestPixel\nShaderMerge\n/ ROP\nStream\nOutputVertex \nShader\nFrame\nBuffer\nFigure 11.44. The geometry processing and rasterization stages of the rendering pipeline, as implemented by a typical GPU. The\nwhite stages are programmable, the light grey stages are conﬁgurable, and the dark grey boxes are ﬁxed-function.\na GPU is designed to maximize throughput of the graphics pipeline, which it\nachieves through massive parallelization of tasks like vertex processing and\nper-pixel shading calculations. For example, a modern GPU like the AMD\nRadeon™7970canachieveapeakperformanceof4TFLOPS,whichitdoesby\nexecuting workloads in parallel across 32 compute units, each of which con-\ntains four 16-lane SIMD VPUs, which in turn execute pipelined wavefronts\nconsisting of 64 threads each. A GPU can be used to render graphics, but to-\nday’s GPUs are also fully programmable, allowing programmers to leverage\nthe awesome computing power of a GPU to execute compute shaders. This is\nknown as general-purpose GPU computing (GPGPU).\nVirtuallyallGPUsbreakthegraphicspipelineintothesubstagesdescribed\nbelow and depicted in Figure 11.44. Each stage is shaded to indicate whether\nits functionality is programmable, fixed but configurable, or fixed and non-\nconfigurable.\n11.2.4.1 Vertex Shader\nThis stage is fully programmable. It is responsible for transformation and\nshading/lighting of individual vertices. The input to this stage is a single ver-\ntex (although in practice many vertices are processed in parallel). Its position\nandnormalaretypicallyexpressedinmodelspaceorworldspace. Thevertex\nshaderhandlestransformationfrommodelspacetoviewspaceviathemodel-\nview transform. Perspective projection is also applied, as well as per-vertex\nlightingandtexturingcalculations,andskinningforanimatedcharacters. The\nvertex shader can also perform procedural animation by modifying the posi-\ntion of the vertex. Examples of this include foliage that sways in the breeze or\nan undulating water surface. The output of this stage is a fully transformed\nand lit vertex, whose position and normal are expressed in homogeneous clip\n674 11. The Rendering Engine\nspace (see Section 11.1.4.4).\nOn modern GPUs, the vertex shader has full access to texture data—a ca-\npability that used to be available only to the pixel shader. This is particularly\nuseful when textures are used as stand-alone data structures like heightmaps\nor look-up tables.\n11.2.4.2 Geometry Shader\nThisoptionalstageisalsofullyprogrammable. Thegeometryshaderoperates\non entire primitives (triangles, lines and points) in homogeneous clip space.\nIt is capable of culling or modifying input primitives, and it can also generate\nnew primitives. Typical uses include shadow volume extrusion (see Section\n11.3.3.1), rendering the six faces of a cube map (see Section 11.3.1.4), fur fin\nextrusion around silhouette edges of meshes, creation of particle quads from\npoint data (see Section 11.4.1), dynamic tessellation, fractal subdivision of line\nsegments for lightning effects, cloth simulations, and the list goes on.\n11.2.4.3 Stream Output\nSome GPUs permit the data that has been processed up to this point in the\npipeline to be written back to memory. From there, it can then be looped back\nto the top of the pipeline for further processing. This feature is called stream\noutput.\nStreamoutputpermitsanumberofintriguingvisualeffectstobeachieved\nwithout the aid of the CPU. An excellent example is hair rendering. Hair is\noften represented as a collection of cubic spline curves. It used to be that hair\nphysicssimulationwouldbedoneontheCPU.TheCPUwouldalsotessellate\nthe splines into line segments. Finally the GPU would render the segments.\nWith stream output, the GPU can do the physics simulation on the con-\ntrol points of the hair splines within the vertex shader. The geometry shader\ntessellates the splines, and the stream output feature is used to write the tes-\nsellated vertex data to memory. The line segments are then piped back into\nthe top of the pipeline so they can be rendered.\n11.2.4.4 Clipping\nThe clipping stage chops off those portions of the triangles that straddle the\nfrustum. Clipping is done by identifying vertices that lie outside the frustum\nand then finding the intersection of the triangle’s edges with the planes of the\nfrustum. These intersection points become new vertices that define one or\nmore clipped triangles.\nThis stage is fixed in function, but it is somewhat configurable. For ex-\nample, user-defined clipping planes can be added in addition to the frustum\n11.2. The Rendering Pipeline 675\nplanes. This stage can also be configured to cull triangles that lie entirely out-\nside the frustum.\n11.2.4.5 Screen Mapping\nScreen mapping simply scales and shifts the vertices from homogeneous clip\nspace into screen space. This stage is entirely fixed and non-configurable.\n11.2.4.6 Triangle Set-up\nDuring triangle set-up, the rasterization hardware is initialized for efficient\nconversion of the triangle into fragments. This stage is not configurable.\n11.2.4.7 Triangle Traversal\nEachtriangleisbrokenintofragments(i.e.,rasterized)bythetriangletraversal\nstage. Usually one fragment is generated for each pixel, although with certain\nantialiasing techniques, multiple fragments may be created per pixel (see Sec-\ntion 11.1.4.7). The triangle traversal stage also interpolates vertex attributes in\norder to generate per-fragment attributes for processing by the pixel shader.\nPerspective-correctinterpolationisusedwhereappropriate. Thisstage’sfunc-\ntionality is fixed and not configurable.\n11.2.4.8 Early z-Test\nMany graphics cards are capable of checking the depth of the fragment at this\npoint in the pipeline, discarding it if it is being occluded by the pixel already\nin the frame buffer. This allows the (potentially very expensive) pixel shader\nstage to be skipped entirely for occluded fragments.\nSurprisingly,notallgraphicshardwaresupportsdepthtestingatthisstage\nof the pipeline. In older GPU designs, the z-test was done along with alpha\ntesting, after the pixel shader had run. For this reason, this stage is called the\nearly z-test orearly depth test stage.\n11.2.4.9 Pixel Shader\nThis stage is fully programmable. Its job is to shade (i.e., light and otherwise\nprocess) each fragment. The pixel shader can also discard fragments, for ex-\nample because they are deemed to be entirely transparent. The pixel shader\ncanaddressoneormoretexturemaps,runper-pixellightingcalculations,and\ndo whatever else is necessary to determine the fragment’s color.\nTheinputtothisstageisacollectionofper-fragmentattributes(whichhave\nbeen interpolated from the vertex attributes by the triangle traversal stage).\nTheoutputisasinglecolorvectordescribingthedesiredcolorofthefragment.\n676 11. The Rendering Engine\n11.2.4.10 Merging / Raster Operations Stage\nThe final stage of the pipeline is known as the merging stage orblending stage ,\nalso known as the raster operations stage or ROP in NVIDIA parlance. This\nstage is not programmable, but it is highly configurable. It is responsible for\nrunning various fragment tests including the depth test (see Section 11.1.4.8),\nalpha test (in which the values of the fragment’s and pixel’s alpha channels\ncan be used to reject certain fragments) and stencil test (see Section 11.3.3.1).\nIf the fragment passes all of the tests, its color is blended (merged) with\nthe color that is already present in the frame buffer. The way in which blend-\ning occurs is controlled by the alphablendingfunction —a function whose basic\nstructure is hard-wired, but whose operators and parameters can be config-\nured in order to produce a wide variety of blending operations.\nAlpha blending is most commonly used to render semitransparent geom-\netry. In this case, the following blending function is used:\nC′\nD=ASCS+ (1 AS)CD.\nThe subscripts SandDstand for “source” (the incoming fragment) and “des-\ntination” (the pixel in the frame buffer), respectively. Therefore, the color that\nis written into the frame buffer ( C′\nD) is aweightedaverage of the existing frame\nbuffer contents ( CD) and the color of the fragment being drawn ( CS). The\nblend weight ( AS) is just the source alpha of the incoming fragment.\nFor alpha blending to look right, the semitransparent and translucent sur-\nfaces in the scene must be sorted and rendered in back-to-front order, af-\nterthe opaque geometry has been rendered to the frame buffer. This is be-\ncause after alpha blending has been performed, the depth of the new frag-\nmentoverwrites the depth of the pixel with which it was blended. In other\nwords, the depth buffer ignores transparency (unless depth writes have been\nturned off, of course). If we are rendering a stack of translucent objects on\ntop of an opaque backdrop, the resulting pixel color should ideally be a blend\nbetween the opaque surface’s color and the colors of allof the translucent\nsurfaces in the stack. If we try to render the stack in any order other than\nback-to-front, depth-test failures will cause some of the translucent fragments\nto be discarded, resulting in an incomplete blend (and a rather odd-looking\nimage).\nOther alpha blending functions can be defined as well, for purposes other\nthan transparency blending. The general blending equation takes the form\nC′\nD= (wS\nCS) + (wD\nCD), where the weighting factors wSandwDcan\nbeselectedbytheprogrammerfromapredefinedsetofvaluesincludingzero,\n11.2. The Rendering Pipeline 677\none, source or destination color, source or destination alpha and one minus\nthe source or destination color or alpha. The operator \nis either a regular\nscalar-vectormultiplicationoracomponent-wisevector-vectormultiplication\n(aHadamardproduct—seeSection5.2.4.1)dependingonthedatatypesof wS\nandwD.\n11.2.5 Programmable Shaders\nNow that we have an end-to-end picture of the GPU pipeline in mind, let’s\ntake a deeper look at the most interesting part of the pipeline—the program-\nmable shaders. Shader architectures have evolved significantly since their\nintroduction with DirectX 8. Early shader models supported only low-level\nassembly language programming, and the instruction set and register set of\nthepixelshaderdifferedsignificantlyfromthoseofthevertexshader. DirectX\n9 brought with it support for high-level C-like shader languages such as Cg\n(C for graphics), HLSL (High-Level Shading Language—Microsoft’s imple-\nmentation of the Cg language) and GLSL (OpenGL shading language). With\nDirectX 10, the geometry shader was introduced, and with it came a unified\nshader architecture called shader model 4.0 in DirectX parlance. In the unified\nshader model, all three types of shaders support roughly the same instruction\nset and have roughly the same set of capabilities, including the ability to read\ntexture memory.\nA shader takes a single element of input data and transforms it into zero\nor more elements of output data.\n• In the case of the vertex shader, the input is a vertex whose position and\nnormal are expressed in model space or world space. The output of the\nvertex shader is a fully transformed and lit vertex, expressed in homo-\ngeneous clip space.\n• The input to the geometry shader is a single n-vertex primitive—a point\n(n=1),linesegment( n=2)ortriangle( n=3)—withupto nadditional\nvertices that act as control points. The output is zero or more primitives,\npossibly of a different type than the input. For example, the geometry\nshader could convert points into two-triangle quads, or it could trans-\nform triangles into triangles but optionally discard some triangles and\nso on.\n• The pixel shader’s input is a fragment whose attributes have been inter-\npolated from the three vertices of the triangle from which it came. The\noutput of the pixel shader is the color that will be written into the frame\nbuffer (presuming the fragment passes the depth test and other optional\n678 11. The Rendering Engine\ntests). Thepixelshaderisalsocapableofdiscardingfragmentsexplicitly,\nin which case it produces no output.\n11.2.5.1 Accessing Memory\nBecause the GPU implements a data processing pipeline, access to RAM is\ncarefully controlled. A shader program usually cannot read from or write to\nmemory directly. Instead, its memory accesses are limited to two methods:\nregisters and texture maps.\nHowever, we should note that these restrictions are lifted on systems in\nwhich the GPU and CPU share memory directly. For example, the AMD\nJaguar system on a chip (SoC) that sits at the heart of the PlayStation 4 is an\nexample of a heterogeneous system architecture (HSA). On a non-HSA system,\nthe CPU and GPU are typically separate devices, each with its own private\nmemory, and each usually residing on a separate circuit board. Transferring\ndata between the two processors requires cumbersome, high-latency commu-\nnication over a specialized bus such as AGP or PCIe. With HSA, the CPU and\nGPU share a single unified memory store called a heterogeneous unified mem-\nory architecture (hUMA). Shaders running on a system with hUMA, like the\nPS4, can therefore be passed a shaderresourcetable (SRT) as input. This is just a\npointer to a C/C++ struct in memory that can be read from or written to by\nboth the CPU and the shader running on the GPU. On the PS4, SRTs take the\nplace of the constant registers described in the following sections.\nShader Registers\nA shader can access RAM indirectly via registers . All GPU registers are in 128-\nbit SIMD format. Each register is capable of holding four 32-bit floating-point\nor integer values (represented by the float4 data type in the Cg language).\nSucharegistercancontainafour-elementvectorinhomogeneouscoordinates\nor a color in RGBA format, with each component in 32-bit floating-point for-\nmat. Matrices can be represented by groups of three or four registers (rep-\nresented by built-in matrix types like float4x4 in Cg). A GPU register can\nalso be used to hold a single 32-bit scalar, in which case the value is usually\nreplicatedacrossallfour32-bitfields. SomeGPUscanoperateon16-bitfields,\nknown as halfs. (Cg provides various built-in types like half4andhalf4x4\nfor this purpose.)\nRegisters come in four flavors, as follows:\n•Input registers. These registers are the shader’s primary source of in-\nput data. In a vertex shader, the input registers contain attribute data\n11.2. The Rendering Pipeline 679\nobtained directly from the vertices. In a pixel shader, the input registers\ncontaininterpolatedvertexattributedatacorrespondingtoasinglefrag-\nment. The values of all input registers are set automatically by the GPU\nprior to invoking the shader.\n•Constant registers. The values of constant registers are set by the ap-\nplication and can change from primitive to primitive. Their values are\nconstant only from the point of view of the shader program. They pro-\nvide a secondary form of input to the shader. Typical contents include\nthe model-view matrix, the projection matrix, light parameters and any\nother parameters required by the shader that are not available as vertex\nattributes.\n•Temporary registers. These registers are for use by the shader program\ninternally and are typically used to store intermediate results of calcula-\ntions.\n•Outputregisters. Thecontentsoftheseregistersarefilledinbytheshader\nandserveasitsonlyformofoutput. Inavertexshader,theoutputregis-\nters contain vertex attributes such as the transformed position and nor-\nmal vectors in homogeneous clip space, optional vertex colors, texture\ncoordinates and so on. In a pixel shader, the output register contains the\nfinal color of the fragment being shaded.\nThe application provides the values of the constant registers when it sub-\nmits primitives for rendering. The GPU automatically copies vertex or frag-\nmentattributedatafromvideoRAMintotheappropriateinputregistersprior\ntocallingtheshaderprogram,anditalsowritesthecontentsoftheoutputreg-\nisters back into RAM at the conclusion of the program’s execution so that the\ndata can be passed to the next stage of the pipeline.\nGPUstypicallycacheoutputdatasothatitcanbereusedwithoutbeingre-\ncalculated. Forexample,the post-transformvertexcache storesthemost-recently\nprocessed vertices emitted by the vertex shader. If a triangle is encountered\nthat refers to a previously processed vertex, it will be read from the post-\ntransformvertexcacheifpossible—thevertexshaderneedonlybecalledagain\nif the vertex in question has since been ejected from the cache to make room\nfor newly processed vertices.\nTextures\nA shader also has direct read-only access to texture maps. Texture data is ad-\ndressed via texture coordinates, rather than via absolute memory addresses.\nTheGPU’stexturesamplersautomatically filterthetexturedata, blendingval-\nuesbetweenadjacenttexelsoradjacentmipmaplevelsasappropriate. Texture\n680 11. The Rendering Engine\nfiltering can be disabled in order to gain direct access to the values of partic-\nular texels. This can be useful when a texture map is used as a data table, for\nexample.\nShaders can only writeto texture maps in an indirect manner—by render-\ning the scene to an off-screen frame buffer that is interpreted as a texture map\nby subsequent rendering passes. This feature is known as renderto texture .\n11.2.5.2 Introduction to High-Level Shader Language Syntax\nHigh-level shader languages like Cg and GLSL are modeled after the C pro-\ngramming language. The programmer can declare functions, define a simple\nstruct, and perform arithmetic. However, as we said above, a shader pro-\ngram only has access to registers and textures. As such, the struct and vari-\nable we declare in Cg or GLSL is mapped directly onto registers by the shader\ncompiler. We define these mappings in the following ways:\n•Semantics . Variables and struct members can be suffixed with a colon\nfollowed by a keyword known as a semantic . The semantic tells the\nshader compiler to bind the variable or data member to a particular ver-\ntex or fragment attribute. For example, in a vertex shader we might de-\nclare an input struct whose members map to the position andcolorat-\ntributes of a vertex as follows:\nstruct VtxOut\n{\nfloat4 pos : POSITION ; // map to position attribute\nfloat4 color : COLOR; // map to color attribute\n};\n•Input versus output. The compiler determines whether a particular vari-\nableor struct shouldmaptoinputoroutputregistersfromthecontext\nin which it is used. If a variable is passed as an argument to the shader\nprogram’s main function, it is assumed to be an input; if it is the return\nvalue of the main function, it is taken to be an output.\nVtxOut vshaderMain(VtxIn in) // maps to input registers\n{\nVtxOut out;\n// ...\nreturn out; // maps to output registers\n}\n•Uniformdeclaration . Togainaccesstothedatasuppliedbytheapplication\nvia the constant registers, we can declare a variable with the keyword\n11.2. The Rendering Pipeline 681\nuniform. For example, the model-view matrix could be passed to a\nvertex shader as follows:\nVtxOut vshaderMain(\nVtxIn in,\nuniform float4x4 modelViewMatrix )\n{\nVtxOut out;\n// ...\nreturn out;\n}\nArithmetic operations can be performed by invoking C-style operators, or\nby calling intrinsic functions as appropriate. For example, to multiply the in-\nput vertex position by the model-view matrix, we could write:\nVtxOut vshaderMain(VtxIn in,\nuniform float4x4 modelViewMatrix )\n{\nVtxOut out;\nout.pos =mul(modelViewMatrix, in.pos);\nout.color = float4(0, 1, 0, 1); // RGBA green\nreturn out;\n}\nData is obtained from textures by calling special intrinsic functions that\nread the value of the texels at a specified texture coordinate. A number of\nvariantsareavailableforreadingone-,two-andthree-dimensionaltexturesin\nvariousformats, withandwithoutfiltering. Specialtextureaddressingmodes\nare also available for accessing cube maps and shadow maps. References to\nthe texture maps themselves are declared using a special data type known as\natexture sampler declaration. For example, the data type sampler2D repre-\nsents a reference to a typical two-dimensional texture. The following simple\nCg pixel shader applies a diffuse texture to a triangle:\nstruct FragmentOut\n{\nfloat4 color : COLOR;\n};\nFragmentOut pshaderMain(float2 uv:TEXCOORD0,\nuniform sampler2D texture )\n{\nFragmentOut out;\n682 11. The Rendering Engine\n// look up texel at (u,v)\nout.color = tex2D(texture, uv);\nreturn out;\n}\n11.2.5.3 Effect Files\nByitself, ashaderprogramisn’tparticularlyuseful. Additionalinformationis\nrequired by the GPU pipeline in order to call the shader program with mean-\ningful inputs. For example, we need to specify how the application-specified\nparameters, like the model-view matrix, light parameters and so on, map to\ntheuniform variables declared in the shader program. In addition, some vi-\nsual effects require two or more rendering passes, but a shader program only\ndescribestheoperationstobeappliedduringasinglerenderingpass. Ifweare\nwritingagameforthePCplatform, wewillneedtodefine“fallback”versions\nof some of our more-advanced rendering effects, so that they will work even\nonoldergraphicscards. Totieourshaderprogram(s)togetherintoacomplete\nvisual effect, we turn to a file format known as an effectfile.\nDifferent rendering engines implement effects in slightly different ways.\nIn Cg, the effect file format is known as CgFX. OGRE uses a file format very\nsimilar to CgFX known as a material file. GLSL effects can be described using\ntheCOLLADAformat, whichisbasedonXML.Despitethedifferences,effects\ngenerally take on the following hierarchical format:\n• At global scope, structs, shader programs (implemented as various\n“main” functions) and global variables (which map to application-\nspecified constant parameters) are defined.\n• One or more techniques are defined. A technique represents one way to\nrender a particular visual effect. An effect typically provides a primary\ntechnique for its highest-quality implementation and possibly a number\nof fallback techniques for use on lower-powered graphics hardware.\n• Within each technique, one or more passesare defined. A pass describes\nhow a single full-frame image should be rendered. It typically includes\nareferenceto a vertex, geometry and/or pixel shader program’s“main”\nfunction, various parameter bindings and optional render state settings.\n11.2.5.4 Further Reading\nIn this section, we’ve only had a small taste of what high-level shader pro-\ngramming is like—a complete tutorial is beyond our scope here. For a much\n11.2. The Rendering Pipeline 683\nFigure 11.45. No antialiasing (left), 4  MSAA (center) and Nvidia’s FXAA, preset 3 (right). Image from\nNvidia’s FXAA white paper by Timothy Lottes (http: / /bit.ly/1mIzCTv). (See Color Plate XIX.)\nmore detailed introduction to Cg shader programming, refer to the Cg tuto-\nrialavailableonNVIDIA’swebsiteathttps://developer.nvidia.com/content/\nhello-cg-introductory-tutorial.\n11.2.6 Antialiasing\nWhen a triangle is rasterized, its edges can look jagged—the familiar “stair\nstep” effect we have all come to know and love (or hate). Technically speak-\ning, aliasing arises because we are using a discrete set of pixels to samplean\nimage that is really a smooth, continuous two-dimensional signal. (See Sec-\ntion 14.3.2.1 for a detailed discussion of sampling and aliasing.)\nThe term antialiasing describes any technique that reduces the visual arti-\nfactscausedbyaliasing. Therearemanydifferentwaystoantialiasarendered\nscene. The net effect of pretty much all of them is to “soften” the edges of ren-\nderedtrianglesbyblendingthemwithsurroundingpixels. Eachtechniquehas\nunique performance, memory-usage and quality characteristics. Figure 11.45\nshows a scene rendered first without antialiasing, then with 4 MSAA and\nfinally with Nvidia’s FXAA technique.\n684 11. The Rendering Engine\n11.2.6.1 Full-Screen Antialiasing (FSAA)\nIn this technique, also known as super-sampledantialiasing (SSAA), the scene is\nrenderedintoaframebufferthatislargerthantheactualscreen. Oncerender-\ning of the frame is complete, the resulting oversized image is downsampled to\nthe desired resolution. In 4 supersampling, the rendered image is twice as\nwide and twice as tall as the screen, resulting in a frame buffer that occupies\nfour times the memory. It also requires four times the GPU processing power\nbecause the pixel shader must be run four times for each screen pixel. As you\ncan see, FSAA is an incredibly expensive technique both in terms of memory\nconsumption and GPU cycles. As such, it is rarely used in practice.\n11.2.6.2 Multisampled Antialiasing (MSAA)\nMultisampled antialiasing is a technique that provides visual quality compa-\nrabletothatofFSAA,whileconsumingagreatdeallessGPUbandwidth(and\nthe same amount of video RAM). The MSAA approach is based on the ob-\nservation that, thanks to the natural antialiasing effect of texture mipmapping,\naliasing tends to be a problem primarily at the edgesof triangles, not in their\ninteriors.\nTo understand how MSAA works, recall that the process of rasterizing a\ntriangle really boils down to three distinct operations: (1) Determining which\npixels the triangle overlaps (coverage), (2) determining whether or not each\npixel is occluded by some other triangle (depth testing) and (3) determining\nthecolorofeachpixel,presumingthatthecoverageanddepthteststellusthat\nthe pixel should in fact be drawn (pixel shading).\nWhen rasterizing a triangle without antialiasing, the coverage test, depth\ntest and pixel shading operations are all run at a single idealized point within\neach screen pixel, usually located at its center. In MSAA, the coverage and\ndepth tests are run for Npoints known as subsamples within each screen pixel.\nNis typically chosen to be 2, 4, 5, 8 or 16. However, the pixel shader is only\nrunonceper screen pixel, no matter how many subsamples we use. This gives\nMSAAabigadvantageoverFSAAintermsofGPUbandwidth,becauseshad-\ning is typically a great deal more expensive than coverage and depth testing.\nInNMSAA,thedepth,stencilandcolorbuffersareeachallocatedtobe N\ntimes as large as they would otherwise be. For each screen pixel, these buffers\ncontain N“slots,” one slot for each subsample. When rasterizing a triangle,\nthe coverage and depth tests are run Ntimes for the Nsubsamples within\neach fragment of the triangle. If at least one of the Ntests indicates that the\nfragment should be drawn, the pixel shader is run once. The color obtained\nfrom the pixel shader is then stored onlyinto those slots that correspond to\n11.2. The Rendering Pipeline 685\nFigure 11.46. Rasterizing a triangle without antialiasing.\nthe subsamples that fell insidethe triangle. Once the entire scene has been\nrendered, the oversized color buffer is downsampled to yield the final screen-\nresolution image. This process involves averaging the color values found in\ntheNsubsample slots for each screen pixel. The net result is an antialiased\nimage with a shading cost equal to that of a non-antialiased image.\nIn Figure 11.46 we see a triangle that has been rasterized without antialias-\ning. Figure 11.47 illustrates the 4 MSAA technique. For more informa-\ntion on MSAA, see http://mynameismjp.wordpress.com/2012/10/24/msaa\n-overview.\n11.2.6.3 Coverage Sample Antialiasing (CSAA)\nThis technique is an optimization of the MSAA technique pioneered by\nNvidia. For 4CSAA, the pixel shader is run once, the depth test and color\nstorageisdoneforfoursubsamplepointsperfragment,butthepixelcoverage\ntest is performed for 16 “coverage subsamples” per fragment. This produces\nfiner-grained color blending at the edges of triangles, similar to what you’d\nsee with 8or 16MSAA, but at the memory and GPU cost of 4 MSAA.\n11.2.6.4 Morphological Antialiasing (MLAA)\nMorphological antialiasing focuses its efforts on correcting only those regions\nof a scene that suffer the most from the effects of aliasing. In MLAA, the scene\nis rendered at normal size, and then scanned in order to identify stair-stepped\npatterns. Whenthesepatternsarefound, theyareblurredtoreducetheeffects\nof aliasing. Fast approximate antialiasing (FXAA) is an optimized technique\n686 11. The Rendering Engine\nPixel shader runs at \npixel center2 of the 4 subsamples \nare inside the triangle\n4 of the 4 subsamples \nare inside the triangle50%\n100%\nFigure 11.47. Multisampled antialiasing (MSAA).\n11.2. The Rendering Pipeline 687\ndeveloped by Nvidia that is similar to MLAA in its approach.\nForadetaileddiscussionofMLAA,seehttps://intel.ly/2HhrQWX.FXAA\nis described in detail here: https://bit.ly/1mIzCTv.\n11.2.6.5 Subpixel Morphological Antialiasing (SMAA)\nSubpixel Morphological Antialiasing (SMAA) combines morphological an-\ntialiasing(MLAAandFXAA)techniqueswithmultisampling/supersampling\nstrategies (MSAA, SSAA) to produce more accurate subpixel features. Like\nFXAA, it’s an inexpensive technique, but it blurs the final image less than\nFXAA. For these reasons, it’s arguably the best AA solution available today.\nAdetailedcoverageofthistopicisbeyondourscopeinthisbook, butyoucan\nread more about SMAA at http://www.iryoku.com/smaa/.\n11.2.7 The Application Stage\nNow that we understand how the GPU works, we can discuss the pipeline\nstage that is responsible for driving it—the application stage. This stage has\nthree roles:\n1.Visibility determination. Only objects that are visible (or at least poten-\ntiallyvisible) should be submitted to the GPU, lest we waste valuable\nresources processing triangles that will never be seen.\n2.SubmittinggeometrytotheGPUforrendering . Submesh-material pairs are\nsent to the GPU via a rendering call like DrawIndexedPrimitive()\n(DirectX) or glDrawArrays() (OpenGL), or via direct construction of\ntheGPUcommandlist. Thegeometrymaybesortedforoptimalrender-\ning performance. Geometry might be submitted more than once if the\nscene needs to be rendered in multiple passes.\n3.Controlling shader parameters and render state. The uniform parameters\npassed to the shader via constant registers are configured by the appli-\ncation stage on a per-primitive basis. In addition, the application stage\nmust set all of the configurable parameters of the non-programmable\npipeline stages to ensure that each primitive is rendered appropriately.\nIn the following sections, we’ll briefly explore how the application stage per-\nforms these tasks.\n11.2.7.1 Visibility Determination\nThe cheapest triangles are the ones you never draw. So it’s incredibly impor-\ntant tocullobjects from the scene that do not contribute to the final rendered\n688 11. The Rendering Engine\nimage prior to submitting them to the GPU. The process of constructing the\nlist of visible mesh instances is known as visibility determination.\nFrustum Culling\nInfrustumculling,allobjectsthatlieentirelyoutsidethefrustumareexcluded\nfrom our render list. Given a candidate mesh instance, we can determine\nwhether or not it lies inside the frustum by performing some simple tests be-\ntween the object’s bounding volume and the six frustum planes. The bounding\nvolume is usually a sphere, because spheres are particularly easy to cull. For\neachfrustumplane,wemovetheplaneoutwardadistanceequaltotheradius\nof the sphere, then we determine on which side of each modified plane the\ncenter point of the sphere lies. If the sphere is found to be on the front side of\nall six modified planes, the sphere is inside the frustum.\nInpractice,wedon’tneedtoactuallymovethefrustumplanes. Recallfrom\nEquation(5.13)thattheperpendiculardistance hfromapointtoaplanecanbe\ncalculated by plugging the point directly into the plane equation as follows:\nh=ax+by+cz+d=nP nP0(see Section 5.6.3). So all we need to\ndo is plug the center point of our bounding sphere into the plane equations\nfor each frustum plane, giving us a value of hifor each plane i, and then we\ncan compare the hivalues to the radius of the bounding sphere to determine\nwhether or not it lies inside each plane.\nA scene graph data structure, described in Section 11.2.7.4, can help opti-\nmizefrustumcullingbyallowingustoignoreobjectswhoseboundingspheres\nare nowhere close to being inside the frustum.\nOcclusion and Potentially Visible Sets\nEven when objects lie entirely within the frustum, they may occlude one an-\nother. Removing objects from the visible list that are entirely occluded by\notherobjectsiscalled occlusionculling. Incrowdedenvironmentsviewedfrom\nground level, there can be a great deal of inter-object occlusion, making oc-\nclusion culling extremely important. In less crowded scenes, or when scenes\nare viewed from above, much less occlusion may be present, and the cost of\nocclusion culling may outweigh its benefits.\nGross occlusion culling of a large-scale environment can be done by pre-\ncalculating a potentially visible set (PVS). For any given camera vantage point,\na PVS lists those scene objects that might be visible. A PVS errs on the side of\nincludingobjectsthataren’tactuallyvisible,ratherthanexcludingobjectsthat\nactually would have contributed to the rendered scene.\nOne way to implement a PVS system is to chop the level up into regions\nof some kind. Each region can be provided with a list of the other regions\n11.2. The Rendering Pipeline 689\nthat can be seen when the camera is inside it. These PVSs might be manually\nspecified by the artists or game designers. More commonly, an automated\noffline tool generates the PVS based on user-specified regions. Such a tool\nusually operates by rendering the scene from various randomly distributed\nvantagepointswithinaregion. Everyregion’sgeometryiscolorcoded, sothe\nlist of visible regions can be found by scanning the resulting frame buffer and\ntabulating the region colors that are found. Because automated PVS tools are\nimperfect, they typically provide the user with a mechanism for tweaking the\nresults, either by manually placing vantage points for testing, or by manually\nspecifyingalistofregionsthatshouldbeexplicitlyincludedorexcludedfrom\na particular region’s PVS.\nPortals\nAnother way to determine what portions of a scene are visible is to use por-\ntals. Inportalrendering,thegameworldisdividedupintosemiclosedregions\nthat are connected to one another via holes, such as windows and doorways.\nThese holes are called portals. They are usually represented by polygons that\ndescribe their boundaries.\nTo render a scene with portals, we start by rendering the region that con-\ntains the camera. Then, for each portal in the region, we extend a frustum-like\nvolume consisting of planes extending from the camera’s focal point through\neach edge of the portal’s bounding polygon. The contents of the neighboring\nregioncanbeculledtothisportalvolumeinexactlythesamewaygeometryis\nculledagainstthecamerafrustum. Thisensuresthatonlythevisiblegeometry\nin the adjacent regions will be rendered. Figure 11.48 provides an illustration\nof this technique.\nOcclusion Volumes (Antiportals)\nIf we flip the portal concept on its head, pyramidal volumes can also be used\nto describe regions of the scene that cannotbe seen because they are being oc-\ncluded by an object. These volumes are known as occlusionvolumes orantipor-\ntals. To construct an occlusion volume, we find the silhouette edges of each\noccluding object and extend planes outward from the camera’s focal point\nthrough each of these edges. We test more-distant objects against these oc-\nclusion volumes and cull them if they lie entirely within the occlusion region.\nThis is illustrated in Figure 11.49.\nPortals are best used when rendering enclosed indoor environments with\na relatively small number of windows and doorways between “rooms.” In\nthis kind of scene, the portals occupy a relatively small percentage of the total\n690 11. The Rendering Engine\nA\nB\nEC\nD\nFG\nFigure 11.48. Portals are used to deﬁne frustum-like volumes, which are used to cull the contents\nof neighboring regions. In this example, objects A, B and D will be culled because they lie outside\none of the portals; the other objects will be visible.\nvolume of the camera frustum, resulting in a large number of objects outside\ntheportalsthatcanbeculled. Antiportalsarebestappliedtolargeoutdooren-\nvironments, in which nearby objects often occlude large swaths of the camera\nfrustum. In this case, the antiportals occupy a relatively large percentage of\nthe total camera frustum volume, resulting in large numbers of culled objects.\nAH\nEDF\nGBC\nFigure 11.49. As a result of the antiportals corresponding to objects A, B and C, objects D, E, F and\nG are culled. Therefore, only A, B, C and H are visible.\n11.2. The Rendering Pipeline 691\n11.2.7.2 Primitive Submission\nOnce a list of visible geometric primitives has been generated, the individual\nprimitives must be submitted to the GPU pipeline for rendering. This can be\naccomplished by making calls to DrawIndexedPrimitive() in DirectX or\nglDrawArrays() in OpenGL.\nRender State\nAswelearnedinSection11.2.4,thefunctionalityofmanyoftheGPUpipeline’s\nstages is fixed but configurable. And even programmable stages are driven in\npartbyconfigurableparameters. Someexamplesoftheseconfigurableparam-\neters are listed below (although this is by no means a complete list):\n• world-view matrix;\n• light direction vectors;\n• texturebindings(i.e.,whichtexturestouseforagivenmaterial/shader);\n• texture addressing and filtering modes;\n• time base for scrolling textures and other animated effects;\n•z-test (enabled or disabled); and\n• alpha blending options.\nThe set of all configurable parameters within the GPU pipeline is known\nas thehardware state orrender state. It is the application stage’s responsibility\nto ensure that the hardware state is configured properly and completely for\neachsubmittedprimitive. Ideallythesestatesettingsaredescribedcompletely\nby the material associated with each submesh. So the application stage’s job\nboilsdowntoiteratingthroughthelistofvisiblemeshinstances,iteratingover\neach submesh-material pair, setting the render state based on the material’s\nspecifications and then calling the low-level primitive submission functions\n(DrawIndexedPrimitive(), glDrawArrays() , or similar).\nState Leaks\nIf we forget to set some aspect of the render state between submitted primi-\ntives,thesettingsusedonthepreviousprimitivewill“leak”overontothenew\nprimitive. A render state leak might manifest itself as an object with the wrong\ntexture or an incorrect lighting effect, for example. Clearly it’s important that\nthe application stage never allow state leaks to occur.\n692 11. The Rendering Engine\nThe GPU Command List\nThe application stage actually communicates with the GPU via a command\nlist. These commands interleave render state settings with references to the\ngeometry that should be drawn. For example, to render objects A and B with\nmaterial 1, followed by objects C, D and E using material 2, the command list\nmight look like this:\n• Set render state for material 1 (multiple commands, one per render state\nsetting).\n• Submit primitive A.\n• Submit primitive B.\n• Set render state for material 2 (multiple commands).\n• Submit primitive C.\n• Submit primitive D.\n• Submit primitive E.\nUnder the hood, API functions like DrawIndexedPrimitive() actually\njust construct and submit GPU command lists. The cost of these API calls\ncan themselves be too high for some applications. To maximize performance,\nsome game engines build GPU command lists manually or by calling a low-\nlevel rendering API like Vulkan (https://www.khronos.org/vulkan/).\n11.2.7.3 Geometry Sorting\nRenderstatesettingsareglobal—theyapplytotheentireGPUasawhole. Soin\norder to change render state settings, the entire GPU pipeline must be flushed\nbefore the new settings can be applied. This can cause massive performance\ndegradation if not managed carefully.\nClearlywe’dliketochangerendersettingsasinfrequentlyaspossible. The\nbest way to accomplish this is to sort our geometry by material. That way, we\ncan install material A’s settings, render all geometry associated with material\nA and then move on to material B.\nUnfortunately, sorting geometry by material can have a detrimental effect\non rendering performance because it increases overdraw—a situation in which\nthe same pixel is filled multiple times by multiple overlapping triangles. Cer-\ntainlysomeoverdrawisnecessaryanddesirable, asitistheonlywaytoprop-\nerly alpha-blend transparent and translucent surfaces into a scene. However,\noverdraw of opaquepixels is always a waste of GPU bandwidth.\nThe early z-test is designed to discard occluded fragments before the ex-\npensivepixelshaderhasachancetoexecute. Buttotakemaximumadvantage\nof early z, we need to draw the triangles in front-to-back order. That way, the\n11.2. The Rendering Pipeline 693\nclosest triangles will fill the z-buffer right off the bat, and all of the fragments\ncoming from more-distant triangles behind them can be quickly discarded,\nwith little or no overdraw.\nz-Prepass to the Rescue\nHow can we reconcile the need to sort geometry by material with the conflict-\ning need to render opaque geometry in a front-to-back order? The answer lies\nin a GPU feature known as z-prepass.\nThe idea behind z-prepass is to render the scene twice: the first time to\ngenerate the contents of the z-buffer as efficiently as possible and the second\ntime to populate the frame buffer with full color information (but this time\nwithnooverdraw, thankstothecontentsofthe z-buffer). TheGPUprovidesa\nspecialdouble-speed renderingmodein which the pixelshaders aredisabled,\nand only the z-buffer is updated. Opaque geometry can be rendered in front-\nto-back order during this phase, to minimize the time required to generate\nthez-buffer contents. Then the geometry can be resorted into material order\nand rendered in full color with minimal state changes for maximum pipeline\nthroughput.\nOnce the opaque geometry has been rendered, transparent surfaces can\nbe drawn in back-to-front order. This brute-force method allows us to achieve\ntheproperalpha-blendedresult. Order-independenttransparency (OIT)isatech-\nnique that permits transparent geometry to be rendered in an arbitrary order.\nIt works by storing multiple fragments per pixel, sorting each pixel’s frag-\nments and blending them only after the entire scene has been rendered. This\ntechnique produces correct results without the need for pre-sorting the geom-\netry,butitcomesatahighmemorycostbecausetheframebuffermustbelarge\nenough to store all of the translucent fragments for each pixel.\n11.2.7.4 Scene Graphs\nModern game worlds can be very large. The majority of the geometry in most\nscenes does not lie within the camera frustum, so frustum culling all of these\nobjects explicitly is usually incredibly wasteful. Instead, we would like to de-\nvise a data structure that manages all of the geometry in the scene and allows\nustoquicklydiscardlargeswathsoftheworldthatarenowherenearthecam-\nera frustum prior to performing detailed frustum culling. Ideally, this data\nstructure should also help us to sort the geometry in the scene, either in front-\nto-back order for the z-prepass or in material order for full-color rendering.\nSuchadatastructureisoftencalleda scenegraph, inreferencetothegraph-\nlike data structures often used by film rendering engines and DCC tools like\nMaya. However,agame’sscenegraphneedn’tactuallybeagraph, andinfact\nthe data structure of choice is usually some kind of tree (which is, of course,\n694 11. The Rendering Engine\na special case of a graph). The basic idea behind most of these data structures\nis to partition three-dimensional space in a way that makes it easy to discard\nregions that do not intersect the frustum, without having to frustum cull all of\nthe individual objects within them. Examples include quadtrees and octrees,\nBSP trees, kd-trees and spatial hashing techniques.\nQuadtrees and Octrees\nAquadtree divides space into quadrants recursively. Each level of recursion is\nrepresented by a node in the quadtree with four children, one for each quad-\nrant. Thequadrantsaretypicallyseparatedbyverticallyoriented,axis-aligned\nplanes, so that the quadrants are square or rectangular. However, some quad-\ntrees subdivide space using arbitrarily shaped regions.\nQuadtreescanbeusedtostoreandorganizevirtuallyanykindofspatially\ndistributeddata. Inthecontextofrenderingengines, quadtreesareoftenused\nto store renderable primitives such as mesh instances, subregions of terrain\ngeometry or individual triangles of a large static mesh, for the purposes of ef-\nficientfrustumculling. Therenderableprimitivesarestoredattheleavesofthe\ntree, and we usually aim to achieve a roughly uniform number of primitives\nwithineachleafregion. Thiscanbeachievedbydecidingwhethertocontinue\nor terminate the subdivision based on the number of primitives within a re-\ngion.\nTo determine which primitives are visible within the camera frustum, we\nwalk the tree from the root to the leaves, checking each region for intersection\nwith the frustum. If a given quadrant does not intersect the frustum, then we\nknowthatnoneofitschildregionswilldosoeither,andwecanstoptraversing\nthat branch of the tree. This allows us to search for potentially visible primi-\ntives much more quickly than would be possible with a linear search (usually\ninO(logn)time). An example of a quadtree subdivision of space is shown in\nFigure 11.50.\nAnoctreeis the three-dimensional equivalent of a quadtree, dividing space\ninto eight subregionsat each level of the recursivesubdivision. The regionsof\nan octree are often cubes or rectangular prisms but can be arbitrarily shaped\nthree-dimensional regions in general.\nBounding Sphere Trees\nIn the same way that a quadtree or octree subdivides space into (usually) rect-\nangular regions, a bounding sphere tree divides space into spherical regions hi-\nerarchically. The leaves of the tree contain the bounding spheres of the ren-\nderable primitives in the scene. We collect these primitives into small logical\n11.2. The Rendering Pipeline 695\nFigure 11.50. A top-down view of a space divided recursively into quadrants for storage in a quad-\ntree, based on the criterion of one point per region.\ngroups and calculate the net bounding sphere of each group. The groups are\nthemselves collected into larger groups, and this process continues until we\nhave a single group with a bounding sphere that encompasses the entire vir-\ntualworld. Togeneratealistofpotentiallyvisibleprimitives,wewalkthetree\nfrom the root to the leaves, testing each bounding sphere against the frustum,\nand only recursing down branches that intersect it.\nBSP Trees\nA binary space partitioning (BSP) tree divides space in half recursively until\nthe objects within each half-space meet some predefined criteria (much as a\nquadtree divides space into quadrants). BSP trees have numerous uses, in-\ncluding collision detection and constructive solid geometry, as well as their\nmost well-known application as a method for increasing the performance of\nfrustum culling and geometry sorting for 3D graphics. A kd-tree is a general-\nization of the BSP tree concept to kdimensions.\nIn the context of rendering, a BSP tree divides space with a single plane at\neach level of the recursion. The dividing planes can be axis-aligned, but more\ncommonly each subdivision corresponds to the plane of a single triangle in\nthe scene. All of the other triangles are then categorized as being either on the\nfrontsideorthebacksideoftheplane. Anytrianglesthatintersectthedividing\nplane are themselves divided into three new triangles, so that every triangle\nlies either entirely in front of or entirely behind the plane, or is coplanar with\nit. The result is a binary tree with a dividing plane and one or more triangles\nat each interior node and triangles at the leaves.\n696 11. The Rendering Engine\nA BSP tree can be used for frustum culling in much the same way a quad-\ntree, octree or bounding sphere tree can. However, when generated with in-\ndividualtrianglesasdescribedabove, aBSPtreecanalsobeusedtosorttrian-\ngles into a strictly back-to-front or front-to-back order. This was particularly\nimportant for early 3D games like Doom, which did not have the benefit of a\nz-buffer and so were forced to use the painter’s algorithm (i.e., to render the\nscene from back to front) to ensure proper inter-triangle occlusion.\nGiven a camera view point in 3D space, a back-to-front sorting algorithm\nwalks the tree from the root. At each node, we check whether the view point\nis in front of or behind that node’s dividing plane. If the camera is in front\nof a node’s plane, we visit the node’s back children first, then draw any tri-\nangles that are coplanar with its dividing plane, and finally we visit its front\nchildren. Likewise, when the camera’s view point is found to be behind a\nnode’s dividing plane, we visit the node’s front children first, then draw the\ntriangles coplanar with the node’s plane and finally we visit its back children.\nThis traversal scheme ensures that the triangles farthest from the camera will\nbe visited before those that are closer to it, and hence it yields a back-to-front\nordering. Because this algorithm traverses allof the triangles in the scene, the\norder of the traversal is independent of the direction the camera is looking. A\nsecondary frustum culling step would be required in order to traverse only\nvisible triangles. A simple BSP tree is shown in Figure 11.51, along with the\ntree traversal that would be done for the camera position shown.\nFull coverage of BSP tree generation and usage algorithms is beyond our\nscope here. See http://www.gamedev.net/reference/articles/article657.asp\nAB\nCD2\nD1Camera Visit A\nCam is in front    Visit B        Leaf node\nDraw B\nDraw A\n    Visit C        Cam is in front\n            Visit D\n1\n                Leaf node\nDraw D1\nDraw C\n            Visit D 2\n                Leaf node\nDraw D2A\nD2 D1C B\nFigure 11.51. An example of back-to-front traversal of the triangles in a BSP tree. The triangles are\nshown edge-on in two dimensions for simplicity, but in a real BSP tree the triangles and dividing\nplanes would be arbitrarily oriented in space.",62228
75-11.3 Advanced Lighting and Global Illumination.pdf,75-11.3 Advanced Lighting and Global Illumination,"11.3. Advanced Lighting and Global Illumination 697\nfor more details on BSP trees.\n11.2.7.5 Choosing a Scene Graph\nClearly there are many different kinds of scene graphs. Which data structure\ntoselectforyourgamewilldependuponthenatureofthescenesyouexpectto\nbe rendering. To make the choice wisely, you must have a clear understand-\ning of what is required—and more importantly what is notrequired—when\nrendering scenes for your particular game.\nFor example, if you’re implementing a fighting game, in which two char-\nacters battle it out in a ring surrounded by a mostly static environment, you\nmay not need much of a scene graph at all. If your game takes place primar-\nily in enclosed indoor environments, a BSP tree or portal system may serve\nyou well. If the action takes place outdoors on relatively flat terrain, and the\nscene is viewed primarily from above (as might be the case in a strategy game\nor god game), a simple quadtree might be all that’s required to achieve high\nrendering speeds. On the other hand, if an outdoor scene is viewed primarily\nfrom the point of view of someone on the ground, we may need additional\nculling mechanisms. Densely populated scenes can benefit from an occlusion\nvolume (antiportal) system, because there will be plenty of occluders. On the\nother hand, if your outdoor scene is very sparse, adding an antiportal system\nprobably won’t pay dividends (and might even hurt your frame rate).\nUltimately, your choice of scene graph should be based on hard data ob-\ntained by actually measuring the performance of your rendering engine. You\nmay be surprised to learn where all your cycles are actually going! But once\nyou know, you can select scene graph data structures and/or other optimiza-\ntions to target the specific problems at hand.\n11.3 Advanced Lighting and Global Illumination\nIn order to render photorealistic scenes, we need physically accurate global\nillumination algorithms. A complete coverage of these techniques is beyond\nourscope. Inthefollowingsections,wewillbrieflyoutlinethemostprevalent\ntechniques in use within the game industry today. Our goal here is to provide\nyouwithanawarenessofthesetechniquesandajumping-offpointforfurther\ninvestigation. For an excellent in-depth coverage of this topic, see [10].\n11.3.1 Image-Based Lighting\nA number of advanced lighting and shading techniques make heavy use of\nimage data, usually in the form of two-dimensional texture maps. These are\n698 11. The Rendering Engine\nFigure 11.52. An example of a normal-mapped surface.\ncalledimage-basedlighting algorithms.\n11.3.1.1 Normal Mapping\nAnormal map specifies a surface normal direction vector at each texel. This\nallows a 3D modeler to provide the rendering engine with a highly detailed\ndescription of a surface’s shape, without having to tessellate the model to a\nhighdegree(aswouldberequiredifthissameinformationweretobeprovided\nvia vertex normals). Using a normal map, a single flat triangle can be made to\nlookasthoughitwereconstructedfrommillionsoftinytriangles. Anexample\nof normal mapping is shown in Figure 11.52.\nThe normal vectors are typically encoded in the RGB color channels of the\ntexture,withasuitablebiastoovercomethefactthatRGBchannelsarestrictly\npositive while normal vector components can be negative. Sometimes only\ntwo coordinates are stored in the texture; the third can be easily calculated at\nruntime, given the assumption that the surface normals are unit vectors.\n11.3.1.2 Heightmaps: Bump, Parallax and Displacement Mapping\nAs its name implies, a heightmap encodes the height of the ideal surface above\nor below the surface of the triangle. Heightmaps are typically encoded as\ngrayscale images, since we only need a single height value per texel. Height-\nmaps can be used for bump mapping, parallax occlusion mapping anddisplace-\nmentmapping—threetechniquesthatcanmakeaplanarsurfaceappeartohave\nheight variation.\nIn bump mapping, a heightmap is used as a cheap way to generate sur-\nface normals. This technique was primarily used in the early days of 3D\ngraphics—nowadays, most game engines store surface normal information\n11.3. Advanced Lighting and Global Illumination 699\nFigure 11.53. Comparison of bump mapping (left), parallax occlusion mapping (center) and displacement mapping (right).\nexplicitly in a normal map, rather than calculating the normals from a height-\nmap.\nParallax occlusion mapping uses the information in a heightmap to arti-\nficially adjust the texture coordinates used when rendering a flat surface, in\nsuch a way as to make the surface appear to contain surface details that move\nsemi-correctly as the camera moves. (This technique was used to produce the\nbullet impact decals in the Uncharted series of games by Naughty Dog.)\nDisplacement mapping (also known as relief mapping) produces real sur-\nfacedetailsbyactuallytessellatingandthenextrudingsurfacepolygons,again\nusing a heightmap to determine how much to displace each vertex. This pro-\nduces the most convincing effect—one that properly self-occludes and self-\nshadows—because real geometry is being generated. Figure 11.53 compares\nbump mapping, parallax mapping and displacement mapping. Figure 11.54\nshows an example of displacement mapping implemented in DirectX 9.\n11.3.1.3 Specular/Gloss Maps\nWhen light reflects directly off a shiny surface, we call this specular reflection.\nThe intensity of a specular reflection depends on the relative angles of the\nviewer, the light source and the surface normal. As we saw in Section 11.1.3.2,\nthe specular intensity takes the form kS(RV)a, where Ris the reflection of\nthe light’s direction vector about the surface normal, Vis the direction to the\nviewer, kSis the overall specular reflectivity of the surface and ais called the\nspecular power.\nMany surfaces aren’t uniformly glossy. For example, when a person’s face\nis sweaty and dirty, wet regions appear shiny, while dry or dirty areas appear\ndull. We can encode high-detail specularity information in a special texture\nmap known as a specular map.\nIf we store the value of kSin the texels of a specular map, we can control\nhow much specular reflection should be applied at each texel. This kind of\nspecular map is sometimes called a gloss map. It is also called a specular mask ,\nbecause zero-valued texels can be used to “mask off” regions of the surface\nwhere we do not want specular reflection applied. If we store the value of\n700 11. The Rendering Engine\nFigure 11.54. DirectX 9 displacement mapping. Simple source geometry is tessellated at runtime to\nproduce the surface details.\nain our specular map, we can control the amount of “focus” our specular\nhighlightswillhaveateachtexel. Thiskindoftextureiscalleda specularpower\nmap. An example of a gloss map is shown in Figure 11.55.\n11.3.1.4 Environment Mapping\nAn environment map looks like a panoramic photograph of the environment\ntaken from the point of view of an object in the scene, covering a full 360 de-\ngrees horizontally and either 180 degrees or 360 degrees vertically. An envi-\nronment map acts like a description of the general lighting environment sur-\nrounding an object. It is generally used to inexpensively render reflections.\nThe two most common formats are spherical environment maps andcubic\nenvironment maps . A spherical map looks like a photograph taken through a\nfisheye lens, and it is treated as though it were mapped onto the inside of a\nsphere whose radius is infinite, centered about the object being rendered. The\nproblem with sphere maps is that they are addressed using spherical coordi-\nnates. Around the equator, there is plenty of resolution both horizontally and\nvertically. However, as the vertical (azimuthal) angle approaches vertical, the\nresolutionofthetexturealongthehorizontal(zenith)axisdecreasestoasingle\n11.3. Advanced Lighting and Global Illumination 701\nFigure 11.55. This screenshot from EA’s Fight Night Round 3 shows how a gloss map can be used to\ncontrol the degree of specular reﬂection that should be applied to each texel of a surface. (See\nColor Plate XX.)\ntexel. Cube maps were devised to avoid this problem.\nA cube map looks like a composite photograph pieced together from pho-\ntos taken in the six primary directions (up, down, left, right, front and back).\nDuring rendering, a cube map is treated as though it were mapped onto the\nsix inner surfaces of a box at infinity, centered on the object being rendered.\nTo read the environment map texel corresponding to a point Pon the sur-\nface of an object, we take the ray from the camera to the point Pand reflect\nit about the surface normal at P. The reflected ray is followed until it inter-\nsects the sphere or cube of the environment map. The value of the texel at this\nintersection point is used when shading the point P.\n11.3.1.5 Three-Dimensional Textures\nModern graphics hardware also includes support for three-dimensional tex-\ntures. A 3D texture can be thought of as a stack of 2D textures. The GPU\nknows how to address and filter a 3D texture, given a three-dimensional tex-\nture coordinate (u,v,w).\nThree-dimensional textures can be useful for describing the appearance or\nvolumetric properties of an object. For example, we could render a marble\nsphere and allow it to be cut by an arbitrary plane. The texture would look\ncontinuous and correct across the cut no matter where it was made, because\nthe texture is well-defined and continuous throughout the entire volume of\nthe sphere.\n702 11. The Rendering Engine\n11.3.2 High Dynamic Range Lighting\nA display device like a television set or CRT monitor can only produce a lim-\nited range of intensities. This is why the color channels in the frame buffer are\nlimited to a zero to one range. But in the real world, light intensities can grow\narbitrarily large. High dynamic range (HDR) lighting attempts to capture this\nwide range of light intensities.\nHDR lighting performs lighting calculations without clamping the result-\ning intensities arbitrarily. The resulting image is stored in a format that per-\nmits intensities to grow beyond one. The net effect is an image in which ex-\ntreme dark and light regions can be represented without loss of detail within\neither type of region.\nPrior to display on-screen, a process called tone mapping is used to shift\nand scale the image’s intensity range into the range supported by the display\ndevice. Doingthispermitstherenderingenginetoreproducemanyreal-world\nvisual effects, like the temporary blindness that occurs when you walk from\na dark room into a brightly lit area, or the way light seems to bleed out from\nbehind a brightly back-lit object (an effect known as bloom).\nOne way to represent an HDR image is to store the R, G and B channels\nusing 32-bit floating-point numbers, instead of 8-bit integers. Another alter-\nnative is to employ an entirely different color model altogether. The log-LUV\ncolor model is a popular choice for HDR lighting. In this model, color is rep-\nresentedas an intensity channel ( L) and two chromaticitychannels ( UandV).\nBecause the human eye is more sensitive to changes in intensity than it is to\nchanges in chromaticity, the Lchannel is stored in 16 bits while UandVare\ngiven only eight bits each. In addition, Lis represented using a logarithmic\nscale (base two) in order to capture a very wide range of light intensities.\n11.3.3 Global Illumination\nAs we noted in Section 11.1.3.1, global illumination refers to a class of light-\ningalgorithmsthataccountforlight’sinteractionswithmultipleobjectsinthe\nscene, on its way from the light source to the virtual camera. Global illumina-\ntionaccountsforeffectsliketheshadowsthatarisewhenonesurfaceoccludes\nanother, reflections, caustics and the way the color of one object can “bleed”\nonto the objects around it. In the following sections, we’ll take a brief look\nat some of the most common global illumination techniques. Some of these\nmethods aim to reproduce a single isolated effect, like shadows or reflections.\nOthers like radiosity and ray tracing methods aim to provide a holistic model\nof global light transport.\n11.3. Advanced Lighting and Global Illumination 703\n11.3.3.1 Shadow Rendering\nShadows are created when a surface blocks light’s path. The shadows caused\nby an ideal point light source would be sharp, but in the real world shadows\nhave blurry edges; this is called the penumbra. A penumbra arises because\nreal-world light sources cover some area and so produce light rays that graze\nthe edges of an object at different angles.\nThe two most prevalent shadow rendering techniques are shadow volumes\nandshadow maps. We’ll briefly describe each in the sections below. In both\ntechniques, objects in the scene are generally divided into three categories:\nobjects that cast shadows, objects that are to receive shadows and objects that\nare entirely excluded from consideration when rendering shadows. Likewise,\nthelightsaretaggedtoindicatewhetherornottheyshouldgenerateshadows.\nThis important optimization limits the number of light-object combinations\nthat need to be processed in order to produce the shadows in a scene.\nShadow Volumes\nIn the shadow volume technique, each shadow caster is viewed from the van-\ntage point of a shadow-generating light source, and the shadow caster’s sil-\nhouette edges are identified. These edges are extruded in the direction of the\nlight rays emanating from the light source. The result is a new piece of geom-\netry that describes the volume of space in which the light is occluded by the\nshadow caster in question. This is shown in Figure 11.56.\nFigure 11.56. A shadow volume generated by extruding the silhouette edges of a shadow casting\nobject as seen from the point of view of the light source.\n704 11. The Rendering Engine\nAshadowvolumeisusedtogenerateashadowbymakinguseofaspecial\nfull-screenbufferknownasthestencilbuffer. Thisbufferstoresasingleinteger\nvalue corresponding to each pixel of the screen. Rendering can be masked by\nthe values in the stencil buffer—for example, we could configure the GPU to\nonly render fragments whose corresponding stencil values are nonzero. In\naddition, the GPU can be configured so that rendered geometry updates the\nvalues in the stencil buffer in various useful ways.\nTo render shadows, the scene is first drawn to generate an unshadowed\nimage in the frame buffer, along with an accurate z-buffer. The stencil buffer\nis cleared so that it contains zeros at every pixel. Each shadow volume is then\nrendered from the point of view of the camera in such a way that front-facing\ntriangles increase the values in the stencil buffer by one, while back-facing tri-\nanglesdecreasethembyone. Inareasofthescreenwheretheshadowvolume\ndoesnotappearatall,ofcoursethestencilbuffer’spixelswillbeleftcontaining\nzero. The stencil buffer will also contain zeros where both the front and back\nfacesoftheshadowvolumearevisible, becausethefrontfacewillincreasethe\nstencil value but the back face will decrease it again. In areas where the back\nface of the shadow volume has been occluded by “real” scene geometry, the\nstencilvaluewillbeone. Thistellsuswhichpixelsofthescreenareinshadow.\nSo we can render shadows in a third pass, by simply darkening those regions\nof the screen that contain a nonzero stencil buffer value.\nShadow Maps\nThe shadow mapping technique is effectively a per-fragment depth test per-\nformed from the point of view of the light instead of from the point of view of\nthe camera. The scene is rendered in two steps: First, a shadow map texture is\ngeneratedbyrenderingthescenefromthepointofviewofthelightsourceand\nsaving off the contents of the depth buffer. Second, the scene is rendered as\nusual,andtheshadowmapisusedtodeterminewhetherornoteachfragment\nis in shadow. At each fragment in the scene, the shadow map tells us whether\nor not the light is being occluded by some geometry that is closer to the light\nsource, in just the same way that the z-buffer tells us whether a fragment is\nbeing occluded by a triangle that is closer to the camera.\nA shadow map contains only depth information—each texel records how\nfar away it is from the light source. Shadow maps are therefore typically ren-\nderedusingthehardware’sdouble-speed z-onlymode(sinceallwecareabout\nis the depth information). For a point light source, a perspective projection is\nusedwhenrenderingtheshadowmap; foradirectionallightsource,anortho-\ngraphic projection is used instead.\n11.3. Advanced Lighting and Global Illumination 705\nFigure 11.57. The far left image is a shadow map—the contents of the z-buffer as rendered from\nthe point of view of a particular light source. The pixels of the center image are black where the\nlight-space depth test failed (fragment in shadow) and white where it succeeded (fragment not in\nshadow). The far right image shows the ﬁnal scene rendered with shadows.\nTorenderasceneusingashadowmap,wedrawthesceneasusualfromthe\npoint of view of the camera. For each vertex of every triangle, we calculate its\npositionin lightspace—i.e., inthesame“viewspace”thatwasusedwhengen-\nerating the shadow map in the first place. These light-space coordinates can\nbe interpolated across the triangle, just like any other vertex attribute. This\ngives us the position of each fragment in light space . To determine whether\na given fragment is in shadow or not, we convert the fragment’s light-space\n(x,y)-coordinates into texture coordinates (u,v)within the shadow map. We\nthen compare the fragment’s light-space z-coordinate with the depth stored\nat the corresponding texel in the shadow depth map. If the fragment’s light-\nspace zis farther away from the light than the texel in the shadow map, then\nit must be occluded by some other piece of geometry that is closer to the light\nsource—hence it is in shadow. Likewise, if the fragment’s light-space zis\ncloser to the light source than the texel in the shadow map, then it is not oc-\ncluded and is not in shadow. Based on this information, the fragment’s color\ncan be adjusted accordingly. The shadow mapping process is illustrated in\nFigure 11.57.\n11.3.3.2 Ambient Occlusion\nAmbient occlusion is a technique for modeling contact shadows—the soft shad-\nowsthatarisewhenasceneisilluminatedbyonlyambientlight. Ineffect,am-\nbient occlusion describes how “accessible” each point on a surface is to light\nin general. For example, the interior of a section of pipe is less accessible to\nambient light than its exterior. If the pipe were placed outside on an overcast\nday, its interior would generally appear darker than its exterior.\nFigure 11.58 shows how ambient occlusion produces shadows underneath\na car and in its wheel wells, as well as within the seams between body pan-\nels. Ambient occlusion is measured at a point on a surface by constructing\n706 11. The Rendering Engine\nFigure 11.58. A car rendered with ambient occlusion. Notice the darkened areas underneath the\nvehicle and in the wheel wells.\na hemisphere with a very large radius centered on that point and determing\nwhatpercentageofthathemisphere’sareaisvisiblefromthepointinquestion.\nIt can be precomputed offline for static objects, because ambient occlusion is\nindependentofviewdirectionandthedirectionofincidentlight. Itistypically\nstoredinatexturemapthatrecordsthelevelofambientocclusionateachtexel\nacross the surface.\n11.3.3.3 Reﬂections\nReflectionsoccurwhenlightbouncesoffahighlyspecular(shiny)surfacepro-\nducinganimageofanotherportionofthesceneinthesurface. Reflectionscan\nbeimplementedinanumberofways. Environmentmapsareusedtoproduce\ngeneral reflections of the surrounding environment on the surfaces of shiny\nobjects. Direct reflections in flat surfaces like mirrors can be produced by re-\nflectingthecamera’spositionabouttheplaneofthereflectivesurfaceandthen\nrenderingthescenefromthatreflectedpointofviewintoatexture. Thetexture\nis then applied to the reflective surface in a second pass (see Figure 11.59).\n11.3.3.4 Caustics\nCaustics are the bright specular highlights arising from intense reflections or\nrefractions from very shiny surfaces like water or polished metal. When the\nreflective surface moves, as is the case for water, the caustic effects glimmer\nand “swim” across the surfaces on which they fall. Caustic effects can be pro-\nduced by projecting a (possibly animated) texture containing semi-random\nbright highlights onto the affected surfaces. An example of this technique is\nshown in Figure 11.60.\n11.3. Advanced Lighting and Global Illumination 707\nFigure 11.59. Mirror reﬂections in The Last of Us: Remastered (© 2014/™ SIE. Created and devel-\noped by Naughty Dog, PlayStation 4) implemented by rendering the scene to a texture that is\nsubsequently applied to the mirror’s surface. (See Color Plate XXI.)\nFigure 11.60. Water caustics produced by projecting an animated texture onto the affected sur-\nfaces.\n11.3.3.5 Subsurface Scattering\nWhen light enters a surface at one point, is scattered beneath the surface, and\nthen reemerges at a different point on the surface, we call this subsurface scat-\ntering. This phenomenon is responsible for the “warm glow” of human skin,\nwax and marble statues (e.g., Figure 11.61). Subsurface scattering is described\nby a more-advanced variant of the BRDF (see Section 11.1.3.2) known as the\nBSSRDF (bidirectionalsurface scattering reflectancedistribution function).\nSubsurface scattering can be simulated in a number of ways. Depth-map–\nbased subsurface scattering renders a shadow map (see Section 11.3.3.1), but\ninstead of using it to determine which pixels are in shadow, it is used to mea-\nsure how far a beam of light would have to travel in order to pass all the\n708 11. The Rendering Engine\nFigure 11.61. On the left, a dragon rendered without subsurface scattering (i.e., using a BRDF lighting\nmodel). On the right, the same dragon rendered with subsurface scattering (i.e., using a BSSRDF\nmodel). Images rendered by Rui Wang at the University of Virginia.\nway through the occluding object. The shadowed side of the object is then\ngiven an artificial diffuse lighting term whose intensity is inversely propor-\ntional to the distance the light had to travel in order to emerge on the oppo-\nsite side of the object. This causes objects to appear to be glowing slightly\non the side opposite to the light source but only where the object is rela-\ntively thin. For more information on subsurface scattering techniques, see\nhttp://http.developer.nvidia.com/GPUGems/gpugems_ch16.html.\n11.3.3.6 Precomputed Radiance Transfer (PRT)\nPrecomputedradiancetransfer (PRT)isapopulartechniquethatattemptstosim-\nulate the effects of radiosity-based rendering methods in real time. It does so\nby precomputing and storing a complete description of how an incident light\nraywouldinteractwithasurface(reflect, refract, scatter, etc.)whenapproach-\ning from every possible direction. At runtime, the response to a particular\nincident light ray can be looked up and quickly converted into very accurate\nlighting results.\nIn general the light’s response at a point on the surface is a complex func-\ntion defined on a hemisphere centered about the point. A compact repre-\nsentation of this function is required to make the PRT technique practical. A\ncommon approach is to approximate the function as a linear combination of\nspherical harmonic basis functions. This is essentially the three-dimensional\nequivalent of encoding a simple scalar function f(x)as a linear combination\nof shifted and scaled sine waves.\nThe details of PRT are far beyond our scope. For more information,\nsee http://web4.cs.ucl.ac.uk/staff/j.kautz/publications/prtSIG02.pdf. PRT\nlightingtechniquesaredemonstratedinaDirectXsampleprogramavailablein\nthe DirectX SDK—see http://msdn.microsoft.com/en-us/library/bb147287.\naspx for more details.\n11.3. Advanced Lighting and Global Illumination 709\n11.3.4 Deferred Rendering\nIn traditional triangle-rasterization–based rendering, all lighting and shading\ncalculations are performed on the triangle fragments in world space, view\nspace or tangent space. The problem with this technique is that it is inher-\nently inefficient. For one thing, we potentially do work that we don’t need\nto do. We shade the vertices of triangles, only to discover during the rasteri-\nzation stage that the entire triangle is being depth-culled by the z-test. Early\nz-testshelpeliminateunnecessarypixelshaderevaluations,buteventhisisn’t\nperfect. What’s more, in order to handle a complex scene with lots of lights,\nwe end up with a proliferation of different versions of our vertex and pixel\nshaders—versions that handle different numbers of lights, different types of\nlights, different numbers of skinning weights, etc.\nDeferred rendering is an alternative way to shade a scene that addresses\nmany of these problems. In deferred rendering, the majority of the lighting\ncalculations are done in screen space, not view space. We efficiently render\nthe scene without worrying about lighting. During this phase, we storeall the\ninformation we’re going to need to light the pixels in a “deep” frame buffer\nknown as the G-buffer. Once the scene has been fully rendered, we use the\ninformation in the G-buffer to perform our lighting and shading calculations.\nThis is usually much more efficient than view-space lighting, avoids the pro-\nliferation of shader variants and permits some very pleasing effects to be ren-\ndered relatively easily.\nThe G-buffer may be physically implemented as a collection of buffers,\nbut conceptually it is a single frame buffer containing a rich set of informa-\ntion about the lighting and surface properties of the objects in the scene at\nevery pixel on the screen. A typical G-buffer might contain the following per-\npixel attributes: depth, surface normal in view space or world space, diffuse\ncolor, specular power, even precomputed radiance transfer (PRT) coefficients.\nThe following sequence of screenshots from Guerrilla Games’ Killzone 2 (Fig-\nure 11.62) shows some of the typical components of the G-buffer.\nAn in-depth discussion of deferred rendering is beyond our scope, but the\nfolks atGuerrilla Games have preparedan excellent presentationon the topic,\nwhich is available at http://www.slideshare.net/guerrillagames/deferred\n-rendering-in-killzone-2-9691589.\n11.3.5 Physically Based Shading\nTraditional game lighting engines have required artists and lighters to tweak\na wide variety of sometimes non-intuitive parameters, across numerous dis-\nparaterenderingenginesystems,inordertoachieveadesired“look”in-game.",26791
76-11.4 Visual Effects and Overlays.pdf,76-11.4 Visual Effects and Overlays,"710 11. The Rendering Engine\nFigure 11.62. Screenshots from Killzone 2 by Guerrilla Games, showing some of the typical com-\nponents of the G-buffer used in deferred rendering. The upper image shows the ﬁnal rendered\nimage. Below it, clockwise from the upper left, are the albedo (diffuse) color, depth, view-space\nnormal, screen-space 2D motion vector (for motion blurring), specular power and specular inten-\nsity. (See Color Plate XXII.)\nThis can be an arduous and time-consuming process. What’s worse, parame-\ntersettingsthatworkwellunderonesetoflightingconditionsmightnotwork\nwellunderotherlightingscenarios. Toaddresstheseproblems,renderingpro-\ngrammers are turning toward physically based shading models.\nA physically based shading model attempts to approximate the ways in\nwhich light travels and interacts with materials in the real world, allowing\nartists and lighters to tweak shader parameters using intuitive, real-world\nquantities measured in real-world units. A complete discussion of physically\nbasedshadingisbeyondthescopeofthisbook,butyoucanstarttolearnmore\nabout it here: https://www.marmoset.co/toolbag/learn/pbr-theory.\n11.4 Visual Effects and Overlays\nThe rendering pipeline we’ve discussed to this point is responsible primarily\nfor rendering three-dimensional solid objects. A number of specialized ren-\ndering systems are typically layered on top of this pipeline, responsible for\nrendering visual elements like particle effects, decals (small geometry over-\nlays that represent bullet holes, cracks, scratches and other surface details),\nhair and fur, rain or falling snow, water and other specialized visual effects.\nFull-screen post effects may be applied, including vignette (a reduction of\nbrightness and saturation around the edges of the screen), motion blur, depth\n11.4. Visual Effects and Overlays 711\noffieldblurring, artificial/enhancedcolorization, andthelistgoeson. Finally,\nthe game’s menu system and heads-up display (HUD) are typically realized\nbyrenderingtextandothertwo-orthree-dimensionalgraphicsinscreenspace\noverlaid on top of the three-dimensional scene.\nAn in-depth coverage of these engine systems is beyond our scope. In the\nfollowing sections, we’ll provide a brief overview of these rendering systems,\nand point you in the direction of additional information.\n11.4.1 Particle Effects\nA particle rendering system is concerned with rendering amorphous objects\nlike clouds of smoke, sparks, flame and so on. These are called particle effects.\nThe key features that differentiate a particle effect from other kinds of render-\nable geometry are as follows:\n• It is composed of a very large number ofrelatively simple pieces of geom-\netry—most often simple cards called quads, composed of two triangles\neach.\n• The geometry is often camera-facing (i.e., billboarded), meaning that the\nengine must take steps to ensure that the face normals of each quad al-\nways point directly at the camera’s focal point.\n• Its materials are almost always semitransparent ortranslucent. As such,\nparticle effects have some stringent rendering order constraints that do\nnot apply to the majority of opaque objects in a scene.\n• Particles animate in a rich variety of ways. Their positions, orientations,\nsizes (scales), texture coordinates and many of their shader parameters\nvary from frame to frame. These changes are defined either by hand-\nauthored animation curves or via procedural methods.\n• Particlesaretypically spawnedandkilled continually. Aparticleemitteris\na logical entity in the world that creates particles at some user-specified\nrate;particlesarekilledwhentheyhitapredefineddeathplane,orwhen\nthey have lived for a user-defined length of time, or as decided by some\nother user-specified criteria.\nParticle effects could be rendered using regular triangle mesh geometry\nwith appropriate shaders. However, because of the unique characteristics\nlisted above, a specialized particle effect animation and rendering system is\nalways used to implement them in a real production game engine. A few ex-\nample particle effects are shown in Figure 11.63.\n712 11. The Rendering Engine\nFigure 11.63. Flame, smoke and bullet tracer particle effects in Uncharted 3: Drake’s Deception\n(© 2011/™ SIE. Created and developed by Naughty Dog, PlayStation 3). (See Color Plate XXIII.)\nParticlesystemdesignandimplementationisarichtopicthatcouldoccupy\nmanychaptersallonitsown. Formoreinformationonparticlesystems,see[2,\nSection 10.7], [16, Section 20.5], [11, Section 13.7] and [12, Section 4.1.2].\n11.4.2 Decals\nAdecalis a relatively small piece of geometry that is overlaid on top of the\nregulargeometryinthescene,allowingthevisualappearanceofthesurfaceto\nbemodifieddynamically. Examplesincludebulletholes,footprints,scratches,\ncracks, etc.\nThe approach most often used by modern engines is to model a decal as a\nrectangularareathatistobeprojectedalongarayintothescene. Thisgivesrise\nto a rectangular prism in 3D space. Whatever surface the prism intersects first\nbecomesthesurfaceofthedecal. Thetrianglesoftheintersectedgeometryare\nextractedandclippedagainstthefourboundingplanesofthedecal’sprojected\nprism. Theresultingtrianglesaretexture-mappedwithadesireddecaltexture\nby generating appropriate texture coordinates for each vertex. These texture-\nmapped triangles are then rendered over the top of the regular scene, often\nusing parallax mapping to give them the illusion of depth and with a slight\nz-bias (usually implemented by shifting the near plane slightly) so they don’t\nexperience z-fightingwiththegeometryonwhichtheyareoverlaid. Theresult\nistheappearanceofabullethole,scratchorotherkindofsurfacemodification.\nSome bullet-hole decals are depicted in Figure 11.64.\nFor more information on creating and rendering decals, see [9, Section 4.8]\nand [32, Section 9.2].\n11.4. Visual Effects and Overlays 713\nFigure 11.64. Parallax-mapped decals from Uncharted 3: Drake’s Deception (© 2011/™ SIE. Created\nand developed by Naughty Dog, PlayStation 3). (See Color Plate XXIV.)\n11.4.3 Environmental Effects\nAny game that takes place in a somewhat natural or realistic environment re-\nquiressomekindofenvironmentalrenderingeffects. Theseeffectsareusually\nimplementedviaspecializedrenderingsystems. We’lltakeabrieflookatafew\nof the more common of these systems in the following sections.\n11.4.3.1 Skies\nTheskyinagameworldneedstocontainvividdetail,yettechnicallyspeaking\nitliesanextremelylongdistanceawayfromthecamera. Therefore, wecannot\nmodel it as it really is and must turn instead to various specialized rendering\ntechniques.\nOne simple approach is to fill the frame buffer with the sky texture prior\nto rendering any 3D geometry. The sky texture should be rendered at an ap-\nproximate 1:1 texel-to-pixel ratio, so that the texture is roughly or exactly the\nresolution of the screen. The sky texture can be rotated and scrolled to corre-\nspond to the motions of the camera in-game. During rendering of the sky, we\nmake sure to set the depth of all pixels in the frame buffer to the maximum\npossibledepthvalue. Thisensuresthatthe3Dsceneelementswillalwayssort\non top of the sky. The arcade hit Hydro Thunder rendered its skies in exactly\nthis manner.\nOn modern game platforms, where pixel shading costs can be high, sky\nrendering is often done afterthe rest of the scene has been rendered. First\nthez-buffer is cleared to the maximum z-value. Then the scene is rendered.\n714 11. The Rendering Engine\nFinally the sky is rendered, with z-testing enabled, zwriting turned off, and\nusing a z-test value that is one less than the maximum. This causes the sky to\nbedrawnonlywhereitisnotoccludedbycloserobjectsliketerrain, buildings\nand trees. Drawing the sky last ensures that its pixel shader is run for the\nminimum possible number of screen pixels.\nFor games in which the player can look in any direction, we can use a sky\ndomeorsky box. The dome or box is rendered with its center always at the\ncamera’s current location, so that it appears to lie at infinity, no matter where\nthe camera moves in the game world. As with the sky texture approach, the\nskyboxordomeisrenderedbeforeanyother3Dgeometry,andallofthepixels\nin the frame buffer are set to the maximum z-value when the sky is rendered.\nThis means that the dome or box can actually be tiny, relative to other objects\nin the scene. Its size is irrelevant, as long as it fills the entire frame buffer\nwhen it is drawn. For more information on sky rendering, see [2, Section 10.3]\nand [44, page 253].\nClouds are often implemented with a specialized rendering and anima-\ntion system as well. In early games like DoomandQuake, the clouds were\njust planes with scrolling semitransparent cloud textures on them. More-\nrecent cloud techniques include camera-facing cards (billboards), particle-\neffect based clouds and volumetric cloud effects.\n11.4.3.2 Terrain\nThe goal of a terrain system is to model the surface of the earth and provide a\ncanvas of sorts upon which other static and dynamic elements can be laid out.\nTerrain is sometimes modeled explicitly in a package like Maya. But if the\nplayer can see far into the distance, we usually want some kind of dynamic\ntessellation or other level of detail (LOD) system. We may also need to limit\nthe amount of data required to represent very large outdoor areas.\nHeight field terrain is one popular choice for modeling large terrain areas.\nThe data size can be kept relatively small because a height field is typically\nstoredinagrayscaletexturemap. Inmostheight-field–basedterrainsystems,\nthe horizontal ( y=0) plane is tessellated in a regular grid pattern, and the\nheightsofthe terrainverticesaredeterminedbysamplingtheheightfieldtex-\nture. The number of triangles per unit area can be varied based on distance\nfrom the camera, thereby allowing large-scale features to be seen in the dis-\ntance, while still permitting a good deal of detail to be represented for nearby\nterrain. An example of a terrain defined via a height field bitmap is shown in\nFigure 11.65.\nTerrainsystemsusuallyprovidespecializedtoolsfor“painting”theheight\nfield itself, carving out terrain features like roads, rivers and so on. Texture\n11.4. Visual Effects and Overlays 715\nFigure 11.65. A grayscale height ﬁeld bitmap (left) can be used to control the vertical positions of\nthe vertices in a terrain grid mesh (right). In this example, a water plane intersects the terrain mesh\nto create islands.\nmapping in a terrain system is often a blend between four or more textures.\nThis allows artists to “paint” in grass, dirt, gravel and other terrain features\nby simply exposing one of the texture layers. The layers can be cross-blended\nfromonetoanothertoprovidesmoothtexturaltransitions. Someterraintools\nalso permit sections of the terrain to be cut out to permit buildings, trenches\nandotherspecializedterrainfeaturestobeinsertedintheformofregularmesh\ngeometry. Terrain authoring tools are sometimes integrated directly into the\ngame world editor, while in other engines they may be stand-alone tools.\nOf course, height field terrain is just one of many options for modeling the\nsurface of the Earth in a game. For more information on terrain rendering,\nsee [8, Sections 4.16 through 4.19] and [9, Section 4.2].\n11.4.3.3 Water\nWater renderers are commonplace in games nowadays. There are lots of dif-\nferent kinds of water, including oceans, pools, rivers, waterfalls, fountains,\njets, puddles and damp solid surfaces. Each type of water generally requires\nsome specialized rendering technology. Some also require dynamic motion\nsimulations. Large bodies of water may require dynamic tessellation or other\nLOD methodologies similar to those employed in a terrain system.\nWater systems sometimes interact with a game’s rigid body dynamics sys-\ntem (flotation, force from water jets, etc.) and with gameplay (slippery sur-\nfaces, swimming mechanics, diving mechanics, riding vertical jets of water\nand so on). Water effects are often created by combining disparate render-\ning technologies and subsystems. For example, a waterfall might make use\n716 11. The Rendering Engine\nof specialized water shaders, scrolling textures, particle effects for mist at the\nbase, a decal-like overlay for foam, and the list goes on. Today’s games offer\nsome pretty amazing water effects, and active research into technologies like\nreal-time fluid dynamics promises to make water simulations even richer and\nmore realistic in the years ahead. For more information on water rendering\nand simulation techniques, see [2, Sections 9.3, 9.5 and 9.6], [15] and [8, Sec-\ntions 2.6 and 5.11].\n11.4.4 Overlays\nMost games have heads-up displays, in-game graphical user interfaces and\nmenu systems. These overlays are typically comprised of two- and three-\ndimensional graphics rendered directly in view space or screen space.\nOverlaysaregenerallyrenderedaftertheprimaryscene,with z-testingdis-\nabled to ensure that they appear on top of the three-dimensional scene. Two-\ndimensional overlays are typically implemented by rendering quads (triangle\npairs) in screen space using an orthographic projection. Three-dimensional\noverlays may be rendered using an orthographic projection or via the regular\nperspective projection with the geometry positioned in view space so that it\nfollows the camera around.\n11.4.4.1 Text and Fonts\nAgameengine’stext/fontsystemistypicallyimplementedasaspecialkindof\ntwo-dimensional (or sometimes three-dimensional) overlay. At its core, a text\nrendering system needs to be capable of displaying a sequence of character\nglyphs corresponding to a text string, arranged in various orientations on the\nscreen.\nAfontisoftenimplementedviaatexturemapknownasa glyphatlas,which\ncontains the various required glyphs. This texture typically consists of a sin-\ngle alpha channel—the value at each pixel representing the percentage of that\npixelthatiscoveredbytheinteriorofaglyph. Afontdescriptionfileprovides\ninformation such as the bounding boxes of each glyph within the texture, and\nfont layout information such as kerning, baseline offsets and so on. A glyph\nis rendered by drawing a quad whose (u,v)coordinates correspond to the\nbounding box of the desired glyph within the atlas texture map. The texture\nmapprovidesthealphavalue,whilethecolorisspecifiedseparately,allowing\nglyphs of any color to be rendered from the same atlas.\nAnother option for font rendering is to make use of a font library like\nFreeType (https://www.freetype.org/). The FreeType library enables a game\nor other application to read fonts in a wide variety of formats, including\n11.4. Visual Effects and Overlays 717\nTrueType (TTF) and OpenType (OTF), and to render glyphs into in-memory\npixmaps at any desired point size. FreeType renders each glyph using its\nBezier curve outlines, so it produces very accurate results.\nTypicallyareal-timeapplicationlikeagamewilluseFreeTypetoprerender\nthenecessaryglyphsintoanatlas,whichisinturnusedasatexturemaptoren-\nder glyphs as simple quads every frame. However, by embedding FreeType\nor a similar library in your engine, it’s possible to render some glyphs into the\natlas on the fly, on an as-needed basis. This can be useful when rendering text\nin a language with a very large number of possible glyphs, like Chinese or\nKorean.\nYet another way to render high-quality character glyphs is to use signed\ndistance fields to describe the glyphs. In this approach, glyphs are rendered to\npixmaps (as they would be with a library like FreeType), but the value at each\npixel is no longer an alpha “coverage” value. Instead, each pixel contains a\nsigned distance from that pixel center to the nearest edge of the glyph. Inside\nthe glyph, the distances are negative; outside the glyph’s outlines, they are\npositive. When rendering a glyph from a signed distance field texture atlas,\nthe distances are used in the pixel shader to calculate highly accurate alpha\nvalues. The net result is text that looks smooth at any distance or viewing an-\ngle. Youcanreadmoreaboutsigneddistancefieldtextrenderingbysearching\nonline for Konstantin Käfer’s article entitled “Drawing Text with Signed Dis-\ntance Fields in Mapbox GL,” or the article written by Chris Green of Valve\nentitled “Improved Alpha-Tested Magnification for Vector Textures and Spe-\ncial Effects.”\nGlyphscanalsoberendereddirectlyfromtheBéziercurveoutlinesthatde-\nfinethem. TheSlugfontrenderinglibrarybyTerathonSoftwareLLCperforms\nits outline-based glyph rendering on the GPU, thereby making this technique\npractical for use in a real-time game application.\nA good text/font system must account for the differences in character sets\nand reading directions inherent in various languages. Laying out the charac-\nters ina text string is a processknown as shaping thestring. The characters are\nlaidoutfromlefttorightorrighttoleft,dependingonthelanguage,witheach\ncharacter aligned to a common baseline. The spacing between characters is de-\ntermined in part by metrics provided by the creator of the font (and stored in\nthe font file), and partly by kerning rules that dictate contextual intercharacter\nspacing adjustments.\nSome text systems also provide various fun features like the ability to an-\nimate characters across the screen in various ways, the ability to animate in-\ndividual characters and so on. However, it’s important to remember when\nimplementing a game font system that only those features that are actually re-\n718 11. The Rendering Engine\nFigure 11.66. The effect of a CRT’s gamma response on im-\nage quality and how the effect can be corrected for. Image\ncourtesy of www.wikipedia.org.\nFigure 11.67. Gamma encoding and decoding curves. Image\ncourtesy of www.wikipedia.org.\nquiredby the game should be implemented. There’s no point in furnishing\nyour engine with an advanced text animation if your game never needs to\ndisplay animated text, for example.\n11.4.5 Gamma Correction\nCRT monitors tend to have a nonlinear response to luminance values. That\nis, if a linearly increasing ramp of R, G or B values were to be sent to a CRT,\nthe image that would result on-screen would be perceptually nonlinear to the\nhuman eye. Visually, the dark regions of the image would look darker than\nthey should. This is illustrated in Figure 11.66.\nThe gamma response curve of a typical CRT display can be modeled quite\nsimply by the formula\nVout=Vg\nin\nwhere gCRT>1. Tocorrectforthiseffect,thecolorssenttotheCRTdisplayare\nusually passed through an inverse transformation (i.e., using a gamma value\ngcorr<1). Thevalueof gCRTforatypicalCRTmonitoris2.2, sothecorrection\nvalue is usually gcorr1/2.2 =0.455. These gamma encoding and decoding\ncurves are shown in Figure 11.67.\nGamma encoding can be performed by the 3D rendering engine to ensure\nthatthevaluesinthefinalimageareproperlygamma-corrected. Oneproblem\nthat is encountered, however, is that the bitmap images used to represent tex-\nture maps are often gamma-corrected themselves. A high-quality rendering\nengine takes this fact into account, by gamma-decoding the textures prior to",19265
77-11.5 Further Reading.pdf,77-11.5 Further Reading,"11.5. Further Reading 719\nrendering and then re-encoding the gamma of the final rendered scene so that\nits colors can be reproduced properly on-screen.\n11.4.6 Full-Screen Post Effects\nFull-screen posteffects areeffectsappliedtoarenderedthree-dimensionalscene\nthat provide additional realism or a stylized look. These effects are often im-\nplemented by passing the entire contents of the screen through a pixel shader\nthat applies the desired effect(s). This can be accomplished by rendering a\nfull-screenquadthathasbeenmappedwithatexturecontainingtheunfiltered\nscene. A few examples of full-screen post effects are given below:\n•Motion blur. This is typically implemented by rendering a buffer of\nscreen-space velocity vectors and using this vector field to selectively\nblur the rendered image. Blurring is accomplished by passing a convo-\nlution kernel over the image (see “Image Smoothing and Sharpening by\nDiscrete Convolution” by Dale A. Schumacher, published in [5], for de-\ntails).\n•Depthoffieldblur . This blur effect can be produced by using the contents\nof the depth buffer to adjust the degree of blur applied at each pixel.\n•Vignette. In this filmic effect, the brightness or saturation of the image is\nreduced at the corners of the screen for dramatic effect. It is sometimes\nimplementedbyliterallyrenderingatextureoverlayontopofthescreen.\nA variation on this effect is used to produce the classic circular effect\nused to indicate that the player is looking through a pair of binoculars\nor a weapon scope.\n•Colorization . The colors of screen pixels can be altered in arbitrary ways\nas a post-processing effect. For example, all colors except red could be\ndesaturated to grey to produce a striking effect similar to the famous\nscene of the little girl in the red coat from Schindler’sList .\n11.5 Further Reading\nWe’ve covered a lot of material in a very short space in this chapter, but we’ve\nonlyjustscratchedthesurface. Nodoubtyou’llwanttoexploremanyofthese\ntopicsinmuchgreaterdetail. Foranexcellentoverviewoftheentireprocessof\ncreating three-dimensional computer graphics and animation for games and\nfilm, I highly recommend [27]. The technology that underlies modern real-\ntime rendering is covered in excellent depth in [2], while [16] is well known\nas the definitive reference guide to all things related to computer graphics.\n720 11. The Rendering Engine\nOther great books on 3D rendering include [49], [11] and [12]. The mathemat-\nics of 3D rendering is covered very well in [32]. No graphics programmer’s\nlibrary would be complete without one or more books from the GraphicsGems\nseries ([20], [5], [28], [22] and [42]) and/or the GPU Gems series ([15], [44]\nand [40]). Of course, this short reference list is only the beginning—you will\nundoubtedly encounter a great many more excellent books on rendering and\nshaders over the course of your career as a game programmer.",2934
78-12 Animation Systems.pdf,78-12 Animation Systems,,0
79-12.1 Types of Character Animation.pdf,79-12.1 Types of Character Animation,"12\nAnimation Systems\nThemajorityofmodern3Dgamesrevolvearound characters —oftenhuman\nor humanoid, sometimes animal or alien. Characters are unique because\nthey need to move in a fluid, organic way. This poses a host of new tech-\nnical challenges, over and above what is required to simulate and animate\nrigid objects like vehicles, projectiles, soccer balls and Tetris pieces. The task\nof imbuing characters with natural-looking motion is handled by an engine\ncomponent known as the characteranimation system.\nAs we’ll see, an animation system gives game designers a powerful suite\nof tools that can be applied to non-characters as well as characters. Any game\nobject that is not 100% rigid can take advantage of the animation system. So\nwhenever you see a vehicle with moving parts, a piece of articulated machin-\nery,treeswavinggentlyinthebreezeorevenanexplodingbuildinginagame,\nchancesaregoodthattheobjectmakesatleastpartialuseofthegameengine’s\nanimation system.\n12.1 Types of Character Animation\nCharacter animation technology has come a long way since Donkey Kong. At\nfirst,gamesemployedverysimpletechniquestoprovidetheillusionoflifelike\nmovement. Asgamehardwareimproved,more-advancedtechniquesbecame\n721\n722 12. Animation Systems\nfeasibleinrealtime. Today,gamedesignershaveahostofpowerfulanimation\nmethodsattheirdisposal. Inthissection,we’lltakeabrieflookattheevolution\nofcharacteranimationandoutlinethethreemost-commontechniquesusedin\nmodern game engines.\n12.1.1 Cel Animation\nThe precursor to all game animation techniques is known as traditionalanima-\ntion, orhand-drawn animation. This is the technique used in the earliest ani-\nmated cartoons. The illusion of motion is produced by displaying a sequence\nof still pictures known as framesin rapid succession. Real-time 3D rendering\ncan be thought of as an electronic form of traditional animation, in that a se-\nquence of still full-screen images is presented to the viewer over and over to\nproduce the illusion of motion.\nCelanimation is a specific type of traditional animation. A celis a transpar-\nent sheet of plastic on which images can be painted or drawn. An animated\nsequence of cels can be placed on top of a fixed background painting or draw-\ningtoproducetheillusionofmotionwithouthavingtoredrawthestaticback-\nground over and over.\nThe electronic equivalent to cel animation is a technology known as sprite\nanimation. Aspriteisasmallbitmapthatcanbeoverlaidontopofafull-screen\nbackground image without disrupting it, often drawn with the aid of special-\nized graphics hardware. Hence, a sprite is to 2D game animation what a cel\nwas to traditional animation. This technique was a staple during the 2D game\nera. Figure 12.1 shows the famous sequence of sprite bitmaps that were used\nto produce the illusion of a running humanoid character in almost every Mat-\ntelIntellivisiongameevermade. Thesequenceofframeswasdesignedsothat\nit animates smoothly even when it is repeated indefinitely—this is known as\nalooping animation. This particular animation would be called a run cycle in\nmodern parlance, because it makes the character appear to be running. Char-\nacters typically have a number of looping animation cycles, including various\nidle cycles, a walk cycle and a run cycle.\nFigure 12.1. The sequence of sprite bitmaps used in most Intellivision games.\n12.1. Types of Character Animation 723\n12.1.2 Rigid Hierarchical Animation\nEarly 3D games like Doomcontinued to make use of a sprite-like animation\nsystem: Its monsters were nothing more than camera-facing quads, each of\nwhich displayed a sequence of texture bitmaps (known as an animatedtexture )\nto produce the illusion of motion. And this technique is still used today for\nlow-resolution and/or distant objects—for example crowds in a stadium, or\nhordes of soldiers fighting a distant battle in the background. But for high-\nquality foreground characters, 3D graphics brought with it the need for im-\nproved character animation methods.\nThe earliest approach to 3D character animation is a technique known as\nrigid hierarchical animation. In this approach, a character is modeled as a col-\nlection of rigid pieces. A typical breakdown for a humanoid character might\nbe pelvis, torso, upper arms, lower arms, upper legs, lower legs, hands, feet\nand head. The rigid pieces are constrained to one another in a hierarchical\nfashion, analogous to the manner in which a mammal’s bones are connected\nat the joints. This allows the character to move naturally. For example, when\nthe upper arm is moved, the lower arm and hand will automatically follow it.\nA typical hierarchy has the pelvis at the root, with the torso and upper legs as\nits immediate children and so on as shown below:\nPelvis\nTorso\nUpperRightArm\nLowerRightArm\nRightHand\nUpperLeftArm\nUpperLeftArm\nLeftHand\nHead\nUpperRightLeg\nLowerRightLeg\nRightFoot\nUpperLeftLeg\nUpperLeftLeg\nLeftFoot\nThe big problem with the rigid hierarchy technique is that the behavior of\nthe character’s body is often not very pleasing due to “cracking” at the joints.\nThis is illustrated in Figure 12.2. Rigid hierarchical animation works well for\nrobots and machinery that really are constructed of rigid parts, but it breaks\ndown under scrutiny when applied to “fleshy” characters.\n724 12. Animation Systems\n12.1.3 Per-Vertex Animation and Morph Targets\nRigid hierarchical animation tends to look unnatural because it is rigid. What\nwereallywantisawaytomoveindividualverticessothattrianglescanstretch\nto produce more natural-looking motion.\nOne way to achieve this is to apply a brute-force technique known as per-\nvertex animation . In this approach, the vertices of the mesh are animated by\nan artist, and motion data is exported, which tells the game engine how to\nmove each vertex at runtime. This technique can produce any mesh deforma-\ntion imaginable (limited only by the tessellation of the surface). However, it\nis a data-intensive technique, since time-varying motion information must be\nstored for each vertex of the mesh. For this reason, it has little application to\nreal-time games.\nA variation on this technique known as morph target animation is used in\nsome real-time games. In this approach, the vertices of a mesh are moved by\nananimatortocreatearelativelysmallsetoffixed,extremeposes. Animations\nareproducedby blending betweentwoormoreofthesefixedposesatruntime.\nThe position of each vertex is calculated using a simple linear interpolation\n(LERP) between the vertex’s positions in each of the extreme poses.\nThe morph target technique is often used for facial animation, because the\nhuman face is an extremely complex piece of anatomy, driven by roughly 50\nmuscles. Morph target animation gives an animator full control over every\nvertexofafacialmesh,allowinghimorhertoproducebothsubtleandextreme\nmovements that approximate the musculature of the face well. Figure 12.3\nshows a set of facial morph targets.\nAs computing power continues to increase, some studios are using jointed\nfacial rigs containing hundreds of joints as an alternative to morph targets.\nOther studios combine the two techniques, using jointed rigs to achieve the\nprimary pose of the face and then applying small tweaks via morph targets.\nFigure 12.2. Cracking at the joints is a big problem in rigid hierarchical animation.\n12.1. Types of Character Animation 725\n12.1.4 Skinned Animation\nAsthecapabilitiesofgamehardwareimprovedfurther,ananimationtechnol-\nogy known as skinned animation was developed. This technique has many of\nthe benefits of per-vertex and morph target animation—permitting the trian-\ngles of an animated mesh to deform. But it also enjoys the much more effi-\ncient performance and memory usage characteristics of rigid hierarchical an-\nimation. It is capable of producing reasonably realistic approximations to the\nmovement of skin and clothing.\nSkinned animation was first used by games like Super Mario 64, and it is\nstillthe mostprevalenttechniquein use today, bothby the gameindustry and\nthe feature film industry. A host of famous modern game and movie char-\nacters, including the dinosaurs from Jurrassic Park, Solid Snake (Metal Gear\nSolid4), Gollum (LordoftheRings ), Nathan Drake (Uncharted), Buzz Lightyear\n(Toy Story), Marcus Fenix (Gears of War ) and Joel ( The Last of Us) were all ani-\nmated, in whole or in part, using skinned animation techniques. The remain-\nder of this chapter will be devoted primarily to the study of skinned/skeletal\nanimation.\nIn skinned animation, a skeleton is constructed from rigid “bones,” just as\ninrigidhierarchicalanimation. However,insteadofrenderingtherigidpieces\non-screen, they remain hidden. A smooth continuous triangle mesh called a\nskinis bound to the joints of the skeleton; its vertices track the movements of\nthe joints. Each vertex of the skin mesh can be weighted to multiple joints, so\nthe skin can stretch in a natural way as the joints move.\nIn Figure 12.4, we see Crank the Weasel, a game character designed by\nEric Browning for Midway Home Entertainment in 2001. Crank’s outer skin\nis composed of a mesh of triangles, just like any other 3D model. However,\ninside him we can see the rigid bones and joints that make his skin move.\nFigure 12.3. A set of facial morph targets for the Ellie character in The Last of Us: Remastered\n(© 2014/™ SIE. Created and developed by Naughty Dog, PlayStation 4).\n726 12. Animation Systems\n12.1.5 Animation Methods as Data Compression Techniques\nThemostflexibleanimationsystemconceivablewouldgivetheanimatorcon-\ntrol over literally every infinitesimal point on an object’s surface. Of course,\nanimating like this would result in an animation that contains a potentially\ninfinite amount of data! Animating the vertices of a triangle mesh is a simpli-\nfication of this ideal—in effect, we are compressing the amount of information\nneeded to describe an animation by restricting ourselves to moving only the\nvertices. (Animating a set of control points is the analog of vertex animation\nfor models constructed out of higher-order patches.) Morph targets can be\nthought of as an additional level of compression, achieved by imposing addi-\ntional constraints on the system—vertices are constrained to move only along\nlinear paths between a fixed number of predefined vertex positions. Skeletal\nanimation is just another way to compressvertex animation data by imposing\nconstraints. In this case, the motions of a relatively large number of vertices\nare constrained to follow the motions of a relatively small number of skeletal\njoints.\nWhen considering the trade-offs between various animation techniques, it\ncan be helpful to think of them as compression methods, analogous in many\nrespects to video compression techniques. We should generally aim to select\nthe animation method that provides the best compression without producing\nunacceptable visual artifacts. Skeletal animation provides the best compres-\nsion when the motion of a single joint is magnified into the motions of many\nvertices. Acharacter’slimbsactlikerigidbodiesforthemostpart,sotheycan\nFigure 12.4. Eric Browning’s Crank the Weasel character, with internal skeletal structure.",11286
80-12.2 Skeletons.pdf,80-12.2 Skeletons,"12.2. Skeletons 727\nFigure 12.5. The pelvis joint of this character connects to four other joints (tail, spine and two legs),\nand so it produces four bones.\nbemovedveryefficientlywithaskeleton. However,themotionofafacetends\nto be much more complex, with the motions of individual vertices being more\nindependent. To convincingly animate a face using the skeletal approach, the\nrequirednumberofjointsapproachesthenumberofverticesinthemesh,thus\ndiminishing its effectiveness as a compression technique. This is one reason\nwhy morph target techniques are often favored over the skeletal approach for\nfacial animation. (Another common reason is that morph targets tend to be a\nmore natural way for animators to work.)\n12.2 Skeletons\nA skeleton is comprised of a hierarchy of rigid pieces known as joints. In the\ngameindustry,weoftenusetheterms“joint”and“bone”interchangeably,but\nthe term boneis actually a misnomer. Technically speaking, the joints are the\nobjectsthataredirectlymanipulatedbytheanimator,whilethebonesaresim-\nply the empty spaces between the joints. As an example, consider the pelvis\njointintheCranktheWeaselcharactermodel. Itisasinglejoint,butbecauseit\nconnectstofourotherjoints(thetail,thespineandtheleftandrighthipjoints),\nthis one joint appears to have four bones sticking out of it. This is shown in\nmore detail in Figure 12.5. Game engines don’t care a whip about bones—\nonly the joints matter. So whenever you hear the term “bone” being used in\nthe industry, remember that 99% of the time we are actually speaking about\njoints.\n728 12. Animation Systems\n12.2.1 The Skeleal Hierarchy\nFigure 12.6. Example\nof a skeletal hier-\narchy, as it would\nappear in Maya’s Hy-\npergraph Hierarchy\nview.Aswe’vementioned,thejointsinaskeletonformahierarchyortreestructure.\nOne joint is selected as the root, and all other joints are its children, grandchil-\ndren and so on. A typical joint hierarchy for skinned animation looks almost\nidentical to a typical rigid hierarchy. For example, a humanoid character’s\njoint hierarchy might look something like the one depicted in Figure 12.6.\nWe usually assign each joint an index from 0toN 1. Because each joint\nhas one and only one parent, the hierarchical structure of a skeleton can be\nfullydescribedbystoringtheindexofitsparentwitheachjoint. Therootjoint\nhas no parent, so its parent index is usually set to an invalid value such as\n 1.\n12.2.2 Representing a Skeleton in Memory\nA skeleton is usually represented by a small top-level data structure that con-\ntainsanarrayofdatastructuresfortheindividualjoints. Thejointsareusually\nlisted in an order that ensures a child joint will always appear after its parent\nin the array. This implies that joint zero is always the root of the skeleton.\nJoint indices are usually used to refer to joints within animation data struc-\ntures. Forexample,achildjointtypicallyreferstoitsparentjointbyspecifying\nits index. Likewise, in a skinned triangle mesh, a vertex refers to the joint or\njointstowhichitisboundbyindex. Thisismuchmoreefficientthanreferring\nto joints by name, both in terms of the amount of storage required (a joint in-\ndex can be 8 bits wide, as long as we are willing to accept a maximum of 256\njoints per skeleton) and in terms of the amount of time it takes to look up a\nreferenced joint (we can use the joint index to jump immediately to a desired\njoint in the array).\nEach joint data structure typically contains the following information:\n• Thenameof the joint, either as a string or a hashed 32-bit string id.\n• Theindexof the joint’s parentwithin the skeleton.\n• Theinversebindposetransform of the joint. The bind pose of a joint is the\nposition,orientationandscaleofthatjointatthetimeitwasboundtothe\nverticesoftheskinmesh. Weusuallystorethe inverseofthistransforma-\ntion for reasons we’ll explore in more depth in the following sections.\nA typical skeleton data structure might look something like this:\nstruct Joint\n{\nMatrix4x3 m_invBindPose; // inverse bind pose",4042
81-12.3 Poses.pdf,81-12.3 Poses,"12.3. Poses 729\n// transform\nconst char* m_name; // human-readable joint\n// name\nU8 m_iParent; // parent index or 0xFF\n// if root\n};\nstruct Skeleton\n{\nU32 m_jointCount; // number of joints\nJoint* m_aJoint; // array of joints\n};\n12.3 Poses\nNo matter what technique is used to produce an animation, be it cel-based,\nrigid hierarchical or skinned/skeletal, every animation takes place over time.\nAcharacterisimbuedwiththeillusionofmotionbyarrangingthecharacter’s\nbody into a sequence of discrete, still posesand then displaying those poses\nin rapid succession, usually at a rate of 30 or 60 poses per second . (Actually, as\nwe’ll see in Section 12.4.1.1, we often interpolate between adjacent poses rather\nthan displaying a single pose verbatim.) In skeletal animation, the pose of the\nskeletondirectlycontrolstheverticesofthemesh,andposingistheanimator’s\nprimary tool for breathing life into her characters. So clearly, before we can\nanimate a skeleton, we must first understand how to poseit.\nAskeletonisposedbyrotating,translatingandpossiblyscalingitsjointsin\narbitrary ways. The poseof a joint is defined as the joint’s position, orientation\nand scale, relative to some frame of reference. A joint pose is usually repre-\nsentedbya 44or43matrix,orbyanSRTdatastructure(scale,quaternion\nrotation and vector translation). The pose of a skeleton is just the set of all of\nits joints’ poses and is normally represented as a simple array of matrices or\nSRTs.\n12.3.1 Bind Pose\nTwo different poses of the same skeleton are shown in Figure 12.7. The pose\non the left is a special pose known as the bind pose, also sometimes called the\nreference pose or therest pose. This is the pose of the 3D mesh prior to being\nbound to the skeleton (hence the name). In other words, it is the pose that the\nmeshwouldassumeifitwererenderedasaregular,unskinnedtrianglemesh,\nwithout any skeleton at all. The bind pose is also called the T-posebecause\n730 12. Animation Systems\nthe character is usually standing with his feet slightly apart and his arms out-\nstretched in the shape of the letter T. This particular stance is chosen because\nit keeps the limbs away from the body and each other, making the process of\nbinding the vertices to the joints easier.\nFigure 12.7. Two different poses of the same skeleton. The pose on the left is the special pose\nknown as bind pose.\n12.3.2 Local Poses\nA joint’s pose is most often specified relative to its parentjoint. A parent-\nrelative pose allows a joint to move naturally. For example, if we rotate the\nshoulder joint, but leave the parent-relative poses of the elbow, wrist and fin-\ngers unchanged, the entire arm will rotate about the shoulder in a rigid man-\nner, as we’d expect. We sometimes use the term localpose to describe a parent-\nrelative pose. Local poses are almost always stored in SRT format, for reasons\nwe’ll explore when we discuss animation blending.\nGraphically, many 3D authoring packages like Maya represent joints as\nsmallspheres. However,ajointhasarotationandascale,notjustatranslation,\nso this visualization can be a bit misleading. In fact, a joint actually defines a\ncoordinatespacenodifferentinprinciplefromtheotherspaceswe’veencoun-\ntered (like model space, world space or view space). So it is best to picture a\njoint as a set of Cartesian coordinate axes. Maya gives the user the option of\ndisplaying a joint’s local coordinate axes—this is shown in Figure 12.8.\nMathematically, a joint pose is nothing more than an affine transformation .\nThe pose of joint jcan be written as the 44affine transformation matrix Pj,\nwhich is comprised of a translation vector Tj, a33diagonal scale matrix\n12.3. Poses 731\nFigure 12.8. Every joint in a skeletal hierarchy deﬁnes a set of local coordinate space axes, known\nas joint space.\nSjand a 33rotation matrix Rj. The pose of an entire skeleton Pskelcan be\nwritten as the set of all poses Pj, where jranges from 0 to N 1:\nPj=[SjRj0\nTj1]\n,\nPskel={\nPj}N 1\nj=0.\n12.3.2.1 Joint Scale\nSome game engines assume that joints will never be scaled, in which case Sj\nis simply omitted and assumed to be the identity matrix. Other engines make\nthe assumption that scale will be uniform if present, meaning it is the same\nin all three dimensions. In this case, scale can be represented using a single\nscalarvalue sj. Someenginesevenpermit nonuniform scale,inwhichcasescale\ncanbecompactlyrepresentedbythethree-elementvector sj=[sjxsjysjz]\n.\nThe elements of the vector sjcorrespond to the three diagonal elements of the\n33scaling matrix Sj, so it is not really a vector per se. Game engines almost\nneverpermitshear,so Sjisalmostneverrepresentedbyafull 33scale/shear\nmatrix, although it certainly couldbe.\nThere are a number of benefits to omitting or constraining scale in a pose\noranimation. Clearlyusingalower-dimensionalscalerepresentationcansave\nmemory. (Uniform scale requires a single floating-point scalar per joint per\nanimationframe, whilenonuniformscalerequiresthreefloats, andafull 33\nscale-shear matrix requires nine.) Restricting our engine to uniform scale has\nthe added benefit of ensuring that the bounding sphere of a joint will never\n732 12. Animation Systems\nbe transformed into an ellipsoid, as it could be when scaled in a nonuniform\nmanner. This greatly simplifies the mathematics of frustum and collision tests\nin engines that perform such tests on a per-joint basis.\n12.3.2.2 Representing a Joint Pose in Memory\nAs we mentioned above, joint poses are usually stored in SRT format. In C++,\nsuch a data structure might look like this, where Q is first to ensure proper\nalignment and optimal structure packing. (Can you see why?)\nstruct JointPose\n{\nQuaternion m_rot; // R\nVector3 m_trans; // T\nF32 m_scale; // S (uniform scale only)\n};\nIfnonuniformscaleispermitted,wemightdefineajointposelikethisinstead:\nstruct JointPose\n{\nQuaternion m_rot; // R\nVector4 m_trans; // T\nVector4 m_scale; // S\n};\nThe local pose of an entire skeleton can be represented as follows, where it\nis understood that the array m_aLocalPose is dynamically allocated to con-\ntain just enough occurrences of JointPose to match the number of joints in\nthe skeleton.\nstruct SkeletonPose\n{\nSkeleton* m_pSkeleton; // skeleton + num joints\nJointPose* m_aLocalPose; // local joint poses\n};\n12.3.2.3 The Joint Pose as a Change of Basis\nIt’s important to remember that a localjoint pose is specified relative to the\njoint’simmediateparent. Anyaffinetransformationcanbethoughtofastrans-\nformingpointsandvectorsfromonecoordinatespacetoanother. Sowhenthe\njointposetransform Pjisappliedtoapointorvectorthatisexpressedintheco-\nordinate system of the joint j, the result is that same point or vector expressed\nin the space of the parent joint.\n12.3. Poses 733\nAswe’vedoneinearlierchapters,we’lladopttheconventionofusingsub-\nscripts to denote the direction of a transformation. Since a joint pose takes\npoints and vectors from the childjoint’s space (C) to that of its parentjoint (P),\nwe can write it (PC!P)j. Alternatively, we can introduce the function p(j),\nwhich returns the parent index of joint j, and write the local pose of joint jas\nPj!p(j).\nOn occasion we will need to transform points and vectors in the opposite\ndirection—from parent space into the space of the childjoint. This transfor-\nmation is just the inverse of the local joint pose. Mathematically, Pp(j)!j=\n(\nPj!p(j)) 1\n.\n12.3.3 Global Poses\nSometimes it is convenient to express a joint’s pose in model space or world\nspace. This is called a globalpose. Some engines express global poses in matrix\nform, while others use the SRT format.\nMathematically, the model-space pose of a joint (j!M)can be found\nby walking the skeletal hierarchy from the joint in question all the way to the\nroot, multiplying the local poses (j!p(j))as we go. Consider the hierarchy\nshown in Figure 12.9. The parent space of the root joint is defined to be model\nspace, so p(0)M. The model-space pose of joint J2can therefore be written\nas follows:\nP2!M=P2!1P1!0P0!M.\nLikewise, the model-space pose of joint J5is just\nP5!M=P5!4P4!3P3!0P0!M.\nIn general, the global pose (joint-to-model transform) of any joint jcan be\nwritten as follows:\nPj!M=0\nÕ\ni=jPi!p(i), (12.1)\nwhere it is understood that ibecomes p(i)(the parent of joint i) after each\niteration in the product, and p(0)M.\n12.3.3.1 Representing a Global Pose in Memory\nWe can extend our SkeletonPose data structure to include the global pose\nas follows, where again we dynamically allocate the m_aGlobalPose array\nbased on the number of joints in the skeleton:",8689
82-12.4 Clips.pdf,82-12.4 Clips,"734 12. Animation Systems\n01 2\n3 4 5\nxMyM\nFigure 12.9. A global pose can be calculated by walking the hierarchy from the joint in question\ntowards the root and model-space origin, concatenating the child-to-parent (local) transforms\nof each joint as we go.\nstruct SkeletonPose\n{\nSkeleton* m_pSkeleton; // skeleton + num joints\nJointPose* m_aLocalPose; // local joint poses\nMatrix44* m_aGlobalPose; // global joint poses\n};\n12.4 Clips\nIn a film, every aspect of each scene is carefully planned out before any ani-\nmationsarecreated. Thisincludesthemovementsofeverycharacterandprop\nin the scene, and even the movements of the camera. This means that an en-\ntire scene can be animated as one long, contiguous sequence of frames. And\ncharacters need not be animated at all whenever they are off-camera.\nGame animation is different. A game is an interactive experience, so one\ncannot predict beforehand how the characters are going to move and behave.\nThe player has full control over his or her character and usually has partial\ncontrol over the camera as well. Even the decisions of the computer-driven\nnon-player characters are strongly influenced by the unpredictable actions of\nthehumanplayer. Assuch,gameanimationsarealmostnevercreatedaslong,\ncontiguous sequences of frames. Instead, a game character’s movement must\nbe broken down into a large number of fine-grained motions. We call these\nindividual motions animationclips, or sometimes just animations.\nEach clip causes the character to perform a single well-defined action.\nSome clips are designed to be looped—for example, a walk cycle or run cy-\ncle. Others are designed to be played once—for example, throwing an object\nor tripping and falling to the ground. Some clips affect the entire body of the\ncharacter—the character jumping into the air for instance. Other clips affect\n12.4. Clips 735\nt= 0 t= (0.4) Tt=Tt= (0.8) T\nFigure 12.10. The local timeline of an animation showing poses at selected time indices. Images\ncourtesy of Naughty Dog, Inc., © 2014/™ SIE.\nonly a part of the body—perhaps the character waving his right arm. The\nmovements of any one game character are typically broken down into liter-\nally thousands of clips.\nThe only exception to this rule is when game characters are involved in a\nnoninteractive portion of the game, known as an in-gamecinematic (IGC),non-\ninteractive sequence (NIS) or full-motion video (FMV). Noninteractive sequences\nare typically used to communicate story elements that do not lend themselves\nwell to interactive gameplay, and they are created in much the same way\ncomputer-generatedfilmsaremade(althoughtheyoftenmakeuseofin-game\nassets like character meshes, skeletons and textures). The terms IGC and NIS\ntypically refer to noninteractive sequences that are rendered in real time by\nthe game engine itself. The term FMV applies to sequences that have been\nprerendered to an MP4, WMV or other type of movie file and are played back\nat runtime by the engine’s full-screen movie player.\nAvariationonthisstyleofanimationisasemi-interactivesequenceknown\nas aquick time event (QTE). In a QTE, the player must hit a button at the right\nmoment during an otherwise noninteractive sequence in order to see the suc-\ncess animation and proceed; otherwise, a failure animation is played, and the\nplayer must try again, possibly losing a life or suffering some other conse-\nquence as a result.\n12.4.1 The Local Timeline\nWe can think of every animation clip as having a local timeline, usually de-\nnoted by the independent variable t. At the start of a clip, t=0, and at the\nend, t=T, where Tis the duration of the clip. Each unique value of the vari-\nabletis called a timeindex . An example of this is shown in Figure 12.10.\n736 12. Animation Systems\n12.4.1.1 Pose Interpolation and Continuous Time\nIt’s important to realize that the rate at which frames are displayed to the\nviewer is not necessarily the same as the rate at which poses are created by\nthe animator. In both film and game animation, the animator almost never\nposesthecharacterevery1/30or1/60ofasecond. Instead, theanimatorgen-\neratesimportantposesknownas keyposes orkeyframes atspecifictimeswithin\nthe clip, and the computer calculates the poses in between via linear or curve-\nbased interpolation. This is illustrated in Figure 12.11.\nBecause of the animation engine’s ability to interpolate poses (which we’ll\nexplore in depth later in this chapter), we can actually sample the pose of the\ncharacterat anytime duringtheclip—notjustonintegerframeindices. Inother\nwords, an animation clip’s timeline is continuous. In computer animation, the\ntime variable tis areal(floating-point) number, not an integer.\nFilm animation doesn’t take full advantage of the continuous nature of\nthe animation timeline, because its frame rate is locked at exactly 24, 30 or\n60 frames per second. In film, the viewer sees the characters’ poses at frames\n1, 2, 3 and so on—there’s never any need to find a character’s pose on frame\n3.7, for example. So in film animation, the animator doesn’t pay much (if any)\nattention to how the character looks in between the integral frame indices.\nIn contrast, a real-time game’s frame rate always varies a little, depending\nonhowmuchloadiscurrentlybeingplacedontheCPUandGPU.Also,game\nanimations are sometimes time-scaled in order to make the character appear\nto move faster or slower than originally animated. So in a real-time game,\nan animation clip is almost neversampled on integer frame numbers. In the-\nory, with a time scale of 1.0, a clip should be sampled at frames 1, 2, 3 and\nso on. But in practice, the player might actually see frames 1.1, 1.9, 3.2 and\nso on. And if the time scale is 0.5, then the player might actually see frames\nFigure 12.11. An animator creates a relatively small number of key poses, and the engine ﬁlls in the\nrest of the poses via interpolation.\n12.4. Clips 737\n26 27 28 29 30 1 2 3 4 5 ...\n31 Samp les:Frames:\n30 29 28 27 26 6 5 4 3 2 1\nFigure 12.12. A one-second animation sampled at 30 frames per second is 30 frames in duration\nand consists of 31 samples.\n1.1, 1.4, 1.9, 2.6, 3.2 and so on. A negative time scale can even be used to play\nan animation in reverse. So in game animation, time is both continuous and\nscalable.\n12.4.1.2 Time Units\nBecause an animation’s timeline is continuous, time is best measured in units\nof seconds. Time can also be measured in units of frames, presuming we de-\nfine the duration of a frame beforehand. Typical frame durations are 1/30 or\n1/60 of a second for game animation. However, it’s important not to make\nthe mistake of defining your time variable tas an integer that counts whole\nframes. No matter which time units are selected, tshould be a real (floating-\npoint) quantity, a fixed-point number or an integer that measures very small\nsubframe time intervals. The goal is to have sufficient resolution in your time\nmeasurements for doing things like “tweening” between frames or scaling an\nanimation’s playback speed.\n12.4.1.3 Frame versus Sample\nUnfortunately,theterm framehasmorethanonecommonmeaninginthegame\nindustry. Thiscanleadtoagreatdealofconfusion. Sometimesaframeistaken\nto be aperiod of time that is 1/30 or 1/60 of a second in duration. But in other\ncontexts,thetermframeisappliedtoa singlepointintime (e.g.,wemightspeak\nof the pose of the character “at frame 42”).\nI personally prefer to use the term sampleto refer to a single point in time,\nand I reserve the word frameto describe a time period that is 1/30 or 1/60\nof a second in duration. So for example, a one-second animation created at\na rate of 30 frames per second would consist of 31 samples and would be 30\nframesin duration, as shown in Figure 12.12. The term “sample” comes from\nthe field of signal processing. A continuous-time signal (i.e., a function f(t))\ncan be converted into a set of discrete data points by sampling that signal at\nuniformly spaced time intervals. See Section 14.3.2.1 for more information on\nsampling.\n738 12. Animation Systems\n12.4.1.4 Frames, Samples and Looping Clips\nWhen a clip is designed to be played over and over repeatedly, we say it is\nlooped. If we imagine two copies of a 1 s (30-frame/31-sample) clip laid back-\nto-front, then sample 31 of the first clip will coincide exactly in time with sam-\nple 1 of the second clip, as shown in Figure 12.13. For a clip to loop properly,\nthen,wecanseethattheposeofthecharacterattheendoftheclipmustexactly\nmatch the pose at the beginning. This, in turn, implies that the last sample of\na looping clip (in our example, sample 31) is redundant. Many game engines\ntherefore omit the last sample of a looping clip.\nThis leads us to the following rules governing the number of samples and\nframes in any animation clip:\n• If a clip is non-looping , an N-frame animation will have N+1unique\nsamples.\n• If a clip is looping, then the last sample is redundant, so an N-frame ani-\nmation will have Nunique samples.\n30 29 28 27 26\n6 5 4 3 230 29 28 27 26 5 4 3 2 1\n31\n1... ......\n...\nFigure 12.13. The last sample of a looping clip coincides in time with its ﬁrst sample and is, therefore,\nredundant.\n12.4.1.5 Normalized Time (Phase)\nItissometimesconvenienttoemployanormalizedtimeunit u,suchthat u=0\nat the start of the animation, and u=1at the end, no matter what its duration\nTmaybe. Wesometimesrefertonormalizedtimeasthe phaseoftheanimation\nclip,because uactslikethephaseofasinewavewhentheanimationislooped.\nThis is illustrated in Figure 12.14.\nNormalized time is useful when synchronizing two or more animation\nclips that are not necessarily of the same absolute duration. For example, we\nmight want to smoothly cross-fade from a 2-second (60-frame) run cycle into\na 3-second (90-frame) walk cycle. To make the cross-fade look good, we want\ntoensurethatthetwoanimationsremainsynchronizedatalltimes,sothatthe\n12.4. Clips 739\nu= 0 u= 0.4u= 1u= 0.8\nFigure 12.14. An animation clip, showing normalized time units. Images courtesy of Naughty Dog,\nInc., © 2014/™ SIE.\nfeet line up properly in both clips. We can accomplish this by simply setting\nthenormalizedstarttimeofthewalkclip, uwalk,tomatchthenormalizedtime\nindexoftherunclip, urun. Wethenadvancebothclipsatthesamenormalized\nrate so that they remain in sync. This is quite a bit easier and less error-prone\nthan doing the synchronization using the absolute time indices twalkandtrun.\n12.4.2 The Global Timeline\nJust as every animation clip has a local timeline (whose clock starts at 0 at the\nbeginning of the clip), every character in a game has a global timeline (whose\nclock starts when the character is first spawned into the game world, or per-\nhapsatthestartofthelevelortheentiregame). Inthisbook,we’llusethetime\nvariable tto measure global time, so as not to confuse it with the local time\nvariable t.\nWe can think of playing an animation as simply mapping that clip’s local\ntimeline onto the character’s global timeline. For example, Figure 12.15 illus-\ntratesplayinganimationclipAstartingataglobaltimeof tstart=102seconds.\nAs we saw above, playing a looping animation is like laying down an in-\nfinite number of back-to-front copies of the clip onto the global timeline. We\ncan also imagine looping an animation a finite number of times, which corre-\nsponds to laying down a finite number of copies of the clip. This is illustrated\nin Figure 12.16.\nClip A\nt= 0 sec 5 sec\nstar t\n102 sec\n105 sec 110 sec\nFigure 12.15. Playing animation clip A starting at a global time of 102 seconds.\n740 12. Animation Systems\nClip A110 sec\nstart\n 102 sec\nClip A ...\n105 sec\nFigure 12.16. Playing a looping animation corresponds to laying down multiple back-to-back copies\nof the clip.\nTime-scaling a clip makes it appear to play back more quickly or more\nslowly than originally animated. To accomplish this, we simply scale the im-\nage of the clip when it is laid down onto the global timeline. Time-scaling is\nmost naturally expressed as a playback rate, which we’ll denote R. For exam-\nple, if an animation is to play back at twice the speed (R=2), then we would\nscaletheclip’slocaltimelinetoone-half (1/R=0.5)ofitsnormallengthwhen\nmapping it onto the global timeline. This is shown in Figure 12.17.\nPlaying a clip in reverse corresponds to using a time scale of  1, as shown\nin Figure 12.18.\nIn order to map an animation clip onto a global timeline, we need the fol-\nlowing pieces of information about the clip:\n• its global start time tstart,\nstart\nR\n(scale t by 1/ R= 0.5)t t\nt\nFigure 12.17. Playing an animation at twice the speed corresponds to scaling its local timeline by a\nfactor of 1/2 .\nt= 5 sec 0 sec\nstart\n102 sec\n105 sec 110 sec\n Clip A\nClip A\nt= 0 sec 5 secR= –1\n(ﬂip t)\nFigure 12.18. Playing a clip in reverse corresponds to a time scale of  1.\n12.4. Clips 741\n• its playback rate R,\n• its duration T, and\n• the number of times it should loop, which we’ll denote N.\nGiventhisinformation,wecanmapfromanyglobaltime ttothecorrespond-\ning local time t, and vice versa, using the following two relations:\nt= (t tstart)R, (12.2)\nt=tstart+1\nRt.\nIf the animation doesn’t loop (N=1), then we should clamp tinto the\nvalid range [0,T]before using it to sample a pose from the clip:\nt=clamp[\n(t tstart)R]T\n0.\nIf the animation loops forever (N=¥), then we bring tinto the valid\nrange by taking the remainder of the result after dividing by the duration T.\nThisisaccomplishedviathe modulooperator(mod, or%inC/C++), asshown\nbelow:\nt=((t tstart)R)\nmod T.\nIf the clip loops a finitenumber of times (1<N<¥), we must first clamp\ntinto the range [0,NT]and then modulo thatresult by Tin order to bring t\ninto a valid range for sampling the clip:\nt=(\nclamp[\n(t tstart)R]NT\n0)\nmod T.\nMost game engines work directly with local animation timelines and don’t\nuse the global timeline directly. However, working directly in terms of global\ntimes can have some incredibly useful benefits. For one thing, it makes syn-\nchronizing animations trivial.\n12.4.3 Comparison of Local and Global Clocks\nThe animation system must keep track of the time indices of every animation\nthat is currently playing. To do so, we have two choices:\n•Local clock. In this approach, each clip has its own local clock, usually\nrepresented by a floating-point time index stored in units of seconds or\nframes, or in normalized time units (in which case it is often called the\nphaseof the animation). At the moment the clip begins to play, the local\n742 12. Animation Systems\ntime index tis usually taken to be zero. To advance the animations for-\nward in time, we advance the local clocks of each clip individually. If a\nclip has a non-unit playback rate R, the amount by which its local clock\nadvances must be scaled by R.\n•Global clock. In this approach, the character has a global clock, usually\nmeasured in seconds, and each clip simply records the global time at\nwhich it started playing, tstart. The clips’ local clocks are calculated from\nthis information using Equation (12.2).\nThe local clock approach has the benefit of being simple, and it is the most\nobvious choice when designing an animation system. However, the global\nclockapproachhassomedistinctadvantages,especiallywhenitcomestosyn-\nchronizinganimations, eitherwithinthecontextofasinglecharacteroracross\nmultiple characters in a scene.\n12.4.3.1 Synchronizing Animations with a Local Clock\nWith a local clock approach, we said that the origin of a clip’s local timeline\n(t=0)is usually defined to coincide with the moment at which the clip starts\nplaying. Thus, to synchronize two or more clips, they must be played at ex-\nactly the same moment in game time. This seems simple enough, but it can\nbecomequitetrickywhenthecommandsusedtoplaytheanimationsarecom-\ning from disparate engine subsystems.\nForexample,let’ssaywewanttosynchronizetheplayercharacter’spunch\nanimationwithanon-playercharacter’scorrespondinghitreactionanimation.\nThe problem is that the player’s punch is initiated by the player subsystem in\nresponse to detecting that a button was hit on the joy pad. Meanwhile, the\nnon-player character’s (NPC) hit reaction animation is played by the artificial\nintelligence (AI) subsystem. If the AI code runs beforethe player code in the\ngame loop, there will be a one-frame delay between the start of the player’s\npunch and the start of the NPC’s reaction. And if the player code runs before\ntheAIcode,thentheoppositeproblemoccurswhenanNPCtriestopunchthe\nplayer. If a message-passing (event) system is used to communicate between\nthe two subsystems, additional delays might be incurred (see Section 16.8 for\nmore details). This problem is illustrated in Figure 12.19.\nvoid GameLoop()\n{\nwhile (!quit)\n{\n// preliminary updates...\n12.4. Clips 743\nUpdateAllNpcs(); // react to punch event\n// from last frame\n// more updates...\nUpdatePlayer(); // punch button hit - start punch\n// anim, and send event to NPC to\n// react\n// still more updates...\n}\n}\nFram eN+1 Fram eN\nNPC\nPlayerUpda te\nUpdateUpdate\nsend: Punch\nPlayer \nAnim\nNPC\nAnimplay an implay an imQueue  Event\nPlayer Punch\n(local t= 0)request start (frame N)\nHit Reaction\n(local t= 0)start (frame N+1)\nFigure 12.19. The order of execution of disparate gameplay systems can introduce animation syn-\nchronization problems when local clocks are used.\n12.4.3.2 Synchronizing Animations with a Global Clock\nAglobalclockapproachhelpstoalleviatemanyofthesesynchronizationprob-\nlems, because the origin of the timeline (t=0)is common across all clips by\ndefinition. Iftwoormoreanimations’globalstarttimesarenumericallyequal,\nthe clips will start in perfect synchronization. If their playback rates are also\nequal, then they will remain in sync with no drift. It no longer matters when\nthe code that plays each animation executes. Even if the AI code that plays\nthe hit reaction ends up running a frame later than the player’s punch code,\nit is still trivial to keep the two clips in sync by simply noting the global start\ntimeofthepunchandsettingtheglobalstarttimeofthereactionanimationto\nmatch it. This is shown in Figure 12.20.\nOf course, we do need to ensure that the two characters’ global clocks\nmatch, but this is trivial to do. We can either adjust the global start times to\n744 12. Animation Systems\nFrame N+1 FrameN\nNPC\nPlayerUpdate\nUpdateUpdate\nsend: Punch\nPlayer\nAnim\nNPC\nAnimplay animplay animQueue Event\nPlayer Punch\n(global start time: \n )start at global \n (frame N)\nNPC Hit Reaction\n(global start time: \n )start at global \n (frame N)\nFigure 12.20. A global clock approach can alleviate animation synchronization problems.\ntakeaccountofanydifferencesinthecharacters’clocks,orwecansimplyhave\nall characters in the game share a single master clock.\n12.4.4 A Simple Animation Data Format\nTypically, animation data is extracted from a Maya scene file by sampling the\nposeoftheskeletondiscretelyatarateof30or60samplespersecond. Asam-\nple comprises a full pose for each joint in the skeleton. The poses are usually\nstored in SRT format: For each joint j, the scale component is either a single\nfloating-point scalar Sjor a three-element vector Sj=[SjxSjySjz]\n. The\nrotational component is of course a four-element quaternion Qj= [QjxQjy\nQjzQjw]. And the translational component is a three-element vector Tj=\n0 1 2 3 4 5 6 7 8 9\nT0\nQ0\nS0\nT1\nQ1\nS1...\nyx\nz\nyx\nz\nw\nyx\nz\nFigure 12.21. An uncompressed animation clip contains 10 channels of ﬂoating-point data per sam-\nple, per joint.\n12.4. Clips 745\n[TjxTjyTjz]\n. Wesometimessaythatananimationconsistsofupto10 chan-\nnelsper joint, in reference to the 10 components of Sj,Qj, and Tj. This is illus-\ntrated in Figure 12.21.\nInC++,ananimationclipcanberepresentedinmanydifferentways. Here\nis one possibility:\nstruct JointPose { ... }; // SRT, defined as above\nstruct AnimationSample\n{\nJointPose* m_aJointPose; // array of joint\n// poses\n};\nstruct AnimationClip\n{\nSkeleton* m_pSkeleton;\nF32 m_framesPerSecond;\nU32 m_frameCount;\nAnimationSample * m_aSamples; // array of samples\nbool m_isLooping;\n};\nAn animation clip is authored for a specific skeleton and generally won’t\nwork on any other skeleton. As such, our example AnimationClip data\nstructure contains a reference to its skeleton, m_pSkeleton . (In a real engine,\nthis might be a unique skeleton id rather than a Skeleton* pointer. In this\ncase,theenginewouldpresumablyprovideawaytoquicklyandconveniently\nlook up a skeleton by its unique id.)\nThenumberof JointPosesinthe m_aJointPose arraywithineachsam-\nple is presumed to match the number of joints in the skeleton. The number\nof samples in the m_aSamples array is dictated by the frame count and by\nwhether or not the clip is intended to loop. For a non-looping animation,\nthe number of samples is (m_frameCount + 1). However, if the anima-\ntion loops, then the last sample is identical to the first sample and is usually\nomitted. In this case, the sample count is equal to m_frameCount.\nIt’s important to realize that in a real game engine, animation data isn’t\nactually stored in this simplistic format. As we’ll see in Section 12.8, the data\nis usually compressed in various ways to save memory.\n12.4.5 Continuous Channel Functions\nThesamplesofananimationcliparereallyjustdefinitionsofcontinuousfunc-\ntions over time. You can think of these as 10 scalar-valued functions of time\n746 12. Animation Systems\nper joint, or as two vector-valued functions and one quaternion-valued func-\ntionperjoint. Theoretically,these channelfunctions aresmoothandcontinuous\nacrosstheentireclip’slocaltimeline,asshowninFigure12.22(withtheexcep-\ntion of explicitly authored discontinuities like camera cuts). In practice, how-\never, many game engines interpolate linearly between the samples, in which\ncase the functions actually used are piecewise linear approximations to the un-\nderlying continuous functions. This is depicted in Figure 12.23.\n12.4.6 Metachannels\nMany games permit additional “metachannels” of data to be defined for an\nanimation. These channels can encode game-specific information that doesn’t\nhave to do directly with posing the skeleton but which needs to be synchro-\nnized with the animation.\nIt is quite common to define a special channel that contains event triggers\nat various time indices, as shown in Figure 12.24. Whenever the animation’s\nlocaltimeindexpassesoneofthesetriggers,an eventissenttothegameengine,\nwhich can respond as it sees fit. (We’ll discuss events in detail in Chapter 16.)\nOne common use of event triggers is to denote at which points during the\nFigure 12.22. The animation samples in a clip deﬁne continuous functions over time.\ntQy3\nFigure 12.23. Many game engines use a piecewise linear approximation when interpolating channel\nfunctions.\n12.4. Clips 747\n0 1 2 3 4 5 6 7 8 9\nT0\nQ0\nS0\nT1\nQ1\nS1\nFootstep\nLeftFootstep\nRightReload\nWeaponEvents...\nFigure 12.24. A special event trigger channel can be added to an animation clip in order to\nsynchronize sound effects, particle effects and other game events with an animation.\nanimation certain sound or particle effects should be played. For example,\nwhen the left or right foot touches the ground, a footstep sound and a “cloud\nof dust” particle effect could be initiated.\nAnother common practice is to permit special joints, known in Maya as\nlocators, to be animated along with the joints of the skeleton itself. Because a\njoint or locator is just an affine transform, these special joints can be used to\nencode the position and orientation of virtually any object in the game.\nA typical application of animated locators is to specify how the game’s\ncamera should be positioned and oriented during an animation. In Maya, a\nlocatorisconstrainedtoacamera,andthecameraisthenanimatedalongwith\nthejointsofthecharacter(s)inthescene. Thecamera’slocatorisexportedand\nused in-game to move the game’s camera around during the animation. The\nfieldofview(focallength)ofthecamera,andpossiblyothercameraattributes,\ncan also be animated by placing the relevant data into one or more additional\nfloating-point channels .\nOther examples of non-joint animation channels include:\n• texture coordinate scrolling,\n• textureanimation(aspecialcaseoftexturecoordinatescrollinginwhich\nframes are arranged linearly within a texture, and the texture is scrolled\nby one complete frame at each iteration),\n748 12. Animation Systems\n• animated material parameters (color, specularity, transparency, etc.),\n• animated lighting parameters (radius, cone angle, intensity, color, etc.),\nand\n• anyotherparametersthatneedtochangeovertimeandareinsomeway\nsynchronized with an animation.\n12.4.7 Relationship between Meshes, Skeletons and Clips\nThe UML diagram in Figure 12.25 shows how animation clip data interfaces\nwiththeskeletons,poses,meshesandotherdatainagameengine. Paypartic-\nularattentiontothe cardinality anddirection oftherelationshipsbetweenthese\nclasses. The cardinality is shown just beside the tip or tail of the relationship\narrow between classes—a one represents a single instance of the class, while\nan asterisk indicates many instances. For any one type of character, there will\nbe one skeleton, one or more meshes and one or more animation clips. The\nskeleton is the central unifying element—the skins are attached to the skele-\nton but don’t have any relationship with the animation clips. Likewise, the\nclips are targeted at a particular skeleton, but they have no “knowledge” of\nthe skin meshes. Figure 12.26 illustrates these relationships.\nGame designers often try to reduce the number of unique skeletons in the\ngame to a minimum, because each new skeleton generally requires a whole\nnew set of animation clips. To provide the illusion of many different types of\ncharacters, it is usually better to create multiple meshes skinned to the same\nskeleton when possible, so that all of the characters can share a single set of\nanimations.\n12.4.7.1 Animation Retargeting\nWe said above that an animation is typically only compatible with a single\nskeleton. Thislimitationcanbeovercomevia animationretargeting techniques.\nRetargeting means using an animation authored for one skeleton to ani-\nmate a different skeleton. If the two skeletons are morphologically identical,\nretargeting may boil down to a simple matter of joint index remapping. But\nwhen the two skeletons don’t match exactly, the retargeting problem becomes\nmore complex. At Naughty Dog, the animators define a special pose known\nas theretarget pose. This pose captures the essential differences between the\nbindposesofthesourceandtargetskeletons,allowingtheruntimeretargeting\nsystem to adjust source poses so they will work more naturally on the target\ncharacter.\nOthermore-advancedtechniquesexistforretargetinganimationsauthored\nfor one skeleton so that they work on a different skeleton. For more infor-\n12.4. Clips 749\n1\n*1 *\n1\n*-uniqueId : int\n-jointCount :  int\n-joints : SkeletonJointSkeleto n\n-name : string-parentIndex : int\n-invBindPose : Matrix44Skeleto nJoint\n1\n*\n1 *1\n*-indices : int\n-vertices : Vertex\n-skeletonId : intMesh\n-nameId : int\n-duration : float\n-poseSamples : AnimationPoseAnimationClip\n-position : Vector3\n-normal : Vector3\n-uv : V ector2\n-jointIndices : int\n-jointWeights : floatVertex-scale : Vector3\n-rotation : Quaternion-translation : Vector3SQT\n-jointPoses : SQTAnimat ionPose\nFigure 12.25. UML diagram of shared animation resources.\nmation, see “Feature Points Based Facial Animation Retargeting” by Ludovic\nDutreve et al. (https://bit.ly/2HL9Cdr) and “Real-time Motion Retargeting\ntoHighlyVariedUser-CreatedMorphologies”byChrisHeckeretal. (https://\nbit.ly/2vviG3x).\nSkeleton\nClipN...Skin A\nSkin B\nSkin CClip 1\nClip 2\nClip 3\nother skeletons...\n... ...\nFigure 12.26. Many animation clips and one or more meshes target a single skeleton.",28145
83-12.5 Skinning and Matrix Palette Generation.pdf,83-12.5 Skinning and Matrix Palette Generation,"750 12. Animation Systems\n12.5 Skinning and Matrix Palette Generation\nWe’veseenhowtoposeaskeletonbyrotating,translatingandpossiblyscaling\nits joints. And we know that any skeletal pose can be represented mathemat-\nically as a set of local(\nPj!p(j))\nor global(\nPj!M)\njoint pose transformations,\none for each joint j. Next, we will explore the process of attaching the vertices\nof a 3D mesh to a posed skeleton. This process is known as skinning.\n12.5.1 Per-Vertex Skinning Information\nA skinned mesh is attached to a skeleton by means of its vertices. Each vertex\ncan beboundto one or more joints. If bound to a single joint, the vertex tracks\nthat joint’s movement exactly. If bound to two or more joints, the vertex’s\nposition becomes a weighted average of the positions it would have assumed\nhad it been bound to each joint independently.\nTo skin a mesh to a skeleton, a 3D artist must supply the following addi-\ntional information at each vertex:\n• theindexorindicesof the joint(s) to which it is bound, and\n• foreachjoint,a weightingfactor describinghowmuchinfluencethatjoint\nshould have on the final vertex position.\nThe weighting factors are assumed to add to one, as is customary when calcu-\nlating any weighted average.\nUsually a game engine imposes an upper limit on the number of joints to\nwhichasinglevertexcanbebound. Afour-jointlimitistypicalforanumberof\nreasons. First, four 8-bit joint indices can be packed into a 32-bit word, which\nisconvenient. Also,whileit’sprettyeasytoseeadifferenceinqualitybetween\na two-, three- and even a four-joint-per-vertex model, most people cannot see\naqualitydifferenceasthenumberofjointspervertexisincreasedbeyondfour.\nBecause the joint weights must sum to one, the last weight can be omitted\nand often is. (It can be calculated at runtime as w3=1 (w0+w1+w2).) As\nsuch, a typical skinned vertex data structure might look as follows:\nstruct SkinnedVertex\n{\nfloat m_position[3]; // (Px, Py, Pz)\nfloat m_normal[3]; // (Nx, Ny, Nz)\nfloat m_u, m_v; // texture coords (u,v)\nU8 m_jointIndex[4]; // joint indices\nfloat m_jointWeight[3]; // joint weights (last\n// weight omitted)\n};\n12.5. Skinning and Matrix Palette Generation 751\n12.5.2 The Mathematics of Skinning\nThe vertices of a skinned mesh track the movements of the joint(s) to which\nthey are bound. To make this happen mathematically, we would like to find a\nmatrixthatcantransformtheverticesofthemeshfromtheiroriginalpositions\n(in bind pose) into new positions that correspond to the current pose of the\nskeleton. We shall call such a matrix a skinningmatrix .\nLikeallmeshvertices,thepositionofaskinnedvertexisspecifiedinmodel\nspace. This is true whether its skeleton is in bind pose or in any other pose.\nSo the matrix we seek will transform vertices from model space (bind pose)\nto model space (current pose). Unlike the other transforms we’ve seen thus\nfar, such as the model-to-world transform or the world-to-view transform, a\nskinning matrix is nota change of basis transform. It morphs vertices into\nnew positions, but the vertices are in model space both before and after the\ntransformation.\n12.5.2.1 Simple Example: One-Jointed Skeleton\nLetusderivethebasicequationforaskinningmatrix. Tokeepthingssimpleat\nfirst, we’ll work with a skeleton consisting of a single joint. We therefore have\ntwo coordinate spaces to work with: model space, which we’ll denote with\nthe subscript M, and the joint space of our one and only joint, which will be\nindicated by the subscript J. The joint’s coordinate axes start out in bind pose,\nwhich we’ll denote with the superscript B. At any given moment during an\nanimation, the joint’s axes move to a new position and orientation in model\nspace—we’ll indicate this currentpose with the superscript C.\nNow consider a single vertex that is skinned to our joint. In bind pose,\nits model-space position is vB\nM. The skinning process calculates the vertex’s\nnew model-space position in the current pose, vC\nM. This is illustrated in Fig-\nure 12.27.\nThe “trick” to finding the skinning matrix for a given joint is to realize that\nthepositionofavertexboundtoajointis constant whenexpressedin thatjoint’s\ncoordinatespace. Sowetakethebind-posepositionofthevertexinmodelspace,\nconvert it into joint space, move the joint into its current pose, and finally con-\nvert the vertex back into model space. The net effect of this round trip from\nmodel space to joint space and back again is to “morph” the vertex from bind\npose into the current pose.\nReferring to the illustration in Figure 12.28, let’s assume that the coordi-\nnates of the vertex vB\nMare (4, 6) in model space (when the skeleton is in bind\npose). We convert this vertex into its equivalent joint-space coordinates vj,\nwhichareroughly(1,3)asshowninthediagram. Becausethevertexisbound\n752 12. Animation Systems\nxMyMxByB\nxCyC\nModel Space AxesBind pose \nvertex position, \nin model spaceBind Pose \nJoint Space \nAxes\nCurrent\nPose Joint \nSpace Axes\nCurrent pose \nvertex position, \nin model spacevMB\nvMC\nFigure 12.27. Bind pose and current pose of a simple, one-joint skeleton and a single vertex bound\nto that joint.\nxMyMxByB\nxCyC1. Transform into\n    joint space\nvMB\nvMCvjvj\n3. Transform back\n    into model space2. Move joint into    current pose\nFigure 12.28. By transforming a vertex’s position into joint space, it can be made to “track” the\njoint’s movements.\nto the joint, its joint-space coordinates will alwaysbe (1, 3) no matter how the\njoint may move. Once we have the joint in the desired current pose, we con-\nvert the vertex’s coordinates back into model space, which we’ll denote with\nthe symbol vC\nM. In our diagram, these coordinates are roughly (18, 2). So the\nskinningtransformationhasmorphedourvertexfrom(4,6)to(18,2)inmodel\nspace, due entirely to the motion of the joint from its bind pose to the current\npose shown in the diagram.\nLooking at the problem mathematically, we can denote the bindpose of the\njoint jin model space by the matrix Bj!M. This matrix transforms a point or\nvector whose coordinates are expressed in joint j’s space into an equivalent\nsetofmodel-spacecoordinates. Now,consideravertexwhosecoordinatesare\nexpressed in model space with the skeleton in bind pose. To convert these\nvertexcoordinatesintothespaceofjoint j,wesimplymultiplyitbythe inverse\n12.5. Skinning and Matrix Palette Generation 753\nbind pose matrix, BM!j=(\nBj!M) 1:\nvj=vB\nMBM!j=vB\nM(\nBj!M) 1. (12.3)\nLikewise, we can denote the joint’s current pose (i.e., any pose that is not\nbindpose)bythematrix Cj!M. Toconvert vjfromjointspacebackintomodel\nspace, we simply multiply it by the current pose matrix as follows:\nvC\nM=vjCj!M.\nIf we expand vjusing Equation (12.3), we obtain an equation that takes our\nvertexdirectlyfromitspositioninbindposetoitspositioninthecurrentpose:\nvC\nM=vjCj!M\n=vB\nM(\nBj!M) 1Cj!M (12.4)\n=vB\nMKj.\nThe combined matrix Kj=(\nBj!M) 1Cj!Mis known as a skinningmatrix.\n12.5.2.2 Extension to Multijointed Skeletons\nIntheexampleabove,weconsideredonlyasinglejoint. However,themathwe\nderivedaboveactuallyappliestoanyjointinanyskeletonimaginable,because\nwe formulated everything in terms of global poses (i.e., joint space to model\nspace transforms). To extend the above formulation to a skeleton containing\nmultiple joints, we therefore need to make only two minor adjustments:\n1. We must make sure that our Bj!MandCj!Mmatrices are calculated\nproperlyforthejointinquestion,usingEquation(12.1). Bj!MandCj!M\nare just the bind pose and current pose equivalents, respectively, of the\nmatrix Pj!Mused in that equation.\n2. We must calculate an array of skinning matrices Kj, one for each joint j.\nThisarrayisknownasa matrixpalette. Thematrixpaletteispassedtothe\nrendering engine when rendering a skinned mesh. For each vertex, the\nrenderer looks up the appropriate joint’s skinning matrix in the palette\nand uses it to transform the vertex from bind pose into current pose.\nWe should note here that the current pose matrix Cj!Mchanges every\nframe as the character assumes different poses over time. However, the in-\nverse bind-pose matrix is constant throughout the entire game, because the\nbind pose of the skeleton is fixed when the model is created. Therefore, the\n754 12. Animation Systems\nmatrix(\nBj!M) 1is generally cached with the skeleton, and needn’t be calcu-\nlated at runtime. Animation engines generally calculate local poses for each\njoint(\nCj!p(j))\n, then use Equation (12.1) to convert these into global poses(\nCj!M)\n, and finally multiply each global pose by the corresponding cached\ninversebindposematrix(\nBj!M) 1inordertogenerateaskinningmatrix (Kj)\nfor each joint.\n12.5.2.3 Incorporating the Model-to-World Transform\nEvery vertex must eventually be transformed from model space into world\nspace. Someenginesthereforepremultiplythepaletteofskinningmatricesby\nthe object’s model-to-world transform. This can be a useful optimization, as\nit saves the rendering engine one matrix multiply per vertex when rendering\nskinned geometry. (With hundreds of thousands of vertices to process, these\nsavings can really add up!)\nTo incorporate the model-to-world transform into our skinning matrices,\nwe simply concatenate it to the regular skinning matrix equation, as follows:\n(\nKj)\nW=(\nBj!M) 1Cj!MMM!W.\nSome engines bake the model-to-world transform into the skinning matri-\nces like this, while others don’t. The choice is entirely up to the engineering\nteam and is driven by all sorts of factors. For example, one situation in which\nwewoulddefinitely notwanttodothisiswhenasingleanimationisbeingap-\nplied to multiple characters simultaneously—a technique known as animation\ninstancing that is sometimes used for animating large crowds of characters. In\nthis case we need to keep the model-to-world transforms separate so that we\ncan share a single matrix palette across all characters in the crowd.\n12.5.2.4 Skinning a Vertex to Multiple Joints\nWhen a vertex is skinned to more than one joint, we calculate its final position\nby assuming it is skinned to each joint individually, calculating a model-space\nposition for each joint and then taking a weightedaverage of the resulting posi-\ntions. The weights are provided by the character rigging artist, and they must\nalways sum to one. (If they do not sum to one, they should be renormalized\nby the tools pipeline.)\nThe general formula for a weighted average of Nquantities a0through\naN 1, with weights w0through wN 1and with åwi=1is:\na=N 1\nå\ni=0wiai.",10616
84-12.6 Animation Blending.pdf,84-12.6 Animation Blending,"12.6. Animation Blending 755\nThis works equally well for vector quantities ai. So, for a vertex skinned to\nNjoints with indices j0through jN 1and weights w0through wN 1, we can\nextend Equation (12.4) as follows:\nvC\nM=N 1\nå\ni=0wivB\nMKji,\nwhere Kjiis the skinning matrix for the joint ji.\n12.6 Animation Blending\nThe term animationblending refers to any technique that allows more than one\nanimation clip to contribute to the final pose of the character. To be more pre-\ncise, blending combines two or more input poses to produce an output pose for\nthe skeleton.\nBlendingusuallycombinestwoormoreposesatasinglepointintime,and\ngenerates an output at that same moment in time. In this context, blending\nis used to combine two or more animations into a host of new animations,\nwithout having to create them manually. For example, by blending an injured\nwalkanimationwithanuninjuredwalk,wecangeneratevariousintermediate\nlevels of apparent injury for our character while he is walking. As another\nexample,wecanblendbetweenananimationinwhichthecharacterisaiming\nto the left and one in which he’s aiming to the right, in order to make the\ncharacter aim along any desired angle between the two extremes. Blending\ncan be used to interpolate between extreme facial expressions, body stances,\nlocomotion modes and so on.\nBlendingcanalsobeusedtofindanintermediateposebetweentwoknown\nposes at different points in time. This is used when we want to find the pose\nof a character at a point in time that does not correspond exactly to one of the\nsampled frames available in the animation data. We can also use temporal\nanimation blending to smoothly transition from one animation to another, by\ngradually blending from the source animation to the destination over a short\nperiod of time.\n12.6.1 LERP Blending\nGiven a skeleton with Njoints, and two skeletal poses Pskel\nA={(PA)j}jN 1\nj=0\nandPskel\nB={(PB)j}jN 1\nj=0, we wish to find an intermediate pose Pskel\nLERPbe-\ntween these two extremes. This can be done by performing a linear interpola-\ntion(LERP) between the local poses of each individual joint in each of the two\n756 12. Animation Systems\nsource poses. This can be written as follows:\n(PLERP )j=LERP((PA)j,(PB)j,b)\n(12.5)\n= (1 b)(PA)j+b(PB)j.\nThe interpolated pose of the whole skeleton is simply the set of interpolated\nposes for all of the joints:\nPskel\nLERP ={(PLERP )j}N 1\nj=0. (12.6)\nIn these equations, bis called the blend percentage orblend factor. When\nb=0, the final pose of the skeleton will exactly match Pskel\nA; when b=1,\nthefinal pose will match Pskel\nB. When bis between zero and one, the final\npose is an intermediate between the two extremes. This effect is illustrated in\nFigure 12.11.\nWe’veglossedoveronesmalldetailhere: Wearelinearlyinterpolating joint\nposes, which means interpolating 44transformation matrices. But, as we saw\nin Chapter 5, interpolating matrices directly is not practical. This is one of the\nreasonswhylocalposesareusuallyexpressedinSRTformat—doingsoallows\nus to apply the LERP operation defined in Section 5.2.5 to each component of\nthe SRT individually. The linear interpolation of the translation component T\nof an SRT is just a straightforward vector LERP:\n(TLERP )j=LERP((TA)j,(TB)j,b)\n(12.7)\n= (1 b)(TA)j+b(TB)j.\nThe linear interpolation of the rotation component is a quaternion LERP or\nSLERP (spherical linear interpolation):\n(QLERP )j=normalize(\nLERP((QA)j,(QB)j,b))\n(12.8)\n=normalize((1 b)(QA)j+b(QB)j)\n.\nor\n(QSLERP )j=SLERP((QA)j,(QB)j,b)\n(12.9)\n=sin((1 b)q)\nsin(q)(QA)j+sin(bq)\nsin(q)(QB)j.\nFinally,thelinearinterpolationofthescalecomponentiseitherascalarorvec-\ntor LERP, depending on the type of scale (uniform or nonuniform scale) sup-\nported by the engine:\n(SLERP )j=LERP((SA)j,(SB)j,b)\n(12.10)\n= (1 b)(SA)j+b(SB)j.\n12.6. Animation Blending 757\nor\n(SLERP )j=LERP((SA)j,(SB)j,b)\n(12.11)\n= (1 b)(SA)j+b(SB)j.\nWhen linearly interpolating between two skeletal poses, the most natural-\nlookingintermediate pose is generallyone in which eachjoint pose is interpo-\nlatedindependentlyoftheothers,inthespaceofthatjoint’simmediateparent.\nInotherwords,poseblendingisgenerallyperformedon localposes. Ifwewere\nto blend global poses directly in model space, the results would tend to look\nbiomechanically implausible.\nBecause pose blending is done on local poses, the linear interpolation of\nany one joint’s pose is totally independent of the interpolations of the other\njoints in the skeleton. This means that linear pose interpolation can be per-\nformed entirely in parallel on multiprocessor architectures.\n12.6.2 Applications of LERP Blending\nNowthatweunderstandthebasicsofLERPblending,let’shavealookatsome\ntypical gaming applications.\n12.6.2.1 Temporal Interpolation\nAs we mentioned in Section 12.4.1.1, game animations are almost never sam-\npled exactly on integer frame indices. Because of variable frame rate, the\nplayer might actually see frames 0.9, 1.85 and 3.02, rather than frames 1, 2 and\n3 as one might expect. In addition, some animation compression techniques\ninvolve storing only disparate key frames, spaced at uneven intervals across\nthe clip’s local timeline. In either case, we need a mechanism for finding in-\ntermediate poses between the sampled poses that are actually present in the\nanimation clip.\nLERP blending is typically used to find these intermediate poses. As an\nexample, let’s imagine that our animation clip contains evenly spaced pose\nsamples at times 0, ∆t,2∆t,3∆tand so on. To find a pose at time t=2.18∆t,\nwe simply find the linear interpolation between the poses at times 2∆tand\n3∆t, using a blend percentage of b=0.18.\nIn general, we can find the pose at time tgiven pose samples at any two\ntimes t1andt2that bracket t, as follows:\nPj(t) = LERP(\nPj(t1),Pj(t2),b(t))\n(12.12)\n=(1 b(t))Pj(t1) +b(t)Pj(t2), (12.13)\nwhere the blend factor b(t)can be determined by the ratio\nb(t) =t t1\nt2 t1. (12.14)\n758 12. Animation Systems\n12.6.2.2 Motion Continuity: Cross-Fading\nGame characters are animated by piecing together a large number of fine-\ngrained animation clips. If your animators are any good, the character will\nappear to move in a natural and physically plausible way withineach indi-\nvidual clip. However, it is notoriously difficult to achieve the same level of\nquality when transitioning from one clip to the next. The vast majority of the\n“pops” we see in game animations occur when the character transitions from\none clip to the next.\nIdeally, we would like the movements of each part of a character’s body\nto be perfectly smooth, even during transitions. In other words, the three-\ndimensional paths traced out by each joint in the skeleton as it moves should\ncontain no sudden “jumps.” We call this C0 continuity; it is illustrated in Fig-\nure 12.29.\nNotonlyshouldthepathsthemselvesbecontinuous, buttheirfirstderiva-\ntives (velocity) should be continuous as well. This is called C1 continuity (or\ncontinuity of velocity and momentum). The perceived quality and realism\nof an animated character’s movement improves as we move to higher- and\nhigher-order continuity. For example, we might want to achieve C2 continu-\nity, in which the second derivatives of the motion paths (acceleration curves)\nare also continuous.\nStrict mathematical continuity up to C1 or higher is often infeasible to\nachieve. However, LERP-based animation blending can be applied to achieve\na reasonably pleasing form of C0 motion continuity. It usually also does a\npretty good job of approximating C1 continuity. When applied to transitions\nbetween clips in this manner, LERP blending is sometimes called cross-fading .\nLERPblendingcanintroduceunwantedartifacts,suchasthedreaded“sliding\nfeet” problem, so it must be applied judiciously.\nTocross-fadebetweentwoanimations,weoverlapthetimelinesofthetwo\nclips by some reasonable amount, and then blend the two clips together. The\nblend percentage bstarts at zero at time tstart, meaning that we see only clip A\ntTx7\ntTx7discontinuity\nC0 continuous not C0 continuous\nFigure 12.29. The channel function on the left has C0 continuity, while the path on the right does\nnot.\n12.6. Animation Blending 759\nwhenthecross-fadebegins. Wegraduallyincrease buntilitreachesavalueof\none at time tend. At this point only clip B will be visible, and we can retire clip\nA altogether. The time interval over which the cross-fade occurs (∆tblend =\ntend tstart)is sometimes called the blend time.\nTypes of Cross-Fades\nThere are two common ways to perform a cross-blended transition:\n•Smoothtransition . Clips A and B both play simultaneously as bincreases\nfromzerotoone. Forthistoworkwell,thetwoclipsmustbeloopingan-\nimations, and their timelines must be synchronized so that the positions\nofthelegs and armsin oneclip matchup roughlywiththeir positionsin\nthe other clip. (If this is not done, the cross-fade will often look totally\nunnatural.) This technique is illustrated in Figure 12.30.\n•Frozen transition. The local clock of clip A is stopped at the moment clip\nB starts playing. Thus, the pose of the skeleton from clip A is frozen\nwhileclipBgraduallytakesoverthemovement. Thiskindoftransitional\nblend works well when the two clips are unrelated and cannot be time-\nsynchronized,astheymustbewhenperforminga smoothtransition. This\napproach is depicted in Figure 12.31.\nWe can also control how the blend factor bvaries during the transition.\nIn Figure 12.30 and Figure 12.31, the blend factor varied linearly with time.\nTo achieve an even smoother transition, we could vary baccording to a cubic\nfunction of time, such as a one-dimensional Bézier. When such a curve is ap-\nplied to a currently running clip that is being blended out, it is known as an\nease-out curve ; when it is applied to a new clip that is being blended in, it is\nknown as an ease-in curve . This is shown in Figure 12.32.\nClip A\ntClip B\ntstart tend\nFigure 12.30. A smooth transition, in which the local clocks of both clips keep running during the\ntransition.\n760 12. Animation Systems\nClip A\ntClip B\nA’s local ti meline \nfreezes here\ntstart tend\nFigure 12.31. A frozen transition, in which clip A’s local clock is stopped during the transition.\nThe equation for a Bézier ease-in/ease-out curve is given below. It returns\nthe value of bat any time twithin the blend interval. bstartis the blend factor\nat the start of the blend interval tstart, and bendis the final blend factor at time\ntend. Theparameter uisthenormalizedtime between tstartandtend,andforcon-\nvenience we’ll also define v=1 u(theinversenormalized time). Note that\nthe Bézier tangents TstartandTendare taken to be equal to the corresponding\nblend factors bstartandbend, because this yields a well-behaved curve for our\npurposes:\nletu=(t tstart\ntend tstart)\nandv=1 u.\nb(t) = ( v3)bstart+ (3v2u)Tstart+ (3vu2)Tend+ (u3)bend\n= (v3+3v2u)bstart+ (3vu2+u3)bend.\nCore Poses\nThis is an appropriate time to mention that motion continuity can actually be\nClip A\ntClip B\ntstart tend\nFigure 12.32. A smooth transition, with a cubic ease-in/ease-out curve applied to the blend factor.\n12.6. Animation Blending 761\nTargeted PivotalPath of \nMovement\nFigure 12.33. In pivotal movement, the character faces the direction she is moving and pivots about\nher vertical axis to turn. In targeted movement, the movement direction need not match the facing\ndirection.\nachieved without blending if the animator ensures that the last pose in any\ngiven clip matches the first pose of the clip that follows it. In practice, anima-\ntors often decide upon a set of core poses—for example, we might have a core\npose for standing upright, one for crouching, one for lying prone and so on.\nBymakingsurethatthecharacterstartsinoneofthesecoreposesatthebegin-\nning of every clip and returns to a core pose at the end, C0 continuity can be\nachieved by simply ensuring that the core poses match when animations are\nspliced together. C1 or higher-order motion continuity can also be achieved\nby ensuring that the character’s movement at the end of one clip smoothly\ntransitions into the motion at the start of the next clip. This can be achieved\nbyauthoringasinglesmoothanimationandthenbreakingitintotwoormore\nclips.\n12.6.2.3 Directional Locomotion\nLERP-based animation blending is often applied to character locomotion.\nWhen a real human being walks or runs, he can change the direction in which\nhe is moving in two basic ways: First, he can turn his entire body to change\ndirection, in which case he always faces in the direction he’s moving. I’ll call\nthispivotal movement , because the person pivots about his vertical axis when\nhe turns. Second, he can keep facing in one direction while walking forward,\nbackward or sideways (known as strafing in the gaming world) in order to\nmove in a direction that is independent of his facing direction. I’ll call this tar-\ngeted movement, because it is often used in order to keep one’s eye—or one’s\nweapon—trained on a target while moving. These two movement styles are\nillustrated in Figure 12.33.\nTargeted Movement\nTo implement targeted movement , the animator authors three separate looping\n762 12. Animation Systems\nanimation clips—one moving forward, one strafing to the left, and one straf-\ning to the right. I’ll call these directional locomotion clips. The three directional\nclips are arranged around the circumference of a semicircle, with forward at\n0degrees, left at 90degrees and right at  90degrees. With the character’s\nfacingdirectionfixedat 0degrees, wefindthedesiredmovementdirectionon\nthe semicircle, select the two adjacent movement animations and blend them\ntogether via LERP-based blending. The blend percentage bis determined by\nhow close the angle of movement is to the angles of two adjacent clips. This is\nillustrated in Figure 12.34.\nNote that we did not include backward movement in our blend, for a full\ncircularblend. Thisisbecauseblendingbetweenasidewaysstrafeandaback-\nwardruncannotbemadetolooknaturalingeneral. Theproblemisthatwhen\nstrafing to the left, the character usually crosses its right foot in front of its left\nso that the blend into the pure forward runanimation looks correct. Likewise,\nthe right strafe is usually authored with the left foot crossing in front of the\nright. When we try to blend such strafe animations directly into a backward\nrun, one leg will start to pass through the other, which looks extremely awk-\nward and unnatural. There are a number of ways to solve this problem. One\nfeasible approach is to define two hemispherical blends, one for forward mo-\ntionandoneforbackwardmotion, eachwithstrafeanimationsthathavebeen\ncrafted to work properly when blended with the corresponding straight run.\nWhenpassingfromonehemispheretotheother,wecanplaysomekindofex-\nplicit transition animation so that the character has a chance to adjust its gait\nand leg crossing appropriately.\nStrafe\nRightStrafe\nLeftRun\nForward\nFigure 12.34. Targeted movement can be implemented by blending together looping locomotion\nclips that move in each of the four principal directions.\n12.6. Animation Blending 763\nClip A\nb0 b1 b2 b3 b4Clip B Clip C Clip D Clip E\nb\n1 21\nb bb b\n−−=β\nFigure 12.35. A generalized linear blend between N animation clips.\nPivotal Movement\nTo implement pivotal movement , we can simply play the forward locomotion\nloop while rotating the entire character about its vertical axis to make it turn.\nPivotal movement looks more natural if the character’s body doesn’t remain\nbolt upright when it is turning—real humans tend to lean into their turns a\nlittle bit. We could try slightly tilting the vertical axis of the character as a\nwhole, but that would cause problems with the inner foot sinking into the\nground while the outer foot comes off the ground. A more natural-looking\nresultcanbeachievedbyanimatingthreevariationsonthebasicforwardwalk\norrun—onegoingperfectlystraight, onemakinganextremeleftturnandone\nmaking an extreme right turn. We can then LERP-blend between the straight\nclip and the extreme left turn clip to implement any desired lean angle.\n12.6.3 Complex LERP Blends\nIn a real game engine, characters make use of a wide range of complex blends\nfor various purposes. It can be convenient to “prepackage” certain commonly\nused types of complex blends for ease of use. In the following sections, we’ll\ninvestigate a few popular types of prepackaged complex blends.\n12.6.3.1 Generalized One-Dimensional LERP Blending\nLERP blending can be easily extended to more than two animation clips, us-\ning a technique I call one-dimensional LERP blending. We define a new blend\nparameter bthat lies in any linear range desired (e.g., from  1to+1, or from\n0 to 1,or even from 27 to 136). Any number of clips can be positioned at arbi-\ntrary points along this range, as shown in Figure 12.35. For any given value of\nb, we select the two clips immediately adjacent to it and blend them together\nusing Equation (12.5). If the two adjacent clips lie at points b1andb2, then\ntheblendpercentage bcanbedeterminedusingatechniqueanalogoustothat\nused in Equation (12.14), as follows:\nb(t) =b b1\nb2 b1. (12.15)\n764 12. Animation Systems\nStrafe\nRightStrafe\nLeftRun\nForwardStrafe\nRight\nb2Run\nFwdStrafe\nLeft\nbb3 b1\nFigure 12.36. The directional clips used in targeted movement can be thought of as a special case\nof one-dimensional LERP blending.\nTargeted movement is just a special case of one-dimensional LERP blend-\ning. We simply straighten out the circle on which the directional animation\nclips were placed and use the movement direction angle qas the parameter b\n(with a range of 90to90degrees). Any number of animation clips can be\nplacedontothisblendrangeatarbitraryangles. ThisisshowninFigure12.36.\n12.6.3.2 Simple Two-Dimensional LERP Blending\nSometimeswewouldliketosmoothlyvary twoaspectsofacharacter’smotion\nsimultaneously. For example, we might want the character to be capable of\naiminghisweaponverticallyandhorizontally. Orwemightwanttoallowour\ncharacter to vary her pace length and the separation of her feet as she moves.\nWe can extend one-dimensional LERP blending to two dimensions in order to\nachieve these kinds of effects.\nIf we know that our 2D blend involves only four animation clips, and if\nthose clips are positioned at the four corners of a square region, then we can\nfind a blended pose by performing two 1D blends. Our generalized blend\nfactor bbecomesatwo-dimensionalblendvector b=[bxby]\n. Ifblieswithin\nthesquareregionboundedbyourfourclips,wecanfindtheresultingposeby\nfollowing these steps:\n1. Using the horizontal blend factor bx, find two intermediate poses, one\nbetween the top two animation clips and one between the bottom two\nclips. These two poses can be found by performing two simple one-\n12.6. Animation Blending 765\nbxby\nFigure 12.37. A simple formulation for 2D animation blending between four clips at the corners of\na square region.\ndimensional LERP blends.\n2. Using the vertical blend factor by, find the final pose by LERP-blending\nthe two intermediate poses together.\nThis technique is illustrated in Figure 12.37.\n12.6.3.3 Triangular Two-Dimensional LERP Blending\nThesimple2Dblendingtechniqueweinvestigatedintheprevioussectiononly\nworks when the animation clips we wish to blend lie at the corners of a rect-\nangular region. How can we blend between an arbitrary number of clips po-\nsitioned at arbitrary locations in our 2D blend space?\nLet’s imagine that we have three animation clips that we wish to blend to-\ngether. Each clip, designated by the index i, corresponds to a particular blend\ncoordinate bi=[bixbiy]\nin our two-dimensional blend space; these three\nblend coordinates form a triangle within the blend space. Each of the three\nclips defines a set of joint poses{(Pi)j}N 1\nj=0, where (Pi)jis the pose of joint j\nas defined by clip i, and Nis the number of joints in the skeleton. We wish to\nfind the interpolated pose of the skeleton corresponding to an arbitrary point\nbwithin the triangle, as illustrated in Figure 12.38.\nBut how can we calculate a LERP blend between three animation clips?\nThankfully, the answer is simple: the LERP function can actually operate on\nany number of inputs, because it is really just a weighted average. As with any\nweighted average, the weights must add to one. In the case of a two-input\nLERP blend, we used the weights band (1 b), which of course add to one.\nForathree-inputLERP,wesimplyusethreeweights, a,bandg= (1 a b).\n766 12. Animation Systems\nClip A\nb0by\nClip B\nClip Cbb1\nb2bxFinal\nBlend\nFigure 12.38. Two-dimensional animation blending between three animation clips.\nThen we calculate the LERP as follows:\n(PLERP )j=a(P0)j+b(P1)j+g(P2)j. (12.16)\nGiven the two-dimensional blend vector b, we find the blend weights a,\nbandgby finding the barycentric coordinates of the point brelative to the tri-\nangle formed by the three clips in two-dimensional blend space (http://en.\nwikipedia.org/wiki/Barycentric_coordinates_%28mathematics%29). In gen-\neral,thebarycentriccoordinatesofapoint bwithinatrianglewithvertices b1,\nb2andb3are three scalar values (a,b,g)that satisfy the relations\nb=ab0+bb1+gb2, (12.17)\nanda+b+g=1.\nThese are exactly the weights we seek for our three-clip weighted average.\nBarycentric coordinates are illustrated in Figure 12.39.\nNotethatpluggingthebarycentriccoordinate(1,0,0)intoEquation(12.17)\nyields b0, while (0, 1, 0 )gives us b1and (0, 0, 1 )produces b2. Likewise, plug-\nging these blend weights into Equation (12.16) gives us poses (P0)j,(P1)jand\n(P2)jfor each joint j, respectively. Furthermore, the barycentric coordinate (1\n3,\n1\n3,1\n3)liesatthecentroidofthetriangleandgivesusan equalblendbetweenthe\nthree poses. This is exactly what we’d expect.\n12.6.3.4 Generalized Two-Dimensional LERP Blending\nThe barycentric coordinate technique can be extended to an arbitrary number\nofanimationclipspositionedatarbitrarylocationswithinthetwo-dimension-\nal blend space. We won’t describe it in its entirety here, but the basic idea is\nto use a technique known as Delaunaytriangulation (http://en.wikipedia.org/\n12.6. Animation Blending 767\nb0by\nbb1\nb2α\nβ\nγ\nbx\nFigure 12.39. Various barycentric coordinates within a triangle.\nwiki/Delaunay_triangulation) to find a set of triangles given the positions of\nthe various animation clips bi. Once the triangles have been determined, we\ncanfindthetrianglethatenclosesthedesiredpoint bandthenperformathree-\nclip LERP blend as described above. This technique was used in FIFA soccer\nby EA Sports in Vancouver, implemented within their proprietary “ANT” an-\nimation framework. It is shown in Figure 12.40.\n12.6.4 Partial-Skeleton Blending\nA human being can control different parts of his or her body independently.\nFor example, I can wave my right arm while walking and pointing at some-\nthing with my left arm. One way to implement this kind of movement in a\nClip A\nb0\nClip Bb1\nClip C\nClip DClip E\nClip F\nClip G\nClip H Clip IClip Jb2\nb3b4 b5\nb6\nb7\nb8b9by\nbx\nFigure 12.40. Delaunay triangulation between an arbitrary number of animation clips positioned\nat arbitrary locations in two-dimensional blend space.\n768 12. Animation Systems\ngame is via a technique known as partial-skeletonblending.\nRecall from Equations (12.5) and (12.6) that when doing regular LERP\nblending, the same blend percentage bwas used for every joint in the skele-\nton. Partial-skeleton blending extends this idea by permitting the blend per-\ncentage to vary on a per-joint basis. In other words, for each joint j, we define\na separate blend percentage bj. The set of all blend percentages for the entire\nskeleton{\nbj}N 1\nj=0is sometimes called a blend mask because it can be used to\n“mask out” certain joints by setting their blend percentages to zero.\nAs an example, let’s say we want our character to wave at someone using\nhis right arm and hand. Moreover, we want him to be able to wave whether\nhe’swalking, runningorstandingstill. Toimplementthisusingpartialblend-\ning, the animator defines three full-body animations: Walk,RunandStand.\nThe animator also creates a single waving animation, Wave. A blend mask\nis created in which the blend percentages are zero everywhere except for the\nright shoulder, elbow, wrist and finger joints, where they are equal to one:\nbj={1when jwithin right arm,\n0otherwise.\nWhenWalk,RunorStandis LERP-blended with Waveusing this blend mask,\ntheresultisacharacterwhoappearstobewalking, runningorstandingwhile\nwaving his right arm.\nPartialblendingisuseful,butithasatendencytomakeacharacter’smove-\nments look unnatural. This occurs for two basic reasons:\n• Anabruptchangeintheper-jointblendfactorscancausethemovements\nof one part of the body to appear disconnected from the rest of the body.\nIn our example, the blend factors change abruptly at the right shoulder\njoint. Hence the animation of the upper spine, neck and head are being\ndriven by one animation, while the right shoulder and arm joints are\nbeing entirely driven by a different animation. This can look odd. The\nproblem can be mitigated somewhat by gradually changing the blend\nfactors rather than doing it abruptly. (In our example, we might select a\nblend percentage of 0.9 at the right shoulder, 0.5 on the upper spine and\n0.2 on the neck and mid-spine.)\n• Themovementsofarealhumanbodyarenevertotallyindependent. For\nexample, one would expect a person’s wave to look more “bouncy” and\noutofcontrolwhenheorsheisrunningthanwhenheorsheisstanding\nstill. Yetwithpartialblending,therightarm’sanimationwillbeidentical\nno matter what the rest of the body is doing. This problem is difficult to\n12.6. Animation Blending 769\novercome using partial blending. Instead, many game developers have\nturned to a more natural-looking technique known as additive blending.\n12.6.5 Additive Blending\nAdditive blending approaches the problem of combining animations in a to-\ntally new way. It introduces a new kind of animation called a difference clip ,\nwhich, as its name implies, represents the difference between two regular an-\nimation clips. A difference clip can be added onto a regular animation clip in\norder to produce interesting variations in the pose and movement of the char-\nacter. In essence, a difference clip encodes the changes that need to be made to\none pose in order to transform it into another pose. Difference clips are often\ncalledadditive animation clips in the game industry. We’ll stick with the term\ndifference clip in this book because it more accurately describes what is going\non.\nConsider two input clips called the source clip (S) and the reference clip (R).\nConceptually, the difference clip is D =S R. If a difference clip D is added\nto its original reference clip, we get back the source clip (S=D+R). We\ncan also generate animations that are partway between R and S by adding a\npercentage of D to R, in much the same way that LERP blending finds inter-\nmediate animations between two extremes. However, the real beauty of the\nadditive blending technique is that once a difference clip has been created, it\ncan be added to other unrelated clips, not just to the original reference clip.\nWe’ll call these animations targetclips and denote them with the symbol T.\nAsanexample,ifthereferencecliphasthecharacterrunningnormallyand\nthe source clip has him running in a tired manner, then the difference clip will\ncontain only the changes necessary to make the character look “tired” while\nrunning. Ifthisdifferenceclipisnowappliedtoaclipofthecharacterwalking,\nthe resulting animation can make the character look tired while walking. A\nwhole host of interesting and very natural-looking animations can be created\nby adding a single difference clip onto various “regular” animation clips, or a\ncollectionofdifferenceclipscanbecreated,eachofwhichproducesadifferent\neffect when added to a single target animation.\n12.6.5.1 Mathematical Formulation\nA difference animation D is defined as the difference between some source\nanimation S and some reference animation R. So conceptually, the difference\npose (at a single point in time) is D =S R. Of course, we’re dealing with\njoint poses, not scalar quantities, so we cannot simply subtract the poses. In\ngeneral, a joint pose is a 44affine transformation matrix that transforms\n770 12. Animation Systems\npoints and vectors from the child joint’s local space to the space of its parent\njoint. The matrix equivalent of subtraction is multiplication by the inverse\nmatrix. So given the source pose Sjand the reference pose Rjfor any joint jin\nthe skeleton, we can define the difference pose Djat that joint as follows. (For\nthisdiscussion,we’lldroptheC !Porj!p(j)subscript,asitisunderstood\nthat we are dealing with child-to-parent pose matrices.)\nDj=SjR 1\nj.\n“Adding” a difference pose Djonto a target pose Tjyields a new additive\npose Aj. This is achieved by simply concatenating the difference transform\nand the target transform as follows:\nAj=DjTj=(\nSjR 1\nj)\nTj. (12.18)\nWe can verify that this is correct by looking at what happens when the differ-\nence pose is “added” back onto the original reference pose:\nAj=DjRj\n=SjR 1\njRj\n=Sj.\nIn other words, adding the difference animation D back onto the original ref-\nerence animation R yields the source animation S, as we’d expect.\nTemporal Interpolation of Difference Clips\nAs we learned in Section 12.4.1.1, game animations are almost never sampled\non integer frame indices. To find a pose at an arbitrary time t, we must of-\ntentemporally interpolate between adjacent pose samples at times t1andt2.\nThankfully, difference clips can be temporally interpolated just like their non-\nadditive counterparts. We can simply apply Equations (12.12) and (12.14) di-\nrectly to our difference clips as if they were ordinary animations.\nNote that a difference animation can only be found when the input clips\nS and R are of the same duration. Otherwise there would be a period of time\nduring which either S or R is undefined, meaning D would be undefined as\nwell.\nAdditive Blend Percentage\nIngames,weoftenwishtoblendinonlyapercentageofadifferenceanimation\ntoachievevaryingdegreesoftheeffectitproduces. Forexample,ifadifference\nclip causes the character to turn his head 80 degrees to the right, blending in\n12.6. Animation Blending 771\n50% of the difference clip should make him turn his head only 40 degrees to\nthe right.\nTo accomplish this, we turn once again to our old friend LERP. We wish\nto interpolate between the unaltered target animation and the new animation\nthat would result from a full application of the difference animation. To do\nthis, we extend Equation (12.18) as follows:\nAj=LERP(\nTj,DjTj,b)\n(12.19)\n=(1 b)(\nTj)+b(\nDjTj)\n.\nAs we saw in Chapter 5, we cannot LERP matrices directly. So Equation\n(11.16) must be broken down into three separate interpolations for S, Q and T,\njust as we did in Equations (12.7) through (12.11).\n12.6.5.2 Additive Blending versus Partial Blending\nAdditive blending is similar in some ways to partial blending. For example,\nwecantakethedifferencebetweenastandingclipandaclipofstandingwhile\nwaving the right arm. The result will be almost the same as using a partial\nblend to make the right arm wave. However, additive blends suffer less from\nthe “disconnected” look of animations combined via partial blending. This is\nbecause, with an additive blend, we are not replacing the animation for a sub-\nset of joints or interpolating between two potentially unrelated poses. Rather,\nweareaddingmovementtotheoriginalanimation—possiblyacrosstheentire\nskeleton. In effect, a difference animation “knows” how to change a charac-\nter’sposeinordertogethimtodosomethingspecific, likebeingtired, aiming\nhis head in a certain direction, or waving his arm. These changes can be ap-\nplied to a reasonably wide variety of animations, and the result often looks\nvery natural.\n12.6.5.3 Limitations of Additive Blending\nOfcourse, additiveanimationisnotasilverbullet. Becauseitaddsmovement\nto an existing animation, it can have a tendency to over-rotate the joints in the\nskeleton,especiallywhenmultipledifferenceclipsareappliedsimultaneously.\nAs a simple example, imagine a target animation in which the character’s left\narm is bent at a 90 degree angle. If we add a difference animation that also\nrotates the elbow by 90 degrees, then the net effect would be to rotate the arm\nby90+90=180degrees. This would cause the lower arm to interpenetrate\nthe upper arm—not a comfortable position for most individuals!\nClearlywemustbecarefulwhenselectingthereferenceclipandalsowhen\nchoosing the target clips to which to apply it. Here are some simple rules of\nthumb:\n772 12. Animation Systems\n• Keep hip rotations to a minimum in the reference clip.\n• The shoulder and elbow joints should usually be in neutral poses in the\nreference clip to minimize over-rotation of the arms when the difference\nclip is added to other targets.\n• Animators should create a new difference animation for each core pose\n(e.g., standing upright, crouched down, lying prone, etc.). This allows\nthe animator to account for the way in which a real human would move\nwhen in each of these stances.\nThese rules of thumb can be a helpful starting point, but the only way to\nreally learn how to create and apply difference clips is by trial and error or by\napprenticing with animators or engineers who have experience creating and\napplying difference animations. If your team hasn’t used additive blending\nin the past, expect to spend a significant amount of time learning the art of\nadditive blending.\n12.6.6 Applications of Additive Blending\n12.6.6.1 Stance Variation\nOne particularly striking application of additive blending is stance variation.\nFor each desired stance, the animator creates a one-frame difference anima-\ntion. When one of these single-frame clips is additively blended with a base\nanimation, it causes the entire stance of the character to change drastically\nwhile he continues to perform the fundamental action he’s supposed to per-\nform. This idea is illustrated in Figure 12.41.\nTarget +\nDifference ATarget +\nDifference BTarget Clip\n(and Reference)\nFigure 12.41. Two single-frame difference animations A and B can cause a target animation clip to\nassume two totally different stances. (Character from Uncharted: Drake’s Fortune , © 2007/® SIE.\nCreated and developed by Naughty Dog.)\n12.6. Animation Blending 773\nTarget Clip\n(and Reference )\nTarget  +\nDifference ATarget  +\nDifference BTarget +\nDifference C\nFigure 12.42. Additive blends can be used to add variation to a repetitive idle animation. Images\ncourtesy of Naughty Dog, Inc., © 2014/™ SIE.\n12.6.6.2 Locomotion Noise\nRealhumansdon’trunexactlythesamewaywitheveryfootfall—thereisvari-\nation in their movement over time. This is especially true if the person is dis-\ntracted (for example, by attacking enemies). Additive blending can be used to\nlayer randomness, or reactions to distractions, on top of an otherwise entirely\nrepetitive locomotion cycle. This is illustrated in Figure 12.42.\n12.6.6.3 Aim and Look-At\nAnother common use for additive blending is to permit the character to look\naround or to aim his weapon. To accomplish this, the character is first ani-\nmated doing some action, such as running, with his head or weapon facing\nstraight ahead. Then the animator changes the direction of the head or the\naim of the weapon to the extreme right and saves off a one-frame or multi-\nframe difference animation. This process is repeated for the extreme left, up\nand down directions. These four difference animations can then be additively\nblendedonto theoriginal straight-ahead animationclip, causing the character\nto aim right, left, up, down or anywhere in between.\nThe angle of the aim is governed by the additive blend factor of each clip.\nForexample,blendingin100%oftherightadditivecausesthecharactertoaim\nas far right as possible. Blending 50% of the left additive causes him to aim at",36343
85-12.7 Post-Processing.pdf,85-12.7 Post-Processing,"774 12. Animation Systems\nTarget +\nDifference RightTarget +\nDifference LeftTarget Clip\n(and Reference)\n0% Right\n0% Left100% Right 100% Left\nFigure 12.43. Additive blending can be used to aim a weapon. Screenshots courtesy of Naughty\nDog, Inc., © 2014/™ SIE.\nan angle that is one-half of his leftmost aim. We can also combine this with an\nup or down additive to aim diagonally. This is demonstrated in Figure 12.43.\n12.6.6.4 Overloading the Time Axis\nIt’s interesting to note that the time axis of an animation clip needn’t be used\nto represent time. For example, a three-frame animation clip could be used to\nprovide three aim poses to the engine—a left aim pose on frame 1, a forward\naim pose on frame 2 and a right aim pose on frame 3. To make the character\naimtotheright,wecansimplyfixthelocalclockoftheaimanimationonframe\n3. To perform a 50% blend between aiming forward and aiming right, we can\ndial in frame 2.5. This is a great example of leveraging existing features of the\nengine for new purposes.\n12.7 Post-Processing\nOnceaskeletonhasbeenposedbyoneormoreanimationclipsandtheresults\nhave been blended together using linear interpolation or additive blending, it\nis often necessary to modify the pose prior to rendering the character. This is\ncalledanimation post-processing . In this section, we’ll look at a few of the most\ncommon kinds of animation post-processing.\n12.7. Post-Processing 775\n12.7.1 Procedural Animations\nAproceduralanimation isanyanimationgeneratedatruntimeratherthanbeing\ndriven by data exported from an animation tool such as Maya. Sometimes,\nhand-animated clips are used to pose the skeleton initially, and then the pose\nis modified in some way via procedural animation as a post-processing step.\nA procedural animation can also be used as an input to the system in place of\na hand-animated clip.\nForexample,imaginethataregularanimationclipisusedtomakeavehicle\nappeartobebouncingupanddownontheterrainasitmoves. Thedirectionin\nwhich the vehicle travels is under player control. We would like to adjust the\nrotationofthefrontwheelsandsteeringwheelsothattheymoveconvincingly\nwhen the vehicle is turning. This can be done by post-processing the pose\ngenerated by the animation. Let’s assume that the original animation has the\nfronttirespointingstraightaheadandthesteeringwheelinaneutralposition.\nWe can use the current angle of turn to create a quaternion about the vertical\naxisthatwilldeflectthefronttiresbythedesiredamount. Thisquaternioncan\nbe multiplied with the front tire joints’ Q channel to produce the final pose of\nthetires. Likewise,wecangenerateaquaternionabouttheaxisofthesteering\ncolumn and multiply it into the steering wheel joint’s Q channel to deflect it.\nThese adjustments are made to the local pose, prior to global pose calculation\nand matrix palette generation (see Section 12.5).\nAs another example, let’s say that we wish to make the trees and bushes\nin our game world sway naturally in the wind and get brushed aside when\ncharacters move through them. We can do this by modeling the trees and\nbushes as skinned meshes with simple skeletons. Procedural animation can\nbe used, in place of or in addition to hand-animated clips, to cause the joints\nto move in a natural-looking way. We might apply one or more sinusoids, or\na Perlin noise function, to the rotation of various joints to make them sway in\nthe breeze, and when a character moves through a region containing a bush\nor grass, we can deflect its root joint quaternion radially outward to make it\nappear to be pushed over by the character.\n12.7.2 Inverse Kinematics\nLet’s say we have an animation clip in which a character leans over to pick up\nanobjectfromtheground. InMaya,thecliplooksgreat,butinourproduction\ngame level, the ground is not perfectly flat, so sometimes the character’s hand\nmisses the object or appears to pass through it. In this case, we would like to\nadjust the final pose of the skeleton so that the hand lines up exactly with the\ntargetobject. Atechniqueknownas inversekinematics (IK)canbeusedtomake\n776 12. Animation Systems\nFigure 12.44. Inverse kinematics attempts to bring an end effector joint into a target global pose\nby minimizing the error between them.\nthis happen.\nA regular animation clip is an example of forward kinematics (FK). In for-\nward kinematics, the input is a set of local joint poses, and the output is a\nglobal pose and a skinning matrix for each joint. Inverse kinematics goes in\ntheotherdirection: Theinputisthedesiredglobalposeofasinglejoint,which\nis known as the end effector. We solve for the localposes of other joints in the\nskeleton that will bring the end effector to the desired location.\nMathematically, IK boils down to an error minimization problem. As with\nmostminimizationproblems,theremightbeonesolution,manyornoneatall.\nThismakesintuitivesense: IfItrytoreachadoorknobthatisontheotherside\noftheroom,Iwon’tbeabletoreachitwithoutwalkingovertoit. IKworksbest\nwhen the skeleton starts out in a pose that is reasonably close to the desired\ntarget. This helps the algorithm to focus in on the “closest” solution and to do\nsoinareasonableamountofprocessingtime. Figure12.44showsIKinaction.\nImagine a two-joint skeleton, each of which can rotate only about a single\naxis. The rotation of these two joints can be described by a two-dimensional\nangle vector =[\nq1q2]\n. The set of all possible angles for our two joints\nforms a two-dimensional space called configurationspace. Obviously, for more\ncomplexskeletonswithmoredegreesoffreedomperjoint,configurationspace\nbecomesmultidimensional,buttheconceptsdescribedhereworkequallywell\nno matter how many dimensions we have.\nNow imagine plotting a three-dimensional graph, where for each combi-\nnation of joint rotations (i.e., for each point in our two-dimensional configu-\nration space), we plot the distance from the end effector to the desired target.\nAn example of this kind of plot is shown in Figure 12.45. The “valleys” in\nthis three-dimensional surface represent regions in which the end effector is\nas close as possible to the target. When the height of the surface is zero, the\nend effector has reached its target. Inverse kinematics, then, attempts to find",6276
86-12.8 Compression Techniques.pdf,86-12.8 Compression Techniques,"12.8. Compression Techniques 777\n 1\n 2dtarge t\nMinimum\nFigure 12.45. A three-dimensional plot of the distance from the end effector to the target for each\npoint in two-dimensional conﬁguration space. IK ﬁnds the local minimum.\nminima (low points) on this surface.\nWe won’t get into the details of solving the IK minimization problem\nhere. You can read more about IK at http://en.wikipedia.org/wiki/Inverse_\nkinematics and in Jason Weber’s article, “Constrained Inverse Kinematics”\n[47].\n12.7.3 Rag Dolls\nA character’s body goes limp when he dies or becomes unconscious. In such\nsituations, we want the body to react in a physically realistic way with its sur-\nroundings. To do this, we can use a rag doll. A rag doll is a collection of phys-\nically simulated rigid bodies, each one representing a semi-rigid part of the\ncharacter’s body, such as his lower arm or his upper leg. The rigid bodies are\nconstrained to one another at the joints of the character in such a way as to\nproduce natural-looking “lifeless” body movement. The positions and orien-\ntations of the rigid bodies are determined by the physics system and are then\nused to drive the positions and orientations of certain key joints in the charac-\nter’s skeleton. The transfer of data from the physics system to the skeleton is\ntypically done as a post-processing step.\nToreallyunderstandragdollphysics,wemustfirsthaveanunderstanding\nof how the collision and physics systems work. Rag dolls are covered in more\ndetail in Sections 13.4.8.7 and 13.5.3.8.\n12.8 Compression Techniques\nAnimationdatacantakeupalotofmemory. Asinglejointposemightbecom-\nposedoftenfloating-pointchannels(threefortranslation,fourforrotationand\n778 12. Animation Systems\nup to three more for scale). Assuming each channel contains a 4-byte floating-\npointvalue,aone-secondclipsampledat30samplespersecondwouldoccupy\n4 bytes10 channels30 samples/second = 1200 bytes per joint per second,\nor a data rate of about 1.17 KiB per joint per second. For a 100-joint skeleton\n(whichissmallbytoday’sstandards),anuncompressedanimationclipwould\noccupy 117 KiB per joint per second. If our game contained 1,000 seconds of\nanimation (which is on the low side for a modern game), the entire dataset\nwould occupy a whopping 114.4 MiB. That’s quite a lot, considering that a\nPlayStation 3 has only 256 MiB of main RAM and 256 MiB of video RAM.\nSure, the PS4 has 8 GiB of RAM. But even so—we would rather have much\nricher animations with a lot more variety than waste memory unnecessarily.\nTherefore,gameengineersinvestasignificantamountofeffortintocompress-\ning animation data in order to permit the maximum richness and variety of\nmovement at the minimum memory cost.\n12.8.1 Channel Omission\nOne simple way to reduce the size of an animation clip is to omit channels\nthat are irrelevant. Many characters do not require nonuniform scaling, so\nthe three scale channels can be reduced to a single uniform scale channel. In\nsome games, the scale channel can actually be omitted altogether for all joints\n(except possibly the joints in the face). The bones of a humanoid character\ngenerally cannot stretch, so translation can often be omitted for all joints ex-\ncept the root, the facial joints and sometimes the collar bones. Finally, because\nquaternions are always normalized, we can store only three components per\nquat (e.g., x,yandz) and reconstruct the fourth component (e.g., w) at run-\ntime.\nAs a further optimization, channels whose pose does not change over the\ncourse of the entire animation can be stored as a single sample at time t=0\nplus a single bit indicating that the channel is constant for all other values of t.\nChannel omission can significantly reduce the size of an animation clip. A\n100-joint character with no scale and no translation requires only 303 chan-\nnels—three channels for the quaternions at each joint, plus three channels for\nthe root joint’s translation. Compare this to the 1,000 channels that would be\nrequired if all ten channels were included for all 100 joints.\n12.8.2 Quantization\nAnother way to reduce the size of an animation is to reduce the size of each\nchannel. A floating-point value is normally stored in 32-bit IEEE format. This\nformat provides 23 bits of precision in the mantissa and an 8-bit exponent.\n12.8. Compression Techniques 779\nHowever, it’s often not necessary to retain that kind of precision and range in\nan animation clip. When storing a quaternion, the channel values are guaran-\nteed to lie in the range [ 1, 1]. At a magnitude of 1, the exponent of a 32-bit\nIEEE float is zero, and 23 bits of precision give us accuracy down to the sev-\nenth decimal place. Experience shows that a quaternion can be encoded well\nwith only 16 bits of precision, so we’re really wasting 16 bits per channel if we\nstore our quats using 32-bit floats.\nConverting a 32-bit IEEE float into an n-bit integer representation is called\nquantization. There are actually two components to this operation: Encoding is\nthe process of converting the original floating-point value to a quantized in-\nteger representation. Decoding is the process of recovering an approximation\nto the original floating-point value from the quantized integer. (We can only\nrecover an approximation to the original data—quantization is a lossycompres-\nsionmethodbecauseiteffectivelyreducesthenumberofbitsofprecisionused\nto represent the value.)\nTo encode a floating-point value as an integer, we first divide the valid\nrange of possible input values into Nequally sized intervals . We then deter-\nminewithinwhichintervalaparticularfloating-pointvalueliesandrepresent\nthat value by the integer index of its interval. To decode this quantized value,\nwe simply convert the integer index into floating-point format and shift and\nscale it back into the original range. Nis usually chosen to correspond to the\nrange of possible integer values that can be represented by an n-bit integer.\nFor example, if we’re encoding a 32-bit floating-point value as a 16-bit integer,\nthe number of intervals would be N=216=65,536.\nJonathan Blow wrote an excellent article on the topic of floating-point\nscalarquantizationinthe InnerProduct columnofGameDeveloperMagazine,\navailable at https://bit.ly/2J92oiU. The article presents two ways to map a\nfloating-point value to an interval during the encoding process: We can either\ntruncate the float to the next lowest interval boundary ( T encoding ), or we can\nroundthe float to the center of the enclosing interval (R encoding). Likewise,\nit describes two approaches to reconstructing the floating-point value from its\ninteger representation: We can either return the value of the left-hand side of\nthe interval to which our original value was mapped (L reconstruction ), or we\ncan return the value of the center of the interval (C reconstruction ). This gives\nus four possible encode/decode methods: TL, TC, RL and RC. Of these, TL\nand RC are to be avoided because they tend to remove or add energy to the\ndataset, which can often have disastrous effects. TC has the benefit of being\nthe most efficient method in terms of bandwidth, but it suffers from a severe\nproblem—there is no way to represent the value zero exactly. (If you encode\n0.0f, it becomes a small positive value when decoded.) RL is therefore usually\n780 12. Animation Systems\nthe best choice and is the method we’ll demonstrate here.\nThe article only talks about quantizing positive floating-point values, and\nin the examples, the input range is assumed to be [0, 1]for simplicity. How-\never, we can always shift and scale any floating-point range into the range\n[0, 1]. For example, the range of quaternion channels is [ 1, 1], but we can\nconvert this to the range [0, 1]by adding one and then dividing by two.\nThe following pair of routines encode and decode an input floating-point\nvalue lying in the range [0, 1]into an n-bit integer, according to Jonathan\nBlow’s RL method. The quantized value is always returned as a 32-bit un-\nsigned integer ( U32), but only the least-significant nbits are actually used, as\nspecified by the nBitsargument. For example, if you pass nBits==16, you\ncan safely cast the result to a U16.\nU32 CompressUnitFloatRL (F32 unitFloat, U32 nBits)\n{\n// Determine the number of intervals based on the\n// number of output bits we've been asked to produce.\nU32 nIntervals = 1u << nBits;\n// Scale the input value from the range [0, 1] into\n// the range [0, nIntervals - 1]. We subtract one\n// interval because we want the largest output value\n// to fit into nBits bits.\nF32 scaled = unitFloat * (F32)( nIntervals - 1u);\n// Finally, round to the nearest interval center. We\n// do this by adding 0.5f and then truncating to the\n// next-lowest interval index (by casting to U32).\nU32 rounded = (U32)( scaled + 0.5f);\n// Guard against invalid input values.\nif (rounded > nIntervals - 1u)\nrounded = nIntervals - 1u;\nreturn rounded;\n}\nF32 DecompressUnitFloatRL (U32 quantized , U32 nBits)\n{\n// Determine the number of intervals based on the\n// number of bits we used when we encoded the value.\nU32 nIntervals = 1u << nBits;\n// Decode by simply converting the U32 to an F32, and\n// scaling by the interval size.\n12.8. Compression Techniques 781\nF32 intervalSize = 1.0f / (F32)( nIntervals - 1u);\nF32 approxUnitFloat = (F32)quantized *intervalSize ;\nreturn approxUnitFloat ;\n}\nTo handle arbitrary input values in the range [min,max], we can use these\nroutines:\nU32 CompressFloatRL (F32 value, F32 min, F32 max,\nU32 nBits)\n{\nF32 unitFloat = (value - min) / (max - min) ;\nU32 quantized = CompressUnitFloatRL(unitFloat,\nnBits);\nreturn quantized;\n}\nF32 DecompressFloatRL (U32 quantized , F32 min, F32 max,\nU32 nBits )\n{\nF32 unitFloat = DecompressUnitFloatRL(quantized,\nnBits);\nF32 value = min + (unitFloat * (max - min)) ;\nreturn value;\n}\nLet’s return to our original problem of animation channel compression. To\ncompress and decompress a quaternion’s four components into 16 bits per\nchannel,wesimplycall CompressFloatRL() andDecompressFloatRL()\nwith min = 1,max =1andn=16:\ninline U16 CompressRotationChannel(F32 qx)\n{\nreturn (U16) CompressFloatRL(qx, -1.0f, 1.0f, 16u);\n}\ninline F32 DecompressRotationChannel(U16 qx)\n{\nreturn DecompressFloatRL ((U32)qx, -1.0f, 1.0f, 16u);\n}\nCompressionoftranslationchannelsisabittrickierthanrotations,because\nunlike quaternion channels, the range of a translation channel could theoreti-\ncally be unbounded. Thankfully, the joints of a character don’t move very far\nin practice, so we can decide upon a reasonable range of motion and flag an\n782 12. Animation Systems\nerror if we ever see an animation that contains translations outside the valid\nrange. In-game cinematics are an exception to this rule—when an IGC is ani-\nmated in world space, the translations of the characters’ root joints can grow\nvery large. To address this, we can select the range of valid translations on\na per-animation or per-joint basis, depending on the maximum translations\nactually achieved within each clip. Because the data range might differ from\nanimationtoanimation,orfromjointtojoint,wemuststoretherangewiththe\ncompressed clip data. This will add a tiny amount of data to each animation\nclip, but the impact is generally negligible.\n// We'll use a 2 m range -- your mileage may vary.\nF32 MAX_TRANSLATION = 2.0f;\ninline U16 CompressTranslationChannel (F32 vx)\n{\n// Clamp to valid range...\nif (vx < -MAX_TRANSLATION)\nvx = -MAX_TRANSLATION;\nif (vx > MAX_TRANSLATION)\nvx = MAX_TRANSLATION;\nreturn (U16) CompressFloatRL (vx,\n-MAX_TRANSLATION, MAX_TRANSLATION, 16);\n}\ninline F32 DecompressTranslationChannel(U16 vx)\n{\nreturn DecompressFloatRL ((U32)vx,\n-MAX_TRANSLATION, MAX_TRANSLATION, 16);\n}\n12.8.3 Sampling Frequency and Key Omission\nAnimation data tends to be large for three reasons: first, because the pose of\neach joint can contain upwards of ten channels of floating-point data; second,\nbecause a skeleton contains a large number of joints (250 or more for a hu-\nmanoid character on PS3 or Xbox 360, and more than 800 on some PS4 and\nXboxOnegames);third,becausetheposeofthecharacteristypicallysampled\nat a high rate (e.g., 30 frames per second). We’ve seen some ways to address\nthe first problem. We can’t really reduce the number of joints for our high-\nresolution characters, so we’re stuck with the second problem. To attack the\nthird problem, we can do two things:\n•Reduce the sample rate overall . Some animations look fine when exported\n12.8. Compression Techniques 783\nat 15 samples per second, and doing so cuts the animation data size in\nhalf.\n•Omit some of the samples . If a channel’s data varies in an approximately\nlinear fashion during some interval of time within the clip, we can omit\nallofthesamplesinthisintervalexcepttheendpoints. Then,atruntime,\nwe can use linear interpolation to recover the dropped samples.\nThelattertechniqueisabitinvolved,anditrequiresustostoreinformation\nabout the timeof each sample. This additional data can erode the savings we\nachieved by omitting samples in the first place. However, some game engines\nhave used this technique successfully.\n12.8.4 Curve-Based Compression\nOneofthemostpowerful,easiest-to-useandbest-thought-outanimationAPIs\nI’ve ever worked with is Granny, by Rad Game Tools. Granny stores anima-\ntions not as a regularly spaced sequence of pose samples but as a collection of\nnth-order, nonuniform, nonrational B-splines, describing the paths of a joint’s\nS, Q and T channels over time. Using B-splines allows channels with a lot of\ncurvature to be encoded using only a few data points.\nGranny exports an animation by sampling the joint poses at regular inter-\nvals, much like traditional animation data. For each channel, Granny then fits\na set of B-splines to the sampled dataset to within a user-specified tolerance.\nThe end result is an animation clip that is usually significantly smaller than\nits uniformly sampled, linearly interpolated counterpart. This processis illus-\ntrated in Figure 12.46.\ntQx1\nFigure 12.46. One form of animation compression ﬁts B-splines to the animation channel data.\n12.8.5 Wavelet Compression\nAnother way to compress animation data is to apply signal processing theory\nto the problem, via a technique known as wavelet compression. A wavelet is a\nfunction whose amplitude oscillates like a wave but whose duration is very",14540
87-12.9 The Animation Pipeline.pdf,87-12.9 The Animation Pipeline,"784 12. Animation Systems\nshort, like a brief ripple in a pond. Wavelet functions are carefully crafted to\ngive them desirable properties for use in signal processing.\nIn wavelet compression, an animation curve is decomposed into a sum of\northonormal wavelets, in much the same way that an arbitrary signal can be\nrepresented as a train of delta functions or a sum of sinusoids. We discuss sig-\nnalprocessingandlineartime-invariantsystemsinsomedepthinSection14.2;\nthe concepts presented there form the foundations necessary to understand\nwavelet compression. A full discussion of wavelet-based compression tech-\nniques is well beyond the scope of this book, but you can read more about it\nonline. Searchfor“wavelet”tofindintroductoryarticlesonthetopic,andthen\ntry searching for “Animation Compression: Signal Processing” on Nicholas\nFrechette’s blog for a great article on how wavelet compression was imple-\nmented for Thief(2014) by Eidos Montreal.\n12.8.6 Selective Loading and Streaming\nThe cheapest animation clip is the one that isn’t in memory at all. Most games\ndon’t need every animation clip to be in memory simultaneously. Some clips\napply only to certain classes of character, so they needn’t be loaded during\nlevels in which that class of character is never encountered. Other clips ap-\nply to one-off moments in the game. These can be loaded or streamed into\nmemory just before being needed and dumped from memory once they have\nplayed.\nMostgamesloadacoresetofanimationclipsintomemorywhenthegame\nfirst boots and keep them there for the duration of the game. These include\nthe player character’s core move set and animations that apply to objects that\nreappearover and over throughoutthe game, such as weapons or power-ups.\nAll other animations are usually loaded on an as-needed basis. Some game\nengines load animation clips individually, but many package them together\ninto logical groups that can be loaded and unloaded as a unit.\n12.9 The Animation Pipeline\nThe operations performed by the low-level animation engine form a pipeline\nthat transforms its inputs (animation clips and blend specifications) into the\ndesired outputs (local and global poses, plus a matrix palette for rendering).\nFor each animating character and object in the game, the animation pipe-\nline takes one or more animation clips and corresponding blend factors as in-\nput, blends them together, and generates a single local skeletal pose as out-\nput. It also calculates a global pose for the skeleton and a palette of skinning\n12.9. The Animation Pipeline 785\nmatrices for use by the rendering engine. Post-processing hooks are usually\nprovided,whichpermitthelocalposetobemodifiedpriortofinalglobalpose\nand matrix palette generation. This is where inverse kinematics (IK), rag doll\nphysics and other forms of procedural animation are applied to the skeleton.\nThe stages of this pipeline are:\n1.Clipdecompressionandposeextraction . In this stage, each individual clip’s\ndata is decompressed, and a static pose is extracted for the time index in\nquestion. The output of this phase is a local skeletal pose for each input\nclip. This pose might contain information for every joint in the skeleton\n(afull-body pose), for only a subset of joints (a partial pose), or it might be\nadifferencepose for use in additive blending.\n2.Pose blending . In this stage, the input poses are combined via full-body\nLERP blending, partial-skeleton LERP blending and/or additive blend-\ning. The output of this stage is a single local pose for all joints in the\nskeleton. Thisstageisofcourseonlyexecutedwhenblendingmorethan\noneanimationcliptogether—otherwisetheoutputposefromstage1can\nbe used directly.\n3.Globalposegeneration. In this stage, the skeletal hierarchy is walked, and\nlocal joint poses are concatenated in order to generate a global pose for\nthe skeleton.\n4.Post-processing . In this optional stage, the local and/or global poses of\nthe skeleton can be modified prior to finalization of the pose. Post-\nprocessing is used for inverse kinematics, rag doll physics and other\nforms of procedural animation adjustment.\n5.Recalculationofglobalposes . Manytypesofpost-processingrequireglobal\npose information as input but generate local poses as output. After such\napost-processingstephasrun,wemustrecalculatetheglobalposefrom\nthe modified local pose. Obviously, a post-processing operation that\ndoes not require global pose information can be done between stages\n2 and 3, thus avoiding the need for global pose recalculation.\n6.Matrix palette generation. Once the final global pose has been generated,\neach joint’s global pose matrix is multiplied by the corresponding in-\nverse bind pose matrix. The output of this stage is a palette of skinning\nmatrices suitable for input to the rendering engine.\nA typical animation pipeline is depicted in Figure 12.47.",4902
88-12.10 Action State Machines.pdf,88-12.10 Action State Machines,"786 12. Animation Systems\nOutputsInputs\nDecompression\nand\nPose Extraction\nBlend\nSpecificationPose\nBlending\nSkinning \nMatrix\nCalc.Global\nPose Calc.Local \nPose\nRendering\nEngineMatrix\nPalettePost-\nProcessingSkeleton\nClip(s)Local \nClock(s)\nGlobal\nPoseGame Play \nSystems\nFigure 12.47. A typical animation pipeline.\n12.10 Action State Machines\nTheactionsofagamecharacter(standing,walking,running,jumping,etc.)are\nusuallybestmodeledviaafinitestatemachine,commonlyknownasthe action\nstatemachine (ASM). The ASM subsystem sits atop the animation pipeline and\nprovidesastate-drivenanimationinterfaceforusebyvirtuallyallhigher-level\ngame code.\nEachstateinanASMcorrespondstoanarbitrarilycomplexblendofsimul-\ntaneous animation clips. Some states might be very simple—for example, the\n“idle” state might be comprised of a single full-body animation. Other states\nmightbemorecomplex. A“running”statemightcorrespondtoasemicircular\nblend,withstrafingleft,runningforwardandstrafingrightatthe  90degree,\n0degree and +90degree points, respectively. The “running while shooting”\nstate might include a semicircular directional blend, plus additive or partial-\nskeleton blend nodes for aiming the character’s weapon up, down, left and\nright, and additional blends to permit the character to look around with its\neyes, head and shoulders. More additive animations might be included to\ncontrol the character’s overall stance, gait and foot spacing while locomoting\nand to provide a degree of “humanness” through random movement varia-\ntions.\n12.10. Action State Machines 787\nBase Laye r\nState A State B State CVariation Layer (A dditive)\nD E GGesture  Layer (Additive )\nH IGesture Layer (LERP )\nJ K\nF\nTime (\n )\nFigure 12.48. A layered action state machine, showing how each layer’s state transitions are tem-\nporally independent. In this example, the base layer describes the character’s full-body stance and\nmovement. A variation layer provides variety by applying additive clips to the character’s pose. Fi-\nnally, two gesture layers, one additive and one partial, permit the character to aim or point at\nobjects in the world around it.\nA character’s ASM also ensures that characters can transition smoothly\nfrom state to state. During a transition from state A to state B, the final output\nposesofbothstatesareusuallyblendedtogethertoprovideasmooth cross-fade\nbetween them.\nMost high-quality animation engines also permit different parts of a char-\nacter’s body to be doing different, independent or semi-independent actions\nsimultaneously. For instance, a character might be running, aiming and fir-\ning a weapon with its arms, and speaking a line of dialog with its facial joints.\nThe movements of different parts of the body aren’t generally in perfect sync\neither—certain parts of the body tend to “lead” the movements of other parts\n(e.g., the head leads a turn, followed by the shoulders, the hips and finally the\nlegs). In traditional animation, this well-known technique is known as antici-\npation[51]. This kind of complex movement can be realized by allowing mul-\ntiple independent state machines to control a single character. Usually each\nstate machine exists in a separate state layer, as shown in Figure 12.48. The\noutputposesfromeachlayer’sASMareblendedtogetherintoafinalcompos-\nite pose.\nAll of this means that at any given moment in time, multiple animation\n788 12. Animation Systems\nclips are contributing to the final pose of a character’s skeleton. For each char-\nacter, then, we need a way to track all of the currently-playing clips, and to\ndescribe how exactly they should be blended together in order to produce the\ncharacter’s final pose. Generally speaking, there are two ways to do this:\n1.Flat weighted average. In this approach, the engine maintains a flat list\nof all animation clips that are currently contributing to a character’s fi-\nnal pose, with one blend weight per clip. The animations are blended\ntogether as one big weighted average to produce the final pose.\n2.Blend trees. In this approach, each contributing clip is represented by\nthe leaf nodes of a tree. The interior nodes of this tree represent vari-\nous blending operations that are being performed on the clips. Multiple\nblend operations are composed to form action states. Additional blend\nnodesareintroducedtorepresenttransientcross-fades. Andinalayered\nASM, the output poses obtained from the action states in each layer are\nblended together. The final pose of the character is thus produced at the\nroot of this potentially complex blend tree.\n12.10.1 The Flat Weighted Average Approach\nIn the flat weighted average approach, every animation clip that is currently\nplaying on a given character is associated with a blend weight indicating how\nmuch it should contribute to its final pose. A flat list of all activeanimation\nclips (i.e., clips whose blend weights are nonzero) is maintained. To calculate\nthe final pose of the skeleton, we extract a pose at the appropriate time index\nfor each of the Nactive clips. Then, for each joint of the skeleton, we calculate\na simple N-point weighted average of the translation vectors, rotation quater-\nnionsandscalefactorsextractedfromthe Nactiveanimations. Thisyieldsthe\nfinal pose of the skeleton.\nThe equation for the weighted average of a set of Nvectorsfvigis as fol-\nlows:\nvavg=N 1\nå\ni=0wivi\nN 1\nå\ni=0wi.\nIf the weights are normalized, meaning they sum to one, then this equation can\nbe simplified to the following:\nvavg=N 1\nå\ni=0wivi,whenN 1\nå\ni=0wi=1.\n12.10. Action State Machines 789\nIn the case of N=2, if we let w0= (1 b)andw1=b, theweighted average\nreduces to the familiar equation for the linear interpolation (LERP) between\ntwo vectors:\nvavg=w0vA+w1vB\n=(1 b)vA+bvB\n=LERP [vA,vB,b].\nWe can apply this same weighted average formulation equally well to quater-\nnions by simply treating them as four-element vectors.\n12.10.1.1 Example: OGRE\nThe OGRE animation system works in exactly this way. An Ogre::Entity\nrepresents an instance of a 3D mesh (e.g., one particular character walking\naround in the game world). The Entity aggregates an object called an\nOgre::AnimationStateSet, which in turn maintains a list of\nOgre::AnimationState objects, one for each active animation. The\nOgre::AnimationState class is shown in the code snippet below. (A few\nirrelevant details have been omitted for clarity.)\n/** Represents the state of an animation clip and the\nweight of its influence on the overall pose of the\ncharacter.\n*/\nclass AnimationState\n{\nprotected:\nString mAnimationName; // reference to\n// clip\nReal mTimePos; // local clock\nReal mWeight; // blend weight\nbool mEnabled; // is this anim\n// running?\nbool mLoop; // should the\n// anim loop?\npublic:\n/// API functions...\n};\nEach AnimationState keepstrackofoneanimationclip’slocalclockand\nits blend weight. When calculating the final pose of the skeleton for a partic-\nularOgre::Entity , OGRE’s animation system simply loops through each\n790 12. Animation Systems\nactive AnimationState in its AnimationStateSet. A skeletal pose is ex-\ntracted from the animation clip corresponding to each state at the time index\nspecified by that state’s local clock. For each joint in the skeleton, an N-point\nweightedaverageisthencalculatedforthetranslationvectors,rotationquater-\nnions and scales, yielding the final skeletal pose.\nIt is interesting to note that OGRE has no concept of a playback rate ( R). If\nit did, we would have expected to see a data member like this in the\nOgre::AnimationState class:\nReal mPlaybackRate;\nOf course, we can still make animations play more slowly or more quickly\nin OGRE by simply scaling the amount of time we pass to the addTime()\nfunction, but unfortunately, OGRE does not support animation time scaling\nout of the box.\n12.10.1.2 Example: Granny\nThe Granny animation system, by Rad Game Tools (http://www.radgame\ntool.com/granny.html),providesaflat,weightedaverageanimationblending\nsystem similar to OGRE’s. Granny permits any number of animations to be\nplayed on a single character simultaneously. The state of each active anima-\ntion is maintained in a data structure known as a granny_control . Granny\ncalculates a weighted average to determine the final pose, automatically nor-\nmalizingtheweightsofallactiveclips. Inthissense,itsarchitectureisvirtually\nidentical to that of OGRE’s animation system.\nWhere Granny really shines is in its handling of time. Granny uses the\nglobal clock approach discussed in Section 12.4.3. It allows each clip to be\nloopedanarbitrarynumberoftimesorinfinitely. Clipscanalsobetime-scaled;\na negative time scale allows an animation to be played in reverse.\n12.10.1.3 Cross-Fades with a Flat Weighted Average\nIn an animation engine that employs the flat weighted average architecture,\ncross-fades are implemented by adjusting the weights of the clips themselves.\nRecall that any clip whose weight wi=0will not contribute to the current\npose of the character, while those whose weights are nonzero are averaged\ntogether to generate the final pose. If we wish to transition smoothly from\nclip A to clip B, we simply ramp up clip B’s weight wB, while simultaneously\nramping down clip A’s weight wA. This is illustrated in Figure 12.49.\nCross-fading in a weighted average architecture becomes a bit trickier\nwhen we wish to transition from one complex blend to another. As an ex-\nample, let’s say we wish to transition the character from walking to jumping.\n12.10. Action State Machines 791\ntw\ntstart tendw\n w\nFigure 12.49. A simple cross-fade from clip A to clip B, as implemented in a weighted average ani-\nmation architecture.\nLet’s assume that the walk movement is produced by a three-way average be-\ntweenclipsA,BandC,andthatthejumpmovementisproducedbyatwo-way\naverage between clips D and E.\nWe want the character to look like he’s smoothly transitioning from walk-\ning to jumping, without affecting how the walk or jump animations look in-\ndividually. So during the transition, we want to ramp down the ABC clips\nand ramp up the DE clips while keeping the relative weights of the ABC and\nDE clip groups constant. If the cross-fade’s blend factor is denoted by l, we\ncan meet this requirement by simply setting the weights of bothclip groups to\ntheir desired values and then multiplying the weights of the source group by\n(1 l)and the weights of the destination group by l.\nLet’s look at a concrete example to convince ourselves that this will work\nproperly. Imagine that before the transition from ABC to DE, the nonzero\nweights are as follows: wA=0.2,wB=0.3andwC=0.5. After the tran-\nsition, we want the nonzero weights to be wD=0.33andwE=0.66. So, we\nset the weights as follows:\nwA= (1 l)(0.2), wD=l(0.33),\nwB= (1 l)(0.3), wE=l(0.66). (12.20)\nwC= (1 l)(0.5),\nFromEquations(12.20), youshouldbeabletoconvinceyourselfofthefollow-\ning:\n1. When l=0, the output pose is the correct blend of clips A, B and C,\nwith zero contribution from clips D and E.\n2. When l=1, the output pose is the correct blend of clips D and E, with\nno contribution from A, B or C.\n3. When 0<l<1, therelativeweights of both the ABC group and the\nDE group remain correct, although they no longer add to one. (In fact,\ngroup ABC’s weights add to (1 l), and group DE’s weights add to l.)\n792 12. Animation Systems\nFigure 12.50. A binary LERP blend, represented by a binary expression tree.\nFor this approach to work, the implementation must keep track of\nthe logical groupings between clips (even though, at the lowest level, all\nof the clips’ states are maintained in one big, flat array—for example, the\nOgre::AnimationStateSet in OGRE). In our example above, the system\nmust “know” that A, B and C form a group, that D and E form another group,\nand that we wish to transition from group ABC to group DE. This requires\nadditional metadata to be maintained, on top of the flat array of clip states.\n12.10.2 Blend Trees\nSomeanimationenginesrepresentacharacter’sclipstatenotasaflatweighted\naveragebutratherasatreeofblendoperations. Ananimationblendtreeisan\nexample of what is known in compiler theory as an expression tree or asyntax\ntree. The interior nodes of such a tree are operators, and the leaf nodes serve\nas the inputs to those operators. (More correctly, the interior nodes represent\nthenonterminals of the grammar, while the leaf nodes represent the terminals.)\nInthefollowingsections,we’llbrieflyrevisitthevariouskindsofanimation\nblends we learned about in Sections 12.6.3 and 12.6.5 and see how each can be\nrepresented by an expression tree.\n12.10.2.1 Binary LERP Blend Trees\nAs we saw in Section 12.6.1, a binary linear interpolation (LERP) blend takes\ntwo input poses and blends them together into a single output pose. A blend\nweight bcontrols the percentage of the second input pose that should appear\nat the output, while (1 b)specifies the percentage of the first input pose.\nThis can be represented by the binary expression tree shown in Figure 12.50.\n12.10.2.2 Generalized One-Dimensional Blend Trees\nInSection12.6.3.1,welearnedthatitcanbeconvenienttodefineageneralized\none-dimensional LERP blend by placing an arbitrary number of clips along a\nlinear scale. A blend factor bspecifies the desired blend along this scale. Such\na blend can be pictured as an n-input operator, as shown in Figure 12.51.\nGiven a specific value for b, such a linear blend can always be transformed\nintoabinaryLERPblend. Wesimplyusethetwoclipsimmediatelyadjacentto\n12.10. Action State Machines 793\nFor this specific value of \nb, this tree converts to...\n = 0\n = 1\nbb\nb\nb\nb\nLERP Output Poseb\nClip A\nClip B\nClip C\nClip D\nLERPClip B\nClip COutput Pose\nFigure 12.51. A multi-input expression tree can be used to represent a generalized 1D blend. Such a\ntree can always be transformed into a binary expression tree for any speciﬁc value of the blend\nfactor b.\nb\nLERPBotto m Lef t\nBottom RightLERPTop Left\nTop Right\nOutput Pose LERPb\nFigure 12.52. A simple 2D LERP blend, implemented as cascaded binary blends.\nbastheinputstothebinaryblendandcalculatetheblendweight basspecified\nin Equation (12.15)\n12.10.2.3 Two-Dimensional LERP Blend Trees\nInSection12.6.3.2,wesawhowatwo-dimensionalLERPblendcanberealized\nby simply cascading the results of two binary LERP blends. Given a desired\ntwo-dimensional blend point b=[\nbxby]\n, Figure 12.52 shows how this kind\nof blend can be represented in tree form.\n12.10.2.4 Additive Blend Trees\nSection12.6.5describedadditiveblending. Thisisabinaryoperation,soitcan\nberepresentedbyabinarytreenode, asshowninFigure12.53. Asingleblend\nweight bcontrols the amount of the additive animation that should appear at\n794 12. Animation Systems\nFigure 12.53. An additive blend represented as a binary tree.\nClip A\nDiff C lip B+\nDiff Clip C+\nOutput Pose\nDiff Clip D+\nFigure 12.54. In order to additively blend more than one difference pose onto a regular “base” pose,\na cascaded binary expression tree must be used.\nthe output—when b=0, the additive clip does not affect the output at all,\nwhile when b=1, the additive clip has its maximum effect on the output.\nAdditive blend nodes must be handled carefully, because the inputs are\nnot interchangeable (as they are with most types of blend operators). One of\nthe two inputs is a regular skeletal pose, while the other is a special kind of\npose known as a difference pose (also known as an additive pose). A difference\nposemay onlybeappliedtoaregularpose, andtheresultofanadditiveblend\nis another regular pose. This implies that the additive input of a blend node\nmustalwaysbealeafnode,whiletheregularinputmaybealeaforaninterior\nnode. If we want to apply more than one additive animation to our character,\nwe must use a cascaded binary tree with the additive clips always applied to\nthe additive inputs, as shown in Figure 12.54.\n12.10.2.5 Layered Blend Trees\nWe said at the beginning of Section 12.10 that complex character movement\ncan be produced by arranging multiple independent state machines into state\nlayers. The output poses from each layer’s ASM are blended together into a\nfinal composite pose. When this is implemented using blend trees, the net\neffect is to combine the blend trees of each active state together into one über\ntree, as illustrated in Figure 12.55.\n12.10. Action State Machines 795\nNet blend  tree\nat time \nTimeH\nF\nB\nK\nLERP+Tree\nB\nTree\nF\nTree\nH+\nTree\nK\nFigure 12.55. A layered state machine converts the blend trees from multiple states into a single,\nuniﬁed tree.\n12.10.2.6 Cross-Fades with Blend Trees\nAs a character transitions from state to state within each layer of a layered\nASM, we often wish to provide a smooth cross-fade between states. Imple-\nmenting a cross-fade in an expression tree based ASM is a bit more intuitive\nthanitisinaweightedaveragearchitecture. Whetherwe’retransitioningfrom\none clip to another or from one complex blend to another, the approach is al-\nways the same: We simply introduce a transient binary LERP node between\nthe roots of the blend trees of each state to handle the cross-fade.\nWe’ll denote the blend factor of the cross-fade node with the symbol las\nbefore. Its top input is the source state’s blend tree (which can be a single clip\noracomplexblend),anditsbottominputisthedestinationstate’stree(againa\nclip or a complex blend). During the transition, lis ramped from zero to one.\nOnce l=1, the transition is complete, and the cross-fade LERP node and its\ntop input tree can be retired. This leaves its bottom input tree as the root of\n796 12. Animation Systems\nTree\nA\nTree\nA\nTree\nB\nTree\nBBefore\nCross-Fade\nDuring\nCross-Fade\nAfter\nCross-Fade\nFigure 12.56. A cross-fade between two arbitrary blend trees A and B.\nthe overall blend tree for the given state layer, thus completing the transition.\nThis process is illustrated in Figure 12.56.\n12.10.3 State and Blend Tree Speciﬁcations\nAnimators, game designers and programmers usually cooperate to create the\nanimation and control systems for the central characters in a game. These de-\nvelopers need a way to specify the states that make up a character’s ASM,\nto lay out the tree structure of each blend tree, and to select the clips that\nwill serve as their inputs. Although the states and blend trees could be hard-\ncoded, most modern game engines provide a data-driven means of defining\nanimationstates. Thegoalofadata-drivenapproachistopermitausertocre-\nate new animation states, remove unwanted states, fine-tune existing states\nand then see the effects of his or her changes reasonably quickly. In other\nwords, the central goal of a data-driven animation engine is to enable rapid\niteration.\nTo build an arbitrarily complex blend tree, we really only require four\natomictypesofblendnodes: clips,binaryLERPblends,binaryadditiveblends\nand possibly ternary (triangular) LERP blends. Virtually any blend tree imag-\ninable can be created as compositions of these atomic nodes.\n12.10. Action State Machines 797\nA blendtreebuiltexclusively fromatomic nodescan quicklybecome large\nand unwieldy. As a result, many game engines permit custom compound\nnode types to be predefined for convenience. The N-dimensional linear blend\nnodediscussedinSections12.6.3.4and12.10.2.2isanexampleofacompound\nnode. One can imagine myriad complex blend node types, each one address-\ning a particular problem specific to the particular game being made. A soccer\ngame might define a node that allows the character to dribble the ball. A war\ngame could define a special node that handles aiming and firing a weapon.\nA brawler could define custom nodes for each fight move the characters can\nperform. Once we have the ability to define custom node types, the sky’s the\nlimit.\nThe means by which the users enter animation state data varies widely.\nSome game engines employ a simple, bare-bones approach, allowing anima-\ntionstatestobespecifiedinatextfilewithasimplesyntax. Otherenginespro-\nvideaslick,graphicaleditorthatpermitsanimationstatestobeconstructedby\ndraggingatomiccomponentssuchasclipsandblendnodesontoacanvasand\nlinking them together in arbitrary ways. Such editors usually provide a live\npreview of the character so that the user can see immediately how the charac-\nter will look in the final game. In my opinion, the specific method chosen has\nlittle bearing on the quality of the final game—what matters most is that the\nusercanmakechangesandseetheresultsofthosechangesreasonablyquickly\nand easily.\n12.10.3.1 Example: The Naughty Dog Engine\nThe animation engine used in Naughty Dog’s Uncharted andThe Last of Us\nfranchises employs a simple, text-based approach to specifying animation\nstates. For reasons related to Naughty Dog’s rich history with the Lisp lan-\nguage (see Section 16.9.5.1), state specifications in the Naughty Dog engine\nare written in a customized version of the Scheme programming language\n(which itself is a Lisp variant). Two basic state types can be used: simpleand\ncomplex.\nSimple States\nAsimplestate contains a single animation clip. For example:\n(define-state simple\n:name "" pirate-b-bump-back ""\n:clip ""pirate-b-bump-back""\n:flags (anim-state-flag no-adjust-to-ground)\n)\n798 12. Animation Systems\nDon’t let the Lisp-style syntax throw you. All this block of code does is to de-\nfine a state named “pirate-b-bump-back” whose animation clip also happens\nto be named “pirate-b-bump-back.” The :flags parameter allows users to\nspecify various Boolean options on the state.\nComplex States\nAcomplex state contains an arbitrary tree of LERP or additive blends. For ex-\nample, the following state defines a tree that contains a single binary LERP\nblend node, with two clips (“walk-l-to-r” and “run-l-to-r”) as its inputs:\n(define-state complex\n:name ""move-l-to-r""\n:tree\n(anim-node-lerp\n(anim-node-clip ""walk-l-to-r"")\n(anim-node-clip ""run-l-to-r"")\n)\n)\nThe:treeargument allows the user to specify an arbitrary blend tree, com-\nposedofLERPoradditiveblendnodesandnodesthatplayindividualanima-\ntion clips.\nFrom this, we can see how the (define-state simple ...) example\nshown above might really work under the hood—it probably defines a com-\nplex blend tree containing a single “clip” node, like this:\n(define-state complex\n:name ""pirate-b-unimog-bump-back""\n:tree (anim-node-clip ""pirate-b-unimog-bump-back"")\n:flags (anim-state-flag no-adjust-to-ground)\n)\nThefollowing complex state shows how blend nodes can be cascaded into\narbitrarily deep blend trees:\n(define-state complex\n:name ""move-b-to-f""\n:tree\n(anim-node-lerp\n(anim-node-additive\n(anim-node-additive\n(anim-node-clip ""move-f"")\n(anim-node-clip ""move-f-look-lr"")\n)\n12.10. Action State Machines 799\nLERPmove-f\nmove-f-look-lr+\nmove-f-look-ud\nmove-b\nmove-b-look-lr+\nmove-b-look-ud+\n+\nFigure 12.57. Blend tree corresponding to the example state “move-b-to-f.”\n(anim-node-clip ""move-f-look-ud"")\n)\n(anim-node-additive\n(anim-node-additive\n(anim-node-clip ""move-b"")\n(anim-node-clip ""move-b-look-lr"")\n)\n(anim-node-clip ""move-b-look-ud"")\n)\n)\n)\nThis corresponds to the tree shown in Figure 12.57.\nRapid Iteration\nNaughty Dog’s animation team achieves rapid iteration with the help of four\nimportant tools:\n1. An in-game animation viewer allows a character to be spawned into the\ngame and its animations controlled via an in-game menu.\n2. A simple command-line tool allows animation scripts to be recompiled\nand reloaded into the running game on the fly. To tweak a character’s\nanimations, the user can make changes to the text file containing the an-\nimation state specifications, quickly reload the animation states and im-\nmediately see the effectsof his or her changes on an animating character\nin the game.\n3. The engine continually keeps track of all state transitions performed by\neach character during the last few seconds of gameplay. This allows us\nto pause the game and then literally rewind the animations to scrutinize\nthem and debug problems that are noticed while playing.\n800 12. Animation Systems\n4. The Naughty Dog engine also offers a host of “live update” tools. For\nexample, animators can tweak their animations in Maya and see them\nupdate virtually instantaneously in the game.\n12.10.3.2 Example: Unreal Engine 4\nUnrealEngine4(UE4)providesitsuserswithfivetoolsforworkingwithskele-\ntal animations and skeletal meshes: The Skeleton Editor, the Skeletal Mesh\nEditor, the Animation Editor, the Animation Blueprint Editor, and the Physics\nEditor.\n• The Skeleton Editor is essentially a rigging tool. It allows users to view\nandmodifyskeletons,add socketstojoints,andtestoutthemovementof\ntheskeleton. Asocketissometimescalledan attachpoint inotherengines\n(see Section 12.11.1).\n• The Skeletal Mesh Editor allows users to edit properties of the meshes\nthat are skinned to animating skeletons.\n• TheAnimationEditorallowsuserstoimport,createandmanageanima-\ntion assets. In this editor, the compression and timing of animation clips\n(which UE4 calls Sequences) can be adjusted. Clips can be combined\nintopredefinedBlendSpaces, andin-gamecinematicscanbedefinedby\ncreating Animation Montages.\n• The Animation Blueprint Editor allows users to apply the power of Un-\nrealEngine’sBlueprintsvisualscriptingsystemtocontrollingcharacters’\nanimation state machines. This editor is depicted in Figure 12.58.\n• ThePhysicsEditorallowsuserstomodelahierarchyofrigidbodiesthat\ndrive the skeleton’s motion when ragdoll physics is active.\nAcompletediscussionofUnrealEngine’sanimationtoolsisbeyondourscope\nhere, but you can read more about it by searching for “Unreal Skeletal Mesh\nAnimation System” online.\n12.10.4 Transitions\nTo create a high-quality animating character, we must carefully manage the\ntransitions between states in the action state machine to ensure that the splices\nbetween animations do not have a jarring and unpolished appearance. Most\nmodern animation engines provide a data-driven mechanism for specifying\nexactly how transitions should be handled. In this section, we’ll explore how\nthis mechanism works.\n12.10. Action State Machines 801\nFigure 12.58. The Unreal Engine 4 animation Blueprints editor. (See Color Plate XXVI.)\n12.10.4.1 Kinds of Transitions\nThere are many different ways to manage the transition between states. If we\nknow that the final pose of the source state exactly matches the first pose of\nthe destination state, we can simply “pop” from one state to another. Other-\nwise, we can cross-fade from one state to the next. Cross-fading is not always\na suitable choice when transitioning from state to state. For example, there\nis no way that a cross-fade can produce a realistic transition from lying on\nthe ground to standing upright. For this kind of state transition, we need one\nor more custom animations. This kind of transition is often implemented by\nintroducing special transitional states into the state machine. These states are\nintended for use only when going from one state to another—they are never\nused as a steady-state node. But because they are full-fledged states, they can\nbecomprisedofarbitrarilycomplexblendtrees. Thisprovidesmaximumflex-\nibility when authoring custom-animated transitions.\n12.10.4.2 Transition Parameters\nWhendescribingaparticulartransitionbetweentwostates,wegenerallyneed\nto specify various parameters, controlling exactly how the transition will oc-\ncur. These include but are not limited to the following.\n•Sourceanddestinationstates. To which state(s) does this transition apply?\n802 12. Animation Systems\n•Transitiontype. Isthetransitionimmediate,cross-fadedorperformedvia\na transitional state?\n•Duration. For cross-faded transitions, we need to specify how long the\ncross-fade should take.\n•Ease-in/ease-out curve type. In a cross-faded transition, we may wish to\nspecifythetypeofease-in/ease-outcurvetousetovarytheblendfactor\nduring the fade.\n•Transitionwindow. Certaintransitionscanonlybetakenwhenthesource\nanimation is within a specified window of its local timeline. For exam-\nple, a transition from a punch animation to an impact reaction might\nonlymakesensewhenthearmisinthesecondhalfofitsswing. Ifanat-\ntempttoperformthetransitionismadeduringthefirsthalfoftheswing,\nthetransitionwouldbedisallowed(oradifferenttransitionmightbese-\nlected instead).\n12.10.4.3 The Transition Matrix\nSpecifying transitions between states can be challenging, because the number\nof possible transitions is usually very large. In a state machine with nstates,\nthe worst-case number of possible transitions is n2. We can imagine a two-\ndimensional square matrix with every possible state listed along both the ver-\nticalandhorizontalaxes. Suchatablecanbeusedtospecifyallofthepossible\ntransitions from any state along the vertical axis to any other state along the\nhorizontal axis.\nIn a real game, this transitionmatrix is usually quite sparse, because not all\nstate-to-statetransitionsarepossible. Forexample, transitionsareusuallydis-\nallowed from a death state to any other state. Likewise, there is probably no\nway to go from a driving state to a swimming state (without going through at\nleast one intermediate state that causes the character to jump out of his vehi-\ncle). The number of unique transitions in the table may be significantly less\neven than the number of valid transitions between states. This is because we\ncan often reuse a single transition specification between many different pairs\nof states.\n12.10.4.4 Implementing a Transition Matrix\nThere are all sorts of ways to implement a transition matrix. We could use a\nspreadsheet application to tabulate all the transitions in matrix form, or we\nmight permit transitions to be authored in the same text file used to author\nour action states. If a graphical user interface is provided for state editing,\ntransitions could be added to this GUI as well. In the following sections, we’ll\n12.10. Action State Machines 803\ntake a brief look at a few transition matrix implementations from real game\nengines.\nExample: Wildcarded Transitions in Medal of Honor: Paciﬁc Assault\nOnMedal of Honor: Pacific Assault (MOHPA), we used the sparseness of the\ntransition matrix to our advantage by supporting wildcarded transition spec-\nifications. For each transition specification, the names of both the source and\ndestination states could contain asterisks (*) as a wildcard character. This al-\nlowedustospecifyasingledefaulttransitionfromanystatetoanyotherstate\n(via the syntax from=""*"" to=""*"") and then refine this global default eas-\nily for entire categories of states. The refinement could be taken all the way\ndown to custom transitions between specific state pairs when necessary. The\nMOHPA transition matrix looked something like this:\n<transitions>\n<!-- global default -->\n<trans from="" *"" to="" *""\ntype=frozen duration=0.2>\n<!-- default for any walk to any run -->\n<trans from="" walk*"" to="" run* ""\ntype=smooth\nduration=0.15>\n<!-- special handling from any prone to any getting-up\n-- action (only valid from 2 sec to 7.5 sec on the\n-- local timeline) -->\n<trans from="" *prone"" to="" *get-up ""\ntype=smooth\nduration=0.1\nwindow-start=2.0\nwindow-end=7.5>\n...\n</transitions>\nExample: First-Class Transitions in Uncharted\nIn some animation engines, high-level game code requests transitions from\nthe current state to a new state by naming the destination state explicitly. The\nproblemwiththisapproachisthatthecallingcodemusthaveintimateknowl-\nedge of the names of the states and of which transitions are valid when in a\nparticular state.\n804 12. Animation Systems\nIn Naughty Dog’s engine, this problem is overcome by turning state tran-\nsitions from secondary implementation details into first-class entities. Each\nstate provides a list of valid transitions to other states, and each transition is\ngiven a unique name. The names of the transitions are standardized in order\nto make the effectof each transition predictable. For example, if a transition\nis called “walk,” then it alwaysgoes from the current state to a walking state\nof some kind, no matter what the current state is. Whenever the high-level\nanimation control code wants to transition from state A to state B, it asks for a\ntransition by name (rather than requesting the destination state explicitly). If\nsuch a transition can be found and is valid, it is taken; otherwise, the request\nfails.\nThe following example state defines four transitions named “reload,”\n“step-left,” “step-right” and “fire.” The (transition-group ...) line in-\nvokes a previously defined group of transitions; it is useful when the same set\nof transitions is to be used in multiple states. The (transition-end ...)\ncommand specifies a transition that is taken upon reaching the end of the\nstate’s local timeline if no other transition has been taken before then.\n(define-state complex\n:name ""s_turret-idle""\n:tree (aim-tree\n(anim-node-clip ""turret-aim-all--base"")\n""turret-aim-all--left-right""\n""turret-aim-all--up-down""\n)\n:transitions (\n(transition ""reload"" ""s_turret-reload""\n(range - -) :fade-time 0.2)\n(transition ""step-left"" ""s_turret-step-left""\n(range - -) :fade-time 0.2)\n(transition ""step-right"" ""s_turret-step-right""\n(range - -) :fade-time 0.2)\n(transition ""fire"" ""s_turret-fire""\n(range - -) :fade-time 0.1)\n(transition-group ""combat-gunout-idle^move"")\n(transition-end ""s_turret-idle"")\n)\n)\nThe beauty of this approach may be difficult to see at first. Its primary\n12.10. Action State Machines 805\npurposeistoallowtransitionsandstatestobemodifiedinadata-drivenman-\nner, without requiring changes to the C++ source code in many cases. This\ndegree of flexibility is accomplished by shielding the animation control code\nfrom knowledge of the structure of the state graph. For example, let’s say that\nwe have ten different walking states (normal, scared, crouched, injured and\nso on). All of them can transition into a jumping state, but different kinds\nof walks might require different jump animations (e.g., normal jump, scared\njump,jumpfromcrouch,injuredjump,etc.). Foreachofthetenwalkingstates,\nwe define a transition simply called “jump.” At first, we can point all of these\ntransitions to a single generic “jump” state, just to get things up and running.\nLater, we can fine-tune some of these transitions so that they point to custom\njump states. We can even introduce transitional states between some of the\n“walk” states and their corresponding “jump” states. All sorts of changes can\nbemadetothestructureofthestategraphandtheparametersofthetransitions\nwithout affecting the C++ source code—as long as the namesof the transitions\ndon’t change.\n12.10.5 Control Parameters\nFrom a software engineering perspective, it can be challenging to orchestrate\nalloftheblendweights,playbackratesandothercontrolparametersofacom-\nplexanimatingcharacter. Differentblendweightshavedifferenteffectsonthe\nwaythecharacteranimates. Forexample,oneweightmightcontrolthecharac-\nter’smovementdirection,whileotherscontrolitsmovementspeed,horizontal\nand vertical weapon aim, head/eye look direction and so on. We need some\nway of exposing all of these blend weights to the code that is responsible for\ncontrolling them.\nIn a flat weighted average architecture, we have a flat list of all the anima-\ntion clips that could possibly be played on the character. Each clip state has a\nblendweight,aplaybackrateandpossiblyothercontrolparameters. Thecode\nthatcontrolsthecharactermustlookupindividualclipstatesbynameandad-\njust each one’s blend weight appropriately. This makes for a simple interface,\nbut it shifts most of the responsibility for controlling the blend weights to the\ncharacter control system. For example, to adjust the direction in which a char-\nacter is running, the character controlcode must know that the “run” action is\ncomprised of a group of animation clips, named something like “StrafeLeft,”\n“RunForward,”“StrafeRight”and“RunBackward.” Itmustlookuptheseclip\nstatesbynameandmanuallycontrolallfourblendweightsinordertoachieve\na particular angled run animation. Needless to say, controlling animation pa-\nrameters in such a fine-grained way can be tedious and can lead to difficult-",36485
89-12.11 Constraints.pdf,89-12.11 Constraints,"806 12. Animation Systems\nto-understand source code.\nIn a blend tree, a different set of problems arise. Thanks to the tree struc-\nture, the clips are grouped naturally into functional units. Custom tree nodes\ncan encapsulate complex character motions. These are both helpful advan-\ntages over the flat weighted average approach. However, the control param-\neters are buried within the tree. Code that wishes to control the horizontal\nlook-at direction of the head and eyes needs a priori knowledge of the struc-\nture of the blend tree so that it can find the appropriate nodes in the tree in\norder to control their parameters.\nDifferent animation engines solve these problems in different ways. Here\nare some examples:\n•Node search . Some engines provide a way for higher-level code to find\nblend nodes in the tree. For example, relevant nodes in the tree can be\ngiven special names, such as “HorizAim” for the node that controls hor-\nizontal weapon aiming. The control code can simply search the tree for\na node of a particular name; if one is found, then we know what effect\nadjusting its blend weight will have.\n•Named variables. Some engines allow names to be assigned to the indi-\nvidual control parameters. The controlling code can look up a control\nparameter by name in order to adjust its value.\n•Control structure . In other engines, a simple data structure, such as an\narray of floating-point values or a C struct, contains all of the control\nparameters for the entire character. The nodes in the blend tree(s) are\nconnected to particular control parameters, either by being hard-coded\nto use certain struct members or by looking up the parameters by name\nor index.\nOf course, there are many other alternatives as well. Every animation en-\nginetacklesthisprobleminaslightlydifferentway,buttheneteffectisalways\nroughly the same.\n12.11 Constraints\nWe’ve seen how action state machines can be used to specify complex blend\ntrees and how a transition matrix can be used to control how transitions be-\ntween states should work. Another important aspect of character animation\ncontrol is to constrain the movement of the characters and/or objects in the\nscene in various ways. For example, we might want to constrain a weapon\nso that it always appears to be in the hand of the character who is carrying it.\n12.11. Constraints 807\n… child \nskeleton\nfollows\nparent\nskeleton\nmoves…child\nskeleton\nmoves…\n… parent \nskeleton\nunaffected\nFigure 12.59. An attachment, showing how movement of the parent automatically produces move-\nment of the child but not vice versa.\nWe might wish to constrain two characters so that they line up properly when\nshaking hands. A character’s feet are often constrained so that they line up\nwith the floor, and its hands might be constrained to line up with the rungs\non a ladder or the steering wheel of a vehicle. In this section, we’ll take a brief\nlook at how these constraints are handled in a typical animation system.\n12.11.1 Attachments\nVirtually all modern game engines permit objects to be attached to one an-\nother. At its simplest, object-to-object attachment involves constraining the\nposition and/or orientation of a particular joint JAwithin the skeleton of ob-\nject A so that it coincides with a joint JBin the skeleton of object B. An at-\ntachment is usually a parent-child relationship. When the parent’s skeleton\nmoves, the child object is adjusted to satisfy the constraint. However, when\nthechildmoves,theparent’sskeletonisusuallynotaffected. Thisisillustrated\nin Figure 12.59.\nSometimes it can be convenient to introduce an offsetbetween the parent\njoint and the child joint. For example, when placing a gun into a character’s\nhand, we could constrain the “Grip” joint of the gun so that it coincides with\nthe “RightWrist” joint of the character. However, this might not produce the\ncorrect alignment of the gun with the hand. One solution to this problem is\nto introduce a special joint into one of the two skeletons. For example, we\ncould add a “RightGun” joint to the character’s skeleton, make it a child of\nthe “RightWrist” joint, and position it so that when the “Grip” joint of the\ngun is constrained to it, the gun looks like it is being held naturally by the\ncharacter. The problem with this approach, however, is that it increases the\n808 12. Animation Systems\nFigure 12.60. An attach point acts like an extra joint between the parent and the child.\nnumber of joints in the skeleton. Each joint has a processing cost associated\nwithanimationblendingandmatrixpalettecalculationandamemorycostfor\nstoring its animation keys. So adding new joints is often not a viable option.\nWe know that an additional joint added for attachment purposes will not\ncontribute to the pose of the character—it merely introduces an additional\ntransform between the parent and child joint in an attachment. What we re-\nally want, then, is a way to mark certain joints so that they can be ignored by\ntheanimationblendingpipelinebutcanstillbeusedforattachmentpurposes.\nSuch special joints are sometimes called attach points. They are illustrated in\nFigure 12.60.\nAttachpointsmightbemodeledinMayajustlikeregularjointsorlocators,\nalthough many game engines define attach points in a more convenient man-\nner. For example, they might be specified as part of the action state machine\ntext file or via a custom GUI within the animation authoring tool. This allows\nthe animators to focus only on the joints that affect the look of the character,\nwhile the power to control attachments is put conveniently into the hands of\nthe people who need it—the game designers and the engineers.\n12.11.2 Interobject Registration\nThe interactions between game characters and their environments is growing\never more complex and nuanced with each new title. Hence, it is important\nto have a system that allows characters and objects to be aligned with one\nanother when animating. Such a system can be used for in-game cinematics\nand interactive gameplay elements alike.\nImagine that an animator, working in Maya or some other animation tool,\nsetsupasceneinvolvingtwocharactersandadoorobject. Thetwocharacters\nshakehands,andthenoneofthemopensthedoorandtheybothwalkthrough\nit. The animator can ensure that all three actors in the scene line up perfectly.\n12.11. Constraints 809\nymaya\nxmaya\nFigure 12.61. Original Maya scene containing three actors and\na reference locator.\nyA\nxAyB\nxB xCyCFigure 12.62. The reference locator is encoded in each\nactor’s animation ﬁle.\nHowever,whentheanimationsareexported,theybecomethreeseparateclips,\nto be played on three separate objects in the game world. The two characters\nmight have been under AI or player control prior to the start of this animated\nsequence. How, then, can we ensure that the three objects line up correctly\nwith one another when the three clips are played back in-game?\n12.11.2.1 Reference Locators\nOne good solution is to introduce a common reference point into all three an-\nimation clips. In Maya, the animator can drop a locator(which is just a 3D\ntransform, much like a skeletal joint) into the scene, placing it anywhere that\nseems convenient. Its location and orientation are actually irrelevant, as we’ll\nsee. The locator is tagged in some way to tell the animation export tools that\nit is to be treated specially.\nWhen the three animation clips are exported, the tools store the position\nand orientation of the reference locator, expressed in coordinates that are rela-\ntive to the local object space of each actor, into all three clip’s data files. Later,\nwhen the three clips are played back in-game, the animation engine can look\nup the relative position and orientation of the reference locator in all three\nclips. It can then transform the origins of the three objects in such a way as\nto make all three reference locators coincide in world space. The reference\nlocator acts much like an attach point (Section 12.11.1) and, in fact, could be\nimplemented as one. The net effect—all three actors now line up with one\nanother, exactly as they had been aligned in the original Maya scene.\nFigure12.61illustrateshowthedoorandthetwocharactersfromtheabove\nexample might be set up in a Maya scene. As shown in Figure 12.62, the refer-\nencelocatorappearsineachexportedanimationclip(expressedinthatactor’s\nlocalspace). In-game,theselocal-spacereferencelocatorsarealignedtoafixed\nworld-space locator in order to realign the actors, as shown in Figure 12.63.\n810 12. Animation Systems\nyworld\nxworld\nFigure 12.63. At runtime, the local-space reference transforms are aligned to a world-space refer-\nence locator, causing the actors to line up properly.\n12.11.2.2 Finding the World-Space Reference Location\nWe’ve glossed over one important detail here—who decides what the world-\nspacepositionandorientationofthereferencelocatorshouldbe? Eachanima-\ntion clip provides the reference locator’s transform in the coordinate space of\nits actor. But we need some way to define where that reference locator should\nbe in world space.\nInourexamplewiththedoorandthetwocharactersshakinghands,oneof\nthe actors is fixed in the world (the door). So one viable solution is to ask the\ndoor for the location of the reference locator and then align the two characters\nto it. The commands to accomplish this might look similar to the following\npseudocode.\nvoid playShakingHandsDoorSequence(\nActor& door,\nActor& characterA,\nActor& characterB)\n{\n// Find the world-space transform of the reference\n// locator as specified in the door's animation.\nTransform refLoc =getReferenceLocatorWs(door,\n""shake-hands-door"");\n// Play the door's animation in-place. (It's\n// already in the correct place.)\nplayAnimation (""shake-hands-door"", door);\n// Play the two characters' animations relative to\n// the world-space reference locator obtained from\n// the door.\nplayAnimationRelativeToReference (\n""shake-hands-character-a"", characterA, refLoc);\nplayAnimationRelativeToReference (\n""shake-hands-character-b"", characterB, refLoc);\n}\n12.11. Constraints 811\nAnotheroptionistodefinetheworld-spacetransformofthereferenceloca-\ntorindependentlyofthethreeactorsinthescene. Wecouldplacethereference\nlocator into the world using our world-building tool, for example (see Section\n15.3). Inthiscase,thepseudocodeaboveshouldbechangedtolooksomething\nlike this:\nvoid playShakingHandsDoorSequence(\nActor& door,\nActor& characterA,\nActor& characterB,\nActor& refLocatorActor)\n{\n// Find the world-space transform of the reference\n// locator by simply querying the transform of an\n// independent actor (presumably placed into the\n// world manually).\nTransform refLoc =getActorTransformWs(\nrefLocatorActor);\n// Play all animations relative to the world-space\n// reference locator obtained above.\nplayAnimationRelativeToReference(""shake-hands-door"",\ndoor, refLoc);\nplayAnimationRelativeToReference(\n""shake-hands-character-a"", characterA, refLoc);\nplayAnimationRelativeToReference(\n""shake-hands-character-b"", characterB, refLoc);\n}\n12.11.3 Grabbing and Hand IK\nEvenafterusinganattachmenttoconnecttwoobjects,wesometimesfindthat\nthe alignment does not look exactly right in-game. For example, a character\nmight be holding a rifle in her right hand, with her left hand supporting the\nstock. As the character aims the weapon in various directions, we may notice\nthatthelefthandnolongeralignsproperlywiththestockatcertainaimangles.\nThiskindofjointmisalignmentiscausedbyLERPblending. Evenifthejoints\nin question are aligned perfectly in clip A and in clip B, LERP blending does\nnotguaranteethatthosejointswillbeinalignmentwhenAandBareblended\ntogether.\nOne solution to this problem is to use inverse kinematics (IK) to correct the\nposition of the left hand. The basic approach is to determine the desired tar-\nget position for the joint in question. IK is then applied to a short chain of\njoints (usually two, threeor four joints), starting with the joint in question and\n812 12. Animation Systems\nprogressing up the hierarchy to its parent, grandparent and so on. The joint\nwhose position we are trying to correct is known as the end effector. The IK\nsolver adjusts the orientations of the end effector’s parent joint(s) in order to\nget the end effector as close as possible to the target.\nThe API for an IK system usually takes the form of a request to enable or\ndisableIKonaparticularchainofjoints,plusaspecificationofthedesiredtar-\nget point. The actual IK calculation is usually done internally by the low-level\nanimation pipeline. This allows it to do the calculation at the proper time—\nnamely,afterintermediatelocalandglobalskeletalposeshavebeencalculated\nbut before the final matrix palette calculation.\nSome animation engines allow IK chains to be defined a priori. For ex-\nample, we might define one IK chain for the left arm, one for the right arm\nand two for the two legs. Let’s assume for the purposes of this example that\na particular IK chain is identified by the name of its end-effector joint. (Other\nengines might use an index or handle or some other unique identifier, but the\nconcept remains the same.) The function to enable an IK calculation might\nlook something like this:\nvoid enableIkChain(Actor& actor,\nconst char* endEffectorJointName,\nconst Vector3& targetLocationWs);\nand the function to disable an IK chain might look like this:\nvoid disableIkChain(Actor& actor,\nconst char* endEffectorJointName);\nIK is usually enabled and disabled relatively infrequently, but the world-\nspacetargetlocationmustbekeptup-to-dateeveryframe(ifthetargetismov-\ning). Therefore,thelow-levelanimationpipelinealwaysprovidessomemech-\nanism for updating an active IK target point. For example, the pipeline might\nallow us to call enableIkChain() multiple times. The first time it is called,\nthe IK chain is enabled, and its target point is set. All subsequent calls sim-\nply update the target point. Another way to keep IK targets up-to-date is to\nlink them to dynamic objects in the game. For example, an IK target might\nbe specified as a handle to a rigid game object, or a joint within an animated\nobject.\nIK is well-suited to making minor corrections to joint alignment when the\njoint is already reasonably close to its target. It does not work nearly as well\nwhentheerrorbetweenajoint’sdesiredlocationanditsactuallocationislarge.\nNotealsothatmostIKalgorithmssolveonlyforthe position ofajoint. Youmay\n12.11. Constraints 813\nFigure 12.64. In the animation authoring package, the character moves forward in space, and\nits feet appear grounded. Image courtesy of Naughty Dog, Inc. (UNCHARTED: Drake’s Fortune\n© 2007/® SIE. Created and developed by Naughty Dog.)\nneed to write additional code to ensure that the orientation of the end effector\naligns properly with its target as well. IK is not a cure-all, and it may have\nsignificant performance costs. So always use it judiciously.\n12.11.4 Motion Extraction and Foot IK\nIngames,weusuallywantthelocomotionanimationsofourcharacterstolook\nrealistic and “grounded.” One of the biggest factors contributing to the real-\nism of a locomotion animation is whether or not the feet slide around on the\nground. Foot sliding can be overcome in a number of ways, the most common\nof which are motionextraction andfootIK.\n12.11.4.1 Motion Extraction\nLet’s imagine how we’d animate a character walking forward in a straight\nline. In Maya (or his or her animation package of choice), the animator makes\nthe character take one complete step forward, first with the left foot and then\nwiththerightfoot. Theresultinganimationclipisknownasa locomotioncycle,\nbecause it is intended to be looped indefinitely, for as long as the character\nis walking forward in-game. The animator takes care to ensure that the feet\nof the character appear grounded and don’t slide as it moves. The character\nmoves from its initial location on frame 0 to a new location at the end of the\ncycle. This is shown in Figure 12.64.\nNotice that the local-space origin of the character remains fixed during the\nentire walk cycle. In effect, the character is “leaving his origin behind him” as\nhe takes his step forward. Now imagine playing this animation as a loop. We\n814 12. Animation Systems\nFigure 12.65. Walk cycle after zeroing out the root joint’s forward motion. Image courtesy\nof Naughty Dog, Inc. (UNCHARTED: Drake’s Fortune © 2007/® SIE. Created and developed by\nNaughty Dog.)\nwould see the character take one complete step forward, and then pop back\nto where he was on the first frame of the animation. Clearly this won’t work\nin-game.\nTomakethiswork,weneedtoremovetheforwardmotionofthecharacter,\nso that his local-space origin remains roughly under the center of mass of the\ncharacter at all times. We could do this by zeroing out the forward translation\noftherootjointofthecharacter’sskeleton. Theresultinganimationclipwould\nmake the character look like he’s “moonwalking,” as shown in Figure 12.65.\nIn order to get the feet to appear to “stick” to the ground the way they did\nin the original Maya scene, we need the character to move forward by just the\nright amount each frame. We could look at the distance the character moved,\ndivide by the amount of time it took for him to get there, and hence find his\naverage movement speed. But a character’s forward speed is not constant\nwhen walking. This is especially evident when a character is limping (quick\nforward motion on the injured leg, followed by slower motion on the “good”\nleg), but it is true for all natural-looking walk cycles.\nTherefore, before we zero out the forward motion of the root joint, we first\nsave the animation data in a special “extracted motion” channel. This data\ncan be used in-game to move the local-space origin of the character forward\nbytheexactamountthattherootjointhadmovedinMayaeachframe. Thenet\nresult is that the character will walk forward exactly as he was authored, but\nnow his local-space origin comes along for the ride, allowing the animation to\nloop properly. This is shown in Figure 12.66.\nIfthecharactermovesforwardby4feetintheanimationandtheanimation\n12.11. Constraints 815\nFigure 12.66. Walk cycle in-game, with extracted root motion data applied to the local-space origin\nof the character. Image courtesy of Naughty Dog, Inc. (UNCHARTED: Drake’s Fortune © 2007/®\nSIE. Created and developed by Naughty Dog.)\ntakes one second to complete, then we know that the character is moving at\nan average speed of 4 feet/second. To make the character walk at a different\nspeed, we can simply scale the playback rate of the walk cycle animation. For\nexample, to make the character walk at 2 feet/second, we can simply play the\nanimation at half speed (R=0.5).\n12.11.4.2 Foot IK\nMotionextractiondoesagoodjobofmakingacharacter’sfeetappearground-\ned when it is moving in a straight line (or, more correctly, when it moves in a\npaththatexactlymatchesthepathanimatedbytheanimator). However,areal\ngame character must be turned and moved in ways that don’t coincide with\nthe original hand-animated path of motion (e.g., when moving over uneven\nterrain). This results in additional foot sliding.\nOne solution to this problem is to use IK to correct for any sliding in the\nfeet. The basic idea is to analyze the animations to determine during which\nperiods of time each foot is fully in contact with the ground. At the moment\na foot contacts the ground, we note its world-space location. For all subse-\nquent frames while that foot remains on the ground, we use IK to adjust the\npose of the leg so that the foot remains fixed to the proper location. This tech-\nnique sounds easy enough, but getting it to look and feel right can be very\nchallenging. It requires a lot of iteration and fine-tuning. And some natural\nhumanmotions—likeleadingintoaturnbyincreasingyourstride—cannotbe\nproduced by IK alone.\nIn addition, there is a big trade-off between the lookof the animations and\nthefeelofthecharacter,particularlyforahuman-controlledcharacter. It’sgen-\nerallymoreimportantfortheplayercharactercontrolsystemtofeelresponsive\n816 12. Animation Systems\nand fun than it is for the character’s animations to look perfect. The upshot is\nthis: Do not take the task of adding foot IK or motion extraction to your game\nlightly. Budget time for a lot of trial and error, and be prepared to make trade-\noffs to ensure that your player character not only looks good but feelsgood as\nwell.\n12.11.5 Other Kinds of Constraints\nThere are plenty of other possible kinds of constraint systems that can be\nadded to a game animation engine. Some examples include:\n•Look-at. This is the ability for characters to look at points of interest in\nthe environment. A character might look at a point with only his or her\neyes, with eyes and head, or with eyes, head and a twist of the entire\nupper body. Look-at constraints are sometimes implemented using IK\nor procedural joint offsets, although a more natural look can often be\nachieved via additive blending.\n•Coverregistration . Thisistheabilityforacharactertoalignperfectlywith\nan object that is serving as cover. This is often implemented via the ref-\nerence locator technique described above.\n•Coverentryanddeparture . If a character can take cover, animation blend-\ningandcustomentryanddepartureanimationsmustusuallybeusedto\nget the character into and out of cover.\n•Traversalaids . The ability for a character to navigate over, under, around\nor through obstacles in the environment can add a lot of life to a game.\nThisisoftendonebyprovidingcustomanimationsandusingareference\nlocator to ensure proper registration with the obstacle being overcome.",21843
90-13.1 Do You Want Physics in Your Game.pdf,90-13.1 Do You Want Physics in Your Game,"13\nCollision and Rigid\nBody Dynamics\nIn the real world, solid objects are inherently, well…solid. They generally\navoid doing impossible things, like passing through one another, all by\nthemselves. But in a virtual game world, objects don’t do anything unless\nwe tell them to, and game programmers must make an explicit effort to en-\nsure that objects do not pass through one another. This is the role of one of the\ncentral components of any game engine—the collisiondetection system.\nA game engine’s collision system is often closely integrated with a physics\nengine. Of course, the field of physics is vast, and what most of today’s game\nengines call “physics” is more accurately described as a rigid body dynamics\nsimulation. A rigid body is an idealized, infinitely hard, non-deformable solid\nobject. The term dynamics refers to the process of determining how these rigid\nbodiesmoveandinteract over time under the influence of forces. A rigid body\ndynamics simulation allows motion to be imparted to objects in the game in a\nhighly interactive and naturally chaotic manner—an effect that is much more\ndifficult to achieve when using canned animation clips to move things about.\nA dynamics simulation makes heavy use of the collision detection system\nin order to properly simulate various physical behaviors of the objects in the\nsimulation, including bouncing off one another, sliding under friction, rolling\nand coming to rest. Of course, a collision detection system can be used stand-\nalone, without a dynamics simulation—many games do not have a “physics”\n817\n818 13. Collision and Rigid Body Dynamics\nsystematall. Butallgamesthatinvolveobjectsmovingaboutintwo-orthree-\ndimensional space have some form of collision detection.\nIn this chapter, we’ll investigate the architecture of both a typical collision\ndetection system and a typical physics (rigid body dynamics) system. As we\ninvestigate the components of these two closely interrelated systems, we’ll\ntake a look at the mathematics and the theory that underlie them.\n13.1 Do You Want Physics in Your Game?\nNowadays, most game engines have some kind of physical simulation capa-\nbilities. Some physical effects, like rag doll deaths, are simply expected by\ngamers. Other effects, like ropes, cloth, hair or complex physically driven ma-\nchinery can add that jenesaisquoi that sets a game apart from its competitors.\nInrecentyears,somegamestudioshavestartedexperimentingwithadvanced\nphysical simulations, including approximate real-time fluid mechanics effects\nand simulations of deformable bodies. But adding physics to a game is not\nwithout costs, and before we commit ourselves to implementing an exhaus-\ntive list of physics-driven features in our game, we should (at the very least)\nunderstand the trade-offs involved.\n13.1.1 Things You Can Do with a Physics System\nHere are just a few of the things you can do or have with a game physics sys-\ntem.\n• Detect collisions between dynamic objects and static world geometry.\n• Simulatefreerigidbodiesundertheinfluenceofgravityandotherforces.\n• Spring-mass systems.\n• Destructible buildings and structures.\n• Ray and shape casts (to determine line of sight, bullet impacts, etc.).\n• Trigger volumes (determine when objects enter, leave or are inside pre-\ndefined regions in the game world).\n• Complex machines (cranes, moving platform puzzles and so on).\n• Traps (such as an avalanche of boulders).\n• Drivable vehicles with realistic suspensions.\n• Rag doll character deaths.\n• Powered rag doll: a realistic blend between traditional animation and\nrag doll physics.\n13.1. Do You Want Physics in Your Game? 819\n• Dangling props (canteens, necklaces, swords), semi-realistic hair, cloth-\ning movements.\n• Cloth simulations.\n• Water surface simulations and buoyancy.\n• Audio propagation.\nAnd the list goes on.\nWe should note here that in addition to running a physics simulation at\nruntime in our game, we can also run a simulation as part of an offline pre-\nprocessing step in order to generate an animation clip. A number of physics\nplug-ins are available for animation tools like Maya. This is also the ap-\nproachtakenbythe Endorphin1packagebyNaturalMotion,Inc.(http://www.\nnaturalmotion.com/endorphin.htm). In this chapter, we’ll restrict our discus-\nsion to runtime rigid body dynamics simulations, but offline tools are a pow-\nerful option, of which we should always remain aware as we plan our game\nprojects.\n13.1.2 Is Physics Fun?\nThe presence of a rigid body dynamics system in a game does not necessarily\nmake the game fun. More often than not, the inherently chaotic behavior of a\nphysicssimcanactuallydetractfromthegameplayexperienceratherthanen-\nhancing it. The fun derived from physics depends on many factors, including\nthe quality of the simulation itself, the care with which it has been integrated\nwithotherenginesystems, theselectionofphysics-drivengameplayelements\nversus elements that are controlled in a more direct manner, how the physical\nelements interact with the goals of the player and the abilities of the player\ncharacter, and the genre of game being made.\nLet’stakealookatafewbroadgamegenresandhowarigidbodydynam-\nics system might fit into each one.\n13.1.2.1 Simulations (Sims)\nThe primary goal of a sim is to accurately reproduce a real-life experience.\nExamplesincludethe FlightSimulator, GranTurismo andNASCARRacing series\nof games. Clearly, the realism provided by a rigid body dynamics system fits\nextremely well into these kinds of games.\n1NaturalMotion also offers a runtime version of Endorphin called Euphoria .\n820 13. Collision and Rigid Body Dynamics\n13.1.2.2 Physics Puzzle Games\nThe whole idea of a physics puzzle is to let the user play around with dy-\nnamically simulated toys. So obviously this kind of game relies almost en-\ntirely on physics for its core mechanic. Examples of this genre include Bridge\nBuilder, The Incredible Machine , theonline game Fantastic Contraption, and\nCrayon Physics for the iPhone.\n13.1.2.3 Sandbox Games\nIn a sandbox game, there may be no objectives at all, or there may be a large\nnumber of optional objectives. The player’s primary objective is usually to\n“mess around” and explore what the objects in the game world can be made\nto do. Examples of sandbox games include Besiege,Spore, theLittleBigPlanet\nseries, and of course Minecraft.\nSandbox games can put a realistic dynamics simulation to good use, es-\npecially if much of the fun is derived from playing with realistic (or semi-\nrealistic) interactions between objects in the game world. So in these contexts,\nphysics can be fun in and of itself. However, many games trade realism for an\nincreased fun factor (e.g., larger-than-life explosions, gravity that is stronger\nor weaker than normal, etc.). So the dynamics simulation may need to be\ntweaked in various ways to achieve the right “feel.”\n13.1.2.4 Goal-Based and Story-Driven Games\nA goal-based game has rules and specific objectives that the player must\naccomplish in order to progress; in a story-driven game, telling a story is\nof paramount importance. Integrating a physics system into these kinds of\ngames can be tricky. We generally give away controlin exchange for a realistic\nsimulation,andthislossofcontrolcaninhibittheplayer’sabilitytoaccomplish\ngoals or the game’s ability to tell the story.\nFor example, in a character-based platformer game, we want the player\ncharacter to move in ways that are fun and easy to control but not necessarily\nphysically realistic. In a war game, we might want a bridge to explode in a\nrealistic way, but we also may want to ensure that the debris doesn’t end up\nblocking the player’s only path forward. In these kinds of games, physics is\noften not necessarily fun, and in fact it can often get in the way of fun when\nthe player’s goals are at odds with the physically simulated behaviors of the\nobjects in the game world. Therefore, developers must be careful to apply\nphysics judiciously and take steps to control the behavior of the simulation\nin various ways to ensure it doesn’t get in the way of gameplay. It’s usually\na good idea to provide the player with a way out of difficult situations, too.\n13.1. Do You Want Physics in Your Game? 821\nA good example of this can be found in the Haloseries of games, where the\nplayer can press X to flip over a vehicle that has landed upside-down.\n13.1.3 Impact of Physics on a Game\nAdding a physics simulation to a game can have all sorts of impacts on the\nproject and the gameplay. Here are a few examples across various game de-\nvelopment disciplines.\n13.1.3.1 Design Impacts\n•Predictability . The inherent chaos and variability that sets a physically\nsimulated behavior apart from an animated one is also a source of un-\npredictability. If something absolutely must happen a certain way every\ntime, it’s usually better to animate it than to try to coerce your dynamics\nsimulation into producing the motion reliably.\n•Tuning and control . The laws of physics (when modeled accurately) are\nfixed. In a game, we can tweak the value of gravity or the coefficient\nof restitution of a rigid body, which gives back some degree of control.\nHowever, the results of tweaking physics parameters are often indirect\nand difficult to visualize. It’s much harder to tweak a force in order to\nget a character to move in the desired direction than it is to tweak an\nanimation of a character walking.\n•Emergent behaviors. Sometimes physics introduces unexpected features\nintoagame—forexample,therocket-launcherjumptrickin TeamFortress\nClassic, the high-flying exploding Warthog in Haloand the flying “surf-\nboards” in PsyOps.\nIngeneral,thegamedesignshouldusuallydrivethephysicsrequirements\nof a game engine—not the other way around.\n13.1.3.2 Engineering Impacts\n•Tools pipeline. A good collision/physics pipeline takes time to build and\nmaintain.\n•User interface. How does the player control the physics objects in the\nworld? Does he or she shoot them? Walk into them? Pick them up?\nDoes he or she hold them using virtual arms, as in Trespasser ? Or using\na “gravity gun,” as in Half-Life 2?\n•Collision detection. Collision models intended for use within a dynamics\nsimulationmayneed tobemoredetailedandmorecarefullyconstructed\nthan their non-physics-driven counterparts.\n822 13. Collision and Rigid Body Dynamics\n•AI. Pathing may not be predictable in the presence of physically simu-\nlated objects. The engine may need to handle dynamic cover points that\ncan move or blow up. Can the AI use the physics to its advantage?\n•Misbehaved objects. Animation-driven objects can clip slightly through\none another with few or no ill effects. But when driven by a dynamics\nsimulation, objects may bounce off one another in unexpected ways or\njitter badly. Collision filtering may need to be applied to permit objects\nto interpenetrate slightly. Mechanisms may need to be put in place to\nensure that objects settle and go to sleep properly.\n•Rag doll physics. Rag dolls require a lot of fine-tuning and often suffer\nfrom instability in the simulation. An animation may drive parts of a\ncharacter’s body into penetration with other collision volumes—when\nthecharacterturnsintoaragdoll,theseinterpenetrationscancauseenor-\nmous instability. Steps must be taken to avoid this.\n•Graphics . Physics-driven motion can have an effect on renderable ob-\njects’ bounding volumes (where they would otherwise be static or more\npredictable). The presence of destructible buildings and objects can in-\nvalidate some kinds of precomputed lighting and shadow methods.\n•Networking and multiplayer. Physics effects that do not affect gameplay\nmay be simulated exclusively (and independently) on each client ma-\nchine. However, physics that has an effect on gameplay (such as the\ntrajectory that a grenade follows) must be simulated on the server and\naccurately replicated on all clients.\n•Record and playback . The ability to record gameplay and play it back at a\nlatertimeisveryusefulasadebugging/testingaid,anditcanalsoserve\nas a fun game feature. This feature is difficult to implement because it\nrequires every engine system to behave in a deterministic manner, so\nthat everything will play out exactly in the same way during playback\nas it did when the recording was made. If your physics simulation isn’t\ndeterministic, this can become a major fly in the ointment.\n13.1.3.3 Art Impacts\n•Additional tool and workflow complexity. The need to rig up objects with\nmass, friction, constraints and other attributes for consumption by the\ndynamics simulation makes the art department’s job more difficult as\nwell.\n•More complex content. We may need multiple visually identical versions\nof an object with different collision and dynamics configurations for dif-",12909
91-13.2 CollisionPhysics Middleware.pdf,91-13.2 CollisionPhysics Middleware,"13.2. Collision/Physics Middleware 823\nferent purposes—for example, a pristine version and a destructible ver-\nsion.\n•Loss of control . The unpredictability of physics-driven objects can make\nit difficult to control the artistic composition of a scene.\n13.1.3.4 Other Impacts\n•Interdisciplinaryimpacts . The introduction of a dynamics simulation into\nyour game requires close cooperation between engineering, art, audio\nand design.\n•Production impacts. Physics can add to a project’s development costs,\ntechnical and organizational complexity and risk.\nHaving explored the impacts, most teams today do choose to integrate a\nrigidbodydynamicssystemintotheirgames. Withsomecarefulplanningand\nwisechoicesalongtheway,addingphysicstoyourgamecanberewardingand\nfruitful. And as we’ll see below, third-party middleware is making physics\nmore accessible than ever.\n13.2 Collision/Physics Middleware\nWriting a collision system and rigid body dynamics simulation is challenging\nand time-consuming work. The collision/physics system of a game engine\ncan account for a significant percentage of the source code in a typical game\nengine. That’s a lot of code to write and maintain!\nThankfully,anumberofrobust,high-qualitycollision/physicsenginesare\nnow available, either as commercial products or in open source form. Some\nof these are listed below. For a discussion of the pros and cons of various\nphysics SDKs, check out the online game development forums (e.g., http://\nwww.gamedev.net/community/forums/topic.asp?topic_id=463024).\n13.2.1 ODE\nODEstandsfor“OpenDynamicsEngine”(http://www.ode.org). Asitsname\nimplies,ODEisanopensourcecollisionandrigidbodydynamicsSDK.Itsfea-\nture set is similar to a commercial product like Havok. Its benefits include be-\ning free (a big plus for small game studios and school projects!) and the avail-\nabilityoffullsourcecode(whichmakesdebuggingmucheasierandopensup\nthe possibility of modifying the physics engine to meet the specific needs of a\nparticular game).\n824 13. Collision and Rigid Body Dynamics\n13.2.2 Bullet\nBullet is an open source collision detection and physics library used by both\nthe game and film industries. Its collision engine is integrated with its dy-\nnamics simulation, but hooks are provided so that the collision system can\nbe used stand-alone or integrated with other physics engines. It supports\ncontinuous collision detection (CCD)—also known as time of impact (TOI) col-\nlision detection—which, as we’ll see below, can be extremely helpful when\na simulation includes small, fast-moving objects. The Bullet SDK is avail-\nablefordownloadathttp://code.google.com/p/bullet/,andtheBulletwikiis\nlocated at http://www.bulletphysics.com/mediawiki-1.5.8/index.php?title=\nMain_Page.\n13.2.3 TrueAxis\nTrueAxis is another collision/physics SDK. It is free for non-commercial use.\nYou can learn more about TrueAxis at http://trueaxis.com.\n13.2.4 PhysX\nPhysX started out as a library called Novodex, produced and distributed by\nAgeiaaspartoftheirstrategytomarkettheirdedicatedphysicscoprocessor. It\nwas bought by NVIDIA and retooled so that it can run using NVIDIA’s GPUs\nas a coprocessor. (It can also run entirely on a CPU, without GPU support.)\nIt is available at http://www.nvidia.com/object/nvidia_physx.html. Part of\nAgeia’s and NVIDIA’s marketing strategy has been to provide the CPU ver-\nsion of the SDK entirely for free, in order to drive the physics coprocessor\nmarket forward. Developers can also pay a fee to obtain full source code and\nthe ability to customize the library as needed. PhysX is now combined with\nAPEX, NVIDIA’s scalable multiplatform dynamics framework. PhysX/APEX\nis available for Windows, Linux, Mac, Android, Xbox 360, PlayStation 3, Xbox\nOne, PlayStation 4 and Wii.\n13.2.5 Havok\nHavok is the gold standard in commercial physics SDKs, providing one of the\nrichest feature sets available and boasting excellent performance characteris-\nticsonallsupportedplatforms. (It’salsothemostexpensivesolution.) Havok\nis comprised of a core collision/physics engine, plus a number of optional\nadd-on products including a vehicle physics system, a system for modeling\ndestructible environments and a fully featured animation SDK with direct in-\ntegration into Havok’s rag doll physics system. It is available on Xbox 360,",4346
92-13.3 The Collision Detection System.pdf,92-13.3 The Collision Detection System,"13.3. The Collision Detection System 825\nPlayStation 3, Xbox One, PlayStation 4, PlayStation Vita, Wii, Wii U, Win-\ndows 8, Android, Apple Mac and iOS. You can learn more about Havok at\nhttp://www.havok.com.\n13.2.6 Physics Abstraction Layer (PAL)\nThe Physics Abstraction Layer (PAL) is an open source library that allows\ndevelopers to work with more than one physics SDK on a single project.\nIt provides hooks for PhysX (Novodex), Newton, ODE, OpenTissue, Toka-\nmak, TrueAxis and a few other SDKs. You can read more about PAL at\nhttp://www.adrianboeing.com/pal/index.html.\n13.2.7 Digital Molecular Matter (DMM)\nPixelux Entertainment S.A., located in Geneva, Switzerland, has produced a\nunique physics engine that uses finite element methods to simulate the dy-\nnamics of deformable bodies and breakable objects, called Digital Molecu-\nlar Matter (DMM). The engine has both an offline and a runtime compo-\nnent. It was released in 2008 and can be seen in action in LucasArts’ Star\nWars: The Force Unleashed . A discussion of deformable body mechanics is\nbeyond our scope here, but you can read more about DMM at http://www.\npixeluxentertainment.com.\n13.3 The Collision Detection System\nThe primary purpose of a game engine’s collision detection system is to de-\ntermine whether any of the objects in the game world have come into contact.\nTo answer this question, each logical object is represented by one or more ge-\nometricshapes. These shapes are usually quite simple, such as spheres, boxes\nand capsules. However , more complex shapes can also be used. The colli-\nsion system determines whether or not any of the shapes are intersecting (i.e.,\noverlapping) at any given moment in time. So a collision detection system is\nessentially a glorified geometric intersection tester.\nOf course, the collision system does more than answer yes/no questions\nabout shape intersection. It also provides relevant information about the na-\nture of each contact. Contact information can be used to prevent unrealistic\nvisual anomalies on-screen, such as objects interpenetrating one another. This\nis generally accomplished by moving all interpenetrating objects apart prior\nto rendering the next frame. Collisions can provide support for an object—one\nor more contacts that together allow the object to come to rest, in equilibrium\nwith gravity and/or any other forces acting on it. Collisions can also be used\n826 13. Collision and Rigid Body Dynamics\nfor other purposes, such as to cause a missile to explode when it strikes its\ntarget or to give the player character a health boost when he passes through\na floating health pack. A rigid body dynamics simulation is often the most\ndemanding client of the collision system, using it to mimic physically realis-\ntic behaviors like bouncing, rolling, sliding and coming to rest. But, of course,\nevengamesthathavenophysicssystemcanstillmakeheavyuseofacollision\ndetection engine.\nIn this chapter, we’ll go on a brief high-level tour of how collision detec-\ntion engines work. For an in-depth treatment of this topic, a number of ex-\ncellent books on real-time collision detection are available, including [14], [48]\nand [11].\n13.3.1 Collidable Entities\nIfwewantaparticularlogicalobjectinourgametobecapableofcollidingwith\nother objects, we need to provide it with a collision representation , describing\nthe object’s shape and its position and orientation in the game world. This\nis a distinct data structure, separate from the object’s gameplay representation\n(the code and data that define its role and behavior in the game) and separate\nfrom itsvisual representation (which might be an instance of a triangle mesh, a\nsubdivision surface, a particle effect or some other visual representation).\nFrom the point of view of detecting intersections, we generally favor\nshapesthataregeometricallyandmathematicallysimple. Forexample, arock\nmight be modeled as a sphere for collision purposes; the hood of a car might\nbe represented by a rectangular box; a human body might be approximated\nby a collection of interconnected capsules (pill-shaped volumes). Ideally, we\nshould resort to a more complex shape only when a simpler representation\nproves inadequate to achieve the desired behavior in the game. Figure 13.1\nshows a few examples of using simple shapes to approximate object volumes\nfor collision detection purposes.\nHavok uses the term collidable to describe a distinct, rigid object that can\ntake part in collision detection. It represents each collidable with an instance\nof the C++ class hkpCollidable . PhysX calls its rigid objects actorsand rep-\nresents them as instances of the class NxActor . In both of these libraries, a\ncollidableentitycontainstwobasicpiecesofinformation—a shapeandatrans-\nform. The shape describes the collidable’s geometric form, and the transform\ndescribes the shape’s position and orientation in the game world. Collidables\nneed transforms for three reasons:\n1. Technically speaking, a shape only describes the form of an object (i.e.,\nwhether it is a sphere, a box, a capsule or some other kind of volume).\n13.3. The Collision Detection System 827\nFigure 13.1. Simple geometric shapes are often used to approximate the collision volumes of the\nobjects in a game.\nIt may also describe the object’s size (e.g., the radius of a sphere or the\ndimensionsofabox). Butashapeisusuallydefinedwithitscenteratthe\noriginandinsomesortofcanonicalorientationrelativetothecoordinate\naxes. To be useful, a shape must therefore be transformed in order to\nposition and orient it appropriately in world space.\n2. Many of the objects in a game are dynamic. Moving an arbitrarily com-\nplex shape through space could be expensive if we had to move the fea-\nturesof the shape (vertices, planes, etc.) individually. But with a trans-\nform, any shape can be moved in space inexpensively, no matter how\nsimple or complex the shape’s features may be.\n3. The information describing some of the more complex kinds of shapes\ncan take up a nontrivial amount of memory. So, it can be beneficial to\npermit more than one collidable to share a single shape description. For\nexample, in a racing game, the shape information for many of the cars\nmight be identical. In that case, all of the car collidables in the game can\nshare a single car shape.\nAnyparticularobjectinthegamemayhavenocollidableatall(ifitdoesn’t\nrequirecollisiondetectionservices),asinglecollidable(iftheobjectisasimple\nrigid body) or multiple collidables (each representing one rigid component of\nan articulated robot arm, for example).\n828 13. Collision and Rigid Body Dynamics\n13.3.2 The Collision/Physics World\nA collision system typically keeps track of all of its collidable entities via a\nsingleton data structure known as the collision world. The collision world is a\ncomplete representation of the game world designed explicitly for use by the\ncollision detection system. Havok’s collision world is an instance of the class\nhkpWorld. Likewise, the PhysX world is an instance of NxScene . ODE uses\nan instance of class dSpace to represent the collision world; it is actually the\nrootofahierarchyofgeometricvolumesrepresentingallthecollidableshapes\nin the game.\nMaintainingallcollisioninformationinaprivatedatastructurehasanum-\nberofadvantagesoverattemptingtostorecollisioninformationwiththegame\nobjects themselves. For one thing, the collision world need only contain col-\nlidables for those game objects that can potentially collide with one another.\nThis eliminates the need for the collision system to iterate over any irrelevant\ndata structures. This design also permits collision data to be organized in the\nmost efficient manner possible. The collision system can take advantage of\ncache coherency to maximize performance, for example. The collision world\nis also an effective encapsulation mechanism, which is generally a plus from\nthe perspectives of understandability, maintainability, testability and the po-\ntential for software reuse.\n13.3.2.1 The Physics World\nIf a game has a rigid body dynamics system, it is usually tightly integrated\nwiththecollisionsystem. Ittypicallysharesits“world”datastructurewiththe\ncollision system, and each rigid body in the simulation is usually associated\nwith a single collidable in the collision system. This design is commonplace\namong physics engines because of the frequent and detailed collision queries\nrequired by the physics system. It’s typical for the physics system to actually\ndrivethe operation of the collision system, instructing it to run collision tests\natleastonce, andsometimesmultipletimes, persimulationtimestep. Forthis\nreason, the collision world is often called the collision/physics world or some-\ntimes just the physics world .\nEach dynamic rigid body in the physics simulation is usually associated\nwith a single collidable object in the collision system (although not all collid-\nables need be dynamic rigid bodies). For example, in Havok, a rigid body is\nrepresented by an instance of the class hkpRigidBody, and each rigid body\nhasapointertoexactlyone hkpCollidable. InPhysX,theconceptsofcollid-\nable and rigid body are comingled—the NxActor class serves both purposes\n(although the physical properties of the rigid body are stored separately, in\n13.3. The Collision Detection System 829\nan instance of NxBodyDesc ). In both SDKs, it is possible to tell a rigid body\nthat its location and orientation are to be fixed in space, meaning that it will\nbe omitted from the dynamics simulation and will serve as a collidable only.\nDespite this tight integration, most physics SDKs do make at least some\nattempt to separate the collision library from the rigid body dynamics simu-\nlation. This permits the collision system to be used as a stand-alone library\n(which is important for games that don’t need physics but do need to detect\ncollisions). Italsomeansthatagamestudiocould theoretically replaceaphysics\nSDK’s collision system entirely, without having to rewrite the dynamics sim-\nulation. (Practically speaking, this may be a bit harder than it sounds!)\n13.3.3 Shape Concepts\nA rich body of mathematical theory underlies the everyday concept of shape\n(see http://en.wikipedia.org/wiki/Shape). For our purposes, we can think of\na shape simply as a region of space described by a boundary, with a definite\ninsideandoutside. In two dimensions, a shape has area, and its boundary is\ndefined either by a curved line or by three or more straight edges (in which\ncaseit’sa polygon). Inthreedimensions,ashapehasvolume,anditsboundary\niseitheracurvedsurfaceoriscomposedofpolygons(inwhichcaseisitcalled\napolyhedron).\nIt’s important to note that some kinds of game objects, like terrain, rivers\nor thin walls, might be best represented by surfaces. In three-space, a surface\nis a two-dimensional geometric entity with a frontand abackbut no inside or\noutside. Examplesincludeplanes,triangles,subdivisionsurfacesandsurfaces\nconstructed from a group of connected triangles or other polygons. Most col-\nlision SDKs provide support for surface primitives and extend the term shape\nto encompass both closed volumes and open surfaces.\nIt’s commonplace for collision libraries to allow surfaces to be given vol-\nume via an optional extrusion parameter. Such a parameter specifies how\n“thick” a surface should be. Doing this helps reduce the occurrence of missed\ncollisionsbetweensmall, fast-movingobjectsandinfinitesimallythinsurfaces\n(the so-called “bullet through paper” problem—see Section 13.3.5.7).\n13.3.3.1 Intersection\nWeallhaveanintuitivenotionofwhatan intersection is. Technicallyspeaking,\nthe term comes from set theory (http://en.wikipedia.org/wiki/Intersection_\n(set_theory)). The intersection of two sets is comprised of the subset of mem-\nbers that are common to both sets. In geometrical terms, the intersection be-\ntween two shapes is just the (infinitely large!) set of all points that lie inside\nboth shapes.\n830 13. Collision and Rigid Body Dynamics\n13.3.3.2 Contact\nIngames,we’renotusuallyinterestedinfindingtheintersectioninthestrictest\nsense, as a set of points. Instead, we want to know simply whether or not\ntwo objects are intersecting. In the event of a collision, the collision system\nwill usually provide additional information about the nature of the contact.\nThis information allows us to separate the objects in a physically plausible\nand efficient way, for example.\nCollision systems usually package contact information into a convenient\ndata structure that can be instanced for each contact detected. For example,\nHavok returns contacts as instances of the class hkContactPoint. Contact\ninformation often includes a separating vector—a vector along which we can\nslide the objects in order to efficiently move them out of collision. It also typ-\nically contains information about which two collidables were in contact, in-\ncluding which individual shapes were intersecting and possibly even which\nindividual features of those shapes were in contact. The system may also re-\nturn additional information, such as the velocity of the bodies projected onto\nthe separating normal.\n13.3.3.3 Convexity\nOne of the most important concepts in the field of collision detection is the\ndistinction between convexandnon-convex (i.e.,concave) shapes. Technically, a\nconvex shape is defined as one for which no ray originating inside the shape\nwill pass through its surface more than once. A simple way to determine if\na shape is convex is to imagine shrink-wrapping it with plastic film—if it’s\nconvex, no air pockets will be left under the film. So in two dimensions, cir-\ncles, rectangles and triangles are all convex, but Pac Man is not. The concept\nextends equally well to three dimensions.\nThe property of convexity is important because, as we’ll see, it’s generally\nsimpler and less computationally intensive to detect intersections between\nconvex shapes than concave ones. See http://en.wikipedia.org/wiki/Convex\nfor more information about convex shapes.\n13.3.4 Collision Primitives\nCollision detection systems can usually work with a relatively limited set of\nshape types. Some collision systems refer to these shapes as collisionprimitives\nbecause they are the fundamental building blocks out of which more complex\nshapescanbeconstructed. Inthissection,we’lltakeabrieflookatsomeofthe\nmost common types of collision primitives.\n13.3. The Collision Detection System 831\n13.3.4.1 Spheres\nThesimplestthree-dimensionalvolumeisasphere. Andasyoumightexpect,\nspheres are the most efficient kind of collision primitive. A sphere is repre-\nsented by a center point and a radius. This information can be conveniently\npacked into a four-element floating-point vector—a format that works partic-\nularly well with SIMD math libraries.\n13.3.4.2 Capsules\nAcapsuleisapill-shapedvolume,composedofacylinderandtwohemispher-\nical end caps. It can be thought of as a swept sphere —the shape that is traced\nout as a sphere moves from point A to point B. (There are, however, some im-\nportant differences between a static capsule and a sphere that sweeps out a\ncapsule-shaped volume over time, so the two are not identical.) Capsules are\noften represented by two points and a radius (Figure 13.2). Capsules are more\nefficient to intersect than cylinders or boxes, so they are often used to model\nobjects that are roughly cylindrical, such as the limbs of a human body.\nr r2 1\nFigure 13.2. A capsule can be represented by two points and a radius.\n13.3.4.3 Axis-Aligned Bounding Boxes\nAn axis-aligned bounding box (AABB) is a rectangular volume (technically\nknown as a cuboid) whose faces are parallel to the axes of the coordinate sys-\ntem. Of course, a box that is axis-aligned in one coordinate system will not\nnecessarily be axis-aligned in another. So we can only speak about an AABB\nin the context of the particular coordinate frame(s) with which it aligns.\nAn AABB can be conveniently defined by two points: one containing the\nminimumcoordinatesoftheboxalongeachofthethreeprincipalaxesandthe\nother containing its maximum coordinates. This is depicted in Figure 13.3.\nThe primary benefit of axis-aligned boxes is that they can be tested for\ninterpenetration with other axis-aligned boxes in a highly efficient manner.\nThe big limitation of using AABBs is that they must remain axis-aligned at\nall times if their computational advantages are to be maintained. This means\nthat if an AABB is used to approximate the shape of an object in the game,\n832 13. Collision and Rigid Body Dynamics\ny\nx xmin xmaxyminymax\nFigure 13.3. An axis-aligned box.\nthe AABB will have to be recalculated whenever that object rotates. Even if\nan object is roughly box-shaped, its AABB may degenerate into a very poor\napproximation to its shape when the object rotates off-axis. This is shown in\nFigure 13.4.\ny\nxy\nx\nFigure 13.4. An AABB is only a good approximation to a box-shaped object when the object’s prin-\ncipal axes are roughly aligned with the coordinated system’s axes.\n13.3.4.4 Oriented Bounding Boxes\nIf we permit an axis-aligned box to rotate relative to its coordinate system, we\nhavewhatisknownasanorientedboundingbox(OBB).Itisoftenrepresented\nby three half-dimensions (half-width, half-depth and half-height) and a trans-\nformation, which positions the center of the box and defines its orientation\nrelative to the coordinate axes. Oriented boxes are a commonly used collision\nprimitivebecausetheydoabetterjobatfittingarbitrarilyorientedobjects, yet\ntheir representation is still quite simple.\n13.3.4.5 Discrete Oriented Polytopes (DOP)\nA discrete oriented polytope (DOP) is a more-general case of the AABB and\nOBB. It is a convex polytope that approximates the shape of an object. A DOP\ncan be constructed by taking a number of planes at infinity and sliding them\nalong their normal vectors until they come into contact with the object whose\n13.3. The Collision Detection System 833\nshapeistobeapproximated. AnAABBisa6-DOPinwhichtheplanenormals\nare taken parallel to the coordinate axes. An OBB is also a 6-DOP in which the\nplane normals are parallel to the object’s natural principal axes. A k-DOP is\nconstructed from an arbitrary number of planes k. A common method of con-\nstructingaDOPistostartwithanOBBfortheobjectinquestionandthenbevel\nthe edges and/or corners at 45 degrees with additional planes in an attempt\nto yield a tighter fit. An example of a k-DOP is shown in Figure 13.5.\nFigure 13.5. An OBB that has been beveled on all eight corners is known as a 14-DOP.\n13.3.4.6 Arbitrary Convex Volumes\nMost collision engines permit arbitrary convex volume to be constructed by a\n3D artist in a package like Maya. The artist builds the shape out of polygons\n(triangles or quads). An offline tool analyzes the triangles to ensure that they\nactually do form a convex polyhedron. If the shape passes the convexity test,\nitstrianglesareconvertedintoacollectionofplanes(essentiallya k-DOP),rep-\nresented by kplane equations, or kpoints and knormal vectors. (If it is found\nto be non-convex, it can still be represented by a polygon soup—described in\nthe next section.) This approach is depicted in Figure 13.6.\nConvex volumes are more expensive to intersection-test than the simpler\ngeometricprimitiveswe’vediscussedthusfar. However,aswe’llseeinSection\nFigure 13.6. An arbitrary convex volume can be represented by a collection of intersecting planes.\n834 13. Collision and Rigid Body Dynamics\n13.3.5.5, certain highly efficient intersection-finding algorithms such as GJK\nare applicable to these shapes because they are convex.\n13.3.4.7 Poly Soup\nSome collision systems also support totally arbitrary, non-convex shapes.\nThese are usually constructed out of triangles or other simple polygons. For\nthis reason, this type of shape is often called a polygon soup, or poly soup for\nshort. Poly soups are often used to model complex static geometry, such as\nterrain and buildings (Figure 13.7).\nAs you might imagine, detecting collisions with a poly soup is the most\nexpensive kind of collision test. In effect, the collision engine must test every\nindividual triangle, and it must also properly handle spurious intersections\nwith triangle edges that are shared between adjacent triangles. As a result,\nmost games try to limit the use of poly soup shapes to objects that will not\ntake part in the dynamics simulation.\nDoes a Poly Soup Have an Inside?\nUnlike convex and simple shapes, a poly soup does not necessarily represent\na volume—it can represent an open surface as well. Poly soup shapes often\ndon’tincludeenoughinformationtoallowthecollisionsystemtodifferentiate\nbetween a closed volume and an open surface. This can make it difficult to\nknow in which direction to push an object that is interpenetrating a poly soup\nin order to bring the two objects out of collision.\nThankfully, this is by no means an intractable problem. Each triangle in\nFigure 13.7. A poly soup is often used to model complex static surfaces such as terrain or build-\nings.\n13.3. The Collision Detection System 835\na poly soup has a front and a back, as defined by the winding order of its\nvertices. Therefore, it is possible to carefully construct a poly soup shape so\nthat all of the polygons’ vertex winding orders are consistent (i.e., adjacent\ntriangles always “face” in the same direction). This gives the entire poly soup\na notion of “front” and “back.” If we also store information about whether\na given poly soup shape is open or closed (presuming that this fact can be\nascertained by offline tools), then for closed shapes, we can interpret “front”\nand “back” to mean “outside” and “inside” (or vice versa, depending on the\nconventions used when constructing the poly soup).\nWecanalso“fake”aninsideandoutsideforcertainkindsof openpolysoup\nshapes(i.e.,surfaces). Forexample,iftheterraininourgameisrepresentedby\nan open poly soup, then we can decide arbitrarily that the front of the surface\nalways points away from the Earth. This implies that “front” should always\ncorrespond to “outside.” Practically speaking, to make this work, we would\nprobablyneedtocustomizethecollisionengineinsomewayinordertomake\nit aware of our particular choice of conventions.\n13.3.4.8 Compound Shapes\nSome objects that cannot be adequately approximated by a single shape can\nbe approximated well by a collection of shapes. For example, a chair might be\nmodeledoutoftwoboxes—oneforthebackofthechairandoneenclosingthe\nseat and all four legs. This is shown in Figure 13.8.\nAcompoundshapecanoftenbeamore-efficientalternativetoapolysoup\nfor modeling non-convex objects; two or more convex volumes can often out-\nperform a single poly soup shape. What’s more, some collision systems can\ntake advantage of the convex bounding volume of the compound shape as a\nwhole when testing for collisions. In Havok, this is called midphase collision\ndetection. As the example in Figure 13.9 shows, the collision system first tests\nthe convex bounding volumes of the two compound shapes. If they do not\nintersect, the system needn’t test the subshapes for collisions at all.\nFigure 13.8. A chair can be modeled using a pair of interconnected box shapes.\n836 13. Collision and Rigid Body Dynamics\nB2B3\nB1\nB4A1\nA2Sphere A\nSphere B\nA1\nA2\nB1\nB2\nB3\nB4Bounding Volume \nHierarchies:\nSphere A\nSphere B\nFigure 13.9. A collision system need only test the subshapes of a pair of compound shapes when\ntheir convex bounding volumes (in this case, Sphere A and Sphere B) are found to be intersecting.\n13.3.5 Collision Testing and Analytical Geometry\nA collision system makes use of analytical geometry —mathematical descrip-\ntions of three-dimensional volumes and surfaces—in order to detect intersec-\ntions between shapes computationally. See http://en.wikipedia.org/wiki/\nAnalytic_geometry for more details on this profound and broad area of re-\nsearch. In this section, we’ll briefly introduce the concepts behind analytical\ngeometry, show a few common examples and then discuss the generalized\nGJK intersection testing algorithm for arbitrary convex polyhedra.\n13.3.5.1 Point versus Sphere\nWe can determine whether a point plies within a sphere by simply forming\nthe separation vector sbetween the point and the sphere’s center cand then\ncheckingitslength. Ifitisgreaterthantheradiusofthesphere r,thenthepoint\nlies outside the sphere; otherwise, it lies inside:\ns=c p;\nifjsjr,thenpis inside.\n13.3.5.2 Sphere versus Sphere\nDetermining if two spheres intersect is almost as simple as testing a point\nagainst a sphere. Again, we form a vector sconnecting the center points of\nthe two spheres. We take its length and compare it with the sum of the radii\nof the two spheres. If the length of the separating vector is less than or equal\nto the sum of the radii, the spheres intersect; otherwise, they do not:\ns=c1 c2; (13.1)\nifjsj(r1+r2),then spheres intersect.\n13.3. The Collision Detection System 837\nTo avoid the square root operation inherent in calculating the length of\nvector s,wecansimplysquaretheentireequation. SoEquation(13.1)becomes\ns=c1 c2;\njsj2=ss;\nifjsj2(r1+r2)2,then spheres intersect.\n13.3.5.3 The Separating Axis Theorem\nMost collision detection systems make heavy use of a theorem known as\ntheseparating axis theorem (http://en.wikipedia.org/wiki/Separating_axis\n_theorem). It states that if an axis can be found along which the projections of\ntwo convex shapes do not overlap, then we can be certain that the two shapes\ndo not intersect at all. If such an axis does not exist andthe shapes are convex,\nthen we know for certain that they do intersect. (If the shapes are concave,\nthen they may not be interpenetrating despite the lack of a separating axis.\nThis is one reason why we tend to favor convex shapes in collision detection.)\nThis theorem is easiest to visualize in two dimensions. Intuitively, it says\nthat if a line can be found, such that object A is entirely on one side of the\nline and object B is entirely on the other side, then objects A and B do not\noverlap. Such a line is called a separating line, and it is always perpendicular to\nthe separating axis. So once we’ve found a separating line, it’s a lot easier to\nconvinceourselvesthatthetheoryisinfactcorrectbylookingattheprojections\nof our shapes onto the axis that is perpendicular to the separating line.\nThe projection of a two-dimensional convexshape onto an axis acts like the\nshadow that the object would leave on a thin wire. It is always a line seg-\nment, lying on the axis, that represents the maximum extents of the object in\nthe direction of the axis. We can also think of a projection as a minimum and\nmaximum coordinate along the axis, which we can write as the fully closed\ninterval [cmin,cmax]. AsyoucanseeinFigure13.10, whenaseparatinglineex-\nists between two shapes, their projections do not overlap along the separating\naxis. However, the projections may overlap along other, non-separating axes.\nIn three dimensions, the separating line becomes a separating plane, but\nthe separating axis is still an axis (i.e., an infinite line). Again, the projection\nof a three-dimensional convexshape onto an axis is a line segment, which we\ncan represent by the fully closed interval [cmin,cmax].\nSome types of shapes have properties that make the potential separating\naxes obvious. To detect intersections between two such shapes A and B, we\ncan project the shapes onto each potential separating axis in turn and then\ncheck whether or not the two projection intervals, [cA\nmin,cA\nmax]and [cB\nmin,cB\nmax],\n838 13. Collision and Rigid Body Dynamics\nABNon-SeparatingAxis\nSeparatingAxisSeparating \nLine/Plane\nProjectionofAProjectionofBAB\nFigure 13.10. The projections of two shapes onto a separating axis are always two disjoint line\nsegments. The projections of these same shapes onto a non-separating axis are not necessarily\ndisjoint. If no separating axis exists, the shapes intersect.\nare disjoint (i.e., do not overlap). In math terms, the intervals are disjoint if\ncA\nmax<cB\nminor if cB\nmax<cA\nmin. If the projection intervals along one of the\npotential separating axes are disjoint, then we’ve found a separating axis, and\nwe know the two shapes do not intersect.\nOne example of this principle in action is the sphere-versus-sphere test. If\ntwo spheres do not intersect, then the axis parallel to the line segment join-\ning the spheres’ center points will always be a valid separating axis (although\nother separating axes may exist, depending on how far apart the two spheres\nare). Tovisualizethis,considerthelimitwhenthetwospheresarejustaboutto\ntouch but have not yet come into contact. In that case, the onlyseparating axis\nis the one parallel to the center-to-center line segment. As the spheres move\napart,wecanrotatetheseparatingaxismoreandmoreineitherdirection. This\nis shown in Figure 13.11.\n13.3.5.4 AABB versus AABB\nTo determine whether two AABBs are intersecting, we can again apply the\nseparatingaxistheorem. ThefactthatthefacesofbothAABBsareguaranteed\nto lie parallel to a common set of coordinate axes tells us that if a separating\naxis exists, it will be one of these three coordinate axes.\nSo, to test for intersections between two AABBs, which we’ll call A and B,\nwe merely inspect the minimum and maximum coordinates of the two boxes\nalong each axis independently. Along the x-axis, we have the two intervals\n13.3. The Collision Detection System 839\nSeparating\nLine/Plane\nSeparatingAxisMany\nSeparating AxesMany\nSeparating\nLines/Planes\nFigure 13.11. When two spheres are an inﬁnitesimal distance apart, the only separating axis lies\nparallel to the line segment formed by the two spheres’ center points.\n[xA\nmin,xA\nmax]and [xB\nmin,xB\nmax], and we have corresponding intervals for the y-\nandz-axes. Iftheintervalsoverlapalong allthreeaxes ,thenthetwoAABBsare\nintersecting—inallothercases,theyarenot. Examplesofintersectingandnon-\nintersecting AABBs are shown in Figure 13.12 (simplified to two dimensions\nforthepurposesofillustration). Foranin-depthdiscussionofAABBcollision,\nsee http://www.gamasutra.com/features/20000203/lander_01.htm.\n13.3.5.5 Detecting Convex Collisions: The GJK Algorithm\nA very efficient algorithm exists for detecting intersections between arbitrary\nconvexpolytopes (i.e., convex polygons in two dimensions, or convex poly-\nhedra in three dimensions). It is known as the GJK algorithm, named after\nits inventors, E. G. Gilbert, D. W. Johnson and S. S. Keerthi of the University\nof Michigan. Many papers have been written on the algorithm and its vari-\ny\nxy\nx\nFigure 13.12. A two-dimensional example of intersecting and non-intersecting AABBs. Notice that\neven though the second pair of AABBs are intersecting along the x-axis, they are not intersecting\nalong the y-axis.\n840 13. Collision and Rigid Body Dynamics\nants,includingtheoriginalpaper(http://ieeexplore.ieee.org/xpl/freeabs_all.\njsp?&arnumber=2083), an excellent SIGGRAPH PowerPoint presentation by\nChrister Ericson (http://realtimecollisiondetection.net/pubs/SIGGRAPH04_\nEricson_the_GJK_algorithm.ppt) and another great PowerPoint presenta-\ntionbyGinovandenBergen(www.laas.fr/~nic/MOVIE/Workshop/Slides/\nGino.vander.Bergen.ppt). However, the easiest-to-understand (and most en-\ntertaining) description of the algorithm is probably Casey Muratori’s in-\nstructional video entitled “Implementing GJK,” available online at http://\nmollyrocket.com/849. Because these descriptions are so good, I’ll just give\nyouafeelfortheessenceofthealgorithmhereandthendirectyoutotheMolly\nRocket website and the other references cited above for additional details.\nTheGJKalgorithmreliesonageometricoperationknownasthe Minkowski\ndifference . This fancy-sounding operation is really quite simple: We take every\npoint that lies within shape B and subtract it pairwise from every point inside\nshape A. The resulting set of points{(Ai Bj)}\nis the Minkowski difference.\nThe useful thing about the Minkowski difference is that, when applied to\ntwo convex shapes, it will contain the origin if and only if those two shapes\nintersect. Proof of this statement is a bit beyond our scope, but we can intuit\nwhyitistruebyrememberingthatwhenwesaytwoshapesAandBintersect,\nwe really mean that there are points within A that are alsowithin B. During\nthe process of subtracting every point in B from every point in A, we would\nexpecttoeventuallyhitoneofthosesharedpointsthatlieswithinbothshapes.\nA point minus itself is all zeros, so the Minkowski difference will contain the\norigin if (and only if) sphere A and sphere B have points in common. This is\nillustrated in Figure 13.13.\nThe Minkowski difference of two convex shapes is itself a convex shape.\nAll we care about is the convex hull of the Minkowski difference, not all of the\ninterior points. The basic procedure of GJK is to try to find a tetrahedron (i.e.,\na four-sided shape made out of triangles) that lies on the convex hull of the\nMinkwoski difference and that encloses the origin. If one can be found, then\nthe shapes intersect; if one cannot be found, then they don’t.\nA tetrahedron is just one case of a geometrical object known as a simplex.\nBut don’t let that name scare you—a simplex is just a collection of points. A\nsingle-point simplex is a point, a two-point simplex is a line segment, a three-\npoint simplex is a triangle and a four-point simplex is a tetrahedron (see Fig-\nure 13.14).\nGJKisaniterativealgorithmthatstartswithaone-pointsimplexlyingany-\nwhere within the Minkowski difference hull. It then attempts to build higher-\norder simplexes that might potentially contain the origin. During each itera-\ntionoftheloop,wetakealookatthesimplexwecurrentlyhaveanddetermine\n13.3. The Collision Detection System 841\nContains the Originy\nx\nA – B\nDoes not Contain \nthe Originy\nA – BAB\nAB\nx\nFigure 13.13. The Minkowski difference of two intersecting convex shapes contains the origin, but\nthe Minkowski difference of two non-intersecting shapes does not.\nin which direction the origin lies relative to it. We then find a supportingvertex\nof the Minkowski difference in that direction—i.e., the vertex of the convex\nhull that is closest to the origin in the direction we’re currently going. We add\nthat new point to the simplex, creating a higher-order simplex (i.e., a point\nbecomes a line segment, a line segment becomes a triangle and a triangle be-\ncomes a tetrahedron). If the addition of this new point causes the simplex to\nsurround the origin, then we’re done—we know the two shapes intersect. On\nthe other hand, if we are unable to find a supporting vertex that is closer to\ntheoriginthanthecurrentsimplex,thenweknowthatwecannevergetthere,\nwhich implies that the two shapes do notintersect. This idea is illustrated in\nFigure 13.15.\nTotrulyunderstandtheGJKalgorithm,you’llneedtocheckoutthepapers\nand video I referenced previously. But hopefully this description will whet\nyour appetite for deeper investigation. Or, at the very least, you can impress\nLine Segment Point Triangle Tetrahedron\nFigure 13.14. Simplexes containing one, two, three and four points.\n842 13. Collision and Rigid Body Dynamics\nNew Point\ny\nxNew Point\ny\nx\nSearch\nDirectionSearch\nDirection\nFigure 13.15. In the GJK algorithm, if adding a point to the current simplex creates a shape that\ncontains the origin, we know the shapes intersect; if there is no supporting vertex that will bring\nthe simplex any closer to the origin, then the shapes do not intersect.\nyour friends by dropping the name “GJK” at parties. (Just don’t try this at job\ninterviews unless you really do understand the algorithm!)\n13.3.5.6 Other Shape-Shape Combinations\nWe won’t cover any of the other shape-shape intersection combinations here,\nastheyarecoveredwellinothertextssuchas[14],[48]and[11]. Thekeypoint\nto recognize here, however, is that the number of shape-shape combinations is\nvery large. In fact, for Nshape types, the number of pairwise tests required\nisO(N2). Much of the complexity of a collision engine arises because of the\nsheer number of intersection cases it must handle. This is one reason why\nthe authors of collision engines usually try to limit the number of primitive\ntypes—doing so drastically reduces the number of cases the collision detector\nmust handle. (This is also why GJK is popular—it handles collision detection\nbetween allconvex shape types in one fell swoop. The only thing that differs\nfrom shape type to shape type is the supportfunction used in the algorithm.)\nThere’s also the practical matter of how to implement the code that se-\nlects the appropriate collision-testing function given two arbitrary shapes\nthat are to be tested. Many collision engines use a double dispatch method\n(http://en.wikipedia.org/wiki/Double_dispatch). Insingledispatch(i.e.,vir-\ntual functions), the type of a single object is used to determine which concrete\nimplementation of a particular abstract function should be called at runtime.\nDouble dispatch extends the virtual function concept to two object types. It\ncan be implemented via a two-dimensional function look-up table keyed by\nthe types of the two objects being tested. It can also be implemented by ar-\nranging for a virtual function based on the type of object A to call a second\nvirtual function based on the type of object B.\nLet’s take a look at a real-world example. Havok uses objects known as\ncollisionagents (classesderivedfrom hkpCollisionAgent )tohandlespecific\n13.3. The Collision Detection System 843\nFigure 13.16. A small, fast-moving object can leave gaps in its motion path between consecutive\nsnapshots of the collision world, meaning that collisions might be missed entirely.\nintersection test cases. Concrete agent classes include hkpSphereSphere -\nAgent, hkpSphereCapsuleAgent, hkpGskConvexConvexAgent and so\non. The agent types are referenced by what amounts to a two-dimensional\ndispatch table, managed by the class hkpCollisionDispatcher. As you’d\nexpect,thedispatcher’sjobistoefficientlylookuptheappropriateagentgiven\na pair of collidables that are to be collision-tested and then call it, passing the\ntwo collidables as arguments.\n13.3.5.7 Detecting Collisions between Moving Bodies\nThusfar,we’veconsideredonlystaticintersectiontestsbetweenstationaryob-\njects. Whenobjectsmove,thisintroducessomeadditionalcomplexity. Motion\ningamesisusuallysimulatedindiscretetimesteps. Soonesimpleapproachis\nto treat the positions and orientations of each rigid body as stationary at each\ntime step and use static intersection tests on each “snapshot” of the collision\nworld. This technique works as long as objects aren’t moving too fast relative\nto their sizes. In fact, it works so well that many collision/physics engines,\nincluding Havok, use this approach by default.\nHowever,thistechniquebreaksdownforsmall,fast-movingobjects. Imag-\nineanobjectthatismovingsofastthatitcoversadistance largerthanitsownsize\n(measured in the direction of travel) between time steps. If we were to over-\nlay two consecutive snapshots of the collision world, we’d notice that there\nis now a gap between the fast-moving object’s images in the two snapshots.\nIf another object happens to lie within this gap, we’ll miss the collision with\nit entirely. This problem, illustrated in Figure 13.16, is known as the “bullet\nthrough paper” problem, also known as “tunneling.” The following sections\ndescribe a number of common ways to overcome this problem.\nSwept Shapes\nOne way to avoid tunneling is to make use of swept shapes . A swept shape\n844 13. Collision and Rigid Body Dynamics\nis a new shape formed by the motion of a shape from one point to another\nover time. For example, a swept sphere is a capsule, and a swept triangle is a\ntriangular prism (see Figure 13.17).\nRatherthantestingstaticsnapshotsofthecollisionworldforintersections,\nwecantestthesweptshapesformedbymovingtheshapesfromtheirpositions\nand orientations in the previous snapshot to their positions and orientations\nin the current snapshot. This approach amounts to linearly interpolating the\nmotion of the collidables between snapshots, because we generally sweep the\nshapes along line segments from snapshot to snapshot.\nOfcourse,linearinterpolationmaynotbeagoodapproximationofthemo-\ntion of a fast-moving collidable. If the collidable is following a curved path,\nthentheoreticallyweshouldsweepitsshapealongthatcurvedpath. Unfortu-\nnately, a convex shape that has been swept along a curve is not itself convex,\nso this can make our collision tests much more complex and computationally\nintensive.\nIn addition, if the convex shape we are sweeping is rotating, the resulting\nswept shape is not necessarily convex, even when it is swept along a line seg-\nment. As Figure 13.18 shows, we canalways form a convex shape by linearly\nextrapolatingtheextremefeaturesoftheshapesfromthepreviousandcurrent\nsnapshots—but the resulting convex shape is not necessarily an accurate rep-\nresentation of what the shape really would have done over the time step. Put\nanother way, a linear interpolation is not appropriate in general for rotating\nshapes. So unless our shapes are not permitted to rotate, intersection testing\nof swept shapes becomes much more complex and computationally intensive\nthan its static snapshot-based counterpart.\nSwept shapes can be a useful technique for ensuring that collisions are not\nmissed between static snapshots of the collision world state. However, the\nresults are generally inaccurate when linearly interpolating curved paths or\nFigure 13.17. A swept sphere is a capsule; a swept triangle is a triangular prism.\n13.3. The Collision Detection System 845\nFigure 13.18. A rotating object swept along a line segment does not necessarily generate a convex shape (left). A linear in-\nterpolation of the motion does form a convex shape (right), but it can be a fairly inaccurate approximation of what actually\nhappened during the time step.\nrotating collidables, so more-detailed techniques may be required depending\non the needs of the game.\nContinuous Collision Detection (CCD)\nAnother way to deal with the tunneling problem is to employ a technique\nknown as continuous collision detection (CCD). The goal of CCD is to find the\nearliesttime of impact (TOI) between two moving objects over a given time in-\nterval.\nCCD algorithms are generally iterative in nature. For each collidable, we\nmaintainbothitspositionandorientationattheprevioustimestepanditspo-\nsition and orientation at the current time. This information can be used to lin-\nearly interpolate the position and rotation independently, yielding an approx-\nimation of the collidable’s transform at any time between the previous and\ncurrent time steps. The algorithm then searches for the earliest TOI along the\nmotion path. A number of search algorithms are commonly used, including\nBrian Mirtich’s conservative advancement method, performing a ray cast on the\nMinkowskisum, orconsideringtheminimumTOIofindividualfeaturepairs.\nErwin Coumans of Sony Interactive Entertainment describes some of these\nalgorithms in http://gamedevs.org/uploads/continuous-collision-detection\n-and-physics.pdf along with his own novel variation on the conservative ad-\nvancement approach.\n13.3.6 Performance Optimizations\nCollision detection is a CPU-intensive task for two reasons:\n1. Thecalculations requiredto determine whethertwo shapes intersectare\nthemselves nontrivial.\n846 13. Collision and Rigid Body Dynamics\n2. Most game worlds contain a large number of objects, and the number\nof intersection tests required grows rapidly as the number of objects in-\ncreases.\nTo detect intersections between nobjects, the brute-force technique would be\nto test every possible pair of objects, yielding an O(n2)algorithm. However,\nmuch more efficient algorithms are used in practice. Collision engines typ-\nically employ some form of spatial hashing (http://bit.ly/1fLtX1D), spatial\nsubdivision or hierarchical bounding volumes in order to reduce the number\nof intersection tests that must be performed.\n13.3.6.1 Temporal Coherency\nOne common optimization technique is to take advantage of temporal coher-\nency, also known as frame-to-frame coherency . When collidables are moving at\nreasonable speeds, their positions and orientations are usually quite similar\nfrom time step to time step. We can often avoid recalculating certain kinds\nof information every frame by caching the results across multiple time steps.\nFor example, in Havok, collision agents (hkpCollisionAgent) are usually\npersistent between frames, allowing them to reuse calculations from previous\ntime steps as long as the motion of the collidables in question hasn’t invali-\ndated those calculations.\n13.3.6.2 Spatial Partitioning\nThe basic idea of spatial partitioning is to greatly reduce the number of collid-\nables that need to be checked for intersection by dividing space into a number\nof smaller regions. If we can determine (in an inexpensive manner) that a pair\nof collidables do not occupy the same region, then we needn’t perform more-\ndetailed intersection tests on them.\nVarious hierarchical partitioning schemes, such as octrees, binary space\npartitioning trees (BSPs), kd-trees or sphere trees, can be used to subdivide\nspace for the purposes of collision detection optimization. These trees sub-\ndivide space in different ways, but they all do so in a hierarchical fashion,\nstarting with a gross subdivision at the root of the tree and further subdivid-\ningeachregionuntilsufficientlyfine-grainedregionshavebeenobtained. The\ntree can then be walked in order to find and test groups of potentially collid-\ningobjectsforactualintersections. Becausethetreepartitionsspace, weknow\nthat when we traverse down one branch of the tree, the objects in that branch\ncannot be colliding with objects in other sibling branches.\n13.3. The Collision Detection System 847\n13.3.6.3 Broad Phase, Midphase and Narrow Phase\nHavokusesathree-tieredapproachtoprunethesetofcollidablesthatneedto\nbe tested for collisions during each time step.\n• First, gross AABB tests are used to determine which collidables are po-\ntentially intersecting. This is known as broadphase collision detection.\n• Second, the coarse bounding volumes of compound shapes are tested.\nThis is known as midphase collision detection. For example, in a com-\npound shape composed of three spheres, the bounding volume might\nbe a fourth, larger sphere that encloses the other spheres. A compound\nshape may contain other compound shapes, so in general a compound\ncollidable has a bounding volume hierarchy. The midphase traverses\nthis hierarchy in search of subshapes that are potentially intersecting.\n• Finally, the collidables’ individual primitives are tested for intersection.\nThis is known as narrowphase collision detection.\nThe Sweep and Prune Algorithm\nInallofthemajorcollision/physicsengines(e.g., Havok, ODE,PhysX),broad\nphase collision detection employs an algorithm known as sweep and prune\n(http://en.wikipedia.org/wiki/Sweep_and_prune). The basic idea is to sort\nthe minimum and maximum dimensions of the collidables’ AABBs along the\nthree principal axes, and then check for overlapping AABBs by traversing the\nsorted lists. Sweep and prune algorithms can make use of frame-to-frame co-\nherency (see Section 13.3.6.1) to reduce an O(nlogn)sort operation to an ex-\npected O(n)running time. Frame coherency can also aid in the updating of\nAABBs when objects rotate.\n13.3.7 Collision Queries\nAnotherresponsibilityofthecollisiondetectionsystemistoanswerhypotheti-\ncalquestionsaboutthecollisionvolumesinthegameworld. Examplesinclude\nthe following:\n• If a bullet travels from the player’s weapon in a given direction, what is\nthe first target it will hit, if any?\n• Can a vehicle move from point A to point B without striking anything\nalong the way?\n• Find all enemy objects within a given radius of a character.\nIn general, such operations are known as collisionqueries .\n848 13. Collision and Rigid Body Dynamics\nThe most common kind of query is a collision cast, sometimes just called a\ncast. (Theterms traceandprobeareothercommonsynonymsfor“cast.”) Acast\ndetermines what, if anything, a hypothetical object would hit if it were to be\nplaced into the collision world and moved along a ray or line segment. Casts\nare different from regular collision detection operations because the entity be-\ningcastisnotreallyinthecollisionworld—itcannotaffecttheotherobjectsin\nthe world in any way. This is why we say that a collision cast answers hypo-\ntheticalquestions about the collidables in the world.\n13.3.7.1 Ray Casting\nThesimplesttypeofcollisioncastisa raycast, althoughthisnameisactuallya\nbitof amisnomer. What we’rereallycastingis a directedlinesegment—inother\nwords, our casts always have a start point ( p0) and an end point ( p1). The cast\nline segment is tested against the collidable objects in the collision world. If it\nintersects any of them, the contact point or points are returned.\nRay casting systems typically describe the line segment via its start point\np0and a delta vector dthat, when added to p0, yields the end point p1. Any\npoint on this line segment can be found via the following parametric equation,\nwhere the parameter tis permitted to vary between zero and one:\np(t) =p0+td, t2[0, 1].\nClearly, p0=p(0)andp1=p(1). In addition, any contact point along the\nsegment can be uniquely described by specifying the value of the parame-\ntertcorresponding to the contact. Most ray casting APIs return their contact\npoints as “ tvalues,” or they permit a contact point to be converted into its\ncorresponding tby making an additional function call.\nMost collision detection systems are capable of returning the earliest con-\ntact—i.e., the contact point that lies closest to p0and corresponds to the small-\nest value of t. Some systems are also capable of returning a complete list of all\ncollidables that were intersected by the ray or line segment. The information\nreturned for each contact typically includes the tvalue, some kind of unique\nidentifier for the collidable entity that was hit, and possibly other information\nsuch as the surface normal at the point of contact or other relevant proper-\nties of the shape or surface that was struck. One possible contact point data\nstructure is shown below.\nstruct RayCastContact\n{\nF32 m_t; // the t value for this\n// contact\n13.3. The Collision Detection System 849\nU32 m_collidableId ; // which collidable did we\n// hit?\nVector m_normal; // surface normal at\n// contact pt.\n// other information...\n};\nApplications of Ray Casts\nRay casts are used heavily in games. For example, we might want to ask the\ncollision system whether character A has a direct line of sight to character B.\nTo determine this, we simply cast a directed line segment from the eyes of\ncharacterAtothechestofcharacterB.IftherayhitscharacterB,weknowthat\nA can “see” B. But if the ray strikes some other object beforereaching character\nB, we know that the line of sight is being blocked by that object. Ray casts\nare used by weapon systems (e.g., to determine bullet hits), player mechanics\n(e.g.,todeterminewhetherornotthereissolidgroundbeneaththecharacter’s\nfeet), AI systems (e.g., line of sight checks, targeting, movement queries, etc.),\nvehicle systems (e.g., to locate and snap the vehicle’s tires to the terrain) and\nso on.\n13.3.7.2 Shape Casting\nAnothercommonqueryinvolvesaskingthecollisionsystemhowfaranimagi-\nnaryconvexshapewouldbeabletotravelalongadirectedlinesegmentbefore\nit hits something solid. This is known as a sphere cast when the volume being\ncast is a sphere, or a shape cast in general. (Havok calls them linear casts.) As\nwith ray casts, a shape cast is usually described by specifying the start point\np0, the distance to travel dand of course the type, dimensions and orientation\nof the shape we wish to cast.\nThere are two cases to consider when casting a convex shape.\n1. Thecastshapeisalreadyinterpenetratingorcontactingatleastoneother\ncollidable, preventing it from moving away from its starting location.\n2. The cast shape is not intersecting with anything else at its starting loca-\ntion, so it is free to move a nonzero distance along its path.\nIn the first scenario, the collision system typically reports the contact(s)\nbetween the cast shape and all of the collidables with which it is initially in-\nterpenetrating. These contacts might be insidethe cast shape or on its surface,\nas shown in Figure 13.19.\n850 13. Collision and Rigid Body Dynamics\nFigure 13.19. A cast\nsphere that starts in\npenetration will be un-\nable to move, and possi-\nbly many contact points\nwill lie inside the cast\nshape in general.In the second case, the shape can move a nonzero distance along the line\nsegment before striking something. Presuming that it hits something, it will\nusually hit only a single collidable. However, it is possible for a cast shape\nto strike more than one collidable simultaneously if its trajectory is just right.\nAnd of course, if the impacted collidable is a non-convex poly soup, the cast\nshape may end up touching more than one part of the poly soup simultane-\nously. We can safely say that no matter what kind of convex shape is cast, it\nispossible for the cast to generate multiple contact points. The contacts will al-\nwaysbeonthe surfaceofthecastshapeinthiscase,neverinsideit(becausewe\nknowthatthecastshapewasnotinterpenetratinganythingwhenitstartedits\njourney). This case is illustrated in Figure 13.20.\nAs with ray casts, some shape casting APIs report only the earliestcontact\nexperienced by the cast shape, while others allow the shape to continue along\nits hypothetical path, returning all the contacts it experiences on its journey.\nThis is illustrated in Figure 13.21.\nThe contact information returned by a shape cast is necessarily a bit more\ncomplex than it is for a ray cast. We cannot simply return one or more tval-\nues, because a tvalue only describes the location of the center point of the\nshape along its path. It tells us nothing of where, on the surface or interior\nof the shape, it came into contact with the impacted collidable. As a result,\nmost shape casting APIs return both a tvalue and the actual contact point,\nalong with other relevant information (such as which collidable was struck,\nthe surface normal at the contact point, etc.).\nUnlike ray casting APIs, a shape casting system must always be capable of\nreporting multiple contacts. This is because even if we only report the contact\nwith the earliest tvalue, the shape may have touched multiple distinct collid-\nables in the game world, or it may be touching a single non-convex collidable\nat more than one point. As a result, collision systems usually return an array\nContact\nCont actsdd\nFigure 13.20. If the starting location of a cast shape is not interpenetrating anything, then the\nshape will move a nonzero distance along its line segment, and its contacts (if any) will always be\non its surface.\n13.3. The Collision Detection System 851\nContact 1dContact 2Contact 3\nFigure 13.21. A shape casting API might return all contacts instead of only the earliest contact.\nor list of contact point data structures, each of which might look something\nlike this:\nstruct ShapeCastContact\n{\nF32 m_t; // the t value for this\n// contact\nU32 m_collidableId; // which collidable did we\n// hit?\nPoint m_contactPoint ; // location of actual\n// contact\nVector m_normal; // surface normal at\n// contact pt.\n// other information...\n};\nGiven a list of contact points, we often want to distinguish between the\ngroups of contact points for each distinct tvalue. For example, the earliest\ncontact is actually described by the group of contact points that all share the\nminimum tin the list. It’s important to realize that collision systems may or\nmaynotreturntheircontactpointssortedby t. Ifitdoesnot,it’salmostalways\na good idea to sort the results by tmanually. This ensures that if one looks at\nthe first contact point in the list, it will be guaranteed to be among the earliest\ncontact points along the shape’s path.\n852 13. Collision and Rigid Body Dynamics\nApplications of Shape Casts\nShape casts are extremely useful in games. Sphere casts can be used to deter-\nminewhetherthevirtualcameraisincollisionwithobjectsinthegameworld.\nSphereorcapsulecastsarealsocommonlyusedtoimplementcharactermove-\nment. For example, in order to slide the character forward on uneven terrain,\nwe can cast a sphere or capsule that lies between the character’s feet in the di-\nrectionofmotion. Wecanadjustitupordownviaasecondcast,toensurethat\nit remains in contact with the ground. If the sphere hits a very short vertical\nobstruction, such as a street curb, it can “pop up” over the curb. If the vertical\nobstructionistootall,likeawall,thecastspherecanbeslidhorizontallyalong\nthewall. Thefinalrestingplaceofthecastspherebecomesthecharacter’snew\nlocation next frame.\n13.3.7.3 Phantoms\nSometimes, games need to determine which collidable objects lie within some\nspecific volume in the game world. For example, we might want the list of all\nenemies that are within a certain radius of the player character. Havok sup-\nports a special kind of collidable object known as a phantom for this purpose.\nA phantom acts much like a shape cast whose distance vector dis zero.\nAt any moment, we can ask the phantom for a list of its contacts with other\ncollidablesintheworld. Itreturnsthisdatainessentiallythesameformatthat\nwould be returned by a zero-distance shape cast.\nHowever, unlike a shape cast, a phantom is persistent in the collision\nworld. This means that it can take full advantage of the temporal coherency\noptimizations used by the collision engine when detecting collisions between\n“real” collidables. In fact, the only difference between a phantom and a reg-\nular collidable is that it is “invisible” to all other collidables in the collision\nworld (and it does not take part in the dynamics simulation). This allows it to\nanswer hypothetical questions about what objects it would collide with were\nit a “real” collidable, but it is guaranteed not to have any effect on the other\ncollidables—including other phantoms—in the collision world.\n13.3.7.4 Other Types of Queries\nSome collision engines support other kinds of queries in addition to casts. For\nexample,Havoksupports closestpoint queries,whichareusedtofindthesetof\npoints on other collidables that are closest to a given collidable in the collision\nworld.\n13.3.8 Collision Filtering\nIt is quite common for game developers to want to enable or disable collisions\nbetween certain kinds of objects. For example, most objects are permitted to\n13.3. The Collision Detection System 853\npass through the surface of a body of water—we might employ a buoyancy\nsimulation to make them float, or they might just sink to the bottom, but in\neither case we do not want the water’s surface to appear solid. Most collision\nenginesallowcontactsbetweencollidablestobeacceptedorrejectedbasedon\ngame-specific critiera. This is known as collisionfiltering.\n13.3.8.1 Collision Masking and Layers\nOne common filtering approach is to categorize the objects in the world and\nthenusealook-uptabletodeterminewhethercertaincategoriesarepermitted\nto collide with one another or not. For example, in Havok, a collidable can be\na member of one (and only one) collision layer. The default collision filter in\nHavok, represented by an instance of the class hkpGroupFilter, maintains\na 32-bit mask for each layer, each bit of which tells the system whether or not\nthat particular layer can collide with one of the other layers.\n13.3.8.2 Collision Callbacks\nAnother filtering technique is to arrange for the collision library to invoke\nacallback function whenever a collision is detected. The callback can in-\nspect the specifics of the collision and make the decision to either allow\nor reject the collision based on suitable criteria. Havok also supports this\nkind of filtering. When contact points are first added to the world, the\ncontactPointAdded() callback is invoked. If the contact point is later de-\ntermined to be valid (it may not be if an earlier TOI contact was found), the\ncontactPointConfirmed() callback is invoked. The application may re-\nject contact points in these callbacks if desired.\n13.3.8.3 Game-Speciﬁc Collision Materials\nGame developers often need to categorize the collidable objects in the game\nworld,inparttocontrolhowtheycollide(aswithcollisionfiltering)andinpart\ntocontrolothersecondaryeffects,suchasthesoundthatismadeortheparticle\neffect that is generated when one type of object hits another. For example,\nwe might want to differentiate between wood, stone, metal, mud, water and\nhuman flesh.\nTo accomplish this, many games implement a collision shape categoriza-\ntion mechanism similar in many respects to the material system used in the\nrendering engine. In fact, some game teams use the term collision material to\ndescribe this categorization. The basic idea is to associate with each collidable\nsurface a set of properties that defines how that particular surface should be-\nhavefromaphysicalandcollisionstandpoint. Collisionpropertiescaninclude\nsound and particle effects, physical properties like coefficient of restitution or",61911
93-13.4 Rigid Body Dynamics.pdf,93-13.4 Rigid Body Dynamics,"854 13. Collision and Rigid Body Dynamics\nfriction coefficients, collision filtering information and whatever other infor-\nmation the game might require.\nFor simple convex primitives, the collision properties are usually associ-\natedwiththeshapeasawhole. Forpolygonsoupshapes,thepropertiesmight\nbespecifiedonaper-trianglebasis. Becauseofthislatterusage,weusuallytry\nto keep the binding between the collision primitive and its collision material\nas compact as possible. A typical approach is to bind collision primitives to\ncollision materials via an 8-, 16- or 32-bit integer, or a pointer to the material\ndata. This integer indexes into a global array of data structures containing the\ndetailed collision properties themselves.\n13.4 Rigid Body Dynamics\nIn a game engine, we are particularly concerned with the kinematics of obj-\nects—how they move over time. Many game engines include a physics sys-\ntemfor the purposes of simulating the motion of the objects in the virtual\ngame world in a somewhat physically realistic way. Technically speaking,\ngamephysicsenginesaretypicallyconcernedwithaparticularfieldofphysics\nknown as dynamics . This is the study of how forcesaffect the movement of ob-\njects. Until very recently, game physics systems have been focused almost\nexclusively on a specific subdiscipline known as classical rigid body dynamics.\nThis name implies that in a game’s physics simulation, two important simpli-\nfying assumptions are made:\n•Classical (Newtonian) mechanics. The objects in the simulation are as-\nsumed to obey Newton’s laws of motion. The objects are large enough\nthat there are no quantum effects, and their speeds are low enough that\nthere are no relativistic effects.\n•Rigid bodies. All objects in the simulation are perfectly solid and cannot\nbe deformed. In other words, their shape is constant. This idea meshes\nwell with the assumptions made by the collision detection system. Fur-\nthermore, the assumption of rigidity greatly simplifies the mathematics\nrequired to simulate the dynamics of solid objects.\nGame physics engines are also capable of ensuring that the motions of the\nrigid bodies in the game world conform to various constraints. The most com-\nmon constraint is that of non-penetration—in other words, objects aren’t al-\nlowed to pass through one another. Hence the physics system attempts to\nprovide realistic collision responses whenever bodies are found to be interpen-\n13.4. Rigid Body Dynamics 855\netrating.2This is one of the primary reasons for the tight interconnection be-\ntween the physics engine and the collision detection system.\nMost physics systems also allow game developers to set up other kinds of\nconstraints in order to define realistic interactions between physically simu-\nlated rigid bodies. These may include hinges, prismatic joints (sliders), ball\njoints, wheels, “rag dolls” to emulate unconscious or dead characters and so\non.\nThe physics system usually shares the collision world data structure, and\nin fact it usually drives the execution of the collision detection algorithm as\npart of its time step update routine. There is typically a one-to-one mapping\nbetween the rigid bodies in the dynamics simulation and the collidables man-\naged by the collision engine. For example, in Havok, an hkpRigidBody ob-\nject maintains a reference to one and only one hkpCollidable (although it\nis possible to create a collidable that has no rigid body). In PhysX, the two\nconcepts are a bit more tightly integrated—an NxActor serves both as a coll-\nidable object and as a rigid body for the purposes of the dynamics simulation.\nTheserigidbodiesandtheircorrespondingcollidablesareusuallymaintained\nina singletondata structureknown as the collision/physicsworld, orsometimes\njust thephysics world.\nThe rigid bodies in the physics engine are typically distinct from the logi-\ncal objects that make up the virtual world from a gameplay perspective. The\npositions and orientations of game objects can be driven by the physics sim-\nulation. To accomplish this, we query the physics engine every frame for the\ntransform of each rigid body, and apply it in some way to the transform of\nthe corresponding game object. It’s also possible for a game object’s motion,\nas determined by some other engine system (such as the animation system or\nthe character control system) to drive the position and rotation of a rigid body\ninthephysicsworld. AsmentionedinSection13.3.1,asinglelogicalgameob-\nject may be represented by one rigid body in the physics world, or by many.\nA simple object like a rock, weapon or barrel, might correspond to one rigid\nbody. But an articulated character or a complex machine might be composed\nof many interconnected rigid pieces.\nThe remainder of this chapter will be devoted to investigating how game\nphysics engines work. We’ll briefly introduce the theory that underlies rigid\nbodydynamicssimulations. Thenwe’llinvestigatesomeofthemostcommon\nfeatures of a game physics system and have a look at how a physics engine\nmight be integrated into a game.\n2Or in the case of continuous collision detection, the collision response actually prevents the\npenetration from occurring.\n856 13. Collision and Rigid Body Dynamics\n13.4.1 Some Foundations\nA great many excellent books, articles and slide presentations have been writ-\nten on the topic of classical rigid body dynamics. A solid foundation in ana-\nlytical mechanics theory can be obtained from [17]. Even more relevant to our\ndiscussionaretextslike[39],[13]and[29],whichhavebeenwrittenspecifically\naboutthekindofphysicssimulationsdonebygames. Othertexts,like[2],[11]\nand [32], include chapters on rigid body dynamics for games. Chris Hecker\nwrote a series of helpful articles on the topic of game physics for Game Devel-\noperMagazine ;Chrishaspostedtheseandavarietyofotherusefulresourcesat\nhttp://chrishecker.com/Rigid_Body_Dynamics. Aninformativeslidepresen-\ntation on dynamics simulation for games was produced by Russell Smith, the\nprimary author of ODE; it is available at http://www.ode.org/slides/parc/\ndynamics.pdf.\nIn this section, I’ll summarize the fundamental theoretical concepts that\nunderlie the majority of game physics engines. This will be a whirlwind tour\nonly, and by necessity I’ll have to omit some details. Once you’ve read this\nchapter, I strongly encourage you to read at least a few of the additional re-\nsources cited previously.\n13.4.1.1 Units\nMost rigid body dynamics simulations operate in the MKS system of units. In\nthis system, distance is measured in meters (abbreviated “m”), mass is mea-\nsured in kilograms (abbreviated “kg”) and time is measured in seconds (ab-\nbreviated “s”). Hence the name MKS.\nYou could configure your physics system to use other units if you wanted\nto, but if you do this, you need to make sure everything in the simulation\nis consistent. For example, constants like the acceleration due to gravity g,\nwhichismeasuredinm /s2intheMKSsystem,wouldhavetobere-expressed\nin whatever unit system you select. Most game teams just stick with MKS to\nkeep life simple.\n13.4.1.2 Separability of Linear and Angular Dynamics\nAnunconstrained rigid body is one that can translate freely along all three\nCartesian axes and that can rotate freely about these three axes as well. We\nsay that such a body has six degreesof freedom (DOF).\nIt is perhaps somewhat surprising that the motion of an unconstrained\nrigid body can be separated into two independent components:\n•Linear dynamics . This is a description of the motion of the body when\nwe ignore all rotational effects. (We can use linear dynamics alone to\n13.4. Rigid Body Dynamics 857\ndescribe the motion of an idealized point mass —i.e., a mass that is in-\nfinitesimally small and cannot rotate.)\n•Angular dynamics. This is a description of the rotational motion of the\nbody.\nAs you can well imagine, this ability to separate the linear and angular\ncomponents of a rigid body’s motion is extremely helpful when analyzing or\nsimulating its behavior. It means that we can calculate a body’s linear motion\nwithout regard to rotation—as if it were an idealized point mass—and then\nlayer its angular motion on top in order to arrive at a complete description of\nthe body’s motion.\n13.4.1.3 Center of Mass\nFor the purposes of lineardynamics, an unconstrained rigid body acts as\nthough all of its mass were concentrated at a single point known as the center\nofmass(abbreviatedCM,orsometimesCOM).Thecenterofmassisessentially\nthe balancing point of the body for all possible orientations. In other words,\nthe mass of a rigid body is distributed evenly around its center of mass in all\ndirections.\nForabodywithuniformdensity,thecenterofmassliesatthe centroid ofthe\nbody. Thatis,ifweweretodividethebodyupinto Nverysmallpieces,addup\nthepositionsofallthesepiecesasavectorsumandthendividebythenumber\nofpieces, we’dendupwithaprettygoodapproximationtothelocationofthe\ncenter of mass. If the body’s density is not uniform, the position of each little\npiece would need to be weighted by that piece’s mass, meaning that in general\nthe center of mass is really a weighted average of the pieces’ positions. So we\nhave\nrCM=å\n8imiri\nå\n8imi=å\n8imiri\nm,\nwhere the symbol mrepresents the total mass of the body, and the symbol r\nrepresents a radius vector orposition vector—i.e., a vector extending from the\nworld-space origin to the point in question. (These sums become integrals in\nthe limit as the sizes and masses of the little pieces approach zero.)\nThe center of mass always lies inside a convex body, although it may actu-\nallylieoutsidethebodyifitisconcave. (Forexample, wherewouldthecenter\nof mass of the letter “C” lie?)\n858 13. Collision and Rigid Body Dynamics\n13.4.2 Linear Dynamics\nFor the purposes of linear dynamics, the position of a rigid body can be fully\ndescribed by a position vector rCMthat extends from the world-space origin\nto the center of mass of the body, as shown in Figure 13.22. Since we’re using\nthe MKS system, position is measured in meters (m). For the remainder of\nthis discussion, we’ll drop the CM subscripts, as it is understood that we are\ndescribing the motion of the body’s center of mass.\ny\nxCM\nFigure 13.22. For the purposes of linear dynamics, the position of a rigid body can be fully described\nby the position of its center of mass.\n13.4.2.1 Linear Velocity and Acceleration\nThelinearvelocity of a rigid body defines the speed and direction in which the\nbody’s CM is moving. It is a vector quantity, typically measured in meters per\nsecond (m/s). Velocity is the first time derivative of position, so we can write\nv(t) =dr(t)\ndt=˙r(t),\nwhere the dot over the vector rdenotes taking the derivative with respect to\ntime. Differentiating a vector is the same as differentiating each component\nindependently, so\nvx(t) =drx(t)\ndt=˙rx(t),\nand so on for the y- and z-components.\nLinear acceleration is the first derivative of linear velocity with respect to\ntime, or the second derivative of the position of a body’s CM versus time. Ac-\nceleration is a vector quantity, usually denoted by the symbol a. So we can\n13.4. Rigid Body Dynamics 859\nwrite\na(t) =dv(t)\ndt=˙v(t)\n=d2r(t)\ndt2=¨r(t).\n13.4.2.2 Force and Momentum\nAforceis defined as anything that causes an object with mass to accelerate or\ndecelerate. Aforcehasbothamagnitudeandadirectioninspace, soallforces\narerepresentedby vectors. Aforceis oftendenoted by thesymbol F. When N\nforces are applied to a rigid body, their net effect on the body’s linear motion\nis found by simply adding up the force vectors:\nFnet=N\nå\ni=1Fi.\nNewton’s famous Second Law states that force is proportional to accelera-\ntion and mass:\nF(t) =ma(t) =m¨r(t). (13.2)\nAs Newton’s law implies, force is measured in units of kilogram-meters per\nsecond squared (kg-m /s2). This unit is also called the Newton.\nWhenwemultiplyabody’slinearvelocitybyitsmass,theresultisaquan-\ntity known as linear momentum . It is customary to denote linear momentum\nwith the symbol p:\np(t) =mv(t).\nWhen mass is constant, Equation (13.2) holds true. But if mass is not con-\nstant, as would be the case for a rocket whose fuel is being gradually used up\nand converted into energy, Equation (13.2) is not exactly correct. The proper\nformulation is actually as follows:\nF(t) =dp(t)\ndt=d(m(t)v(t))\ndt.\nwhichofcoursereducestothemorefamiliar F=mawhenthemassisconstant\nand can be brought outside the derivative. Linear momentum is not of much\nconcerntous. However,theconceptofmomentumwillbecomerelevantwhen\nwe discuss angular dynamics.\n860 13. Collision and Rigid Body Dynamics\n13.4.3 Solving the Equations of Motion\nThe central problem in rigid body dynamics is to solve for the motion of the\nbody,givenasetofknownforcesactingonit. Forlineardynamics, thismeans\nfinding v(t)andr(t)given knowledge of the net force Fnet(t)and possibly\nother information, such as the position and velocity at some previous time.\nAs we’ll see below, this amounts to solving a pair of ordinary differential\nequations—one to find v(t)given a(t)and the other to find r(t)given v(t).\n13.4.3.1 Force as a Function\nAforcecanbeconstant,oritcanbeafunctionoftimeasshownabove. Aforce\ncan also be a function of the position of the body, its velocity, or any number\nof other quantities. So in general, the expression for force should really be\nwritten as follows:\nF(t,r(t),v(t), . . . )=ma(t). (13.3)\nThis can be rewritten in terms of the position vector and its first and second\nderivatives as follows:\nF(t,r(t),˙r(t), . . . )=m¨r(t).\nFor example, the force exerted by a spring is proportional to how far it has\nbeen stretched away from its natural resting position. In one dimension, with\nthe spring’s resting position at x=0, we can write\nF(t,x(t))= kx(t),\nwhere kis thespringconstant, a measure of the spring’s stiffness.\nAs another example, the damping force exerted by a mechanical viscous\ndamper (a so-called dashpot) is proportional to the velocity of the damper’s\npiston. So in one dimension, we can write\nF(t,v(t))= bv(t),\nwhere bis aviscousdamping coefficient .\n13.4.3.2 Ordinary Differential Equations\nIn general, an ordinary differential equation (ODE) is an equation involving a\nfunctionofoneindependentvariableandvariousderivativesofthatfunction.\nIf our independent variable is time and our function is x(t), then an ODE is a\nrelation of the form\ndnx\ndtn=f(\nt,x(t),dx(t)\ndt,d2x(t)\ndt2, . . . ,dn 1x(t)\ndtn 1)\n.\n13.4. Rigid Body Dynamics 861\nPut another way, the nth derivative of x(t)is expressed as a function f\nwhose arguments can be time (t), position (x(t)), and any number of deriva-\ntives of x(t)as long as those derivatives are of lower order than n.\nAs we saw in Equation (13.3), force is a function of time, position and ve-\nlocity in general:\n¨r(t) =1\nmF(t,r(t),˙r(t)).\nThis clearly qualifies as an ODE. We wish to solve this ODE in order to find\nv(t)andr(t).\n13.4.3.3 Analytical Solutions\nIn some rare situations, the differential equations of motion can be solved an-\nalytically, meaning that a simple, closed-form function can be found that de-\nscribes the body’s position for allpossiblevalues of time t. A common example\nis the vertical motion of a projectile under the influence of a constant acceler-\nation due to gravity, a(t) = [ 0,g, 0], where g= 9.8m /s2. In this case, the\nODE of motion boils down to\n¨y(t) =g.\nIntegrating once yields\n˙y(t) =gt+v0,\nwhere v0is the vertical velocity at time t=0. Integrating a second time yields\nthe familiar solution\ny(t)=1\n2gt2+v0t+y0,\nwhere y0is the initial vertical position of the object.\nHowever, analytical solutions are almost never possible in game physics.\nThis is due in part to the fact that closed-form solutions to some differential\nequations are simply not known. Moreover, a game is an interactive simula-\ntion, so we usually cannot predict how the forces in a game will behave over\ntime. This makes it impossible to find simple, closed-form expressions for the\npositions and velocities of the objects in the game as functions of time.\nThereareofcourseexceptionstothisruleofthumb. Forexample,it’spretty\ncommontosolveforaclosed-formexpressioninordertodeterminewithwhat\nvelocity a projectile must be launched in order to hit a predefined target.\n13.4.4 Numerical Integration\nFor the reasons cited above, game physics engines turn to a technique known\nasnumerical integration . With this technique, we solve our differential equa-\ntions in a time-stepped manner—using the solution from a previous time step\n862 13. Collision and Rigid Body Dynamics\nto arrive at the solution for the next time step. The duration of the time step is\nusuallytakentobe(roughly)constantandisdenotedbythesymbol ∆t. Given\nthat we know the body’s position and velocity at the current time t1and that\nthe force is known as a function of time, position and/or velocity, we wish\nto find the position and velocity at the next time step t2=t1+∆t. In other\nwords, given r(t1),v(t1)andF(t,r,v), the problem is to find r(t2)andv(t2).\n13.4.4.1 Explicit Euler\nOneofthesimplestnumericalsolutionstoanODEisknownasthe explicitEu-\nler method. This is the intuitive approach often taken by new game program-\nmers. Let’s assume for the moment that we already know the current velocity\nand that we wish to solve the following ODE to find the body’s position on\nthe next frame:\nv(t) = ˙r(t). (13.4)\nUsing the explicit Euler method, we simply convert the velocity from meters\nper second into meters per frame by multiplying by the time delta, and then\nwe add “one frame’s worth” of velocity onto the current position in order to\nfindthenewpositiononthenextframe. Thisyieldsthefollowingapproximate\nsolution to the ODE given by Equation (13.4):\nr(t2) =r(t1) +v(t1)∆t. (13.5)\nWe can take an analogous approach to find the body’s velocity next frame\ngiven the net force acting this frame. Hence, the approximate explicit Euler\nsolution to the ODE\na(t) =Fnet(t)\nm=˙v(t)\nis as follows:\nv(t2) =v(t1) +Fnet(t)\nm∆t. (13.6)\nInterpretations of Explicit Euler\nWhat we’re really doing in Equation (13.5) is assuming that the velocity of the\nbody is constant during the time step. Therefore, we can use the currentve-\nlocity to predict the body’s position on the nextframe. The change in position\n∆rbetween times t1andt2is hence ∆r=v(t1)∆t. Graphically, if we imagine\na plot of the position of the body versus time, we are taking the slopeof the\nfunctionattime t1(whichisjust v(t1))andextrapolatingitlinearlytothenext\ntime step t2. As we can see in Figure 13.23, linear extrapolation does not nec-\nessarilyprovideuswithaparticularlygoodestimateofthetruepositionatthe\n13.4. Rigid Body Dynamics 863\nnext time step r(t2), but it does work reasonably well as long as the velocity is\nroughly constant.\nFigure13.23suggestsanother wayto interpretthe explicit Eulermethod—\nas an approximation of a derivative. By definition, any derivative is the quo-\ntient of two infinitesimally small differences (in our case, dr/dt). The explicit\nEuler method approximates this using the quotient of two finite differences . In\nother words, drbecomes ∆randdtbecomes ∆t. This yields\ndr\ndt∆r\n∆t;\nv(t1)r(t2) r(t1)\nt2 t1,\nwhich again simplifies to Equation (13.5). This approximation is really only\nvalidwhenthevelocityisconstantoverthetimestep. Itisalsovalidinthelimit\nas∆ttends toward zero (at which point it becomes exactlyright). Obviously,\nthis same analysis can be applied to Equation (13.6) as well.\n13.4.4.2 Properties of Numerical Methods\nWe’veimpliedthattheexplicitEulermethodisnotparticularlyaccurate. Let’s\npin this idea down more concretely. A numerical solution to an ordinary dif-\nferential equation actually has three important and interrelated properties:\n•Convergence . As the time step ∆ttends toward zero, does the approxi-\nmate solution get closer and closer to the real solution?\n•Order. Given a particular numerical approximation to the solution of\nan ODE, how “bad” is the error? Errors in numerical ODE solutions\nare typically proportional to some power of the time step duration ∆t,\nso they are often written using “big O” notation (e.g., O(∆t2)). We say\nΔr\nΔt\ntr(t1)rapprox(t2)r(t2)r(t)\nt1 t2= v(t1)Δr\nΔt\nFigure 13.23. In the explicit Euler method, the slope of r(t)at time t1 is used to linearly extrapolate\nfrom r(t1)to an estimate of the true value of r(t2).\n864 13. Collision and Rigid Body Dynamics\nthat a particular numerical method is of “order n” when its error term is\nO(∆t(n+1)).\n•Stability. Does the numerical solution tend to “settle down” over time?\nIfanumerical methodaddsenergyintothe system, objectvelocitieswill\neventually“explode,”andthesystemwillbecome unstable . Ontheother\nhand, if a numerical method tends to remove energy from the system, it\nwill have an overall damping effect, and the system will be stable.\nThe concept of order warrants a little more explanation. We usually mea-\nsure the error of a numerical method by comparing its approximate equation\nwith the infinite Taylor series expansion of the exact solution to the ODE. We\nthen cancel terms by subtracting the two equations. The remaining Taylor\nterms represent the error inherent in the method. For example, the explicit\nEuler equation is\nr(t2) =r(t1) +˙r(t1)∆t.\nThe infinite Taylor series expansion of the exact solution is\nr(t2) =r(t1) +˙r(t1)∆t+1\n2¨r(t1)∆t2+1\n6r(3)(t1)∆t3+. . . ,\nwhere r(3)represents the third derivative with respect to time. Therefore, the\nerror is represented by all of the terms after the v∆tterm, which is of order\nO(∆t2)(because this term dwarfs the other higher-order terms):\nE=1\n2¨r(t1)∆t2+1\n6r(3)(t1)∆t3+. . .\n=O(\n∆t2)\n.\nTo make the error of a method explicit, we’ll often write its equation with\nthe error term added in “big O” notation at the end. For example, the explicit\nEuler method’s equation is most accurately written as follows:\nr(t2) =r(t1) +˙r(t1)∆t+O(\n∆t2)\n.\nWe say that the explicit Euler method is a “first-order” method because it is\naccurate up to and including the Taylor series term involving ∆tto the first\npower. In general, if a method’s error term is O(∆t(n+1)), then it is said to be\nan “order n” method.\n13.4.4.3 Alternatives to Explicit Euler\nTheexplicitEulermethodseesquitealotofuseforsimpleintegrationtasksin\ngames, producing the best results when the velocity is nearly constant. How-\never, it is not used in general-purpose dynamics simulations because of its\n13.4. Rigid Body Dynamics 865\nhigh error and poor stability. There are all sorts of other numerical meth-\nodsforsolvingODEs,includingbackwardEuler(anotherfirst-ordermethod),\nmidpoint Euler (a second-order method) and the family of Runge-Kutta\nmethods. (The fourth-order Runge-Kutta, often abbreviated “RK4,” is par-\nticularly popular.) We won’t describe these in any detail here, as you can\nfind voluminous amounts of information about them online and in the litera-\nture. TheWikipediapagehttp://en.wikipedia.org/wiki/Numerical_ordinary\n_differential_equations serves as an excellent jumping-off point for learning\nthese methods.\n13.4.4.4 Verlet Integration\nThenumericalODEmethodmostoftenusedininteractivegamesthesedaysis\nprobablythe Verletmethod, so I’ll take a moment to describe it in some detail.\nThereareactuallytwovariantsofthismethod: regularVerletandtheso-called\nvelocityVerlet. I’llpresentbothmethodshere,butI’llleavethetheoryanddeep\nexplanations to the myriad papers and Web pages available on the topic. (For\na start, check out http://en.wikipedia.org/wiki/Verlet_integration.)\nThe regular Verlet method is attractive because it achieves a high order\n(low error), is relatively simple and inexpensive to evaluate, and produces a\nsolution for position directly in terms of acceleration in one step (as opposed\ntothetwostepsnormallyrequiredtogofromaccelerationtovelocityandthen\nfromvelocitytoposition). TheformulaisderivedbyaddingtwoTaylorseries\nexpansions, one going forward in time and one going backward in time:\nr(t1+∆t) =r(t1) +˙r(t1)∆t+1\n2¨r(t1)∆t2+1\n6r(3)(t1)∆t3+O(∆t4);\nr(t1 ∆t) =r(t1) ˙r(t1)∆t+1\n2¨r(t1)∆t2 1\n6r(3)(t1)∆t3+O(∆t4).\nAdding these expressions causes the negative terms to cancel with the corre-\nsponding positive ones. The result gives us the position at the next time step\nin terms of the acceleration and the two (known) positions at the current and\nprevious time steps. This is the regular Verlet method:\nr(t1+∆t) = 2r(t1) r(t1 ∆t) +a(t1)∆t2+O(∆t4).\nIn terms of net force, the Verlet method becomes\nr(t1+∆t) = 2r(t1) r(t1 ∆t) +Fnet(t1)\nm∆t2+O(∆t4).\nThe velocity is conspicuously absent from this expression. However, it\ncanbefoundusingthefollowingsomewhatinaccurateapproximation(among\nother alternatives):\nv(t1+∆t) =r(t1+∆t) r(t1)\n∆t+O(∆t).\n866 13. Collision and Rigid Body Dynamics\n13.4.4.5 Velocity Verlet\nThemorecommonlyused velocityVerlet methodisafour-stepprocessinwhich\nthe time step is divided into two parts to facilitate the solution. Given that\na(t1) =1\nmF(\nt1,r(t1),v(t1))\nis known, we do the following:\n1. Calculate r(t1+∆t) =r(t1) +v(t1)∆t+1\n2a(t1)∆t2.\n2. Calculate v(t1+1\n2∆t) =v(t1) +1\n2a(t1)∆t.\n3. Determine a(t1+∆t) =a(t2) =1\nmF(\nt2,r(t2),v(t2))\n.\n4. Calculate v(t1+∆t) =v(t1+1\n2∆t) +1\n2a(t1+∆t)∆t.\nNoticeinthethirdstepthattheforcefunctiondependsonthepositionand\nvelocity on the nexttime step, r(t2)andv(t2). We already calculated r(t2)\nin step 1, so we have all the information we need as long as the force is not\nvelocity-dependent. If it is velocity-dependent, then we must approximate\nthe next frame’s velocity, perhaps using the explicit Euler method.\n13.4.5 Angular Dynamics in Two Dimensions\nUp until now, we’ve focused on analyzing the linear motion of a body’s cen-\nter of mass (which acts as if it were a point mass). As I said earlier, an uncon-\nstrainedrigidbodywillrotateaboutitscenterofmass. Thismeansthatwecan\nlayer the angular motion of a body on top of the linear motion of its center of\nmass in order to arrive at a complete description of the body’s overall motion.\nThe study of a body’s rotational motion in response to applied forces is called\nangular dynamics.\nIn two dimensions, angular dynamics works almost identically to linear\ndynamics. For each linear quantity, there’s an angular analog, and the math-\nematics works out quite neatly. So let’s investigate two-dimensional angular\ndynamics first. As we’ll see, when we extend the discussion into three di-\nmensions, things get a bit messier, but we’ll burn that bridge when we get to\nit!\n13.4.5.1 Orientation and Angular Speed\nIn two dimensions, every rigid body can be treated as a thin sheet of material.\n(Some physics texts refer to such a body as a plane lamina.) All linear mo-\ntion occurs in the xy-plane, and all rotations occur about the z-axis. (Visualize\nwooden puzzle pieces sliding about on an air hockey table.)\nThe orientation of a rigid body in 2D is fully described by an angle q, mea-\nsured in radians relative to some agreed-upon zero rotation. For example, we\n13.4. Rigid Body Dynamics 867\nmight specify that q=0when a race car is facing directly down the positive\nx-axis in world space. This angle is of course a time-varying function, so we\ndenote it q(t).\n13.4.5.2 Angular Speed and Acceleration\nAngular velocity measures the rate at which a body’s rotation angle changes\novertime. Intwodimensions,angularvelocityisascalar,morecorrectlycalled\nangularspeed, since the term “velocity” really only applies to vectors. It is de-\nnotedbythe scalarfunction w(t)andmeasuredinradianspersecond(rad/s).\nAngular speed is the derivative of the orientation angle q(t)with respect to\ntime:\nAngular: Linear:\nw(t) =dq(t)\ndt=˙q(t)v(t) =dr(t)\ndt=˙r(t).\nAnd as we’d expect, angular acceleration, denoted a(t)and measured in\nradians per second squared (rad /s2), is the rate of change of angular speed:\nAngular: Linear:\na(t) =dw(t)\ndt=˙w(t) = ¨q(t)a(t) =dv(t)\ndt=˙v(t) = ¨r(t).\n13.4.5.3 Moment of Inertia\nThe rotational equivalent of mass is a quantity known as the momentofinertia.\nJust as mass describes how easy or difficult it is to change the linear velocity\nof a point mass, the moment of inertia measures how easy or difficult it is to\nchange the angular speed of a rigid body about a particular axis. If a body’s\nmassisconcentratednearanaxisofrotation,itwillberelativelyeasiertorotate\naboutthataxis,anditwillhencehaveasmallermomentofinertiathanabody\nwhose mass is spread out away from that axis.\nSince we’re focusing on two-dimensional angular dynamics right now, the\naxis of rotation is always z, and a body’s moment of inertia is a simple scalar\nvalue. Momentofinertiaisusuallydenotedbythesymbol I. Wewon’tgetinto\nthedetailsofhowtocalculatethemomentofinertiahere. Forafullderivation,\nsee [17].\n13.4.5.4 Torque\nUntil now, we’ve assumed that all forces are applied to the center of mass of a\nrigid body. However, in general, forces can be applied at arbitrary points on a\n868 13. Collision and Rigid Body Dynamics\nbody. If the line of action of a force passes through the body’s center of mass,\nthen the force will produce linear motion only, as we’ve already seen. Other-\nwise, the force will introduce a rotational force known as a torquein addition\nto the linear motion it normally causes. This is illustrated in Figure 13.24.\nWecancalculatetorqueusingacrossproduct. First,weexpressthelocation\nat which the force is applied as a vector rextending from the body’s center of\nmass to the point of application of the force. (In other words, the vector ris in\nbodyspace, where the origin of body space is defined to be the center of mass.)\nThis is illustrated in Figure 13.25. The torque Ncaused by a force Fapplied at\na location ris\nN=rF. (13.7)\nEquation (13.7) implies that torque increases as the force is applied farther\nfromthecenterofmass. Thisexplainswhyalevercanhelpustomoveaheavy\nobject. Italsoexplainswhyaforceapplieddirectlythroughthecenterofmass\nproduces no torque and no rotation—the magnitude of the vector ris zero in\nthis case.\nWhen two or more forces are applied to a rigid body, the torque vectors\nproducedbyeachonecanbesummed,justaswecansumforces. Soingeneral\nwe are interested in the net torque, Nnet.\nIn two dimensions, the vectors randFmust both lie in the xy-plane, so N\nwillalwaysbedirectedalongthepositiveornegative z-axis. Assuch,we’llde-\nnoteatwo-dimensionaltorqueviathescalar Nz,whichisjustthe z-component\nof the vector N.\nTorque is related to angular acceleration and moment of inertia in much\nF1F2\nFigure 13.24. On the left, a force applied to a body’s CM produces purely linear motion. On the\nright, a force applied off-center will give rise to a torque, producing rotational motion as well as\nlinear motion.\n13.4. Rigid Body Dynamics 869\nFigure 13.25. Torque is calculated by taking the cross product between a force’s point of application\nin body space (i.e., relative to the center of mass) and the force vector. The vectors are shown\nhere in two dimensions for ease of illustration; if it could be drawn, the torque vector would be\ndirected into the page.\nthe same way that force is related to linear acceleration and mass:\nAngular: Linear:\nNz(t) =Ia(t) =I˙w(t) =I¨q(t)F(t) =ma(t) =m˙v(t) =m¨r(t).(13.8)\n13.4.5.5 Solving the Angular Equations of Motion in Two Dimensions\nFor the two-dimensional case, we can solve the angular equations of motion\nusingexactlythesamenumericalintegrationtechniquesweappliedtothelin-\near dynamics problem. The pair of ODEs that we wish to solve is as follows:\nAngular: Linear:\nNnet(t) =I˙w(t)Fnet(t) =m˙v(t)\nw(t) = ˙q(t) v(t) = ˙r(t),\nand their approximate explicit Euler solutions are\nAngular: Linear:\nw(t2) =w(t1) +I 1Nnet(t1)∆tv(t2) =v(t1) +m 1Fnet(t1)∆t\nq(t2) =q(t1) +w(t1)∆t r(t2) =r(t1) +v(t1)∆t.\nOfcourse,wecouldapplyanyoftheothermore-accuratenumericalmeth-\nods as well, such as the velocity Verlet method (I’ve omitted the linear case\nhere for compactness, but compare this to the steps given in Section 13.4.4.5):\n1. Calculate q(t1+∆t) =q(t1) +w(t1)∆t+1\n2a(t1)∆t2.\n2. Calculate w(t1+1\n2∆t) =w(t1) +1\n2a(t1)∆t.\n870 13. Collision and Rigid Body Dynamics\n3. Calculate a(t1+∆t) =a(t2) =I 1Nnet(\nt2,q(t2),w(t2))\n.\n4. Calculate w(t1+∆t) =w(t1+1\n2∆t) +1\n2a(t1+∆t)∆t.\n13.4.6 Angular Dynamics in Three Dimensions\nAngular dynamics in three dimensions is a somewhat more complex topic\nthan its two-dimensional counterpart, although the basic concepts are of\ncourse very similar. In the following section, I’ll give a very brief overview\nof how angular dynamics works in 3D, focusing primarily on the things that\nare typically confusing to someone who is new to the topic. For further in-\nformation, check out Glenn Fiedler’s series of articles on the topic, available\nat http://gafferongames.com/game-physics/physics-in-3d/. Another help-\nfulresourceisthepaperentitled“AnIntroductiontoPhysicallyBasedModel-\ning” by David Baraff of the Robotics Institute at Carnegie Mellon University,\navailable at http://www-2.cs.cmu.edu/~baraff/sigcourse/notesd1.pdf.\n13.4.6.1 The Inertia Tensor\nArigidbodymayhaveaverydifferentdistributionofmassaboutthethreeco-\nordinateaxes. Assuch, weshouldexpectabodytohavedifferentmomentsof\ninertia about different axes. For example, a long thin rod should be relatively\neasy to make rotate about its long axis because all the mass is concentrated\nvery close to the axis of rotation. Likewise, the rod should be relatively more\ndifficult to make rotate about its short axis because its mass is spread out far-\nther from the axis. This is indeed the case, and it is why a figure skater spins\nfaster when she tucks her limbs in close to her body.\nInthreedimensions, therotationalmassofarigidbodyisrepresentedbya\n33matrixknownasits inertiatensor. Itisusuallyrepresentedbythesymbol\nI(asbefore,wewon’tdescribehowtocalculatetheinertiatensorhere;see[17]\nfor details):\nI=2\n4IxxIxyIxz\nIyxIyyIyz\nIzxIzyIzz3\n5.\nThe elements lying along the diagonal of this matrix are the moments of\ninertia of the body about its three principal axes, Ixx,Iyyand Izz. The off-\ndiagonal elements are called products of inertia . They are zero when the body\nis symmetrical about all three principal axes (as would be the case for a rect-\nangular box). When they are nonzero, they tend to produce physically real-\nistic yet somewhat unintuitive motions that the average game player would\nprobably think were “wrong” anyway. Therefore, the inertia tensor is often\n13.4. Rigid Body Dynamics 871\nsimplified down to the three-element vector[IxxIyyIzz]\nin game physics\nengines.\n13.4.6.2 Orientation in Three Dimensions\nIn two dimensions, we know that the orientation of a rigid body can be de-\nscribed by a single angle q, which measures rotation about the z-axis (assum-\ning the motion is taking place in the xy-plane). In three dimensions, a body’s\norientation could be represented using three Euler angles[qxqyqz]\n, each\nrepresenting the body’s rotation about one of the three Cartesian axes. How-\never, as we saw in Chapter 5, Euler angles suffer from gimbal lock problems\nand can be difficult to work with mathematically. Therefore, the orientation\nof a body is more often represented using either a 33matrix Ror a unit\nquaternion q. We’ll use the quaternion form exclusively in this chapter.\nRecall that a quaternion is a four-element vector whose x-,y- and z-com-\nponents can be interpreted as a unit vector ulying along the axis of rotation,\nscaled by the sine of the half-angle and whose wcomponent is the cosine of\nthe half-angle:\nq=[qxqyqzqw]\n=[qqw]\n=[\nusinq\n2cosq\n2]\n.\nA body’s orientation is of course a function of time, so we should write it q(t).\nAgain, we need to select an arbitrary direction to be our zero rotation. For\nexample, we might say that by default, the front of every object will lie along\nthepositive z-axisinworldspace,with yupand xtotheleft. Anynon-identity\nquaternionwillservetorotatetheobjectawayfromthiscanonicalworld-space\norientation. The choice of the canonical orientation is arbitrary, but of course\nit’s important to be consistent across all assets in the game.\n13.4.6.3 Angular Velocity and Momentum in Three Dimensions\nIn three dimensions, angular velocity is a vector quantity, denoted by !(t).\nThe angular velocity vector can be visualized as a unit-length vector uthat\ndefines the axis of rotation, scaled by the two-dimensional angular velocity\nwu=˙quof the body about the u-axis. Hence,\n!(t) =wu(t)u=˙qu(t)u.\nIn linear dynamics, we saw that if there are no forces acting on a body,\nthen the linear acceleration is zero, and linear velocity is constant. In two-\ndimensional angular dynamics, this again holds true: If there are no torques\n872 13. Collision and Rigid Body Dynamics\nFigure 13.26. A rectangular object that is spun about its shortest or longest axis has a constant angular velocity vector.\nHowever, when spun about its medium-sized axis, the direction of the angular velocity vector changes wildly.\nacting on a body in two dimensions, then the angular acceleration ais zero,\nand the angular speed wabout the z-axis is constant.\nUnfortunately,thisis notthecaseinthreedimensions. Itturnsoutthateven\nwhen a rigid body is rotating in the absence of all forces, its angular velocity\nvector !(t)may not be constant because the axis of rotation can continually\nchange direction. You can see this effect in action when you try to spin a rect-\nangular object, like a block of wood, in mid-air in front of you. If you throw\ntheblocksothatitisrotatingaboutitsshortestaxis,itwillspininastableway.\nThe orientation of the axis stays roughly constant. The same thing happens if\nyou try to spin the block about its longest axis. But if you try to spin the block\naround the remaining axis (the one that’s neither the shortest nor the longest),\nthe rotation will be utterly unstable. (Try it! Go steal a wooden block from\na baby and spin it in various ways. On second thought, make sure to give it\nbackwhenyou’redone.) Theaxisofrotationitselfchangesdirectionwildlyas\nthe object spins. This is illustrated in Figure 13.26.\nThe fact that the angular velocity vector can change in the absence of\ntorques is another way of saying that angular velocity is not conserved. How-\never, a related quantity called the angular momentum does remain constant in\nthe absence of forces and hence isconserved. Angular momentum is the rota-\ntional equivalent of linear momentum:\nAngular: Linear:\nL(t) =I!(t)p(t) =mv(t).\nLike the linear case, angular momentum L(t)is a three-element vector.\nHowever, unlike the linear case, rotational mass (the inertia tensor) is not a\nscalar but rather a 33matrix. As such, the expression I!is computed via a\n13.4. Rigid Body Dynamics 873\nmatrix multiplication:\n2\n4Lx(t)\nLy(t)\nLz(t)3\n5=2\n4IxxIxyIxz\nIyxIyyIyz\nIzxIzyIzz3\n52\n4wx(t)\nwy(t)\nwz(t)3\n5.\nBecausetheangularvelocity !isnotconserved,wedonottreatitasapri-\nmary quantity in our dynamics simulations the way we do the linear velocity\nv. Instead, we treat angular momentum Las the primary quantity. The angu-\nlarvelocityisasecondaryquantity,determinedonlyafterwehavedetermined\nthe value of Lat each time step of the simulation.\n13.4.6.4 Torque in Three Dimensions\nIn three dimensions, we still calculate torque as the cross product between the\nradialpositionvectorofthepointofforceapplicationandtheforcevectoritself\n(N=rF). Equation (13.8) still holds, but we always write it in terms of the\nangular momentum because angular velocity is not a conserved quantity:\nN=I(t)\n=Id!(t)\ndt\n=d\ndt(\nI!(t))\n=dL(t)\ndt.\n13.4.6.5 Solving the Equations of Angular Motion in Three Dimensions\nWhen solving the equations of angular motion in three dimensions, we might\nbe tempted to take exactly the same approach we used for linear motion and\ntwo-dimensional angular motion. We might guess that the differential equa-\ntions of motion should be written\nAngular 3D? Linear:\nNnet(t) =I˙!(t)Fnet(t) =m˙v(t)\n!(t) = ˙(t) v(t) = ˙r(t),\nand using the explicit Euler method, we might guess that the approximate\nsolutions to these ODEs would look something like this:\nAngular 3D? Linear:\n!(t2) =!(t1) +I 1Nnet(t1)∆tv(t2) =v(t1) +m 1Fnet(t)∆t\n(t2) =(t1) +!(t1)∆t r(t2) =r(t1) +v(t1)∆t.\n874 13. Collision and Rigid Body Dynamics\nHowever,thisis notactuallycorrect. Thedifferentialequationsofthree-dimen-\nsional angular motion differ from their linear and two-dimensional angular\ncounterparts in two important ways:\n1. Instead of solving for the angular velocity !, we solve for the angular\nmomentum Ldirectly. Wethencalculatetheangularvelocityvectorasa\nsecondary quantity using IandL. We do this because angular momen-\ntum is conserved, while angular velocity is not.\n2. When solving for the orientation given the angular velocity, we have a\nproblem: The angular velocity is a three-element vector, while the orien-\ntation is a four-element quaternion . How can we write an ODE relating\na quaternion to a vector? The answer is that we cannot, at least not di-\nrectly. But what we can do is convert the angular velocity vector into\nquaternion form and then apply a slightly odd-looking equation that re-\nlates the orientation quaternion to the angular velocity quaternion.\nItturnsoutthatwhenweexpressarigidbody’sorientationasaquaternion,\nthederivativeofthisquaternionisrelatedtothebody’sangularvelocityvector\nin the following way. First, we construct an angular velocity quaternion. This\nquaternion contains the three components of the angular velocity vector in x,\nyandz, with its w-component set to zero:\n!=[\nwxwywz0]\n.\nNow the differential equation relating the orientation quaternion to the angu-\nlar velocity quaternion is (for reasons we won’t get into here) as follows:\nd!(t)\ndt=˙q(t) =1\n2!(t)q(t).\nIt’s important to remember here that !(t)is the angular velocity quaternion\nas described above and that the product !(t)q(t)is aquaternion product (see\nSection 5.4.2.1 for details).\nSo, we actually need to write the ODEs of motion as follows (note that I’ve\nrecastthelinearODEsintermsoflinearmomentumaswell,tounderscorethe\nsimilarities between the two cases):\nAngular 3D: Linear:\nNnet(t) = ˙L(t) Fnet(t) = ˙p(t)\n!(t) =I 1L(t)v(t) =m 1p(t)\n!(t) =[\n!(t)0]\nv(t) = ˙r(t).\n1\n2!(t)q(t) = ˙q(t)\n13.4. Rigid Body Dynamics 875\nUsing the explicit Euler method, the final approximate solution to the angular\nODEs in three dimensions is actually as follows:\nL(t2) =L(t1) +Nnet(t1)∆t (vectors)\n=L(t1) +∆tå\n8i(\nriFi(t1))\n; (vectors)\n!(t2) =[\nI 1L(t2)0]\n; (quaternions)\nq(t2) =q(t1) +1\n2!(t1)q(t1)∆t.(quaternions)\nTheorientationquaternion q(t)shouldberenormalizedperiodicallytoreverse\nthe effects of the inevitable accumulation of floating-point error.\nAsalways,theexplicitEulermethodisbeingusedherejustasanexample.\nIn a real engine, we would employ velocity Verlet, RK4 or some other more-\nstable and more-accurate numerical method.\n13.4.7 Collision Response\nEverything we’ve discussed so far assumes that our rigid bodies are neither\ncolliding with anything, nor is their motion constrained in any other way.\nWhen bodies collide with one another, the dynamics simulation must take\nsteps to ensure that they respond realistically to the collision and that they\nare never left in a state of interpenetration after the simulation step has been\ncompleted. This is known as collision response .\n13.4.7.1 Energy\nBefore we discuss collision response, we must understand the concept of en-\nergy. When a force moves a body over a distance, we say that the force does\nwork. Work represents a change in energy—that is, a force either adds energy\nto a system of rigid bodies (e.g., an explosion) or it removes energy from the\nsystem (e.g., friction). Energy comes in two forms. The potential energy V\nof a body is the energy it has simply because of where it is relative to a force\nfield such as a gravitational or a magnetic field. (For example, the higher up\na body is above the surface of the Earth, the more gravitational potential en-\nergy it has.) The kinetic energy of a body Trepresents the energy arising from\nthe fact that it is moving relative to other bodies in a system. The total energy\nE=V+Tofanisolatedsystemofbodiesisa conserved quantity,meaningthat\nit remains constant unless energy is being drained from the system or added\nfrom outside the system.\nThe kinetic energy arising from linear motion can be written\nTlinear =1\n2mv2,\n876 13. Collision and Rigid Body Dynamics\nor in terms of the linear momentum and velocity vectors:\nTlinear =1\n2pv.\nAnalogously, the kinetic energy arising from a body’s rotational motion is as\nfollows:\nTangular =1\n2L!.\nEnergy and its conservation can be extremely useful concepts when solving\nall sorts of physics problems. We’ll see the role that energy plays in the deter-\nmination of collision responses in the following section.\n13.4.7.2 Impulsive Collision Response\nWhentwobodiescollideintherealworld,acomplexsetofeventstakesplace.\nThe bodies compress slightly and then rebound, changing their velocities and\nlosing energy to sound and heat in the process. Most real-time rigid body\ndynamics simulations approximate all of these details with a simple model\nbased on an analysis of the momenta and kinetic energies of the colliding ob-\njects, called Newton’slawofrestitutionforinstantaneouscollisionswithnofriction.\nIt makes the following simplifying assumptions about the collision:\n• Thecollisionforceactsoveraninfinitesimallyshortperiodoftime,turn-\ningitintowhatwecallanidealized impulse. Thiscausesthevelocitiesof\nthe bodies to changeinstantaneously as a result of the collision.\n• There is no friction at the point of contact between the objects’ surfaces.\nThisisanotherwayofsayingthattheimpulseactingtoseparatethebod-\nies during the collision is normal to both surfaces—there is no tangen-\ntial component to the collision impulse. (This is just an idealization of\ncourse; we’ll get to friction in Section 13.4.7.5.)\n• The nature of the complex submolecular interactions between the bod-\niesduringthecollisioncanbeapproximatedbyasinglequantityknown\nas thecoefficientof restitution , customarily denoted by the symbol #. This\ncoefficientdescribeshowmuchenergyislostduringthecollision. When\n#=1, thecollisionisperfectlyelastic, andnoenergyislost. (Picturetwo\nbilliard balls colliding in mid-air.) When #=0, the collision is perfectly\ninelastic , also known as perfectly plasticand the kinetic energy of both\nbodies is lost. The bodies will stick together after the collision, continu-\ning to move in the direction that their mutual center of mass had been\nmoving before the collision. (Picture pieces of putty being slammed to-\ngether.)\n13.4. Rigid Body Dynamics 877\nAll collision analysis is based around the idea that linear momentum is\nconserved. So for two bodies 1 and 2, we can write\np1+p2=p′\n1+p′\n2,or\nm1v1+m2v2=m′\n1v′\n1+m′\n2v′\n2\nwheretheprimedsymbolsrepresentthemomentaandvelocitiesafter thecol-\nlision. The kinetic energy of the system is conserved as well, but we must ac-\ncount for the energy lost due to heat and sound by introducing an additional\nenergy loss term Tlost:\n1\n2m1v2\n1+1\n2m2v2\n2=1\n2m′\n1v′\n12+1\n2m′\n2v′\n22+Tlost.\nIf the collision is perfectly elastic, the energy loss Tlostis zero. If it is perfectly\nplastic, the energy loss is equal to the original kinetic energy of the system, the\nprimed kinetic energy sum becomes zero and the bodies stick together after\nthe collision.\nToresolveacollisionusingNewton’slawofrestitution, weapplyanideal-\nizedimpulse to the two bodies. An impulse is like a force that acts over an in-\nfinitesimallyshortperiodoftimeandtherebycausesaninstantaneouschange\nin the velocity of the body to which it is applied. We could denote an impulse\nwith the symbol ∆p, since it is a change in momentum (∆p=m∆v). How-\never, most physics texts use the symbol ˆp(pronounced “p-hat”) instead, so\nwe’ll do the same.\nBecause we assume that there is no friction involved in the collision, the\nimpulse vector must be normal to both surfaces at the point of contact. In\notherwords, ˆp=ˆpn,where nistheunitvectornormaltobothsurfaces. Thisis\nillustratedinFigure13.27. Ifweassumethatthesurfacenormalpointstoward\nbody 1, then body 1 experiences an impulse of ˆp, and body 2 experiences an\nequal but opposite impulse  ˆp. Hence, the momenta of the two bodies after\nthecollisioncanbewrittenintermsoftheirmomentapriortothecollisionand\nthe impulse ˆpas follows:\np′\n1=p1+ˆp p′\n2=p2 ˆp\nm1v′\n1=m1v1+ˆp m2v′\n2=m2v2 ˆp (13.9)\nv′\n1=v1+ˆp\nm1n v′\n2=v2+ˆp\nm2n.\nThecoefficient of restitution provides the key relationship between the rela-\ntive velocities of the bodies before and after the collision. Given that the cen-\nters of mass of the bodies have velocities before the collision and afterward,\nthe coefficient of restitution #is defined as follows:\n(v′\n2 v′\n1) =#(v2 v1). (13.10)\n878 13. Collision and Rigid Body Dynamics\nn\nBody 1Body 2p^\nFigure 13.27. In a frictionless collision, the impulse acts along a line normal to both surfaces at the\npoint of contact. This line is deﬁned by the unit normal vector n.\nSolving Equations (13.9) and (13.10) under the temporary assumption that\nthe bodies cannot rotate yields\nˆp=ˆpn=(#+1)(v2n v1n)\n1\nm1+1\nm2n.\nNotice that if the coefficient of restitution is one (perfectly elastic collision)\nandifthemassofbody2iseffectivelyinfinite(asitwouldbefor,say,aconcrete\ndriveway),then (1/m2) = 0,v2=0,andthisexpressionreducestoareflection\nof the other body’s velocity vector about the contact normal, as we’d expect:\nˆp= 2m1(v1n)n;\nv′\n1=p1+p2\nm1\n=m1v1 2m1(v1n)n\nm1\n=v1 2m1(v1n)n.\nThe solution gets a bit hairier when we take the rotations of the bodies\ninto account. In this case, we need to look at the velocities of the points of\ncontact on the two bodies rather than the velocities of their centers of mass,\nand we need to calculate the impulse in such a way as to impart a realistic\nrotational effect as a result of the collision. We won’t get into the details here,\nbutChrisHecker’sarticle,availableathttp://chrishecker.com/images/e/e7/\nGdmphys3.pdf, does an excellent job of describing both the linear and the ro-\ntational aspects of collision response. The theory behind collision response is\nexplained more fully in [17].\n13.4. Rigid Body Dynamics 879\n13.4.7.3 Penalty Forces\nAnotherapproachtocollisionresponseistointroduceimaginaryforcescalled\npenaltyforces intothesimulation. Apenaltyforceactslikeastiffdampedspring\nattached to the contact points between two bodies that have just interpene-\ntrated. Such a force induces the desired collision response over a short but\nfinite period of time. Using this approach, the spring constant keffectively\ncontrols the duration of the interpenetration, and the damping coefficient b\nacts a bit like the restitution coefficient. When b=0, there is no damping—no\nenergy is lost, and the collision is perfectly elastic. As bincreases, the collision\nbecomes more plastic.\nLet’s take a brief look at some of the pros and cons of the penalty force\napproach to resolving collisions. On the positive side, penalty forces are easy\ntoimplementandunderstand. Theyalsoworkwellwhenthreeormorebodies\nare interpenetrating each other. This problem is very difficult to solve when\nresolving collisions one pair at a time. A good example is the Sony PS3 demo\nin which a huge number of rubber duckies are poured into a bathtub—the\nsimulation was nice and stable despite the very large number of collisions.\nThe penalty force method is a great way to achieve this.\nUnfortunately, because penalty forces respond to penetration (i.e., relative\nposition) rather than to relative velocity, the forces may not align with the di-\nrection we would intuitively expect, especially during a high-speed collision.\nA classic example is a car driving head-on into a truck. The car is low while\nthe truck is tall. Using only the penalty force method, it is easy to arrive at a\nsituation in which the penalty force is vertical, rather than horizontal as we\nwould expect given the velocities of the two vehicles. This can cause the truck\nto pop its nose up into the air while the car drives under it.\nIn general, the penalty force technique works well for low-speed impacts,\nbut it does not work well at all when objects are moving quickly. It is pos-\nsible to combine the penalty force method with other collision resolution ap-\nproaches in order to strike a balance between stability in the presence of large\nnumbersofinterpenetrationsandresponsivenessandmore-intuitivebehavior\nat high velocities.\n13.4.7.4 Using Constraints to Resolve Collisions\nAs we’ll investigate in Section 13.4.8, most physics systems permit various\nkinds of constraints to be imposed on the motion of the bodies in the simula-\ntion. If collisions are treated as constraints that disallow object interpenetra-\ntion, then they can be resolved by simply running the simulation’s general-\npurpose constraint solver. If the constraint solver is fast and produces high-\n880 13. Collision and Rigid Body Dynamics\nquality visual results, this can be an effective way to resolve collisions.\n13.4.7.5 Friction\nFrictionisaforcethatarisesbetweentwobodiesthatareincontinuouscontact,\nresisting their movement relative to one another. There are a number of types\nof friction. Static friction is the resistance one feels when trying to start a sta-\ntionary object sliding along a surface. Dynamic friction is a resisting force that\narises when objects are actually moving relative to one another. Sliding fric-\ntionis a type of dynamic friction that resists movement when an object slides\nalong a surface. Rolling friction is a type of static or dynamic friction that acts\nat the point of contact between a wheel or other round object and the surface\nit is rolling on. When the surface is very rough, the rolling friction is exactly\nstrong enough to cause the wheel to roll without sliding, and it acts as a form\nof static friction. If the surface is somewhat smooth, the wheel may slip, and a\ndynamicformofrollingfrictioncomesintoplay. Collisionfriction isthefriction\nthatactsinstantaneouslyatthepointofcontactwhentwobodiescollidewhile\nmoving. (This is the friction force that we ignored when discussing Newton’s\nlawofrestitutioninSection13.4.7.1.) Variouskindsof constraints canhavefric-\ntion as well. For example, a rusted hinge or axle might resist being turned by\nintroducing a friction torque.\nLet’s look at an example to understand the essence of how friction works.\nLinear sliding friction is proportional to the component of an object’s weight\nthat is acting normal to the surface on which it is sliding. The weight of an\nobjectisjusttheforceduetogravity, G=mg,whichisalwaysdirecteddown-\nward. The component of this force normal to an inclined surface that makes\nan angle qwith the horizontal is just GN=mgcosq. The friction force fis\nthen\nf=mmgcosq,\nwhere the constant of proportionality mis called the coefficient of friction. This\nforce acts tangentially to the surface, in a direction opposite to the attempted\nor actual motion of the object. This is illustrated in Figure 13.28.\nFigure13.28alsoshowsthecomponentofthegravitationalforceactingtan-\ngent to the surface, GT=mgsinq. This force tends to make the object accel-\nerate down the plane, but in the presence of sliding friction, it is counteracted\nbyf. Hence, the net force tangent to the surface is\nFnet=GT f=mg(sinq mcosq).\nIftheangleofinclinationissuchthattheexpressioninparenthesesiszero, the\nobject will slide at a constant speed (if already moving) or be at rest. If the\n13.4. Rigid Body Dynamics 881\nexpression is greater than zero, the object will accelerate down the surface. If\nit is less than zero, the object will decelerate and eventually come to rest.\n13.4.7.6 Welding\nAn additional problem arises when an object is sliding across a polygon soup.\nRecallthatapolygonsoupisjustwhatitsnameimplies—asoupofessentially\nunrelated polygons (usually triangles). As an object slides from one triangle\nof this soup to the next, the collision detection system will generate additional\nspurious contacts because it will think that the object is about to hit the edge\nof the next triangle. This is illustrated in Figure 13.29.\nThere are a number of solutions to this problem. One is to analyze the\nset of contacts and discard ones that appear to be spurious, based on various\nheuristics and possibly some knowledge of the object’s contacts on a previous\nframe (e.g., if we know the object was sliding along a surface and a contact\nnormalarisesthatisduetotheobjectbeingneartheedgeofitscurrenttriangle,\nthen discard that contact normal). Versions of Havok prior to 4.5 employed\nthis approach.\nStartingwithHavok4.5,anewtechniquewasimplementedthatessentially\nannotates the mesh with triangle adjacency information. The collision detec-\ntionsystemtherefore“knows”whichedgesareinterioredgesandcandiscard\nspuriouscollisionsreliablyandquickly. Havokdescribesthissolutionas weld-\ning, because in effect the edges of the triangles in the poly soup are welded to\none another.\n13.4.7.7 Coming to Rest, Islands and Sleeping\nWhen energy is removed from a simulated system via friction, damping or\nother means, moving objects will eventually come to rest. This seems like a\nnatural consequence of the simulation—something that would just “fall out”\nof the differential equations of motion. Unfortunately, in a real computerized\n=m| N|=\nmg cos\n| T|=\nmg sin\n|| =\nmg cos\nFigure 13.28. The force of friction f is proportional to the normal component of the object’s\nweight. The proportionality constant m is called the coefﬁcient of friction.\n882 13. Collision and Rigid Body Dynamics\nSpurious Contacts \nwith Tria ngle Edge\nFigure 13.29. When an object slides between two adjacent triangles, spurious contacts with the\nnew triangle’s edge can be generated.\nsimulation, coming to rest is never quite that simple. Various factors such\nas floating-point error, inaccuracies in the calculation of restitution forces and\nnumerical instability can cause objects to jitter forever rather than coming to\nrestastheyshould. Forthisreason,mostphysicsenginesusevariousheuristic\nmethodstodetectwhenobjectsareoscillatinginsteadofcomingtorestasthey\nshould. Additionalenergycanberemovedfromthesystemtoensurethatsuch\nobjects eventually settle down, or they can simply be stopped abruptly once\ntheir average velocity drops below a threshold.\nWhen an object really does stop moving (finds itself in a state of equilib-\nrium), there is no reason to continue integrating its equations of motion every\nframe. To optimize performance, most physics engines allow dynamic objects\ninthesimulationtobe puttosleep. Thisexcludesthemfromthesimulationtem-\nporarily, although sleeping objects are still active from a collision standpoint.\nIf any force or impulse begins acting on a sleeping object, or if the object loses\noneofthecontactsthatwasholdingitinequilibrium,itwillbeawokensothat\nits dynamic simulation can be resumed.\nSleep Criteria\nVarious criteria can be used to determine whether or not a body qualifies for\nsleep. It’s not always easy to make this determination in a robust manner for\nall situations. For example, a long pendulum might have very low angular\nmomentum and yet still be moving visibly on-screen.\nThe most commonly used criteria for equilibrium detection include:\n• The body is supported. This means it has three or more contact points\n(or one or more planarcontacts) that allow it to attain equilibrium with\ngravity and any other forces that might be affecting it.\n• The body’s linear and angular momentum are below a predefined thresh-\nold.\n13.4. Rigid Body Dynamics 883\n• Arunningaverage of the linear and angular momentum are below a pre-\ndefined threshold.\n• The total kinetic energy of the body (T=1\n2pv+1\n2L!)is below a\npredefined threshold. The kinetic energy is usually mass-normalized\nso that a single threshold can be used for all bodies regardless of their\nmasses.\nThe motion of a body that is about to go to sleep might be progressivelydamped\nso that it comes to a smooth stop rather than stopping abruptly.\nSimulation Islands\nBoth Havok and PhysX further optimize their performance by automatically\ngrouping objects that either are interacting or have the potential to interact in\nthenearfutureintosetscalled simulationislands. Eachsimulationislandcanbe\nsimulated independently of all the other islands—an approach that is highly\nconducive to cache coherency optimizations and parallel processing.\nHavok and PhysX both put entire islands to sleep rather than individual\nrigid bodies. This approach has its pros and cons. The performance boost\nis obviously larger when a whole group of interacting objects can be put to\nsleep. On the other hand, if even one object in an island is awake, the entire\nisland is awake. Overall, it seems that the pros tend to outweigh the cons, so\nthe simulation island design is one we’re likely to continue to see in future\nversions of these SDKs.\n13.4.8 Constraints\nAn unconstrained rigid body has six degrees of freedom (DOF): It can trans-\nlate in three dimensions, and it can rotate about the three Cartesian axes. Con-\nstraints restrict an object’s motion, reducing its degrees of freedom either par-\ntially or completely. Constraints can be used to model all sorts of interesting\nbehaviors in a game. Here are a few examples:\n• a swinging chandelier (point-to-point constraint);\n• a door that can be kicked, slammed, blown of its hinges (hinge con-\nstraint);\n• avehicle’swheelassembly(axleconstraintwithdampedspringsforsus-\npension);\n• a train or a car pulling a trailer (stiff spring/rod constraint);\n• a rope or chain (chain of stiff springs or rods); and\n884 13. Collision and Rigid Body Dynamics\nFigure 13.31. A stiff spring constraint requires that a point on body A be separated from a point on\nbody B by a user-speciﬁed distance.\n• a rag doll (specialized constraints that mimic the behavior of various\njoints in the human skeleton).\nIn the sections that follow, we’ll briefly investigate these and some of the\nothermostcommonkindsofconstraintstypicallyprovidedbyaphysicsSDK.\n13.4.8.1 Point-to-Point Constraints\nFigure 13.30. A\npoint-to-point\nconstraint requires\nthat a point on body\nA aligns with a point\non body B.Apoint-to-pointconstraintisthesimplesttypeofconstraint. Itactslikeaball-\nand-socket joint—bodies can move in any way they like, as long as a specified\npoint on one body lines up with a specified point on the other body. This is\nillustrated in Figure 13.30.\n13.4.8.2 Stiff Springs\nA stiff spring constraint is a lot like a point-to-point constraint except that it\nkeepsthetwopointsseparatedbyaspecifieddistance. Thiskindofconstraint\nacts like an invisible rod between the two constrained points. Figure 13.31\nillustrates this constraint.\n13.4.8.3 Hinge Constraints\nAhingeconstraintlimitsrotationalmotiontoonlyasingledegreeoffreedom,\nabout the hinge’s axis. An unlimited hinge acts like an axle, allowing the con-\nstrainedobjecttocompleteanunlimitednumberoffullrotations. It’scommon\nto define limited hinges that can only move through a predefined range of an-\ngles about the one allowed axis. For example, a one-way door can only move\nthrough a 180 degree arc, because otherwise it would pass through the adja-\ncent wall. Likewise, a two-way door is constrained to move through a 180\ndegree arc. Hinge constraints may also be given a degree of friction in the\nform of a torque that resists rotation about the hinge’s axis. A limited hinge\nconstraint is shown in Figure 13.32.\n13.4. Rigid Body Dynamics 885\n13.4.8.4 Prismatic Constraints\nPrismatic constraints act like a piston: A constrained body’s motion is re-\nstricted to a single translational degree of freedom. A prismatic constraint\nmay or may not permit rotation about the translation axis of the piston. Pris-\nmatic constraints can of course be limited or unlimited and may or may not\ninclude friction. A prismatic constraint is illustrated in Figure 13.33.\n13.4.8.5 Other Common Constraint Types\nMany other types of constraints are possible, of course. Here are just a few\nexamples:\n•Planar. Objects are constrained to move in a two-dimensional plane.\n•Wheel. This is typically a hinge constraint with unlimited rotation,\ncoupled with some form of vertical suspension simulated via a spring-\ndamper assembly.\n•Pulley. In this specialized constraint, an imaginary rope passes through\na pulley and is attached to two bodies. The bodies move along the line\nof the rope via a leverage ratio.\nConstraints may be breakable, meaning that after enough force is applied,\ntheyautomaticallycomeapart. Alternatively,thegamecanturntheconstraint\non and off at will, using its own criteria for when the constraint should break.\n13.4.8.6 Constraint Chains\nLong chains of linked bodies are sometimes difficult to simulate in a stable\nmanner because of the iterative nature of the constraint solver. A constraint\nchainis a specialized group of constraints with information that tells the con-\nstraint solver how the objects are connected. This allows the solver to deal\nwith the chain in a more stable manner than would otherwise be possible.\nFigure 13.32. A limited hinge constraint mimics the behavior of a door.\n886 13. Collision and Rigid Body Dynamics\nFigure 13.33. A prismatic constraint acts like a piston.\n13.4.8.7 Rag Dolls\nAragdollisaphysicalsimulationofthewayahumanbodymightmovewhen\nit is dead or unconscious and hence entirely limp. Rag dolls are created by\nlinkingtogetheracollectionofrigidbodies, oneforeachsemi-rigidpartofthe\nbody. For example, we might have capsules for the feet, calves, thighs, hands,\nupper and lower arms and head and possibly a few for the torso to simulate\nthe flexibility of the spine.\nThe rigid bodies in a rag doll are connected to one another via constraints.\nRagdollconstraintsarespecializedtomimicthekindsofmotionsthejointsin\na real human body can perform. We usually make use of constraint chains to\nimprove the stability of the simulation.\nA rag doll simulation is always tightly integrated with the animation sys-\ntem. As the rag doll moves in the physics world, we extract the positions and\nrotationsoftherigidbodies,andweusethisinformationtodrivethepositions\nand orientations of certain joints in the animated skeleton. So in effect, a rag\ndoll is really just a form of procedural animation that happens to be driven by\nthe physics system. (See Chapter 12 for more details on skeletal animation.)\nOf course, implementing a rag doll is not quite as simple as I’ve made it\nsound here. For one thing, there’s usually not a one-to-one mapping between\nthe rigid bodies in the rag doll and the joints in the animated skeleton—the\nskeleton usually has more joints than the rag doll has bodies. Therefore, we\nneed a system that can map rigid bodies to joints (i.e., one that “knows” to\nwhich joint each rigid body in the rag doll corresponds). There may be addi-\ntional joints between those that are being driven by the rag doll bodies, so the\nmapping system must also be capable of determining the correct pose trans-\nforms for these intervening joints. This is not an exact science. We must apply\nartisticjudgmentand/orsomeknowledgeofhumanbiomechanicsinorderto\nachieve a natural-looking rag doll.\n13.4. Rigid Body Dynamics 887\nBoneCollision \nCapsuleCapsule strikes \nan obstacleBone continues \nto move\nFigure 13.34. With a powered rag doll constraint, and in the absence of any additional forces or\ntorques, a rigid body representing the lower arm can be made to exactly track the movements of\nan animated elbow joint (left). If an obstacle blocks the motion of the body, it will diverge from\nthat of the animated elbow joint in a realistic way (right).\n13.4.8.8 Powered Constraints\nConstraints can also be “powered,” meaning that an external engine system\nsuchastheanimationsystemcanindirectlycontrolthetranslationsandorien-\ntations of the rigid bodies in the rag doll.\nLet’s take an elbow joint as an example. An elbow acts pretty much like a\nlimited hinge, with a little less than 180 degrees of free rotation. (Actually, an\nelbow can also rotate axially, but we’ll ignore that for the purposes of this dis-\ncussion.) To power this constraint, we model the elbow as a rotational spring.\nSuch a spring exerts a torque proportional to the spring’s angle of deflection\naway from some predefined rest angle, N= k(q qrest). Now imagine\nchanging the rest angle externally, say by ensuring that it always matches the\nangleoftheelbowjointinananimatedskeleton. Astherestanglechanges,the\nspring will find itself out of equilibrium, and it will exert a torque that tends\nto rotate the elbow back into alignment with qrest. In the absence of any other\nforces or torques, the rigid bodies will exactly track the motion of the elbow\njoint in the animated skeleton. But if other forces are introduced (for example,\nthe lower arm comes in contact with an immovable object), then these forces\nwillplayintotheoverallmotionoftheelbowjoint,allowingittodivergefrom\nthe animated motion in a somewhat realistic manner. As illustrated in Fig-\nure 13.34, this provides the illusion of a human who is trying her best to move\nin a certain way (i.e., the “ideal” motion provided by the animation) but who\nis sometimes unable to do so due to the limitations of the physical world (e.g.,\nher arm gets caught on something as she tries to swing it forward).\n888 13. Collision and Rigid Body Dynamics\n13.4.9 Controlling the Motions of Rigid Bodies\nMostgamedesignscallforadegreeofcontroloverthewayrigidbodiesmove\nover and above the way they would move naturally under the influence of\ngravityandinresponsetocollisionswithotherobjectsinthescene. Forexam-\nple:\n• An air vent applies an upward force to any object that enters its shaft of\ninfluence.\n• A car is coupled to a trailer and exerts a pulling force on it as it moves.\n• A tractor beam exerts a force on an unwitting spacecraft.\n• An anti-gravity device causes objects to hover.\n• The flow of a river creates a force field that causes objects floating in the\nriver to move downstream.\nAnd the list goes on. Most physics engines typically provide their users with\na number of ways to exert control over the bodies in the simulation. We’ll\noutline the most common of these mechanisms in the following sections.\n13.4.9.1 Gravity\nGravityisubiquitousinmostgamesthattakeplaceonthesurfaceoftheEarth\nor some other planet (or on a spacecraft with simulated gravity). Gravity is\ntechnically not a force but rather a (roughly) constant acceleration, so it af-\nfects all bodies equally regardless of their mass. Because of its ubiquitous and\nspecialnature,themagnitudeanddirectionofthegravitationalaccelerationis\nspecified via a global setting in most SDKs. (If you’re writing a space game,\nyou can always set gravity to zero to eliminate it from the simulation.)\n13.4.9.2 Applying Forces\nAny number of forces can be applied to the bodies in a game physics simula-\ntion. Aforcealwaysactsoverafinitetimeinterval. (Ifitactedinstantaneously,\nit would be called an impulse—more on that in Section 13.4.9.4 below.) The\nforces in a game are often dynamic in nature—they often change their direc-\ntions and/or their magnitudes every frame. So the force-application function\nin most physics SDKs is designed to be called once per frame for the duration\nof the force’s influence. The signature of such a function usually looks some-\nthing like this: applyForce(const Vector& forceInNewtons), where\nthe duration of the force is assumed to be ∆t.\n13.4. Rigid Body Dynamics 889\n13.4.9.3 Applying Torques\nWhenaforceisappliedsuchthatitslineofactionpassesthroughthecenterof\nmassofabody, notorqueisgenerated, andonlythebody’slinearacceleration\nisaffected. Ifitisappliedoff-center,itwillinduce bothalinearandarotational\nacceleration. A pure torque can be applied to a body as well by applying two\nequal and opposite forces to points equidistant from the center of mass. The\nlinearmotionsinducedbysuchapairofforceswillcanceleachotherout(since\nfor the purposes of linear dynamics, the forces both act at the center of mass).\nThis leaves only their rotational effects. A pair of torque-inducing forces like\nthisisknownasa couple(http://en.wikipedia.org/wiki/Couple_(mechanics).\nA special function such as applyTorque(const Vector& torque) may\nbe provided for this purpose. However, if your physics SDK provides no\napplyTorque() function, you can always write one and have it generate a\nsuitable couple instead.\n13.4.9.4 Applying Impulses\nAs we saw in Section 13.4.7.2, an impulse is an instantaneous change in veloc-\nity (or actually, a change in momentum). Technically speaking, an impulse\nis a force that acts for an infinitesimal amount of time. However, the short-\nest possible duration of force application in a time-stepped dynamics simu-\nlation is ∆t, which is not short enough to simulate an impulse adequately.\nAs such, most physics SDKs provide a function with a signature such as\napplyImpulse(const Vector& impulse) for the purposes of applying\nimpulses to bodies. Of course, impulses come in two flavors—linear and\nangular—and a good SDK should provide functions for applying both types.\n13.4.10 The Collision/Physics Step\nNow that we’ve covered the theory and some of the technical details behind\nimplementing a collision and physics system, let’s take a brief look at how\nthese systems actually perform their updates every frame.\nEvery collision/physics engine performs the following basic tasks during\nits update step. Different physics SDKs may perform these phases in different\norders. Thatsaid, thetechniqueI’veseenusedmostoftengoessomethinglike\nthis:\n1. Theforcesandtorquesactingonthebodiesinthephysicsworldareinte-\ngrated forward by ∆tin order to determine their tentative positions and\norientations next frame.\n890 13. Collision and Rigid Body Dynamics\n2. The collision detection library is called to determine if any new contacts\nhave been generated between any of the objects as a result of their ten-\ntative movement. (The bodies normally keep track of their contacts in\norder to take advantage of temporal coherency. Hence, at each step of\nthe simulation, the collision engine need only determine whether any\nprevious contacts have been lost and whether any new contacts have\nbeen added.)\n3. Collisions are resolved, often by applying impulses or penalty forces or\naspartoftheconstraint-solvingstepbelow. DependingontheSDK,this\nphasemayormaynotincludecontinuouscollisiondetection(CCD,oth-\nerwise known as time of impact detection or TOI).\n4. Constraints are satisfied by the constraint solver.\nAt the conclusion of step 4, some of the bodies may have moved away from\ntheir tentative positions as determined in step 1. This movement may cause\nadditional interpenetrations between objects or cause other previously satis-\nfied constraints to be broken. Therefore, steps 1 through 4 (or sometimes only\n2 through 4, depending on how collisions and constraints are resolved) are\nrepeated until either (a) all collisions have been successfully resolved and all\nconstraints are satisfied, or (b) a predefined maximum number of iterations\nhasbeenexceeded. Inthelattercase,thesolvereffectively“givesup,”withthe\nhope that things will resolve themselves naturally during subsequent frames\nof the simulation. This helps to avoid performance spikes by amortizing the\ncost of collision and constraint resolution over multiple frames. However, it\ncan lead to incorrect-looking behavior if the errors are too large or if the time\nstep is too long or is inconsistent. Penalty forces can be blended into the sim-\nulation in order to gradually resolve these problems over time.\n13.4.10.1 The Constraint Solver\nAconstraintsolverisessentiallyaniterativealgorithmthatattemptstosatisfy\na large number of constraints simultaneously by minimizing the error between\nthe actual positions and rotations of the bodies in the physics world and their\nidealpositionsandrotationsasdefinedbytheconstraints. Assuch,constraint\nsolvers are essentially iterative error-minimization algorithms.\nLet’s take a look first at how a constraint solver works in the trivial case\nof a single pair of bodies connected by a single hinge constraint. During each\nstep of the physics simulation, the numerical integrator will find new tenta-\ntive transforms for the two bodies. The constraint solver then evaluates their\n13.4. Rigid Body Dynamics 891\nrelative positions and calculates the error between the positions and orienta-\ntions of their shared axis of rotation. If any error is detected, the solver moves\nthe bodies in such a way as to minimize or eliminate it. Since there are no\nother bodies in the system, the second iteration of the step should discover no\nnew contacts, and the constraint solver will find that the one hinge constraint\nis now satisfied. Hence the loop can exit without further iterations.\nWhen more than one constraint must be satisfied simultaneously, more\niterations may be required. During each iteration, the numerical integrator\nwill sometimes tend to move the bodies out of alignment with their con-\nstraints, while the constraint solver tends to put them back into alignment.\nWith luck, and a carefully designed approach to minimizing error in the con-\nstraint solver, this feedback loop should eventually settle into a valid solu-\ntion. However, the solution may not always be exact. This is why, in games\nwithphysicsengines,yousometimeswitnessseeminglyimpossiblebehaviors,\nlike chains that stretch (opening up little gaps between the links), objects that\ninterpenetrate briefly or hinges that momentarily move beyond their allow-\nable ranges. The goal of the constraint solver is to minimize error—it’s not\nalways possible to eliminate it completely.\n13.4.10.2 Variations between Engines\nThe description given above is of course an over-simplification of what re-\nally goes on in a physics/collision engine every frame. The way in which\nthe various phases of computation are performed, and their order relative\nto one another, may vary from physics SDK to physics SDK. For example,\nsome kinds of constraints are modeled as forces and torques that are taken\ncare of by the numerical integration step rather than being resolved by the\nconstraint solver. Collision may be run before the integration step rather\nthan after. Collisions may be resolved in any number of different ways. Our\ngoal here is merely to give you a taste of how these systems work. For a\ndetailed understanding of how any one SDK operates, you’ll want to read\nits documentation and probably also inspect its source code (presuming the\nrelevant bits are available for you to read). The curious and industrious\nreader can get a good start by downloading and experimenting with Open\nDynamics Engine (ODE) and/or PhysX, as these two SDKs are available for\nfree. You can also learn a great deal from ODE’s wiki, which is available at\nhttp://opende.sourceforge.net/wiki/index.php/Main_Page.",79760
94-13.5 Integrating a Physics Engine into Your Game.pdf,94-13.5 Integrating a Physics Engine into Your Game,"892 13. Collision and Rigid Body Dynamics\nDebug DrawDrive Update\nSubmit\nFigure 13.35. Rigid bodies are linked to their visual representations by way of game objects. An\noptional direct rendering path is usually provided so that the locations of the rigid bodies can be\nvisualized for debugging purposes.\n13.5 Integrating a Physics Engine into Your Game\nObviously, a collision/physics engine is of little use by itself—it must be inte-\ngrated into your game engine. In this section, we’ll discuss the most common\ninterfacepointsbetweenthecollision/physicsengineandtherestofthegame\ncode.\n13.5.1 Linking Game Objects and Rigid Bodies\nThe rigid bodies and collidables in the collision/physics world are nothing\nmore than abstract mathematical descriptions. In order for them to be useful\nin the context of a game, we need to link them in some way to their visual\nrepresentations on-screen. Usually, we don’t draw the rigid bodies directly\n(exceptfordebuggingpurposes). Instead,therigidbodiesareusedtodescribe\nthe shape, size, and physical behavior of the logical objects that make up the\nvirtualgameworld. We’lldiscussgameobjectsindepthinChapter16, butfor\nthe time being, we’ll rely on our intuitive notion of what a game object is—\na logical entity in the game world, such as a character, a vehicle, a weapon,\na floating power-up and so on. So the linkage between a rigid body in the\nphysics world and its visual representation on-screen is usually indirect, with\nthe logical game object serving as the hub that links the two together. This is\nillustrated in Figure 13.35.\nIn general, a game object is represented in the collision/physics world by\nzeroormorerigidbodies. Thefollowinglistdescribesthreepossiblescenarios:\n•Zero rigid bodies . Game objects without any rigid bodies in the physics\n13.5. Integrating a Physics Engine into Your Game 893\nworld act as though they are not solid, because they have no collision\nrepresentation at all. Decorative objects with which the player or non-\nplayer characters cannot interact, such as birds flying overhead or por-\ntions of the game world that can be seen but never reached, might have\nno collision. This scenario can also apply to objects whose collision de-\ntection is handled manually (without the help of the collision/physics\nengine) for some reason.\n•One rigid body . Most simple game objects need only be represented by a\nsingle rigid body. In this case, the shape of the rigid body’s collidable is\nchosen to closely approximate the shape of the game object’s visual rep-\nresentation, and the rigid body’s position and orientation exactly match\nthe position and orientation of the game object itself.\n•Multiplerigidbodies. Somecomplexgameobjectsarerepresentedbymul-\ntiplerigidbodiesinthecollision/physicsworld. Examplesincludechar-\nacters, machinery, vehicles or any object that is composed of multiple\nsolid pieces that can move relative to one another. Such game objects\nusually make use of a skeleton (i.e., a hierarchy of affine transforms) to\ntrack the locations of their component pieces (although other means are\ncertainly possible as well). The rigid bodies are usually linked to the\njoints of the skeleton in such a way that the position and orientation\nof each rigid body corresponds to the position and orientation of one\nof the joints. The joints in the skeleton might be driven by an anima-\ntion, inwhichcasetheassociatedrigidbodiessimplycomealongforthe\nride. Alternatively, the physics system might drive the locations of rigid\nbodies and hence indirectly control the locations of the joints. The map-\nping from joints to rigid bodies may or may not be one-to-one—some\njoints might be controlled entirely by animation, while others are linked\nto rigid bodies.\nThe linkage between game objects and rigid bodies must be managed by\nthe engine, of course. Typically, each game object will manage its own rigid\nbodies, creating and destroying them when necessary, adding and removing\nthem from the physics world as needed, and maintaining the connection be-\ntween each rigid body’s location and the location of the game object and/or\none of its joints. For complex game objects consisting of multiple rigid bodies,\na wrapper class of some kind may be used to manage them. This insulates\nthe game objects from the nitty-gritty details of managing a collection of rigid\nbodiesandallowsdifferentkindsofgameobjectstomanagetheirrigidbodies\nin a consistent way.\n894 13. Collision and Rigid Body Dynamics\n13.5.1.1 Physics-Driven Bodies\nIf our game has a rigid body dynamics system, then presumably we want the\nmotions of at least some of the objects in the game to be driven entirely by the\nsimulation. Such game objects are called physics-driven objects. Bits of debris,\nexplodingbuildings,rocksrollingdownahillside,emptymagazinesandshell\ncasings—these are all examples of physics-driven objects.\nA physics-driven rigid body is linked to its game object by stepping the\nsimulation and then querying the physics system for the body’s position and\norientation. Thistransformisthenappliedeithertothegameobjectasawhole\nor to a joint or some other data structure within the game object.\nExample: Building a Safe with a Detachable Door\nWhenphysics-drivenrigidbodiesarelinkedtothejointsofaskeleton,thebod-\nies are often constrained to produce a desired kind of motion. As an example,\nlet’s look at how a safe with a detachable door might be modeled.\nVisually, let’s assume that the safe consists of a single triangle mesh with\ntwosubmeshes,oneforthehousingandoneforthedoor. Atwo-jointskeleton\nis used to control the motions of these two pieces. The root joint is bound to\nthehousingofthesafe,whilethechildjointisboundtothedoorinsuchaway\nthat rotating the door joint causes the door submesh to swing open and shut\nin a suitable way.\nThe collision geometry for the safe is broken into two independent pieces\nas well, one for the housing and one for the door. These two pieces are used\nto create two totally separate rigid bodies in the collision/physics world. The\nrigid body for the safe’s housing is attached to the root joint in the skeleton,\nand the door’s rigid body is linked to the door joint. A hinge constraint is\nthenaddedtothephysicsworldtoensurethatthedoorbodyswingsproperly\nrelative to the housing when the dynamics of the two rigid bodies are simu-\nlated. The motions of the two rigid bodies representing the housing and the\ndoor are used to update the transforms of the two joints in the skeleton. Once\nthe skeleton’s matrix palette has been generated by the animation system, the\nrenderingenginewillendupdrawingthehousinganddoorsubmeshesinthe\nlocations of the rigid bodies within the physics world.\nIf the door needs to be blown off at some point, the constraint can be bro-\nken, and impulses can be applied to the rigid bodies to send them flying. Vis-\nibly, it will appear to the human player that the door and the housing have\nbecome separate objects. But in reality, it’s still a single game object and a\nsingle triangle mesh with two joints and two rigid bodies.\n13.5. Integrating a Physics Engine into Your Game 895\n13.5.1.2 Game-Driven Bodies\nIn most games, certain objects in the game world need to be moved about in\na non-physical way. The motions of such objects might be determined by an\nanimation or by following a spline path, or they might be under the control\nof the human player. We often want these objects to participate in collision\ndetection—to be capable of pushing the physics-driven objects out of their\nway, for example—but we do not want the physics system to interfere with\ntheir motion in any way. To accommodate such objects, most physics SDKs\nprovide a special type of rigid body known as a game-driven body. (Havok\ncalls these “key framed” bodies.)\nGame-driven bodies do not experience the effects of gravity. They are also\nconsidered to be infinitely massive by the physics system (usually denoted by\na mass of zero, since this is an invalid mass for a physics-driven body). The\nassumption of infinite mass ensures that forces and collision impulses within\nthe simulation can never change the velocity of a game-driven body.\nTomoveagame-drivenbodyaroundinthephysicsworld, wecannotsim-\nplysetitspositionandorientationeveryframetomatchthelocationofthecor-\nrespondinggameobject. Doingsowouldintroducediscontinuitiesthatwould\nbeverydifficultforthephysicalsimulationtoresolve. (Forexample,aphysics-\ndriven body might find itself suddenly interpenetrating a game-driven body,\nbut it would have no information about the game-driven body’s momentum\nwith which to resolve the collision.) As such, game-driven bodies are usually\nmoved using impulses—instantaneous changes in velocity that, when inte-\ngrated forward in time, will position the bodies in the desired places at the\nend of the time step. Most physics SDKs provide a convenience function that\nwill calculate the linear and angular impulses required in order to achieve a\ndesired position and orientation on the next frame. When moving a game-\ndriven body, we do have to be careful to zero out its velocity when it is sup-\nposedtostop. Otherwise,thebodywillcontinueforeveralongitslastnonzero\ntrajectory.\nExample: Animated Safe Door\nLet’s continue our example of the safe with a detachable door. Imagine that\nwe want a character to walk up to the safe, dial the combination, open the\ndoor, deposit some money and close and lock the door again. Later, we want\na different character to get the money in a rather less-civilized manner—by\nblowing the door off the safe. To do this, the safe would be modeled with an\nadditional submesh for the dial and an additional joint that allows the dial to\nbe rotated. No rigid body is required for the dial, however, unless of course\n896 13. Collision and Rigid Body Dynamics\nwe want it to fly off when the door explodes.\nDuring the animated sequence of the person opening and closing the safe,\nits rigid bodies can be put into game-driven mode. The animation now drives\nthe joints, which in turn drive the rigid bodies. Later, when the door is to be\nblown off, we can switch the rigid bodies into physics-driven mode, break the\nhinge constraint, apply the impulse and watch the door fly.\nAs you’ve probably already noticed, the hinge constraint is not actually\nneeded in this particular example. It would only be required if the door is to\nbe left open at some point and we want to see the door swinging naturally in\nresponse to the safe being moved or the door being bumped.\n13.5.1.3 Fixed Bodies\nMostgameworldsarecomposedofbothstaticgeometryanddynamicobjects.\nTomodelthestaticcomponentsofthegameworld,mostphysicsSDKsprovide\na special kind of rigid body known as a fixed body. Fixed bodies act a bit like\ngame-driven bodies, but they do not take part in the dynamics simulation at\nall. They are, in effect, collision-only bodies. This optimization can give a big\nperformanceboosttomostgames,especiallythosewhoseworldscontainonly\nasmallnumberofdynamicobjectsmovingaroundwithinalargestaticworld.\n13.5.1.4 Havok’s Motion Type\nInHavok,alltypesofrigidbodyarerepresentedbyinstancesoftheclass hkp-\nRigidBody. Each instance contains a field that specifies its motion type. The\nmotion type tells the system whether the body is fixed, game-driven (what\nHavok calls “key framed”) or physics-driven (what Havok calls “dynamic”).\nIf a rigid body is created with the fixed motion type, its type can never be\nchanged. Otherwise,themotiontypeofabodycanbechangeddynamicallyat\nruntime. Thisfeaturecanbeincrediblyuseful. Forexample,anobjectthatisin\na character’s hand would be game-driven. But as soon as the character drops\nor throws the object, it would be changed to physics-driven so the dynamics\nsimulation can take over its motion. This is easily accomplished in Havok by\nsimply changing the motion type at the moment of release.\nThemotiontypealsodoublesasawaytogiveHavoksomehintsaboutthe\ninertia tensor of a dynamic body. As such, the “dynamic” motion type is bro-\nken into subcategories such as “dynamic with sphere inertia,” “dynamic with\nboxinertia”andsoon. Usingthebody’smotiontype,Havokcandecidetoap-\nply various optimizations based on assumptions about the internal structure\nof the inertia tensor.\n13.5. Integrating a Physics Engine into Your Game 897\n13.5.2 Updating the Simulation\nThe physics simulation must of course be updated periodically, usually once\nper frame. This does not merelyinvolve stepping the simulation (numerically\nintegrating, resolving collisions and applying constraints). The linkages be-\ntween the game objects and their rigid bodies must be maintained as well. If\nthe game needs to apply any forces or impulses to any of the rigid bodies, this\nmustalsobedoneeveryframe. Thefollowingstepsarerequiredtocompletely\nupdate the physics simulation:\n•Update game-driven rigid bodies. The transforms of all game-driven rigid\nbodies in the physics world are updated so that they match the trans-\nforms of their counterparts (game objects or joints) in the game world.\n•Update phantoms. A phantom shape acts like a game-driven collidable\nwith no corresponding rigid body. It is used to perform certain kinds\nof collision queries. The locations of all phantoms are updated prior to\nthe physics step, so that they will be in the right places when collision\ndetection is run.\n•Update forces, apply impulses and adjust constraints. Any forces being ap-\nplied by the game are updated. Any impulses caused by game events\nthat occurred this frame are applied. Constraints are adjusted if neces-\nsary. (For example, a breakable hinge might be checked to determine if\nit has been broken; if so, the physics engine is instructed to remove the\nconstraint.)\n•Step the simulation. We saw in Section 13.4.10 that the collision and\nphysics engines must both be updated periodically. This involves nu-\nmerically integrating the equations of motion to find the physical state of\nall bodies on the next frame, running the collision detection algorithm to\nadd and remove contacts from all rigid bodies in the physics world, re-\nsolving collisions andapplying constraints. Depending on the SDK, these\nupdate phases may be hidden behind a single atomic step() function,\nor it may be possible to run them individually.\n•Update physics-driven game objects. The transforms of all physics-driven\nobjects are extracted from the physics world, and the transforms of the\ncorresponding game objects or joints are updated to match.\n•Query phantoms. The contacts of each phantom shape are read after the\nphysics step and used to make decisions.\n•Perform collision cast queries. Ray casts and shape casts are kicked off, ei-\nthersynchronouslyorasynchronously. Whentheresultsofthesequeries\n898 13. Collision and Rigid Body Dynamics\nbecome available, they are used by various engine systems to make de-\ncisions.\nThese tasks are usually performed in the order shown above, with the ex-\nception of ray and shape casts, which can theoretically be done at any time\nduring the game loop. Clearly it makes sense to update game-driven bod-\nies and apply forces and impulses prior to the step, so that the effects will\nbe “seen” by the simulation. Likewise, physics-driven game objects should\nalways be updated after the step, to ensure that we’re using the most up-to-\ndate body transforms. Rendering typically happens after everything else in\nthe game loop. This ensures that we are rendering a consistent view of the\ngame world at a particular instant in time.\n13.5.2.1 Timing Collision Queries\nIn order to query the collision system for up-to-date information, we need to\nrun our collision queries (ray and shape casts) after the physics step has run\nduring the frame. However, the physics step is usually run toward the end\nof the frame, after the game logic has made most of its decisions and the new\nlocations of any game-driven physics bodies have been determined. When,\nthen, should collision queries be run?\nThisquestiondoesnothaveaneasyanswer. Wehaveanumberofoptions,\nand most games end up using some or all of them:\n•Base decisions on last frame’s state. In many cases, decisions can be made\ncorrectly based on last frame’s collision information. For example, we\nmight want to know whether or not the player was standing on some-\nthinglastframe, inordertodecidewhetherornotheshouldstartfalling\nthis frame. In this case, we can safely run our collision queries prior to\nthe physics step.\n•Accept a one-frame lag. Even if we really want to know what is happen-\ningthisframe , we may be able to tolerate a one-frame lag in our collision\nquery results. This is usually only true if the objects in question aren’t\nmovingtoofast. Forexample,wemightmoveoneobjectforwardintime\nand then want to know whether or not that object is now in the player’s\nline of sight. A one-frame-off error in this kind of query may not be no-\nticeable to the player. If this is the case, we can run the collision query\nprior to the physics step (returning collision information from the previ-\nous frame) and then use these results as if they were an approximation to\nthe collision state at the end of the current frame.\n•Run the query after the physics step . Another approach is to run certain\nqueries after the physics step. This is feasible when the decisions being\n13.5. Integrating a Physics Engine into Your Game 899\nmade based on the results of the query can be deferred until late in the\nframe. For example, a rendering effect that depends on the results of a\ncollision query could be implemented this way.\n13.5.2.2 Single-Threaded Updating\nA very simple single-threaded game loop might look something like this:\nF32 dt = 1.0f/30.0f;\nfor (;;) // main game loop\n{\ng_hidManager->poll();\ng_gameObjectManager-> preAnimationUpdate(dt);\ng_animationEngine->updateAnimations(dt);\ng_gameObjectManager-> postAnimationUpdate(dt);\ng_physicsWorld ->step(dt);\ng_animationEngine->updateRagDolls(dt);\ng_gameObjectManager-> postPhysicsUpdate(dt);\ng_animationEngine->finalize();\ng_effectManager->update(dt);\ng_audioEngine->udate(dt);\n// etc.\ng_renderManager->render();\ndt= calcDeltaTime();\n}\nInthisexample,ourgameobjectsareupdatedinthreephases: oncebeforean-\nimation runs (during which they can queue up new animations, for example),\nonce after the animation system has calculated final local poses and a tenta-\ntive global pose (but before the final global pose and matrix palette has been\ngenerated) and once after the physics system has been stepped.\n• The locations of all game-driven rigid bodies are generally updated in\npreAnimationUpdate() orpostAnimationUpdate() . Eachgame-\ndriven body’s transform is set to match the location of either the game\nobject that owns it or a joint in the owner’s skeleton.\n900 13. Collision and Rigid Body Dynamics\n• Thelocationofeachphysics-drivenrigidbodyisgenerallyreadin post-\nPhysicsUpdate() and used to update the location of either the game\nobject or one of the joints in its skeleton.\nOne important concern is the frequency with which you are stepping the\nphysicssimulation. Mostnumericalintegrators,collisiondetectionalgorithms\nand constraint solvers operate best when the time between steps ( ∆t) is con-\nstant. It’susuallyagoodideatostepyourphysics/collisionSDKwithanideal\n1/30 second or 1/60 second time delta and then govern the frame rate of your\noverall game loop. If your game drops below its target frame rate, it’s better\nto let the physics slow down visually than to try to adjust the simulation time\nstep to match the actual frame rate.\n13.5.3 Example Uses of Collision and Physics in a Game\nTo make our discussion of collision and physics more concrete, let’s take a\nhigh-level look at a few common examples of how collision and/or physics\nsimulations are commonly used in real games.\n13.5.3.1 Simple Rigid Body Game Objects\nMany games include simple physically simulated objects like weapons, rocks\nthat can be picked up and thrown, empty magazines, furniture, objects on\nshelves that can be shot and so on. Such objects might be implemented by\ncreating a custom game object class and giving it a reference to a rigid body in\nthe physics world (e.g., hkpRigidBody if we’re using Havok). Or we might\ncreateanadd-oncomponentclassthathandlessimplerigidbodycollisionand\nphysics, allowing this feature to be added to virtually any type of game object\nin the engine.\nSimple physics objects usually change their motion type at runtime. They\nare game-driven when being held in a character’s hand and physics-driven\nwhen in free fall after having been dropped.\n13.5.3.2 Bullet Traces\nWhetherornotyouapproveofgameviolence,thefactremainsthatlaserguns\nand projectile weapons of one form or another are a big part of most games.\nLet’s look at how these are typically implemented.\nSometimes projectiles are implemented using ray casts. On the frame that\ntheweaponisfired, weshootoffaraycast, determinewhatobjectwashitand\nimmediately impart the impact to the affected object.\nUnfortunately, the ray cast approach does not account for the travel time\nof the projectile. It also does not account for the slight downward trajectory\n13.5. Integrating a Physics Engine into Your Game 901\ncaused by the influence of gravity. If these details are important to the game,\nwe can model our projectiles using real rigid bodies that move through the\ncollision/physicsworldovertime. Thisisespeciallyusefulforslower-moving\nprojectiles,likethrownobjectsorrockets. ThethrownbricksinNaughtyDog’s\nThe Last of Us used such an approach.\nThere are plenty of issues to consider and deal with when implementing\nlaser beams and projectiles. A few of the most common ones are discussed\nbelow.\nBullet Ray Casting\nWhen using ray casting to check for bullet hits, the question arises: Does the\nray come from the camera focal point or from the tip of the gun in the player\ncharacter’s hands? This is especially problematic in a third-person shooter,\nwhere the ray coming out of the player’s gun usually does not align with the\nray coming from the camera focal point through the reticle in the center of the\nscreen. Thiscanleadtosituationsinwhichthereticleappearstobeontopofa\ntarget, yet the third-person character is clearly behind an obstacle and would\nnot be able to shoot that target from his point of view. Various “tricks” must\nusually be employed to ensure that the player feels like he or she is shooting\nwhat he or she is aiming at while maintaining plausible visuals on the screen.\nMismatches between Collision and Visible Geometry\nMismatchesbetweencollisiongeometryandvisiblegeometrycanleadtositu-\nations in which the player can see the target through a small crack or just over\nthe edge of some other object, and yet the collision geometry is solid, so the\nbullet cannot reach the target. (This is usually only a problem for the player\ncharacter.) One solution to this problem is to use a render query instead of\na collision query to determine if the ray actually hit the target. For example,\nduring one of the rendering passes, we could generate a texturein which each\npixel stores the unique identifier of the game object to which it corresponds.\nWe can then query this texture to determine whether or not an enemy char-\nacter or other suitable target occupies the pixel(s) underneath the weapon’s\nreticle.\nAiming in a Dynamic Environment\nAI-controlled characters may need to “lead” their shots if projectiles take a\nfinite amount of time to reach their targets.\n902 13. Collision and Rigid Body Dynamics\nImpact Effects\nWhen bullets hit their targets, we may want to trigger a sound or a particle\neffect, lay down a decal or perform other tasks.\nIn the Unreal engine, this is accomplished via a system of physical materi-\nals. Visible geometry can be tagged not only with visual materials, but with\nphysicalmaterialsas well. Theformerdefineshowthesurfacelooks,thelatter\ndefines how it reacts to physical interactions, including impact sounds, bullet\n“squib” particle effects, decals and so on. (See http://udn.epicgames.com/\nThree/PhysicalMaterialSystem.html for more details.)\nAt Naughty Dog, we use a very similar system: Collision geometry can\nbe tagged with polygon attributes (PATs for short), which define certain physi-\ncal behaviors like footstep sounds. But bullet impacts are treated in a special\nway, because we need them to interact directly with visible geometry rather\nthan the crude collision geo. As such, visible materials can be tagged with an\noptional bullet effect that defines which bullet squib, impact sound and decal\nshouldbelaiddownforeachpossibletypeofprojectilethatmightimpactthat\nsurface.\n13.5.3.3 Grenades\nGrenades in games are sometimes implemented as free-moving physics ob-\njects. However, this leads to a significant loss of control. Some control can be\nregained by imposing various artificial forces or impulses on the grenade. For\nexample, we could apply an extreme air drag once the grenade bounces for\nthe first time, in an attempt to limit the distance it can bounce away from its\ntarget.\nSomegameteamsactuallygosofarastomanagethegrenade’smotionen-\ntirelymanually. Thearcofagrenade’strajectorycanbecalculatedbeforehand,\nusingaseriesofraycaststodeterminewhattargetitwouldhitifreleased. The\ntrajectory can even be shown to the player via some kind of on-screen display.\nWhen the grenade is thrown, the game moves it along its arc and can then\ncarefully control the bounce so that it never goes too far away from its target,\nwhile still looking natural.\n13.5.3.4 Explosions\nIn a game, an explosion typically has a few components: some kind of visual\neffect like a fireball and smoke, audio effects to mimic the sound of the explo-\nsion and its impacts with objects in the world, and a growing damage radius\nthat affects any objects in its wake.\n13.5. Integrating a Physics Engine into Your Game 903\nWhen an object finds itself in the radius of an explosion, its health is typ-\nically reduced, and we often also want to impart some motion to mimic the\neffect of the shock wave. This might be done via an animation. (For exam-\nple, the reaction of a character to an explosion might best be done this way.)\nWe might also wish to allow the impact reaction to be driven entirely by the\ndynamics simulation. We can accomplish this by having the explosion apply\nimpulses to any suitable objects within its radius. It’s pretty easy to calculate\ndirection of these impulses—they are typically radial, calculated by normaliz-\ning the vector from the center of the explosion to the center of the impacted\nobjectandthenscalingthisvectorbythemagnitudeoftheexplosion(andper-\nhaps falling off as the distance from the epicenter increases).\nExplosions may interact with other engine systems as well. For example,\nwe might want to impart a “force” to the animated foliage system, causing\ngrass,plantsandtreestomomentarilybendasaresultoftheexplosion’sshock\nwave.\n13.5.3.5 Destructible Objects\nDestructibleobjectsarecommonplaceinmanygames. Theseobjectsarepecu-\nliar because they start out in an undamaged state in which they must appear\nto be a single cohesive object, and yet they must be capable of breaking into\nmany separate pieces. We may want the pieces to break off one by one, al-\nlowing the object to be “whittled down” gradually, or we may only require a\nsingle catastrophic explosion.\nDeformable body simulations like DMM can handle destruction naturally.\nHowever, we can also implement breakable objects using rigid body dynam-\nics. This is typically done by dividing a model into a number of breakable\npieces and assigning a separate rigid body to each one. This is the approach\ntaken by Havok Destruction, for example.\nFor reasons of performance optimization and/or visual quality, we might\ndecide to use special “undamaged” versions of the visual and collision geom-\netry, each of which is constructed as a single solid piece. This model can be\nswapped out for the damaged version when the object needs to start breaking\napart. In other cases, we may want to model the object as separate pieces at\nall times. This might be appropriate if the object is a stack of bricks or a pile of\npots and pans, for example.\nTo model a multi-piece object, we could simply stack a bunch of rigid bod-\nies and let the physics simulation take care of it. This can be made to work\nin good-quality physics engines (although it’s not always trivial to get right).\nHowever,wemaywantsomeHollywood-styleeffectsthatcannotbeachieved\nwith a simple stack of rigid bodies.\n904 13. Collision and Rigid Body Dynamics\nForexample,wemaywanttodefinethestructureoftheobject. Somepieces\nmight be indestructible , like the base of a wall or the chassis of a car. Others\nmight be non-structural—they just fall off when hit by bullets or other objects.\nStill other pieces might be structural—if they are hit, not only do they fall, but\ntheyalsoimpartforcestootherpieceslyingontopofthem. Somepiecescould\nbeexplosive—whentheyarehit,theycreatesecondaryexplosionsorpropagate\ndamage throughout the structure. We may want some pieces to act as valid\ncoverpointsforsomecharacters,butnotothers. Thisimpliesthatourbreakable\nobject system may have some connections to the cover system.\nWemightalsowantourbreakableobjectstohaveanotionofhealth. Dam-\nage might build up until eventually the whole thing collapses, or each piece\nmight have a health, requiring multiples shots or impacts before it is allowed\nto break. Constraints might also be employed to allow broken pieces to hang\noff the object rather than coming away from it completely.\nWe may also want our structures to take time to collapse completely. For\nexample, if a long bridge is hit by an explosion at one end, the collapse should\nslowly propagate from one end to the other so that the bridge looks massive.\nThis is another example of a feature the physics system won’t give you for\nfree—it would just wake up all rigid bodies in the simulation island simulta-\nneously. These kinds of effects can be implemented through judicious use of\nthe game-driven motion type.\n13.5.3.6 Character Mechanics\nFor a game like bowling, pinball or Marble Madness, the “main character” is\na ball that rolls around in an imaginary game world. For this kind of game,\nwe could very well model the ball as a free-moving rigid body in the physics\nsimulation and control its movements by applying forces and impulses to it\nduring gameplay.\nIn character-based games, however, we usually don’t take this kind of ap-\nproach. The locomotion of a humanoid or animal character is usually far too\ncomplex to be controlled adequately with forces and impulses. Instead, we\nusuallymodelcharactersasasetofgame-drivencapsule-shapedrigidbodies,\neach one linked to a joint in the character’s animated skeleton. These bodies\nareprimarilyusedforbullethitdetectionortogeneratesecondaryeffectssuch\nas when a character’s arm bumps an object off a table. Because these bodies\nare game-driven, they won’t avoid interpenetrations with immovable objects\nin the physics world, so it is up to the animator to ensure that the character’s\nmovements appear believable.\nTo move the character around in the game world, most games use sphere\nor capsule casts to probe in the direction of desired motion. Collisions are\n13.5. Integrating a Physics Engine into Your Game 905\nresolved manually. This allows us to do cool stuff like:\n• having the character slide along walls when he runs into them at an\noblique angle;\n• allowing the character to “pop up” over low curbs rather than getting\nstuck;\n• preventing the character from entering a “falling” state when he walks\noff a low curb;\n• preventingthecharacterfromwalkingupslopesthataretoosteep(most\ngameshaveacutoffangleafterwhichthecharacterwillslidebackrather\nthan being able to walk up the slope); and\n• adjusting animations to accommodate collisions.\nAs an example of this last point, if the character is running directly into a\nwall at a roughly 90 degree angle, we can let the character “moonwalk” into\nthe wall forever, or we can slow down his animation. We can also do some-\nthing even more slick, like playing an animation in which the character sticks\nout his hand and touches the wall and then idles sensibly until the movement\ndirection changes.\nHavok provides a character controller system that handles many of these\nthings. In Havok’s system, illustrated in Figure 13.36, a character is modeled\nas a capsule phantom that is moved each frame to find a potential new loca-\ntion. A collision contact manifold (i.e., a collection of contact planes, cleaned\nup to eliminate noise) is maintained for the character. This manifold can be\nanalyzed each frame in order to determine how best to move the character,\nadjust animations and so on.\n13.5.3.7 Camera Collision\nIn many games, the camera follows the player’s character or vehicle around\nin the game world, and it can often be rotated or controlled in limited ways\nby the person playing the game. It’s important in such games to never permit\nthe camera to interpenetrate geometry in the scene, as this would break the\nillusion of realism. The camera system is therefore another important client of\nthe collision engine in many games.\nThe basic idea behind most camera collision systems is to surround the\nvirtual camera with one or more sphere phantoms or sphere cast queries that\ncan detect when it is getting close to colliding with something. The system\ncan respond by adjusting the camera’s position and/or orientation in some\nway to avoid the potential collision before the camera actually passes through\nthe object in question.\n906 13. Collision and Rigid Body Dynamics\nFigure 13.36. Havok’s character controller models a character as a capsule-shaped phantom. The\nphantom maintains a noise-reduced collision manifold (a collection of contact planes) that can be\nused by the game to make movement decisions.\nThis sounds simple enough, but it is actually an incredibly tricky problem\nrequiring a great deal of trial and error to get right. To give you a feel for how\nmuch effort can be involved, many game teams have a dedicated engineer\nworking on the camera system for the entire duration of the project. We can’t\npossiblycovercameracollisiondetectionandresolutioninanydepthhere,but\nthe following list should give you a sense of some of the most pertinent issues\nto be aware of:\n• Zooming the camera in to avoid collisions works well in a wide variety\nof situations. In a third-person game, you can zoom all the way in to\na first-person view without causing too much trouble (other than mak-\ning sure the camera doesn’t interpenetrate the character’s head in the\nprocess).\n• It’s usually a bad idea to drastically change the horizontal angle of the\ncamera in response to collisions, as this tends to mess with camera-\nrelativeplayercontrols. However,somedegreeofhorizontaladjustment\ncan work well, depending on what the player is expected to be doing at\nthe time. If she is aiming at a target, she’ll be angry with you if you\nthrowoffheraimtobringthecameraoutofcollision. Butifshe’sjustlo-\ncomoting through the world, the change in camera orientation may feel\nentirely natural. Because of this, you might want to allow adjustments\ntothehorizontalangleofthecameraonlywhenthemaincharacterisnot\n13.5. Integrating a Physics Engine into Your Game 907\nin the heat of a battle.\n• You can adjust the vertical angle of the camera to some degree, but it’s\nimportant not to do too much of this, or the player will lose track of the\nhorizon and end up looking down onto the top of the player character’s\nhead!\n• Some games allow the camera to move along an arc lying in a vertical\nplane, perhaps described by a spline. This permits a single HID control\nsuch as the vertical deflection of the left thumb stick to control both the\nzoomandtheverticalangleofthecamerainanintuitiveway. (Thecam-\neraintheNaughtyDogengineworksthisway.) Whenthecameracomes\ninto collision with objects in the world, it can be automatically moved\nalong this same arc to avoid the collision, the arc might be compressed\nhorizontally, or any number of other approaches might be taken.\n• It’s important to consider not only what’s behind and beside the camera\nbut what is in front of it as well. For example, what should happen if\na pillar or another character comes between the camera and the player\ncharacter? In some games, the offending object becomes translucent; in\nothers,thecamerazoomsinorswingsaroundtoavoidthecollision. This\nmay or may not feel good to the person playing the game! How you\nhandlethesekindsofsituationscanmakeorbreaktheperceivedquality\nof your game.\nEven after taking account of these and many other problematic situations,\nyour camera may not look or feel right! Always budget plenty of time for trial\nand error when implementing a camera collision system.\n13.5.3.8 Rag Doll Integration\nIn Section 13.4.8.7, we learned how special types of constraints can be used to\nlinkacollectionofrigidbodiestogethertomimicthebehaviorofalimp(dead\nor unconscious) human body. In this section, we’ll investigate a few of the\nissues that arise when integrating rag doll physics into your game.\nAswesawinSection13.5.3.6,thegrossmovementsofaconsciouscharacter\nareusuallydeterminedbyperformingshapecastsormovingaphantomshape\naround in the game world. The detailed movements of the character’s body\nare typically driven by animations. Game-driven rigid bodies are sometimes\nattached to the limbs for the purposes of weapons targeting or to allow the\ncharacter to knock over other objects in the world.\nWhen a character becomes unconscious, the rag doll system kicks in. The\ncharacter’s limbs are modeled as capsule-shaped rigid bodies connected via\nconstraints and linked to joints in the character’s animated skeleton. The\n908 13. Collision and Rigid Body Dynamics\nphysics system simulates the motions of these bodies, and we update the\nskeletal joints to match, thereby allowing physics to move the character’s\nbody.\nThesetofrigidbodiesusedforragdollphysicsmightnotbethesameones\naffixedtothecharacter’slimbswhenitwasalive. Thisisbecausethetwocolli-\nsion models have very different requirements. When the character is alive, its\nrigid bodies are game-driven, so we don’t care if they interpenetrate. And in\nfact,weusuallywantthemtooverlap,sotherearen’tanyholesthroughwhich\nan enemy character might shoot. But when the character turns into a rag doll,\nit’s important that the rigid bodies do not interpenetrate, as this would cause\nthe collision resolution system to impart large impulses that would tend to\nmake the limbs explode outward! For these reasons, it’s actually quite com-\nmonforcharacterstohaveentirelydifferentcollision/physicsrepresentations\ndepending on whether they’re conscious or unconscious.\nAnother issue is how to transition from the conscious state to the uncon-\nsciousstate. AsimpleLERPblendbetweenanimation-generatedandphysics-\ngeneratedposesusuallydoesn’tworkverywell,becausethephysicsposevery\nquicklydivergesfromtheanimationpose. (Ablendbetweentwototallyunre-\nlated poses usually doesn’t look natural.) As such, we may want to use pow-\nered constraints during the transition (see Section 13.4.8.8).\nCharacters often interpenetrate background geometry when they are con-\nscious(i.e.,whentheirrigidbodiesaregame-driven). Thismeansthattherigid\nbodies might be inside another solid object when the character transitions to\nragdoll(physics-driven)mode. Thiscangiverisetohugeimpulsesthatcause\nrather wild-looking rag doll behavior in-game. To avoid these problems, it is\nbesttoauthordeathanimationscarefully,sothatthecharacter’slimbsarekept\nout of collision as best as possible. It’s also important to detect collisions via\nphantomsorcollisioncallbacksduring thegame-drivenmodesothat youcan\ndropthecharacterintoragdollmodethemomentanypartofhisbodytouches\nsomething solid.\nEven when these steps are taken, rag dolls have a tendency to get stuck\ninside other objects. Single-sided collision can be an incredibly important fea-\nture when trying to make rag dolls look good. If a limb is partly embedded\nin a wall, it will tend to be pushed out of the wall rather than staying stuck\ninside it. However, even single-sided collision doesn’t solve all problems. For\nexample, when the character is moving quickly or if the transition to rag doll\nisn’t executed properly, one rigid body in the rag doll can end up on the far\nside of a thin wall. This causes the character to hang in mid-air rather than\nfalling properly to the ground.\nAnother rag doll feature that can be useful is the ability for unconscious",40965
95-13.6 Advanced Physics Features.pdf,95-13.6 Advanced Physics Features,"13.6. Advanced Physics Features 909\ncharacters to regain consciousness and get back up. To implement this, we\nneed a way to search for a suitable “stand up” animation. We want to find\nan animation whose pose on frame zero most closely matches the rag doll’s\npose after it has come to rest (which is totally unpredictable in general). This\ncan be done by matching the poses of only a few key joints, like the upper\nthighsandtheupperarms. Anotherapproachistomanuallyguidetheragdoll\ninto a pose suitable for getting up by the time it comes to rest, using powered\nconstraints.\nAs a final note, we should mention that setting up a rag doll’s constraints\ncanbeatrickybusiness. Wegenerallywantthelimbstomovefreelybutwith-\noutdoinganythingbiomechanicallyimpossible. Thisisonereasonspecialized\ntypes of constraints are often used when constructing rag dolls. Nonetheless,\nyou shouldn’t assume that your rag dolls will look great without some effort.\nHigh-qualityphysicsengineslikeHavokprovidearichsetofcontentcreation\ntoolsthatallowanartisttosetupconstraintswithinaDCCpackagelikeMaya\nand then test them in real time to see how they might look in-game.\nAll in all, getting rag doll physics to workin your game isn’t particularly\ndifficult,butgettingittolook goodcantakealotofwork! Aswithmanythings\nin game programming, it’s a good idea to budget plenty of time for trial and\nerror, especially when it’s your first time working with rag dolls.\n13.6 Advanced Physics Features\nA rigid body dynamics simulation with constraints can cover an amazing\nrange of physics-driven effects in a game. However, such a system clearly has\nitslimitations. Recentresearchanddevelopmentisseekingtoexpandphysics\nengines beyond constrained rigid bodies. Here are just a few examples:\n•Deformable bodies. As hardware capabilities improve and more-efficient\nalgorithmsaredeveloped,physicsenginesarebeginningtoprovidesup-\nport for deformable bodies. DMM is an excellent example of such an\nengine.\n•Cloth. Cloth can be modeled as a sheet of point masses, connected by\nstiff springs. Cloth is notoriously difficult to get right, as many difficul-\nties arise with respect to collision between cloth and other objects, nu-\nmerical stability of the simulation, etc. That being said, many games\nand third-party physics SDKs like Havok now provide powerful and\nwell-behaved cloth simulations for use in games and other real-time\napplications.\n910 13. Collision and Rigid Body Dynamics\n•Hair. Hair can be modeled by a large number of small physically simu-\nlatedfilments,orasimplerapproachcanbeusedinwhichsheetsofcloth\naretexture-mappedtolooklikehair,andtheclothsimulationistunedto\nmake the character’s hair move in a believable way. This is how Chloe’s\nhair inUncharted: TheLostLegacy works. Hair simulation and rendering\nremains an active area of research, and the quality of hair in games will\ncertainly continue to improve.\n•Water surface simulations and buoyancy. Games have been doing water\nsurface simulations and buoyancy for some time now. Buoyancy can be\nimplemented via a special-case system (not part of the physics engine\nper se), or it can be modeled via forces within the physics simulation.\nOrganic movement of the water surface is often a rendering effect only\nand does not affect the physics simulation at all. From the point of view\nof physics, the water surface is often modeled as a plane. For large dis-\nplacementsinthewatersurface,theentireplanemightbemoved. How-\never, some game teams and researchers are pushing the limits of these\nsimulations, allowing for dynamic water surfaces, waves that crest, re-\nalistic current simulations and more. One good example is the water in\nthe god game FromDust.\n•General fluid dynamics simulations. Right now, fluid dynamics falls pri-\nmarily into the realm of specialized simulation libraries. However, this\nis an active area of research and development, and some games already\nmakeuseoffluidsimulationstoproducesomeastoundingvisualeffects.\nForexample,the LittleBigPlanet seriesmakesuseofa2Dfluidsimulation\nforitssmokeandfireeffects;andthePhysXSDKoffersa3D positionbased\nfluid simulation that produces stunningly realistic results.\n•Physically based audio synthesis. When physically simulated objects col-\nlide,bounce,rollandslide,it’simportanttobeabletogenerateappropri-\nate audio to reinforce the believability of the simulation. These sounds\ncan be created in games via controlled playback of pre-recorded audio\nclips. But dynamic synthesis of such sounds is becoming a viable alter-\nnative, and is currently an active area of research.\n•GPGPU. As GPUs become more and more powerful, there has been a\nshift toward harnessing their awesome parallel processing power for\ntasks other than graphics. One obvious application of general-purpose\nGPU (GPGPU) computing is for collision and physics simulation. For\nexample, Naughty Dog’s cloth simulation engine was ported to run en-\ntirely on the GPU for PlayStation 4.",5009
96-14.1 The Physics of Sound.pdf,96-14.1 The Physics of Sound,"14\nAudio\nIf you’ve ever watched a horror film with your speakers muted, you know\njust how important audio is to immersiveness. (If not, try it! It’s a real ear-\nopener.) Be it a film or a video game, sound can quite literally make the dif-\nference between a gripping, emotional, unforgettable multimedia experience\nand a lackluster yawnfest.\nModerngamesimmersetheplayerinarealistic(orasemi-realisticbutstyl-\nized) virtual environment. The graphics engine is charged with the task of\nreproducing as accurately and believably as possible what the player would\nactuallysee,ifheorshewerepresentwithinthisvirtualenvironment(whilere-\nmaining true to the art style of the game). In exactly the same sense, the audio\nengineischargedwiththetaskofaccuratelyandbelievablyreproducingwhat\nthe player would actually hear, if he or she were present in the game world\n(while remaining true to the fiction and tonal style of the game). Sound pro-\ngrammers today use the term audio rendering engine to underscore its many\nparallels with the graphics rendering engine.\nIn this chapter, we’re going to explore both the theory and practice of cre-\natingaudio fora triple-Agame. We’llintroducean areaofmathematics called\nsignalprocessingtheory thatunderliesalmosteveryaspectofdigitalaudiotech-\nnology, including digital sound recording and playback, filtering, reverb and\nother digital signal processor (DSP) effects. We’ll explore game audio from\na software engineering standpoint, by investigating a number of widely used\n911\n912 14. Audio\naudioAPIs,breakingdownthecomponentsthatcompriseatypicalaudioren-\nderingengineandlearninghowtheaudiosystemisinterconnectedwithother\ngame engine systems. We’ll also see how environmental acoustic modeling\nand character dialog were handled in Naughty Dog’s popular game The Last\nof Us. So hold on tight, keep your hands inside the car at all times and enjoy\nthe noisy ride!\n14.1 The Physics of Sound\nSound is a compression wave that travels through the air (or some other com-\npressible medium). A sound wave gives rise to alternating regions of air com-\npressionanddecompression(alsoknownas rarefaction)relativetotheaverage\natmospheric pressure. As such, we measure the amplitude of a sound wave in\nunits ofpressure . In SI units, pressure is measured in Pascals, abbreviated Pa.\nOnePascalistheforceofoneNewtonappliedoveranareaofonesquaremeter\n(1Pa = 1N/m2=1kg/(ms2)).\nTheinstantaneous acoustic pressure is the ambient atmospheric pressure\n(considered a constant for our purposes) plus the perturbation caused by the\nsound wave at one specific instant in time:\npinst=patmos +psound .\nOf course, sound is a dynamic phenomenon—the sound pressure varies over\ntime. We can plot the instantaneous sound pressure as a function of time,\npinst(t). Insignalprocessingtheory—the area of mathematics that underlies vir-\ntually every aspect of digital audio technology—such a time-varying function\nis called a signal. Figure 14.1 illustrates a typical sound wave signal p(t), os-\ncillating about the average atmospheric pressure.\np(t)\ntpatmos+\n–\nFigure 14.1. A signal p(t)can be used to model the time-varying acoustic pressure of a sound.\n14.1. The Physics of Sound 913\nT\ntp(t)\nFigure 14.2. The period T of an arbitrary periodic signal is the minimum time between repeated\npatterns in the waveform.\n14.1.1 Properties of Sound Waves\nWhenamusicalinstrumentplaysalongsteadynote,theresultingsoundpres-\nsure signal is periodic, meaning the waveform consists of a repeating pattern\ncharacteristic to that particular kind of instrument. The period Tof any re-\npeating pattern describes the minimum amount of time that passes between\nsuccessive instances of the pattern. For example, for a sinusoidal sound wave\ntheperiodmeasuresthetimebetweensuccessivepeaksortroughs. InSIunits,\nperiod is typically measured in seconds (s). This is illustrated in Figure 14.2.\nThefrequency of a wave is just the inverse of its period ( f=1/T). Fre-\nquency is measured in Hertz (Hz), which means “cycles per second.” A “cy-\ncle” is technically a dimensionless quantity, so the Hertz is the inverse of the\nsecond (Hz =1/s).\nMany scientists and mathematicians make use of a quantity known as the\nangularfrequency ,typicallydenotedbythesymbol w. Theangularfrequencyis\njust the rate of oscillation measured in radiansper second instead of cyclesper\nsecond. Since one complete circular rotation is 2pradians, w=2pf=2p/T.\nAngular frequency is very useful when analyzing sinusoidal waves, because\na circular motion in two dimensions gives rise to a sinusoidal motion when\nprojected onto a single dimensional axis.\nThe amount by which a periodic signal such as a sine wave is shiftedleft or\nright along the time axis is known as its phase. Phase is a relative term. For ex-\nample, sin(t)is really just a version of cos(t)that has been phase-shifted by+p\n2\nalong the taxis (i.e., sin(t) = cos(t p\n2)). Likewise cos(t)is just sin(t)phase-\nshifted by p\n2(i.e., cos(t) = sin(t+p\n2)). Phase is illustrated in Figure 14.3.\nThespeed vat which a sound wave propagates through its medium de-\npends upon the material and physical properties of the medium, including\nphase (solid, gas or liquid), temperature, pressure and density. In 20°C dry\n914 14. Audio\ntsin( t) cos( t)\n/2\n /2 –\n/2\nFigure 14.3. The sine and cosine functions are just phase-shifted versions of one another.\nair, the speed of sound is approximately 343.2 m/s, which is 767.7 mph or\n1235.6 km/h.\nThewavelength lof a sinusoidal wave measures the spatial distance be-\ntween successive peaks or troughs. It depends in part on the frequency of the\nwave, but because it is a spatialmeasurement it also depends on the speed of\nthewave. Specifically, l=v/fwhere visthespeedofthewave(measuredin\nm/s) and fis the frequency (measured in Hz or 1/s). The seconds in the nu-\nmeratoranddenominatorcanceloneanother,andweareleftwithwavelength\nmeasured in units of meters.\n14.1.2 Perceived Loudness and the Decibel\nIn order to judge the “loudness” of the sounds we hear, our ears continuously\naveragethe amplitude of the incoming sound signal over a short, sliding time\nwindow. This averaging effect is modeled well by a quantity known as the\neffective sound pressure . This is defined as the root mean square (RMS) of the\ninstantaneous sound pressure measured over a specific interval of time.\nIf we were to take a series of ndiscrete sound pressure measurements pi,\nequally spaced in time, the RMS sound pressure prmswould be\nprms=√\n1\nnn\nå\ni=1p2\ni. (14.1)\nHowever, our ears take pressure measurements continuously, rather than at\ndiscrete points in time. If we imagine measuring the instantaneous sound\npressure continuously, starting at time T1and lasting until time T2, the sum-\nmation in Equation (14.1) would become an integral as follows:\nprms=√\n1\nT2 T1∫T2\nT1(p(t))2dt. (14.2)\n14.1. The Physics of Sound 915\nHowever, the story doesn’t end here. Perceived loudness is actually pro-\nportional to the acousticintensity I, which is itself proportional to the squareof\nthe RMS sound pressure:\nIµp2\nrms.\nHumans can perceive a very wide range of sound pressure variations—\nfrom the flutter of a piece of paper falling to the ground to the boom of an\naircraft breaking mach one. In order to manage such a wide dynamic range,\nwe normally measure sound intensity in units of decibels (dB). The decibel is\nalogarithmic unit that expresses the ratiobetween two values. By employing\na logarithmic scale, the decibel allows a wide range of measurements to be\nrepresented by a relatively narrow range of values. A decibel is actually one-\ntenth of a bel, a unit named in honor of Alexander Graham Bell.\nWhen sound intensity is measured in decibels, it is called sound pressure\nlevel(SPL) and represented by the symbol Lp. Sound pressure level is defined\nastheratiooftheacousticintensity(i.e.,thesquaredpressure)ofasoundrela-\ntive to a reference intensity prefthat represents the lower limit of human hear-\ning. So we have:\nLp=10 log10(\np2\nrms\np2\nref)\ndB\n=20 log10(prms\npref)\ndB,\nwhere the 20 arises because when we take the square outside the logarithm, it\nbecomes a multiplication by two. The commonly used reference sound pres-\nsure in air is pref=20mPa (RMS). For more information on sound pressure,\nthe physics of sound and human aural perception, see [6].\nBy the way, if you’re feeling a bit rusty, the following identities may help\nto refresh your memory on logarithms. In Equations (14.3), b,xandyare\npositive real numbers with b̸=1,canddare any real numbers, c=logbx,\nandd=logby(or written another way, bc=xandbd=y).\nlogbx=c when bc=x(definition);\nlogb1=0 because b0=1;\nlogbb=1 because b1=b;\nlogb(xy) = logbx+logbybecause bcbd=bc+d; (14.3)\nlogb(x/y) = logbx logbybecause bc/bd=bc d;\nlogbxd=dlogbx because (bc)d=bcd.\n916 14. Audio\nFigure 14.4. The human ear is most sensitive in the frequency range between 2 and 5 kHz. As the\nfrequency decreases or increases beyond this range, more and more acoustic pressure is required\nto produce the same perception of “loudness.”\n14.1.2.1 Equal-Loudness Contours\nThe human ear does not have the same response to sound waves of different\nfrequencies. The human ear is most sensitive in the frequency range between\n2 and 5 kHz. As the frequency decreases or increases beyond this range, more\nand more acoustic intensity (i.e., pressure) is required to produce the same\nperception of “loudness.”\nFigure 14.4 shows a number of equal-loudness countours, each corre-\nspondingtoadifferentperceivedloudnesslevel. Thesecurvesshowthatmore\npressureisrequiredatlowandhighfrequenciestoachievethesameperceived\nloudness than is needed at mid-range frequencies. Or put another way, if we\nweretokeeptheamplitudeofanacousticpressurewavethesamewhilevary-\ningthefrequency,thehumanearwouldactuallyperceivethelowerandhigher\nfrequencies as “less loud” than the mid-range frequencies. The lowest equal-\nloudness contour represents the quietest audible tone and is also known as\nthe absolute threshold of hearing. The highest contour passes through the\nhuman threshold of pain, which lies roughly at the 120 dB level for audible\nsounds.\nFor more information on equal-loudness contours, and the Fletcher-Mun-\nson curves on which they are based, see https://bit.ly/2HfCjCs.\n14.1. The Physics of Sound 917\n14.1.2.2 The Audible Frequency Band\nA typical adult can hear sounds with frequencies as low as 20 Hz and as high\nas20,000Hz(20kHz)(althoughtheupperlimitgenerallydecreaseswithage).\nThe equal-loudness contours help to explain why the human ear can perceive\nsounds only within this limited “band” of frequencies. As the frequency be-\ncomes lower or higher, more and more acoustic pressure is required to pro-\nduce the same perceived loudness. As the frequency approaches the lower or\nupper limits of human hearing, the countours become asympotically vertical,\nmeaning we’d need an effectively infinite acoustic pressure to produce any\nperception of loudness at all. Or put another way, human audio perception\ndrops off to effectively zero outside the audible frequency band.\n14.1.3 Sound Wave Propagation\nLike any kind of wave, an acoustic pressure wave propagates through space\nand can be absorbed orreflected by surfaces, diffracted around corners and\nthrough narrow “slits,” and refracted as it passes across the boundary between\ndifferent transmission media. Sound waves exhibit no polarization1because\nthe acoustic pressure oscillation occurs in the direction of wave travel (this is\nknown as a longitudinal wave), rather than perpendicular to it as with light\nwaves (a transverse wave). In games, we typically model the absorption, re-\nflection and sometimes the diffraction (e.g., bending slightly around corners)\nof our virtual sound waves, but we generally ignore refraction effects because\nthese effects are not easily noticed by a human listener.\n14.1.3.1 Fall-Off with Distance\nIn an open space with otherwise perfectly still air, and assuming a sound\nsourcethatradiatesequallyinalldirections,theintensityofthesoundpressure\nwave it produces falls off with distance, following a 1/r2law, while pressure\nfollows a 1/rlaw.\np(r)µ1\nr;\nI(r)µ1\nr2.\nHere, rmeasures the radial distance of the listener or microphone from the\nsound source, and both pressure and intensity are expressed as functions of r.\n1Sound waves in solids can be transverse and therefore can exhibit polarization.\n918 14. Audio\nFigure 14.5. Three types of sound sources and their sound radiation patterns (in two dimensions\nfor ease of illustration). From left to right: omnidirectional, conical and directional.\nMore precisely, the sound pressure level for a spherically radiating (omni-\ndirectional) sound wave in open space can be written as follows:\nLp(r) =Lp(0) +10 log10(1\n4pr2)\ndB\n=Lp(0) 10 log10(\n4pr2)\ndB,\nwhere Lp(r)is the SPL at the listener as a function of its radial distance from\nthe sound source, and Lp(0)represents the unattenuated or “natural” sound\nintensity of the source.\nSoundsourcesarenotalwaysomnidirectional. Forexample, whenalarge,\nflatwallreflectssoundwaves,itactslikeapurely directional soundsource—the\nreflected waves propagate in a single direction, and the pressure wavefronts\nare essentially parallel.\nAbullhornprojectssoundinaparticulardirectionbutwitha conicalfall-off,\nmeaning that the intensity of the sound waves is maximum along the center-\nline of the projection “cone,” but falls off as the angle between the listener and\nthis centerline increases.\nVarious sound radiation patterns are illustrated in Figure 14.5.\n14.1.3.2 Atmospheric Absorption\nThe 1/rfall-off of sound pressure with distance arises because energy is dis-\nsipated as the waveform expands geometrically. This fall-off affects sounds\nof all frequencies equally. Sound intensity also falls off with distance due to\nenergy absorption by the atmosphere. Atmospheric absorption effects are not\nuniformacrosstheentirefrequencyspectrum. Ingeneral,theabsorptioneffect\nbecomes greater as the frequency of the sound increases.\nI’m reminded of a story I heard when I was in high school: A woman was\nwalkingdownaquietvillagestreetatnight. Sheheardasporadicsequenceof\n14.1. The Physics of Sound 919\nlow tones with long, silent gaps between them. Curious what might be mak-\ning these strange tones, she walked toward them. As she walked, the tones\nbecame louder and the spaces between the tones seemed to get shorter. After\na few minutes’ walk, the tones had resolved into a beautiful piece of music.\nThe woman arrived at an open window to discover a viola player practicing\nwithin. The musician stopped playing to say “hello,” and the woman asked\nhimwhyhehadbeenplayingrandomnotesafewminutesbefore. Hereplied,\n“Ihaven’tbeenplayingrandomnotes—I’vebeenplayingthispiecethewhole\ntime.” The explanation for what the woman heard, of course, is that lower-\nfrequency sounds can be heard over longer distances than higher-frequency\nsounds due to atmospheric absorption. You can learn more about sound\nwave propagation at http://www.sfu.ca/sonic-studio/handbook/Sound_\nPropagation.html.\nOther factors also affect the intensity of sound waves as they propagate\nthrough their medium. In general, fall-off depends on distance, frequency,\ntemperature and humidity. See http://sengpielaudio.com/calculator-air.htm\nfor an online calculator that lets you experiment with the effects of these\nfactors.\n14.1.3.3 Phase Shift and Interference\nWhen multiple sound waves overlap in space, their amplitudes add toge-\nther—thisiscalled superposition. Considertwoperiodicsoundwaveswiththe\nsamefrequency. (Thesimplestexamplewouldbetwosinusoids.) Ifthewaves\nareinphase—thatis, theirpeaksandtroughslineup—thenthewaveswill pos-\nitivelyreinforce each other, and the result is a wave with larger amplitude than\neither of the original waves. Likewise, if the waves are out of phase, the peaks\nof one wave can tend to cancelthe troughs of the other and vice versa, and the\nresult is a wave with lower (or even zero) amplitude.\nWhen multiple waves interact, we call this interference .Constructive inter-\nferencedescribesthecaseinwhichthewavesreinforceoneanotherandtheam-\nplitude increases. Destructive interference occurs when the waves cancel each\nother out, resulting in lower amplitude.\nThe frequency of the waves has an important effect on this phenomenon:\nIf the frequencies of the two waves match closely, the interference simply in-\ncreases or decreases the overall amplitude. If the frequencies differ signifi-\ncantly, we can get an effect called beating, wherein the frequency difference\ncauses alternating periods of the waves being in and out of phase, resulting in\nalternating periods of higher and lower amplitude.\nInterference can occur between two totally unrelated sound signals, or it\ncan occur if a single sound signal takes multiple paths from the source to the\n920 14. Audio\nlistener. In the latter case, the difference in path lengths introduces a phase\nshift that can cause either constructive or destructive interference, depending\non the amount of the phase shift.\nComb Filtering\nInterference can lead to an effect known as combfiltering . This is caused when\nsound waves reflect off surfaces in such a way as to either almost completely\ncancel or completely reinforce certain frequencies. The result is a frequency\nresponse (see Section 14.2.5.7) with lots of narrow peaks and troughs, which\nwhenplottedlookabitlikeacomb(hencethename). Thiseffectcanhaveabig\nimpact on audio reproduction and recording—sometimes it is an undesirable\nartifact, and sometimes it is used as a tool. The existence of comb filtering\nis also one of the key reasons why it is generally better to spend money on\nacoustic room treatment than to spend money on high-end audio equipment:\nIf the room exhibits comb effects, you’re wasting your time trying to get a flat\nresponsefromyourgear. Seehttp://www.realtraps.com/video_comb.htmfor\na great video by Ethan Winer on the subject.\n14.1.3.4 Reverb and Echo\nIn any environment containing sound-reflective surfaces, a listener generally\nreceives three kinds of sound waves from a sound source:\n•Direct (dry) . Sound waves that arrive at the listener via a direct, un-\nobstructed path from the source are collectively known as directordry\nsound.\n•Earlyreflections(echo). Soundwavesthatarriveatthelistenerviaanindi-\nrectpath, afterbeingreflectedfromandpartiallyabsorbedbysurround-\ning surfaces, take a longer time to reach the listener because their path\nis longer. As such, there will be a delaybetween the arrival of the di-\nrect sound waves and the arrival of the reflected waves. The first group\nof reflected sound waves arriving at the ear have only interacted with\none or two surfaces. As such, they are relatively “clean” signals, and we\nperceive them as distinct new “copies” of the sound or echos.\n•Latereverberations(tail) . Oncethesoundwaveshavebouncedaroundthe\nlistening space more than a few times, they superimpose and interfere\nwith one another so much that the brain can no longer detect distinct\nechos. These are known as latereverberations or thediffusetail. The prop-\nerties of the reflective surfaces cause the amplitudes of the waves to be\nattenuatedbyvaryingamounts. Andbecausethereflectedsoundwaves\n14.1. The Physics of Sound 921\ntp(t)\nDry Wet\nEarly\nReflections\nLate Reverberations\n(Diffuse Tail)\n1600 ms 100 ms 50 ms 0 ms\nFigure 14.6. Direct sound waves, early reﬂections and late reverberations.\nare delayed, phase shifts occur causing the waves to interfere with one\nanother. This causes certain frequencies to be attenuated relative to the\nothers. When we speak of the acoustics of a space, we are largely speak-\ning about the effects of late reverberations on the perceived “quality” or\n“timbre” of the sound.\nCollectively, the echos and the tail combine with the dry sound to create what\nis known as wetsound. Figure 14.6 illustrates the wet and dry components of\na single abrupt clap.\nThe early reflections and late reverberations provide the brain with a\nwealth of cues that tell us quite a lot about the type of space we’re in. The\npre-delay isthetimeintervalbetweenthearrivalofthedirectsoundwavesand\nthe arrival of the very first reflected waves. From the pre-delay, the brain can\ndetermine the approximate size of the room or space in which we are listen-\ning. The decayis the time it takes for the reflected sound waves to die away.\nThis tells our brains how much of the sound has been absorbed by the sur-\nroundings, and so indirectly tells us something about the materials that make\nup the space we’re in. For example, a small tiled bathroom would produce\nlate reverberations with a very short pre-delay (due to its small size) and a\nlong decay (due to the tile’s ability to reflect sound waves efficiently, with\nlittle absorption). A large granite-walled room like Grand Central Terminal\n(a.k.a. Grand Central Station) in New York City will have a much longer pre-\ndelay and a lot more echos, but the decay will be similar to that of the tiled\nbathroom.\nIf we were to hang curtains in that bathroom, or if the walls were covered\nwith wood panels instead of tile, the pre-delay would remain the same, but\n922 14. Audio\nthe decay, along with other factors such as density(how closely spaced in time\nthe individual reflections are) and diffusion (the rate at which the reflections\nincrease in density over time), would change. This explains how a person\ncan guess where they are even when blindfolded, or how the blind can learn\nto navigate with only a cane to aid them. Sound provides us with a lotof\ninformation about our surroundings!\nThe term reverbis used to describe the quality of a sound in terms of\nits wet components. In the early days of audio recording, sound engineers\nhad little control over reverb, relying entirely on the shape and construction\nof the room in which the recording was made. Later, simple artificial re-\nverb devices were created, from the use of a speaker and microphone in a\nbathroom by Bill Putman Sr. (founder of Universal Audio), to the use of a\nlong metal plate or spring to introduce a delay in a sound signal, to modern\ndigital techniques. Today, digital signal processor (DSP) chips and/or soft-\nware are used not only to recreate natural reverb effects in recorded sound\neffects and music, but also to augment recordings with all sorts of interest-\ning effects that are not normally heard in nature. We’ll learn more about\ndigital signal processing in Section 14.2. You can read more about reverb at\nhttp://www.uaudio.com/blog/the-basics-of-reverb.\nAnanechoic chamber is a room especially designed to entirely eliminate re-\nflectedsoundwaves. Thisisaccomplishedbyliningthewalls,floorandceiling\noftheroomwiththickcorrugatedfoampaddingthatabsorbsessentiallyallof\nthe reflected sound waves. As a result, only the direct (dry) sound reaches\nthe listener or microphone. Sound in an anechoic chamber has a completely\n“dead” timbre. Anechoic chambers are useful for recording “pure” sounds\nthat contain no reverb. Such pure sounds are often perfect candidates for in-\nput into a digital signal processing pipeline, giving a sound designer maxi-\nmum flexibility to control the timbre of the sound.\n14.1.3.5 Sound in Motion: The Doppler Effect\nIf you’ve ever stood at a railway crossing when a train goes by, you’ve heard\ntheDopplereffect inaction. Thesoundofthetrainseemshigherpitchedwhenit\nisapproachingyou,andbecomeslowerpitchedasitracesoffintothedistance.\nSoundwavestravelataroughlyconstantspeedthroughtheair,butthesound\nsource(inthiscase,thetrain)isalsomoving. Thesoundwavesthataremoving\nin the same direction as the train become “squashed together,” and the waves\nthataremovingoppositetothemotionofthetrainbecome“spreadout,” each\nby an amount proportional to the difference between the speed of sound in\nair and the speed of the train through the air. The frequency of the squashed\nwavesisthereforeincreased,becausethespacebetweenthepeaksandtroughs\n14.1. The Physics of Sound 923\nofthesoundwaveshasbeeneffectivelyreduced, resultinginahigher-pitched\nsound. Likewise,thefrequencyofthespread-outwavesisdecreased,resulting\nin a lower-pitched sound. The Doppler effect was named after the Austrian\nphysicist Christian Doppler, who identified it in 1842.\nThe Doppler effect also occurs when the listener is moving but the sound\nsource is stationary. In general, the Doppler shift is dependent upon the rel-\native velocity (as a vector) between the listener and the sound source. In one\ndimension, the Doppler shift amounts to a change in frequency, and can be\nquantified as follows:\nf′=(c+vl\nc+vs)\nf,\nwhere fis the original frequency, f′is the Doppler-shifted (observed) fre-\nquency at the listener, cis the speed of sound in air and vlandvsare the\nspeeds of the listener and sound source, respectively. If the speeds of the\nsound sources are very small relative to the speed of sound, we can approxi-\nmate this relationship as follows:\nf′=(1+ (vl vs)\nc)\nf\n=(1+∆v\nc)\nf.\nThis expression makes the relative velocity, ∆v, apparent. The Doppler effect\ncan be easily visualized by looking at the following animated GIF: http://en.\nwikipedia.org/wiki/File:Dopplereffectsourcemovingrightatmach0.7.gif .\n14.1.4 Perception of Position\nThe human auditory system has evolved to allow a reasonably accurate per-\nception of the position of sounds in the space around us. A number of factors\ncontribute to our perception of sound position:\n•Fall-off with distance provides us with a rough idea of how far away the\nsource of a sound is. In order for this to work, we must have some idea\nof the loudness of the sound when heard at close range to serve as a\n“baseline.”\n•Atmospheric absorption causes the higher frequencies in a sound to drop\nout as the source moves farther away from the listener. This can serve\nas an important cue in perceiving the difference between, for example, a\nperson speaking at normal volume but far away and a person speaking\nat reduced volume up close.",26289
97-14.2 The Mathematics of Sound.pdf,97-14.2 The Mathematics of Sound,"924 14. Audio\n•Having two ears, one on the left and one on the right, gives us a great\ndeal of positional information. A sound that is to our right will sound\nlouderintherightearthanintheleft. An interauraltimedifference (ITD)of\napproximately one millisecond also arises, because a sound to one side\nof your head will take just a little bit longer to reach the opposite ear.\nFinally, theheaditselfobstructssounds, sotheearoppositetothesound\nsourcewill perceiveaslightly muffled version ofthe sound reachingthe\nnear ear. This is known as interaural intensity difference (IID).\n•Ear shape has an effect as well. Our ears are cupped slightly forward, so\nsoundscomingfrombehindusareveryslightlymuffledrelativetothose\ncoming from in front of us.\n•The head-related transfer function (HRTF) is a mathematical model of the\nminuteeffectsthatthefoldsofourears(the pinnae)haveonsoundscom-\ning from different directions.\n14.2 The Mathematics of Sound\nSignalprocessingandsystemstheoryistheareaofmathematicsthatliesatthe\nheart of virtually all modern audio technology. It is also extensively used in a\nwide variety of other technological and engineering endeavors, including im-\nage processing and machine vision, aeronautics, electronics, fluid dynamics,\nand the list goes on. In this section, we’ll embark on a whirlwind tour of the\nkey concepts in signals and systems theory, because it will help us to under-\nstand some of the more advanced topics in game audio later in the chapter.\n(It’s also an important area of mathematical theory that can benefit any game\nprogrammer—so what the heck!) An in-depth treatment of the topic can be\nobtained from [41].\n14.2.1 Signals\nAsignalis any function of one or more independent variables, typically de-\nscribing the behavior of some kind of physical phenomenon. In Section 14.1,\nwe used the signal p(t)to represent the time-varying acoustic pressure of an\naudio compression wave. Of course, many other kinds of signals are possi-\nble. Asignal v(t)mightrepresentthevoltageproducedbyamicrophoneover\ntime, while w(t)might model the time-varying water pressure in a system of\npipes, or we could use f(t)to represent the varying population of foxes in an\necosystem.\nIn studying signal theory, we often refer to the independent variable as\n“time”andrepresentitwiththesymbol t—butofcoursetheindependentvari-\n14.2. The Mathematics of Sound 925\nable might represent some other quantity, and there may be more than one in-\ndependent variable. For example, one can think of a 2D greyscale image as a\nsignal i(x,y), where the two independent variables, xandy, represent the or-\nthogonal coordinate axes, and the signal value irepresents the intensity of the\ngreyscale image at each pixel. A color image could be similarly represented\nby three signals, r(x,y)for the red channel, g(x,y)for the green channel and\nb(x,y)for the blue channel.\n14.2.1.1 Be Discrete, Continuously\nThe2Dimageexamplesabovebringtolightanimportantdistinctionbetween\ntwo fundamental kinds of signal: continuous anddiscrete.\n• If the independent variable is a real number (t2R), we call the signal a\ncontinuous-timesignal. Inthischapter, we’llusethesymbol ttorepresent\ncontinuous “time,” and we’ll use round parentheses for our functional\nnotation (e.g., x(t)) to remind us that we’re working with a continuous-\ntime signal.\n• If the independent variable(s) is an integer(n2I), we call the signal a\ndiscrete-time signal . We’ll use the symbol nto represent discrete “time,”\nand we’ll use square brackets for our functional notation (e.g., x[n]) to\nremind us that we’re working with a discrete-time signal. Note that the\nvalueof a discrete-time signal might still be a real number ( x[n]2R)—\nthe only thing the term “discrete-time signal” prescribes is that the inde-\npendent variable is an integer ( n2I).\nInFigure 14.1, wesawthatwecanvisualizeacontinuous-timesignalasanor-\ndinary function plot, with time ton the horizontal axis and the signal value\np(t)on the vertical axis. We can plot a discrete-time signal x[n]in similar\nfashion, although the function’s values are only defined for integer values of\nthe independent variable n(see Figure 14.7). One common way to think of a\ndiscrete-time signal is as a sampled version of a continuous-time signal. The\nsampling process (also known as digitization oranalog-to-digitalconversion ) lies\nat the heart of digital audio recording and playback. See Section 14.3.2.1 for\nmore information on sampling.\n14.2.2 Manipulating Signals\nIt will be important in the following discussions for us to understand some\nbasic ways to manipulate a signal by making changes to its independent vari-\nable. For example, to reflecta signal about t=0, we simply replace twith t\nin the signal’s equation. To time-shift the entire signal to the right(i.e., in the\n926 14. Audio\npositive direction) by a distance s, we replace twith t sin the signal’s equa-\ntion. (Time shifting to the left/negative direction is accomplished by replacing\ntwith t+s.) Wecanalsoexpandorcompressthedomainofthesignalbyscal-\ning the independent variable. These simple transformations are illustrated in\nFigure 14.8.\n14.2.3 Linear Time-Invariant (LTI) Systems\nIn the context of signal processing theory, a systemis defined as any device or\nprocess that transforms an inputsignal into a new outputsignal. The math-\nematical concept of a system can be used to describe, analyze and manipu-\nlate many real-world systems that arise in audio processing, including micro-\nphones, speakers, analog-to-digital converters, reverb units, equalizers and\nfilters and even the acoustics of a room.\nAs a simple example, an amplifier is a system that increases the amplitude\nof its input signal by a factor Aknown as the gainof the amp. Given an in-\nput signal x(t), such an amplification system would produce an output signal\ny(t) =Ax(t).\nAtime-invariant system is one for which a time shift in the input signal\ncauses an equal time shift in the output signal. In other words, the behavior\nof the system does not change over time.\nAlinearsystem is one that possesses the property of superposition. This\nmeans that if an input signal consists of a weighted sum of other signals, then\nthe output is a weighted sum of the individual outputs that would have been\nproduced, had each of the other signals been fed throughthe system indepen-\ndently.\nLinear time-invariant (LTI) systems are extremely useful for two reasons.\nx[n]\nn\nFigure 14.7. The value of a discrete-time signal x[n]is deﬁned only for integer values of n.\n14.2. The Mathematics of Sound 927\nx(t)\ntx(–t)\nt\nx(t – s )\ntx(2t)\nts\nFigure 14.8. Simple manipulations of a signal’s independent variable.\nFirst, their behaviors are well understood and relatively easy to work with\nmathematically. Second, many real physical systems in the fields of audio\npropagation theory, electronics, mechanics, fluid flow, etc. can be modeled ac-\ncurately using LTI systems. As such, we will restrict ourselves to a discussion\nof LTI systems for our purposes of understanding audio technology.\nFigure 14.9. A system as\na black box.We can visualize any system as a black box with an input signal and an\noutput signal, as shown in Figure 14.9.\nUsing this black-box notation, simple systems can be conveniently inter-\nconnected to construct more complex systems. For example:\n• The output of system A could be connected to the input of system B,\nyielding a composite system that performs operation A followed by op-\neration B. This is called a serialconnection.\n• The outputs of two systems could be added together.\n• The output of a system could be fed back into an earlier input, yielding\nwhat is known as a feedback loop.\nSee Figure 14.10 for examples of all of these kinds of connections.\nOne very important property of all LTI systems is that their interconnec-\ntions are order-independent . So if we have a serial connection of system A fol-\nlowedbysystemB,wecanreversetheorderofthetwosystemsandtheoutput\nwill remain unchanged.\n928 14. Audio\n14.2.4 Impulse Response of an LTI System\nIt’s all fine and dandy to talk about systems that convert an input signal into\nan output signal, and it’s even pretty intuitive to draw diagrams of system\ninterconnections. But how can we describe the operation of a system mathe-\nmatically?\nRecall from Section 14.2.3 that for a linearsystem, if the input consists of a\nlinearcombination(weightedsum)ofinputsignals,theoutputwillbealinear\ncombination (weighted sum) of the individual outputs (had each of the input\nsignalsbeenfedintothesystemindependently). So,ifwecanfigureoutaway\ntorepresentanarbitraryinputsignalasaweightedsumofverysimplesignals,\nweshouldbeable todescribethebehaviorof thesystembydescribing onlyits\nresponse to those very simple signals.\n14.2.4.1 The Unit Impulse\nIf we are going to describe an input signal as a linear combination of simple\nsignals, the question arises: Which simple signal shall we use? For reasons\nthat will become clear in a moment, our signal of choice is going to be the unit\nimpulse. Thissignalisoneofafamilyofrelatedfunctionsknownas singularity\nfunctions because they all contain at least one discontinuity or “singularity.”\nIn discrete time, the unit impulse d[n]is as simple as it gets: It is a signal\nwhose value is zero everywhere except at n=0, where its value is one:\nd[n] ={\n1ifn=0,\n0otherwise.\nThediscrete-time unit impulse is illustrated in Figure 14.11.\nSerial\nAx(t)\nBy(t)\nFeedback Loop\ny(t) x(t)\n+\n–ad\ndt\ny(t)Parallel\ny(t)A\nB+x(t)a\nb\nFigure 14.10. Various ways to interconnect systems. In the serial connection, y(t) =B(A(x(t))) . In\nthe parallel connection, y(t) =aA(x(t)) +bB(x(t)) . In the feedback loop, y(t) =x(t) a˙y(t).\n14.2. The Mathematics of Sound 929\nIn continuous time, the unit impulse d(t)is a bit trickier to define. It is a\nfunction whose value is zero everywhere except at t=0, where its value is\ninfinite—but the areaunder the curve is equal to one.\nTo see how such a strange beast of a function might be formally defined,\nimagine a “box” function b(t), whose value is zero everwhere except in the\ninterval [0,T), where its value is 1/T. The area under this curve is just the\narea of the box, width times height, or T1\nT=1. Now imagine the limit\nasT!0. As this happens, the width of the box approaches zero and its\nheight approaches infinity, but its area remains equal to 1. This is shown in\nFigure 14.12.\nThe unit impulse function is typically denoted by the symbol d(t). It can\nbe formally defined as follows:\nd(t) = lim\nT!0b(t),\nwhere\nb(t) ={1/Tift0andt<T,\n0otherwise.\nAs shown in Figure 14.13, we typically plot the unit impulse by drawing\nan arrow whose height represents the area under the curve (since the actual\n“height” of the function at t=0is infinite).\n14.2.4.2 Using an Impulse Train to Represent a Signal\nNow that we know what the unit impulse signal is, let’s see if we can describe\nanarbitrarysignal x[n]asalinearcombinationofunitimpulses. (Spoileralert:\nIt turns out we can.)\nThefunction d[n k]isatime-shifteddiscreteunitimpulse,whosevalueis\nzeroeverywhereexceptattime n=k,whereitisequaltoone. Inotherwords,\ntheunitimpulse d[n k]is“positioned”attime k. Consideranimpulseatone\nparticular value of k(say, k=3). Let’s make sure that the “height” of that\nimpulse matches the value of the original function at k=3by “scaling” the\nimpulse by x[3], yielding x[3]d[n 3]. If we rinse and repeat for all possible\nFigure 14.11. The unit impulse in discrete time.\n930 14. Audio\nbt\nt T1/T\nt bt\nT\nFigure 14.12. The unit impulse can be deﬁned as the limit of a\nbox function b(t)whose width approaches zero.\nFigure 14.13. The value of the unit impulse function d(t) is\nzero everywhere except at t=0, where it is inﬁnite. It is\ndrawn as an arrow of unit height to indicate that the area\nunder the curve is 1.\nvaluesof k,wegetatrainofimpulsesoftheform x[k]d[n k]. Addingallthese\nscaled, time-shifted impulse functions together is just another way of writing\nthe original signal x[n]:\nx[n] =+¥\nå\nk= ¥x[k]d[n k]. (14.4)\nWe won’t give a rigorous proof here, but it’s probably not too hard to be-\nlieve that doing this in continuous time works in pretty much the same way.\nThe only difficulty is that for continuous time, the sum in Equation (14.4) be-\ncomes an integral. Let’s imagine an infinite sequence of time-shifted unit im-\npulses d(t t),eachonelocatedatadifferenttime t. Wecanbuildupanarbi-\ntrary signal x(t)in an analogous fashion to the discrete-time case, as follows:\nx(t) =∫+¥\nt= ¥x(t)d(t t)dt. (14.5)\n14.2.4.3 Convolution\nEquation (14.4) tells us how to represent a signal x[n]as alinear combination\nof simple, time-shifted unit impulse signals d[n k]. Let’s imagine putting\njustoneof these weighted impulse inputs ( x[k]d[n k]) through the system. It\ndoesn’tmatterwhichonewechoose, solet’sselecttheoneat k=0. Thisgives\nus the input signal x[0]d[n].\nWe’ll use the notation x[n] =)y[n]to indicate that an input signal x[n]\nis being transformed by an LTI system into an output signal y[n]. So we can\nwrite:\nx[0]d[n] =)y[n].\n14.2. The Mathematics of Sound 931\nh[n]\nn n\n[n]\nh(t)\nt t\n(t)\nFigure 14.14. Examples of the impulse response of a system in discrete and continuous time.\nThe value of x[0]is just a constant, so because we’re dealing with a linear sys-\ntem,theoutput y[n]willjustbethatsameconstanttimesthesystem’sresponse\nto the unit impulse d[n]. Let’s use the signal h[n]to represent the system’s re-\nsponse to a “bare” unit impulse: d[n] =)h[n]. The signal h[n]is called the\nimpulse response of the system. So we can write the system’s response to our\nsimple input signal as follows:\nx[0]d[n] =)x[0]h[n].\nThe concept of impulse response is illustrated in Figure 14.14.\nThe response of an LTI system to a time-shifted unit impulse is just a time-\nshifted impulse response ( d[n k] =)h[n k]). So for values of kother than\nzero, everything works out exactly the same except that now the input and\noutput signals are both time-shifted byk:\nx[k]d[n k] =)x[k]h[n k].\nTo find the system’s response to the entire input signal x[n], we just sum\nup the responses to each individual time-shifted component, like this:\n+¥\nå\nk= ¥x[k]d[n k] =)+¥\nå\nk= ¥x[k]h[n k].\nIn other words, the output of our system can be written as follows:\ny[n] =+¥\nå\nk= ¥x[k]h[n k]. (14.6)\nThis very important equation is known as the convolution sum. It’s conve-\nnient to introduce a new mathematical operator to represent the operation\n932 14. Audio\nof convolution:\nx[n]h[n] =+¥\nå\nk= ¥x[k]h[n k]. (14.7)\nEquations (14.6) and (14.7) give us a way to calculate an LTI system’s re-\nsponse y[n]toanyarbitraryinputsignal x[n],givenonlythe impulseresponse of\nthe system, h[n]. In other words, for LTI systems, the impulse response signal\nh[n]completelydescribes the system . Pretty cool stuff.\nConvolution in Continuous Time\nInourdiscussionsabove,weworkedindiscretetimetokeepthingssimple. In\ncontinuoustime,everythingworksoutinprettymuchthesameway. Theonly\ndifference is that summations become integrals, and we need to remember to\ninclude the differential dtin our equations.\nWhen we apply an arbitrary signal x(t)to the input of a continuous-time\nLTI system, the output signal can be written as follows:\ny(t) =∫+¥\nt= ¥x(t)h(t t)dt. (14.8)\nAs before, we’ll use the operator as a shorthand for convolution:\nx(t)h(t) =∫+¥\nt= ¥x(t)h(t t)dt. (14.9)\nAnalogous to the convolution sum, the integral in Equations (14.8) and (14.9)\nis known as the convolutionintegral.\n14.2.4.4 Visualizing Convolution\nIntegrate to find area \nunder this curve.x(\n)\nh(t–\n)\nth(\n)\nx(\n)h(t–\n)\nFigure 14.15. Visualization\nof the convolution oper-\nation in continuous time.Let’strytovisualizetheconvolutionoperationinthecontinuoustimecase. To\nevaluate y(t) =x(t)h(t)for one specific value of t(say, t=4), we perform\nthe following steps as illustrated in Figure 14.15:\n1. Plot x(t), using tas the time variable because tis fixed (at t=4in this\nexample).\n2. Plot h(t t). We can rewrite this as h( t+t). Because tis negated,\nwe know that the impulse response has been flipped about t=0. And\nbecause we’ve added tto the independent variable, we know the signal\nhas been shifted to the leftbyt=4units.\n3. Multiply the two signals together across the entire taxis.\n14.2. The Mathematics of Sound 933\n4. Integrate from  ¥to+¥along the taxis to find the area under the\nresulting curve. This is the value of y(t)at this one specific value of t(in\nthis example, t=4).\nRemember that we must repeat this procedure for every possible value of tin\norder to determine the complete output signal y(t).\n14.2.4.5 Some Properties of Convolution\nThe properties of the convolution operation are surprisingly analogous to\nthose of ordinary multiplication. Convolution is:\n•commutative: x(t)h(t) =h(t)x(t);\n•associative: x(t)(\nh1(t)h2(t))=(\nx(t)h1(t))h2(t); and\n•distributive: x(t)(\nh1(t) +h2(t))=(\nx(t)h1(t))+(\nx(t)h2(t))\n.\n14.2.5 The Frequency Domain and the Fourier Transform\nInordertoarriveattheconceptsofimpulseresponseandconvolution, wede-\nscribed a signal as a weighted sum of unit impulses. We can also represent\na signal as a weighted sum of sinusoids. Representing a signal in this man-\nner essentially breaks it up into its frequency components. This will allow us to\nderive another incredibly powerful mathematical tool—the Fouriertransform.\n14.2.5.1 The Sinusoidal Signal\nA sinusoidal signal is produced when a circular motion in two dimensions is\nprojectedontoasingleaxis. Anaudiosignalintheformofasinusoidproduces\na “pure” tone at one specific frequency.\nThe most basic sinusoidal signal is the sine (or cosine) function. The signal\nx(t) = sinttakes on the value 0 at t=0,pand 2p, has avalue of 1 at t=p\n2\nand has a value of  1att=3p\n2.\nThe most general form of a real-valued sinusoidal signal is\nx(t) =Acos(w0t+ϕ). (14.10)\nHere, Arepresents the amplitude of the sine wave (i.e., the peaks and troughs\nof the cosine wave hit maximum and minimum values of Aand A, respec-\ntively). The angular frequency isw0, measured in radians/second (see Section\n14.1.1 for a discussion of frequency and angular frequency). ϕrepresents a\nphase offset (also measured in radians) that shifts the cosine wave to the left or\nright along the time axis.\n934 14. Audio\nWhen A=1,w0=1andϕ=0, Equation (14.10) reduces to x(t) = cost.\nWhen ϕ=p\n2,theexpressionbecomes x(t) = sint. The cosfunctionrepresents\ntheprojectionofacircularmotionontothehorizontalaxis,while sinrepresents\nits projection onto the vertical axis.\n14.2.5.2 The Complex Exponential Signal\nThe cosine function isn’t actually the best tool for representing a signal as a\nsum of sinusoids. The math is much simpler and more elegant if we make use\nofcomplex numbers instead. In order to understand how this works, we need\nto review complex math, and take a look at how multiplication of complex\nnumbersworks. Sobearwithmehere—allwillbecomeclearbythetimewe’re\ndone.\nA Brief Review of Complex Numbers\nYou’llprobablyrememberfromhigh-schoolmathclassthatacomplexnumber\nis a kind of two-dimensional quantity consisting of a real part and an imagi-\nnary part. Any complex number can be written as follows: c=a+jb, where\naandbare real numbers and j=p 1is theimaginary unit . The real part of c\nisa=Re(c), and its imaginary part is b=Im(c).\nYou can visualize a complex number as a kind of “vector” [a,b]in a two-\ndimensional space known as the Argand plane. It’s important to remember,\nhowever, that complex numbers and vectors are notinterchangeable—their\nmathematical behaviors are quite different.\nWe define the magnitude of a complex number as the length of its 2D “vec-\ntor”representationinthecomplexplane: jcj=p\na2+b2. Theanglethevector\nmakes with the real axis is known as its argument: argc=tan 1(b/a). (The\nargument of a complex number is sometimes called its phase. As we’ll see, the\nterm “phase” is closely related to the phase offset ϕin Equation (14.10).) The\nmagnitude and argument of a complex number are depicted in Figure 14.16.\nFigure 14.16. The magnitude jcj=p\na2+b2of a complex number is its length in the complex plane,\nand its argument argc=tan 1(b/a)is the angle it makes with the Re axis.\n14.2. The Mathematics of Sound 935\nComplex Multiplication and Rotation\nWe won’t get into all of the properties of complex numbers here. Check out\nhttp://www.math.wisc.edu/ angenent/Free-Lecture-Notes/freecomplexnu\nmbers.pdf for an in-depth discussion of complex number theory. However,\nthere is one mathematical operation that does concern us here: the operation\nof complex multiplication .\nComplex numbers are multiplied algebraically (no dot or cross products\nhere):\nc1c2= (a1+jb1)(a2+jb2)\n= (a1a2) +j(a1b2+a2b1) +j2b1b2\n= (a1a2 b1b2) +j(a1b2+a2b1). (14.11)\nIf you work out2the magnitude and argument (angle) of the product c1c2,\nyou’ll find that the magnitude is equal to the product of the two input magni-\ntudes, and the argument is the sumof the input arguments:\njc1c2j=jc1jjc2j;\narg(c1c2) = argc1+argc2. (14.12)\nThe fact that multiplication of complex numbers causes their angles (ar-\nguments) to addmeans that complex multiplication produces a rotation in the\ncomplex plane. If the magnitude of c1is unity (jc1j=1), then the magnitude\nof the pr oduct will be equal to the magnitude of c2(jc1c2j=jc2j). In this case,\nthe product represents a purerotation ofc2by an angle equal to argc1(see Fig-\nure 14.17). Ifjc1j̸=1, then the product’s magnitude will be scaledbyjc1j, and\nthe result is that c2undergoes a spiral motion in the complex plane.\nThisexplainswhy unit-length quaternionsoperateasrotationsin3Dspace!\nAquaternionisessentiallyafour-dimensionalcomplexnumber, withonereal\npartandthreeimaginaryparts. Soaquaternionfollowsthesamebasicrulesin\nthree dimensions that a regular complex number follows in two dimensions.\nThefactthatcomplexmultiplicationproducesarotationmakessensewhen\nwe consider what happens when we multiply jby itself many times:\n1j=j,\njj=p\n 1p\n 1= 1,\n 1j= j,\n jj=1,\n. . .\n2Yikes—this sounds an awful lot like an exercise for the reader…\n936 14. Audio\nIm\nRearg c= 30\n1arg c= 80\n2 c1c2\n+1 –1+j\n–jc c1   2\narg c c= 110\n= 30\n  + 80\n1   2\n|c | = |c | = 11                2\nFigure 14.17. Multiplying two complex numbers together that both have magnitudes of 1 produces\napure rotation in the complex plane.\n0 21\n3\nFigure 14.18. Multiplying the imaginary number j=p 1 by itself acts like rotating a unit vector by\n90 degrees in the complex plane.\nSomultiplying jbyitselfislike rotating aunitvectorby90degreesinthecom-\nplex plane. In fact, multiplying anycomplex number by jhas the effect of\nrotating it by 90 degrees. This is illustrated in Figure 14.18.\nThe Complex Exponential and Euler’s Formula\nFor any complex number cwithjcj=1, the function f(n) =cn, with ntaking\nona sequence ofincreasingpositive realvalues, willtrace out a circularpath in\n14.2. The Mathematics of Sound 937\nIm\nRe+1 –1+j\n–j|c|= 1c5\nc4\nc3\nc\nnIm(c) = sin( n arg c)n\nc2\nFigure 14.19. Multiplying a complex number by itself repeatedly traces out a circular path in the\nArgand plane, producing a sinusoid when projected onto any axis through the origin.\nthecomplexplane. Anycircularpathintwodimensionstracesoutasinecurve\nalong the vertical axis, and a corresponding cosine curve along the horizontal\naxis. This is illustrated in Figure 14.19.\nRaisinga complex numbertoa realpower( cn)producesrotationinthecom-\nplex plane, and therefore yields a sinusoid when projected onto any axis in\nthe plane. As it turns out, we can also get this rotational effect by raising a\nrealnumber to a complex power ( nc). This means that we can write Equation\n(14.10) in terms of complex numbers as follows:\nejw0t=cosw0t+jsinw0t,t2R; (14.13)\nRe[\nejw0t]\n=cosw0t;\nIm[\nejw0t]\n=sinw0t,\nwhere e2.71828 is the real transcendental number that defines the base of\nthe natural logarithm function.\nEquation(14.13)isoneofthemostimportantequationsinallofmathemat-\nics. It is known as Euler’s formula. Why it works is a bit of a mystery (even to\nsome seasoned mathematicians). The theorem can be explained by looking at\nthe Taylor series expansion of ejt, or by considering the derivative of exand\nthen allowing xto become a complex number. But for our purposes, it should\nsuffice to rely on the intuitions we gained from looking at how complex mul-\ntiplication results in rotation in the complex plane.\n938 14. Audio\n14.2.5.3 The Fourier Series\nNow that we have the mathematical tools we need to represent sinusoids as\ncomplex numbers, let’s turn our attention again to the task of representing a\nsignal as a sum of sinusoids.\nDoing this is easiest when the signal is periodic. In this case, we can write\nthe signal as a sum of harmonicallyrelated sinusoids:\nx(t) =+¥\nå\nk= ¥akej(kw0)t. (14.14)\nWe call this the Fourier series representation of the signal. Here, the complex\nexponential functions ej(kw0)tare the sinusoidal components from which we\nare building up the signal. These components are harmonically related, in that\neachonehasafrequencythatisanintegermultiple koftheso-called fundamen-\ntal frequency w0. The coefficients akrepresent the “amount” of each harmonic\npresent in the signal x(t).\n14.2.5.4 The Fourier Transform\nA full explanation of this topic is beyond the scope of this book, but for our\npurposes it will suffice to state (without any proof whatsoever!) that anyrea-\nsonably well-behaved signal,3even signals that are non-periodic, can be repre-\nsentedasalinearcombinationofsinusoids. Ingeneral,anarbitrarysignalmay\ncontain components at anyfrequency, not just frequencies that are harmoni-\ncally related. As such, the discrete set of harmonic coefficients akin Equation\n(14.14) becomes a continuum of values representing “how much” of each fre-\nquency the signal contains.\nWe can envision a new function X(w)whose independent variable is the\nfrequency wratherthantime t,andwhosevaluerepresentstheamountofeach\nfrequencypresentintheoriginalsignal x(t). Wesaythat x(t)isthetimedomain\nrepresentation of the signal, while X(w)is itsfrequencydomain representation.\nMathematically, we can find the frequency domain representation of a sig-\nnal from its time domain representation, and vice versa, by using the Fourier\ntransform:\nX(w) =∫+¥\n ¥x(t)e jwtdt; (14.15)\nx(t) =1\n2p∫+¥\n ¥X(w)ejwtdw. (14.16)\n3All signals that meet the so-called Dirichlet conditions have Fourier transforms and are there-\nfore “reasonably well-behaved” for our purposes.\n14.2. The Mathematics of Sound 939\nx(t) = e–at\n|X(\n)|= \na2+ \n21t\narg X(\n) = tan–1\n a\nFrequency Domain \n(Bode Plot) Time Domain\nFigure 14.20. The Fourier transform yields a complex-valued frequency domain signal. A Bode\nplot is used to visualize this complex-valued signal in terms of its magnitude and its phase (or\nargument).\nIf you compare Equation (14.16) to the Fourier series from Equation (14.14),\nyou can see the similarity. Rather than describing the “amounts” of the fre-\nquency components via a discrete series of coefficients ak, we’re now describ-\ning them using the continuous function X(w). But in both cases we’re repre-\nsenting x(t)as a “sum” of sinusoids.\n14.2.5.5 Bode Plots\nIn general the Fourier transform of a real-valued signal is a complex-valued sig-\nnal (X(w)2C). When visualizing the Fourier transform, we often draw it\nusing two plots. For example, we might plot its real and imaginary compo-\nnents. Or we might plot its magnitude and its argument (angle) on two dif-\nferent plots—a visualization known as a Bode plot (pronounced “Boh-dee”).\nFigure 14.20 shows an example of a signal and its Bode plot.\n14.2.5.6 The Fast Fourier Transform (FFT)\nA collection of fast algorithms exist for calculating the Fourier transform in\ndiscretetime. Thisfamilyofalgorithmsiscalled, aptlyenough, the fastFourier\ntransform orFFT.YoucanreadmoreabouttheFFTathttp://en.wikipedia.org/\nwiki/Fast_Fourier_transform.\n940 14. Audio\n14.2.5.7 Fourier Transforms and Convolution\nIt is interesting to note that convolution in the time domain corresponds to\nmultiplicationinthefrequencydomainandviceversa. Givenasystemwhose\nimpulse response is h(t), we know that we can find the output of the system\ny(t)in response to an input x(t)as follows:\ny(t) =x(t)h(t).\nInthefrequencydomain,giventheFouriertransformsoftheimpulseresponse\nH(w)and the input X(w), we can find the Fourier transform of the output as\nfollows:\nY(w) =X(w)H(w).\nThis result is pretty incredible, and it’s also very handy. Sometimes it is more\nconvenient to perform a convolution on the time axis using a system’s im-\npulse response h(t), while at other times it’s more convenient to perform a\nmultiplication in the frequency domain using the system’s frequency response\nH(w).\nAs it turns out, LTI systems exhibit a property called duality, which says\nthat you can reverse the roles of time and frequency and virtually the same\nmathematical rules continue to apply. So, for example, we can understand\nhow signal modulation (the multiplication of one signal by another) works in\nthe time domain by looking at what happens when we convolve the Fourier\ntransformsofthetwosignalsonthefrequencyaxis. Havingtwowaysoftack-\nling a problem is always better than one!\n14.2.5.8 Filtering\nThe Fourier transform allows us to visualize the set of frequencies that make\nup virtually any audio signal. A filteris an LTI system that attenuates a se-\nlectedrangeofinputfrequencieswhileleavingallotherfrequenciesunaltered.\nAlow-pass filter retains low frequencies while attenuating high frequencies. A\nhigh-pass filter does the opposite, retaining high frequencies and attenuating\nlower frequencies. A band-pass filter attenuates both low and high frequencies\nbut retains frequencies within a limited passband . Anotchfilter does the oppo-\nsite, retaining low and high frequencies but attenuating frequencies within a\nlimitedstopband.\nFilters are used in a stereo system’s equalizer, by attenuating or boosting\nspecific frequencies based on user inputs. Filters can also be used to attenuate\nnoise,ifthespectraofthenoisesignalandthedesirablesignaloccupydifferent\nregions of the frequency axis. For example, if a high-frequency noise signal is",30530
98-14.3 The Technology of Sound.pdf,98-14.3 The Technology of Sound,"14.3. The Technology of Sound 941\nc\n c\nFigure 14.21. The frequency response H(w) for an ideal ﬁlter has a value of one in the passband\nand zero in the stopband.\nadversely affecting a lower-frequency voice or music signal, a low-pass filter\ncould be used to eliminate the noise.\nThefrequencyresponse H(w)ofanidealfilterlookslikearectangularbox,\nwith a value of one in the passband and zero in the stopband. When we\nmultiply this by the Fourier transform of our input signal X(w), the output\nY(w) =X(w)H(w)will have its passband frequencies preserved exactly, and\nits stopband frequencies all set to zero. The frequency response for an ideal\nfilter is shown in Figure 14.21.\nOf course, an ideal filter that completely passes certain frequencies and\ncompletely suppresses others may not be desirable. The frequency responses\nof most real-world filters have a gradual fall-off between the passband and\nstopband. This aids filtering in situations where there is no single, clear-cut\nline between the desirable frequencies and the unwanted frequencies. The\nfrequency response of a low-pass filter with a gradual fall-off is shown in Fig-\nure 14.22.\nTheequalizer (EQ) found on most high-fidelity audio equipment permits\nthe user to adjust the amount of bass, mid-range and treble that is output. An\nEQ is really just a collection of filters tuned to different frequency ranges and\napplied in series to an audio signal.\nFiltering theory is an immense field of study, so we can’t possibly do it\njustice here. For a great deal more information, see [41, Chapter 6].\n14.3 The Technology of Sound\nBefore we can fully understand the software that comprises a game’s audio\nengine, we need a firm grasp of audio hardware and technology and of the\nterminology used by industry professionals to describe it.\n942 14. Audio\n14.3.1 Analog Audio Technology\nThe earliest audio hardware was based on analog electronics. This was the\neasiest way to record, manipulate and play back audio compression waves,\nbecause sound is itself an analog physical phenomenon. In this section, we’ll\nbriefly explore some key analog audio technologies.\n14.3.1.1 Microphones\nAmicrophone(alsoknownasa“mic”or“mike”)isatransducerthatconverts\nan audio compression wave into an electronic signal. Microphones make use\nof various technologies in order to convert the mechanical pressure variations\nof a sound wave into an equivalent signal based on variations in electric volt-\nage. Adynamic microphone uses electromagnetic induction, while a condenser\nmicrophone utilizes changes in capacitance. Other types of mics use piezoelec-\ntric generation or light modulation to produce a voltage signal.\nDifferent microphones have different sensitivity patterns, known as polar\npatterns. These patterns describe how sensitive the mic is to sound at various\nangles about its central axis. An omnidirectional mic is equally sensitive in all\ndirections. A bidirectional mic has two sensitivity “lobes” in the shape of a\nfigure eight. A cardioid mic has essentially a unidirectional sensitivity profile,\nsonamedbecauseofitssomewhatheart-shapedpolarpattern. Somecommon\narg H(\n)20 log  |H(\n)|10\n0 dB\n–20 dB\n–40 dB\n0\n–\n/4\n–\n/20.1/RC 1/RC 10/RC 100/RC\n0.1/RC 1/RC 10/RC 100/RC\nFigure 14.22. The frequency response H(w) for an RC (resistor-capacitor) low-pass ﬁlter with a\ngradual fall-off. Both the horizontal and vertical axes of both plots are drawn using a logarithmic\nscale.\n14.3. The Technology of Sound 943\n270\n 90\n0\n180\n–25 dB–20 dB–15 dB–10 dB–5 dB270\n 90\n0\n180\n–25 dB–20 dB–15 dB–10 dB–5 dB\n270\n 90\n0\n180\n–25 dB–20 dB–15 dB–10 dB–5 dB\nFigure 14.23. Three typical microphone polar patterns, clockwise from upper left: omnidirectional, cardioid and bidirectional.\nmicrophone polar patterns are illustrated in Figure 14.23.\n14.3.1.2 Speakers\nAspeakerisbasicallyamicrophoneoperatedinreverse—itisatransducerthat\nconverts a varying input voltage signal into vibrations in a membrane, which\ninturngivesrisetoairpressurevariationsthatresultinasoundpressurewave.\n944 14. Audio\n14.3.1.3 Speaker Layouts: Stereo\nSound systems usually support multiple speaker output channels. A stereo\ndevice such as an iPod, the sound system in your car or your grandpa’s port-\nable “boom box” supports at least two speakers for the left and right stereo\nchannels. Some high-fidelity stereo systems also boast two additional “tweet-\ners”—tiny speakers that are capable of reproducing the highest-frequency\nsounds within the left and right channels. This allows the two main speak-\ners to be larger, and therefore better at covering the bass. Some stereo systems\nalso support a subwoofer or LFE (low-frequency effects) speaker. Such sys-\ntems are sometimes called 2.1 systems—two for the left and right, and “dot\none” for the LFE speaker.\nHeadphones versus Speakers\nIt’s important to distinguish between stereo speakers in an open room and\nstereo headphones. Stereo speakers in a room will typically be positioned in\nfront of the listener and offset to either side. This means that the sound waves\ncoming from the leftspeaker are actually received by the rightear as well, and\nviceversa. Thewavesfromthemore-distantspeakerwillarriveattheearwith\na slight time delay (phase shift) and a slight attenuation. The phase-shifted\nsound waves from the more-distant speaker will tend to interfere with those\ncoming from the closer speaker. The sound system should take this interfer-\nence into account in order to produce the highest quality sound.\nHeadphones, on the other hand, come in direct contact with the ears, so\nthe left and right channels are perfectly isolated and do not interfere with one\nanother. Also, because headphones deliver sound almost directly to the ear\ncanal, the head-related transfer effects (HRTF) of the shape of the ears them-\nselves do not come into play (see Section 14.1.4), meaning that somewhat less\nspatial information is received by the listener.\n14.3.1.4 Speaker Layouts: Surround Sound\nHome theater surround sound systems typically come in two flavors: 5.1 and\n7.1. As you undoubtedly guessed, these numbers refer to the five or seven\n“main” speakers, plus the one subwoofer. The goal of a surround sound sys-\ntemistoimmersethelistenerinarealisticsoundscape, byproviding positional\ninformation as well as high-fidelity sound reproduction (see Section 14.1.4).\nThe main speaker channels in a 5.1 system are: center, front left, front right,\nrear left and rear right. A 7.1 system adds two additional speakers, surround\nleft and surround right, which are intended to be placed directly to either side\nof the listener. Dolby Digital AC-3 and DTS are two popular surround sound\n14.3. The Technology of Sound 945\nFigure 14.24. Speaker arrangement for a 7.1 surround sound home theater system.\ntechnologies. The speaker layout of a typical 7.1 home theater is shown in\nFigure 14.24.\nDolby Surround, Dolby Pro Logic and Dolby Pro Logic II are technologies\nfor expanding a stereo source signal into 5.1 surround sound. A stereo signal\nlacks the positional information necessary to drive the 5.1 speaker configu-\nration directly. But using these Dolby technologies, an approximation of the\nmissing positional information can be generated heuristically using various\ncues found within the original stereo source signal.\n14.3.1.5 Analog Signal Levels\nAudio voltage signals may be transmitted at various voltage levels. A micro-\nphoneusuallyproducesalow-amplitudevoltagesignal—thesearecalled mic-\nlevelsignals. For connections between components, higher-voltage line-level\nsignals are used. There’s a big difference between professional audio equip-\nment and consumer electronics when it comes to line-level voltages. Profes-\nsional gear is usually designed to work with line levels ranging from 2.191 V\n(volts) peak-to-peak for a nominal signal up to a maximum voltage of 3.472 V\npeak-to-peak. The peak-to-peak voltage of a “line level” signal on consumer\nequipment varies quite a bit, but most consumer devices output up to 1.0 V\npeak-to-peak, and have inputs that can handle up to 2.0 V signals. It’s impor-\n946 14. Audio\ntant to match the levels of input and output signals when connecting audio\nequipment. Passing a voltage that is too high for the device to handle will\ncause clipping of the signal. And passing a voltage that is too low will result\nin audio that sounds quieter than it should.\n14.3.1.6 Ampliﬁers\nThe small voltages produced by a microphone must be amplified in order to\ndrive speakers with enough force to produce audible sound waves. An am-\nplifieris an analog electronic circuit that produces at its output a nearly exact\nreplica of its input signal, but with the amplitude of the signal increased sig-\nnificantly. An amp essentially increases the powercontent of a signal. It does\nthis by drawing from some kind of power source, and driving the increased\nvoltageproducedbythispowersourceinsuchawayastomimicthebehavior\nof the input signal over time. In other words, an amp modulates the output of\nits power source to match its much lower-voltage input signal.\nThecoretechnologybehindanamplifieristhe transistor —thatwell-known\nand utterly ingenious device that sits at the heart of many modern electronic\ndevices, including its crowning achievement—the computer. A transistor\nmakes use of a semiconducting material in order to link the voltages between\ntwo otherwise isolated, independent circuits. As such, a low-voltage signal\ncan be used to drive a higher-voltage circuit. This is exactly what is required\nof an amplifier. We won’t get into the details of how transistors and ampli-\nfiers work under the hood here. But if you’re curious, you can whet your\nwhistlewiththisgreatYouTubevideoonhowtheveryfirsttransistorworked:\nhttps://www.youtube.com/watch?v=RdYHljZi7ys. And you can read more\nabout amplifier circuits here: http://en.wikipedia.org/wiki/Amplifier.\nThegain Aof an amplification system is defined as the ratio of output\npower Poutto input power Pin. Like sound pressure level, gain is typically\nmeasured in decibels:\nA=10 log10(Pout\nPin)\ndB.\n14.3.1.7 Volume/Gain Controls\nAvolume control is basically an inverse amplifier, also known as an attenua-\ntor. Ratherthanincreasingtheamplitudeofanelectricalsignal, it decreases the\namplitude, while keeping all other aspects of the waveform intact. In a home\ntheatersystem,theD/Aconverterproducesavoltagesignalwithaverysmall\namplitude. The power amp boosts this signal up to the maximum “safe” out-\nput power, beyond which the sound produced by your speakers would begin\nto clip and distort (or even damage your hardware). The volume control then\n14.3. The Technology of Sound 947\nattenuates this maximum output power to produce sound at the desired lis-\ntening volume.\nA volume control is much simpler to make than an amplifier. One can be\nconstructed by introducing a variable resistor, also known as a potentiometer ,\ninto the circuit somewhere between the amplifier’s output and the speakers.\nWhen the resistance is at its minimum (at or very close to zero), the ampli-\ntude of the input signal isn’t changed, and a sound of maximum volume is\nproduced. When the resistance is at its maximum setting, the input signal’s\namplitude is maximally attenuated, and a sound of minimum volume is pro-\nduced.\nIf your stereo system at home reports the volume in decibels, you’ve prob-\nably noticed that the values are always negative. This is because the volume\ncontrol is attenuating the output of the power amp. The volume meter is still\nmeasured like a gain, but the “input” power is the maximum power of the\namp, and the “output” power is the volume selected by the user:\nA=10 log10(Pvolume\nPmax)\ndB,\nwhich will be negative as long as Pvolume<Pmax.\n14.3.1.8 Analog Wiring and Connectors\nAn analog monophonic audio voltage signal can be carried by a pair of wires;\na stereo signal requires three wires (two channels plus a common ground).\nThe wiring can be internal to a device, in which case it is usually called a bus.\nWiring can also be external, for use in connecting different devices to one an-\nother.\nExternal wiring is typically connected to audio hardware either via a di-\nrect “clip” or screw-post connector, of the kind found on high-end speakers,\nor via various standardized connectors. Examples include RCA jacks, large\nTRS(tip/ring/sleeve)jacks (thekindused bytelephone operatorsinthe early\n1900s),TRSmini-jacks(foundonyouriPod,mobilephoneandmostPCsound\ncards),keyedjacks(foundmostoftenonhigh-qualitymicrophonesandpower\namps), and the list goes on.\nAudio wiring is available in a wide range of quality levels. Thicker-gauge\nwiring offers less resistance and therefore can transmit signals over farther\ndistances without unacceptable levels of attenuation. Optional shielding can\nhelp reduce noise. And of course the choice of which metal to use in the con-\nstruction of the wires and connectors can make a difference in the quality of\nthe wiring as well.\n948 14. Audio\n14.3.2 Digital Audio Technology\nThe introduction of the compact disc (CD) marked a turning point in the au-\ndio industry toward digital audio storage and processing. Digital technology\nopens up a great many new possibilities, from reducing the size and increas-\ning the capacity of storage media, to using powerful computer hardware and\nsoftwaretosynthesizeandmanipulateaudioinpreviouslyunimaginedways.\nToday, analog audio storage devices are a thing of the past, and analog audio\nsignals are typically employed only where necessary—at the microphone and\nthe speaker.\nAs we saw in Section 14.2.1.1, the distinction between analog and digital\naudio technologies corresponds exactly to the distinction between continuous-\ntimeanddiscrete-time signals in the study of signal processing theory.\n14.3.2.1 Analog-to-Digital Conversion: Pulse-Code Modulation\nTo record audio for use in a digitial system, such as a computer or game con-\nsole,thetime-varyingvoltageofananalogaudiosignalmustfirstbeconverted\nintodigitalform. Pulse-codemodulation(PCM)isthestandardmethodforen-\ncoding a sampled analog sound signal so that it can be stored in a computer’s\nmemory, transmitted over a digital telephony network or burned onto a com-\npact disc.\nInpulse-codemodulation,voltagemeasurementsaretakenatregulartime\nintervals. The voltage measurements may be stored in floating-point format,\nor they may be quantized so that each measurement can be stored in an integer\nwithafixednumberofbits(typically8,16,24or32). Thesequenceofmeasured\nvoltagevaluesisthenstoredintoanarrayinmemory, orwrittenouttoalong-\nterm storage medium. The process of measuring a single analog voltage and\nconverting it to quantized numeric form is called analog-to-digital conversion\nor A/D conversion. Specialized hardware is typically used to perform A/D\nconversions. When we repeat this process at regular time intervals, it is called\nsampling. A hardware or software component that performs A/D conversion\nand/or sampling is referred to as an A/D converter or ADC.\nIn math terms, given the continuous-time audio signal p(t), we construct\nthesampled version p[n]such that for each sample, p[n] =p(nTs), where nis a\nnon-negative integer used to index the samples, and Tsis the amount of time\nbetween each sample, known as the sampling period. The basics of sampling\nare illustrated in Figure 14.25.\nThedigitalsignalthatresultsfromPCMsamplinghastwoimportantprop-\nerties:\n•Sampling rate . This is the frequency at which the voltage measurements\n14.3. The Technology of Sound 949\nss\nFigure 14.25. A discrete-time signal can be thought of as a sampled version of a continuous-time\nsignal.\n(samples) are taken. In principle an analog signal can be recorded dig-\nitally without anyloss of fidelity, provided that it is sampled at a fre-\nquencytwicethat of the highest-frequency component present in the\noriginal signal. This somewhat astounding and incredibly useful fact\nis known as the Shannon-Nyquistsamplingtheorem . As we saw in Section\n14.1.2.2, humanscanonlyhearsoundswithinalimitedbandoffrequen-\ncies (from 20 Hz to 20 kHz). So all audio signals of interest to human\nbeings are band-limited, and can be faithfully recorded using a sampling\nrateofalittleover40kHz. (Voicesignalsoccupyanarrowerbandoffre-\nquencies, from 300 Hz to 3.4 kHz, so digital telephony systems can get\naway with a sampling frequency of only 8 kHz.)\n•Bitdepth . This describes the number of bits used to represent each quan-\ntized voltage measurement. Quantizationerror is the error introduced by\nrounding the measured voltage values to the nearest quantized value.\nAll other things being equal, a greater bit depth results in lower quan-\ntization error, and therefore yields a higher-quality audio recording. A\nbit depth of 16 is typical among uncompressed audio data formats. Bit\ndepth is sometimes referred to as resolution .\nThe Shannon-Nyquist Sampling Theorem\nTheShannon-Nyquist sampling theorem states that if a band-limited continuous-\ntime signal (i.e., a signal whose Fourier transform is zero everywhere outside\na limited band of frequencies) is sampled to produce its discrete-time coun-\nterpart, the original continuous-time signal can be recovered exactlyfrom the\ndiscrete signal, provided that the sampling rate is high enough. The mini-\nmum sampling frequency for which this relation holds is called the Nyquist\n950 14. Audio\nfrequency .\nws>2wmax,\nwhere\nws=2p\nTs.\nClearly, it is the existence of this theorem that allows digital technology to be\nused in audio processing. Without it, digital audio would be doomed never\nto sound as good as analog audio, and computers would not be playing the\nsignificant role in the production of high-fidelity audio that they do today.\nWe won’t get into all of the gory details of why the sampling theorem\nworks. But we can gain some insight by realizing that the act of sampling\na signal at regularly spaced intervals in time causes its frequency spectrum\n(Fourier transform) to be duplicated over and over along the frequency axis.\nThe higher the sampling frequency, the more “spaced out” these copies of the\nsignal’s frequency spectrum will be. So if the original signal is band-limited,\n... ...\n... ...X(\n)\nX (\n)s\nX (\n)s\nmax\n max\ns\n s\n s\n s\ns max\nAliasing\ns max\nFigure 14.26. The frequency spectrum of a band-limited signal is zero everywhere except within\na limited frequency band (top). If the sampling frequency exceeds the Nyquist frequency, the\nspectrum copies do not overlap and the original signal can be recovered exactly (middle). If the\nsampling frequency is too low, the spectrum copies overlap and aliasing results (bottom).\n14.3. The Technology of Sound 951\nandifthesamplingfrequencyishighenough,wecanguaranteethatthecopies\nofthefrequencyspectrumwillbespacedfarenoughapartsoasnottooverlap\nwith one another. When this happens, we can recover the original frequency\nspectrum exactly via a low-pass filter that filters out all of the copies of the\nspectrum except the original. However, if the sampling frequency is too low,\nthe spectrum copies will overlap with one another. This is called aliasing, and\nit prevents us from exactly recovering the original signal’s spectrum. See Fig-\nure 14.26 for an illustration of aliased and unaliased sampling.\n14.3.2.2 Digital-to-Analog Conversion: Demodulation\nWhen a digital sound signal is to be played back, a process opposite to that of\nanalog-to-digital conversion is required. We call this, sensibly enough, digital-\nto-analogconversion orD/Aconversionforshort. Itisalsotermed demodulation\nbecause it undoes the effects of pulse-code modulation. A digital-to-analog\nconversion circuit is called a DAC.\nD/A conversion hardware generates an analog voltage corresponding to\neach sampled voltage value in a digital signal, as represented by an array of\nquantizedPCMvaluesinmemory. Ifwedrivethishardwarewith newvalues\nperiodically,attherateatwhichthesamplesweremeasuredduringPCM,and\npresuming that the sample rate was high enough as per the Shannon-Nyquist\nsampling theorem, the analog voltage signal produced should exactly match\nthe original voltage signal.\nPractically speaking, when we drive an analog voltage circuit with a se-\nquence of discrete voltage levels, unwanted high-frequency oscillations are\noften introduced as the hardware tries to rapidly change from one voltage\nlevel to another. D/A hardware typically includes a low-pass or band-pass\nfilter to remove these unwanted oscillations, thereby ensuring an accurate re-\nproduction of the original analog signal. For more information on filtering,\nsee Section 14.2.5.8.\n14.3.2.3 Digital Audio Formats and Codecs\nVarious data formats exist for storing PCM audio data on disc or transmitting\nit over the Internet. Each format has its history, and its pros and cons. Some\nformats such as AVI are actually “container” formats, which can encapsulate\ndigital audio signals in more than one data format.\nSome audio formats store the PCM data in an uncompressed form. Oth-\ners utilize various forms of data compression to reduce the required file size\nor transmission bandwidth. Some compression schemes are lossy, meaning\nthat some of the fidelity of the original signal is lost in the compression/de-\ncompression process. Other compression schemes are lossless, meaning that\n952 14. Audio\nthe original PCM data can be recovered exactly after a round-trip compres-\nsion/decompression cycle.\nLet’s take a look at a few of the most common audio data formats.\n•Raw header-less PCM data is sometimes used in situations where the\nmeta-informationaboutthesignal,suchasthesamplerateandbitdepth,\nis known a priori.\n•Linear PCM (LPCM) is an uncompressed audio format that can support\nup to eight channels of audio at a 48 kHz or 96 kHz sampling frequency,\nand 16, 20 or 24 bits per sample. The “linear” in LPCM refers to the fact\nthattheamplitudemeasurementsaretakenonalinearscale(asopposed\nto, say, a logarithmic scale).\n•WAVis an uncompressed file format created by Microsoft and IBM. Its\nuseiscommonplaceontheWindowsoperatingsystem. Itscorrectname\nis “waveform audio file format” although it is also rarely referred to as\n“audio for windows.” The WAV file format is actually one of a family\nof formats known as resource interchange file format (RIFF). The contents\nof a RIFF file are arranged in chunks, each with a four-character code\n(FOURCC) that defines the contents of the chunk and a chunk size field.\nThe bitstream in a WAV file conforms to the linear pulse-code modula-\ntion (LPCM) format. WAV files can also contain compressed audio, but\nthey are most commonly used for storing uncompressed audio data.\n•WMA(Windows Media Audio) is proprietary audio compression tech-\nnology designed by Microsoft as an alternative to MP3. See http://en.\nwikipedia.org/wiki/Windows_Media_Audio for details.\n•AIFF(audio interchange file format) is a format developed by Apple\nComputer, Inc.andusedwidelyonMacintoshcomputers. LikeaWAV/\nRIFFfile,anAIFFfiletypicallycontainsuncompressedPCMdata,andis\ncomprised of chunks, each prefaced by a four-character code and a size\nfield. AIFF-C is a compressed variant of the AIFF format.\n•MP3is a lossy compressed audio file format that has become the de\nfacto standard on most digital audio players, and is also widely used\nby games and multimedia systems and services. The full name of this\nformat is actually MPEG-1 or MPEG-2 audio layer III. MP3 compression\ncan result in files that are one-tenth the size, but with very little per-\nceptual difference from the original uncompressed audio. These results\nare achieved by making use of perceptualcoding—a technique that elimi-\nnatesportionsoftheaudiosignalthatarebeyondtheperceptionofmost\npeople anyway.\n14.3. The Technology of Sound 953\n•ATRAC stands for Adaptive Transform Acoustic Coding—a family of\nproprietary audio compression techniques developed by Sony. The for-\nmat was originally developed to allow Sony’s MiniDisc media to con-\ntain audio with the same running time as a CD while occupying signifi-\ncantlyless space andundergoingan imperceptabledegradation inqual-\nity. See http://en.wikipedia.org/wiki/Adaptive_Transform_Acoustic_\nCoding for more details.\n•Ogg Vorbis is an open source file format that offers lossy compression.\nOggreferstoa“container”formatthatiscommonlyusedinconjunction\nwith the Vorbis data format.\n•Dolby Digital (AC-3) is a lossy compression format supporting channel\nformats from mono to 5.1 surround sound.\n•DTSis a collection of theater audio technologies developed by DTS,\nInc. DTS Coherent Acoustics is a digital audio format transportable\nthrough S/PDIF interfaces (see Section 14.3.2.5) and used on DVDs and\nLaserdiscs.\n•VAGis a proprietary audio file format available for use by all PlaySta-\ntion3developers. Itmakesuseof adaptivedifferentialPCM (ADPCM),an\nanalog-to-digital conversion scheme based on PCM. Differential PCM\n(DPCM) stores the deltas between samples rather than the absolute val-\nues of the samples themselves, in order to allow the signal to be com-\npressed more effectively. Adaptive DPCM varies the sample rate dy-\nnamically in order to further improve the achievable compression ratio.\n•MPEG-4SLS,MPEG-4ALSandMPEG-4DST are formats that offer loss-\nless compression.\nThis list is by no means comprehensive. In fact, there are a dizzying num-\nber of audio file formats, and an even longer list of compression/decompres-\nsion algorithms. For an introduction to the fascinating world of audio data\nformats, check out our old friend Wikipedia: http://en.wikipedia.org/wiki/\nDigital_audio_format. The“PlayStation3Secrets”websitealsoprovidessome\nexcellent information on audio formats: https://bit.ly/2HOVtvR.\n14.3.2.4 Parallel and Interleaved Audio Data\nOne way to organize multi-channel audio data is to store the samples for each\nmonophonic channel into a separate buffer. In this case, you’d need six par-\nallel buffers to describe a 5.1 audio signal. This arrangement is shown in Fig-\nure 14.27.\nMulti-channel audio data can also be interleaved within a single buffer. In\nthis case, all of the samples for each time index are grouped together in a pre-\n954 14. Audio\ndefined order. Figure 14.28 depicts an interleaved PCM buffer containing a\nsix-channel (5.1) audio signal.\n14.3.2.5 Digital Wiring and Connectors\nRR[n]RL[n]FR[n]FL[n]C[n]\nFL[n+1]C[n+1]LFE[n]\nFR[n+1]\nFigure 14.28. Six-\nchannel (5.1) PCM bus\ndata in interleaved\nformat.S/PDIF(Sony/PhilipsDigitalInterconnectFormat)isaninterconnecttechnol-\nogythattransmitsaudiosignals digitally,therebyeliminatingthepossibilityof\nnoise being introduced by analog wiring. The S/PDIF standard is physically\nrealized either via a coaxial cable connection (also called S/PDIF) or a fiber-\noptic connection (known as TOSLINK).\nRegardless of the physical interface (S/PDIF coaxial or TOSLINK optical),\nthe S/PDIF transport protocol is limited to 2-channel 24-bit LPCM uncom-\npressed audio at standard sampling rates ranging from 32 kHz to 192 kHz.\nHowever, not all equipment works at all sample rates. The same physical in-\nterfaces can be also be used to transport bitstream-encoded audio (e.g., Dolby\nDigital or DTS lossy compressed data) at bitrates ranging from 32 kpbs to\n640 kbps for Dolby Digital and 768 kbps to 1536 kbps for DTS, respectively.\nUncompressedmulti-channelLPCM(i.e.,greaterthantwostereochannels)\ncanonlybesentoveranHDMI(high-definitionmultimediainterface)connec-\ntion on consumer audio equipment. HDMI connectors are used for transmis-\nsion of both uncompressed digital video and either compressed or uncom-\npressed digital audio signals. HDMI supports up to a 36.86 Mbps bitrate for\nmulti-channelorbitstreamaudio. However,HDMIbitratesforaudiovaryde-\npendingonthevideomode—only720p/50Hzmodesorhigherarecapableof\nutilizing the full audio bandwidth. See the HDMI specification under the sec-\ntion “video dependency” for more information on this. Apple’s DisplayPort\nand Thunderbolt connectors are other high-bandwidth alternatives similar in\nmany respects to HDMI.\nUSBconnectionsaresometimesusedtosendaudiosignals. Onmostgame\nconsoles, the USB output is intended only to drive headphones.\nWirelessaudioconnectionsarealsopossible. TheBluetoothstandardisthe\nFR[n+1] FL[n+1] C[n+1]\nFR[n+2] FL[n+2] C[n+2]\n... ... ...Parallel\nRR[n+1] RL[n+1] LFE[n+1]\nRR[n+2] RL[n+2] LFE[n+2]\n... ... ...FR[n] FL[n] C[n] RR[n] RL[n] LFE[n]\nFigure 14.27. Six-channel (5.1) PCM bus data in parallel format.",28758
99-14.4 Rendering Audio in 3D.pdf,99-14.4 Rendering Audio in 3D,"14.4. Rendering Audio in 3D 955\nmost commonly used method of transmitting audio signals wirelessly.\n14.4 Rendering Audio in 3D\nThus far, we’ve learned about the physics of sound, the mathematics of signal\nprocessing and the various technologies that are used to record and play back\nsounds. In this section, we’ll explore how all of this theory and technology\ncan be put to use in a game engine, in order to produce realistic, immersive\nsoundscapes for our games.\nAny game that takes place in a virtual 3D world requires some sort of 3D\naudio rendering engine. A high-quality 3D audio system should provide the\nplayer with a rich, immersive and believable soundscape that matches what’s\ngoing on in this 3D world, while supporting the story and remaining true to\nthe tonal design of the game.\n• Theinputstothissystemarethemyriad 3Dsounds thatemanatefromall\nover the game world: footsteps, speech, the sound of objects bumping\ninto one another, gunfire, ambient sounds like wind or rainfall and so\non.\n• Itsoutputisahandfulofsoundchannelsthat,whenplayedinthespeak-\ners, reproduce as believably as possible what the player would actually\nhear if he or she were really there in the virtual game world.\nIdeally we’d like our audio engine to produce its output in full 7.1 or 5.1 sur-\nround sound, because this gives the ears the richest possible set of positional\ncues. However, audio engines must also support stereo output for players\nwho don’t have fancy home theater systems—or who just want to play their\ngame using headphones so they don’t wake their neighbors.\nA game’s audio engine is also responsible for playing sounds that do not\noriginateinthevirtualworld. Examplesincludethemusictrack,soundsmade\nby the in-game menu system, a narrator’s voice-over, the voice of the player\ncharacter (especially in first-person shooters) and possibly certain ambient\nsounds. We call these 2D sounds. Such sounds are designed to be played “di-\nrectly” in the speakers, after having been mixed with the outputs of the 3D\nspatialization engine.\n14.4.1 Overview of 3D Sound Rendering\nThe primary tasks performed by the 3D audio engine are as follows:\n956 14. Audio\n•Soundsynthesis is the process of generating the sound signals that corre-\nspond to the events occurring in the game world. These might be pro-\nduced by playing back pre-recorded sound clips, or they might be proce-\ndurallygenerated at runtime.\n•Spatialization producestheillusionthateach3Dsoundiscomingfromthe\nproperlocationinthegameworld, fromthepointofviewofthelistener.\nSpatializationisaccomplishedbycontrollingthe amplitude ofeachsound\nwave (i.e., its gain or volume) in two ways:\n◦Distance-based attenuation controls the overall volume of a sound in\norder to provide an indication of its radialdistance from the listener.\n◦Pancontrols a sound’s relative volume in each of the available\nspeakers in order to provide an indication of direction from which\nthe sound is arriving.\n•Acousticalmodeling heightens the realism of the rendered soundscape by\nmimicking the early reflections and late reverberations that character-\nize the listening space, and by accounting for the presence of obstacles\nthatpartiallyorcompletelyblockthepathbetweenthesoundsourceand\nthe listener. Some sound engines also model the frequency-dependent\neffects of atmospheric absorption (Section 14.1.3.2) and/or HRTF effects\n(Section 14.1.4).\n•Doppler shifting may also be applied to account for any relative move-\nment between a sound source and the listener.\n•Mixingistheprocessofcontrollingtherelativevolumesofallthe2Dand\n3D sounds in our game. The mix is driven in part by physics and in part\nby aesthetic choices made by the game’s sound designers.\n14.4.2 Modeling the Audio World\nIn order to render the soundscape of a virtual world, we must first describe\nthat world to the engine. The “audio world model” consists of the following\nelements:\n•3D sound sources. Each 3D sound in the game world consists of a mono-\nphonic audio signal, emanating from a specific position. We must also\nprovide the engine with its velocity,radiation pattern (omnidirectional,\nconical, planar) and range(beyond which the sound is inaudible).\n•Listener. The listener is a “virtual microphone” located in the game\nworld. It is defined by its position,velocity andorientation .\n14.4. Rendering Audio in 3D 957\n•Environmental model . This model either describes the geometry andprop-\nertiesof the surfaces and objects present in the virtual world, and/or it\ndescribesthe acousticproperties ofthelisteningspacesinwhichgameplay\ntakes place.\nThepositions of the source and listener are used for distance-based attenu-\nation; the radiation pattern of thesound source also factors into the distance-\nbased attenuation calculation. The orientation of the listener defines a refer-\nence frame in which the angularposition of the sound is calculated. This angle\nin turn determines the pan—the relative volumes of the sound in the five or\nseven main speakers of 5.1 or 7.1 surround sound, respectively. The relative\nvelocity of source and listener is used when applying a Doppler shift . And last\nbut not least, the environmentalmodel is used for modeling the acoustics of the\nlistening space and to account for partial or complete blockage of the sound\npath.\n14.4.3 Distance-Based Attenuation\nDistance-based attenuation reduces the volume of a 3D sound as the radial\ndistance between it and the listener increases.\n14.4.3.1 Fall-Off Min and Max\nThe number of sound sources in a typical game world is very large. Due to\nhardware and CPU bandwidth limitations, we couldn’t possibly render them\nall. And we wouldn’t want to, because thanks to distance-based fall-off all\nsoundsbeyondacertaindistancefromthelistenercan’tbeheardanyway. For\nthis reason, each sound source is usually annoted with fall-off (FO) parame-\nters.\nThefall-off min (“FO min” for short) is a minimum radius, which we’ll de-\nnote rmin, within which the sound doesn’t fall off at all and is heard at full\nvolume. The fall-offmax or “FO max” is a maximum radius, denoted rmax, be-\nyond which the sound source is considered to be silent and can therefore be\nignored. Between the FO min and FO max, we need to blend smoothly from\nfull volume down to zero.\n14.4.3.2 Blending to Zero\nOnewaytoblendfrommaximumvolumedowntozeroistousealinearramp\nbetweenFOminandFOmax. Dependingonthetypeofsound,alinearfall-off\nmight sound just fine.\nInSection14.1.3.1,welearnedthatsoundintensity,whichiscloselyrelated\nto our perception of “loudness,” falls off with radial distance according to a\n958 14. Audio\n1/r2rule. Gain, whichis proportionalto theamplitude of the sound pressure,\nfalls of as 1/r. So really the right thing to do is to use a 1/rcurve to blend the\ngain of a sound from full volume down to zero.\nOne problem with the function 1/ris that it is asymptotic—it never quite\nreaches zero, no matter how large rgets. We can fix this by shifting the curve\nslightlydownwardsothatitcrossesthe raxisat rmax. Orwecansimplyclamp\nthe sound intensity to zero for all r>rmax.\n14.4.3.3 Bending the Rules\nWhen making The Last of Us , Naughty Dog’s sound department discovered\nthat attenuating character dialog using the 1/r2rule caused speech to become\nunintelligibletooquicklyforcharactersthatwereonlyamodestdistanceaway.\nThiswasaseriousproblem,especiallyduringthestealthsectionsofgameplay,\nwhere hearing the enemies’ ambient conversations was important both as a\ntactical tool and as a means of advancing the storyline.\nTo solve this problem, the sound department at Naughty Dog utilized a\nsophisticated fall-off curve that causes dialog to roll off more slowly near the\nlistener, more quickly in the mid-range, and then more slowly again as the\ndistance to the listener grows very large. This allows speech to be audible\nover longer distances, while still retaining a natural-sounding fall-off.\nThedialogfall-offcurveswerealsoadjusteddynamicallyatruntime,based\non the current “tension level” of the game (i.e., whether the enemies are un-\nawareoftheplayer,aresearchingforhimorareengagedindirectcombatwith\nhim). Thisiswhatallowsthevoicesin TheLastofUs toprojectoverlongerdis-\ntances during stealth gameplay, while not rising to overpowering levels when\ncombat breaks out.\nFinally, a “sweetener” reverb could be optionally enabled to allow char-\nacter voices to bleed around corners, even when the direct path is 100% ob-\nstructed. This tool is incredibly helpful in situations where modeling realistic\nfall-off is less important than ensuring that the player can hear a conversation\nclearly.\nThereareallsortsofwaysto“cheat”whendesigningyour3Daudiomodel.\nBut no matter what you do, always remember this simple lesson: Never be\nafraid to do whatever it takes to satisfy the needs of your game. Don’t worry—\nthe laws of physics won’t be offended!\n14.4.3.4 Atmospheric Attenuation\nAswesawinSection14.1.3.2,low-pitchedsoundsareattenuatedbytheatmo-\nspherelessthan high-pitched sounds. Some games, including Naughty Dog’s\nThe Last of Us, model this phenomenon by applying a low-pass filter to each\n14.4. Rendering Audio in 3D 959\n3D sound whose passband slides toward lower and lower frequencies as the\ndistance between the sound source and the listener increases.\n14.4.4 Pan\nPanning is a technique used to provide the illusion that a 3D sound is coming\nfrom a particular direction. By controlling the volume (i.e., gain) of the sound\nin each of the available speakers, we can induce the perception of a phantom\nimageof the sound in three-dimensional space. This method of panning is\ncalledamplitudepanning because we are providing angular information to the\nlistenerbyadjustingonlytheamplitudesofthesoundwavesproducedateach\nspeaker (as opposed to using phase offsets, reverb or filtering to provide posi-\ntional cues). It is sometimes referred to as IID panning because it relies on the\nperceptual effects of interaural intensity difference (IID) to produce a sound’s\nphantom image.\nThe term “pan” comes from early technology that used a “panoramic po-\ntentiometer” (variable resistor) or “pan pot” to control the relative volumes\nof the left and right speakers of a stereo system. Dialing the pan pot to one\nextreme would produce sound only in the left speaker; dialing it to the other\nextreme would drive the right speaker exclusively; and centering the pan pot\ndial would distribute the sound equally to both speakers.\nTounderstandhowpanworks, weenvisionourlistenerlocatedatthecen-\nter of a circle. The speakers are positioned at various points on the circumfer-\nence of this circle, so we’ll call it the speaker circle in this book. The radius of\nthe circle approximates the average distance between the listener and any one\nspeaker.\nForastereosoundsystem,thefrontandrightspeakersarelocatedroughly\nat45degrees to the left and right of center. For stereo headphones, they\nare positioned at90degrees (and the radius is much smaller). For 7.1 sur-\nroundsound,weconsideronlytheseven“main”speakers,astheLFEchannel\nprovides no positional cues. These speakers are located roughly as shown in\nFigure 14.29. When panning to a 5.1 system, we simply omit the surround left\nand surround right speakers.\nFor the time being, let’s treat each 3D sound as a point source. To pan\na sound, we first determine its azimuthal (horizontal) angle. The azimuthal\nangle must be measured in the local space of the listener, so that an angle of\nzerocorrespondstothepositiondirectlyinfrontofthelistener. Next,wefigure\nout which two speakers around the circumference of our circle are adjacent to\nthisazimuthalangle. Weconverttheangleintoapercentageofthearcbetween\nthe two speakers. Finally, we use this percentage to determine the gains of the\n960 14. Audio\nFigure 14.29. Speaker layout for 7.1 pan.\ns\n2\n1\ns\n 2\n 1\nFigure 14.30. Treating the sound as a point source, the pan blend percentage b is calculated be-\ntween the two speakers immediately adjacent to the source.\nsound in each speaker.\nTo formulate this mathematically, let’s use the symbol qsfor the azimuthal\nangle of the sound. We’ll call the angles of the two adjacent speakers q1and\nq2. The percentage bis then calculated as follows:\nb=qs q1\nq2 q1.\nThis calculation is illustrated in Figure 14.30.\n14.4. Rendering Audio in 3D 961\n14.4.4.1 Constant Gain Panning\nOur first instinct might be to use the percentage bto perform a simple linear\ninterpolation between the gains of the two speakers. Given the gain Aof the\nunpanned sound, the gains of that sound as played in each speaker would be\ncalculated as follows:\nA1= (1 b)A;\nA2=bA.\nThis is known as constant gain panning, because the net gain A=A1+A2is\nconstant, independent of the values of qsandb.\nThe main problem with constant gain panning is that it does notproduce\nthe perception of constant loudness as the sound moves around the acoustic\nfield. Gain controls the amplitude of the sound pressure wave, and therefore\ncontrolsthe soundpressurelevel (SPL).However,aswelearnedinSection14.1.2,\nhumanperceptionofloudnessisactuallyproportionaltothe intensity orpower\nof a sound wave, both of which vary as the squareof the SPL.\nAs an illustration of the problem, imagine that our sound is panned to the\nhalfway point between our two speakers. Constant gain panning would have\nus set the gains A1andA2to1\n2Aeach. But this yields a total power of A2\n1+\nA2\n2= (1\n2A)2+ (1\n2A)2=1\n2A2. In other words, the loudness of the sound will\nbeone-half of what it would have been, had the sound been panned to only\nthe left or the right speaker.\n14.4.4.2 The Constant Power Pan Law\nClearlyinordertokeeptheperceptionofloudnessconstantasasound’simage\nmoves about the listener, we need to keep the powerconstant. This rule is\nknown as the constant power pan law, or just the panlaw for short.\nThere’saveryeasywaytoimplementtheconstantpowerpanlaw. Instead\nof linearly interpolating the gains, we use the sine and cosine of the blend\npercentage bto calculate them:\nA1=sin(p\n2b)A;\nA2=cos(p\n2b)A.\nConsider again a sound image that is panned to halfway between the two\nspeakers ( b=1\n2). With constant power panning, the two speakers’ gains\nwill be set to A1=A2=1p\n2A. This yields a total power of A2\n1+A2\n2=\n(1p\n2A)2+ (1p\n2A)2=A2. This works for any value of b, so the power A2is\nconstant no matter where our sound image is placed around the circle.\n962 14. Audio\nSound designers often apply a “3 dB rule” to account for the pan law: If a\nsound is to be mixed equally to two speakers, the gain in each speaker should\nbereducedby3dBrelativetothegainthatwouldbeusedifthesoundwereto\nbe played in only one speaker. The value  3dB arises because log10(\n1p\n2)\n\n 0.15. Voltage gain (or amplitude gain) is defined as 20 log10(Aout/Ain), and\n20 0.15 = 3dB. (The20 infrontofthe logarithmarises becausea decibel\nisone-tenthofabel,multipliedbytwotoaccountforthefactthatwe’redealing\nwith A2and not A.)\n14.4.4.3 Headroom\nPanning causes sounds to be rendered entirely by one speaker in some situa-\ntions,andbytwo(ormore,aswe’llsee)speakersinothers. Let’ssayasoundis\nbeing played equally by two adjacent speakers, and its volume is so loud that\neach speaker is outputting its maximum power. What happens when that\nsound pans around to only one speaker? The answer is that we’d probably\nblow out the speaker, because our constant power pan law requires us to use\nmore gain for one speaker than for two.\nTo prevent this problem, we need to artificially lower the maximum gains\nofoursoundsacrosstheboard,suchthattheworst-casescenarioofplayingthe\nsound in one speaker won’t overdrive that speaker. The practice of artificially\nreducing the maximum range of volume is known as “leaving oneself some\nheadroom .”\nTheconceptofheadroomalsoappliesto mixing. Whentwoormoresounds\nare mixed, their amplitudes add up. By leaving some headroom in our mix,\nwe can accomodate worst-case scenarios where a large number of high-vol-\nume sounds play simultaneously.\n14.4.4.4 To Center or Not to Center?\nIn cinema, the center channel was historically used for speech; only the sound\neffects would be panned to the other speakers around the room. The idea\nbehindthispracticewasthatthecharactersinthemovieareusuallyon-screen\nwhentheyspeak,sotheaudienceexpectstoheartheirvoicesfront-and-center.\nThisapproachhastheniceside-effectofseparatingoutthespeechfromtherest\nof the sounds in the film, meaning that loud sound effects won’t use up all the\navailable headroom and drown out the dialog.\nIn 3D games, the situation is quite different. The player generally wants to\nheardialogcomingfromthe“correct”locationaroundhimorher. Iftheplayer\nswingsthecameraby180degrees, thedialogshouldlikewiseswingaboutthe\n14.4. Rendering Audio in 3D 963\nsound field by 180 degrees. As such, games usually do not assign all dialog to\nthecenterspeaker; instead, it isincludedin thepanfor bothsoundeffectsand\ndialog.\nOf course, this brings us back to the headroom problem—loud gunfire can\nnow completely drown out the speech. At Naughty Dog, we overcame this\nproblem by “splitting the difference” and always playing someof the dialog\nin the center channel, as well as panning some of it to the rest of the speakers\nalong with the sound effects.\n14.4.4.5 Focus\nWhen the source of a sound is far away from the listener, we can treat it as a\npointsource. Wesimplycalculateasingleazimuthalangleandfeeditintoour\nconstant power panning system. However, when a sound source approaches\noractuallyentersintothecirclethatdefinestheradialdistanceofthespeakers\nfrom the listener, it can no longer be accurately modeled as a point source\nrepresented by a single angle.\nConsider the case of moving toward and past a sound source. At first, the\nsound source appears entirely in the front speakers. As it passes the listener,\nwe somehow need to transfer the sound to the rear speakers. If we model the\nsound as a point source, our only option is to “pop” the sound from the fronts\nto the rears.\nIdeally we’d like the sound’s image to gradually “spread out” around the\nspeaker circle as it approaches. That way, as it nears the listener, we can start\nplaying more of it in the side speakers. When the sound source is coincident\nwith the listener, it can be played in all seven (or five) speakers. And once it\npasses, we can smoothly transition the sound to the rears, dropping the front\ngains to zero as it recedes behind the listener.\nWe can do this kind of thing and more if we model a sound source not\nas a point on the speaker circle but as an arc. Looking at it another way, we\ncan think of each sound source as having an arbitrary shape in 3D space, and\nitsprojection onto the speaker circle subtends a certain angle, defining a “pie\nwedge” shape within the circle. This is analogous to the concept of solid angle\noften used in the calculation of ambient occlusion in 3D graphics—see http://\nen.wikipedia.org/wiki/Solid_angle for details.\nWe’llcalltheanglesubtendedbyanextendedsoundsourcethe focusangle,\nand we’ll denote it a. A point source can be thought of as an “edge case” in\nwhich a=0. The focus angle is depicted in Figure 14.31.\nTo render a sound with a nonzero focus angle, we must first determine\nthe subset of speakers that either intersect its projected arc on the speaker cir-\ncle, or are immediately adjacent to the arc. Then we must divide the sound’s\n964 14. Audio\nFigure 14.31. The focus angle a deﬁnes the projection of an extended sound source on the speaker\ncircle.\nintensity/power among these speakers in order to induce the perception of a\nphantom image that extends across the projected arc.\nWe can divide the sound amongst the relevant speakers in various ways.\nForexample,wecouldarrangeforallthespeakersthatlie withinthefocus“pie\nslice” to receive equal maximum power, and then apportion less of the sound\nto the two speakers immediately adjacent the arc to create a fall-off. But no\nmatter how we do it, we must remember to always obey the constant power\npan law. So, we must set the gains in such a way that the sum of their squares\n(i.e.,thesumoftheirpowers)equalsthesquaredgainoftheoriginalunpanned\nsound source.\n14.4.4.6 Dealing with Verticality\nIn both stereo and surround sound set-ups, the speakers all lie roughly in a\nhorizontal plane. This arrangement makes it tricky to position sounds above\nor below the plane of the listener’s ears.\nThe ideal of course would be to model a true “periphonic” sound field\nby using a spherical speaker arrangement. A technology known as Ambison-\nics(http://en.wikipedia.org/wiki/Ambisonics) is capable of accommodating\nbothplanarandsphericalspeakerarrangements. However,itisnotsupported\nby any game console—at least not yet. Sony now offers a 3D audio technol-\nogy in their Platinum Wireless Headset for PS4, and games are beginning to\nsupport it. But even in the presence of 3D audio technology, games still need\nto support a planar speaker arrangement for 5.1 and 7.1 sound systems.\nIt turns out that the concept of focuscan be leveraged to simulate some de-\n14.4. Rendering Audio in 3D 965\ngree of verticality in our sound imagery. We simply projectall sounds onto the\nhorizontal plane, and then use a nonzero focus angle for any sounds whose\nprojections fall too close to or within the speaker circle. An elevated sound\nthat is far away will be rendered in virtually the same way as one that is not\nelevated. Butastheelevatedsoundpassesoverhead,weblenditacrossmulti-\nplespeakers,therebyproducingaphantomimagewithinthespeakercircle. If\nwecombinethiswithdistance-basedattenuationandfrequency-dependentat-\nmosphericabsorptions, wecanprovidethelistenerwithenoughcuestomake\nthe sound seem to be located above or below the listener.\n14.4.4.7 Further Reading on Pan\nThe basics of the constant power pan law can be found here: http://www.\nrs-met.com/documents/tutorials/PanRules.pdf. The following site is also a\ngreat resource on the topic: http://www.music.miami.edu/programs/mue/\nResearch/jwest/Chap_3/Chap_3_IID_Based_Panning_Methods.html.\nThe paper entitled “Spatial Sound Generation and Perception by Ampli-\ntude Panning Techniques” by Ville Pukki of the Helsinki University of Tech-\nnology, available at https://aaltodoc.aalto.fi/bitstream/handle/123456789/\n2345/isbn9512255324.pdf?sequence=1,providesacleardescriptionofthespa-\ntialization problem and outlines the vector based amplitude panning (VBAP)\nmethod, as well as providing an extensive bibliography for further reading.\nDavid Griesinger’s paper, “Stereo and Surround Panning in Practice,”\nalso makes for a very interesting read; it is available at http://www.\ndavidgriesinger.com/pan_laws.pdf. David’s website is chock full of research\non sound perception and audio reproduction technologies.\n14.4.5 Propagation, Reverb and Acoustics\nEven if we were to implement distance-based attenuation, pan and Doppler,\nour 3D sound engine still wouldn’t be able to generate a realistic soundscape.\nThis is because a lot of the auditory cues we humans use to sort out what\nkind of space we’re in come from the early reflections, late reverberations and\nhead-related transfer function (HRTF) effects caused by sound waves taking\nmultiple paths to reach our ears. The term “sound propagation modeling” can\nbe applied to any technique that is designed to take into account the ways in\nwhich sound waves propagate through a space.\nMany different approaches are used, both in research and in interactive\nmedia and games. These technologies fall into three basic categories:\n•geometricanalysis attemptstomodeltheactualpathwaystakenbysound\nwaves,\n966 14. Audio\n•perceptuallybasedmodels focus on reproducing what the ear perceives us-\ning an LTI system model of the acoustics of a listening space, and\n•ad hoc methods employ various kinds of approximations to produce rea-\nsonably accurate acoustics with minimal data and/or processing band-\nwidth.\nThe following paper does a good job of surveying many of the techniques\nthat fall into the first two categories: http://www-sop.inria.fr/reves/Nicolas.\nTsingos/publis/presence03.pdf. In this section, we’ll briefly discuss LTI sys-\ntems modeling, and then turn our attention to a few ad hoc methods, because\nthey tend to be more practical for use in real games.\n14.4.5.1 Modeling Propagation Effects with an LTI System\nImagine that I am standing in a room containing various objects made of var-\nious materials. A sound is made in the room. It reflects and diffracts and\nbounces around the room, and eventually reaches my ears. If you think about\nit, it doesn’t really matter which specific paths those sound waves took. The\nonly thing that affects my perception is the specific superposition of the dry\ndirect sound waves and the various time-shifted and possibly muffled or oth-\nerwise altered wet indirect waves.\nIt turns out that all of these effects can be modeled with a linear time-\ninvariant (LTI) system. Theoretically, if we could measure the impulseresponse\nof the room for a given pair of points that represent the source of the sound\nand the listener, we can determine exactly how anysound we might play at\nthatsourcelocationshouldsoundifheardatthelistenerposition. Allweneed\nto do is convolve the dry sound with the impulse response!\npwet(t) =pdry(t)h(t).\nThis technique seems like a silver bullet at first blush. However, it is ac-\ntually more difficult and less practical than it may at first seem. It’s pretty\neasy to determine the impulse response of a space in real life—you can record\nthe sound of a short “click” that approximates the unit impulse d(t), and the\nrecorded signal will approximate h(t). But in a virtual space, we’d need to\nperform a complex and expensive simulation of each play space in order to\ndetermine h(t). Also, to model the room’s acoustics accurately, we’d need\nto perform this calculation for a large number of source-listener point pairs\nthroughout the game world, and once calculated the size of this data would\nbe immense. Finally, the operation of convolution is itself not inexpensive,\nand game consoles and sound cards have in the past lacked the horsepower\nto do this for every sound in the game in real time.\n14.4. Rendering Audio in 3D 967\nFigure 14.32. It’s a good idea to cross-blend between reverb settings based on the position of the\nlistener.\nModern gaming hardware is getting more powerful all the time, and a\nconvolution-based approach to propagation modeling is becoming more fea-\nsible. Forexample,MicahTayloretal.createdareal-timedemoofconvolution\nreverb that produced promising results—see https://intel.ly/2J8Gpsu. That\nsaid,mostgamesstilldon’tusethisapproach,butinsteadtheyrelyonvarious\nad hoc methods and approximations to model environmental reverb.\n14.4.5.2 Reverb Regions\nOne common approach to modeling the wet characteristics of a play space is\nto annotate the game world with manually placed regions, each of which is\ntagged with appropriate reverb settings such as pre-delay, decay, density and\ndiffusion. See Section 14.1.3.4 for a discussion of these parameters. As the\nvirtual listener moves through these regions, we can light up the appropriate\nreverbmode: Iftheplayerentersalargetiledroom,wecanbumpuptheechos;\nwhen the player enters a small closet, we can virtually eliminate the reverb to\nproduce a very dry sound.\nIt’s a good idea to smoothly cross-blend between reverb settings as the lis-\ntenermoves throughthe playspace. Wecan usesimple linear interpolation to\nperform this cross-blend for each parameter. The blend percentage is best cal-\nculated using a measure of how far “into” the region the listener is. For exam-\nple, imagine moving between an outdoor space and an indoor space through\na doorway. We could define a region around the doorway within which the\nblend occurs. If the listener is entirely outside the blend region, the blend per-\ncentageshouldyield100%oftheoutdoorreverbsettingsand0%oftheindoor\nsettings. If the listener is standing at the halfway point within the blend re-\ngion, we’d want a 50/50 mix of the reverb settings. Once the listener passes\nout of the blend region inside the building, we’ll have reached a 0% outdoor\n/ 100% indoor blend. This idea is illustrated in Figure 14.32.\n968 14. Audio\n14.4.5.3 Obstruction, Occlusion and Exclusion\nWhenusingregionstodefinetheacousticsofourplayspaces, wetypicallyas-\nsign asingleimpulse response function or a singlecollection of reverb settings\nto each region. This captures the essence of each play space (e.g., large tiled\nhall, small closet lined with coats, flat outdoor plain, etc.). But it results in a\nless-than-perfect reproduction of the acoustics that arise due to obstacles. For\nexample, imagine a square room with a large pillar in the center. If a sound\nsource is located in the corner of the room, a listener will perceive a very dif-\nferent timbre as he or she moves about the room, depending on whether the\ndirect path is obstructed by the pillar or not. If we use a single set of reverb\nparameters for this room, we cannot capture these subtleties.\nTo address this problem, we can attempt to model the geometry and mate-\nrial properties of the environment in some way, determine how sound waves\nare affected by the obstacles in their path, and then use the results of this anal-\nysis to alter the “base” reverb settings associated with the room.\nFigure 14.33 shows the three ways in which the objects and surfaces in the\ngame world can affect the transmission of sound waves:\n•Occlusion. This describes a situation in which there exists no unfettered\npath from the sound source to the listener. A listener might still be able\nto hear a fully occluded sound, if for example there is only a thin wall\nor door between it and the source of the sound. Either the dry and wet\ncomponents of an occluded sound are both attenuated and/or muffled,\nor the sound is entirely silent from the point of view of the listener.\n•Obstruction . This describes a case in which the direct path between the\nsound and the listener is blocked, but an indirect path is available. Ob-\nstruction can occur for example when a sound source passes behind a\ncar, pillar or other obstacle. The dry component of an obstructed sound\nis either entirely absent or greatly muffled, and the wet component may\nbealteredaswelltoaccountforthesoundwaveshavingtotakealonger,\nmore reflected path to the listener.\n•Exclusion. This describes a case in which there is a free direct path be-\ntween source and listener, but the indirect path is compromised in some\nway. This can happen if a sound is produced in one room and passes\nthroughanarrowopeningsuchasadoororwindowtoreachthelistener.\nIn an exclusion situation, the dry component of the sound remainsunal-\ntered but the wet component is attenuated, muffled or, for very narrow\nopenings, entirely absent.\n14.4. Rendering Audio in 3D 969\nsoundindirect\ndirect\nsoundindirect\nsoundindirect\ndirec t\ndirec t\nFigure 14.33. From top to bottom: occlusion, obstruction and exclusion.\nAnalyzing the Direct Path\nDetermining whether the direct path is blocked or not is not difficult. We sim-\nply cast a ray (see Section 13.3.7.1) from the listener to each sound source. If it\nis blocked, the direct path is occluded. If not, it is free.\nIf we wish to model sound transmission through walls and other obstacles,\nraycastingcanstillbeused. Wecastarayfromsourcetolistener, andforeach\ncontact we query the material properties of the impacted surface to determine\nhow much of the sound’s energy it absorbs. If it allows some energy to pass\nthrough, we can cast another ray starting on the other side of the obstacle and\ncontinue tracing the path to the listener. Once all of the sound’s energy has\nbeen absorbed, we can conclude that the sound cannot be heard. But if the\nraymakesitallthewaytothelistenerwithoutlosingallsoundenergy,wecan\nattenuate the gain of the dry sound component by the corresponding amount\nto simulate transmission of the sound.\n970 14. Audio\nAnalyzing the Indirect Path\nDetermining whether the indirect path is occluded is a much more difficult\nproblem. Ideally, we’d perform some kind of search (A* perhaps) to deter-\nmine whether or not a path exists from the source to the listener, and also\nhow much attenuation and reflection is introduced by each viable path. In\npractice, this path tracing method is rarely used because it is processor- and\nmemory-intensive. And at the end of the day, we game programmers aren’t\nreally interested in creating physically accurate simulations that will win us\nNobel prizes in physics. We merely want to produce a soundscape that is im-\nmersive andbelievable.\nNever fear, all is not lost. There are all sorts of ways in which we can ob-\ntain anapproximate model of the indirect path of a sound. For example, if we\nare using reverb regions to model the overall acoustics of the various spaces in\nour game (see Section 14.4.5.2), we could leverage these regions to determine\nwhether an indirect path exists. For example, we could use some simple rules\nof thumb:\n1. If the source and listener are in the same region, assume an indirectpath\nexists.\n2. If the source and listener are in different regions, assume the indirect\npath is occluded.\nUsingtheseassumptionscombinedwiththeresultsofourdirectpathraycast,\nwe can differentiate between the four cases: free, occluded, obstructed or ex-\ncluded.\nAccounting for Diffraction\nWhen any wave passes through a narrow opening or interacts with a corner,\nit spreads out as shown in Figure 14.34. We call this phenomenon diffraction.\nBecause of diffraction, sounds can be heard around corners as if a direct path\nexisted, as long as the angular difference between the direct path and curved\npath is not too great.\nOne way to determine whether sound can diffract in order to reach the lis-\ntener is to cast a few “curved” rays around the central “direct” ray. Most colli-\nsion engines don’t support curved path tracing, but we can emulate a curved\npath by using multiple straight-line ray casts. Figure 14.35 shows a simple ex-\nample, in which five rays are cast from the sound source to the listener—one\ndirect ray, plus two “curved” traces comprised of two straight-line ray casts\neach. Technically speaking we’re employing a piecewise-linearapproximation to\neach curved path we wish to trace.\n14.4. Rendering Audio in 3D 971\nIf the direct ray is occluded but the curved traces can “see” the listener,\nthis tells us that the listener is within the “diffraction region” around a nearby\ncorner, and should hear the sound as if it is not occluded.\nApplying the Model Using Reverb and Gain\nThus far, we’ve discussed how to determine whether the direct and indirect\npaths are blocked or not. This analysis can also tell us something about the\nacoustic impact of an occlusion or obstruction. (For example, sound passing\nthroughawallcanbemuffled;soundtakingalong“bouncy”pathmightintro-\nduce a lot of reverb.) The question now is: How do we apply this knowledge\nwhen rendering the sound?\nOne simple approach is to simply attenuate the dry and wet components\nof the sound individually, based on whether the direct or indirect paths are\ntotally or partially blocked, respectively. To finesse the results, we can also\napplymoreorlessreverbtoeachcomponentofthesound, basedonwhatever\nheuristic information we gathered when determining the path(s) taken by the\nsound. The needs of every game are different, so this is one of those times\nwhen trial and error is your best and only option!\nBlending Obstructed Sounds\nIf you were to go off and implement everything we talked about in the sec-\ntions above, you’d notice a glaring problem. As a sound source moves be-\ntween the four states described above—for example, from being free to being\nobstructed—the timbre and loudness of the sound will seem to “pop.” There\nare a number of ways to smooth out such transitions. You could apply a little\nhysteresis ,meaningthatyoudelaytheresponseofthesoundsystemtochanges\nin the obstructed state of each sound, and then use this short delay window\ntosmoothly cross-blendbetween thetwo sets of reverbsettings. But the delay\nFigure 14.34. Diffraction causes the dry component of a sound to be clearly audible even when the\ndirect path is blocked.\n972 14. Audio\nFigure 14.35. Curved ray casts can be approximated using multiple straight-line rays.\nmight be noticeable, so this isn’t an ideal solution.\nFor theUncharted andThe Last of Us series, Naughty Dog’s senior sound\nprogrammerJonathanLanierinventedaproprietarysystemthathecalled sto-\ncastic propagation modeling. Without giving away any trade secrets, I can tell\nyou that this system involves casting a bunch of rays to each sound source,\nsome direct and some indirect, and accumulating these hit/miss results over\nmany frames. From this data, we generate a probabalistic model of the de-\ngree of occlusion experienced by both the dry and wet components of each\nsound source. This allows us to smoothly transition a sound from being fully\nobstructed to fully free without noticeable “pops.”\n14.4.5.4 Sound Portals in The Last of Us\nForThe Last of Us, Naughty Dog needed a way to model the actual pathways\nthat sounds take through the environment. If an enemy NPC is speaking\nwhile standing in a long hallway that connects to the room the player is in,\nwe wanted to be able to hear the sound of his voice coming from the doorway,\nnot “through the wall” along a straight-line path.\nTo do this, we used a network of interconnected regions. There were two\nkinds of regions: roomsandportals. For each sound source, we found a path\nfrom the listener to the sound by using connectivity information provided by\nthe sound designer when laying out the regions. If both the sound source and\nlistenerwereinthesameroom,we’dusethetriedandtruemethodofperform-\ning obstruction/occlusion/exclusion analysis that we used on the Uncharted\nseries. But if the sound source was in a room directly connected to the lis-\ntener’s room via a portal, we would play the sound asifit were located in the\nportalregion. Wefoundthatweonlyneededtogo“onehop”intheroomcon-\nnectivitygraphtomakethisworkforallrealsituationsthataroseinthegame.\nObviously I’m leaving out a lot of important details here, but Figure 14.36 il-\nlustrates the basics of how this system worked.\n14.4. Rendering Audio in 3D 973\nPortal\nRegionFake\nSound\nSource\nFigure 14.36. The portal-based audio propagation model used in The Last of Us by Naughty Dog,\nInc.\n14.4.5.5 Further Reading on Environmental Acoustics\nAudio propagation modeling and acoustics analysis are areas of active re-\nsearch,andmoreandmoreadvancedtechniquesarebeingappliedinthegame\nindustry as hardware capabilities continue to improve. A few links are listed\nbelow to whet your appetite, but a Google search for “sound propagation” or\n“acoustics modeling” will provide many more hours of enjoyment!\n• “Real-Time Sound Propagation in Video Games” by Jean-François Guay\nof Ubisoft Montreal (https://bit.ly/2HdBiLc);\n• “Modern Audio Technologies in Games” presented at GDC 2003 by A.\nMenshikov (https://bit.ly/2J7FYyD);\n• “3D Sound in Games” by Jake Simpson (https://bit.ly/2HfVFTU).\n14.4.6 Doppler Shift\nAs we saw in Section 14.1.3.5, the Doppler effect is a change in frequency\nthat’s dependent upon the relative velocity between source and listener: vrel=\nvsource vlistener. Thisfrequencychangecanbeapproximatedbysimplytime-\nscaling the sound signal. This results in the “chipmunk effect” with which\nAlvin and the Chipmunks have made us all so familiar—by speeding up a\nsound, the pitch also rises. Because our sound signals are digital (i.e., sam-\npled discrete-time signals), this kind of time scaling can be accomplished via\nsample rate conversion (see Section 14.5.4.4). However, this is not strictly the",40149
100-14.5 Audio Engine Architecture.pdf,100-14.5 Audio Engine Architecture,"974 14. Audio\ncorrect thing to do, because the speeding up or slowing down of the sound\ncan become noticable.\nThe ideal solution is to apply a pitch shift without affecting the time\naxis. This can be done in a number of ways, including the phase vocoder\nandtime domain harmonic scaling approaches. A complete description of these\ntechniques is beyond our scope here, but you can read more about them at\nhttp://www.dspdimension.com/admin/time-pitch-overview.\nTime-independent pitch shifting technology is an extremely powerful\nthing to have in your audio engine, in part because it also allows you to per-\nform frequency-independent time scaling. So not only can you alter the pitch\nofsoundswithoutchangingtimingforDoppler,youcanalsospeeduporslow\ndown sounds without altering their pitch for all sorts of other cool effects.\n14.5 Audio Engine Architecture\nTo this point, we’ve discussed the concepts and methodologies behind 3D\nsound rendering, and the theory and technologies that underlie them. In this\nsection, we’ll turn our attention to the architecture of the software and hard-\nware components used to implement a 3D audio rendering engine.\nAswithmostcomputersystems,agameengine’saudiorenderingsoftware\nis typically arranged into a “stack” of layered hardware and software compo-\nnents (see Figure 14.37).\n•Hardware inevitablyservesasthefoundationofthisstructure, providing\natminimumthenecessarycircuitrytodrivethedigitaloranalogspeaker\noutputs that connect our PC or game console to a pair of headphones, a\nTV or a surround sound home theater system. Audio hardware may\nalso provide “acceleration” to the software above it in the stack by sup-\nplyingcodecs,mixers,reverbtanks,effectsunits,waveformsynthesizers\nand/orDSP chips in silicon. Thishardwareis often called the soundcard\nFigure 14.37. The audio hardware/software “stack.”\n14.5. Audio Engine Architecture 975\nbecausePCssometimesprovidetheiraudiocapabilitiesviaaplug-inpe-\nripheral card.\n• On a personal computer, the hardware is typically encapsulated in a\ndriverlayer, allowing the OS to support sound cards from a wide range\nof vendors.\n• On both PCs and game consoles, the hardware and drivers are usually\nwrapped in a low-level application programming interface (API) designed\nto free the programmer from having to deal with the minutia of control-\nling the hardware and drivers directly.\n• The3Daudio engine itself is built on top of these foundations.\nThe feature set presented to the programmer by the audio hardware/soft-\nwarestackisusuallymodeledafterthefeaturesetofa multi-channelmixercon-\nsole(http://en.wikipedia.org/wiki/Mixing_console) of the sort used in rec-\nordingstudiosandatliveconcerts(seeFigure14.38). Amixerboardcanaccept\na relatively large number of audio inputs obtained from microphones and/or\nelectronicinstruments. Theinputsoundscanbefilteredandequalized,andre-\nverb and other effects can be applied to them. The console is then used to mix\nallofthesignalstogether,settingtherelativevolumesofthesoundsasdesired\nby the sound designer. The final mixed output is routed to the speakers (for a\nlive performance) or to the individual channels of a multi-track recording.\nIn the same sense, the audio HW/SW stack must accept a large number of\ninputs (2D and 3D sounds), process them in various ways, mix them together\nso that their relative gains are set appropriately and finally pan these signals\nto the speaker output channels to produce the illusion of a three-dimensional\nsoundscape for the human player.\n14.5.1 The Audio Processing Pipeline\nAs we learned in Section 14.4.1, the process of rendering a 3D sound involves\na number of discrete steps:\n• For each 3D sound, a “dry” digital (PCM) signal must be synthesized.\n• Distance-basedattenuationisappliedtoprovideasenseofdistancefrom\nthe listener, and reverbis applied to the signal to model the acoustics\nof the virtual listening space and to provide spatialization cues to the\nlistener. This produces a new “wet” signal.\n• The wet and dry signals are panned (independently) to one or more\nspeakers in order to produce the final “image” of each signal in three-\ndimensional space.\n976 14. Audio\nFigure 14.38. A multi-channel mixer console by Focusrite with support for 72 inputs and 48\noutputs.\n• Thepannedmulti-channelsignalsofallthe3Dsoundsare mixedtogether\ninto a single multi-channel signal, which is either sent through a paral-\nlel bank of DACs and amps to drive the analog speaker outputs or sent\ndirectly to a digital output such as HDMI or S/PDIF.\nClearly, we think of the process of rendering 3D audio as a pipeline. And\nbecause a game world typically contains a large number of sound sources,\nmultipleinstancesofthispipelineareinflightsimultaneously. Forthisreason,\nwetdry SynthDistance\nAttenuation\nReverbPan\nPan\nwetdry SynthDistance\nAttenuation\nReverbPan\nPanMixer7.1 Out\nLFE \nGen6-Channel\nFigure 14.39. The audio processing graph (pipeline).\n14.5. Audio Engine Architecture 977\nthe audio processing pipeline is sometimes called the audioprocessinggraph. It\ntruly is a graph of interconnected components, ultimately culminating in the\nhandful of speaker channels that comprise the final mixed, panned output.\nFigure 14.39 presents a high-level view of the audio graph.\n14.5.2 Concepts and Terminology\nBefore we can explore the audio processing pipeline in any depth, we need\nto become familiar with a few concepts and the terminology used to describe\nthem.\n14.5.2.1 Voices\nEach 2D or 3D sound passing through the audio rendering graph is called a\nvoice. This term comes from the early days of electronic music: A synthesizer\nwouldproduce musicalnotesviaasetofwaveformgeneratorscalled“voices.”\nA synthesizer contains a limited number of waveform generator circuits,\nso electronic musicians speak of how many simultaneous voices their synth\ncan produce. In the same sense, a game’s audio rendering engine typically\nhas a limited number of codecs, reverb units and so on. The maximum num-\nber of voices supported by a particular audio HW/SW stack is dictated by\nthe number of independent parallel pathways through the audio graph. This\nnumberisgenerallyboundedbylimitedmemoryresources,limitedhardware\nresourcesand/orprocessingpowerlimitations. Thisnumberissometimesre-\nferred to as the degree of polyphony supported by the system.\n2D Voices\nA game’s audio rendering pipeline must also be capable of handling 2D\nsounds, such as music, menu sound effects, narrator voice-overs and so on.\n2Dvoicesarealsoprocessedbytheaudiorenderingpipeline. Themainthings\nthat differentiate 2D sound processing from 3D processing are:\n• 2D sounds originate as multi-channel signals, one for each available\nspeaker, whereas 3D sounds originate as dry monophonic signals. As\nsuch, 2D sounds do not pass through a pan pot.\n• A2Dsoundmaycontain“baked”reverborothereffects. Ifso,thesound\nmay not make use of the reverb capabilities of the rendering engine.\nAs such, 2D sounds typically enter the pipeline just prior to the master mixer,\nwhere they are combined with the 3D sounds to produce the final “mix.”\n978 14. Audio\nCodec\nPre-Send\nFilterPan7-ChannelGain\nPost-Send\nFilter\nwet Reverb Panwet Reverb Pandry\nwet Reverb PanDistortion\nFigure 14.40. The pipeline through which an individual 3D voice passes on its way through the\naudio graph.\n14.5.2.2 Buses\nThe interconnections between the components that make up the audio graph\nare called buses. In electronics, a bus is a circuit whose primary purpose is to\nconnect other circuits to one another. In software, a bus is nothing more than\na logical construct that describes the presence of an interconnection between\ncomponents.\n14.5.3 The Voice Bus\nFigure 14.40 presents a more-detailed view of the pipeline of components\nthrough which a single 3D voice passes as it is rendered by the audio engine.\nInthefollowingsections, we’llexploreeachofthesecomponentsindetailand\nlearn why they are interconnected in the way that they are.\n14.5.3.1 Sound Synthesis: Codecs\nAn audio signal passes through the rendering graph in digital form. The\ntermsynthesis describes the process of generating these digital signals. Au-\ndio signals may be synthesized by simply “playing back” a pre-recorded au-\ndio clip. They might also be procedurally generated, perhaps by combining\none or more fundamental waveforms (sinusoid, square wave, sawtooth, etc.),\nand/or by applying various filters to a harmonically rich noise signal. Since\n14.5. Audio Engine Architecture 979\nmost games use pre-recorded audio clips almost exclusively, we’ll restrict our\ndiscussion to them here.\nPre-recorded audio clips can be provided to the game engine in any one\nof the myriad compressed and uncompressed audio file formats in use today\n(see Section 14.3.2.3). Raw PCM data is the “canonical” format accepted by\nthe various components in the audio processing graph. Therefore, a device\nor software component known as a codecis used to convert each source audio\nclip into a raw PCM data stream. The codec interprets the source data format,\ndecompresses the data if necessary, and then transmits it onto the voice bus\nfor its journey through the audio processing graph.\n14.5.3.2 Gain Control\nTheloudnessofeachsourcesoundinthe3Dworldcanbecontrolledinanum-\nber of ways: When recording the audio clip, we can set the recording levels to\nproduce a sound at the desired loudness. We can process the clip in an offline\ntool to adjust its gain. At runtime, we can also dynamically adjust the volume\nof the clip using a gaincontrol component within the audio graph. See Section\n14.3.1.7 for a detailed discussion of gain control.\n14.5.3.3 Aux Sends\nWhen a sound engineer at a recording studio or live concert wants to apply\neffects to a sound, he or she can route the sound out of the multi-channel mix-\ning console, through an effects “pedal,” and then back into the mixing board\nfor further processing. These outputs are known as auxiliary send outputs, or\naux sends for short.\nWithintheaudioprocessinggraph, theterm“auxsend”isusedinananal-\nogous manner: It describes a bifurcation point in the pipeline, splitting the\nsignal into two parallel signals. One of these signals is for the dry component\nofthesound. Theotherispipedthroughareverb/effectscomponenttocreate\nthe wet component of the sound.\n14.5.3.4 Reverb\nThe wet signal path is typically routed through a component that adds early\nreflectionsandlatereverberations. Reverbmightbeimplementedusingacon-\nvolution, as described in Section 14.4.5.1. If convolution is not practical in\nreal time, either because the console or PC lacks DSP hardware or because the\ngame’s CPU and/or memory budgets are insufficient, reverb can be imple-\nmented using a reverb tank . This is essentially a buffering system that caches\ntime-delayed copies of a sound that are then mixed with the original to mimic\nearlyreflectionsand/orlatereverberations,combinedwitha filtertomimicthe\n980 14. Audio\ninterference effects and general attenuation of high-frequency components in\nthe reflected sound waves.\n14.5.3.5 Pre-Send Filter\nThevoicepipelinetypicallyincludesafilterthatisappliedbeforetheauxsend\nbifurcation, and therefore applies to both the dry and wet components of the\nsound. This is called a pre-sendfilter. It is generally used to model phenomena\nthat occur at the source of the sound. For example, we could mimic the sound\nof someone wearing a gas mask with a pre-send filter.\n14.5.3.6 Post-Send Filter\nAnother filter is typically provided after the aux send bifurcation. As such,\nthis filter only applies to the dry component of the sound. This filter can be\nuseful for modeling the muffling effect of an obstruction/occlusion on the di-\nrect sound path. At Naughty Dog, we also use a post-send filter to implement\nthe frequency-specific fall-off that occurs due to atmospheric absorption (see\nSection 14.1.3.2).\n14.5.3.7 Pan Pots\nThe dry and wet components of a 3D sound are monophonic signals through-\nouttheirjourneyalongthevoicebus. Attheveryendofthepipeline,eachone\nof these two mono signals must be panned to the two stereo speakers/head-\nphones or the five or seven surround sound speakers. For this reason, every\n3D voice bus terminates in two or more pan pots, one for the dry signal and\none or more for the wet. The components may be panned differently. The dry\nsignal is panned according to the actual location of the source. The wet sig-\nnal, however,maybepannedwithawiderfocustosimulatethewayinwhich\nreflected sound waves tend to impinge on the listener’s head from various di-\nrections. If the sound is coming from a narrow doorway, the focus of the wet\nsignal might be only a few degrees. But if the listener is standing in the cen-\nter of a cavernous hall, the wet signal should probably be given a 360-degree\nfocus (i.e., it should be rendered in all speakers equally).\n14.5.4 Master Mixer\nEach pan pot’s output is a multi-channel bus, containing signals for each of\nthe desired output channels (stereo or surround sound). A game typically has\na large number of 3D sounds playing simultaneously. The master mixer takes\nall of these multi-channel inputs and mixes them together into a single multi-\nchannel signal for output to the speakers.\n14.5. Audio Engine Architecture 981\nDepending on the specifics of the implementation, the master mixer might\nbeimplementedinhardware,oritmightliveentirelyinsoftware. Ifthemaster\nmix is performed in hardware, the sound card designer has the option of per-\nforming an analog mix or a digital mix. (Software can only do digital mixing,\nfor obvious reasons.)\n14.5.4.1 Analog Mixing\nAn analog mixer is essentially just an summation circuit—the amplitudes of\nthe individual input signals are added together, and the resultant wave’s am-\nplitude is then attenuated to fall back within the desired signal voltage range.\n14.5.4.2 Digital Mixing\nMixing can also be performed digitally by software running on a dedicated\nDSP chip or a general-purpose CPU. A digital mixer takes multiple PCM data\nstreams as its inputs, and produces a single PCM data stream at its output.\nA digital mixer’s job is a little more complicated than that of an analog\nmixer, because the collection of PCM channels it is combining may have been\nrecorded at different sample rates and/or different bit depths. Two processes\nknown as sample depth conversion andsample rate conversion must be executed\non all of the mixer’s input signals to bring them into a common format. Once\nthis has been done, mixing again becomes trivial. At each time index, the val-\nues of all the input samples are simply added together, and the final output\namplitude is adjusted if necessary to bring the combined signal into the de-\nsired volume range.\n14.5.4.3 Sample Depth Conversion\nIf thebit depths of the mixer’s input signals differ, sample depth conversion can\nbe used to convert them to a common format. This operation is trivial. We\nsimply de-quantize the input sample values into floating-point format, and\nthen re-quantize each one at the desired output bit depth. See Section 12.8.2\nfor all the gory details on quantization.\n14.5.4.4 Sample Rate Conversion\nIfthesamplerates oftheinputsignalsdiffer, samplerateconversion mustbeused\nto convert them all into the desired output sample rate prior to mixing. In\nprinciple, this involves converting the signal into analog form, and then re-\nsampling it at the desired rate (which could be done using D/A and A/D\nhardware). In practice, analog sample rate conversion tends to introduce un-\nwanted noise, so the conversion is almost always accomplished by running a\ndirect digital-to-digital algorithm directly on the PCM data stream.\n982 14. Audio\nPre-\nAmp\nLFE\nGenEQ\nCompressor7.1 Out\nVol.7.1-Channel 7-Channel\nFigure 14.41. A typical master output bus.\nAn understanding of signal processing theory (see Section 14.2) is neces-\nsarytofullyunderstandhowthesealgorithmsfunction,andafulldiscussionis\nbeyondourscopehere. Butincertainsimplecases,theconceptiseasyenough\nto grasp. For example, if we are doubling the sample rate, we can interpo-\nlate adjacent samples and insert these values as new samples, thereby dou-\nbling the number of samples. It’s not quite as simple as this—one must take\ncare to avoid introducing aliasing into the resulting signal, for example. See\nhttp://en.wikipedia.org/wiki/Sample_rate_conversion for a detailed discus-\nsion of sample rate conversion.\n14.5.5 The Master Output Bus\nOnce the voices have been mixed, they are processed by the masteroutputbus.\nThisisacollectionofcomponentsthatprocesstheoutputpriortosendingitto\nthe speakers. A typical master output bus is depicted in Figure 14.41, and its\ncomponents are described briefly below. Every audio engine does things a bit\ndifferently,andnotallenginessupportallofthecomponentsdescribedbelow.\nSome engines may also introduce additional components not shown here.\n•Pre-amp. The pre-amp allows the master signal’s gain to be trimmed\nprior to passing through the remainder of the output bus.\n•LFE generator . As we mentioned in Section 14.4.4, a pan pot only drives\nthetwo,fiveorseven“main”speakersofastereoorsurroundsoundsys-\ntem. TheLFE(subwoofer)channeldoesnotcontributetothepositioning\nofasound’s3Dimage. An LFEgenerator isacomponentthatextractsthe\nvery lowest frequencies of the final mixed signal and uses this to drive\nthe LFE channel.\n14.5. Audio Engine Architecture 983\n•Equalizer . Most audio engines provide some kind of equalizer (EQ). As\ndescribed in Section 14.2.5.8, an EQ allows specific frequency bands in\nthesignaltobeboostedorattenuatedindividually. AtypicalEQdivides\nthespectrumupintoanywherefromfourtotensofindividuallytunable\nbands.\n•Compressor. A compressor performs dynamic range compression (DRC)\non the audio signal. A compressor reduces the volume of the loudest\nportions of the signal and/or increases the volume of the quietest mo-\nments. Itdoesthisautomaticallybyanalyzingtheinputsignal’svolume\ncharacteristics and adjusting the compression dynamically. See http://\nen.wikipedia.org/wiki/Dynamic_range_compressionforadetaileddis-\ncussion of DRC.\n•Master gain control . This component allows the overall volume of the\nentire game to be controlled.\n•Outputs. The output of the master bus is a collection of line-level analog\nsignals corresponding to the speaker channels and/or a digital HDMI\nor S/PDIF multi-channel signal, suitable for transmission to a TV or a\nhome theater system.\n14.5.6 Implementing a Bus\n14.5.6.1 Analog Buses\nAnanalogbusisimplementedviaanumberofparallelelectronicconnections.\nTo carry a monophonic audio signal, we need two parallel wires or “lines” on\nthe circuit board: one to carry the voltage signal itself, and one to serve as\nground.\nAn analog bus operates pretty much instantaneously. The output signal\nfrom an upstream component is immediately consumed by the next down-\nstream component, because the signal is a continuous physical phenomenon.\nSuch circuits are quite simple. The only real complication is ensuring that the\nvoltage levels and impedances of the input and output signals match.\n14.5.6.2 Digital Buses\nOne could imagine using simple digital circuitry to build instantaneous con-\nnections between our digital components. However, this would require the\nconnected components to run in perfect lock-step: At the exact moment that\nthe sender produces a byte of data, the receiver would have to consume it.\nOtherwise, the byte would be lost.\nToovercomethesynchronizationproblemsinherentinconnectingtwodig-\nital components, ring buffers are typically used at the input and/or output of\n984 14. Audio\neach component. A ring buffer is a buffer that can be shared by two clients—\nonereaderand one writer. To make this work, we maintain two pointers or\nindices within the buffer, called the read head and thewrite head. The reader\nconsumes data at the read head, advancing it forward in the buffer as data is\nconsumed, and wrapping when the end of the buffer is reached. The writer\nstores data into the buffer at the write head, advancing and wrapping as well.\nNeither head is permitted to “pass” the other, which guarantees that the two\nclientscannotconflictwithoneother(i.e.,readingdatathathasn’tbeenwritten\nyet, or writing over top of data that is currently being read).\nThesimplestwaytoconnect,say,thedigitaloutputofacodectothedigital\ninputofaDACistousea sharedringbuffer. Thecodecwritestotheexactsame\nbuffer read by the DAC.\nWhilesimple,thesharedbufferapproachonlyworkswhenthetwocompo-\nnents have access to the same physical memory. This is trivial to do when the\ncomponents are running in threads on a single CPU. To make a shared mem-\nory approach work between two separate operating system processes, each of\nwhich has its own private virtual memory space, the OS needs to provide a\nmechanism for mapping the same physical memory into each process’s vir-\ntual address space. This is usually only possible when the two processes\nare running on the same core, or on different cores within a multicore com-\nputer.\nIf the two components are running on different cores that cannot share\nmemory (as would be the case if one were running on the PC and the other on\na plug-in sound card, for example), then each component needs its own input\noroutputbuffer. Datamustbe copiedfromtheoutputbufferofonecomponent\nto the input buffer of the other. This might be accomplished via a direct mem-\noryaccesscontroller (DMAC),asisthecasewhentransferringdatabetweenthe\nPPU and the SPUs on the PS3. It might also be accomplished via a specialized\nbus, such as the ubiquitous PCI Express (PCIe) bus that is used to connect the\nmain CPU core(s) to plug-in peripheral cards on a PC.\n14.5.6.3 Bus Latency\nIn order to play sound, the game or application must feed audio data period-\nically into the codecs that ultimately drive the speaker outputs. We call this\nservicing the audio. The rate at which the game or app services its audio is\ncrucial to proper sound production: If packets are sent too infrequently, the\nbufferswill underflow, meaningthatthedeviceconsumesallofthedatabefore\na new packet arrives. This causes the audio to drop out briefly while the soft-\nware catches up. If packets are sent too often, the PCM buffers can overflow,\ncausing packets to be lost. This causes the audio to seem to “skip.”\n14.5. Audio Engine Architecture 985\nThesizeoftheinputandoutputbuffersthatcompriseadigitalbusdictates\nthelatencyof the sound system—in other words, how much delay is intro-\nduced by the bus. If the buffers are very small, latency is minimized, but this\nplaces a greater burden on the CPU because it must feed the buffers more fre-\nquently. Likewise, larger buffers translate into less CPU load but also higher\nlatency. We usually measure the latency of a piece of audio hardware in mil-\nliseconds, rather than measuring the size of the buffers in bytes. This is done\nbecause buffer size depends upon the data format and the degree of compres-\nsionsupportedbythecodec,butthelatencyisreallywhatwecareaboutwhen\ntrying to produce high-fidelity sound.\nHowmuchlatencyisacceptable? Thisdependsontheapplication. Profes-\nsionalaudiosystemsrequireveryshortlatencies—ontheorderof0.5ms. This\nis because audio signals are often fed through a network of audio hardware\nbefore being synchronized with each other and often to a video signal as well.\nEvery time latency is introduced by the hardware, accurate synchronization\nbecomes more difficult.\nGameconsoles, ontheotherhand, cantoleratealongerlatency. Inagame,\nall we care about is synchronizing the audio and the graphics. If the game is\nrendering at 60 FPS, this translates into 1/60 =16.6ms per frame. As long\nas the audio isn’t delayed by more than 16 ms, we know it will be in sync\nwith graphics rendered for that same frame. (In fact, many games use double\nor triple buffering for their rendering engines, which introduces one or two\nframes of delay between the time the game requests that a frame be drawn\nand when that frame will actually appear on the TV screen. The television\nmayalsointroduceadelay. Assuch,atriple-buffered60Hzgamecanactually\ntolerateanaudiolatencyof 316=48msormore.) ThePlayStation3’sDMA\ncontroller runs every 5.5 ms, so PS3 audio systems are typically configured\nsuch that the audio buffers can hold an integer multiple of 5.5 ms worth of\naudio.\n14.5.7 Asset Management\n14.5.7.1 Audio Clips\nThemostatomicaudioassetisa clip—asingledigitalsoundassetwithitsown\nlocal timeline (analogous to an animation clip). A clip is sometimes called\nasound buffer, because the digital sample data is stored in a buffer. A clip\nmight encapsulate monophonic audio data (typical for 3D sound assets), or\nit might contain multi-channel audio (typically used for 2D assets or stereo\nsound sources in 3D). A clip may be stored in any of the audio file formats\nsupported by your engine.\n986 14. Audio\n14.5.7.2 Sound Cues\nAsound cue is a collection of audio clips plus metadata that describes how\nthey should be processed and played. Cues are usually the primary means\nby which the game can request sounds to be played. (Playing individual clips\nmay or may not be supported by the engine.) Cues also serve as a convenient\ndivision-of-labormechanism: Thesounddesignerscancraftthecuesusingan\noffline tool, without having to worry about how or when they’ll be played in-\ngame. Andthegameprogrammerscanplaythecuesconvenientlyinresponse\ntorelevanteventsinthegame,withouthavingtoworryaboutmicromanaging\nthe details of playback.\nThere are many ways in which the collection of clips in a cue could be\ninterpreted and played back. A cue might contain clips representing the six\nchannels of a pre-mixed 5.1 music recording. A cue might also collect up a set\nof raw sounds, from which a random selection can be made, for the sake of\nvariety. A cue might also be set up to play its collection of raw sounds in a\npredefined sequence. A cue typically specifies whether the sound(s) it encap-\nsulates represent a one-shot sound or a looping sound.\nSome audio engines permit a cue to provide one or more optional sound\nclips that only play if the main sound is interrupted midway through playing.\nFor example, a vocal cue might include a “glottal stop” sound that is played\nonly if the person’s line of dialog is interrupted. This feature can also be used\ntoprovideadistinct“tail”soundthatisplayedwhenaloopingcueisstopped.\nFor example, a looping machine gun sound cue might use this “tail” clip fea-\nture to produce a suitable echoing fall-off sound when the firing ceases.\nAcue’smetadatamightincludewhetheritisintendedtobeplayedin3Dor\n2D;theFOmin,FOmaxandfall-offcurveofthesoundsource;groupmember-\nship (see Section 14.5.8.1); and possibly any special effects, filtering or equal-\nization that should be applied when the sound is played. In Sony’s Scream\nengine—the sound engine used by Naughty Dog in its Uncharted andTheLast\nof Usseries—a cue can contain arbitrary script code that allows a sound de-\nsigner to completely control how the encapsulated sound asset(s) are played\nwhen the cue plays.\nPlaying a Cue\nEvery audio engine that supports the concept of cues provides an API for\nplaying them. This API is usually the primary way—and sometimes the only\nway—for the game code to request that a sound be played.\nThe cue playing API generally allows the programmer to specify whether\nthe cue should be played as a 2D or 3D sound, to provide 3D position and\n14.5. Audio Engine Architecture 987\nvelocity parameters, to specify whether the sound should loop or play only\nonce, and to specify whether the source buffer is in-memory or streamed. The\nAPI usually also allows us to control the volume of the sound and possibly\nother aspects of playback.\nMost APIs return a sound handle to the caller. This handle allows the pro-\ngram to keep track of a sound as it is playing, so that it can be modified or\ncanceled before the sound ends. A sound handle is usually implemented un-\nderthehoodasanindexintoaglobalhandletable,ratherthanasarawpointer\ntothedatathatdescibesthesoundinstance. Thatway, ifthesoundendsnatu-\nrally, the handle can be “nulled out” automatically. A handle mechanism can\nalso be used to make the system thread-safe—if one thread kills the sound,\nother threads that have handles to the sound will automatically see their han-\ndles become invalid.\n14.5.7.3 Sound Banks\nA 3D audio engine manages a lotof assets. The game world contains a large\nnumber of objects. Each object can generate a variety of sounds. And in ad-\ndition to 3D sound effects, we have music, speech, menu sound effects and so\non.\nAll of this audio data takes up an immense amount of space, so we can’t\nkeepitallinmemoryatonce. Ontheotherhand,theindividualaudioclipsare\ntoo fine-grained and too numerous for them to be managed on an individual\nbasis. As such, most game engines package their sound clips and cues into\ncoarse units called soundbanks.\nSomesoundbanks areloadedwhenthegamestartsupandleftinmemory\nforever. Forexample,thecollectionofsoundsmadebytheplayercharacterare\nalways needed, so we could keep them in memory indefinitely. Other banks\nmight be loaded and unloaded dynamically as the needs of the game change.\nForexample,thesoundsinlevelAmightnotbeusedinlevelB,sowecanload\nthe “A” bank only when level A is being played. For example, in Naughty\nDog’sThe Last of Us, the sounds of rain, flowing water and the creaking of\nbeams on the verge of collapse were only loaded when the player was in the\ntilted building in Boston.\nSome sound engines permit banks to be relocated in memory. This feature\ncan entirely eliminate the memory fragmentation problems that would other-\nwise arise as lots of differently sized banks are loaded and unloaded during\ngameplay. See Section 6.2.2.2 for more information on memory relocation.\n988 14. Audio\n14.5.7.4 Streaming Sounds\nSome sounds are so long in duration that they cannot be conveniently stored\nin memory all at once. Music and speech are common examples. For these\nkinds of sounds, many game engines support streamingaudio.\nStreaming audio is possible because when playing a sound, the only in-\nformation we actually need is the signal data at and around the current time\nindex. To implement streaming, we maintain a relatively small ring buffer for\neach streaming sound. Prior to playing the sound, we pre-load a small chunk\nof it into the buffer, then play the sound normally. The audio pipeline con-\nsumes the data from the ring buffer as it plays, making room for us to load\nmore data. As long as we keep filling the buffer with data before it is all con-\nsumed, our sound will play seamlessly.\n14.5.8 Mixing Your Game\nIfweweretoplayeverysoundcomingfromeverygameobject,properlyatten-\nuated and spatialized and acoustically modeled, using all the techniques and\ntechnology we’ve discussed thus far, what would be the result? We might ex-\npecttheanswertothisquestiontobe“anincrediblyimmersiveandbelievable\nsoundscape that wins awards and makes us rich!” But what we’d actually get\nis cacophony.\nWhatseparatesagoodgamefromagreatgameisthe mix—whatyouhear,\nhow much of it you hear, and just as importantly what you don’t hear. The\ngoal of a game’s sound designer is to produce a final mix that:\n• sounds realistic and emersive;\n• isn’t too distracting, annoying or difficult to listen to;\n• conveys all information relevant to gameplay and/or story effectively;\nand\n• maintains a mood and tonal color that is always appropriate, given the\nevents taking place in the game and the overall design of the game.\nAll sorts of different kinds of sounds must come together in the game’s\nmix. These include music, speech, ambient sounds like rain, wind, insects or\nthe creak of an old building, sound effects such as weapons fire, explosions\nand vehicles, and the bumps, slides and rolling sounds made by physically\nsimulated objects.\nVarioustechniquesareemployedtoensurethemixofthegamemeetsthese\ngoals. We’ll explore a few of them in the following sections.\n14.5. Audio Engine Architecture 989\n14.5.8.1 Groups\nThe most obvious thing we can do to improve the mix of our game is to set\nthe levels of all the source sounds in the 3D world appropriately. The impor-\ntant thing here is to ensure that each sound’s gain is appropriate relative to\nthe other sounds in the game. For example, footsteps should be quieter than\ngunfire.\nIn some games, the loudness of certain sounds needs to change dynami-\ncally. Often we want to control an entire category of sounds at once. For ex-\nample, during a frenetic fight sequence, we might want to bring up the levels\nofthemusicandtheweapons,anddropthevolumeofancillarysoundeffects.\nOr during a quiet moment with characters talking to one another, we might\nwant to boost the speech a little and tone down ambient sounds to ensure the\ndialog can be heard.\nFor this reason, many audio engines support the concept of groups—a con-\ncept“stolen”fromouroldfriendthemulti-channelmixingconsole. Onamix-\ning board, a collection of sound inputs can be routed to an intermediate mixer\ncircuit, combining them into a single “group signal.” The gain of this signal\ncan then be controlled with a single knob on the board, thereby allowing the\nsound engineer to control the loudness of all input signals at once.\nIn the software world, groups are implemented by simply categorizing\nsound cues, rather than physically mixing their signals together. For exam-\nple, we can classify a cue as being music, a sound effect, a weapon, a line of\nspeech and so on. Then, the engine can provide a means of controlling the\ngains of all sounds in each category with a single control value. Groups usu-\nally also allow entire categories of sound to be paused, restarted and muted\nconveniently with a simple API call.\nSome sound engines do also provide a mechanism for physically mixing\ngroupsofaudiosignalsintoasinglesignal,justasisdonewhenworkingwith\ngroupsonamixingconsole. InSony’sScreamengine, thisiscalledgenerating\napre-master submix. After the relative gains of the signals in the group have\nbeen locked down by the submix, the resulting signal can be routed through\nadditional filters or other processing stages. This gives the sound designer\neven more control over the mix of the game.\n14.5.8.2 Ducking\nDucking is a technique in which the volume/gain of certain sounds are tem-\nporarily reduced in order to make other sounds more audible. For example,\nwhen a character is speaking, the level of background noise could be reduced\nautomatically to make the dialog more audible.\n990 14. Audio\nDucking can be triggered in numerous ways. The presence of one particu-\nlar type of sound might be used to duck another category of sounds. A game\neventmighttriggeraduckprogrammatically. Anytriggeringmechanismthat\nis deemed appropriate can be used to initiate a duck.\nThe reduction of volume caused by a duck is typically accomplished via\nthe group categorization system: When one category of sound is playing, it\ncan automatically duck one or more other categories by various amounts. Or\nthegamecodecancallafunctiontoduckagroupofsoundsprogrammatically.\nDucking can also be performed by routing one sound signal into the side-\nchaininputofthedynamicrangecompressor(DRC)onadifferentvoice’sbus.\nRecall from Section 14.5.5 that a DRC analyzes the volume characteristics of a\nsignal and automatically compresses the loudness of the signal appropriately.\nWhenaside-chaininputisconnectedtoaDRC,itanalyzesthe side-chain signal\nwhen deciding how to adjust the volume. So, we can arrange for increased\nloudness in one signal to cause a decrease in the dynamic range of another\nsignal.\n14.5.8.3 Bus Presets and Mix Snapshots\nManysoundenginesallowthesounddesignertosetupconfigurationparam-\neters, save them off and then recall and apply them conveniently at runtime.\nIn Sony’s Scream engine, these come in two basic flavors: bus presets andmix\nsnapshots.\nA bus preset is a set of configuration parameters that control aspects of the\ncomponents on a single bus (voice bus or master output bus). For example,\na bus preset might describe one particular reverb set-up that mimics, say, the\nacousticsofalargeopenhall,ortheinteriorofacar,orasmallbroomcloset. Or\na bus preset might control the DRC settings on the master output bus. Many\nsuch presets can be created by the sound designer, and the appropriate ones\nactivated at runtime as the game requires.\nA mix snapshot is the same kind of idea applied to gain control. The gains\nof the various channels within a group can be established a priori and then\napplied at runtime as needed.\n14.5.8.4 Instance Limiting\nInstancelimiting isameansofcontrollingthenumberofsoundsthatarepermit-\nted to play simultaneously. For example, even though 20 NPCs are all firing\ntheir guns at once, we might only play the three or four gun sounds that are\nnearest to the listener. Instance limiting is important for two reasons: First,\nit’s a great way to prevent cacophony. Second, a sound engine typically sup-\nports only a fixed number of simultaneous voices, either because of hardware\n14.5. Audio Engine Architecture 991\nlimitations (e.g., the sound card only has Ncodecs) or because of memory or\nprocessor bandwidth limitations in the software, so we must use them wisely.\nPer-Group Limits\nInstance limiting is sometimes applied differently to different groups of\nsounds. For example, we might specify that we should play up to four guns\nsimultaneously, hear up to three people speaking at a time and allow up to\nfive other sound effects to play at once, plus up to two overlapping music\ntracks.\nPrioritization and Voice Stealing\nIn a 3D game with lots of dynamic elements, there may be more sounds play-\ning at any given time than the system has voices for. Some sound engines\nsupportalargenumber(orevenaninfinitenumber)of virtualvoices. Eachvir-\ntualvoicerepresentsasoundthatistechnicallyplaying,butthatcanbemuted\nor stopped temporarily so it ceases to occupy valuable hardware or software\nresources. The engine uses various criteria to dynamically determine which\nvirtual voices should be mapped to “real” voices at any given moment.\nOneofthesimplestwaystolimitthenumberofsoundsplayingsimultane-\nously is to assign a maximum radius to every 3D sound source. As we saw in\nSection14.4.3.1,thisistheFOmaxradius. Ifthelistenerisbeyondthisdistance\nfrom the sound, it is considered inaudible and its virtual voice is temporarily\nmuted or stopped, freeing its resources for use by other voices. The process of\nautomatically silencing virtual voices is called voice stealing.\nAnothercommonapproachistoassigneachcueorgroupofcuesa priority.\nWhen too many virtual voices are playing at once, those with lower priorities\ncan be silenced (stolen) in favor of higher priority voices.\nSoundenginesmayalsoprovidevariousothermechanismsforcontrolling\nthe details of the voice stealing algorithm. For example, a cue might be given\na minimum play time, after which its voice is permitted to be stolen. Sounds\nmightbefadedoutratherthancutoffabruptlywhentheirvoiceisstolen. And\nsome cues might be temporarily marked as “unstealable” to ensure that they\nplay, even despite their priority settings.\n14.5.8.5 Mixing In-Game Cinematics\nUnder normal gameplay conditions, the listener or “virtual microphone” is\ntypically positioned at or near the location of the camera, and the sound\nsourcesaremodeledwheretheyreallyareintheenvironment. Distance-based\nattenuation, direct and indirect sound path determination, voice limiting—all\n992 14. Audio\nare determined using these realistic positions.\nHowever, during an in-game cinematic—a portion of the game in which\nplayer control is suspended so that a story moment can take place—the cam-\nera often pans out away from the player’s head. This kind of thing tends to\nwreak havoc with our 3D audio system. We could just keep the listener/mic\nlocked to the location of the camera; but this is not always appropriate. For\nexample, if there’s a long shot of two characters speaking, we probably still\nwant to mix so that the characters’ voices can be heard, even though physi-\ncally speaking they are too far away to be heard. In this case, we might want\nto detach the listener from the camera, and artificially position it nearer to the\ncharacters.\nMixingin-gamecinematicsisalotclosertomixingafilm. Assuch,asound\nengineneedstobecapableof“breakingtherules”anddoingthingsthataren’t\nnecessarily physically realistic.\n14.5.9 Audio Engine Survey\nIt should be evident by now that creating a 3D audio engine is a massive un-\ndertaking. Luckily for us, lots of people have already put a great deal of effort\ninto this task, and the result is a wide range of audio software that we can use\npretty much out of the box. This ranges from low-level sound libraries all the\nway to fully featured 3D audio rendering engines.\nIn the following sections, we’ll survey a few of the most common audio\nlibrariesandengines. Someofthesearespecifictoaparticulartargetplatform,\nwhile others are cross-platform.\n14.5.9.1 Windows: The Universal Audio Architecture\nIn the early days of PC gaming, the feature set and architecture of PC sound\ncards varied a great deal from platform to platform and vendor to vendor.\nMicrosoftattemptedtoencapsulateallofthisdiversitywithinitsDirectSound\nAPI, supported by the Windows Driver Model (WDM) and the Kernel Audio\nMixer (KMixer) driver. However, because vendors could not agree on a com-\nmon feature set or set of standard interfaces, the same functionality would of-\nten be realized in very different ways on different sound cards. This required\nthe operating system to manage a very large number of incompatible driver\ninterfaces.\nFor Windows Vista and beyond, Microsoft introduced a new standard\ncalled the Universal Audio Architecture (UAA). Only a limited set of hard-\nware features are supported by the standard UAA driver API—all remain-\ning features are implemented in software (although hardware manufacturers\n14.5. Audio Engine Architecture 993\nare still free to provide additional “hardware acceleration” features, as long\nas they provide custom drivers to expose them). Although the introduction\nof UAA limited the competitive advantage of prominent sound card vendors\nlikeCreativeLabs,itdidhavethedesiredeffectofcreatingasolid,feature-rich\nstandard, which could be used by games and PC applications in a convenient\nway.\nThe UAA standard had another positive effect on the user’s aural experi-\nence. In the old DirectSound days, a game could take complete control of the\nsound card, meaning that sounds coming from the OS or other applications\nsuch as an email program would be “locked out” and their sounds would not\nplay while the game was running. The new UAA architecture allowed the OS\nto claim ultimate control over the final mix heard through the PC’s speakers.\nMultiple applications could finally share the sound card in a reasonable and\nconsistentway. Searchonlinefor“UniversalAudioArchitecture”tofindmore\ninformation on UAA.\nThe UAA is implemented on Windows by the so-called Windows Audio\nSession API, or WASAPI for short. This API is not really intended for use by\ngames. It supports most advanced audio processing features in software only,\nwith limited support for hardware acceleration. Instead, games usually make\nuse of the XAudio2 API, which is described in the next section.\n14.5.9.2 XAudio2\nXAudio 2 is the high-powered low-level API that provides access to the au-\ndio hardware on Xbox 360, Xbox One and Windows. It replaces DirectAudio\nand provides access to a wide range of hardware-accelerated features includ-\ning programmable DSP effects, submixing, support for a wide range of com-\npressedanduncompressedaudioformats,andmultirateprocessingtolighten\nthe load on the main CPU(s).\nAtop the XAudio2 API sits a 3D audio rendering library called X3DAudio.\nThese APIs are also available on the Windows platform for use by PC games.\nMicrosoft used to offer a powerful audio authoring tool called the “cross-\nplatform audio creation tool” or XACT for short, which was intended for\nuse with XNA Game Studio, but neither XNA nor XACT are supported any\nlonger.\n14.5.9.3 Scream and BoomRangBuss\nOn the PS3 and PS4, Naughty Dog uses Sony’s 3D audio engine Scream and\nits synth library, BoomRangBuss.\nTheaudiohardwareonaPlayStation3isverymuchlikeaUAA-compliant\naudio device, supporting up to eight channels of audio for full 7.1 surround\n994 14. Audio\nsound support, plus a hardware mixer and HDMI, S/PDIF, analog and USB/\nBluetoothoutputs. ThisaudiohardwareisencapsulatedbyacollectionofOS-\nlevel libraries including libaudio, libsynth and libmixer. On top of these li-\nbraries, game makers are free to implement their own audio software stacks.\nSonyalsoprovidesapowerful3D-capableaudiostackofitsowncalled Scream\nwhich game studios can use “out of the box.” Scream is available on the\nPS3, PS4 and PSVita platforms. Its architecture mimics a fully featured multi-\nchannel mixer console.\nOn top of Scream, Naughty Dog implemented a proprietary 3D envi-\nronmental audio system for use on the Uncharted andThe Last of Us series.\nThissystemprovidesstochasticobstruction/occlusionmodelingandaportal-\nbasedaudiorenderingsystemthatpermitsrenderingahighlyrealisticsound-\nscape.\nAdvanced Linux Sound Architecture\nThe Linux equivalent of the UAA driver model is called the Advanced Linux\nSound Architecture (ALSA). This Linux kernel component replaced the orig-\ninal Open Sound System (OSSv3) as the standard way to expose audio func-\ntionality to applications and games. See http://www.alsa-project.org/main/\nindex.php/Main_Page for more information on ALSA.\nQNX Sound Architecture\nQNX Sound Architecture (QSA) is a driver-level audio API for the QNX\nNeutrino real-time OS. As a game programmer, you’ll probably never use\nQNX. But its documentation does provide an excellent picture of the concepts\nand the typical feature set of audio hardware. See http://www.qnx.com/\ndevelopers/docs/6.5.0/index.jsp?topic=%2Fcom.qnx.doc.neutrino_audio%2\nFmixer.html for these docs.\n14.5.9.4 Multiplatform 3D Audio Engines\nA number of powerful, ready-to-use cross-platform 3D audio engines are\navailable. We’ll outline the most well-known of these below.\n•OpenAL is a cross-platform 3D audio rendering API that has been de-\nliberately designed to mimic the design of the OpenGL graphics li-\nbrary. Early versions of the library were open source, but it is now li-\ncensed software. A number of vendors provide implementations of the\nOpenALAPIspec,includingOpenALSoft(http://kcat.strangesoft.net/\nopenal.html and AeonWave-OpenAL (http://www.adalin.com).",47082
101-14.6 Game-Specific Audio Features.pdf,101-14.6 Game-Specific Audio Features,"14.6. Game-Speciﬁc Audio Features 995\n•AeonWave 4D is a low-cost audio library for Windows and Linux by\nAdalin B.V.\n•FMODStudio isanaudioauthoringtoolthatfeaturesa“proaudio”look\nand feel (http://www.fmod.org). A full-featured runtime 3D audio API\nallows assets created in FMOD Studio to be rendered in real time on the\nWindows, Mac, iOS and Android platforms.\n•MilesSoundSystem isapopularaudiomiddlewaresolutionbyRadGame\nTools (http://www.radgametools.com/miles.htm). It provides a pow-\nerful audio processing graph and is available on virtually every gaming\nplatform imaginable.\n•Wwiseis a 3D audio rendering engine by Audiokinetic (https://www.\naudiokinetic.com). It is notably not based around the concepts and fea-\ntures of a multi-channel mixing console, but rather presents the sound\ndesignerandprogrammerwithauniqueinterfacebasedongameobjects\nand events.\n• TheUnrealEngine of course provides its own 3D audio engine and pow-\nerful integrated tool chain (http://www.unrealengine.com). For an in-\ndepth look at Unreal’s audio feature set and tools, see [45].\n14.6 Game-Speciﬁc Audio Features\nOntopofthe3Daudiorenderingpipeline,gamestypicallyimplementallsorts\nof game-specific features and systems. Some examples include:\n•Split-screen support. Multiplayer games that support split-screen play\nmust provide some mechanism that allows multiple listeners in the 3D\ngame world to sharea single set of speakers in the living room.\n•Physics-drivenaudio . Games that support dynamic, physically simulated\nobjects like debris, destructible objects and rag dolls require a means of\nplaying appropriate audio in response to impacts, sliding, rolling and\nbreaking.\n•Dynamic music system. Many story-driven games require the music to\nadapt in real time to the mood and tension of events in the game.\n•Character dialog system. AI-driven characters seem a great deal more re-\nalistic when they speak to one another and to the player’s character.\n•Sound synthesis. Some engines continue to provide the ability to synthe-\nsize sounds “from scratch” by combining various kinds of waveforms\n(sinusoid, square, sawtooth, etc.) at various volumes and frequencies.\nAdvanced synthesis techniques are also becoming practical for use in\nreal-time games. For example:\n996 14. Audio\n◦Musical instrument synthesizers reproduce the natural sound of an\nanalog musical instrument without the use of pre-recorded audio.\n◦Physically based sound synthesis encompasses a broad range of tech-\nniques that attempt to accurately reproduce the sound that would\nbe made by an object as it physically interacts with a virtual en-\nvironment. Such systems make use of the contact, momentum,\nforce, torque and deformation information available from a mod-\nern physics simulation engine, in concert with the properties of the\nmaterial from which the object is made and its geometric shape,\nin order to synthesize suitable sounds for impacts, sliding, rolling,\nbending and so on. Here are just a few links to research on this fas-\ncinating topic: http://gamma.cs.unc.edu/research/sound, http://\ngamma.cs.unc.edu/AUDIO_MATERIAL, http://www.cs.cornell.\nedu/projects/sound, and https://ccrma.stanford.edu/ bilbao/\nbooktop/node14.html.\n◦Vehicle engine synthesizers aim to reproduce the sounds made by\na vehicle, given inputs such as the acceleration, RPM and load\nplaced on a virtual engine, and the mechanical movements of the\nvehicle. (The vehicle chase sequences in Naughty Dog’s three Un-\nchartedgames all used various forms of dynamic engine modeling,\nalthough technically these systems were not synthesizers, because\nthey produced their output by cross-fading between various pre-\nrecorded sounds.)\n◦Articulatory speech synthesizers produce human speech “from\nscratch” via a 3D model of the human vocal tract. VocalTractLab\n(http://www.vocaltractlab.de) is a free tool that allows students to\nlearn about and experiment with vocal synthesis.\n•Crowd Modeling . Games that feature crowds of people (audiences, city\ndwellers,etc.)requiresomemeansofrenderingthesoundofthatcrowd.\nThisis notas simple asplaying lots andlots of humanvoices over topof\none another. Instead, it is usually necessary to model the crowd as mul-\ntiplelayersofsounds,includingabackgroundambienceplusindividual\nvocalizations.\nWe can’t possibly cover everything from the above list in one chapter. But\nlet’sspendafewmorepagescoveringsomeofthemostcommongame-specific\nfeatures.\n14.6. Game-Speciﬁc Audio Features 997\n14.6.1 Supporting Split-Screen\nSupportingsplit-screenmultiplayerisatrickyproblem,becauseyouhave mul-\ntiple listeners in the virtual game world, but they must sharea single set of\nspeakers in the player’s living room. If you simply pan the sounds multi-\nple times, once for each listener, and then mix the results into the speakers\nevenly, the result won’t always sound sensible. There is no perfect solution:\nFor example, if player A is standing right next to an explosion and player B\nis standing far away, the person playing player B will still hear the explosion\nloud and clear. The best a game can do is cobble together a hybrid solution, in\nwhich some sounds are handled in a “physically correct” way and others are\n“fudged” in order to produce the most sensible-sounding experience for the\nplayers.\n14.6.2 Character Dialog\nEvenifwe’vecreatedcharactersforourgamethatlooklikephotographsofreal\nhumanbeings, andeveniftheymoveinastoundinglyrealisticways, theystill\nwon’t seem real to the player until they can speakrealistically. Speech commu-\nnicates information crucial to gameplay. It’s a central story-telling tool. And\nit cements the emotional bond between the human player and the characters\nin the game. Speech can also be the deciding factor in the player’s perception\nofintelligence among the AI-controlled characters in a game.\nAt the Game Developer’s Conference (GDC) in 2002, Chris Butcher and\nJaime Griesemer of Bungie gave a talk entitled, “The Illusion of Intelligence:\nThe Integration of AI and Level Design in Halo” (http://bit.ly/1g7FbhD). In\ntheirtalk,theysharedananecdoteabouthowimportantspeechcanbetocom-\nmunicating the motivations of an AI-driven character to the player. In Halo,\nwhen the Elite leader of a squad of Covenant was killed, the grunts would all\nrun away in fear. In playtest after playtest, no one seemed to understand that\nit was the killing of the Elite that had triggered the grunts to flee. Finally, the\ngrunts were given lines of dialog saying something to the effect of, “Leader\ndead—run away!” Only then did play testers start to really grok what was\ngoing on!\nIn this section, we’ll explore some of the fundamental subsystems you’ll\nfind in the character dialog system of pretty much any character-based game.\nWe’ll also discuss some of the specific techniques and technologies used by\nNaughty Dog to create rich, realistic conversations in TheLastofUs . For more\ninformation and in-game videos of Naughty Dog’s character dialog system in\naction, check out the talk I gave at GDC 2014 entitled, “Context-Aware Char-\nacter Dialog in The Last of Us ,” available in PDF and QuickTime formats at\n998 14. Audio\nhttp://www.gameenginebook.com.\n14.6.2.1 Giving a Character a Voice\nIt’seasyenoughtogiveagamecharacteravoice—simplyplayanappropriate\npre-recorded sound whenever the character needs to speak. However, things\nare never quite that simple. The dialog system in a game engine is typically a\nreasonably complex beast. Here are just a few of the reasons why:\n• We need a way to catalog all the possible lines of dialog that each char-\nacter might be called upon to say, and give each of these lines some kind\nof unique id so that the game can trigger them when needed.\n• We need to make sure that each uniquely identifiable character in the\ngamespeakswitharecognizableandconsistentvoice. Forexample,each\nof the hunters in the Pittsburgh section of The Last of Us was assigned\nto one of eight unique voices, so that no two hunters in a battle would\nsound the same.\n• We may not know a priori which character is going to be called upon\nto say a specific line, so we often need to record the same line multiple\ntimes, spoken by various voice actors, so that the appropriate voice can\nbe used to say the line when needed.\n• We also usually want a lot of variety in the things that are said. So most\ndialogsystemsprovideameansofselectingspecificlinesatrandomfrom\na pool of possibilities.\n• Speech audio assets tend to be of relatively long duration, which means\nthey occupy a lot of memory. Many lines of dialog are part of cinematic\nsequences, and hence areonly spoken once in the entire game. For these\nreasons, it’s usually wasteful to store speech assets in memory. Instead,\nit’stypicalforspeechaudioassetstobe streamed ondemand(seeSection\n14.5.7.4).\nUsuallyothervocalsounds,likethe“efforts”madewhenliftingsomething\nheavy,jumpingoveranobstacleorgettingpunchedinthegut, arehandledby\nthe same system that handles spoken dialog. This is done largely because a\ncharacter’s efforts need to match his or her spoken voice. So we may as well\nleverage the dialog system to produce effort sounds as well.\n14.6.2.2 Deﬁning a Line of Dialog\nMostdialogsystemsintroducealevelofindirectionbetweena requesttospeak\nandthechoiceoftheparticular audioclip toplay. Thegameprogrammerorde-\nsignerrequests logicallinesofdialog,eachofwhichisrepresentedbyaunique\n14.6. Game-Speciﬁc Audio Features 999\nidentifier such as a string, or better yet a hashed string id (see Section 6.4.3.1).\nThe sound designers can then “fill out” each logical line with one or more au-\ndio clips in order to provide the necessary variety both in voice quality and in\nterms of what exactly is said.\nFor example, let’s imagine a logical line in which the character says some-\nthing to the effect of, “I’m out of ammo.” We’ll assign this logical dialog line\nthe unique id 'line-out-of-ammo, where the leading single quote indi-\ncates a hashed string id. Let’s assume also that there are ten different char-\nacters that might say this line: the player character (call him “drake”), the\nplayer’s sidekick (call her “elena”) and up to eight enemy characters (call\nthem “pirate-a” through “pirate-h”). We’ll need some kind of data structure\nto define all the physical audio assets that make up this one logical line of\ndialog.\nAtNaughtyDog,sounddesignersusetheSchemeprogramminglanguage\nto define logical dialog lines using a custom syntax. We’ll use a similar syntax\nin our example below. However, the specifics of the implementation are not\nimportant here. All we’re interested in is the structure of the data itself:\n(define-dialog-line 'line-out-of-ammo\n(character 'drake\n(lines\ndrk-out-of-ammo-01 ;; ""Dammit, I'm out!""\ndrk-out-of-ammo-02 ;; ""Crap, need more bullets.""\ndrk-out-of-ammo-03 ;; ""Oh, now I'm REALLY mad.""\n)\n)\n(character 'elena\n(lines\neln-out-of-ammo-01 ;; ""Help, I'm out!""\neln-out-of-ammo-02 ;; ""Got any more bullets?""\n)\n)\n(character 'pirate-a\n(lines\npira-out-of-ammo-01 ;; ""I'm out!""\npira-out-of-ammo-02 ;; ""Need more ammo!""\n;; ...\n)\n)\n;; ...\n(character 'pirate-h\n(lines\npirh-out-of-ammo-01 ;; ""I'm out!""\npirh-out-of-ammo-02 ;; ""Need more ammo!""\n1000 14. Audio\n;; ...\n)\n)\n)\nRather than define your dialog lines in one monolithic data structure like\nthe one shown above, it’s usually better to break the lines out into separate\nfiles by character. For example, all of Drake’s lines can be managed in one file,\nElena’s in another file and all the pirates’ lines can be stored in a third file.\nThis helps to prevent the sound designers from stepping on each others’ toes.\nIt also means that we can manage our memory more efficiently. For example,\nif there are no pirates in a given section of the game, there’s no need to keep\nthe data for the pirates’ dialog lines in memory. It’s also a good idea to split\nthe dialog data up by level, for the same reason.\n14.6.2.3 Playing a Line of Dialog\nGiventhisdata,thedialogsystemcaneasilyconvertarequestforalogicalline\nof dialog such as 'line-out-of-ammo into a specific audio clip. It simply\nlooksupthecharacter’sspecificvoiceidinthetable,andthenmakesarandom\nchoice amongst the various possible lines for that character.\nIt’s usually a good idea to put in place some kind of mechanism to ensure\nthat lines aren’t repeated too often. One way to accomplish this is to store the\nindices of the various lines in an array and then randomly shuffle its contents.\nTo select a line, we simply cycle through the shuffled array in order. Once\nall possible lines have been exhausted, we reshuffle the array, taking care that\nthe most recently played line doesn’t end up in the first slot. This prevents all\nrepetition while keeping the line selections sounding random.\nDialog line requests are typically made by gameplay code in C++, Java,\nC# or whatever language your game is written in. Game designers may also\nrequest lines of dialog via script (Lua, Python, etc.). The dialog system’s API\nis usually designed with simplicity of use in mind. If an AI programmer or\ngame designer has to jump through a lot of hoops just to get a line of dialog\nto play, you may discover that your characters are uncannily silent! It’s best\nto provide a simple, fire-and-forget interface. Leave all the hard work to the\nprogrammer who is crafting the dialog system.\nFor example, in Uncharted 3: Drake’s Deception, C++ code could request a\ncharacter to play a line of dialog by calling a simple PlayDialog() member\nfunction of the Npcclass. These calls would be peppered throughout the AI\ndecision-making code in order to trigger appropriate lines of dialog at key\nmoments in the game. For example:\n14.6. Game-Speciﬁc Audio Features 1001\nvoid Skill::OnEvent(const Event& evt)\n{\nNpc* pNpc = GetSelf(); // grab a pointer to the NPC\nswitch (evt.GetMessage())\n{\ncase SID(""player-seen""):\n// play a line of dialog...\npNpc->PlayDialog(SID(""line-player-seen""));\n// ... and move to closest cover\npNpc->MoveTo(GetClosestCover());\nbreak;\n// ...\n}\n// ...\n}\n14.6.2.4 Priority and Interruption\nWhat happens if a character is asked to speak while he’s already speaking?\nWhat if he receives more than one speech command on the same frame? In\nboth cases, a prioritysystem is a good way to resolve ambiguities.\nTo implement such a system, we simply assign each line of dialog to a pri-\nority level. When a request to say a line of dialog comes in, the system looks\nat the priority of the currently playing line if any, and the priorities of the line\nor lines that have been requested this frame. It finds the highest priority line\namong these. If the currently playing line “wins,” it continues to play and\nthe requested lines are ignored. If one of the requested lines is higher priority\nthan the current line, or if the character isn’t speaking yet, the new line plays,\ninterrupting the current line if necessary.\nImplementing the interruption of the speech itself is actually a bit tricky.\nWecan’tperformacross-fade(i.e.,fadethevolumeoftheplayingsounddown\nand the new sound up) because this sounds strange and wrong when applied\nto the speech of a single character. Ideally, we’d want to play at least some\nkind of glottal stop sound just prior to starting the new line. It might even be\nappropriate to play a short phrase indicating that the character is surprised\nand/orannoyedbytheinterruption,and thenplaythenewlineofdialog. The\ndialog system in The Last of Us doesn’t do any of these fancy things. It simply\nstops the current line and immediately plays the new one. This sounds pretty\ngood most of the time. Of course, each game has its own unique speech pat-\nterns, and what works in one game may not work well in another. So as the\nsaying goes, “Your mileage may vary.”\n1002 14. Audio\n14.6.2.5 Conversations\nInTheLastofUs , Naughty Dog wanted the enemy NPCs to sound like they’re\nhaving real conversations with one another. This meant that the characters\nwould need to be capable of saying relatively long chains of lines, with back-\nand-forth banter between two or more characters. Likewise, in Uncharted4: A\nThief’s End andUncharted: The Lost Legacy, we wanted the characters to have\nconversationswhiledrivingaroundMadagascarandIndiaintheirjeep. These\nconversations could even be interrupted (for example by the player deciding\nto exit the jeep to explore the area), and would continue where they left off\nwhen the player returned to the vehicle.\nConversations in The Last of Us ,Uncharted 4 andThe Lost Legacy are con-\nstructed from logical segments. Each segment corresponds to one logical line,\nspoken by one particular actor in the conversation. Each segment is given a\nunique id, and the segments are chainedtogether into a conversation via these\nids. Asanexample,let’sseehowwewoulddefinethefollowingconversation:\nA:“Hey, did you find anything?”\nB:“No, I’ve been looking for an hour and I ain’t found nothin’.”\nA:“Well then shut up and keep looking!”\nThisconversationcouldbeexpressedintheNaughtyDogconversationsystem\nas follows:\n(define-conversation-segment 'conv-searching-for-stuff-01\n:rule []\n:line 'line-did-you-find-anything\n;; ""Hey, did you find anything?""\n:next-seg 'conv-searching-for-stuff-02\n)\n(define-conversation-segment 'conv-searching-for-stuff-02\n:rule []\n:line 'line-nope-not-yet\n;; ""I've been looking for an hour...""\n:next-seg 'conv-searching-for-stuff-03\n)\n(define-conversation-segment 'conv-searching-for-stuff-03\n:rule []\n:line 'line-shut-up-keep-looking\n;; ""Well then shut up and keep looking!""\n)\nThis syntax might seem a bit verbose at first glace. But as we’ll see in Sec-\ntion 14.6.2.8, breaking the conversation out like this gives us a great deal of\n14.6. Game-Speciﬁc Audio Features 1003\nflexibility. For example, it allows branching conversations to be defined in a\nnatural and reasonably convenient way.\n14.6.2.6 Interrupting Conversations\nWe saw in Section 14.6.2.4 that a simple priority system can be used to han-\ndle interruptions and to resolve contention when more than one logical line is\nrequested simultaneously.\nWhenconversations are in play, a priority system can still be used. But its\nimplementation is a bit more complex in this case. For example, imagine a\nconversation between characters A and B. A says his line, then B says her line\nwhile A waits his turn. During the time that B is speaking, A is asked to play\nanentirelydifferentlineofdialog. He’snottehnicallyspeaking,sobytherules\nfor dialog prioritization, applied to each character individually, there would\nbe no problem and the line would play. But this could sound very jarring,\ndepending on what’s being said.\nA:“Hey, did you find anything?”\nB:“No, I’ve been looking for an hour and…”\nA:“Look, a shiny object!”\n(interruptionby an unrelatedline of dialog)\nB:“…I ain’t found nothin’.”\nToovercomethisissueon TheLastofUs, weintroducedtheconceptofcon-\nversations as “first-class entities.” When a conversation is started, the sys-\ntem “knows” thateach of the characters is involved in that conversation, even\nwhenhe or sheisn’t speaking. Each conversationhas a priority, and the prior-\nitizationrulesareappliedtoentireconversations,nottheindividuallinesona\nper-character basis. So for example, when charater A is asked to say, “Look, a\nshiny object!” the system knows that he is currently involved in the “Hey, did\nyoufindanything?” conversation. Persumablytheline“Look,ashinyobject!”\nisatthesameorlowerpriorityasthecurrentconversation,sotheinterruption\nisn’t allowed.\nIf the interrupting line is something higher priority like, “Holy cow, he’s\npointingagunatus!” thentheline isallowedtointerrupttheexistingconver-\nsation. In that case, all of the characters in the conversation are interrupted.\nThe result is an interruption that sounds natural and intelligent.\nA:“Hey, did you find anything?”\nB:“No, I’ve been looking for an hour and…”\n1004 14. Audio\nA:“Holy cow, he’s pointing a gun at us!”\n(interruptionby a higher-priority conversation)\nB:“Get him!”\n(The original conversation is interrupted by the new one, and A and B go into\ncombatmode.)\n14.6.2.7 Exclusivity\nOnThe Last of Us, we also introduced the concept of exclusivity . Any line of\ndialog or conversation can be marked as either non-exclusive, faction-exclusive\norglobally exclusive. This mark-up controls how interruptions work for the\ngiven line or conversation.\n• Anon-exclusive line or conversation is permitted to play overtop of other\nlines or conversations. For example, during a search for the player, it’s\nnot a big problem if one hunter is mumbling to himself, “Huh, there’s\nnothing over here.” while another hunter is saying, “I’m getting tired\nof this.” The two hunters aren’t speaking to each other, so the overlap\nsounds perfectly natural.\n• Afaction-exclusive line or conversation interrupts all other lines or con-\nversations within that character’s faction. For example, if the player\n(Joel) is spotted during a search, the hunter that saw him might say,\n“He’s over here!” The other hunters should immediately stop speak-\ning, because we want to make it seem as if the hunters can hear one an-\nother, and also to communicate to the player that their collective focus\nhas shifted. However, if Joel’s companion Ellie is whispering a warn-\ning to him at the time, we probably do not want to interrupt her. She is\nnot part of the hunter gang, and what she has to say to Joel is relevant\nwhether or not the hunters have spotted him.\n• Aglobally exclusive line or conversation interrupts all other lines, across\nfaction boundaries. This is useful in situations in which everycharacter\nwithin earshot should react to hearing whatever is being said.\n14.6.2.8 Choices and Branching Conversations\nIt’s often desirable to allow conversations to play out in different ways de-\npending on what the player does, on what decisions the AI characters make\nand/or on other aspects of game world state. When authoring or editing con-\nversations, the writers and sound designers would like to have control not\nonlyoverwhichlinesaresaid, butalsooverthelogicalconditionsthatcontrol\nwhich branch of the conversation will be taken at any given moment during\n14.6. Game-Speciﬁc Audio Features 1005\ngameplay. This puts the creative power in the hands of the people who need\nit, instead of forcing them to work through a programmer.\nNaughty Dog implemented such a system for use on TheLastofUs . It was\ninspired in part by an earlier system developed by Valve and described by\nElan Ruskin in his talk, “Rule Databased for Contextual Dialog and Game\nLogic,” which he delivered at the Game Developer’s Conference in 2012. The\ntalk is available here: http://www.gdcvault.com/play/1015317/AI-driven\n-Dynamic-Dialog-through. Naughty Dog’s conversation system differs from\nValve’sinanumberofsignificantways,butthecoreideabehindbothsystems\nissimilar. We’lldescribetheNaughtyDogsystemhere,sincethat’sthesystem\nwith which the author has the most experience.\nIn Naughty Dog’s conversation system, each segment of a conversation\ncan consist of one or more alternative lines of dialog. Each alternative within\nthe segment carries with it a selection rule. If the rule evaluates to true, that\nalternative is selected; if the rule evaluates to false, the alternative is ignored.\nA rule is comprised of one or more criteria. Each criterion is a simple logi-\ncal expression that evaluates to a Boolean. The expressions ('health > 5)\nand('player-death-count == 1) are examples of criteria. If more than\none criterion is provided within a rule, they are logically combined using the\nBoolean AND operator. A rule only evaluates to true when allof its criteria\nevaluate to true.\nHere’sanexampleofonesegmentofaconversation,withthreealternatives\nthat depend upon the health of the character doing the talking:\n(define-conversation-segment 'conv-player-hit-by-bullet\n(\n:rule [ ('health < 25) ]\n:line 'line-i-need-a-doctor\n;; ""I'm bleeding bad... need a doctor!""\n)\n(\n:rule [ ('health < 75) ]\n:line 'line-im-in-trouble\n;; ""Now I'm in real trouble.""\n)\n(\n:rule [ ] ;; no criteria acts as an ""else"" case\n:line 'line-that-was-close\n;; ""Ah! Can't let that happen again!""\n)\n)\n1006 14. Audio\nBranching Dialog\nBybreakingaconversationintosegments,eachofwhichcontainsoneormore\nalternative lines, we open up the possibility of crafting branching conversa-\ntions. For example, let’s consider a conversation in which Ellie (the player’s\ncompanion in The Last of Us ) asks Joel (the player character) if he’s all right\nwhen he’s been shot at. If the player wasn’t actually hit by the bullet, the con-\nversation goes like this:\nEllie: “Are you OK?”\nJoel:“Yeah, I’m fine.”\nEllie: “Geez. Keep your head down!”\nIf Joel has been hit, the conversation plays out differently:\nEllie: “Are you OK?”\nJoel:“(panting) Not exactly.”\nEllie: “You’re bleeding!”\nWe can expressthis branching conversation using the conversation syntax de-\nscribed above:\n(define-conversation-segment 'conv-shot-at--start\n(\n:rule [ ]\n:line 'line-are-you-ok ;; ""Are you OK?""\n:next-seg 'conv-shot-at--health-check\n:next-speaker 'listener ;; *** see comments below\n)\n)\n(define-conversation-segment 'conv-shot-at--health-check\n(\n:rule [ (('speaker 'shot-recently) == false) ]\n:line 'line-yeah-im-fine ;; ""Yeah, I'm fine.""\n:next-seg 'conv-shot-at--not-hit\n:next-speaker 'listener ;; *** see comments below\n)\n(\n:rule [ (('speaker 'shot-recently) == true) ]\n:line 'line-not-exactly ;; ""(panting) Not exactly.""\n:next-seg 'conv-shot-at--hit\n:next-speaker 'listener ;; *** see comments below\n)\n)\n14.6. Game-Speciﬁc Audio Features 1007\n(define-conversation-segment 'conv-shot-at--not-hit\n(\n:rule [ ]\n:line 'line-keep-head-down ;; ""Geez. Keep your head down!""\n)\n)\n(define-conversation-segment 'conv-shot-at--hit\n(\n:rule [ ]\n:line 'line-youre-bleeding ;; ""You're bleeding!""\n)\n)\nSpeaker and Listener\nThere’sasubtleaspecttowhat’sgoingoninthebranchingconverstionabove.\nAtanygivenmomentinatwo-personconversation, onepersonisthespeaker\nand the other is the listener. The roles of speaker and listener ping-pong back\nand forth as the conversation progresses. In the first segment of the conver-\nsation, 'conv-shot-at--start , Ellie is the speaker and Joel is the listener.\nWhen we chain to the next segment, 'conv-shot-at--health-check , we\nspecify the value 'listener for the field :next-speaker. This tells the\nsystem to use the current listener (Joel) as the next segment’s speaker, thereby\nreversing the roles. In that segment, we check whether the speaker has been\nshot recently via the criteria (('speaker 'shot-recently) == false)\nand(('speaker 'shot-recently) == true). But now Joel is the\nspeaker, so everything works out as we’d expect.\nAn abstract speaker/listener system doesn’t seem all that useful for a con-\nversation between two principal characters like Joel and Ellie. But by keeping\nthe definition of the conversation abstract, we gain a significant amount of\nflexibility. For one thing, we could use the same conversation specification\nto define a conversation in which Joel asks Ellie if she’s OK. This works be-\ncause the entire conversation is defined in a way that is independent of which\ncharacter is saying each line. Moreover, for enemy characters it’s absolutely\nessential that conversations be defined in a generic manner, because we don’t\nknowwhichspecificcharacterswillbedoingthespeakingapriori. Forenemy\nbattle chatter, we typically select a pair of characters dynamically and fire off\nthe conversation. It has to work, no matter which two characters are selected.\nThe speaker/listener system can be extended to two- or three-person con-\nversations. The Naughty Dog conversation system supported up to three lis-\nteners, although the vast majority of our conversations were between only\n1008 14. Audio\ntwo characters.\nFact Dictionaries\nThe criteria within a rule reference symbolic quantities like 'health and\n'player-death-count. These symbolic quantities are implemented under\nthe hood as entries in a dictionary data structure—basically a table containing\nkey-value pairs. We call these factdictionaries . An example of a fact dictionary\nis shown in Table 14.1.\nYoumayhavenoticedinTable14.1thateachvalueinthedictionaryhasan\nassociated data type. In other words, the values in the dictionary are variants.\nA variant is a data object that is capable of holding values of various types,\nmuch like a union in C or C++. However, unlike a union, a variant also\nstores information about the type of data it currently contains. This allows us\ntovalidatethetypeofavaluepriortousingit. Italsoletsusconvertdatafrom\none type to another. For example, if our variant holds the integer value 42,\nwe could ask the variant to return it to us as the floating-point value 42.0f\ninstead.\nInTheLastofUs, eachcharacterhasitsownfactdictionarycontainingfacts\nabout the character itself like health, weapon type, awareness level and so on.\nEach “faction” of characters also has a fact dictionary. This allows us to ex-\npress facts about the faction as a whole, like how many characters remain\nalive within the group. Finally, there is a singleton “global” fact dictionary\nthat contains information about the game as a whole, without respect to fac-\ntion. Things like the amount of time spent playing, the name of the current\nlevel or how many times the player has retried a particular task are all things\nthat can go into the global fact dictionary.\nCriterion Syntax\nWhen writing a criterion, the syntax allows for facts to be pulled from any\ndictionary by name. For example, (('self 'health) > 5) tells the sys-\nKey Value Data Type\n'name 'ellie StringId\n'faction 'buddy StringId\n'health 82 int\n'is-joels-friend true bool\n… … …\nTable 14.1. An example of a fact dictionary.\n14.6. Game-Speciﬁc Audio Features 1009\ntem to grab the fact dictionary of the character itself, look up the value of the\n'health factinthatdictionaryandthencheckifitisgreaterthan5. Likewise,\n(('global 'seconds-playing) <= 23.5) instructs the system to look\nup the 'seconds-playing fact from the global fact dictionary, and check\nthat it is less than or equal to 23.5 seconds.\nIftheuserdoesn’tspecifyadictionaryexplicity,asin ('health > 5),the\nsystem searches for the named fact by following a predefined search order.\nCheck the character’s fact dictionary first. If that fails, try to find it in the\ndictionary that matches the character’s faction. Finally, if all else fails, look\nfor the fact in the global dictionary. This “search path” feature allows sound\ndesigners to be as brief as possible when writing criteria (albeit with the loss\nof some specificity and clarity in the rules).\n14.6.2.9 Context-Sensitive Dialog\nInThe Last of Us, we wanted to have enemy characters call out the location of\nthe player in an intelligent way. If the player is hiding in a store, the enemies\nshould shout, “He’s in the store!” If he’s hiding behind a car, we want the bad\nguys to say, “He’s behind that car!” This makes the characters sound incredi-\nbly intelligent, yet it turns out to be a relatively simple thing to implement.\nTo make this work, the sound designers mark up our game worlds with\nregions. Each region is tagged with one of two kinds of locationtags . Aspecific\ntag marks the region with a very specifc location like “behind the counter”\nor “by the cash register.” A generaltag marks the region with a more general\nlocation like “in the store” or “in the street.”\nTo determine which line of dialog to play, the system determines within\nwhich region the player is located, and within which region the enemy NPC\nislocated. Iftheyarebothinthesame generalregion, theplayer’s specifictagis\nusedtoselectdialoglines. WhentheNPCandplayerresidein different general\nregions, we fall back to using the player’s general region tag to select the line.\nSoiftheenemyandtheplayerarebothinthestore, wemightselectalinelike,\n“He’s by the window!” But if the NPC is in the store and the player is out in\nthe street, we might hear the NPC say, “He’s out in the street! Get him!” See\nFigure 14.42 for an illustration of how this system works.\nThis very simple system proved incredibly powerful. It was difficult to set\nup due to the sheer number of combinations of lines that had to be recorded\nand configured, but the final result in-game was worth the effort.\n14.6.2.10 Dialog Actions\nLines of dialog delivered without body language usually look uncanny and\nunrealistic. Some dialog lines are delivered as part of a full-body animation—\nanin-gamecinematicforexample. Butsomelinesmustbedeliveredwhilethe\n1010 14. Audio\ngeneral: storegeneral: garagegeneral: street\nspecific: counter\nspecific: soda\nmachinespecific:car\nspecific: treespecific:\ncabinet\nNPC2PlayerNPC1“He’s by \nthat tree!”\n“He’s out in \nthe street!”\nFigure 14.42. General and speciﬁc regions for context-sensitive dialog line selection.\ncharacter is busy doing something else, like walking, running or firing their\nweapon. Ideally we’d like to spice up such lines of dialog with some gestures\nto breathe life into them.\nOnTheLastofUs,weimplementedagesturesystemusingadditiveanima-\ntion technology (see Section 12.6.5). These gestures could be explicitly called\nout by C++ code or script. In addition, each line of dialog could have a script\nassociated with it whose timeline was synchronized with the audio. This al-\nlowed us to trigger gestures at precise moments during key lines of dialog.\n14.6.3 Music\nMusic is an incredibly important aspect of pretty much any good game. It\nsets the tone, drives the player’s sense of tension, and can make (or break) an\nemotional scene. A game engine’s music system is typically charged with the\nfollowing duties:\n• Provide the ability to play back music tracks as streaming audio clips\n(because music clips are almost always too large to fit in memory).\n• Provide musical variety.\n• Match the music to the events occurring in the game.\n• Seamlessly transition from one piece of music to the next.\n14.6. Game-Speciﬁc Audio Features 1011\n• Mix the audio with the other sounds in the game in a suitable and pleas-\ning manner.\n• Allow music to be temporarily ducked to enhance the audibility of spe-\ncific sounds or conversations in-game.\n• Permit brief pieces of music or sound effects known as stingers to tem-\nporarily interrupt the currently playing music track.\n• Allowmusictobepausedandrestarted. (Youdon’tneedafullorchestra\nplaying a grandiose theme during every single second of gameplay, you\nknow!)\nWe generally expect the music to change to match the changing levels of\ntension and/or emotional moods of the events happening in the game. One\nwaytoaccomplishthisistocreatemultiple playlists,eachofwhichisintended\nfor a different level of tension or emotional mood in the game. Each playlist\ncontains one or more pieces of music, from which selections may be made\nrandomly or sequentially. As the tension and mood change in the game—as\nbattles begin and end, touching cutscenes come and go and so on—the mu-\nsic system detects these changes and selects new music playlists as appropri-\nate. Some games implement a “stack” of music selections at increasing ten-\nsion levels—calm music for when no enemies are around, tense music when\nthe player is approaching an unsuspecting group of enemies, startling music\nat first contact and fast-paced music during battle.\nStingers are another way to match the music to the events in the game. A\nstinger is a short musical clip or sound effect that can temporarily interrupt\nthe currently playing music track, or play over top of it while the main track’s\nvolume is ducked down. For example, the first time the player makes line-\nof-sight with a new enemy, we might want to play an ominous “rumbling”\nsound to give the player a cue that danger is near. Or when the player dies,\nwe may want to quickly switch to a snippet of “death music.” Both of these\nare situations in which a stinger might be used.\nTransitioning smoothly between different music streams is somewhat of a\nchallenge. We cannot blindly cross-blend between totally unrelated pieces of\nmusic and expect it to always sound good. The tempos of the two pieces may\nnot match, and the “beat” of one piece of music might not line up with that of\nthe next. The key is to time each transition properly. A rapid cross-fade can\nbeusefulifthetemposdon’tmatch; alongercross-fademightworkwellifthe\ntempos are nearly identical. This takes some trial and error to get right. Even\ngettingapieceofmusictoloopproperlyrequiressometweakingbythesound\nengineer.\nThe topic of game music is a broad one, and we can’t really do it justice\nhere. If you are interested in learning more, [45] is a great book to start with.\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",37122
102-IV Gameplay.pdf,102-IV Gameplay,Part IV\nGameplay\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com,90
103-15.1 Anatomy of a Game World.pdf,103-15.1 Anatomy of a Game World,"15\nIntroduction to\nGameplay Systems\nUp until now, everything we’ve talked about in this book has focused on\ntechnology. We’velearnedthatagameengineisacomplex, layeredsoft-\nwaresystembuiltontopofthehardware, driversandoperatingsystemofthe\ntarget machine. We’ve seen how low-level engine systems provide services\nthat are required by the rest of the engine; how human interface devices such\nas joypads, keyboards, mice and other devices can allow a human player to\nprovide inputs to the engine; how the rendering engine produces 3D images\non-screen; how the animation system allows characters and objects to move\nnaturally; how the collision system detects and resolves interpenetrations be-\ntweenshapes;howthephysicssimulationcausesobjectstomoveinphysically\nrealistic ways; how the 3D audio engine renders a believable and immersive\nsoundscape for our game world. But despite the wide range of powerful fea-\ntures provided by these components, if we were to put them all together, we\nstillwouldn’t have a game!\nA game is defined not by its technology but by its gameplay. Gameplay\ncan be defined as the overall experience of playing a game. The term game\nmechanics pins down this idea a bit more concretely—it is usually defined as\nthe set of rulesthat govern the interactions between the various entities in the\ngame. It also defines the objectives of the player(s), criteriafor success and fail-\nure, the player character’s abilities, the number and types of non-player entities\n1015\n1016 15. Introduction to Gameplay Systems\nthat exist within the game’s virtual world and the overall flow of the gaming\nexperience as a whole. In many games, these elements are intertwined with a\ncompelling story and a rich cast of characters. However, story and characters\naredefinitelynotanecessarypartofeveryvideogame,asevidencedbywildly\nsuccessfulpuzzlegameslike Tetris. Intheirpaper, “ASurveyof‘Game’Porta-\nbility” (http://www.dcs.shef.ac.uk/intranet/research/resmes/CS0705.pdf),\nAhmed BinSubaih, Steve Maddock and Daniela Romano of the University of\nSheffield refer to the collection of software systems used to implement game-\nplay as a game’s G-factor . In the next three chapters, we’ll explore the crucial\ntools and engine systems that define and manage the game mechanics (a.k.a.\ngameplay, a.k.a. G-factor) of a game.\n15.1 Anatomy of a Game World\nGameplay designs vary widely from genre to genre and from game to game.\nThat said, most 3D games, and a good number of 2D games as well, conform\nmore or less to a few basic structural patterns. We’ll discuss these patterns\nin the following sections, but please keep in mind that there are bound to be\ngames out there that do not fit neatly into this mold.\n15.1.1 World Elements\nMostvideogamestakeplaceinatwo-orthree-dimensionalvirtual gameworld.\nThis world is typically comprised of numerous discrete elements. Generally,\ntheseelementsfallintotwocategories: staticelementsanddynamicelements.\nStaticelementsincludeterrain,buildings,roads,bridgesandprettymuchany-\nthing that doesn’t move or interact with gameplay in an active way. Dy-\nnamic elements include characters, vehicles, weaponry, floating power-ups\nand health packs, collectible objects, particle emitters, dynamic lights, invisi-\nbleregionsusedtodetectimportanteventsinthegame,splinesthatdefinethe\npaths of objects and so on. This breakdown of the game world is illustrated in\nFigure 15.1.\nGameplay is generally concentrated within the dynamic elements of a\ngame. Clearly, the layout of the static background plays a crucial role in how\nthe game plays out. For example, a cover-based shooter wouldn’t be very\nmuch fun if it were played in a big, empty, rectangular room. However, the\nsoftware systems that implement gameplay are primarily concerned with up-\ndating the locations, orientations and internal states of the dynamic elements,\nsince they are the elements that change over time. The term game state refers\nto the current state of all dynamic game world elements taken as a whole.\n15.1. Anatomy of a Game World 1017\nFigure 15.1. A game world from Uncharted: The Lost Legacy (© 2017/™ SIE. Created and developed\nby Naughty Dog, PlayStation 4) showing static and dynamic elements.\nThe ratio of dynamic to static elements also varies from game to game.\nMost3Dgamesconsistofarelativelysmallnumberofdynamicelementsmov-\ning about within a relatively large static background area. Other games, like\nthe arcade classic Asteroids or the Xbox 360 retro hit Geometry Wars, have no\nstatic elements to speak of (other than a black screen). The dynamic elements\nofagameareusuallymoreexpensivethanthestaticelementsintermsofCPU\nresources, so most 3D games are constrained to a limited number of dynamic\nelements. However, the higher the ratio of dynamic to static elements, the\nmore “alive” the game world can seem to the player. As gaming hardware\nbecomes more and more powerful, games are achieving higher and higher\ndynamic-to-static ratios.\nIt’simportanttonotethatthedistinctionbetweenthedynamicandstaticel-\nements in a game world is often a bit blurry. For example, in the arcade game\nHydro Thunder, the waterfalls were dynamic in the sense that their textures\nwereanimated,theyhaddynamicmisteffectsattheirbases,andtheycouldbe\nplacedintothegameworldandpositionedbyagamedesignerindependently\nof the terrain and water surface. However, from an engineering standpoint,\nwaterfallsweretreatedasstaticelementsbecausetheydidnotinteractwiththe\n1018 15. Introduction to Gameplay Systems\nboats in the race in any way (other than to obscure the player’s view of hid-\nden boost power-ups and secret passageways). Different game engines draw\ndifferent lines between static and dynamic elements, and some don’t draw a\ndistinction at all (i.e., everything is potentially a dynamic element).\nThe distinction between static and dynamic serves primarily as an opti-\nmization tool—we can do less work when we know that the state of an object\nisn’t going to change. For example, when we know a mesh is static and will\nnever move, its lighting can be precomputed in the form of static vertex light-\ning, light maps, shadow maps, static ambient occlusion information or pre-\ncomputed radiance transfer (PRT) spherical harmonics coefficients. Virtually\nany computation that must be done at runtime for a dynamic world element\nis a good candidate for precomputation or omission when applied to a static\nelement.\nGames with destructible environments are an example of how the line be-\ntweenthestaticanddynamicelementsinagameworldcanblur. Forinstance,\nwe might define three versions of every static element—an undamaged ver-\nsion,adamagedversion,andafullydestroyedversion. Thesebackgroundele-\nmentsactlikestaticworldelementsmostofthetime,buttheycanbeswapped\ndynamically during an explosion to produce the illusion of becoming dam-\naged. In reality, static and dynamic world elements are just two extremes\nalong a gamut of possible optimizations. Where we draw the line between\nthe two categories (if we draw one at all) shifts as our optimization method-\nologies change and adapt to the needs of the game design.\n15.1.1.1 Static Geometry\nThe geometry of a static world element is often defined in a tool like Maya.\nIt might be one giant triangle mesh, or it might be broken up into discrete\npieces. The static portions of the scene are sometimes built out of instancedge-\nometry. Instancing is a memory conservation technique in which a relatively\nsmallnumberofuniquetrianglemeshesarerenderedmultipletimesthrough-\noutthegameworld,atdifferentlocationsandorientations,inordertoprovide\nthe illusion of variety. For example, a 3D modeler might create five different\nkindsofshortwallsectionsandthenpiecethemtogetherinrandomcombina-\ntions in order to construct miles of unique-looking walls.\nStatic visual elements and collision data might also be constructed from\nbrush geometry. This kind of geometry originated with the Quake family of\nengines. A brushdescribes a shape as a collection of convex volumes, each\nbounded by a set of planes. Brush geometry is fast and easy to create and in-\ntegrates well into a BSP-tree-based rendering engine. Brushes can be really\nuseful for rapidly blocking out the contents of a game world. This allows\n15.1. Anatomy of a Game World 1019\ngameplay to be tested early, when it is cheap to do so. If the layout proves\nits worth, the art team can either texture map and fine-tune the brush geome-\ntry or replace it with more-detailed custom mesh assets. On the other hand, if\nthe level requires redesign, the brush geometry can be easily revised without\ncreating a lot of extra work for the art team.\n15.1.2 World Chunks\nWhen a game takes place in a very large virtual world, it is typically divided\ninto discrete playable regions, which we’ll call world chunks. Chunks are also\nknown as levels,maps,stagesorareas.The player can usually see only a hand-\nful of chunks at any given moment while playing the game, and he or she\nprogresses from chunk to chunk as the game unfolds.\nOriginally, the concept of “levels” was invented as a mechanism to pro-\nvide greater variety of gameplay within the memory limitations of early gam-\ning hardware. Only one level could exist in memory at a time, but the player\ncould progress from level to level for a much richer overall experience. Since\nthen, game designs have branched out in many directions, and linear level-\nbased games are much less common today. Some games are essentially still\nlinear, but the delineations between world chunks are usually not as obvious\nto the player as they once were. Other games use a star topology, in which the\nplayer starts in a central hub area and can access other areas at random from\nthe hub (perhaps only after they have been unlocked). Others use a graph-\nlike topology, where areas are connected to one another in arbitrary ways.\nStill others provide the illusion of a vast, open world, and use level-of-detail\n(LOD) techniques to reduce memory overhead and improve performance.\nDespite the richness of modern game designs, all but the smallest of game\nworldsarestilldividedintochunksofsomekind. Thisisdoneforanumberof\nreasons. First of all, memory limitations are still an important constraint (and\nwill be until game machines with infinite memory hit the market!). World\nchunksarealsoaconvenientmechanismforcontrollingtheoverallflowofthe\ngame. Chunkscanserveasadivision-of-labormechanismaswell;eachchunk\ncan be constructed and managed by a relatively small group of designers and\nartists. World chunks are illustrated in Figure 15.2.\n15.1.3 High-Level Game Flow\nA game’s high-level flow defines a sequence, tree or graph of player objectives.\nObjectives are sometimes called tasks,stages,levels(a term that can also apply\nto world chunks) or waves(if the game is primarily about defeating hordes of\nattackingenemies). Thehigh-levelflowalsoprovidesthedefinitionofsuccess\nfor each objective (e.g., clear all the enemies and get the key) and the penalty\n1020 15. Introduction to Gameplay Systems\nFigure 15.2. Many game worlds are divided into chunks for various reasons, including memory\nlimitations, the need to control the ﬂow of the game through the world, and as a division-of-\nlabor mechanism during development.\nfor failure (e.g., go back to the start of the current area, possibly losing a “life”\nin the process). In a story-driven game, this flow might also include various\nin-game movies that serve to advance the player’s understanding of the story\nas it unfolds. These sequences are sometimes called cut-scenes ,in-game cine-\nmatics(IGC) or noninteractive sequences (NIS). When they are rendered offline\nand played back as a full-screen movie, such sequences are usually called full-\nmotion videos (FMV).\nEarly games mapped the objectives of the player one-to-one to particular\nworld chunks (hence the dual meaning of the term “level”). For example, in\nDonkey Kong , each new level presents Mario with a new objective (namely, to\nreach the top of the structure and progress to the next level). However, this\none-to-one mapping between world chunks and objectives is less popular in\nmodern game design. Each objective is associated with one or more world\nchunks, but the coupling between chunks and objectives remains deliberately\nloose. This kind of design offers the flexibility to alter game objectives and\nworld subdivision independently, which is extremely helpful from a logistic\nand practical standpoint when developing a game. Many games group their\nobjectives into coarser sections of gameplay, often called chapters oracts. A\ntypical gameplay architecture is shown in Figure 15.3.",12769
104-15.2 Implementing Dynamic Elements Game Objects.pdf,104-15.2 Implementing Dynamic Elements Game Objects,"15.2. Implementing Dynamic Elements: Game Objects 1021\nChapter 1\nChunk 1\nChunk 2\nChunk 3Objective 1BObjective 1A\nObjective 1C\nOptional\nObjective 1D\nObjective 1E\nObjective 1GOptoinal\nObjective 1FChapter 2\nChunk  4\nChunk  5\nChunk  6\nChunk 7Objective 2BObjective 2A\nObjective 2C\nObjective 2D\nObjective 2G\nOptoinal\nObjective 2HOptiona l\nObjective 2FOptional\nObje ctive 2E\nObjective 2I\nFigure 15.3. Gameplay objectives are typically arranged in a sequence, a tree, or a generalized graph, and each one maps to\none or more game world chunks.\n15.2 Implementing Dynamic Elements:\nGame Objects\nThe dynamic elements of a game are usually designed in an object-oriented\nfashion. This approach is intuitive and natural and maps well to the game\ndesigner’s notion of how the world is constructed. He or she can visualize\ncharacters,vehicles,floatinghealthpacks,explodingbarrelsandmyriadother\ndynamic objects moving about in the game. So it is only natural to want to be\nable to create and manipulate these elements in the game world editor. Like-\nwise, programmers usually find it natural to implement dynamic elements as\nlargelyautonomousagentsatruntime. Inthisbook,we’llusetheterm gameob-\nject(GO)torefertovirtuallyanydynamicelementwithinagameworld. How-\never, this terminology is by no means standard within the industry. Game ob-\njects are commonly referred to as entities,actorsoragents, and the list of terms\ngoes on.\n1022 15. Introduction to Gameplay Systems\nAsiscustomaryinobject-orienteddesign,agameobjectisessentiallyacol-\nlection of attributes (the current state of the object) and behaviors (how the state\nchanges over time and in response to events). Game objects are usually clas-\nsified by type. Different types of objects have different attribute schemas and\ndifferent behaviors. All instances of a particular type share the same attribute\nschema and the same set of behaviors, but the valuesof the attributes differ\nfrom instance to instance. (Note that if a game object’s behavior is data-driven,\nsay through script code or via a set of data-driven rules governing the object’s\nresponses to events, then behavior too can vary on an instance-by-instance\nbasis.)\nThedistinctionbetweena typeandaninstance ofatypeisacrucialone. For\nexample,thegameof Pac-Man involvesfourgameobjecttypes: ghosts,pellets,\npower pills and Pac-Man. However, at any moment in time, there may be up\ntofourinstancesofthetype“ghost,”50–100instancesofthetype“pellet,”four\n“power pill” instances and one instance of the “Pac-Man” type.\nMost object-oriented systems provide some mechanism for the inheritance\nofattributes, behaviororboth. Inheritanceencouragescodeanddesignreuse.\nThe specifics of how inheritance works vary widely from game to game, but\nmost game engines support it in some form.\n15.2.1 Game Object Models\nIn computer science, the term object model has two related but distinct mean-\nings. It can refer to the set of features provided by a particular programming\nlanguage or formal design language. For example, we might speak of the\nC++ object model or theOMT object model. It can also refer to a specific object-\noriented programming interface (i.e., a collection of classes, methods and in-\nterrelationships designed to solve a particular problem). One example of this\nlatter usage is the MicrosoftExcelobjectmodel, which allows external programs\nto control Excel in various ways. (See http://en.wikipedia.org/wiki/Object_\nmodel for further discussion of the term object model.)\nIn this book, we will use the term game object model to describe the facili-\nties provided by a game engine in order to permit the dynamic entities in the\nvirtual game world to be modeled and simulated. In this sense, the term game\nobject model has aspects of bothof the definitions given above:\n• A game’s object model is a specific object-oriented programming inter-\nface intended to solve the particular problem of simulating the specific\nset of entities that make up a particular game.\n• Additionally,agame’sobjectmodeloftenextendstheprogramminglan-\nguage in which the engine was written. If the game is implemented\n15.2. Implementing Dynamic Elements: Game Objects 1023\nin a non-object-oriented language like C, object-oriented facilities can\nbe added by the programmers. And even if the game is written in an\nobject-orientedlanguagelikeC++,advancedfeatureslikereflection,per-\nsistence and network replication are often added. A game object model\nsometimes melds the features of multiple languages. For example, a\ngameenginemightcombineacompiledprogramminglanguagesuchas\nC or C++ with a scripting language like Python, Lua or Pawn and pro-\nvide a unified object model that can be accessed from either language.\n15.2.2 Tool-Side Design versus Runtime Design\nTheobjectmodelpresentedtothedesignersviatheworldeditor(discussedin\nSection 15.4) needn’t be the same object model used to implement the game at\nruntime.\n• Thetool-sidegameobjectmodelmightbeimplementedatruntimeusing\na language with no native object-oriented features at all, like C.\n• A single GO type on the tool side might be implemented as a collection\nof classes at runtime (rather than as a single class as one might at first\nexpect).\n• Each tool-side GO might be nothing more than a unique id at runtime,\nwith all of its state data stored in tables or collections of loosely coupled\nobjects.\nTherefore,agamereallyhastwodistinctbutcloselyinterrelatedobjectmodels:\n• Thetool-side object model is defined by the set of game object types seen by\nthe designers within the world editor.\n• Theruntimeobjectmodel isdefinedbywhateversetoflanguageconstructs\nandsoftwaresystemstheprogrammershaveusedtoimplementthetool-\nside object model at runtime. The runtime object model might be iden-\ntical to the tool-side model or map directly to it, or it might be entirely\ndifferent than the tool-side model under the hood.\nIn some game engines, the line between the tool-side and runtime designs\nisblurredornonexistent. Inothers,itisverywelldelineated. Insomeengines,\nthe implementation is actually shared between the tools and the runtime. In\nothers, the runtime implementation looks almost totally alien relative to the\ntool-side view of things. Some aspects of the implementation almost always\ncreep up into the tool-side design, and game designers must be cognizant of\ntheperformance-andmemory-consumptionimpactsofthegameworldsthey",6469
105-15.4 The Game World Editor.pdf,105-15.4 The Game World Editor,"1024 15. Introduction to Gameplay Systems\nconstructandthegameplayrulesandobjectbehaviorstheydesign. Thatsaid,\nvirtually all game engines have some form of tool-side object model and a\ncorresponding runtime implementation of that object model.\n15.3 Data-Driven Game Engines\nIn the early days of game development, games were largely hard-coded by\nprogrammers. Tools, if any, were primitive. This worked because the amount\nof content in a typical game was miniscule, and the bar wasn’t particularly\nhigh, thanks in part to the primitive graphics and sound of which early game\nhardware was capable.\nToday, games are orders of magnitude more complex, and the quality bar\nis so high that game content is often compared to the computer-generated ef-\nfects in Hollywood blockbusters. Game teams have grown much larger, but\nthe amount of game content is growing faster than team size. In the eighth\ngeneration of consoles, defined by the Xbox One and the PlayStation 4, game\nteamsroutinelyspeakoftheneedtoproducetentimesthecontentwithteams\nthatarenotthatmuchlargerthaninthepreviousgeneration. Thistrendmeans\nthat a game team must be capable of producingvery largeamounts of content\nin an extremely efficient manner.\nEngineering resources are often a production bottleneck because high-\nqualityengineeringtalentislimitedandexpensiveandbecauseengineerstend\ntoproducecontentmuchmoreslowlythanartistsandgamedesigners(dueto\nthe complexities inherent in computer programming). Most teams now be-\nlieve that it’s a good idea to put at least some of the power to create content\ndirectly into the hands of the folks responsible for producing that content—\nnamely the designers and the artists. When the behavior of a game can be\ncontrolled,inwholeorinpart,by dataprovidedbyartistsanddesignersrather\nthan exclusively by software produced by programmers, we say the engine is\ndata driven.\nData-driven architectures can improve team efficiency by fully leveraging\nall staff members to their fullest potential and by taking some of the heat off\nthe engineering team. It can also lead to improved iteration times. Whether a\ndeveloper wants to make a slight tweak to the game’s content or completely\nrevise an entire level, a data-driven design allows the developer to see the\neffects of the changes quickly, ideally with little or no help from an engineer.\nThis saves valuable time and can permit the team to polish their game to a\nvery high level of quality.\nThat being said, it’s important to realize that data-driven features often\ncome at a heavy cost. Tools must be provided to allow game designers and\n15.4. The Game World Editor 1025\nartiststodefinegamecontentinadata-drivenmanner. Theruntimecodemust\nbe changed to handle the wide range of possible inputs in a robust way. Tools\nmust also be provided in-game to allow artists and designers to preview their\nworkandtroubleshootproblems. Allofthissoftwarerequiressignificanttime\nand effort to write, test and maintain.\nSadly,manyteamsmakeamadrushintodata-drivenarchitectureswithout\nstopping to study the impacts of their efforts on their particular game design\nand the specific needs of their team members. In their haste, such teams often\ndramatically overshoot the mark, producing overly complex tools and engine\nsystems that are difficult to use, bug-ridden and virtually impossible to adapt\ntothechangingrequirementsoftheproject. Ironically,intheireffortstorealize\nthebenefitsofadata-drivendesign,ateamcaneasilyendupwithsignificantly\nlower productivity than the old-fashioned hard-coded methods.\nEverygameengineshouldhavesomedata-drivencomponents,butagame\nteam must exercise extreme care when selecting which aspects of the engine\nto data-drive. It’s crucial to weigh the costs of creating a data-driven or rapid-\niteration feature against the amount of time the feature is expected to save\nthe team over the course of the project. It’s also incredibly important to keep\ntheKISSmantra(“keepitsimple,stupid”)inmindwhendesigningandimple-\nmentingdata-driventoolsandenginesystems. ToparaphraseAlbertEinstein,\neverythinginagameengineshouldbemadeassimpleaspossible,butnosim-\npler.\n15.4 The Game World Editor\nWe’ve already discussed data-driven asset-creation tools, such as Maya, Pho-\ntoshop, Havok content tools and so on. These tools generate individual assets\nfor consumption by the rendering engine, animation system, audio system,\nphysics system and so on. The analog to these tools in the gameplay space\nis thegame world editor—a tool (or a suite of tools) that permits game world\nchunks to be defined and populated with static and dynamic elements.\nAll commercial game engines have some kind of world editor tool.\n• Awell-knowntoolcalled Radiant isusedtocreatemapsforthe Quakeand\nDoomfamilyofengines. AscreenshotofRadiantisshowninFigure15.4.\n• Valve’s Sourceengine, the engine that drives Half-Life 2, The Orange Box ,\nTeam Fortress 2, the Portalseries, the Left 4 Dead series and Titanfall, pro-\nvides an editor called Hammer (previously distributed under the names\nWorldcraft andThe Forge ). Figure 15.5 shows a screenshot of Hammer.\n1026 15. Introduction to Gameplay Systems\nFigure 15.4. The Radiant world editor for the Quake and Doom family of engines.\n• Crytek’s CRYENGINE provides a powerful suite of world creation and\nediting tools. These tools support real-time editing of multiplatform\ngame environments simultaneously, both in 2D and true stereoscopic\n3D. Crytek’s Sandbox editor is depicted in Figure 15.6.\nThe game world editor generally permits the initial states of game objects\n(i.e., the values of their attributes) to be specified. Most game world editors\nalsogivetheiruserssomesortofabilitytocontrolthe behaviors ofthedynamic\nobjectsinthegameworld. Thiscontrolmightbeviadata-drivenconfiguration\nparameters(e.g., objectAshouldstartinaninvisiblestate, objectBshouldim-\nmediately attack the player when spawned, object C is flammable, etc.), or be-\nhavioral control might be via a scripting language, thereby shifting the game\ndesigners’ tasks into the realm of programming. Some world editors even\nallow entirely new types of game objects to be defined, with little or no pro-\ngrammer intervention.\n15.4. The Game World Editor 1027\nFigure 15.5. Valve’s Hammer editor for the Source engine.\n1028 15. Introduction to Gameplay Systems\nFigure 15.6. The Sandbox editor for CRYENGINE. (See Color Plate XXV.)\n15.4. The Game World Editor 1029\n15.4.1 Typical Features of a Game World Editor\nThe design and layout of game world editors vary widely, but most editors\nprovide a reasonably standard set of features. These include, but are certainly\nnot limited to, the following.\n15.4.1.1 World Chunk Creation and Management\nThe unit of world creation is usually a chunk (also known as a level or map—\nsee Section 15.1.2). The game world editor typically allows new chunks to be\ncreatedandexistingchunkstoberenamed,brokenup,combinedordestroyed.\nEach chunk can be linked to one or more static meshes and/or other static\ndata elements such as AI navigation maps, descriptions of ledges that can be\ngrabbed by the player, cover point definitions and so on. In some engines, a\nchunk is defined by a single background mesh and cannot exist without one.\nInotherengines,achunkmayhaveanindependentexistence,perhapsdefined\nby a bounding volume (e.g., AABB, OBB or arbitrary polygonal region), and\ncanbepopulatedbyzeroormoremeshesand/orbrushgeometry(seeSection\n1.7.2.1).\nSome world editors provide dedicated tools for authoring terrain, water\nand other specialized static elements. In other engines, these elements might\nbe authored using standard DCC applications but tagged in some way to in-\ndicate to the asset conditioning pipeline and/or the runtime engine that they\narespecial. (For example, in the Uncharted andTheLastofUs series, water was\nauthoredasatrianglemesh,butitwasmappedwithaspecialmaterialthatin-\ndicated that it was to be treated as water.) Sometimes, special world elements\nare created and edited in a separate, stand-alone tool. For example, the height\nfieldterrainin MedalofHonor: PacificAssault wasauthoredusingacustomized\nversion of a tool obtained from another team within Electronic Arts because\nthis was more expedient than trying to integrate a terrain editor into Radiant,\nthe world editor being used on the project at the time.\n15.4.1.2 Game World Visualization\nIt’s important for the user of a game world editor to be able to visualize the\ncontentsofthegameworld. Assuch, virtuallyallgameworldeditorsprovide\na three-dimensional perspective view of the world and/or a two-dimensional\northographic projection. It’s common to see the view pane divided into four\nsections, three for top, side and front orthographic elevations and one for the\n3D perspective view.\nSomeeditors providethese worldviews viaa customrenderingenginein-\ntegrated directly into the tool. Other editors are themselves integrated into\n1030 15. Introduction to Gameplay Systems\na 3D geometry editor like Maya or 3ds Max, so they can simply leverage the\ntool’s viewports. Still other editors are designed to communicate with the ac-\ntual game engine and use it to render the 3D perspective view. Some editors\nare even integrated into the engine itself.\n15.4.1.3 Navigation\nClearly, a world editor wouldn’t be of much use if the user weren’t able to\nmove around within the game world. In an orthographic view, it’s important\nto be able to scroll and zoom in and out. In a 3D view, various camera control\nschemes are used. It may be possible to focus on an individual object and\nrotate around it. It may also be possible to switch into a “fly through” mode\nwherethecamerarotatesaboutitsownfocalpointandcanbemovedforward,\nbackward, up and down and panned left and right.\nSome editors provide a host of convenience features for navigation. These\ninclude the ability to select an object and focus in on it with a single key press,\nthe ability to save various relevant camera locations and then jump between\nthem, various camera movement speed modes for coarse navigation and fine\ncamera control, a Web-browser-like navigation history that can be used to\njump around the game world and so on.\n15.4.1.4 Selection\nA game world editor is primarily designed to allow the user to populate a\ngame world with static and dynamic elements. As such, it’s important for the\nuser to be able to select individual elements for editing. Some editors only\nallow a single object to be selected at a time, while more-advanced editors\npermitmultiobjectselections. Objectsmightbeselectedviaarubber-bandbox\nin the orthographic view or by ray cast style picking in the 3D view. Many\neditors also display a list of all world elements in a scrolling list or tree view\nso that objects can be found and selected by name. Some world editors also\nallow selections to be named and saved for later retrieval.\nGameworldsareoftenquitedenselypopulated. Assuch,itcansometimes\nbe difficult to select a desired object because other objects are in the way. This\nproblemcanbeovercomeinanumberofways. Whenusingaraycasttoselect\nobjectsin3D,theeditormightallowtheusertocyclethroughalloftheobjects\nthat the ray is currently intersecting rather than always selecting the nearest\none. Manyeditorsallowthecurrentlyselectedobject(s)tobetemporarilyhid-\nden from view. That way, if you don’t get the object you want the first time,\nyou can always hide it and try again. As we’ll see in the next section, layers\ncan also be an effective way to reduce clutter and improve the user’s ability to\nselect objects successfully.\n15.4. The Game World Editor 1031\n15.4.1.5 Layers\nSome editors also allow objects to be grouped into predefined or user-defined\nlayers. This can be an incredibly useful feature, allowing the contents of the\ngame world to be organized sensibly. Entire layers can be hidden or shown to\nreduce clutter on-screen. Layers might be color-coded for easy identification.\nLayers can be an important part of a division-of-labor strategy, as well. For\nexample, when the lighting team is working on a world chunk, they can hide\nall of the elements in the scene that are not relevant to lighting.\nWhat’s more, if the game world editor is capable of loading and saving\nlayers individually, conflicts can be avoided when multiple people are work-\ning on a single world chunk at the same time. For example, all of the lights\nmightbestoredinonelayer,allofthebackgroundgeometryinanotherandall\nAI characters in a third. Since each layer is totally independent, the lighting,\nbackground and NPC teams can all work simultaneously on the same world\nchunk.\n15.4.1.6 Property Grid\nThe static and dynamic elements that populate a game world chunk typically\nhave various properties (also known as attributes) that can be edited by the\nuser. Properties might be simple key-value pairs and be limited to simple\natomic data types like Booleans, integers, floating-point numbers and strings.\nIn some editors, more complex properties are supported, including arrays of\ndata and nested compound data structures. More complex data types may be\nsupported too, such as vectors, RGB colors and references to external assets\n(audio files, meshes, animations, etc.)\nMostworldeditorsdisplaytheattributesofthecurrentlyselectedobject(s)\nin a scrollable property grid view. An example of a property grid is shown in\nFigure15.7. Thegridallowstheusertoseethecurrentvaluesofeachattribute\nand edit the values by typing, using check boxes or drop-down combo boxes,\ndragging spinner controls up and down and so on.\nEditing Multiobject Selections\nIn editors that support multiobject selection, the property grid may support\nmultiobjecteditingaswell. Thisadvancedfeaturedisplaysanamalgamofthe\nattributes of all objects in the selection. If a particular attribute has the same\nvalue across all objects in the selection, the value is shown as-is, and editing\nthe value in the grid causes the property value to be updated in all selected\nobjects. Iftheattribute’svaluediffersfromobjecttoobjectwithintheselection,\nthe property grid typically shows no value at all. In this case, if a new value\n1032 15. Introduction to Gameplay Systems\nFigure 15.7. A typical property grid.\nis typed into the field in the grid, it will overwrite the values in all selected\nobjects, bringing them all into agreement.\nAnother problem arises when the selection contains a heterogeneous col-\nlection of objects (i.e., objects whose types differ). Each type of object can po-\ntentially have a different set of attributes, so the property grid must display\nonly those attributes that are common to all object types in the selection. This\ncan still be useful, however, because game object types often inherit from a\ncommonbasetype. Forexample,mostobjectshaveapositionandorientation.\nInaheterogeneousselection,theusercanstilleditthesesharedattributeseven\nthough more-specific attributes are temporarily hidden from view.\nFree-Form Properties\nNormally, thesetofpropertiesassociatedwithanobject, andthedatatypesof\nthoseproperties,aredefinedonaper-object-typebasis. Forexample,arender-\nable object has a position, orientation, scale and mesh, while a light has posi-\ntion, orientation, color, intensity and light type. Some editors also allow addi-\ntional “free-form” properties to be defined by the user on a per-instance basis.\n15.4. The Game World Editor 1033\nThese properties are usually implemented as a flat list of key-value pairs. The\nuser is free to choose the name (key) of each free-form property, along with\nits data type and its value. This can be incredibly useful for prototyping new\ngameplay features or implementing one-off scenarios.\n15.4.1.7 Object Placement and Alignment Aids\nSome object properties are treated in a special way by the world editor. Typi-\ncally the position, orientation and scale of an object can be controlled via spe-\ncial handles in the orthographic and perspective viewports, just like in Maya\nor Max. In addition, asset linkages often need to be handled in a special way.\nForexample,ifwechangethemeshassociatedwithanobjectintheworld,the\neditor should display this mesh in the orthographic and 3D perspective view-\nports. As such, the game world editor must have special knowledge of these\nproperties—it cannot treat them generically, as it can most object properties.\nManyworldeditorsprovideahostofobjectplacementandalignmentaids\nin addition to the basic translation, rotation and scale tools. Many of these\nfeatures borrow heavily from the feature sets of commercial graphics and 3D\nmodeling tools like Photoshop, Maya, Visio and others. Examples include\nsnap to grid, snap to terrain, align to object and many more.\n15.4.1.8 Special Object Types\nJust as some object properties must be handled in a special way by the world\neditor,certaintypesofobjectsalsorequirespecialhandling. Examplesinclude:\n•Lights. The world editor usually uses special icons to represent lights,\nsince they have no mesh. The editor may attempt to display the light’s\napproximateeffectonthegeometryinthesceneaswell,sothatdesigners\ncan move lights around in real time and get a reasonably good feel for\nhow the scene will ultimately look.\n•Particle emitters . Visualization of particle effects can also be problematic\nin editors that are built on a stand-alone rendering engine. In this case,\nparticle emitters might be displayed using icons only, or some attempt\nmightbemadetoemulatetheparticleeffectintheeditor. Ofcourse, this\nis not a problem if the editor is in-game or can communicate with the\nrunning game for live tweaking.\n•Sound sources . As we discussed in Chapter 14, a 3D rendering engine\nmodels sound sources as 3D points or volumes. It may be convenient\nto provide specialized editing tools for these in the world editor. For\nexample, sound designers will find it helpful if they can visualize the\n1034 15. Introduction to Gameplay Systems\nmaximum radius of an omnidirectional sound emitter, or the direction\nvector and cone of a directional emitter.\n•Regions. A region is a volume of space that is used by the game to detect\nrelevant events such as objects entering or leaving the volume or to de-\nmark areas for various purposes. Some game engines restrict regions to\nbeingmodeledasspheresororientedboxes,whileothersmaypermitar-\nbitrary convex polygonal shapes when viewed from above, with strictly\nhorizontal sides. Still others might allow regions to be constructed out\nof more complex geometry, such as k-DOPs (see Section 13.3.4.5). If re-\ngions are always spherical then the designers might be able to make do\nwith a “Radius” property in the property grid, but to define or modify\nthe extents of an arbitrarily shaped region, a special-case editing tool is\nalmost certainly required.\n•Splines. A spline is a three-dimensional curve defined by a set of control\npointsandpossiblytangentvectorsatthepoints, dependingonthetype\nof mathematical curve used. Catmull-Rom splines are commonly used\nbecause they are fully defined by a set of control points (without tan-\ngents),andthecurvealwayspassesthroughallofthecontrolpoints. But\nno matter what type of splines are supported, the world editor typically\nneeds to provide the ability to display the splines in its viewports, and\ntheusermustbeabletoselectandmanipulateindividualcontrolpoints.\nSome world editors actually support two selection modes—a “coarse”\nmode for selecting objects in the scene and a “fine” mode for selecting\ntheindividualcomponentsofaselectedobject,suchasthecontrolpoints\nof a spline or the vertices of a region.\n•Nav meshes for AI. In many games, NPCs navigate by running path-\nfinding algorithms within the navigable regions of the game world.\nThese navigable regions must be defined, and the world editor usually\nplays a central role in allowing AI designers to create, visualize and edit\nthese regions. For example, a nav mesh is a 2D triangle mesh that pro-\nvides a simple description of the boundaries of the navigable region, as\nwell as providing connectivity information to the path finder.\n•Other custom data. Of course, every game has its own specific data re-\nquirements. The world editor may be called upon to provide custom\nvisualization and editing facilities for these pieces of data. Examples in-\nclude a description of the “affordances” (windows, doorways, possible\npointsofattackordefense)withinaplayspaceforusebytheAIsystem,\nor geometric features that describe things like cover points or grabbable\nledges for use by the player character and/or NPCs.\n15.4. The Game World Editor 1035\n15.4.1.9 Saving and Loading World Chunks\nOf course, no world editor would be complete if it were unable to load and\nsave world chunks. The granularity with which world chunks can be loaded\nand saved differs widely from engine to engine. Some engines store each\nworld chunk in a single file, while others allow individual layers to be loaded\nand saved independently. Data formats also vary across engines. Some use\ncustombinaryformats,otherstextformatslikeXMLorJSON.Eachdesignhas\nits pros and cons, but every editor provides the ability to load and save world\nchunks in some form—and every game engine is capable of loading world\nchunks so that they can be played at runtime.\n15.4.1.10 Rapid Iteration\nA good game world editor usually supports some degree of dynamic tweak-\ning for rapid iteration. Some editors run within the game itself, allowing the\nuser to see the effects of his or her changes immediately. Others provide a\nlive connection from the editor to the running game. Still other world editors\noperate entirely offline, either as a stand-alone tool or as a plug-in to a DCC\napplication like Lightwave or Maya. These tools sometimes permit modified\ndata to be reloaded dynamically into the running game. The specific mech-\nanism isn’t important—all that matters is that users have a reasonably short\nround-trip iteration time (i.e., the time between making a change to the game\nworld and seeing the effects of that change in-game). It’s important to real-\nize that iterations don’t have to be instantaneous. Iteration times should be\ncommensurate with the scope and frequency of the changes being made. For\nexample,wemightexpecttweakingacharacter’smaximumhealthtobeavery\nfast operation, but when making major changes to the lighting environment\nfor an entire world chunk, a much longer iteration time might be acceptable.\n15.4.2 Integrated Asset Management Tools\nIn some engines, the game world editor is integrated with other aspects of\ngame asset database management, such as defining mesh and material prop-\nerties, defining animations, blend trees, animation state machines, setting up\ncollision and physical properties of objects, managing texture resources and\nso on. (See Section 7.2.1.2 for a discussion of the game asset database.)\nPerhaps the best-known example of this design in action is UnrealEd, the\neditor used to create content for games built on the Unreal Engine. UnrealEd\nis integrated directly into the game engine, so any changes made in the editor\nare made directly to the dynamic elements in the running game. This makes\nrapid iteration very easy to achieve. But UnrealEd is much more than a game\n1036 15. Introduction to Gameplay Systems\nFigure 15.8. UnrealEd’s Generic Browser provides access to the entire game asset database.\nworld editor—it is actually a complete content-creation package. It manages\nthe entire database of game assets, from animations to audio clips to triangle\nmeshes to textures to materials and shaders and much more. UnrealEd pro-\nvides its user with a unified, real-time, WYSIWYG view into the entire asset\ndatabase, making it a powerful enabler of any rapid, efficient game develop-\nment process. A few screenshots from UnrealEd are shown in Figures 15.8\nand 15.9.\n15.4.2.1 Data Processing Costs\nIn Section 7.2.1, we learned that the asset conditioning pipeline (ACP) con-\nverts game assets from their various source formats into the formats required\nby the game engine. This is typically a two-step process. First, the asset is\nexported from the DCC application to a platform-independent intermediate\nformat that only contains the data that is relevant to the game. Second, the\nasset is processed into a format that is optimized for a specific platform. On\na project targeting multiple gaming platforms, a single platform-independent\nasset gives rise to multiple platform-specific assets during this second phase.\nOneofthekeydifferencesbetweentoolspipelinesisthepointatwhichthis\nsecond platform-specific optimization step is performed. UnrealEd performs\nit when assets are first imported into the editor. This approach pays off in\n15.4. The Game World Editor 1037\nFigure 15.9. UnrealEd also provides a world editor.\nrapid iteration time when iterating on level design. However, it can make the\ncost of changing source assets like meshes, animations, audio assets and so\non more painful. Other engines like the Source engine and the Quakeengine\npay the asset optimization cost when baking out the level prior to running\nthe game. Halogives the user the option to change raw assets at any time;\nthey are converted into an optimized form when they are first loaded into the\nengine, andtheresultsarecached topreventtheoptimizationstepfrombeing\nperformed needlessly every time the game is run.\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",25621
106-16 Runtime Gameplay Foundation Systems.pdf,106-16 Runtime Gameplay Foundation Systems,,0
107-16.1 Components of the Gameplay Foundation System.pdf,107-16.1 Components of the Gameplay Foundation System,"16\nRuntime Gameplay\nFoundation Systems\n16.1 Components of the Gameplay\nFoundation System\nMost game engines provide a suite of runtime software components that\ntogether provide a framework upon which a game’s unique rules, ob-\njectivesanddynamicworldelementscanbeconstructed. Thereisnostandard\nnameforthesecomponentswithinthegameindustry,butwewillrefertothem\ncollectivelyastheengine’s gameplayfoundationsystem. Ifalinebetweenengine\nand game can reasonably be drawn between the game engine and the game\nitself, then these systems lie just beneath this line. In theory, one can construct\ngameplayfoundationsystemsthatareforthemostpartgame-agnostic. How-\never, in practice, these systems almost always contain genre- or game-specific\ndetails. In fact, the line between the engine and the game can probably be\nbest visualized as one big blur—a gradient that arcs across these components\nas it links the engine to the game. In some game engines, one might even go\nso far as to consider the gameplay foundation systems as lying entirely above\nthe engine-game line. The differences between game engines are most acute\nwhen it comes to the design and implementation of their gameplay compo-\nnents. That said, there are a surprising number of common patterns across\nengines, and those commonalities will be the topic of our discussions here.\n1039\n1040 16. Runtime Gameplay Foundation Systems\nEvery game engine approaches the problem of gameplay software design\na bit differently. However, most engines provide the following major subsys-\ntems in some form:\n•Runtime game object model. This is an implementation of the abstract\ngame object model advertised to the game designers via the world\neditor.\n•Level management and streaming . This system loads and unloads the con-\ntents of the virtual worlds in which gameplay takes place. In many en-\ngines, level data is streamed into memory during gameplay, thus pro-\nviding the illusion of a large seamless world (when in fact it is broken\ninto discrete chunks).\n•Real-timeobjectmodelupdating. In order to permit the game objects in the\nworld to behave autonomously, each object must be updated periodi-\ncally. This is where all of the disparate systems in a game engine truly\ncome together into a cohesive whole.\n•Messaging and event handling. Most game objects need to communicate\nwithoneanother. Thisisusuallydoneviaanabstractmessagingsystem.\nInter-objectmessagesoftensignalchangesinthestateofthegameworld\ncalledevents. So the messaging system is referred to as the event system\nin many studios.\n•Scripting. Programming high-level game logic in a language like C or\nC++ can be cumbersome. To improve productivity, allow rapid itera-\ntion, and put more power into the hands of the non-programmers on\nthe team, a scripting language is often integrated into the game engine.\nThis language might be text-based, like Python or Lua, or it might be a\ngraphical language, like Unreal’s Blueprints.\n•Objectives and game flow management. This subsystem manages the\nplayer’s objectives and the overall flow of the game. This is usually\ndescribed by a sequence, a tree, or a generalized graph of player objec-\ntives. Objectivesareoftengroupedintochapters,especiallyifthegameis\nhighlystory-drivenasmanymoderngamesare. Thegameflowmanage-\nment system manages the overall flow of the game, tracks the player’s\naccomplishment of objectives and gates the player from one area of the\ngameworldtothenextastheobjectivesareaccomplished. Somedesign-\ners refer to this as the “spine” of the game.\nOfthesemajorsystems,theruntimeobjectmodelisprobablythemostcom-\nplex. It typically provides most, if not all, of the following features:\n•Spawning and destroying game objects dynamically. The dynamic elements\nin a game world often need to come and go during gameplay. Health\n16.1. Components of the Gameplay Foundation System 1041\npacks disappear once they have been picked up, explosions appear\nand then dissipate and enemy reinforcements mysteriously come from\naround a corner just when you think you’ve cleared the level. Many\ngame engines provide a system for managing the memory and other re-\nsources associated with dynamically spawned game objects. Other en-\ngines simply disallow dynamic creation or destruction of game objects\naltogether.\n•Linkage to low-level engine systems. Every game object has some kind of\nlinkage to one or more underlying engine systems. Most game objects\narevisuallyrepresentedbyrenderabletrianglemeshes. Somehaveparti-\ncle effects. Many generate sounds. Some animate. Many have collision,\nand some are dynamically simulated by the physics engine. One of the\nprimary responsibilities of the gameplay foundation system is to ensure\nthat every game object has access to the services of the engine systems\nupon which it depends.\n•Real-timesimulationofobjectbehaviors. Atitscore, agameengineisareal-\ntimedynamiccomputer simulationofanagent-basedmodel. Thisisjust\na fancy way of saying that the game engine needs to update the states\nof all the game objects dynamically over time. The objects may need\nto be updated in a very particular order, dictated in part by dependen-\nciesbetweentheobjects,inpartbytheirdependenciesonvariousengine\nsubsystems,andinpartbecauseoftheinterdependenciesbetweenthose\nengine subsystems themselves.\n•Abilitytodefinenewgameobjecttypes. Every game’s requirements change\nandevolveasthegameisdeveloped. It’simportantthatthegameobject\nmodel be flexible enough to permit new object types to be added easily\nand exposed to the world editor. In an ideal world, it should be pos-\nsible to define a new type of object in an entirely data-driven manner.\nHowever, in many engines, the services of a programmer are required\nin order to add new game object types.\n•Unique object ids. Typical game worlds contain hundreds or even thou-\nsands of individual game objects of various types. At runtime, it’s im-\nportanttobeabletoidentifyorsearchforaparticularobject. Thismeans\neach object needs some kind of unique identifier. A human-readable\nname is the most convenient kind of id, but we must be wary of the\nperformance costs of using strings at runtime. Integer ids are the most\nefficient choice, but they are very difficult for human game developers\nto work with. Arguably the best solution is to use hashed string ids (see\nSection6.4.3.1)asourobjectidentifiers,astheyareasefficientasintegers\n1042 16. Runtime Gameplay Foundation Systems\nbut can be converted back into string form for ease of reading.\n•Gameobjectqueries. Thegameplayfoundationsystemmustprovidesome\nmeans of finding objects within the game world. We might want to find\na specific object by its unique id, or all the objects of a particular type, or\nwe might want to perform advanced queries based on arbitrary criteria\n(e.g., find all enemies within a 20 m radius of the player character).\n•Game object references. Once we’ve found the objects, we need some\nmechanism for holding references to them, either briefly within a single\nfunction or for much longer periods of time. An object reference might\nbeassimpleasapointertoaC++classinstance,oritmightbesomething\nmore sophisticated, like a handle or a reference-counted smart pointer.\n•Finitestatemachinesupport. Manytypesofgameobjectsarebestmodeled\nasfinitestatemachines(FSM).Somegameenginesprovidetheabilityfor\na game object to exist in one of many possible states, each with its own\nattributes and behavioral characteristics.\n•Network replication . In a networked multiplayer game, multiple game\nmachines are connected together via a LAN or the Internet. The state of\naparticulargameobjectisusuallyownedandmanagedbyonemachine.\nHowever,thatobject’sstatemustalsobe replicated (communicated)tothe\nothermachinesinvolvedinthemultiplayergamesothatallplayershave\na consistent view of the object.\n•Saving and loading / object persistence. Many game engines allow the cur-\nrent states of the game objects in the world to be saved to disk and later\nreloaded. This might be done to support a “save anywhere” save-game\nsystemorasawayofimplementingnetworkreplication,oritmightsim-\nply be the primary means of loading game world chunks that were au-\nthored in the world editor tool. Object persistence usually requires cer-\ntainlanguagefeatures,suchas runtimetypeidentification (RTTI),reflection\nandabstract construction. RTTI and reflection provide software with a\nmeans of determining an object’s type, and what attributes andmethods\nits class provides, dynamically at runtime. Abstract construction allows\ninstances of a class to be created without having to hard-code the name\nof the class—a very useful feature when serializing an object instance\ninto memory from disk. If RTTI, reflection and abstract construction are\nnot natively supported in your language of choice, these features can be\nadded manually.\nWe’ll spend the remainder of this chapter delving into each of these sub-\nsystems in depth.",9007
108-16.2 Runtime Object Model Architectures.pdf,108-16.2 Runtime Object Model Architectures,"16.2. Runtime Object Model Architectures 1043\n16.2 Runtime Object Model Architectures\nIn the world editor, the game designer is presented with an abstract game\nobject model, which defines the various types of dynamic elements that can\nexist in the game, how they behave and what kinds of attributes they have.\nAt runtime, the gameplay foundation system must provide a concrete imple-\nmentation of this object model. This is by far the largest component of any\ngameplay foundation system.\nThe runtime object model implementation may or may not bear any re-\nsemblance to the abstract tool-side object model. For example, it might not be\nimplemented in an object-oriented programming language at all, or it might\nuse a collection of interconnected class instances to represent a single abstract\ngame object. Whatever its design, the runtime object model must provide a\nfaithful reproduction of the object types, attributes and behaviors advertised\nby the world editor.\nTheruntimeobjectmodelisthein-gamemanifestationoftheabstracttool-\nsideobjectmodelpresentedtothedesignersintheworldeditor. Designsvary\nwidely, but most game engines follow one of two basic architectural styles:\n•Object-centric . In this style, each tool-side game object is represented at\nruntime by a single class instance or a small collection of interconnected\ninstances. Each object has a set of attributes andbehaviors that are encap-\nsulated within the class (or classes) of which the object is an instance.\nThe game world is just a collection of game objects.\n•Property-centric. In this style, each tool-side game object is represented\nonly by a unique id (implemented as an integer, hashed string id or\nstring). The properties of each game object are distributed across many\ndata tables, one per property type, and keyed by object id (rather than\nbeing centralized within a single class instance or collection of intercon-\nnected instances). The properties themselves are often implemented as\ninstances of hard-coded classes. The behavior of a game object is implic-\nitly defined by the collection of properties of which it is composed. For\nexample, if an object has the “Health” property, then it can be damaged,\nlosehealthandeventuallydie. Ifanobjecthasthe“MeshInstance”prop-\nerty, then it can be rendered in 3D as an instance of a triangle mesh.\nThere are distinct advantages and disadvantages to each of these architec-\nturalstyles. We’llinvestigateeachoneinsomedetailandnotewhereonestyle\nhas significant potential benefits over the other as they arise.\n1044 16. Runtime Gameplay Foundation Systems\n16.2.1 Object-Centric Architectures\nIn anobject-centric game world object architecture, each logical game object\nis implemented as an instance of a class, or possibly a collection of intercon-\nnected class instances. Under this broad umbrella, many different designs are\npossible. We’ll investigate a few of the most common designs in the following\nsections.\n16.2.1.1 A Simple Object-Based Model in C: Hydro Thunder\nGame object models needn’t be implemented in an object-oriented language\nlike C++ at all. For example, the arcade hit Hydro Thunder , by Midway Home\nEntertainmentinSanDiego, waswrittenentirelyinC. Hydroemployedavery\nsimple game object model consisting of only a few object types:\n• boats (player- and AI-controlled),\n• floating blue and red boost icons,\n• ambient animated objects (animals on the side of the track, etc.),\n• the water surface,\n• ramps,\n• waterfalls,\n• particle effects,\n• race track sectors (two-dimensional polygonal regions connected to one\nanother that together define the watery region in which boats could\nrace),\n• static geometry (terrain, foliage, buildings along the sides of the track,\netc.), and\n• two-dimensional heads-up display (HUD) elements.\nA few screenshots of Hydro Thunder are shown in Figure 16.1. Notice the\nhovering boost icons in both screenshots and the shark swimming by in the\nleft image (an example of an ambient animated object).\nHydrohad a C struct named World_t that stored and managed the con-\ntents of a game world (i.e., a single race track). The world contained pointers\nto arrays of various kinds of game objects. The static geometry was a single\nmesh instance. The water surface, waterfalls and particle effects were each\nrepresented by custom data structures. The boats, boost icons and other dy-\nnamic objects in the game were represented by instances of a general-purpose\nstruct called WorldOb_t (i.e., a world object). This was Hydro’s equivalent of\nagame object as we’ve defined it in this chapter.\n16.2. Runtime Object Model Architectures 1045\nFigure 16.1. Screenshots from the arcade game Hydro Thunder, developed by Midway Home Enter-\ntainment in San Diego.\nTheWorldOb_t data structure contained data members encoding the po-\nsitionandorientationoftheobject,the3Dmeshusedtorenderit,asetofcolli-\nsion spheres, simple animation state information (Hydro only supported rigid\nhierarchical animation), physical properties like velocity, mass and buoyancy,\nand other data common to all of the dynamic objects in the game. In addition,\neach WorldOb_t contained three pointers: a void* “user data” pointer, a\npointertoacustom“update”functionandapointertoacustom“draw”func-\ntion. So while Hydro Thunder was not object-oriented in the strictest sense,\ntheHydroengine did extend its non-object-oriented language (C) to support\nrudimentaryimplementationsoftwoimportantOOPfeatures: inheritance and\npolymorphism. The user data pointer permitted each type of game object to\nmaintain custom state information specific to its type while inheriting the fea-\ntures common to all world objects. For example, the Banshee boat had a dif-\nferent booster mechanism than the Rad Hazard, and each booster mechanism\nrequired different state information to manage its deployment and stowing\nanimations. The two function pointers acted like virtual functions, allowing\nworld objects to have polymorphic behaviors (via their “update” functions)\nand polymorphic visual appearances (via their “draw” functions).\nstruct WorldOb_s\n{\nOrient_t m_transform; /* position/rotation */\nMesh3d* m_pMesh; /* 3D mesh */\n/* ... */\nvoid* m_pUserData; /* custom state */\nvoid (* m_pUpdate)(); /* polymorphic update */\nvoid (* m_pDraw )(); /* polymorphic draw */\n};\ntypedef struct WorldOb_s WorldOb_t;\n1046 16. Runtime Gameplay Foundation Systems\nFigure 16.2. A hypothetical class hierarchy for the game Pac-Man.\n16.2.1.2 Monolithic Class Hierarchies\nIt’s natural to want to classify game object types taxonomically. This tends\nto lead game programmers toward an object-oriented language that supports\ninheritance. Aclasshierarchyisthemostintuitiveandstraightforwardwayto\nrepresent a collection of interrelated game object types. So it is not surprising\nthat the majority of commercial game engines employ a class hierarchy based\ntechnique.\nFigure16.2showsasimpleclasshierarchythatcouldbeusedtoimplement\nthe game Pac-Man. This hierarchy is rooted (as many are) at a common class\ncalled GameObject, which might provide some facilities needed by all object\ntypes, such as RTTI or serialization. The MovableObject class represents\nany object that has a position and orientation. RenderableObject gives the\nobject an ability to be rendered (in the case of traditional Pac-Man, via a sprite,\nor in the case of a modern 3D Pac-Man game, perhaps via a triangle mesh).\nFrom RenderableObject are derived classes for the ghosts, Pac-Man, pel-\nlets and power pills that make up the game. This is just a hypothetical ex-\nample, but it illustrates the basic ideas that underlie most game object class\nhierarchies—namely that common, generic functionality tends to exist at the\nroot of the hierarchy, while classes toward the leaves of the hierarchy tend to\nadd increasingly specific functionality.\nA game object class hierarchy usually begins small and simple, and in that\nform, it can be a powerful and intuitive way to describe a collection of game\nobject types. However, as class hierarchies grow, they have a tendency to\ndeepen and widen simultaneously, leading to what I call a monolithic class hi-\nerarchy. This kind of hierarchy arises when virtually all classes in the game\nobject model inherit from a single, common base class. The Unreal Engine’s\ngame object model is a classic example, as Figure 16.3 illustrates.\n16.2. Runtime Object Model Architectures 1047\nFigure 16.3. An excerpt from the game object class hierarchy in the Unreal engine.\n16.2.1.3 Problems with Deep, Wide Hierarchies\nMonolithicclasshierarchiestendtocauseproblemsforthegamedevelopment\nteam for a wide range of reasons. The deeper and wider a class hierarchy\ngrows, the more extreme these problems can become. In the following sec-\ntions,we’llexploresomeofthemostcommonproblemscausedbywide,deep\nclass hierarchies.\nUnderstanding, Maintaining and Modifying Classes\nThe deeper a class lies within a class hierarchy, the harder it is to understand,\nmaintain and modify. This is because to understand a class, you really need\nto understand all of its parent classes as well. For example, modifying the\n1048 16. Runtime Gameplay Foundation Systems\nbehavior of an innocuous-looking virtual function in a derived class could vi-\nolate the assumptions made by any one of the many base classes, leading to\nsubtle, difficult-to-find bugs.\nInability to Describe Multidimensional Taxonomies\nA hierarchy inherently classifies objects according to a particular system of\ncriteria known as a taxonomy. For example, biological taxonomy (also known\nasalpha taxonomy ) classifies all living things according to genetic similarities,\nusing a tree with eight levels: domain, kingdom, phylum, class, order, family,\ngenus and species. At each level of the tree, a different criterion is used to\ndividethemyriadlifeformsonourplanetintomoreandmorerefinedgroups.\nOne of the biggest problems with any hierarchy is that it can only classify\nobjects along a single “axis”—according to one particular set of criteria—at\neach level of the tree. Once the criteria have been chosen for a particular hier-\narchy, it becomes difficult or impossible to classify along an entirely different\nset of “axes.” For example, biological taxonomy classifies objects according to\ngenetic traits, but it says nothing about the colors of the organisms. In order\nto classify organisms by color, we’d need an entirely different tree structure.\nIn object-oriented programming, this limitation of hierarchical classifica-\ntion often manifests itself in the form of wide, deep and confusing class hier-\narchies. When one analyzes a real game’s class hierarchy, one often finds that\nitsstructureattemptstomeldanumberofdifferentclassificationcriteriaintoa\nsingle class tree. In other cases, concessions are made in the class hierarchy to\naccommodate a new type of object whose characteristics were not anticipated\nwhen the hierarchy was first designed. For example, imagine the seemingly\nlogical class hierarchy describing different types of vehicles, depicted in Fig-\nure 16.4.\nVehicle\nMotorcycle SpeedBoatCar Truck Hovercraft YachtLandVehicle WaterVehicle\nFigure 16.4. A seemingly logical class hierarchy describing various kinds of vehicles.\n16.2. Runtime Object Model Architectures 1049\nWhat happens when the game designers announce to the programmers\nthat they now want the game to include an amphibious vehicle? Such a vehicle\ndoes not fit into the existing taxonomic system. This may cause the program-\nmers to panic or, more likely, to “hack” their class hierarchy in various ugly\nand error-prone ways.\nMultiple Inheritance: The Deadly Diamond\nOne solution to the amphibious vehicle problem is to utilize C++’s multiple\ninheritance (MI) features, as shown in Figure 16.5. At first glance, this seems\nlike a good solution. However, multiple inheritance in C++ poses a number\nof practical problems. For example, multiple inheritance can lead to an object\nthat contains multiple copies of its base class’ members—a condition known\nas the “deadly diamond” or “diamond of death.” (See Section 3.1.1.3 for more\ndetails.)\nThe difficulties in building an MI class hierarchy that works and that is\nunderstandable and maintainable usually outweigh the benefits. As a result,\nmostgamestudios prohibitor severelylimit theuseof multipleinheritance in\ntheir class hierarchies.\nFigure 16.5. A diamond-shaped class hierarchy for amphibious vehicles.\nMix-In Classes\nSome teams do permit a limited form of MI, in which a class may have any\nnumber of parent classes but only onegrandparent. In other words, a class\nmay inherit from one and only one class in the main inheritance hierarchy,\nbut it may also inherit from any number of mix-in classes (stand-alone classes\nwithnobaseclass). Thispermitscommonfunctionalitytobefactoredoutinto\na mix-in class and then spot-patched into the main hierarchy wherever it is\nneeded. ThisisshowninFigure16.6. However, aswe’llseebelow,it’susually\nbetter to compose oraggregate such classes than to inheritfrom them.\n1050 16. Runtime Gameplay Foundation Systems\nGameObject+GetHealth()\n+ApplyDamage()\n+IsDead()+OnDeath()MHealth+PickUp()+Drop()+IsBeingCarried()MCar ryable\nNPC Player Tank Jeep Pistol MG Canteen AmmoCharacter Vehicle Weapon Item\nFigure 16.6. A class hierarchy with mix-in classes. The MHealth mix-in class adds the notion of\nhealth and the ability to be killed to any class that inherits it. The MCarryable mix-in class allows\nan object that inherits it to be carried by a Character.\nThe Bubble-Up Effect\nWhenamonolithicclasshierarchyisfirstdesigned,therootclassorclassesare\nusually very simple, each one exposing only a minimal feature set. However,\nas more and more functionality is added to the game, the desire to share code\nbetween two or more unrelated classes begins to cause features to “bubble up”\nthe hierarchy.\nForexample,wemightstartoutwithadesigninwhichonlywoodencrates\ncan float in water. However, once our game designers see those cool floating\ncrates, they begin to ask for other kinds of floating objects, like characters,\nbits of paper, vehicles and so on. Because “floating versus non-floating” was\nnotoneoftheoriginalclassificationcriteriawhenthehierarchywasdesigned,\nthe programmers quickly discover the need to add flotation to classes that are\ntotally unrelated within the class hierarchy. Multiple inheritance is frowned\nupon, so the programmers decide to move the flotation code up the hierarchy,\ninto a base class that is common to all objects that need to float. The fact that\nsomeoftheclassesthatderivefromthiscommonbaseclass cannotfloat isseen\naslessofaproblemthanduplicatingtheflotationcodeacrossmultipleclasses.\n(ABooleanmembervariablecalledsomethinglike m_bCanFloat mighteven\nbe added to make the distinction clear.) The ultimate result is that flotation\neventually becomes a feature of the root object in the class hierarchy (along\nwith pretty much every other feature in the game).\nTheActorclassinUnrealisaclassicexampleofthis“bubble-upeffect.” It\ncontainsdatamembersandcodeformanagingrendering,animation,physics,\nworld interaction, audio effects, network replication for multiplayer games,\n16.2. Runtime Object Model Architectures 1051\nobject creation and destruction, actor iteration (i.e., the ability to iterate over\nallactorsmeetingacertaincriteriaandperformsomeoperationonthem),and\nmessage broadcasting. Encapsulating the functionality of various engine sub-\nsystems is difficult when features are permitted to “bubble up” to the root-\nmost classes in a monolithic class hierarchy.\n16.2.1.4 Using Composition to Simplify the Hierarchy\nPerhaps the most prevalent cause of monolithic class hierarchies is over-use\nof the “is-a” relationship in object-oriented design. For example, in a game’s\nGUI,aprogrammermightdecidetoderivetheclass Window fromaclasscalled\nRectangle, using the logic that GUI windows are always rectangular. How-\never, a window is not arectangle—it has arectangle, which defines its bound-\nary. So a more workable solution to this particular design problem is to em-\nbedaninstanceofthe Rectangle classinsidethe Window class,ortogivethe\nWindow a pointer or reference to a Rectangle .\nInobject-orienteddesign, the“has-a”relationshipisknownas composition.\nIn composition, a class A either contains an instance of class B directly, or con-\ntains apointerorreference to an instance of B. Strictly speaking, in order for the\nterm“composition”tobeapplicable,classAmust ownclassB.Thismeansthat\nwhen an instance of class A is created, it automatically creates an instance of\nclass B as well; when that instance of A is destroyed, its instance of B is de-\nstroyed, too. We can also link classes to one another via a pointer or reference\nwithout having one of the classes manage the other’s lifetime. In that case, the\ntechnique is usually called aggregation.\nConverting Is-A to Has-A\nConverting“is-a”relationshipsinto“has-a”relationshipscanbeausefultech-\nnique for reducing the width, depth and complexity of a game’s class hier-\narchy. To illustrate, let’s take a look at the hypothetical monolithic hierar-\nchy shown in Figure 16.7. The root GameObject class provides some ba-\nsic functionality required by all game objects (e.g., RTTI, reflection, persis-\ntence via serialization, network replication, etc.). The MovableObject class\nrepresents any game object that has a transform (i.e., a position, orientation\nand optional scale). RenderableObject adds the ability to be rendered on-\nscreen. (Not all game objects need to be rendered—for example, an invisible\nTriggerRegion classcouldbederiveddirectlyfrom MovableObject.) The\nCollidableObject classprovidescollisioninformationtoitsinstances. The\nAnimatingObject classgrantstoitsinstancestheabilitytobeanimatedviaa\nskeletal joint hierarchy. Finally, the PhysicalObject gives its instances the\n1052 16. Runtime Gameplay Foundation Systems\nabilitytobephysicallysimulated(e.g.,arigidbodyfallingundertheinfluence\nof gravity and bouncing around in the game world).\nFigure 16.7. A hypo-\nthetical game object\nclass hierarchy us-\ning only inheritance\nto associate the\nclasses.One big problem with this class hierarchy is that it limits our design\nchoices when creating new types of game objects. If we want to define an\nobject type that is physically simulated, we are forced to derive its class\nfrom PhysicalObject even though it may not require skeletal anima-\ntion. If we want a game object class with collision, it must inherit from\nCollidableObject even though it may be invisible and hence not require\nthe services of RenderableObject.\nA second problem with the hierarchy shown in Figure 16.7 is that it is\ndifficult to extend the functionality of the existing classes. For example,\nlet’s imagine we want to support morph target animation, so we derive two\nnew classes from AnimatingObject called SkeletalObject andMorph-\nTargetObject. If we wanted both of these new classes to have the ability to\nbephysicallysimulated,we’dbeforcedtorefactor PhysicalObject intotwo\nnearly identical classes, one derived from SkeletalObject and one from\nMorphTargetObject, or turn to multiple inheritance.\nOne solution to these problems is to isolate the various features of a\nGameObject into independent classes, each of which provides a single, well-\ndefinedservice. Suchclassesaresometimescalled components orserviceobjects .\nA componentized design allows us to select only those features we need for\neach type of game object we create. In addition, it permits each feature to\nbe maintained, extended or refactored without affecting the others. The indi-\nvidual components are also easier to understand, and easier to test, because\nthey are decoupled from one another. Some component classes correspond\ndirectly to a single engine subsystem, such as rendering, animation, collision,\nphysics, audio, etc. This allows these subsystems to remain distinct and well-\nencapsulated when they are integrated together for use by a particular game\nobject.\nFigure 16.8 shows how our class hierarchy might look after refactoring it\nintocomponents. Inthisreviseddesign,the GameObject classactslikeahub,\ncontaining pointers to each of the optional components we’ve defined. The\nMeshInstance component is our replacement for the RenderableObject\nclass—itrepresentsaninstanceofatrianglemeshandencapsulatestheknowl-\nedge of how to render it. Likewise, the AnimationController compo-\nnentreplaces AnimatingObject ,exposingskeletalanimationservicestothe\nGameObject. Class Transform replaces MovableObject by maintaining\nthe position, orientation and scale of the object. The RigidBody class repre-\nsents the collision geometry of a game object and provides its GameObject\nwith an interface into the low-level collision and physics systems, replacing\n16.2. Runtime Object Model Architectures 1053\nGameObjectTransform\nMeshInstance AnimationController\nRigidBody11\n1 1\n1\n11 1\nFigure 16.8. Our hypothetical game object class hierarchy, refactored to favor class composition\nover inheritance.\nCollidableObject andPhysicalObject.\nComponent Creation and Ownership\nIn this kind of design, it is typical for the “hub” class to ownits compo-\nnents, meaningthatitmanagestheir lifetimes. Buthowshoulda GameObject\n“know” which components to create? There are numerous ways to solve this\nproblem, but one of the simplest is to provide the root GameObject class\nwith pointers to all possible components. Each unique type of game object\nis defined as a derived class of GameObject. In the GameObject construc-\ntor, all of the component pointers are initially set to nullptr . Each derived\nclass’s constructor is then free to create whatever components it may need.\nFor convenience, the default GameObject destructor can clean up all of the\ncomponents automatically. In this design, the hierarchy of classes derived\nfrom GameObject serves as the primary taxonomy for the kinds of objects\nwe want in our game, and the component classes serve as optional add-on\nfeatures.\nOne possible implementation of the component creation and destruction\nlogic for this kind of hierarchy is shown below. However, it’s important to\nrealize that this code is just an example—implementation details vary widely,\nevenbetweenenginesthatemployessentiallythesamekindofclasshierarchy\ndesign.\nclass GameObject\n{\nprotected:\n1054 16. Runtime Gameplay Foundation Systems\n// My transform (position, rotation, scale).\nTransform m_transform;\n// Standard components:\nMeshInstance* m_pMeshInst;\nAnimationController* m_pAnimController;\nRigidBody* m_pRigidBody ;\npublic:\nGameObject()\n{\n// Assume no components by default.\n// Derived classes will override.\nm_pMeshInst = nullptr;\nm_pAnimController = nullptr;\nm_pRigidBody = nullptr;\n}\n~GameObject()\n{\n// Automatically delete any components created by\n// derived classes. (Deleting null pointers OK.)\ndelete m_pMeshInst;\ndelete m_pAnimController;\ndelete m_pRigidBody;\n}\n// ...\n};\nclass Vehicle : public GameObject\n{\nprotected:\n// Add some more components specific to Vehicles...\nChassis* m_pChassis;\nEngine* m_pEngine;\n// ...\npublic:\nVehicle()\n{\n// Construct standard GameObject components.\nm_pMeshInst = new MeshInstance ;\nm_pRigidBody = new RigidBody ;\n// NOTE: We'll assume the animation controller\n// must be provided with a reference to the mesh\n16.2. Runtime Object Model Architectures 1055\n// instance so that it can provide it with a\n// matrix palette.\nm_pAnimController\n=new AnimationController (*m_pMeshInst);\n// Construct vehicle-specific components.\nm_pChassis = new Chassis(*this,\n*m_pAnimController);\nm_pEngine = new Engine (*this);\n}\n~Vehicle()\n{\n// Only need to destroy vehicle-specific\n// components, as GameObject cleans up the\n// standard components for us.\ndelete m_pChassis;\ndelete m_pEngine;\n}\n};\n16.2.1.5 Generic Components\nAnothermoreflexible(butalsotrickiertoimplement)alternativeistoprovide\nthe root game object class with a generic linked list of components. The com-\nponents in such a design usually all derive from a common base class—this\nallows us to iterate over the linked list and perform polymorphic operations,\nsuch as asking each component what type it is or passing an event to each\ncomponent in turn for possible handling. This design allows the root game\nobject class to be largely oblivious to the component types that are available\nand thereby permits new types of components to be created without modify-\ningthegameobjectclassinmanycases. Italsoallowsaparticulargameobject\nto contain an arbitrary number of instances of each type of component. (The\nhard-coded design permits only a fixed number, determined by how many\npointers to each component exist within the game object class.)\nThis kind of design is illustrated in Figure 16.9. It is trickier to implement\nthan a hard-coded component model because the game object code must be\nwritten in a totally generic way. The component classes can likewise make no\nassumptions about what other components might or might not exist within\nthe context of a particular game object. The choice between hard-coding the\ncomponentpointersorusingagenericlinkedlistofcomponentsisnotaneasy\nonetomake. Neitherdesignisclearlysuperior—theyeachhavetheirprosand\ncons, and different game teams take different approaches.\n1056 16. Runtime Gameplay Foundation Systems\nAsterisk indicates zero \nor more instances \n(e.g., linked list).\nFigure 16.9. A linked list of components can provide ﬂexibility by allowing the hub game object to\nbe unaware of the details of any particular component.\n16.2.1.6 Pure Component Models\nWhat would happen if we were to take the componentization concept to its\nextreme? We would move literally allof the functionality out of our root\nGameObject class into various component classes. At this point, the game\nobjectclasswouldquiteliterallybeabehavior-lesscontainer, withauniqueid\nand a bunch of pointers to its components, but otherwise containing no logic\nof its own. So why not eliminate the class entirely? One way to do this is to\ngive each component a copy of the game object’s unique id. The components\nare now linked together into a logical grouping by id. Given a way to quickly\nlook up any component by id, we would no longer need the GameObject\n“hub”classatall. Iwillusetheterm purecomponentmodel todescribethiskind\nof architecture. It is illustrated in Figure 16.10.\n-m_uniqueId : int = 72GameObject-m_uniqueId : int = 72Transform\n-m_uniqueId : int = 72MeshInstance\n-m_uniqueId : int = 72AnimationController\n-m_uniqueId : int = 72RigidBody\nFigure 16.10. In a pure component model, a logical game object is comprised of many components,\nbut the components are linked together only indirectly, by sharing a unique id.\n16.2. Runtime Object Model Architectures 1057\nA pure component model is not quite as simple as it first sounds, and it\nis not without its share of problems. For one thing, we still need some way\nof defining the various concrete types of game objects our game needs and\nthen arranging for the correct component classes to be instantiated whenever\nan instance of the type is created. Our GameObject hierarchy used to handle\nconstruction of components for us. Instead, we might use a factory pattern,\nin which we define factory classes, one per game object type, with a virtual\nconstruction function that is overridden to create the proper components for\neach game object type. Or we might turn to a data-driven model, where the\ngame object types are defined in a text file that can be parsed by the engine\nand consulted whenever a type is instantiated.\nAnotherissuewithacomponents-onlydesignisinter-componentcommu-\nnication. Ourcentral GameObject actedasa“hub,”marshallingcommunica-\ntions between the various components. In pure component architectures, we\nneed an efficient way for the components making up a single game object to\ntalktooneanother. Thiscouldbedonebyhavingeachcomponentlookupthe\nother components using the game object’s unique id. However, we probably\nwant a much more efficient mechanism—for example the components could\nbe prewired into a circular linked list.\nInthesamesense,sendingmessagesfromonegameobjecttoanotherisdif-\nficult in a pure componentized model. We can no longer communicate with\ntheGameObject instance,soweeitherneedtoknowaprioriwithwhichcom-\nponent we wish to communicate, or we must multicast to all components that\nmake up the game object in question. Neither option is ideal.\nPure component models can and have been made to work on real game\nprojects. These kinds of models have their pros and cons, but again, they are\nnot clearly better than any of the alternative designs. Unless you’re part of a\nresearchanddevelopmenteffort,youshouldprobablychoosethearchitecture\nwith which you are most comfortable and confident, and which best fits the\nneeds of the particular game you are building.\n16.2.2 Property-Centric Architectures\nProgrammers who work frequently in an object-oriented programming lan-\nguage tend to think naturally in terms of objects that contain attributes (data\nmembers) and behaviors (methods, member functions). This is the object-\ncentric view:\n• Object1\n◦Position = (0, 3, 15)\n◦Orientation = (0, 43, 0)\n1058 16. Runtime Gameplay Foundation Systems\n• Object2\n◦Position = ( 12, 0, 8)\n◦Health = 15\n•Object3\n◦Orientation = (0, 87, 10)\nHowever, it is possible to think primarily in terms of the attributes, rather\nthan the objects. We define the set of all properties that a game object might\nhave. Then for each property, we build a table containing the values of that\nproperty corresponding to each game object that has it. The property values\nare keyed by the objects’ unique ids. This is what we will call the property-\ncentric view:\n• Position\n◦Object1 = (0, 3, 15)\n◦Object2 = ( 12, 0, 8)\n• Orientation\n◦Object1 = (0, 43, 0)\n◦Object3 = (0, 87, 10)\n• Health\n◦Object2 = 15\nProperty-centric object models have been used very successfully on many\ncommercial games, including Deus Ex 2 and the Thiefseries of games. See\nSection 16.2.2.5 for more details on exactly how these projects designed their\nobject systems.\nA property-centric design is more akin to a relational database than an ob-\nject model. Each attribute acts like a table in a relational database, with the\ngame objects’ unique id as the primary key. Of course, in object-oriented de-\nsign, an object is defined not only by its attributes , but also by its behavior. If all\nwe have are tables of properties, then where do we implement the behavior?\nThe answer to this question varies somewhat from engine to engine, but most\noften the behaviors are implemented in one or both of the following places:\n• in the properties themselves, and/or\n• via script code.\nLet’s explore each of these ideas further.\n16.2. Runtime Object Model Architectures 1059\n16.2.2.1 Implementing Behavior via Property Classes\nEachtypeofpropertycanbeimplementedasa propertyclass. Propertiescanbe\nassimpleasasingleBooleanorfloating-pointvalueorascomplexasarender-\nabletrianglemeshoranAI“brain.” Eachpropertyclasscanprovidebehaviors\nviaitshard-codedmethods(memberfunctions). Theoverallbehaviorofapar-\nticular game object is determined by the aggregation of the behaviors of all its\nproperties.\nFor example, ifa game object contains an instance of the Health property,\nit can be damaged and eventually destroyed or killed. The Health object\ncan respond to any attacks made on the game object by decrementing the ob-\nject’shealthlevelappropriately. Apropertyobjectcanalsocommunicatewith\nother property objects within the same game object to produce cooperative\nbehaviors. For example, when the Health property detects and responds to\nan attack, it could possibly send a message to the AnimatedSkeleton prop-\nerty,therebyallowingthegameobjecttoplayasuitablehitreactionanimation.\nSimilarly, when the Health property detects that the game object is about to\ndie or be destroyed, it can talk to the RigidBodyDynamics property to acti-\nvate a physics-driven explosion or a “rag doll” dead body simulation.\n16.2.2.2 Implementing Behavior via Script\nAnother option is to store the property values as raw data in one or more\ndatabase-like tables and use script code to implement a game object’s behav-\niors. Every game object could have a special property called something like\nScriptId, which, if present, specifies the block of script code (script func-\ntion, or script object if the scripting language is itself object-oriented) that will\nmanage the object’s behavior. Script code could also be used to allow a game\nobject to respond to events that occur within the game world. See Section 16.8\nfor more details on event systems and Section 16.9 for a discussion of game\nscripting languages.\nInsomeproperty-centricengines,acoresetofhard-codedpropertyclasses\nis provided by the engineers, but a facility is provided allowing game design-\nersandprogrammerstoimplementnewpropertytypesentirelyinscript. This\napproach was used successfully on the Dungeon Siege project, for example.\n16.2.2.3 Properties versus Components\nIt’s important to note that many of the authors cited in Section 16.2.2.5 use the\nterm “component” to refer to what I call a “property object” here. In Section\n16.2.1.4,Iusedtheterm“component”torefertoasubobjectinanobject-centric\ndesign, which isn’t quite the same as a property object.\n1060 16. Runtime Gameplay Foundation Systems\nHowever, property objects are very closely related to components in many\nways. Inbothdesigns,asinglelogicalgameobjectismadeupofmultiplesub-\nobjects. The main distinction lies in the roles of the subobjects. In a property-\ncentric design, each subobject defines a particular attribute of the game object\nitself (e.g., health, visual representation, inventory, a particular magic power,\netc.), whereas in a component-based (object-centric) design, the subobjects of-\ntenrepresentlinkagestoparticularlow-levelenginesubsystems(renderer,an-\nimation, collision and dynamics, etc.) This distinction is so subtle as to be\nvirtually irrelevant in many cases. You can call your design a pure component\nmodel(Section 16.2.1.6) or a property-centricdesign as you see fit, but at the end\nof the day, you’ll have essentially the same result—a logical game object that\nis comprised of, and derives its behavior from, a collection of subobjects.\n16.2.2.4 Pros and Cons of Property-Centric Designs\nThere are a number of potential benefits to an attribute-centric approach. It\ntends to be more memory-efficient, because we need only store attribute data\nthatisactuallyinuse(i.e.,therearenevergameobjectswithunuseddatamem-\nbers). It is also easier to construct such a model in a data-driven manner—\ndesigners can define new attributes easily, without recompiling the game, be-\ncause there are no game object class definitions to be changed. Programmers\nneedonlygetinvolvedwhenentirelynewtypesofpropertiesneedtobeadded\n(presuming the property cannot be added via script).\nA property-centric design can also be more cache-friendly than an object-\ncentricmodel, becausedataofthesametypeisstored contiguously inmemory.\nThis is a commonplace optimization technique on modern gaming hardware,\nwhere the cost of accessing memory is far higher than the cost of executing\ninstructions and performing calculations. (For example, on the PlayStation 3,\nthe cost of a single cache miss is equivalent to the cost of executing literally\nthousands of CPU instructions.) By storing data contiguously in RAM, we\ncan reduce or eliminate cache misses, because when we access one element of\na data array, a large number of its neighboring elements are loaded into the\nsame cache line. This approach to data design is sometimes called the structof\narraystechnique, in contrast to the more-traditional array of structs approach.\nThedifferencesbetweenthesetwomemorylayoutsareillustratedbythecode\nsnippet below. (Note that we wouldn’t really implement a game object model\nin exactly this way—this example is meant only to illustrate the way in which\na property-centric design tends to produce many contiguous arrays of like-\ntyped data, rather than a single array of complex objects.)\nstatic const U32 MAX_GAME_OBJECTS = 1024;\n16.2. Runtime Object Model Architectures 1061\n// Traditional array-of-structs approach.\nstruct GameObject\n{\nU32 m_uniqueId;\nVector m_pos;\nQuaternion m_rot;\nfloat m_health;\n// ...\n};\nGameObject g_aAllGameObjects[MAX_GAME_OBJECTS];\n// Cache-friendlier struct-of-arrays approach.\nstruct AllGameObjects\n{\nU32 m_aUniqueId[MAX_GAME_OBJECTS];\nVector m_aPos[MAX_GAME_OBJECTS];\nQuaternion m_aRot [MAX_GAME_OBJECTS];\nfloat m_aHealth[MAX_GAME_OBJECTS];\n// ...\n};\nAllGameObjects g_allGameObjects;\nAttribute-centric modelshavetheirshareofproblemsaswell. Forexample,\nwhen a game object is just a grab bag of properties, it becomes much more\ndifficult to enforce relationships between those properties. It can be hard to\nimplementadesiredlarge-scalebehaviormerelybycobblingtogetherthefine-\ngrained behaviors of a group of property objects. It’s also much trickier to\ndebug such systems, as the programmer cannot slap a game object into the\nwatch window in the debugger in order to inspect all of its properties at once.\n16.2.2.5 Further Reading\nA number of interesting PowerPoint presentations on the topic of property-\ncentric architectures have been given by prominent engineers in the game in-\ndustry at various game development conferences.\n• Rob Fermier, “Creating a Data Driven Engine,” Game Developer’s Con-\nference, 2002.\n• Scott Bilas, “A Data-Driven Game Object System,” Game Developer’s\nConference, 2002.",37997
109-16.3 World Chunk Data Formats.pdf,109-16.3 World Chunk Data Formats,"1062 16. Runtime Gameplay Foundation Systems\n• AlexDuran,“BuildingObjectSystems: Features,Tradeoffs,andPitfalls,”\nGame Developer’s Conference, 2003.\n• Jeremy Chatelaine, “Enabling Data Driven Tuning via Existing Tools,”\nGame Developer’s Conference, 2003.\n• Doug Church, “Object Systems,” presented at a game development con-\nference in Seoul, Korea, 2003; conference organized by Chris Hecker,\nCasey Muratori, Jon Blow and Doug Church. http://chrishecker.com/\nimages/6/6f/ObjSys.ppt.\n16.3 World Chunk Data Formats\nAswe’veseen,aworldchunkgenerallycontainsboth staticanddynamic world\nelements. The static geometry might be represented by one big triangle mesh,\nor it might be comprised of many smaller meshes. Each mesh might be in-\nstancedmultiple times—for example, a single door mesh might be reused for\nall of the doorways in the chunk. The static data usually includes collision\ninformation stored as a triangle soup, a collection of convex shapes and/or\nother simpler geometric shapes like planes, boxes, capsules or spheres. Other\nstatic elements include volumetric regionsthat can be used to detect events or\ndelineate areas within the game world, an AI navigationmesh, a set of line seg-\nments delineating edgeswithin the background geometry that can be grabbed\nby the player character and so on. We won’t get into the details of these\ndata formats here, because we’ve already discussed most of them in previous\nsections.\nThe dynamic portion of the world chunk contains some kind of represen-\ntation of the game objects within that chunk. A game object is defined by its\nattributes and itsbehaviors, and an object’s behaviors are determined either di-\nrectly or indirectly by its type. In an object-centric design, the object’s type\ndirectly determines which class(es) to instantiate in order to represent the ob-\nject at runtime. In a property-centric design, a game object’s behavior is de-\ntermined by the amalgamation of the behaviors of its properties, but the type\nstilldetermineswhichpropertiestheobjectshouldhave(oronemightsaythat\nan object’s properties define its type). So, for each game object, a world chunk\ndata file generally contains:\n•Theinitialvaluesoftheobjects’attributes. Theworldchunkdefinesthestate\nof each game object as it should exist when first spawned into the game\nworld. An object’s attribute data can be stored in a number of different\nformats. We’ll explore a few popular formats below.\n16.3. World Chunk Data Formats 1063\n•Somekindofspecificationoftheobject’stype. Inanobject-centricengine,this\nmight be a string, a hashed string id or some other unique type id. In a\nproperty-centric design, the type might be stored explicitly, or it might\nbe defined implicitly by the collection of properties/attributes of which\nthe object is comprised.\n16.3.1 Binary Object Images\nOnewaytostoreacollectionofgameobjectsintoadiskfileistowriteabinary\nimageofeachobjectintothefile,exactlyasitlooksinmemoryatruntime. This\nmakes spawning game objects trivial. Once the game world chunk has been\nloaded into memory, we have ready-made images of all our objects, so we\nsimply let them fly.\nWell,notquite. Storingbinaryimagesof“live”C++classinstancesisprob-\nlematic for a number of reasons, including the need to handle pointers andvir-\ntualtables inaspecialway,andthepossibilityofhavingto endian-swap thedata\nwithineachclassinstance. (ThesetechniquesaredescribedindetailinSection\n7.2.2.9.) Moreover, binary object images are inflexible and not robust to mak-\ning changes. Gameplay is one of the most dynamic and unstable aspects of\nany game project, so it is wise to select a data format that supports rapid de-\nvelopmentandisrobusttofrequentchanges. Assuch,thebinaryobjectimage\nformatisnotusuallyagoodchoiceforstoringgameobjectdata(althoughthis\nformat can be suitable for more stable data structures, like mesh data or colli-\nsion geometry).\n16.3.2 Serialized Game Object Descriptions\nSerialization is another means of storing a representation of a game object’s in-\nternalstatetoadiskfile. Thisapproachtendstobemoreportableandsimpler\ntoimplementthanthebinaryobjectimagetechnique. Toserializeanobjectout\nto disk, the object is asked to produce a stream of data that contains enough\ndetail to permit the original object to be reconstructed later. When an object is\nserialized back into memory from disk, an instance of the appropriate class is\ncreated, and then the stream of attribute data is read in order to initialize the\nnewobject’sinternalstate. Iftheoriginalserializeddatastreamwascomplete,\nthe new object should be identical to the original for all intents and purposes.\nSerialization is supported natively by some programming languages. For\nexample, C# and Java both provide standardized mechanisms for serializing\nobject instances to and from an XML text format. The C++ language unfortu-\nnately does not provide a standardized serialization facility. However, many\nC++ serialization systems have been successfully built, both inside and out-\n1064 16. Runtime Gameplay Foundation Systems\nsidethegameindustry. Wewon’tgetintoallthedetailsofhowtowriteaC++\nobject serialization system here, but we’ll describe the data format and a few\nofthemainsystemsthatneedtobewritteninordertogetserializationtowork\nin C++.\nSerialization data isn’t a binary image of the object. Instead, it is usually\nstored in a more-convenient and more-portable format. XML is a popular for-\nmatforobjectserializationbecauseitiswell-supportedandstandardized, itis\nsomewhat human-readable and it has excellent support for hierarchical data\nstructures, which arise frequently when serializing collections of interrelated\ngame objects. Unfortunately, XML is notoriously slow to parse, which can\nincrease world chunk load times. For this reason, some game engines use a\nproprietary binary format that is faster to parse and more compact than XML\ntext.\nMany game engines (and non-game object serialization systems) have\nturned to the text-based JSON data format (http://www.json.org) as an alter-\nnative to XML. JSON is also used ubiquitously for data communication over\nthe World Wide Web. For example, the Facebook API communicates exclu-\nsively using JSON.\nThe mechanics of serializing an object to and from disk are usually imple-\nmented in one of two basic ways:\n• We can introduce a pair of virtual functions called something like\nSerializeOut() andSerializeIn() in our base class and arrange\nfor each derived class to provide custom implementations of them that\n“know” how to serialize the attributes of that particular class.\n• We can implement a reflection system for our C++ classes. We can then\nwrite a generic system that can automatically serialize any C++ object\nfor which reflection information is available.\nReflection is a term used by the C# language, among others. In a nutshell,\nreflectiondataisaruntimedescriptionofthecontentsofaclass. Itstoresinfor-\nmation about the name of the class, what data members it contains, the types\nof each data member and the offset of each member within the object’s mem-\nory image, and it also contains information about all of the class’s member\nfunctions. Given reflection information for an arbitrary C++ class, we could\nquite easily write a general-purpose object serialization system.\nThe tricky part of a C++ reflection system is generating the reflection data\nfor all of the relevant classes. This can be done by encapsulating a class’s data\nmembers in #define macros that extract relevant reflection information by\nproviding a virtual function that can be overridden by each derived class in\n16.3. World Chunk Data Formats 1065\norder to return appropriate reflection data for that class, by hand-coding a re-\nflection data structure for each class, or via some other inventive approach.\nIn addition to attribute information, the serialization data stream invari-\nably includes the name or unique id of each object’s classortype. The class id\nis used to instantiate the appropriate class when the object is serialized into\nmemory from disk. A class id can be stored as a string, a hashed string id, or\nsome other kind of unique id.\nUnfortunately, C++ provides no way to instantiate a class given only its\nname as a string or id. The class name must be known at compile time, and\nso it must be hard-coded by a programmer (e.g., new ConcreteClass). To\nwork around this limitation of the language, C++ object serialization systems\ninvariably include a class factory of some kind. A factory can be implemented\nin any number of ways, but the simplest approach is to create a data table that\nmaps each class name/id to some kind of function or functor object that has\nbeen hard-coded to instantiate that particular class. Given a class name or id,\nwe simply look up the corresponding function or functor in the table and call\nit to instantiate the class.\n16.3.3 Spawners and Type Schemas\nBoth binary object images and serialization formats have an Achilles heel.\nTheyarebothdefinedbytheruntimeimplementationofthegameobjecttypes\nthey store, and hence they both require the world editor to contain intimate\nknowledgeofthegameengine’sruntimeimplementation. Forexample, inor-\nder for the world editor to write out a binary image of a heterogeneous collec-\ntion of game objects, it must either link directly with the runtime game engine\ncode, or it must be painstakingly hand-coded to produce blocks of bytes that\nexactly match the data layout of the game objects at runtime. Serialization\ndata is less-tightly coupled to the game object’s implementation, but again,\nthe world editor either needs to link with runtime game object code in order\nto gain access to the classes’ SerializeIn() andSerializeOut() func-\ntions, or it needs access to the classes’ reflection information in some way.\nThe coupling between the game world editor and the runtimeengine code\ncan be broken by abstracting the descriptions of our game objects in an imple-\nmentation-independent way. For each game object in a world chunk data\nfile, we store a little block of data, often called a spawner. A spawner is a\nlightweight, data-only representation of a game object that can be used to\ninstantiate and initialize that game object at runtime. It contains the id of\nthe game object’s tool-side type. It also contains a table of simple key-value\npairs that describe the initial attributes of the game object. These attributes\n1066 16. Runtime Gameplay Foundation Systems\noftenincludeamodel-to-worldtransform,sincemostgameobjectshaveadis-\ntinct position, orientation and scale in world space. When the game object is\nspawned, the appropriate class or classes are instantiated, as determined by\nthe spawner’s type. These runtime objects can then consult the dictionary of\nkey-value pairs in order to initialize their data members appropriately.\nA spawner can be configured to spawn its game object immediately upon\nbeing loaded, or it can lie dormant until asked to spawn at some later time\nduring the game. Spawners can be implemented as first-class objects, so they\ncan have a convenient functional interface and can store useful metadata in\naddition to object attributes. A spawner can even be used for purposes other\nthan spawning game objects. For example, in the Naughty Dog engine, de-\nsigners used spawners to define important points or coordinate axes in the\ngame world. These were called position spawners orlocator spawners. Locators\nhave many uses in a game, such as:\n• defining points of interest for an AI character,\n• definingasetofcoordinateaxesrelativetowhichasetofanimationscan\nbe played in perfect synchronization,\n• definingthelocationatwhichaparticleeffectoraudioeffectshouldorig-\ninate,\n• defining waypoints along a race track,\nand the list goes on.\n16.3.3.1 Object Type Schemas\nA game object’s attributes and behaviors are defined by its type. In a game\nworld editor that employs a spawner-based design, a game object type can\nbe represented by a data-driven schemathat defines the collection of attributes\nthatshouldbevisibletotheuserwhencreatingoreditinganobjectofthattype.\nAt runtime, the tool-side object type can be mapped in either a hard-coded or\ndata-driven way to a class or collection of classes that must be instantiated in\norder to spawn a game object of the given type.\nType schemas can be stored in a simple text file for consumption by the\nworldeditorandforinspectionandeditingbyitsusers. Forexample,aschema\nfile might look something like this:\nenum LightType\n{\nAmbient, Directional, Point, Spot\n}\n16.3. World Chunk Data Formats 1067\ntype Light\n{\nString UniqueId;\nLightType Type;\nVector Pos;\nQuaternion Rot;\nFloat Intensity : min(0.0), max(1.0);\nColorARGB DiffuseColor;\nColorARGB SpecularColor;\n// ...\n}\ntype Vehicle\n{\nString UniqueId;\nVector Pos;\nQuaternion Rot;\nMeshReference Mesh;\nInt NumWheels : min(2), max(4);\nFloat TurnRadius;\nFloat TopSpeed : min(0.0);\n// ...\n}\n//...\nThe above example brings a few important details to light. You’ll no-\ntice that the data types of each attribute are defined, in addition to their\nnames. These can be simple types like strings, integers and floating-\npoint values, or they can be specialized types like vectors, quaternions,\nARGB colors, or references to special asset types like meshes, collision\ndata and so on. In this example, we’ve even provided a mechanism for\ndefining enumerated types, like LightType . Another subtle point is that\nthe object type schema provides additional information to the world edi-\ntor, such as what type of GUI element to use when editing the attribute.\nSometimes an attribute’s GUI requirements are implied by its data type—\nstrings are generally edited with a text field, Booleans via a check box,\nvectors via three text fields for the x-,y- and z-coordinates or perhaps\nvia a specialized GUI element designed for manipulating vectors in 3D.\nThe schema can also specify meta-information for use by the GUI, such\nas minimum and maximum allowable values for integer and floating-point\nattributes, lists of available choices for drop-down combo boxes and so\non.\nSome game engines permit object type schemas to be inherited, much like\n1068 16. Runtime Gameplay Foundation Systems\nclasses. For example, every game object needs to know its typeand must have\naunique id so that it can be distinguished from all the other game objects at\nruntime. Theseattributescouldbespecifiedinatop-levelschema,fromwhich\nall other schemas are derived.\n16.3.3.2 Default Attribute Values\nAs you can well imagine, the number of attributes in a typical game object\nschema can grow quite large. This translates into a lot of data that must be\nspecified by the game designer for each instance of each game object type he\nor she places into the game world. It can be extremely helpful to define default\nvaluesintheschemaformanyoftheattributes. Thispermitsgamedesignersto\nplace“vanilla”instancesofagameobjecttypewithlittleeffortbutstillpermits\nhim or her to fine-tune the attribute values on specific instances as needed.\nOneinherentproblemwithdefaultvaluesariseswhenthedefaultvalueof\na particular attribute changes. For example, our game designers might have\noriginally wanted Orcs to have 20 hit points. After many months of produc-\ntion, the designers might decide that Orcs should be more powerful and have\n30 hit points by default. Any new Orcs placed into a game world will now\nhave 30 hit points unless otherwise specified. But what about all the Orcs that\nwere placed into game world chunks prior to the change? Do we need to find\nall of these previously created Orcs and manually change their hit points to\n30?\nIdeally, we’d like to design our spawner system so that changes in default\nvalues automatically propagate to all preexisting instances that have not had\ntheirdefaultvaluesoverriddenexplicitly. Oneeasywaytoimplementthisfea-\ntureistosimplyomitkey-valuepairsforattributeswhosevaluedoesnotdiffer\nfrom the default value. Whenever an attribute is missing from the spawner,\nthe appropriate default can be used. (This presumes that the game engine has\naccess to the object type schema file, so that it can read in the attributes’ de-\nfault values. Either that or the tool can do it—in which case, propagating new\ndefault values requires a simple rebuild of all world chunk(s) affected by the\nchange.) In our example, most of the preexisting Orc spawners would have\nhadno HitPoints key-valuepairatall(unlessofcourseoneofthespawner’s\nhit points had been changed from the default value manually). So when the\ndefaultvaluechangesfrom20to30, theseOrcswillautomaticallyusethenew\nvalue.\nSomeenginesallowdefaultvaluestobeoverriddeninderivedobjecttypes.\nFor example, the schema for a type called Vehicle might define a default\nTopSpeed of 80 miles per hour. A derived Motorcycle type schema could\noverride this TopSpeed to be 100 miles per hour.",17017
110-16.4 Loading and Streaming Game Worlds.pdf,110-16.4 Loading and Streaming Game Worlds,"16.4. Loading and Streaming Game Worlds 1069\n16.3.3.3 Some Beneifts of Spawners and Type Schemas\nThe key benefits of separating the spawner from the implementation of the\ngame object are simplicity, flexibility androbustness . From a data management\npoint of view, it is much simpler to deal with a table of key-value pairs than it\nis to manage a binary object image with pointer fix-ups or a custom serialized\nobject format. The key-value pairs approach also makes the data format ex-\ntremely flexible and robust to changes. If a game object encounters key-value\npairs that it is not expecting to see, it can simply ignore them. Likewise, if the\ngame object is unable to find a key-value pair that it needs, it has the option\nof using a default value instead. This makes a key-value pair data format ex-\ntremely robust to changes made by both the designers and the programmers.\nSpawners also simplify the design and implementation of the game world\neditor, because it only needs to know how to manage lists of key-value pairs\nand object type schemas. It doesn’t need to share code with the runtime game\nengine in any way, and it is only very loosely coupled to the engine’s imple-\nmentation details.\nSpawners and archetypes give game designers and programmers a great\ndeal of flexibility and power. Designers can define new game object type\nschemas within the world editor with little or no programmer intervention.\nThe programmer can implement the runtime implementation of these new\nobject types whenever his or her schedule allows it. The programmer does\nnot need to immediately provide an implementation of each new object type\nas it is added in order to avoid breaking the game. New object data can exist\nsafelyintheworldchunkfileswithorwithoutaruntimeimplementation,and\nruntime implementations can exist with or without corresponding data in the\nworld chunk file.\n16.4 Loading and Streaming Game Worlds\nTo bridge the gap between the offline world editor and our runtime game ob-\njectmodel,weneedawaytoloadworldchunksintomemoryandunloadthem\nwhen they are no longer needed. The game world loading system has two\nmain responsibilities: to manage the file I/O necessary to load game world\nchunks and other needed assets from disk into memory and to manage the\nallocation and deallocation of memory for these resources. The engine also\nneeds to manage the spawning anddestruction of game objects as they come\nand go in the game, both in terms of allocating and deallocating memory\nfor the objects and ensuring that the proper classes are instantiated for each\ngame object. In the following sections, we’ll investigate how game worlds\n1070 16. Runtime Gameplay Foundation Systems\nare loaded and also have a look at how object spawning systems typically\nwork.\n16.4.1 Simple Level Loading\nThemost straightforwardgame world loadingapproach, andthe one used by\nalloftheearliestgames,istoallowoneandonlyonegameworldchunk(a.k.a.\nlevel)tobeloadedatatime. Whenthegameisfirststarted,andbetweenpairs\nof levels, the player sees a static or simply animated two-dimensional loading\nscreen while he or she waits for the level to load.\nMemory management in this kind of design is quite straightforward. As\nwe mentioned in Section 7.2.2.7, a stack-based allocator is very well-suited to\na one-level-at-a-time world loading design. When the game first runs, any re-\nsource data that is required across all game levels is loaded at the bottom of\nthe stack. We’ll call these load and stay resident assets (LSR) for the purposes\nof this discussion. The location of the stack pointer is recorded after the LSR\nassets have been fully loaded. Each game world chunk, along with its asso-\nciated mesh, texture, audio, animation and other resource data, is loaded on\ntop of the LSR assets on the stack. When the level has been completed by the\nplayer, all of its memory can be freed by simply resetting the stack pointer to\nthe top of the LSR asset block. At this point, a new level can be loaded in its\nplace. This is illustrated in Figure 16.11.\nWhile this design is very simple, it has a number of drawbacks. For one\nthing,theplayeronlyseesthegameworldindiscretechunks—thereisnoway\nto implement a vast, contiguous, seamless world using this technique. An-\notherproblemisthatduringthetimethelevel’sresourcedataisbeingloaded,\nthere is no game world in memory. So, the player is forced to watch a two-\ndimensional loading screen of some sort.\n16.4.2 Toward Seamless Loading: Air Locks\nThe best way to avoid boring level-loading screens is to permit the player to\ncontinue playing the game while the next world chunk and its associated re-\nsource data are being loaded. One simple approach would be to divide the\nmemory that we’ve set aside for game world assets into two equally sized\nblocks. WecouldloadlevelAintoonememoryblock,allowtheplayertostart\nplaying level A and then load level B into the other block using a streaming\nfile I/O library (i.e., the loading code would run in a separate thread). The big\nproblem with this technique is that it cuts the size of each level in half relative\nto what would be possible with a one-level-at-a-time approach.\n16.4. Loading and Streaming Game Worlds 1071\nLoad LSR data, then obtain marker.\nLoad-and-\nstay-resident\n(LSR) data\nLoad level A.\nLSR dataLevel A’s\nresources\nUnload level A, free back to marker.\nLSR data\nLoad level B.\nLSR dataLevel B’s\nresources\nFigure 16.11. A stack-based memory allocator is extremely well-suited to a one-level-at-a-time\nworld loading system.\nWe can achieve a similar effect by dividing the game world memory into\ntwounequallysizedblocks—alargeblockthatcancontaina“full”gameworld\nchunk and a small block that is only large enough to contain a tiny world\nchunk. The small chunk is sometimes known as an “air lock.”\nWhen the game starts, a “full” chunk and an “air lock” chunk are loaded.\nThe player progresses through the full chunk and into the air lock, at which\npoint some kind of gate or other impediment ensures that the player can nei-\nther see the previous full world area nor return to it. The full chunk can then\nbe un-loaded, and a new full-sized world chunk can be loaded. During the\nload, the player is kept busy doing some task within the air lock. The task\nmight be as simple as walking from one end of a hallway to the other, or it\ncould be something more engaging, like solving a puzzle or fighting some\nenemies.\n1072 16. Runtime Gameplay Foundation Systems\nAsynchronous file I/O is what enables the full world chunk to be loaded\nwhile the player is simultaneously playing in the air lock region. See Section\n7.1.3formoredetails. It’simportanttonotethatanairlocksystemdoes notfree\nus from displaying a loading screen whenever a new game is started, because\nduring the initial load there is no game world in memory in which to play.\nHowever,oncetheplayerisinthegameworld,heorsheneedn’tseealoading\nscreen ever again, thanks to air locks and asynchronous data loading.\nHalofor the Xbox used a technique similar to this. The large world areas\nwere invariably connected by smaller, more confined areas. As you play Halo,\nwatchforconfined areasthatpreventyoufromback-tracking—you’llfindone\nroughly every 5–10 minutes of gameplay. Jak 2for the PlayStation 2 used the\nair lock technique as well. The game world was structured as a hub area (the\nmain city) with a number of offshoot areas, each of which was connected to\nthe hub via a small, confined air lock region.\n16.4.3 Game World Streaming\nMany game designs call for the player to feel like he or she is playing in a\nhuge, contiguous, seamless world. Ideally, the player should not be confined\nto small air lock regions periodically—it would be best if the world simply\nunfolded in front of the player as naturally and believably as possible.\nModerngameenginessupportthiskindofseamlessworldbyusingatech-\nnique known as streaming. World streaming can be accomplished in various\nways. The main goals are always (a) to load data while the player is engaged\nin regular gameplay tasks and (b) to manage the memory in such a way as to\neliminate fragmentation while permitting data to be loaded and unloaded as\nneeded as the player progresses through the game world.\nRecent consoles and PCs have a lot more memory than their predecessors,\nsoitisnowpossibletokeepmultipleworldchunksinmemorysimultaneously.\nWe could imagine dividing our memory space into, say, three equally sized\nbuffers. At first, we load world chunks A, B and C into these three buffers\nand allow the player to start playing through chunk A. When he or she enters\nchunk B and is far enough along that chunk A can no longer be seen, we can\nunload chunk A and start loading a new chunk D into the first buffer. When\nB can no longer be seen, it can be dumped and chunk E loaded. This recycling\nof buffers can continue until the player has reached the end of the contiguous\ngame world.\nThe problem with a coarse-grained approach to world streaming is that it\nplaces onerous restrictions on the size of a world chunk. All chunks in the en-\ntire game must be roughly the same size—large enough to fill up the majority\nof one of our three memory buffers but never any larger.\n16.4. Loading and Streaming Game Worlds 1073\nOne way around this problem is to employ a much finer-grained subdivi-\nsion of memory. Rather than streaming relatively large chunks of the world,\nwe can divide every game asset, from game world chunks to foreground\nmeshes to textures to animation banks, into equally sized blocks of data. We\ncan then use a chunky, pool-based memory allocation system like the one de-\nscribed in Section 7.2.2.7 to load and unload resource data as needed without\nhaving to worry about memory fragmentation. This is essentially the tech-\nnique employed by Naughty Dog’s engine. (Although Naughty Dog’s imple-\nmentationalsoemployssomesophisticatedtechniquesformakinguseofwhat\nwould otherwise be unused space at the ends of under-full chunks.)\n16.4.3.1 Determining Which Resources to Load\nOne question that arises when using a fine-grained chunky memory allocator\nforworldstreamingishowtheenginewillknowwhatresourcestoloadatany\ngiven moment during gameplay. In the Naughty Dog engine, we use a rela-\ntively simple system of level load regions to control the loading and unloading\nof assets.\nAllofthe Uncharted andTheLastofUs gamesaresetinmultiple,geograph-\nically distinct, contiguous game worlds. For example, Uncharted: Drake’s For-\ntunetakes place in a jungle and on an island. Each of these worlds exists in\na single, consistent world space, but they are divided up into numerous geo-\ngraphically adjacent chunks. A simple convex volume known as a regionen-\ncompasseseachofthechunks;theregionsoverlapeachothersomewhat. Each\nregion contains a list of the world chunks that should be in memory when the\nplayer is in that region.\nAt any given moment, the player is within one or more of these regions.\nTo determine the set of world chunks that should be in memory, we simply\ntaketheunionofthechunklistsfromeachoftheregionsenclosingtheNathan\nDrake character. The level loading system periodically checks this master\nchunk list and compares it against the set of world chunks that are currently\nin memory. If a chunk disappears from the master list, it is unloaded, thereby\nfreeing up all of the allocation blocks it occupied. If a new chunk appears in\nthe list, it is loaded into any free allocation blocks that can be found. The level\nload regions and world chunks are designed in such a way as to ensure that\nthe player never sees a chunk disappear when it is unloaded and that there’s\nenough time between the moment at which a chunk starts loading and the\nmomentitscontentsarefirstseenbytheplayertopermitthechunktobefully\nstreamed into memory. This technique is illustrated in Figure 16.12.\n1074 16. Runtime Gameplay Foundation Systems\n1 234\nLevel 1\nLevel 2\nLevel 2\nLevel 3\nLevel 3\nLevel 4\nFigure 16.12. A game world divided into chunks. Level load regions, each with a requested chunk\nlist, are arranged in such a way as to guarantee that the player never sees a chunk pop in or out\nof view.\n16.4.3.2 PlayGo on the PlayStation 4\nThePlayStation4includesanewfeaturecalledPlayGothatmakestheprocess\nof downloading a game (as opposed to buying it on Blu-ray) a lot less painful\nthan it has traditionally been. PlayGo works by downloading only the mini-\nmumsubsetofdatarequiredinordertoplaythefirstsectionofthegame. The\nPS4 downloads the rest of the game’s content in the background, while the\nplayer continues to experience the game without interruption. In order for\nthis to work well, the game must of course support seamless level streaming,\nas we’ve described above.\n16.4.4 Memory Management for Object Spawning\nOnce a game world has been loaded into memory, we need to manage the\nprocess of spawning the dynamic game objects in the world. Most game en-\ngines have some kind of game object spawning system that manages the in-\nstantiation of the class or classes that make up each game object and handles\ndestruction of game objects when they are no longer needed. One of the cen-\ntral jobs of any object spawning system is to manage the dynamic allocation\nof memory for newly spawned game objects. Dynamic allocation can be slow,\nso steps must be taken to ensure allocations are as efficient as possible. And\nbecause game objects come in a wide variety of sizes, dynamically allocating\nthem can cause memory to become fragmented, leading to premature out-of-\nmemory conditions. There are a number of different approaches to game ob-\nject memory management. We’ll explore a few common ones in the following\nsections.\n16.4.4.1 OffLine Memory Allocation for Object Spawning\nSome game engines solve the problems of allocation speed and memory frag-\nmentation in a rather draconian way, by simply disallowing dynamic mem-\nory allocation during gameplay altogether. Such engines permit game world\n16.4. Loading and Streaming Game Worlds 1075\nchunkstobeloadedandunloadeddynamically,buttheyspawninalldynamic\ngame objects immediately upon loading a chunk. Thereafter, no game objects\ncanbecreatedordestroyed. Youcanthinkofthistechniqueasobeyinga“law\nof conservation of game objects.” No game objects are created or destroyed\nonce a world chunk has been loaded.\nThis technique avoids memory fragmentation because the memory re-\nquirements of all the game objects in a world chunk are (a) known a priori\nand (b) bounded. This means that the memory for the game objects can be\nallocated offline by the world editor and included as part of the world chunk\ndata itself. All game objects are therefore allocated out of the same memory\nused to load the game world and its resources, and they are no more prone\nto fragmentation than any other loaded resource data. This approach also has\nthe benefit of making the game’s memory usage patterns highly predictable.\nThere’snochancethatalargegroupofgameobjectsisgoingtospawnintothe\nworld unexpectedly, and cause the game to run out of memory.\nOn the downside, this approach can be quite limiting for game designers.\nDynamic object spawning can be simulated by allocating a game object in the\nworld editor but instructing it to be invisible and dormant when the world\nis first loaded. Later, the object can “spawn” by simply activating itself and\nmaking itself visible. But the game designers have to predict the total number\nof game objects of each type that they’ll need when the game world is first\ncreated in the world editor. If they want to provide the player with an infinite\nsupply of health packs, weapons, enemies or some other kind of game object,\ntheyeitherneedtoworkoutawaytorecycletheirgameobjects,orthey’reout\nof luck.\n16.4.4.2 Dynamic Memory Management for Object Spawning\nGame designers would probably prefer to work with a game engine that sup-\nports true dynamic object spawning. Although this is more difficult to imple-\nment than a static game object spawning approach, it can be implemented in\na number of different ways.\nAgain, the primary problem is memory fragmentation. Because different\ntypes of game objects (and sometimes even different instances of the same\ntype of object) occupy different amounts of memory, we cannot use our fa-\nvorite fragmentation-free allocator—the pool allocator. And because game\nobjects are generally destroyed in a different order than that in which they\nwere spawned, we cannot use a stack-based allocator either. Our only choice\nappears to be a fragmentation-prone heap allocator. Thankfully, there are\nmany ways to deal with the fragmentation problem. We’ll investigate a few\ncommon ones in the following sections.\n1076 16. Runtime Gameplay Foundation Systems\nOne Memory Pool per Object Type\nIf the individual instances of each game object type are all guaranteed to oc-\ncupy the same amount of memory, we could consider using a separate mem-\norypoolforeachobjecttype. Actually, weonlyneedonepoolper uniquegame\nobject size, so object types of the same size can share a single pool.\nDoing this allows us to completely avoid memory fragmentation, but one\nlimitation of this approach is that we need to maintain lots of separate pools.\nWealsoneedtomakeeducatedguessesabouthowmanyofeachtypeofobject\nwe’ll need. If a pool has too many elements, we end up wasting memory; if it\nhas too few, we won’t be able to satisfy all of the spawn requests at runtime,\nand game objects will fail to spawn.\nSmall Memory Allocators\nWe can transform the idea of one pool per game object type into something\nmore workable by allowing a game object to be allocated out of a pool whose\nelementsarelargerthantheobjectitself. Thiscanreducethenumberofunique\nmemory pools we need significantly, at the cost of some potentially wasted\nmemory in each pool.\nFor example, we might create a set of pool allocators, each one with ele-\nments that are twice as large as those of its predecessor—perhaps 8, 16, 32,\n64, 128, 256 and 512 bytes. We can also use a sequence of element sizes that\nconforms to some other suitable pattern or base the list of sizes on allocation\nstatistics collected from the running game.\nWhenever we try to allocate a game object, we search for the smallest pool\nwhose elements are larger than or equal to the size of the object we’re allocat-\ning. We accept that for some objects, we’ll be wasting space. In return, we\nalleviate all of our memory fragmentation problems—a reasonably fair trade.\nIfweeverencounteramemoryallocationrequestthatislargerthanourlargest\npool, we can always turn it over to the general-purpose heap allocator, know-\ning that fragmentation of large memory blocks is not nearly as problematic as\nfragmentation involving tiny blocks.\nThis type of allocator is sometimes called a small memory allocator . It can\neliminate fragmentation (for allocations that fit into one of the pools). It\ncan also speed up memory allocations significantly for small chunks of data,\nbecauseapoolallocationinvolvestwopointermanipulationstoremovetheel-\nement from the linked list of free elements—a much less-expensive operation\nthan a general-purpose heap allocation.\n16.4. Loading and Streaming Game Worlds 1077\nMemory Relocation\nAnotherwaytoeliminatefragmentationistoattacktheproblemdirectly. This\napproachisknownas memoryrelocation . Itinvolvesshiftingallocatedmemory\nblocks down into adjacent free holes to remove fragmentation. Moving the\nmemory is easy, but because we are moving “live” allocated objects, we need\nto be very careful about fixing up any pointers into the memory blocks we\nmove. See Section 6.2.2.2 for more details.\n16.4.5 Saved Games\nMany games allow the player to save his or her progress, quit the game and\nthen load up the game at a later time in exactly the state he or she left it. A\nsaved game system is similar to the world chunk loading system in that it is\ncapableofloadingthestateofthegameworldfromadiskfileormemorycard.\nBut the requirements of this system differ somewhat from those of a world\nloading system, so the two are usually distinct (or overlap only partially).\nTo understand the differences between the requirements of these two sys-\ntems, let’s briefly compare world chunks to saved game files. World chunks\nspecify the initial conditions of all dynamic objects in the world, but they also\ncontain a full description of all static world elements. Much of the static in-\nformation, such as background meshes and collision data, tends to take up a\nlot of disk space. As such, world chunks are sometimes comprised of multi-\nple disk files, and the total amount of data associated with a world chunk is\nusually large.\nAsavedgamefilemustalsostorethecurrentstateinformationofthegame\nobjects in the world. However, it does not need to store a duplicate copy of\nanyinformationthatcanbedeterminedbyreadingtheworldchunkdata. For\nexample, there’s no need to save out the static geometry in a saved game file.\nA saved game need not store every detail of every object’s state either. Some\nobjects that have no impact on gameplay can be omitted altogether. For the\nother game objects, we may only need to store partial state information. As\nlong as the player can’t tell the difference between the state of the game world\nbefore and after it has been saved and reloaded (or if the differences are irrel-\nevant to the player), then we have a successful saved game system. As such,\nsaved game files tend to be much smaller than world chunk files and may\nplacemoreofanemphasisondata compressionand omission. Smallfile sizes\nare especially important when numerous saved game files must fit onto the\ntiny memory cards that were used on older consoles. But even today, with\nconsoles that are equippedwith large hard drives and linked to a cloud save\nsystem, it’s still a good idea to keep the size of a saved game file as small as\n1078 16. Runtime Gameplay Foundation Systems\npossible.\n16.4.5.1 Checkpoints\nOne approach to save games is to limit saves to specific points in the game,\nknown as checkpoints . The benefit of this approach is that most of the knowl-\nedge about the state of the game is saved in the current world chunk(s) in the\nvicinity of each checkpoint. This data is always exactly the same, no matter\nwhich player is playing the game, so it needn’t be stored in the saved game.\nAsaresult,savedgamefilesbasedoncheckpointscanbeextremelysmall. We\nmightneedtostoreonlythenameofthelastcheckpointreached,plusperhaps\nsome information about the current state of the player character, such as the\nplayer’s health, number of lives remaining, what items he has in his inven-\ntory, which weapon(s) he has and how much ammo each one contains. Some\ngames based on checkpoints don’t even store this information—they start the\nplayer off in a known state at each checkpoint. Of course, the downside of a\ngame based on checkpoints is the possibility of user frustration, especially if\ncheckpoints are few and far between.\n16.4.5.2 Save Anywhere\nSome games support a feature known as save anywhere. As the name implies,\nsuch games permit the state of the game to be saved at literally any point dur-\ning play. To implement this feature, the size of the saved game data file must\nincrease significantly. The current locations and internal states of every game\nobject whose state is relevant to gameplay must be saved and then restored\nwhen the game is loaded again later.\nIn a save-anywhere design, a saved game data file contains basically the\nsame information as a world chunk, minus the world’s static components. It\nis possible to utilize the same data format for both systems, although there\nmay be factors that make this infeasible. For example, the world chunk data\nformat might be designed for flexibility, but the saved game format might be\ncompressed to minimize the size of each saved game.\nAs we’ve mentioned, one way to reduce the amount of data that needs to\nbe stored in a saved game file is to omit certain irrelevant game objects and\nto omit some irrelevant details of others. For example, we needn’t remember\nthe exact time index within every animation that is currently playing or the\nexactmomentumsandvelocitiesofeveryphysicallysimulatedrigidbody. We\ncan rely on the imperfect memories of human gamers and save only a rough\napproximation to the game’s state.",24529
111-16.5 Object References and World Queries.pdf,111-16.5 Object References and World Queries,"16.5. Object References and World Queries 1079\n16.5 Object References and World Queries\nEvery game object generally requires some kind of unique id so that it can be\ndistinguished from the other objects in the game, found at runtime, serve as a\ntarget of inter-object communication and so on. Unique object ids are equally\nhelpful on the tool side, as they can be used to identify and find game objects\nwithin the world editor.\nAt runtime, we invariably need various ways to find game objects. We\nmight want to find an object by its unique id, by its type, or by a set of arbi-\ntrary criteria. We often need to perform proximity-based queries, for example\nfinding all enemy aliens within a 10 m radius of the player character.\nOnceagameobjecthasbeenfoundviaaquery, weneedsomewaytorefer\nto it. In a language like C or C++, object references might be implemented as\npointers,orwemightusesomethingmoresophisticated,likehandlesorsmart\npointers. Thelifetimeofanobjectreferencecanvarywidely, fromthescopeof\na single function call to a period of many minutes. In the following sections,\nwe’llfirstinvestigatevariouswaystoimplementobjectreferences. Thenwe’ll\nexplore the kinds of queries we often require when implementing gameplay\nand how those queries might be implemented.\n16.5.1 Pointers\nIn C or C++, the most straightforward way to implement an object reference\nis via a pointer (or a reference in C++). Pointers are powerful and are just\nabout as simple and intuitive as you can get. However, pointers suffer from a\nnumber of problems:\n•Orphaned objects. Ideally, every object should have an owner—another\nobject that is responsible for managing its lifetime—creating it and then\ndeleting it when it is no longer needed. But pointers don’t give the pro-\ngrammer any help in enforcing this rule. The result can be an orphaned\nobject—an object that still occupies memory but is no longer needed or\nreferenced by any other object in the system.\n•Stale pointers. If an object is deleted, ideally we should null-out any and\nall pointers to that object. If we forget to do so, however, we end up\nwith a stale pointer—a pointer to a block of memory that used to be oc-\ncupied by a valid object but is now free memory. If anyone tries to read\norwritedatathroughastalepointer,theresultcanbeacrashorincorrect\nprogram behavior. Stale pointers can be difficult to track down because\nthey may continue to work for some time after the object has deleted.\nOnlymuchlater, whenanewobjectisallocatedontopofthestalemem-\nory block, does the data actually change and cause a crash.\n1080 16. Runtime Gameplay Foundation Systems\n•Invalid pointers . A programmer is free to store any address in a pointer,\nincludingatotallyinvalidaddress. Acommonproblemisdereferencing\na null pointer. These problems can be guarded against by using asser-\ntion macros to check that pointers are never null prior to dereferencing\nthem. Evenworse, ifapieceofdataismisinterpretedasapointer, deref-\nerencingitcancausetheprogramtoreadorwriteanessentiallyrandom\nmemory address. This usually results in a crash or other major problem\nthat can be very tough to debug.\nMany game engines make heavy use of pointers, because they are by far\nthe fastest, most efficient and easiest-to-work-with way to implement object\nreferences. However, experienced programmers are always wary of pointers,\nand some game teams turn to more sophisticated kinds of object references,\neither out of a desire to use safer programming practices or out of necessity.\nFor example, if a game engine relocates allocated data blocks at runtime to\neliminate memory fragmentation (see Section 6.2.2.2), simple pointers cannot\nbeused. Weeitherneedtouseatypeofobjectreferencethatisrobusttomem-\noryrelocation,orweneedtomanuallyfixupanypointersintoeveryrelocated\nmemory block at the time it is moved.\n16.5.2 Smart Pointers\nAsmartpointer isasmallobjectthatactslikeapointerformostintentsandpur-\nposes but avoids most of the problems inherent with native C/C++ pointers.\nAtitssimplest,asmartpointercontainsanativepointerasadatamemberand\nprovides a set of overloaded operators that make it act like a pointer in most\nways. Pointers can be dereferenced, so the *and->operators are overloaded\nto return a reference and a pointer to the referenced object, respectively, as\nyou’d expect. Pointers can undergo arithmetic operations, so the +,-,++and\n--operators are also overloaded appropriately.\nBecause a smart pointer is an object, it can contain additional metadata\nand/ortakeadditionalstepsnotpossiblewitharegularpointer. Forexample,\nasmartpointermightcontaininformationthatallowsittorecognizewhenthe\nobject to which it points has been deleted and start returning a null address if\nso.\nSmart pointers can also help with object lifetime management by cooper-\nating with one another to determine the number of references to a particular\nobject. This is called reference counting . When the number of smart pointers\nthat reference a particular object drops to zero, we know that the object is\nno longer needed, so it can be automatically deleted. This can free the pro-\ngrammerfromhavingtoworryaboutobjectownershipandorphanedobjects.\n16.5. Object References and World Queries 1081\nReferencecountingusuallyalsoliesatthecoreofthe“garbagecollection”sys-\ntems found in modern programming languages like Java and Python.\nSmart pointers have their share of problems. For one thing, they are rel-\natively easy to implement, but they are tough to get right. There are a great\nmany cases to handle, and the std::auto_ptr class provided by the origi-\nnal C++ standard library is widely recognized to be inadequate in many sit-\nuations. Thankfully most of these issues were resolved in C++11 with the in-\ntroduction of three smart pointer classes: std::shared_ptr ,std::weak_\nptrandstd::unique_ptr.\nTheC++11smartpointerclassesweremodeledaftertherichsmartpointer\nfacilities provided by the Boost C++ template library. It defines six different\nvarieties of smart pointers:\n•boost::scoped_ptr. A pointer to a single object with one owner.\n•boost::scoped_array . A pointer to an array of objects with a single\nowner.\n•boost::shared_ptr. A pointer to an object whose lifetime is shared\nby multiple owners.\n•boost::shared_array . A pointer to an array of objects whose life-\ntimes are shared by multiple owners.\n•boost::weak_ptr . A pointer that does not own or automatically de-\nstroy the object it references (whose lifetime is assumed to be managed\nby ashared_ptr ).\n•boost::intrusive_ptr . A pointer that implements reference count-\ning by assuming that the pointed-to object will maintain the reference\ncount itself. Intrusive pointers are useful because they are the same\nsize as a native C++ pointer (because no reference-counting apparatus\nis required) and because they can be constructed directly from native\npointers.\nProperly implementing a smart pointer class can be a daunting task. Have\na glance at the Boost smart pointer documentation (http://www.boost.org/\ndoc/libs/1_36_0/libs/smart_ptr/smart_ptr.htm) to see what I mean. All\nsorts of issues come up, including:\n• type safety of smart pointers,\n• the ability for a smart pointer to be used with an incomplete type,\n• correct smart pointer behavior when an exception occurs, and\n• runtime costs, which can be high.\n1082 16. Runtime Gameplay Foundation Systems\nNULL\nNULLObject1\nObject2\nObject3\nObject4\nObject5Handle Table\nm_handleIndex == 60\n1\n23\n4\n56Handle to Object 5\nFigure 16.13. A handle table contains raw object pointers. A handle is simply an index into this table.\nIworkedonaprojectthatattemptedtoimplementitsownsmartpointers,and\nwe were fixing all sorts of nasty bugs with them up until the very end of the\nproject. My personal recommendation is to stay away from smart pointers; if\nyou must use them, use a mature implementation such as the C++11 standard\nlibrary or Boost, rather than rolling your own.\n16.5.3 Handles\nAhandleactslikeasmartpointerinmanyways, butitissimplertoimplement\nand tends to be less prone to problems. A handle is basically an integer index\ninto a global handle table. The handle table, in turn, contains pointers to the\nobjects to which the handles refer. To create a handle, we simply search the\nhandle table for the address of the object in question and store its index in the\nhandle. To dereference a handle, the calling code simply indexes the appro-\npriate slot in the handle table and dereferences the pointer it finds there. This\nis illustrated in Figure 16.13.\nBecauseofthesimplelevelofindirectionaffordedbythehandletable,han-\ndlesaremuchsaferandmoreflexiblethanrawpointers. Ifanobjectisdeleted,\nitcansimplynulloutitsentryinthehandletable. Thiscausesallexistinghan-\ndles to the object to be immediately and automatically converted to null refer-\nences. Handles also support memory relocation. When an object is relocated\nin memory, its address can be found in the handle table and updated appro-\npriately. Again, all existing handles to the object are automatically updated as\na result.\nA handle can be implemented as a raw integer. However, the handle table\nindex is usually wrapped in a simple class so that a convenient interface for\ncreating and dereferencing the handle can be provided.\n16.5. Object References and World Queries 1083\nHandles are prone to the possibility of referencing a stale object. For ex-\nample, let’s say we create a handle to object A, which occupies slot 17 in the\nhandle table. Later, object A is deleted, and slot 17 is nulled out. Later still,\na new object B is created, and it just happens to occupy slot 17 in the handle\ntable. If there are still any handles to object A lying around when object B is\ncreated, they will suddenly start referring to object B (instead of null). This is\ncertainly not desirable behavior.\nOne simple solution to the stale handle problem is to include a unique ob-\nject id in each handle. That way, when a handle to object A is created, it con-\ntainsnotonlyslotindex17,buttheobjectid“A.”WhenobjectBtakesA’splace\nin the handle table, any leftover handles to A will agree on the handle index\nbut disagree on the object id. This allows stale object A handles to continue\nto return null when dereferenced rather than returning a pointer to object B\nunexpectedly.\nThe following code snippet shows how a simple handle class might be im-\nplemented. Notice that we’ve also included the handle index in the Game-\nObject class itself—this allows us to create new handles to a GameObject\nvery quickly without having to search the handle table for its address to de-\ntermine its handle index.\n// Within the GameObject class, we store a unique id,\n// and also the object's handle index, for efficient\n// creation of new handles.\nclass GameObject\n{\nprivate:\n// ...\nGameObjectId m_uniqueId; // object's unique id\nU32 m_handleIndex ; // speedier handle\n// creation\nfriend class GameObjectHandle; // access to id and\n// index\n// ...\npublic:\nGameObject() // constructor\n{\n// The unique id might come from the world editor,\n// or it might be assigned dynamically at runtime.\nm_uniqueId = AssignUniqueObjectId();\n1084 16. Runtime Gameplay Foundation Systems\n// The handle index is assigned by finding the\n// first free slot in the handle table.\nm_handleIndex = FindFreeSlotInHandleTable();\n// ...\n}\n// ...\n};\n// This constant defines the size of the handle table,\n// and hence the maximum number of game objects that can\n// exist at any one time.\nstatic const U32 MAX_GAME_OBJECTS = 2048;\n// This is the global handle table -- a simple array of\n// pointers to GameObjects.\nstatic GameObject* g_apGameObject [MAX_GAME_OBJECTS];\n// This is our simple game object handle class.\nclass GameObjectHandle\n{\nprivate:\nU32 m_handleIndex ; // index into the handle\n// table\nGameObjectId m_uniqueId; // unique id avoids stale\n// handles\npublic:\nexplicit GameObjectHandle (GameObject& object) :\nm_handleIndex(object.m_handleIndex),\nm_uniqueId(object.m_uniqueId)\n{\n}\n// This function dereferences the handle.\nGameObject* ToObject() const\n{\nGameObject* pObject\n=g_apGameObject [m_handleIndex];\nif (pObject != nullptr\n&& pObject->m_uniqueId == m_uniqueId)\n{\nreturn pObject;\n}\n16.5. Object References and World Queries 1085\nreturn nullptr;\n}\n};\nThisexampleisfunctionalbutincomplete. Wemightwanttoimplementcopy\nsemantics, provide additional constructor variants and so on. The entries in\nthe global handle table might contain additional information, not just a raw\npointer to each game object. And of course, a fixed size handle table imple-\nmentation like this one isn’t the only possible design; handle systems vary\nsomewhat from engine to engine.\nWe should note that one fortunate side benefit of a global handle table is\nthat it gives us a ready-made list of all active game objects in the system. The\nglobal handle table can be used to quickly and efficiently iterate over all game\nobjects in the world, for example. It can also make implementing other kinds\nof queries easier in some cases.\n16.5.4 Game Object Queries\nEvery game engine provides at least a few ways to find game objects at run-\ntime. We’llcallthesesearches gameobjectqueries . Thesimplesttypeofqueryis\nto find a particular game object by its unique id. However, a real game engine\nmakes many other types of game object queries. Here are just a few examples\nof the kinds of queries a game developer might want to make:\n• Find all enemy characters with line of sight to the player.\n• Iterate over all game objects of a certain type.\n• Find all destructible game objects with more than 80% health.\n• Transmit damage to all game objects within the blast radius of an explo-\nsion.\n• Iterateoverallobjectsinthepathofabulletorotherprojectile,innearest-\nto-farthest order.\nThis list could go on for many pages, and of course its contents are highly\ndependent upon the design of the particular game being made.\nFor maximum flexibility in performing game object queries, we could im-\nagine a general-purpose game object database, complete with the ability to\nformulate arbitrary queries using arbitrary search criteria. Ideally, our game\nobject database would perform all of these queries extremely efficiently and\nrapidly, making maximum use of whatever hardware and software resources\nare available.",14453
112-16.6 Updating Game Objects in Real Time.pdf,112-16.6 Updating Game Objects in Real Time,"1086 16. Runtime Gameplay Foundation Systems\nIn reality, such an ideal combination of flexibility and blinding speed is\ngenerallynotpossible. Instead, gameteamsusuallydeterminewhichtypesof\nqueriesaremostlikelytobeneededduringdevelopmentofthegame,andspe-\ncialized data structures are implemented to accelerate those particular types\nof queries. As new queries become necessary, the engineers either leverage\npreexistingdata structuresto implement them, or they invent new ones if suf-\nficient speed cannot be obtained. Here are a few examples of specialized data\nstructures that can accelerate specific types of game object queries:\n•Findinggameobjectsbyuniqueid. Pointers or handles to the game objects\ncould be stored in a hash table or binary search tree keyed by unique id.\n•Iterating over all objects that meet a particular criterion . The game objects\ncould be presorted into linked lists based on various criteria (presuming\nthecriteriaareknownapriori). Forexample,wemightconstructalistof\nall game objects of a particular type, maintain a list of all objects within\na particular radius of the player, etc.\n•Findingallobjectsinthepathofaprojectileorwithlineofsighttosometarget\npoint. The collision system is usually leveraged to perform these kinds\nof game object queries. Most collision systems provide fast ray casts,\nand some also provide the ability to cast other shapes such as spheres\nor arbitrary convex volumes into the world to determine what they hit.\n(See Section 13.3.7.)\n•Finding all objects within a given region or radius . We might consider stor-\ning our game objects in some kind of spatial hash data structure. This\ncouldbeassimpleasahorizontalgridplacedovertheentiregameworld\nor something more sophisticated, such as a quadtree, octree, kd-tree or\nother data structure that encodes spatial proximity.\n16.6 Updating Game Objects in Real Time\nEvery game engine, from the simplest to the most complex, requires some\nmeans of updating the internal state of every game object over time. The state\nof a game object can be defined as the values of all its attributes (sometimes\ncalled its properties, and called data members in the C++ language). For exam-\nple, the state of the ball in Pongis described by its (x,y)position on the screen\nand its velocity (speed and direction of travel). Because games are dynamic,\ntime-basedsimulations, agameobject’sstatedescribesitsconfigurationat one\nspecificinstantintime. In other words, a game object’s notion of time is discrete\nrather than continuous. (However, as we’ll see, it’s helpful to think of the ob-\n16.6. Updating Game Objects in Real Time 1087\njects’ states as changing continuously and then being sampled discretely by\nthe engine, because it helps you to avoid some common pitfalls.)\nIn the following discussions, we’ll use the symbol Si(t)to denote the state\nof object iat an arbitrary time t. The use of vector notation here is not strictly\nmathematically correct, but it reminds us that a game object’s state acts like\na heterogeneous n-dimensional vector, containing all sorts of information of\nvarious data types. We should note that this usage of the term “state” is not\nthe same as the states in a finite state machine. A game object may very well\nbe implemented in terms of one—or many—finite state machines, but in that\ncase, aspecification of thecurrentstateof eachFSM wouldmerelybe apart of\nthe game object’s overall state vector S(t).\nMost low-level engine subsystems (rendering, animation, collision, phys-\nics, audio and so on) require periodic updating, and the game object system\nis no exception. As we saw in Chapter 8, updating is usually done via a sin-\ngle master loop called the game loop. Virtually all game engines update game\nobject states as part of their main game loop—in other words, they treat the\ngame object model as just another engine subsystem that requires periodic\nservicing.\nGame object updating can therefore be thought of as the process of de-\ntermining the state of each object at the current time Si(t)given its state at a\nprevioustime Si(t ∆t). Onceallobjectstateshavebeenupdated,thecurrent\ntime tbecomes the new previous time ( t ∆t), and this process repeats for as\nlongasthegameisrunning. Usually, oneormore clocksaremaintainedbythe\nengine—one that tracks real time exactly and possibly others that may or may\nnot correspond to real time. These clocks provide the engine with the abso-\nlute time tand/or with the change in time ∆tfrom iteration to iteration of the\ngame loop. The clock that drives the updating of game object states is usually\npermitted to diverge from real time. This allows the behaviors of the game\nobjectstobepaused,sloweddown,speduporevenruninreverse—whatever\nis required in order to suit the needs of the game design. These features are\nalso invaluable for debugging and development of the game.\nAs we learned in Chapter 1, a game object updating system is an exam-\nple of what is known as a dynamic, real-time, agent-based computer simulation in\ncomputer science. Game object updating systems also exhibit some aspects of\ndiscrete event simulations (see Section 16.8 for more details on events). These\nare well-researched areas of computer science, and they have many applica-\ntionsoutsidethefieldofinteractiveentertainment. Gamesareoneofthemore\ncomplex kinds of agent-based simulation—as we’ll see, updating game object\nstates over time in a dynamic, interactive virtual environment can be surpris-\ningly difficult to get right. Game programmers can learn a lot about game\n1088 16. Runtime Gameplay Foundation Systems\nobject updating by studying the wider field of agent-based and discrete event\nsimulations. And researchers in those fields can probably learn a thing or two\nfrom game engine design as well!\nAs with all high-level game engine systems, every engine takes a slightly\n(or sometimes radically) different approach. However, as before, most game\nteams encounter a common set of problems, and certain design patterns tend\nto crop up again and again in virtually every engine. In this section, we’ll\ninvestigate these common problems and some common solutions to them.\nPlease bear in mind that game engines may exist that employ very different\nsolutionstotheonesdescribedhere,andsomegamedesignsfaceuniqueprob-\nlems that we can’t possibly cover here.\n16.6.1 A Simple Approach (That Doesn’t Work)\nThe simplest way to update the states of a collection of game objects is to\niterate over the collection and call a virtual function, named something like\nUpdate(), on each object in turn. This is typically done once during each it-\neration of the main game loop (i.e., once per frame). Game object classes can\nprovide custom implementations of the Update() function in order to per-\nform whatever tasks are required to advance the state of that type of object\nto the next discrete time index. The time delta from the previous frame can\nbe passed to the update function so that objects can take proper account of\nthe passage of time. At its simplest, then, our Update() function’s signature\nmight look something like this:\nvirtual void Update(float dt);\nFor the purposes of the following discussions, we’ll assume that our en-\ngine employs a monolithic object hierarchy, in which each game object is rep-\nresented by a single instance of a single class. However, we can easily extend\nthe ideas here to virtually any object-centric design. For example, to update\na component-based object model, we could call Update() on every compo-\nnentthatmakesupeachgameobject,orwecouldcall Update() onthe“hub”\nobject and let it update its associated components as it sees fit. We can also ex-\ntendtheseideastoproperty-centricdesigns,bycallingsomesortof Update()\nfunction on each property instance every frame.\nThey say that the devil is in the details, so let’s investigate two important\ndetails here. First, how should we maintain the collection of all game objects?\nAnd second, what kinds of things should the Update() function be respon-\nsible for doing?\n16.6. Updating Game Objects in Real Time 1089\n16.6.1.1 Maintaining a Collection of Active Game Objects\nThe collection of active game objects is often maintained by a singleton\nmanager class, perhaps named something like GameWorld orGameObject-\nManager. The collection of game objects generally needs to be dynamic, be-\ncause game objects are spawned and destroyed as the game is played. Hence\nalinked list of pointers, smart pointers or handles to game objects is one sim-\nple and effective approach. (Some game engines disallow dynamic spawning\nanddestroyingofgameobjects; suchenginescanuseastaticallysized arrayof\ngame object pointers, smart pointers or handles rather than a linked list.) As\nwe’ll see below, most engines use more complex data structures to keep track\nof their game objects rather than just a simple, flat linked list. But for the time\nbeing, we can visualize the data structure as a linked list for simplicity.\n16.6.1.2 Responsibilities of the Update() Function\nA game object’s Update() function is primarily responsible for determining\nthe state of that game object at the current discrete time index Si(t)given its\nprevious state Si(t ∆t). Doing this may involve applying a rigid body dy-\nnamics simulation to the object, sampling a preauthored animation, reacting\nto events that have occurred during the current time step and so on.\nMost game objects interact with one or more engine subsystems. They\nmay need to animate, be rendered, emit particle effects, play audio, collide\nwith other objects and static geometry and so on. Each of these systems has\nan internal state that must also be updated over time, usually once or a few\ntimesperframe. Itmightseemreasonableandintuitivetosimplyupdateallof\nthese subsystems directly from within the game object’s Update() function.\nFor example, consider the following hypothetical update function for a Tank\nobject:\nvirtual void Tank::Update(float dt)\n{\n//Update the state of the tank itself.\nMoveTank(dt);\nDeflectTurret(dt);\nFireIfNecessary();\n// Now update low-level engine subsystems on behalf\n// of this tank. (NOT a good idea... see below!)\nm_pAnimationComponent->Update(dt);\nm_pCollisionComponent->Update(dt);\nm_pPhysicsComponent->Update(dt);\nm_pAudioComponent->Update(dt);\nm_pRenderingComponent->draw();\n1090 16. Runtime Gameplay Foundation Systems\n}\nGiven that our Update() functions are structured like this, the game loop\ncould be driven almost entirely by the updating of the game objects, like this:\nwhile (true)\n{\nPollJoypad();\nfloat dt = g_gameClock.CalculateDeltaTime();\nfor (each gameObject)\n{\n// This hypothetical Update() function updates\n// all engine subsystems!\ngameObject.Update(dt);\n}\ng_renderingEngine.SwapBuffers();\n}\nHowever attractive the simple approach to object updating shown above\nmay seem, it is usually not viable in a commercial-grade game engine. In\nthe following sections, we’ll explore some of the problems with this simplis-\ntic approach and investigate common ways in which each problem can be\nsolved.\n16.6.2 Performance Constraints and Batched Updates\nMost low-level engine systems have extremely stringent performance con-\nstraints. They operate on a large quantity of data, and they must do a large\nnumber of calculations every frame as quickly as possible. As a result, most\nengine systems benefit from batched updating. For example, it is usually far\nmoreefficienttoupdatealargenumberofanimationsinonebatchthanitisto\nupdate each object’s animation interleaved with other unrelated operations,\nsuch as collision detection, physical simulation and rendering.\nIn most commercial game engines, each engine subsystem is updated di-\nrectly or indirectly by the main game loop rather than being updated on a\nper-gameobjectbasisfromwithineachobject’s Update() function. Ifagame\nobject requires the services of a particular engine subsystem, it asks that sub-\nsystemtoallocatesomesubsystem-specificstateinformationonitsbehalf. For\nexample, a game object that wishes to be rendered via a triangle mesh might\nrequest the rendering subsystem to allocate a meshinstance for its use. (As de-\nscribed in Section 11.1.1.5, a mesh instance represents a single instance of a\n16.6. Updating Game Objects in Real Time 1091\ntriangle mesh—it keeps track of the position, orientation and scale of the in-\nstance in world space whether or not it is visible, per-instance material data\nand any other per-instance information that may be relevant.) The rendering\nengine maintains a collection of mesh instances internally. It can manage the\nmesh instances however it sees fit in order to maximize its own runtime per-\nformance. The game object controls howit is rendered by manipulating the\nproperties of the mesh instance object, but the game object does not control\nthe rendering of the mesh instance directly. Instead, after all game objects\nhave had a chance to update themselves, the rendering engine draws all visi-\nble mesh instances in one efficient batch update.\nWith batched updating, a particular game object’s Update() function,\nsuch as that of our hypothetical tank object, might look more like this:\nvirtual void Tank::Update(float dt)\n{\n//Update the state of the tank itself.\nMoveTank(dt);\nDeflectTurret(dt);\nFireIfNecessary();\n// Control the properties of my various engine\n// subsystem components, but do NOT update\n// them here...\nif (justExploded)\n{\nm_pAnimationComponent->PlayAnimation(""explode"");\n}\nif (isVisible)\n{\nm_pCollisionComponent->Activate();\nm_pRenderingComponent->Show();\n}\nelse\n{\nm_pCollisionComponent->Deactivate();\nm_pRenderingComponent->Hide();\n}\n// etc.\n}\nThe game loop then ends up looking more like this:\nwhile (true)\n{\n1092 16. Runtime Gameplay Foundation Systems\nPollJoypad();\nfloat dt = g_gameClock.CalculateDeltaTime();\nfor (each gameObject)\n{\ngameObject.Update(dt);\n}\ng_animationEngine. Update(dt);\ng_physicsEngine. Simulate(dt);\ng_collisionEngine. DetectAndResolveCollisions(dt);\ng_audioEngine. Update(dt);\ng_renderingEngine. RenderFrameAndSwapBuffers();\n}\nBatched updating provides many performance benefits, including but not\nlimited to:\n•Maximal cache coherency . Batched updating allows an engine subsystem\ntoachievemaximumcachecoherencybecauseitsper-objectdataismain-\ntained internally and can be arranged in a single, contiguous region of\nRAM.\n•Minimalduplicationofcomputations. Globalcalculationscanbedoneonce\nand reused for many game objects rather than being redone for each ob-\nject.\n•Reducedreallocationofresources. Enginesubsystemsoftenneedtoallocate\nandmanagememoryand/orotherresourcesduringtheirupdates. Ifthe\nupdate of a particular subsystem is interleaved with those of other en-\ngine subsystems, these resources must be freed and reallocated for each\ngame object that is processed. But if the updates are batched, the re-\nsources can be allocated once per frame and reused for all objects in the\nbatch.\n•Efficient pipelining. Many engine subsystems perform a virtually identi-\ncalsetofcalculationsoneachandeveryobjectinthegameworld. When\nupdates are batched, a scatter/gather approach can be employed to di-\nvidelargeworkloadsacrossmultipleCPUcores. Thiskindofparallelism\ncannot be achieved when processing each object in isolation.\nPerformance benefits aren’t the only reason to favor a batch updating ap-\nproach. Some engine subsystems simply don’t work at all when updated on\na per-object basis. For example, if we are trying to resolve collisions within\na system of multiple dynamic rigid bodies, a satisfactory solution cannot be\n16.6. Updating Game Objects in Real Time 1093\nfound in general by considering each object in isolation. The interpenetra-\ntionsbetweentheseobjectsmustberesolvedasagroup, eitherviaaniterative\napproach or by solving a linear system.\n16.6.3 Object and Subsystem Interdependencies\nEvenifwedidn’tcareaboutperformance,asimplisticper-objectupdatingap-\nproachbreaksdownwhengameobjects dependononeanother. Forexample,a\nhuman character might be holding a cat in her arms. In order to calculate the\nworld-space pose of the cat’s skeleton, we first need to calculate the world-\nspace pose of the human. This implies that the orderin which objects are up-\ndated is important to the proper functioning of the game.\nAnotherrelatedproblemariseswhenengine subsystems dependononean-\nother. For example, a rag doll physics simulation must be updated in concert\nwith the animation engine. Typically, the animation system produces an in-\ntermediate, local-space skeletal pose. These joint transforms are converted to\nworld space and applied to a system of connected rigid bodies that approxi-\nmate the skeleton within the physics system. The rigid bodies are simulated\nforward in time by the physics system, and then the final resting places of\nthe joints are applied back to their corresponding joints in the skeleton. Fi-\nnally, the animation system calculates the world-space pose and skinning ma-\ntrixpalette. Soonceagain, theupdatingoftheanimationandphysicssystems\nmust occur in a particular order in order to produce correct results. These\nkinds of inter-subsystem dependencies are commonplace in game engine\ndesign.\n16.6.3.1 Phased Updates\nTo account for inter-subsystem dependencies, we can explicitly code our en-\ngine subsystem updates in the proper order within the main game loop. For\nexample, to handle the interplay between the animation system and rag doll\nphysics, we might write something like this:\nwhile (true) // main game loop\n{\n// ...\ng_animationEngine. CalculateIntermediatePoses(dt);\ng_ragdollSystem. ApplySkeletonsToRagDolls();\ng_physicsEngine. Simulate(dt); // runs ragdolls too\ng_collisionEngine. DetectAndResolveCollisions(dt);\ng_ragdollSystem. ApplyRagDollsToSkeletons();\ng_animationEngine. FinalizePoseAndMatrixPalette();\n1094 16. Runtime Gameplay Foundation Systems\n// ...\n}\nWemustbecarefultoupdatethestatesofourgameobjectsattherighttime\nduringthegameloop. Thisisoftennotassimpleascallingasingle Update()\nfunction per game object per frame. Game objects may depend upon the in-\ntermediate results of calculations performed by various engine subsystems.\nFor example, a game object might request that animations be played prior to\ntheanimationsystemrunningitsupdate. However, thatsameobjectmayalso\nwanttoprocedurallyadjusttheintermediateposegeneratedbytheanimation\nsystem prior to that pose being used by the rag doll physics system and/or\nthe final pose and matrix palette being generated. This implies that the object\nmust be updated twice, once before the animation calculates its intermediate\nposes and once afterward.\nMany game engines allow their game objects to run update logic at multi-\nplepointsduringtheframe. Forexample,theNaughtyDogengine(theengine\nthatdrivesthe Uncharted andTheLastofUs game series)updates game objects\nthree times—once before animation blending, once after animation blending\nbutpriortofinalposegenerationandonceafterfinalposegeneration. Thiscan\nbe accomplished by providing each game object class with three virtual func-\ntions that act as “hooks.” In such a system, the game loop ends up looking\nsomething like this:\nwhile (true) // main game loop\n{\n// ...\nfor (each gameObject)\n{\ngameObject.PreAnimUpdate (dt);\n}\ng_animationEngine. CalculateIntermediatePoses(dt);\nfor (each gameObject)\n{\ngameObject.PostAnimUpdate (dt);\n}\ng_ragdollSystem. ApplySkeletonsToRagDolls();\ng_physicsEngine. Simulate(dt); // runs ragdolls too\ng_collisionEngine. DetectAndResolveCollisions(dt);\ng_ragdollSystem. ApplyRagDollsToSkeletons();\n16.6. Updating Game Objects in Real Time 1095\ng_animationEngine. FinalizePoseAndMatrixPalette();\nfor (each gameObject)\n{\ngameObject.FinalUpdate(dt);\n}\n// ...\n}\nWe can provide our game objects with as many update phases as we see\nfit. But we must be careful, because iterating over all game objects and calling\na virtual function on each one can be expensive. Also, not all game objects\nrequireallupdatephases—iteratingoverobjectsthatdon’trequireaparticular\nphase is a pure waste of CPU bandwidth.\nActually, the above example isn’t completely realistic. Iterating directly\nover all game objects to call their PreAnimUpdate() ,PostAnimUpdate()\nandFinalUpdate() hook functions would be highly inefficient, because\nonly a small percentage of the objects might actually need to perform any\nlogic in each hook. It’s also an inflexible design, because only game objects\nare supported—if we wanted to update a particle system during the post-\nanimation phase, we’d be out of luck. Finally, such a design would lead to\nunnecessary coupling between the low-level engine systems and the game ob-\nject system.\nA generic callback mechanism would be a much better design choice. In\nsuch a design, the animation system would provide a facility by which any\nclient code (game objects or any other engine system) could register a callback\nfunction for each of the three update phases (pre-animation, post-animation\nandfinal). Theanimationsystemwoulditeratethroughallregisteredcallbacks\nand call them, without any “knowledge” of game objects per se. This design\nmaximizes performance, because only those clients that actually needupdates\nregister callbacks and are called each frame. It also maximizes flexibility and\neliminates unnecessary coupling between the game object system and other\nengine subsystems, because any client is allowed to register a callback, not\njust game objects.\n16.6.3.2 Bucketed Updates\nIn the presence of inter-object dependencies, the phased updates technique de-\nscribed above must be adjusted a little. This is because inter-object dependen-\ncies can lead to conflicting rules governing the order of updating. For exam-\nple, let’s imagine that object B is being held by object A. Further, let’s assume\nthat we can only update object B after A has been fullyupdated, including the\n1096 16. Runtime Gameplay Foundation Systems\nDepends\nOn\nFigure 16.14. Inter-object update order dependencies can be viewed as a forest of dependency\ntrees.\ncalculationofitsfinalworld-spaceposeandmatrixpalette. Thisconflictswith\nthe need to batch animation updates of all game objects together in order to\nallow the animation system to achieve maximum throughput.\nInter-objectdependenciescanbevisualizedasaforestofdependencytrees.\nThe game objects with no parents (no dependencies on any other object) rep-\nresent the roots of the forest. An object that depends directly on one of these\nroot objects resides in the first tier of children in one of the trees in the forest.\nAn object that depends on a first-tier child becomes a second-tier child and so\non. This is illustrated in Figure 16.14.\nOne solution to the problem of conflicting update order requirements is to\ncollectobjectsintoindependentgroups,whichwe’llcall bucketshereforlackof\nabettername. Thefirstbucketconsistsofallrootobjectsintheforest. Thesec-\nond bucket is comprised of all first-tier children. The third bucket contains all\nsecond-tier children and so on. For each bucket, we run a complete update of\nthe game objects and the engine systems, complete with all update phases.\nThen we repeat the entire process for each bucket until there are no more\nbuckets.\nIn theory, the depths of the trees in our dependency forest are unbounded.\nHowever, in practice, they are usually quite shallow. For example, we might\nhave characters holding weapons, and those characters might or might not be\nriding on a moving platform or a vehicle. To implement this, we only need\nthree tiers in our dependency forest, and hence only three buckets: one for\n16.6. Updating Game Objects in Real Time 1097\nplatforms/vehicles, one for characters and one for the weapons in the charac-\nters’hands. Manygameenginesexplicitlylimitthedepthoftheirdependency\nforest so that they can use a fixed number of buckets (presuming they use a\nbucketed approach at all—there are of course many other ways to architect a\ngame loop).\nHere’s what a bucketed, phased, batched update loop might look like:\nenum Bucket\n{\nkBucketVehiclesPlatforms,\nkBucketCharacters,\nkBucketAttachedObjects,\nkBucketCount\n};\nvoid UpdateBucket (Bucket bucket )\n{\n// ...\nfor (each gameObject in bucket )\n{\ngameObject.PreAnimUpdate(dt);\n}\ng_animationEngine.CalculateIntermediatePoses\n(bucket, dt);\nfor (each gameObject inbucket )\n{\ngameObject.PostAnimUpdate(dt);\n}\ng_ragdollSystem.ApplySkeletonsToRagDolls( bucket);\ng_physicsEngine.Simulate( bucket , dt); // ragdolls etc.\ng_collisionEngine.DetectAndResolveCollisions\n(bucket, dt);\ng_ragdollSystem.ApplyRagDollsToSkeletons( bucket);\ng_animationEngine.FinalizePoseAndMatrixPalette\n(bucket );\nfor (each gameObject inbucket )\n{\ngameObject.FinalUpdate(dt);\n}\n1098 16. Runtime Gameplay Foundation Systems\n// ...\n}\nvoid RunGameLoop()\n{\nwhile (true)\n{\n// ...\nUpdateBucket (kBucketVehiclesAndPlatforms);\nUpdateBucket (kBucketCharacters);\nUpdateBucket (kBucketAttachedObjects);\n// ...\ng_renderingEngine.RenderSceneAndSwapBuffers();\n}\n}\nIn practice, things might be a bit more complex than this. For example,\nsomeenginesubsystemslikethephysicsenginemightnotsupporttheconcept\nof buckets, perhaps because they are third-party SDKs or because they cannot\nbe practically updated in a bucketed manner. However, this bucketed update\nis essentially what we used at Naughty Dog to implement all of the games in\ntheUncharted series as well as The Last of Us . So it’s a method that has proven\nto be practical and reasonably efficient.\n16.6.3.3 Object State Inconsistencies and One-Frame-Off Lag\nLet’s revisit game object updating, but this time thinking in terms of each ob-\nject’s local notion of time. We said in Section 16.6 that the state of game object\niat time tcan be denoted by a state vector Si(t). When we update a game ob-\nject, we are converting its previous state vector Si(t1)into a new current state\nvector Si(t2)(where t2=t1+∆t).\nIn theory, the states of all game objects are updated from time t1to time\nt2instantaneously and in parallel, as depicted in Figure 16.15. However, pre-\nsuming that our game update loop is single-threaded, we actually update the\nobjects one by one—we loop over each game object and call some kind of up-\ndate function on each one in turn. If we were to stop the program halfway\nthrough this update loop, half of our game objects’ states would have been\nupdated to Si(t2), while the remaining half would still be in their previous\nstates Si(t1). This implies that if we were to ask two of our game objects what\nthecurrenttimeisduringtheupdateloop,theymayormaynotagree! What’s\nmore, depending on where exactly we interrupt the update loop, the objects\n16.6. Updating Game Objects in Real Time 1099\nt1\ntSA ObjectA SA\nObjectB SB\nObjectC SC\nObjectD SDt2\nSB\nSC\nSD\nt\nFigure 16.15. In theory, the states of all game objects are updated instantaneously and in parallel\nduring each iteration of the game loop.\nmay all be in a partially updated state. For example, animation pose blending\nmayhavebeenrun,butphysicsandcollisionresolutionmaynotyethavebeen\napplied. This leads us to the following rule:\nThe states of all game objects are consistent beforeandafterthe up-\ndate loop, but they may be inconsistent duringit.\nThis is illustrated in Figure 16.16.\nThe inconsistency of game object states during the update loop is a major\nsource of confusion and bugs, even among professionals within the game in-\nt1\ntSA ObjectA\nObjectBSA\nObjectC\nObjectDSCt2\nSB\nSDSB\nSC\nFigure 16.16. In practice, the states of the game objects are updated one by one. This means that\nat some arbitrary moment during the update loop, some objects will think the current time is t2\nwhile others think it is still t1. Some objects may be only partially updated, so their states will be\ninternally inconsistent. In effect, the state of such an object lies at a point between t1 andt2.\n1100 16. Runtime Gameplay Foundation Systems\ndustry. The problem rears its head most often when game objects query one\nanotherforstateinformationduringtheupdateloop(whichimpliesthatthere\nis a dependency between them). For example, if object B looks at the velocity\nof object A in order to determine its own velocity at time t, then the program-\nmer must be clear about whether he or she wants to read the previous state of\nobject A, SA(t1), or thenewstate, SA(t2). If the new state is needed but object\nA has not yet been updated, then we have an update order problem that can\nlead to a class of bugs known as one-frame-offlags . In this type of bug, the state\nof one object lags one frame behind the states of its peers, which manifests\nitself on-screen as a lack of synchronization between game objects.\n16.6.3.4 Object State Caching\nAs described above, one solution to this problem is to group the game ob-\njects into buckets (Section 16.6.3.2). One problem with a simple bucketed up-\ndate approach is that it imposes somewhat arbitrary limitations on the way\nin which game objects are permitted to query one another for state informa-\ntion. If a game object A wants the updated state vector SB(t2)of another object\nB, then object B must reside in a previously updated bucket. Likewise, if object\nA wants the previous state vector SB(t1)of object B, then object B must reside\nin ayet-to-be-updated bucket. Object A should never ask for the state vector\nof an object within its own bucket, because, as we stated in the rule above,\nthosestatevectorsmaybeonlypartiallyupdated. Oratbest, there’ssomeun-\ncertainty as to whether you’re accessing the other object’s state at time t1or\ntime t2.\nOnewaytoimproveconsistencyistoarrangeforeachgameobjectto cache\nitspreviousstatevector Si(t1)whileitiscalculatingitsnewstatevector Si(t2)\nrather than overwriting it in-place during its update. This has two immediate\nbenefits. First, it allows any object to safely query the previous state vector of\nany other object without regard to update order. Second, it guarantees that\na totally consistent state vector ( Si(t1)) will always be available, even during\nthe update of the new state vector. To my knowledge there is no standard\nterminology for this technique, so I’ll call it state caching for lack of a better\nname.\nAnotherbenefitofstatecachingisthatwecanlinearlyinterpolatebetween\nthe previous and next states in order to approximate the state of an object at\nany moment between these two points in time. The Havok physics engine\nmaintains the previous and current state of every rigid body in the simulation\nfor just this purpose.\nThe downside of state caching is that it consumes twice the memory of the\nupdate-in-place approach. It also only solves half the problem, because while",30978
113-16.7 Applying Concurrency to Game Object Updates.pdf,113-16.7 Applying Concurrency to Game Object Updates,"16.7. Applying Concurrency to Game Object Updates 1101\nthe previous states at time t1are fully consistent, the new states at time t2still\nsuffer from potential inconsistency. Nonetheless, the technique can be useful\nwhen applied judiciously.\nThistechniqueisinfluencedstronglybythedesignprinciplesofpure func-\ntionalprogramming (see Section 16.9.2). In a pure functional programming lan-\nguage,alloperationsareperformedbyfunctionswithaclearinputandoutput,\nand no side-effects. All data is considered constant and immutable—rather\nthan mutating an input datum in place, a brand new datum is always pro-\nduced.\n16.6.3.5 Time-Stamping\nOne easy and low-cost way to improve the consistency of game object states\nis to time-stamp them. It is then a trivial matter to determine whether a game\nobject’s state vector corresponds to its configuration at a previous time or the\ncurrent time. Any code that queries the state of another game object during\ntheupdateloopcanassertorexplicitlycheckthetimestamptoensurethatthe\nproper state information is being obtained.\nTime-stamping does not address the inconsistency of states during the up-\ndateofabucket. However,wecansetaglobalorstaticvariabletoreflectwhich\nbucket is currently being updated. Presumably every game object “knows” in\nwhich bucket it resides. So we can check the bucket of a queried game ob-\nject against the currently updating bucket and assert that they are not equal in\norder to guard against inconsistent state queries.\n16.7 Applying Concurrency to Game Object Updates\nIn Chapter 4 we explored hardware parallelism and how to take advantage of\nthe explicitly parallel computing hardware that has become the norm in re-\ncent gaming hardware. This is done via concurrent programming techniques.\nIn Section 8.6, we introduced a number of approaches that allow a game en-\ngine to take advantage of the parallel processing resources. In this section,\nwe’lltakealookatwaysinwhichconcurrencyandparallelismcanbeapplied\nto the problem of updating the states of our game objects.\n16.7.1 Concurrent Engine Subsystems\nClearly, themostperformance-criticalpartsofourengine—suchasrendering,\nanimation,audioandphysics—aretheonesthatwillbenefitmostfromparallel\nprocessing. So, whether or not our game object model is being updated in a\n1102 16. Runtime Gameplay Foundation Systems\nsingle thread or across multiple cores, it needs to be able to interface with low-\nlevel engine systems that are almost certainly multithreaded.\nIf our engine supports a general-purpose job system (see Section 8.6.4), we\ncan use that job system to make our engine subsystems execute concurrently.\nInthisscenario,eachsubsystem’sframeupdatemightbekickedoffasasingle\njobeachframe. However,it’sprobablyabetterideaforeachsubsystemtokick\noff multiple jobs to perform its work each frame. For example, an animation\nsystem might kick off one job for each object in the game world that requires\nanimation blending. Later in the frame, when the animation system performs\nits world matrix and skinning matrix computations, it could potentially em-\nploy a scatter/gather approach to divide this work across the available cores.\nWhen making our low-level engine systems update concurrently, we need\ntoensurethattheir interfaces arethread-safe. Wewanttobesurethatnoexternal\ncode can get into a data race situation, either in contention with other external\nclients, or in contention with the inner workings of the subsystem itself. This\ntypically involves using locks on all external calls into each subsystem.\nIf our job system employs user-level threads (coroutines or fibers), then we\nwill need to use spin locks to make our subsystems thread-safe. However,\nif our subsystems are updated by OS threads, then mutexes are also a viable\noption for this purpose.\nIf we can be certain that a particular engine subsystem only ever runs dur-\ning one particular phase of the game loop, we may be able to employ lock-not-\nneededassertions ratherthanactuallyhavingtolockcertainpartsofthesubsys-\ntem. See Section 4.9.7.5 for more information on these useful assertions.\nOf course, another option is to attempt to use lock-free data structures to\nimplement our engine subsystems critical (shared) data. Lock-free data struc-\ntures are tricky to implement, and some data structures still have no known\nlock-free implementation. As such, if you opt for a lock-free approach, it’s\nprobably wise to restrict your efforts only to engine subsystems with the most\nstringent performance requirements.\n16.7.2 Asynchronous Program Design\nWhen interfacing with concurrent engine subsystems (for example from\nwithinagameobjectupdate),wemustbeginthinking asynchronously . Whena\ntime-consuming operation is to be performed, we should therefore avoid call-\ning ablocking function—a function that does its work directly in the context of\nthe calling thread, thereby blocking that thread or job until the work has been\ncompleted. Instead, whenever possible, large or expensive jobs should be re-\nquested by calling a non-blocking function—a function that sends the request\n16.7. Applying Concurrency to Game Object Updates 1103\nto be executed by another thread, core or processor and then immediately re-\nturnscontroltothecallingfunction. Thecallingthreadorjobcanproceedwith\nother unrelated work, perhaps including updating other game objects, while\nwewaitfortheresultsofourrequest. Laterinthesameframe,orinnextframe,\nwe can pick up the results of the request and make use of them.\nForexample,agamemightrequestthataraybecastintotheworldinorder\nto determine whether the player has line-of-sight to an enemy character. In a\nsynchronous design, the ray cast would be done immediately in response to\nthe request, and when the ray casting function returned, the results would be\navailable, as shown below.\nSomeGameObject::Update()\n{\n// ...\n// Cast a ray to see if the player has line of sight\n// to the enemy.\nRayCastResult r=castRay(playerPos, enemyPos);\n// Now process the results...\nif ( r.hitSomething() && isEnemy(r.getHitObject() ))\n{\n// Player can see the enemy.\n// ...\n}\n// ...\n}\nIn an asynchronous design, a ray cast request would be made by calling\na function that simply sets up and enqueues a ray cast job, and then returns\nimmediately. The calling thread or job can continue doing other unrelated\nwork while the job is being processed by another CPU or core. Later, once\nthe ray cast job has been completed, the calling thread or job can pick up the\nresults of the ray cast query and process them:\nSomeGameObject::Update()\n{\n// ...\n// Cast a ray to see if the player has line of sight\n// to the enemy.\nRayCastResult r;\nrequestRayCast (playerPos, enemyPos, &r);\n1104 16. Runtime Gameplay Foundation Systems\n// Do other unrelated work while we wait for the\n// other CPU to perform the ray cast for us.\n// ...\n// OK, we can't do any more useful work. Wait for the\n// results of our ray cast job. If the job is\n// complete, this function will return immediately.\n// Otherwise, the main thread will idle until the\n// results are ready...\nwaitForRayCastResults (&r);\n// Process results...\nif(r.hitSomething() && isEnemy( r.getHitObject()))\n{\n// Player can see the enemy.\n// ...\n}\n// ...\n}\nIn many instances, asynchronous code can kick off a request on one frame,\nand pick up the results on the next. In this case, you may see code that looks\nlike this:\nRayCastResult r;\nbool rayJobPending = false;\nSomeGameObject::Update()\n{\n// ...\n//Wait for the results of last frame's ray cast job.\nif (rayJobPending )\n{\nwaitForRayCastResults (&r);\n// Process results...\nif (r.hitSomething() && isEnemy(r.getHitObject() ))\n{\n// Player can see the enemy.\n// ...\n}\n}\n16.7. Applying Concurrency to Game Object Updates 1105\n// Cast a new ray for next frame.\nrayJobPending = true;\nrequestRayCast (playerPos, enemyPos, &r);\n// Do other work...\n// ...\n}\n16.7.2.1 When to Make Asynchronous Requests\nOne particularly tricky aspect of converting synchronous, unbatched code to\nuse an asynchronous, batched approachis determining whenduring the game\nloop (a) to kick off the request and (b) to wait for and utilize the results. In\ndoing this, it is often helpful to ask ourselves the following questions:\n•Howearlycanwekickoffthisrequest? Theearlierwemaketherequest, the\nmore likely it is to be done when we actually need the results—and this\nmaximizes CPU utilization by helping to ensure that the main thread is\nnever idle waiting for an asynchronous request to complete. So for any\ngiven request, we should determine the earliest point during the frame\nat which we have enough information to kick it off, and kick it there.\n•How long can we wait before we need the results of this request? Perhaps\nwe can wait until later in the update loop to do the second half of an\noperation. Perhaps we can tolerate a one-frame lag and use last frame’s\nresults to update the object’s state this frame. (Some subsystems like AI\ncan tolerate even longer lag times because they update only every few\nseconds.) In many circumstances, code that uses the results of a request\ncaninfactbedeferreduntillaterintheframe,givenalittlethought,some\ncode refactoring, and possibly some additional caching of intermediate\ndata.\n16.7.3 Job Dependencies and Degree of Parallelism\nIn order to make the best use of a parallel computing platform, we’d like to\nkeep all cores busy at all times. Presuming we’re using a job system to paral-\nlelize our engine, each iteration of our game loop will be comprised of hun-\ndreds or even thousands of jobs running concurrently. However, dependencies\nbetweenthesejobscanleadtoless-than-idealutilizationoftheavailablecores.\nIf job B takes as its input some data that is produced by job A, then job B can-\nnot begin until job A is done. This creates a dependency between job B and\njob A.\nThedegree of parallelism (DOP) of a system, also known as its degree of con-\ncurrency (DOC), measures the theoretical maximum number of jobs that can\n1106 16. Runtime Gameplay Foundation Systems\nDOP = 4A\nF\nE DB\nC\nA F E D B C\nA F E D B CDOP = 1\nDOP = 6\nFigure 16.17. Three job dependency trees. The nodes of the tree are jobs, and the arrows represent\ndependencies between them. The number of leaves in such a tree indicates the system’s degree\nof parallelism (DOP).\nbe running in parallel at any given moment in time. The degree of parallelism\nof a group of jobs can be determined by drawing a dependency graph. In such\na graph, the jobs form the nodes of the tree, and the parent-child relationships\nrepresent dependencies. The number of leaves of the tree tells us the degree\nofparallelismofthatcollectionofjobs. Figure16.17illustratesafewexamples\nof job dependency trees and their corresponding degrees of parallelism.\nInordertoachievefullutilizationofCPUresourcesinanexplicitlyparallel\ncomputer, we would like the DOP of our system to match or exceed the num-\nber of available cores. If the DOP of the software exactly equals the number of\ncores, we get maximum throughput. When the DOP of our system is higher\nthan the number of cores, throughput is reduced because some jobs must run\nserially,butnocoreisidle. Butwhenoursystem’sDOPislessthanthenumber\nof cores, some cores won’t have any work to do.\nWhenever a job is forced to wait for the jobs on which it depends to com-\npletetheirtasks,a synchronizationpoint (or“syncpoint”forshort)isintroduced\ninto the system. Each sync point represents an opportunity for valuable CPU\nresources to be wasted while a job waits for its dependent jobs to complete\ntheir work. This is illustrated in Figure 16.18.\nTo maximize hardware utilization, we can attempt to increase the DOP of\n16.7. Applying Concurrency to Game Object Updates 1107\nZCore 0\nCore 1\nCore 2\nCore 3C DFA\nB\nESync Points\nTime\nFigure 16.18. A synchronization point is introduced whenever one job is dependent upon the com-\npletion of one or more other jobs. Here, job D depends on job C, and job F depends on jobs A, B,\nD and E.\nour system by reducing dependencies between jobs. We could also try to find\nsome other unrelated work to do during idle periods. We can also reduce or\neliminate the impact of a sync point by deferring it. For example, let’s say that\njob D cannot commence its work until jobs A, B and C are done. If we try to\nschedule job D before A, B and C are all done, then it will obviously have to\nsit idle for some time. But if we defer job D so that it runs well after A, B and\nC are done, then we can be confident that D will never have to wait. This idea\nis illustrated in Figure 16.19. In his talk entitled “Diving Down the Concur-\nrency Rabbit Hole” (http://cellperformance.beyond3d.com/articles/public/\nconcurrency_rabit_hole.pdf), Mike Acton says, “The secret of optimized con-\ncurrentdesignis delay.” Thisiswhathe’stalkingabout: Deferringsyncpoints\nas a means of reducing or eliminating idle time in a concurrent system.\n16.7.4 Parallelizing the Game Object Model Itself\nGame object models are notoriously difficult to parallelize for a few reasons.\nGame objects tend to be highly interdependent, and are typically dependent\non numerous engine subsystems as well. Inter-object dependencies arise be-\ncause game objects routinely communicate with one another and query one\nanother’s states during their updates. These interactions tend to occur mul-\ntiple times during the update loop, and the pattern of communication can be\nunpredictable and highly sensitive to the inputs of the human player and the\nevents that are occurring in the game world. This makes it difficult to pro-\ncessgameobjectupdates concurrently (usingmultiplethreadsormultipleCPU\ncores).\nThat being said, it certainly is possible to update a game object model con-\n1108 16. Runtime Gameplay Foundation Systems\nSync point creates \nidle time on Core 2\nSync point is no \nlonger problematicidleCore 0\nCore 1\nCore 2BE F A\nD C\nCore 0\nCore 1\nCore 2E F A\nIG\nJ CB HG H\nD\nFigure 16.19. Job D depends on jobs A, B and C. Top: If we attempt to schedule job D immediately\nafter job C on Core 2, the core will sit idle waiting for job B to complete. Bottom: If we delay job\nD’s invocation until well after jobs A, B and C have completed, we free up Core 2 to run other jobs,\nthereby avoiding the idle time that had been caused by the sync point.\ncurrently. As one example, Naughty Dog implemented a concurrent game\nobject model when we ported our engine from PS3 to PS4 for The Last of Us:\nRemastered . In this section, we’ll explore some of the problems one encoun-\nters when updating game objects concurrently, and we’ll have a look at some\nof the solutions that Naughty Dog uses in our engine. Of course, these tech-\nniques are just one possible way to tackle the problem—other engines may\nuse different approaches. And who knows? Perhaps youwill invent a novel\nway of solving some of the vexing problems of concurrent game object model\nupdates!\n16.7.4.1 Game Object Updates as Jobs\nIf our game engine has a job system, it will very likely be used to parallelize\nthevariouslow-levelsubsystemsinourengine,suchasanimation,audio,coll-\nsion/physics, rendering, file I/O, and so on. So why not parallelize our game\nobject updates by making them jobs, too? This is the approach used in the\nNaughtyDogengine. Itworks, butgettingittoworkcorrectly, andefficiently,\nis a non-trivial undertaking.\n16.7. Applying Concurrency to Game Object Updates 1109\nWe discussed in Section 16.6.3.2 that, due to interdependencies between\ngame objects, we need to control the order in which they update. One way to\nachieve this is to divide all of the objects in the game into Nbuckets, such that\nthe objects in bucket Bdepend only on objects in buckets 0 through B 1. If\nwe take thisapproach, we could update all the game objects in each bucket by\nkickingthemoffasjobs, andletthejobsystemschedulethemacrosstheavail-\nablecores(andinterspersedwithwhateverotherjobshappentoberunningat\nthetime). ThisisroughlythetechniqueemployedintheNaughtyDogengine.\nvoid UpdateBucket(int iBucket)\n{\njob::Declaration aDecl[kMaxObjectsPerBucket];\nconst int count = GetNumObjectsInBucket(iBucket);\nfor (int jObject = 0; jObject < count; ++jObject)\n{\njob::Declaration& decl = aDecl[jObject];\ndecl.m_pEntryPoint = UpdateGameObjectJob;\ndecl.m_param = reinterpret_cast<uintptr_t>(\nGetGameObjectInBucket(iBucket, jObject));\n}\njob::KickJobsAndWait (aDecl, count);\n}\nAnother way to deal with inter-object dependencies is to make them ex-\nplicit. In this case, we would presumably have some way of declaring that\ngame object A depends on game objects B, C and D, for example. These de-\npendencies would form a simple directed graph . To update the game objects,\nwe’dstartbykickingoffupdatejobsforallofthegameobjectswithnodepen-\ndencies on any other game object (i.e., those nodes of the dependency graph\nwith no outgoing edges). As each job completes, we would walk to each de-\npendent game object, and wait until all of its dependent objects’ update jobs\narecomplete. Thisprocesswouldrepeatuntiltheentiregraphofgameobjects\nhas been updated.\nWe can get into trouble with a graph of game object dependencies if the\ngraph contains any cycles. A dependency cycle involving two or more game\nobjects indicates that those objects cannot simply be updated in dependency\norder. Tohandlecycles,weeitherneedtoeliminatethembychangingtheway\nthe objects interact (turning our graph into a directedacyclic graph or DAG), or\nwe need to isolate these “clumps” of cyclically-dependent game objects and\nupdate each clump serially on a single core.\n1110 16. Runtime Gameplay Foundation Systems\n16.7.4.2 Asynchronous Game Object Updates\nWe said in Section 16.7.1 that game object updates are usually done asynch-\nronously. For example, rather than calling a blocking function to cast a ray, we\nwouldkickoffanasynchronous requestforaraycast,andthecollisionsubsys-\ntem would process this request at some future time during the frame. Later in\nthe frame, or next frame, we would pick up the results of the ray cast, and act\non them.\nThis approach continues to work well when the game objects themselves\nare also updating concurrently (across multiple cores). However, if our job\nsystem is based on user-level threads (coroutines or fibers), then blocking calls\nbecome a viable option, too. This works because a coroutine has the unique\nproperty of being able to yield(to another coroutine) and then continue where\nit left off at some later time (when another coroutine yields back to it). In a\nfiber-based job system (like the one used in the Naughty Dog engine), jobs\naren’tcoroutines perse,buttheydohavethissameproperty: Afiber-basedjob\nis able to “go to sleep” midway through its execution, and later to be “woken\nup” to continue execution where it left off.\nHere’s an example ray cast implemented using a blocking call, in a co-\nroutine- or fiber-based job system:\nSomeGameObject::Update()\n{\n// ...\n// Cast a ray to see if the player has line of sight\n// to the enemy.\nRayCastResult r=castRayAndWait (playerPos, enemyPos);\n//zzz...\n//Wake up when ray cast result is ready!\n// Now process the results...\nif (r.hitSomething() && isEnemy( r.getHitObject()))\n{\n// Player can see the enemy.\n// ...\n}\n// ...\n}\nNotice that this implementation looks nearly identical to the example of what\nnot to do that we presented in Section 16.7.2! Thanks to user-level threads, the\n16.7. Applying Concurrency to Game Object Updates 1111\nUpdate\ncastRayUpdate (cont ’d)\nTime\nFigure 16.20. If our job system is implemented with user-level threads, a job can be interrupted\npart-way through its execution in order to perform an asynchronous operation, and resumed\nonce that operation has been completed.\nexecution of our job can make use of blocking function calls after all! Figure\n16.20 illustrates what is happening: The job has effectively been cleaved into\ntwo parts—the portion that runsbeforethe blocking ray cast call, and the por-\ntion that runs after.\nThis mechanism is what allows us to implement job system functions like\nWaitForCounter() KickJobsAndWait(). Thesefunctions blocktheirjobs,\nputting them to sleep and permitting other jobs to execute while they wait for\nthe counter(s) in question to hit zero.\n16.7.4.3 Locking During Game Object Updates\nBucketedupdatesgoalongwaytosolvingtheproblemsthatariseduetointer-\nobject dependencies. They ensure that the game objects are updated in the\ncorrect global order (e.g., the train car updates before the objects sitting on it).\nAnd they help with inter-object state queries. An object in bucket Bcan safely\nquery the state of objects in buckets B ∆, and those in buckets B+∆(where\n∆>0), because in both cases we know that those objects won’t be updating\nconcurrently with the objects in bucket B. There is still a one-frame-off issue,\nhowever: Ifonframe N,anobjectinbucket Bqueriesanobjectinbucket B ∆,\nit will see the state of that object on frame N; however, if it queries an object in\nbucket B+∆, it will see the state of that object as it was last frame (on frame\nN 1) because it won’t have updated yet.\nSo, with a bucketed updating system, we can safely access game objects in\notherbuckets without needing any kind of lock. However, what about when\ngame objects in the samebucket need to interact or query one another? Here,\nwe are once again prone to concurrent race conditions, so we can’t just do\nnothing and cross our fingers.\nWemightbetemptedtointroduceasingle,globallock(mutexorspinlock)\ninto the game object system. Every game object within one particular bucket\ncould acquire this lock, perform its update (possibly interacting with other\n1112 16. Runtime Gameplay Foundation Systems\ngame objects in the process), and then release the lock when it’s done. This\nwouldcertainlyguaranteethatinter-objectcommunicationwouldbefreefrom\ndata races. However, it would also have the highly undesirable effect of seri-\nalizingthe updates of all game objects within the bucket, reducing our “con-\ncurrent”game object update in effectto a single-threadedupdate! This occurs\nbecausethelockwouldpreventanytwogameobjectsfromeverupdatingcon-\ncurrently, even when those two objects don’t interact in any way.\nThere are all sorts of other ways to tackle this problem. One approach we\ntriedearlyonatNaughtyDogwastointroduceagloballockingsystem,butto\nonlyacquirethelockwheneveragameobjecthandlewas dereferenced withina\ngame object’s update function. Game objects in our engine are referenced by\nhandle rather than by raw pointers, to support memory defragmentation. So\ntogetarawpointertoagameobject,onemustfirstdereferenceitshandle. This\nis a perfect opportunity to detect that one game object intends to interact with\nanother. By only taking locks when interactions are actually likely to happen,\nwe were able to recover some degree of concurrency. However, this locking\nsystem was complex, difficult to work with, and still led to inefficient use of\nthe CPU cores during bucket updates.\n16.7.4.4 Object Snapshots\nUpon analyzing the inter-dependencies between the game objects in a real\ngameengine,wecanmakeanobservation: Alargemajorityoftheinteractions\nbetween game objects during their updates are state queries. In other words,\ngame object A reaches out and queries the current state of game objects B, C,\nandsoon. WhengameobjectAdoesthis,itreallyonlyneedstohaveaccessto\naread-onlycopy ofthestatesoftheseothergameobjects. Itneedn’tinteractwith\nthe objects themselves (which might or might not be concurrently updating at\nthe moment of such interactions).\nGiventhisstateofaffairs, itmakessensetoarrangeforeachgameobjectto\nprovide a snapshot of its relevant state information—a read-only copy which\ncanbequeried,withoutlocksorfearofdataraces,byanyothergameobjectin\nthe system. Snapshots are really just an example of the statecaching technique\nwe described in Section 16.6.3.4. We used this approach at Naughty Dog for\nThe Last of Us: Remastered ,Uncharted 4: A Thief’s End andUncharted: The Lost\nLegacy.\nHere’s how snapshots work in the Naughty Dog engine: At the start of\neach bucket update, we ask each game object to update its snapshot. These\nupdates never query the state of other game objects, so they can be run con-\ncurrentlywithoutlocks. Onceallupdatesarecomplete,wekickoffconcurrent\njobs to update the game objects’ internal states. These too run concurrently.\n16.7. Applying Concurrency to Game Object Updates 1113\nWheneveragameobjectinbucket Bneedstoquerythestateofanotherobject,\nit now has three options:\n1. When querying an object in buckets B ∆, it can query it directly or\nquery the object’s snapshot.\n2. When querying an object in bucket B, it queries the snapshot.\n3. When querying an object in buckets B+∆, it again can query either the\nobject itself, or its snapshot.\n16.7.4.5 Handling Inter-Object Mutation\nSnapshotsallowustoavoidlockswhengameobjects readoneanother’sgame\nstate. However,theydonothingtoaddressthedataracesthatcanoccurwhen\none game object needs to mutatethe state of another object in the same bucket.\nTohandlethesesituations, NaughtyDogappliedacombinationofthefollow-\ning rules of thumb and techniques:\n• Minimize inter-object mutation wherever possible.\n• Inter-object mutation between buckets is safe, but\n• inter-object mutation withina bucket must be handled carefully...\n◦with locks,\n◦or by requesting mutations by placing them onto a request queue,\nratherthanapplyingthemutationsimmediately. Therequestqueue\nis itself protected by a lock, and the handling of the requests in the\nqueue is deferred until after the bucket update has completed.\nAnotheroptionforinter-objectmutationwithinasinglebucketistospawn\na job in the nextbucket whose job it is to synchronize these mutation opera-\ntions. Bydeferringtheactiontoalaterbucket,wecanbecertainthattheobjects\nin question have completed their updated and will not be updating concur-\nrently with our work. We did this at Naughty Dog to handle synchronized\nmelee moves involving the player and an NPC.\n16.7.4.6 Future Improvements\nThe bucketed updating system we’ve described in this section isn’t perfect by\nanymeans. Bucketedupdatesaren’tasefficientastheycouldbe,becauseeach\ntransitionbetweenbucketsintroducesa syncpoint intothegameloop. Atthese\nsyncpoints, someCPUcoresmaysitidlewhilewewaitforallgameobjectsin\nthe bucket to complete their updates.",26616
114-16.8 Events and Message-Passing.pdf,114-16.8 Events and Message-Passing,"1114 16. Runtime Gameplay Foundation Systems\nSnapshotting is also an imperfect solution to within-bucket dependency\nproblems, because it only handles read-only queries; locks are still required\nfor inter-object mutations. The snapshots themselves can be expensive to up-\ndateaswell(althoughoneeasy-to-applyoptimizationhereistoonlygenerate\nsnapshots for objects on an as-needed basis).\nThere are lots of other ways to tackle these problems. The best way to dis-\ncover how to update game objects concurrently is to experiment. Hopefully\nwe’ve presented some useful ideas in this section that will pave the way for-\nward as you experiment for yourself!\n16.8 Events and Message-Passing\nGames are inherently event-driven. An eventis anything of interest that hap-\npensduringgameplay. Anexplosiongoingoff, theplayerbeingsightedbyan\nenemy, a health pack getting picked up—these are all events. Games gener-\nallyneedawayto(a)notifyinterestedgameobjectswhenaneventoccursand\n(b)arrangeforthoseobjectstorespondtointerestingeventsinvariousways—\nwe call this handling the event. Different types of game objects will respond\nin different ways to an event. The way in which a particular type of game ob-\nject responds to an event is a crucial aspect of its behavior, just as important\nas how the object’s state changes over time in the absence of any external in-\nputs. For example, the behavior of the ball in Pongis governed in part by its\nvelocity, in part by how it reacts to the event of striking a wall or paddle and\nbouncing off, and in part by what happens when the ball is missed by one of\nthe players.\n16.8.1 The Problem with Statically Typed Function Binding\nOne simple way to notify a game object that an event has occurred is to sim-\nply call a method (member function) on the object. For example, when an\nexplosion goes off, we could query the game world for all objects within\nthe explosion’s damage radius and then call a virtual function named some-\nthing like OnExplosion() on each one. This is illustrated by the following\npseudocode:\nvoid Explosion::Update()\n{\n// ...\nif (ExplosionJustWentOff())\n{\nGameObjectCollection damagedObjects;\n16.8. Events and Message-Passing 1115\ng_world.QueryObjectsInSphere(GetDamageSphere(),\ndamagedObjects );\nfor (each object indamagedObjects)\n{\nobject.OnExplosion(*this);\n}\n}\n// ...\n}\nThe call to OnExplosion() is an example of statically typed late function\nbinding. Function binding is the process of determining which function im-\nplementation to invoke at a particular call location—the implementation is,\nin effect, bound to the call. Virtual functions, such as our OnExplosion()\nevent-handling function, are said to be late-bound. This means that the com-\npiler doesn’t actually know whichof the many possible implementations of\nthe function is going to be invoked at compile time—only at runtime, when\nthe type of the target object is known, will the appropriate implementation\nbe invoked. We say that a virtual function call is statically typed because the\ncompiler doesknow which implementation to invoke given a particular object\ntype. It knows, for example, that Tank::OnExplosion() should be called\nwhen the target object is a Tankand that Crate::OnExplosion() should\nbe called when the object is a Crate.\nThe problem with statically typed function binding is that it introduces\na degree of inflexibility into our implementation. For one thing, the virtual\nOnExplosion() functionrequiresallgameobjectstoinheritfromacommon\nbase class. Moreover, it requires that base class to declarethe virtual function\nOnExplosion(), even if not all game objects can respond to explosions. In\nfact, using statically typed virtual functions as event handlers would require\nour base GameObject class to declare virtual functions for all possible events\nin the game! This would make adding new events to the system difficult. It\nprecludes events from being created in a data-driven manner—for example,\nwithin the world editing tool. It also provides no mechanism for certain types\nof objects, or certain individual object instances, to register interest in certain\nevents but not others. Every object in the game, in effect, “knows” about ev-\nery possible event, even if its response to the event is to do nothing (i.e., to\nimplement an empty, do-nothing event handler function).\nWhat we really need for our event handlers, then, is dynamically typed late\nfunction binding. Some programming languages support this feature natively\n(e.g., C#’s delegates). In other languages, the engineers must implement it\n1116 16. Runtime Gameplay Foundation Systems\nmanually. Therearemanywaystoapproachthisproblem,butmostboildown\nto taking a data-driven approach. In other words, we encapsulate the notion\nof a function call in an object and pass that object around at runtime in order\nto implement a dynamically typed late-bound function call.\n16.8.2 Encapsulating an Event in an Object\nAn event is really comprised of two components: its type(explosion, friend\ninjured, player spotted, health pack picked up, etc.) and its arguments. The\narguments provide specifics about the event. (How much damage did the ex-\nplosion do? Which friend was injured? Where was the player spotted? How\nmuch health was in the health pack?) We can encapsulate these two com-\nponents in an object, as shown by the following rather over-simplified code\nsnippet:\nstruct Event\n{\nconst U32 MAX_ARGS = 8;\nEventType m_type;\nU32 m_numArgs;\nEventArg m_aArgs[MAX_ARGS];\n};\nSomegameenginescallthesethings messages orcommands insteadof events.\nThese names emphasize the idea that informing objects about an event is es-\nsentially equivalent to sending a message or command to those objects.\nPractically speaking, event objects are usually not quite this simple. We\nmight implement different types of events by deriving them from a root event\nclass, for example. The arguments might be implemented as a linked list or a\ndynamically allocated array capable of containing arbitrary numbers of argu-\nments, and the arguments might be of various data types.\nEncapsulating an event (or message) in an object has many benefits:\n•Single event handling function. Because the event object encodes its type\ninternally, any number of different event types can be represented by an\ninstance of a single class (or the root class of an inheritance hierarchy).\nThis means that we only need onevirtual function to handle alltypes of\nevents (e.g., virtual void OnEvent(Event& event);).\n•Persistence. Unlike a function call, whose arguments go out of scope af-\nter the function returns, an event object stores both its type and its argu-\nmentsasdata. Aneventobjectthereforehaspersistence. Itcanbestored\nin a queue for handling at a later time, copied and broadcast to multiple\nreceivers and so on.\n16.8. Events and Message-Passing 1117\n•Blind event forwarding. An object can forward an event that it receives to\nanother object without having to “know” anything about the event. For\nexample, if a vehicle receives a Dismount event, it can forward it to all\nof its passengers, thereby allowing them to dismount the vehicle, even\nthough the vehicle itself knows nothing about dismounting.\nThis idea of encapsulating an event/message/command in an object is\ncommonplaceinmanyfieldsofcomputerscience. Itisfoundnotonlyingame\nenginesbutinothersystemslikegraphicaluserinterfaces,distributedcommu-\nnication systems and many others. The well-known “Gang of Four” design\npatterns book [19] calls this the Command design pattern.\n16.8.3 Event Types\nThere are many ways to distinguish between different types of events. One\nsimple approach in C or C++ is to define a global enum that maps each event\ntype to a unique integer.\nenum EventType\n{\nEVENT_TYPE_LEVEL_STARTED,\nEVENT_TYPE_PLAYER_SPAWNED,\nEVENT_TYPE_ENEMY_SPOTTED,\nEVENT_TYPE_EXPLOSION,\nEVENT_TYPE_BULLET_HIT,\n// ...\n}\nThis approach enjoys the benefits of simplicity and efficiency (since integers\nare usually extremely fast to read, write and compare). However, it also suf-\nfers from two problems. First, knowledge of allevent types in the entire game\nis centralized, which can be seen as a form of broken encapsulation (for better\nor for worse—opinions on this vary). Second, the event types are hard-coded,\nwhich means new event types cannot easily be defined in a data-driven man-\nner. Third,enumeratorsarejustindices,sotheyareorder-dependent. Ifsome-\none accidentally adds a new event type in the middle of the list, the indices\nof all subsequent event ids change, which can cause problems if event ids are\nstoredindatafiles. Assuch,anenumeration-basedeventtypingsystemworks\nwell for small demos and prototypes but does not scale very well at all to real\ngames.\nAnother way to encode event types is via strings. This approach is totally\nfree-form, and it allows a new event type to be added to the system by merely\nthinking up a name for it. But it suffers from many problems, including a\n1118 16. Runtime Gameplay Foundation Systems\nstrong potential for event name conflicts, the possibility of events not work-\ning because of a simple typo, increased memory requirements for the strings\nthemselves, and the relatively high cost of comparing strings relative to that\nof comparing integers. Hashed string ids can be used instead of raw strings\nto eliminate the performance problems and increased memory requirements,\nbuttheydonothingtoaddresseventnameconflictsortypos. Nonetheless,the\nextremeflexibility anddata-driven natureof a string- or string-id-based event\nsystemisconsideredworththerisksbymanygameteams,includingNaughty\nDog.\nTools can be implemented to help avoid some of the risks involved in us-\ningstringstoidentifyevents. Forexample, acentraldatabaseofalleventtype\nnamescouldbemaintained. Auserinterfacecouldbeprovidedtopermitnew\nevent types to be added to the database. Naming conflicts could be automat-\nically detected when a new event is added, and the user could be disallowed\nfrom adding duplicate event types. When selecting a preexisting event, the\ntool could provide a sorted list in a drop-down combo box rather than requir-\ning the user to remember the name and type it manually. The event database\ncould also store metadata about each type of event, including documentation\nabout its purpose and proper usage and information about the number and\ntypes of arguments it supports. This approach can work really well, but we\nshould not forget to account for the costs of setting up and maintaining such\na system, as they are not insignificant.\n16.8.4 Event Arguments\nThe arguments of an event usually act like the argument list of a function,\nproviding information about the event that might be useful to the receiver.\nEvent arguments can be implemented in all sorts of ways.\nWe might derive a new type of Eventclass for each unique type of event.\nThe arguments can then be hard-coded as data members of the class. For ex-\nample:\nclass ExplosionEvent : public Event\n{\nPoint m_center;\nfloat m_damage;\nfloat m_radius;\n};\nAnother approach is to store the event’s arguments as a collection of vari-\nants. Avariantisadataobjectthatiscapableofholdingmorethanonetypeof\ndata. It usually stores information about the data type that is currently being\nstored, as well as the data itself. In an event system, we might want our argu-\n16.8. Events and Message-Passing 1119\nments to be integers, floating-point values, Booleans or hashed string ids. So\nin C or C++, we could define a variant class that looks something like this:\nstruct Variant\n{\nenum Type\n{\nTYPE_INTEGER,\nTYPE_FLOAT,\nTYPE_BOOL,\nTYPE_STRING_ID,\nTYPE_COUNT // number of unique types\n};\nType m_type;\nunion\n{\nI32 m_asInteger;\nF32 m_asFloat;\nbool m_asBool;\nU32 m_asStringId;\n};\n};\nThe collection of variants within an Event might be implemented as an\narray with a small, fixed maximum size (say 4, 8 or 16 elements). This im-\nposes an arbitrary limit on the number of arguments that can be passed with\nan event, but it also side-steps the problems of dynamically allocating mem-\nory for each event’s argument payload, which can be a big benefit, especially\nin memory-constrained console games.\nThe collection of variants might be implemented as a dynamically sized\ndata structure, like a dynamically sized array (like std::vector ) or a linked\nlist (like std::list). This provides a great deal of additional flexibility over\nafixedsizedesign,butitincursthecostofdynamicmemoryallocation. Apool\nallocator could be used to great effect here, presuming that each Variant is\nthe same size.\n16.8.4.1 Event Arguments as Key-Value Pairs\nAfundamentalproblemwithan indexedcollectionofeventargumentsisorder\ndependency. Both the sender and the receiver of an event must “know” that\nthe arguments are listed in a specific order. This can lead to confusion and\nbugs. For example, a required argument might be accidentally omitted or an\nextra one added.\n1120 16. Runtime Gameplay Foundation Systems\nThis problem can be avoided by implementing event arguments as key-\nvalue pairs. Each argument is uniquely identified by its key, so the arguments\ncan appear in any order, and optional arguments can be omitted altogether.\nTheargumentcollectionmightbeimplementedasaclosedoropenhashtable,\nwith the keys used to hash into the table, or it might be an array, linked list or\nbinary search tree of key-value pairs. These ideas are illustrated in Table 16.1.\nThe possibilities are numerous, and the specific choice of implementation is\nlargely unimportant as long as the game’s particular requirements have been\neffectively and efficiently met.\n16.8.5 Event Handlers\nWhen an event, message or command is received by a game object, it needs\nto respond to the event in some way. This is known as handling the event, and\nit is usually implemented by a function or a snippet of script code called an\nevent handler. (We’ll have more to learn about game scripting later on.)\nOften an event handler is a single native virtual function or script func-\ntion that is capable of handling all types of events (e.g., virtual void On-\nEvent(Event& event)). In this case, the function usually contains some\nkind of switch statement or cascaded if/else-if clause to handle the various\ntypesofeventsthatmightbereceived. Atypicaleventhandlerfunctionmight\nlook something like this:\nvirtual void SomeObject:: OnEvent(Event& event)\n{\nswitch (event. GetType())\n{\ncase SID(""EVENT_ATTACK"") :\nRespondToAttack(event.GetAttackInfo());\nbreak;\ncase SID(""EVENT_HEALTH_PACK"") :\nType\nTable 16.1. The arguments of an event object can be implemented as a collection of key-value pairs.\nThe keys prevent order-dependency problems because each event argument is uniquely identiﬁed\nby its key.\n16.8. Events and Message-Passing 1121\nAddHealth(event.GetHealthPack().GetHealth());\nbreak;\n// ...\ndefault:\n// Unrecognized event.\nbreak;\n}\n}\nAlternatively, we might implement a suite of handler functions, one for\neachtypeofevent(e.g., OnThis(), OnThat() ,…). However,aswediscussed\nabove, a proliferation of event handler functions can be problematic.\nA Windows GUI toolkit called Microsoft Foundation Classes (MFC) was\nwell-known for its messagemaps—a system that permitted any Windows mes-\nsage to be bound at runtime to an arbitrary non-virtual or virtual function.\nThis avoided the need to declare handlers for all possible Windows messages\ninasinglerootclass, whileatthesametimeavoidingthebigswitchstatement\nthat is commonplace in non-MFC Windows message-handling functions. But\nsuch a system is probably not worth the hassle—a switch statement works re-\nally well and is simple and clear.\n16.8.6 Unpacking an Event’s Arguments\nThe example above glosses over one important detail—namely, how to ex-\ntract data from the event’s argument list in a type-safe manner. For example,\nevent.GetHealthPack() presumablyreturnsa HealthPack gameobject,\nwhichinturnwepresumeprovidesamemberfunctioncalled GetHealth().\nThisimpliesthattheroot Eventclass“knows”abouthealthpacks(aswellas,\nby extension, every other type of event argument in the game). This is prob-\nably an impractical design. In a real engine, there might be derived Event\nclassesthatprovideconvenientdata-accessAPIssuchas GetHealthPack().\nOr the event handler might have to unpack the data manually and cast them\nto the appropriate types. This latter approach raises type safety concerns, al-\nthough practically speaking it usually isn’t a huge problem because the type\nof the event is always known when the arguments are unpacked.\n16.8.7 Chains of Responsibility\nGameobjectsarealmostalwaysdependentupononeanotherinvariousways.\nForexample,gameobjectsusuallyresideinatransformationhierarchy,which\nallows an object to rest on another object or be held in the hand of a charac-\nter. Game objects might also be made up of multiple interacting components,\n1122 16. Runtime Gameplay Foundation Systems\nleading to a star topology or a loosely connected “cloud” of component ob-\njects. A sports game might maintain a list of all the characters on each team.\nIngeneral,wecanenvisiontheinterrelationshipsbetweengameobjectsasone\nor more relationship graphs (remembering that a list and a tree are just special\ncases of a graph). A few examples of relationship graphs are shown in Fig-\nure 16.21.\nIt often makes sense to be able to pass events from one object to the next\nwithin these relationship graphs. For example, when a vehicle receives an\nevent, it may be convenient to pass the event to all of the passengers riding on\nthevehicle,andthosepassengersmaywishtoforwardtheeventtotheobjects\nintheirinventories. Whenamulticomponentgameobjectreceivesanevent, it\nmaybenecessarytopasstheeventtoallofthecomponentssothattheyallget\na crack at handling it. Or when an event is received by a character in a sports\ngame, we might want to pass it on to all of his or her teammates as well.\nThe technique of forwarding events within a graph of objects is a common\ndesign pattern in object-oriented, event-driven programming, sometimes re-\nferred to as Chain of Responsibility [19]. Usually, the order in which the event\nis passed around the system is predetermined by the engineers. The event is\npassed to the first object in the chain, and the event handler returns a Boolean\nor an enumerated code indicating whether or not it recognized and handled\nthe event. If the event is consumed by a receiver, the process of event for-\nwarding stops; otherwise, the event is forwarded on to the next receiver in\nthe chain. An event handler that supports Chain of Responsibility style event\nforwarding might look something like this:\nAttachment\nGraph\nEvent1\nEvent3Component\nGraph\nEvent2Team\nGraph\nTeamCarter Evan\nQuinn CooperObjectAComponentA2 ComponentA1\nComponentA3Clip Weapon Character Vehicle\nFigure 16.21. Game objects are interrelated in various ways, and we can draw graphs depicting\nthese relationships. Any such graph might serve as a distribution channel for events.\n16.8. Events and Message-Passing 1123\nvirtual bool SomeObject:: OnEvent (Event& event)\n{\n// Call the base class' handler first.\nif ( BaseClass::OnEvent (event))\n{\nreturn true;\n}\n// Now try to handle the event myself.\nswitch (event. GetType())\n{\ncase SID(""EVENT_ATTACK"") :\nRespondToAttack(event.GetAttackInfo());\nreturn false; // OK to forward this event to others.\ncase SID(""EVENT_HEALTH_PACK""):\nAddHealth(event.GetHealthPack().GetHealth());\nreturn true; // I consumed the event; don't forward.\n// ...\ndefault:\nreturn false; // I didn't recognize this event.\n}\n}\nWhen a derived class overrides an event handler, it can be appropriate to\ncall the base class’s implementation as well if the class is augmenting but not\nreplacingthebaseclass’sresponse. Inothersituations,thederivedclassmight\nbe entirely replacing the response of the base class, in which case the base\nclass’s handler should not be called. This is another kind of responsibility\nchain.\nEvent forwarding has other applications as well. For example, we might\nwant to multicast an event to all objects within a radius of influence (for an\nexplosion,forexample). Toimplementthis,wecanleverageourgameworld’s\nobjectquerymechanismtofindallobjectswithintherelevantsphereandthen\nforward the event to all of the returned objects.\n16.8.8 Registering Interest in Events\nIt’s reasonably safe to say that most objects in a game do not need to respond\nto every possible event. Most types of game objects have a relatively small set\nof events in which they are “interested.” This can lead to inefficiencies when\nmulticasting or broadcasting events, because we need to iterate over a group\n1124 16. Runtime Gameplay Foundation Systems\nof objects and call each one’s event handler, even if the object is not interested\nin that particular kind of event.\nOne way to overcome this inefficiency is to permit game objects to regis-\nter interest in particular kinds of events. For example, we could maintain one\nlinked list of interested game objects for each distinct type of event, or each\ngame object could maintain a bit array, in which the setting of each bit corre-\nsponds to whether or not the object is interested in a particular type of event.\nBy doing this, we can avoid calling the event handlers of any objects that do\nnot care about the event.\nEven better, we might be able to restrict our original game object query to\ninclude only those objects that are interested in the event we wish to multi-\ncast. Forexample,whenanexplosiongoesoff,wecanaskthecollisionsystem\nforallobjectsthatarewithinthedamageradius andthatcanrespondtoExplo-\nsionevents. Thiscansavetimeoverall,becauseweavoiditeratingoverobjects\nthatweknowaren’tinterestedintheeventwe’remulticasting. Whetherornot\nsuch an approach will produce a net gain depends on how the query mecha-\nnism is implemented and the relative costs of filtering the objects during the\nquery versus filtering them during the multicast iteration.\n16.8.9 To Queue or Not to Queue\nMost game engines provide a mechanism for handling events immediately\nwhen they are sent. In addition to this, some engines also permit events to\nbe queued for handling at an arbitrary future time. Event queuing has some\nattractivebenefits, butitalsoincreasesthecomplexityoftheeventsystemsig-\nnificantly and poses some unique problems. We’ll investigate the pros and\ncons of event queuing in the following sections and learn how such systems\nare implemented in the process.\n16.8.9.1 Some Beneﬁts of Event Queuing\nThe following sections outline some of the benefits of event queuing.\nControl over When Events Are Handled\nWe have seen that we must be careful to update engine subsystems and game\nobjects in a specific order to ensure correct behavior and maximize runtime\nperformance. In the same sense, certain kinds of events may be highly sen-\nsitive to exactly when within the game loop they are handled. If all events\nare handled immediately upon being sent, the event handler functions end\nup being called in unpredictable and difficult-to-control ways throughout the\ncourse of the game loop. By deferring events via an event queue, the engi-\n16.8. Events and Message-Passing 1125\nneers can take steps to ensure that events are only handled when it is safe and\nappropriate to do so.\nAbility to Post Events into the Future\nWhen an event is sent, the sender can usually specify a delivery time—for\nexample, we might want the event to be handled later in the same frame, next\nframe or some number of seconds after it was sent. This feature amounts to\nan ability to post events into the future, and it has all sorts of interesting uses.\nWecan implementa simplealarm clockby posting anevent intothe future. A\nperiodic task, such as blinking a light every two seconds, can be executed by\nposting an event whose handler performs the periodic task and then posts a\nnew event of the same type one time period into the future.\nTo implement the ability to post events into the future, each event is\nstamped with a desired delivery time prior to being queued. An event is only\nhandled when the current game clock matches or exceeds its delivery time.\nAn easy way to make this work is to sort the events in the queue in order of\nincreasing delivery time. Each frame, the first event on the queue can be in-\nspected and its delivery time checked. If the delivery time is in the future, we\nabort immediately because we know that all subsequent events are also in the\nfuture. But if we see an event whose delivery time is now or in the past, we\nextract it from the queue and handle it. This continues until an event is found\nwhosedeliverytimeisinthefuture. Thefollowingpseudocodeillustratesthis\nprocess:\n// This function is called at least once per frame. Its\n// job is to dispatch all events whose delivery time is\n// now or in the past.\nvoid EventQueue:: DispatchEvents(F32 currentTime)\n{\n// Look at, but don't remove, the next event on the\n// queue.\nEvent* pEvent = PeekNextEvent ();\nwhile ( pEvent\n&&pEvent->GetDeliveryTime() <= currentTime )\n{\n// Remove the event from the queue.\nRemoveNextEvent ();\n// Dispatch it to its receiver's event handler.\npEvent->Dispatch();\n// Peek at the next event on the queue (again\n1126 16. Runtime Gameplay Foundation Systems\n// without removing it).\npEvent = PeekNextEvent ();\n}\n}\nEvent Prioritization\nEven if our events are sorted by delivery time in the event queue, the order\nof delivery is still ambiguous when two or more events have exactly the same\ndelivery time. This can happen more often than you might think, because it is\nquitecommonforevents’deliverytimestobequantizedtoanintegralnumber\nof frames. For example, if two senders request that events be dispatched “this\nframe,” “next frame” or “in seven frames from now,” then those events will\nhave identical delivery times.\nOnewaytoresolvetheseambiguitiesistoassign priorities toevents. When-\nevertwoeventshavethesametimestamp,theonewithhigherpriorityshould\nalways be serviced first. This is easily accomplished by first sorting the event\nqueuebyincreasingdeliverytimesandthensortingeachgroupofeventswith\nidentical delivery times in order of decreasing priority.\nWe could allow up to four billion unique priority levels by encoding our\npriorities in a raw, unsigned 32-bit integer, or we could limit ourselves to only\ntwo or three unique priority levels (e.g., low, medium and high). In every\ngame engine, there exists some minimum number of priority levels that will\nresolve all real ambiguities in the system. It’s usually best to aim as close to\nthis minimum as possible. With a very large number of priority levels, it can\nbecome a small nightmare to figure out which event will be handled first in\nany given situation. However, the needs of every game’s event system are\ndifferent, and your mileage may vary.\n16.8.9.2 Some Problems with Event Queuing\nIncreased Event System Complexity\nIn order to implement a queued event system, we need more code, additional\ndata structures and more complex algorithms than would be necessary to im-\nplement an immediate event system. Increased complexity usually translates\ninto longer development times and a higher cost to maintain and evolve the\nsystem during development of the game.\nDeep-Copying Events and Their Arguments\nWithanimmediateeventhandlingapproach,thedatainanevent’sarguments\nneed only persist for the duration of the event handling function (and any\n16.8. Events and Message-Passing 1127\nfunctions it may call). This means that the event and its argument data can\nreside literally anywhere in memory, including on the call stack. For example,\nwe could write a function that looks something like this:\nvoid SendExplosionEventToObject(GameObject& receiver)\n{\n// Allocate event args on the call stack.\nPoint centerPoint(-2.0f, 31.5f, 10.0f);\nF32 damage = 5.0f;\nF32 radius = 2.0f;\n// Allocate the event on the call stack.\nEvent event(""Explosion"");\nevent.SetArgFloat(""Damage"", damage);\nevent.SetArgPoint(""Center"", &centerPoint);\nevent.SetArgFloat(""Radius"", radius);\n// Send the event, which causes the receiver's event\n// handler to be called immediately, as shown below.\nevent.Send(receiver);\n//{\n// receiver.OnEvent(event);\n//}\n}\nWhen an event is queued, its arguments must persist beyond the scope of\nthe sending function. This implies that we must copy the entire event object\npriortostoringtheeventinthequeue. Wemustperforma deep-copy, meaning\nthat we copy not only the event object itself but its entire argument payload\nas well, including any data to which it may be pointing. Deep-copying the\neventensuresthattherearenodanglingreferencestodatathatexistonlyinthe\nsendingfunction’sscope,anditpermitstheeventtobestoredindefinitely. The\nexample event-sending function shown above still looks basically the same\nwhen using a queued event system, but as you can see in the italicized code\nbelow, the implementation of the Event::Queue() function is a bit more\ncomplex than its Send() counterpart:\nvoid SendExplosionEventToObject(GameObject& receiver)\n{\n// We can still allocate event args on the call\n// stack.\nPoint centerPoint(-2.0f, 31.5f, 10.0f);\nF32 damage = 5.0f;\nF32 radius = 2.0f;\n1128 16. Runtime Gameplay Foundation Systems\n// Still OK to allocate the event on the call stack\nEvent event(""Explosion\nevent.SetArgFloat(""Damage"", damage);\nevent.SetArgPoint(""Center"", &centerPoint);\nevent.SetArgFloat(""Radius"", radius);\n// This stores the event in the receiver's queue for\n// handling at a future time. Note how the event\n// must be deep-copied prior to being enqueued, since\n// the original event resides on the call stack and\n// will go out of scope when this function returns.\nevent.Queue(receiver);\n//{\n// Event* pEventCopy = DeepCopy (event);\n// receiver.EnqueueEvent (pEventCopy);\n//}\n}\nDynamic Memory Allocation for Queued Events\nDeep-copyingofeventobjectsimpliesaneedfordynamicmemoryallocation,\nand as we’ve already noted many times, dynamic allocation is undesirable in\na game engine due to its potential cost and its tendency to fragment memory.\nNonetheless, if we want to queue events, we’ll need to dynamically allocate\nmemory for them.\nAs with all dynamic allocation in a game engine, it’s best if we can select a\nfast and fragmentation-free allocator. We might be able to use a pool allocator,\nbut this will only work if all of our event objects are the same size and if their\nargumentlistsarecomprisedofdataelementsthatarethemselvesallthesame\nsize. This may well be the case—for example, the arguments might each be a\nVariant , as described above. If our event objects and/or their arguments\ncan vary in size, a small memory allocator might be applicable. (Recall that a\nsmall memory allocator maintains multiple pools, one for each of a few pre-\ndetermined small allocation sizes.) When designing a queued event system,\nalways be careful to take dynamic allocation requirements into account.\nOther designs are possible, of course. For example, at Naughty Dog we\nallocate queued events as relocatable memory blocks. See Section 6.2.2.2 for\nmore information on relocatable memory.\nDebugging Difﬁculties\nWith queued events, the event handler is not called directly by the sender of\nthat event. So, unlike in immediate event handling, the call stack does not\ntell us where the event came from. We cannot walk up the call stack in the\ndebugger to inspect the state of the sender or the circumstances under which\nthe event was sent. This can make debugging deferred events a bit tricky, and\n16.8. Events and Message-Passing 1129\nthings get even more difficult when events are forwarded from one object to\nanother.\nSome engines store debugging information that forms a paper trail of the\nevent’s travels throughout the system, but no matter how you slice it, event\ndebugging is usually much easier in the absence of queuing.\nEvent queuing also leads to interesting and hard-to-track-down racecondi-\ntionbugs. We may need to pepper multiple event dispatches throughout our\ngame loop, to ensure that events are delivered without incurring unwanted\none-framedelaysyetstillensuringthatgameobjectsareupdatedintheproper\norder during the frame. For example, during the animation update, we might\ndetect that a particular animation has run to completion. This might cause an\nevent to be sent whose handler wants to play a new animation. Clearly, we\nwanttoavoidaone-framedelaybetweentheendofthefirstanimationandthe\nstart of the next. To make this work, we need to update animation clocks first\n(so that the end of the animation can be detected and the event sent); then we\nshoulddispatchevents(sothattheeventhandlerhasachancetorequestanew\nanimation),andfinallywecanstartanimationblending(sothatthefirstframe\nof the new animation can be processed and displayed). This is illustrated in\nthe code snippet below:\nwhile (true) // main game loop\n{\n// ...\n// Update animation clocks. This may detect the end\n// of a clip, and cause EndOfAnimation events to\n// be sent.\ng_animationEngine. UpdateLocalClocks(dt);\n// Next, dispatch events. This allows an\n// EndOfAnimation event handler to start up a new\n// animation this frame if desired.\ng_eventSystem. DispatchEvents();\n// Finally, start blending all currently playing\n// animations (including any new clips started\n// earlier this frame).\ng_animationEngine. StartAnimationBlending();\n// ...\n}\n1130 16. Runtime Gameplay Foundation Systems\n16.8.10 Some Problems with Immediate Event Sending\nNotqueuingeventsalsohasitsshareofissues. Forexample, immediateevent\nhandling can lead to extremely deep call stacks. Object A might send object B\nan event, and in its event handler, B might send another event, which might\nsend another event, and another and so on. In a game engine that supports\nimmediate event handling, it’s not uncommon to see a call stack that looks\nsomething like this:\n...\nShoulderAngel:: OnEvent()\nEvent::Send()\nCharacer::OnEvent()\nEvent::Send()\nCar::OnEvent()\nEvent::Send()\nHandleSoundEffect()\nAnimationEngine::PlayAnimation()\nEvent::Send()\nCharacter::OnEvent()\nEvent::Send()\nCharacter::OnEvent()\nEvent::Send()\nCharacter::OnEvent()\nEvent::Send()\nCar::OnEvent()\nEvent::Send()\nCar::OnEvent()\nEvent::Send()\nCar::Update()\nGameWorld::UpdateObjectsInBucket()\nEngine::GameLoop()\nmain()\nAdeepcallstacklikethiscanexhaustavailablestackspaceinextremecases\n(especiallyifwehaveaninfiniteloopofeventsending),buttherealcruxofthe\nproblem here is that every event handler function must be written to be fully\nre-entrant . Thismeansthattheeventhandlercanbecalledrecursivelywithout\nanyillside-effects. Asacontrivedexample,imagineafunctionthatincrements\nthevalueofaglobalvariable. Iftheglobalissupposedtobeincrementedonly\nonceperframe, thenthisfunctionis notre-entrant, becausemultiplerecursive\ncalls to the function will increment the variable multiple times.\n16.8. Events and Message-Passing 1131\n16.8.11 Data-Driven Event/Message-Passing Systems\nEvent systems give the game programmer a great deal of flexibility over and\nabove what can be accomplished with the statically typed function calling\nmechanisms provided by languages like C and C++. However, we can do\nbetter. In our discussions thus far, the logic for sending and receiving events\nis still hard-coded and therefore under the exclusive control of the engineers.\nIf we could make our event system data-driven, we could extend its power\ninto the hands of our game designers.\nThere are many ways to make an event system data-driven. Starting with\nthe extreme of an entirely hard-coded (non-data-driven) event system, we\ncould imagine providing some simple data-driven configurability. For exam-\nple, designers might be allowed to configure how individual objects, or entire\nclassesofobject,respondtocertainevents. Intheworldeditor,wecanimagine\nselecting an object and then bringing up a scrolling list of all possible events\nthat it might receive. For each one, the designer could use drop-down combo\nboxes and check boxes to control if, and how, the object responds, by select-\ningfromasetofhard-coded,predefinedchoices. Forexample,giventheevent\n“PlayerSpotted,”AI-controlledcharactersmightbeconfiguredtodooneofthe\nfollowing actions: run away, attack or ignore the event altogether. The event\nsystemsofsomerealcommercialgameenginesareimplementedinessentially\nthis way.\nAt the other end of the gamut, our engine might provide the game design-\ners with a simple scripting language (a topic we’ll explore in detail in Section\n16.9). In this case, the designer can literally write code that defines how a\nparticular kind of game object will respond to a particular kind of event. In\na scripted model, the designers are really just programmers (working with a\nsomewhat less powerful but also easier-to-use and hopefully less error-prone\nlanguage than the engineers), so anything is possible. Designers might define\nnew types of events, send events and receive and handle events in arbitrary\nways. This is what we do at Naughty Dog.\nTheproblemwithasimple,configurableeventsystemisthatitcanseverely\nlimit what the game designers are capable of doing on their own, without the\nhelp of a programmer. On the other hand, a fully scripted solution has its\nown share of problems: Many game designers are not professional software\nengineers by training, so some designers find learning and using a scripting\nlanguage a daunting task. Designers are also probably more prone to intro-\nducing bugs into the game than their engineer counterparts, unless they have\npracticedscriptingorprogrammingforsometime. Thiscanleadtosomenasty\nsurprises during alpha.\n1132 16. Runtime Gameplay Foundation Systems\nAsaresult, somegameenginesaimforamiddleground. Theyemployso-\nphisticated graphical user interfaces to provide a great deal of flexibility with-\nout going so far as to provide users with a full-fledged, free-form scripting\nlanguage. One approach is to provide a flowchart-style graphical program-\nming language. The idea behind such a system is to provide the user with a\nlimitedandcontrolledsetof atomicoperationsfromwhichtochoose butwith\nplentyoffreedomtowirethemupinarbitraryways. Forexample,inresponse\nto an event like “PlayerSpotted,” the designer could wire up a flowchart that\ncauses a character to retreat to the nearest cover point, play an animation,\nwait 5 seconds, and then attack. A GUI can also provide error-checking and\nvalidation to help ensure that bugs aren’t inadvertently introduced. Unreal’s\nBlueprints is an example of such a system—see the following section for more\ndetails.\n16.8.11.1 Data Pathway Communication Systems\nOne of the problems with converting a function-call-like event system into\na data-driven system is that different types of events tend to be incompati-\nble. For example, let’s imagine a game in which the player has an electro-\nmagnetic pulse gun. This pulse causes lights and electronic devices to turn\noff, scares small animals and produces a shock wave that causes any nearby\nplants to sway. Each of these game object types may already have an event\nresponse that performs the desired behavior. A small animal might respond\nto the “Scare” event by scurrying away. An electronic device might respond\nto the “TurnOff” event by turning itself off. And plants might have an event\nhandler for a “Wind” event that causes them to sway. The problem is that our\nEMP gun is not compatible with any of these objects’ event handlers. As a re-\nsult,weenduphavingtoimplementaneweventtype,perhapscalled“EMP,”\nand then write custom event handlers for every type of game object in order\nto respond to it.\nOne solution to this problem is to take the event type out of the equation\nand to think solely in terms of sending streams of data from one game object to\nanother. In such a system, every game object has one or more input ports to\nwhich a data stream can be connected, and one or more output ports through\nwhichdatacanbesenttootherobjects. Providedwehavesomewayofwiring\nthese ports together, such as a graphical user interface in which ports can be\nconnected to each other via rubber-band lines, then we can construct arbitrar-\nily complex behaviors. Continuing our example, the EMP gun would have\nan output port, perhaps named “Fire,” that sends a Boolean signal. Most of\nthe time, the port produces the value 0 (false), but when the gun is fired, it\nsendsabrief (one-frame)pulseof thevalue1 (true). The othergameobjects in\n16.8. Events and Message-Passing 1133\nAnimalScare\nFoliageSwayRadioTurnOn InvertIn Out EMP GunFire\nFigure 16.22. The EMP gun produces a 1 at its “Fire” output when ﬁred. This can be connected to\nany input port that expects a Boolean value, in order to trigger the behavior associated with that\ninput.\ntheworldhavebinaryinputportsthattriggervariousresponses. Theanimals\nmight have a “Scare” input, the electronic devices a “TurnOn” input and the\nfoliageobjectsa“Sway”input. IfweconnecttheEMPgun’s“Fire”outputport\ntotheinputportsofthesegameobjects,wecancausetheguntotriggerthede-\nsired behaviors. (Note that we’d have to pipe the gun’s “Fire” output through\nanodethat invertsitsinput,priortoconnectingittothe“TurnOn”inputofthe\nelectronic devices. This is because we want them to turn offwhen the gun is\nfiring.) The wiring diagram for this example is shown in Figure 16.22.\nProgrammers decide what kinds of port(s) each type of game object will\nhave. Designers using the GUI can then wire these ports together in arbitrary\nways in order to construct arbitrary behaviors in the game. The programmers\nalso provide various other kinds of nodes for use within the graph, such as a\nnode that inverts its input, a node that produces a sine wave or a node that\noutputs the current game time in seconds.\nVarious types of data might be sent along a data pathway. Some ports\nmight produce or expect Boolean data, while others might be coded to pro-\nduceorexpectdataintheformofaunitfloat. Stillothersmightoperateon3D\nvectors, colors, integers and so on. It’s important in such a system to ensure\nthat connections are only made between ports with compatible data types, or\nwe must provide some mechanism for automatically converting data types\nwhen two differently typed ports are connected together. For example, con-\nnecting a unit-float output to a Boolean input might automatically cause any\nvaluelessthan0.5tobeconvertedtofalse,andanyvaluegreaterthanorequal\nto 0.5 to be converted to true. This is the essence of GUI-based event systems\nlike Unreal Engine 4’s Blueprints. A screenshot of Blueprints is shown in Fig-\nure 16.23.",42960
115-16.9 Scripting.pdf,115-16.9 Scripting,"1134 16. Runtime Gameplay Foundation Systems\nFigure 16.23. Unreal Engine 4’s Blueprints.\n16.8.11.2 Some Pros and Cons of GUI-Based Programming\nThe benefits of a graphical user interface over a straightforward, text-file-\nbased scripting language are probably pretty obvious: ease of use, a grad-\nual learning curve with the potential for in-tool help and tool tips to guide\nthe user, and plenty of error-checking. The downsides of a flowchart style\nGUI include the high cost to develop, debug, and maintain such a system, the\nadditional complexity, which can lead to annoying or sometimes schedule-\nkillingbugs,andthefactthatdesignersaresometimeslimitedinwhattheycan\ndo with the tool. A text-file-based programming language has some distinct\nadvantagesoveraGUI-basedprogrammingsystem,includingitsrelativesim-\nplicity (meaning that it is much less prone to bugs), the ability to easily search\nandreplacewithinthesourcecode,andthefreedomofeachusertochoosethe\ntext editor with which they are most comfortable.\n16.9 Scripting\nAscriptinglanguage canbedefinedasaprogramminglanguagewhoseprimary\npurposeistopermitusers tocontrolandcustomizethebehaviorof asoftware\napplication. For example, the Visual Basic language can be used to customize\nthebehaviorofMicrosoftExcel; bothMELlanguageandPythonlanguagecan\nbe used to customize the behavior of Maya. In the context of game engines, a\n16.9. Scripting 1135\nscriptinglanguage isahigh-level,relativelyeasy-to-useprogramminglanguage\nthat provides its users with convenient access to most of the commonly used\nfeatures of the engine. As such, a scripting language can be used by program-\nmers and non-programmers alike to develop a new game or to customize—or\n“mod”—an existing game.\n16.9.1 Runtime versus Data Deﬁnition\nWe should be careful to make an important distinction here. Game scripting\nlanguages generally come in two flavors:\n•Data-definition languages. The primary purpose of a data-definition lan-\nguage is to permit users to create and populate data structures that are\nlater consumed by the engine. Such languages are often declarative (see\nbelow) and are either executed or parsed offline or at runtime when the\ndata is loaded into memory.\n•Runtime scripting language. Runtime scripting languages are intended\nto be executed within the context of the engine at runtime. These\nlanguages are usually used to extend or customize the hard-coded\nfunctionality of the engine’s game object model and/or other engine\nsystems.\nInthissection,we’llfocusprimarilyonusingaruntimescriptinglanguage\nfor the purpose of implementing gameplay features by extending and cus-\ntomizing the game’s object model.\n16.9.2 Programming Language Characteristics\nInourdiscussionofscriptinglanguages,itwillbehelpfulforusalltobeonthe\nsame page with regard to programming language terminology. There are all\nsorts of programming languages out there, but they can be classified approx-\nimately according to a relatively small number of criteria. Let’s take a brief\nlook at these criteria:\n•Interpreted versus compiled languages. The source code of a compiled lan-\nguage is translated by a program called a compiler into machine code,\nwhich can be executed directly by the CPU. In contrast, the source code\nof aninterpreted language is either parsed directly at runtime or is pre-\ncompiled into platform-independent byte code, which is then executed\nby avirtualmachine (VM) at runtime. A virtual machine acts like an em-\nulation of an imaginary CPU, and byte code acts like a list of machine\n1136 16. Runtime Gameplay Foundation Systems\nlanguage instructions that are consumed by this virtual CPU. The bene-\nfit of a virtual machine is that it can be quite easily ported to almost any\nhardware platform and embedded within a host application like a game\nengine. The biggest cost we pay for this flexibility is execution speed—a\nvirtual machine usually executes its byte code instructions much more\nslowly than the native CPU executes its machine language instructions.\n•Imperative languages. In an imperative language, a program is described\nbyasequence ofinstructions,eachofwhichperformsanoperationand/\nor changes the state of data in memory. C and C++ are imperative lan-\nguages.\n•Declarativelanguages. Adeclarativelanguagedescribes whatistobedone\nbut does not specify exactly howthe result should be obtained. That de-\ncision is left up to the people implementing the language. Prolog is an\nexample of a declarative language. Mark-up languages like HTML and\nTeX can also be classified as declarative languages.\n•Functional languages. Functional languages, which are technically a sub-\nset of declarative languages, aim to avoid state altogether. In a func-\ntional language, programs are defined by a collection of functions. Each\nfunction produces its results with no side-effects (i.e., it causes no ob-\nservable changes to the system, other than to produce its output data).\nAprogramisconstructedbypassinginputdatafromonefunctiontothe\nnext until the final desired result has been generated. These languages\ntend to be well-suited to implementing data processing pipelines. They\nalso offer distinct advantages when implementing multithreaded appli-\ncations, because with no mutable state, a functional language requires\nno mutex locking. OCaml, Haskell and F# are examples of functional\nlanguages.\n•Procedural versus object-oriented languages. In a procedural language, the\nprimary atom of program construction is the procedure (orfunction).\nThese pr ocedures and functions perform operations, calculate results\nand/or change the state of various data structures in memory. In con-\nstrast, an object-oriented language’s primary unit of program construc-\ntion is the class, a data structure that is tightly coupled with a set of pro-\ncedures/functionsthat “know”howtomanageandmanipulatethedata\nwithin that data structure.\n•Reflective languages. In a reflective language, information about the data\ntypes, data member layouts, functions and hierarchical class relation-\nships in the system is available for inspection at runtime. In a non-\nreflective language, the majority of this meta-information is known only\n16.9. Scripting 1137\nat compile time; only a very limited amount of it is exposed to the run-\ntime code. C# is an example of a reflective language, while C and C++\nare examples of non-reflective languages.\n16.9.2.1 Typical Characteristics of Game Scripting Languages\nThe characteristics that set a game scripting language apart from its nativepro-\ngramming language brethren include:\n•Interpreted. Most game scripting languages are interpreted by a virtual\nmachine, not compiled. This choice is made in the interest of flexibility,\nportability and rapid iteration (see below). When code is represented as\nplatform-independent byte code, it can easily be treated like data by the\nengine. Itcanbeloadedintomemoryjustlikeanyotherassetratherthan\nrequiring help from the operating system (as is necessary with a DLL on\na PC platform or a PRX on the PlayStation 3, for example). Because the\ncode is executed by a virtual machine rather than directly by the CPU,\nthegameengineisaffordedagreatdealofflexibilityregardinghowand\nwhen script code will be run.\n•Lightweight . Most game scripting languages have been designed for use\nin an embedded system. As such, their virtual machines tend to be sim-\nple, and their memory footprints tend to be quite small.\n•Supportforrapiditeration. Whenevernativecodeischanged,theprogram\nmust be recompiled and relinked, and the game must be shut down and\nrerun in order to see the effects of the changes (unless your develop-\nment environment supports some form of edit-and-continue). On the\nother hand, when script code is changed, the effects of the changes can\nusually be seen very rapidly. Some game engines permit script code\nto be reloaded on the fly, without shutting down the game at all. Oth-\ners require the game to be shut down and rerun. But either way, the\nturnaroundtimebetweenmakingachangeandseeingitseffectsin-game\nisusuallymuchfasterthanwhenmakingchangestothenativelanguage\nsource code.\n•Convenience and ease of use. Scripting languages are often customized to\nsuit the needs of a particular game. Features can be provided that make\ncommon tasks simple, intuitive and less error-prone. For example, a\ngame scripting language might provide functions or custom syntax for\nfinding game objects by name, sending and handling events, pausing or\nmanipulatingthepassageoftime,waitingforaspecifiedamountoftime\ntopass, implementingfinitestatemachines, exposingtweakableparam-\n1138 16. Runtime Gameplay Foundation Systems\neterstotheworldeditorforusebythegamedesigners,orevenhandling\nnetwork replication for multiplayer games.\n16.9.3 Some Common Game Scripting Languages\nWhen implementing a runtime game scripting system, we have one funda-\nmental choice to make: Do we select a third-party commercial or open source\nlanguage and customize it to suit our needs, or do we design and implement\na custom language from scratch?\nCreating a custom language from scratch is usually not worth the hassle\nand the cost of maintenance throughout the project. It can also be difficult or\nimpossibletohiregamedesignersandprogrammerswhoarealreadyfamiliar\nwith a custom, in-house language, so there’s usually a training cost as well.\nHowever,thisisclearlythemostflexibleandcustomizableapproach,andthat\nflexibility can be worth the investment.\nFor many studios, it is more convenient to select a reasonably well-known\nand mature scripting language and extend it with features specific to your\ngame engine. There are a great many third-party scripting languages from\nwhichtochoose,andmanyarematureandrobust,havingbeenusedinagreat\nmany projects both within and outside the game industry.\nIn the following sections, we’ll explore a number of custom game script-\ning languages and a number of game-agnostic languages that are commonly\nadapted for use in game engines.\n16.9.3.1 QuakeC\nId Software’s John Carmack implemented a custom scripting language for\nQuake,knownas QuakeC (QC).Thislanguage wasessentiallyasimplifiedvari-\nant of the C programming language with direct hooks into the Quakeengine.\nIt had no support for pointers or defining arbitrary structs, but it could ma-\nnipulate entities(Quake’s name for game objects) in a convenient manner, and\nit could be used to send and receive/handle game events. QuakeC is an inter-\npreted, imperative, procedural programming language.\nThe power that QuakeC put into the hands of gamers is one of the fac-\ntors that gave birth to what is now known as the mod community . Scripting\nlanguagesandotherformsofdata-drivencustomizationallowgamerstoturn\nmanycommercialgamesintoallsortsofnewgamingexperiences—fromslight\nmodifications on the original theme to entirely new games.\n16.9.3.2 UnrealScript\nProbably the best-known example of an entirely custom scripting language is\nUnreal Engine’s UnrealScript. This language is based on a C++-like syntacti-\n16.9. Scripting 1139\ncal style, and it supports most of the concepts that C and C++ programmers\nhavebecomeaccustomedto,includingclasses,localvariables,looping,arrays\nand structs for data organization, strings, hashed string ids (called FNamein\nUnreal) and object references (but not free-form pointers). UnrealScript is an\ninterpreted, imperative, object-oriented language.\nEpic no longer supports the UnrealScript language. Instead, developers\ncustomize game behavior either via the Blueprints graphical “scripting” sys-\ntem, or by writing C++ code.\n16.9.3.3 Lua\nLua is a well-known and popular scripting language that is easy to integrate\ninto an application such as a game engine. The Lua website (http://www.lua.\norg/about.html)callsthelanguagethe“leadingscriptinglanguageingames.”\nAccording to the Lua website, Lua’s key benefits are:\n•Robust and mature . Lua has been used on numerous commercial prod-\nucts,includingAdobe’s PhotoshopLightroom ,andmanygames,including\nWorldof Warcraft .\n•Good documentation . Lua’s reference manual [25] is complete and un-\nderstandable and is available online and in book formats. A number\nof books have been written about Lua, including [26] and [50].\n•Excellent runtime performance. Lua executes its byte code more quickly\nand efficiently than many other scripting languages.\n•Portable . Out of the box, Lua runs on all flavors of Windows and UNIX,\nmobile devices and embedded microprocessors. Lua is written in a\nportable manner, making it easy to adapt to new hardware platforms.\n•Designedforembeddedsystems. Lua’s memory footprint is very small (ap-\nproximately 350 KiB for the interpreter and all libraries).\n•Simple, powerful and extensible . The core Lua language is very small and\nsimple, but it is designed to support meta-mechanisms that extend its\ncore functionality in virtually limitless ways. For example, Lua itself\nis not an object-oriented language, but OOP support can and has been\nadded via a meta-mechanism.\n•Free. Lua is open source and is distributed under the very liberal MIT\nlicense.\nLua is a dynamically typed language, meaning that variables don’t have\ntypes—onlyvaluesdo. (Everyvaluecarriesitstypeinformationalongwithit.)\nLua’sprimarydatastructureisthe table, alsoknownasanassociativearray. A\n1140 16. Runtime Gameplay Foundation Systems\ntable is essentially a list of key-value pairs with an optimized ability to index\ninto the array by key.\nLua provides a convenient interface to the C language—the Lua virtual\nmachinecancallandmanipulatefunctionswritteninCaseasilyasitcanthose\nwritten in Lua itself.\nLua treats blocks of code, called chunks, as first-class objects that can be\nmanipulated by the Lua program itself. Code can be executed in source code\nformat or in precompiled byte code format. This allows the virtual machine\nto execute a string that contains Lua code, just as if the code were compiled\ninto the original program. Lua also supports some powerful advanced pro-\ngramming constructs, including coroutines . This is a simple form of coopera-\ntive multitasking, in which each thread must yield the CPU to other threads\nexplicitly(ratherthanbeingtime-slicedasinapreemptivemultithreadingsys-\ntem).\nLua does have some pitfalls. For example, its flexible function binding\nmechanism makes it possible (and quite easy) to redefine an important global\nfunction like sin()to perform a totally different task (which is usually not\nsomething one intends to do). But all in all, Lua has proven itself to be an\nexcellent choice for use as a game scripting language.\n16.9.3.4 Python\nPythonisaprocedural,object-oriented,dynamicallytypedscriptinglanguage\ndesigned with ease of use, integration with other programming languages,\nand flexibility in mind. Like Lua, Python is a common choice for use as a\ngame scripting language. According to the official Python website (http://\nwww.python.org), some of Python’s best features include:\n•Clear and readable syntax . Python code is easy to read, in part because\nthe syntax enforces a specific indentation style. (It actually parses the\nwhitespace used for indentation in order to determine the scope of each\nline of code.)\n•Reflective language. Python includes powerful runtime introspection fa-\ncilities. Classes in Python are first-class objects, meaning they can be\nmanipulated and queried at runtime, just like any other object.\n•Object-oriented. One advantage of Python over Lua is that OOP is built\ninto the core language. This makes integrating Python with a game’s\nobject model a little easier.\n•Modular. Pythonsupportshierarchicalpackages,encouragingcleansys-\ntem design and good encapsulation.\n16.9. Scripting 1141\n•Exception-based error handling. Exceptions make error-handling code in\nPython simpler, more elegant and more localized than similar code in a\nnon-exception-based language.\n•Extensive standard libraries and third-party modules. Python libraries exist\nfor virtually every task imaginable. (Really!)\n•Embeddable. Python can be easily embedded into an application, such as\na game engine.\n•Extensive documentation. There’s plenty of documentation and tutorials\non Python, both online and in book form. A good place to start is the\nPython website, http://www.python.org.\nPython syntax is reminiscent of C in many respects (for example, its use\nof the =operator for assignment and ==for equality testing). However, in\nPython,codeindentation servesastheonlymeansofdefining scope(asopposed\nto C’s opening and closing braces). Python’s primary data structures are the\nlist—a linearly indexed sequence of atomic values or other nested lists—and\nthedictionary —a table of key-value pairs. Each of these two data structures\ncanholdinstancesoftheother,allowingarbitrarilycomplexdatastructuresto\nbeconstructedeasily. Inaddition, classes—unifiedcollectionsofdataelements\nand functions—are built right into the language.\nPython supports duck typing, which is a style of dynamic typing in which\nthe functional interface of an object determines its type (rather than being de-\nfinedbyastaticinheritancehierarchy). Inotherwords,anyclassthatsupports\naparticularinterface(i.e.,acollectionoffunctionswithspecificsignatures)can\nbe used interchangeably with any other class that supports that same inter-\nface. This is a powerful paradigm: In effect, Python supports polymorphism\nwithout requiring the use of inheritance. Duck typing is similar in some re-\nspects to C++ template metaprogramming, although it is arguably more flexi-\nble because the bindings between caller and callee are formed dynamically, at\nruntime. Duck typing gets its name from the well-known phrase (attributed\nto James Whitcomb Riley), “If it walks like a duck and quacks like a duck,\nI would call it a duck.” See http://en.wikipedia.org/wiki/Duck_typing for\nmore information on duck typing.\nIn summary, Python is easy to use and learn, embeds easily into a game\nengine, integrates well with a game’s object model, and can be an excellent\nand powerful choice as a game scripting language.\n16.9.3.5 Pawn/Small/Small-C\nPawnis a lightweight, dynamically typed, C-like scripting language created\nby Marc Peter. The language was formerly known as Small, which itself was\n1142 16. Runtime Gameplay Foundation Systems\nan evolution of an earlier subset of the C language called Small-C, written by\nRon Cain and James Hendrix. It is an interpreted language—the source code\nis compiled into byte code (also known as P-code), which is interpreted by a\nvirtual machine at runtime.\nPawn was designed to have a small memory footprint and to execute its\nbyte code very quickly. Unlike C, Pawn’s variables are dynamically typed.\nPawn also supports finite state machines, including state-local variables. This\nunique feature makes it a good fit for many game applications. Good online\ndocumentation is available for Pawn (http://www.compuphase.com/pawn/\npawn.htm). Pawn is open source and can be used free of charge under the\nZlib/libpng license (http://www.opensource.org/licenses/zlib-license.php).\nPawn’s C-like syntax makes it easy to learn for any C/C++ programmer\nand easy to integrate with a game engine written in C. Its finite state machine\nsupport can be very useful for game programming. It has been used success-\nfully on a number of game projects, including FreakyFlyers by Midway. Pawn\nhas shown itself to be a viable game scripting language.\n16.9.4 Architectures for Scripting\nScript code can play all sorts of roles within a game engine. There’s a gamut\nof possible architectures, from tiny snippets of script code that perform sim-\nple functions on behalf of an object or engine system to high-level scripts that\nmanage the operation of the game. Here are just a few of the possible archi-\ntectures:\n•Scripted callbacks. In this approach, the engine’s functionality is largely\nhard-coded in the native programming language, but certain key bits\nof functionality are designed to be customizable. This is often imple-\nmented via a hook function orcallback—a user-supplied function that is\ncalled by the engine for the purpose of allowing customization. Hook\nfunctions can be written in the native language, of course, but they can\nalso be written in a scripting language. For example, when updating\ngame objects during the game loop, the engine might call an optional\ncallback function that can be written in script. This gives users the op-\nportunity to customize the way in which the game object updates itself\nover time.\n•Scripted event handler. An event handler is really just a special type of\nhook function whose purpose is to allow a game object to respond to\nsome relevant occurrence within the game world (e.g., responding to\nan explosion going off) or within the engine itself (e.g., responding to\n16.9. Scripting 1143\nan out-of-memory condition). Many game engines allow users to write\nevent handler hooks in script as well as in the native language.\n•Extendinggameobjecttypes,ordefiningnewones,withscript. Somescripting\nlanguages allow game object types that have been implemented in the\nnative language to be extended via script. In fact, callbacks and event\nhandlers are examples of this on a small scale, but the idea can be ex-\ntended even to the point of allowing entirely new types of game objects\ntobedefinedinscript. Thismightbedonevia inheritance (i.e., derivinga\nclass written in script from a class written in the native language) or via\ncomposition (i.e.,attachinganinstanceofascriptedclasstoanativegame\nobject).\n•Scripted components or properties. In a component- or property-based\ngame object model, it only makes sense to permit new components or\nproperty objects to be constructed partially or entirely in script. This\napproach was used by Gas Powered Games for Dungeon Siege. The\ngame object model was property-based, and it was possible to imple-\nment properties in either C++ or Gas Powered Games’ custom scripting\nlanguage, Skrit(http://ds.heavengames.com/library/dstk/skrit/skrit).\nBy the end of the project, they had approximately 148 scripted property\ntypes and 21 native C++ property types.\n•Script-driven engine. Script might be used to drive an entire engine sys-\ntem. For example, the game object model could conceivably be written\nentirelyinscript,callingintothenativeenginecodeonlywhenitrequires\nthe services of lower-level engine components.\n•Script-driven game. Some game engines actually flip the relationship\nbetween the native language and the scripting language on its head.\nIn these engines, the script code runs the whole show, and the na-\ntive engine code acts merely as a library that is called to access certain\nhigh-speed features of the engine. The Panda3D engine (http://www.\npanda3d.org)isanexampleofthiskindofarchitecture. Panda3Dgames\ncan be written entirely in the Python language, and the native engine\n(implemented in C++) acts like a library that is called by script code.\n(Panda3D games can also be written entirely in C++.)\n16.9.5 Features of a Runtime Game Scripting Language\nThe primary purpose of many game scripting languages is to implement\ngameplay features, and this is often accomplished by augmenting and cus-\ntomizingagame’sobjectmodel. Inthissection,we’llexploresomeofthemost\ncommon requirements and features of such a scripting system.\n1144 16. Runtime Gameplay Foundation Systems\n16.9.5.1 Interface with the Native Programming Language\nInorderforascriptinglanguagetobeuseful, itmustnotoperateinavacuum.\nIt’s imperative for the game engine to be able to execute script code, and it’s\nusuallyequallyimportantforscriptcodetobecapableofinitiatingoperations\nwithin the engine as well.\nA runtime scripting language’s virtual machine (VM) is generally embed-\nded within the game engine. The engine initializes the virtual machine, runs\nscriptcodewheneverrequired,andmanagesthosescripts’execution. Theunit\nof execution varies depending on the specifics of the language and the game’s\nimplementation.\n• In a functional scripting language, the function is often the primary unit\nofexecution. Inorderfortheenginetocallascriptfunction,itmustlook\nup the byte code corresponding to the name of the desired function and\nspawn a virtual machine to execute it (or instruct an existing VM to do\nso).\n• Inanobject-orientedscriptinglanguage, classesaretypicallytheprimary\nunit of execution. In such a system, objects can be spawned and de-\nstroyed, and methods (member functions) can be invoked on individual\nclass instances.\nIt’susuallybeneficialtoallowtwo-waycommunicationbetweenscriptand\nnative code. Therefore, most scripting languages allowing native code to be\ninvoked from script as well. The details are language- and implementation-\nspecific, but the basic approach is usually to allow certain script functions to\nbe implemented in the native language rather than in the scripting language.\nTocallanenginefunction, scriptcodesimplymakesanordinaryfunctioncall.\nThe virtual machine detects that the function has a native implementation,\nlooksupthecorrespondingnativefunction’saddress(perhapsbynameorvia\nsomeotherkindofuniquefunctionidentifier), andcallsit. Forexample, some\norallofthememberfunctionsofaPythonclassormodulecanbeimplemented\nusingCfunctions. Pythonmaintainsadatastructure,knownasa methodtable ,\nthat maps the name of each Python function (represented as a string) to the\naddress of the C function that implements it.\nCase Study: Naughty Dog’s DC Language\nAsanexample,let’shaveabrieflookathowNaughtyDog’sruntimescripting\nlanguage, a language called DC, was integrated into the engine.\nDCisavariantoftheSchemelanguage(whichisitselfavariantofLisplan-\nguage). Chunks of executable code in DC are known as script lambdas , which\n16.9. Scripting 1145\nare the approximate equivalent of functions or code blocks in the Lisp family\noflanguages. ADCprogrammerwritesscriptlambdasandidentifiesthemby\ngiving them globally unique names. The DC compiler converts these script\nlambdas into chunks of byte code, which are loaded into memory when the\ngame runs and can be looked up by name using a simple functional interface\nin C++.\nOnce the engine has a pointer to a chunk of script lambda byte code, it\ncan execute the code by calling a “virtual machine execution” function in the\nengine and passing the byte code pointer to it. The function itself is surpris-\ningly simple. It spins in a loop, reading byte code instructions one-by-one,\nandexecutingeachinstruction. Whenallinstructionshavebeenexecuted, the\nfunction returns.\nThe virtual machine contains a bank of registers, which can hold any kind\nof data the script may want to deal with. This is implemented using a vari-\nantdata type—a union of all the data types (see Section 16.8.4 for a discus-\nsion of variants). Some instructions cause data to be loaded into a register;\nothers cause the data held in a register to be looked up and used. There\nare instructions for performing all of the mathematical operations available\nin the language, as well as instructions for performing conditional checks—\nimplementationsofDC’s (if ...), (when ...) and(cond ...) instruc-\ntions and so on.\nThe virtual machine also supports a function call stack . Script lambdas in\nDC can call other script lambdas (i.e., functions) that have been defined by a\nscript programmer via DC’s (defun ...) syntax. Just like any procedural\nprogramming language, a stack is needed to keep track of the states of the\nregisters and the return address when one function calls another. In the DC\nvirtual machine, the call stack is literally a stack of register banks—each new\nfunction gets its own private bank of registers. This prevents us from having\nto save off the state of the registers, call the function, and then restore the reg-\nisters when the called function returns. When the virtual machine encounters\na byte code instruction that tells it to call another script lambda, the byte code\nforthatscriptlambdaislookedupbyname, anewstackframeispushed, and\nexecutioncontinuesatthefirstinstructionofthatscriptlambda. Whenthevir-\ntual machine encounters a return instruction, the stack frame is popped from\nthe stack, along with the return“address”(which is reallyjust the index of the\nbyte code instruction in the calling script lambda after the one that called the\nfunction in the first place).\nThe following pseudocode should give you a feel for what the core in-\nstruction-processing loop of the DC virtual machine looks like:\n1146 16. Runtime Gameplay Foundation Systems\nvoid DcExecuteScript (DCByteCode* pCode )\n{\nDCStackFrame* pCurStackFrame\n=DcPushStackFrame (pCode);\n// Keep going until we run out of stack frames (i.e.,\n// the top-level script lambda ""function"" returns).\nwhile (pCurStackFrame != nullptr )\n{\n// Get the next instruction. We will never run\n// out, because the return instruction is always\n// last, and it will pop the current stack frame\n// below.\nDCInstruction& instr\n= pCurStackFrame-> GetNextInstruction();\n// Perform the operation of the instruction.\nswitch (instr. GetOperation ())\n{\ncase DC_LOAD_REGISTER_IMMEDIATE:\n{\n// Grab the immediate value to be loaded\n// from the instruction.\nVariant& data = instr. GetImmediateValue ();\n// Also determine into which register to\n// put it.\nU32 iReg = instr. GetDestRegisterIndex();\n// Grab the register from the stack frame.\nVariant& reg\n= pCurStackFrame-> GetRegister (iReg);\n// Store the immediate data into the\n// register.\nreg = data ;\n}\nbreak;\n// Other load and store register operations...\ncase DC_ADD_REGISTERS :\n{\n// Determine the two registers to add. The\n// result will be stored in register A.\nU32 iRegA = instr. GetDestRegisterIndex();\nU32 iRegB = instr. GetSrcRegisterIndex();\n16.9. Scripting 1147\n// Grab the 2 register variants from the\n// stack.\nVariant& dataA\n= pCurStackFrame-> GetRegister(iRegA);\nVariant& dataB\n= pCurStackFrame-> GetRegister(iRegB);\n// Add the registers and store in\n// register A.\ndataA = dataA + dataB ;\n}\nbreak;\n// Other math operations...\ncase DC_CALL_SCRIPT_LAMBDA:\n{\n// Determine in which register the name of\n// the script lambda to call is stored.\n// (Presumably it was loaded by a previous\n// load instr.)\nU32 iReg = instr. GetSrcRegisterIndex ();\n// Grab the appropriate register, which\n// contains the name of the lambda to call.\nVariant& lambda\n= pCurStackFrame-> GetRegister(iReg);\n// Look up the byte code of the lambda by\n// name.\nDCByteCode* pCalledCode\n=DcLookUpByteCode(lambda.AsStringId());\n// Now ""call"" the lambda by pushing a new\n// stack frame.\nif (pCalledCode)\n{\npCurStackFrame\n=DcPushStackFrame(pCalledCode);\n}\n}\nbreak;\ncase DC_RETURN :\n{\n1148 16. Runtime Gameplay Foundation Systems\n// Just pop the stack frame. If we're in\n// the top lambda on the stack, this\n// function will return nullptr, and the\n// loop will terminate.\npCurStackFrame =DcPopStackFrame();\n}\nbreak;\n// Other instructions...\n// ...\n} // end switch\n} // end while\n}\nIn the above example, we assume that the global functions DcPushStack\nFrame() andDcPopStackFrame() manage the stack of register banks for\nus in some suitable way and that the global function DcLookUpByteCode()\nis capable of looking up any script lambda by name. We won’t show imple-\nmentations of those functions here, because the purpose of this example is\nsimply to show how the inner loop of a script virtual machine might work,\nnot to provide a complete functional implementation.\nDC script lambdas can also call native functions—i.e., global functions\nwritteninC++thatserveashooksintotheengineitself. Whenthevirtualma-\nchinecomesacrossaninstructionthatcallsanativefunction,theaddressofthe\nC++ function is looked up by name using a global table that has been hard-\ncoded by the engine programmers. If a suitable C++ function is found, the\narguments to the function are taken from registers in the current stack frame,\nand the function is called. This implies that the C++ function’s arguments are\nalways of type Variant. If the C++ function returns a value, it too must be a\nVariant , anditsvaluewillbestoredintoaregisterinthecurrentstackframe\nfor possible use by subsequent instructions.\nThe global function table might look something like this:\ntypedef Variant DcNativeFunction (U32 argCount,\nVariant* aArgs);\nstruct DcNativeFunctionEntry\n{\nStringId m_name;\nDcNativeFunction* m_pFunc;\n};\n16.9. Scripting 1149\nDcNativeFunctionEntry g_aNativeFunctionLookupTable[] =\n{\n{ SID(""get-object-pos""), DcGetObjectPos },\n{ SID(""animate-object""), DcAnimateObject },\n// etc.\n};\nA native DC function implementation might look something like the fol-\nlowing. Notice how the Variant arguments are passed to the function as an\narray. The function must verify that the number of arguments passed to it\nequals the number of arguments it expects. It must also verify that the types\nof the argument(s) are as expected and be prepared to handle errors that the\nDCscriptprogrammermayhavemadewhencallingthefunction. AtNaughty\nDog, we wrote an argument iterator which allows us to extract and verify the\narguments one by one in a convenient manner.\nVariant DcGetObjectPos (U32 argCount, Variant* aArgs)\n{\n// Argument iterator expecting at most 2 args.\nDcArgIterator args(argCount ,aArgs, 2);\n// Set up a default return value.\nVariant result;\nresult.SetAsVector(Vector(0.0f, 0.0f, 0.0f));\n// Use iterator to extract the args. It flags missing\n// or invalid arguments as errors automatically.\nStringId objectName = args.NextStringId ();\nPoint* pDefaultPos = args. NextPoint (kDcOptional);\nGameObject* pObject\n= GameObject:: LookUpByName (objectName);\nif (pObject)\n{\nresult.SetAsVector(pObject->GetPosition());\n}\nelse\n{\nif (pDefaultPos)\n{\nresult.SetAsVector(*pDefaultPos);\n}\nelse\n{\nDcErrorMessage(""get-object-pos: ""\n""Object '%s' not found.\n"",\nobjectName.ToDebugString());\n}\n1150 16. Runtime Gameplay Foundation Systems\n}\nreturn result;\n}\nNote that the function StringId::ToDebugString() performs a re-\nverse look-up to convert a string id back to its original string. This re-\nquires the game engine to maintain some kind of database mapping each\nstring id to its original string. During development such a database\ncan make life much easier, but because it consumes a lot of memory,\nthe database should be omitted from the final shipped product. (The\nfunction name ToDebugString() reminds us that the reverse conver-\nsion from string id back to string should only be performed for de-\nbugging purposes—the game itself must neverrely on this functional-\nity!)\n16.9.5.2 Game Object References\nScript functions often need to interact with game objects, which themselves\nmaybeimplementedpartiallyorentirelyintheengine’snativelanguage. The\nnative language’s mechanisms for referencing objects (e.g., pointers or refer-\nences in C++) won’t necessarily be valid in the scripting language. (It may\nnot support pointers at all, for example.) Therefore, we need to come up with\nsome reliable way for script code to reference game objects.\nThere are a number of ways to accomplish this. One approach is to refer\nto objects in script via opaque numeric handles. The script code can obtain\nobject handles in various ways. It might be passed a handle by the engine,\nor it might perform some kind of query, such as asking for the handles of all\ngame objects within a radius of the player or looking up the handle that cor-\nresponds to a particular object name. The script can then perform operations\non the game object by calling native functions and passing the object’s handle\nas an argument. On the native language side, the handle is converted back\ninto a pointer to the native object, and then the object can be manipulated as\nappropriate.\nNumeric handles have the benefit of simplicity and should be easy to sup-\nport in any scripting language that supports integer data. However, they can\nbe unintuitive and difficult to work with. Another alternative is to use the\nnames of the objects, represented as strings, as our handles. This has some\ninteresting benefits over the numeric handle technique. For one thing, strings\nare human-readable and intuitive to work with. There is a direct correspon-\ndence to the names of the objects in the game’s world editor. In addition,\nwe can choose to reserve certain special object names and give them “magic”\n16.9. Scripting 1151\nmeanings. For example, in Naughty Dog’s scripting language, the reserved\nname“self”alwaysreferstotheobjecttowhichthecurrentlyrunningscriptis\nattached. This allows game designers to write a script, attach it to an object in\nthegame, and then use the scriptto play an animation on the object by simply\nwriting (animate 'self name-of-animation).\nUsing strings as object handles has its pitfalls, of course. Strings typically\noccupy more memory than integer ids. And because strings vary in length,\ndynamicmemoryallocationisrequiredinordertocopythem. Stringcompar-\nisons are slow. Script programmers are apt to make mistakes when typing the\nnames of game objects, which can lead to bugs. In addition, script code can\nbe broken if someone changes the name of an object in the game world editor\nbut forgets to update the name of the object in script.\nHashed string ids overcome most of these problems by converting any\nstrings(regardlessoflength)intoaninteger. Intheory,hashedstringidsenjoy\nthe best of both worlds—they can be read by users just like strings, but they\nhave the runtime performance characteristics of an integer. However, for this\nto work, your scripting language needs to support hashed string ids in some\nway. Ideally, we’d like the script compiler to convert our strings into hashed\nids for us. That way, the runtime code doesn’t have to deal with the strings at\nall, only the hashed ids (except possibly for debugging purposes—it’s nice to\nbe able to see the string corresponding to a hashed id in the debugger). How-\never, this isn’t always possible in all scripting languages. Another approach is\nto allow the user to use strings in script and convert them into hashed ids at\nruntime, whenever a native function is called.\nNaughtyDog’sDCscriptinglanguageleveragestheconceptof“symbols,”\nwhich are native to the Scheme programming language, to encode its string\nids. Writing 'foo—or more verbosely, (quote foo)—in DC/Scheme cor-\nresponds to the string id SID(""foo"") in C++.\n16.9.5.3 Receiving and Handling Events in Script\nEvents are a ubiquitous communication mechanism in most game engines.\nBy permitting event handler functions to be written in script, we open up a\npowerful avenue for customizing the hard-coded behavior of our game.\nEvents are usually sent to individual objects and handled within the con-\ntext of that object. Hence, scripted event handlers need to be associated with\nan object in some way. Some engines use the game object type system for this\npurpose—scripted event handlers can be registered on a per-object-type ba-\nsis. This allows different types of game objects to respond in different ways to\nthe same event but ensures that all instances of each type respond in a consis-\ntent and uniform way. The event handler functions themselves can be simple\nscript functions, or they can be members of a class if the scripting language is\n1152 16. Runtime Gameplay Foundation Systems\nobject-oriented. In either case, the event handler is typically passed a handle\nto the particular object to which the event was sent, much as C++ member\nfunctions are passed the thispointer.\nIn other engines, scripted event handlers are associated with individual\nobject instances rather than with object types. In this approach, different in-\nstances of the same type might respond differently to the same event.\nThereareallsortsofotherpossibilities,ofcourse. Forexample,inNaughty\nDog’sengine(usedtocreatethe Uncharted andTheLastofUs series),scriptsare\nobjects in their own right. They can be associated with individual game ob-\njects, they can be attached to regions (convex volumes that are used to trigger\ngameevents), ortheycanexistasstand-aloneobjectsinthegameworld. Each\nscript can have multiple states (that is, scripts are finite state machines in the\nNaughty Dog engine). In turn, each state can have one or more event handler\ncode blocks. When a game object receives an event, it has the option of han-\ndling the event in native C++. It also checks for an attached script object, and\nifoneisfound, theeventissenttothatscript’scurrentstate. Ifthestatehasan\nevent handler for the event, it is called. Otherwise, the script simply ignores\nthe event.\n16.9.5.4 Sending Events\nAllowing scripts to handle game events that are generated by the engine is\ncertainly a powerful feature. Even more powerful is the ability to generate\nand send events from script code either back to the engine or to other scripts.\nIdeally, we’d like to be able not only to send predefined types of events\nfrom script but to define entirely new event types in script. Implementing\nthis is trivial if event types are strings or string ids. To define a new event\ntype, the script programmer simply comes up with a new event type name\nand types it into his or her script code. This can be a highly flexible way for\nscripts to communicate with one another. Script A can define a new event\ntypeandsendittoScriptB.IfScriptBdefinesaneventhandlerforthistypeof\nevent, we’ve implemented a simple way for Script A to “talk” to Script B. In\nsome game engines, event- or message-passing is the only supported means\nof inter-object communication in script. This can be an elegant yet powerful\nand flexible solution.\n16.9.5.5 Object-Oriented Scripting Languages\nSome scripting languages are inherently object-oriented. Others do not sup-\nport objects directly but provide mechanisms that can be used to implement\nclasses and objects. In many engines, gameplay is implemented via an object-\n16.9. Scripting 1153\noriented game object model of some kind. So it makes sense to permit some\nform of object-oriented programming in script as well.\nDeﬁning Classes in Scripts\nA class is really just a bunch of data with some associated functions. So any\nscripting language that permits new data structures to be defined, and pro-\nvides some way to store and manipulate functions, can be used to implement\nclasses. For example, in Lua, a class can be built out of a table that stores data\nmembers and member functions.\nInheritance in Script\nObject-oriented languages do not necessarily support inheritance. However,\nif this feature is available, it can be extremely useful, just as it is in native pro-\ngramming languages like C++.\nIn the context of game scripting languages, there are two kinds of in-\nheritance: deriving scripted classes from other scripted classes and deriv-\ning scripted classes from native classes. If your scripting language is object-\noriented,chancesaretheformerissupportedoutofthebox. However,thelat-\nter is tough to implement even if the scripting language supports inheritance.\nTheproblemisbridgingthegapbetweentwolanguagesandtwolow-levelob-\nject models. We won’t get into the details of how this might be implemented\nhere, as the implementation is bound to be specific to the pair of languages\nbeingintegrated. UnrealScriptistheonlyscriptinglanguageI’veencountered\nthat allows scripted classes to derive from native classes in a seamless way.\nComposition/Aggregation in Script\nWe don’t need to rely on inheritance to extend a hierarchy of classes—we can\nalso use composition or aggregation to similar effect. In script, then, all we\nreally need is a way to define classes and associate instances of those classes\nwith objects that have been defined in the native programming language. For\nexample, a game object could hold a pointer or reference to an optional com-\nponent written entirely in script. We can delegate certain key functionality\nto the script component, if it exists. The script component might have an\nUpdate() function that is called whenever the game object is updated, and\nthe scripted component might also be permitted to register some of its mem-\nber functions/methods as event handlers. When an event is sent to the game\nobject, it calls the appropriate event handler on the scripted component, thus\ngivingthescriptprogrammeranopportunitytomodifyorextendthebehavior\n1154 16. Runtime Gameplay Foundation Systems\nof the natively implemented game object.\n16.9.5.6 Scripted Finite State Machines\nMany problems in game programming can be solved naturally using finite\nstate machines (FSMs). For this reason, some engines build the concept of\nFSMsrightintothecoregameobjectmodel. Insuchengines,everygameobject\ncan have one or more states, and it is the states—not the game object itself—\nthat contain the update function, event handler functions and so on. Simple\ngameobjectscanbecreatedbydefiningasinglestate,butmorecomplexgame\nobjectshavethefreedomtodefinemultiplestates,eachwithadifferentupdate\nand event-handling behavior.\nIf your engine supports a state-driven game object model, it makes a lot of\nsensetoprovidefinitestatemachinesupportinthescriptinglanguageaswell.\nAnd of course, even if the core game object model doesn’t support finite state\nmachines natively, one can still provide state-driven behavior by using a state\nmachineonthescriptside. AnFSMcanbeimplementedinanyprogramming\nlanguage by using class instances to represent states, but some scripting lan-\nguages provide tools especially for this purpose. An object-oriented scripting\nlanguage might provide custom syntax that allows a class to contain multiple\nstates, or it might provide tools that help the script programmer easily aggre-\ngate state objects together within a central hub object and then delegate the\nupdate and event-handling functions to it in a straightforward way. But even\nif your scripting language provides no such features, you can always adopt a\nmethodology for implementing FSMs and follow those conventions in every\nscript you write.\n16.9.5.7 Multithreaded Scripts\nIt’s often useful to be able to execute multiple scripts in parallel. This is espe-\ncially true on today’s highly parallelized hardware architectures. If multiple\nscripts can run at the same time, we are in effect providing parallel threads of\nexecution in script code, much like the threads provided by most multitasking\noperating systems. Of course, the scripts may not actually run in parallel—if\nthey are all running on a single CPU, the CPU must take turns executing each\none. However, fromthepointofviewofthescriptprogrammer, theparadigm\nis one of parallel programming.\nMostscriptingsystemsthatprovideparallelismdosovia cooperativemulti-\ntasking. Thismeansthatascriptwillexecuteuntilitexplicitlyyieldstoanother\nscript. This is in contrast with a preemptive multitasking approach, in which\nthe execution of any script could be interrupted at any time to permit another\n16.9. Scripting 1155\nscript to execute.\nOne simple approach to cooperative multitasking in script is to permit\nscripts to explicitly go to sleep, waiting for something relevant to happen.\nA script might wait for a specified number of seconds to elapse, or it might\nwait until a particular event is received. It might wait until another thread\nof execution has reached a predefined synchronization point. Whatever the\nreason, whenever a script goes to sleep, it puts itself on a list of sleeping script\nthreadsandtellsthevirtualmachinethatitcanstartexecutinganothereligible\nscript. The system keeps track of the conditions that will wake up each sleep-\ning script—when one of these conditions becomes true, the script or scripts\nwaiting on the condition are woken up and allowed to continue executing.\nTo see how this works in practice, let’s look at an example of a multi-\nthreaded script. This script manages the animations of two characters and\na door. The two characters are instructed to walk up to the door—each one\nmight take a different, and unpredictable, amount of time to reach it. We’ll\nput the script’s threads to sleep while they wait for the characters to reach the\ndoor. Once they both arrive at the door, one of the two characters opens the\ndoor, which it does by playing an “open door” animation. Note that we don’t\nwant to hard-code the duration of the animation into the script itself. That\nway, if the animators change the animation, we won’t have to go back and\nmodify our script. So we’ll put the threads to sleep again while the wait for\nthe animation to complete. A script that accomplishes this is shown below,\nusing a simple C-like pseudocode syntax.\nprocedure DoorCinematic()\n{\nthread Guy1()\n{\n// Ask guy1 to walk to the door.\nCharacterWalkToPoint(guy1, doorPosition);\n// Go to sleep until he gets there.\nWaitUntil(CHARACTER_ARRIVAL);\n// OK, we're there. Tell the other threads\n// via a signal.\nRaiseSignal(""Guy1Arrived"");\n// Wait for the other guy to arrive as well.\nWaitUntil(SIGNAL, ""Guy2Arrived"");\n// Now tell guy1 to play the ""open door""\n// animation.\nCharacterAnimate(guy1, ""OpenDoor"");\n1156 16. Runtime Gameplay Foundation Systems\nWaitUntil(ANIMATION_DONE);\n// OK, the door is open. Tell the other threads.\nRaiseSignal(""DoorOpen"");\n// Now walk thru the door.\nCharacterWalkToPoint(guy1, beyondDoorPosition);\n}\nthread Guy2()\n{\n// Ask guy2 to walk to the door.\nCharacterWalkToPoint(guy2, doorPosition);\n// Go to sleep until he gets there.\nWaitUntil(CHARACTER_ARRIVAL);\n// OK, we're there. Tell the other threads\n// via a signal.\nRaiseSignal(""Guy2Arrived"");\n// Wait for the other guy to arrive as well.\nWaitUntil(SIGNAL, ""Guy1Arrived"");\n// Now wait until guy1 opens the door for me.\nWaitUntil(SIGNAL, ""DoorOpen"");\n// OK, the door is open. Now walk thru the door.\nCharacterWalkToPoint(guy2, beyondDoorPosition);\n}\n}\nIntheabove,weassumethatourhypotheticalscriptinglanguageprovides\na simple syntax for defining threads of execution within a single function. We\ndefine two threads, one for Guy1 and one for Guy2.\nThethreadforGuy1tellsthecharactertowalktothedoorandthengoesto\nsleep waiting for his arrival. We’re hand-waving a bit here, but let’s imagine\nthat the scripting language magically allows a thread to go to sleep, waiting\nuntilacharacterinthegamearrivesatatargetpointtowhichhewasrequested\nto walk. In reality, this might be implemented by arranging for the character\nto send an event back to the script and then waking the thread up when the\nevent arrives.\nOnce Guy1 arrives at the door, his thread does two things that warrant\nfurther explanation. First, it raises a signal called “Guy1Arrived.” Second, it\ngoes to sleep waiting for another signal called “Guy2Arrived.” If we look at\nthe thread for Guy2, we see a similar pattern, only reversed. This pattern of",51153
116-16.10 High-Level Game Flow.pdf,116-16.10 High-Level Game Flow,"16.10. High-Level Game Flow 1157\nraising a signal and then waiting for another signal is to synchronize the two\nthreads.\nInourhypotheticalscriptinglanguage, a signalisjustaBooleanflagwitha\nname. Theflagstartsoutfalse,butwhenathreadcalls RaiseSignal(name),\nthe named flag’s value changes to true. Other threads can go to sleep, wait-\ning for a particular named signal to become true. When it does, the sleeping\nthread(s) wake up and continue executing. In this example, the two threads\nareusingthe“Guy1Arrived”and“Guy2Arrived”signalstosynchronizewith\none another. Each thread raises its signal and then waits for the other thread’s\nsignal. It does not matter which signal is raised first—only when both sig-\nnals have been raised will the two threads wake up. And when they do, they\nwill be in perfect synchronization. Two possible scenarios are illustrated in\nFigure 16.24, one in which Guy1 arrives first, the other in which Guy2 arrives\nfirst. Asyoucansee,theorderinwhichthesignalsareraisedisirrelevant,and\nthe threads always end up in sync after both signals have been raised.\n16.10 High-Level Game Flow\nA game object model provides the foundations upon which a rich and enter-\ntainingcollectionofgameobjecttypescanbeimplementedwithwhichtopop-\nulate our game worlds. However, by itself, a game object model only permits\nus to define the kinds of objects that exist in our game world and how they\nbehave individually. It says nothing of the player’s objectives, what happens\nif he or she completes them, and what fate should befall the player if he or she\nfails.\nForthis,weneedsomekindofsystemtocontrolhigh-levelgameflow. This\nis often implemented as a finite state machine. Each state usually represents a\nWalk\nSignal\nWaitWalk\nSignal\n(No Wait)Guy1 Guy2\nSyncWalk\nSignal\nWaitWalk\nSignal\n(No Wait)Guy1 Guy2\nSync\nFigure 16.24. Two examples showing how a simple pattern of raising one signal and then waiting\non another can be used to synchronize a pair of script threads.\n1158 16. Runtime Gameplay Foundation Systems\nsingle player objective or encounter and is associated with a particular locale\nwithin the virtual game world. As the player completes each task, the state\nmachine advances to the next state, and the player is presented with a new\nset of goals. The state machine also defines what should happen in the event\nof the player’s failure to accomplish the necessary tasks or objectives. Often,\nfailure sends the player back to the beginning of the current state, so he or\nshe can try again. Sometimes after enough failures, the player has run out of\n“lives” and will be sent back to the main menu, where he or she can choose\nto play a new game. The flow of the entire game, from the menus to the first\n“level” to the last, can be controlled through this high-level state machine.\nThe task system used in Naughty Dog’s Jak and Daxter ,Uncharted andThe\nLast of Us franchises is an example of such a state-machine-based system. It\nallows for linear sequences of states (called tasksat Naughty Dog). It also per-\nmitsparalleltasks,whereonetaskbranchesoutintotwoormoreparalleltasks,\nwhich eventually merge back into the main task sequence. This parallel task\nfeature sets the Naughty Dog task graph apart from a regular state machine,\nsince state machines typically can only be in one state at a time.",3362
117-V Conclusion.pdf,117-V Conclusion,Part V\nConclusion\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com,91
118-17 You Mean Theres More.pdf,118-17 You Mean Theres More,,0
119-17.2 Gameplay Systems.pdf,119-17.2 Gameplay Systems,"17\nYou Mean There’s More?\nCongratulations! You’vereachedtheendofyourjourneythroughtheland-\nscape of game engine architecture in one piece (and hopefully none the\nworse for wear). With any luck, you’ve learned a great deal about the major\ncomponents that comprise a typical game engine. But of course, every jour-\nney’sendisanother’sbeginning. There’sagreatdealmoretobelearnedabout\neach and every topic covered within these pages. As technology and com-\nputing hardware continue to improve, more things will become possible in\ngames—and more engine systems will be invented to support them. What’s\nmore, this book’s focus was on the game engine itself. We haven’t even be-\ngun to discuss the rich world of gameplay programming, a topic that could\nfill many more volumes.\nIn the following brief sections, I’ll identify a few of the engine and game-\nplay systems we didn’t have room to cover in any depth in this book, and I’ll\nsuggest some resources for those who wish to learn more about them.\n17.1 Some Engine Systems We Didn’t Cover\n17.1.1 Movie Player\nMost games include a movie player for displaying prerendered movies, also\nknownasfull-motionvideo(FMV).Thebasiccomponentsofthemovieplayer\n1161\n1162 17. You Mean There’s More?\nare an interface to the streaming file I/O system (see Section 7.1.3), a codec to\ndecode the compressed video stream, and some form of synchronization with\nthe audio playback system for the sound track.\nAnumberofdifferentvideoencodingstandardsandcorrespondingcodecs\nare available, each one suited to a particular type of application. For example,\nvideo CDs (VCD) and DVDs use MPEG-1 and MPEG-2 (H.262) codecs, re-\nspectively. The H.261 and H.263 standards are designed primarily for online\nvideoconferencingapplications. GamesoftenusestandardslikeMPEG-4part\n2(e.g.,DivX),MPEG-4Part10/H.264,WindowsMediaVideo(WMV)orBink\nVideo (a standard designed specifically for games by Rad Game Tools, Inc.).\nSee http://en.wikipedia.org/wiki/Video_codec and http://www.radgame\ntools.com/bnkmain.htm for more information on video codecs.\n17.1.2 Multiplayer Networking\nAlthough the concepts of concurrent programming covered in Chapter 4 are\nrelevant to multiplayer game architecture and distributed network program-\nming, this book doesn’t address either topic directly. For an in-depth treat-\nment of multiplayer networking, see [4].\n17.2 Gameplay Systems\nA game is of course much more than just its engine. On top of the game-\nplay foundation layer (discussed in Chapter 16), you’ll find a rich assortment\nof genre- and game-specific gameplay systems. These systems tie the myr-\niad game engine technologies described in this book together into a cohesive\nwhole, breathing life into the game.\n17.2.1 Player Mechanics\nPlayer mechanics are of course the most important gameplay system. Each\ngenre is defined by a general style of player mechanics and gameplay, and\nof course every game within a genre has its own specific designs. As such,\nplayer mechanics is a huge topic. It involves the integration of human in-\nterface device systems, motion simulation, collision detection, animation and\naudio, not to mention integration with other gameplay systems like the game\ncamera, weapons, cover, specialized traversal mechanics (ladders, swinging\nropes, etc.), vehicle systems, puzzle mechanics and so on.\nClearly, playermechanicsareasvariedasthegamesthemselves, sothere’s\nno one place you can go to learn all about them. It’s best to tackle this topic\nby studying a single genre at a time. Play games and try to reverse-engineer\n17.2. Gameplay Systems 1163\ntheir player mechanics. Then try to implement them yourself! And as a very\nmodest start on reading, you can check out [9, Section 4.11] for a discussion of\nMario-style platformer player mechanics.\n17.2.2 Cameras\nA game’s camera system is almost as important as the player mechanics. In\nfact,thecameracanmakeorbreakthegameplayexperience. Eachgenretends\nto have its own camera control style, although of course every game within a\nparticulargenredoesitalittlebitdifferently(andsome verydifferently). See[8,\nSection 4.3] for some basic game camera control techniques. In the following\nparagraphs, I’ll briefly outline some of the most prevalent kinds of cameras in\n3D games, but please note that this is far from a complete list.\n•Look-atcameras . This type of camera rotates about a target point and can\nbe moved in and out relative to this point.\n•Follow cameras. This type of camera is prevalent in platformer, third-\npersonshooterandvehicle-basedgames. Itactsmuchlikealook-atcam-\nera focused on the player character/avatar/vehicle, but its motion typ-\nically lags the player. A follow camera also includes advanced collision\ndetectionandavoidancelogicandprovidesthehumanplayerwithsome\ndegree of control over the camera’s orientation relative to the player\navatar.\n•First-person cameras . As the player character moves about in the game\nworld, a first-person camera remains affixed to the character’s virtual\neyes. Theplayertypicallyhasfullcontroloverthedirectioninwhichthe\ncamera should be pointed, either via mouse or joypad control. The look\ndirection of the camera also translates directly into the aim direction of\ntheplayer’sweapon,whichistypicallyindicatedbyasetofdisembodied\narms and a gun attached to the bottom of the screen, and a reticle at the\ncenter of the screen.\n•RTScameras. Real-timestrategyandgodgamestendtoemployacamera\nthat floats above the terrain, looking down at an angle. The camera can\nbe panned about over the terrain, but the pitch and yaw of the camera\nare usually not under direct player control.\n•Cinematiccameras. Mostthree-dimensionalgameshaveatleastsomecin-\nematic moments in which the camera flies about within the scene in a\nmore filmic manner rather than being tethered to an object in the game.\nThese camera movements are typically controlled by the animators.\n1164 17. You Mean There’s More?\n17.2.3 Artiﬁcial Intelligence\nAnother major component of most character-based games is artificial intelli-\ngence(AI). At its lowest level, an AI system is usually founded in technologies\nlike basic path finding (which commonly makes use of the well-known A* al-\ngorithm), perception systems (line of sight, vision cones, knowledge of the\nenvironment, etc.) and some form of memory or knowledge.\nOn top of these foundations, character control logic is implemented. A\ncharacter control system determines how to make the character perform\nspecific actions like locomoting, navigating unusual terrain features, using\nweapons, driving vehicles, taking cover and so on. It typically involves com-\nplex interfaces to the collision, physics and animation systems within the en-\ngine. Character control is discussed in detail in Section 12.10.\nAbove the character control layer, an AI system typically has goal setting\nanddecisionmakinglogic, andpossiblyalsoemotionalstatemodeling, group\nbehaviors (coordination, flanking, crowd and flocking behaviors, etc.), and\nperhaps some advanced features like an ability to learn from past mistakes\nor adapt to a changing environment.\nOfcourse, theterm“artificialintelligence”isoneofthebiggestmisnomers\naroundinthegameindustry. GameAIisalwaysmoreofasmokeandmirrors\njob than an attempt at truly mimicking human intelligence. Your AI charac-\nters might have all sorts of complex internal emotional states and finely tuned\nperception of the game world. But if the player cannot perceive the characters’\nmotivations, it’s all for naught.\nAIprogrammingisarichtopic,andwecertainlyhavenotdoneitjusticein\nthis book. For more information, see [18], [8, Section 3], [9, Section 3] and [47,\nSection3]. AnothergoodstartingpointistheGDC2002talkentitled,“TheIllu-\nsionofIntelligence: TheIntegrationofAIandLevelDesigninHalo,”byChris\nButcher and Jaime Griesemer of Bungie (http://bit.ly/1g7FbhD). And while\nyou’re online, search for “game AI programming” too. You’ll find all sorts of\nlinks to talks, papers and books on game AI. The websites http://aigamedev.\ncom and http://www.gameai.com are great resources as well.\n17.2.4 Other Gameplay Systems\nClearly there’s a lot more to a game than just player mechanics, cameras\nand AI. Some games have drivable vehicles, implement specialized types of\nweaponry, allow the player to destroy the environment with the help of a dy-\nnamicphysicssimulation,lettheplayercreatehisorherowncharacters,build\ncustom levels, require the player to solve puzzles, …. Of course, the list of\ngenre- and game-specific features, and all of the specialized software systems\n17.2. Gameplay Systems 1165\nthat implement them, could go on forever. Gameplay systems are as rich and\nvaried as games are. Perhaps this is where your next journey as a game pro-\ngrammer will begin!\nTaylor & Francis \nTaylor & Francis Group \nhttp://taylorandfrancis.com",8922
120-Bibliography.pdf,120-Bibliography,"Bibliography\n[1] Michael Abrash. Michael Abrash’s Graphics Programming Black Book (Special Edi-\ntion). Scottsdale, AZ: Coriolis Group Books, 1997. (Available online at http://\nwww.jagregory.com/abrash-black-book.)\n[2] Tomas Akenine-Moller, Eric Haines and Naty Hoffman. Real-Time Rendering ,\nThird Edition. Wellesley, MA: A K Peters, 2008.\n[3] Andrei Alexandrescu. Modern C++ Design: Generic Programming and Design Pat-\nterns Applied. Reading, MA: Addison-Wesley, 2001.\n[4] Grenville Armitage, Mark Claypool and Philip Branch. Networking and Online\nGames: UnderstandingandEngineeringMultiplayerInternetGames . New York, NY:\nJohn Wiley and Sons, 2006.\n[5] James Arvo (editor). Graphics Gems II. San Diego, CA: Academic Press, 1991.\n[6] David A. Bies and Colin H. Hansen. Engineering Noise Control, Fourth Edition.\nNew York, NY: CRC Press, 2014.\n[7] Grady Booch, Robert A. Maksimchuk, Michael W. Engel, Bobbi J. Young, Jim\nConallen and Kelli A. Houston. Object-OrientedAnalysisandDesignwithApplica-\ntions, Third Edition. Reading, MA: Addison-Wesley, 2007.\n[8] Mark DeLoura (editor). Game Programming Gems. Hingham, MA: Charles River\nMedia, 2000.\n[9] MarkDeLoura(editor). GameProgrammingGems2. Hingham,MA:CharlesRiver\nMedia, 2001.\n1167\n1168 Bibliography\n[10] PhilipDutré,KavitaBalaandPhilippeBekaert. AdvancedGlobalIllumination ,Sec-\nond Edition. Wellesley, MA: A K Peters, 2006.\n[11] David H. Eberly. 3D Game Engine Design: A Practical Approach to Real-Time Com-\nputer Graphics. San Francisco, CA: Morgan Kaufmann, 2001.\n[12] David H. Eberly. 3DGameEngineArchitecture: EngineeringReal-TimeApplications\nwith WildMagic . San Francisco, CA: Morgan Kaufmann, 2005.\n[13] David H. Eberly. Game Physics. San Francisco, CA: Morgan Kaufmann, 2003.\n[14] Christer Ericson. Real-Time Collision Detection. San Francisco, CA: Morgan Kauf-\nmann, 2005.\n[15] Randima Fernando (editor). GPU Gems: Programming Techniques, Tips and Tricks\nfor Real-TimeGraphics. Reading, MA: Addison-Wesley, 2004.\n[16] JamesD.Foley,AndriesvanDam,StevenK.FeinerandJohnF.Hughes. Computer\nGraphics: Principles and Practice in C , Second Edition. Reading, MA: Addison-\nWesley, 1995.\n[17] Grant R. Fowles and George L. Cassiday. Analytical Mechanics , Seventh Edition.\nPacific Grove, CA: Brooks Cole, 2005.\n[18] John David Funge. AI for Games and Animation: A Cognitive Modeling Approach .\nWellesley, MA: A K Peters, 1999.\n[19] Erich Gamma, Richard Helm, Ralph Johnson and John M. Vlissiddes. Design\nPatterns: Elements of Reusable Object-Oriented Software. Reading, MA: Addison-\nWesley, 1994.\n[20] Andrew S. Glassner (editor). Graphics Gems I . San Francisco, CA: Morgan Kauf-\nmann, 1990.\n[21] AnanthGrama,AnshulGupta,GeorgeKarypis,VipinKumar. IntroductiontoPar-\nallelComputing, Second Edition. Reading, MA: Addison Wesley, 2003. (Available\nonline at http://srmcse.weebly.com/uploads/8/9/0/9/8909020/introduction_\nto_parallel_computing_second_edition-ananth_grama..pdf [sic].)\n[22] PaulS.Heckbert(editor). GraphicsGemsIV. SanDiego,CA:AcademicPress,1994.\n[23] JohnL.HennesseyandDavidA.Patterson. ComputerArchitecture: AQuantitative\nApproach. San Francisco, CA: Morgan Kaufmann, 2011.\n[24] MauriceHerlihyandNirShavit. TheArtofMultiprocessorProgramming. SanFran-\ncisco, CA: Morgan Kaufmann, 2008.\n[25] Roberto Ierusalimschy, Luiz Henrique de Figueiredo and Waldemar Celes. Lua\n5.1 ReferenceManual . Lua.org, 2006.\n[26] Roberto Ierusalimschy. Programmingin Lua , Second Edition. Lua.org, 2006.\n[27] Isaac Victor Kerlow. The Art of 3-D Computer Animation and Imaging (Second Edi-\ntion). New York, NY: John Wiley and Sons, 2000.\n[28] David Kirk (editor). Graphics Gems III . San Francisco, CA: Morgan Kaufmann,\n1994.\nBibliography 1169\n[29] Danny Kodicek. Mathematics and Physics for Game Programmers. Hingham, MA:\nCharles River Media, 2005.\n[30] Raph Koster. ATheory of Fun for Game Design. Phoenix, AZ: Paraglyph, 2004.\n[31] John Lakos. Large-Scale C++ Software Design. Reading, MA: Addison-Wesley,\n1995.\n[32] Eric Lengyel. Mathematics for 3D Game Programming and Computer Graphics , Sec-\nond Edition. Hingham, MA: Charles River Media, 2003.\n[33] GaryB.Little. InsidetheApple//e.Bowie, MD:BradyCommunicationsCompany,\nInc., 1985. (Available online at http://www.apple2scans.net/files/InsidetheIIe.\npdf.)\n[34] TuocV.Luong,JamesS.H.Lok,DavidJ.TaylorandKevinDriscoll. International-\nization: DevelopingSoftwareforGlobalMarkets .NewYork, NY:JohnWiley&Sons,\n1995.\n[35] Steve Maguire. Writing Solid Code: Microsoft’s Techniques for Developing Bug-Free\nC Programs. Bellevue, WA: Microsoft Press, 1993.\n[36] ScottMeyers. EffectiveC++: 55SpecificWaystoImproveYourProgramsandDesigns ,\nThird Edition. Reading, MA: Addison-Wesley, 2005.\n[37] Scott Meyers. More Effective C++: 35 New Ways to Improve Your Programs and De-\nsigns.Reading, MA: Addison-Wesley, 1996.\n[38] Scott Meyers. Effective STL: 50 Specific Ways to Improve Your Use of the Standard\nTemplateLibrary. Reading, MA: Addison-Wesley, 2001.\n[39] Ian Millington. Game Physics Engine Development. San Francisco, CA: Morgan\nKaufmann, 2007.\n[40] Hubert Nguyen (editor). GPUGems 3 . Reading, MA: Addison-Wesley, 2007.\n[41] Alan V. Oppenheim and Alan S. Willsky. Signals and Systems . Englewood Cliffs,\nNJ: Prentice-Hall, 1983.\n[42] Alan W. Paeth (editor). GraphicsGemsV . San Francisco, CA: Morgan Kaufmann,\n1995.\n[43] C. Michael Pilato, Ben Collins-Sussman and Brian W. Fitzpatrick. VersionControl\nwith Subversion , Second Edition. Sebastopol, CA: O’Reilly Media, 2008. (Com-\nmonly known as “The Subversion Book.” Available online at http://svnbook.\nred-bean.com.)\n[44] Matt Pharr (editor). GPU Gems 2: Programming Techniques for High-Performance\nGraphics and General-Purpose Computation. Reading, MA: Addison-Wesley, 2005.\n[45] Richard Stevens and Dave Raybould. The Game Audio Tutorial: A Practical Guide\nto Sound and Music for Interactive Games . Burlington, MA: Focal Press, 2011.\n[46] Bjarne Stroustrup. The C++ Programming Language , Special Edition (Third Edi-\ntion). Reading, MA: Addison-Wesley, 2000.\n[47] Dante Treglia (editor). Game Programming Gems 3. Hingham, MA: Charles River\nMedia, 2002.\n1170 Bibliography\n[48] GinovandenBergen. CollisionDetectioninInteractive3DEnvironments. SanFran-\ncisco, CA: Morgan Kaufmann, 2003.\n[49] Alan Watt. 3DComputerGraphics, Third Edition. Reading, MA: Addison Wesley,\n1999.\n[50] James Whitehead II, Bryan McLemore and Matthew Orlando. World of Warcraft\nProgramming: A Guide and Reference for Creating WoW Addons . New York, NY:\nJohn Wiley & Sons, 2008.\n[51] Richard Williams. TheAnimator’sSurvivalKit . London, UK: Faber & Faber, 2002.",6687
121-Index.pdf,121-Index,"Index 1171\nIndex\n#include, 148\n_DEBUG, 82\n__m128, seesingle instruction multiple\ndata\n2-blade, 373\n2D sound, 955\n3-blade, 374\n3D Studio Max, 62, 669\n3D sound, 955\n80/20 rule, 99, 216, 317, 608\nA* algorithm, 1164\nAABB,seebounding box\nABA problem, 297\nABI,seeapplication binary interface\nabsorption\natmospheric, 918, 923, 956, 958,\n964, 980\nlight, 633\nsound, 917\nabstract factory, seedesign pattern\nAC-3,seeaudio, file formats\naccelerated processing unit, 227\nacoustic intensity, 915, 961acoustic pressure, seesound\nacoustical modeling, 956\nacoustics, 920\nACP,seeasset conditioning pipeline\nacquire fence, seememory ordering\nsemantics\nacquire memory order, seememory\nordering semantics\naction state machine\nstate layer, 787, 794\ntransition, 786\nADC,seeanalog-to-digital conversion\nadding across a register, seesingle\ninstruction multiple data\nadditive blending, 769\naddressing mode, seeCPU\nADPCM, seepulse-code modulation\nAdvanced Linux Sound Architecture,\n994\nadvanced vector extensions, 331\naffinity,seethread\naggregation, 111, 1049, 1051, 1153\nAI,seeartificial intelligence\nAIFF,seeaudio, file formats\n1172 Index\nalbedo map, 640\nalgebra, 359\nalgebraic simplification, seecompiler,\noptimizations\naliasing, 683, 950\nAlienbrain, seeversion control\nalignment, seememory\nallocation\nchunky, 451, 514\ndelete operator (C++), 156\ndouble-buffered, 435\ndynamic, 155, 156, 426\nheap memory, 156, 511\njanitor,seealsodesign pattern, 112\nnew operator (C++), 156\noptimization, 426\npool, 430, 438, 512\nresource chunk allocator, 514\nsingle-frame allocator, 435\nstack, 112\nstack-based, 427, 438, 512\nstatic, 155\nalpha, 623, 635\nblending function, 676\ntesting, 676\nALSA,seeAdvanced Linux Sound\nArchitecture\nAltivec,seesingle instruction multiple\ndata\nALU,seearithmetic/logic unit\nambient lighting, seelighting\nambient occlusion, seelighting\nAmbisonics, 964\namplifier, 926, 946, 982\namplitude, 912, 933, 946, see alsoaudio\nanalog-to-digital conversion, 925, 948\nanalytical geometry, 836\nanechoic chamber, 922\nangular frequency, 913, 933\nangular momentum, 872\nangular velocity, 867\nanimation, 52, see alsoaction state\nmachine\nadditive blending, 769\nattach point, 800, 808\nblending, 755, 763\ncamera, 747cel, 722\nchannel, 744, 747\nclip, 734\nconstraint, 806, 816\ncross-fade, 758, 759\ndifference clip, 769\nease curve, 759\nEndorphin, 43\nEuphoria, 43\nevent trigger, 746\nfilm, 734, 736\nflat weighted average, 788\nfloating-point channel, 747\nframe, 722, 737\nglobal timeline, 739\nGranny, 42, 507, 516, 783, 790\nhand-drawn, 722\nHavok Animation, 42\nidle, 722\ninstancing, 754\ninterobject registration, 808\njoint,seejoint\nlook-at, 816\nlooping, 722, 734, 738\nmetachannel, 746\nmorph target, 724\nmotion capture, 6\nOrbisAnim library, 42\nparticle, 711\nper-vertex, 724\nphase, 738\nplayback rate, 739\npost-processing, 774\nprocedural, 775\nretargeting, 748\nrigid, 723\nrun cycle, 734\nsample, 737\nskeletal, 62\nsocket,seeanimation, attach point\nsynchronization, 742\ntexture, 723\ntime scaling, 736, 739\ntraditional, 722\ntransition, 758, 759\nupdating, 1089\nwalk cycle, 734\nIndex 1173\nanimation compression, 726, 777\nchannel omission, 778\ncurve-based, 783\nkey omission, 782\nquantization, 778\nsampling frequency, 782\nwavelet compression, 783\nanisotropic, seetexture, filtering\nANSI, 462\nanti-commutative, 372\nantialiasing, 48, 683\nAPI,seeapplication programming\ninterface\napplication binary interface, 176\napplication programming interface, 40,\n43, 975\napproximation\npiecewise-linear, 746, 970\narchitecture\nruntime, 38\narchive file, 505\narea light, seelighting\nArgand plane, 934\nargument (complex), 934\narithmetic/logic unit, 166\narray of structs, 1060\narticulatory speech synthesizer, 996\nartificial intelligence, 58, 538, 597, 822,\n849, 901, 997, 1000, 1004,\n1029, 1034, 1044, 1059, 1062,\n1066, 1105, 1131, 1164\nASM,seeaction state machine\nassembly language, 105\nassertion, 44, 86, 122, 125–127, 325\ncompile-time, 128, 129\nlock-not-needed, 325, 1102\nstatic, 128, 129\nasset conditioning pipeline, 59, 61, 499,\n501, 668, 671, 1036\nassociative, see alsocache\narray, 1139\nproperty, 933\nasymmetric multiprocessing, 228\nasynchronous, seemultitasking\natomic, 262, 264, 267, 289\ninstruction, 292\noperation, 262, 276, 549, 555variable, 314\nATRAC, seeaudio, file formats\nattach point, seeanimation\nattenuation\ndistance-based, 956, 957\naudible frequency band, 917\naudio,seealsosound, 54\namplitude panning, 959\nanalog, 942\nbeating, 919\ndecay, 921\ndensity, 921\ndiffuse tail, 920\ndiffusion, 921\nducking, 989\nearly reflections, 920\nexclusion, 968\nfile formats, 951, 952\ngroup, 989\ninstance limiting, 990\njack, 947\nlate reverberations, 920\nmixing, 956, 962, 975, 980, 981, 988\nobstruction, 968\nocclusion, 968\npan, 956, 957, 975\nconstant gain, 961\nconstant power, 961\nfocus, 963, 964\npan pot, 980\nperceived loudness, 914\nperception of position, 923\npre-delay, 921\nprocessing graph, 976\nrendering, 54, 911, 955\nspatialization, 956\nupdating, 1089\nvoice, 977\nvoice stealing, 991\naudio engine, 975\nQuake, 54\nScream, 54\nUnreal, 54\nX3DAudio, 993\nXACT, 993\nXAudio2, 54, 993\naugmented reality, 27\n1174 Index\nauxiliary send, 979\naverage\nweighted, 765\nAVX,seeadvanced vector extensions\naxis+angle rotation, 404\nazimuthal angle, 959\nband-limited, 948, 949\nbandwidth, seepipeline\nbank,seesound, bank\nbarrier\ncompiler, 303\nmemory, 310\nbarycentric coordinates, 766\nbase,see alsonumeric base\nbaseline, 717\nbatched updating, 1090\nbattle royale, seegenre\nbeating, seeaudio\nbel,seedecibel\nBézier triangle, 624\n“big O” notation, 445\nbig-endian, seeendian\nbilinear, seetexture, filtering\nbillboard, 49, 711, 714\nbinary,seenumeric base, 131\nbinary heap, 442\nbinary search tree, 442\nbinary semaphore, 276\nbinary space partitioning tree, 1018, see\ntree\nBink Video, 1162\nbiomechanical models, 43\nbit depth, 949\nbitwise operator, 176, 346\nbivector, 373\nbleach bypass, 48\nblend tree, 788, 792–796, 798, 801, 806\nblending, seealsoanimation\naudio, 957, 971\nstage (graphics), 676\nBlinn-Phong lighting model, see\nlighting\nblocking algorithm, 289\nblocking function, 245–247, 251, 268,\n270, 273, 289, 1102, 1110, 1111\nbloom,seelightingBlueprints, seeUnreal Engine 4\nBluetooth, 562\nBode plot, 939\nBoost library, 40, 45, 253, 254, 447–449,\n1081\nbounding box\naxis-aligned, 411, 831\noriented, 411, 832\nbounding sphere tree, seetree\nbounding volume, 688\nbranch dependency, 218\nbranch penalty, 218\nbranch prediction, 218, 219\nbrawler, seegenre\nBRDF,seelighting\nbreakpoint, seedebugger\nbrush geometry, 62, 1018\nBSP tree, seetree\nBSS segment, seeexecutable file\nBSSRDF, seelighting\nbubble-up effect, 1050\nbuild configuration\ndebug build, 86\ndevelopment build, 86\nhybrid build, 86\nrelease build, 82\nship build, 86\nbuild rules, 502\nBullet, 824\nbullet through paper, seetunneling\nbump mapping, seerendering\nbus\naddress, 174\naudio, 947, 978\nanalog, 983\ndigital, 983\nimplementation, 983\nlatency, 984\nmaster output, 982\npreset, 990\nvoice, 978\ndata, 174\nbusy-wait, 245, 251, 270, 273, 289, 294,\n298, 328, 555\nbyte, 132\nbyte code, 1135, 1145\nIndex 1175\nC standard library, 43, 89, 412, 465,\n486–488, 539, 589\nC++, 4\nbest practices, 105\nbitwise operator, 176, 346\nclass, 106\ndeclaration, 146–148, 150\ndefinition, 146, 148\ndelete, 156\nheader file, 79\ninheritance, 106\ninitialization order, 418\nmanaged, 489\nmultiple inheritance, 107\nnew, 156\nobject model, 1022\npostincrement, 445\npreincrement, 445\npreprocessor, 79\nprivate, 149\npublic, 149\nsource file, 79\nstandard library, 40, 89, 114, 269,\n448\nstandardization, 113\nstatic, 149, 157\ntranslation unit, 79, 144\nuser-defined literals, 459\nversions of, 113\nvirtual inheritance, 107\nvolatile ,seevolatile\nC4 Engine, seeTombstone Engine\nC#, 34, 531, 1063\ndelegate, 1115\nreflection, 1064, 1136\ncache, 191\ncoherency, 197, 630, 1092\ncoherency domain, 308\ncoherency protocol, 307\ncopy-back, 196\ndirect-mapped, 195\neviction, 195\nfriendly design, 1060\nhit, 191\nhit rate, 196\nline, 192\nMESI protocol, 307miss, 191, 198\nMOESI protocol, 307\nmultilevel, 196\nreplacement policy, 196\nset associativity, 195\nwrite-back, 196\nwrite-through, 196\ncall stack, 92, 153, 610\nstack frame, 153\ncallback function, 489, 492, 530\ncamera, 47, 622, 655, 1163\ndebug, 605\nimaging rectangle, 655\nCamtasia, 607\nCartesian coordinates, 360\nCastle Wolfenstein 3D, 31, 625\ncaustics, seelighting\nCCD,seecontinuous collision detection\nCD,seecompact disc\ncel animation, 722\ncell broadband engine, 228\ncenter of mass, 857\ncentral arbitor, 285\ncentral processing unit, seeCPU\nCg, 672\nCgFX, 682\nChain of Responsibility, 1122\nChandra-Misra, 285\nchange of basis, 388\nchannel, seeanimation\ncharacter dialog, 997\ncharacter set\nANSI, 462\nUnicode, 462\ncheats, 606\ncircular wait, 282\nclass, 106, 156\naggregation, 1049, 1051\ncomposition, 1049, 1051\nconstructor, 520\ncoupling, seecoupling\ndiagram, 107\nhierarchy, 1046\nbubble-up effect, 1050\nmonolithic, 1046\nUnreal, 1046\ninstance, 106, 156\n1176 Index\nmemory layout, 158\nmix-in, 107, 1049\npacking, 159\npure virtual function, 163\nreflection, 1064\nscripted, 1153\nvirtual function, 162\nclassical mechanics, 854\nClearCase, seeversion control\nclip,seeanimation, sound\nclipping, 411, 659, 674\nclipping plane, 46\nclock\nglobal, 741\nlocal, 741\nclosed hash table, seecontainer\ncloth\nrendering, 649, 670\nsimulation, 674, 818, 909\ncloud computing, 225\nclouds, 714\nCLR,seeCommon Language Runtime\nCM,seecenter of mass\ncode reuse, 38\ncode segment, seeexecutable file\ncodec, 979\nvideo, 1162\ncoding standards, 118\ncoefficient of restitution, 877\nCoffman conditions, 282\nCOLLADA, 682\ncollection, seecontainer\ncollinear vectors, 369\ncollision, see alsohashing\ncontact, 830, 849\ndetection, 42, 51, 52, 207, 359, 384,\n411, 527, 595, 823, 825\nshape, 829\nsweep and prune algorithm, 847\nupdating, 1089\ncolor, 622, 633\nchannel, 634\nlog-LUV, 634, 702\nmodel, 634\nRGB, 634\nspace, 634\nspectral, 633colorization, 719\ncommand line arguments, 473, 477\nCommon Language Runtime, 34, 489\ncommutative property, 368, 372, 933\ncompact disc, 948\ncompare-and-swap instruction, 295\ncompile-time assertion, seeassertion\ncompiled language, 1135\ncompiler, 78\nbuild configurations, 81\ndebugging information, 83\nGNU compiler, 83\noptimizations, 81, 84, seealsolinker\nalgebraic simplification, 85\ncode inlining, 85\ncompile-time vs. link-time, 85\nconstant folding, 85\nconstant propagation, 85\ndead code elimination, 85\ninstruction reordering, 85\nlocal vs. global, 84\nloop unrolling, 85\noperator strength reduction, 85\npeep-hole, 84\nprofile-guided, 85\noptimizing, 316\nproject configuration tutorial, 88\nproject files, 80\nsolution files, 80\nunresolved symbol error, 144\nwarnings, 81\ncompiler barrier, seebarrier\ncompiler intrinsic, 294, 295, 333\ncomplex exponential, 934\ncomplex instruction set, 223\ncomplex number, 934\nmultiplication, 935\nrotation, 935\ncomposition, 111, 1049, 1051, 1052, 1153\ncompression, seealsoanimation\ncompression\nlossless, 951\nquantization, 564, 778, 948, 949\ntexture, 642\ncompressor, 983\ncompute shader, 348, 350, 351, 672\nconcurrency, 204, 205, 256, 544\nIndex 1177\nmessage passing, 257\nmonitor, 288\nprogress, 289\nshared memory, 204, 257\ncondition variable, 273, 278, 548, 555\nconfiguration file, 471\nconfiguration space, 776\nconical sound source, 918\nconsensus problem, 300\nconservative advancement, 845\nconsistency model, seedata-centric\nconsistency model\nconsole\ngaming platform, 8\nin-game, 50, 604\nconstant folding, seecompiler,\noptimizations\nconstant power pan law, seeaudio\nconstant propagation, seecompiler,\noptimizations\nconstant register, seeGPU\nconstraint, 407, 806, 816, 854, 879, 883\nconsume memory order, seememory\nordering semantics\ncontact,seecollision\ncontainer, 40, 441\narray, 441\nbinary heap, 442\nbinary search tree, 442, 452\nbuilding custom, 447\ndeque, 442\ndictionary, 442, 452, 508, 1008,\n1065, 1141\ndynamic array, 442, 451\ngraph, 443\nhash table, 442, 452\nopen & closed, 452\nlist, 41, 442, 1141\nmap, 442\npriority queue, 442\nqueue, 442\nset, 443\nstack, 442\ntree, 442\nvector (STL), 41, 442\ncontention, 287\ncontext switch, 553continuity\nC0, C1, C2, 758\nmotion, 758\ncontinuous collision detection, 845\ncontinuous-time signal, seesignal\ncontrol dependency, 218\ncontroller, seehuman interface device\nconvergence, 863\nconversation, 1002\naction, 1009\nbranching, 1004\ncontext-sensitive, 1009\ncriterion, 1008\nexclusivity, 1004\nrule, 1008\nspeaker and listener, 1007\nconvex polyhedron, 412\nconvolution, 719, 930, 932, 933\ncooperative multitasking, 1140, 1154\ncoordinate system, 384\nCartesian, 360\ncylindrical, 360\nhierarchy, 388\nhomogeneous clip space, 659\nleft-handed, 361\nlight space, 655, 703, 704\nmodel space, 385, 630, 751\nright-handed, 361\nscreen space, 663, 716\nspherical, 360\ntangent space, 635\ntexture space, 641\nview space, 387, 656\nworld space, 386, 631\ncopy on write, 457\ncopy-back cache, 196\ncore,seeCPU\ncoroutine, 204, 554, 1140\ncoupling, 38, 46, 53, 66, 144, 1020, 1052,\n1095\nCPU, 166\naddressing mode, 178, 180\ncore, 226, 353\nCPU-dependent game, 536\nmicro-operations, 172\nout-of-order execution, 171, 217,\n222, 224, 225, 239, 291,\n1178 Index\n300–302, 304, 309, 311, 315,\n355\npipelined, 171, seealsopipeline,\n211, 213, 217, 221, 225, 445\nregister, 153\nspeculative execution, 239\nstage, 212, see alsoGPU, stage\nsuperscalar, 211, 221\nutilization, 1105\nvery long instruction word, 211,\n224\ncrash report, 594\ncritical\noperation, 262, 264, 267, 289, 302,\n303, 325, 327\nrace, 258\nsection, 270\ncross product, seevector\ncross-fade, 758, 759, 786, 1001\ncrowd modeling, 996\nCrystal Space, 36\nCSV file, 467, 614\ncube map, 700\ncue,seesound\nculling, 15, 47\nantiportal, 689\nback-face, 627\nfrustum, 688, seefrustum\nocclusion, 15, 18, 47, 688\nportal, 15, 689\npotentially visible set, 688\nvisibility determination, 688\ncurve, 759\ncvar, 474\nCVS,seeversion control\ncylindrical coordinates, 360\nD-cache, seedata cache\nDAC,seedigital-to-analog conversion\nDAG,seedirected acyclic graph\ndamping, 860\ndashpot, seedamping\ndata cache, 196\ndata definition language, 1135\ndata dependency, seedependencies\ndata parallelism, seeparallelism\ndata pathway, 1132data race, 205, 259, 281\ndata segment, seeexecutable file\ndata type, 138, 146\ndata-centric consistency model, 266\ndata-driven, 12, 1024\ncost of, 1024\nDCC,seedigital content creation app\ndead code elimination, seecompiler,\noptimizations\ndeadlock, 275, 281\ndebug build, seebuild configuration\ndebug drawing, 50, 595\ndebugger, 50, seealsocall stack\nbreakpoint, 92, 95–97\ndebugging optimized builds, 97\nsingle-step, 92\nwatch window, 93\ndecal, 48, 712\ndecay,seeaudio\ndecibel, 914, 915\ndecimal, seenumeric base\ndeclaration, seeC++\ndeclarative language, 1136\ndecode,seeCPU, stage\ndecomposition, seetask decomposition\ndeep copy, 1127\ndeferred rendering, 709\ndefinition, seeC++\ndeformable body, 825, 903, 909\ndegree of parallelism, 1105\ndegrees of freedom, 406, 856, 883\nDelaunay triangulation, 766\ndelay slot, 215, 225\ndelegate (C#), 1115\ndelete, seeC++\ndemodulation, 951\ndenormalized, seefloating-point\ndensity,seeaudio\ndependencies, 214, 222\ncycle, 38\ndata, 215\nengine subsystems, 1093\ngame objects, 1093\nthread,seethread\ndependency graph, 1105\ndepth buffer, 664, 704\nshadow mapping, 704\nIndex 1179\ntesting, 676, 716\nz-testing, 675, 692, 693\ndepth of field blur, 719\ndeque, 442\nderivative of a vector, 858\ndesign pattern, 111\nabstract factory, 111, 1042\nChain of Responsibility, 1122\ncommand, 1117\nfactory, 1065\niterator, 111, 118, 430, 443, 445,\n1149\njanitor, 111, 112, 320, 327\nRAII, 111, 125, 320, 327\nsingleton, 111, 418\ndestructible objects, 903, 1018\ndevelopment build, seebuild\nconfiguration\ndevelopment kit, 461, 616\ndevice driver, 975\ndialog action, 1009\ndialog system, 997\ndiamond inheritance problem, 1049\ndictionary, seecontainer\ndifference clip, 769\ndiffraction, seelighting, 917, 970\ndiffuse\nlighting, seelighting\ntail, 920\ntexture map, 640\ndiffusion, seeaudio\ndigital content creation app, 59, 62, 501,\n669\nDigital Molecular Matter, 825\ndigital signal processing, seesignal\nprocessing\ndigital-to-analog conversion, 951\ndining philosophers, 285\ndirect lighting, seelighting\ndirect memory access controller, 984\ndirected acyclic graph, 61, 443, 1109\ndirected graph, 1109\ndirection vector, 365\ndirectional light, seelighting\ndirectional sound source, 918\nDirectX, 41, 46, 91\nperspective projection matrix, 660view space, 387\nDirichlet conditions, 938\ndisassembly, 98\ndiscrete-time signal, seesignal\ndisk operating systems, 233\ndispatching, 224\ndisplacement mapping, seerendering\ndisplay device, 622\ndistance field, seesigned distance field\ndistance-based attenuation, see\nattenuation\ndistributed computing, 229\ndistributed shared memory, 257\ndistributive property, 368, 372, 933\ndivision by w, 381\nDivX, 1162\nDMAC,seedirect memory access\ncontroller\nDMM,seeDigital Molecular Matter\nDOC,seedegree of parallelism\nDOF,seedegrees of freedom, depth of\nfield\nDolby Digital, seeaudio, file formats\nDoom, 11, 31, 625, 1025\nDOP,seedegree of parallelism\nDoppler effect, 922, 956, 957, 973\ndot product, seevector\ndouble dispatch, 842\nDRC,seedynamic range compression\ndry sound, seesound\nDSP,seesignal processing\nDTS,seeaudio, file formats, see\nsurround sound\nduality, 940\nduck typing, 1141\nducking, seeaudio\ndynamic allocation, seeallocation\ndynamic array, 442\ndynamic link library, 80\ndynamic range compression, 983\ndynamics (physics), 854\nearly reflections, 920\nease curve, 759\necho, 920\nEdge,seePlayStation Edge library\neffect file, seeshader\n1180 Index\neffective sound pressure, seesound\nELF,seeexecutable\nelision, 115\nemissive, seelighting\nencapsulation, 106\nendian, 140\nbig, 140\nlittle, 140, 334\nmultibyte quantity, 140\nswapping, 141\nEndorphin, 43\nenergy, 875\nengine, 11\nline between engine and game, 11\nreusability gamut, 12\nengine configuration, 470\nOGRE, 475\nQuake, 474\nUncharted/TLOU, 475\nenvironment map, 48, 700, 706\nenvironment variable, 236, 473\nEQ,seeequalizer\nequal-loudness contour, 916\nequalizer, 940, 941, 982\nequilibrium, 882\nerror,seealsocompiler\nconditions, 119\nnumerical methods, 863\nquantization, 949\nEuler angles, 386, 403\nEuler’s formula, 936, 937\nEuphoria, 43\nevent, 57, 531, 1040, 1114\nanimation trigger, 746\narguments, 1118\ndata-driven, 1131\ndebugging, 1128\ndriven, 37, 57, 1114, 1122\nforwarding, 1116, 1122\nGUI-based, 1133\nhandling, 1120, 1151\nmemory allocation, 1128\nobject, 279\npriorities, 1126\nqueuing, 1124\nregistering interest, 1123\nsending, 1152trigger, 746\ntypes, 1117\nevict,seecache\nexception handling, 123\nexception object, 123\nexchange instruction, 295\nexclusion, seeaudio\nexecutable\nBSS segment, 152\ncode segment, 151\ndata segment, 151\nELF format, 151\nfile, 79, 151\nread-only data segment, 152\nrodata segment, 152\nsegment, 151\ntext segment, 151\nexplicit parallelism, 206, 225\nexplosions, 902\nexponent, seefloating-point\nexpression tree, 792\nF#, 1136\nfacing, 369\nfact dictionary, 1008\nfactory,seedesign pattern\nfall-off,seesound, pressure\nfan,seetriangle\nfast Fourier transform, 939\nfeedback, 927\nfence,seealsobarrier,seememory\nordering semantics\nfetch,seeCPU, stage\nFFT,seefast Fourier transform\nfiber, 204, 554\nfield of view, 46\nfighting game, seegenre\nfile I/O\nasynchronous, 489, 1072\nbuffered, 486\ndeadline, 492\nsynchronous, 488\nfile scope, seescope\nfile system, 482\npath, 458, 482, 484\nsearch path, 485\nwrapping, 487\nIndex 1181\nfilter, 940, 979\ncomb, 920\nlow-pass, 940\nnotch, 940\npassband, 940\npost-send, 980\npre-send, 980\nstopband, 940\ntexture,seetexture, filtering\nfinite state machine, 786, 1042, 1087,\n1154, 1157\nfirst-person shooter, seegenre\nfixed-function pipeline, 672\nfixed-point, seeinteger, 133\nflashlight, seelighting\nflat weighted average, seeanimation\nfloating-point, 133\ndenormalized, 136\nexponent, 134\ninfinity, 135\nmachine epsilon, 136\nmagnitude, 134\nmantissa, 134\nnot-a-number, 135\nprecision, 134\nsignificant digits, 134, 135\nsubnormal, 136\nunit in the last place, 137\nfloating-point unit, 166\nflow control, 1157\nFLT_MAX, 135\nfluid dynamics, 715, 910, 924\nfluid simulation\nposition based, 910\nflushed, 218\nFlynn’s taxonomy, 207\nFMOD Studio, 995\nFMV,seefull-motion video\nFName, 459\nFO min and max, seesound, fall-off\nfocus,seeaudio\nFolly library, 40, 45, 450\nfont rendering, seetext rendering\nfoot sliding, 813, 815\nforce, 373, 859\nas a function, 860\nforward kinematics, 775Fourier transform, 933, 938, 940\nFPS,seegenre,seeframe rate\nFPU,seefloating-point unit\nfragment, 664\nframe,seeanimation\nframe buffer, 663\nback buffer, 538\ndouble buffering, 663\nfront buffer, 538\nswapping, 538\ntriple buffering, 664\nframe rate, 10, 534, 736\ngoverning, 537, 538\nframe-to-frame coherency, seetemporal\ncoherency\nframework, 529\nFraps, 607\nfree store, seealsoallocation, 156\nfrequency, 913\nangular, 913\ndomain, 933, 938\nfundamental, 938\nresponse, 920, 940\nfriction, 880\nfrustum, 47\nFSM,seefinite state machine\nfull fence, 310, seememory ordering\nsemantics\nfull-motion video, 49, 735, 1019, 1161\nfunction\nsignature, 146\nfunctional language, seelanguage\nFusion, 37\nfutex, 271\nFx Composer, 670\nG-buffer, 709\nG-factor, 1015\ngain, 926, 946, 979, 983\ngame\ndefinition of, 8\ngame controller, seehuman interface\ndevice\ngame development team\nartist, 6\ncomposer, 6\nengineer, 5\n1182 Index\ngame designer, 7\nproducer, 7\npublisher, 8\nsound designer, 6\nvoice actor, 6\nwriter, 7\ngame engine, seeengine\ngame flow control, 1157\ngame loop, 526, 544, 899\nparallelizing, 545\npausing, 605\nPong, 527\nsingle-stepping, 532, 605\nsleep,537\nslow motion, 605\ngame object, 1021\nasynchronous updating, 1102\nattribute, 1021\nbatched updating, 1090\nbehavior, 1021, 1041\ndependencies, 1093\ninstance, 1021\nlinkage to engine, 1041\npersistence, 1042\nqueries, 1042\nquery, 1085\nreference, 1150\nreferences, 1042, 1079\nspawning, 1040\nstate, 1087, 1100\nstate caching, 1100\ntime-stamping, 1101\ntype, 1021\ntypes, 1041\nunique id, 458, 1041, 1079, 1086\nupdating, 527, 1040, 1086, 1093,\n1095\ngame object model, seeobject model\ngame state, 1016\ngame world, 9, 56, 64, 1016, 1018\nchunk, 1019, 1029, 1062\neditor, 59, 64, 714, 1021, 1023, 1025,\n1030–1033, 1035, 1043\nHammer, 64, 1025\nRadiant, 64, 1025\nSandbox, 1025\nUnrealEd, 64, 66, 1025gameplay, 1015\nflow, 1015, 1019, 1040\nfoundation system, 55, 1039\nhub, 1019\nlinear, 1019\nobjectives, 1015, 1019, 1040\nopen world, 1019\nplayer mechanics, 55, 1015, 1162\nregion, 1033\ntask, 1019\nwater, 715\nGameSalad, 37\ngamma\ncorrection, 718\nresponse curve, 718\ngamut,seelighting\ngarbage collection, 1081\nGaussian elimination, 378\ngcc,seeGNU C/C++ compiler\ngeneral-purpose GPU programming,\nseeGPGPU\ngenre, 13\nbattle royale, 24\nbrawler, 19\nfighting, 17\nfirst-person shooter, 14\nmassively multiplayer online\ngame, 9, 13, 23\nplatformer, 15\nracing, 19\nreal-time strategy, 21\nsimulation, 31\nstrategy, 21\nturn-based strategy, 21\ngeometry, seealsomesh\nbrush,seebrush geometry\ngimbal lock, 403, 404, 871\ngit,seeversion control\nGJK algorithm, 839\nGlide, 41\nglobal illumination model, seelighting\nglobal namespace, 118\nglobal optimizations, seecompiler,\noptimizations\nglobally unique identifier, 458, 507, 517\ngloss map, 699\nglyph atlas, 716\nIndex 1183\nGNU C/C++ compiler, 78, 336\nGouraud shading, 637\nGPGPU, 41, 348, 672\nGPU, 672\ncommand list, 692\ncompute unit, 353, 354\nkernel, 350\nlock step execution, 355\nregister, 678\nSIMD unit, 350, 354\nstage, 355\nthread, 354, 355\nthread block, 353\nthread group, 353, 354\nwarp, 355\nwavefront, 355\ngrammar, 792\nGranny, 42, 507, 516, 783, 790\ngraphical shading language, 670\ngraphical user interface, 49\ngraphics processing unit, seeGPU\ngravity, 856, 875, 888\ngrenade physics, 902\ngroup,seeaudio\nGUI,seegraphical user interface\nGUID,seeglobally unique identifier\nH.26x, 1162\nHadamard product, 364, 676\nhair, 647, 674, 710, 909\nHalf-Life 2, 32, 33\nHammer, seegame world editor\nhandle, 440, 554, 1042, 1079, 1082\nsound, 987\nstale, 1083\nhardware T&L, 41, 672\nharmonic, 938\nscaling in time domain, 974\nhashing, seealsocontainer, 453\nchaining, 453\ncollision, 452, 455, 459\nlinear probing, 455\nopen addressing, 453\nquadratic probing, 455\nRobin Hood hashing, 456\nHaskell, 1136\nHavok, 42, 824Havok Animation, 42\nHDMI, 954\nhead-related transfer function, 924\nheader file, seeC++\nheadphones, 944\nheadroom, 962\nheads-up display, 49\nheap memory, 156\nheight field, seeterrain\nheightmap, seetexture\nHeroEngine, 35\nHertz, 534\nheterogeneous system architecture, 678\nhex editor, 102\nhexadecimal, seenumeric base, 132\nHID,seehuman interface device\nhierarchy, see alsocoordinate system, see\nalsojoint\nhigh dynamic range, seelighting\nhigh water mark, 451, 616\nhit rate,seecache\nhold and wait, 282\nhomogeneous coordinates, 379\nhorizontal add, seesingle instruction\nmultiple data\nHoudini, 64\nHRTF,seehead-related transfer function\nHSA,seeheterogeneous system\narchitecture\nHTML, 1136\nHUD,seeheads-up display\nhUMA,seeheterogeneous system\narchitecture\nhuman interface device, 53, 559\nabstract control indices, 584\naccelerometer, 566, 567\nactuator, 570\nanalog axis, 564\nanalog input, 564\nand game loop, 527\naudio, 570\nbutton, 563, 564\nchord, 575\ncontext-sensitive controls, 585\ncontrol mapping, 584\ncross-platform, 582\ndead zone, 571\n1184 Index\ndisabling, 586\nDualShock, 562, 566, 567\nforce-feedback, 53, 570\ngesture, 577\ninfrared sensor, 567\ninput event, 574\ninterrupt, 562\nkeyboard, 54\nmouse, 566\nmultiplayer, 582\nPlayStation Eye, 568\npolling, 561\nrelative axis, 566\nrumble, 569\nsequence, 577\nsignal filtering, 572\nsystem requirements, 571\nWiimote, 559, 566, 567\nXInput, 562, 563\nhybrid build, seebuild configuration\nHydroThunder , 1044\nhyperthreading, 225\nhysteresis, 971\nI-cache,seeinstruction cache\nI-Collide, 52\nIDE,seeintegrated development\nenvironment\nIGC,seein-game cinematic\nIID,seeinteraural intensity difference\nIK,seeinverse kinematics\nimage\nbitmapped, 634\nbits per pixel, 634\nof program, 151\nsampling, 683\nimage-based lighting, seelighting\nimaginary number, 934\nimperative language, 1136\nimplicit parallelism, 206, 211\nimpulse\nphysics, 876, 877\nresponse, 928, 932, 966\nunit,seeunit impulse\nimpulse response, 931\nin-game cinematic, 49, 735, 991, 1019\nindependent variable, 924index buffer, 628, 630\nindirect lighting, seelighting\ninertia tensor, 870\ninheritance, 106, 162, 1022, 1067, 1153\ndeadly diamond, 107\ndiamond problem, 1049\nmultiple, 107, 162, 1049\nvirtual, 107\ninitialization order (C++), 418\ninline function, 83, seecompiler,\noptimizations, 148\ninner product, 367\ninput register, seeGPU\ninstance limiting, seeaudio\ninstanced geometry, 1018\ninstancing, seeanimation\ninstantaneous acoustic pressure, see\nsound\ninstruction cache, 196\ninstruction reordering, seecompiler,\noptimizations\ninstruction stream, 354\ninstruction-level parallelism, 214\nInsure++, 50, 102\ninteger, 132\nfixed-point, 133\nsign and magnitude, 132\nsign bit, 132\nsigned, 132\ntwo’s complement, 132\nunsigned, 132\nintegrated development environment,\n78\nintegration\nexplicit Euler, 366, 535\nnumerical, 10, 535\nintensity, seelighting\ninter-object communication, seeevent\nsystem\ninteraural\nintensity difference, 959\ntime difference, 923\ninterconnect bus, 308\ninterface, 118\ninterference, 919, 944\nconstructive and destructive, 919\ninterpolation\nIndex 1185\nlinear, 375, 400, 546, 755\nspherical linear interpolation, 401\ntemporal, 757\nvertex attributes, 635\ninterpreted language, 1135\ninterrupt, 95, 186, 232, 234, 247, 292\nservice routine, 177, 562\ninterruption\nof character dialog, 1001, 1003,\n1004\nof critical operation, 261, 264, 291\nintersection, seealsocollision, 829\nAABB versus AABB, 838\npoint versus sphere, 836\nsphere versus sphere, 836\nintrinsic, seecompiler intrinsic\ninverse kinematics, 775, 811\ninvocation, 262\nIrrlicht, 36\nISR,seeinterrupt, service routine\nITD,seeinteraural time difference\niterator,seedesign pattern\njanitor,seedesign pattern\nJava, 1063\njob, 549\ncounter, 554\ndeclaration, 549\nkicking, 549\nsynchronization, 555\nsystem, 545, 549, 1102\njoin,seethread\njoint,seealsoanimation\ncoordinate, 730\nindex, 728\nname, 728\nnonuniform scale, 731\nparent, 728\nroot, 728\nscale, 731\njoystick, seehuman interface device\nkd-tree, 47, 695\nkernel,seeGPU\nkernel call, 267\nkerning, 717\nkey frame, 736key-value pair, 452, 508, 1031, 1032,\n1065, 1119, 1139\nkeyboard, seehuman interface device\nkicking a job, seejob\nKillzone2 , 709\nkinematics, 785\nend effector, 775\nforward, 775\ninverse, 775, 811\nKismet,seeUnreal Engine 4, Blueprints\nKynapse, seeartificial intelligence\nL1 cache, seecache\nlambda, 1144\nlane,seesingle instruction multiple data\nlanguage, 106\ncompiled, 1135\ndeclarative, 1136\nfunctional, 1101, 1136\nimperative, 1136\ninterpreted, 1135\nobject-oriented, 106, 1136\nprocedural, 1136\nreflection, 1022, 1042, 1052, 1064,\n1065, 1136\nlate function binding, 1115\nlate reverberations, 920\nlatency\naudio bus, 984\nmemory access, 188\npipeline, 214\nLeadWerks Engine, 35\nleft-hand rule, 371\nlevel of detail, 625, 714, 715\nlexically scoped, seescope\nLFE,seelow-frequency effects\nlibgcm, 41\nlibrary, 79, 529\nlifetime\ndebug primitive, 597\nresource, 503\nlight map, 48, 653, 672\nlighting, 633, 647\nabsorption, 633\nambient, 649, 654\nambient occlusion, 705\narea light, 655\n1186 Index\nBlinn-Phong lighting model, 652\nbloom, 48, 655, 702\nBRDF, 652\nBSSRDF, 707\ncaustics, 706\ndiffraction, 633\ndiffuse, 649\ndirect, 647\ndirectional, 654\nemissive, 655\nflashlight, 655\ngamut, 633\nglobal illumination model, 648,\n702\nhigh dynamic range, 634, 655, 702\nimage-based, 698\nindirect, 648, 702\nintensity, 633\ninteractions with matter, 633\nlocal illumination model, 647\nmedium, 633\nper-pixel shading, 637\nPhong reflection model, 649\npoint light, 654\nprecomputed radiance transfer,\n708\nradiosity, 648\nray tracing, 648\nreflection, 633, 706\nrefraction, 633\nscattering, 634, 707\nsource, 622, 653\nspec map, 699\nspecular, 649\nspecular power map, 670, 699\nspot light, 654\nstatic, 653, 672\ntransmission, 633\ntransport model, 622, 647\nviewing direction, 650\nwavelength, 633\nLightwave, 669\nline, 408\nline of sight, 595, 849, 1103\nline-level, 945\nlinear, 926\nalgebra, 359approximation, 625, 745\nmomentum, 859\nprobing, seehashing\ntime-invariant system, 926\nvelocity, 858\nlinearizability, 266\nlink register, 297\nlinkage, 149\nexternal, 149\ninternal, 149, 151, 153\nlinker, 78\noptimizations, 85\nLisp, 477, 797, 1144\nlist,seetriangle, seecontainer\nlistener, 956\nlittle-endian, seeendian\nlivelock, 283\nload linked/store conditional, 297\nlocal illumination model, seelighting\nlocal optimizations, seecompiler,\noptimizations\nlocal variable, seevariable\nlocalization, 456, 462, 466\nlocation tag, 1009\nlocation-based entertainment, 30\nlocator, 747\nlock-free algorithm, 267, 287, 290, 328,\n1102\nlock-not-needed assertion, seeassertion\nlocomotion\ncycle, 722\nnoise, 773\npivotal, 761\ntargeted, 761\nLOD,seelevel of detail\nlogarithmic, 915\nlogging, 589\nchannel, 592\nto file, 593\nverbosity, 591\nLoki library, 40, 450\nlook-up table, 517, 521, 522, 640, 674\nloop unrolling, seecompiler,\noptimizations\nlooping, seeanimation\nlossless compression, seecompression\nloudness, seeaudio\nIndex 1187\nlow-frequency effects, 944, 959, 982\nlow-pass filter, 572\nLPCM,seepulse-code modulation\nLTI system, seelinear time-invariant\nsystem\nLU decomposition, 378\nLua, 1139\nLumberyard, 36\nLUT,seelook-up table\nmachine epsilon, seefloating-point\nmacro, 333\nMacromedia Fusion, 37\nmadd instruction, 340\nmagnitude, see alsofloating-point\ncomplex, seecomplex number\nvector,seevector\nmake, 86\nmanaged C++, 489\nmanager, 418\nmanifest constants, 152\nmantissa, seefloating-point\nmanycore, see alsoGPU, 210\nmass, 857\nmassively multiplayer online game, see\ngenre\nmaterial, 682\neditor, 670\nsystem, 46\nvisual, 646, 670\nmath library, 44\nmatrix, 375\n33, 380, 404\n43, 384\n44, 343, 363, 376, 380, 382, 390,\n430, 730, 756, 769\naffine, 376\ncamera-to-world, 46\ncolumn matrix, 377\nconversion to quaternion, 399\nidentity, 378\nin-memory representation, 392\ninverse, 378\nisotropic, 376\njoint-to-model, 752\nmodel-to-world, 369, 390, 631, 754\northographic projection, 662orthonormal, 376\nperspective projection matrix, 660\nproduct, 376\npure rotation, 404\nrow matrix, 377\nspecial orthogonal, 376\ntranspose, 379\nview-to-world, 656\nworld-to-view, 656\nmatrix palette, 753\nMaya, 62, 669\nmechanics\nclassical, 854\nmedium, seealsolighting\nMEL language, 502, 1134\nMeltdown exploit, 239\nmemory\naccess cycle, 293\naccess patterns, 426\nalignment, 159, 333, 431\ncache,seecache\ncard, 570\ncontroller, 160, 166, 176, 184, 212,\n227, 291, 356\ncorruption, 101\ndebug memory, 461\ndefragmentation, 439, 1077\nfence, 304, seealsobarrier, 316\nfragmentation, 437\nin-game statistics, 615\nleak, 101, 615\nmanagement, 44, 426, 511\nmanagement unit, seememory,\ncontroller\nordering bugs, 291\nordering semantics, 305, 311, 315\nacquire, 311, 312, 316\nconsume, 316\nfull fence, 311, 316\nrelaxed, 315\nrelease, 311, 312, 316\nrelocation, 439, 987, 1077\nshared,seeconcurrency\nsmall memory allocator, 1076\nstick, 487\nvirtual, 437, 615\nmenu\n1188 Index\nin-game, 50, 475, 601\nmesh, 61, 121, 625\nconstructing, 627\ninstance, 631, 1090\nprogressive, 626\nstatic, 1018, 1029\nsubmesh, 646\nMESI, 197\nMESIF, 197\nmessage, see alsoevent\nmap, 1121\npassing, seeconcurrency\npump, 46, 529\nmetric,seeSI units\nmic-level, 945\nmicro-operations, 172\nmicrophone, 942\nMicrosoft Excel, 467, 614\nMicrosoft Visual Studio, 70, 78\nMiles Sound System, 995\nMIMD,seemultiple instruction\nmultiple data\nMinkowski sum/difference, 839\nmipmapping, 643, 679\nMISD,seemultiple instruction single\ndata\nmix snapshot, 990\nmix-in class, 1049\nmixed reality, 27\nmixing,seeaudio\nMKS system of units, 856\nMMO,seegenre\nMMX,seestreaming SIMD extensions\nmod community, 11\nmodel\n3D,seemesh\nanalytic, 10\nclosed-form, 10\nmathematical, 9\nnumerical, 10\nmodeler (3D), 6\nmodulation, 940\nMOESI, 197\nmoiré pattern, 643\nmoment of inertia, 867\nmomentum\nangular, 872linear, 859\nmonitor, seeconcurrency\nMonoGame, 34\nmonolithic class hierarchy, 1046\nmotion blur, 719\nmotion capture, 6\nmovie capture, 607\nmovie player, 1161\nmoving average, 537, 572\nMP3,seeaudio, file formats\nMPEG, 1162, see alsoaudio, file formats\nmultibyte quantity, seeendian\nmulticore, 210, 226, 437\nmultilevel cache, seecache\nmultiplayer, 55\nhuman interface devices, 582\nnetworked, 55, 1162\nreplication, 1042\nsplit-screen, 55\nmultiple inheritance, 1049\nmultiple instruction multiple data, 207,\n348\nmultiple instruction single data, 207\nmultiplication as complex rotation, 935\nmultiply defined symbol error, 146\nmultitasking, 540, 545\nasynchronous programming, 1102\ncooperative, 234, 250, 554, 1140,\n1154\nGPU, 672\ninterfacing with game object\nupdate, 1102\njob, 545\npreemptive, 40, 230, 234, 247, 250,\n554, 1140\npthreads, 545\nsleep, 492, 537, 546\nSPURS, 545\nthread, 487, 492, 545\nmusic, 1010\nmutex, 112, 245, 264, 267–269, 271, 273,\n282, 292, 555\nmutual exclusion, seemutex\nnamespace, 118\nnatural number, 132\nNDEBUG, 82\nIndex 1189\nnearest neighbor, seetexture, filtering\nnegative reinforcement, 919\nnew,seeC++\nNewton’s law of restitution, 876\nNewton’s laws of motion, 859\nNewtonian mechanics, 854\nnon-blocking algorithm, 289, 290\nlock-free, 289\nobstruction-free, 289\nwait-free, 289\nnon-blocking function, 268, seealso\nblocking function, 1102\nnon-player character, 15, 56, 734, 742\nnoninteractive sequence, 735\nnonuniform rational B-spline, 624\nnormal,seealsovector\nmap,seetexture\nof plane, 369\nNovodex, seePhysX\nNPC,seenon-player character\nNTSC, 534, 542, 663\nnumeric base\nbinary, 131\ndecimal, 131\nhexadecimal, 132\nnumerical methods, 863\nNURBS, seenonuniform rational\nB-spline\nNyquist frequency, 949\nOBB,seebounding box\nobject file, 79\nobject model, seealsogame object model\nC++, 1022\ncomponent, 1052, 1055\ncomponent creation, 1053\ncomponent ownership, 1053\nExcel, 1022\ngame, 56, 1022, 1040\nHydroThunder , 1044\ninterface, 1022\nmultitasking, 1107\nobject-centric, 1043, 1044\nOMT, 1022\nproperty-centric, 1043, 1060\nruntime, 1023, 1043\nsoftware, 1022tool-side, 1023\nunique ids, 1056\nobject-oriented, seelanguage\nobstruction, seeaudio\nobstruction-free algorithm, 290\nOcaml, 1136\nocclusion, seeaudio\noctal,seenumeric base\noctree, 47, 694\nODE,seeOpen Dynamics Engine\nOgg Vorbis, 953\nOGRE, 36, 45, 47, 91, 139, 140, 789\none-frame-off bug, 1099\nOOO execution, seeCPU\nopacity, 623\nopen addressing, seehashing\nOpen Dynamics Engine, 42, 823\nopen hash table, seecontainer\nopen-source software, 34\nOpenAL, 994\nOpenGL, 41, 46\nperspective projection matrix, 660\nview space, 387\nview volume, 659\nOpenTissue, 825\noperating system, 40, 43, 79, 80,\n151–153, 156\nDOS, 483\nMac OS, 483\nMicrosoft Windows, 483\nUNIX, 483\noperator overloading, 445\noperator strength reduction, see\ncompiler, optimizations\noptical audio connector, 954\noptimization, seealsocompiler,\noptimizations, 99, 165, 216,\n317, 447, 608\nOrbisAnim library, seeanimation\nordinary differential equation, 860, 869,\n873\northogonal, 363\nOS,seeoperating system\nout-of-order execution, seeCPU\nouter product, 367, 370\noutput register, seeGPU\noverdraw, 692\n1190 Index\noverlay, 49, 657, 716, seerendering\ngraphical user interface, 32, 80, 111\nheads-up display, 657\npackage file, 497, 499, 506\npage fault, 186, 239\npainter’s algorithm, 664\nPAL, 534, 542, 663\npan,seeaudio\nPanda3D, 36, 1143\nparallax occlusion mapping, see\nrendering\nparallelism, 203, 205\ndata, 207, 348, 544, 546\nexplicit, 206, 225\nimplicit, 206, 211\ntask, 207, 544, 545\nparallelogram\narea of, 371\nparametric equation, 408, 848\nsurface, 624\nPareto principle, see80/20 rule\nparticle system, 46, 48, 64, 711, 1089\npassing\nof reads and writes, 310\npatch, 624\nBézier, 624\nbicubic, 624\nN-patch, 624\nnonuniform rational B-spline, 624\npath tracing, 970\nPawn, 1141\nPCM,seepulse-code modulation\npeep-hole optimization, seecompiler,\noptimizations\npenumbra, 655, 703\nper-pixel shading, seelighting\nper-user options, 474\nperceived loudness, seeaudio\nperception of position, seeaudio\nperceptual coding, 952\nPerforce, seeversion control\nperiodic, 913, 938\nperiphonic, 964\nperpendicular\naxes, 360, 837\ndistance, 410, 658, 688vectors, 363, 369, 370, 392, 635, 638\nwave, 917\npersistence, 1042\nperspective, 657\nphantom image, 959\nphase,see alsoanimation\nof complex number, 934\nshift, 913, 919\nvocoder, 974\nPhong reflection model, seelighting\nphotorealism, 622\nPhyreEngine, 34\nphysically based sound synthesis, 996\nphysics, see alsocollision, 51, 527, 674\nand fun, 819\nHavok, 52\nlibrary, 42\nHavok, 42\nOpen Dynamics Engine, 42\nPhysX, 42\nOpen Dynamics Engine, 52\nPhysX, 52\nrigid body dynamics, 51, 726, 854\nsimulation, 51\nupdating, 1089\nwater, 715\nworld, 828\nPhysics Abstraction Layer, 825\nPhysX, 42, 824\npiecewise-linear approximation, 746,\n970,seeapproximation\npinnae, 924\npipeline, 213, 667, 672, 976, 1092\nasset conditioning, seeasset\nconditioning pipeline\nbandwidth, 214\nlatency, 214, 667\nthroughput, 214, 667\ntools,seeasset conditioning\npipeline\npitch, 386\npixel, 622, 655\nplacement new, 520\nplain old data structure, 114, 520\nplan view, 657\nplane, 369, 409, 658\ndistance to origin, 409\nIndex 1191\npoint-normal form, 409, 658\nplatform independence layer, 43\nplatformer, seegenre\nplayer I/O, seehuman interface device\nplaylist, 1011\nPlayStation Edge library, 41, 42\nPlayStation Network, 481, 487\nPOD,seeplain old data structure\npoint, 360\narithmetic, 365\npoint light, seelighting\npointer fix-up, 519\npolar pattern, 942\npolarization, 917\npolygon\nrendering, 624, 625\nsoup, 854\npolyhedron\nconvex, 412\npolymorphism, 109, 162\npolyphony, 977\nPortable Network Graphics, 642\nportal, 47\nculling, 689\nsound, 972\npose\nas change of basis, 732\nbind pose, 728, 729, 751\ncurrent, 729\ncurrent pose, 751\nglobal, 733\nin-memory representation, 732\ninterpolation, 736\nlocal, 730\nT-pose, 729\nposition based fluid simulation, 910\npositive reinforcement, 919\npost effect, 48, 719\npost-load initialization, 521\npostincrement (C++), 445\npotentially visible set, 47\npotentiometer, 947\npower, 961\npower processing unit, seePPU\npre-amp, 982\npre-delay, seeaudio\nprecision, seefloating-pointprecomputed radiance transfer, see\nlighting\npredication, 221, 346\nvector, 344, 346, 347\npreemption, seemultitasking\npreincrement (C++), 445\npreprocessor, seeC++\npressure, see alsosound, 912\nprimitive\ndebug drawing, 597\ngeometric, 46, 646\nmesh-material pair, 646\nsubmission, 691\nprimitive data type, 146\nprintf debugging, 589\npriority, see alsothread\ndialog, 1001\ninversion, 284\nvoice, 991\nprivate,seeC++\nprocedural language, 1136\nprocessor utilization, 545, 1105\nproducer-consumer problem, 271\nprofile trap, 557\nprofile-guided optimizations, see\ncompiler, optimizations\nprofiling tools, 50, 99\nhierarchical, 609\nin-game, 608\ninstrumenting, 100, 611\nstatistical, 100\nprogram order, 262\nprogram stack, seecall stack\nprogress, seeconcurrency\nprojection, 368, 657\northographic, 21, 657, 662, 716,\n1029\nperspective, 22, 657, 660, 661, 1029\nperspective foreshortening, 657,\n661\nProlog, 1136\npropagation modeling, 965\nwith LTI system, 966\nproperty class, 1059\nproperty grid, 1031\nproperty object, 1059\nversus component, 1059\n1192 Index\nproperty-centric, seeobject model\npseudovector, 362\nPUBG,seegenre, battle royale\npublic,seeC++\npulse-code modulation, 948\npunning, seetype punning\npure component model, 1056\nPurify, 50\npush lock, 324\nPVS,seepotentially visible set\nPython, 1134, 1140\nmethod table, 1144\nquadratic probing, seehashing\nquadtree, 47, 694\nQuake C, 1138\nQuake Engine, 11, 31, 35, 45, 64, 1018,\n1025\nQuantify, 50\nquantization, 136, seecompression\nquaternion, 394, 405\nconcatenation, 398\nconjugate, 397\nconversion to matrix, 399\ndual, 406\ninverse, 396\nproduct, 396\nrotating vectors with, 397\nqueue, 442\nquick time event, 735\nrace,seedata race\nrace condition, 205, 258\nracing game, seegenre\nRadiant, seegame world editor\nradiosity, seelighting\nrag doll, 777, 886\nRAGE,seeRockstar Advanced Game\nEngine\nRAII,seedesign pattern\nrandom number, 412\nDiehard tests, 413\nKISS99, 414\nlinear congruential, 412\nMersenne Twister, 413\nMother of All, 413\nPCG, 414Xorshift, 414\nRAPID, 52\nrapid iteration, 1024\nrarefaction, 912\nrasterization, 664, 675\nRational Purify, 101\nray, 408\nray cast, 1103\nray tracing, seelighting\nRC filter, seefilter\nRCA jack, seeaudio\nRCS,seeversion control\nRCU,seeread-copy-update\nread-acquire, 311, 312, 319\nread-copy-update, 324\nread-modify-write, 259\nread-only data segment, seeexecutable\nfile, 151\nreaders-writer lock, 324\nreal-time strategy, seegenre\nreality,seevirtual reality\nrecord and play back, 50, 538\nrectangle invalidation, 525\nRedis, 67\nreduced instruction set, 223\nreference counting, 510, 1080\nreferential integrity, 495, 498, 503, 516\nreflection, see alsolighting, 1042, see also\nlanguage\nanisotropic, 634\ndiffuse, 634\nreflectivity, 650\nspecular, 634\nwave, 917\nrefraction, seelighting, 917\nregister, see alsoCPU\nregistry (Microsoft Windows), 473\nrelative velocity, seevelocity\nrelaxed memory order, seememory\nordering semantics\nrelease build, seebuild configuration\nrelease fence, seememory ordering\nsemantics\nrelease memory order, seememory\nordering semantics\nrelief mapping, seerendering\nrender loop, 526\nIndex 1193\nrender state, 691, 692\nleak, 691\nrender target, 664\nrendering\naudio,seeaudio\nbillboard, seebillboard\nbump mapping, 699\ndeferred, 709\ndisplacement mapping, 698\nG-buffer, 709\nparallax occlusion mapping, 698\nrelief mapping, 698\nrendering engine, 45\ngraphics device interface, 46\nlow-level renderer, 46, 47\nrender packet, 46\nscene graph, 47, 622, 693, 697\nrendering equation, 622, 649\nrendering pipeline\napplication stage, 668, 687\nasset conditioning stage, 668, 671\ndata transformation, 669\ngeometry processing stage, 668\nGPU pipeline, 672\nmerging, 676\nrasterization stage, 668\nstream output, 674\ntools stage, 668\ntriangle set-up, 675\ntriangle traversal, 675\nreplacement policy, seecache\nrepository, 69\nresource, 59\nbinary, 520\ncompiler, 502\ncomposite, 516, 521\ndatabase, 494, 1035\ndependencies, 502, 516\ndirectory organization, 504\nexporting, 499, 502\nfile formats, 507\nGUID, 507\nlinker, 502\nmemory, 511\nmetadata, 494\nregistry, 508\nsectioned files, 516source asset, 59, 495, 501\nresource acquisition is initialization, see\ndesign pattern, RAII\nresource dependency, 222\nresource interchange file format, 952\nresource manager, 45, 481, 493\nOGRE, 501\nruntime, 503\nUncharted/TLOU, 498\nUnreal, 496\nXNA, 501\nresponse, 262\nrestitution, seeNewton’s law of\nretargeting, seeanimation\nreturn address, 153\nreverb, 920, 922, 975\nregion, 967\ntank, 979\nRIFF,seeresource interchange file\nformat\nright-hand rule, 371, 395\nrigid body dynamics, seephysics\nring buffer, 983\nRMS,seeroot mean square\nRMW,seeread-modify-write\nRobin Hood hashing, seehashing\nRockstar Advanced Game Engine, 33\nrodata segment, seeexecutable file\nroll, 386\nroot mean square, 914\nrope simulation, 818\nRTS,seegenre\nRTTI, 1042\nrun cycle, 734\nruntime scripting language, 1135\nruntime type identification, 1042\nS/PDIF, 954\nsampling, 683, seealsoanimation, 737,\n925, 948\ndepth conversion, 981\nrate conversion, 981\nSandbox, seegame world editor\nsaved games, 1042, 1078\nscalar, 221, 362\nscatter/gather, 546, 547\nscattering, seelighting\n1194 Index\nSCCS,seeversion control\nscene graph, seerendering engine\nschema, 1066\ninheritance, 1067\nScheme, 477, 797, 1144\nscope, 152, 153\nfile, 152\nlexical, 153\nscoped lock, 320\nScream,seeaudio engine\nscreen\naspect ratio, 659, 663\nmapping, 663, 675\nresolution, 659\nscreenshot, 606\nscripting language, 58, 1040, 1134\nclass, 1153\ndata definition, 477, 1135\ninterface to native language, 1144\nlambda, 1144\nLua, 1139\nmultitasking, 1154\nobject-oriented, 1152\nPawn, 1141\nPython, 502, 1140\nQuake C, 11, 1138\nruntime, 1135\nUnrealScript, 1138\nSDK,seesoftware development kit\nSECAM, 534, 542, 663\nsegment, seeexecutable file\nselect, 221\nsemaphore, 275, 278, 492, 548, 555\nbinary, 276\nseparating axis theorem, 837\nseparating vector, 830\nsequential lock, 324\nsequential programming, 204, 544\nserial computer, 233\nserialization, 1063\nset associativity, seecache\nshader, 672\narchitecture, 677\nCg, 680\neffect file, 670, 682\ngeometry, 674High-Level Shading Language,\n677\nmemory access, 678\nOpenGL shader language, 680\npass, 682\npixel, 675\npixel shader, 46\nregister, 678\nsemantic, 680\nshader model 4.0, 677\ntechnique, 682\ntexture access, 679\ntexture sampler, 681\nuniform declaration, 680\nvertex, 629, 673\nshader resource table, 678\nshading, 633, 647\nshading language, 351\nshadow, 48, 655, 703\ncontact, 705\nmapping, 703, 704\nvolume, 703\nShannon-Nyquist sampling theorem,\n948, 949\nshape,see alsocollision\nsphere, 408\nshaping, seetext rendering\nshared memory, seeconcurrency\nshared-exclusive lock, 324\nship build, seebuild configuration\nshuffle,seeSIMD instruction\nSI units, 912\nside-chain input, 990\nsign bit, seeinteger,see also\nfloating-point, 134\nsignal, 912, 924\nbetween threads, 276\ncontinuous-time, 925, 948\ndiscrete-time, 925, 948\nmanipulating, 925\nperiodic, 913, 938\nprocessing theory, 783, 924\nsignaled kernel object, 268, 276\nsigned distance field, 717\nsignificant digits, seefloating-point\nsilhouette edge, 625, 689, 703\nIndex 1195\nSIMD,seesingle instruction multiple\ndata\nSIMD unit, seeGPU\nsimplex, 840\nSIMT,seesingle instruction multiple\nthread\nsimulation, 9, 51\nagent-based, 9, 1041\ndiscrete event, 1087\ngame genre, seegenre\nhard real-time, 10\ninteractive, 9, 525\nphysics, 10\nreal-time, 9, 525, 532, 1041\nsoft real-time, 10\ntemporal, 9\nsingle instruction multiple data, 160,\n168, 170, 207, 221, 331, 336,\n341, 348, 355, 393, 408, 413,\n431, 548, 672, 678, 831\n__m128, 332, 350\nAltiVec, 336\nhorizontal add, 338\ninstruction, 334\nlanes, 343\nshuffle, 341, 342\nvector float, 336, 350\nVF32, 139\nsingle instruction multiple thread, 209,\n331, 348, 355\nsingle instruction single data, 207\nsingle-step, seedebugger\nsingleton, seedesign pattern\nsingularity function, 928\nsinusoid, 933\nSISD,seesingle instruction single data\nskeletal animation, seeanimation\nskeleton, 670\nin memory, 728\nSketchUp, 669\nskinning, 62, 670, 750\nto multiple joints, 754\nweights, 635, 725, 750\nsky, 713\nbox, 714\ndome, 714\nsmall memory allocator, 1076smart pointer, 440, 1042, 1079, 1080\nsmoothing, 639\nSMP,seesymmetric multiprocessing\nsnapshot, 1112\nSoC,seesystem on a chip\nsocket,seeanimation\nSoftimage/XSI, 62, 669\nsoftware development kit, 40\nsoftware object model, seeobject model\nsolid angle, 963\nsorting, 443\nfor rendering, 692\nsound,see alsoaudio, 912\n2D, 955\n3D, 955\nbank, 987\ncard, 974\nclip, 955, 985, 998\ncue, 986\ndry, 920\nfall-off of pressure, 917, 923, 957\nportals, 972\npressure, 912, 914\npressure level, 915, 961\nradiation patterns, 918\nsynthesis, 955, 975, 978, 995\nwave, 913\nwet, 921\nSound Forge, 63\nSource engine, 33\nsource file, seeC++\nSourceSafe, seeversion control\nspatial hash, 693, 846, 1086\nspatial partitioning, 693\nspatialization, seeaudio\nspawner, 1065\npros and cons, 1069\nspeakers, 943\nspeaker circle, 959\nspectral plot, 633\nSpectre exploit, 239\nspecular, seelighting\nspecular lighting, seelighting\nspeculative execution, 218, seeCPU\nspeech, 997\nspeed, 535\nof sound, 913\n1196 Index\nsphere hierarchy, seetree\nsphere map, 700\nspherical\ncoordinates, 360\nharmonic basis function, 708\nspin lock, 271, 290, 294, 555\nspin-wait, seebusy-wait\nSPL,seesound, pressure level\nspline, 624, 1033\nB-spline, 624, 783\nBézier, 624\nsplit-screen, 995, 997\nspot light, seelighting\nspring, 884\nSPU, 545\nSQL Server, 498\nSQT transform, 405\nSRT,see alsoshader resource table\ntransform, 405, 729\nSSE,seestreaming SIMD extensions\ncompiler intrinsics, 333\nstability (numerical), 863\nstack (data structure), 442\nstack frame, seecall stack\nstage,seeCPU, stage\nstall, 215\nstance variation, 772\nstandard C library, seeC standard\nlibrary\nstandard C++ library, seeC++\nstandard template library, 40, 41, 442,\n448\nstd::list, 41\nstd::string, 457, 465\nstd::vector, 41\nstart-up and shut-down\nconstruct on demand, 419\nengine subsystem, 417\nmanual, 420\nOGRE, 422\nUncharted/TLOU, 424\nstarvation, 284\nstate caching, 1100\nstatic\nallocation, seeallocation\nassertion, seeassertion\nlights,seelightingvariable, 149, 157\nstencil buffer, 664, 703\ntesting, 676\nstereo, 944\nstinger, 1011\nSTL,seestandard template library\nstocastic propagation modeling, 972\nstrategy game, seegenre\nstreaming\naudio streaming, 988\ndialog streaming, 998\nlevel streaming, 1040\nstreaming SIMD extensions, 331\nstrict aliasing, 143\nstring, 456\nclass, 457, 465\ndatabase, 467\nstring id, 459, 1138, 1151\nstrip,seetriangle\nstruct of arrays, 1060\nstructured bindings, 115\nstudio, 5\nfirst-party developer, 8\nsubdivision surface, 624\nsubnormal, seefloating-point\nSubversion, seeversion control\nsubwoofer, seelow-frequency effects\nsuperposition, 919, 926\nsuperscalar, seeCPU\nsurface, 622, 623\nvisual properties, 622, 632\nsurround sound, 944\nsweep and prune algorithm, 847\nSWIFT, 52\nsymbolic link, 494\nsymmetric multiprocessing, 227\nsynchronization point, 1106, 1113\nsynergistic processing unit, seeSPU\nsyntax tree, 792\nsynthesizer, 995\nsystem,seelinear time-invariant system\nsystem on a chip, 678\nT&L,seehardware T&L\ntable (data structure), 1139\nTagged Image File Format, 642\nTarga, 642\nIndex 1197\ntarget hardware, 38\nTarget Manager, 590\ntask decomposition, 544\ntask parallelism, seeparallelism\ntaxonomy, 1048\ntearing, 537, 538, 663\ntechnical requirements checklist, 587\ntemplate metaprogramming, 118\ntemporal coherency, 537, 846\ntemporal multithreading, 234\ntemporary register, seeGPU\nterrain, 714\nheight field, 60, 714, 1029\ntessellation, 625, 674\ndynamic, 626, 714\ntest-and-set instruction, 294\ntestability, 87\nTeX, 1136\ntexel, 639\ndensity, 643, 644\ntext rendering, seealsosigned distance\nfield, 716\nshaping, 717\ntext segment, seeexecutable file\ntexture, 46, 121, 639, 670, 678, 679\n1D, 641\n3D, 701\naddressing modes, 641\nalbedo map, 640\nanimated, 52, 723\ncompression, 642\ncoordinates, 641\ncube map, 700\ndiffuse map, 640\nenvironment map, 48, 700, 706\nfiltering, 645, 679\nformats, 642\ngloss map, 699\nheightmap, 698\nlight map, 48, 653, 672\nnormal map, 698\nrendering to, 680, 706\nscrolling, 691\nshadow map, 703\nspecular map, 699\nspecular power map, 699\nsphere map, 700thread, 204, see alsoGPU\naffinity, 227, 248\ndependencies, 548, 1105\ngroup,seeGPU\njoin, 243, 554, 557\npriority, 248\nsafety, 288, 321, 1102\nsynchronization primitive, 265,\n267, 555\nuser-level, 253, 1102, 1110\nthroughput, seepipeline\ntimbre, 920\ntime\nabstract timeline, 532\nclock, 532\nclock drift, 540\nclock variable, 540\ndelta, 535–537, 572\ndomain, 924, 938, 974\nfloating-point, 541\ngame, 525, 532\nhigh-resolution timer, 539\nindex, 735\nlocal, 533\nmeasuring, 539, 543\nof impact, 845\nscaling, 739\nshift, 925\nunits, 540\ntime-invariant, 926\ntime-slicing, seealsomultitasking, 208,\n234, 356, 545\ntimeline\nglobal, 739\nlocal, 735\nTLB,seetranslation lookaside buffer\nTOI,seetime of impact\nTokamak, 825\nTombstone Engine, 35\ntone mapping, 702\ntool, 59, 61, 669\ntools pipeline, seeasset conditioning\npipeline\ntorque, 373, 867\nin three dimensions, 873\nTorque Engine, 36\nTOSLINK, 954\n1198 Index\ntransaction, 287, 328\ncommitting, 328\ntransformation, 376\nand lighting, 672\ncoordinate axes, 384, 390\nmatrix, 376\nrotation, 376, 382, 403\nscale, 364, 376, 383\ntranslation, 376, 382\ntransistor, 946\ntransition, seeanimation\ntranslation unit, seeC++, 148\ntranslucency, 623\ntransmission, seelighting\ntransparency, 623\ntransport model, seelighting\ntranspose\nmatrix, 339\ntree, 442, see alsoblend tree\nbinary space partitioning tree, 47,\n695\nbounding sphere, 695, 846\nkd-tree, 47, 695\noctree, 47, 694\nquadtree, 47, 694\nsphere hierarchy, 47\ntriangle, 625\narea of, 371\nface normal, 627\nfan, 629, 630\nindexed list, 628\nlist, 628, 630\nmesh,seemesh\nocclusion, 664\nstrip, 629, 630\nwinding order, 627\ntriangulation, 625\ntrilinear, seetexture, filtering\nTRS jack, seeaudio\nTrueAxis, 824, 825\nTTY, 590\ntunneling, 843\nturbo button, 536\nturn-based strategy, seegenre\ntwo’s complement, seeinteger\ntype punning, 143UAA,seeUniversal Audio Architecture\nUDN, 11, seeUnreal Developer\nNetwork\nULP,seefloating-point\numbra, 655\nUNC, 483\nUnicode, 462\nUTF-16, 463\nUTF-32, 463\nUTF-8, 463\nUnified Modeling Language, 107\nunit impulse, 928\nunit in the last place, seefloating-point\nUnity, 35\nUniversal Audio Architecture, 992\nuniversal serial bus, seeUSB\nUnreal Developer Network, 33\nUnreal Engine, 11, 32, 35, 45, seealso\naudio engine\nBlueprints, 32, 1133\nUnreal Tournament 2004, 32\nUnrealEd, 496, 1035, seegame world\neditor\nUnrealScript, 1138\nunresolved symbol error, seecompiler,\n146\nUSB, 570, 954\nuser-defined literals, seeC++\nuser-level thread, seethread\nUTF-x,seeUnicode\nV-Collide, 52\nVAG,seeaudio, file formats\nValgrind, 50, 102\nvariable\nclass static, 157\nglobal, 151\nlocal, 153\nmember, 156\nstatic, 151\nvariant, 1008, 1118, 1145\nvector, 362, seealsoperpendicular\naddition, 364\narithmetic, 365\nbasis, 363, 390\ncollinear, 369\ncross product, 367, 370\nIndex 1199\ndirection, 363, 381\ndot product, 367\nfront, left and up, 385, 631\nmagnitude, 365\nmultiplication by scalar, 364\nnormal, 367, 369, 392, 638\nnormalization, 367\nperpendicular, 635\nposition, 363\nprojection, 368\nquaternion form, 397\nsquared magnitude, 367\nSTL, 442\nsubtraction, 365\nunit, 363, 367\nvector processing unit, 168, 208, 224, see\nalsoGPU, SIMD unit, 350, 672\nvector unit (PS2), 224\nvectorization, 168, 337, 344, 348, 548\nvehicle engine sound, 996\nvelocity, 366, 528, 535\nangular, 867\nanimation, 758\nlinear, 858\nrelative, 923, 957, 973\nscreen space, 719\nversion control, 69\nAlienbrain, 70, 494\nassets, 493\nchecking in and out, 75\nClearCase, 70\ncommitting, 75\nCVS, 70\ndeleting files, 78\ndiff, 75\nexclusive check-out, 76\ngit, 70\nhistory, 74\nhosting, 72\nlocking, 76\nmerging, 76\nmultiple check-out, 76\nPerforce, 70, 493\nRCS, 70\nrebasing, 70\nrepository, 72\nresources, 493SCCS, 70\nSourceSafe, 70\nsubmitting, 75\nSubversion, 70\nthree-way merge, 76, 102\nTortoiseSVN, 73\nupdating, 74\nvertex, 635\nattribute interpolation, 637, 662,\n675\nattributes, 635\nbinormal, 635\nbitangent, 635\ncache optimization, 629, 679\nformats, 636\nnormal, 635\ntangent, 635\nvertex buffer, 628\nvertical blanking interval, 538, 663\nview volume, 658, 674\nfar plane, 658\nfrustum, 411, 658, 674\nnear plane, 658\nviewing direction, seelighting\nviewport, 46\nvignette, 719\nvirtual machine, 317, 1135, 1144\nvirtual reality, 27, 35\nvisibility determination, 47, 157\nVisual Basic, 1134\nvisual effects, 48, 64\nVLIW,seeCPU\nvoice,seeaudio\nVoIP, 570\nvolatile, 302, 318, 326\nvolume control, 946, 979, 983\nVoodoo, 672\nVPU,seevector processing unit\nVR,seevirtual reality\nVTune, 50\nVU0 (PS2), 224\nVulkan, 41, 46, 692\nwbuffer, 664, 666, 704\nwait-free algorithm, 290\nwait-free consensus problem, see\nconsensus problem\n1200 Index\nwalk cycle, 734\nwarp,seeGPU\nWASAPI, seeWindows Audio Session\nAPI\nwater, 626, 632, 647, 648, 673, 706, 710,\n715, 910, 1017, 1029, 1044\nWAV,seeaudio, file formats\nwave\nlongitudinal, 917\npropagation, 917\ntransverse, 917\nwavefront, seeGPU\nwavelength, 633, seealsolighting, 914\nWDM,seeWindows Driver Model\nweighted average, 375, 635, 676, 765,\n788\nwet sound, seesound\nwhole number, 132\nWiimote, 53\nWindows\nAudio Session API, 993\nBitmap, 642\nDriver Model, 992\nMedia Video (WMV), 1162\nWMA,seeaudio, file formats\nWMV, 1162\nwoofer,seelow-frequency effects\nwork, 875\nworld,seegame world\nworld-space texel density, 644\nwrite\nback, 196, see alsoCPU, stage, 306,\n307, 309\nrelease, 311, 312, 319\nthrough, 196, 306\nWwise, 995\nX3DAudio, seeaudio engine\nXACT,seeaudio engine\nXAudio2, seeaudio engine\nXbox Live, 34, 473, 481, 487\nXML, 1136\nXNA Game Studio, 34, 993\nYake, 36\nyaw, 386\nyieldingcoroutine, 253, 554, 1110\nfiber, 250, 251, 554\nthread’s timeslice, 234, 241, 244,\n246, 250\nzbias, 712\nzbuffer, 704\nzfighting, 666, 712\nZBrush, 62\nZIP archive, seearchive file\nPlate I. Overwatch by Blizzard Entertainment (Xbox One, PlayStation 4, Windows). (See Figure 1.2 on page 14.)\nPlate II. Jak II by Naughty Dog (Jak, Daxter, Jak and Daxter, and Jak II © 2003, 2013/™ SIE. Created and developed by Naughty\nDog, PlayStation 2). (See Figure 1.3 on page 16.)\nPlate III. Gears of War 4 by The Coalition (Xbox One). (See Figure 1.4 on page 17.)\nPlate IV. Tekken 3 by Namco (PlayStation). (See Figure 1.5 on page 18.)\nPlate V. Injustice 2 by NetherRealm Studios (PlayStation 4, Xbox One, Android, iOS, Microsoft Windows). (See Figure 1.6 on\npage 19.)\nPlate VI. Gran Turismo Sport by Polyphony Digital (PlayStation 4). (See Figure 1.7 on page 20.)\nPlate VII. Age of Empires by Ensemble Studios (Windows). (See Figure 1.8 on page 21.)\nPlate VIII. Total War: Warhammer 2 by Creative Assembly (Windows). (See Figure 1.9 on page 22.)\nPlate IX. World of Warcraft by Blizzard Entertainment (Windows, MacOS). (See Figure 1.10 on page 23.)\nPlate X. Destiny 2 by Bungie, © 2018 Bungie Inc. (Xbox One, PlayStation 4, PC). (See Figure 1.11 on page 24.)\nPlate XI. LittleBigPlanet™ 2 by Media Molecule, © 2014 Sony Interactive Entertainment (PlayStation 3). (See Figure 1.12 on\npage 25.)\nPlate XII. Dreams by Media Molecule, © 2017 Sony Interactive Entertainment (PlayStation 4). (See Figure 1.13 on page 26.)\nPlate XIII. Minecraft by Markus “Notch” Persson / Mojang AB (Windows, MacOS, Xbox 360, PlayStation 3, PlayStation Vita, iOS).\n(See Figure 1.14 on page 26.)\nPlate XIV. Accounting by Squanchtendo and Crows Crows Crows (HTC Vive). (See Figure 1.15 on page 29.)\nPlate XV. A scene from The Last of Us: Remastered (© 2014/™ SIE. Created and developed by Naughty Dog, PlayStation 4)\nrendered without textures. (See Figure 11.20 on page 647.)\nPlate XVI. The same scene from The Last of Us: Remastered (© 2014/™ SIE. Created and developed by Naughty Dog, PlaySta-\ntion 4) with only diffuse textures applied. (See Figure 11.21 on page 648.)\nPlate XVII. Scene from The Last of Us: Remastered (© 2014/™ SIE. Created and developed by Naughty Dog, PlayStation 4) with\nfull lighting. (See Figure 11.22 on page 648.)\nPlate XVIII. The ﬂashlight in Luigi’s Mansion by Nintendo (Wii) is composed of numerous visual effects, including a cone of\ntranslucent geometry for the beam, a dynamic spot light to cast light into the scene, an emissive texture on the lens and\ncamera-facing cards for the lens ﬂare. (See Figure 11.30 on page 656.)\nPlate XIX. Left: No antialiasing. Center: 4  MSAA. Right: Nvidia’s FXAA, preset 3. Image from Nvidia’s FXAA white paper by\nTimothy Lottes (http: / /bit.ly/1mIzCTv). (See Figure 11.45 on page 683.)\nPlate XX. This screenshot from EA’s Fight Night Round 3 shows how a gloss map can be used to control the degree of specular\nreﬂection that should be applied to each texel of a surface. (See Figure 11.55 on page 701.)\nPlate XXI. Mirror reﬂections in The Last of Us: Remastered (© 2014/™ SIE. Created and developed by Naughty Dog, PlayStation 4)\nimplemented by rendering the scene to a texture that is subsequently applied to the mirror’s surface. (See Figure 11.59 on\npage 707.)\nPlate XXII. Screenshots from Killzone 2 by Guerrilla Games, showing some of the typical components of the G-buffer used in\ndeferred rendering. The upper image shows the ﬁnal rendered image. Below it, clockwise from the upper left, are the albedo\n(diffuse) color, depth, view-space normal, screen-space 2D motion vector (for motion blurring), specular power and specular\nintensity. (See Figure 11.62 on page 710.)\nPlate XXIII. Flame, smoke and bullet tracer particle effects in Uncharted 3: Drake’s Deception (© 2011/™ SIE. Created and devel-\noped by Naughty Dog, PlayStation 3). (See Figure 11.63 on page 712.)\nPlate XXIV. Parallax-mapped decals from Uncharted 3: Drake’s Deception (© 2011/™ SIE. Created and developed by Naughty\nDog, PlayStation 3). (See Figure 11.64 on page 713.)\nPlate XXV. The Sandbox editor for CRYENGINE. (See Figure 15.6 on page 1028.)\nPlate XXVI. The Unreal Engine 4 animation Blueprints editor. (See Figure 12.58 on page 801.)",63512
