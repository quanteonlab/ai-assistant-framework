filename,title,text,len
01-Cover.pdf,01-Cover,"Joe Reis &  \n Matt Housley\nFundamentals \nof Data \nEngineering\nPlan and Build \nRobust Data Systems\nReis &  \n Housley\nDATA“The world of data has been \nevolving for a while now. First \nthere were designers. Then \ndatabase administrators. \nThen CIOs. Then data \narchitects. This book signals \nthe next step in the evolution \nand maturity of the industry. It \nis a must read for anyone who \ntakes their profession and \ncareer honestly.” \n—Bill Inmon\ncreator of the data warehouse\n“Fundamentals of Data \nEngineering  is a great \nintroduction to the business \nof moving, processing, and \nhandling data. I’d highly \nrecommend it for anyone \nwanting to get up to speed \nin data engineering or \nanalytics, or for existing \npractitioners who want \nto fill in any gaps in their \nunderstanding.” \n—Jordan Tigani\nfounder and CEO, MotherDuck,  \nand founding engineer and  \ncocreator of BigQuery   Fundamentals of Data Engineering\nUS $69.99  CAN $87 .99\nISBN: 978-1-098-10830-4Twitter: @oreillymedia\nlinkedin.com/company/oreilly-media\nyoutube.com/oreillymedia Data engineering has grown rapidly in the past decade, \nleaving many software engineers, data scientists, and \nanalysts looking for a comprehensive view of this practice. \nWith this practical book, you’ll learn how to plan and \nbuild systems to serve the needs of your organization and \ncustomers by evaluating the best technologies available \nthrough the framework of the data engineering lifecycle.\nAuthors Joe Reis and Matt Housley walk you through the data \nengineering lifecycle and show you how to stitch together \na variety of cloud technologies to serve the needs of down -\nstream data consumers. You’ll understand how to apply the \nconcepts of data generation, ingestion, orchestration, trans -\nformation, storage, and governance that are critical in any \ndata environment regardless of the underlying technology.\nThis book will help you:\n• Get a concise overview of the entire data engineering \nlandscape\n• Assess data engineering problems using an end-to-end \nframework of best practices\n• Cut through marketing hype when choosing data \ntechnologies, architecture, and processes\n• Use the data engineering lifecycle to design and build a \nrobust architecture\n• Incorporate data governance and security across the data \nengineering lifecycle\nJoe Reis is a “recovering data scientist,” and a data engineer and \narchitect.\nMatt Housley  is a data engineering consultant and cloud specialist.\nPraise for Fundamentals of Data Engineering\nThe world of data has been evolving for a while now. First there were designers. Then\ndatabase administrators. Then CIOs. Then data architects. This book signals the next step\nin the evolution and maturity of the industry. It is a must read for anyone who takes their\nprofession and career honestly.\n—Bill Inmon, creator of the data warehouse\nFundamentals of Data Engineering  is a great introduction to the business of moving,\nprocessing, and handling data. It explains the taxonomy of data concepts, without\nfocusing too heavily on individual tools or vendors, so the techniques and ideas should\noutlast any individual trend or product. I’ d highly recommend it for anyone wanting to\nget up to speed in data engineering or analytics, or for existing practitioners who want to\nfill in any gaps in their understanding.\n—Jordan Tigani, founder and CEO, MotherDuck, and founding\nengineer and cocreator of BigQuery\nIf you want to lead in your industry, you must build the capabilities required to provide\nexceptional customer and employee experiences. This is not just a technology problem.\nIt’s a people opportunity. And it will transform your business. Data engineers are at the\ncenter of this transformation. But today the discipline is misunderstood. This book will\ndemystify data engineering and become your ultimate guide to succeeding with data.\n—Bruno Aziza, Head of Data Analytics, Google Cloud\nWhat a book! Joe and Matt are giving you the answer to the question, “What must I\nunderstand to do data engineering?” Whether you are getting started as a data engineer or\nstrengthening your skills, you are not looking for yet another technology handbook. Y ou\nare seeking to learn more about the underlying principles and the core concepts of the\nrole, its responsibilities, its technical and organizational environment, its mission—that’s\nexactly what Joe and Matt offer in this book.\n—Andy Petrella, founder of Kensu\nThis is the missing book in data engineering. A wonderfully thorough account of what it\ntakes to be a good practicing data engineer, including thoughtful real-life considerations.\nI’ d recommend all future education of data professionals include Joe and Matt’s work.\n—Sarah Krasnik, data engineering leader\nIt is incredible to realize the breadth of knowledge a data engineer must have. But don’t\nlet it scare you. This book provides a great foundational overview of various architectures,\napproaches, methodologies, and patterns that anyone working with data needs to be\naware of. But what is even more valuable is that this book is full of golden nuggets of\nwisdom, best-practice advice, and things to consider when making decisions related to\ndata engineering. It is a must read for both experienced and new data engineers.\n—Veronika Durgin, data and analytics leader\nI was honored and humbled to be asked by Joe and Matt to help technical review\ntheir masterpiece of data knowledge, Fundamentals of Data Engineering . Their ability to\nbreak down the key components that are critical to anyone wanting to move into a data\nengineering role is second to none. Their writing style makes the information easy to\nabsorb, and they leave no stone unturned. It was an absolute pleasure to work with some\nof the best thought leaders in the data space. I can’t wait to see what they do next.\n—Chris Tabb, cofounder of LEIT DATA\nFundamentals of Data Engineering  is the first book to take an in-depth and holistic look\ninto the requirements of today’s data engineer. As you’ll see, the book dives into the\ncritical areas of data engineering including skill sets, tools, and architectures used to\nmanage, move, and curate data in today’s complex technical environments.\nMore importantly, Joe and Matt convey their master of understanding data engineering\nand take the time to further dive into the more nuanced areas of data engineering and\nmake it relatable to the reader. Whether you’re a manager, experienced data engineer, or\nsomeone wanting to get into the space, this book provides practical insight into today’s\ndata engineering landscape.\n—Jon King, Principal Data Architect\nTwo things will remain relevant to data engineers in 2042: SQL and this book. Joe and\nMatt cut through the hype around tools to extract the slowly changing dimensions of our\ndiscipline. Whether you’re starting your journey with data or adding stripes to your black\nbelt, Fundamentals of Data Engineering  lays the foundation for mastery.\n—Kevin Hu, CEO of Metaplane\nIn a field that is rapidly changing, with new technology solutions popping up\nconstantly, Joe and Matt provide clear, timeless guidance, focusing on the core\nconcepts and foundational knowledge required to excel as a data engineer. This book\nis jam packed with information that will empower you to ask the right questions,\nunderstand trade-offs, and make the best decisions when designing your data architecture\nand implementing solutions across the data engineering lifecycle. Whether you’re\njust considering becoming a data engineer or have been in the field for years,\nI guarantee you’ll learn something from this book!\n—Julie Price, Senior Product Manager, SingleStore\nFundamentals of Data Engineering  isn’t just an instruction manual—it teaches you how to\nthink like a data engineer. Part history lesson, part theory, and part acquired knowledge\nfrom Joe and Matt’s decades of experience, the book has definitely earned its place on\nevery data professional’s bookshelf.\n—Scott Breitenother, founder and CEO, Brooklyn Data Co.\nThere is no other book that so comprehensively covers what it means to be a data\nengineer. Joe and Matt dive deep into responsibilities, impacts, architectural choices, and\nso much more. Despite talking about such complex topics, the book is easy to read\nand digest. A very powerful combination.\n—Danny Leybzon, MLOps Architect\nI wish this book was around years ago when I started working with data engineers. The\nwide coverage of the field makes the involved roles clear and builds empathy with the\nmany roles it takes to build a competent data discipline.\n—Tod Hansmann, VP Engineering\nA must read and instant classic for anyone in the data engineering field. This book fills\na gap in the current knowledge base, discussing fundamental topics not found in other\nbooks. Y ou will gain understanding of foundational concepts and insight into historical\ncontext about data engineering that will set up anyone to succeed.\n—Matthew Sharp, Data and ML Engineer\nData engineering is the foundation of every analysis, machine learning model, and data\nproduct, so it is critical that it is done well. There are countless manuals, books, and\nreferences for each of the technologies used by data engineers, but very few (if any)\nresources that provide a holistic view of what it means to work as a data engineer. This\nbook fills a critical need in the industry and does it well, laying the foundation for new\nand working data engineers to be successful and effective in their roles. This is the book\nthat I’ll be recommending to anyone who wants to work with data at any level.\n—Tobias Macey, host of The Data Engineering Podcast\nJoe Reis and Matt HousleyFundamentals of Data Engineering\nPlan and Build Robust Data Systems\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing",9940
02-Table of Contents.pdf,02-Table of Contents,"978-1-098-10830-4\n[LSI]Fundamentals of Data Engineering\nby Joe Reis and Matt Housley\nCopyright © 2022 Joseph Reis and Matthew Housley. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com .\nAcquisitions Editor:  Jessica Haberman\nDevelopment Editor:  Michele Cronin\nProduction Editor:  Gregory Hyman\nCopyeditor:  Sharon Wilkey\nProofreader:  Amnet Systems, LLCIndexer:  Judith McConville\nInterior Designer:  David Futato\nCover Designer:  Karen Montgomery\nIllustrator:  Kate Dullea\nJuly 2022:  First Edition\nRevision History for the First Edition\n2022-06-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098108304  for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Fundamentals of Data Engineering , the\ncover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\nof or reliance on this work. Use of the information and instructions contained in this work is at your\nown risk. If any code samples or other technology this work contains or describes is subject to open\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xiii\nPart I. Foundation and Building Blocks\n1.Data Engineering Described. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\nWhat Is Data Engineering?                                                                                             3\nData Engineering Defined                                                                                          4\nThe Data Engineering Lifecycle                                                                                 5\nEvolution of the Data Engineer                                                                                  6\nData Engineering and Data Science                                                                        11\nData Engineering Skills and Activities                                                                        13\nData Maturity and the Data Engineer                                                                     13\nThe Background and Skills of a Data Engineer                                                     17\nBusiness Responsibilities                                                                                           18\nTechnical Responsibilities                                                                                         19\nThe Continuum of Data Engineering Roles, from A to B                                    21\nData Engineers Inside an Organization                                                                      22\nInternal-Facing Versus External-Facing Data Engineers                                     23\nData Engineers and Other Technical Roles                                                            24\nData Engineers and Business Leadership                                                               28\nConclusion                                                                                                                      31\nAdditional Resources                                                                                                    32\n2.The Data Engineering Lifecycle. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  33\nWhat Is the Data Engineering Lifecycle?                                                                    33\nThe Data Lifecycle Versus the Data Engineering Lifecycle                                  34\nGeneration: Source Systems                                                                                     35\niii\nStorage                                                                                                                         38\nIngestion                                                                                                                      39\nTransformation                                                                                                           43\nServing Data                                                                                                                44\nMajor Undercurrents Across the Data Engineering Lifecycle                                 48\nSecurity                                                                                                                        49\nData Management                                                                                                      50\nDataOps                                                                                                                       59\nData Architecture                                                                                                       64\nOrchestration                                                                                                              64\nSoftware Engineering                                                                                                66\nConclusion                                                                                                                      68\nAdditional Resources                                                                                                    69\n3.Designing Good Data Architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  71\nWhat Is Data Architecture?                                                                                          71\nEnterprise Architecture Defined                                                                              72\nData Architecture Defined                                                                                        75\n“Good” Data Architecture                                                                                         76\nPrinciples of Good Data Architecture                                                                        77\nPrinciple 1: Choose Common Components Wisely                                             78\nPrinciple 2: Plan for Failure                                                                                      79\nPrinciple 3: Architect for Scalability                                                                        80\nPrinciple 4: Architecture Is Leadership                                                                   80\nPrinciple 5: Always Be Architecting                                                                        81\nPrinciple 6: Build Loosely Coupled Systems                                                          81\nPrinciple 7: Make Reversible Decisions                                                                  83\nPrinciple 8: Prioritize Security                                                                                 84\nPrinciple 9: Embrace FinOps                                                                                    85\nMajor Architecture Concepts                                                                                       87\nDomains and Services                                                                                               87\nDistributed Systems, Scalability, and Designing for Failure                                88\nTight Versus Loose Coupling: Tiers, Monoliths, and Microservices                  90\nUser Access: Single Versus Multitenant                                                                  94\nEvent-Driven Architecture                                                                                       95\nBrownfield Versus Greenfield Projects                                                                   96\nExamples and Types of Data Architecture                                                                 98\nData Warehouse                                                                                                         98\nData Lake                                                                                                                   101\nConvergence, Next-Generation Data Lakes, and the Data Platform                102\nModern Data Stack                                                                                                  103\nLambda Architecture                                                                                               104\niv | Table of Contents\nKappa Architecture                                                                                                  105\nThe Dataflow Model and Unified Batch and Streaming                                    105\nArchitecture for IoT                                                                                                 106\nData Mesh                                                                                                                 109\nOther Data Architecture Examples                                                                       110\nWho’s Involved with Designing a Data Architecture?                                            111\nConclusion                                                                                                                    111\nAdditional Resources                                                                                                  111\n4.Choosing Technologies Across the Data Engineering Lifecycle. . . . . . . . . . . . . . . . . . .  115\nTeam Size and Capabilities                                                                                         116\nSpeed to Market                                                                                                           117\nInteroperability                                                                                                            117\nCost Optimization and Business Value                                                                    118\nTotal Cost of Ownership                                                                                         118\nTotal Opportunity Cost of Ownership                                                                  119\nFinOps                                                                                                                       120\nToday Versus the Future: Immutable Versus Transitory Technologies               120\nOur Advice                                                                                                                122\nLocation                                                                                                                         123\nOn Premises                                                                                                              123\nCloud                                                                                                                         124\nHybrid Cloud                                                                                                            127\nMulticloud                                                                                                                 128\nDecentralized: Blockchain and the Edge                                                              129\nOur Advice                                                                                                                129\nCloud Repatriation Arguments                                                                              130\nBuild Versus Buy                                                                                                          132\nOpen Source Software                                                                                             133\nProprietary Walled Gardens                                                                                   137\nOur Advice                                                                                                                138\nMonolith Versus Modular                                                                                          139\nMonolith                                                                                                                    139\nModularity                                                                                                                140\nThe Distributed Monolith Pattern                                                                         142\nOur Advice                                                                                                                142\nServerless Versus Servers                                                                                            143\nServerless                                                                                                                   143\nContainers                                                                                                                 144\nHow to Evaluate Server Versus Serverless                                                            145\nOur Advice                                                                                                                146\nOptimization, Performance, and the Benchmark Wars                                         147\nTable of Contents | v\nBig Data...for the 1990s                                                                                           148\nNonsensical Cost Comparisons                                                                             148\nAsymmetric Optimization                                                                                      148\nCaveat Emptor                                                                                                          149\nUndercurrents and Their Impacts on Choosing Technologies                             149\nData Management                                                                                                    149\nDataOps                                                                                                                     149\nData Architecture                                                                                                     150\nOrchestration Example: Airflow                                                                            150\nSoftware Engineering                                                                                              151\nConclusion                                                                                                                    151\nAdditional Resources                                                                                                  151\nPart II. The Data Engineering Lifecycle in Depth\n5.Data Generation in Source Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\nSources of Data: How Is Data Created?                                                                    156\nSource Systems: Main Ideas                                                                                       156\nFiles and Unstructured Data                                                                                  156\nAPIs                                                                                                                            157\nApplication Databases (OLTP Systems)                                                               157\nOnline Analytical Processing System                                                                    159\nChange Data Capture                                                                                              159\nLogs                                                                                                                            160\nDatabase Logs                                                                                                           161\nCRUD                                                                                                                        162\nInsert-Only                                                                                                                162\nMessages and Streams                                                                                             163\nTypes of Time                                                                                                           164\nSource System Practical Details                                                                                 165\nDatabases                                                                                                                   166\nAPIs                                                                                                                            174\nData Sharing                                                                                                             176\nThird-Party Data Sources                                                                                       177\nMessage Queues and Event-Streaming Platforms                                               177\nWhom Y ou’ll Work With                                                                                            181\nUndercurrents and Their Impact on Source Systems                                            183\nSecurity                                                                                                                      183\nData Management                                                                                                    184\nDataOps                                                                                                                     184\nData Architecture                                                                                                     185\nvi | Table of Contents\nOrchestration                                                                                                            186\nSoftware Engineering                                                                                              187\nConclusion                                                                                                                    187\nAdditional Resources                                                                                                  188\n6.Storage. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  189\nRaw Ingredients of Data Storage                                                                               191\nMagnetic Disk Drive                                                                                                191\nSolid-State Drive                                                                                                      193\nRandom Access Memory                                                                                        194\nNetworking and CPU                                                                                              195\nSerialization                                                                                                               195\nCompression                                                                                                             196\nCaching                                                                                                                      197\nData Storage Systems                                                                                                  197\nSingle Machine Versus Distributed Storage                                                         198\nEventual Versus Strong Consistency                                                                     198\nFile Storage                                                                                                                199\nBlock Storage                                                                                                            202\nObject Storage                                                                                                          205\nCache and Memory-Based Storage Systems                                                        211\nThe Hadoop Distributed File System                                                                    211\nStreaming Storage                                                                                                    212\nIndexes, Partitioning, and Clustering                                                                   213\nData Engineering Storage Abstractions                                                                    215\nThe Data Warehouse                                                                                               215\nThe Data Lake                                                                                                           216\nThe Data Lakehouse                                                                                                216\nData Platforms                                                                                                          217\nStream-to-Batch Storage Architecture                                                                  217\nBig Ideas and Trends in Storage                                                                                218\nData Catalog                                                                                                              218\nData Sharing                                                                                                             219\nSchema                                                                                                                       219\nSeparation of Compute from Storage                                                                   220\nData Storage Lifecycle and Data Retention                                                          223\nSingle-Tenant Versus Multitenant Storage                                                           226\nWhom Y ou’ll Work With                                                                                            227\nUndercurrents                                                                                                              228\nSecurity                                                                                                                      228\nData Management                                                                                                    228\nDataOps                                                                                                                     229\nTable of Contents | vii\nData Architecture                                                                                                     230\nOrchestration                                                                                                            230\nSoftware Engineering                                                                                              230\nConclusion                                                                                                                    230\nAdditional Resources                                                                                                  231\n7.Ingestion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  233\nWhat Is Data Ingestion?                                                                                              234\nKey Engineering Considerations for the Ingestion Phase                                     235\nBounded Versus Unbounded Data                                                                        236\nFrequency                                                                                                                  237\nSynchronous Versus Asynchronous Ingestion                                                    238\nSerialization and Deserialization                                                                           239\nThroughput and Scalability                                                                                    239\nReliability and Durability                                                                                        240\nPayload                                                                                                                       241\nPush Versus Pull Versus Poll Patterns                                                                   244\nBatch Ingestion Considerations                                                                                 244\nSnapshot or Differential Extraction                                                                      246\nFile-Based Export and Ingestion                                                                            246\nETL Versus ELT                                                                                                        246\nInserts, Updates, and Batch Size                                                                            247\nData Migration                                                                                                         247\nMessage and Stream Ingestion Considerations                                                       248\nSchema Evolution                                                                                                    248\nLate-Arriving Data                                                                                                   248\nOrdering and Multiple Delivery                                                                            248\nReplay                                                                                                                         249\nTime to Live                                                                                                              249\nMessage Size                                                                                                              249\nError Handling and Dead-Letter Queues                                                             249\nConsumer Pull and Push                                                                                        250\nLocation                                                                                                                     250\nWays to Ingest Data                                                                                                     250\nDirect Database Connection                                                                                  251\nChange Data Capture                                                                                              252\nAPIs                                                                                                                            254\nMessage Queues and Event-Streaming Platforms                                               255\nManaged Data Connectors                                                                                     256\nMoving Data with Object Storage                                                                         257\nEDI                                                                                                                             257\nDatabases and File Export                                                                                      257\nviii | Table of Contents\nPractical Issues with Common File Formats                                                        258\nShell                                                                                                                            258\nSSH                                                                                                                             259\nSFTP and SCP                                                                                                           259\nWebhooks                                                                                                                  259\nWeb Interface                                                                                                            260\nWeb Scraping                                                                                                            260\nTransfer Appliances for Data Migration                                                               261\nData Sharing                                                                                                             262\nWhom Y ou’ll Work With                                                                                            262\nUpstream Stakeholders                                                                                           262\nDownstream Stakeholders                                                                                      263\nUndercurrents                                                                                                              263\nSecurity                                                                                                                      264\nData Management                                                                                                    264\nDataOps                                                                                                                     266\nOrchestration                                                                                                            268\nSoftware Engineering                                                                                              268\nConclusion                                                                                                                    268\nAdditional Resources                                                                                                  269\n8.Queries, Modeling, and Transformation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  271\nQueries                                                                                                                          272\nWhat Is a Query?                                                                                                      273\nThe Life of a Query                                                                                                  274\nThe Query Optimizer                                                                                              275\nImproving Query Performance                                                                              275\nQueries on Streaming Data                                                                                    281\nData Modeling                                                                                                              287\nWhat Is a Data Model?                                                                                            288\nConceptual, Logical, and Physical Data Models                                                  289\nNormalization                                                                                                           290\nTechniques for Modeling Batch Analytical Data                                                 294\nModeling Streaming Data                                                                                       307\nTransformations                                                                                                           309\nBatch Transformations                                                                                            310\nMaterialized Views, Federation, and Query Virtualization                               323\nStreaming Transformations and Processing                                                        326\nWhom Y ou’ll Work With                                                                                            329\nUpstream Stakeholders                                                                                           329\nDownstream Stakeholders                                                                                      330\nUndercurrents                                                                                                              330\nTable of Contents | ix\nSecurity                                                                                                                      330\nData Management                                                                                                    331\nDataOps                                                                                                                     332\nData Architecture                                                                                                     333\nOrchestration                                                                                                            333\nSoftware Engineering                                                                                              333\nConclusion                                                                                                                    334\nAdditional Resources                                                                                                  335\n9.Serving Data for Analytics, Machine Learning, and Reverse ETL. . . . . . . . . . . . . . . . .  337\nGeneral Considerations for Serving Data                                                                338\nTrust                                                                                                                           338\nWhat’s the Use Case, and Who’s the User?                                                           339\nData Products                                                                                                           340\nSelf-Service or Not?                                                                                                 341\nData Definitions and Logic                                                                                     342\nData Mesh                                                                                                                 343\nAnalytics                                                                                                                        344\nBusiness Analytics                                                                                                    344\nOperational Analytics                                                                                              346\nEmbedded Analytics                                                                                                348\nMachine Learning                                                                                                        349\nWhat a Data Engineer Should Know About ML                                                    350\nWays to Serve Data for Analytics and ML                                                               351\nFile Exchange                                                                                                            351\nDatabases                                                                                                                   352\nStreaming Systems                                                                                                   354\nQuery Federation                                                                                                     354\nData Sharing                                                                                                             355\nSemantic and Metrics Layers                                                                                  355\nServing Data in Notebooks                                                                                     356\nReverse ETL                                                                                                                  358\nWhom Y ou’ll Work With                                                                                            360\nUndercurrents                                                                                                              360\nSecurity                                                                                                                      361\nData Management                                                                                                    362\nDataOps                                                                                                                     362\nData Architecture                                                                                                     363\nOrchestration                                                                                                            363\nSoftware Engineering                                                                                              364\nConclusion                                                                                                                    365\nAdditional Resources                                                                                                  365\nx | Table of Contents\nPart III. Security, Privacy, and the Future of Data Engineering\n10. Security and Privacy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  369\nPeople                                                                                                                            370\nThe Power of Negative Thinking                                                                           370\nAlways Be Paranoid                                                                                                 370\nProcesses                                                                                                                       371\nSecurity Theater Versus Security Habit                                                                371\nActive Security                                                                                                          371\nThe Principle of Least Privilege                                                                             372\nShared Responsibility in the Cloud                                                                       372\nAlways Back Up Y our Data                                                                                     372\nAn Example Security Policy                                                                                    373\nTechnology                                                                                                                   374\nPatch and Update Systems                                                                                      374\nEncryption                                                                                                                375\nLogging, Monitoring, and Alerting                                                                       375\nNetwork Access                                                                                                        376\nSecurity for Low-Level Data Engineering                                                            377\nConclusion                                                                                                                    378\nAdditional Resources                                                                                                  378\n11. The Future of Data Engineering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  379\nThe Data Engineering Lifecycle Isn’t Going Away                                                  380\nThe Decline of Complexity and the Rise of Easy-to-Use Data Tools                   380\nThe Cloud-Scale Data OS and Improved Interoperability                                    381\n“Enterprisey” Data Engineering                                                                                383\nTitles and Responsibilities Will Morph...                                                                 384\nMoving Beyond the Modern Data Stack, Toward the Live Data Stack                385\nThe Live Data Stack                                                                                                 385\nStreaming Pipelines and Real-Time Analytical Databases                                 386\nThe Fusion of Data with Applications                                                                  387\nThe Tight Feedback Between Applications and ML                                           388\nDark Matter Data and the Rise of...Spreadsheets?!                                             388\nConclusion                                                                                                                    389\nA.Serialization and Compression Technical Details. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  391\nB.Cloud Networking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  399\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  403\nTable of Contents | xi",42897
03-Preface.pdf,03-Preface,,0
04-What This Book Isnt.pdf,04-What This Book Isnt,,0
05-Prerequisites.pdf,05-Prerequisites,"Preface\nHow did this book come about? The origin is deeply rooted in our journey from\ndata science into data engineering. We often jokingly refer to ourselves as recovering\ndata scientists . We both had the experience of being assigned to data science projects,\nthen struggling to execute these projects due to a lack of proper foundations. Our\njourney into data engineering began when we undertook data engineering tasks to\nbuild foundations and infrastructure.\nWith the rise of data science, companies splashed out lavishly on data science talent,\nhoping to reap rich rewards. Very often, data scientists struggled with basic problems\nthat their background and training did not address—data collection, data cleansing,\ndata access, data transformation, and data infrastructure. These are problems that\ndata engineering aims to solve.\nWhat This Book Isn’t\nBefore  we cover what this book is about and what you’ll get out of it, let’s quickly\ncover what this book isn’t. This book isn’t about data engineering using a particular\ntool, technology, or platform. While many excellent books approach data engineering\ntechnologies from this perspective, these books have a short shelf life. Instead, we\nfocus on the fundamental concepts behind data engineering.\nWhat This Book Is About\nThis book aims to fill a gap in current data engineering content and materials.\nWhile there’s no shortage of technical resources that address specific data engineering\ntools and technologies, people struggle to understand how to assemble these compo‐\nnents into a coherent whole that applies in the real world. This book connects the\ndots of the end-to-end data lifecycle. It shows you how to stitch together various\ntechnologies to serve the needs of downstream data consumers such as analysts,\ndata scientists, and machine learning engineers. This book works as a complement\nxiii\nto O’Reilly books that cover the details of particular technologies, platforms, and\nprogramming languages.\nThe big idea of this book is the data engineering lifecycle : data generation, storage,\ningestion, transformation, and serving. Since the dawn of data, we’ve seen the rise\nand fall of innumerable specific technologies and vendor products, but the data engi‐\nneering lifecycle stages have remained essentially unchanged. With this framework,\nthe reader will come away with a sound understanding for applying technologies to\nreal-world business problems.\nOur goal here is to map out principles that reach across two axes. First, we wish to\ndistill data engineering into principles that can encompass any relevant technology .\nSecond, we wish to present principles that will stand the test of time . We hope that\nthese ideas reflect lessons learned across the data technology upheaval of the last\ntwenty years and that our mental framework will remain useful for a decade or more\ninto the future.\nOne thing to note: we unapologetically take a cloud-first approach. We view the\ncloud as a fundamentally transformative development that will endure for decades;\nmost on-premises data systems and workloads will eventually move to cloud hosting.\nWe assume that infrastructure and systems are ephemeral  and scalable , and that data\nengineers will lean toward deploying managed services in the cloud. That said, most\nconcepts in this book will translate to non-cloud environments.\nWho Should Read This Book\nOur primary intended audience for this book consists of technical practitioners, mid-\nto senior-level software engineers, data scientists, or analysts interested in moving\ninto data engineering; or data engineers working in the guts of specific technologies,\nbut wanting to develop a more comprehensive perspective. Our secondary target\naudience consists of data stakeholders who work adjacent to technical practition‐\ners—e.g., a data team lead with a technical background overseeing a team of data\nengineers, or a director of data warehousing wanting to migrate from on-premises\ntechnology to a cloud-based solution.\nIdeally, you’re curious and want to learn—why else would you be reading this book?\nY ou stay current with data technologies and trends by reading books and articles on\ndata warehousing/data lakes, batch and streaming systems, orchestration, modeling,\nmanagement, analysis, developments in cloud technologies, etc. This book will help\nyou weave what you’ve read into a complete picture of data engineering across\ntechnologies and paradigms.\nxiv | Preface",4503
06-How to Contact Us.pdf,06-How to Contact Us,"Prerequisites\nWe assume a good deal of familiarity with the types of data systems found in a\ncorporate setting. In addition, we assume that readers have some familiarity with\nSQL and Python (or some other programming language), and experience with cloud\nservices.\nNumerous resources are available for aspiring data engineers to practice Python and\nSQL. Free online resources abound (blog posts, tutorial sites, Y ouTube videos), and\nmany new Python books are published every year.\nThe cloud provides unprecedented opportunities to get hands-on experience with\ndata tools. We suggest that aspiring data engineers set up accounts with cloud services\nsuch as AWS, Azure, Google Cloud Platform, Snowflake, Databricks, etc. Note that\nmany of these platforms have free tier  options, but readers should keep a close eye on\ncosts and work with small quantities of data and single node clusters as they study.\nDeveloping familiarity with corporate data systems outside of a corporate environ‐\nment remains difficult, and this creates certain barriers for aspiring data engineers\nwho have yet to land their first data job. This book can help. We suggest that\ndata novices read for high-level ideas and then look at materials in the Additional\nResources section at the end of each chapter. On a second read through, note any\nunfamiliar terms and technologies. Y ou can utilize Google, Wikipedia, blog posts,\nY ouTube videos, and vendor sites to become familiar with new terms and fill gaps in\nyour understanding.\nWhat You’ll Learn and How It Will Improve Your Abilities\nThis  book aims to help you build a solid foundation for solving real-world data\nengineering problems.\nBy the end of this book you will understand:\n•How data engineering impacts your current role (data scientist, software engi‐•\nneer, or data team lead)\n•How to cut through the marketing hype and choose the right technologies, data•\narchitecture, and processes\n•How to use the data engineering lifecycle to design and build a robust•\narchitecture\n•Best practices for each stage of the data lifecycle•\nPreface | xv\nAnd you will be able to:\n•Incorporate data engineering principles in your current role (data scientist, ana‐•\nlyst, software engineer, data team lead, etc.)\n•Stitch together a variety of cloud technologies to serve the needs of downstream•\ndata consumers\n•Assess data engineering problems with an end-to-end framework of best•\npractices\n•Incorporate data governance and security across the data engineering lifecycle•\nNavigating This Book\nThis book is composed of four parts:\n•Part I, “Foundation and Building Blocks”•\n•Part II, “The Data Engineering Lifecycle in Depth”•\n•Part III, “Security, Privacy, and the Future of Data Engineering”•\n•Appendices A and B: covering serialization and compression, and cloud net‐ •\nworking, respectively\nIn Part I , we begin by defining data engineering in Chapter 1 , then map out the\ndata engineering lifecycle in Chapter 2 . In Chapter 3 , we discuss good architecture . In\nChapter 4 , we introduce a framework for choosing the right technology—while we\nfrequently see technology and architecture conflated, these are in fact very different\ntopics.\nPart II  builds on Chapter 2  to cover the data engineering lifecycle in depth; each\nlifecycle stage—data generation, storage, ingestion, transformation and serving—is\ncovered in its own chapter. Part II  is arguably the heart of the book, and the other\nchapters exist to support the core ideas covered here.\nPart III  covers additional topics. In Chapter 10 , we discuss security and privacy . While\nsecurity has always been an important part of the data engineering profession, it has\nonly become more critical with the rise of for profit hacking and state sponsored\ncyber attacks. And what can we say of privacy? The era of corporate privacy nihilism\nis over—no company wants to see its name appear in the headline of an article on\nsloppy privacy practices. Reckless handling of personal data can also have significant\nlegal ramifications with the advent of GDPR, CCPA, and other regulations. In short,\nsecurity and privacy must be top priorities in any data engineering work.\nxvi | Preface\nIn the course of working in data engineering, doing research for this book and\ninterviewing numerous experts, we thought a good deal about where the field is going\nin the near and long term. Chapter 11  outlines our highly speculative ideas on the\nfuture of data engineering. By its nature, the future is a slippery thing. Time will tell if\nsome of our ideas are correct. We would love to hear from our readers on how their\nvisions of the future agree with or differ from our own.\nIn the appendices, we cover a handful of technical topics that are extremely relevant\nto the day-to-day practice of data engineering but didn’t fit into the main body of\nthe text. Specifically, engineers need to understand serialization and compression\n(see Appendix A ) both to work directly with data files and to assess performance\nconsiderations in data systems, and cloud networking (see Appendix B ) is a critical\ntopic as data engineering shifts into the cloud.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program\nelements such as variable or function names, databases, data types, environment\nvariables, statements, and keywords\nThis element signifies a tip or suggestion.\nThis element signifies a general note.\nThis element indicates a warning or caution.\nPreface | xvii",5720
07-Acknowledgments.pdf,07-Acknowledgments,"How to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. Y ou can access this page at https://oreil.ly/fundamentals-of-data .\nEmail bookquestions@oreilly.com  to comment or ask technical questions about this\nbook.\nFor news and information about our books and courses, visit https://oreilly.com .\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media\nFollow us on Twitter: https://twitter.com/oreillymedia\nWatch us on Y ouTube: https://www.youtube.com/oreillymedia\nAcknowledgments\nWhen we started writing this book, we were warned by many people that we faced a\nhard task. A book like this has a lot of moving parts, and due to its comprehensive\nview of the field of data engineering, it required a ton of research, interviews, discus‐\nsions, and deep thinking. We won’t claim to have captured every nuance of data\nengineering, but we hope that the results resonate with you. Numerous individuals\ncontributed to our efforts, and we’re grateful for the support we received from many\nexperts.\nFirst, thanks to our amazing crew of technical reviewers. They slogged through many\nreadings and gave invaluable (and often ruthlessly blunt) feedback. This book would\nbe a fraction of itself without their efforts. In no particular order, we give endless\nthanks to Bill Inmon, Andy Petrella, Matt Sharp, Tod Hanseman, Chris Tabb, Danny\nLebzyon, Martin Kleppman, Scott Lorimor, Nick Schrock, Lisa Steckman, Veronika\nDurgin, and Alex Woolford.\nSecond, we’ve had a unique opportunity to talk with the leading experts in the field\nof data on our live shows, podcasts, meetups, and endless private calls. Their ideas\nhelped shape our book. There are too many people to name individually, but we’ d\nlike to give shoutouts to Jordan Tigani, Zhamak Dehghani, Ananth Packkildurai,\nxviii | Preface\nShruti Bhat, Eric Tschetter, Benn Stancil, Kevin Hu, Michael Rogove, Ryan Wright,\nAdi Polak, Shinji Kim, Andreas Kretz, Egor Gryaznov, Chad Sanderson, Julie Price,\nMatt Turck, Monica Rogati, Mars Lan, Pardhu Gunnam, Brian Suk, Barr Moses, Lior\nGavish, Bruno Aziza, Gian Merlino, DeVaris Brown, Todd Beauchene, Tudor Girba,\nScott Taylor, Ori Rafael, Lee Edwards, Bryan Offutt, Ollie Hughes, Gilbert Eijkelen‐\nboom, Chris Bergh, Fabiana Clemente, Andreas Kretz, Ori Reshef, Nick Singh, Mark\nBalkenende, Kenten Danas, Brian Olsen, Lior Gavish, Rhaghu Murthy, Greg Coquillo,\nDavid Aponte, Demetrios Brinkmann, Sarah Catanzaro, Michel Tricot, Levi Davis,\nTed Walker, Carlos Kemeny, Josh Benamram, Chanin Nantasenamat, George Firican,\nJordan Goldmeir, Minhaaj Rehmam, Luigi Patruno, Vin Vashista, Danny Ma, Jesse\nAnderson, Alessya Visnjic, Vishal Singh, Dave Langer, Roy Hasson, Todd Odess, Che\nSharma, Scott Breitenother, Ben Taylor, Thom Ives, John Thompson, Brent Dykes,\nJosh Tobin, Mark Kosiba, Tyler Pugliese, Douwe Maan, Martin Traverso, Curtis\nKowalski, Bob Davis, Koo Ping Shung, Ed Chenard, Matt Sciorma, Tyler Folkman,\nJeff Baird, Tejas Manohar, Paul Singman, Kevin Stumpf, Willem Pineaar, and Michael\nDel Balso from Tecton, Emma Dahl, Harpreet Sahota, Ken Jee, Scott Taylor, Kate\nStrachnyi, Kristen Kehrer, Taylor Miller, Abe Gong, Ben Castleton, Ben Rogojan,\nDavid Mertz, Emmanuel Raj, Andrew Jones, Avery Smith, Brock Cooper, Jeff Larson,\nJon King, Holden Ackerman, Miriah Peterson, Felipe Hoffa, David Gonzalez, Richard\nWellman, Susan Walsh, Ravit Jain, Lauren Balik, Mikiko Bazeley, Mark Freeman,\nMike Wimmer, Alexey Shchedrin, Mary Clair Thompson, Julie Burroughs, Jason\nPedley, Freddy Drennan, Jason Pedley, Kelly and Matt Phillipps, Brian Campbell,\nFaris Chebib, Dylan Gregerson, Ken Myers, Jake Carter, Seth Paul, Ethan Aaron, and\nmany others.\nIf you’re not mentioned specifically, don’t take it personally. Y ou know who you are.\nLet us know and we’ll get you on the next edition.\nWe’ d also like to thank the Ternary Data team (Colleen McAuley, Maike Wells, Patrick\nDahl, Aaron Hunsaker, and others), our students, and the countless people around\nthe world who’ve supported us. It’s a great reminder the world is a very small place.\nWorking with the O’Reilly crew was amazing! Special thanks to Jess Haberman\nfor having confidence in us during the book proposal process, our amazing and\nextremely patient development editors Nicole Taché and Michele Cronin for invalua‐\nble editing, feedback, and support. Thank you also to the superb production team at\nO’Reilly (Greg and crew).\nJoe would like to thank his family—Cassie, Milo, and Ethan—for letting him write a\nbook. They had to endure a ton, and Joe promises to never write a book again. ;)\nMatt would like to thank his friends and family for their enduring patience and\nsupport. He’s still hopeful that Seneca will deign to give a five-star review after a good\ndeal of toil and missed family time around the holidays.\nPreface | xix",5174
08-Part I. Foundation and Building Blocks.pdf,08-Part I. Foundation and Building Blocks,PART I\nFoundation and Building Blocks,38
09-Chapter 1. Data Engineering Described.pdf,09-Chapter 1. Data Engineering Described,,0
10-Evolution of the Data Engineer.pdf,10-Evolution of the Data Engineer,"CHAPTER 1\nData Engineering Described\nIf you work in data or software, you may have noticed data engineering emerging\nfrom the shadows and now sharing the stage with data science. Data engineering\nis one of the hottest fields in data and technology, and for a good reason. It builds\nthe foundation for data science and analytics in production. This chapter explores\nwhat data engineering is, how the field was born and its evolution, the skills of data\nengineers, and with whom they work.\nWhat Is Data Engineering?\nDespite  the current popularity of data engineering, there’s a lot of confusion about\nwhat data engineering means and what data engineers do. Data engineering has exis‐\nted in some form since companies started doing things with data—such as predictive\nanalysis, descriptive analytics, and reports—and came into sharp focus alongside the\nrise of data science in the 2010s. For the purpose of this book, it’s critical to define\nwhat data engineering  and data engineer  mean.\nFirst, let’s look at the landscape of how data engineering is described and develop\nsome terminology we can use throughout this book. Endless definitions of data\nengineering  exist. In early 2022, a Google exact-match search for “what is data engi‐\nneering?” returns over 91,000 unique results. Before we give our definition, here are a\nfew examples of how some experts in the field define data engineering:\nData engineering is a set of operations aimed at creating interfaces and mechanisms\nfor the flow and access of information. It takes dedicated specialists—data engineers—\nto maintain data so that it remains available and usable by others. In short, data\nengineers set up and operate the organization’s data infrastructure, preparing it for\nfurther analysis by data analysts and scientists.\n3\n1“Data Engineering and Its Main Concepts, ” AlexSoft, last updated August 26, 2021, https://oreil.ly/e94py .\n2ETL stands for extract, transform, load , a common pattern we cover in the book.\n3Jesse Anderson, “The Two Types of Data Engineering, ” June 27, 2018, https://oreil.ly/dxDt6 .\n4Maxime Beauchemin, “The Rise of the Data Engineer, ” January 20, 2017, https://oreil.ly/kNDmd .\n5Lewis Gavin, What Is Data Engineering?  (Sebastapol, CA: O’Reilly, 2020), https://oreil.ly/ELxLi .—From “Data Engineering and Its Main Concepts” by AlexSoft1\nThe first type of data engineering is SQL-focused. The work and primary storage of\nthe data is in relational databases. All of the data processing is done with SQL or a\nSQL-based language. Sometimes, this data processing is done with an ETL tool.2 The\nsecond type of data engineering is Big Data–focused. The work and primary storage of\nthe data is in Big Data technologies like Hadoop, Cassandra, and HBase. All of the data\nprocessing is done in Big Data frameworks like MapReduce, Spark, and Flink. While\nSQL is used, the primary processing is done with programming languages like Java,\nScala, and Python.\n—Jesse Anderson3\nIn relation to previously existing roles, the data engineering field could be thought of\nas a superset of business intelligence and data warehousing that brings more elements\nfrom software engineering. This discipline also integrates specialization around the\noperation of so-called “big data” distributed systems, along with concepts around the\nextended Hadoop ecosystem, stream processing, and in computation at scale.\n—Maxime Beauchemin4\nData engineering is all about the movement, manipulation, and management of data.\n—Lewis Gavin5\nWow! It’s entirely understandable if you’ve been confused about data engineering.\nThat’s only a handful of definitions, and they contain an enormous range of opinions\nabout the meaning of data engineering .\nData Engineering Defined\nWhen we unpack the common threads of how various people define data engineer‐\ning, an obvious pattern emerges: a data engineer gets data, stores it, and prepares it\nfor consumption by data scientists, analysts, and others. We define data engineering\nand data engineer  as follows:\nData engineering  is the development, implementation, and maintenance of systems\nand processes that take in raw data and produce high-quality, consistent information\nthat supports downstream use cases, such as analysis and machine learning. Data engi‐\nneering is the intersection of security, data management, DataOps, data architecture,\norchestration, and software engineering. A data engineer  manages the data engineering\nlifecycle, beginning with getting data from source systems and ending with serving\ndata for use cases, such as analysis or machine learning.\n4 | Chapter 1: Data Engineering Described\nThe Data Engineering Lifecycle\nIt is all too easy to fixate on technology and miss the bigger picture myopically. This\nbook centers around a big idea called the data engineering lifecycle  (Figure 1-1 ), which\nwe believe gives data engineers the holistic context to view their role.\nFigure 1-1. The data engineering lifecycle\nThe data engineering lifecycle shifts the conversation away from technology and\ntoward the data itself and the end goals that it must serve. The stages of the data\nengineering lifecycle are as follows:\n•Generation•\n•Storage•\n•Ingestion•\n•Transformation•\n•Serving•\nThe data engineering lifecycle also has a notion of  undercurrents— critical ideas across\nthe entire lifecycle. These include security, data management, DataOps, data architec‐\nture, orchestration, and software engineering. We cover the data engineering lifecycle\nand its undercurrents more extensively in Chapter 2 . Still, we introduce it here\nbecause it is essential to our definition of data engineering and the discussion that\nfollows in this chapter.\nNow that you have a working definition of data engineering and an introduction to\nits lifecycle, let’s take a step back and look at a bit of history.\nWhat Is Data Engineering? | 5\nEvolution of the Data Engineer\nHistory doesn’t repeat itself, but it rhymes.\n—A famous adage often attributed to Mark Twain\nUnderstanding  data engineering today and tomorrow requires a context of how the\nfield evolved. This section is not a history lesson, but looking at the past is invaluable\nin understanding where we are today and where things are going. A common theme\nconstantly reappears: what’s old is new again.\nThe early days: 1980 to 2000, from data warehousing to the web\nThe birth of the data engineer arguably has its roots in data warehousing, dating as\nfar back as the 1970s, with the business data warehouse  taking shape in the 1980s\nand Bill Inmon officially coining the term data warehouse  in 1989. After engineers\nat IBM developed the relational database and Structured Query Language (SQL),\nOracle popularized the technology. As nascent data systems grew, businesses needed\ndedicated tools and data pipelines for reporting and business intelligence (BI). To\nhelp people correctly model their business logic in the data warehouse, Ralph Kimball\nand Inmon developed their respective eponymous data-modeling techniques and\napproaches, which are still widely used today.\nData warehousing ushered in the first age of scalable analytics, with new massively\nparallel processing (MPP) databases that use multiple processors to crunch large\namounts of data coming on the market and supporting unprecedented volumes\nof data. Roles such as BI engineer, ETL developer, and data warehouse engineer\naddressed the various needs of the data warehouse. Data warehouse and BI engineer‐\ning were a precursor to today’s data engineering and still play a central role in the\ndiscipline.\nThe internet went mainstream around the mid-1990s, creating a whole new genera‐\ntion of web-first companies such as AOL, Y ahoo, and Amazon. The dot-com boom\nspawned a ton of activity in web applications and the backend systems to support\nthem—servers, databases, and storage. Much of the infrastructure was expensive,\nmonolithic, and heavily licensed. The vendors selling these backend systems likely\ndidn’t foresee the sheer scale of the data that web applications would produce.\nThe early 2000s: The birth of contemporary data engineering\nFast-forward to the early 2000s, when the dot-com boom of the late ’90s went bust,\nleaving behind a tiny cluster of survivors. Some of these companies, such as Y ahoo,\nGoogle, and Amazon, would grow into powerhouse tech companies. Initially, these\ncompanies continued to rely on the traditional monolithic, relational databases and\ndata warehouses of the 1990s, pushing these systems to the limit. As these systems\n6 | Chapter 1: Data Engineering Described\n6Cade Metz, “How Y ahoo Spawned Hadoop, the Future of Big Data, ” Wired , October 18, 2011,\nhttps://oreil.ly/iaD9G .\n7Ron Miller, “How AWS Came to Be, ” TechCrunch , July 2, 2016, https://oreil.ly/VJehv .buckled, updated approaches were needed to handle data growth. The new genera‐\ntion of the systems must be cost-effective, scalable, available, and reliable.\nCoinciding with the explosion of data, commodity hardware—such as servers, RAM,\ndisks, and flash drives—also became cheap and ubiquitous. Several innovations\nallowed distributed computation and storage on massive computing clusters at a\nvast scale. These innovations started decentralizing and breaking apart traditionally\nmonolithic services. The “big data” era had begun.\nThe Oxford English Dictionary  defines big data  as “extremely large data sets that may\nbe analyzed computationally to reveal patterns, trends, and associations, especially\nrelating to human behavior and interactions. ” Another famous and succinct descrip‐\ntion of big data is the three Vs of data: velocity, variety, and volume.\nIn 2003, Google published a paper on the Google File System, and shortly after\nthat, in 2004, a paper on MapReduce, an ultra-scalable data-processing paradigm.\nIn truth, big data has earlier antecedents in MPP data warehouses and data manage‐\nment for experimental physics projects, but Google’s publications constituted a “big\nbang” for data technologies and the cultural roots of data engineering as we know it\ntoday. Y ou’ll learn more about MPP systems and MapReduce in Chapters 3 and 8,\nrespectively.\nThe Google papers inspired engineers at Y ahoo to develop and later open source\nApache Hadoop in 2006.6 It’s hard to overstate the impact of Hadoop. Software\nengineers interested in large-scale data problems were drawn to the possibilities of\nthis new open source technology ecosystem. As companies of all sizes and types\nsaw their data grow into many terabytes and even petabytes, the era of the big data\nengineer was born.\nAround the same time, Amazon had to keep up with its own exploding data needs\nand created elastic computing environments (Amazon Elastic Compute Cloud, or\nEC2), infinitely scalable storage systems (Amazon Simple Storage Service, or S3),\nhighly scalable NoSQL databases (Amazon DynamoDB), and many other core data\nbuilding blocks.7 Amazon elected to offer these services for internal and external\nconsumption through Amazon Web Services  (AWS), becoming the first popular\npublic cloud. AWS created an ultra-flexible pay-as-you-go resource marketplace by\nvirtualizing and reselling vast pools of commodity hardware. Instead of purchasing\nhardware for a data center, developers could simply rent compute and storage from\nAWS.\nWhat Is Data Engineering? | 7\nAs AWS became a highly profitable growth engine for Amazon, other public clouds\nwould soon follow, such as Google Cloud, Microsoft Azure, and DigitalOcean. The\npublic cloud is arguably one of the most significant innovations of the 21st century\nand spawned a revolution in the way software and data applications are developed\nand deployed.\nThe early big data tools and public cloud laid the foundation for today’s data ecosys‐\ntem. The modern data landscape—and data engineering as we know it now—would\nnot exist without these innovations.\nThe 2000s and 2010s: Big data engineering\nOpen source big data tools in the Hadoop ecosystem rapidly matured and spread\nfrom Silicon Valley to tech-savvy companies worldwide. For the first time, any busi‐\nness had access to the same bleeding-edge data tools used by the top tech companies.\nAnother revolution occurred with the transition from batch computing to event\nstreaming, ushering in a new era of big “real-time” data. Y ou’ll learn about batch and\nevent streaming throughout this book.\nEngineers could choose the latest and greatest—Hadoop, Apache Pig, Apache Hive,\nDremel, Apache HBase, Apache Storm, Apache Cassandra, Apache Spark, Presto,\nand numerous other new technologies that came on the scene. Traditional enterprise-\noriented and GUI-based data tools suddenly felt outmoded, and code-first engineer‐\ning was in vogue with the ascendance of MapReduce. We (the authors) were around\nduring this time, and it felt like old dogmas died a sudden death upon the altar of big\ndata.\nThe explosion of data tools in the late 2000s and 2010s ushered in the big data\nengineer . To effectively use these tools and techniques—namely, the Hadoop eco‐\nsystem including Hadoop, YARN, Hadoop Distributed File System (HDFS), and\nMapReduce—big data engineers had to be proficient in software development and\nlow-level infrastructure hacking, but with a shifted emphasis. Big data engineers\ntypically maintained massive clusters of commodity hardware to deliver data at scale.\nWhile they might occasionally submit pull requests to Hadoop core code, they shifted\ntheir focus from core technology development to data delivery.\nBig data quickly became a victim of its own success. As a buzzword, big data  gained\npopularity during the early 2000s through the mid-2010s. Big data captured the\nimagination of companies trying to make sense of the ever-growing volumes of data\nand the endless barrage of shameless marketing from companies selling big data tools\nand services. Because of the immense hype, it was common to see companies using\nbig data tools for small data problems, sometimes standing up a Hadoop cluster to\nprocess just a few gigabytes. It seemed like everyone wanted in on the big data action.\nDan Ariely tweeted , “Big data is like teenage sex: everyone talks about it, nobody\n8 | Chapter 1: Data Engineering Described\nreally knows how to do it, everyone thinks everyone else is doing it, so everyone\nclaims they are doing it. ”\nFigure 1-2  shows a snapshot of Google Trends for the search term “big data” to get an\nidea of the rise and fall of big data.\nFigure 1-2. Google Trends for “big data” (March 2022)\nDespite the term’s popularity, big data has lost steam. What happened? One word:\nsimplification. Despite the power and sophistication of open source big data tools,\nmanaging them was a lot of work and required constant attention. Often, companies\nemployed entire teams of big data engineers, costing millions of dollars a year, to\nbabysit these platforms. Big data engineers often spent excessive time maintaining\ncomplicated tooling and arguably not as much time delivering the business’s insights\nand value.\nOpen source developers, clouds, and third parties started looking for ways to abstract,\nsimplify, and make big data available without the high administrative overhead and\ncost of managing their clusters, and installing, configuring, and upgrading their open\nsource code. The term big data  is essentially a relic to describe a particular time and\napproach to handling large amounts of data.\nToday, data is moving faster than ever and growing ever larger, but big data process‐\ning has become so accessible that it no longer merits a separate term; every company\naims to solve its data problems, regardless of actual data size. Big data engineers are\nnow simply data engineers .\nWhat Is Data Engineering? | 9\n8DataOps  is an abbreviation for data operations . We cover this topic in Chapter 2 . For more information, read\nthe DataOps Manifesto .The 2020s: Engineering for the data lifecycle\nAt the time of this writing, the data engineering role is evolving rapidly. We expect\nthis evolution to continue at a rapid clip for the foreseeable future. Whereas data\nengineers historically tended to the low-level details of monolithic frameworks such\nas Hadoop, Spark, or Informatica, the trend is moving toward decentralized, modu‐\nlarized, managed, and highly abstracted tools.\nIndeed, data tools have proliferated at an astonishing rate (see Figure 1-3 ). Popular\ntrends in the early 2020s include the modern data stack , representing a collection\nof off-the-shelf open source and third-party products assembled to make analysts’\nlives easier. At the same time, data sources and data formats are growing both in\nvariety and size. Data engineering is increasingly a discipline of interoperation, and\nconnecting various technologies like LEGO bricks, to serve ultimate business goals.\nFigure 1-3. Matt Turck’s Data Landscape  in 2012 versus 2021\nThe data engineer we discuss in this book can be described more precisely as a  data\nlifecycle engineer.  With greater abstraction and simplification, a data lifecycle engineer\nis no longer encumbered by the gory details of yesterday’s big data frameworks.\nWhile data engineers maintain skills in low-level data programming and use these as\nrequired, they increasingly find their role focused on things higher in the value chain:\nsecurity, data management, DataOps, data architecture, orchestration, and general\ndata lifecycle management.8\nAs tools and workflows simplify, we’ve seen a noticeable shift in the attitudes of\ndata engineers. Instead of focusing on who has the “biggest data, ” open source\nprojects and services are increasingly concerned with managing and governing data,\nmaking it easier to use and discover, and improving its quality. Data engineers are\n10 | Chapter 1: Data Engineering Described",17945
11-Data Engineering and Data Science.pdf,11-Data Engineering and Data Science,"9These acronyms stand for California Consumer Privacy Act  and General Data Protection Regulation ,\nrespectively.now conversant in acronyms such as CCPA  and GDPR ;9 as they engineer pipelines,\nthey concern themselves with privacy, anonymization, data garbage collection, and\ncompliance with regulations.\nWhat’s old is new again. While “enterprisey” stuff like data management (including\ndata quality and governance) was common for large enterprises in the pre-big-data\nera, it wasn’t widely adopted in smaller companies. Now that many of the challenging\nproblems of yesterday’s data systems are solved, neatly productized, and packaged,\ntechnologists and entrepreneurs have shifted focus back to the “enterprisey” stuff, but\nwith an emphasis on decentralization and agility, which contrasts with the traditional\nenterprise command-and-control approach.\nWe view the present as a golden age of data lifecycle management. Data engineers\nmanaging the data engineering lifecycle have better tools and techniques than ever\nbefore. We discuss the data engineering lifecycle and its undercurrents in greater\ndetail in the next chapter.\nData Engineering and Data Science\nWhere does data engineering fit in with data science? There’s some debate, with some\narguing data engineering is a subdiscipline of data science. We believe data engineer‐\ning is separate  from data science and analytics. They complement each other, but they\nare distinctly different. Data engineering sits upstream from data science ( Figure 1-4 ),\nmeaning data engineers provide the inputs used by data scientists (downstream from\ndata engineering), who convert these inputs into something useful.\nFigure 1-4. Data engineering sits upstream from data science\nConsider  the Data Science Hierarchy of Needs ( Figure 1-5 ). In 2017, Monica Rogati\npublished this hierarchy in an article  that showed where AI and machine learning\n(ML) sat in proximity to more “mundane” areas such as data movement/storage,\ncollection, and infrastructure.\nWhat Is Data Engineering? | 11\nFigure 1-5. The Data Science Hierarchy of Needs\nAlthough many data scientists are eager to build and tune ML models, the reality is\nan estimated 70% to 80% of their time is spent toiling in the bottom three parts of\nthe hierarchy—gathering data, cleaning data, processing data—and only a tiny slice\nof their time on analysis and ML. Rogati argues that companies need to build a solid\ndata foundation (the bottom three levels of the hierarchy) before tackling areas such\nas AI and ML.\nData scientists aren’t typically trained to engineer production-grade data systems, and\nthey end up doing this work haphazardly because they lack the support and resources\nof a data engineer. In an ideal world, data scientists should spend more than 90% of\ntheir time focused on the top layers of the pyramid: analytics, experimentation, and\nML. When data engineers focus on these bottom parts of the hierarchy, they build a\nsolid foundation for data scientists to succeed.\nWith data science driving advanced analytics and ML, data engineering straddles\nthe divide between getting data and getting value from data (see Figure 1-6 ). We\nbelieve data engineering is of equal importance and visibility to data science, with\ndata engineers playing a vital role in making data science successful in production.\nFigure 1-6. A data engineer gets data and provides value from the data\n12 | Chapter 1: Data Engineering Described",3480
12-Data Engineering Skills and Activities.pdf,12-Data Engineering Skills and Activities,,0
13-Data Maturity and the Data Engineer.pdf,13-Data Maturity and the Data Engineer,"Data Engineering Skills and Activities\nThe skill set of a data engineer encompasses the “undercurrents” of data engineering:\nsecurity, data management, DataOps, data architecture, and software engineering.\nThis skill set requires an understanding of how to evaluate data tools and how they\nfit together across the data engineering lifecycle. It’s also critical to know how data is\nproduced in source systems and how analysts and data scientists will consume and\ncreate value after processing and curating data. Finally, a data engineer juggles a lot\nof complex moving parts and must constantly optimize along the axes of cost, agility,\nscalability, simplicity, reuse, and interoperability ( Figure 1-7 ). We cover these topics\nin more detail in upcoming chapters.\nFigure 1-7. The balancing act of data engineering\nAs we discussed, in the recent past, a data engineer was expected to know and under‐\nstand how to use a small handful of powerful and monolithic technologies (Hadoop,\nSpark, Teradata, Hive, and many others) to create a data solution. Utilizing these\ntechnologies often requires a sophisticated understanding of software engineering,\nnetworking, distributed computing, storage, or other low-level details. Their work\nwould be devoted to cluster administration and maintenance, managing overhead,\nand writing pipeline and transformation jobs, among other tasks.\nNowadays, the data-tooling landscape is dramatically less complicated to manage and\ndeploy. Modern data tools considerably abstract and simplify workflows. As a result,\ndata engineers are now focused on balancing the simplest and most cost-effective,\nbest-of-breed services that deliver value to the business. The data engineer is also\nexpected to create agile data architectures that evolve as new trends emerge.\nWhat are some things a data engineer does not do? A data engineer typically does not\ndirectly build ML models, create reports or dashboards, perform data analysis, build\nkey performance indicators (KPIs), or develop software applications. A data engineer\nshould have a good functioning understanding of these areas to serve stakeholders\nbest.\nData Maturity and the Data Engineer\nThe level of data engineering complexity within a company depends a great deal on\nthe company’s data maturity. This significantly impacts a data engineer’s day-to-day\njob responsibilities and career progression. What is data maturity, exactly?\nData maturity  is the progression toward higher data utilization, capabilities, and\nintegration across the organization, but data maturity does not simply depend on the\nData Engineering Skills and Activities | 13\nage or revenue of a company. An early-stage startup can have greater data maturity\nthan a 100-year-old company with annual revenues in the billions. What matters is\nthe way data is leveraged as a competitive advantage.\nData maturity models have many versions, such as Data Management Maturity\n(DMM)  and others, and it’s hard to pick one that is both simple and useful for\ndata engineering. So, we’ll create our own simplified data maturity model. Our data\nmaturity model ( Figure 1-8 ) has three stages: starting with data, scaling with data,\nand leading with data. Let’s look at each of these stages and at what a data engineer\ntypically does at each stage.\nFigure 1-8. Our simplified  data maturity model for a company\nStage 1: Starting with data\nA company getting started with data is, by definition, in the very early stages of\nits data maturity. The company may have fuzzy, loosely defined goals or no goals.\nData architecture and infrastructure are in the very early stages of planning and\ndevelopment. Adoption and utilization are likely low or nonexistent. The data team\nis small, often with a headcount in the single digits. At this stage, a data engineer is\nusually a generalist and will typically play several other roles, such as data scientist or\nsoftware engineer. A data engineer’s goal is to move fast, get traction, and add value.\nThe practicalities of getting value from data are typically poorly understood, but the\ndesire exists. Reports or analyses lack formal structure, and most requests for data\nare ad hoc. While it’s tempting to jump headfirst into ML at this stage, we don’t\nrecommend it. We’ve seen countless data teams get stuck and fall short when they try\nto jump to ML without building a solid data foundation.\nThat’s not to say you can’t get wins from ML at this stage—it is rare but possible.\nWithout a solid data foundation, you likely won’t have the data to train reliable\nML models nor the means to deploy these models to production in a scalable and\nrepeatable way. We half-jokingly call ourselves “recovering data scientists” , mainly\nfrom personal experience with being involved in premature data science projects\nwithout adequate data maturity or data engineering support.\nA data engineer should focus on the following in organizations getting started with\ndata:\n14 | Chapter 1: Data Engineering Described\n•Get buy-in from key stakeholders, including executive management. Ideally, the•\ndata engineer should have a sponsor for critical initiatives to design and build a\ndata architecture to support the company’s goals.\n•Define the right data architecture (usually solo, since a data architect likely isn’t•\navailable). This means determining business goals and the competitive advantage\nyou’re aiming to achieve with your data initiative. Work toward a data architec‐\nture that supports these goals. See Chapter 3  for our advice on “good” data\narchitecture.\n•Identify and audit data that will support key initiatives and operate within the•\ndata architecture you designed.\n•Build a solid data foundation for future data analysts and data scientists to•\ngenerate reports and models that provide competitive value. In the meantime,\nyou may also have to generate these reports and models until this team is hired.\nThis is a delicate stage with lots of pitfalls. Here are some tips for this stage:\n•Organizational willpower may wane if a lot of visible successes don’t occur with•\ndata. Getting quick wins will establish the importance of data within the organi‐\nzation. Just keep in mind that quick wins will likely create technical debt. Have a\nplan to reduce this debt, as it will otherwise add friction for future delivery.\n•Get out and talk to people, and avoid working in silos. We often see the data team•\nworking in a bubble, not communicating with people outside their departments\nand getting perspectives and feedback from business stakeholders. The danger is\nyou’ll spend a lot of time working on things of little use to people.\n•Avoid undifferentiated heavy lifting. Don’t box yourself in with unnecessary•\ntechnical complexity. Use off-the-shelf, turnkey solutions wherever possible.\n•Build custom solutions and code only where this creates a competitive advantage.•\nStage 2: Scaling with data\nAt this point, a company has moved away from ad hoc data requests and has formal\ndata practices. Now the challenge is creating scalable data architectures and planning\nfor a future where the company is genuinely data-driven. Data engineering roles\nmove from generalists to specialists, with people focusing on particular aspects of the\ndata engineering lifecycle.\nIn organizations that are in stage 2 of data maturity, a data engineer’s goals are to do\nthe following:\n•Establish formal data practices•\n•Create scalable and robust data architectures•\nData Engineering Skills and Activities | 15\n•Adopt DevOps and DataOps practices•\n•Build systems that support ML•\n•Continue to avoid undifferentiated heavy lifting and customize only when a•\ncompetitive advantage results\nWe return to each of these goals later in the book.\nIssues to watch out for include the following:\n•As we grow more sophisticated with data, there’s a temptation to adopt bleeding-•\nedge technologies based on social proof from Silicon Valley companies. This is\nrarely a good use of your time and energy. Any technology decisions should be\ndriven by the value they’ll deliver to your customers.\n•The main bottleneck for scaling is not cluster nodes, storage, or technology but•\nthe data engineering team. Focus on solutions that are simple to deploy and\nmanage to expand your team’s throughput.\n•Y ou’ll be tempted to frame yourself as a technologist, a data genius who can•\ndeliver magical products. Shift your focus instead to pragmatic leadership and\nbegin transitioning to the next maturity stage; communicate with other teams\nabout the practical utility of data. Teach the organization how to consume and\nleverage data.\nStage 3: Leading with data\nAt this stage, the company is data-driven. The automated pipelines and systems cre‐\nated by data engineers allow people within the company to do self-service analytics\nand ML. Introducing new data sources is seamless, and tangible value is derived.\nData engineers implement proper controls and practices to ensure that data is always\navailable to the people and systems. Data engineering roles continue to specialize\nmore deeply than in stage 2.\nIn organizations in stage 3 of data maturity, a data engineer will continue building on\nprior stages, plus they will do the following:\n•Create automation for the seamless introduction and usage of new data•\n•Focus on building custom tools and systems that leverage data as a competitive•\nadvantage\n•Focus on the “enterprisey” aspects of data, such as data management (including•\ndata governance and quality) and DataOps\n•Deploy tools that expose and disseminate data throughout the organization,•\nincluding data catalogs, data lineage tools, and metadata management systems\n16 | Chapter 1: Data Engineering Described",9791
14-Technical Responsibilities.pdf,14-Technical Responsibilities,"•Collaborate efficiently with software engineers, ML engineers, analysts, and•\nothers\n•Create a community and environment where people can collaborate and speak•\nopenly, no matter their role or position\nIssues to watch out for include the following:\n•At this stage, complacency is a significant danger. Once organizations reach•\nstage 3, they must constantly focus on maintenance and improvement or risk\nfalling back to a lower stage.\n•Technology distractions are a more significant danger here than in the other•\nstages. There’s a temptation to pursue expensive hobby projects that don’t deliver\nvalue to the business. Utilize custom-built technology only where it provides a\ncompetitive advantage.\nThe Background and Skills of a Data Engineer\nData engineering is a fast-growing field, and a lot of questions remain about how\nto become a data engineer. Because data engineering is a relatively new discipline,\nlittle formal training is available to enter the field. Universities don’t have a standard\ndata engineering path. Although a handful of data engineering boot camps and online\ntutorials cover random topics, a common curriculum for the subject doesn’t yet exist.\nPeople entering data engineering arrive with varying backgrounds in education,\ncareer, and skill set. Everyone entering the field should expect to invest a significant\namount of time in self-study. Reading this book is a good starting point; one of the\nprimary goals of this book is to give you a foundation for the knowledge and skills we\nthink are necessary to succeed as a data engineer.\nIf you’re pivoting your career into data engineering, we’ve found that the transition is\neasiest when moving from an adjacent field, such as software engineering, ETL devel‐\nopment, database administration, data science, or data analysis. These disciplines\ntend to be “data aware” and provide good context for data roles in an organization.\nThey also equip folks with the relevant technical skills and context to solve data\nengineering problems.\nDespite the lack of a formalized path, a requisite body of knowledge exists that we\nbelieve a data engineer should know to be successful. By definition, a data engineer\nmust understand both data and technology. With respect to data, this entails knowing\nabout various best practices around data management. On the technology end, a\ndata engineer must be aware of various options for tools, their interplay, and their\ntrade-offs. This requires a good understanding of software engineering, DataOps, and\ndata architecture.\nData Engineering Skills and Activities | 17\nZooming out, a data engineer must also understand the requirements of data con‐\nsumers (data analysts and data scientists) and the broader implications of data across\nthe organization. Data engineering is a holistic practice; the best data engineers view\ntheir responsibilities through business and technical lenses.\nBusiness Responsibilities\nThe macro responsibilities we list in this section aren’t exclusive to data engineers but\nare crucial for anyone working in a data or technology field. Because a simple Google\nsearch will yield tons of resources to learn about these areas, we will simply list them\nfor brevity:\nKnow how to communicate with nontechnical and technical people.\nCommunication is key, and you need to be able to establish rapport and trust\nwith people across the organization. We suggest paying close attention to organi‐\nzational hierarchies, who reports to whom, how people interact, and which silos\nexist. These observations will be invaluable to your success.\nUnderstand how to scope and gather business and product requirements.\nY ou need to know what to build and ensure that your stakeholders agree with\nyour assessment. In addition, develop a sense of how data and technology deci‐\nsions impact the business.\nUnderstand the cultural foundations of Agile, DevOps, and DataOps.\nMany technologists mistakenly believe these practices are solved through tech‐\nnology. We feel this is dangerously wrong. Agile, DevOps, and DataOps are\nfundamentally cultural, requiring buy-in across the organization.\nControl costs.\nY ou’ll be successful when you can keep costs low while providing outsized value.\nKnow how to optimize for time to value, the total cost of ownership, and oppor‐\ntunity cost. Learn to monitor costs to avoid surprises.\nLearn continuously.\nThe data field feels like it’s changing at light speed. People who succeed in it are\ngreat at picking up new things while sharpening their fundamental knowledge.\nThey’re also good at filtering, determining which new developments are most\nrelevant to their work, which are still immature, and which are just fads. Stay\nabreast of the field and learn how to learn.\nA successful data engineer always zooms out to understand the big picture and\nhow to achieve outsized value for the business. Communication is vital, both for\ntechnical and nontechnical people. We often see data teams succeed based on their\ncommunication with other stakeholders; success or failure is rarely a technology\nissue. Knowing how to navigate an organization, scope and gather requirements,\n18 | Chapter 1: Data Engineering Described\ncontrol costs, and continuously learn will set you apart from the data engineers who\nrely solely on their technical abilities to carry their career.\nTechnical Responsibilities\nY ou must understand how to build architectures that optimize performance and cost\nat a high level, using prepackaged or homegrown components. Ultimately, architec‐\ntures and constituent technologies are building blocks to serve the data engineering\nlifecycle. Recall the stages of the data engineering lifecycle:\n•Generation•\n•Storage•\n•Ingestion•\n•Transformation•\n•Serving•\nThe undercurrents of the data engineering lifecycle are the following:\n•Security•\n•Data management•\n•DataOps•\n•Data architecture•\n•Orchestration•\n•Software engineering•\nZooming in a bit, we discuss some of the tactical data and technology skills you’ll\nneed as a data engineer in this section; we discuss these in more detail in subsequent\nchapters.\nPeople often ask, should a data engineer know how to code? Short answer: yes. A data\nengineer should have production-grade software engineering chops. We note that the\nnature of software development projects undertaken by data engineers has changed\nfundamentally in the last few years. Fully managed services now replace a great deal\nof low-level programming effort previously expected of engineers, who now use man‐\naged open source, and simple plug-and-play software-as-a-service (SaaS) offerings.\nFor example, data engineers now focus on high-level abstractions or writing pipelines\nas code within an orchestration framework.\nEven in a more abstract world, software engineering best practices provide a competi‐\ntive advantage, and data engineers who can dive into the deep architectural details of\na codebase give their companies an edge when specific technical needs arise. In short,\na data engineer who can’t write production-grade code will be severely hindered, and\nData Engineering Skills and Activities | 19\nwe don’t see this changing anytime soon. Data engineers remain software engineers,\nin addition to their many other roles.\nWhat languages should a data engineer know? We divide data engineering program‐\nming languages into primary and secondary categories. At the time of this writing,\nthe primary languages of data engineering are SQL, Python, a Java Virtual Machine\n(JVM) language (usually Java or Scala), and bash:\nSQL\nThe most common interface for databases and data lakes. After briefly being\nsidelined by the need to write custom MapReduce code for big data processing,\nSQL (in various forms) has reemerged as the lingua franca of data.\nPython\nThe bridge language between data engineering and data science. A growing\nnumber of data engineering tools are written in Python or have Python APIs.\nIt’s known as “the second-best language at everything. ” Python underlies popular\ndata tools such as pandas, NumPy, Airflow, sci-kit learn, TensorFlow, PyTorch,\nand PySpark. Python is the glue between underlying components and is fre‐\nquently a first-class API language for interfacing with a framework.\nJVM languages such as Java and Scala\nPrevalent  for Apache open source projects such as Spark, Hive, and Druid.\nThe JVM is generally more performant than Python and may provide access to\nlower-level features than a Python API (for example, this is the case for Apache\nSpark and Beam). Understanding Java or Scala will be beneficial if you’re using a\npopular open source data framework.\nbash\nThe command-line interface for Linux operating systems. Knowing bash com‐\nmands and being comfortable using CLIs will significantly improve your pro‐\nductivity and workflow when you need to script or perform OS operations.\nEven today, data engineers frequently use command-line tools like awk or sed\nto process files in a data pipeline or call bash commands from orchestration\nframeworks. If you’re using Windows, feel free to substitute PowerShell for bash.\nThe Unreasonable Effectiveness  of SQL\nThe advent of MapReduce and the big data era relegated SQL to passé status. Since\nthen, various developments have dramatically enhanced the utility of SQL in the data\nengineering lifecycle. Spark SQL, Google BigQuery, Snowflake, Hive, and many other\ndata tools can process massive amounts of data by using declarative, set-theoretic\nSQL semantics. SQL is also supported by many streaming frameworks, such as\nApache Flink, Beam, and Kafka. We believe that competent data engineers should be\nhighly proficient in SQL.\n20 | Chapter 1: Data Engineering Described",9776
15-Data Engineers and Other Technical Roles.pdf,15-Data Engineers and Other Technical Roles,"Are we saying that SQL is a be-all and end-all language? Not at all. SQL is a powerful\ntool that can quickly solve complex analytics and data transformation problems.\nGiven that time is a primary constraint for data engineering team throughput, engi‐\nneers should embrace tools that combine simplicity and high productivity. Data\nengineers also do well to develop expertise in composing SQL with other operations,\neither within frameworks such as Spark and Flink or by using orchestration to\ncombine multiple tools. Data engineers should also learn modern SQL semantics for\ndealing with JavaScript Object Notation (JSON) parsing and nested data and consider\nleveraging a SQL management framework such as dbt (Data Build Tool) .\nA proficient data engineer also recognizes when SQL is not the right tool for the job\nand can choose and code in a suitable alternative. A SQL expert could likely write a\nquery to stem and tokenize raw text in a natural language processing (NLP) pipeline\nbut would also recognize that coding in native Spark is a far superior alternative to\nthis masochistic exercise.\nData engineers may also need to develop proficiency in secondary programming\nlanguages, including R, JavaScript, Go, Rust, C/C++, C#, and Julia. Developing\nin these languages is often necessary when popular across the company or used\nwith domain-specific data tools. For instance, JavaScript has proven popular as a\nlanguage for user-defined functions in cloud data warehouses. At the same time, C#\nand PowerShell are essential in companies that leverage Azure and the Microsoft\necosystem.\nKeeping Pace in a Fast-Moving Field\nOnce a new technology rolls over you, if you’re not part of the steamroller, you’re\npart of the road.\n—Stewart Brand\nHow do you keep your skills sharp in a rapidly changing field like data engineering?\nShould you focus on the latest tools or deep dive into fundamentals? Here’s our\nadvice: focus on the fundamentals to understand what’s not going to change; pay\nattention to ongoing developments to know where the field is going. New paradigms\nand practices are introduced all the time, and it’s incumbent on you to stay current.\nStrive to understand how new technologies will be helpful in the lifecycle.\nThe Continuum of Data Engineering Roles, from A to B\nAlthough  job descriptions paint a data engineer as a “unicorn” who must possess\nevery data skill imaginable, data engineers don’t all do the same type of work or have\nthe same skill set. Data maturity is a helpful guide to understanding the types of data\nchallenges a company will face as it grows its data capability. It’s beneficial to look\nData Engineering Skills and Activities | 21\n10Robert Chang, “Doing Data Science at Twitter, ” Medium , June 20, 2015, https://oreil.ly/xqjAx .at some critical distinctions in the kinds of work data engineers do. Though these\ndistinctions are simplistic, they clarify what data scientists and data engineers do and\navoid lumping either role into the unicorn bucket.\nIn data science, there’s the notion of type A and type B data scientists.10 Type A\ndata scientists —where A stands for analysis —focus on understanding and deriving\ninsight from data. Type B data scientists —where B stands for building —share similar\nbackgrounds as type A data scientists and possess strong programming skills. The\ntype B data scientist builds systems that make data science work in production.\nBorrowing from this data scientist continuum, we’ll create a similar distinction for\ntwo types of data engineers:\nType A data engineers\nA stands for abstraction . In this case, the data engineer avoids undifferentiated\nheavy lifting, keeping data architecture as abstract and straightforward as pos‐\nsible and not reinventing the wheel. Type A data engineers manage the data\nengineering lifecycle mainly by using entirely off-the-shelf products, managed\nservices, and tools. Type A data engineers work at companies across industries\nand at all levels of data maturity.\nType B data engineers\nB stands for build . Type B data engineers build data tools and systems that\nscale and leverage a company’s core competency and competitive advantage. In\nthe data maturity range, a type B data engineer is more commonly found at\ncompanies in stage 2 and 3 (scaling and leading with data), or when an initial\ndata use case is so unique and mission-critical that custom data tools are required\nto get started.\nType A and type B data engineers may work in the same company and may even\nbe the same person! More commonly, a type A data engineer is first hired to set the\nfoundation, with type B data engineer skill sets either learned or hired as the need\narises within a company.\nData Engineers Inside an Organization\nData engineers don’t work in a vacuum. Depending on what they’re working on,\nthey will interact with technical and nontechnical people and face different directions\n(internal and external). Let’s explore what data engineers do inside an organization\nand with whom they interact.\n22 | Chapter 1: Data Engineering Described\nInternal-Facing Versus External-Facing Data Engineers\nA data engineer serves several end users and faces many internal and external direc‐\ntions ( Figure 1-9 ). Since not all data engineering workloads and responsibilities are\nthe same, it’s essential to understand whom the data engineer serves. Depending\non the end-use cases, a data engineer’s primary responsibilities are external facing,\ninternal facing, or a blend of the two.\nFigure 1-9. The directions a data engineer faces\nAn external-facing  data engineer typically aligns with the users of external-facing\napplications, such as social media apps, Internet of Things (IoT) devices, and ecom‐\nmerce platforms. This data engineer architects, builds, and manages the systems that\ncollect, store, and process transactional and event data from these applications. The\nsystems built by these data engineers have a feedback loop from the application to the\ndata pipeline, and then back to the application ( Figure 1-10 ).\nFigure 1-10. External-facing data engineer systems\nExternal-facing data engineering comes with a unique set of problems. External-\nfacing query engines often handle much larger concurrency loads than internal-\nfacing systems. Engineers also need to consider putting tight limits on queries that\nusers can run to limit the infrastructure impact of any single user. In addition, secu‐\nrity is a much more complex and sensitive problem for external queries, especially\nif the data being queried is multitenant (data from many customers and housed in a\nsingle table).\nAn internal-facing data engineer  typically focuses on activities crucial to the needs of\nthe business and internal stakeholders ( Figure 1-11 ). Examples include creating and\nData Engineers Inside an Organization | 23\nmaintaining data pipelines and data warehouses for BI dashboards, reports, business\nprocesses, data science, and ML models.\nFigure 1-11. Internal-facing data engineer\nExternal-facing and internal-facing responsibilities are often blended. In practice,\ninternal-facing data is usually a prerequisite to external-facing data. The data engi‐\nneer has two sets of users with very different requirements for query concurrency,\nsecurity, and more.\nData Engineers and Other Technical Roles\nIn practice, the data engineering lifecycle cuts across many domains of responsibility.\nData engineers sit at the nexus of various roles, directly or through managers, inter‐\nacting with many organizational units.\nLet’s look at whom a data engineer may impact. In this section, we’ll discuss technical\nroles connected to data engineering ( Figure 1-12 ).\nFigure 1-12. Key technical stakeholders of data engineering\nThe data engineer is a hub between data producers , such as software engineers, data\narchitects, and DevOps or site-reliability engineers (SREs), and data consumers , such\nas data analysts, data scientists, and ML engineers. In addition, data engineers will\ninteract with those in operational roles, such as DevOps engineers.\nGiven the pace at which new data roles come into vogue (analytics and ML engineers\ncome to mind), this is by no means an exhaustive list.\n24 | Chapter 1: Data Engineering Described\n11Paramita (Guha) Ghosh, “Data Architect vs. Data Engineer, ” Dataversity, November 12, 2021,\nhttps://oreil.ly/TlyZY .Upstream stakeholders\nTo be successful as a data engineer, you need to understand the data architecture\nyou’re using or designing and the source systems producing the data you’ll need.\nNext, we discuss a few familiar upstream stakeholders: data architects, software\nengineers, and DevOps engineers.\nData architects.    Data architects  function at a level of abstraction one step removed\nfrom data engineers. Data architects design the blueprint for organizational data\nmanagement, mapping out processes and overall data architecture and systems.11\nThey also serve as a bridge between an organization’s technical and nontechnical\nsides. Successful data architects generally have “battle scars” from extensive engi‐\nneering experience, allowing them to guide and assist engineers while successfully\ncommunicating engineering challenges to nontechnical business stakeholders.\nData architects implement policies for managing data across silos and business units,\nsteer global strategies such as data management and data governance, and guide\nsignificant initiatives. Data architects often play a central role in cloud migrations and\ngreenfield cloud design.\nThe advent of the cloud has shifted the boundary between data architecture and\ndata engineering. Cloud data architectures are much more fluid than on-premises\nsystems, so architecture decisions that traditionally involved extensive study, long\nlead times, purchase contracts, and hardware installation are now often made during\nthe implementation process, just one step in a larger strategy. Nevertheless, data\narchitects will remain influential visionaries in enterprises, working hand in hand\nwith data engineers to determine the big picture of architecture practices and data\nstrategies.\nDepending on the company’s data maturity and size, a data engineer may overlap\nwith or assume the responsibilities of a data architect. Therefore, a data engineer\nshould have a good understanding of architecture best practices and approaches.\nNote that we have placed data architects in the upstream stakeholders  section. Data\narchitects often help design application data layers that are source systems for data\nengineers. Architects may also interact with data engineers at various other stages of\nthe data engineering lifecycle. We cover “good” data architecture in Chapter 3 .\nSoftware engineers.    Software engineers build the software and systems that run a\nbusiness; they are largely responsible for generating the internal data  that data engi‐\nneers will consume and process. The systems built by software engineers typically\ngenerate application event data and logs, which are significant assets in their own\nData Engineers Inside an Organization | 25\n12A variety of references exist for this notion. Although this cliche is widely known, a healthy debate has arisen\naround its validity in different practical settings. For more details, see Leigh Dodds, “Do Data Scientists Spend\n80% of Their Time Cleaning Data? Turns Out, No?” Lost Boy blog, January 31, 2020, https://oreil.ly/szFww ;\nand Alex Woodie, “Data Prep Still Dominates Data Scientists’ Time, Survey Finds, ” Datanami , July 6, 2020,\nhttps://oreil.ly/jDVWF .right. This internal data contrasts with external data  pulled from SaaS platforms or\npartner businesses. In well-run technical organizations, software engineers and data\nengineers coordinate from the inception of a new project to design application data\nfor consumption by analytics and ML applications.\nA data engineer should work together with software engineers to understand the\napplications that generate data, the volume, frequency, and format of the generated\ndata, and anything else that will impact the data engineering lifecycle, such as data\nsecurity and regulatory compliance. For example, this might mean setting upstream\nexpectations on what the data software engineers need to do their jobs. Data engi‐\nneers must work closely with the software engineers.\nDevOps engineers and site-reliability engineers.    DevOps and SREs often  produce data\nthrough operational monitoring. We classify them as upstream of data engineers, but\nthey may also be downstream, consuming data through dashboards or interacting\nwith data engineers directly in coordinating operations of data systems.\nDownstream stakeholders\nData engineering exists to serve downstream data consumers and use cases. This\nsection discusses how data engineers interact with various downstream roles. We’ll\nalso introduce a few service models, including centralized data engineering teams and\ncross-functional teams.\nData scientists.    Data scientists build forward-looking models to make predictions and\nrecommendations. These models are then evaluated on live data to provide value\nin various ways. For example, model scoring might determine automated actions in\nresponse to real-time conditions, recommend products to customers based on the\nbrowsing history in their current session, or make live economic predictions used by\ntraders.\nAccording to common industry folklore, data scientists spend 70% to 80% of their\ntime collecting, cleaning, and preparing data.12 In our experience, these numbers\noften reflect immature data science and data engineering practices. In particular,\nmany popular data science frameworks can become bottlenecks if they are not\nscaled up appropriately. Data scientists who work exclusively on a single worksta‐\ntion force themselves to downsample data, making data preparation significantly\nmore complicated and potentially compromising the quality of the models they\n26 | Chapter 1: Data Engineering Described\nproduce. Furthermore,  locally developed code and environments are often difficult\nto deploy in production, and a lack of automation significantly hampers data science\nworkflows. If data engineers do their job and collaborate successfully, data scientists\nshouldn’t spend their time collecting, cleaning, and preparing data after initial explor‐\natory work. Data engineers should automate this work as much as possible.\nThe need for production-ready data science is a significant driver behind the emer‐\ngence of the data engineering profession. Data engineers should help data scientists to\nenable a path to production. In fact, we (the authors) moved from data science to data\nengineering after recognizing this fundamental need. Data engineers work to provide\nthe data automation and scale that make data science more efficient.\nData analysts.    Data analysts (or business analysts) seek to understand business per‐\nformance and trends. Whereas data scientists are forward-looking, a data analyst\ntypically focuses on the past or present. Data analysts usually run SQL queries in a\ndata warehouse or a data lake. They may also utilize spreadsheets for computation\nand analysis and various BI tools such as Microsoft Power BI, Looker, or Tableau.\nData analysts are domain experts in the data they work with frequently and become\nintimately familiar with data definitions, characteristics, and quality problems. A\ndata analyst’s typical downstream customers are business users, management, and\nexecutives.\nData engineers work with data analysts to build pipelines for new data sources\nrequired by the business. Data analysts’ subject-matter expertise is invaluable in\nimproving data quality, and they frequently collaborate with data engineers in this\ncapacity.\nMachine learning engineers and AI researchers.    Machine learning engineers (ML engi‐\nneers)  overlap with data engineers and data scientists. ML engineers develop\nadvanced ML techniques, train models, and design and maintain the infrastructure\nrunning ML processes in a scaled production environment. ML engineers often have\nadvanced working knowledge of ML and deep learning techniques and frameworks\nsuch as PyTorch or TensorFlow.\nML engineers also understand the hardware, services, and systems required to run\nthese frameworks, both for model training and model deployment at a production\nscale. It’s common for ML flows to run in a cloud environment where ML engineers\ncan spin up and scale infrastructure resources on demand or rely on managed\nservices.\nAs we’ve mentioned, the boundaries between ML engineering, data engineering, and\ndata science are blurry. Data engineers may have some operational responsibilities\nover ML systems, and data scientists may work closely with ML engineering in\ndesigning advanced ML processes.\nData Engineers Inside an Organization | 27",16976
16-Data Engineers and Business Leadership.pdf,16-Data Engineers and Business Leadership,"The world of ML engineering is snowballing and parallels a lot of the same develop‐\nments occurring in data engineering. Whereas several years ago, the attention of ML\nwas focused on how to build models, ML engineering now increasingly emphasizes\nincorporating best practices of machine learning operations (MLOps) and other\nmature practices previously adopted in software engineering and DevOps.\nAI researchers work on new, advanced ML techniques. AI researchers may\nwork inside large technology companies, specialized intellectual property startups\n(OpenAI, DeepMind), or academic institutions. Some practitioners are dedicated\nto part-time research in conjunction with ML engineering responsibilities inside a\ncompany. Those working inside specialized ML labs are often 100% dedicated to\nresearch. Research problems may target immediate practical applications or more\nabstract demonstrations of AI. DALL-E, Gato AI, AlphaGo, and GPT-3/GPT-4 are\ngreat examples of ML research projects. Given the pace of advancements in ML,\nthese examples will very likely be quaint in a few years’ time. We’ve provided some\nreferences in “ Additional Resources” on page 32 .\nAI researchers in well-funded organizations are highly specialized and operate with\nsupporting teams of engineers to facilitate their work. ML engineers in academia\nusually have fewer resources but rely on teams of graduate students, postdocs, and\nuniversity staff to provide engineering support. ML engineers who are partially dedi‐\ncated to research often rely on the same support teams for research and production.\nData Engineers and Business Leadership\nWe’ve  discussed technical roles with which a data engineer interacts. But data engi‐\nneers also operate more broadly as organizational connectors, often in a nontechnical\ncapacity. Businesses have come to rely increasingly on data as a core part of many\nproducts or a product in itself. Data engineers now participate in strategic planning\nand lead key initiatives that extend beyond the boundaries of IT. Data engineers\noften support data architects by acting as the glue between the business and data\nscience/analytics.\nData in the C-suite\nC-level executives are increasingly involved in data and analytics, as these are recog‐\nnized as significant assets for modern businesses. For example, CEOs now concern\nthemselves with initiatives that were once the exclusive province of IT, such as cloud\nmigrations or deployment of a new customer data platform.\nChief executive officer .    Chief executive officers (CEOs) at nontech companies gener‐\nally don’t concern themselves with the nitty-gritty of data frameworks and software.\nInstead, they define a vision in collaboration with technical C-suite roles and com‐\npany data leadership. Data engineers provide a window into what’s possible with\n28 | Chapter 1: Data Engineering Described\ndata. Data engineers and their managers maintain a map of what data is available\nto the organization—both internally and from third parties—in what time frame.\nThey are also tasked to study primary data architectural changes in collaboration\nwith other engineering roles. For example, data engineers are often heavily involved\nin cloud migrations, migrations to new data systems, or deployment of streaming\ntechnologies.\nChief information officer .    A chief information officer (CIO) is the senior C-suite\nexecutive responsible for information technology within an organization; it is an\ninternal-facing role. A CIO must possess deep knowledge of information technology\nand business processes—either alone is insufficient. CIOs direct the information\ntechnology organization, setting ongoing policies while also defining and executing\nsignificant initiatives under the direction of the CEO.\nCIOs often collaborate with data engineering leadership in organizations with a well-\ndeveloped data culture. If an organization is not very high in its data maturity, a CIO\nwill typically help shape its data culture. CIOs will work with engineers and architects\nto map out major initiatives and make strategic decisions on adopting major architec‐\ntural elements, such as enterprise resource planning (ERP) and customer relationship\nmanagement (CRM) systems, cloud migrations, data systems, and internal-facing IT.\nChief technology officer .    A chief technology officer (CTO) is similar to a CIO but faces\noutward. A CTO owns the key technological strategy and architectures for external-\nfacing applications, such as mobile, web apps, and IoT—all critical data sources for\ndata engineers. The CTO is likely a skilled technologist and has a good sense of\nsoftware engineering fundamentals and system architecture. In some organizations\nwithout a CIO, the CTO or sometimes the chief operating officer (COO) plays the\nrole of CIO. Data engineers often report directly or indirectly through a CTO.\nChief data officer .    The chief data officer (CDO) was created in 2002 at Capital One\nto recognize the growing importance of data as a business asset. The CDO is respon‐\nsible for a company’s data assets and strategy. CDOs are focused on data’s business\nutility but should have a strong technical grounding. CDOs oversee data products,\nstrategy, initiatives, and core functions such as master data management and privacy.\nOccasionally, CDOs manage business analytics and data engineering.\nChief analytics officer .    The chief analytics officer (CAO) is a variant of the CDO\nrole. Where both roles exist, the CDO focuses on the technology and organization\nrequired to deliver data. The CAO is responsible for analytics, strategy, and decision\nmaking for the business. A CAO may oversee data science and ML, though this\nlargely depends on whether the company has a CDO or CTO role.\nData Engineers Inside an Organization | 29\nChief algorithms officer .    A chief algorithms officer (CAO-2) is a recent innovation\nin the C-suite, a highly technical role focused specifically on data science and ML.\nCAO-2s typically have experience as individual contributors and team leads in data\nscience or ML projects. Frequently, they have a background in ML research and a\nrelated advanced degree.\nCAO-2s are expected to be conversant in current ML research and have deep tech‐\nnical knowledge of their company’s ML initiatives. In addition to creating business\ninitiatives, they provide technical leadership, set research and development agendas,\nand build research teams.\nData engineers and project managers\nData engineers often work on significant initiatives, potentially spanning many years.\nAs we write this book, many data engineers are working on cloud migrations,\nmigrating pipelines and warehouses to the next generation of data tools. Other data\nengineers are starting greenfield projects, assembling new data architectures from\nscratch by selecting from an astonishing number of best-of-breed architecture and\ntooling options.\nThese large initiatives often benefit from project management  (in contrast to product\nmanagement, discussed next). Whereas data engineers function in an infrastructure\nand service delivery capacity, project managers direct traffic and serve as gatekeepers.\nMost project managers operate according to some variation of Agile and Scrum, with\nWaterfall still appearing occasionally. Business never sleeps, and business stakehold‐\ners often have a significant backlog of things they want to address and new initiatives\nthey want to launch. Project managers must filter a long list of requests and prioritize\ncritical deliverables to keep projects on track and better serve the company.\nData engineers interact with project managers, often planning sprints for projects\nand ensuing standups related to the sprint. Feedback goes both ways, with data\nengineers informing project managers and other stakeholders about progress and\nblockers, and project managers balancing the cadence of technology teams against\nthe ever-changing needs of the business.\nData engineers and product managers\nProduct managers oversee product development, often owning product lines. In the\ncontext of data engineers, these products are called data products . Data products\nare either built from the ground up or are incremental improvements upon existing\nproducts. Data engineers interact more frequently with product managers  as the\ncorporate world has adopted a data-centric focus . Like project managers, product\nmanagers balance the activity of technology teams against the needs of the customer\nand business.\n30 | Chapter 1: Data Engineering Described",8619
17-Chapter 2. The Data Engineering Lifecycle.pdf,17-Chapter 2. The Data Engineering Lifecycle,"Data engineers and other management roles\nData engineers interact with various managers beyond project and product managers.\nHowever, these interactions usually follow either the services or cross-functional\nmodels. Data engineers either serve a variety of incoming requests as a centralized\nteam or work as a resource assigned to a particular manager, project, or product.\nFor more information on data teams and how to structure them, we recommend\nJohn Thompson’s Building Analytics Teams  (Packt) and Jesse Anderson’s Data Teams\n(Apress). Both books provide strong frameworks and perspectives on the roles of\nexecutives with data, who to hire, and how to construct the most effective data team\nfor your company.\nCompanies don’t hire engineers simply to hack on code in isola‐\ntion. To be worthy of their title, engineers should develop a deep\nunderstanding of the problems they’re tasked with solving, the\ntechnology tools at their disposal, and the people they work with\nand serve.\nConclusion\nThis chapter provided you with a brief overview of the data engineering landscape,\nincluding the following:\n•Defining data engineering and describing what data engineers do•\n•Describing the types of data maturity in a company•\n•Type A and type B data engineers•\n•Whom data engineers work with•\nWe hope that this first chapter has whetted your appetite, whether you are a soft‐\nware development practitioner, data scientist, ML engineer, business stakeholder,\nentrepreneur, or venture capitalist. Of course, a great deal still remains to elucidate\nin subsequent chapters. Chapter 2  covers the data engineering lifecycle, followed\nby architecture in Chapter 3 . The following chapters get into the nitty-gritty of\ntechnology decisions for each part of the lifecycle. The entire data field is in flux, and\nas much as possible, each chapter focuses on the immutables —perspectives that will\nbe valid for many years amid relentless change.\nConclusion | 31\nAdditional Resources\n•“The AI Hierarchy of Needs”  by Monica Rogati •\n•The AlphaGo research web page•\n•“Big Data Will Be Dead in Five Y ears”  by Lewis Gavin •\n•Building Analytics Teams  by John K. Thompson (Packt) •\n•Chapter 1 of What Is Data Engineering?  by Lewis Gavin (O’Reilly) •\n•“Data as a Product vs. Data as a Service”  by Justin Gage •\n•“Data Engineering: A Quick and Simple Definition”  by James Furbush (O’Reilly) •\n•Data Teams  by Jesse Anderson (Apress) •\n•“Doing Data Science at Twitter”  by Robert Chang •\n•“The Downfall of the Data Engineer”  by Maxime Beauchemin •\n•“The Future of Data Engineering Is the Convergence of Disciplines”  by Liam •\nHausmann\n•“How CEOs Can Lead a Data-Driven Culture”  by Thomas H. Davenport and •\nNitin Mittal\n•“How Creating a Data-Driven Culture Can Drive Success”  by Frederik Bussler •\n•The Information Management Body of Knowledge website•\n•“Information Management Body of Knowledge” Wikipedia page•\n•“Information Management” Wikipedia page•\n•“On Complexity in Big Data”  by Jesse Anderson (O’Reilly) •\n•“OpenAI’s New Language Generator GPT-3 Is Shockingly Good—and Com‐•\npletely Mindless”  by Will Douglas Heaven\n•“The Rise of the Data Engineer”  by Maxime Beauchemin •\n•“ A Short History of Big Data”  by Mark van Rijmenam •\n•“Skills of the Data Architect”  by Bob Lambert •\n•“The Three Levels of Data Analysis: A Framework for Assessing Data Organiza‐•\ntion Maturity”  by Emilie Schario\n•“What Is a Data Architect? IT’s Data Framework Visionary”  by Thor Olavsrud •\n•“Which Profession Is More Complex to Become, a Data Engineer or a Data•\nScientist?” thread on Quora\n•“Why CEOs Must Lead Big Data Initiatives”  by John Weathington •\n32 | Chapter 1: Data Engineering Described",3740
18-Generation Source Systems.pdf,18-Generation Source Systems,"CHAPTER 2\nThe Data Engineering Lifecycle\nThe major goal of this book is to encourage you to move beyond viewing data engi‐\nneering as a specific collection of data technologies. The data landscape is undergoing\nan explosion of new data technologies and practices, with ever-increasing levels of\nabstraction and ease of use. Because of increased technical abstraction, data engineers\nwill increasingly become data lifecycle engineers , thinking and operating in terms of\nthe principles  of data lifecycle management.\nIn this chapter, you’ll learn about the data engineering lifecycle , which is the central\ntheme of this book . The data engineering lifecycle is our framework describing “cra‐\ndle to grave” data engineering . Y ou will also learn about the undercurrents of the data\nengineering lifecycle, which are key foundations that support all data engineering\nefforts.\nWhat Is the Data Engineering Lifecycle?\nThe data engineering lifecycle comprises stages that turn raw data ingredients into a\nuseful end product, ready for consumption by analysts, data scientists, ML engineers,\nand others. This chapter introduces the major stages of the data engineering lifecycle,\nfocusing on each stage’s core concepts and saving details for later chapters.\nWe divide the data engineering lifecycle into five stages ( Figure 2-1 , top):\n•Generation•\n•Storage•\n•Ingestion•\n•Transformation•\n•Serving data•\n33\nFigure 2-1. Components and undercurrents of the data engineering lifecycle\nWe begin the data engineering lifecycle by getting data from source systems and\nstoring it. Next, we transform the data and then proceed to our central goal, serving\ndata to analysts, data scientists, ML engineers, and others. In reality, storage occurs\nthroughout the lifecycle as data flows from beginning to end—hence, the diagram\nshows the storage “stage” as a foundation that underpins other stages.\nIn general, the middle stages—storage, ingestion, transformation—can get a bit jum‐\nbled. And that’s OK. Although we split out the distinct parts of the data engineering\nlifecycle, it’s not always a neat, continuous flow. Various stages of the lifecycle may\nrepeat themselves, occur out of order, overlap, or weave together in interesting and\nunexpected ways.\nActing as a bedrock are undercurrents  (Figure 2-1 , bottom) that cut across multiple\nstages of the data engineering lifecycle: security, data management, DataOps, data\narchitecture, orchestration, and software engineering. No part of the data engineering\nlifecycle can adequately function without these undercurrents.\nThe Data Lifecycle Versus the Data Engineering Lifecycle\nY ou may be wondering about the difference between the overall data lifecycle and\nthe data engineering lifecycle. There’s a subtle distinction between the two. The data\nengineering lifecycle is a subset of the whole data lifecycle ( Figure 2-2 ). Whereas the\nfull data lifecycle encompasses data across its entire lifespan, the data engineering\nlifecycle focuses on the stages a data engineer controls.\n34 | Chapter 2: The Data Engineering Lifecycle\nFigure 2-2. The data engineering lifecycle is a subset of the full data lifecycle\nGeneration: Source Systems\nA source system  is the origin of the data used in the data engineering lifecycle. For\nexample, a source system could be an IoT device, an application message queue, or\na transactional database. A data engineer consumes data from a source system but\ndoesn’t typically own or control the source system itself. The data engineer needs to\nhave a working understanding of the way source systems work, the way they generate\ndata, the frequency and velocity of the data, and the variety of data they generate.\nEngineers  also need to keep an open line of communication with source system\nowners on changes that could break pipelines and analytics. Application code might\nchange the structure of data in a field, or the application team might even choose to\nmigrate the backend to an entirely new database technology.\nA major challenge in data engineering is the dizzying array of data source systems\nengineers must work with and understand. As an illustration, let’s look at two com‐\nmon source systems, one very traditional (an application database) and the other a\nmore recent example (IoT swarms).\nFigure 2-3  illustrates  a traditional source system with several application servers\nsupported by a database. This source system pattern became popular in the 1980s\nwith the explosive success of relational database management systems (RDBMSs).\nThe application + database pattern remains popular today with various modern evo‐\nlutions of software development practices. For example, applications often consist of\nmany small service/database pairs with microservices rather than a single monolith.\nFigure 2-3. Source system example: an application database\nWhat Is the Data Engineering Lifecycle? | 35\nLet’s look at another example of a source system. Figure 2-4  illustrates an IoT swarm:\na fleet of devices (circles) sends data messages (rectangles) to a central collection\nsystem. This IoT source system is increasingly common as IoT devices such as\nsensors, smart devices, and much more increase in the wild.\nFigure 2-4. Source system example: an IoT swarm and message queue\nEvaluating source systems: Key engineering considerations\nThere are many things to consider when assessing source systems, including how the\nsystem handles ingestion, state, and data generation. The following is a starting set of\nevaluation questions of source systems that data engineers must consider:\n•What are the essential characteristics of the data source? Is it an application? A•\nswarm of IoT devices?\n•How is data persisted in the source system? Is data persisted long term, or is it•\ntemporary and quickly deleted?\n•At what rate is data generated? How many events per second? How many giga‐•\nbytes per hour?\n•What level of consistency can data engineers expect from the output data? If•\nyou’re running data-quality checks against the output data, how often do data\ninconsistencies occur—nulls where they aren’t expected, lousy formatting, etc.?\n•How often do errors occur?•\n•Will the data contain duplicates?•\n•Will some data values arrive late, possibly much later than other messages pro‐•\nduced simultaneously?\n•What is the schema of the ingested data? Will data engineers need to join across•\nseveral tables or even several systems to get a complete picture of the data?\n36 | Chapter 2: The Data Engineering Lifecycle\n•If schema changes (say, a new column is added), how is this dealt with and•\ncommunicated to downstream stakeholders?\n•How frequently should data be pulled from the source system?•\n•For stateful systems (e.g., a database tracking customer account information), is•\ndata provided as periodic snapshots or update events from  change data capture\n(CDC)? What’s the logic for how changes are performed, and how are these\ntracked in the source database?\n•Who/what is the data provider that will transmit the data for downstream•\nconsumption?\n•Will reading from a data source impact its performance?•\n•Does the source system have upstream data dependencies? What are the charac‐•\nteristics of these upstream systems?\n•Are data-quality checks in place to check for late or missing data?•\nSources produce data consumed by downstream systems, including human-\ngenerated spreadsheets, IoT sensors, and web and mobile applications. Each source\nhas its unique volume and cadence of data generation. A data engineer should know\nhow the source generates data, including relevant quirks or nuances. Data engineers\nalso need to understand the limits of the source systems they interact with. For\nexample, will analytical queries against a source application database cause resource\ncontention and performance issues?\nOne of the most challenging nuances of source data is the schema. The schema\ndefines the hierarchical organization of data. Logically, we can think of data at the\nlevel of a whole source system, drilling down into individual tables, all the way to\nthe structure of respective fields. The schema of data shipped from source systems is\nhandled in various ways. Two popular options are schemaless and fixed schema.\nSchemaless  doesn’t mean the absence of schema. Rather, it means that the application\ndefines the schema as data is written, whether to a message queue, a flat file, a blob, or\na document database such as MongoDB. A more traditional model built on relational\ndatabase storage uses a fixed  schema  enforced in the database, to which application\nwrites must conform.\nEither of these models presents challenges for data engineers. Schemas change over\ntime; in fact, schema evolution is encouraged in the Agile approach to software\ndevelopment. A key part of the data engineer’s job is taking raw data input in the\nsource system schema and transforming this into valuable output for analytics. This\njob becomes more challenging as the source schema evolves.\nWe dive into source systems in greater detail in Chapter 5 ; we also cover schemas and\ndata modeling in Chapters 6 and 8, respectively.\nWhat Is the Data Engineering Lifecycle? | 37",9263
19-Ingestion.pdf,19-Ingestion,"Storage\nY ou need a place to store data. Choosing a storage solution is key to success in\nthe rest of the data lifecycle, and it’s also one of the most complicated stages of the\ndata lifecycle for a variety of reasons. First, data architectures in the cloud often lever‐\nage several  storage solutions. Second, few data storage solutions function purely as\nstorage, with many supporting complex transformation queries; even object storage\nsolutions may support powerful query capabilities—e.g., Amazon S3 Select . Third,\nwhile storage is a stage of the data engineering lifecycle, it frequently touches on other\nstages, such as ingestion, transformation, and serving.\nStorage runs across the entire data engineering lifecycle, often occurring in multiple\nplaces in a data pipeline, with storage systems crossing over with source systems,\ningestion, transformation, and serving. In many ways, the way data is stored impacts\nhow it is used in all of the stages of the data engineering lifecycle. For example, cloud\ndata warehouses can store data, process data in pipelines, and serve it to analysts.\nStreaming frameworks such as Apache Kafka and Pulsar can function simultaneously\nas ingestion, storage, and query systems for messages, with object storage being a\nstandard layer for data transmission.\nEvaluating storage systems: Key engineering considerations\nHere are a few key engineering questions to ask when choosing a storage system for a\ndata warehouse, data lakehouse, database, or object storage:\n•Is this storage solution compatible with the architecture’s required write and read•\nspeeds?\n•Will storage create a bottleneck for downstream processes?•\n•Do you understand how this storage technology works? Are you utilizing the•\nstorage system optimally or committing unnatural acts? For instance, are you\napplying a high rate of random access updates in an object storage system? (This\nis an antipattern with significant performance overhead.)\n•Will this storage system handle anticipated future scale? Y ou should consider all•\ncapacity limits on the storage system: total available storage, read operation rate,\nwrite volume, etc.\n•Will downstream users and processes be able to retrieve data in the required•\nservice-level agreement (SLA)?\n•Are you capturing metadata about schema evolution, data flows, data lineage,•\nand so forth? Metadata has a significant impact on the utility of data. Meta‐\ndata represents an investment in the future, dramatically enhancing discoverabil‐\nity and institutional knowledge to streamline future projects and architecture\nchanges.\n38 | Chapter 2: The Data Engineering Lifecycle\n•Is this a pure storage solution (object storage), or does it support complex query•\npatterns (i.e., a cloud data warehouse)?\n•Is the storage system schema-agnostic (object storage)? Flexible schema (Cassan‐•\ndra)? Enforced schema (a cloud data warehouse)?\n•How are you tracking master data, golden records data quality, and data lineage•\nfor data governance? (We have more to say on these in “Data Management” on\npage 50 .)\n•How are you handling regulatory compliance and data sovereignty? For example,•\ncan you store your data in certain geographical locations but not others?\nUnderstanding data access frequency\nNot all data is accessed in the same way. Retrieval patterns will greatly vary based on\nthe data being stored and queried. This brings up the notion of the “temperatures” of\ndata. Data access frequency will determine the temperature of your data.\nData that is most frequently accessed is called hot data . Hot data is commonly\nretrieved many times per day, perhaps even several times per second—for example,\nin systems that serve user requests. This data should be stored for fast retrieval,\nwhere “fast” is relative to the use case. Lukewarm data  might be accessed every so\noften—say, every week or month.\nCold data  is seldom queried and is appropriate for storing in an archival system. Cold\ndata is often retained for compliance purposes or in case of a catastrophic failure in\nanother system. In the “old days, ” cold data would be stored on tapes and shipped\nto remote archival facilities. In cloud environments, vendors offer specialized storage\ntiers with very cheap monthly storage costs but high prices for data retrieval.\nSelecting a storage system\nWhat  type of storage solution should you use? This depends on your use cases,\ndata volumes, frequency of ingestion, format, and size of the data being ingested—\nessentially, the key considerations listed in the preceding bulleted questions. There\nis no one-size-fits-all universal storage recommendation. Every storage technology\nhas its trade-offs. Countless varieties of storage technologies exist, and it’s easy to be\noverwhelmed when deciding the best option for your data architecture.\nChapter 6  covers storage best practices and approaches in greater detail, as well as the\ncrossover between storage and other lifecycle stages.\nIngestion\nAfter  you understand the data source, the characteristics of the source system you’re\nusing, and how data is stored, you need to gather the data. The next stage of the data\nengineering lifecycle is data ingestion from source systems.\nWhat Is the Data Engineering Lifecycle? | 39\nIn our experience, source systems and ingestion represent the most significant bot‐\ntlenecks of the data engineering lifecycle. The source systems are normally outside\nyour direct control and might randomly become unresponsive or provide data of\npoor quality. Or, your data ingestion service might mysteriously stop working for\nmany reasons. As a result, data flow stops or delivers insufficient data for storage,\nprocessing, and serving.\nUnreliable source and ingestion systems have a ripple effect across the data engineer‐\ning lifecycle. But you’re in good shape, assuming you’ve answered the big questions\nabout source systems.\nKey engineering considerations for the ingestion phase\nWhen  preparing to architect or build a system, here are some primary questions\nabout the ingestion stage:\n•What are the use cases for the data I’m ingesting? Can I reuse this data rather•\nthan create multiple versions of the same dataset?\n•Are the systems generating and ingesting this data reliably, and is the data•\navailable when I need it?\n•What is the data destination after ingestion?•\n•How frequently will I need to access the data?•\n•In what volume will the data typically arrive?•\n•What format is the data in? Can my downstream storage and transformation•\nsystems handle this format?\n•Is the source data in good shape for immediate downstream use? If so, for how•\nlong, and what may cause it to be unusable?\n•If the data is from a streaming source, does it need to be transformed before•\nreaching its destination? Would an in-flight transformation be appropriate,\nwhere the data is transformed within the stream itself?\nThese are just a sample of the factors you’ll need to think about with ingestion, and\nwe cover those questions and more in Chapter 7 . Before we leave, let’s briefly turn\nour attention to two major data ingestion concepts: batch versus streaming and push\nversus pull.\nBatch versus streaming\nVirtually all data we deal with is inherently  streaming . Data is nearly always produced\nand updated continually at its source. Batch ingestion  is simply a specialized and\nconvenient way of processing this stream in large chunks—for example, handling a\nfull day’s worth of data in a single batch.\n40 | Chapter 2: The Data Engineering Lifecycle\nStreaming ingestion allows us to provide data to downstream systems—whether\nother applications, databases, or analytics systems—in a continuous,  real-time fash‐\nion. Here, real-time  (or near real-time ) means that the data is available to a down‐\nstream system a short time after it is produced (e.g., less than one second later). The\nlatency required to qualify as real-time varies by domain and requirements.\nBatch data is ingested either on a predetermined time interval or as data reaches a\npreset size threshold. Batch ingestion is a one-way door: once data is broken into\nbatches, the latency for downstream consumers is inherently constrained. Because of\nlimitations of legacy systems, batch was for a long time the default way to ingest data.\nBatch processing remains an extremely popular way to ingest data for downstream\nconsumption, particularly in analytics and ML.\nHowever, the separation of storage and compute in many systems and the ubiquity\nof event-streaming and processing platforms make the continuous processing of data\nstreams much more accessible and increasingly popular. The choice largely depends\non the use case and expectations for data timeliness.\nKey considerations for batch versus stream ingestion\nShould you go streaming-first? Despite the attractiveness of a streaming-first\napproach, there are many trade-offs to understand and think about. The following\nare some questions to ask yourself when determining whether streaming ingestion is\nan appropriate choice over batch ingestion:\n•If I ingest the data in real time, can downstream storage systems handle the rate•\nof data flow?\n•Do I need millisecond real-time data ingestion? Or would a micro-batch•\napproach work, accumulating and ingesting data, say, every minute?\n•What are my use cases for streaming ingestion? What specific benefits do I•\nrealize by implementing streaming? If I get data in real time, what actions can I\ntake on that data that would be an improvement upon batch?\n•Will my streaming-first approach cost more in terms of time, money, mainte‐•\nnance, downtime, and opportunity cost than simply doing batch?\n•Are my streaming pipeline and system reliable and redundant if infrastructure•\nfails?\n•What tools are most appropriate for the use case? Should I use a managed service•\n(Amazon Kinesis, Google Cloud Pub/Sub, Google Cloud Dataflow) or stand up\nmy own instances of Kafka, Flink, Spark, Pulsar, etc.? If I do the latter, who will\nmanage it? What are the costs and trade-offs?\n•If I’m deploying an ML model, what benefits do I have with online predictions•\nand possibly continuous training?\nWhat Is the Data Engineering Lifecycle? | 41\n•Am I getting data from a live production instance? If so, what’s the impact of my•\ningestion process on this source system?\nAs you can see, streaming-first might seem like a good idea, but it’s not always\nstraightforward; extra costs and complexities inherently occur. Many great ingestion\nframeworks do handle both batch and micro-batch ingestion styles. We think batch\nis an excellent approach for many common use cases, such as model training and\nweekly reporting. Adopt true real-time streaming only after identifying a business use\ncase that justifies the trade-offs against using batch.\nPush versus pull\nIn the push  model of data ingestion, a source system writes data out to a target,\nwhether a database, object store, or filesystem. In the pull model, data is retrieved\nfrom the source system. The line between the push and pull paradigms can be quite\nblurry; data is often pushed and pulled as it works its way through the various stages\nof a data pipeline.\nConsider, for example, the  extract, transform, load (ETL) process, commonly used in\nbatch-oriented ingestion workflows. ETL ’s extract  (E) part clarifies that we’re dealing\nwith a pull ingestion model. In traditional ETL, the ingestion system queries a current\nsource table snapshot on a fixed schedule. Y ou’ll learn more about ETL and extract,\nload, transform (ELT) throughout this book.\nIn another example, consider continuous CDC, which is achieved in a few ways.\nOne common method triggers a message every time a row is changed in the source\ndatabase. This message is pushed  to a queue, where the ingestion system picks it up.\nAnother common CDC method uses binary logs, which record every commit to the\ndatabase. The database pushes  to its logs. The ingestion system reads the logs but\ndoesn’t directly interact with the database otherwise. This adds little to no additional\nload to the source database. Some versions of batch CDC use the pull pattern. For\nexample, in timestamp-based CDC, an ingestion system queries the source database\nand pulls the rows that have changed since the previous update.\nWith streaming ingestion, data bypasses a backend database and is pushed directly\nto an endpoint, typically with data buffered by an event-streaming platform. This\npattern is useful with fleets of IoT sensors emitting sensor data. Rather than relying\non a database to maintain the current state, we simply think of each recorded reading\nas an event. This pattern is also growing in popularity in software applications as\nit simplifies real-time processing, allows app developers to tailor their messages for\ndownstream analytics, and greatly simplifies the lives of data engineers.\nWe discuss ingestion best practices and techniques in depth in Chapter 7 . Next, let’s\nturn to the transformation stage of the data engineering lifecycle.\n42 | Chapter 2: The Data Engineering Lifecycle",13181
20-Serving Data.pdf,20-Serving Data,"Transformation\nAfter  you’ve ingested and stored data, you need to do something with it. The next\nstage of the data engineering lifecycle is transformation , meaning data needs to be\nchanged from its original form into something useful for downstream use cases.\nWithout proper transformations, data will sit inert, and not be in a useful form for\nreports, analysis, or ML. Typically, the transformation stage is where data begins to\ncreate value for downstream user consumption.\nImmediately after ingestion, basic transformations map data into correct types\n(changing ingested string data into numeric and date types, for example), putting\nrecords into standard formats, and removing bad ones. Later stages of transformation\nmay transform the data schema and apply normalization. Downstream, we can apply\nlarge-scale aggregation for reporting or featurize data for ML processes.\nKey considerations for the transformation phase\nWhen considering data transformations within the data engineering lifecycle, it helps\nto consider the following:\n•What’s the cost and return on investment (ROI) of the transformation? What is•\nthe associated business value?\n•Is the transformation as simple and self-isolated as possible?•\n•What business rules do the transformations support?•\nY ou can transform data in batch or while streaming in flight. As mentioned in\n“Ingestion”  on page 39, virtually all data starts life as a continuous stream; batch\nis just a specialized way of processing a data stream. Batch transformations are over‐\nwhelmingly popular, but given the growing popularity of stream-processing solutions\nand the general increase in the amount of streaming data, we expect the popularity\nof streaming transformations to continue growing, perhaps entirely replacing batch\nprocessing in certain domains soon.\nLogically, we treat transformation as a standalone area of the data engineering life‐\ncycle, but the realities of the lifecycle can be much more complicated in practice.\nTransformation is often entangled in other phases of the lifecycle. Typically, data is\ntransformed in source systems or in flight during ingestion. For example, a source\nsystem may add an event timestamp to a record before forwarding it to an ingestion\nprocess. Or a record within a streaming pipeline may be “enriched” with additional\nfields and calculations before it’s sent to a data warehouse. Transformations are\nubiquitous in various parts of the lifecycle. Data preparation, data wrangling, and\ncleaning—these transformative tasks add value for end consumers of data.\nBusiness logic is a major driver of data transformation, often in data modeling. Data\ntranslates business logic into reusable elements (e.g., a sale means “somebody bought\nWhat Is the Data Engineering Lifecycle? | 43\n12 picture frames from me for $30 each, or $360 in total”). In this case, somebody\nbought 12 picture frames for $30 each. Data modeling is critical for obtaining a clear\nand current picture of business processes. A simple view of raw retail transactions\nmight not be useful without adding the logic of accounting rules so that the CFO\nhas a clear picture of financial health. Ensure a standard approach for implementing\nbusiness logic across your transformations.\nData featurization for ML is another data transformation process. Featurization\nintends to extract and enhance data features useful for training ML models. Featuri‐\nzation can be a dark art, combining domain expertise (to identify which features\nmight be important for prediction) with extensive experience in data science. For this\nbook, the main point is that once data scientists determine how to featurize data,\nfeaturization processes can be automated by data engineers in the transformation\nstage of a data pipeline.\nTransformation is a profound subject, and we cannot do it justice in this brief intro‐\nduction. Chapter 8  delves into queries, data modeling, and various transformation\npractices and nuances.\nServing Data\nY ou’ve  reached the last stage of the data engineering lifecycle. Now that the data has\nbeen ingested, stored, and transformed into coherent and useful structures, it’s time\nto get value from your data. “Getting value” from data means different things to\ndifferent users.\nData has value  when it’s used for practical purposes. Data that is not consumed or\nqueried is simply inert. Data vanity projects are a major risk for companies. Many\ncompanies pursued vanity projects in the big data era, gathering massive datasets in\ndata lakes that were never consumed in any useful way. The cloud era is triggering\na new wave of vanity projects built on the latest data warehouses, object storage\nsystems, and streaming technologies. Data projects must be intentional across the\nlifecycle. What is the ultimate business purpose of the data so carefully collected,\ncleaned, and stored?\nData serving is perhaps the most exciting part of the data engineering lifecycle. This\nis where the magic happens. This is where ML engineers can apply the most advanced\ntechniques. Let’s look at some of the popular uses of data: analytics, ML, and reverse\nETL.\nAnalytics\nAnalytics  is the core of most data endeavors. Once your data is stored and trans‐\nformed, you’re ready to generate reports or dashboards and do ad hoc analysis on the\ndata. Whereas the bulk of analytics used to encompass BI, it now includes other facets\n44 | Chapter 2: The Data Engineering Lifecycle\nsuch as operational analytics and embedded analytics ( Figure 2-5 ). Let’s briefly touch\non these variations of analytics.\nFigure 2-5. Types of analytics\nBusiness intelligence.    BI marshals collected data to describe a business’s past and cur‐\nrent state. BI requires using business logic to process raw data. Note that data serving\nfor analytics is yet another area where the stages of the data engineering lifecycle can\nget tangled. As we mentioned earlier, business logic is often applied to data in the\ntransformation stage of the data engineering lifecycle, but a logic-on-read approach\nhas become increasingly popular. Data is stored in a clean but fairly raw form,\nwith minimal postprocessing business logic. A BI system maintains a repository of\nbusiness logic and definitions. This business logic is used to query the data warehouse\nso that reports and dashboards align with business definitions.\nAs a company grows its data maturity, it will move from ad hoc data analysis to\nself-service analytics, allowing democratized data access to business users without\nneeding IT to intervene. The capability to do self-service analytics assumes that\ndata is good enough that people across the organization can simply access it them‐\nselves, slice and dice it however they choose, and get immediate insights. Although\nself-service analytics is simple in theory, it’s tough to pull off in practice. The main\nreason is that poor data quality, organizational silos, and a lack of adequate data skills\noften get in the way of allowing widespread use of analytics.\nOperational analytics.    Operational analytics  focuses on the fine-grained details of\noperations, promoting actions that a user of the reports can act upon immediately.\nOperational analytics could be a live view of inventory or real-time dashboarding of\nwebsite or application health. In this case, data is consumed in real time, either\ndirectly from a source system or from a streaming data pipeline. The types of\ninsights in operational analytics differ from traditional BI since operational analytics\nis focused on the present and doesn’t necessarily concern historical trends.\nEmbedded analytics.    Y ou may wonder why we’ve broken out embedded analytics\n(customer-facing analytics) separately from BI. In practice, analytics provided to\ncustomers on a SaaS platform come with a separate set of requirements and\nWhat Is the Data Engineering Lifecycle? | 45\ncomplications.  Internal BI faces a limited audience and generally presents a limited\nnumber of unified views. Access controls are critical but not particularly complicated.\nAccess is managed using a handful of roles and access tiers.\nWith embedded analytics, the request rate for reports, and the corresponding bur‐\nden on analytics systems, goes up dramatically; access control is significantly more\ncomplicated and critical. Businesses may be serving separate analytics and data to\nthousands or more customers. Each customer must see their data and only their\ndata. An internal data-access error at a company would likely lead to a procedural\nreview. A data leak between customers would be considered a massive breach of trust,\nleading to media attention and a significant loss of customers. Minimize your blast\nradius related to data leaks and security vulnerabilities. Apply tenant- or data-level\nsecurity within your storage and anywhere there’s a possibility of data leakage.\nMultitenancy\nMany  current storage and analytics systems support multitenancy in various ways.\nData engineers may choose to house data for many customers in common tables to\nallow a unified view for internal analytics and ML. This data is presented externally\nto individual customers through logical views with appropriately defined controls and\nfilters. It is incumbent on data engineers to understand the minutiae of multitenancy\nin the systems they deploy to ensure absolute data security and isolation.\nMachine learning\nThe emergence and success of ML is one of the most exciting technology revolutions.\nOnce organizations reach a high level of data maturity, they can begin to identify\nproblems amenable to ML and start organizing a practice around it.\nThe responsibilities of data engineers overlap significantly in analytics and ML, and\nthe boundaries between data engineering, ML engineering, and analytics engineering\ncan be fuzzy. For example, a data engineer may need to support Spark clusters that\nfacilitate analytics pipelines and ML model training. They may also need to provide\na system that orchestrates tasks across teams and support metadata and cataloging\nsystems that track data history and lineage. Setting these domains of responsibility\nand the relevant reporting structures is a critical organizational decision.\nThe feature store is a recently developed tool that combines data engineering and\nML engineering. Feature stores are designed to reduce the operational burden for\nML engineers by maintaining feature history and versions, supporting feature sharing\namong teams, and providing basic operational and orchestration capabilities, such as\nbackfilling. In practice, data engineers are part of the core support team for feature\nstores to support ML engineering.\n46 | Chapter 2: The Data Engineering Lifecycle\nShould a data engineer be familiar with ML? It certainly helps. Regardless of the\noperational boundary between data engineering, ML engineering, business analytics,\nand so forth, data engineers should maintain operational knowledge about their\nteams. A good data engineer is conversant in the fundamental ML techniques\nand related data-processing requirements, the use cases for models within their\ncompany, and the responsibilities of the organization’s various analytics teams. This\nhelps maintain efficient communication and facilitate collaboration. Ideally, data\nengineers will build tools in partnership with other teams that neither team can make\nindependently.\nThis book cannot possibly cover ML in depth. A growing ecosystem of books, videos,\narticles, and communities is available if you’re interested in learning more; we include\na few suggestions in “ Additional Resources” on page 69 .\nThe following are some considerations for the serving data phase specific to ML:\n•Is the data of sufficient quality to perform reliable feature engineering? Quality•\nrequirements and assessments are developed in close collaboration with teams\nconsuming the data.\n•Is the data discoverable? Can data scientists and ML engineers easily find valua‐•\nble data?\n•Where are the technical and organizational boundaries between data engineering•\nand ML engineering? This organizational question has significant architectural\nimplications.\n•Does the dataset properly represent ground truth? Is it unfairly biased?•\nWhile ML is exciting, our experience is that companies often prematurely dive into\nit. Before investing a ton of resources into ML, take the time to build a solid data\nfoundation. This means setting up the best systems and architecture across the data\nengineering and ML lifecycle. It’s generally best to develop competence in analytics\nbefore moving to ML. Many companies have dashed their ML dreams because they\nundertook initiatives without appropriate foundations.\nReverse ETL\nReverse ETL has long been a practical reality in data, viewed as an antipattern that\nwe didn’t like to talk about or dignify with a name. Reverse ETL  takes processed data\nfrom the output side of the data engineering lifecycle and feeds it back into source\nsystems, as shown in Figure 2-6 . In reality, this flow is beneficial and often necessary;\nreverse ETL allows us to take analytics, scored models, etc., and feed these back into\nproduction systems or SaaS platforms.\nWhat Is the Data Engineering Lifecycle? | 47",13346
21-Data Management.pdf,21-Data Management,"Figure 2-6. Reverse ETL\nMarketing analysts might calculate bids in Microsoft Excel by using the data in their\ndata warehouse, and then upload these bids to Google Ads. This process was often\nentirely manual and primitive.\nAs we’ve written this book, several vendors have embraced the concept of reverse\nETL and built products around it, such as Hightouch and Census. Reverse ETL\nremains nascent as a practice, but we suspect that it is here to stay.\nReverse ETL has become especially important as businesses rely increasingly on SaaS\nand external platforms. For example, companies may want to push specific metrics\nfrom their data warehouse to a customer data platform or CRM system. Advertising\nplatforms are another everyday use case, as in the Google Ads example. Expect to\nsee more activity in reverse ETL, with an overlap in both data engineering and ML\nengineering.\nThe jury is out on whether the term reverse ETL  will stick. And the practice may\nevolve. Some engineers claim that we can eliminate reverse ETL by handling data\ntransformations in an event stream and sending those events back to source systems\nas needed. Realizing widespread adoption of this pattern across businesses is another\nmatter. The gist is that transformed data will need to be returned to source systems\nin some manner, ideally with the correct lineage and business process associated with\nthe source system.\nMajor Undercurrents Across the Data\nEngineering Lifecycle\nData engineering is rapidly maturing. Whereas prior cycles of data engineering sim‐\nply focused on the technology layer, the continued abstraction and simplification\n48 | Chapter 2: The Data Engineering Lifecycle\nof tools and practices have shifted this focus. Data engineering now encompasses\nfar more than tools and technology. The field is now moving up the value chain,\nincorporating traditional enterprise practices such as data management and cost\noptimization and newer practices like DataOps.\nWe’ve termed these practices undercurrents —security, data management, DataOps,\ndata architecture, orchestration, and software engineering—that support every aspect\nof the data engineering lifecycle ( Figure 2-7 ). In this section, we give a brief overview\nof these undercurrents and their major components, which you’ll see in more detail\nthroughout the book.\nFigure 2-7. The major undercurrents of data engineering\nSecurity\nSecurity  must be top of mind for data engineers, and those who ignore it do so\nat their peril. That’s why security is the first undercurrent. Data engineers must\nunderstand both data and access security, exercising the principle of least privilege.\nThe principle of least privilege  means giving a user or system access to only the\nessential data and resources to perform an intended function. A common antipattern\nwe see with data engineers with little security experience is to give admin access to all\nusers. This is a catastrophe waiting to happen!\nGive users only the access they need to do their jobs today, nothing more. Don’t\noperate from a root shell when you’re just looking for visible files with standard user\naccess. When querying tables with a lesser role, don’t use the superuser role in a\ndatabase. Imposing the principle of least privilege on ourselves can prevent accidental\ndamage and keep you in a security-first mindset.\nPeople and organizational structure are always the biggest security vulnerabilities in\nany company. When we hear about major security breaches in the media, it often\nturns out that someone in the company ignored basic precautions, fell victim to a\nphishing attack, or otherwise acted irresponsibly. The first line of defense for data\nsecurity is to create a culture of security that permeates the organization. All individ‐\nuals who have access to data must understand their responsibility in protecting the\ncompany’s sensitive data and its customers.\nMajor Undercurrents Across the Data Engineering Lifecycle | 49\nData security is also about timing—providing data access to exactly the people and\nsystems that need to access it and only for the duration necessary to perform their\nwork . Data should be protected from unwanted visibility, both in flight and at rest, by\nusing encryption, tokenization, data masking, obfuscation, and simple, robust access\ncontrols.\nData engineers must be competent security administrators, as security falls in their\ndomain. A data engineer should understand security best practices for the cloud and\non prem. Knowledge of user and identity access management (IAM) roles, policies,\ngroups, network security, password policies, and encryption are good places to start.\nThroughout the book, we highlight areas where security should be top of mind in the\ndata engineering lifecycle. Y ou can also gain more detailed insights into security in\nChapter 10 .\nData Management\nY ou probably think that data management sounds very…corporate. “Old school” data\nmanagement practices make their way into data and ML engineering. What’s old\nis new again. Data management has been around for decades but didn’t get a lot\nof traction in data engineering until recently. Data tools are becoming simpler, and\nthere is less complexity for data engineers to manage. As a result, the data engineer\nmoves up the value chain toward the next rung of best practices. Data best practices\nonce reserved for huge companies—data governance, master data management, data-\nquality management, metadata management—are now filtering down to companies\nof all sizes and maturity levels. As we like to say, data engineering is becoming\n“enterprisey. ” This is ultimately a great thing!\nThe Data Management Association International (DAMA) Data Management Body of\nKnowledge  (DMBOK ), which we consider to be the definitive book for enterprise data\nmanagement, offers this definition:\nData management is the development, execution, and supervision of plans, policies,\nprograms, and practices that deliver, control, protect, and enhance the value of data\nand information assets throughout their lifecycle.\nThat’s a bit lengthy, so let’s look at how it ties to data engineering. Data engineers\nmanage the data lifecycle, and data management encompasses the set of best practices\nthat data engineers will use to accomplish this task, both technically and strategically.\nWithout a framework for managing data, data engineers are simply technicians\noperating in a vacuum. Data engineers need a broader perspective of data’s utility\nacross the organization, from the source systems to the C-suite, and everywhere in\nbetween.\nWhy is data management important? Data management demonstrates that data is\nvital to daily operations, just as businesses view financial resources, finished goods,\n50 | Chapter 2: The Data Engineering Lifecycle\n1Evren Eryurek et al., Data Governance: The Definitive  Guide  (Sebastopol, CA: O’Reilly, 2021), 1,\nhttps://oreil.ly/LFT4d .or real estate as assets. Data management practices form a cohesive framework that\neveryone can adopt to ensure that the organization gets value from data and handles\nit appropriately.\nData management has quite a few facets, including the following:\n•Data governance, including discoverability and accountability•\n•Data modeling and design•\n•Data lineage•\n•Storage and operations•\n•Data integration and interoperability•\n•Data lifecycle management•\n•Data systems for advanced analytics and ML•\n•Ethics and privacy•\nWhile this book is in no way an exhaustive resource on data management, let’s briefly\ncover some salient points from each area as they relate to data engineering.\nData governance\nAccording  to Data Governance: The Definitive  Guide,  “Data governance is, first and\nforemost, a data management function to ensure the quality, integrity, security, and\nusability of the data collected by an organization. ”1\nWe can expand on that definition and say that data governance engages people,\nprocesses, and technologies to maximize data value across an organization while\nprotecting data with appropriate security controls. Effective data governance is devel‐\noped with intention and supported by the organization. When data governance is\naccidental and haphazard, the side effects can range from untrusted data to security\nbreaches and everything in between. Being intentional about data governance will\nmaximize the organization’s data capabilities and the value generated from data.\nIt will also (hopefully) keep a company out of the headlines for questionable or\ndownright reckless data practices.\nThink of the typical example of data governance being done poorly. A business\nanalyst gets a request for a report but doesn’t know what data to use to answer the\nquestion. They may spend hours digging through dozens of tables in a transactional\ndatabase, wildly guessing at which fields might be useful. The analyst compiles a\n“directionally correct” report but isn’t entirely sure that the report’s underlying data is\nMajor Undercurrents Across the Data Engineering Lifecycle | 51\n2Eryurek, Data Governance , 5.accurate or sound. The recipient of the report also questions the validity of the data.\nThe integrity of the analyst—and of all data in the company’s systems—is called into\nquestion. The company is confused about its performance, making business planning\nimpossible.\nData governance is a foundation for data-driven business practices and a mission-\ncritical part of the data engineering lifecycle. When data governance is practiced well,\npeople, processes, and technologies align to treat data as a key business driver; if data\nissues occur, they are promptly handled.\nThe core categories of data governance are discoverability, security, and accountabil‐\nity.2 Within these core categories are subcategories, such as data quality, metadata,\nand privacy. Let’s look at each core category in turn.\nDiscoverability.    In a data-driven company, data must be available and discoverable.\nEnd users should have quick and reliable access to the data they need to do their jobs.\nThey should know where the data comes from, how it relates to other data, and what\nthe data means.\nSome key areas of data discoverability include metadata management and master\ndata management. Let’s briefly describe these areas.\nMetadata.    Metadata  is “data about data, ” and it underpins every section of the data\nengineering lifecycle. Metadata is exactly the data needed to make data discoverable\nand governable.\nWe divide metadata into two major categories: autogenerated and human generated.\nModern data engineering revolves around automation, but metadata collection is\noften manual and error prone.\nTechnology can assist with this process, removing much of the error-prone work of\nmanual metadata collection. We’re seeing a proliferation of data catalogs, data-lineage\ntracking systems, and metadata management tools. Tools can crawl databases to look\nfor relationships and monitor data pipelines to track where data comes from and\nwhere it goes. A low-fidelity manual approach uses an internally led effort where\nvarious stakeholders crowdsource metadata collection within the organization. These\ndata management tools are covered in depth throughout the book, as they undercut\nmuch of the data engineering lifecycle.\nMetadata becomes a byproduct of data and data processes. However, key challenges\nremain. In particular, interoperability and standards are still lacking. Metadata tools\nare only as good as their connectors to data systems and their ability to share\n52 | Chapter 2: The Data Engineering Lifecycle\n3Chris Williams et al., “Democratizing Data at Airbnb, ” The Airbnb Tech Blog , May 12, 2017,\nhttps://oreil.ly/dM332 .metadata. In addition, automated metadata tools should not entirely take humans out\nof the loop.\nData has a social element; each organization accumulates social capital and knowl‐\nedge around processes, datasets, and pipelines. Human-oriented metadata systems\nfocus on the social aspect of metadata. This is something that Airbnb has emphasized\nin its various blog posts on data tools, particularly its original Dataportal concept.3\nSuch tools should provide a place to disclose data owners, data consumers, and\ndomain experts. Documentation and internal wiki tools provide a key foundation\nfor metadata management, but these tools should also integrate with automated data\ncataloging. For example, data-scanning tools can generate wiki pages with links to\nrelevant data objects.\nOnce metadata systems and processes exist, data engineers can consume metadata in\nuseful ways. Metadata becomes a foundation for designing pipelines and managing\ndata throughout the lifecycle.\nDMBOK  identifies four main categories of metadata that are useful to data engineers:\n•Business metadata•\n•Technical metadata•\n•Operational metadata•\n•Reference metadata•\nLet’s briefly describe each category of metadata.\nBusiness metadata  relates  to the way data is used in the business, including business\nand data definitions, data rules and logic, how and where data is used, and the data\nowner(s).\nA data engineer uses business metadata to answer nontechnical questions about who,\nwhat, where, and how. For example, a data engineer may be tasked with creating a\ndata pipeline for customer sales analysis. But what is a customer? Is it someone who’s\npurchased in the last 90 days? Or someone who’s purchased at any time the business\nhas been open? A data engineer would use the correct data to refer to business\nmetadata (data dictionary or data catalog) to look up how a “customer” is defined.\nBusiness metadata provides a data engineer with the right context and definitions to\nproperly use data.\nTechnical metadata  describes  the data created and used by systems across the data\nengineering lifecycle. It includes the data model and schema, data lineage, field\nMajor Undercurrents Across the Data Engineering Lifecycle | 53\nmappings, and pipeline workflows. A data engineer uses technical metadata to create,\nconnect, and monitor various systems across the data engineering lifecycle.\nHere are some common types of technical metadata that a data engineer will use:\n•Pipeline metadata (often produced in orchestration systems)•\n•Data lineage•\n•Schema•\nOrchestration is a central hub that coordinates workflow across various systems.\nPipeline metadata  captured  in orchestration systems provides details of the workflow\nschedule, system and data dependencies, configurations, connection details, and\nmuch more.\nData-lineage metadata  tracks  the origin and changes to data, and its dependencies,\nover time. As data flows through the data engineering lifecycle, it evolves through\ntransformations and combinations with other data. Data lineage provides an audit\ntrail of data’s evolution as it moves through various systems and workflows.\nSchema metadata  describes the structure of data stored in a system such as a database,\na data warehouse, a data lake, or a filesystem; it is one of the key differentiators\nacross different storage systems. Object stores, for example, don’t manage schema\nmetadata; instead, this must be managed in a metastore . On the other hand, cloud\ndata warehouses manage schema metadata internally.\nThese are just a few examples of technical metadata that a data engineer should\nknow about. This is not a complete list, and we cover additional aspects of technical\nmetadata throughout the book.\nOperational metadata  describes  the operational results of various systems and\nincludes statistics about processes, job IDs, application runtime logs, data used in\na process, and error logs. A data engineer uses operational metadata to determine\nwhether a process succeeded or failed and the data involved in the process.\nOrchestration systems can provide a limited picture of operational metadata, but\nthe latter still tends to be scattered across many systems. A need for better-quality\noperational metadata, and better metadata management, is a major motivation for\nnext-generation orchestration and metadata management systems.\nReference metadata  is data used to classify other data. This is also referred to  as lookup\ndata . Standard examples of reference data are internal codes, geographic codes, units\nof measurement, and internal calendar standards. Note that much of reference data\nis fully managed internally, but items such as geographic codes might come from\nstandard external references. Reference data is essentially a standard for interpreting\nother data, so if it changes, this change happens slowly over time.\n54 | Chapter 2: The Data Engineering Lifecycle\n4Eryurek, Data Governance , 113.Data accountability.    Data accountability  means  assigning an individual to govern a\nportion of data. The responsible person then coordinates the governance activities of\nother stakeholders. Managing data quality is tough if no one is accountable for the\ndata in question.\nNote that people accountable for data need not be data engineers. The accountable\nperson might be a software engineer or product manager, or serve in another role.\nIn addition, the responsible person generally doesn’t have all the resources necessary\nto maintain data quality. Instead, they coordinate with all people who touch the data,\nincluding data engineers.\nData accountability can happen at various levels; accountability can happen at the\nlevel of a table or a log stream but could be as fine-grained as a single field entity\nthat occurs across many tables. An individual may be accountable for managing a\ncustomer ID across many systems. For enterprise data management, a data domain\nis the set of all possible values that can occur for a given field type, such as in\nthis ID example. This may seem excessively bureaucratic and meticulous, but it can\nsignificantly affect data quality.\nData quality.   \nCan I trust this data?\n—Everyone in the business\nData quality  is the optimization of data toward the desired state and orbits the\nquestion, “What do you get compared with what you expect?” Data should conform\nto the expectations in the business metadata. Does the data match the definition\nagreed upon by the business?\nA data engineer ensures data quality across the entire data engineering lifecycle. This\ninvolves performing data-quality tests, and ensuring data conformance to schema\nexpectations, data completeness, and precision.\nAccording to Data Governance: The Definitive  Guide , data quality is defined by three\nmain characteristics:4\nAccuracy\nIs the collected data factually correct? Are there duplicate values? Are the\nnumeric values accurate?\nCompleteness\nAre the records complete? Do all required fields contain valid values?\nMajor Undercurrents Across the Data Engineering Lifecycle | 55\n5Tyler Akidau et al., “The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost\nin Massive-Scale, Unbounded, Out-of-Order Data Processing, ” Proceedings of the VLDB Endowment  8 (2015):\n1792–1803, https://oreil.ly/Z6XYy .Timeliness\nAre records available in a timely fashion?\nEach of these characteristics is quite nuanced. For example, how do we think about\nbots and web scrapers when dealing with web event data? If we intend to analyze\nthe customer journey, we must have a process that lets us separate humans from\nmachine-generated traffic. Any bot-generated events misclassified as human  present\ndata accuracy issues, and vice versa.\nA variety of interesting problems arise concerning completeness and timeliness. In\nthe Google paper introducing the  Dataflow model, the authors give the example of an\noffline video platform that displays ads.5 The platform downloads video and ads while\na connection is present, allows the user to watch these while offline, and then uploads\nad view data once a connection is present again. This data may arrive late, well after\nthe ads are watched. How does the platform handle billing for the ads?\nFundamentally, this problem can’t be solved by purely technical means. Rather, engi‐\nneers will need to determine their standards for late-arriving data and enforce these\nuniformly, possibly with the help of various technology tools.\nMaster Data Management\nMaster data  is data about business entities such as employees, customers, products,\nand locations. As organizations grow larger and more complex through organic\ngrowth and acquisitions, and collaborate with other businesses, maintaining a consis‐\ntent picture of entities and identities becomes more and more challenging.\nMaster data management  (MDM) is the practice of building consistent entity defi‐\nnitions known as golden records . Golden records harmonize entity data across an\norganization and with its partners. MDM is a business operations process facilitated\nby building and deploying technology tools. For example, an MDM team might\ndetermine a standard format for addresses, and then work with data engineers to\nbuild an API to return consistent addresses and a system that uses address data to\nmatch customer records across company divisions.\nMDM reaches across the full data cycle into operational databases. It may fall directly\nunder the purview of data engineering but is often the assigned responsibility of a\ndedicated team that works across the organization. Even if they don’t own MDM, data\nengineers must always be aware of it, as they might collaborate on MDM initiatives.\n56 | Chapter 2: The Data Engineering Lifecycle\nData quality sits across the boundary of human and technology problems. Data\nengineers need robust processes to collect actionable human feedback on data quality\nand use technology tools to detect quality issues preemptively before downstream\nusers ever see them. We cover these collection processes in the appropriate chapters\nthroughout this book.\nData modeling and design\nTo derive business insights from data, through business analytics and data science,\nthe data must be in a usable form. The process for converting data into a usable\nform is known as data modeling and design . Whereas we traditionally think of data\nmodeling as a problem for database administrators (DBAs) and ETL developers,\ndata modeling can happen almost anywhere in an organization. Firmware engineers\ndevelop the data format of a record for an IoT device, or web application developers\ndesign the JSON response to an API call or a MySQL table schema—these are all\ninstances of data modeling and design.\nData modeling has become more challenging because of the variety of new data\nsources and use cases. For instance, strict normalization doesn’t work well with event\ndata. Fortunately, a new generation of data tools increases the flexibility of data\nmodels, while retaining logical separations of measures, dimensions, attributes, and\nhierarchies. Cloud data warehouses support the ingestion of enormous quantities of\ndenormalized and semistructured data, while still supporting common data modeling\npatterns, such as Kimball, Inmon, and Data Vault. Data processing frameworks\nsuch as Spark can ingest a whole spectrum of data, from flat structured relational\nrecords to raw unstructured text. We discuss these data modeling and transformation\npatterns in greater detail in Chapter 8 .\nWith the wide variety of data that engineers must cope with, there is a temptation\nto throw up our hands and give up on data modeling. This is a terrible idea with\nharrowing consequences, made evident when people murmur of the write once, read\nnever (WORN) access pattern or refer to a data swamp . Data engineers need to\nunderstand modeling best practices as well as develop the flexibility to apply the\nappropriate level and type of modeling to the data source and use case.\nData lineage\nAs data moves through its lifecycle, how do you know what system affected the data\nor what the data is composed of as it gets passed around and transformed? Data\nlineage  describes the recording of an audit trail of data through its lifecycle, tracking\nboth the systems that process the data and the upstream data it depends on.\nData lineage helps with error tracking, accountability, and debugging of data and\nthe systems that process it. It has the obvious benefit of giving an audit trail for the\ndata lifecycle and helps with compliance. For example, if a user would like their data\nMajor Undercurrents Across the Data Engineering Lifecycle | 57\ndeleted from your systems, having lineage for that data lets you know where that data\nis stored and its dependencies.\nData lineage has been around for a long time in larger companies with strict compli‐\nance standards. However, it’s now being more widely adopted in smaller companies\nas data management becomes mainstream. We also note that Andy Petrella’s con‐\ncept of Data Observability Driven Development (DODD)  is closely related to data\nlineage. DODD observes data all along its lineage. This process is applied during\ndevelopment, testing, and finally production to deliver quality and conformity to\nexpectations.\nData integration and interoperability\nData integration and interoperability  is the process of integrating data across tools and\nprocesses. As we move away from a single-stack approach to analytics and toward a\nheterogeneous cloud environment in which various tools process data on demand,\nintegration and interoperability occupy an ever-widening swath of the data engineer’s\njob.\nIncreasingly, integration happens through general-purpose APIs rather than custom\ndatabase connections. For example, a data pipeline might pull data from the Sales‐\nforce API, store it to Amazon S3, call the Snowflake API to load it into a table, call the\nAPI again to run a query, and then export the results to S3 where Spark can consume\nthem.\nAll of this activity can be managed with relatively simple Python code that talks to\ndata systems rather than handling data directly. While the complexity of interacting\nwith data systems has decreased, the number of systems and the complexity of pipe‐\nlines has dramatically increased. Engineers starting from scratch quickly outgrow the\ncapabilities of bespoke scripting and stumble into the need for orchestration.  Orches‐\ntration is one of our undercurrents, and we discuss it in detail in “Orchestration”  on\npage 64 .\nData lifecycle management\nThe advent of data lakes encouraged organizations to ignore data archival and\ndestruction. Why discard data when you can simply add more storage ad infinitum?\nTwo changes have encouraged engineers to pay more attention to what happens at the\nend of the data engineering lifecycle.\nFirst, data is increasingly stored in the cloud. This means we have pay-as-you-go\nstorage costs instead of large up-front capital expenditures for an on-premises data\nlake. When every byte shows up on a monthly AWS statement, CFOs see opportuni‐\nties for savings. Cloud environments make data archival a relatively straightforward\nprocess. Major cloud vendors offer archival-specific object storage classes that allow\nlong-term data retention at an extremely low cost, assuming very infrequent access\n58 | Chapter 2: The Data Engineering Lifecycle",27297
22-DataOps.pdf,22-DataOps,"6We espouse the notion that ethical behavior is doing the right thing when no one is watching, an idea that\noccurs in the writings of C. S. Lewis, Charles Marshall, and many other authors.(it should be noted that data retrieval isn’t so cheap, but that’s for another conversa‐\ntion). These storage classes also support extra policy controls to prevent accidental or\ndeliberate deletion of critical archives.\nSecond, privacy and data retention laws such as the GDPR and the CCPA require\ndata engineers to actively manage data destruction to respect users’ “right to be\nforgotten. ” Data engineers must know what consumer data they retain and must have\nprocedures to destroy data in response to requests and compliance requirements.\nData destruction is straightforward in a cloud data warehouse. SQL semantics allow\ndeletion of rows conforming to a where  clause. Data destruction was more challeng‐\ning in data lakes, where write-once, read-many was the default storage pattern. Tools\nsuch as Hive ACID and Delta Lake allow easy management of deletion transactions\nat scale. New generations of metadata management, data lineage, and cataloging tools\nwill also streamline the end of the data engineering lifecycle.\nEthics and privacy\nThe last several years of data breaches, misinformation, and mishandling of data\nmake one thing clear: data impacts people. Data used to live in the Wild West, freely\ncollected and traded like baseball cards. Those days are long gone. Whereas data’s\nethical and privacy implications were once considered nice to have, like security,\nthey’re now central to the general data lifecycle. Data engineers need to do the right\nthing when no one else is watching, because everyone will be watching someday.6\nWe hope that more organizations will encourage a culture of good data ethics and\nprivacy.\nHow do ethics and privacy impact the data engineering lifecycle? Data engineers\nneed to ensure that datasets mask personally identifiable information (PII) and other\nsensitive information; bias can be identified and tracked in datasets as they are trans‐\nformed. Regulatory requirements and compliance penalties are only growing. Ensure\nthat your data assets are compliant with a growing number of data regulations, such\nas GDPR and CCPA. Please take this seriously. We offer tips throughout the book to\nensure that you’re baking ethics and privacy into the data engineering lifecycle.\nDataOps\nDataOps  maps the best practices of Agile methodology, DevOps, and statistical pro‐\ncess control (SPC) to data. Whereas DevOps aims to improve the release and quality\nof software products, DataOps does the same thing for data products.\nMajor Undercurrents Across the Data Engineering Lifecycle | 59\n7“What Is DataOps, ” DataKitchen FAQ page, accessed May 5, 2022, https://oreil.ly/Ns06w .Data products differ from software products because of the way data is used. A soft‐\nware product provides specific functionality and technical features for end users. By\ncontrast, a data product is built around sound business logic and metrics, whose users\nmake decisions or build models that perform automated actions. A data engineer\nmust understand both the technical aspects of building software products and the\nbusiness logic, quality, and metrics that will create excellent data products.\nLike  DevOps, DataOps borrows much from lean manufacturing and supply chain\nmanagement, mixing people, processes, and technology to reduce time to value. As\nData Kitchen (experts in DataOps) describes it:7\nDataOps is a collection of technical practices, workflows, cultural norms, and architec‐\ntural patterns that enable:\n•Rapid innovation and experimentation delivering new insights to customers with•\nincreasing velocity\n•Extremely high data quality and very low error rates•\n•Collaboration across complex arrays of people, technology, and environments•\n•Clear measurement, monitoring, and transparency of results•\nLean practices (such as lead time reduction and minimizing defects) and the resulting\nimprovements to quality and productivity are things we are glad to see gaining\nmomentum both in software and data operations.\nFirst and foremost, DataOps is a set of cultural habits; the data engineering team\nneeds to adopt a cycle of communicating and collaborating with the business,\nbreaking down silos, continuously learning from successes and mistakes, and rapid\niteration. Only when these cultural habits are set in place can the team get the best\nresults from technology and tools.\nDepending on a company’s data maturity, a data engineer has some options to build\nDataOps into the fabric of the overall data engineering lifecycle. If the company\nhas no preexisting data infrastructure or practices, DataOps is very much a green‐\nfield opportunity that can be baked in from day one. With an existing project or\ninfrastructure that lacks DataOps, a data engineer can begin adding DataOps into\nworkflows. We suggest first starting with observability and monitoring to get a\nwindow into the performance of a system, then adding in automation and incident\nresponse. A data engineer may work alongside an existing DataOps team to improve\nthe data engineering lifecycle in a data-mature company. In all cases, a data engineer\nmust be aware of the philosophy and technical aspects of DataOps.\n60 | Chapter 2: The Data Engineering Lifecycle\nDataOps  has three core technical elements: automation, monitoring and observabil‐\nity, and incident response ( Figure 2-8 ). Let’s look at each of these pieces and how they\nrelate to the data engineering lifecycle.\nFigure 2-8. The three pillars of DataOps\nAutomation\nAutomation  enables reliability and consistency in the DataOps process and allows\ndata engineers to quickly deploy new product features and improvements to existing\nworkflows. DataOps automation has a similar framework and workflow to DevOps,\nconsisting of change management (environment, code, and data version control),\ncontinuous integration/continuous deployment (CI/CD), and configuration as code.\nLike DevOps, DataOps practices monitor and maintain the reliability of technology\nand systems (data pipelines, orchestration, etc.), with the added dimension of check‐\ning for data quality, data/model drift, metadata integrity, and more.\nLet’s briefly discuss the evolution of DataOps automation within a hypothetical orga‐\nnization. An organization with a low level of DataOps maturity often attempts to\nschedule multiple stages of data transformation processes using cron jobs. This works\nwell for a while. As data pipelines become more complicated, several things are likely\nto happen. If the cron jobs are hosted on a cloud instance, the instance may have an\noperational problem, causing the jobs to stop running unexpectedly. As the spacing\nbetween jobs becomes tighter, a job will eventually run long, causing a subsequent job\nto fail or produce stale data. Engineers may not be aware of job failures until they\nhear from analysts that their reports are out-of-date.\nAs the organization’s data maturity grows, data engineers will typically adopt an\norchestration framework, perhaps Airflow or Dagster. Data engineers are aware that\nAirflow presents an operational burden, but the benefits of orchestration eventually\noutweigh the complexity. Engineers will gradually migrate their cron jobs to Airflow\njobs. Now, dependencies are checked before jobs run. More transformation jobs can\nbe packed into a given time because each job can start as soon as upstream data is\nready rather than at a fixed, predetermined time.\nThe data engineering team still has room for operational improvements. A data\nscientist eventually deploys a broken DAG, bringing down the Airflow web server\nand leaving the data team operationally blind. After enough such headaches, the\ndata engineering team members realize that they need to stop allowing manual DAG\ndeployments. In their next phase of operational maturity, they adopt automated\nDAG deployment. DAGs are tested before deployment, and monitoring processes\nensure that the new DAGs start running properly. In addition, data engineers block\nMajor Undercurrents Across the Data Engineering Lifecycle | 61\n8Andy Petrella, “Data Observability Driven Development: The Perfect Analogy for Beginners, ” Kensu, accessed\nMay 5, 2022, https://oreil.ly/MxvSX .the deployment of new Python dependencies until installation is validated. After\nautomation is adopted, the data team is much happier and experiences far fewer\nheadaches.\nOne of the tenets of the DataOps Manifesto  is “Embrace change. ” This does not\nmean change for the sake of change but rather goal-oriented change. At each stage\nof our automation journey, opportunities exist for operational improvement. Even at\nthe high level of maturity that we’ve described here, further room for improvement\nremains. Engineers might embrace a next-generation orchestration framework that\nbuilds in better metadata capabilities. Or they might try to develop a framework that\nbuilds DAGs automatically based on data-lineage specifications. The main point is\nthat engineers constantly seek to implement improvements in automation that will\nreduce their workload and increase the value that they deliver to the business.\nObservability and monitoring\nAs we tell our clients, “Data is a silent killer. ” We’ve seen countless examples of bad\ndata lingering in reports for months or years. Executives may make key decisions\nfrom this bad data, discovering the error only much later. The outcomes are usually\nbad and sometimes catastrophic for the business. Initiatives are undermined and\ndestroyed, years of work wasted. In some of the worst cases, bad data may lead\ncompanies to financial ruin.\nAnother horror story occurs when the systems that create the data for reports ran‐\ndomly stop working, resulting in reports being delayed by several days. The data team\ndoesn’t know until they’re asked by stakeholders why reports are late or producing\nstale information. Eventually, various stakeholders lose trust in the capabilities of\nthe core data team and start their own splinter teams. The result is many different\nunstable systems, inconsistent reports, and silos.\nIf you’re not observing and monitoring your data and the systems that produce the\ndata, you’re inevitably going to experience your own data horror story. Observability,\nmonitoring, logging, alerting, and tracing are all critical to getting ahead of any\nproblems along the data engineering lifecycle. We recommend you incorporate SPC\nto understand whether events being monitored are out of line and which incidents\nare worth responding to.\nPetrella’s DODD method mentioned previously in this chapter provides an excellent\nframework for thinking about data observability. DODD is much like test-driven\ndevelopment (TDD) in software engineering:8\n62 | Chapter 2: The Data Engineering Lifecycle\nThe purpose of DODD is to give everyone involved in the data chain visibility into\nthe data and data applications so that everyone involved in the data value chain has\nthe ability to identify changes to the data or data applications at every step—from\ningestion to transformation to analysis—to help troubleshoot or prevent data issues.\nDODD focuses on making data observability a first-class consideration in the data\nengineering lifecycle.\nWe cover many aspects of monitoring and observability throughout the data engi‐\nneering lifecycle in later chapters.\nIncident response\nA high-functioning data team using DataOps will be able to ship new data products\nquickly. But mistakes will inevitably happen. A system may have downtime, a new\ndata model may break downstream reports, an ML model may become stale and\nprovide bad predictions—countless problems can interrupt the data engineering life‐\ncycle. Incident response  is about using the automation and observability capabilities\nmentioned previously to rapidly identify root causes of an incident and resolve it as\nreliably and quickly as possible.\nIncident response isn’t just about technology and tools, though these are beneficial;\nit’s also about open and blameless communication, both on the data engineering team\nand across the organization. As Werner Vogels, CTO of Amazon Web Services, is\nfamous for saying, “Everything breaks all the time. ” Data engineers must be prepared\nfor a disaster and ready to respond as swiftly and efficiently as possible.\nData engineers should proactively find issues before the business reports them.\nFailure happens, and when the stakeholders or end users see problems, they will\npresent them. They will be unhappy to do so. The feeling is different when they\ngo to raise those issues to a team and see that they are actively being worked on\nto resolve already. Which team’s state would you trust more as an end user? Trust\ntakes a long time to build and can be lost in minutes. Incident response is as much\nabout retroactively responding to incidents as proactively addressing them before\nthey happen.\nDataOps summary\nAt this point, DataOps is still a work in progress. Practitioners have done a good job\nof adapting DevOps principles to the data domain and mapping out an initial vision\nthrough the DataOps Manifesto and other resources. Data engineers would do well\nto make DataOps practices a high priority in all of their work. The up-front effort\nwill see a significant long-term payoff through faster delivery of products, better\nreliability and accuracy of data, and greater overall value for the business.\nThe state of operations in data engineering is still quite immature compared with\nsoftware engineering. Many data engineering tools, especially legacy monoliths, are\nMajor Undercurrents Across the Data Engineering Lifecycle | 63",13866
23-Data Architecture.pdf,23-Data Architecture,,0
24-Orchestration.pdf,24-Orchestration,"9Ternary Data, “ An Introduction to Dagster: The Orchestrator for the Full Data Lifecycle - UDEM June 2021, ”\nY ouTube video, 1:09:40, https://oreil.ly/HyGMh .not automation-first. A recent movement has arisen to adopt automation best practi‐\nces across the data engineering lifecycle. Tools like Airflow have paved the way for a\nnew generation of automation and data management tools. The general practices we\ndescribe for DataOps are aspirational, and we suggest companies try to adopt them to\nthe fullest extent possible, given the tools and knowledge available today.\nData Architecture\nA data architecture reflects the current and future state of data systems that support\nan organization’s long-term data needs and strategy. Because an organization’s data\nrequirements will likely change rapidly, and new tools and practices seem to arrive on\na near-daily basis, data engineers must understand good data architecture. Chapter 3\ncovers data architecture in depth, but we want to highlight here that data architecture\nis an undercurrent of the data engineering lifecycle.\nA data engineer should first understand the needs of the business and gather require‐\nments for new use cases. Next, a data engineer needs to translate those requirements\nto design new ways to capture and serve data, balanced for cost and operational\nsimplicity. This means knowing the trade-offs with design patterns, technologies, and\ntools in source systems, ingestion, storage, transformation, and serving data.\nThis doesn’t imply that a data engineer is a data architect, as these are typically two\nseparate roles. If a data engineer works alongside a data architect, the data engineer\nshould be able to deliver on the data architect’s designs and provide architectural\nfeedback.\nOrchestration\nWe think that orchestration matters because we view it as really the center of gravity of\nboth the data platform as well as the data lifecycle, the software development lifecycle\nas it comes to data.\n—Nick Schrock, founder of Elementl9\nOrchestration  is not only a central DataOps process, but also a critical part of the\nengineering and deployment flow for data jobs. So, what is orchestration?\nOrchestration  is the process of coordinating many jobs to run as quickly and effi‐\nciently as possible on a scheduled cadence. For instance, people often refer to\norchestration tools like Apache Airflow as schedulers . This isn’t quite accurate. A\npure scheduler, such as cron, is aware only of time; an orchestration engine builds\nin metadata on job dependencies, generally in the form of a  directed acyclic graph\n64 | Chapter 2: The Data Engineering Lifecycle\n(DAG). The DAG can be run once or scheduled to run at a fixed interval of daily,\nweekly, every hour, every five minutes, etc.\nAs we discuss orchestration throughout this book, we assume that an orchestration\nsystem stays online with high availability. This allows the orchestration system to\nsense and monitor constantly without human intervention and run new jobs anytime\nthey are deployed. An orchestration system monitors jobs that it manages and kicks\noff new tasks as internal DAG dependencies are completed. It can also monitor\nexternal systems and tools to watch for data to arrive and criteria to be met. When\ncertain conditions go out of bounds, the system also sets error conditions and sends\nalerts through email or other channels. Y ou might set an expected completion time of\n10 a.m. for overnight daily data pipelines. If jobs are not done by this time, alerts go\nout to data engineers and consumers.\nOrchestration systems also build job history capabilities, visualization, and alerting.\nAdvanced orchestration engines can backfill new DAGs or individual tasks as they\nare added to a DAG. They also support dependencies over a time range. For example,\na monthly reporting job might check that an ETL job has been completed for the full\nmonth before starting.\nOrchestration has long been a key capability for data processing but was not often\ntop of mind nor accessible to anyone except the largest companies. Enterprises used\nvarious tools to manage job flows, but these were expensive, out of reach of small\nstartups, and generally not extensible. Apache Oozie was extremely popular in the\n2010s, but it was designed to work within a Hadoop cluster and was difficult to use in\na more heterogeneous environment. Facebook developed Dataswarm for internal use\nin the late 2000s; this inspired popular tools such as Airflow, introduced by Airbnb in\n2014.\nAirflow was open source from its inception and was widely adopted. It was written in\nPython, making it highly extensible to almost any use case imaginable. While many\nother interesting open source orchestration projects exist, such as Luigi and Conduc‐\ntor, Airflow is arguably the mindshare leader for the time being. Airflow arrived just\nas data processing was becoming more abstract and accessible, and engineers were\nincreasingly interested in coordinating complex flows across multiple processors and\nstorage systems, especially in cloud environments.\nAt this writing, several nascent open source projects aim to mimic the best elements\nof Airflow’s core design while improving on it in key areas. Some of the most inter‐\nesting examples are Prefect and Dagster, which aim to improve the portability and\ntestability of DAGs to allow engineers to move from local development to production\nmore easily. Argo is an orchestration engine built around Kubernetes primitives;\nMetaflow is an open source project out of Netflix that aims to improve data science\norchestration.\nMajor Undercurrents Across the Data Engineering Lifecycle | 65",5710
25-Software Engineering.pdf,25-Software Engineering,"We must point out that orchestration is strictly a batch concept. The streaming\nalternative to orchestrated task DAGs is the streaming DAG. Streaming DAGs remain\nchallenging to build and maintain, but next-generation streaming platforms such as\nPulsar aim to dramatically reduce the engineering and operational burden. We talk\nmore about these developments in Chapter 8 .\nSoftware Engineering\nSoftware engineering has always been a central skill for data engineers. In the early\ndays of contemporary data engineering (2000–2010), data engineers worked on low-\nlevel frameworks and wrote MapReduce jobs in C, C++, and Java. At the peak of\nthe big data era (the mid-2010s), engineers started using frameworks that abstracted\naway these low-level details.\nThis abstraction continues today. Cloud data warehouses support powerful transfor‐\nmations using SQL semantics; tools like Spark have become more user-friendly,\ntransitioning away from low-level coding details and toward easy-to-use dataframes.\nDespite this abstraction, software engineering is still critical to data engineering. We\nwant to briefly discuss a few common areas of software engineering that apply to the\ndata engineering lifecycle.\nCore data processing code\nThough it has become more abstract and easier to manage, core data processing code\nstill needs to be written, and it appears throughout the data engineering lifecycle.\nWhether in ingestion, transformation, or data serving, data engineers need to be\nhighly proficient and productive in frameworks and languages such as Spark, SQL, or\nBeam; we reject the notion that SQL is not code.\nIt’s also imperative that a data engineer understand proper code-testing methodolo‐\ngies, such as unit, regression, integration, end-to-end, and smoke.\nDevelopment of open source frameworks\nMany  data engineers are heavily involved in developing open source frameworks.\nThey adopt these frameworks to solve specific problems in the data engineering\nlifecycle, and then continue developing the framework code to improve the tools for\ntheir use cases and contribute back to the community.\nIn the big data era, we saw a Cambrian explosion of data-processing frameworks\ninside the Hadoop ecosystem. These tools primarily focused on transforming and\nserving parts of the data engineering lifecycle. Data engineering tool speciation has\nnot ceased or slowed down, but the emphasis has shifted up the ladder of abstraction,\naway from direct data processing. This new generation of open source tools assists\nengineers in managing, enhancing, connecting, optimizing, and monitoring data.\n66 | Chapter 2: The Data Engineering Lifecycle\nFor example, Airflow dominated the orchestration space from 2015 until the early\n2020s. Now, a new batch of open source competitors (including Prefect, Dagster, and\nMetaflow) has sprung up to fix perceived limitations of Airflow, providing better\nmetadata handling, portability, and dependency management. Where the future of\norchestration goes is anyone’s guess.\nBefore data engineers begin engineering new internal tools, they would do well to\nsurvey the landscape of publicly available tools. Keep an eye on the total cost of\nownership (TCO) and opportunity cost associated with implementing a tool. There\nis a good chance that an open source project already exists to address the problem\nthey’re looking to solve, and they would do well to collaborate rather than reinventing\nthe wheel.\nStreaming\nStreaming data processing is inherently more complicated than batch, and the tools\nand paradigms are arguably less mature. As streaming data becomes more pervasive\nin every stage of the data engineering lifecycle, data engineers face interesting soft‐\nware engineering problems.\nFor instance, data processing tasks such as joins that we take for granted in the\nbatch processing world often become more complicated in real time, requiring more\ncomplex software engineering. Engineers must also write code to apply a variety\nof windowing  methods. Windowing allows real-time systems to calculate valuable\nmetrics such as trailing statistics. Engineers have many frameworks to choose from,\nincluding various function platforms (OpenFaaS, AWS Lambda, Google Cloud Func‐\ntions) for handling individual events or dedicated stream processors (Spark, Beam,\nFlink, or Pulsar) for analyzing streams to support reporting and real-time actions.\nInfrastructure as code\nInfrastructure as code  (IaC) applies software engineering practices to the configura‐\ntion and management of infrastructure. The infrastructure management burden of\nthe big data era has decreased as companies have migrated to managed big data\nsystems—such as Databricks and Amazon Elastic MapReduce (EMR)—and cloud\ndata warehouses. When data engineers have to manage their infrastructure in a cloud\nenvironment, they increasingly do this through IaC frameworks rather than manually\nspinning up instances and installing software. Several general-purpose and cloud-\nplatform-specific frameworks allow automated infrastructure deployment based on a\nset of specifications. Many of these frameworks can manage cloud services as well as\ninfrastructure. There is also a notion of IaC with containers and Kubernetes, using\ntools like Helm.\nMajor Undercurrents Across the Data Engineering Lifecycle | 67",5359
26-Additional Resources.pdf,26-Additional Resources,"These practices are a vital part of DevOps, allowing version control and repeatability\nof deployments. Naturally, these capabilities are vital throughout the data engineering\nlifecycle, especially as we adopt DataOps practices.\nPipelines as code\nPipelines as code  is the core concept of present-day orchestration systems, which\ntouch every stage of the data engineering lifecycle. Data engineers use code (typically\nPython) to declare data tasks and dependencies among them. The orchestration\nengine interprets these instructions to run steps using available resources.\nGeneral-purpose problem solving\nIn practice, regardless of which high-level tools they adopt, data engineers will run\ninto corner cases throughout the data engineering lifecycle that require them to\nsolve problems outside the boundaries of their chosen tools and to write custom\ncode. When using frameworks like Fivetran, Airbyte, or Matillion, data engineers\nwill encounter data sources without existing connectors and need to write something\ncustom. They should be proficient in software engineering to understand APIs, pull\nand transform data, handle exceptions, and so forth.\nConclusion\nMost discussions we’ve seen in the past about data engineering involve technologies\nbut miss the bigger picture of data lifecycle management. As technologies become\nmore abstract and do more heavy lifting, a data engineer has the opportunity to\nthink and act on a higher level. The data engineering lifecycle, supported by its\nundercurrents, is an extremely useful mental model for organizing the work of data\nengineering.\nWe break the data engineering lifecycle into the following stages:\n•Generation•\n•Storage•\n•Ingestion•\n•Transformation•\n•Serving data•\nSeveral themes cut across the data engineering lifecycle as well. These are the under‐\ncurrents of the data engineering lifecycle. At a high level, the undercurrents are as\nfollows:\n68 | Chapter 2: The Data Engineering Lifecycle\n•Security•\n•Data management•\n•DataOps•\n•Data architecture•\n•Orchestration•\n•Software engineering•\nA data engineer has several top-level goals across the data lifecycle: produce optimum\nROI and reduce costs (financial and opportunity), reduce risk (security, data quality),\nand maximize data value and utility.\nThe next two chapters discuss how these elements impact good architecture design,\nalong with choosing the right technologies. If you feel comfortable with these two\ntopics, feel free to skip ahead to Part II , where we cover each of the stages of the data\nengineering lifecycle.\nAdditional Resources\n•“ A Comparison of Data Processing Frameworks”  by Ludovic Santos •\n•DAMA International website•\n•“The Dataflow Model: A Practical Approach to Balancing Correctness, Latency,•\nand Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing”  by Tyler\nAkidau et al.\n•“Data Processing” Wikipedia page•\n•“Data Transformation” Wikipedia page•\n•“Democratizing Data at Airbnb”  by Chris Williams et al. •\n•“Five Steps to Begin Collecting the Value of Y our Data” Lean-Data web page•\n•“Getting Started with DevOps Automation”  by Jared Murrell •\n•“Incident Management in the Age of DevOps” Atlassian web page•\n•“ An Introduction to Dagster: The Orchestrator for the Full Data Lifecycle” video•\nby Nick Schrock\n•“Is DevOps Related to DataOps?”  by Carol Jang and Jove Kuang •\n•“The Seven Stages of Effective Incident Response” Atlassian web page•\n•“Staying Ahead of Debt”  by Etai Mizrahi •\n•“What Is Metadata”  by Michelle Knight •\nAdditional Resources | 69",3575
27-Chapter 3. Designing Good Data Architecture.pdf,27-Chapter 3. Designing Good Data Architecture,,0
28-Enterprise Architecture Defined.pdf,28-Enterprise Architecture Defined,"CHAPTER 3\nDesigning Good Data Architecture\nGood data architecture provides seamless capabilities across every step of the data\nlifecycle and undercurrent. We’ll begin by defining data architecture  and then dis‐\ncuss components and considerations. We’ll then touch on specific batch patterns\n(data warehouses, data lakes), streaming patterns, and patterns that unify batch and\nstreaming. Throughout, we’ll emphasize leveraging the capabilities of the cloud to\ndeliver scalability, availability, and reliability.\nWhat Is Data Architecture?\nSuccessful  data engineering is built upon rock-solid data architecture. This chapter\naims to review a few popular architecture approaches and frameworks, and then\ncraft our opinionated definition of what makes “good” data architecture. Indeed,\nwe won’t make everyone happy. Still, we will lay out a pragmatic, domain-specific,\nworking definition for data architecture  that we think will work for companies of\nvastly different scales, business processes, and needs.\nWhat is data architecture? When you stop to unpack it, the topic becomes a bit\nmurky; researching data architecture yields many inconsistent and often outdated\ndefinitions. It’s a lot like when we defined data engineering  in Chapter 1 —there’s no\nconsensus. In a field that is constantly changing, this is to be expected. So what do\nwe mean by data architecture  for the purposes of this book? Before defining the term,\nit’s essential to understand the context in which it sits. Let’s briefly cover enterprise\narchitecture, which will frame our definition of data architecture.\n71\n1The Open Group, TOGAF Version 9.1 , https://oreil.ly/A1H67 .\n2Gartner Glossary, s.v. “Enterprise Architecture (EA), ” https://oreil.ly/SWwQF .Enterprise Architecture Defined\nEnterprise architecture has many subsets, including business, technical, application,\nand data ( Figure 3-1 ). As such, many frameworks and resources are devoted to\nenterprise architecture. In truth, architecture is a surprisingly controversial topic.\nFigure 3-1. Data architecture is a subset of enterprise architecture\nThe term enterprise  gets mixed reactions. It brings to mind sterile corporate offi‐\nces, command-and-control/waterfall planning, stagnant business cultures, and empty\ncatchphrases. Even so, we can learn some things here.\nBefore we define and describe enterprise architecture , let’s unpack this term. Let’s\nlook at how enterprise architecture is defined by some significant thought leaders:\nTOGAF, Gartner, and EABOK.\nTOGAF’s definition\nTOGAF  is The Open Group Architecture Framework , a standard of The Open Group.\nIt’s touted as the most widely used architecture framework today. Here’s the TOGAF\ndefinition:1\nThe term “enterprise” in the context of “enterprise architecture” can denote an entire\nenterprise—encompassing all of its information and technology services, processes,\nand infrastructure—or a specific domain within the enterprise. In both cases, the\narchitecture crosses multiple systems, and multiple functional groups within the\nenterprise.\nGartner’s definition\nGartner  is a global research and advisory company that produces research articles\nand reports on trends related to enterprises. Among other things, it is responsible for\nthe (in)famous Gartner Hype Cycle. Gartner’s definition is as follows:2\nEnterprise architecture (EA) is a discipline for proactively and holistically leading\nenterprise responses to disruptive forces by identifying and analyzing the execution of\nchange toward desired business vision and outcomes. EA delivers value by presenting\n72 | Chapter 3: Designing Good Data Architecture\n3EABOK Consortium website, https://eabok.org .\n4Jeff Haden, “ Amazon Founder Jeff Bezos: This Is How Successful People Make Such Smart Decisions, ” Inc.,\nDecember 3, 2018, https://oreil.ly/QwIm0 .business and IT leaders with signature-ready recommendations for adjusting policies\nand projects to achieve targeted business outcomes that capitalize on relevant business\ndisruptions.\nEABOK’s definition\nEABOK  is the Enterprise Architecture Book of Knowledge , an enterprise architecture\nreference produced by the MITRE Corporation. EABOK was released as an incom‐\nplete draft in 2004 and has not been updated since. Though seemingly obsolete,\nEABOK is frequently referenced in descriptions of enterprise architecture; we found\nmany of its ideas helpful while writing this book. Here’s the EABOK definition:3\nEnterprise Architecture (EA) is an organizational model; an abstract representation of\nan Enterprise that aligns strategy, operations, and technology to create a roadmap for\nsuccess.\nOur definition\nWe extract a few common threads in these definitions of enterprise architecture:\nchange, alignment, organization, opportunities, problem-solving, and migration.\nHere is our definition of enterprise architecture , one that we feel is more relevant\nto today’s fast-moving data landscape:\nEnterprise architecture is the design of systems to support change in the enterprise ,\nachieved by flexible  and reversible decisions  reached through careful evaluation of\ntrade-offs .\nHere, we touch on some key areas we’ll return to throughout the book: flexible and\nreversible decisions, change management, and evaluation of trade-offs. We discuss\neach theme at length in this section and then make the definition more concrete in\nthe latter part of the chapter by giving various examples of data architecture.\nFlexible and reversible decisions are essential for two reasons. First, the world is\nconstantly changing, and predicting the future is impossible. Reversible decisions\nallow you to adjust course as the world changes and you gather new information.\nSecond, there is a natural tendency toward enterprise ossification as organizations\ngrow. Adopting a culture of reversible decisions helps overcome this tendency by\nreducing the risk attached to a decision.\nJeff Bezos is credited with the idea of one-way and two-way doors.4 A one-way door\nis a decision that is almost impossible to reverse. For example, Amazon could have\nWhat Is Data Architecture? | 73\ndecided to sell AWS or shut it down. It would be nearly impossible for Amazon to\nrebuild a public cloud with the same market position after such an action.\nOn the other hand, a two-way door  is an easily reversible decision: you walk through\nand proceed if you like what you see in the room or step back through the door if\nyou don’t. Amazon might decide to require the use of DynamoDB for a new micro‐\nservices database. If this policy doesn’t work, Amazon has the option of reversing it\nand refactoring some services to use other databases. Since the stakes attached to each\nreversible decision (two-way door) are low, organizations can make more decisions,\niterating, improving, and collecting data rapidly.\nChange management is closely related to reversible decisions and is a central theme\nof enterprise architecture frameworks. Even with an emphasis on reversible decisions,\nenterprises often need to undertake large initiatives. These are ideally broken into\nsmaller changes, each one a reversible decision in itself. Returning to Amazon,\nwe note a five-year gap (2007 to 2012) from the publication of a paper on the\nDynamoDB concept to Werner Vogels’s announcement of the DynamoDB service on\nAWS. Behind the scenes, teams took numerous small actions to make DynamoDB a\nconcrete reality for AWS customers. Managing such small actions is at the heart of\nchange management.\nArchitects are not simply mapping out IT processes and vaguely looking toward a\ndistant, utopian future; they actively solve business problems and create new oppor‐\ntunities. Technical solutions exist not for their own sake but in support of business\ngoals. Architects identify problems in the current state (poor data quality, scalability\nlimits, money-losing lines of business), define desired future states (agile data-quality\nimprovement, scalable cloud data solutions, improved business processes), and real‐\nize initiatives through execution of small, concrete steps. It bears repeating:\nTechnical solutions exist not for their own sake but in support of business goals.\nWe found significant inspiration in Fundamentals of Software  Architecture  by Mark\nRichards and Neal Ford (O’Reilly). They emphasize that trade-offs are inevitable and\nubiquitous in the engineering space. Sometimes the relatively fluid nature of software\nand data leads us to believe that we are freed from the constraints that engineers face\nin the hard, cold physical world. Indeed, this is partially true; patching a software\nbug is much easier than redesigning and replacing an airplane wing. However, digi‐\ntal systems are ultimately constrained by physical limits such as latency, reliability,\ndensity, and energy consumption. Engineers also confront various nonphysical limits,\nsuch as characteristics of programming languages and frameworks, and practical\nconstraints in managing complexity, budgets, etc. Magical thinking culminates in\npoor engineering. Data engineers must account for trade-offs at every step to design\nan optimal system while minimizing high-interest technical debt.\n74 | Chapter 3: Designing Good Data Architecture",9296
29-Principle 3 Architect for Scalability.pdf,29-Principle 3 Architect for Scalability,"5The Open Group, TOGAF Version 9.1 , https://oreil.ly/A1H67 .\n6DAMA - DMBOK: Data Management Body of Knowledge , 2nd ed. (Technics Publications, 2017).Let’s reiterate one central point in our enterprise architecture definition: enterprise\narchitecture balances flexibility and trade-offs. This isn’t always an easy balance, and\narchitects must constantly assess and reevaluate with the recognition that the world\nis dynamic. Given the pace of change that enterprises are faced with, organizations—\nand their architecture—cannot afford to stand still.\nData Architecture Defined\nNow that you understand enterprise architecture, let’s dive into data architecture by\nestablishing a working definition that will set the stage for the rest of the book. Data\narchitecture  is a subset of enterprise architecture, inheriting its properties: processes,\nstrategy, change management, and evaluating trade-offs. Here are a couple of defini‐\ntions of data architecture that influence our definition.\nTOGAF’s definition\nTOGAF defines data architecture as follows:5\nA description of the structure and interaction of the enterprise’s major types and sour‐\nces of data, logical data assets, physical data assets, and data management resources.\nDAMA’s definition\nThe DAMA DMBOK  defines data architecture as follows:6\nIdentifying the data needs of the enterprise (regardless of structure) and designing and\nmaintaining the master blueprints to meet those needs. Using master blueprints to\nguide data integration, control data assets, and align data investments with business\nstrategy.\nOur definition\nConsidering the preceding two definitions and our experience, we have crafted our\ndefinition of data architecture :\nData architecture is the design of systems to support the evolving data needs of an\nenterprise, achieved by flexible and reversible decisions reached through a careful\nevaluation of trade-offs.\nHow does data architecture fit into data engineering? Just as the data engineering life‐\ncycle is a subset of the data lifecycle, data engineering architecture is a subset of gen‐\neral data architecture. Data engineering architecture  is the systems and frameworks\nWhat Is Data Architecture? | 75\n7Mark Richards and Neal Ford, Fundamentals of Software  Architecture  (Sebastopol, CA: O’Reilly, 2020),\nhttps://oreil.ly/hpCp0 .that make up the key sections of the data engineering lifecycle. We’ll use data architec‐\nture interchangeably with data engineering architecture  throughout this book.\nOther  aspects of data architecture that you should be aware of are operational and\ntechnical ( Figure 3-2 ). Operational architecture  encompasses the functional require‐\nments of what needs to happen related to people, processes, and technology. For\nexample, what business processes does the data serve? How does the organization\nmanage data quality? What is the latency requirement from when the data is pro‐\nduced to when it becomes available to query? Technical architecture  outlines how data\nis ingested, stored, transformed, and served along the data engineering lifecycle. For\ninstance, how will you move 10 TB of data every hour from a source database to your\ndata lake? In short, operational architecture describes what  needs to be done, and\ntechnical architecture details how it will happen.\nFigure 3-2. Operational and technical data architecture\nNow that we have a working definition of data architecture, let’s cover the elements of\n“good” data architecture.\n“Good” Data Architecture\nNever shoot for the best architecture, but rather the least worst architecture.\n—Mark Richards and Neal Ford7\nAccording  to Grady Booch , “ Architecture represents the significant design decisions\nthat shape a system, where significant  is measured by cost of change. ” Data architects\naim to make significant decisions that will lead to good architecture at a basic level.\nWhat do we mean by “good” data architecture? To paraphrase an old cliche, you\nknow good when you see it. Good data architecture  serves business requirements with\na common, widely reusable set of building blocks while maintaining flexibility and\nmaking appropriate trade-offs. Bad architecture is authoritarian and tries to cram a\nbunch of one-size-fits-all decisions into a big ball of mud .\n76 | Chapter 3: Designing Good Data Architecture\nAgility  is the foundation for good data architecture; it acknowledges that the world\nis fluid. Good data architecture is flexible  and easily maintainable . It evolves in\nresponse to changes within the business and new technologies and practices that\nmay unlock even more value in the future. Businesses and their use cases for data are\nalways evolving. The world is dynamic, and the pace of change in the data space is\naccelerating.  Last year’s data architecture that served you well might not be sufficient\nfor today, let alone next year.\nBad data architecture is tightly coupled, rigid, overly centralized, or uses the wrong\ntools for the job, hampering development and change management. Ideally, by\ndesigning architecture with reversibility in mind, changes will be less costly.\nThe undercurrents of the data engineering lifecycle form the foundation of good\ndata architecture for companies at any stage of data maturity. Again, these undercur‐\nrents are security, data management, DataOps, data architecture, orchestration, and\nsoftware engineering.\nGood data architecture is a living, breathing thing. It’s never finished. In fact, per\nour definition, change and evolution are central to the meaning and purpose of data\narchitecture. Let’s now look at the principles of good data architecture.\nPrinciples of Good Data Architecture\nThis section takes a 10,000-foot view of good architecture by focusing on principles—\nkey ideas useful in evaluating major architectural decisions and practices. We borrow\ninspiration for our architecture principles from several sources, especially the AWS\nWell-Architected Framework and Google Cloud’s Five Principles for Cloud-Native\nArchitecture.\nThe AWS Well-Architected Framework  consists of six pillars:\n•Operational excellence•\n•Security•\n•Reliability•\n•Performance efficiency•\n•Cost optimization•\n•Sustainability•\nGoogle Cloud’s Five Principles for Cloud-Native Architecture  are as follows:\n•Design for automation.•\n•Be smart with state.•\n•Favor managed services.•\nPrinciples of Good Data Architecture | 77\n•Practice defense in depth.•\n•Always be architecting.•\nWe advise you to carefully study both frameworks, identify valuable ideas, and deter‐\nmine points of disagreement. We’ d like to expand or elaborate on these pillars with\nthese principles of data engineering architecture:\n1.Choose common components wisely.1.\n2.Plan for failure.2.\n3.Architect for scalability.3.\n4.Architecture is leadership.4.\n5.Always be architecting.5.\n6.Build loosely coupled systems.6.\n7.Make reversible decisions.7.\n8.Prioritize security.8.\n9.Embrace FinOps.9.\nPrinciple 1: Choose Common Components Wisely\nOne  of the primary jobs of a data engineer is to choose common components and\npractices that can be used widely across an organization. When architects choose\nwell and lead effectively, common components become a fabric facilitating team\ncollaboration and breaking down silos. Common components enable agility within\nand across teams in conjunction with shared knowledge and skills.\nCommon components can be anything that has broad applicability within an orga‐\nnization. Common components include object storage, version-control systems,\nobservability, monitoring and orchestration systems, and processing engines. Com‐\nmon components should be accessible to everyone with an appropriate use case,\nand teams are encouraged to rely on common components already in use rather\nthan reinventing the wheel. Common components must support robust permissions\nand security to enable sharing of assets among teams while preventing unauthorized\naccess.\nCloud platforms are an ideal place to adopt common components. For example,\ncompute and storage separation in cloud data systems allows users to access a shared\nstorage layer (most commonly object storage) using specialized tools to access and\nquery the data needed for specific use cases.\n78 | Chapter 3: Designing Good Data Architecture\n8UberPulse, “ Amazon.com CTO: Everything Fails, ” Y ouTube video, 3:03, https://oreil.ly/vDVlX .Choosing common components is a balancing act. On the one hand, you need to\nfocus on needs across the data engineering lifecycle and teams, utilize common\ncomponents that will be useful for individual projects, and simultaneously facilitate\ninteroperation and collaboration. On the other hand, architects should avoid deci‐\nsions that will hamper the productivity of engineers working on domain-specific\nproblems by forcing them into one-size-fits-all technology solutions. Chapter 4  pro‐\nvides additional details.\nPrinciple 2: Plan for Failure\nEverything fails, all the time.\n—Werner Vogels, CTO of Amazon Web Services8\nModern hardware is highly robust and durable. Even so, any hardware component\nwill fail, given enough time. To build highly robust data systems, you must consider\nfailures in your designs. Here are a few key terms for evaluating failure scenarios; we\ndescribe these in greater detail in this chapter and throughout the book:\nAvailability\nThe percentage of time an IT service or component is in an operable state.\nReliability\nThe system’s probability of meeting defined standards in performing its intended\nfunction during a specified interval.\nRecovery time objective\nThe maximum acceptable time for a service or system outage. The recovery time\nobjective (RTO) is generally set by determining the business impact of an outage.\nAn RTO of one day might be fine for an internal reporting system. A website\noutage of just five minutes could have a significant adverse business impact on an\nonline retailer.\nRecovery point objective\nThe acceptable state after recovery. In data systems, data is often lost during an\noutage. In this setting, the  recovery point objective (RPO) refers to the maximum\nacceptable data loss.\nEngineers need to consider acceptable reliability, availability, RTO, and RPO in\ndesigning for failure. These will guide their architecture decisions as they assess\npossible failure scenarios.\nPrinciples of Good Data Architecture | 79",10455
30-Principle 5 Always Be Architecting.pdf,30-Principle 5 Always Be Architecting,"Principle 3: Architect for Scalability\nScalability  in data systems encompasses two main capabilities. First, scalable systems\ncan scale up  to handle significant quantities of data. We might need to spin up a\nlarge cluster to train a model on a petabyte of customer data or scale out a streaming\ningestion system to handle a transient load spike. Our ability to scale up allows us to\nhandle extreme loads temporarily. Second, scalable systems can scale down . Once the\nload spike ebbs, we should automatically remove capacity to cut costs. (This is related\nto principle 9.) An elastic system  can scale dynamically in response to load, ideally in\nan automated fashion.\nSome scalable systems can also scale to zero : they shut down completely when not in\nuse. Once the large model-training job completes, we can delete the cluster. Many ser‐\nverless systems (e.g., serverless functions and serverless online analytical processing,\nor OLAP , databases) can automatically scale to zero.\nNote that deploying inappropriate scaling strategies can result in overcomplicated\nsystems and high costs. A straightforward relational database with one failover node\nmay be appropriate for an application instead of a complex cluster arrangement.\nMeasure your current load, approximate load spikes, and estimate load over the next\nseveral years to determine if your database architecture is appropriate. If your startup\ngrows much faster than anticipated, this growth should also lead to more available\nresources to rearchitect for scalability.\nPrinciple 4: Architecture Is Leadership\nData architects are responsible for technology decisions and architecture descrip‐\ntions and disseminating these choices through effective leadership and training.\nData architects should be highly technically competent but delegate most individual\ncontributor work to others. Strong leadership skills combined with high technical\ncompetence are rare and extremely valuable. The best data architects take this duality\nseriously.\nNote that leadership does not imply a command-and-control approach to technology.\nIt was not uncommon in the past for architects to choose one proprietary database\ntechnology and force every team to house their data there. We oppose this approach\nbecause it can significantly hinder current data projects. Cloud environments allow\narchitects to balance common component choices with flexibility that enables inno‐\nvation within projects.\n80 | Chapter 3: Designing Good Data Architecture",2518
31-Principle 6 Build Loosely Coupled Systems.pdf,31-Principle 6 Build Loosely Coupled Systems,"9Martin Fowler, “Who Needs an Architect?” IEEE Software , July/August 2003, https://oreil.ly/wAMmZ .\n10Google Cloud, “DevOps Tech: Architecture, ” Cloud Architecture Center, https://oreil.ly/j4MT1 .Returning to the notion of technical leadership, Martin Fowler describes a specific\narchetype of an ideal software architect, well embodied in his colleague Dave Rice:9\nIn many ways, the most important activity of Architectus Oryzus  is to mentor the\ndevelopment team, to raise their level so they can take on more complex issues.\nImproving the development team’s ability gives an architect much greater leverage than\nbeing the sole decision-maker and thus running the risk of being an architectural\nbottleneck.\nAn ideal data architect manifests similar characteristics. They possess the technical\nskills of a data engineer but no longer practice data engineering day to day; they\nmentor current data engineers, make careful technology choices in consultation with\ntheir organization, and disseminate expertise through training and leadership. They\ntrain engineers in best practices and bring the company’s engineering resources\ntogether to pursue common goals in both technology and business.\nAs a data engineer, you should practice architecture leadership and seek mentorship\nfrom architects. Eventually, you may well occupy the architect role yourself.\nPrinciple 5: Always Be Architecting\nWe borrow this principle directly from Google Cloud’s Five Principles for Cloud-\nNative Architecture. Data architects don’t serve in their role simply to maintain the\nexisting state; instead, they constantly design new and exciting things in response\nto changes in business and technology. Per the EABOK , an architect’s job is to\ndevelop deep knowledge of the baseline architecture  (current state), develop a  target\narchitecture , and map out a sequencing plan  to determine priorities and the order of\narchitecture changes.\nWe add that modern architecture should not be command-and-control or waterfall\nbut collaborative and agile. The data architect maintains a target architecture and\nsequencing plans that change over time. The target architecture becomes a moving\ntarget, adjusted in response to business and technology changes internally and world‐\nwide. The sequencing plan determines immediate priorities for delivery.\nPrinciple 6: Build Loosely Coupled Systems\nWhen the architecture of the system is designed to enable teams to test, deploy, and\nchange systems without dependencies on other teams, teams require little communica‐\ntion to get work done. In other words, both the architecture and the teams are loosely\ncoupled.\n—Google DevOps tech architecture guide10\nPrinciples of Good Data Architecture | 81\n11“The Bezos API Mandate: Amazon’s Manifesto for Externalization, ” Nordic APIs, January 19, 2021,\nhttps://oreil.ly/vIs8m .In 2002, Bezos wrote an email to Amazon employees that became known as the Bezos\nAPI Mandate:11\n1.All teams will henceforth expose their data and functionality through service1.\ninterfaces.\n2.Teams must communicate with each other through these interfaces.2.\n3.There will be no other form of interprocess communication allowed: no direct3.\nlinking, no direct reads of another team’s data store, no shared-memory model,\nno back-doors whatsoever. The only communication allowed is via service inter‐\nface calls over the network.\n4.It doesn’t matter what technology they use. HTTP , Corba, Pubsub, custom proto‐4.\ncols—doesn’t matter.\n5.All service interfaces, without exception, must be designed from the ground up5.\nto be externalizable. That is to say, the team must plan and design to be able to\nexpose the interface to developers in the outside world. No exceptions.\nThe advent of Bezos’s API Mandate is widely viewed as a watershed moment for\nAmazon. Putting data and services behind APIs enabled the loose coupling and\neventually resulted in AWS as we know it now. Google’s pursuit of loose coupling\nallowed it to grow its systems to an extraordinary scale.\nFor software architecture, a loosely coupled system has the following properties:\n1.Systems are broken into many small components.1.\n2.These systems interface with other services through abstraction layers, such as2.\na messaging bus or an API. These abstraction layers hide and protect internal\ndetails of the service, such as a database backend or internal classes and method\ncalls.\n3.As a consequence of property 2, internal changes to a system component don’t3.\nrequire changes in other parts. Details of code updates are hidden behind stable\nAPIs. Each piece can evolve and improve separately.\n4.As a consequence of property 3, there is no waterfall, global release cycle for4.\nthe whole system. Instead, each component is updated separately as changes and\nimprovements are made.\nNotice that we are talking about technical systems . We need to think bigger. Let’s\ntranslate these technical characteristics into organizational characteristics:\n82 | Chapter 3: Designing Good Data Architecture",5060
32-Principle 9 Embrace FinOps.pdf,32-Principle 9 Embrace FinOps,"12Fowler, “Who Needs an Architect?”1.Many small teams engineer a large, complex system. Each team is tasked with1.\nengineering, maintaining, and improving some system components.\n2.These teams publish the abstract details of their components to other teams2.\nvia API definitions, message schemas, etc. Teams need not concern themselves\nwith other teams’ components; they simply use the published API or message\nspecifications to call these components. They iterate their part to improve their\nperformance and capabilities over time. They might also publish new capabilities\nas they are added or request new stuff from other teams. Again, the latter hap‐\npens without teams needing to worry about the internal technical details of the\nrequested features. Teams work together through loosely coupled communication .\n3.As a consequence of characteristic 2, each team can rapidly evolve and improve3.\nits component independently of the work of other teams.\n4.Specifically, characteristic 3 implies that teams can release updates to their com‐4.\nponents with minimal downtime. Teams release continuously during regular\nworking hours to make code changes and test them.\nLoose coupling of both technology and human systems will allow your data engineer‐\ning teams to more efficiently collaborate with one another and with other parts of the\ncompany. This principle also directly facilitates principle 7.\nPrinciple 7: Make Reversible Decisions\nThe data landscape is changing rapidly. Today’s hot technology or stack is tomorrow’s\nafterthought. Popular opinion shifts quickly. Y ou should aim for reversible decisions,\nas these tend to simplify your architecture and keep it agile.\nAs Fowler wrote, “One of an architect’s most important tasks is to remove architec‐\nture by finding ways to eliminate irreversibility in software designs. ”12 What was true\nwhen Fowler wrote this in 2003 is just as accurate today.\nAs we said previously, Bezos refers to reversible decisions as “two-way doors. ” As he\nsays, “If you walk through and don’t like what you see on the other side, you can’t\nget back to before. We can call these Type 1 decisions. But most decisions aren’t\nlike that—they are changeable, reversible—they’re two-way doors. ” Aim for two-way\ndoors whenever possible.\nGiven the pace of change—and the  decoupling/modularization of technologies across\nyour data architecture—always strive to pick the best-of-breed solutions that work for\ntoday. Also, be prepared to upgrade or adopt better practices as the landscape evolves.\nPrinciples of Good Data Architecture | 83\n13Tom Grey, “5 Principles for Cloud-Native Architecture—What It Is and How to Master It, ” Google Cloud\nblog, June 19, 2019, https://oreil.ly/4NkGf .\n14Amazon Web Services, “Security in AWS W AF, ” AWS W AF documentation, https://oreil.ly/rEFoU .Principle 8: Prioritize Security\nEvery  data engineer must assume responsibility for the security of the systems\nthey build and maintain. We focus now on two main ideas: zero-trust security\nand the shared responsibility security model. These align closely to a cloud-native\narchitecture.\nHardened-perimeter and zero-trust security models\nTo define zero-trust security , it’s helpful to start by understanding the traditional\nhard-perimeter security model and its limitations, as detailed in Google Cloud’s Five\nPrinciples:13\nTraditional architectures place a lot of faith in perimeter security, crudely a hard‐\nened network perimeter with “trusted things” inside and “untrusted things” outside.\nUnfortunately, this approach has always been vulnerable to insider attacks, as well as\nexternal threats such as spear phishing.\nThe 1996 film Mission Impossible  presents a perfect example of the hard-perimeter\nsecurity model and its limitations. In the movie, the CIA hosts highly sensitive data\non a storage system inside a room with extremely tight physical security. Ethan Hunt\ninfiltrates CIA headquarters and exploits a human target to gain physical access to the\nstorage system. Once inside the secure room, he can exfiltrate data with relative ease.\nFor at least a decade, alarming media reports have made us aware of the growing\nmenace of security breaches that exploit human targets inside hardened organiza‐\ntional security perimeters. Even as employees work on highly secure corporate\nnetworks, they remain connected to the outside world through email and mobile\ndevices. External threats effectively become internal threats.\nIn a cloud-native environment, the notion of a hardened perimeter erodes further. All\nassets are connected to the outside world to some degree. While virtual private cloud\n(VPC) networks can be defined with no external connectivity, the API control plane\nthat engineers use to define these networks still faces the internet.\nThe shared responsibility model\nAmazon  emphasizes the shared responsibility model , which divides security into the\nsecurity of the cloud and security in the cloud. AWS is responsible for the security of\nthe cloud:14\n84 | Chapter 3: Designing Good Data Architecture\n15Ericka Chickowski, “Leaky Buckets: 10 Worst Amazon S3 Breaches, ” Bitdefender Business Insights  blog,\nJan 24, 2018, https://oreil.ly/pFEFO .\n16FinOps Foundation, “What Is FinOps, ” https://oreil.ly/wJFVn .\n17J. R. Storment and Mike Fuller, Cloud FinOps  (Sebastapol, CA: O’Reilly, 2019), https://oreil.ly/QV6vF .AWS is responsible for protecting the infrastructure that runs AWS services in the\nAWS Cloud. AWS also provides you with services that you can use securely.\nAWS users are responsible for security in the cloud:\nY our responsibility is determined by the AWS service that you use. Y ou are also\nresponsible for other factors including the sensitivity of your data, your organization’s\nrequirements, and applicable laws and regulations.\nIn general, all cloud providers operate on some form of this shared responsibility\nmodel. They secure their services according to published specifications. Still, it is\nultimately the user’s responsibility to design a security model for their applications\nand data and leverage cloud capabilities to realize this model.\nData engineers as security engineers\nIn the corporate world today, a command-and-control approach to security is quite\ncommon, wherein security and networking teams manage perimeters and general\nsecurity practices. The cloud pushes this responsibility out to engineers who are not\nexplicitly in security roles. Because of this responsibility, in conjunction with more\ngeneral erosion of the hard security perimeter, all data engineers should consider\nthemselves security engineers.\nFailure to assume these new implicit responsibilities can lead to dire consequences.\nNumerous  data breaches have resulted from the simple error of configuring Amazon\nS3 buckets with public access.15 Those who handle data must assume that they are\nultimately responsible for securing it.\nPrinciple 9: Embrace FinOps\nLet’s start by considering a couple of definitions of FinOps. First, the FinOps Founda‐\ntion offers this:16\nFinOps is an evolving cloud financial management discipline and cultural practice that\nenables organizations to get maximum business value by helping engineering, finance,\ntechnology, and business teams to collaborate on data-driven spending decisions.\nIn addition, J. R. Sorment and Mike Fuller provide the following definition in Cloud\nFinOps :17\nThe term “FinOps” typically refers to the emerging professional movement that advo‐\ncates a collaborative working relationship between DevOps and Finance, resulting in\nPrinciples of Good Data Architecture | 85\nan iterative, data-driven management of infrastructure spending (i.e., lowering the\nunit economics of cloud) while simultaneously increasing the cost efficiency and,\nultimately, the profitability of the cloud environment.\nThe cost structure of data has evolved dramatically during the cloud era. In an\non-premises setting, data systems are generally acquired with a capital expenditure\n(described more in Chapter 4 ) for a new system every few years in an on-premises\nsetting. Responsible parties have to balance their budget against desired compute\nand storage capacity. Overbuying entails wasted money, while underbuying means\nhampering future data projects and driving significant personnel time to control\nsystem load and data size; underbuying may require faster technology refresh cycles,\nwith associated extra costs.\nIn the cloud era, most data systems are pay-as-you-go and readily scalable. Systems\ncan run on a cost-per-query model, cost-per-processing-capacity model, or another\nvariant of a pay-as-you-go model. This approach can be far more efficient than the\ncapital expenditure approach. It is now possible to scale up for high performance,\nand then scale down to save money. However, the pay-as-you-go approach makes\nspending far more dynamic. The new challenge for data leaders is to manage budgets,\npriorities, and efficiency.\nCloud tooling necessitates a set of processes for managing spending and resources. In\nthe past, data engineers thought in terms of performance engineering—maximizing\nthe performance for data processes on a fixed set of resources and buying adequate\nresources for future needs. With FinOps, engineers need to learn to think about the\ncost structures of cloud systems. For example, what is the appropriate mix of AWS\nspot instances when running a distributed cluster? What is the most appropriate\napproach for running a sizable daily job in terms of cost-effectiveness and perfor‐\nmance? When should the company switch from a pay-per-query model to reserved\ncapacity?\nFinOps evolves the operational monitoring model to monitor spending on an ongo‐\ning basis. Rather than simply monitor requests and CPU utilization for a web server,\nFinOps might monitor the ongoing cost of serverless functions handling traffic, as\nwell as spikes in spending trigger alerts. Just as systems are designed to fail gracefully\nin excessive traffic, companies may consider adopting hard limits for spending, with\ngraceful failure modes in response to spending spikes.\nOps teams should also think in terms of cost attacks. Just as a distributed denial-of-\nservice (DDoS) attack can block access to a web server, many companies have discov‐\nered to their chagrin that excessive downloads from S3 buckets can drive spending\nthrough the roof and threaten a small startup with bankruptcy. When sharing data\npublicly, data teams can address these issues by setting requester-pays policies, or\nsimply monitoring for excessive data access spending and quickly removing access if\nspending begins to rise to unacceptable levels.\n86 | Chapter 3: Designing Good Data Architecture",10801
33-Major Architecture Concepts.pdf,33-Major Architecture Concepts,,0
34-Distributed Systems Scalability and Designing for Failure.pdf,34-Distributed Systems Scalability and Designing for Failure,"18“FinOps Foundation Soars to 300 Members and Introduces New Partner Tiers for Cloud Service Providers\nand Vendors, ” Business Wire, June 17, 2019, https://oreil.ly/XcwYO .\n19Eric Evans, Domain-Driven Design Reference: Definitions  and Pattern Summaries  (March 2015),\nhttps://oreil.ly/pQ9oq .As of this writing, FinOps is a recently formalized practice. The FinOps Foundation\nwas started only in 2019.18 However, we highly recommend you start thinking about\nFinOps early, before you encounter high cloud bills. Start your journey with the\nFinOps Foundation  and O’Reilly’s Cloud FinOps . We also suggest that data engineers\ninvolve themselves in the community process of creating FinOps practices for data\nengineering— in such a new practice area, a good deal of territory is yet to be mapped\nout.\nNow that you have a high-level understanding of good data architecture principles,\nlet’s dive a bit deeper into the major concepts you’ll need to design and build good\ndata architecture.\nMajor Architecture Concepts\nIf you follow the current trends in data, it seems like new types of data tools and\narchitectures are arriving on the scene every week. Amidst this flurry of activity, we\nmust not lose sight of the main goal of all of these architectures: to take data and\ntransform it into something useful for downstream consumption.\nDomains and Services\nDomain: A sphere of knowledge, influence, or activity. The subject area to which the\nuser applies a program is the domain of the software.\n—Eric Evans19\nBefore  diving into the components of the architecture, let’s briefly cover two terms\nyou’ll see come up very often: domain and services. A domain  is the real-world\nsubject area for which you’re architecting. A service  is a set of functionality whose\ngoal is to accomplish a task. For example, you might have a sales order-processing\nservice whose task is to process orders as they are created. The sales order-processing\nservice’s only job is to process orders; it doesn’t provide other functionality, such as\ninventory management or updating user profiles.\nA domain can contain multiple services. For example, you might have a sales domain\nwith three services: orders, invoicing, and products. Each service has particular tasks\nthat support the sales domain. Other domains may also share services ( Figure 3-3 ).\nIn this case, the accounting domain is responsible for basic accounting functions:\ninvoicing, payroll, and accounts receivable (AR). Notice the accounting domain\nshares the invoice service with the sales domain since a sale generates an invoice,\nMajor Architecture Concepts | 87\nand accounting must keep track of invoices to ensure that payment is received. Sales\nand accounting own their respective domains.\nFigure 3-3. Two domains (sales and accounting) share a common service (invoices), and\nsales and accounting own their respective domains\nWhen thinking about what constitutes a domain, focus on what the domain repre‐\nsents in the real world and work backward. In the preceding example, the sales\ndomain should represent what happens with the sales function in your company.\nWhen architecting the sales domain, avoid cookie-cutter copying and pasting from\nwhat other companies do. Y our company’s sales function likely has unique aspects\nthat require specific services to make it work the way your sales team expects.\nIdentify what should go in the domain. When determining what the domain should\nencompass and what services to include, the best advice is to simply go and talk with\nusers and stakeholders, listen to what they’re saying, and build the services that will\nhelp them do their job. Avoid the classic trap of architecting in a vacuum.\nDistributed Systems, Scalability, and Designing for Failure\nThe discussion in this section is related to our second and third principles of data\nengineering architecture discussed previously: plan for failure and architect for scala‐\nbility. As data engineers, we’re interested in four closely related characteristics of data\nsystems (availability and reliability were mentioned previously, but we reiterate them\nhere for completeness):\nScalability\nAllows  us to increase the capacity of a system to improve performance and\nhandle the demand. For example, we might want to scale a system to handle a\nhigh rate of queries or process a huge data set.\nElasticity\nThe ability of a scalable system to scale dynamically; a highly elastic system can\nautomatically scale up and down based on the current workload. Scaling up is\ncritical as demand increases, while scaling down saves money in a cloud environ‐\nment. Modern systems sometimes scale to zero, meaning they can automatically\nshut down when idle.\n88 | Chapter 3: Designing Good Data Architecture\nAvailability\nThe percentage of time an IT service or component is in an operable state.\nReliability\nThe system’s probability of meeting defined standards in performing its intended\nfunction during a specified interval.\nSee PagerDuty’s “Why Are Availability and Reliability Crucial?”\nweb page  for definitions and background on availability and relia‐\nbility.\nHow are these characteristics related? If a system fails to meet performance\nrequirements during a specified interval, it may become unresponsive. Thus low\nreliability can lead to low availability. On the other hand, dynamic scaling helps\nensure adequate  performance without manual intervention from engineers—elastic‐\nity improves reliability.\nScalability can be realized in a variety of ways. For your services and domains, does\na single machine handle everything? A single machine can be scaled vertically; you\ncan increase resources (CPU, disk, memory, I/O). But there are hard limits to possible\nresources on a single machine. Also, what happens if this machine dies? Given\nenough time, some components will eventually fail. What’s your plan for backup and\nfailover? Single machines generally can’t offer high availability and reliability.\nWe utilize a distributed system to realize higher overall scaling capacity and increased\navailability and reliability. Horizontal scaling  allows you to add more machines to\nsatisfy load and resource requirements ( Figure 3-4 ). Common horizontally scaled\nsystems have a leader node that acts as the main point of contact for the instantiation,\nprogress, and completion of workloads. When a workload is started, the leader node\ndistributes tasks to the worker nodes within its system, completing the tasks and\nreturning the results to the leader node. Typical modern distributed architectures also\nbuild in redundancy. Data is replicated so that if a machine dies, the other machines\ncan pick up where the missing server left off; the cluster may add more machines to\nrestore capacity.\nDistributed systems are widespread in the various data technologies you’ll use across\nyour architecture. Almost every cloud data warehouse object storage system you\nuse has some notion of distribution under the hood. Management details of the\ndistributed system are typically abstracted away, allowing you to focus on high-level\narchitecture instead of low-level plumbing. However, we highly recommend that\nyou learn more about distributed systems because these details can be extremely\nhelpful in understanding and improving the performance of your pipelines; Martin\nKleppmann’s Designing Data-Intensive Applications  (O’Reilly) is an excellent resource.\nMajor Architecture Concepts | 89",7508
35-Tight Versus Loose Coupling Tiers Monoliths and Microservices.pdf,35-Tight Versus Loose Coupling Tiers Monoliths and Microservices,"Figure 3-4. A simple horizontal distributed system utilizing a leader-follower architec‐\nture, with one leader node and three worker nodes\nTight Versus Loose Coupling: Tiers, Monoliths, and Microservices\nWhen  designing a data architecture, you choose how much interdependence you\nwant to include within your various domains, services, and resources. On one end\nof the spectrum, you can choose to have extremely centralized dependencies and\nworkflows. Every part of a domain and service is vitally dependent upon every other\ndomain and service. This pattern is known as tightly coupled .\nOn the other end of the spectrum, you have decentralized domains and services that\ndo not have strict dependence on each other, in a pattern known as loose coupling .\nIn a loosely coupled scenario, it’s easy for decentralized teams to build systems whose\ndata may not be usable by their peers. Be sure to assign common standards, owner‐\nship, responsibility, and accountability to the teams owning their respective domains\nand services. Designing “good” data architecture relies on trade-offs between the tight\nand loose coupling of domains and services.\nIt’s worth noting that many of the ideas in this section originate in software develop‐\nment. We’ll try to retain the context of these big ideas’ original intent and spirit—\nkeeping them agnostic of data—while later explaining some differences you should be\naware of when applying these concepts to data specifically.\nArchitecture tiers\nAs you develop your architecture, it helps to be aware of architecture tiers. Y our\narchitecture has layers—data, application, business logic, presentation, and so forth\n—and you need to know how to decouple these layers. Because tight coupling of\nmodalities presents obvious vulnerabilities, keep in mind how you structure the\nlayers of your architecture to achieve maximum reliability and flexibility. Let’s look at\nsingle-tier and multitier architecture.\nSingle tier.    In a single-tier architecture , your database and application are tightly\ncoupled, residing on a single server ( Figure 3-5 ). This server could be your laptop\nor a single virtual machine (VM) in the cloud. The tightly coupled nature means if\n90 | Chapter 3: Designing Good Data Architecture\nthe server, the database, or the application fails, the entire architecture fails. While\nsingle-tier architectures are good for prototyping and development, they are not\nadvised for production environments because of the obvious failure risks.\nFigure 3-5. Single-tier architecture\nEven when single-tier architectures build in redundancy (for example, a failover\nreplica), they present significant limitations in other ways. For instance, it is often\nimpractical (and not advisable) to run analytics queries against production applica‐\ntion databases. Doing so risks overwhelming the database and causing the application\nto become unavailable. A single-tier architecture is fine for testing systems on a local\nmachine but is not advised for production uses.\nMultitier.    The challenges of a tightly coupled single-tier architecture are solved by\ndecoupling the data and application. A multitier  (also known as n-tier ) architecture\nis composed of separate layers: data, application, business logic, presentation, etc.\nThese layers are bottom-up and hierarchical, meaning the lower layer isn’t necessarily\ndependent on the upper layers; the upper layers depend on the lower layers. The\nnotion is to separate data from the application, and application from the presentation.\nA common multitier architecture is a three-tier architecture, a widely used client-\nserver design. A three-tier architecture  consists of data, application logic, and presen‐\ntation tiers ( Figure 3-6 ). Each tier is isolated from the other, allowing for separation of\nconcerns. With a three-tier architecture, you’re free to use whatever technologies you\nprefer within each tier without the need to be monolithically focused.\nFigure 3-6. A three-tier architecture\nMajor Architecture Concepts | 91\nWe’ve seen many single-tier architectures in production. Single-tier architectures\noffer simplicity but also severe limitations. Eventually, an organization or application\noutgrows this arrangement; it works well until it doesn’t. For instance, in a single-tier\narchitecture, the data and logic layers share and compete for resources (disk, CPU,\nand memory) in ways that are simply avoided in a multitier architecture. Resources\nare spread across various tiers. Data engineers should use tiers to evaluate their\nlayered architecture and the way dependencies are handled. Again, start simple and\nbake in evolution to additional tiers as your architecture becomes more complex.\nIn a multitier architecture, you need to consider separating your layers and the\nway resources are shared within layers when working with a distributed system.\nDistributed systems under the hood power many technologies you’ll encounter across\nthe data engineering lifecycle. First, think about whether you want resource conten‐\ntion with your nodes. If not, exercise a shared-nothing architecture : a single node\nhandles each request, meaning other nodes do not share resources such as memory,\ndisk, or CPU with this node or with each other. Data and resources are isolated to the\nnode. Alternatively, various nodes can handle multiple requests and share resources\nbut at the risk of resource contention. Another consideration is whether nodes should\nshare the same disk and memory accessible by all nodes. This is called a  shared disk\narchitecture  and is common when you want shared resources if a random node failure\noccurs.\nMonoliths\nThe general notion of a monolith includes as much as possible under one roof; in its\nmost extreme version, a monolith consists of a single codebase running on a single\nmachine that provides both the application logic and user interface.\nCoupling within monoliths can be viewed in two ways: technical coupling and\ndomain coupling. Technical coupling  refers to architectural tiers, while domain cou‐\npling  refers to the way domains are coupled together. A monolith has varying degrees\nof coupling among technologies and domains. Y ou could have an application with\nvarious layers decoupled in a multitier architecture but still share multiple domains.\nOr, you could have a single-tier architecture serving a single domain.\nThe tight coupling of a monolith implies a lack of modularity of its components.\nSwapping out or upgrading components in a monolith is often an exercise in trading\none pain for another. Because of the tightly coupled nature, reusing components\nacross the architecture is difficult or impossible. When evaluating how to improve\na monolithic architecture, it’s often a game of whack-a-mole: one component is\nimproved, often at the expense of unknown consequences with other areas of the\nmonolith.\nData teams will often ignore solving the growing complexity of their monolith, letting\nit devolve into a big ball of mud .\n92 | Chapter 3: Designing Good Data Architecture\nChapter 4  provides a more extensive discussion comparing monoliths to distributed\ntechnologies. We also discuss the distributed monolith , a strange hybrid that emerges\nwhen engineers build distributed systems with excessive tight coupling.\nMicroservices\nCompared with the attributes of a monolith—interwoven services, centralization, and\ntight coupling among services—microservices are the polar opposite. Microservices\narchitecture  comprises separate, decentralized, and loosely coupled services. Each\nservice has a specific function and is decoupled from other services operating within\nits domain. If one service temporarily goes down, it won’t affect the ability of other\nservices to continue functioning.\nA question that comes up often is how to convert your monolith into many micro‐\nservices ( Figure 3-7 ). This completely depends on how complex your monolith is\nand how much effort it will be to start extracting services out of it. It’s entirely\npossible that your monolith cannot be broken apart, in which case, you’ll want\nto start creating a new parallel architecture that has the services decoupled in a\nmicroservices-friendly manner. We don’t suggest an entire refactor but instead break\nout services. The monolith didn’t arrive overnight and is a technology issue as an\norganizational one. Be sure you get buy-in from stakeholders of the monolith if you\nplan to break it apart.\nFigure 3-7. An extremely monolithic architecture runs all functionality inside a single\ncodebase, potentially colocating a database on the same host server\nIf you’ d like to learn more about breaking apart a monolith, we suggest reading the\nfantastic, pragmatic guide Software  Architecture: The Hard Parts  by Neal Ford et al.\n(O’Reilly).\nMajor Architecture Concepts | 93",8909
36-Brownfield Versus Greenfield Projects.pdf,36-Brownfield Versus Greenfield Projects,"Considerations for data architecture\nAs we mentioned at the start of this section, the concepts of tight versus loose\ncoupling stem from software development, with some of these concepts dating back\nover 20 years. Though architectural practices in data are now adopting those from\nsoftware development, it’s still common to see very monolithic, tightly coupled data\narchitectures. Some of this is due to the nature of existing data technologies and the\nway they integrate.\nFor example, data pipelines might consume data from many sources ingested into a\ncentral data warehouse. The central data warehouse is inherently monolithic. A move\ntoward a microservices equivalent with a data warehouse is to decouple the workflow\nwith domain-specific data pipelines connecting to corresponding domain-specific\ndata warehouses. For example, the sales data pipeline connects to the sales-specific\ndata warehouse, and the inventory and product domains follow a similar pattern.\nRather than dogmatically preach microservices over monoliths (among other argu‐\nments), we suggest you pragmatically use loose coupling as an ideal, while recogniz‐\ning the state and limitations of the data technologies you’re using within your data\narchitecture. Incorporate reversible technology choices that allow for modularity and\nloose coupling whenever possible.\nAs you can see in Figure 3-7 , you separate the components of your architecture into\ndifferent layers of concern in a vertical fashion. While a multitier architecture solves\nthe technical challenges of decoupling shared resources, it does not address the com‐\nplexity of sharing domains. Along the lines of single versus multitiered architecture,\nyou should also consider how you separate the domains of your data architecture. For\nexample, your analyst team might rely on data from sales and inventory. The sales\nand inventory domains are different and should be viewed as separate.\nOne approach to this problem is centralization: a single team is responsible for\ngathering data from all domains and reconciling it for consumption across the orga‐\nnization. (This is a common approach in traditional data warehousing.) Another\napproach is the data mesh . With the data mesh, each software team is responsible for\npreparing its data for consumption across the rest of the organization. We’ll say more\nabout the data mesh later in this chapter.\nOur advice: monoliths aren’t necessarily bad, and it might make sense to start with\none under certain conditions. Sometimes you need to move fast, and it’s much\nsimpler to start with a monolith. Just be prepared to break it into smaller pieces\neventually; don’t get too comfortable.\nUser Access: Single Versus Multitenant\nAs a data engineer, you have to make decisions about sharing systems across mul‐\ntiple teams, organizations, and customers. In some sense, all  cloud services are\n94 | Chapter 3: Designing Good Data Architecture\nmultitenant,  although this multitenancy occurs at various grains. For example, a\ncloud compute instance is usually on a shared server, but the VM itself provides\nsome degree of isolation. Object storage is a multitenant system, but cloud vendors\nguarantee security and isolation so long as customers configure their permissions\ncorrectly.\nEngineers frequently need to make decisions about multitenancy at a much smaller\nscale. For example, do multiple departments in a large company share the same data\nwarehouse? Does the organization share data for multiple large customers within the\nsame table?\nWe have two factors to consider in multitenancy: performance and security. With\nmultiple large tenants within a cloud system, will the system support consistent\nperformance for all tenants, or will there be a noisy neighbor problem? (That is,\nwill high usage from one tenant degrade performance for other tenants?) Regarding\nsecurity, data from different tenants must be properly isolated. When a company has\nmultiple external customer tenants, these tenants should not be aware of one another,\nand engineers must prevent data leakage. Strategies for data isolation vary by system.\nFor instance, it is often perfectly acceptable to use multitenant tables and isolate\ndata through views. However, you must make certain that these views cannot leak\ndata. Read vendor or project documentation to understand appropriate strategies and\nrisks.\nEvent-Driven Architecture\nY our  business is rarely static. Things often happen in your business, such as getting\na new customer, a new order from a customer, or an order for a product or service.\nThese are all examples of events  that are broadly defined as something that happened,\ntypically a change in the state  of something. For example, a new order might be\ncreated by a customer, or a customer might later make an update to this order.\nAn event-driven workflow ( Figure 3-8 ) encompasses the ability to create, update,\nand asynchronously move events across various parts of the data engineering lifecy‐\ncle. This workflow boils down to three main areas: event production, routing, and\nconsumption. An event must be produced and routed to something that consumes\nit without tightly coupled dependencies among the producer, event router, and\nconsumer.\nFigure 3-8. In an event-driven workflow,  an event is produced, routed, and then\nconsumed\nMajor Architecture Concepts | 95\nAn event-driven architecture ( Figure 3-9 ) embraces the event-driven workflow and\nuses this to communicate across various services. The advantage of an event-driven\narchitecture is that it distributes the state of an event across multiple services. This\nis helpful if a service goes offline, a node fails in a distributed system, or you’ d\nlike multiple consumers or services to access the same events. Anytime you have\nloosely coupled services, this is a candidate for event-driven architecture. Many of\nthe examples we describe later in this chapter incorporate some form of event-driven\narchitecture.\nY ou’ll learn more about event-driven streaming and messaging systems in Chapter 5 .\nFigure 3-9. In an event-driven architecture, events are passed between loosely coupled\nservices\nBrownfield  Versus Greenfield  Projects\nBefore  you design your data architecture project, you need to know whether you’re\nstarting with a clean slate or redesigning an existing architecture. Each type of project\nrequires assessing trade-offs, albeit with different considerations and approaches.\nProjects roughly fall into two buckets: brownfield and greenfield.\nBrownfield  projects\nBrownfield  projects  often involve refactoring and reorganizing an existing architecture\nand are constrained by the choices of the present and past. Because a key part\nof architecture is change management, you must figure out a way around these\nlimitations and design a path forward to achieve your new business and technical\nobjectives. Brownfield projects require a thorough understanding of the legacy archi‐\ntecture and the interplay of various old and new technologies. All too often, it’s\neasy to criticize a prior team’s work and decisions, but it is far better to dig deep,\nask questions, and understand why decisions were made. Empathy and context go a\nlong way in helping you diagnose problems with the existing architecture, identify\nopportunities, and recognize pitfalls.\nY ou’ll need to introduce your new architecture and technologies and deprecate the\nold stuff at some point. Let’s look at a couple of popular approaches. Many teams\njump headfirst into an all-at-once or big-bang overhaul of the old architecture, often\nfiguring out deprecation as they go. Though popular, we don’t advise this approach\n96 | Chapter 3: Designing Good Data Architecture\n20Martin Fowler, “StranglerFigApplication, ” June 29, 2004, https://oreil.ly/PmqxB .\n21Mike Loukides, “Resume Driven Development, ” O’Reilly Radar , October 13, 2004, https://oreil.ly/BUHa8 .because of the associated risks and lack of a plan. This path often leads to disaster,\nwith many irreversible and costly decisions. Y our job is to make reversible, high-ROI\ndecisions.\nA popular alternative to a direct rewrite is the strangler pattern: new systems slowly\nand incrementally replace a legacy architecture’s components.20 Eventually, the legacy\narchitecture is completely replaced. The attraction to the strangler pattern is its\ntarge ted and surgical approach of deprecating one piece of a system at a time.\nThis allows for flexible and reversible decisions while assessing the impact of the\ndeprecation on dependent systems.\nIt’s important to note that deprecation might be “ivory tower” advice and not practi‐\ncal or achievable. Eradicating legacy technology or architecture might be impossible\nif you’re at a large organization. Someone, somewhere, is using these legacy compo‐\nnents. As someone once said, “Legacy is a condescending way to describe something\nthat makes money. ”\nIf you can deprecate, understand there are numerous ways to deprecate your old\narchitecture. It is critical to demonstrate value on the new platform by gradually\nincreasing its maturity to show evidence of success and then follow an exit plan to\nshut down old systems.\nGreenfield  projects\nOn the opposite end of the spectrum, a greenfield  project  allows you to pioneer a\nfresh start, unconstrained by the history or legacy of a prior architecture. Greenfield\nprojects tend to be easier than brownfield projects, and many data architects and\nengineers find them more fun! Y ou have the opportunity to try the newest and\ncoolest tools and architectural patterns. What could be more exciting?\nY ou should watch out for some things before getting too carried away. We see teams\nget overly exuberant with shiny object syndrome. They feel compelled to reach for the\nlatest and greatest technology fad without understanding how it will impact the value\nof the project. There’s also a temptation to do resume-driven development , stacking\nup impressive new technologies without prioritizing the project’s ultimate goals.21\nAlways prioritize requirements over building something cool.\nWhether you’re working on a brownfield or greenfield project, always focus on the\ntenets of “good” data architecture. Assess trade-offs, make flexible and reversible\ndecisions, and strive for positive ROI.\nMajor Architecture Concepts | 97",10420
37-Examples and Types of Data Architecture.pdf,37-Examples and Types of Data Architecture,,0
38-Data Warehouse.pdf,38-Data Warehouse,"22H. W . Inmon, Building the Data Warehouse  (Hoboken: Wiley, 2005).Now, we’ll look at examples and types of architectures—some established for dec‐\nades (the data warehouse), some brand-new (the data lakehouse), and some that\nquickly came and went but still influence current architecture patterns (Lambda\narchitecture).\nExamples and Types of Data Architecture\nBecause  data architecture is an abstract discipline, it helps to reason by example. In\nthis section, we outline prominent examples and types of data architecture that are\npopular today. Though this set of examples is by no means exhaustive, the intention is\nto expose you to some of the most common data architecture patterns and to get you\nthinking about the requisite flexibility and trade-off analysis needed when designing\na good architecture for your use case.\nData Warehouse\nA data warehouse  is a central data hub used for reporting and analysis. Data in a\ndata warehouse is typically highly formatted and structured for analytics use cases. It’s\namong the oldest and most well-established data architectures.\nIn 1989, Bill Inmon originated the notion of the data warehouse, which he described\nas “a subject-oriented, integrated, nonvolatile, and time-variant collection of data\nin support of management’s decisions. ”22 Though technical aspects of the data ware‐\nhouse have evolved significantly, we feel this original definition still holds its weight\ntoday.\nIn the past, data warehouses were widely used at enterprises with significant budgets\n(often in the millions of dollars) to acquire data systems and pay internal teams\nto provide ongoing support to maintain the data warehouse. This was expensive\nand labor-intensive. Since then, the scalable, pay-as-you-go model has made cloud\ndata warehouses accessible even to tiny companies. Because a third-party provider\nmanages the data warehouse infrastructure, companies can do a lot more with fewer\npeople, even as the complexity of their data grows.\nIt’s worth noting two types of data warehouse architecture: organizational and tech‐\nnical. The organizational data warehouse architecture  organizes data associated with\ncertain business team structures and processes. The  technical data warehouse architec‐\nture reflects the technical nature of the data warehouse, such as MPP . A company\ncan have a data warehouse without an MPP system or run an MPP system that\nis not organized as a data warehouse. However, the technical and organizational\narchitectures have existed in a virtuous cycle and are frequently identified with each\nother.\n98 | Chapter 3: Designing Good Data Architecture\nThe organizational data warehouse architecture has two main characteristics:\nSeparates online analytical processing (OLAP) from production databases (online trans‐\naction processing)\nThis separation is critical as businesses grow. Moving data into a separate physi‐\ncal system directs load away from production systems and improves analytics\nperformance.\nCentralizes and organizes data\nTraditionally, a data warehouse pulls data from application systems by using\nETL. The extract phase pulls data from source systems. The transformation\nphase cleans and standardizes data, organizing and imposing business logic in a\nhighly modeled form. ( Chapter 8  covers transformations and data models.) The\nload phase pushes data into the data warehouse target database system. Data is\nloaded into multiple data marts that serve the analytical needs for specific lines\nor business and departments. Figure 3-10  shows the general workflow. The data\nwarehouse and ETL go hand in hand with specific business structures, including\nDBA and ETL developer teams that implement the direction of business leaders\nto ensure that data for reporting and analytics corresponds to business processes.\nFigure 3-10. Basic data warehouse with ETL\nRegarding the technical data warehouse architecture, the first MPP systems in the\nlate 1970s became popular in the 1980s. MPPs support essentially the same SQL\nsemantics used in relational application databases. Still, they are optimized to scan\nmassive amounts of data in parallel and thus allow high-performance aggregation\nand statistical calculations. In recent years, MPP systems have increasingly shifted\nfrom a row-based to a columnar architecture to facilitate even larger data and queries,\nespecially in cloud data warehouses. MPPs are indispensable for running performant\nqueries for large enterprises as data and reporting needs grow.\nOne  variation on ETL is ELT. With the ELT data warehouse architecture, data gets\nmoved more or less directly from production systems into a staging area in the data\nwarehouse. Staging in this setting indicates that the data is in a raw form. Rather than\nusing an external system, transformations are handled directly in the data warehouse.\nThe intention is to take advantage of the massive computational power of cloud data\nExamples and Types of Data Architecture | 99\nwarehouses and data processing tools. Data is processed in batches, and transformed\noutput is written into tables and views for analytics. Figure 3-11  shows the general\nprocess. ELT is also popular in a streaming arrangement, as events are streamed from\na CDC process, stored in a staging area, and then subsequently transformed within\nthe data warehouse.\nFigure 3-11. ELT—extract, load, and transform\nA second version of ELT was popularized during big data growth in the Hadoop\necosystem. This is transform-on-read ELT , which we discuss in “Data Lake”  on page\n101.\nThe cloud data warehouse\nCloud data warehouses  represent  a significant evolution of the on-premises data\nwarehouse architecture and have thus led to significant changes to the organiza‐\ntional architecture. Amazon Redshift kicked off the cloud data warehouse revolution.\nInstead of needing to appropriately size an MPP system for the next several years and\nsign a multimillion-dollar contract to procure the system, companies had the option\nof spinning up a Redshift cluster on demand, scaling it up over time as data and\nanalytics demand grew. They could even spin up new Redshift clusters on demand\nto serve specific workloads and quickly delete clusters when they were no longer\nneeded.\nGoogle BigQuery, Snowflake, and other competitors popularized the idea of separat‐\ning compute from storage. In this architecture, data is housed in object storage,\nallowing virtually limitless storage. This also gives users the option to spin up\ncomputing power on demand, providing ad hoc big data capabilities without the\nlong-term cost of thousands of nodes.\nCloud data warehouses expand the capabilities of MPP systems to cover many big\ndata use cases that required a Hadoop cluster in the very recent past. They can readily\nprocess petabytes of data in a single query. They typically support data structures that\nallow the storage of tens of megabytes of raw text data per row or extremely rich and\ncomplex JSON documents. As cloud data warehouses (and data lakes) mature, the\nline between the data warehouse and the data lake will continue to blur.\n100 | Chapter 3: Designing Good Data Architecture",7212
39-Kappa Architecture.pdf,39-Kappa Architecture,"So significant is the impact of the new capabilities offered by cloud data warehouses\nthat we might consider jettisoning the term data warehouse  altogether. Instead, these\nservices are evolving into a new data platform with much broader capabilities than\nthose offered by a traditional MPP system.\nData marts\nA data mart  is a more refined subset of a warehouse designed to serve analytics and\nreporting, focused on a single suborganization, department, or line of business; every\ndepartment has its own data mart, specific to its needs. This is in contrast to the full\ndata warehouse that serves the broader organization or business.\nData marts exist for two reasons. First, a data mart makes data more easily accessible\nto analysts and report developers. Second, data marts provide an additional stage\nof transformation beyond that provided by the initial ETL or ELT pipelines. This\ncan significantly improve performance if reports or analytics queries require complex\njoins and aggregations of data, especially when the raw data is large. Transform\nprocesses can populate the data mart with joined and aggregated data to improve\nperformance for live queries. Figure 3-12  shows the general workflow. We discuss\ndata marts, and modeling data for data marts, in Chapter 8 .\nFigure 3-12. ETL or ELT plus data marts\nData Lake\nAmong  the most popular architectures that appeared during the big data era is the\ndata lake . Instead of imposing tight structural limitations on data, why not simply\ndump all of your data—structured and unstructured—into a central location? The\ndata lake promised to be a democratizing force, liberating the business to drink from\na fountain of limitless data. The first-generation data lake, “data lake 1.0, ” made solid\ncontributions but generally failed to deliver on its promise.\nData lake 1.0 started with HDFS. As the cloud grew in popularity, these data lakes\nmoved to cloud-based object storage, with extremely cheap storage costs and virtually\nlimitless storage capacity. Instead of relying on a monolithic data warehouse where\nstorage and compute are tightly coupled, the data lake allows an immense amount\nof data of any size and type to be stored. When this data needs to be queried or\nExamples and Types of Data Architecture | 101\ntransformed, you have access to nearly unlimited computing power by spinning up a\ncluster on demand, and you can pick your favorite data-processing technology for the\ntask at hand—MapReduce, Spark, Ray, Presto, Hive, etc.\nDespite the promise and hype, data lake 1.0 had serious shortcomings. The data\nlake became a dumping ground; terms such as data swamp , dark data , and WORN\nwere coined as once-promising data projects failed. Data grew to unmanageable\nsizes, with little in the way of schema management, data cataloging, and discovery\ntools. In addition, the original data lake concept was essentially write-only, creating\nhuge headaches with the arrival of regulations such as GDPR that required targeted\ndeletion of user records.\nProcessing data was also challenging. Relatively banal data transformations such as\njoins were a huge headache to code as MapReduce jobs. Later frameworks such as\nPig and Hive somewhat improved the situation for data processing but did little\nto address the basic problems of data management. Simple data manipulation lan‐\nguage (DML) operations common in SQL—deleting or updating rows—were painful\nto implement, generally achieved by creating entirely new tables. While big data\nengineers radiated a particular disdain for their counterparts in data warehousing,\nthe latter could point out that data warehouses provided basic data management\ncapabilities out of the box, and that SQL was an efficient tool for writing complex,\nperformant queries and transformations.\nData lake 1.0 also failed to deliver on another core promise of the big data movement.\nOpen source software in the Apache ecosystem was touted as a means to avoid\nmultimillion-dollar contracts for proprietary MPP systems. Cheap, off-the-shelf\nhardware would replace custom vendor solutions. In reality, big data costs ballooned\nas the complexities of managing Hadoop clusters forced companies to hire large\nteams of engineers at high salaries. Companies often chose to purchase licensed,\ncustomized versions of Hadoop from vendors to avoid the exposed wires and sharp\nedges of the raw Apache codebase and acquire a set of scaffolding tools to make\nHadoop more user-friendly. Even companies that avoided managing Hadoop clusters\nusing cloud storage had to spend big on talent to write MapReduce jobs.\nWe should be careful not to understate the utility and power of first-generation\ndata lakes. Many organizations found significant value in data lakes—especially huge,\nheavily data-focused Silicon Valley tech companies like Netflix and Facebook. These\ncompanies had the resources to build successful data practices and create their cus‐\ntom Hadoop-based tools and enhancements. But for many organizations, data lakes\nturned into an internal superfund site of waste, disappointment, and spiraling costs.\nConvergence, Next-Generation Data Lakes, and the Data Platform\nIn response to the limitations of first-generation data lakes, various players have\nsought to enhance the concept to fully realize its promise. For example, Databricks\n102 | Chapter 3: Designing Good Data Architecture\nintroduced the notion of a data lakehouse . The lakehouse incorporates the controls,\ndata management, and data structures found in a data warehouse while still housing\ndata in object storage and supporting a variety of query and transformation engines.\nIn particular, the data lakehouse supports atomicity, consistency, isolation, and dura‐\nbility (ACID) transactions, a big departure from the original data lake, where you\nsimply pour in data and never update or delete it. The term data lakehouse  suggests a\nconvergence between data lakes and data warehouses.\nThe technical architecture of cloud data warehouses has evolved to be very similar to\na data lake architecture. Cloud data warehouses separate compute from storage, sup‐\nport petabyte-scale queries, store a variety of unstructured data and semistructured\nobjects, and integrate with advanced processing technologies such as Spark or Beam.\nWe believe that the trend of convergence will only continue. The data lake and the\ndata warehouse will still exist as different architectures. In practice, their capabilities\nwill converge so that few users will notice a boundary between them in their day-to-\nday work. We now see several vendors offering data platforms  that combine data\nlake and data warehouse capabilities. From our perspective, AWS, Azure, Google\nCloud , Snowflake , and Databricks are class leaders, each offering a constellation of\ntightly integrated tools for working with data, running the gamut from relational to\ncompletely unstructured. Instead of choosing between a data lake or data warehouse\narchitecture, future data engineers will have the option to choose a converged data\nplatform based on a variety of factors, including vendor, ecosystem, and relative\nopenness.\nModern Data Stack\nThe modern data stack  (Figure 3-13 ) is currently a trendy analytics architecture\nthat highlights the type of abstraction we expect to see more widely used over the\nnext several years. Whereas past data stacks relied on expensive, monolithic toolsets,\nthe main objective of the modern data stack is to use cloud-based, plug-and-play,\neasy-to-use, off-the-shelf components to create a modular and cost-effective data\narchitecture. These components include data pipelines, storage, transformation, data\nmanagement/governance, monitoring, visualization, and exploration. The domain is\nstill in flux, and the specific tools are changing and evolving rapidly, but the core aim\nwill remain the same: to reduce complexity and increase modularization. Note that\nthe notion of a modern data stack integrates nicely with the converged data platform\nidea from the previous section.\nFigure 3-13. Basic components of the modern data stack\nExamples and Types of Data Architecture | 103\nKey outcomes of the modern data stack are self-service (analytics and pipelines),\nagile data management, and using open source tools or simple proprietary tools with\nclear pricing structures. Community is a central aspect of the modern data stack as\nwell. Unlike products of the past that had releases and roadmaps largely hidden from\nusers, projects and companies operating in the modern data stack space typically\nhave strong user bases and active communities that participate in the development by\nusing the product early, suggesting features, and submitting pull requests to improve\nthe code.\nRegardless of where “modern” goes (we share our ideas in Chapter 11 ), we think\nthe key concept of plug-and-play modularity with easy-to-understand pricing and\nimplementation is the way of the future. Especially in analytics engineering, the\nmodern data stack is and will continue to be the default choice of data architecture.\nThroughout the book, the architecture we reference contains pieces of the modern\ndata stack, such as cloud-based and plug-and-play modular components.\nLambda Architecture\nIn the “old days” (the early to mid-2010s), the popularity of working with streaming\ndata exploded with the emergence of Kafka as a highly scalable message queue\nand frameworks such as Apache Storm and Samza for streaming/real-time analyt‐\nics. These technologies allowed companies to perform new types of analytics and\nmodeling on large amounts of data, user aggregation and ranking, and product\nrecommendations. Data engineers needed to figure out how to reconcile batch and\nstreaming data into a single architecture. The Lambda architecture was one of the\nearly popular responses to this problem.\nIn a Lambda architecture  (Figure 3-14 ), you have systems operating independently of\neach other—batch, streaming, and serving. The source system is ideally immutable\nand append-only, sending data to two destinations for processing: stream and batch.\nIn-stream processing intends to serve the data with the lowest possible latency in a\n“speed” layer, usually a NoSQL database. In the batch layer, data is processed and\ntransformed in a system such as a data warehouse, creating precomputed and aggre‐\ngated views of the data. The serving layer provides a combined view by aggregating\nquery results from the two layers.\nFigure 3-14. Lambda architecture\n104 | Chapter 3: Designing Good Data Architecture",10639
40-Architecture for IoT.pdf,40-Architecture for IoT,"23Jay Kreps, “Questioning the Lambda Architecture, ” O’Reilly Radar , July 2, 2014, https://oreil.ly/wWR3n .Lambda architecture has its share of challenges and criticisms. Managing multiple\nsystems with different codebases is as difficult as it sounds, creating error-prone\nsystems with code and data that are extremely difficult to reconcile.\nWe mention Lambda architecture because it still gets attention and is popular in\nsearch-engine results for data architecture. Lambda isn’t our first recommendation\nif you’re trying to combine streaming and batch data for analytics. Technology and\npractices have moved on.\nNext, let’s look at a reaction to Lambda architecture, the Kappa architecture.\nKappa Architecture\nAs a response to the shortcomings of Lambda architecture, Jay Kreps proposed an\nalternative called Kappa architecture  (Figure 3-15 ).23 The central thesis is this: why not\njust use a stream-processing platform as the backbone for all data handling—inges‐\ntion, storage, and serving? This facilitates a true event-based architecture. Real-time\nand batch processing can be applied seamlessly to the same data by reading the live\nevent stream directly and replaying large chunks of data for batch processing.\nFigure 3-15. Kappa architecture\nThough the original Kappa architecture article came out in 2014, we haven’t seen it\nwidely adopted. There may be a couple of reasons for this. First, streaming itself is\nstill a bit of a mystery for many companies; it’s easy to talk about, but harder than\nexpected to execute. Second, Kappa architecture turns out to be complicated and\nexpensive in practice. While some streaming systems can scale to huge data volumes,\nthey are complex and expensive; batch storage and processing remain much more\nefficient and cost-effective for enormous historical datasets.\nThe Dataflow  Model and Unified  Batch and Streaming\nBoth  Lambda and Kappa sought to address limitations of the Hadoop ecosystem\nof the 2010s by trying to duct-tape together complicated tools that were likely not\nnatural fits in the first place. The central challenge of unifying batch and streaming\ndata remained, and Lambda and Kappa both provided inspiration and groundwork\nfor continued progress in this pursuit.\nExamples and Types of Data Architecture | 105\nOne of the central problems of managing batch and stream processing is unifying\nmultiple code paths. While the Kappa architecture relies on a unified queuing and\nstorage layer, one still has to confront using different tools for collecting real-time\nstatistics or running batch aggregation jobs. Today, engineers seek to solve this in\nseveral ways. Google made its mark by developing the Dataflow model  and the\nApache Beam  framework that implements this model.\nThe core idea in the Dataflow model is to view all data as events, as the aggregation\nis performed over various types of windows. Ongoing real-time event streams are\nunbounded data . Data batches are simply bounded event streams, and the boundaries\nprovide a natural window. Engineers can choose from various windows for real-time\naggregation, such as sliding or tumbling. Real-time and batch processing happens in\nthe same system using nearly identical code.\nThe philosophy of “batch as a special case of streaming” is now more pervasive.\nVarious frameworks such as Flink and Spark have adopted a similar approach.\nArchitecture for IoT\nThe Internet of Things  (IoT) is the distributed collection of devices, aka things —\ncomputers, sensors, mobile devices, smart home devices, and anything else with an\ninternet connection. Rather than generating data from direct human input (think\ndata entry from a keyboard), IoT data is generated from devices that collect data\nperiodically or continuously from the surrounding environment and transmit it to\na destination. IoT devices are often low-powered and operate in low-resource/low\nbandwidth environments.\nWhile the concept of IoT devices dates back at least a few decades, the smartphone\nrevolution created a massive IoT swarm virtually overnight. Since then, numerous\nnew IoT categories have emerged, such as smart thermostats, car entertainment\nsystems, smart TVs, and smart speakers. The IoT has evolved from a futurist fantasy\nto a massive data engineering domain. We expect IoT to become one of the dominant\nways data is generated and consumed, and this section goes a bit deeper than the\nothers you’ve read.\nHaving a cursory understanding of IoT architecture will help you understand broader\ndata architecture trends. Let’s briefly look at some IoT architecture concepts.\nDevices\nDevices  (also known as things ) are the physical hardware connected to the internet,\nsensing the environment around them and collecting and transmitting data to a\ndownstream destination. These devices might be used in consumer applications like\na doorbell camera, smartwatch, or thermostat. The device might be an AI-powered\ncamera that monitors an assembly line for defective components, a GPS tracker to\n106 | Chapter 3: Designing Good Data Architecture\nrecord vehicle locations, or a Raspberry Pi programmed to download the latest tweets\nand brew your coffee. Any device capable of collecting data from its environment is\nan IoT device.\nDevices should be minimally capable of collecting and transmitting data. However,\nthe device might also crunch data or run ML on the data it collects before sending it\ndownstream—edge computing and edge machine learning, respectively.\nA data engineer doesn’t necessarily need to know the inner details of IoT devices but\nshould know what the device does, the data it collects, any edge computations or\nML it runs before transmitting the data, and how often it sends data. It also helps\nto know the consequences of a device or internet outage, environmental or other\nexternal factors affecting data collection, and how these may impact the downstream\ncollection of data from the device.\nInterfacing with devices\nA device isn’t beneficial unless you can get its data. This section covers some of the\nkey components necessary to interface with IoT devices in the wild.\nIoT gateway.    An IoT gateway  is a hub for connecting devices and securely routing\ndevices to the appropriate destinations on the internet. While you can connect a\ndevice directly to the internet without an IoT gateway, the gateway allows devices to\nconnect using extremely little power. It acts as a way station for data retention and\nmanages an internet connection to the final data destination.\nNew low-power WiFi standards are designed to make IoT gateways less critical in\nthe future, but these are just rolling out now. Typically, a swarm of devices will\nutilize many IoT gateways, one at each physical location where devices are present\n(Figure 3-16 ).\nFigure 3-16. A device swarm (circles), IoT gateways, and message queue with messages\n(rectangles within the queue)\nExamples and Types of Data Architecture | 107\nIngestion.    Ingestion  begins  with an IoT gateway, as discussed previously. From there,\nevents and measurements can flow into an event ingestion architecture.\nOf course, other patterns are possible. For instance, the gateway may accumulate\ndata and upload it in batches for later analytics processing. In remote physical envi‐\nronments, gateways may not have connectivity to a network much of the time. They\nmay upload all data only when they are brought into the range of a cellular or WiFi\nnetwork. The point is that the diversity of IoT systems and environments presents\ncomplications—e.g., late-arriving data, data structure and schema disparities, data\ncorruption, and connection disruption—that engineers must account for in their\narchitectures and downstream analytics.\nStorage.    Storage requirements will depend a great deal on the latency requirement\nfor the IoT devices in the system. For example, for remote sensors collecting scientific\ndata for analysis at a later time, batch object storage may be perfectly acceptable.\nHowever, near real-time responses may be expected from a system backend that\nconstantly analyzes data in a home monitoring and automation solution. In this case,\na message queue or time-series database is more appropriate. We discuss storage\nsystems in more detail in Chapter 6 .\nServing.    Serving patterns are incredibly diverse. In a batch scientific application, data\nmight be analyzed using a cloud data warehouse and then served in a report. Data\nwill be presented and served in numerous ways in a home-monitoring application.\nData will be analyzed in the near time using a stream-processing engine or queries\nin a time-series database to look for critical events such as a fire, electrical outage,\nor break-in. Detection of an anomaly will trigger alerts to the homeowner, the fire\ndepartment, or other entity. A batch analytics component also exists—for example, a\nmonthly report on the state of the home.\nOne significant serving pattern for IoT looks like reverse ETL ( Figure 3-17 ), although\nwe tend not to use this term in the IoT context. Think of this scenario: data from\nsensors on manufacturing devices is collected and analyzed. The results of these\nmeasurements are processed to look for optimizations that will allow equipment to\noperate more efficiently. Data is sent back to reconfigure the devices and optimize\nthem.\nFigure 3-17. IoT serving pattern for downstream use cases\n108 | Chapter 3: Designing Good Data Architecture",9517
41-Whos Involved with Designing a Data Architecture.pdf,41-Whos Involved with Designing a Data Architecture,"24Zhamak Dehghani, “Data Mesh Principles and Logical Architecture, ” MartinFowler.com, December 3, 2020,\nhttps://oreil.ly/ezWE7 .\n25Zhamak Dehghani, “How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh, ” Martin‐\nFowler.com, May 20, 2019, https://oreil.ly/SqMe8 .\n26Zhamak Dehghani, “Data Mesh Principles and Logical Architecture. ”Scratching the surface of the IoT\nIoT scenarios are incredibly complex, and IoT architecture and systems are also less\nfamiliar to data engineers who may have spent their careers working with business\ndata. We hope that this introduction will encourage interested data engineers to learn\nmore about this fascinating and rapidly evolving specialization.\nData Mesh\nThe data mesh  is a recent response to sprawling monolithic data platforms, such as\ncentralized data lakes and data warehouses, and “the great divide of data, ” wherein the\nlandscape is divided between operational data and analytical data.24 The data mesh\nattempts to invert the challenges of centralized data architecture, taking the concepts\nof domain-driven design (commonly used in software architectures) and applying\nthem to data architecture. Because the data mesh has captured much recent attention,\nyou should be aware of it.\nA big part of the data mesh is decentralization, as Zhamak Dehghani noted in her\ngroundbreaking article on the topic:25\nIn order to decentralize the monolithic data platform, we need to reverse how we think\nabout data, its locality, and ownership. Instead of flowing the data from domains into\na centrally owned data lake or platform, domains need to host and serve their domain\ndatasets in an easily consumable way.\nDehghani later identified four key components of the data mesh:26\n•Domain-oriented decentralized data ownership and architecture•\n•Data as a product•\n•Self-serve data infrastructure as a platform•\n•Federated computational governance•\nFigure 3-18  shows a simplified version of a data mesh architecture. Y ou can learn\nmore about data mesh in Dehghani’s book Data Mesh  (O’Reilly).\nExamples and Types of Data Architecture | 109\nFigure 3-18. Simplified  example of a data mesh architecture. Source: From Data Mesh ,\nby Zhamak Dehghani. Copyright © 2022 Zhamak Dehghani. Published by O’Reilly\nMedia, Inc. Used with permission.\nOther Data Architecture Examples\nData architectures have countless other variations, such as data fabric, data hub,\nscaled architecture , metadata-first architecture , event-driven architecture, live data\nstack ( Chapter 11 ), and many more. And new architectures will continue to emerge\nas practices consolidate and mature, and tooling simplifies and improves. We’ve\nfocused on a handful of the most critical data architecture patterns that are extremely\nwell established, evolving rapidly, or both.\nAs a data engineer, pay attention to how new architectures may help your orga‐\nnization. Stay abreast of new developments by cultivating a high-level awareness\nof the data engineering ecosystem developments. Be open-minded and don’t get\nemotionally attached to one approach. Once you’ve identified potential value, deepen\nyour learning and make concrete decisions. When done right, minor tweaks—or\nmajor overhauls—in your data architecture can positively impact the business.\n110 | Chapter 3: Designing Good Data Architecture",3361
42-Conclusion.pdf,42-Conclusion,,0
43-Additional Resources.pdf,43-Additional Resources,"Who’s Involved with Designing a Data Architecture?\nData architecture isn’t designed in a vacuum. Bigger companies may still employ data\narchitects, but those architects will need to be heavily in tune and current with the\nstate of technology and data. Gone are the days of ivory tower data architecture. In\nthe past, architecture was largely orthogonal to engineering. We expect this distinc‐\ntion will disappear as data engineering, and engineering in general, quickly evolves,\nbecoming more agile, with less separation between engineering and architecture.\nIdeally, a data engineer will work alongside a dedicated data architect. However, if\na company is small or low in its level of data maturity, a data engineer might work\ndouble duty as an architect. Because data architecture is an undercurrent of the data\nengineering lifecycle, a data engineer should understand “good” architecture and the\nvarious types of data architecture.\nWhen designing architecture, you’ll work alongside  business stakeholders to evaluate\ntrade-offs. What are the trade-offs inherent in adopting a cloud data warehouse\nversus a data lake? What are the trade-offs of various cloud platforms? When might a\nunified batch/streaming framework (Beam, Flink) be an appropriate choice? Studying\nthese choices in the abstract will prepare you to make concrete, valuable decisions.\nConclusion\nY ou’ve learned how data architecture fits into the data engineering lifecycle and\nwhat makes for “good” data architecture, and you’ve seen several examples of data\narchitectures. Because architecture is such a key foundation for success, we encourage\nyou to invest the time to study it deeply and understand the trade-offs inherent in any\narchitecture. Y ou will be prepared to map out architecture that corresponds to your\norganization’s unique requirements.\nNext up, let’s look at some approaches to choosing the right technologies to be used in\ndata architecture and across the data engineering lifecycle.\nAdditional Resources\n•“ AnemicDomainModel”  by Martin Fowler •\n•“Big Data Architectures”  Azure documentation •\n•“BoundedContext”  by Martin Fowler •\n•“ A Brief Introduction to Two Data Processing Architectures—Lambda and•\nKappa for Big Data”  by Iman Samizadeh\n•“The Building Blocks of a Modern Data Platform”  by Prukalpa •\n•“Choosing Open Wisely”  by Benoit Dageville et al. •\nWho’s Involved with Designing a Data Architecture? | 111\n•“Choosing the Right Architecture for Global Data Distribution”  Google Cloud •\nArchitecture web page\n•“Column-Oriented DBMS” Wikipedia page•\n•“ A Comparison of Data Processing Frameworks”  by Ludovik Santos •\n•“The Cost of Cloud, a Trillion Dollar Paradox”  by Sarah Wang and Martin •\nCasado\n•“The Curse of the Data Lake Monster”  by Kiran Prakash and Lucy Chambers •\n•Data Architecture: A Primer for the Data Scientist  by W . H. Inmon et al. (Aca‐ •\ndemic Press)\n•“Data Architecture: Complex vs. Complicated”  by Dave Wells •\n•“Data as a Product vs. Data as a Service”  by Justin Gage •\n•“The Data Dichotomy: Rethinking the Way We Treat Data and Services”  by Ben •\nStopford\n•“Data Fabric Architecture Is Key to Modernizing Data Management and Integra‐•\ntion”  by Ashutosh Gupta\n•“Data Fabric Defined”  by James Serra •\n•“Data Team Platform”  by GitLab Data •\n•“Data Warehouse Architecture: Overview”  by Roelant Vos •\n•“Data Warehouse Architecture” tutorial at Javatpoint•\n•“Defining Architecture” ISO/IEC/IEEE 42010 web page•\n•“The Design and Implementation of Modern Column-Oriented Database Sys‐•\ntems”  by Daniel Abadi et al.\n•“Disasters I’ve Seen in a Microservices World”  by Joao Alves •\n•“DomainDrivenDesign”  by Martin Fowler •\n•“Down with Pipeline Debt: Introducing Great Expectations”  by the Great Expect‐ •\nations project\n•EABOK  draft , edited by Paula Hagan •\n•EABOK website•\n•“EagerReadDerivation”  by Martin Fowler •\n•“End-to-End Serverless ETL Orchestration in AWS: A Guide”  by Rittika Jindal •\n•“Enterprise Architecture” Gartner Glossary definition•\n•“Enterprise Architecture’s Role in Building a Data-Driven Organization”  by •\nAshutosh Gupta\n•“Event Sourcing”  by Martin Fowler •\n112 | Chapter 3: Designing Good Data Architecture\n•“Falling Back in Love with Data Pipelines”  by Sean Knapp •\n•“Five Principles for Cloud-Native Architecture: What It Is and How to Master It”•\nby Tom Grey\n•“Focusing on Events”  by Martin Fowler •\n•“Functional Data Engineering: A Modern Paradigm for Batch Data Processing”•\nby Maxime Beauchemin\n•“Google Cloud Architecture Framework” Google Cloud Architecture web page•\n•“How to Beat the Cap Theorem”  by Nathan Marz •\n•“How to Build a Data Architecture to Drive Innovation—Today and Tomorrow”•\nby Antonio Castro et al.\n•“How TOGAF Defines Enterprise Architecture (EA)”  by Avancier Limited •\n•The Information Management Body of Knowledge website•\n•“Introducing Dagster: An Open Source Python Library for Building Data Appli‐•\ncations”  by Nick Schrock\n•“The Log: What Every Software Engineer Should Know About Real-Time Data’s•\nUnifying Abstraction”  by Jay Kreps\n•“Microsoft Azure IoT Reference Architecture” documentation•\n•Microsoft’s “ Azure Architecture Center” •\n•“Modern CI Is Too Complex and Misdirected”  by Gregory Szorc •\n•“The Modern Data Stack: Past, Present, and Future”  by Tristan Handy •\n•“Moving Beyond Batch vs. Streaming”  by David Y affe •\n•“ A Personal Implementation of Modern Data Architecture: Getting Strava Data•\ninto Google Cloud Platform”  by Matthew Reeve\n•“Polyglot Persistence”  by Martin Fowler •\n•“Potemkin Data Science”  by Michael Correll •\n•“Principled Data Engineering, Part I: Architectural Overview”  by Hussein •\nDanish\n•“Questioning the Lambda Architecture”  by Jay Kreps •\n•“Reliable Microservices Data Exchange with the Outbox Pattern”  by Gunnar •\nMorling\n•“ReportingDatabase”  by Martin Fowler •\n•“The Rise of the Metadata Lake”  by Prukalpa •\n•“Run Y our Data Team Like a Product Team”  by Emilie Schario and Taylor A. •\nMurphy\nAdditional Resources | 113\n•“Separating Utility from Value Add”  by Ross Pettit •\n•“The Six Principles of Modern Data Architecture”  by Joshua Klahr •\n•Snowflake’s “What Is Data Warehouse Architecture” web page •\n•“Software Infrastructure 2.0: A Wishlist”  by Erik Bernhardsson •\n•“Staying Ahead of Data Debt”  by Etai Mizrahi •\n•“Tactics vs. Strategy: SOA and the Tarpit of Irrelevancy”  by Neal Ford •\n•“Test Data Quality at Scale with Deequ”  by Dustin Lange et al. •\n•“Three-Tier Architecture”  by IBM Education •\n•TOGAF framework website•\n•“The Top 5 Data Trends for CDOs to Watch Out for in 2021”  by Prukalpa •\n•“240 Tables and No Documentation?”  by Alexey Makhotkin •\n•“The Ultimate Data Observability Checklist”  by Molly Vorwerck •\n•“Unified Analytics: Where Batch and Streaming Come Together; SQL and•\nBeyond” Apache Flink Roadmap\n•“UtilityVsStrategicDichotomy”  by Martin Fowler •\n•“What Is a Data Lakehouse?”  by Ben Lorica et al. •\n•“What Is Data Architecture? A Framework for Managing Data”  by Thor •\nOlavsrud\n•“What Is the Open Data Ecosystem and Why It’s Here to Stay”  by Casber Wang •\n•“What’s Wrong with MLOps?”  by Laszlo Sragner •\n•“What the Heck Is Data Mesh”  by Chris Riccomini •\n•“Who Needs an Architect”  by Martin Fowler •\n•“Zachman Framework” Wikipedia page•\n114 | Chapter 3: Designing Good Data Architecture",7412
44-Speed to Market.pdf,44-Speed to Market,"CHAPTER 4\nChoosing Technologies Across the Data\nEngineering Lifecycle\nData engineering nowadays suffers from an embarrassment of riches. We have no\nshortage of technologies to solve various types of data problems. Data technologies\nare available as turnkey offerings consumable in almost every way—open source,\nmanaged open source, proprietary software, proprietary service, and more. However,\nit’s easy to get caught up in chasing bleeding-edge technology while losing sight\nof the core purpose of data engineering: designing robust and reliable systems to\ncarry data through the full lifecycle and serve it according to the needs of end users.\nJust as structural engineers carefully choose technologies and materials to realize an\narchitect’s vision for a building, data engineers are tasked with making appropriate\ntechnology choices to shepherd data through the lifecycle to serve data applications\nand users.\nChapter 3  discussed “good” data architecture and why it matters. We now explain\nhow to choose the right technologies to serve this architecture. Data engineers must\nchoose good technologies to make the best possible data product. We feel the criteria\nto choose a good data technology is simple: does it add value to a data product and\nthe broader business?\nA lot of people confuse architecture and tools. Architecture is strategic ; tools are\ntactical . We sometimes hear, “Our data architecture are tools X, Y, and Z. ” This is\nthe wrong way to think about architecture. Architecture is the high-level design,\nroadmap, and blueprint of data systems that satisfy the strategic aims for the business.\nArchitecture is the what , why, and when . Tools are used to make the architecture a\nreality; tools are the how.\nWe often see teams going “off the rails” and choosing technologies before map‐\nping out an architecture. The reasons vary: shiny object syndrome, resume-driven\n115\ndevelopment,  and a lack of expertise in architecture. In practice, this prioritization\nof technology often means they cobble together a kind of Dr. Seuss fantasy machine\nrather than a true data architecture. We strongly advise against choosing technology\nbefore getting your architecture right. Architecture first, technology second.\nThis  chapter discusses our tactical plan for making technology choices once we have\na strategic architecture blueprint. The following are some considerations for choosing\ndata technologies across the data engineering lifecycle:\n•Team size and capabilities•\n•Speed to market•\n•Interoperability•\n•Cost optimization and business value•\n•Today versus the future: immutable versus transitory technologies•\n•Location (cloud, on prem, hybrid cloud, multicloud)•\n•Build versus buy•\n•Monolith versus modular•\n•Serverless versus servers•\n•Optimization, performance, and the benchmark wars•\n•The undercurrents of the data engineering lifecycle•\nTeam Size and Capabilities\nThe first thing you need to assess is your team’s size and its capabilities with technol‐\nogy. Are you on a small team (perhaps a team of one) of people who are expected to\nwear many hats, or is the team large enough that people work in specialized roles?\nWill a handful of people be responsible for multiple stages of the data engineering\nlifecycle, or do people cover particular niches? Y our team’s size will influence the\ntypes of technologies you adopt.\nThere is a continuum of simple to complex technologies, and a team’s size roughly\ndetermines the amount of bandwidth your team can dedicate to complex solutions.\nWe sometimes see small data teams read blog posts about a new cutting-edge tech‐\nnology at a giant tech company and then try to emulate these same extremely com‐\nplex technologies and practices. We call this cargo-cult engineering , and it’s generally\na big mistake that consumes a lot of valuable time and money, often with little to\nnothing to show in return. Especially for small teams or teams with weaker technical\nchops, use as many managed and SaaS tools as possible, and dedicate your limited\nbandwidth to solving the complex problems that directly add value to the business.\n116 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle",4232
45-Cost Optimization and Business Value.pdf,45-Cost Optimization and Business Value,"Take an inventory of your team’s skills. Do people lean toward low-code tools, or\ndo they favor code-first approaches? Are people strong in certain languages like\nJava, Python, or Go? Technologies are available to cater to every preference on the\nlow-code to code-heavy spectrum. Again, we suggest sticking with technologies and\nworkflows with which the team is familiar. We’ve seen data teams invest a lot of time\nin learning the shiny new data technology, language, or tool, only to never use it in\nproduction. Learning new technologies, languages, and tools is a considerable time\ninvestment, so make these investments wisely.\nSpeed to Market\nIn technology, speed to market  wins. This means choosing the right technologies that\nhelp you deliver features and data faster while maintaining high-quality standards\nand security. It also means working in a tight feedback loop of launching, learning,\niterating, and making improvements.\nPerfect is the enemy of good. Some data teams will deliberate on technology choices\nfor months or years without reaching any decisions. Slow decisions and output are\nthe kiss of death to data teams. We’ve seen more than a few data teams dissolve for\nmoving too slow and failing to deliver the value they were hired to produce.\nDeliver value early and often. As we’ve mentioned, use what works. Y our team\nmembers will likely get better leverage with tools they already know. Avoid undiffer‐\nentiated heavy lifting that engages your team in unnecessarily complex work that\nadds little to no value. Choose tools that help you move quickly, reliably, safely, and\nsecurely.\nInteroperability\nRarely  will you use only one technology or system. When choosing a technology or\nsystem, you’ll need to ensure that it interacts and operates with other technologies.\nInteroperability  describes how various technologies or systems connect, exchange\ninformation, and interact.\nLet’s say you’re evaluating two technologies, A and B. How easily does technology\nA integrate with technology B when thinking about interoperability? This is often\na spectrum of difficulty, ranging from seamless to time-intensive. Is seamless integra‐\ntion already baked into each product, making setup a breeze? Or do you need to do a\nlot of manual configuration to integrate these technologies?\nOften, vendors and open source projects will target specific platforms and systems\nto interoperate. Most data ingestion and visualization tools have built-in integrations\nwith popular data warehouses and data lakes. Furthermore, popular data-ingestion\nSpeed to Market | 117",2605
46-FinOps.pdf,46-FinOps,"tools will integrate with common APIs and services, such as CRMs, accounting\nsoftware, and more.\nSometimes standards are in place for interoperability. Almost all databases allow\nconnections via Java Database Connectivity (JDBC) or Open Database Connectivity\n(ODBC), meaning that you can easily connect to a database by using these standards.\nIn other cases, interoperability occurs in the absence of standards. Representational\nstate transfer (REST) is not truly a standard for APIs; every REST API has its quirks.\nIn these cases, it’s up to the vendor or open source software (OSS) project to ensure\nsmooth integration with other technologies and systems.\nAlways be aware of how simple it will be to connect your various technologies across\nthe data engineering lifecycle. As mentioned in other chapters, we suggest designing\nfor modularity and giving yourself the ability to easily swap out technologies as new\npractices and alternatives become available.\nCost Optimization and Business Value\nIn a perfect world, you’ d get to experiment with all the latest, coolest technologies\nwithout considering cost, time investment, or value added to the business. In reality,\nbudgets and time are finite, and the cost is a major constraint for choosing the\nright data architectures and technologies. Y our organization expects a positive ROI\nfrom your data projects, so you must understand the basic costs you can control.\nTechnology is a major cost driver, so your technology choices and management\nstrategies will significantly impact your budget. We look at costs through three main\nlenses: total cost of ownership, opportunity cost, and FinOps.\nTotal Cost of Ownership\nTotal cost of ownership  (TCO) is the total estimated cost of an initiative, including\nthe direct and indirect costs of products and services utilized. Direct costs  can be\ndirectly attributed to an initiative. Examples are the salaries of a team working on\nthe initiative or the AWS bill for all services consumed. Indirect costs , also known\nas overhead , are independent of the initiative and must be paid regardless of where\nthey’re attributed.\nApart from direct and indirect costs, how something is purchased  impacts the way\ncosts are accounted for. Expenses fall into two big groups: capital expenses and\noperational expenses.\nCapital expenses , also known as capex , require an up-front investment. Payment is\nrequired today . Before the cloud existed, companies would typically purchase hard‐\nware and software up front through large acquisition contracts. In addition, signifi‐\ncant investments were required to host hardware in server rooms, data centers, and\ncolocation facilities. These up-front investments—commonly hundreds of thousands\n118 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle\n1For more details, see “Total Opportunity Cost of Ownership” by Joseph Reis in 97 Things  Every Data Engineer\nShould Know  (O’Reilly).to millions of dollars or more—would be treated as assets and slowly depreciate over\ntime. From a budget perspective, capital was required to fund the entire purchase.\nThis is capex, a significant capital outlay with a long-term plan to achieve a positive\nROI on the effort and expense put forth.\nOperational expenses , also known as opex , are the opposite of capex in certain\nrespects. Opex is gradual and spread out over time. Whereas capex is long-term\nfocused, opex is short-term. Opex can be pay-as-you-go or similar and allows a lot of\nflexibility. Opex is closer to a direct cost, making it easier to attribute to a data project.\nUntil recently, opex wasn’t an option for large data projects. Data systems often\nrequired multimillion-dollar contracts. This has changed with the advent of the\ncloud, as data platform services allow engineers to pay on a consumption-based\nmodel. In general, opex allows for a far greater ability for engineering teams to\nchoose their software and hardware. Cloud-based services let data engineers iterate\nquickly with various software and technology configurations, often inexpensively.\nData engineers need to be pragmatic about flexibility. The data landscape is changing\ntoo quickly to invest in long-term hardware that inevitably goes stale, can’t easily\nscale, and potentially hampers a data engineer’s flexibility to try new things. Given the\nupside for flexibility and low initial costs, we urge data engineers to take an opex-first\napproach centered on the cloud and flexible, pay-as-you-go technologies.\nTotal Opportunity Cost of Ownership\nAny choice inherently excludes other possibilities. Total opportunity cost of ownership\n(TOCO) is the cost of lost opportunities that we incur in choosing a technology,\nan architecture, or a process.1 Note that ownership in this setting doesn’t require\nlong-term purchases of hardware or licenses. Even in a cloud environment, we\neffectively own a technology, a stack, or a pipeline once it becomes a core part of\nour production data processes and is difficult to move away from. Data engineers\noften fail to evaluate TOCO when undertaking a new project; in our opinion, this is a\nmassive blind spot.\nIf you choose data stack A, you’ve chosen the benefits of data stack A over all other\noptions, effectively excluding data stacks B, C, and D. Y ou’re committed to data stack\nA and everything it entails—the team to support it, training, setup, and maintenance.\nWhat happens if data stack A was a poor choice? What happens when data stack A\nbecomes obsolete? Can you still move to other data stacks?\nHow quickly and cheaply can you move to something newer and better? This is a\ncritical question in the data space, where new technologies and products seem to\nCost Optimization and Business Value | 119",5792
47-Today Versus the Future Immutable Versus Transitory Technologies.pdf,47-Today Versus the Future Immutable Versus Transitory Technologies,"2J. R. Storment and Mike Fuller, Cloud FinOps  (Sebastopol, CA: O’Reilly, 2019), 6, https://oreil.ly/RvRvX .appear at an ever-faster rate. Does the expertise you’ve built up on data stack A\ntranslate to the next wave? Or are you able to swap out components of data stack\nA and buy yourself some time and options?\nThe first step to minimizing opportunity cost is evaluating it with eyes wide open.\nWe’ve seen countless data teams get stuck with technologies that seemed good at the\ntime and are either not flexible for future growth or simply obsolete. Inflexible data\ntechnologies are a lot like bear traps. They’re easy to get into and extremely painful to\nescape.\nFinOps\nWe already touched on FinOps in “Principle 9: Embrace FinOps” on page 85. As\nwe’ve discussed, typical cloud spending is inherently opex: companies pay for services\nto run critical data processes rather than making up-front purchases and clawing\nback value over time. The goal of FinOps is to fully operationalize financial account‐\nability and business value by applying the DevOps-like practices of monitoring and\ndynamically adjusting systems.\nIn this chapter, we want to emphasize one thing about FinOps that is well embodied\nin this quote:2\nIf it seems that FinOps is about saving money, then think again. FinOps is about\nmaking money. Cloud spend can drive more revenue, signal customer base growth,\nenable more product and feature release velocity, or even help shut down a data center.\nIn our setting of data engineering, the ability to iterate quickly and scale dynamically\nis invaluable for creating business value. This is one of the major motivations for\nshifting data workloads to the cloud.\nToday Versus the Future: Immutable Versus Transitory\nTechnologies\nIn an exciting domain like data engineering, it’s all too easy to focus on a rapidly\nevolving future while ignoring the concrete needs of the present. The intention to\nbuild a better future is noble but often leads to  overarchitecting and overengineering.\nTooling chosen for the future may be stale and out-of-date when this future arrives;\nthe future frequently looks little like what we envisioned years before.\nAs many life coaches would tell you, focus on the present. Y ou should choose the\nbest technology for the moment and near future, but in a way that supports future\nunknowns and evolution. Ask yourself: where are you today, and what are your goals\n120 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle\nfor the future? Y our answers to these questions should inform your decisions about\nyour architecture and thus the technologies used within that architecture. This is\ndone by understanding what is likely to change and what tends to stay the same.\nWe have two classes of tools to consider: immutable and transitory. Immutable tech‐\nnologies  might be components that underpin the cloud or languages and paradigms\nthat have stood the test of time. In the cloud, examples of immutable technologies are\nobject storage, networking, servers, and security. Object storage such as Amazon S3\nand Azure Blob Storage will be around from today until the end of the decade, and\nprobably much longer. Storing your data in object storage is a wise choice. Object\nstorage continues to improve in various ways and constantly offers new options, but\nyour data will be safe and usable in object storage regardless of the rapid evolution of\ntechnology as a whole.\nFor languages, SQL and bash have been around for many decades, and we don’t see\nthem disappearing anytime soon. Immutable technologies benefit from the Lindy\neffect: the longer a technology has been established, the longer it will be used.\nThink of the power grid, relational databases, the C programming language, or the\nx86 processor architecture. We suggest applying the Lindy effect as a litmus test to\ndetermine whether a technology is potentially immutable.\nTransitory technologies  are those that come and go. The typical trajectory begins\nwith a lot of hype, followed by meteoric growth in popularity, then a slow descent\ninto obscurity. The JavaScript frontend landscape is a classic example. How many\nJavaScript frontend frameworks have come and gone between 2010 and 2020? Back‐\nbone.js, Ember.js, and Knockout were popular in the early 2010s, and React and\nVue.js have massive mindshare today. What’s the popular frontend JavaScript frame‐\nwork three years from now? Who knows.\nNew well-funded entrants and open source projects arrive on the data front every\nday. Every vendor will say their product will change the industry and “make the\nworld a better place” . Most of these companies and projects don’t get long-term trac‐\ntion and fade slowly into obscurity. Top VCs are making big-money bets, knowing\nthat most of their data-tooling investments will fail. If VCs pouring millions (or\nbillions) into data-tooling investments can’t get it right, how can you possibly know\nwhich technologies to invest in for your data architecture? It’s hard. Just consider the\nnumber of technologies in Matt Turck’s (in)famous depictions of the ML, AI, and\ndata (MAD) landscape  that we introduced in Chapter 1  (Figure 4-1 ).\nEven relatively successful technologies often fade into obscurity quickly, after a few\nyears of rapid adoption, a victim of their success. For instance, in the early 2010s,\nHive was met with rapid uptake because it allowed both analysts and engineers to\nquery massive datasets without coding complex MapReduce jobs by hand. Inspired\nToday Versus the Future: Immutable Versus Transitory Technologies | 121",5628
48-Location.pdf,48-Location,"by the success of Hive but wishing to improve on its shortcomings, engineers devel‐\noped Presto and other technologies. Hive now appears primarily in legacy deploy‐\nments. Almost every technology follows this inevitable path of decline.\nFigure 4-1. Matt Turck’s 2021 MAD data landscape\nOur Advice\nGiven the rapid pace of tooling and best-practice changes, we suggest evaluating tools\nevery two years ( Figure 4-2 ). Whenever possible, find the immutable technologies\nalong the data engineering lifecycle, and use those as your base. Build transitory tools\naround the immutables.\nFigure 4-2. Use a two-year time horizon to reevaluate your technology choices\nGiven the reasonable probability of failure for many data technologies, you need to\nconsider how easy it is to transition from a chosen technology. What are the barriers\nto leaving? As mentioned previously in our discussion about opportunity cost, avoid\n122 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle",1000
49-Cloud.pdf,49-Cloud,"“bear traps. ” Go into a new technology with eyes wide open, knowing the project\nmight get abandoned, the company may not be viable, or the technology simply isn’t a\ngood fit any longer.\nLocation\nCompanies  now have numerous options when deciding where to run their technol‐\nogy stacks. A slow shift toward the cloud culminates in a veritable stampede of\ncompanies spinning up workloads on AWS, Azure, and Google Cloud Platform\n(GCP). In the last decade, many CTOs have come to view their decisions around\ntechnology hosting as having existential significance for their organizations. If they\nmove too slowly, they risk being left behind by their more agile competition; on the\nother hand, a poorly planned cloud migration could lead to technological failure and\ncatastrophic costs.\nLet’s look at the principal places to run your technology stack: on premises, the cloud,\nhybrid cloud, and multicloud.\nOn Premises\nWhile  new startups are increasingly born in the cloud, on-premises systems are\nstill the default for established companies. Essentially, these companies own their\nhardware, which may live in data centers they own or in leased colocation space.\nIn either case, companies are operationally responsible for their hardware and the\nsoftware that runs on it. If hardware fails, they have to repair or replace it. They also\nhave to manage upgrade cycles every few years as new, updated hardware is released\nand older hardware ages and becomes less reliable. They must ensure that they have\nenough hardware to handle peaks; for an online retailer, this means hosting enough\ncapacity to handle the load spikes of Black Friday. For data engineers in charge of\non-premises systems, this means buying large-enough systems to allow good perfor‐\nmance for peak load and large jobs without overbuying and overspending.\nOn the one hand, established companies have established operational practices that\nhave served them well. Suppose a company that relies on information technology has\nbeen in business for some time. This means it has managed to juggle the cost and\npersonnel requirements of running its hardware, managing software environments,\ndeploying code from dev teams, and running databases and big data systems.\nOn the other hand, established companies see their younger, more agile competition\nscaling rapidly and taking advantage of cloud-managed services. They also see estab‐\nlished competitors making forays into the cloud, allowing them to temporarily scale\nup enormous computing power for massive data jobs or the Black Friday shopping\nspike.\nLocation | 123\nCompanies in competitive sectors generally don’t have the option to stand still.\nCompetition is fierce, and there’s always the threat of being “disrupted” by more agile\ncompetition, often backed by a large pile of venture capital dollars. Every company\nmust keep its existing systems running efficiently while deciding what moves to\nmake next. This could involve adopting newer DevOps practices, such as containers,\nKubernetes, microservices, and continuous deployment while keeping their hardware\nrunning on premises. It could involve a complete migration to the cloud, as discussed\nnext.\nCloud\nThe cloud flips the on-premises model on its head. Instead of purchasing hardware,\nyou simply rent hardware and managed services from a cloud provider (such as AWS,\nAzure, or Google Cloud). These resources can often be reserved on an extremely\nshort-term basis; VMs spin up in less than a minute, and subsequent usage is billed\nin per-second increments. This allows cloud users to dynamically scale resources that\nwere inconceivable with on-premises servers.\nIn a cloud environment, engineers can quickly launch projects and experiment\nwithout worrying about long lead time hardware planning. They can begin running\nservers as soon as their code is ready to deploy. This makes the cloud model\nextremely appealing to startups that are tight on budget and time.\nThe early cloud era was dominated by infrastructure as a service (IaaS) offerings—\nproducts such as VMs and virtual disks that are essentially rented slices of hardware.\nSlowly, we’ve seen a shift toward platform as a service (PaaS), while SaaS products\ncontinue to grow at a rapid clip.\nPaaS includes IaaS products but adds more sophisticated managed services to support\napplications. Examples are managed databases such as Amazon Relational Database\nService (RDS) and Google Cloud SQL, managed streaming platforms such as Ama‐\nzon Kinesis and Simple Queue Service (SQS), and managed Kubernetes such as Goo‐\ngle Kubernetes Engine (GKE) and Azure Kubernetes Service (AKS). PaaS services\nallow engineers to ignore the operational details of managing individual machines\nand deploying frameworks across distributed systems. They provide turnkey access to\ncomplex, autoscaling systems with minimal operational overhead.\nSaaS offerings move one additional step up the ladder of abstraction. SaaS typically\nprovides a fully functioning enterprise software platform with little operational man‐\nagement. Examples of SaaS include Salesforce, Google Workspace, Microsoft 365,\nZoom, and Fivetran. Both the major public clouds and third parties offer SaaS\nplatforms. SaaS covers a whole spectrum of enterprise domains, including video\nconferencing, data management, ad tech, office applications, and CRM systems.\n124 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle\nThis chapter also discusses serverless, increasingly important in PaaS and SaaS offer‐\nings. Serverless products generally offer automated scaling from zero to extremely\nhigh usage rates. They are billed on a pay-as-you-go basis and allow engineers to\noperate without operational awareness of underlying servers. Many people quibble\nwith the term serverless ; after all, the code must run somewhere. In practice, server‐\nless usually means many invisible servers .\nCloud services have become increasingly appealing to established businesses with\nexisting data centers and IT infrastructure. Dynamic, seamless scaling is extremely\nvaluable to businesses that deal with seasonality (e.g., retail businesses coping with\nBlack Friday load) and web traffic load spikes. The advent of COVID-19 in 2020\nwas a major driver of cloud adoption, as companies recognized the value of rapidly\nscaling up data processes to gain insights in a highly uncertain business climate;\nbusinesses also had to cope with substantially increased load due to a spike in online\nshopping, web app usage, and remote work.\nBefore we discuss the nuances of choosing technologies in the cloud, let’s first discuss\nwhy migration to the cloud requires a dramatic shift in thinking, specifically on the\npricing front; this is closely related to FinOps, introduced in “FinOps”  on page 120.\nEnterprises that migrate to the cloud often make major deployment errors by not\nappropriately adapting their practices to the cloud pricing model.\nA Brief Detour on Cloud Economics\nTo understand how to use cloud services efficiently through cloud native architecture ,\nyou need to know how clouds make money. This is an extremely complex concept\nand one on which cloud providers offer little transparency. Consider this sidebar a\nstarting point for your research, discovery, and process development.\nCloud Services and Credit Default Swaps\nLet’s go on a little tangent about credit default swaps. Don’t worry, this will make\nsense in a bit. Recall that credit default swaps rose to infamy after the 2007 global\nfinancial crisis. A credit default swap was a mechanism for selling different tiers of\nrisk attached to an asset (e.g., a mortgage). It is not our intention to present this\nidea in any detail, but rather to offer an analogy wherein many cloud services are\nsimilar to financial derivatives; cloud providers not only slice hardware assets into\nsmall pieces through virtualization, but also sell these pieces with varying technical\ncharacteristics and risks attached. While providers are extremely tight-lipped about\ndetails of their internal systems, there are massive opportunities for optimization and\nscaling by understanding cloud pricing and exchanging notes with other users.\nLocation | 125\nLook at the example of archival cloud storage. At the time of this writing, GCP openly\nadmits that its archival class storage runs on the same clusters as standard cloud\nstorage, yet the price per gigabyte per month of archival storage is roughly 1/17 that\nof standard storage. How is this possible?\nHere’s our educated guess. When purchasing cloud storage, each disk in a storage\ncluster has three assets that cloud providers and consumers use. First, it has a certain\nstorage capacity—say, 10 TB. Second, it supports a certain number of input/output\noperations (IOPs) per second—say, 100. Third, disks support a certain maximum\nbandwidth, the maximum read speed for optimally organized files. A magnetic drive\nmight be capable of reading at 200 MB/s.\nAny of these limits (IOPs, storage capacity, bandwidth) is a potential bottleneck for\na cloud provider. For instance, the cloud provider might have a disk storing 3 TB of\ndata but hitting maximum IOPs. An alternative to leaving the remaining 7 TB empty\nis to sell the empty space without selling IOPs. Or, more specifically, sell cheap storage\nspace and expensive IOPs to discourage reads.\nMuch like traders of financial derivatives, cloud vendors also deal in risk. In the case\nof archival storage, vendors are selling a type of insurance, but one that pays out for\nthe insurer rather than the policy buyer in the event of a catastrophe. While data\nstorage costs per month are extremely cheap, I risk paying a high price if I ever need\nto retrieve data. But this is a price that I will happily pay in a true emergency.\nSimilar considerations apply to nearly any cloud service. While on-premises servers\nare essentially sold as commodity hardware, the cost model in the cloud is more\nsubtle. Rather than just charging for CPU cores, memory, and features, cloud vendors\nmonetize characteristics such as durability, reliability, longevity, and predictability; a\nvariety of compute platforms discount their offerings for workloads that are ephem‐\neral or can be arbitrarily interrupted  when capacity is needed elsewhere.\nCloud ≠ On Premises\nThis  heading may seem like a silly tautology, but the belief that cloud services are\njust like familiar on-premises servers is a widespread cognitive error that plagues\ncloud migrations and leads to horrifying bills. This demonstrates a broader issue in\ntech that we refer to as the curse of familiarity . Many new technology products are\nintentionally designed to look like something familiar to facilitate ease of use and\naccelerate adoption. But, any new technology product has subtleties and wrinkles that\nusers must learn to identify, accommodate, and optimize.\nMoving  on-premises servers one by one to VMs in the cloud—known as simple lift\nand shift—is a perfectly reasonable strategy for the initial phase of cloud migration,\nespecially when a company is facing some kind of financial cliff, such as the need\nto sign a significant new lease or hardware contract if existing hardware is not shut\ndown. However, companies that leave their cloud assets in this initial state are in for\n126 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle",11479
50-Decentralized Blockchain and the Edge.pdf,50-Decentralized Blockchain and the Edge,"3This is a major point of emphasis in Storment and Fuller, Cloud FinOps .a rude shock. On a direct comparison basis, long-running servers in the cloud are\nsignificantly more expensive than their on-premises counterparts.\nThe key to finding value in the cloud is understanding and optimizing the cloud\npricing model. Rather than deploying a set of long-running servers capable of han‐\ndling full peak load, use autoscaling to allow workloads to scale down to minimal\ninfrastructure when loads are light and up to massive clusters during peak times. To\nrealize discounts through more ephemeral, less durable workloads, use reserved or\nspot instances, or use serverless functions in place of servers.\nWe often think of this optimization as leading to lower costs, but we should also\nstrive to increase business value  by exploiting the dynamic nature of the cloud.3\nData engineers can create new value in the cloud by accomplishing things that were\nimpossible in their on-premises environment. For example, it is possible to quickly\nspin up massive compute clusters to run complex transformations at scales that were\nunaffordable for on-premises hardware.\nData Gravity\nIn addition to basic errors such as following on-premises operational practices in\nthe cloud, data engineers need to watch out for other aspects of cloud pricing and\nincentives that frequently catch users unawares.\nVendors want to lock you into their offerings. Getting data onto the platform is cheap\nor free on most cloud platforms, but getting data out can be extremely expensive. Be\naware of data egress fees and their long-term impacts on your business before getting\nblindsided by a large bill. Data gravity  is real: once data lands in a cloud, the cost to\nextract it and migrate processes can be very high.\nHybrid Cloud\nAs more established businesses migrate into the cloud, the hybrid cloud model\nis growing in importance. Virtually no business can migrate all of its workloads\novernight. The hybrid cloud model assumes that an organization will indefinitely\nmaintain some workloads outside the cloud.\nThere are many reasons to consider a hybrid cloud model. Organizations may\nbelieve that they have achieved operational excellence in certain areas, such as their\napplication stack and associated hardware. Thus, they may migrate only to specific\nworkloads where they see immediate benefits in the cloud environment. For example,\nan on-premises Spark stack is migrated to ephemeral cloud clusters, reducing the\noperational burden of managing software and hardware for the data engineering\nteam and allowing rapid scaling for large data jobs.\nLocation | 127\n4Examples include Google Cloud Anthos  and AWS Outposts .This pattern of putting analytics in the cloud is beautiful because data flows primarily\nin one direction, minimizing data egress costs ( Figure 4-3 ). That is, on-premises\napplications generate event data that can be pushed to the cloud essentially for free.\nThe bulk of data remains in the cloud where it is analyzed, while smaller amounts\nof data are pushed back to on premises for deploying models to applications, reverse\nETL, etc.\nFigure 4-3. A hybrid cloud data flow model minimizing egress costs\nA new generation of managed hybrid cloud service offerings also allows customers\nto locate cloud-managed servers in their data centers.4 This gives users the ability to\nincorporate the best features in each cloud alongside on-premises infrastructure.\nMulticloud\nMulticloud  simply  refers to deploying workloads to multiple public clouds. Compa‐\nnies may have several motivations for multicloud deployments. SaaS platforms often\nwish to offer services close to existing customer cloud workloads. Snowflake and\nDatabricks provide their SaaS offerings across multiple clouds for this reason. This\nis especially critical for data-intensive applications, where network latency and band‐\nwidth limitations hamper performance, and data egress costs can be prohibitive.\nAnother common motivation for employing a multicloud approach is to take advan‐\ntage of the best services across several clouds. For example, a company might want to\nhandle its Google Ads and Analytics data on Google Cloud and deploy Kubernetes\nthrough GKE. And the company might also adopt Azure specifically for Microsoft\nworkloads. Also, the company may like AWS because it has several best-in-class\nservices (e.g., AWS Lambda) and enjoys huge mindshare, making it relatively easy\nto hire AWS-proficient engineers. Any mix of various cloud provider services is\npossible. Given the intense competition among the major cloud providers, expect\nthem to offer more best-of-breed services, making multicloud more compelling.\n128 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle",4816
51-Cloud Repatriation Arguments.pdf,51-Cloud Repatriation Arguments,"A multicloud methodology has several disadvantages. As we just mentioned, data\negress costs and networking bottlenecks are critical. Going multicloud can introduce\nsignificant complexity. Companies must now manage a dizzying array of services\nacross several clouds; cross-cloud integration and security present a considerable\nchallenge; multicloud networking can be diabolically complicated.\nA new generation of “cloud of clouds” services aims to facilitate multicloud with\nreduced complexity by offering services across clouds and seamlessly replicating data\nbetween clouds or managing workloads on several clouds through a single pane of\nglass. To cite one example, a Snowflake account runs in a single cloud region, but\ncustomers can readily spin up other accounts in GCP , AWS, or Azure. Snowflake\nprovides simple scheduled data replication between these various cloud accounts.\nThe Snowflake interface is essentially the same in all of these accounts, removing the\ntraining burden of switching between cloud-native data services.\nThe “cloud of clouds” space is evolving quickly; within a few years of this book’s pub‐\nlication, many more of these services will be available. Data engineers and architects\nwould do well to maintain awareness of this quickly changing cloud landscape.\nDecentralized: Blockchain and the Edge\nThough  not widely used now, it’s worth briefly mentioning a new trend that might\nbecome popular over the next decade: decentralized computing. Whereas today’s\napplications mainly run on premises and in the cloud, the rise of blockchain, Web\n3.0, and edge computing may invert this paradigm. For the moment, decentralized\nplatforms have proven extremely popular but have not had a significant impact in the\ndata space; even so, keeping an eye on these platforms is worthwhile as you assess\ntechnology decisions.\nOur Advice\nFrom our perspective, we are still at the beginning of the transition to the cloud. Thus\nthe evidence and arguments around workload placement and migration are in flux.\nThe cloud itself is changing, with a shift from the IaaS model built around Amazon\nEC2 that drove the early growth of AWS and more generally toward more managed\nservice offerings such as AWS Glue, Google BigQuery, and Snowflake.\nWe’ve also seen the emergence of new workload placement abstractions. On-premises\nservices are becoming more cloud-like and abstracted. Hybrid cloud services allow\ncustomers to run fully managed services within their walls while facilitating tight\nintegration between local and remote environments. Further, the “cloud of clouds” is\nbeginning to take shape, fueled by third-party services and public cloud vendors.\nLocation | 129\nChoose technologies for the present, but look toward the future\nAs we mentioned in “Today Versus the Future: Immutable Versus Transitory Tech‐\nnologies”  on page 120, you need to keep one eye on the present while planning for\nunknowns. Right now is a tough time to plan workload placements and migrations.\nBecause of the fast pace of competition and change in the cloud industry, the decision\nspace will look very different in five to ten years. It is tempting to take into account\nevery possible future architecture permutation.\nWe believe that it is critical to avoid this endless trap of analysis. Instead, plan for the\npresent. Choose the best technologies for your current needs and concrete plans for\nthe near future. Choose your deployment platform based on real business needs while\nfocusing on simplicity and flexibility.\nIn particular, don’t choose a complex multicloud or hybrid-cloud strategy unless\nthere’s a compelling reason. Do you need to serve data near customers on multiple\nclouds? Do industry regulations require you to house certain data in your data\ncenters? Do you have a compelling technology need for specific services on two\ndifferent clouds? Choose a single-cloud deployment strategy if these scenarios don’t\napply to you.\nOn the other hand, have an escape plan. As we’ve emphasized before, every technol‐\nogy—even open source software—comes with some degree of lock-in. A single-cloud\nstrategy has significant advantages of simplicity and integration but comes with\nsignificant lock-in attached. In this instance, we’re talking about mental flexibility, the\nflexibility to evaluate the current state of the world and imagine alternatives. Ideally,\nyour escape plan will remain locked behind glass, but preparing this plan will help\nyou to make better decisions in the present and give you a way out if things go wrong\nin the future.\nCloud Repatriation Arguments\nAs we wrote this book, Sarah Wang and Martin Casado published “The Cost of\nCloud, A Trillion Dollar Paradox” , an article that generated significant sound and\nfury in the tech space. Readers widely interpreted the article as a call for the repa‐\ntriation of cloud workloads to on-premises servers. They make a somewhat more\nsubtle argument that companies should expend significant resources to control cloud\nspending and should consider repatriation as a possible option.\nWe want to take a moment to dissect one part of their discussion. Wang and Casado\ncite Dropbox’s repatriation of significant workloads from AWS to Dropbox-owned\nservers as a case study for companies considering similar repatriation moves.\n130 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle\n5Raghav Bhargava, “Evolution of Dropbox’s Edge Network, ” Dropbox.Tech, June 19, 2017,\nhttps://oreil.ly/RAwPf .\n6Akhil Gupta, “Scaling to Exabytes and Beyond, ” Dropbox.Tech, March 14, 2016, https://oreil.ly/5XPKv .\n7“Dropbox Migrates 34 PB of Data to an Amazon S3 Data Lake for Analytics, ” AWS website, 2020,\nhttps://oreil.ly/wpVoM .You are not Dropbox, nor are you Cloudflare\nWe believe that this case study is frequently used without appropriate context and\nis a compelling example of the false equivalence  logical fallacy. Dropbox provides\nparticular services where ownership of hardware and data centers can offer a compet‐\nitive advantage. Companies should not rely excessively on Dropbox’s example when\nassessing cloud and on-premises deployment options.\nFirst, it’s important to understand that Dropbox stores enormous amounts of data.\nThe company is tight-lipped about exactly how much data it hosts but says it is many\nexabytes and continues to grow.\nSecond, Dropbox handles a vast amount of network traffic. We know that its band‐\nwidth consumption in 2017 was significant enough for the company to add “hun‐\ndreds of gigabits of internet connectivity with transit providers (regional and global\nISPs), and hundreds of new peering partners (where we exchange traffic directly\nrather than through an ISP). ”5 The data egress costs would be extremely high in a\npublic cloud environment.\nThird, Dropbox is essentially a cloud storage vendor, but one with a highly spe‐\ncialized storage product that combines object and block storage characteristics.\nDropbox’s core competence is a differential file-update system that can efficiently\nsynchronize actively edited files among users while minimizing network and CPU\nusage. The product is not a good fit for object storage, block storage, or other\nstandard cloud offerings. Dropbox has instead benefited from building a custom,\nhighly integrated software and hardware stack.6\nFourth, while Dropbox moved its core product to its hardware, it continued building\nout other AWS workloads. This allows Dropbox to focus on building one highly\ntuned cloud service at an extraordinary scale rather than trying to replace multiple\nservices. Dropbox can focus on its core competence in cloud storage and data syn‐\nchronization while offloading software and hardware management in other areas,\nsuch as data analytics.7\nOther frequently cited success stories that companies have built outside the cloud\ninclude Backblaze and Cloudflare, but these offer similar lessons. Backblaze  began life\nas a personal cloud data backup product but has since begun to offer B2, an object\nstorage service similar to Amazon S3. Backblaze currently stores over an exabyte of\ndata. Cloudflare  claims to provide services for over 25 million internet properties,\nLocation | 131",8269
52-Open Source Software.pdf,52-Open Source Software,"8Todd Hoff, “The Eternal Cost Savings of Netflix’s Internal Spot Market, ” High Scalability, December 4, 2017,\nhttps://oreil.ly/LLoFt .\n9Todd Spangler, “Netflix Bandwidth Consumption Eclipsed by Web Media Streaming Applications, ” Variety ,\nSeptember 10, 2019, https://oreil.ly/tTm3k .\n10Amir Efrati and Kevin McLaughlin, “ Apple’s Spending on Google Cloud Storage on Track to Soar 50% This\nY ear, ” The Information , June 29, 2021, https://oreil.ly/OlFyR .with points of presence in over 200 cities and 51 terabits per second (Tbps) of total\nnetwork capacity.\nNetflix offers yet another useful example. The company is famous for running its tech\nstack on AWS, but this is only partially true. Netflix does run video transcoding on\nAWS, accounting for roughly 70% of its compute needs in 2017.8 Netflix also runs\nits application backend and data analytics on AWS. However, rather than using the\nAWS content distribution network, Netflix has built a custom CDN  in collaboration\nwith internet service providers, utilizing a highly specialized combination of software\nand hardware. For a company that consumes a substantial slice of all internet traffic,9\nbuilding out this critical infrastructure allowed it to deliver high-quality video to a\nhuge customer base cost-effectively.\nThese case studies suggest that it makes sense for companies to manage their own\nhardware and network connections in particular circumstances. The biggest modern\nsuccess stories of companies building and maintaining hardware involve extraordi‐\nnary scale (exabytes of data, terabits per second of bandwidth, etc.) and limited use\ncases where companies can realize a competitive advantage by engineering highly\nintegrated hardware and software stacks. In addition, all of these companies consume\nmassive network bandwidth, suggesting that data egress charges would be a major\ncost if they chose to operate fully from a public cloud.\nConsider continuing to run workloads on premises or repatriating cloud workloads if\nyou run a truly cloud-scale service. What is cloud scale? Y ou might be at cloud scale\nif you are storing an exabyte of data or handling terabits per second of traffic to and\nfrom the internet . (Achieving a terabit per second of internal  network traffic is fairly\neasy.) In addition, consider owning your servers if data egress costs are a major factor\nfor your business. To give a concrete example of cloud scale workloads that could\nbenefit from repatriation, Apple might gain a significant financial and performance\nadvantage by migrating iCloud storage to its own servers.10\nBuild Versus Buy\nBuild versus buy is an age-old debate in technology. The argument for building\nis that you have end-to-end control over the solution and are not at the mercy\nof a vendor or open source community. The argument supporting buying comes\ndown to resource constraints and expertise; do you have the expertise to build a\n132 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle\nbetter solution  than something already available? Either decision comes down to\nTCO, TOCO, and whether the solution provides a competitive advantage to your\norganization.\nIf you’ve caught on to a theme in the book so far, it’s that we suggest investing in\nbuilding and customizing when doing so will provide a competitive advantage  for your\nbusiness. Otherwise, stand on the shoulders of giants and use what’s already available\nin the market. Given the number of open source and paid services—both of which\nmay have communities of volunteers or highly paid teams of amazing engineers—\nyou’re foolish to build everything yourself.\nAs we often ask, “When you need new tires for your car, do you get the raw materials,\ncreate the tires from scratch, and install them yourself?” Like most people, you’re\nprobably buying tires and having someone install them. The same argument applies\nto build versus buy. We’ve seen teams that have built their databases from scratch. A\nsimple open source RDBMS would have served their needs much better upon closer\ninspection. Imagine the amount of time and money invested in this homegrown\ndatabase. Talk about low ROI for TCO and opportunity cost.\nThis is where the distinction between the type A and type B data engineer comes\nin handy. As we pointed out earlier, type A and type B roles are often embodied in\nthe same engineer, especially in a small organization. Whenever possible, lean toward\ntype A behavior; avoid undifferentiated heavy lifting and embrace abstraction. Use\nopen source frameworks, or if this is too much trouble, look at buying a suitable\nmanaged or proprietary solution. Plenty of great modular services are available to\nchoose from in either case.\nThe shifting reality of how companies adopt software is worth mentioning. Whereas\nin the past, IT used to make most of the software purchase and adoption decisions\nin a top-down manner, these days, the trend is for bottom-up software adoption in\na company, driven by developers, data engineers, data scientists, and other technical\nroles. Technology adoption within companies is becoming an organic, continuous\nprocess.\nLet’s look at some options for open source and proprietary solutions.\nOpen Source Software\nOpen source software  (OSS) is a software distribution model in which software, and\nthe underlying codebase, is made available for general use, typically under specific\nlicensing terms. Often OSS is created and maintained by a distributed team of\ncollaborators. OSS is free to use, change, and distribute most of the time, but with\nspecific caveats. For example, many licenses require that the source code of open\nsource–derived software be included when the software is distributed.\nBuild Versus Buy | 133\nThe motivations for creating and maintaining OSS vary. Sometimes OSS is organic,\nspringing from the mind of an individual or a small team that creates a novel solution\nand chooses to release it into the wild for public use. Other times, a company may\nmake a specific tool or technology available to the public under an OSS license.\nOSS has two main flavors: community managed and commercial OSS.\nCommunity-managed OSS\nOSS projects succeed with a strong community and vibrant user base. Community-\nmanaged OSS  is a prevalent path for OSS projects. The community opens up high\nrates of innovations and contributions from developers worldwide with popular OSS\nprojects.\nThe following are factors to consider with a community-managed OSS project:\nMindshare\nAvoid adopting OSS projects that don’t have traction and popularity. Look at the\nnumber of GitHub stars, forks, and commit volume and recency. Another thing\nto pay attention to is community activity on related chat groups and forums.\nDoes the project have a strong sense of community? A strong community creates\na virtuous cycle of strong adoption. It also means that you’ll have an easier\ntime getting technical assistance and finding talent qualified to work with the\nframework.\nMaturity\nHow long has the project been around, how active is it today, and how usable are\npeople finding it in production? A project’s maturity indicates that people find it\nuseful and are willing to incorporate it into their production workflows.\nTroubleshooting\nHow will you have to handle problems if they arise? Are you on your own to\ntroubleshoot issues, or can the community help you solve your problem?\nProject management\nLook at Git issues and the way they’re addressed. Are they addressed quickly? If\nso, what’s the process to submit an issue and get it resolved?\nTeam\nIs a company sponsoring the OSS project? Who are the core contributors?\nDeveloper relations and community management\nWhat is the project doing to encourage uptake and adoption? Is there a vibrant\nchat community (e.g., in Slack) that provides encouragement and support?\n134 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle\nContributing\nDoes the project encourage and accept pull requests? What are the process and\ntimelines for pull requests to be accepted and included in main codebase?\nRoadmap\nIs there a project roadmap? If so, is it clear and transparent?\nSelf-hosting and maintenance\nDo you have the resources to host and maintain the OSS solution? If so, what’s\nthe TCO and TOCO versus buying a managed service from the OSS vendor?\nGiving back to the community\nIf you like the project and are actively using it, consider investing in it. Y ou can\ncontribute to the codebase, help fix issues, and give advice in the community\nforums and chats. If the project allows donations, consider making one. Many\nOSS projects are essentially community-service projects, and the maintainers\noften have full-time jobs in addition to helping with the OSS project. Sadly, it’s\noften a labor of love that doesn’t afford the maintainer a living wage. If you can\nafford to donate, please do so.\nCommercial OSS\nSometimes  OSS has some drawbacks. Namely, you have to host and maintain the\nsolution in your environment. This may be trivial or extremely complicated and\ncumbersome, depending on the OSS application. Commercial vendors try to solve\nthis management headache by hosting and managing the OSS solution for you, typi‐\ncally as a cloud SaaS offering. Examples of such vendors include Databricks (Spark),\nConfluent (Kafka), DBT Labs (dbt), and there are many, many others.\nThis model is called commercial OSS  (COSS). Typically, a vendor will offer the “core”\nof the OSS for free while charging for enhancements, curated code distributions, or\nfully managed services.\nA vendor is often affiliated with the community OSS project. As an OSS project\nbecomes more popular, the maintainers may create a separate business for a managed\nversion of the OSS. This typically becomes a cloud SaaS platform built around a\nmanaged version of the open source code. This is a widespread trend: an OSS project\nbecomes popular, an affiliated company raises truckloads of venture capital (VC)\nmoney to commercialize the OSS project, and the company scales as a fast-moving\nrocket ship.\nAt this point, the data engineer has two options. Y ou can continue using the\ncommunity-managed OSS version, which you need to continue maintaining on your\nown (updates, server/container maintenance, pull requests for bug fixes, etc.). Or, you\ncan pay the vendor and let it take care of the administrative management of the COSS\nproduct.\nBuild Versus Buy | 135\nThe following are factors to consider with a commercial OSS project:\nValue\nIs the vendor offering a better value than if you managed the OSS technology\nyourself? Some vendors will add many bells and whistles to their managed\nofferings that aren’t available in the community OSS version. Are these additions\ncompelling to you?\nDelivery model\nHow do you access the service? Is the product available via download, API, or\nweb/mobile UI? Be sure you can easily access the initial version and subsequent\nreleases.\nSupport\nSupport cannot be understated, and it’s often opaque to the buyer. What is the\nsupport model for the product, and is there an extra cost for support? Frequently,\nvendors will sell support for an additional fee. Be sure you clearly understand\nthe costs of obtaining support. Also, understand what is covered in support,\nand what is not covered. Anything that’s not covered by support will be your\nresponsibility to own and manage.\nReleases and bug fixes\nIs the vendor transparent about the release schedule, improvements, and bug\nfixes? Are these updates easily available to you?\nSales cycle and pricing\nOften a vendor will offer on-demand pricing, especially for a SaaS product,\nand offer you a discount if you commit to an extended agreement. Be sure to\nunderstand the trade-offs of paying as you go versus paying up front. Is it worth\npaying a lump sum, or is your money better spent elsewhere?\nCompany finances\nIs the company viable? If the company has raised VC funds, you can check their\nfunding on sites like Crunchbase. How much runway does the company have,\nand will it still be in business in a couple of years?\nLogos versus revenue\nIs the company focused on growing the number of customers (logos), or is it\ntrying to grow revenue? Y ou may be surprised by the number of companies\nprimarily concerned with growing their customer count, GitHub stars, or Slack\nchannel membership without the revenue to establish sound finances.\nCommunity support\nIs the company truly supporting the community version of the OSS project? How\nmuch is the company contributing to the community OSS codebase? Controver‐\nsies have arisen with certain vendors co-opting OSS projects and subsequently\n136 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle",12835
53-Monolith Versus Modular.pdf,53-Monolith Versus Modular,"providing little value back to the community. How likely will the product remain\nviable as a community-supported open source if the company shuts down?\nNote also that clouds offer their own managed open source products. If a cloud\nvendor sees traction with a particular product or project, expect that vendor to offer\nits version. This can range from simple examples (open source Linux offered on\nVMs) to extremely complex managed services (fully managed Kafka). The motivation\nfor these offerings is simple: clouds make their money through consumption. More\nofferings in a cloud ecosystem mean a greater chance of “stickiness” and increased\ncustomer spending.\nProprietary Walled Gardens\nWhile  OSS is ubiquitous, a big market also exists for non-OSS technologies. Some\nof the biggest companies in the data industry sell closed source products. Let’s look\nat two major types of proprietary walled gardens , independent companies and cloud-\nplatform offerings.\nIndependent offerings\nThe data-tool landscape has seen exponential growth over the last several years.\nEvery day, new independent offerings arise for data tools. With the ability to raise\nfunds from VCs flush with capital, these data companies can scale and hire great\nengineering, sales, and marketing teams. This presents a situation where users have\nsome great product choices in the marketplace while having to wade through endless\nsales and marketing clutter. At the time of this writing, the good times of freely\navailable capital for data companies are coming to an end, but that’s another long\nstory whose consequences are still unfolding.\nOften a company selling a data tool will not release it as OSS, instead offering\na proprietary solution. Although you won’t have the transparency of a pure OSS\nsolution, a proprietary independent solution can work quite well, especially as a fully\nmanaged service in the cloud.\nThe following are things to consider with an independent offering:\nInteroperability\nMake sure that the tool interoperates with other tools you’ve chosen (OSS, other\nindependents, cloud offerings, etc.). Interoperability is key, so make sure you can\ntry it before you buy.\nMindshare and market share\nIs the solution popular? Does it command a presence in the marketplace? Does it\nenjoy positive customer reviews?\nBuild Versus Buy | 137\nDocumentation and support\nProblems and questions will inevitably arise. Is it clear how to solve your prob‐\nlem, either through documentation or support?\nPricing\nIs the pricing understandable? Map out low-, medium-, and high-probability\nusage scenarios, with respective costs. Are you able to negotiate a contract, along\nwith a discount? Is it worth it? How much flexibility do you lose if you sign a\ncontract, both in negotiation and the ability to try new options? Are you able to\nobtain contractual commitments on future pricing?\nLongevity\nWill the company survive long enough for you to get value from its product?\nIf the company has raised money, search around for its funding situation. Look\nat user reviews. Ask friends and post questions on social networks about users’\nexperiences with the product. Make sure you know what you’re getting into.\nCloud platform proprietary service offerings\nCloud vendors develop and sell their proprietary services for storage, databases, and\nmore. Many of these solutions are internal tools used by respective sibling companies.\nFor example, Amazon created the database DynamoDB to overcome the limitations\nof traditional relational databases and handle the large amounts of user and order\ndata as Amazon.com grew into a behemoth. Amazon later offered the DynamoDB\nservice solely on AWS; it’s now a top-rated product used by companies of all sizes and\nmaturity levels. Cloud vendors will often bundle their products to work well together.\nEach cloud can create stickiness with its user base by creating a strong integrated\necosystem.\nThe following are factors to consider with a proprietary cloud offering:\nPerformance versus price comparisons\nIs the cloud offering substantially better than an independent or OSS version?\nWhat’s the TCO of choosing a cloud’s offering?\nPurchase considerations\nOn-demand pricing can be expensive. Can you lower your cost by purchasing\nreserved capacity or entering into a long-term commitment agreement?\nOur Advice\nBuild versus buy comes back to knowing your competitive advantage and where it\nmakes sense to invest resources toward customization. In general, we favor OSS and\nCOSS by default, which frees you to focus on improving those areas where these\noptions are insufficient. Focus on a few areas where building something will add\nsignificant value or reduce friction substantially.\n138 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle",4834
54-Modularity.pdf,54-Modularity,"Don’t treat internal operational overhead as a sunk cost. There’s excellent value\nin upskilling your existing data team to build sophisticated systems on managed\nplatforms rather than babysitting on-premises servers. In addition, think about how\na company makes money, especially its sales and customer experience teams, which\nwill generally indicate how you’re treated during the sales cycle and when you’re a\npaying customer.\nFinally, who is responsible for the budget at your company? How does this person\ndecide the projects and technologies that get funded? Before making the business case\nfor COSS or managed services, does it make sense to try to use OSS first? The last\nthing you want is for your technology choice to be stuck in limbo while waiting for\nbudget approval. As the old saying goes, time kills deals . In your case, more time spent\nin limbo means a higher likelihood your budget approval will die. Know beforehand\nwho controls the budget and what will successfully get approved.\nMonolith Versus Modular\nMonoliths  versus modular systems is another longtime debate in the software archi‐\ntecture space. Monolithic systems are self-contained, often performing multiple func‐\ntions under a single system. The monolith camp favors the simplicity of having\neverything in one place. It’s easier to reason about a single entity, and you can\nmove faster because there are fewer moving parts. The modular  camp leans toward\ndecoupled, best-of-breed technologies performing tasks at which they are uniquely\ngreat. Especially given the rate of change in products in the data world, the argument\nis you should aim for interoperability among an ever-changing array of solutions.\nWhat approach should you take in your data engineering stack? Let’s explore the\ntrade-offs.\nMonolith\nThe monolith  (Figure 4-4 ) has been a technology mainstay for decades. The old days\nof waterfall meant that software releases were huge, tightly coupled, and moved at\na slow cadence. Large teams worked together to deliver a single working codebase.\nMonolithic data systems continue to this day, with older software vendors such as\nInformatica and open source frameworks such as Spark.\nThe pros of the monolith are it’s easy to reason about, and it requires a lower\ncognitive burden and context switching since everything is self-contained. Instead of\ndealing with dozens of technologies, you deal with “one” technology and typically\none principal programming language. Monoliths are an excellent option if you want\nsimplicity in reasoning about your architecture and processes.\nMonolith Versus Modular | 139\nFigure 4-4. The monolith tightly couples its services\nOf course, the monolith has cons. For one thing, monoliths are brittle. Because of the\nvast number of moving parts, updates and releases take longer and tend to bake in\n“the kitchen sink. ” If the system has a bug—hopefully, the software’s been thoroughly\ntested before release!—it can harm the entire system.\nUser-induced problems also happen with monoliths. For example, we saw a mono‐\nlithic ETL pipeline that took 48 hours to run. If anything broke anywhere in the\npipeline, the entire process had to restart. Meanwhile, anxious business users were\nwaiting for their reports, which were already two days late by default and usually\narrived much later. Breakages were common enough that the monolithic system was\neventually thrown out.\nMultitenancy in a monolithic system can also be a significant problem. It can be\nchallenging to isolate the workloads of multiple users. In an on-prem data warehouse,\none user-defined function might consume enough CPU to slow the system for other\nusers. Conflicts between dependencies and resource contention are frequent sources\nof headaches.\nAnother con of monoliths is that switching to a new system will be painful if the\nvendor or open source project dies. Because all of your processes are contained in\nthe monolith, extracting yourself out of that system, and onto a new platform, will be\ncostly in both time and money.\nModularity\nModularity  (Figure 4-5 ) is an old concept in software engineering, but modular\ndistributed systems truly came into vogue with the rise of microservices. Instead of\nrelying on a massive monolith to handle your needs, why not break apart systems and\nprocesses into their self-contained areas of concern? Microservices can communicate\nvia APIs, allowing developers to focus on their domains while making their applica‐\ntions accessible to other microservices. This is the trend in software engineering and\nis increasingly seen in modern data systems.\n140 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle\nFigure 4-5. With modularity, each service is decoupled from another\nMajor tech companies have been key drivers in the microservices movement. The\nfamous Bezos API mandate decreases coupling between applications, allowing refac‐\ntoring and decomposition. Bezos also imposed the two-pizza rule (no team should\nbe so large that two pizzas can’t feed the whole group). Effectively, this means that a\nteam will have at most five members. This cap also limits the complexity of a team’s\ndomain of responsibility—in particular, the codebase that it can manage. Whereas\nan extensive monolithic application might entail a group of one hundred people,\ndividing developers into small groups of five requires that this application be broken\ninto small, manageable, loosely coupled pieces.\nIn a modular microservice environment, components are swappable, and it’s possi‐\nble to create a polyglot  (multiprogramming language) application; a Java service\ncan replace a service written in Python. Service customers need worry only about\nthe technical specifications of the service API, not behind-the-scenes details of\nimplementation.\nData-processing technologies have shifted toward a modular model by providing\nstrong support for interoperability. Data is stored in object storage in a standard\nformat such as Parquet in data lakes and lakehouses. Any processing tool that sup‐\nports the format can read the data and write processed results back into the lake\nfor processing by another tool. Cloud data warehouses support interoperation with\nobject storage through import/export using standard formats and external tables—\ni.e., queries run directly on data in a data lake.\nNew technologies arrive on the scene at a dizzying rate in today’s data ecosystem, and\nmost get stale and outmoded quickly. Rinse and repeat. The ability to swap out tools\nas technology changes is invaluable. We view data modularity as a more powerful\nparadigm than monolithic data engineering. Modularity allows engineers to choose\nthe best technology for each job or step along the pipeline.\nThe cons of modularity are that there’s more to reason about. Instead of handling a\nsingle system of concern, now you potentially have countless systems to understand\nand operate. Interoperability is a potential headache; hopefully, these systems all play\nnicely together.\nThis very problem led us to break out orchestration as a separate undercurrent\ninstead of placing it under data management. Orchestration is also important for\nmonolithic data architectures; witness the success of tools like BMC Software’s\nControl-M in the traditional data warehousing space. But orchestrating five or ten\nMonolith Versus Modular | 141",7433
55-The Distributed Monolith Pattern.pdf,55-The Distributed Monolith Pattern,,0
56-Serverless Versus Servers.pdf,56-Serverless Versus Servers,"tools is dramatically more complex than orchestrating one. Orchestration becomes\nthe glue that binds data stack modules together.\nThe Distributed Monolith Pattern\nThe distributed monolith pattern  is a distributed architecture that still suffers from\nmany of the limitations of monolithic architecture. The basic idea is that one runs a\ndistributed system with different services to perform different tasks. Still, services and\nnodes share a common set of dependencies or a common codebase.\nOne standard example is a traditional Hadoop cluster. A Hadoop cluster can simulta‐\nneously host several frameworks, such as Hive, Pig, or Spark. The cluster also has\nmany internal dependencies. In addition, the cluster runs core Hadoop components:\nHadoop common libraries, HDFS, YARN, and Java. In practice, a cluster often has\none version of each component installed.\nA standard on-prem Hadoop system entails managing a common environment that\nworks for all users and all jobs. Managing upgrades and installations is a significant\nchallenge. Forcing jobs to upgrade dependencies risks breaking them; maintaining\ntwo versions of a framework entails extra complexity.\nSome modern Python-based orchestration technologies—e.g., Apache Airflow—also\nsuffer from this problem. While they utilize a highly decoupled and asynchronous\narchitecture, every service runs the same codebase with the same dependencies. Any\nexecutor can execute any task, so a client library for a single task run in one DAG\nmust be installed on the whole cluster. Orchestrating many tools entails installing\nclient libraries for a host of APIs. Dependency conflicts are a constant problem.\nOne solution to the problems of the distributed monolith is ephemeral infrastructure\nin a cloud setting. Each job gets its own temporary server or cluster installed with\ndependencies. Each cluster remains highly monolithic, but separating jobs dramati‐\ncally reduces conflicts. For example, this pattern is now quite common for Spark with\nservices like Amazon EMR and Google Cloud Dataproc.\nA second solution is to properly decompose the distributed monolith into multiple\nsoftware environments using containers. We have more to say on containers in\n“Serverless Versus Servers” on page 143 .\nOur Advice\nWhile  monoliths are attractive because of ease of understanding and reduced com‐\nplexity, this comes at a high cost. The cost is the potential loss of flexibility, opportu‐\nnity cost, and high-friction development cycles.\nHere are some things to consider when evaluating monoliths versus modular options:\n142 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle",2671
57-Big Data...for the 1990s.pdf,57-Big Data...for the 1990s,"11Evan Sangaline, “Running FFmpeg on AWS Lambda for 1.9% the Cost of AWS Elastic Transcoder, ” Intoli\nblog, May 2, 2018, https://oreil.ly/myzOv .Interoperability\nArchitect for sharing and interoperability.\nAvoiding the “bear trap”\nSomething that is easy to get into might be painful or impossible to escape.\nFlexibility\nThings are moving so fast in the data space right now. Committing to a monolith\nreduces flexibility and reversible decisions.\nServerless Versus Servers\nA big trend for cloud providers is serverless , allowing developers and data engineers\nto run applications without managing servers behind the scenes. Serverless provides a\nquick time to value for the right use cases. For other cases, it might not be a good fit.\nLet’s look at how to evaluate whether serverless is right for you.\nServerless\nThough serverless has been around for quite some time, the serverless trend kicked\noff in full force with AWS Lambda in 2014. With the promise of executing small\nchunks of code on an as-needed basis without having to manage a server, serverless\nexploded in popularity. The  main reasons for its popularity are cost and convenience.\nInstead of paying the cost of a server, why not just pay when your code is evoked?\nServerless has many flavors. Though function as a service (FaaS) is wildly popular,\nserverless systems predate the advent of AWS Lambda. For example, Google Cloud’s\nBigQuery is serverless in that data engineers don’t need to manage backend infra‐\nstructure, and the system scales to zero and scales up automatically to handle large\nqueries. Just load data into the system and start querying. Y ou pay for the amount\nof data your query consumes and a small cost to store your data. This payment\nmodel—paying for consumption and storage—is becoming more prevalent.\nWhen does serverless make sense? As with many other cloud services, it depends;\nand data engineers would do well to understand the details of cloud pricing to\npredict when serverless deployments will become expensive. Looking specifically at\nthe case of AWS Lambda, various engineers have found hacks to run batch workloads\nat meager costs.11 On the other hand, serverless functions suffer from an inherent\noverhead inefficiency. Handling one event per function call at a high event rate can be\ncatastrophically expensive, especially when simpler approaches like multithreading or\nmultiprocessing are great alternatives.\nServerless Versus Servers | 143\nAs with other areas of ops, it’s critical to monitor and model. Monitor  to determine\ncost per event in a real-world environment and maximum length of serverless execu‐\ntion, and model  using this cost per event to determine overall costs as event rates\ngrow. Modeling should also include worst-case scenarios—what happens if my site\ngets hit by a bot swarm or DDoS attack?\nContainers\nIn conjunction with serverless and microservices, containers  are one of the most\npowerful trending operational technologies as of this writing. Containers play a role\nin both serverless and microservices.\nContainers are often referred to as  lightweight virtual machines . Whereas a traditional\nVM wraps up an entire operating system, a container packages an isolated user\nspace (such as a filesystem and a few processes); many such containers can coexist\non a single host operating system. This provides some of the principal benefits of\nvirtualization (i.e., dependency and code isolation) without the overhead of carrying\naround an entire operating system kernel.\nA single hardware node can host numerous containers with fine-grained resource\nallocations. At the time of this writing, containers continue to grow in popularity,\nalong with Kubernetes, a container management system. Serverless environments\ntypically run on containers behind the scenes. Indeed, Kubernetes is a kind of server‐\nless environment because it allows developers and ops teams to deploy microservices\nwithout worrying about the details of the machines where they are deployed.\nContainers provide a partial solution to problems of the distributed monolith men‐\ntioned earlier in this chapter. For example, Hadoop now supports containers, allow‐\ning each job to have its own isolated dependencies.\nContainer clusters do not provide the same security and isola‐\ntion that full VMs offer. Container escape —broadly, a class of\nexploits whereby code in a container gains privileges outside the\ncontainer at the OS level—is common enough to be considered a\nrisk for multitenancy. While Amazon EC2 is a truly multitenant\nenvironment with VMs from many customers hosted on the same\nhardware, a Kubernetes cluster should host code only within an\nenvironment of mutual trust (e.g., inside the walls of a single\ncompany). In addition, code review processes and vulnerability\nscanning are critical to ensure that a developer doesn’t introduce a\nsecurity hole.\nVarious flavors of container platforms add additional serverless features. Container‐\nized function platforms run containers as ephemeral units triggered by events rather\n144 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle\n12Examples include OpenFaaS , Knative , and Google Cloud Run .than persistent services.12 This gives users the simplicity of AWS Lambda with the\nfull flexibility of a container environment instead of the highly restrictive Lambda\nruntime. And services such as AWS Fargate and Google App Engine run containers\nwithout managing a compute cluster required for Kubernetes. These services also\nfully isolate containers, preventing the security issues associated with multitenancy.\nAbstraction will continue working its way across the data stack. Consider the impact\nof Kubernetes on cluster management. While you can manage your Kubernetes\ncluster—and many engineering teams do so—even Kubernetes is widely available as a\nmanaged service. What comes after Kubernetes? We’re as excited as you to find out.\nHow to Evaluate Server Versus Serverless\nWhy would you want to run your own servers instead of using serverless? There are a\nfew reasons. Cost is a big factor. Serverless makes less sense when the usage and cost\nexceed the ongoing cost of running and maintaining a server ( Figure 4-6 ). However,\nat a certain scale, the economic benefits of serverless may diminish, and running\nservers becomes more attractive.\nFigure 4-6. Cost of serverless versus utilizing a server\nCustomization, power, and control are other major reasons to favor servers over\nserverless. Some serverless frameworks can be underpowered or limited for certain\nuse cases. Here are some things to consider when using servers, particularly in the\ncloud, where server resources are ephemeral:\nExpect servers to fail.\nServer failure will happen. Avoid using a “special snowflake” server that is overly\ncustomized and brittle, as this introduces a glaring vulnerability in your architec‐\nture. Instead, treat servers as ephemeral resources that you can create as needed\nand then delete. If your application requires specific code to be installed on the\nServerless Versus Servers | 145\nserver, use a boot script or build an image. Deploy code to the server through a\nCI/CD pipeline.\nUse clusters and autoscaling.\nTake advantage of the cloud’s ability to grow and shrink compute resources\non demand. As your application increases its usage, cluster your application\nservers, and use autoscaling capabilities to automatically horizontally scale your\napplication as demand grows.\nTreat your infrastructure as code.\nAutomation doesn’t apply to just servers and should extend to your infrastruc‐\nture whenever possible. Deploy your infrastructure (servers or otherwise) using\ndeployment managers such as Terraform, AWS CloudFormation, and Google\nCloud Deployment Manager.\nUse containers.\nFor more sophisticated or heavy-duty workloads with complex installed depen‐\ndencies, consider using containers on either a single server or Kubernetes.\nOur Advice\nHere  are some key considerations to help you determine whether serverless is right\nfor you:\nWorkload size and complexity\nServerless works best for simple, discrete tasks and workloads. It’s not as suit‐\nable if you have many moving parts or require a lot of compute or memory\nhorsepower. In that case, consider using containers and a container workflow\norchestration framework like Kubernetes.\nExecution frequency and duration\nHow many requests per second will your serverless application process? How\nlong will each request take to process? Cloud serverless platforms have limits\non execution frequency, concurrency, and duration. If your application can’t\nfunction neatly within these limits, it is time to consider a container-oriented\napproach.\nRequests and networking\nServerless platforms often utilize some form of simplified networking and don’t\nsupport all cloud virtual networking features, such as VPCs and firewalls.\nLanguage\nWhat language do you typically use? If it’s not one of the officially supported\nlanguages supported by the serverless platform, you should consider containers\ninstead.\n146 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle\nRuntime limitations\nServerless platforms don’t give you complete operating system abstractions.\nInstead, you’re limited to a specific runtime image.\nCost\nServerless functions are incredibly convenient but potentially expensive. When\nyour serverless function processes only a few events, your costs are low; costs rise\nrapidly as the event count increases. This scenario is a frequent source of surprise\ncloud bills.\nIn the end, abstraction tends to win. We suggest looking at using serverless first and\nthen servers—with containers and orchestration if possible—once you’ve outgrown\nserverless options.\nOptimization, Performance, and the Benchmark Wars\nImagine  that you are a billionaire shopping for new transportation. Y ou’ve narrowed\nyour choice to two options:\n•787 Business Jet•\n—Range: 9,945 nautical miles (with 25 passengers)—\n—Maximum speed: 0.90 Mach—\n—Cruise speed: 0.85 Mach—\n—Fuel capacity: 101,323 kilograms—\n—Maximum takeoff weight: 227,930 kilograms—\n—Maximum thrust: 128,000 pounds—\n•Tesla Model S Plaid•\n—Range: 560 kilometers—\n—Maximum speed: 322 kilometers/hour—\n—0–100 kilometers/hour: 2.1 seconds—\n—Battery capacity: 100 kilowatt hours—\n—Nurburgring lap time: 7 minutes, 30.9 seconds—\n—Horsepower: 1020—\n—Torque: 1050 lb-ft—\nWhich of these options offers better performance? Y ou don’t have to know much\nabout cars or aircraft to recognize that this is an idiotic comparison. One option is\na wide-body private jet designed for intercontinental operation, while the other is an\nelectric supercar.\nOptimization, Performance, and the Benchmark Wars | 147",10843
58-Nonsensical Cost Comparisons.pdf,58-Nonsensical Cost Comparisons,,0
59-Caveat Emptor.pdf,59-Caveat Emptor,"13Justin Olsson and Reynold Xin, “Eliminating the Anti-competitive DeWitt Clause for Database Benchmark‐\ning, ” Databricks, November 8, 2021, https://oreil.ly/3iFOE .\n14For a classic of the genre, see William McKnight and Jake Dolezal, “Data Warehouse in the Cloud Bench‐\nmark, ” GigaOm, February 7, 2019, https://oreil.ly/QjCmA .We see such apples-to-oranges comparisons made all the time in the database space.\nBenchmarks either compare databases that are optimized for completely different use\ncases, or use test scenarios that bear no resemblance to real-world needs.\nRecently, we saw a new round of benchmark wars flare up among major vendors in\nthe data space. We applaud benchmarks and are glad to see many database vendors\nfinally dropping DeWitt clauses from their customer contracts.13 Even so, let the\nbuyer beware: the data space is full of nonsensical benchmarks.14 Here are a few\ncommon tricks used to place a thumb on the benchmark scale.\nBig Data...for the 1990s\nProducts  that claim to support “big data” at petabyte scale will often use benchmark\ndatasets small enough to easily fit in the storage on your smartphone. For systems\nthat rely on caching layers to deliver performance, test datasets fully reside in solid-\nstate drive (SSD) or memory, and benchmarks can show ultra-high performance by\nrepeatedly querying the same data. A small test dataset also minimizes RAM and SSD\ncosts when comparing pricing.\nTo benchmark for real-world use cases, you must simulate anticipated real-world data\nand query size. Evaluate query performance and resource costs based on a detailed\nevaluation of your needs.\nNonsensical Cost Comparisons\nNonsensical  cost comparisons are a standard trick when analyzing a price/perfor‐\nmance or TCO. For instance, many MPP systems can’t be readily created and deleted\neven when they reside in a cloud environment; these systems run for years on end\nonce they’ve been configured. Other databases support a dynamic compute model\nand charge either per query or per second of use. Comparing ephemeral and non-\nephemeral systems on a cost-per-second basis is nonsensical, but we see this all the\ntime in benchmarks.\nAsymmetric Optimization\nThe deceit of asymmetric optimization appears in many guises, but here’s one exam‐\nple. Often a vendor will compare a row-based MPP system against a columnar data‐\nbase by using a benchmark that runs complex join queries on highly normalized data.\nThe normalized data model is optimal for the row-based system, but the columnar\n148 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle",2620
60-Undercurrents and Their Impacts on Choosing Technologies.pdf,60-Undercurrents and Their Impacts on Choosing Technologies,,0
61-Data Management.pdf,61-Data Management,,0
62-Data Architecture.pdf,62-Data Architecture,"system would realize its full potential only with some schema changes. To make mat‐\nters worse, vendors juice their systems with an extra shot of join optimization (e.g.,\npreindexing joins) without applying comparable tuning in the competing database\n(e.g., putting joins in a materialized view).\nCaveat Emptor\nAs with all things in data technology, let the buyer beware. Do your homework before\nblindly relying on vendor benchmarks to evaluate and choose technology.\nUndercurrents and Their Impacts\non Choosing Technologies\nAs seen in this chapter, a data engineer has a lot to consider when evaluating technol‐\nogies. Whatever technology you choose, be sure to understand how it supports the\nundercurrents of the data engineering lifecycle. Let’s briefly review them again.\nData Management\nData management is a broad area, and concerning technologies, it isn’t always appa‐\nrent whether a technology adopts data management as a principal concern. For\nexample, behind the scenes, a third-party vendor may use data management best\npractices—regulatory compliance, security, privacy, data quality, and governance—\nbut hide these details behind a limited UI layer. In this case, while evaluating the\nproduct, it helps to ask the company about its data management practices. Here are\nsome sample questions you should ask:\n•How are you protecting data against breaches, both from the outside and within?•\n•What is your product’s compliance with GDPR, CCPA, and other data privacy•\nregulations?\n•Do you allow me to host my data to comply with these regulations?•\n•How do you ensure data quality and that I’m viewing the correct data in your•\nsolution?\nThere are many other questions to ask, and these are just a few of the ways to think\nabout data management as it relates to choosing the right technologies. These same\nquestions should also apply to the OSS solutions you’re considering.\nDataOps\nProblems  will happen. They just will. A server or database may die, a cloud’s region\nmay have an outage, you might deploy buggy code, bad data might be introduced into\nyour data warehouse, and other unforeseen problems may occur.\nUndercurrents and Their Impacts on Choosing Technologies | 149",2218
63-Software Engineering.pdf,63-Software Engineering,"When evaluating a new technology, how much control do you have over deploying\nnew code, how will you be alerted if there’s a problem, and how will you respond\nwhen there’s a problem? The answer largely depends on the type of technology\nyou’re considering. If the technology is OSS, you’re likely responsible for setting up\nmonitoring, hosting, and code deployment. How will you handle issues? What’s your\nincident response?\nMuch of the operations are outside your control if you’re using a managed offering.\nConsider the vendor’s SLA, the way they alert you to issues, and whether they’re\ntransparent about how they’re addressing the case, including providing an ETA to a\nfix.\nData Architecture\nAs discussed in Chapter 3 , good data architecture means assessing trade-offs and\nchoosing the best tools for the job while keeping your decisions reversible. With the\ndata landscape morphing at warp speed, the best tool  for the job is a moving target.\nThe main goals are to avoid unnecessary lock-in, ensure interoperability across the\ndata stack, and produce high ROI. Choose your technologies accordingly.\nOrchestration Example: Airflow\nThroughout  most of this chapter, we have actively avoided discussing any particular\ntechnology too extensively. We make an exception for orchestration because the space\nis currently dominated by one open source technology, Apache Airflow.\nMaxime Beauchemin kicked off the Airflow project at Airbnb in 2014. Airflow\nwas developed from the beginning as a noncommercial open source project. The\nframework quickly grew significant mindshare outside Airbnb, becoming an Apache\nIncubator project in 2016 and a full Apache-sponsored project in 2019.\nAirflow enjoys many advantages, largely because of its dominant position in the open\nsource marketplace. First, the Airflow open source project is extremely active, with a\nhigh rate of commits and a quick response time for bugs and security issues, and the\nproject recently released Airflow 2, a major refactor of the codebase. Second, Airflow\nenjoys massive mindshare. Airflow has a vibrant, active community on many com‐\nmunications platforms, including Slack, Stack Overflow, and GitHub. Users can easily\nfind answers to questions and problems. Third, Airflow is available commercially as\na managed service or software distribution through many vendors, including GCP ,\nAWS, and Astronomer.io.\nAirflow also has some downsides. Airflow relies on a few core nonscalable compo‐\nnents (the scheduler and backend database) that can become bottlenecks for perfor‐\nmance, scale, and reliability; the scalable parts of Airflow still follow a distributed\nmonolith pattern. (See “Monolith Versus Modular” on page 139.) Finally, Airflow\n150 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle",2822
64-Conclusion.pdf,64-Conclusion,,0
65-Additional Resources.pdf,65-Additional Resources,"lacks support for many data-native constructs, such as schema management, lineage,\nand cataloging; and it is challenging to develop and test Airflow workflows.\nWe do not attempt an exhaustive discussion of Airflow alternatives here but just\nmention a couple of the key orchestration contenders at the time of writing. Prefect\nand Dagster aim to solve some of the problems discussed previously by rethinking\ncomponents of the Airflow architecture. Will there be other orchestration frame‐\nworks and technologies not discussed here? Plan on it.\nWe highly recommend that anyone choosing an orchestration technology study the\noptions discussed here. They should also acquaint themselves with activity in the\nspace, as new developments will certainly occur by the time you read this.\nSoftware Engineering\nAs a data engineer, you should strive for simplification and abstraction across the\ndata stack. Buy or use prebuilt open source solutions whenever possible. Eliminating\nundifferentiated heavy lifting should be your big goal. Focus your resources—custom\ncoding and tooling—on areas that give you a solid competitive advantage. For exam‐\nple, is hand-coding a database connection between your production database and\nyour cloud data warehouse a competitive advantage for you? Probably not. This is\nvery much a solved problem. Pick an off-the-shelf solution (open source or managed\nSaaS) instead. The world doesn’t need the millionth +1 database-to-cloud data ware‐\nhouse connector.\nOn the other hand, why do customers buy from you? Y our business likely has\nsomething special about the way it does things. Maybe it’s a particular algorithm that\npowers your fintech platform. By abstracting away a lot of the redundant workflows\nand processes, you can continue chipping away, refining, and customizing the things\nthat move the needle for the business.\nConclusion\nChoosing the right technologies is no easy task, especially when new technologies\nand patterns emerge daily. Today is possibly the most confusing time in history for\nevaluating and selecting technologies. Choosing technologies is a balance of use case,\ncost, build versus buy, and modularization. Always approach technology the same\nway as architecture: assess trade-offs and aim for reversible decisions.\nAdditional Resources\n•Cloud FinOps  by J. R. Storment and Mike Fuller (O’Reilly) •\n•“Cloud Infrastructure: The Definitive Guide for Beginners”  by Matthew Smith •\nConclusion | 151\n•“The Cost of Cloud, a Trillion Dollar Paradox”  by Sarah Wang and Martin •\nCasado\n•FinOps Foundation’s “What Is FinOps” web page •\n•“Red Hot: The 2021 Machine Learning, AI and Data (MAD) Landscape”  by Matt •\nTurck\n•Ternary Data’s “What’s Next for Analytical Databases? w/ Jordan Tigani (Mother‐ •\nDuck)” video\n•“The Unfulfilled Promise of Serverless”  by Corey Quinn •\n•“What Is the Modern Data Stack?”  by Charles Wang •\n152 | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle",2990
66-Part II. The Data Engineering Lifecycle in Depth.pdf,66-Part II. The Data Engineering Lifecycle in Depth,PART II\nThe Data Engineering\nLifecycle in Depth,49
67-Sources of Data How Is Data Created.pdf,67-Sources of Data How Is Data Created,"CHAPTER 5\nData Generation in Source Systems\nWelcome to the first stage of the data engineering lifecycle: data generation in source\nsystems. As we described earlier, the job of a data engineer is to take data from source\nsystems, do something with it, and make it helpful in serving downstream use cases.\nBut before you get raw data, you must understand where the data exists, how it is\ngenerated, and its characteristics and quirks.\nThis chapter covers some popular operational source system patterns and the signifi‐\ncant types of source systems. Many source systems exist for data generation, and we’re\nnot exhaustively covering them all. We’ll consider the data these systems generate and\nthings you should consider when working with source systems. We also discuss how\nthe undercurrents of data engineering apply to this first phase of the data engineering\nlifecycle ( Figure 5-1 ).\nFigure 5-1. Source systems generate the data for the rest of the data engineering lifecycle\n155",997
68-Source Systems Main Ideas.pdf,68-Source Systems Main Ideas,,0
69-APIs.pdf,69-APIs,"As data proliferates, especially with the rise of data sharing (discussed next), we\nexpect that a data engineer’s role will shift heavily toward understanding the interplay\nbetween data sources and destinations. The basic plumbing tasks of data engineer‐\ning—moving data from A to B—will simplify dramatically. On the other hand, it will\nremain critical to understand the nature of data as it’s created in source systems.\nSources of Data: How Is Data Created?\nAs you learn about the various underlying operational patterns of the systems that\ngenerate data, it’s essential to understand how data is created. Data is an unorganized,\ncontext-less collection of facts and figures. It can be created in many ways, both\nanalog and digital.\nAnalog data  creation  occurs in the real world, such as vocal speech, sign language,\nwriting on paper, or playing an instrument. This analog data is often transient; how\noften have you had a verbal conversation whose contents are lost to the ether after the\nconversation ends?\nDigital data  is either created by converting analog data to digital form or is the native\nproduct of a digital system. An example of analog to digital is a mobile texting app\nthat converts analog speech into digital text. An example of digital data creation is a\ncredit card transaction on an ecommerce platform. A customer places an order, the\ntransaction is charged to their credit card, and the information for the transaction is\nsaved to various databases.\nWe’ll utilize a few common examples in this chapter, such as data created when\ninteracting with a website or mobile application. But in truth, data is everywhere\nin the world around us. We capture data from IoT devices, credit card terminals,\ntelescope sensors, stock trades, and more.\nGet familiar with your source system and how it generates data. Put in the effort\nto read the source system documentation and understand its patterns and quirks. If\nyour source system is an RDBMS, learn how it operates (writes, commits, queries,\netc.); learn the ins and outs of the source system that might affect your ability to\ningest from it.\nSource Systems: Main Ideas\nSource systems produce data in various ways. This section discusses the main ideas\nyou’ll frequently encounter as you work with source systems.\nFiles and Unstructured Data\nA file is a sequence of bytes, typically stored on a disk. Applications often write data\nto files. Files may store local parameters, events, logs, images, and audio.\n156 | Chapter 5: Data Generation in Source Systems",2558
70-Application Databases OLTP Systems.pdf,70-Application Databases OLTP Systems,"In addition, files are a universal medium of data exchange. As much as data engineers\nwish that they could get data programmatically, much of the world still sends and\nreceives files. For example, if you’re getting data from a government agency, there’s an\nexcellent chance you’ll download the data as an Excel or CSV file or receive the file in\nan email.\nThe main types of source file formats you’ll run into as a data engineer—files that\noriginate either manually or as an output from a source system process—are Excel,\nCSV , TXT, JSON, and XML. These files have their quirks and can be structured\n(Excel, CSV), semistructured (JSON, XML, CSV), or unstructured (TXT, CSV).\nAlthough you’ll use certain formats heavily as a data engineer (such as Parquet, ORC,\nand Avro), we’ll cover these later and put the spotlight here on source system files.\nChapter 6  covers the technical details of files.\nAPIs\nApplication programming interfaces  (APIs) are  a standard way of exchanging data\nbetween systems. In theory, APIs simplify the data ingestion task for data engineers.\nIn practice, many APIs still expose a good deal of data complexity for engineers\nto manage. Even with the rise of various services and frameworks, and services\nfor automating API data ingestion, data engineers must often invest a good deal of\nenergy into maintaining custom API connections. We discuss APIs in greater detail\nlater in this chapter.\nApplication Databases (OLTP Systems)\nAn application database  stores the state of an application. A standard example is a\ndatabase that stores account balances for bank accounts. As customer transactions\nand payments happen, the application updates bank account balances.\nTypically, an application database is an  online transaction processing  (OLTP) system—\na database that reads and writes individual data records at a high rate. OLTP systems\nare often referred to as transactional databases , but this does not necessarily imply\nthat the system in question supports atomic transactions .\nMore generally, OLTP databases support low latency and high concurrency. An\nRDBMS  database can select or update a row in less than a millisecond (not account‐\ning for network latency) and handle thousands of reads and writes per second. A\ndocument database cluster can manage even higher document commit rates at the\nexpense of potential inconsistency. Some graph databases can also handle transac‐\ntional use cases.\nFundamentally, OLTP databases work well as application backends when thousands\nor even millions of users might be interacting with the application simultaneously,\nSource Systems: Main Ideas | 157\nupdating and writing data concurrently. OLTP systems are less suited to use cases\ndriven by analytics at scale, where a single query must scan a vast amount of data.\nACID\nSupport  for atomic transactions is one of a critical set of database characteristics\nknown together as ACID (as you may recall from Chapter 3 , this stands for atomicity,\nconsistency, isolation, durability ). Consistency  means that any database read will return\nthe last written version of the retrieved item. Isolation  entails  that if two updates are\nin flight concurrently for the same thing, the end database state will be consistent\nwith the sequential execution of these updates in the order they were submitted.\nDurability  indicates that committed data will never be lost, even in the event of power\nloss.\nNote that ACID characteristics are not required to support application backends,\nand relaxing these constraints can be a considerable boon to performance and scale.\nHowever, ACID characteristics guarantee that the database will maintain a consistent\npicture of the world, dramatically simplifying the app developer’s task.\nAll engineers (data or otherwise) must understand operating with and without ACID.\nFor instance, to improve performance, some distributed databases use relaxed consis‐\ntency constraints, such as  eventual consistency , to improve performance. Understand‐\ning the consistency model you’re working with helps you prevent disasters.\nAtomic transactions\nAn atomic transaction  is a set of several changes that are committed as a unit. In\nthe example in Figure 5-2 , a traditional banking application running on an RDBMS\nexecutes a SQL statement that checks two account balances, one in Account A (the\nsource) and another in Account B (the destination). Money is then moved from\nAccount A to Account B if sufficient funds are in Account A. The entire transaction\nshould run with updates to both account balances or fail without updating either\naccount balance. That is, the whole operation should happen as a transaction .\nFigure 5-2. Example of an atomic transaction: a bank account transfer using OLTP\n158 | Chapter 5: Data Generation in Source Systems",4849
71-Online Analytical Processing System.pdf,71-Online Analytical Processing System,,0
72-CRUD.pdf,72-CRUD,"OLTP and analytics\nOften, small companies run analytics directly on an OLTP . This pattern works in the\nshort term but is ultimately not scalable. At some point, running analytical queries on\nOLTP runs into performance issues due to structural limitations of OLTP or resource\ncontention with competing transactional workloads. Data engineers must understand\nthe inner workings of OLTP and application backends to set up appropriate integra‐\ntions with analytics systems without degrading production application performance.\nAs companies offer more analytics capabilities in SaaS applications, the need for\nhybrid capabilities—quick updates with combined analytics capabilities—has created\nnew challenges for data engineers. We’ll use the term data application  to refer to\napplications that hybridize transactional and analytics workloads.\nOnline Analytical Processing System\nIn contrast to an OLTP system, an online analytical processing  (OLAP) system is\nbuilt to run large analytics queries and is typically inefficient at handling lookups\nof individual records. For example, modern column databases are optimized to scan\nlarge volumes of data, dispensing with indexes to improve scalability and scan perfor‐\nmance. Any query typically involves scanning a minimal data block, often 100 MB or\nmore in size. Trying to look up thousands of individual items per second in such a\nsystem will bring it to its knees unless it is combined with a caching layer designed\nfor this use case.\nNote that we’re using the term OLAP  to refer to any database system that supports\nhigh-scale interactive analytics queries; we are not limiting ourselves to systems that\nsupport OLAP cubes (multidimensional arrays of data). The online  part of OLAP\nimplies that the system constantly listens for incoming queries, making OLAP sys‐\ntems suitable for interactive analytics.\nAlthough this chapter covers source systems, OLAPs are typically storage and query\nsystems for analytics. Why are we talking about them in our chapter on source\nsystems? In practical use cases, engineers often need to read data from an OLAP\nsystem. For example, a data warehouse might serve data used to train an ML model.\nOr, an OLAP system might serve a reverse ETL workflow, where derived data in an\nanalytics system is sent back to a source system, such as a CRM, SaaS platform, or\ntransactional application.\nChange Data Capture\nChange data capture  (CDC) is a method for extracting each change event (insert,\nupdate, delete) that occurs in a database. CDC is frequently leveraged to replicate\nbetween databases in near real time or create an event stream for downstream\nprocessing.\nSource Systems: Main Ideas | 159\nCDC is handled differently depending on the database technology. Relational data‐\nbases often generate an event log stored directly on the database server that can be\nprocessed to create a stream. (See “Database Logs” on page 161.) Many cloud NoSQL\ndatabases can send a log or event stream to a target storage location.\nLogs\nA log captures  information about events that occur in systems. For example, a log\nmay capture traffic and usage patterns on a web server. Y our desktop computer’s\noperating system (Windows, macOS, Linux) logs events as the system boots and\nwhen applications start or crash, for example.\nLogs are a rich data source, potentially valuable for downstream data analysis, ML,\nand automation. Here are a few familiar sources of logs:\n•Operating systems•\n•Applications•\n•Servers•\n•Containers•\n•Networks•\n•IoT devices•\nAll logs track events and event metadata. At a minimum, a log should capture who,\nwhat, and when:\nWho\nThe human, system, or service account associated with the event (e.g., a web\nbrowser user agent or a user ID)\nWhat happened\nThe event and related metadata\nWhen\nThe timestamp of the event\nLog encoding\nLogs are encoded in a few ways:\nBinary-encoded logs\nThese encode data in a custom compact format for space efficiency and fast I/O.\nDatabase logs, discussed in “Database Logs” on page 161 , are a standard example.\n160 | Chapter 5: Data Generation in Source Systems\nSemistructured logs\nThese are encoded as text in an object serialization format (JSON, more often\nthan not). Semistructured logs are machine-readable and portable. However,\nthey are much less efficient than binary logs. And though they are nominally\nmachine-readable, extracting value from them often requires significant custom\ncode.\nPlain-text (unstructured) logs\nThese essentially store the console output from software. As such, no general-\npurpose standards exist. These logs can provide helpful information for data\nscientists and ML engineers, though extracting useful information from the raw\ntext data might be complicated.\nLog resolution\nLogs are created at various resolutions and log levels. The log resolution  refers to the\namount of event data captured in a log. For example, database logs capture enough\ninformation from database events to allow reconstructing the database state at any\npoint in time.\nOn the other hand, capturing all data changes in logs for a big data system often isn’t\npractical. Instead, these logs may note only that a particular type of commit event\nhas occurred. The log level  refers to the conditions required to record a log entry,\nspecifically concerning errors and debugging. Software is often configurable to log\nevery event or to log only errors, for example.\nLog latency: Batch or real time\nBatch  logs are often written continuously to a file. Individual log entries can be\nwritten to a messaging system such as Kafka or Pulsar for real-time applications.\nDatabase Logs\nDatabase logs  are essential enough that they deserve more detailed coverage. Write-\nahead logs—typically, binary files stored in a specific database-native format—play a\ncrucial role in database guarantees and recoverability. The database server receives\nwrite and update requests to a database table (see Figure 5-3 ), storing each operation\nin the log before acknowledging the request. The acknowledgment comes with a\nlog-associated guarantee: even if the server fails, it can recover its state on reboot by\ncompleting the unfinished work from the logs.\nDatabase logs are extremely useful in data engineering, especially for CDC to gener‐\nate event streams from database changes.\nSource Systems: Main Ideas | 161",6419
73-Databases.pdf,73-Databases,"Figure 5-3. Database logs record operations on a table\nCRUD\nCRUD , which stands for create , read , update , and delete , is a transactional pattern\ncommonly used in programming and represents the four basic operations of persis‐\ntent storage. CRUD is the most common pattern for storing application state in a\ndatabase. A basic tenet of CRUD is that data must be created before being used. After\nthe data has been created, the data can be read and updated. Finally, the data may\nneed to be destroyed. CRUD guarantees these four operations will occur on data,\nregardless of its storage.\nCRUD is a widely used pattern in software applications, and you’ll commonly find\nCRUD used in APIs and databases. For example, a web application will make heavy\nuse of CRUD for RESTful HTTP requests and storing and retrieving data from a\ndatabase.\nAs with any database, we can use snapshot-based extraction to get data from a\ndatabase where our application applies CRUD operations. On the other hand, event\nextraction with CDC gives us a complete history of operations and potentially allows\nfor near real-time analytics.\nInsert-Only\nThe insert-only pattern  retains history directly in a table containing data. Rather\nthan updating records, new records get inserted with a timestamp indicating when\nthey were created ( Table 5-1 ). For example, suppose you have a table of customer\naddresses. Following a CRUD pattern, you would simply update the record if the\ncustomer changed their address. With the insert-only pattern, a new address record\nis inserted with the same customer ID. To read the current customer address by\ncustomer ID, you would look up the latest record under that ID.\n162 | Chapter 5: Data Generation in Source Systems\nTable 5-1. An insert-only pattern produces multiple versions of a record\nRecord ID Value Timestamp\n1 40 2021-09-19T00:10:23+00:00\n1 51 2021-09-30T00:12:00+00:00\nIn a sense, the insert-only pattern maintains a database log directly in the table itself,\nmaking it especially useful if the application needs access to history. For example, the\ninsert-only pattern would work well for a banking application designed to present\ncustomer address history.\nA separate analytics insert-only pattern is often used with regular CRUD application\ntables. In the insert-only ETL pattern, data pipelines insert a new record in the target\nanalytics table anytime an update occurs in the CRUD table.\nInsert-only has a couple of disadvantages. First, tables can grow quite large, especially\nif data frequently changes, since each change is inserted into the table. Sometimes\nrecords are purged based on a record sunset date or a maximum number of record\nversions to keep table size reasonable. The second disadvantage is that record lookups\nincur extra overhead because looking up the current state involves running MAX\n(created_timestamp) . If hundreds or thousands of records are under a single ID,\nthis lookup operation is expensive to run.\nMessages and Streams\nRelated to event-driven architecture, two terms that you’ll often see used interchange‐\nably are message queue  and streaming platform , but a subtle but essential difference\nexists between the two. Defining and contrasting these terms is worthwhile since they\nencompass many big ideas related to source systems and practices and technologies\nspanning the entire data engineering lifecycle.\nA message  is raw data communicated across two or more systems ( Figure 5-4 ).\nFor example, we have System 1 and System 2, where System 1 sends a message to\nSystem 2 . These systems could be different microservices, a server sending a message\nto a serverless function, etc. A message is typically sent through a message queue  from\na publisher to a consumer, and once the message is delivered, it is removed from the\nqueue.\nFigure 5-4. A message passed between two systems\nMessages are discrete and singular signals in an event-driven system. For example,\nan IoT device might send a message with the latest temperature reading to a message\nSource Systems: Main Ideas | 163\nqueue. This message is then ingested by a service that determines whether the furnace\nshould be turned on or off. This service sends a message to a furnace controller that\ntakes the appropriate action. Once the message is received, and the action is taken,\nthe message is removed from the message queue.\nBy contrast, a stream  is an append-only log of event records. (Streams are ingested\nand stored in event-streaming platforms , which we discuss at greater length in “Mes‐\nsages and Streams” on page 163 .) As events occur, they are accumulated in an ordered\nsequence ( Figure 5-5 ); a timestamp or an ID might order events. (Note that events\naren’t always delivered in exact order because of the subtleties of distributed systems.)\nFigure 5-5. A stream, which is an ordered append-only log of records\nY ou’ll use streams when you care about what happened over many events. Because\nof the append-only nature of streams, records in a stream are persisted over a long\nretention window—often weeks or months—allowing for complex operations on\nrecords such as aggregations on multiple records or the ability to rewind to a point in\ntime within the stream.\nIt’s worth noting that systems that process streams can process messages, and stream‐\ning platforms are frequently used for message passing. We often accumulate messages\nin streams when we want to perform message analytics. In our IoT example, the\ntemperature readings that trigger the furnace to turn on or off might also be later\nanalyzed to determine temperature trends and statistics.\nTypes of Time\nWhile  time is an essential consideration for all data ingestion, it becomes that much\nmore critical and subtle in the context of streaming, where we view data as continu‐\nous and expect to consume it shortly after it is produced. Let’s look at the key types\nof time you’ll run into when ingesting data: the time that the event is generated, when\nit’s ingested and processed, and how long processing took ( Figure 5-6 ).\n164 | Chapter 5: Data Generation in Source Systems\nFigure 5-6. Event, ingestion, process, and processing time\nEvent time  indicates  when an event is generated in a source system, including the\ntimestamp of the original event itself. An undetermined time lag will occur upon\nevent creation, before the event is ingested and processed downstream. Always\ninclude timestamps for each phase through which an event travels. Log events as\nthey occur and at each stage of time—when they’re created, ingested, and processed.\nUse these timestamp logs to accurately track the movement of your data through your\ndata pipelines.\nAfter data is created, it is ingested somewhere. Ingestion time  indicates when an event\nis ingested from source systems into a message queue, cache, memory, object storage,\na database, or any place else that data is stored (see Chapter 6 ). After ingestion, data\nmay be processed immediately; or within minutes, hours, or days; or simply persist in\nstorage indefinitely.\nProcess time  occurs  after ingestion time, when the data is processed (typically, a\ntransformation). Processing time  is how long the data took to process, measured in\nseconds, minutes, hours, etc.\nY ou’ll want to record these various times, preferably in an automated way. Set up\nmonitoring along your data workflows to capture when events occur, when they’re\ningested and processed, and how long it took to process events.\nSource System Practical Details\nThis section discusses the practical details of interacting with modern source systems.\nWe’ll dig into the details of commonly encountered databases, APIs, and other\naspects. This information will have a shorter shelf life than the main ideas discussed\npreviously; popular API frameworks, databases, and other details will continue to\nchange rapidly.\nNevertheless, these details are critical knowledge for working data engineers. We\nsuggest that you study this information as baseline knowledge but read extensively to\nstay abreast of ongoing developments.\nSource System Practical Details | 165\nDatabases\nIn this section, we’ll look at common source system database technologies that you’ll\nencounter as a data engineer and high-level considerations for working with these\nsystems. There are as many types of databases as there are use cases for data.\nMajor considerations for understanding database technologies\nHere, we introduce major ideas that occur across a variety of database technologies,\nincluding those that back software applications and those that support analytics use\ncases:\nDatabase management system\nA database system used to store and serve data. Abbreviated as DBMS, it consists\nof a storage engine, query optimizer, disaster recovery, and other key compo‐\nnents for managing the database system.\nLookups\nHow  does the database find and retrieve data? Indexes can help speed up\nlookups, but not all databases have indexes. Know whether your database uses\nindexes; if so, what are the best patterns for designing and maintaining them?\nUnderstand how to leverage for efficient extraction. It also helps to have a basic\nknowledge of the major types of indexes, including B-tree and log-structured\nmerge-trees (LSM).\nQuery optimizer\nDoes the database utilize an optimizer? What are its characteristics?\nScaling and distribution\nDoes the database scale with demand? What scaling strategy does it deploy? Does\nit scale horizontally (more database nodes) or vertically (more resources on a\nsingle machine)?\nModeling patterns\nWhat  modeling patterns work best with the database (e.g., data normalization or\nwide tables)? (See Chapter 8  for our discussion of data modeling.)\nCRUD\nHow is data queried, created, updated, and deleted in the database? Every type of\ndatabase handles CRUD operations differently.\nConsistency\nIs the database fully consistent, or does it support a relaxed consistency model\n(e.g., eventual consistency)? Does the database support optional consistency\nmodes for reads and writes (e.g., strongly consistent reads)?\n166 | Chapter 5: Data Generation in Source Systems\nWe divide databases into relational and nonrelational categories. In truth, the nonre‐\nlational category is far more diverse, but relational databases still occupy significant\nspace in application backends.\nRelational databases\nA relational database management system  (RDBMS) is one of the most common\napplication backends. Relational databases were developed at IBM in the 1970s and\npopularized by Oracle in the 1980s. The growth of the internet saw the rise of the\nLAMP stack (Linux, Apache web server, MySQL, PHP) and an explosion of vendor\nand open source RDBMS options. Even with the rise of NoSQL databases (described\nin the following section), relational databases have remained extremely popular.\nData is stored in a table of relations  (rows), and each relation contains multiple fields\n(columns); see Figure 5-7 . Note that we use the terms column  and field interchangea‐\nbly throughout this book. Each relation in the table has the same  schema  (a sequence\nof columns with assigned static types such as string, integer, or float). Rows are\ntypically stored as a contiguous sequence of bytes on disk.\nFigure 5-7. RDBMS stores and retrieves data at a row level\nTables are typically indexed by a  primary key , a unique field for each row in the table.\nThe indexing strategy for the primary key is closely connected with the layout of the\ntable on disk.\nTables can also have various foreign keys —fields with values connected with the\nvalues of primary keys in other tables, facilitating joins, and allowing for complex\nschemas that spread data across multiple tables. In particular, it is possible to design\na normalized schema . Normalization is a strategy for ensuring that data in records is\nnot duplicated in multiple places, thus avoiding the need to update states in multiple\nlocations at once and preventing inconsistencies (see Chapter 8 ).\nRDBMS systems are typically ACID compliant. Combining a normalized schema,\nACID compliance, and support for high transaction rates makes relational database\nsystems ideal for storing rapidly changing application states. The challenge for data\nengineers is to determine how to capture state information over time.\nSource System Practical Details | 167\n1Keith D. Foote, “ A Brief History of Non-Relational Databases, ” Dataversity, June 19, 2018,\nhttps://oreil.ly/5Ukg2 .A full discussion of the theory, history, and technology of RDBMS is beyond the\nscope of this book. We encourage you to study RDBMS systems, relational algebra,\nand strategies for normalization because they’re widespread, and you’ll encounter\nthem frequently. See “ Additional Resources” on page 188  for suggested books.\nNonrelational databases: NoSQL\nWhile  relational databases are terrific for many use cases, they’re not a one-size-fits-\nall solution. We often see that people start with a relational database under the\nimpression it’s a universal appliance and shoehorn in a ton of use cases and work‐\nloads. As data and query requirements morph, the relational database collapses under\nits weight. At that point, you’ll want to use a database that’s appropriate for the\nspecific workload under pressure. Enter nonrelational or NoSQL databases. NoSQL ,\nwhich stands for not only SQL , refers to a whole class of databases that abandon the\nrelational paradigm.\nOn the one hand, dropping relational constraints can improve performance, scala‐\nbility, and schema flexibility. But as always in architecture, trade-offs exist. NoSQL\ndatabases also typically abandon various RDBMS characteristics, such as strong con‐\nsistency, joins, or a fixed schema.\nA big theme of this book is that data innovation is constant. Let’s take a quick look at\nthe history of NoSQL, as it’s helpful to gain a perspective on why and how data inno‐\nvations impact your work as a data engineer. In the early 2000s, tech companies such\nas Google and Amazon began to outgrow their relational databases and pioneered\nnew distributed, nonrelational databases to scale their web platforms.\nWhile the term NoSQL  first appeared in 1998, the modern version was coined by Eric\nEvans in the 2000s.1 He tells the story in a 2009 blog post :\nI’ve spent the last couple of days at nosqleast  and one of the hot topics here is the\nname “nosql. ” Understandably, there are a lot of people who worry that the name is\nBad, that it sends an inappropriate or inaccurate message. While I make no claims to\nthe idea, I do have to accept some blame for what it is now being called. How’s that?\nJohan Oskarsson was organizing the first meetup and asked the question “What’s a\ngood name?” on IRC; it was one of three or four suggestions that I spouted off in the\nspan of like 45 seconds, without thinking.\nMy regret, however, isn’t about what the name says; it’s about what it doesn’t. When\nJohan originally had the idea for the first meetup, he seemed to be thinking Big Data\nand linearly scalable distributed systems, but the name is so vague that it opened the\ndoor to talk submissions for literally anything that stored data, and wasn’t an RDBMS.\n168 | Chapter 5: Data Generation in Source Systems\nNoSQL remains vague in 2022, but it’s been widely adopted to describe a universe of\n“new school” databases, alternatives to relational databases.\nThere are numerous flavors of NoSQL database designed for almost any imaginable\nuse case. Because there are far too many NoSQL databases to cover exhaustively\nin this section, we consider the following database types: key-value, document,\nwide-column, graph, search, and time series. These databases are all wildly popular\nand enjoy widespread adoption. A data engineer should understand these types of\ndatabases, including usage considerations, the structure of the data they store, and\nhow to leverage each in the data engineering lifecycle.\nKey-value stores.    A key-value database  is a nonrelational database that retrieves\nrecords using a key that uniquely identifies each record. This is similar to hash\nmap or dictionary data structures presented in many programming languages but\npotentially more scalable. Key-value stores encompass several NoSQL database types\n—for example, document stores and wide column databases (discussed next).\nDifferent types of key-value databases offer a variety of performance characteristics\nto serve various application needs. For example, in-memory key-value databases are\npopular for caching session data for web and mobile applications, where ultra-fast\nlookup and high concurrency are required. Storage in these systems is typically\ntemporary; if the database shuts down, the data disappears. Such caches can reduce\npressure on the main application database and serve speedy responses.\nOf course, key-value stores can also serve applications requiring high-durability per‐\nsistence. An ecommerce application may need to save and update massive amounts\nof event state changes for a user and their orders. A user logs into the ecommerce\napplication, clicks around various screens, adds items to a shopping cart, and then\nchecks out. Each event must be durably stored for retrieval. Key-value stores often\npersist data to disk and across multiple nodes to support such use cases.\nDocument stores.    As mentioned previously, a document store  is a specialized key-value\nstore. In this context, a document  is a nested object; we can usually think of each\ndocument as a JSON object for practical purposes. Documents are stored in collec‐\ntions and retrieved by key. A collection  is roughly equivalent to a  table in a relational\ndatabase (see Table 5-2 ).\nTable 5-2. Comparison of RDBMS and document terminology\nRDBMS Document database\nTable Collection\nRow Document, items, entity\nSource System Practical Details | 169\nOne key difference between relational databases and document stores is that the\nlatter does not support joins. This means that data cannot be easily normalized , i.e.,\nsplit across multiple tables. (Applications can still join manually. Code can look up a\ndocument, extract a property, and then retrieve another document.) Ideally, all related\ndata can be stored in the same document.\nIn many cases, the same data must be stored in multiple documents spread across\nnumerous collections; software engineers must be careful to update a property\neverywhere it is stored. (Many document stores support a notion of transactions\nto facilitate this.)\nDocument databases generally embrace all the flexibility of JSON and don’t enforce\nschema or types; this is a blessing and a curse. On the one hand, this allows the\nschema to be highly flexible and expressive. The schema can also evolve as an\napplication grows. On the flip side, we’ve seen document databases become absolute\nnightmares to manage and query. If developers are not careful in managing schema\nevolution, data may become inconsistent and bloated over time. Schema evolution\ncan also break downstream ingestion and cause headaches for data engineers if it’s\nnot communicated in a timely fashion (before deployment).\nThe following is an example of data that is stored in a collection called users . The\ncollection key is the id. We also have a name  (along with first  and last  as child\nelements) and an array of the user’s favorite bands within each document:\n{\n  ""users"":[\n     {\n     ""id"":1234,\n     ""name"":{\n     ""first"":""Joe"",\n     ""last"":""Reis""\n     },\n     ""favorite_bands"" :[\n     ""AC/DC"",\n     ""Slayer"" ,\n     ""WuTang Clan"" ,\n     ""Action Bronson""\n     ]\n     },\n     {\n     ""id"":1235,\n     ""name"":{\n     ""first"":""Matt"",\n     ""last"":""Housley""\n     },\n     ""favorite_bands"" :[\n     ""Dave Matthews Band"" ,\n     ""Creed"",\n     ""Nickelback""\n170 | Chapter 5: Data Generation in Source Systems\n2Nemil Dalal’s excellent series on the history of MongoDB  recounts some harrowing tales of database abuse\nand its consequences for fledgling startups.     ]\n     }\n  ]\n}\nTo query the data in this example, you can retrieve records by key. Note that most\ndocument databases also support the creation of indexes and lookup tables to allow\nretrieval of documents by specific properties. This is often invaluable in application\ndevelopment when you need to search for documents in various ways. For example,\nyou could set an index on name .\nAnother critical technical detail for data engineers is that document stores are\ngenerally not ACID compliant, unlike relational databases. Technical expertise in\na particular document store is essential to understanding performance, tuning, con‐\nfiguration, related effects on writes, consistency, durability, etc. For example, many\ndocument stores are eventually consistent . Allowing data distribution across a cluster\nis a boon for scaling and performance but can lead to catastrophes when engineers\nand developers don’t understand the implications.2\nTo run analytics on document stores, engineers generally must run a full scan to\nextract all data from a collection or employ a CDC strategy to send events to a target\nstream. The full scan approach can have both performance and cost implications. The\nscan often slows the database as it runs, and many serverless cloud offerings charge a\nsignificant fee for each full scan. In document databases, it’s often helpful to create an\nindex to help speed up queries. We discuss indexes and query patterns in Chapter 8 .\nWide-column.    A wide-column database  is optimized for storing massive amounts\nof data with high transaction rates and extremely low latency. These databases\ncan scale to extremely high write rates and vast amounts of data. Specifically, wide-\ncolumn databases can support petabytes of data, millions of requests per second,\nand sub-10ms latency. These characteristics have made wide-column databases popu‐\nlar in ecommerce, fintech, ad tech, IoT, and real-time personalization applications.\nData engineers must be aware of the operational characteristics of the wide-column\ndatabases they work with to set up a suitable configuration, design the schema,\nand choose an appropriate row key to optimize performance and avoid common\noperational issues.\nThese databases support rapid scans of massive amounts of data, but they do not sup‐\nport complex queries. They have only a single index (the row key) for lookups. Data\nengineers must generally extract data and send it to a secondary analytics system\nto run complex queries to deal with these limitations. This can be accomplished by\nrunning large scans for the extraction or employing CDC to capture an event stream.\nSource System Practical Details | 171\n3Martin Kleppmann, Designing Data-Intensive Applications  (Sebastopol, CA: O’Reilly, 2017), 49,\nhttps://oreil.ly/v1NhG .\n4Aashish Mehra, “Graph Database Market Worth $5.1 Billion by 2026: Exclusive Report by MarketsandMar‐\nkets, ” Cision PR Newswire, July 30, 2021, https://oreil.ly/mGVkY .Graph databases.    Graph databases  explicitly store data with a mathematical graph\nstructure (as a set of nodes and edges).3 Neo4j has proven extremely popular, while\nAmazon, Oracle, and other vendors offer their graph database products. Roughly\nspeaking, graph databases are a good fit when you want to analyze the connectivity\nbetween elements.\nFor example, you could use a document database to store one document for each\nuser describing their properties. Y ou could add an array element for connections\nthat contains directly connected users’ IDs in a social media context. It’s pretty easy\nto determine the number of direct connections a user has, but suppose you want\nto know how many users can be reached by traversing two direct connections.\nY ou could answer this question by writing complex code, but each query would\nrun slowly and consume significant resources. The document store is simply not\noptimized for this use case.\nGraph databases are designed for precisely this type of query. Their data structures\nallow for queries based on the connectivity between elements; graph databases are\nindicated when we care about understanding complex traversals between elements.\nIn the parlance of graphs, we store nodes  (users in the preceding example) and\nedges  (connections between users). Graph databases support rich data models for\nboth nodes and edges. Depending on the underlying graph database engine, graph\ndatabases utilize specialized query languages such as SPARQL, Resource Description\nFramework (RDF), Graph Query Language (GQL), and Cypher.\nAs an example of a graph, consider a network of four users. User 1 follows User 2,\nwho follows User 3 and User 4; User 3 also follows User 4 ( Figure 5-8 ).\nFigure 5-8. A social network graph\nWe anticipate that graph database applications will grow dramatically outside of tech\ncompanies; market analyses also predict rapid growth.4 Of course, graph databases\n172 | Chapter 5: Data Generation in Source Systems\nare beneficial from an operational perspective and support the kinds of complex\nsocial relationships critical to modern applications. Graph structures are also fas‐\ncinating from the perspective of data science and ML, potentially revealing deep\ninsights into human interactions and behavior.\nThis introduces unique challenges for data engineers who may be more accustomed\nto dealing with structured relations, documents, or unstructured data. Engineers\nmust choose whether to do the following:\n•Map source system graph data into one of their existing preferred paradigms•\n•Analyze graph data within the source system itself•\n•Adopt graph-specific analytics tools•\nGraph data can be reencoded into rows in a relational database, which may be a suit‐\nable solution depending on the analytics use case. Transactional graph databases are\nalso designed for analytics, although large queries may overload production systems.\nContemporary cloud-based graph databases support read-heavy graph analytics on\nmassive quantities of data.\nSearch.    A search database  is a nonrelational database used to search your data’s\ncomplex and straightforward semantic and structural characteristics. Two prominent\nuse cases exist for a search database: text search and log analysis. Let’s cover each of\nthese separately.\nText search  involves  searching a body of text for keywords or phrases, matching\non exact, fuzzy, or semantically similar matches. Log analysis  is typically used for\nanomaly detection, real-time monitoring, security analytics, and operational analyt‐\nics. Queries can be optimized and sped up with the use of indexes.\nDepending on the type of company you work at, you may use search databases either\nregularly or not at all. Regardless, it’s good to be aware they exist in case you come\nacross them in the wild. Search databases are popular for fast search and retrieval\nand can be found in various applications; an ecommerce site may power its product\nsearch using a search database. As a data engineer, you might be expected to bring\ndata from a search database (such as Elasticsearch, Apache Solr or Lucene, or Algolia)\ninto downstream KPI reports or something similar.\nTime series.    A time series  is a series of values organized by time. For example, stock\nprices might move as trades are executed throughout the day, or a weather sensor\nwill take atmospheric temperatures every minute. Any events that are recorded over\ntime—either regularly or sporadically—are time-series data. A time-series database  is\noptimized for retrieving and statistical processing of time-series data.\nSource System Practical Details | 173",27827
74-APIs.pdf,74-APIs,"While time-series data such as orders, shipments, logs, and so forth have been stored\nin relational databases for ages, these data sizes and volumes were often tiny. As\ndata grew faster and bigger, new special-purpose databases were needed. Time-series\ndatabases address the needs of growing, high-velocity data volumes from IoT, event\nand application logs, ad tech, and fintech, among many other use cases. Often these\nworkloads are write-heavy. As a result, time-series databases often utilize memory\nbuffering to support fast writes and reads.\nWe should distinguish between measurement and event-based data, common in\ntime-series databases. Measurement data  is generated regularly, such as temperature\nor air-quality sensors. Event-based data  is irregular and created every time an event\noccurs—for instance, when a motion sensor detects movement.\nThe schema for a time series typically contains a timestamp and a small set of fields.\nBecause the data is time-dependent, the data is ordered by the timestamp. This makes\ntime-series databases suitable for operational analytics but not great for BI use cases.\nJoins are not common, though some quasi time-series databases such as Apache\nDruid support joins. Many time-series databases are available, both as open source\nand paid options.\nAPIs\nAPIs  are now a standard and pervasive way of exchanging data in the cloud, for SaaS\nplatforms, and between internal company systems. Many types of API interfaces exist\nacross the web, but we are principally interested in those built around HTTP , the\nmost popular type on the web and in the cloud.\nREST\nWe’ll  first talk about REST, currently the dominant API paradigm. As noted in\nChapter 4 , REST  stands for representational state transfer . This set of practices and\nphilosophies for building HTTP web APIs was laid out by Roy Fielding in 2000 in\na PhD dissertation. REST is built around HTTP verbs, such as GET and PUT; in\npractice, modern REST uses only a handful of the verb mappings outlined in the\noriginal dissertation.\nOne of the principal ideas of REST is that interactions are stateless. Unlike in a Linux\nterminal session, there is no notion of a session with associated state variables such\nas a working directory; each REST call is independent. REST calls can change the\nsystem’s state, but these changes are global, applying to the full system rather than a\ncurrent session.\n174 | Chapter 5: Data Generation in Source Systems\n5For one example, see Michael S. Mikowski, “RESTful APIs: The Big Lie, ” August 10, 2015, https://oreil.ly/rqja3 .Critics point out that REST is in no way a full specification.5 REST stipulates basic\nproperties of interactions, but developers utilizing an API must gain a significant\namount of domain knowledge to build applications or pull data effectively.\nWe see great variation in levels of API abstraction. In some cases, APIs are merely\na thin wrapper over internals that provides the minimum functionality required to\nprotect the system from user requests. In other examples, a REST data API is a\nmasterpiece of engineering that prepares data for analytics applications and supports\nadvanced reporting.\nA couple of developments have simplified setting up data-ingestion pipelines from\nREST APIs. First, data providers frequently supply client libraries in various lan‐\nguages, especially in Python. Client libraries remove much of the boilerplate labor of\nbuilding API interaction code. Client libraries handle critical details such as authenti‐\ncation and map fundamental methods into accessible classes.\nSecond, various services and open source libraries have emerged to interact with\nAPIs and manage data synchronization. Many SaaS and open source vendors provide\noff-the-shelf connectors for common APIs. Platforms also simplify the process of\nbuilding custom connectors as required.\nThere are numerous data APIs without client libraries or out-of-the-box connector\nsupport. As we emphasize throughout the book, engineers would do well to reduce\nundifferentiated heavy lifting by using off-the-shelf tools. However, low-level plumb‐\ning tasks still consume many resources. At virtually any large company, data engineers\nwill need to deal with the problem of writing and maintaining custom code to\npull data from APIs, which requires understanding the structure of the data as\nprovided, developing appropriate data-extraction code, and determining a suitable\ndata synchronization strategy.\nGraphQL\nGraphQL  was created at Facebook as a query language for application data and an\nalternative to generic REST APIs. Whereas REST APIs generally restrict your queries\nto a specific data model, GraphQL opens up the possibility of retrieving multiple data\nmodels in a single request. This allows for more flexible and expressive queries than\nwith REST. GraphQL is built around JSON and returns data in a shape resembling\nthe JSON query.\nThere’s something of a holy war between REST and GraphQL, with some engineering\nteams partisans of one or the other and some using both. In reality, engineers will\nencounter both as they interact with source systems.\nSource System Practical Details | 175",5206
75-Third-Party Data Sources.pdf,75-Third-Party Data Sources,"Webhooks\nWebhooks  are a simple event-based data-transmission pattern. The data source can be\nan application backend, a web page, or a mobile app. When specified events happen\nin the source system, this triggers a call to an HTTP endpoint hosted by the data\nconsumer. Notice that the connection goes from the source system to the data sink,\nthe opposite of typical APIs. For this reason, webhooks are often called reverse APIs .\nThe endpoint can do various things with the POST event data, potentially triggering\na downstream process or storing the data for future use. For analytics purposes, we’re\ninterested in collecting these events. Engineers commonly use message queues to\ningest data at high velocity and volume. We will talk about message queues and event\nstreams later in this chapter.\nRPC and gRPC\nA remote procedure call  (RPC) is commonly used in distributed computing. It allows\nyou to run a procedure on a remote system.\ngRPC  is a remote procedure call library developed internally at Google in 2015 and\nlater released as an open standard. Its use at Google alone would be enough to merit\ninclusion in our discussion. Many Google services, such as Google Ads and GCP ,\noffer gRPC APIs. gRPC is built around the Protocol Buffers open data serialization\nstandard, also developed by Google.\ngRPC emphasizes the efficient bidirectional exchange of data over HTTP/2. Efficiency\nrefers to aspects such as CPU utilization, power consumption, battery life, and band‐\nwidth. Like GraphQL, gRPC imposes much more specific technical standards than\nREST, thus allowing the use of common client libraries and allowing engineers to\ndevelop a skill set that will apply to any gRPC interaction code.\nData Sharing\nThe core concept of cloud data sharing is that a  multitenant system supports security\npolicies for sharing data among tenants. Concretely, any public cloud object storage\nsystem with a fine-grained permission system can be a platform for data sharing.\nPopular cloud data-warehouse platforms also support data-sharing capabilities. Of\ncourse, data can also be shared through download or exchange over email, but a\nmultitenant system makes the process much easier.\nMany modern sharing platforms (especially cloud data warehouses) support row,\ncolumn, and sensitive data filtering. Data sharing also streamlines the notion  of\nthe data marketplace , available on several popular clouds and data platforms. Data\nmarketplaces provide a centralized location for data commerce, where data providers\ncan advertise their offerings and sell them without worrying about the details of\nmanaging network access to data systems.\n176 | Chapter 5: Data Generation in Source Systems",2716
76-Message Queues and Event-Streaming Platforms.pdf,76-Message Queues and Event-Streaming Platforms,"6Martin Fowler, “How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh, ” Martin‐\nFowler.com, May 20, 2019, https://oreil.ly/TEdJF .Data sharing can also streamline data pipelines within an organization. Data sharing\nallows units of an organization to manage their data and selectively share it with\nother units while still allowing individual units to manage their compute and query\ncosts separately, facilitating data decentralization. This facilitates decentralized data\nmanagement patterns such as data mesh.6\nData sharing and data mesh align closely with our philosophy of common architec‐\nture components. Choose common components (see Chapter 3 ) that allow the simple\nand efficient interchange of data and expertise rather than embracing the most\nexciting and sophisticated technology.\nThird-Party Data Sources\nThe consumerization of technology means every company is essentially now a\ntechnology company. The consequence is that these companies—and increasingly\ngovernment agencies—want to make their data available to their customers and\nusers, either as part of their service or as a separate subscription. For example, the\nUS Bureau of Labor Statistics publishes various statistics about the US labor market.\nThe National Aeronautics and Space Administration (NASA) publishes various data\nfrom its research initiatives. Facebook shares data with businesses that advertise on its\nplatform.\nWhy would companies want to make their data available? Data is sticky, and a\nflywheel is created by allowing users to integrate and extend their application into a\nuser’s application. Greater user adoption and usage means more data, which means\nusers can integrate more data into their applications and data systems. The side effect\nis there are now almost infinite sources of third-party data.\nDirect third-party data access is commonly done via APIs, through data sharing on\na cloud platform, or through data download. APIs often provide deep integration\ncapabilities, allowing customers to pull and push data. For example, many CRMs\noffer APIs that their users can integrate into their systems and applications. We see\na common workflow to get data from a CRM, blend the CRM data through the\ncustomer scoring model, and then use reverse ETL to send that data back into CRM\nfor salespeople to contact better-qualified leads.\nMessage Queues and Event-Streaming Platforms\nEvent-driven architectures are pervasive in software applications and are poised to\ngrow their popularity even further. First, message queues and event-streaming plat‐\nforms—critical layers in event-driven architectures—are easier to set up and manage\nSource System Practical Details | 177\nin a cloud environment. Second, the rise of data apps—applications that directly\nintegrate real-time analytics—are growing from strength to strength. Event-driven\narchitectures are ideal in this setting because events can both trigger work in the\napplication and feed near real-time analytics.\nPlease note that streaming data (in this case, messages and streams) cuts across\nmany data engineering lifecycle stages. Unlike an RDBMS, which is often directly\nattached to an application, the lines of streaming data are sometimes less clear-cut.\nThese systems are used as source systems, but they will often cut across the data\nengineering lifecycle because of their transient nature. For example, you can use\nan event-streaming platform for message passing in an event-driven application, a\nsource system. The same event-streaming platform can be used in the ingestion and\ntransformation stage to process data for real-time analytics.\nAs source systems, message queues and event-streaming platforms are used in\nnumerous ways, from routing messages between microservices ingesting millions\nof events per second of event data from web, mobile, and IoT applications. Let’s look\nat message queues and event-streaming platforms a bit more closely.\nMessage queues\nA message queue  is a mechanism to asynchronously send data (usually as small\nindividual messages, in the kilobytes) between discrete systems using a publish and\nsubscribe model. Data is published to a message queue and is delivered to one or\nmore subscribers ( Figure 5-9 ). The subscriber acknowledges receipt of the message,\nremoving it from the queue.\nFigure 5-9. A simple message queue\nMessage queues allow applications and systems to be decoupled from each other and\nare widely used in microservices architectures. The message queue buffers messages\nto handle transient load spikes and makes messages durable through a distributed\narchitecture with replication.\nMessage queues are a critical ingredient for decoupled microservices and event-\ndriven architectures. Some things to keep in mind with message queues are frequency\nof delivery, message ordering, and scalability.\nMessage ordering and delivery.    The order in which messages are created, sent, and\nreceived can significantly impact downstream subscribers. In general, order in\n178 | Chapter 5: Data Generation in Source Systems\n7Whether exactly once  is possible is a semantical debate. Technically, exactly once delivery is impossible to\nguarantee, as illustrated by the Two Generals Problem .distributed  message queues is a tricky problem. Message queues often apply a fuzzy\nnotion of order and first in, first out (FIFO). Strict FIFO means that if message A is\ningested before message B, message A will always be delivered before message B. In\npractice, messages might be published and received out of order, especially in highly\ndistributed message systems.\nFor example, Amazon SQS standard queues  make the best effort to preserve message\norder. SQS also offers FIFO queues , which offer much stronger guarantees at the cost\nof extra overhead.\nIn general, don’t assume that your messages will be delivered in order unless your\nmessage queue technology guarantees it. Y ou typically need to design for out-of-\norder message delivery.\nDelivery frequency.    Messages can be sent exactly once or at least once. If a message\nis sent exactly once , then after the subscriber acknowledges the message, the message\ndisappears and won’t be delivered again.7 Messages sent at least once  can be consumed\nby multiple subscribers or by the same subscriber more than once. This is great when\nduplications or redundancy don’t matter.\nIdeally, systems should be idempotent . In an idempotent system, the outcome of\nprocessing a message once is identical to the outcome of processing it multiple times.\nThis helps to account for a variety of subtle scenarios. For example, even if our system\ncan guarantee exactly-once delivery, a consumer might fully process a message but\nfail right before acknowledging processing. The message will effectively be processed\ntwice, but an idempotent system handles this scenario gracefully.\nScalability.    The most popular message queues utilized in event-driven applications\nare horizontally scalable, running across multiple servers. This allows these queues\nto scale up and down dynamically, buffer messages when systems fall behind, and\ndurably store messages for resilience against failure. However, this can create a variety\nof complications, as mentioned previously (multiple deliveries and fuzzy ordering).\nEvent-streaming platforms\nIn some ways, an event-streaming platform  is a continuation of a message queue in\nthat messages are passed from producers to consumers. As discussed previously in\nthis chapter, the big difference between messages and streams is that a message queue\nis primarily used to route messages with certain delivery guarantees. In contrast, an\nevent-streaming platform is used to ingest and process data in an ordered log of\nSource System Practical Details | 179\nrecords. In an event-streaming platform, data is retained for a while, and it is possible\nto replay messages from a past point in time.\nLet’s describe an event related to an event-streaming platform. As mentioned in\nChapter 3 , an event is “something that happened, typically a change in the  state  of\nsomething. ” An event has the following features: a key, a value, and a timestamp.\nMultiple  key-value timestamps might be contained in a single event. For example, an\nevent for an ecommerce order might look like this:\n{\n  ""Key"":""Order # 12345"" ,\n  ""Value"":""SKU 123, purchase price of $100"" ,\n  ""Timestamp"" :""2023-01-02 06:01:00""\n}\nLet’s look at some of the critical characteristics of an event-streaming platform that\nyou should be aware of as a data engineer.\nTopics.    In an event-streaming platform, a producer streams events to a topic, a\ncollection of related events. A topic might contain fraud alerts, customer orders, or\ntemperature readings from IoT devices, for example. A topic can have zero, one, or\nmultiple producers and customers on most event-streaming platforms.\nUsing the preceding event example, a topic might be web orders . Also, let’s send\nthis topic to a couple of consumers, such as fulfillment  and marketing . This is\nan excellent example of blurred lines between analytics and an event-driven system.\nThe fulfillment  subscriber will use events to trigger a fulfillment process, while\nmarketing  runs real-time analytics or trains and runs ML models to tune marketing\ncampaigns ( Figure 5-10 ).\nFigure 5-10. An order-processing system generates events (small squares) and publishes\nthem to the web orders  topic. Two subscribers— marketing  and fulfillment —pull\nevents from the topic.\nStream partitions.    Stream partitions  are subdivisions of a stream into multiple\nstreams. A good analogy is a multilane freeway. Having multiple lanes allows for\nparallelism and higher throughput. Messages are distributed across partitions by\npartition key . Messages with the same partition key will always end up in the same\npartition.\n180 | Chapter 5: Data Generation in Source Systems",9968
77-Whom Youll Work With.pdf,77-Whom Youll Work With,"In Figure 5-11 , for example, each message has a numeric ID— shown inside the circle\nrepresenting the message—that we use as a partition key. To determine the partition,\nwe divide by 3 and take the remainder. Going from bottom to top, the partitions have\nremainder 0, 1, and 2, respectively.\nFigure 5-11. An incoming message stream broken into three partitions\nSet a partition key so that messages that should be processed together have the same\npartition key. For example, it is common in IoT settings to want to send all messages\nfrom a particular device to the same processing server. We can achieve this by using a\ndevice ID as the partition key, and then setting up one server to consume from each\npartition.\nA key concern with stream partitioning is ensuring that your partition key does\nnot generate hotspotting —a disproportionate number of messages delivered to one\npartition. For example, if each IoT device were known to be located in a particular\nUS state, we might use the state as the partition key. Given a device distribution\nproportional to state population, the partitions containing California, Texas, Florida,\nand New Y ork might be overwhelmed, with other partitions relatively underutilized.\nEnsure that your partition key will distribute messages evenly across partitions.\nFault tolerance and resilience.    Event-streaming platforms are typically distributed sys‐\ntems, with streams stored on various nodes. If a node goes down, another node\nreplaces it, and the stream is still accessible. This means records aren’t lost; you may\nchoose to delete records, but that’s another story. This fault tolerance and resilience\nmake streaming platforms a good choice when you need a system that can reliably\nproduce, store, and ingest event data.\nWhom You’ll Work With\nWhen  accessing source systems, it’s essential to understand the people with\nwhom you’ll work. In our experience, good diplomacy and relationships with the\nWhom You’ll Work With | 181\n8James Denmore, Data Pipelines Pocket Reference  (Sebastopol, CA: O’Reilly), https://oreil.ly/8QdkJ . Read the\nbook for more information on how a data contract should be written.stakeholders  of source systems are an underrated and crucial part of successful data\nengineering.\nWho are these stakeholders? Typically, you’ll deal with two categories of stakeholders:\nsystems and data stakeholders ( Figure 5-12 ). A systems stakeholder  builds and main‐\ntains the source systems; these might be software engineers, application developers,\nand third parties. Data stakeholders own and control access to the data you want,\ngenerally handled by IT, a data governance group, or third parties. The systems and\ndata stakeholders are often different people or teams; sometimes, they are the same.\nFigure 5-12. The data engineer’s upstream stakeholders\nY ou’re often at the mercy of the stakeholder’s ability to follow correct software engi‐\nneering, database management, and development practices. Ideally, the stakeholders\nare doing DevOps and working in an agile manner. We suggest creating a feedback\nloop between data engineers and stakeholders of the source systems to create aware‐\nness of how data is consumed and used. This is among the single most overlooked\nareas where data engineers can get a lot of value. When something happens to\nthe upstream source data—and something will happen, whether it’s a schema or\ndata change, a failed server or database, or other important events—you want to\nmake sure that you’re made aware of the impact these issues will have on your data\nengineering systems.\nIt might help to have a data contract in place with your upstream source system\nowners. What is a data contract? James Denmore offers this definition:8\nA data contract is a written agreement between the owner of a source system and the\nteam ingesting data from that system for use in a data pipeline. The contract should\nstate what data is being extracted, via what method (full, incremental), how often,\nas well as who (person, team) are the contacts for both the source system and the\ningestion. Data contracts should be stored in a well-known and easy-to-find location\nsuch as a GitHub repo or internal documentation site. If possible, format data contracts\nin a standardized form so they can be integrated into the development process or\nqueried programmatically.\n182 | Chapter 5: Data Generation in Source Systems",4440
78-Undercurrents and Their Impact on Source Systems.pdf,78-Undercurrents and Their Impact on Source Systems,,0
79-Data Management.pdf,79-Data Management,"In addition, consider establishing an SLA with upstream providers. An SLA provides\nexpectations of what you can expect from the source systems you rely upon. An\nexample of an SLA might be “data from source systems will be reliably available and\nof high quality. ” A service-level objective (SLO) measures performance against what\nyou’ve agreed to in the SLA. For example, given your example SLA, an SLO might\nbe “source systems will have 99% uptime. ” If a data contract or SLA/SLO seems too\nformal, at least verbally set expectations for source system guarantees for uptime, data\nquality, and anything else of importance to you. Upstream owners of source systems\nneed to understand your requirements so they can provide you with the data you\nneed.\nUndercurrents and Their Impact on Source Systems\nUnlike  other parts of the data engineering lifecycle, source systems are generally\noutside the control of the data engineer. There’s an implicit assumption (some might\ncall it hope ) that the stakeholders and owners of the source systems—and the data\nthey produce—are following best practices concerning data management, DataOps\n(and DevOps), DODD (mentioned in Chapter 2 ) data architecture, orchestration,\nand software engineering. The data engineer should get as much upstream support as\npossible to ensure that the undercurrents are applied when data is generated in source\nsystems. Doing so will make the rest of the steps in the data engineering lifecycle\nproceed a lot more smoothly.\nHow do the undercurrents impact source systems? Let’s have a look.\nSecurity\nSecurity  is critical, and the last thing you want is to accidentally create a point of\nvulnerability in a source system. Here are some areas to consider:\n•Is the source system architected so data is secure and encrypted, both with data at•\nrest and while data is transmitted?\n•Do you have to access the source system over the public internet, or are you•\nusing a virtual private network (VPN)?\n•Keep passwords, tokens, and credentials to the source system securely locked•\naway. For example, if you’re using Secure Shell (SSH) keys, use a key manager to\nprotect your keys; the same rule applies to passwords—use a password manager\nor a single sign-on (SSO) provider.\n•Do you trust the source system? Always be sure to trust but verify that the source•\nsystem is legitimate. Y ou don’t want to be on the receiving end of data from a\nmalicious actor.\nUndercurrents and Their Impact on Source Systems | 183",2505
80-Software Engineering.pdf,80-Software Engineering,"Data Management\nData management of source systems is challenging for data engineers. In most cases,\nyou will have only peripheral control—if any control at all—over source systems and\nthe data they produce. To the extent possible, you should understand the way data\nis managed in source systems since this will directly influence how you ingest, store,\nand transform the data.\nHere are some areas to consider:\nData governance\nAre upstream data and systems governed in a reliable, easy-to-understand fash‐\nion? Who manages the data?\nData quality\nHow do you ensure data quality and integrity in upstream systems? Work with\nsource system teams to set expectations on data and communication.\nSchema\nExpect that upstream schemas will change. Where possible, collaborate with\nsource system teams to be notified of looming schema changes.\nMaster data management\nIs the creation of upstream records controlled by a master data management\npractice or system?\nPrivacy and ethics\nDo you have access to raw data, or will the data be obfuscated? What are the\nimplications of the source data? How long is it retained? Does it shift locations\nbased on retention policies?\nRegulatory\nBased upon regulations, are you supposed to access the data?\nDataOps\nOperational  excellence—DevOps, DataOps, MLOps, XOps—should extend up and\ndown the entire stack and support the data engineering and lifecycle. While this is\nideal, it’s often not fully realized.\nBecause you’re working with stakeholders who control both the source systems and\nthe data they produce, you need to ensure that you can observe and monitor the\nuptime and usage of the source systems and respond when incidents occur. For\nexample, when the application database you depend on for CDC exceeds its I/O\ncapacity and needs to be rescaled, how will that affect your ability to receive data from\nthis system? Will you be able to access the data, or will it be unavailable until the\ndatabase is rescaled? How will this affect reports? In another example, if the software\n184 | Chapter 5: Data Generation in Source Systems\nengineering team is continuously deploying, a code change may cause unanticipated\nfailures in the application itself. How will the failure impact your ability to access the\ndatabases powering the application? Will the data be up-to-date?\nSet up a clear communication chain between data engineering and the teams\nsupporting the source systems. Ideally, these stakeholder teams have incorporated\nDevOps into their workflow and culture. This will go a long way to accomplishing\nthe goals of DataOps (a sibling of DevOps), to address and reduce errors quickly.\nAs we mentioned earlier, data engineers need to weave themselves into the DevOps\npractices of stakeholders, and vice versa. Successful DataOps works when all people\nare on board and focus on making systems holistically work.\nA few DataOps considerations are as follows:\nAutomation\nThere’s the automation impacting the source system, such as code updates and\nnew features. Then there’s the DataOps automation that you’ve set up for your\ndata workflows. Does an issue in the source system’s automation impact your\ndata workflow automation? If so, consider decoupling these systems so they can\nperform automation independently.\nObservability\nHow will you know when there’s an issue with a source system, such as an\noutage or a data-quality issue? Set up monitoring for source system uptime (or\nuse the monitoring created by the team that owns the source system). Set up\nchecks to ensure that data from the source system conforms with expectations\nfor downstream usage. For example, is the data of good quality? Is the schema\nconformant? Are customer records consistent? Is data hashed as stipulated by the\ninternal policy?\nIncident response\nWhat’s your plan if something bad happens? For example, how will your data\npipeline behave if a source system goes offline? What’s your plan to backfill the\n“lost” data once the source system is back online?\nData Architecture\nSimilar to data management, unless you’re involved in the design and maintenance\nof the source system architecture, you’ll have little impact on the upstream source\nsystem architecture. Y ou should also understand how the upstream architecture is\ndesigned and its strengths and weaknesses. Talk often with the teams responsible for\nthe source systems to understand the factors discussed in this section and ensure that\ntheir systems can meet your expectations. Knowing where the architecture performs\nwell and where it doesn’t will impact how you design your data pipeline.\nUndercurrents and Their Impact on Source Systems | 185\nHere are some things to consider regarding source system architectures:\nReliability\nAll systems suffer from entropy at some point, and outputs will drift from what’s\nexpected. Bugs are introduced, and random glitches happen. Does the system\nproduce predictable outputs? How often can we expect the system to fail? What’s\nthe mean time to repair to get the system back to sufficient reliability?\nDurability\nEverything fails. A server might die, a cloud’s zone or region could go offline,\nor other issues may arise. Y ou need to account for how an inevitable failure\nor outage will affect your managed data systems. How does the source system\nhandle data loss from hardware failures or network outages? What’s the plan\nfor handling outages for an extended period and limiting the blast radius of an\noutage?\nAvailability\nWhat guarantees that the source system is up, running, and available when it’s\nsupposed to be?\nPeople\nWho’s in charge of the source system’s design, and how will you know if breaking\nchanges are made in the architecture? A data engineer needs to work with the\nteams who maintain the source systems and ensure that these systems are archi‐\ntected reliably. Create an SLA with the source system team to set expectations\nabout potential system failure.\nOrchestration\nWhen  orchestrating within your data engineering workflow, you’ll primarily be con‐\ncerned with making sure your orchestration can access the source system, which\nrequires the correct network access, authentication, and authorization.\nHere are some things to think about concerning orchestration for source systems:\nCadence and frequency\nIs the data available on a fixed schedule, or can you access new data whenever\nyou want?\nCommon frameworks\nDo the software and data engineers use the same container manager, such as\nKubernetes? Would it make sense to integrate application and data workloads\ninto the same Kubernetes cluster? If you’re using an orchestration framework\nlike Airflow, does it make sense to integrate it with the upstream application\nteam? There’s no correct answer here, but you need to balance the benefits of\nintegration with the risks of tight coupling.\n186 | Chapter 5: Data Generation in Source Systems",6932
81-Chapter 6. Storage.pdf,81-Chapter 6. Storage,"Software Engineering\nAs the data landscape shifts to tools that simplify and automate access to source\nsystems, you’ll likely need to write code. Here are a few considerations when writing\ncode to access a source system:\nNetworking\nMake sure your code will be able to access the network where the source sys‐\ntem resides. Also, always think about secure networking. Are you accessing an\nHTTPS URL over the public internet, SSH, or a VPN?\nAuthentication and authorization\nDo you have the proper credentials (tokens, username/passwords) to access the\nsource system? Where will you store these credentials so they don’t appear in\nyour code or version control? Do you have the correct IAM roles to perform the\ncoded tasks?\nAccess patterns\nHow are you accessing the data? Are you using an API, and how are you\nhandling REST/GraphQL requests, response data volumes, and pagination? If\nyou’re accessing data via a database driver, is the driver compatible with the\ndatabase you’re accessing? For either access pattern, how are things like retries\nand timeouts handled?\nOrchestration\nDoes your code integrate with an orchestration framework, and can it be exe‐\ncuted as an orchestrated workflow?\nParallelization\nHow are you managing and scaling parallel access to source systems?\nDeployment\nHow are you handling the deployment of source code changes?\nConclusion\nSource systems and their data are vital in the data engineering lifecycle. Data engi‐\nneers tend to treat source systems as “someone else’s problem”—do this at your peril!\nData engineers who abuse source systems may need to look for another job when\nproduction goes down.\nIf there’s a stick, there’s also a carrot. Better collaboration with source system teams\ncan lead to higher-quality data, more successful outcomes, and better data products.\nCreate a bidirectional flow of communications with your counterparts on these\nteams; set up processes to notify of schema and application changes that affect\nConclusion | 187\nanalytics and ML. Communicate your data needs proactively to assist application\nteams in the data engineering process.\nBe aware that the integration between data engineers and source system teams is\ngrowing. One example is reverse ETL, which has long lived in the shadows but has\nrecently risen into prominence. We also discussed that the event-streaming platform\ncould serve a role in event-driven architectures and analytics; a source system can\nalso be a data engineering system. Build shared systems where it makes sense to do\nso.\nLook for opportunities to build user-facing data products. Talk to application teams\nabout analytics they would like to present to their users or places where ML could\nimprove the user experience. Make application teams stakeholders in data engineer‐\ning, and find ways to share your successes.\nNow that you understand the types of source systems and the data they generate, we’ll\nnext look at ways to store this data.\nAdditional Resources\n•Confluent’s “Schema Evolution and Compatibility” documentation •\n•Database Internals  by Alex Petrov (O’Reilly) •\n•Database System Concepts  by Abraham (Avi) Silberschatz et al. (McGraw Hill) •\n•“The Log: What Every Software Engineer Should Know About Real-Time Data’s•\nUnifying Abstraction”  by Jay Kreps\n•“Modernizing Business Data Indexing”  by Benjamin Douglas and Mohammad •\nMohtasham\n•“NoSQL: What’s in a Name”  by Eric Evans •\n•“Test Data Quality at Scale with Deequ”  by Dustin Lange et al. •\n•“The What, Why, and When of Single-Table Design with DynamoDB”  by Alex •\nDeBrie\n188 | Chapter 5: Data Generation in Source Systems\nCHAPTER 6\nStorage\nStorage is the cornerstone of the data engineering lifecycle ( Figure 6-1 ) and underlies\nits major stages—ingestion, transformation, and serving. Data gets stored many times\nas it moves through the lifecycle. To paraphrase an old saying, it’s storage all the\nway down. Whether data is needed seconds, minutes, days, months, or years later, it\nmust persist in storage until systems are ready to consume it for further processing\nand transmission. Knowing the use case of the data and the way you will retrieve it\nin the future is the first step to choosing the proper storage solutions for your data\narchitecture.\nFigure 6-1. Storage plays a central role in the data engineering lifecycle\n189\nWe also discussed storage in Chapter 5 , but with a difference in focus and domain of\ncontrol. Source systems are generally not maintained or controlled by data engineers.\nThe storage that data engineers handle directly, which we’ll focus on in this chapter,\nencompasses  the data engineering lifecycle stages of ingesting data from source\nsystems to serving data to deliver value with analytics, data science, etc. Many forms\nof storage undercut the entire data engineering lifecycle in some fashion.\nTo understand storage, we’re going to start by studying the raw ingredients  that\ncompose storage systems, including hard drives, solid state drives, and system mem‐\nory (see Figure 6-2 ). It’s essential to understand the basic characteristics of physical\nstorage technologies to assess the trade-offs inherent in any storage architecture. This\nsection also discusses serialization and compression, key software elements of practi‐\ncal storage. (We defer a deeper technical discussion of serialization and compression\nto Appendix A .) We also discuss caching , which is critical in assembling storage\nsystems.\nFigure 6-2. Raw ingredients, storage systems, and storage abstractions\n190 | Chapter 6: Storage",5593
82-Raw Ingredients of Data Storage.pdf,82-Raw Ingredients of Data Storage,,0
83-Magnetic Disk Drive.pdf,83-Magnetic Disk Drive,"1Andy Klein, “Hard Disk Drive (HDD) vs. Solid-State Drive (SSD): What’s the Diff?, ” Backblaze blog,\nOctober 5, 2021, https://oreil.ly/XBps8 .Next, we’ll look at storage systems . In practice, we don’t directly access system mem‐\nory or hard disks. These physical storage components exist inside servers and clusters\nthat can ingest and retrieve data using various access paradigms.\nFinally, we’ll look at storage abstractions . Storage systems are assembled into a cloud\ndata warehouse, a data lake, etc. When building data pipelines, engineers choose\nthe appropriate abstractions for storing their data as it moves through the ingestion,\ntransformation, and serving stages.\nRaw Ingredients of Data Storage\nStorage is so common that it’s easy to take it for granted. We’re often surprised by the\nnumber of software and data engineers who use storage every day but have little idea\nhow it works behind the scenes or the trade-offs inherent in various storage media.\nAs a result, we see storage used in some pretty...interesting ways. Though current\nmanaged services potentially free data engineers from the complexities of managing\nservers, data engineers still need to be aware of underlying components’ essential\ncharacteristics, performance considerations, durability, and costs.\nIn most data architectures, data frequently passes through magnetic storage, SSDs,\nand memory as it works its way through the various processing phases of a data\npipeline. Data storage and query systems generally follow complex recipes involving\ndistributed systems, numerous services, and multiple hardware storage layers. These\nsystems require the right raw ingredients to function correctly.\nLet’s look at some of the raw ingredients of data storage: disk drives, memory,\nnetworking and CPU, serialization, compression, and caching.\nMagnetic Disk Drive\nMagnetic disks  utilize spinning platters coated with a ferromagnetic film ( Figure 6-3 ).\nThis film is magnetized by a read/write head during write operations to physically\nencode binary data. The read/write head detects the magnetic field and outputs a bit‐\nstream during read operations. Magnetic disk drives have been around for ages. They\nstill form the backbone of bulk data storage systems because they are significantly\ncheaper than SSDs per gigabyte of stored data.\nOn the one hand, these disks have seen extraordinary improvements in perfor‐\nmance, storage density, and cost.1 On the other hand, SSDs dramatically outperform\nmagnetic disks on various metrics. Currently, commercial magnetic disk drives\ncost roughly 3 cents per gigabyte of capacity. (Note that we’ll frequently use the\nRaw Ingredients of Data Storage | 191\nabbreviations  HDD  and SSD to denote rotating magnetic disk and solid-state drives,\nrespectively.)\nFigure 6-3. Magnetic disk head movement and rotation are essential in random access\nlatency\nIBM developed magnetic disk drive technology in the 1950s. Since then, magnetic\ndisk capacities have grown steadily. The first commercial magnetic disk drive, the\nIBM 350, had a capacity of 3.75 megabytes. As of this writing, magnetic drives\nstoring 20 TB are commercially available. In fact, magnetic disks continue to see\nrapid innovation, with methods such as heat-assisted magnetic recording (HAMR),\nshingled magnetic recording (SMR), and helium-filled disk enclosures being used\nto realize ever greater storage densities. In spite of the continuing improvements in\ndrive capacity, other aspects of HDD performance are hampered by physics.\nFirst, disk transfer speed , the rate at which data can be read and written, does not\nscale in proportion with disk capacity. Disk capacity scales with  areal density  (gigabits\nstored per square inch), whereas transfer speed scales with  linear density  (bits per\ninch). This means that if disk capacity grows by a factor of 4, transfer speed increases\nby only a factor of 2. Consequently, current data center drives support maximum\ndata transfer speeds of 200–300 MB/s. To frame this another way, it takes more than\n20 hours to read the entire contents of a 30 TB magnetic drive, assuming a transfer\nspeed of 300 MB/s.\nA second major limitation is seek time. To access data, the drive must physically\nrelocate the read/write heads to the appropriate track on the disk. Third, in order\nto find a particular piece of data on the disk, the disk controller must wait for that\ndata to rotate under the read/write heads. This leads to rotational latency . Typical\n192 | Chapter 6: Storage",4552
84-Networking and CPU.pdf,84-Networking and CPU,"commercial drives spinning at 7,200 revolutions per minute (RPM) seek time, and\nrotational latency, leads to over four milliseconds of overall average latency (time to\naccess a selected piece of data). A fourth limitation is input/output operations per\nsecond (IOPS), critical for transactional databases. A magnetic drive ranges from 50\nto 500 IOPS.\nVarious tricks can improve latency and transfer speed. Using a higher rotational\nspeed can increase transfer rate and decrease rotational latency. Limiting the radius\nof the disk platter or writing data into only a narrow band on the disk reduces seek\ntime. However, none of these techniques makes magnetic drives remotely competitive\nwith SSDs for random access lookups. SSDs can deliver data with significantly lower\nlatency, higher IOPS, and higher transfer speeds, partially because there is no physi‐\ncally rotating disk or magnetic head to wait for.\nAs mentioned earlier, magnetic disks are still prized in data centers for their low data-\nstorage costs. In addition, magnetic drives can sustain extraordinarily high transfer\nrates through parallelism. This is the critical idea behind cloud object storage: data\ncan be distributed across thousands of disks in clusters. Data-transfer rates go up\ndramatically by reading from numerous disks simultaneously, limited primarily by\nnetwork performance rather than disk transfer rate. Thus, network components and\nCPUs are also key raw ingredients in storage systems, and we will return to these\ntopics shortly.\nSolid-State Drive\nSolid-state drives  (SSDs) store data as charges in flash memory cells. SSDs eliminate\nthe mechanical components of magnetic drives; the data is read by purely electronic\nmeans. SSDs can look up random data in less than 0.1 ms (100 microseconds). In\naddition, SSDs can scale both data-transfer speeds and IOPS by slicing storage into\npartitions with numerous storage controllers running in parallel. Commercial SSDs\ncan support transfer speeds of many gigabytes per second and tens of thousands of\nIOPS.\nBecause of these exceptional performance characteristics, SSDs have revolutionized\ntransactional databases and are the accepted standard for commercial deployments of\nOLTP systems. SSDs allow relational databases such as PostgreSQL, MySQL, and SQL\nServer to handle thousands of transactions per second.\nHowever, SSDs are not currently the default option for high-scale analytics data\nstorage. Again, this comes down to cost. Commercial SSDs typically cost 20–30 cents\n(USD) per gigabyte of capacity, nearly 10 times the cost per capacity of a magnetic\ndrive. Thus, object storage on magnetic disks has emerged as the leading option for\nlarge-scale data storage in data lakes and cloud data warehouses.\nRaw Ingredients of Data Storage | 193\nSSDs still play a significant role in OLAP systems. Some OLAP databases leverage\nSSD caching to support high-performance queries on frequently accessed data. As\nlow-latency OLAP becomes more popular, we expect SSD usage in these systems to\nfollow suit.\nRandom Access Memory\nWe commonly use the terms random access memory  (RAM) and memory  inter‐\nchangeably. Strictly speaking, magnetic drives and SSDs also serve as memory\nthat stores data for later random access retrieval, but RAM has several specific\ncharacteristics:\n•It is attached to a CPU and mapped into CPU address space.•\n•It stores the code that CPUs execute and the data that this code directly processes.•\n•It is volatile , while magnetic drives and SSDs are nonvolatile . Though they may •\noccasionally fail and corrupt or lose data, drives generally retain data when\npowered off. RAM loses data in less than a second when it is unpowered.\n•It offers significantly higher transfer speeds and faster retrieval times than SSD•\nstorage. DDR5 memory—the latest widely used standard for RAM—offers data\nretrieval latency on the order of 100 ns, roughly 1,000 times faster than SSD. A\ntypical CPU can support 100 GB/s bandwidth to attached memory and millions\nof IOPS. (Statistics vary dramatically depending on the number of memory\nchannels and other configuration details.)\n•It is significantly more expensive than SSD storage, at roughly $10/GB (at the•\ntime of this writing).\n•It is limited in the amount of RAM attached to an individual CPU and memory•\ncontroller. This adds further to complexity and cost. High-memory servers typ‐\nically utilize many interconnected CPUs on one board, each with a block of\nattached RAM.\n•It is still significantly slower than CPU cache, a type of memory located directly•\non the CPU die or in the same package. Cache stores frequently and recently\naccessed data for ultrafast retrieval during processing. CPU designs incorporate\nseveral layers of cache of varying size and performance characteristics.\nWhen we talk about system memory, we almost always mean  dynamic RAM , a\nhigh-density, low-cost form of memory. Dynamic RAM stores data as charges in\ncapacitors. These capacitors leak over time, so the data must be frequently refreshed\n(read and rewritten) to prevent data loss. The hardware memory controller handles\nthese technical details; data engineers simply need to worry about bandwidth and\nretrieval latency characteristics. Other forms of memory, such as static RAM , are used\nin specialized applications such as CPU caches.\n194 | Chapter 6: Storage",5410
85-Caching.pdf,85-Caching,"Current CPUs virtually always employ the von Neumann architecture , with code and\ndata stored together in the same memory space. However, CPUs typically also sup‐\nport the option to disable code execution in specific pages of memory for enhanced\nsecurity. This feature is reminiscent of the  Harvard architecture , which separates code\nand data.\nRAM is used in various storage and processing systems and can be used for caching,\ndata processing, or indexes. Several databases treat RAM as a primary storage layer,\nallowing ultra-fast read and write performance. In these applications, data engineers\nmust always keep in mind the volatility of RAM. Even if data stored in memory is\nreplicated across a cluster, a power outage that brings down several nodes could cause\ndata loss. Architectures intended to durably store data may use battery backups and\nautomatically dump all data to disk in the event of power loss.\nNetworking and CPU\nWhy  are we mentioning networking and CPU as raw ingredients for storing data?\nIncreasingly, storage systems are distributed to enhance performance, durability, and\navailability. We mentioned specifically that individual magnetic disks offer relatively\nlow-transfer performance, but a cluster of disks parallelizes reads for significant per‐\nformance scaling. While storage standards such as redundant arrays of independent\ndisks (RAID) parallelize on a single server, cloud object storage clusters operate at\na much larger scale, with disks distributed across a network and even multiple data\ncenters and availability zones.\nAvailability zones  are a standard cloud construct consisting of compute environments\nwith independent power, water, and other resources. Multizonal storage enhances\nboth the availability and durability of data.\nCPUs handle the details of servicing requests, aggregating reads, and distributing\nwrites. Storage becomes a web application with an API, backend service components,\nand load balancing. Network device performance and network topology are key\nfactors in realizing high performance.\nData engineers need to understand how networking will affect the systems they\nbuild and use. Engineers constantly balance the durability and availability achieved\nby spreading out data geographically versus the performance and cost benefits of\nkeeping storage in a small geographic area and close to data consumers or writers.\nAppendix B  covers cloud networking and major relevant ideas.\nSerialization\nSerialization  is another raw storage ingredient and a critical element of database\ndesign. The decisions around serialization will inform how well queries perform\nacross a network, CPU overhead, query latency, and more. Designing a data lake, for\nRaw Ingredients of Data Storage | 195\nexample, involves choosing a base storage system (e.g., Amazon S3) and standards for\nserialization that balance interoperability with performance considerations.\nWhat is serialization, exactly? Data stored in system memory by software is generally\nnot in a format suitable for storage on disk or transmission over a network. Serializa‐\ntion is the process of flattening and packing data into a standard format that a reader\nwill be able to decode. Serialization formats provide a standard of data exchange. We\nmight encode data in a row-based manner as an XML, JSON, or CSV file and pass\nit to another user who can then decode it using a standard library. A serialization\nalgorithm has logic for handling types, imposes rules on data structure, and allows\nexchange between programming languages and CPUs. The serialization algorithm\nalso has rules for handling exceptions. For instance, Python objects can contain cyclic\nreferences; the serialization algorithm might throw an error or limit nesting depth on\nencountering a cycle.\nLow-level database storage is also a form of serialization. Row-oriented relational\ndatabases organize data as rows on disk to support speedy lookups and in-place\nupdates. Columnar databases organize data into column files to optimize for highly\nefficient compression and support fast scans of large data volumes. Each serialization\nchoice comes with a set of trade-offs, and data engineers tune these choices to\noptimize performance to requirements.\nWe provide a more detailed catalog of common data serialization techniques and for‐\nmats in Appendix A . We suggest that data engineers become familiar with common\nserialization practices and formats, especially the most popular current formats (e.g.,\nApache Parquet), hybrid serialization (e.g., Apache Hudi), and in-memory serializa‐\ntion (e.g., Apache Arrow).\nCompression\nCompression  is another critical component of storage engineering. On a basic level,\ncompression makes data smaller, but compression algorithms interact with other\ndetails of storage systems in complex ways.\nHighly efficient compression has three main advantages in storage systems. First,\nthe data is smaller and thus takes up less space on the disk. Second, compression\nincreases the practical scan speed per disk. With a 10:1 compression ratio, we go from\nscanning 200 MB/s per magnetic disk to an effective rate of 2 GB/s per disk.\nThe third advantage is in network performance. Given that a network connection\nbetween an Amazon EC2 instance and S3 provides 10 gigabits per second (Gbps) of\nbandwidth, a 10:1 compression ratio increases effective network bandwidth to 100\nGbps.\n196 | Chapter 6: Storage",5479
86-Single Machine Versus Distributed Storage.pdf,86-Single Machine Versus Distributed Storage,"Compression also comes with disadvantages. Compressing and decompressing data\nentails extra time and resource consumption to read or write data. We undertake a\nmore detailed discussion of compression algorithms and trade-offs in Appendix A .\nCaching\nWe’ve already mentioned caching in our discussion of RAM. The core idea of caching\nis to store frequently or recently accessed data in a fast access layer. The faster\nthe cache, the higher the cost and the less storage space available. Less frequently\naccessed data is stored in cheaper, slower storage. Caches are critical for data serving,\nprocessing, and transformation.\nAs we analyze storage systems, it is helpful to put every type of storage we utilize\ninside a cache hierarchy  (Table 6-1 ). Most practical data systems rely on many cache\nlayers assembled from storage with varying performance characteristics. This starts\ninside CPUs; processors may deploy up to four cache tiers. We move down the\nhierarchy to RAM and SSDs. Cloud object storage is a lower tier that supports\nlong-term data retention and durability while allowing for data serving and dynamic\ndata movement in pipelines.\nTable 6-1. A heuristic cache hierarchy displaying storage types with approximate pricing and\nperformance characteristics\nStorage type Data fetch latencyaBandwidth Price\nCPU cache 1 nanosecond 1 TB/s N/A\nRAM 0.1 microseconds 100 GB/s $10/GB\nSSD 0.1 milliseconds 4 GB/s $0.20/GB\nHDD 4 milliseconds 300 MB/s $0.03/GB\nObject storage 100 milliseconds 10 GB/s $0.02/GB per month\nArchival storage 12 hours Same as object storage once data is available $0.004/GB per month\na A microsecond is 1,000 nanoseconds, and a millisecond is 1,000 microseconds.\nWe can think of archival storage as a reverse cache . Archival storage provides inferior\naccess characteristics for low costs. Archival storage is generally used for data back‐\nups and to meet data-retention compliance requirements. In typical scenarios, this\ndata will be accessed only in an emergency (e.g., data in a database might be lost and\nneed to be recovered, or a company might need to look back at historical data for\nlegal discovery).\nData Storage Systems\nThis  section covers the major data storage systems you’ll encounter as a data engi‐\nneer. Storage systems exist at a level of abstraction above raw ingredients. For exam‐\nple, magnetic disks are a raw storage ingredient, while major cloud object storage\nData Storage Systems | 197",2475
87-File Storage.pdf,87-File Storage,"platforms and HDFS are storage systems that utilize magnetic disks. Still higher levels\nof storage abstraction exist, such as data lakes and lakehouses (which we cover in\n“Data Engineering Storage Abstractions” on page 215 ).\nSingle Machine Versus Distributed Storage\nAs data storage and access patterns become more complex and outgrow the useful‐\nness of a single server, distributing data to more than one server becomes necessary.\nData can be stored on multiple servers, known as distributed storage . This is a dis‐\ntributed system whose purpose is to store data in a distributed fashion ( Figure 6-4 ).\nFigure 6-4. Single machine versus distributed storage on multiple servers\nDistributed storage coordinates the activities of multiple servers to store, retrieve,\nand process data faster and at a larger scale, all while providing redundancy in case\na server becomes unavailable. Distributed storage is common in architectures where\nyou want built-in redundancy and scalability for large amounts of data. For example,\nobject storage, Apache Spark, and cloud data warehouses rely on distributed storage\narchitectures.\nData engineers must always be aware of the consistency paradigms of the distributed\nsystems, which we’ll explore next.\nEventual Versus Strong Consistency\nA challenge with distributed systems is that your data is spread across multiple\nservers. How does this system keep the data consistent? Unfortunately, distributed\nsystems pose a dilemma for storage and query accuracy. It takes time to replicate\nchanges across the nodes of a system; often a balance exists between getting current\ndata and getting “sorta” current data in a distributed database. Let’s look at two\ncommon consistency patterns in distributed systems: eventual and strong.\nWe’ve covered ACID compliance throughout this book, starting in Chapter 5 .\nAnother  acronym is BASE , which stands for basically available, soft-state,  eventual\nconsistency . Think of it as the opposite of ACID. BASE is the basis of eventual\nconsistency. Let’s briefly explore its components:\nBasically available\nConsistency is not guaranteed, but database reads and writes are made on a\nbest-effort basis, meaning consistent data is available most of the time.\n198 | Chapter 6: Storage\nSoft-state\nThe state of the transaction is fuzzy, and it’s uncertain whether the transaction is\ncommitted or uncommitted.\nEventual consistency\nAt some  point, reading data will return consistent values.\nIf reading data in an eventually consistent system is unreliable, why use it? Eventual\nconsistency is a common trade-off in large-scale, distributed systems. If you want\nto scale horizontally (across multiple nodes) to process data in high volumes, then\neventually, consistency is often the price you’ll pay. Eventual consistency allows you\nto retrieve data quickly without verifying that you have the latest version across all\nnodes.\nThe opposite of eventual consistency is strong consistency . With strong consistency,\nthe distributed database ensures that writes to any node are first distributed with a\nconsensus and that any reads against the database return consistent values. Y ou’ll use\nstrong consistency when you can tolerate higher query latency and require correct\ndata every time you read from the database.\nGenerally, data engineers make decisions about consistency in three places. First, the\ndatabase technology itself sets the stage for a certain level of consistency. Second,\nconfiguration parameters for the database will have an impact on consistency. Third,\ndatabases often support some consistency configuration at an individual query level.\nFor example, DynamoDB  supports eventually consistent reads and strongly consis‐\ntent reads. Strongly consistent reads are slower and consume more resources, so it is\nbest to use them sparingly, but they are available when consistency is required.\nY ou should understand how your database handles consistency. Again, data engineers\nare tasked with understanding technology deeply and using it to solve problems\nappropriately. A data engineer might need to negotiate consistency requirements\nwith other technical and business stakeholders. Note that this is both a technology\nand organizational problem; ensure that you have gathered requirements from your\nstakeholders and choose your technologies appropriately.\nFile Storage\nWe deal with files every day, but the notion of a file is somewhat subtle. A file is a\ndata entity with specific read, write, and reference characteristics used by software\nand operating systems. We define a file to have the following characteristics:\nFinite length\nA file is a finite-length stream of bytes.\nData Storage Systems | 199\nAppend operations\nWe can append bytes to the file up to the limits of the host storage system.\nRandom access\nWe can read from any location in the file or write updates to any location.\nObject storage  behaves  much like file storage but with key differences. While we set\nthe stage for object storage by discussing file storage first, object storage is arguably\nmuch more important for the type of data engineering you’ll do today. We will\nforward-reference the object storage discussion extensively over the next few pages.\nFile storage systems organize files into a directory tree. The directory reference for a\nfile might look like this:\n/Users/matthewhousley/output.txt\nWhen this file reference is passed to the operating system, it starts at the root\ndirectory /, finds Users , matthewhousley , and finally output.txt . Working from\nthe left, each directory is contained inside a parent directory, until we finally reach\nthe file output.txt . This example uses Unix semantics, but Windows file reference\nsemantics are similar. The filesystem stores each directory as metadata about the files\nand directories that it contains. This metadata consists of the name of each entity,\nrelevant permission details, and a pointer to the actual entity. To find a file on disk,\nthe operating system looks at the metadata at each hierarchy level and follows the\npointer to the next subdirectory entity until finally reaching the file itself.\nNote that other file-like data entities generally don’t necessarily have all these proper‐\nties. For example, objects  in object storage support only the first characteristic, finite\nlength, but are still extremely useful. We discuss this in “Object Storage” on page 205 .\nIn cases where file storage paradigms are necessary for a pipeline, be careful with\nstate and try to use ephemeral environments as much as possible. Even if you must\nprocess files on a server with an attached disk, use object storage for intermediate\nstorage between processing steps. Try to reserve manual, low-level file processing for\none-time ingestion steps or the exploratory stages of pipeline development.\nLocal disk storage\nThe most familiar type of file storage is an operating system–managed filesystem\non a local disk partition of SSD or magnetic disk. New Technology File System\n(NTFS) and ext4 are popular filesystems on Windows and Linux, respectively. The\noperating system handles the details of storing directory entities, files, and metadata.\nFilesystems are designed to write data to allow for easy recovery in the event of power\nloss during a write, though any unwritten data will still be lost.\n200 | Chapter 6: Storage\nLocal filesystems generally support full read after write consistency; reading immedi‐\nately after a write will return the written data. Operating systems also employ various\nlocking strategies to manage concurrent writing attempts to a file.\nLocal disk filesystems may also support advanced features such as journaling, snap‐\nshots, redundancy, the extension of the filesystem across multiple disks, full disk\nencryption, and compression. In “Block Storage” on page 202 , we also discuss RAID.\nNetwork-attached storage\nNetwork-attached storage  (NAS) systems provide a file storage system to clients over\na network. NAS is a prevalent solution for servers; they quite often ship with built-in\ndedicated NAS interface hardware. While there are performance penalties to access‐\ning the filesystem over a network, significant advantages to storage virtualization\nalso exist, including redundancy and reliability, fine-grained control of resources,\nstorage pooling across multiple disks for large virtual volumes, and file sharing across\nmultiple machines. Engineers should be aware of the consistency model provided by\ntheir NAS solution, especially when multiple clients will potentially access the same\ndata.\nA popular alternative to NAS is a storage area network (SAN), but SAN systems\nprovide block-level access without the filesystem abstraction. We cover SAN systems\nin “Block Storage” on page 202 .\nCloud filesystem  services\nCloud filesystem services provide a fully managed filesystem for use with multiple\ncloud VMs and applications, potentially including clients outside the cloud environ‐\nment. Cloud filesystems should not be confused with standard storage attached\nto VMs—generally, block storage with a filesystem managed by the VM operating\nsystem. Cloud filesystems behave much like NAS solutions, but the details of net‐\nworking, managing disk clusters, failures, and configuration are fully handled by the\ncloud vendor.\nFor example, Amazon Elastic File System (EFS) is an extremely popular example of\na cloud filesystem service. Storage is exposed through the NFS 4 protocol , which\nis also used by NAS systems. EFS provides automatic scaling and pay-per-storage\npricing with no advanced storage reservation required. The service also provides local\nread-after-write consistency (when reading from the machine that performed the\nwrite). It also offers open-after-close consistency across the full filesystem. In other\nwords, once an application closes a file, subsequent readers will see changes saved to\nthe closed file.\nData Storage Systems | 201",10033
88-Block Storage.pdf,88-Block Storage,"Block Storage\nFundamentally, block storage  is the type of raw storage provided by SSDs and mag‐\nnetic disks. In the cloud, virtualized block storage is the standard for VMs. These\nblock storage abstractions allow fine control of storage size, scalability, and data\ndurability beyond that offered by raw disks.\nIn our earlier discussion of SSDs and magnetic disks, we mentioned that with these\nrandom-access devices, the operating system can seek, read, and write any data\non the disk. A block  is the smallest addressable unit of data supported by a disk.\nThis was often 512 bytes of usable data on older disks, but it has now grown to\n4,096 bytes for most current disks, making writes less fine-grained but dramatically\nreducing the overhead of managing blocks. Blocks typically contain extra bits for\nerror detection/correction and other metadata.\nBlocks on magnetic disks are geometrically arranged on a physical platter. Two blocks\non the same track can be read without moving the head, while reading two blocks on\nseparate tracks requires a seek. Seek time can occur between blocks on an SSD, but\nthis is infinitesimal compared to the seek time for magnetic disk tracks.\nBlock storage applications\nTransactional database systems generally access disks at a block level to lay out\ndata for optimal performance. For row-oriented databases, this originally meant that\nrows of data were written as continuous streams; the situation has grown more\ncomplicated with the arrival of SSDs and their associated seek-time performance\nimprovements, but transactional databases still rely on the high random access per‐\nformance offered by direct access to a block storage device.\nBlock storage also remains the default option for operating system boot disks on\ncloud VMs. The block device is formatted much as it would be directly on a physical\ndisk, but the storage is usually virtualized. (See “Cloud virtualized block storage”  on\npage 203 .)\nRAID\nRAID  stands  for redundant array of independent disks , as noted previously. RAID\nsimultaneously controls multiple disks to improve data durability, enhance perfor‐\nmance, and combine capacity from multiple drives. An array can appear to the\noperating system as a single block device. Many encoding and parity schemes are\navailable, depending on the desired balance between enhanced effective bandwidth\nand higher fault tolerance (tolerance for many disk failures).\n202 | Chapter 6: Storage\nStorage area network\nStorage area network  (SAN) systems provide virtualized block storage devices over\na network, typically from a storage pool. SAN abstraction can allow fine-grained\nstorage scaling and enhance performance, availability, and durability. Y ou might\nencounter SAN systems if you’re working with on-premises storage systems; you\nmight also encounter a cloud version of SAN, as in the next subsection.\nCloud virtualized block storage\nCloud virtualized block storage  solutions  are similar to SAN but free engineers from\ndealing with SAN clusters and networking details. We’ll look at  Amazon Elastic Block\nStore (EBS) as a standard example; other public clouds have similar offerings. EBS is\nthe default storage for Amazon EC2 virtual machines; other cloud providers also treat\nvirtualized object storage as a key component of their VM offerings.\nEBS offers several tiers of service with different performance characteristics. Gener‐\nally, EBS performance metrics are given in IOPS and throughput (transfer speed).\nThe higher performance tiers of EBS storage are backed by SSD disks, while magnetic\ndisk-backed storage offers lower IOPS but costs less per gigabyte.\nEBS volumes store data separate from the instance host server but in the same zone to\nsupport high performance and low latency ( Figure 6-5 ). This allows EBS volumes to\npersist when an EC2 instance shuts down, when a host server fails, or even when the\ninstance is deleted. EBS storage is suitable for applications such as databases, where\ndata durability is a high priority. In addition, EBS replicates all data to at least two\nseparate host machines, protecting data if a disk fails.\nFigure 6-5. EBS volumes replicate data to multiple hosts and disks for high durability\nand availability, but are not resilient to the failure of an availability zone\nEBS storage virtualization also supports several advanced features. For example,\nEBS volumes allow instantaneous point-in-time snapshots while the drive is used.\nAlthough it still takes some time for the snapshot to be replicated to S3, EBS can\neffectively freeze the state of data blocks when the snapshot is taken, while allowing\nthe client machine to continue using the disk. In addition, snapshots after the initial\nfull backup are differential; only changed blocks are written to S3 to minimize storage\ncosts and backup time.\nData Storage Systems | 203\nEBS volumes are also highly scalable. At the time of this writing, some EBS volume\nclasses can scale up to 64 TiB, 256,000 IOPS, and 4,000 MiB/s.\nLocal instance volumes\nCloud  providers also offer block storage volumes that are physically attached to the\nhost server running a virtual machine. These storage volumes are generally very low\ncost (included with the price of the VM in the case of Amazon’s EC2 instance store)\nand provide low latency and high IOPS.\nInstance store volumes ( Figure 6-6 ) behave essentially like a disk physically attached\nto a server in a data center. One key difference is that when a VM shuts down or is\ndeleted, the contents of the locally attached disk are lost, whether or not this event\nwas caused by intentional user action. This ensures that a new virtual machine cannot\nread disk contents belonging to a different customer.\nFigure 6-6. Instance store volumes offer  high performance and low cost but do not\nprotect data in the event of disk failure or VM shutdown\nLocally attached disks support none of the advanced virtualization features offered by\nvirtualized storage services like EBS. The locally attached disk is not replicated, so a\nphysical disk failure can lose or corrupt data even if the host VM continues running.\nFurthermore, locally attached volumes do not support snapshots or other backup\nfeatures.\nDespite these limitations, locally attached disks are extremely useful. In many cases,\nwe use disks as a local cache and hence don’t need all the advanced virtualization\nfeatures of a service like EBS. For example, suppose we’re running AWS EMR on\nEC2 instances. We may be running an ephemeral job that consumes data from\nS3, stores it temporarily in the distributed filesystem running across the instances,\nprocesses the data, and writes the results back to S3. The EMR filesystem builds in\nreplication and redundancy and is serving as a cache rather than permanent storage.\nThe EC2 instance store is a perfectly suitable solution in this case and can enhance\nperformance since data can be read and processed locally without flowing over a\nnetwork (see Figure 6-7 ).\n204 | Chapter 6: Storage",7071
89-Object Storage.pdf,89-Object Storage,"Figure 6-7. Instance store volumes can be used as a processing cache in an ephemeral\nHadoop cluster\nWe recommend that engineers think about locally attached storage in worst-case\nscenarios. What are the consequences of a local disk failure? Of an accidental VM or\ncluster shutdown? Of a zonal or regional cloud outage? If none of these scenarios will\nhave catastrophic consequences when data on locally attached volumes is lost, local\nstorage may be a cost-effective and performant option. In addition, simple mitigation\nstrategies (periodic checkpoint backups to S3) can prevent data loss.\nObject Storage\nObject storage  contains  objects  of all shapes and sizes ( Figure 6-8 ). The term object\nstorage  is somewhat confusing because object  has several meanings in computer\nscience. In this context, we’re talking about a specialized file-like construct. It could\nbe any type of file—TXT, CSV , JSON, images, videos, or audio.\nFigure 6-8. Object storage contains immutable objects of all shapes and sizes. Unlike files\non a local disk, objects cannot be modified  in place.\nData Storage Systems | 205\nObject stores have grown in importance and popularity with the rise of big data and\nthe cloud. Amazon S3, Azure Blob Storage, and Google Cloud Storage (GCS) are\nwidely used object stores. In addition, many cloud data warehouses (and a growing\nnumber of databases) utilize object storage as their storage layer, and cloud data lakes\ngenerally sit on object stores.\nAlthough many on-premises object storage systems can be installed on server clus‐\nters, we’ll focus mostly on fully managed cloud object stores. From an operational\nperspective, one of the most attractive characteristics of cloud object storage is that\nit is straightforward to manage and use. Object storage was arguably one of the first\n“serverless” services; engineers don’t need to consider the characteristics of underly‐\ning server clusters or disks.\nAn object store is a key-value store for immutable data objects. We lose much of the\nwriting flexibility we expect with file storage on a  local disk in an object store. Objects\ndon’t support random writes or append operations; instead, they are written once as a\nstream of bytes. After this initial write, objects become immutable. To change data in\nan object or append data to it, we must rewrite the full object. Object stores generally\nsupport random reads through range requests, but these lookups may perform much\nworse than random reads from data stored on an SSD.\nFor a software developer used to leveraging local random access file storage, the\ncharacteristics of objects might seem like constraints, but less is more; object stores\ndon’t need to support locks or change synchronization, allowing data storage across\nmassive disk clusters. Object stores support extremely performant parallel stream\nwrites and reads across many disks, and this parallelism is hidden from engineers,\nwho can simply deal with the stream rather than communicating with individual\ndisks. In a cloud environment, write speed scales with the number of streams being\nwritten up to quota limits set by the vendor. Read bandwidth can scale with the\nnumber of parallel requests, the number of virtual machines employed to read data,\nand the number of CPU cores. These characteristics make object storage ideal for\nserving high-volume web traffic or delivering data to highly parallel distributed query\nengines.\nTypical cloud object stores save data in several availability zones, dramatically reduc‐\ning the odds that storage will go fully offline or be lost in an unrecoverable way. This\ndurability and availability are built into the cost; cloud storage vendors offer other\nstorage classes at discounted prices in exchange for reduced durability or availability.\nWe’ll discuss this in “Storage classes and tiers” on page 210 .\nCloud object storage is a key ingredient in separating compute and storage, allowing\nengineers to process data with ephemeral clusters and scale these clusters up and\ndown on demand. This is a key factor in making big data available to smaller\norganizations that can’t afford to own hardware for data jobs that they’ll run only\noccasionally. Some major tech companies will continue to run permanent Hadoop\n206 | Chapter 6: Storage\nclusters on their hardware. Still, the general trend is that most organizations will\nmove data processing to the cloud, using an object store as essential storage and\nserving layer while processing data on ephemeral clusters.\nIn object storage, available storage space is also highly scalable, an ideal characteristic\nfor big data systems. Storage space is constrained by the number of disks the storage\nprovider owns, but these providers handle exabytes of data. In a cloud environment,\navailable storage space is virtually limitless; in practice, the primary limit on storage\nspace for public cloud customers is budget. From a practical standpoint, engineers\ncan quickly store massive quantities of data for projects without planning months in\nadvance for necessary servers and disks.\nObject stores for data engineering applications\nFrom the standpoint of data engineering, object stores provide excellent performance\nfor large batch reads and batch writes. This corresponds well to the use case for\nmassive OLAP systems. A bit of data engineering folklore says that object stores are\nnot good for updates, but this is only partially true. Object stores are an inferior fit for\ntransactional workloads with many small updates every second; these use cases are\nmuch better served by transactional databases or block storage systems. Object stores\nwork well for a low rate of update operations, where each operation updates a large\nvolume of data.\nObject stores are now the gold standard of storage for data lakes. In the early days\nof data lakes, write once, read many (WORM) was the operational standard, but this\nhad more to do with the complexities of managing data versions and files than the\nlimitations of HDFS and object stores. Since then, systems such as Apache Hudi and\nDelta Lake have emerged to manage this complexity, and privacy regulations such\nas GDPR and CCPA have made deletion and update capabilities imperative. Update\nmanagement for object storage is the central idea behind the data lakehouse concept,\nwhich we introduced in Chapter 3 .\nObject storage is an ideal repository for unstructured data in any format beyond\nthese structured data applications. Object storage can house any binary data with no\nconstraints on type or structure and frequently plays a role in ML pipelines for raw\ntext, images, video, and audio.\nObject lookup\nAs we mentioned, object stores are key-value stores. What does this mean for engi‐\nneers? It’s critical to understand that, unlike file stores, object stores do not utilize a\ndirectory tree to find objects. The object store uses a top-level logical container (a\nbucket in S3 and GCS) and references objects by key. A simple example in S3 might\nlook like this:\nS3://oreilly-data-engineering-book/data-example.json\nData Storage Systems | 207\nIn this case, S3://oreilly-data-engineering-book/  is the bucket name, and data-\nexample.json  is the key pointing to a particular object. S3 bucket names must be\nunique across all of AWS. Keys are unique within a bucket. Although cloud object\nstores may appear to support directory tree semantics, no true directory hierarchy\nexists. We might store an object with the following full path:\nS3://oreilly-data-engineering-book/project-data/11/23/2021/data.txt\nOn the surface, this looks like subdirectories you might find in a regular file folder\nsystem: project-data , 11, 23, and 2021 . Many cloud console interfaces allow users\nto view the objects inside a “directory, ” and cloud command-line tools often support\nUnix-style commands such as ls inside an object store directory. However, behind\nthe scenes, the object system does not traverse a directory tree to reach the object.\nInstead, it simply sees a key ( project-data/11/23/2021/data.txt ) that happens\nto match directory semantics. This might seem like a minor technical detail, but\nengineers need to understand that certain “directory”-level operations are costly\nin an object store. To run aws ls S3://oreilly-data-engineering-book/project-\ndata/11/  the object store must filter keys on the key prefix project-data/11/ . If the\nbucket contains millions of objects, this operation might take some time, even if the\n“subdirectory” houses only a few objects.\nObject consistency and versioning\nAs mentioned, object stores don’t support in-place updates or appends as a general\nrule. We write a new object under the same key to update an object. When data\nengineers utilize updates in data processes, they must be aware of the consistency\nmodel for the object store they’re using. Object stores may be eventually consistent\nor strongly consistent. For example, until recently, S3 was eventually consistent ; after\na new version of an object was written under the same key, the object store might\nsometimes return the old version of the object. The eventual  part of eventual consis‐\ntency  means that after enough time has passed, the storage cluster reaches a state\nsuch that only the latest version of the object will be returned. This contrasts with the\nstrong consistency  model we expect of local disks attached to a server: reading after a\nwrite will return the most recently written data.\nIt might be desirable to impose strong consistency on an object store for various\nreasons, and standard methods are used to achieve this. One approach is to add a\nstrongly consistent database (e.g., PostgreSQL) to the mix. Writing an object is now a\ntwo-step process:\n1.Write the object.1.\n2.Write the returned metadata for the object version to the strongly consistent2.\ndatabase.\n208 | Chapter 6: Storage\nThe version metadata (an object hash or an object timestamp) can uniquely identify\nan object version in conjunction with the object key. To read an object, a reader\nundertakes the following steps:\n1.Fetch the latest object metadata from the strongly consistent database.1.\n2.Query object metadata using the object key. Read the object data if it matches the2.\nmetadata fetched from the consistent database.\n3.If the object metadata does not match, repeat step 2 until the latest version of the3.\nobject is returned.\nA practical implementation has exceptions and edge cases to consider, such as when\nthe object gets rewritten during this querying process. These steps can be managed\nbehind an API so that an object reader sees a strongly consistent object store at the\ncost of higher latency for object access.\nObject versioning is closely related to object consistency. When we rewrite an object\nunder an existing key in an object store, we’re essentially writing a brand-new object,\nsetting references from the existing key to the object, and deleting the old object\nreferences. Updating all references across the cluster takes time, hence the potential\nfor stale reads. Eventually, the storage cluster garbage collector deallocates the space\ndedicated to the dereferenced data, recycling disk capacity for use by new objects.\nWith object versioning turned on, we add additional metadata to the object that\nstipulates a version. While the default key reference gets updated to point to the new\nobject, we retain other pointers to previous versions. We also maintain a version list\nso that clients can get a list of all object versions, and then pull a specific version.\nBecause old versions of the object are still referenced, they aren’t cleaned up by the\ngarbage collector.\nIf we reference an object with a version, the consistency issue with some object\nstorage systems disappears: the key and version metadata together form a unique\nreference to a particular, immutable data object. We will always get the same object\nback when we use this pair, provided that we haven’t deleted it. The consistency issue\nstill exists when a client requests the “default” or “latest” version of an object.\nThe principal overhead that engineers need to consider with object versioning is\nthe cost of storage. Historical versions of objects generally have the same associated\nstorage costs as current versions. Object version costs may be nearly insignificant or\ncatastrophically expensive, depending on various factors. The data size is an issue, as\nis update frequency; more object versions can lead to significantly larger data size.\nKeep in mind that we’re talking about brute-force object versioning. Object storage\nsystems generally store full object data for each version, not differential snapshots.\nData Storage Systems | 209\nEngineers also have the option of deploying storage lifecycle policies. Lifecycle poli‐\ncies allow automatic deletion of old object versions when certain conditions are met\n(e.g., when an object version reaches a certain age or many newer versions exist).\nCloud vendors also offer various archival data tiers at heavily discounted prices, and\nthe archival process can be managed using lifecycle policies.\nStorage classes and tiers\nCloud vendors now offer storage classes that discount data storage pricing in\nexchange for reduced access or reduced durability. We use the term reduced access\nhere because many of these storage tiers still make data highly available, but with high\nretrieval costs in exchange for reduced storage costs.\nLet’s look at a couple of examples in S3 since Amazon is a benchmark for cloud ser‐\nvice standards. The S3 Standard-Infrequent Access storage class discounts monthly\nstorage costs for increased data retrieval costs. (See “ A Brief Detour on Cloud\nEconomics”  on page 125 for a theoretical discussion of the economics of cloud\nstorage tiers.) Amazon also offers the Amazon S3 One Zone-Infrequent Access tier,\nreplicating only to a single zone. Projected availability drops from 99.9% to 99.5% to\naccount for the possibility of a zonal outage. Amazon still claims extremely high data\ndurability, with the caveat that data will be lost if an availability zone is destroyed.\nFurther down the tiers of reduced access are the archival tiers in S3 Glacier. S3\nGlacier promises a dramatic reduction in long-term storage costs for much higher\naccess costs. Users have various retrieval speed options, from minutes to hours, with\nhigher retrieval costs for faster access. For example, at the time of this writing, S3\nGlacier Deep Archive discounts storage costs even further; Amazon advertises that\nstorage costs start at $1 per terabyte per month. In exchange, data restoration takes 12\nhours. In addition, this storage class is designed for data that will be stored 7–10 years\nand be accessed only one to two times per year.\nBe aware of how you plan to utilize archival storage, as it’s easy to get into and often\ncostly to access data, especially if you need it more often than expected. See Chapter 4\nfor a more extensive discussion of archival storage economics.\nObject store–backed filesystems\nObject store synchronization solutions have become increasingly popular. Tools like\ns3fs and Amazon S3 File Gateway allow users to mount an S3 bucket as local storage.\nUsers of these tools should be aware of the characteristics of writes to the filesystem\nand how these will interact with the characteristics and pricing of object storage.\nFile Gateway, for example, handles changes to files fairly efficiently by combining\nportions of objects into a new object using the advanced capabilities of S3. However,\nhigh-speed transactional writing will overwhelm the update capabilities of an object\n210 | Chapter 6: Storage",15798
90-Cache and Memory-Based Storage Systems.pdf,90-Cache and Memory-Based Storage Systems,,0
91-Indexes Partitioning and Clustering.pdf,91-Indexes Partitioning and Clustering,"store. Mounting object storage as a local filesystem works well for files that are\nupdated infrequently.\nCache and Memory-Based Storage Systems\nAs discussed in “Raw Ingredients of Data Storage”  on page 191, RAM offers excellent\nlatency and transfer speeds. However, traditional RAM is extremely vulnerable to\ndata loss because a power outage lasting even a second can erase data. RAM-based\nstorage systems are generally focused on caching applications, presenting data for\nquick access and high bandwidth. Data should generally be written to a more durable\nmedium for retention purposes.\nThese ultra-fast cache systems are useful when data engineers need to serve data with\nultra-fast retrieval latency.\nExample: Memcached and lightweight object caching\nMemcached  is a key-value store designed for caching database query results, API\ncall responses, and more. Memcached uses simple data structures, supporting either\nstring or integer types. Memcached can deliver results with very low latency while\nalso taking the load off backend systems.\nExample: Redis, memory caching with optional persistence\nLike Memcached, Redis  is a key-value store, but it supports somewhat more com‐\nplex data types (such as lists or sets). Redis also builds in multiple persistence\nmechanisms, including snapshotting and journaling. With a typical configuration,\nRedis writes data roughly every two seconds. Redis is thus suitable for extremely\nhigh-performance applications but can tolerate a small amount of data loss.\nThe Hadoop Distributed File System\nIn the recent past, “Hadoop” was virtually synonymous with “big data. ” The  Hadoop\nDistributed File System is based on Google File System (GFS)  and was initially\nengineered to process data with the MapReduce programming model . Hadoop is\nsimilar to object storage but with a key difference: Hadoop combines compute and\nstorage on the same nodes, where object stores typically have limited support for\ninternal processing.\nHadoop breaks large files into blocks , chunks of data less than a few hundred\nmegabytes in size. The filesystem is managed by the NameNode , which maintains\ndirectories, file metadata, and a detailed catalog describing the location of file blocks\nin the cluster. In a typical configuration, each block of data is replicated to three\nnodes. This increases both the durability and availability of data. If a disk or node\nfails, the replication factor for some file blocks will fall below 3. The NameNode will\nData Storage Systems | 211\ninstruct other nodes to replicate these file blocks so that they again reach the correct\nreplication factor. Thus, the probability of losing data is very low, barring a  correlated\nfailure  (e.g., an asteroid hitting the data center).\nHadoop is not simply a storage system. Hadoop combines compute resources with\nstorage nodes to allow in-place data processing. This was originally achieved using\nthe MapReduce programming model, which we discuss in Chapter 8 .\nHadoop is dead. Long live Hadoop!\nWe often see claims that Hadoop is dead. This is only partially true. Hadoop is\nno longer a hot, bleeding-edge technology. Many Hadoop ecosystem tools such as\nApache Pig are now on life support and primarily used to run legacy jobs. The pure\nMapReduce programming model has fallen by the wayside. HDFS remains widely\nused in various applications and organizations.\nHadoop still appears in many legacy installations. Many organizations that adopted\nHadoop during the peak of the big data craze have no immediate plans to migrate to\nnewer technologies. This is a good choice for companies that run massive (thousand-\nnode) Hadoop clusters and have the resources to maintain on-premises systems\neffectively. Smaller companies may want to reconsider the cost overhead and scale\nlimitations of running a small Hadoop cluster against migrating to cloud solutions.\nIn addition, HDFS is a key ingredient of many current big data engines, such as\nAmazon EMR. In fact, Apache Spark is still commonly run on HDFS clusters. We\ndiscuss this in more detail in “Separation of Compute from Storage” on page 220 .\nStreaming Storage\nStreaming data has different storage requirements than nonstreaming data. In the\ncase of message queues, stored data is temporal and expected to disappear after a\ncertain duration. However, distributed, scalable streaming frameworks like Apache\nKafka now allow extremely long-duration streaming data retention. Kafka supports\nindefinite data retention by pushing old, infrequently accessed messages down to\nobject storage. Kafka competitors (including Amazon Kinesis, Apache Pulsar, and\nGoogle Cloud Pub/Sub) also support long data retention.\nClosely related to data retention in these systems is the notion of replay. Replay\nallows a streaming system to return a range of historical stored data. Replay is the\nstandard data-retrieval mechanism for streaming storage systems. Replay can be used\nto run batch queries over a time range or to reprocess data in a streaming pipeline.\nChapter 7  covers replay in more depth.\nOther storage engines have emerged for real-time analytics applications. In some\nsense, transactional databases emerged as the first real-time query engines; data\nbecomes visible to queries as soon as it is written. However, these databases have\n212 | Chapter 6: Storage\nwell-known scaling and locking limitations, especially for analytics queries that run\nacross large volumes of data. While scalable versions of row-oriented transactional\ndatabases have overcome some of these limitations, they are still not truly optimized\nfor analytics at scale.\nIndexes, Partitioning, and Clustering\nIndexes  provide a map of the table for particular fields and allow extremely fast\nlookup of individual records. Without indexes, a database would need to scan an\nentire table to find the records satisfying a WHERE  condition.\nIn most RDBMSs, indexes are used for primary table keys (allowing unique identifi‐\ncation of rows) and foreign keys (allowing joins with other tables). Indexes can also\nbe applied to other columns to serve the needs of specific applications. Using indexes,\nan RDBMS can look up and update thousands of rows per second.\nWe do not cover transactional database records in depth in this book; numerous tech‐\nnical resources are available on this topic. Rather, we are interested in the evolution\naway from indexes in analytics-oriented storage systems and some new developments\nin indexes for analytics use cases.\nThe evolution from rows to columns\nAn early data warehouse was typically built on the same type of RDBMS used for\ntransactional applications. The growing popularity of MPP systems meant a shift\ntoward parallel processing for significant improvements in scan performance across\nlarge quantities of data for analytics purposes. However, these row-oriented MPPs\nstill used indexes to support joins and condition checking.\nIn “Raw Ingredients of Data Storage”  on page 191, we discuss  columnar serialization.\nColumnar serialization  allows a database to scan only the columns required for a\nparticular query, sometimes dramatically reducing the amount of data read from the\ndisk. In addition, arranging data by column packs similar values next to each other,\nyielding high-compression ratios with minimal compression overhead. This allows\ndata to be scanned more quickly from disk and over a network.\nColumnar databases perform poorly for transactional use cases—i.e., when we try to\nlook up large numbers of individual rows asynchronously. However, they perform\nextremely well when large quantities of data must be scanned—e.g., for complex\ndata transformations, aggregations, statistical calculations, or evaluation of complex\nconditions on large datasets.\nIn the past, columnar databases performed poorly on joins, so the advice for\ndata engineers was to denormalize data, using wide schemas, arrays, and nested\ndata wherever possible. Join performance for columnar databases has improved\ndramatically in recent years, so while there can still be performance advantages in\nData Storage Systems | 213\n2Benoit Dageville, “The Snowflake Elastic Data Warehouse, ” SIGMOD ’16: Proceedings of the 2016 International\nConference on Management of Data  (June 2016): 215–226, https://oreil.ly/Tc1su .denormalization,  this is no longer a necessity. Y ou’ll learn more about normalization\nand denormalization in Chapter 8 .\nFrom indexes to partitions and clustering\nWhile columnar databases allow for fast scan speeds, it’s still helpful to reduce the\namount of data scanned as much as possible. In addition to scanning only data in\ncolumns relevant to a query, we can partition a table into multiple subtables by\nsplitting it on a field. It is quite common in analytics and data science use cases to\nscan over a time range, so date- and time-based partitioning is extremely common.\nColumnar databases generally support a variety of other partition schemes as well.\nClusters  allow finer-grained organization of data within partitions. A clustering\nscheme applied within a columnar database sorts data by one or a few fields, colocat‐\ning similar values. This improves performance for filtering, sorting, and joining these\nvalues.\nExample: Snowflake  micro-partitioning\nWe mention Snowflake micro-partitioning  because it’s a good example of recent\ndevelopments and evolution in approaches to columnar storage. Micro partitions  are\nsets of rows between 50 and 500 megabytes in uncompressed size. Snowflake uses an\nalgorithmic approach that attempts to cluster together similar rows. This contrasts\nthe traditional naive approach to partitioning on a single designated field, such as a\ndate. Snowflake specifically looks for values that are repeated in a field across many\nrows. This allows aggressive pruning  of queries based on predicates. For example, a\nWHERE  clause might stipulate the following:\nWHERE created_date ='2022-01-02'\nIn such a query, Snowflake excludes any micro-partitions that don’t include this date,\neffectively pruning this data. Snowflake also allows overlapping micro-partitions,\npotentially partitioning on multiple fields showing significant repeats.\nEfficient pruning is facilitated by Snowflake’s metadata database, which stores a\ndescription of each micro-partition, including the number of rows and value ranges\nfor fields. At each query stage, Snowflake analyzes micro-partitions to determine\nwhich ones need to be scanned. Snowflake uses the term hybrid columnar storage ,2\npartially referring to the fact that its tables are broken into small groups of rows, even\nthough storage is fundamentally columnar. The metadata database plays a role similar\nto an index in a traditional relational database.\n214 | Chapter 6: Storage",10847
92-Data Engineering Storage Abstractions.pdf,92-Data Engineering Storage Abstractions,,0
93-The Data Lake.pdf,93-The Data Lake,"Data Engineering Storage Abstractions\nData engineering storage abstractions  are data organization and query patterns that\nsit at the heart of the data engineering lifecycle and are built atop the data storage sys‐\ntems discussed previously (see Figure 6-3 ). We introduced many of these abstractions\nin Chapter 3 , and we will revisit them here.\nThe main types of abstractions we’ll concern ourselves with are those that support\ndata science, analytics, and reporting use cases. These include data warehouse, data\nlake, data lakehouse, data platforms, and data catalogs. We won’t cover source sys‐\ntems, as they are discussed in Chapter 5 .\nThe storage abstraction you require as a data engineer boils down to a few key\nconsiderations:\nPurpose and use case\nY ou must first identify the purpose of storing the data. What is it used for?\nUpdate patterns\nIs the abstraction optimized for bulk updates, streaming inserts, or upserts?\nCost\nWhat are the direct and indirect financial costs? The time to value? The opportu‐\nnity costs?\nSeparate storage and compute\nThe trend is toward separating storage and compute, but most systems hybridize\nseparation and colocation. We cover this in “Separation of Compute from Stor‐\nage” on page 220  since it affects purpose, speed, and cost.\nY ou should know that the popularity of separating storage from compute means the\nlines between OLAP databases and data lakes are increasingly blurring. Major cloud\ndata warehouses and data lakes are on a collision course. In the future, the differences\nbetween these two may be in name only since they might functionally and technically\nbe very similar under the hood.\nThe Data Warehouse\nData warehouses are a standard OLAP data architecture. As discussed in Chapter 3 ,\nthe term data warehouse  refers to technology platforms (e.g., Google BigQuery and\nTeradata), an architecture for data centralization, and an organizational pattern\nwithin a company. In terms of storage trends, we’ve evolved from building data\nwarehouses atop conventional transactional databases, row-based MPP systems (e.g.,\nTeradata and IBM Netezza), and columnar MPP systems (e.g., Vertica and Teradata\nColumnar) to cloud data warehouses and data platforms. (See our data warehousing\ndiscussion in Chapter 3  for more details on MPP systems.)\nData Engineering Storage Abstractions | 215",2375
94-Data Platforms.pdf,94-Data Platforms,"3James Dixon, “Data Lakes Revisited, ” James Dixon’s Blog , September 25, 2014, https://oreil.ly/FH25v .In practice, cloud data warehouses are often used to organize data into a data lake,\na storage area for massive amounts of unprocessed raw data, as originally conceived\nby James Dixon.3 Cloud data warehouses can handle massive amounts of raw text\nand complex JSON documents. The limitation is that cloud data warehouses cannot\nhandle truly unstructured data, such as images, video, or audio, unlike a true data\nlake. Cloud data warehouses can be coupled with object storage to provide a complete\ndata-lake solution.\nThe Data Lake\nThe data lake  was originally conceived as a massive store where data was retained in\nraw, unprocessed form. Initially, data lakes were built primarily on Hadoop systems,\nwhere cheap storage allowed for retention of massive amounts of data without the\ncost overhead of a proprietary MPP system.\nThe last five years have seen two major developments in the evolution of data\nlake storage. First, a major migration toward separation of compute and storage  has\noccurred. In practice, this means a move away from Hadoop toward cloud object\nstorage for long-term retention of data. Second, data engineers discovered that much\nof the functionality offered by MPP systems (schema management; update, merge\nand delete capabilities) and initially dismissed in the rush to data lakes was, in fact,\nextremely useful. This led to the notion of the data lakehouse.\nThe Data Lakehouse\nThe data lakehouse  is an architecture that combines aspects of the data warehouse\nand the data lake. As it is generally conceived, the lakehouse stores data in object\nstorage just like a lake. However, the lakehouse adds to this arrangement features\ndesigned to streamline data management and create an engineering experience simi‐\nlar to a data warehouse. This means robust table and schema support and features\nfor managing incremental updates and deletes. Lakehouses typically also support\ntable history and rollback; this is accomplished by retaining old versions of files and\nmetadata.\nA lakehouse system is a metadata and file-management layer deployed with data\nmanagement and transformation tools. Databricks has heavily promoted the lake‐\nhouse concept with Delta Lake, an open source storage management system.\nWe would be remiss not to point out that the architecture of the data lakehouse\nis similar to the architecture used by various commercial data platforms, including\nBigQuery and Snowflake. These systems store data in object storage and provide\n216 | Chapter 6: Storage",2628
95-Big Ideas and Trends in Storage.pdf,95-Big Ideas and Trends in Storage,"automated metadata management, table history, and update/delete capabilities. The\ncomplexities of managing underlying files and storage are fully hidden from the user.\nThe key advantage of the data lakehouse over proprietary tools is interoperability.\nIt’s much easier to exchange data between tools when stored in an open file format.\nReserializing data from a proprietary database format incurs overhead in processing,\ntime, and cost. In a data lakehouse architecture, various tools can connect to the\nmetadata layer and read data directly from object storage.\nIt is important to emphasize that much of the data in a data lakehouse may not have a\ntable structure imposed. We can impose data warehouse features where we need them\nin a lakehouse, leaving other data in a raw or even unstructured format.\nThe data lakehouse technology is evolving rapidly. A variety of new competitors to\nDelta Lake have emerged, including Apache Hudi and Apache Iceberg. See Appen‐\ndix A  for more details.\nData Platforms\nIncreasingly, vendors are styling their products as data platforms . These vendors\nhave created their ecosystems of interoperable tools with tight integration into the\ncore data storage layer. In evaluating platforms, engineers must ensure that the\ntools offered meet their needs. Tools not directly provided in the platform can still\ninteroperate, with extra data overhead for data interchange. Platforms also emphasize\nclose integration with object storage for unstructured use cases, as mentioned in our\ndiscussion of cloud data warehouses.\nAt this point, the notion of the data platform frankly has yet to be fully fleshed out.\nHowever, the race is on to create a walled garden of data tools, both simplifying the\nwork of data engineering and generating significant vendor lock-in.\nStream-to-Batch Storage Architecture\nThe stream-to-batch storage architecture has many similarities to the Lambda archi‐\ntecture, though some might quibble over the technical details. Essentially, data\nflowing through a topic in the streaming storage system is written out to multiple\nconsumers.\nSome of these consumers might be real-time processing systems that generate statis‐\ntics on the stream. In addition, a batch storage consumer writes data for long-term\nretention and batch queries. The batch consumer could be AWS Kinesis Firehose,\nwhich can generate S3 objects based on configurable triggers (e.g., time and batch\nsize). Systems such as BigQuery ingest streaming data into a streaming buffer. This\nstreaming buffer is automatically reserialized into columnar object storage. The query\nengine supports seamless querying of both the streaming buffer and the object data to\nprovide users a current, nearly real-time view of the table.\nData Engineering Storage Abstractions | 217",2815
96-Data Sharing.pdf,96-Data Sharing,"Big Ideas and Trends in Storage\nIn this section, we’ll discuss some big ideas in storage—key considerations that you\nneed to keep in mind as you build out your storage architecture. Many of these\nconsiderations are part of larger trends. For example, data catalogs fit under the\ntrend toward “enterprisey” data engineering and data management. Separation of\ncompute from storage is now largely an accomplished fact in cloud data systems.\nAnd data sharing is an increasingly important consideration as businesses adopt data\ntechnology.\nData Catalog\nA data catalog  is a centralized metadata store for all data across an organization.\nStrictly speaking, a data catalog is not a top-level data storage abstraction, but it\nintegrates with various systems and abstractions. Data catalogs typically work across\noperational and analytics data sources, integrate data lineage and presentation of data\nrelationships, and allow user editing of data descriptions.\nData catalogs are often used to provide a central place where people can view their\ndata, queries, and data storage. As a data engineer, you’ll likely be responsible for\nsetting up and maintaining the various data integrations of data pipeline and storage\nsystems that will integrate with the data catalog and the integrity of the data catalog\nitself.\nCatalog application integration\nIdeally, data applications are designed to integrate with catalog APIs to handle their\nmetadata and updates directly. As catalogs are more widely used in an organization, it\nbecomes easier to approach this ideal.\nAutomated scanning\nIn practice, cataloging systems typically need to rely on an automated scanning layer\nthat collects metadata from various systems such as data lakes, data warehouses, and\noperational databases. Data catalogs can collect existing metadata and may also use\nscanning tools to infer metadata such as key relationships or the presence of sensitive\ndata.\nData portal and social layer\nData catalogs also typically provide a human access layer through a web interface,\nwhere users can search for data and view data relationships. Data catalogs can be\nenhanced with a social layer offering Wiki functionality. This allows users to provide\ninformation on their datasets, request information from other users, and post updates\nas they become available.\n218 | Chapter 6: Storage",2370
97-Separation of Compute from Storage.pdf,97-Separation of Compute from Storage,"Data catalog use cases\nData catalogs have both organizational and technical use cases. Data catalogs make\nmetadata easily available to systems. For instance, a data catalog is a key ingredient of\nthe data lakehouse, allowing table discoverability for queries.\nOrganizationally, data catalogs allow business users, analysts, data scientists, and\nengineers to search for data to answer questions. Data catalogs streamline cross-\norganizational communications and collaboration.\nData Sharing\nData sharing  allows  organizations and individuals to share specific data and carefully\ndefined permissions with specific entities. Data sharing allows data scientists to\nshare data from a sandbox with their collaborators within an organization. Across\norganizations, data sharing facilitates collaboration between partner businesses. For\nexample, an ad tech company can share advertising data with its customers.\nA cloud multitenant environment makes interorganizational collaboration much eas‐\nier. However, it also presents new security challenges. Organizations must carefully\ncontrol policies that govern who can share data with whom to prevent accidental\nexposure or deliberate exfiltration.\nData sharing is a core feature of many cloud data platforms. See Chapter 5  for a more\nextensive discussion of data sharing.\nSchema\nWhat  is the expected form of the data? What is the file format? Is it structured,\nsemistructured, or unstructured? What data types are expected? How does the data\nfit into a larger hierarchy? Is it connected to other data through shared keys or other\nrelationships?\nNote that schema need not be relational . Rather, data becomes more useful when we\nhave as much information about its structure and organization. For images stored in\na data lake, this schema information might explain the image format, resolution, and\nthe way the images fit into a larger hierarchy.\nSchema can function as a sort of Rosetta stone, instructions that tell us how to read\nthe data. Two major schema patterns exist: schema on write and schema on read.\nSchema on write  is essentially the traditional data warehouse pattern: a table has an\nintegrated schema; any writes to the table must conform. To support schema on\nwrite, a data lake must integrate a schema metastore.\nWith  schema on read , the schema is dynamically created when data is written, and\na reader must determine the schema when reading the data. Ideally, schema on read\nis implemented using file formats that implement built-in schema information, such\nBig Ideas and Trends in Storage | 219\nas Parquet or JSON. CSV files are notorious for schema inconsistency and are not\nrecommended in this setting.\nThe principal advantage of schema on write is that it enforces data standards, mak‐\ning data easier to consume and utilize in the future. Schema on read emphasizes\nflexibility, allowing virtually any data to be written. This comes at the cost of greater\ndifficulty consuming data in the future.\nSeparation of Compute from Storage\nA key idea we revisit throughout this book is the separation of compute from storage.\nThis has emerged as a standard data access and query pattern in today’s cloud era.\nData lakes, as we discussed, store data in object stores and spin up temporary com‐\npute capacity to read and process it. Most fully managed OLAP products now rely\non object storage behind the scenes. To understand the motivations for separating\ncompute and storage, we should first look at the colocation of compute and storage.\nColocation of compute and storage\nColocation  of compute and storage has long been a standard method to improve\ndatabase performance. For transactional databases, data colocation allows fast, low-\nlatency disk reads and high bandwidth. Even when we virtualize storage (e.g., using\nAmazon EBS), data is located relatively close to the host machine.\nThe same basic idea applies for analytics query systems running across a cluster of\nmachines. For example, with HDFS and MapReduce, the standard approach is to\nlocate data blocks that need to be scanned in the cluster, and then push individual\nmap  jobs out to these blocks. The data scan and processing for the map step are\nstrictly local. The reduce  step involves shuffling data across the cluster, but keeping\nmap steps local effectively preserves more bandwidth for shuffling, delivering better\noverall performance; map steps that filter heavily also dramatically reduce the amount\nof data to be shuffled.\nSeparation of compute and storage\nIf colocation of compute and storage delivers high performance, why the shift toward\nseparation of compute and storage? Several motivations exist.\nEphemerality and scalability.    In the cloud, we’ve seen a dramatic shift toward  ephemer‐\nality. In general, it’s cheaper to buy and host a server than to rent it from a cloud\nprovider, provided that you’re running it 24 hours a day nonstop for years on end . In\npractice, workloads vary dramatically, and significant efficiencies are realized with a\npay-as-you-go model if servers can scale up and down. This is true for web servers in\nonline retail, and it is also true for big data batch jobs that may run only periodically.\n220 | Chapter 6: Storage\nEphemeral compute resources allow engineers to spin up massive clusters to com‐\nplete jobs on time and then delete clusters when these jobs are done. The per‐\nformance benefits of temporarily operating at ultra-high scale can outweigh the\nbandwidth limitations of object storage.\nData durability and availability.    Cloud  object stores significantly mitigate the risk of\ndata loss and generally provide extremely high uptime (availability). For example, S3\nstores data across multiple zones; if a natural disaster destroys a zone, data is still\navailable from the remaining zones. Having multiple zones available also reduces the\nodds of a data outage. If resources in one zone go down, engineers can spin up the\nsame resources in a different zone.\nThe potential for a misconfiguration that destroys data in object storage is still some‐\nwhat scary, but simple-to-deploy mitigations are available. Copying data to multiple\ncloud regions reduces this risk since configuration changes are generally deployed to\nonly one region at a time. Replicating data to multiple storage providers can further\nreduce the risk.\nHybrid separation and colocation\nThe practical realities of separating compute from storage are more complicated\nthan we’ve implied. In practice, we constantly hybridize colocation and separation to\nrealize the benefits of both approaches. This hybridization is typically done in two\nways: multitier caching and hybrid object storage.\nWith  multitier caching , we utilize object storage for long-term data retention and\naccess but spin up local storage to be used during queries and various stages of data\npipelines. Both Google and Amazon offer versions of hybrid object storage (object\nstorage that is tightly integrated with compute).\nLet’s look at examples of how some popular processing engines hybridize separation\nand colocation of storage and compute.\nExample: AWS EMR with S3 and HDFS.    Big data services like Amazon EMR spin up tem‐\nporary HDFS clusters to process data. Engineers have the option of referencing both\nS3 and HDFS as a filesystem. A common pattern is to stand up HDFS on SSD drives,\npull from S3, and save data from intermediate processing steps on local HDFS. Doing\nso can realize significant performance gains over processing directly from S3. Full\nresults are written back to S3 once the cluster completes its steps, and the cluster and\nHDFS are deleted. Other consumers read the output data directly from S3.\nExample: Apache Spark.    In practice, Spark generally runs jobs on HDFS or some other\nephemeral distributed filesystem to support performant storage of data between\nprocessing steps. In addition, Spark relies heavily on in-memory storage of data to\nimprove processing. The problem with owning the infrastructure for running Spark\nBig Ideas and Trends in Storage | 221\n4Valliappa Lakshmanan and Jordan Tigani, Google BigQuery: The Definitive  Guide  (Sebastopol, CA: O’Reilly,\n2019), 16–17, 188, https://oreil.ly/5aXXu .is that dynamic RAM (DRAM) is extremely expensive; by separating compute and\nstorage in the cloud, we can rent large quantities of memory and then release that\nmemory when the job completes.\nExample: Apache Druid.    Apache Druid relies heavily on SSDs to realize high perfor‐\nmance. Since SSDs are significantly more expensive than magnetic disks, Druid keeps\nonly one copy of data in its cluster, reducing “live” storage costs by a factor of three.\nOf course, maintaining data durability is still critical, so Druid uses an object store as\nits durability layer. When data is ingested, it’s processed, serialized into compressed\ncolumns, and written to cluster SSDs and object storage. In the event of node failure\nor cluster data corruption, data can be automatically recovered to new nodes. In\naddition, the cluster can be shut down and then fully recovered from SSD storage.\nExample: Hybrid object storage.    Google’s Colossus file storage system  supports fine-\ngrained control of data block location, although this functionality is not exposed\ndirectly to the public. BigQuery uses this feature to colocate customer tables in a\nsingle location, allowing ultra-high bandwidth for queries in that location.4 We refer\nto this as hybrid object storage  because it combines the clean abstractions of object\nstorage with some advantages of colocating compute and storage. Amazon also offers\nsome notion of hybrid object storage through S3 Select, a feature that allows users to\nfilter S3 data directly in S3 clusters before data is returned across the network.\nWe speculate that public clouds will adopt hybrid object storage more widely to\nimprove the performance of their offerings and make more efficient use of available\nnetwork resources. Some may be already doing so without disclosing this publicly.\nThe concept of hybrid object storage underscores that there can still be advantages\nto having low-level access to hardware rather than relying on someone else’s public\ncloud. Public cloud services do not expose low-level details of hardware and systems\n(e.g., data block locations for Colossus), but these details can be extremely useful in\nperformance optimization and enhancement. See our discussion of cloud economics\nin Chapter 4 .\nWhile we’re now seeing a mass migration of data to public clouds, we believe that\nmany hyper-scale data service vendors that currently run on public clouds provided\nby other vendors may build their data centers in the future, albeit with deep network\nintegration into public clouds.\n222 | Chapter 6: Storage",10860
98-Data Storage Lifecycle and Data Retention.pdf,98-Data Storage Lifecycle and Data Retention,"Zero-copy cloning\nCloud-based systems based around object storage support zero-copy cloning . This\ntypically means that a new virtual copy of an object is created (e.g., a new table)\nwithout necessarily physically copying the underlying data. Typically, new pointers\nare created to the raw data files, and future changes to these tables will not be recor‐\nded in the old table. For those familiar with the inner workings of object-oriented\nlanguages such as Python, this type of “shallow” copying is familiar from other\ncontexts.\nZero-copy cloning is a compelling feature, but engineers must understand its\nstrengths and limitations. For example, cloning an object in a data lake environment\nand then deleting the files in the original object might also wipe out the new object.\nFor fully managed object-store-based systems (e.g., Snowflake and BigQuery), engi‐\nneers need to be extremely familiar with the exact limits of shallow copying. Engi‐\nneers have more access to underlying object storage in data lake systems such as\nDatabricks—a blessing and a curse. Data engineers should exercise great caution\nbefore deleting any raw files in the underlying object store. Databricks and other\ndata lake management technologies sometimes also support a notion of deep copying ,\nwhereby all underlying data objects are copied. This is a more expensive process, but\nalso more robust in the event that files are unintentionally lost or deleted.\nData Storage Lifecycle and Data Retention\nStoring data isn’t as simple as just saving it to object storage or disk and forgetting\nabout it. Y ou need to think about the data storage lifecycle and data retention. When\nyou think about access frequency and use cases, ask, “How important is the data to\ndownstream users, and how often do they need to access it?” This is the data storage\nlifecycle. Another question you should ask is, “How long should I keep this data?” Do\nyou need to retain data indefinitely, or are you fine discarding it past a certain time\nframe? This is data retention. Let’s dive into each of these.\nHot, warm, and cold data\nDid you know that data has a temperature? Depending on how frequently data is\naccessed, we can roughly bucket the way it is stored into three categories of persis‐\ntence: hot, warm, and cold. Query access patterns differ for each dataset ( Figure 6-9 ).\nTypically, newer data is queried more often than older data. Let’s look at hot, cold,\nand warm data in that order.\nBig Ideas and Trends in Storage | 223\nFigure 6-9. Hot, warm, and cold data costs associated with access frequency\nHot data.    Hot data  has instant or frequent access requirements. The underlying\nstorage for hot data is suited for fast access and reads, such as SSD or memory.\nBecause of the type of hardware involved with hot data, storing hot data is often the\nmost expensive form of storage. Example use cases for hot data include retrieving\nproduct recommendations and product page results. The cost of storing hot data is\nthe highest of these three storage tiers, but retrieval is often inexpensive.\nQuery results cache is another example of hot data. When a query is run, some\nquery engines will persist the query results in the cache. For a limited time, when the\nsame query is run, instead of rerunning the same query against storage, the query\nresults cache serves the cached results. This allows for much faster query response\ntimes versus redundantly issuing the same query repeatedly. In upcoming chapters,\nwe cover query results caches in more detail.\nWarm data.    Warm data  is accessed semi-regularly, say, once per month. No hard\nand fast rules indicate how often warm data is accessed, but it’s less than hot data\nand more than cold data. The major cloud providers offer object storage tiers that\naccommodate warm data. For example, S3 offers an Infrequently Accessed Tier, and\nGoogle Cloud has a similar storage tier called Nearline. Vendors give their models\nof recommended access frequency, and engineers can also do their cost modeling\nand monitoring. Storage of warm data is cheaper than hot data, with slightly more\nexpensive retrieval costs.\nCold data.    On the other extreme, cold data  is infrequently accessed data. The hard‐\nware used to archive cold data is typically cheap and durable, such as HDD, tape\nstorage, and cloud-based archival systems. Cold data is mainly meant for long-term\narchival, when there’s little to no intention to access the data. Though storing cold\ndata is cheap, retrieving cold data is often expensive.\nStorage tier considerations.    When  considering the storage tier for your data, consider\nthe costs of each tier. If you store all of your data in hot storage, all of the data can\nbe accessed quickly. But this comes at a tremendous price! Conversely, if you store all\n224 | Chapter 6: Storage\ndata in cold storage to save on costs, you’ll certainly lower your storage costs, but at\nthe expense of prolonged retrieval times and high retrieval costs if you need to access\ndata. The storage price goes down from faster/higher performing storage to lower\nstorage.\nCold storage is popular for archiving data. Historically, cold storage involved physical\nbackups and often mailing this data to a third party that would archive it in a literal\nvault. Cold storage is increasingly popular in the cloud. Every cloud vendor offers a\ncold data solution, and you should weigh the cost of pushing data into cold storage\nversus the cost and time to retrieve the data.\nData engineers need to account for spillover from hot to warm/cold storage. Memory\nis expensive and finite. For example, if hot data is stored in memory, it can be spilled\nto disk when there’s too much new data to store and not enough memory. Some\ndatabases may move infrequently accessed data to warm or cold tiers, offloading\nthe data to either HDD or object storage. The latter is increasingly more common\nbecause of the cost-effectiveness of object storage. If you’re in the cloud and using\nmanaged services, disk spillover will happen automatically.\nIf you’re using cloud-based object storage, create  automated lifecycle policies for your\ndata. This will drastically reduce your storage costs. For example, if your data needs\nto be accessed only once a month, move the data to an infrequent access storage\ntier. If your data is 180 days old and not accessed for current queries, move it to\nan archival storage tier. In both cases, you can automate the migration of data away\nfrom regular object storage, and you’ll save money. That said, consider the retrieval\ncosts—both in time and money—using infrequent or archival style storage tiers.\nAccess and retrieval times and costs may vary depending on the cloud provider. Some\ncloud providers make it simple and cheap to migrate data into archive storage, but it\nis costly and slow to retrieve your data.\nData retention\nBack  in the early days of “big data, ” there was a tendency to err on the side of accu‐\nmulating every piece of data possible, regardless of its usefulness. The expectation\nwas, “we might need this data in the future. ” This data hoarding inevitably became\nunwieldy and dirty, giving rise to data swamps and regulatory crackdowns on data\nretention, among other consequences and nightmares. Nowadays, data engineers\nneed to consider data retention: what data do you need  to keep, and how long should\nyou keep it? Here are some things to think about with data retention.\nValue.    Data  is an asset, so you should know the value of the data you’re storing. Of\ncourse, value is subjective and depends on what it’s worth to your immediate use case\nand your broader organization. Is this data impossible to re-create, or can it easily be\nre-created by querying upstream systems? What’s the impact to downstream users if\nthis data is available versus if it is not?\nBig Ideas and Trends in Storage | 225",7944
99-Undercurrents.pdf,99-Undercurrents,"Time.    The value to downstream users also depends upon the age of the data. New\ndata is typically more valuable and frequently accessed than older data. Technical\nlimitations may determine how long you can store data in certain storage tiers. For\nexample, if you store hot data in cache or memory, you’ll likely need to set a time to\nlive (TTL), so you can expire data after a certain point or persist it to warm or cold\nstorage. Otherwise, your hot storage will become full, and queries against the hot data\nwill suffer from performance lags.\nCompliance.    Certain  regulations (e.g., HIPAA and Payment Card Industry, or PCI)\nmight require you to keep data for a certain time. In these situations, the data simply\nneeds to be accessible upon request, even if the likelihood of an access request is low.\nOther regulations might require you to hold data for only a limited period of time,\nand you’ll need to have the ability to delete specific information on time and within\ncompliance guidelines. Y ou’ll need a storage and archival data process—along with\nthe ability to search the data—that fits the retention requirements of the particular\nregulation with which you need to comply. Of course, you’ll want to balance compli‐\nance against cost.\nCost.    Data  is an asset that (hopefully) has an ROI. On the cost side of ROI, an\nobvious storage expense is associated with data. Consider the timeline in which you\nneed to retain data. Given our discussion about hot, warm, and cold data, implement\nautomatic data lifecycle management practices and move the data to cold storage if\nyou don’t need the data past the required retention date. Or delete data if it’s truly not\nneeded.\nSingle-Tenant Versus Multitenant Storage\nIn Chapter 3 , we covered the trade-offs between single-tenant and multitenant\narchitecture. To recap, with single-tenant  architecture, each group of tenants (e.g.,\nindividual users, groups of users, accounts, or customers) gets its own dedicated set\nof resources such as networking, compute, and storage. A multitenant  architecture\ninverts this and shares these resources among groups of users. Both architectures are\nwidely used. This section looks at the implications of single-tenant and multitenant\nstorage.\nAdopting single-tenant storage means that every tenant gets their dedicated storage.\nIn the example in Figure 6-10 , each tenant gets a database. No data is shared among\nthese databases, and storage is totally isolated. An example of using single-tenant\nstorage is that each customer’s data must be stored in isolation and cannot be blended\nwith any other customer’s data. In this case, each customer gets their own database.\n226 | Chapter 6: Storage\nFigure 6-10. In single-tenant storage, each tenant gets their own database\nSeparate data storage implies separate and independent schemas, bucket structures,\nand everything related to storage. This means you have the liberty of designing\neach tenant’s storage environment to be uniform or let them evolve however they\nmay. Schema variation across customers can be an advantage and a complication;\nas always, consider the trade-offs. If each tenant’s schema isn’t uniform across all\ntenants, this has major consequences if you need to query multiple tenants’ tables to\ncreate a unified view of all tenant data.\nMultitenant storage allows for the storage of multiple tenants within a single data‐\nbase. For example, instead of the single-tenant scenario where customers get their\nown database, multiple customers may reside in the same database schemas or tables\nin a multitenant database. Storing multitenant data means each tenant’s data is stored\nin the same place ( Figure 6-11 ).\nFigure 6-11. In this multitenant storage, four tenants occupy the same database\nY ou need to be aware of querying both single and multitenant storage, which we\ncover in more detail in Chapter 8 .\nWhom You’ll Work With\nStorage  is at the heart of data engineering infrastructure. Y ou’ll interact with the\npeople who own your IT infrastructure—typically, DevOps, security, and cloud archi‐\ntects. Defining domains of responsibility between data engineering and other teams\nis critical. Do data engineers have the authority to deploy their infrastructure in an\nAWS account, or must another team handle these changes? Work with other teams to\ndefine streamlined processes so that teams can work together efficiently and quickly.\nWhom You’ll Work With | 227",4485
100-Security.pdf,100-Security,,0
101-Data Architecture.pdf,101-Data Architecture,"The division of responsibilities for data storage will depend significantly on the\nmaturity of the organization involved. The data engineer will likely manage the\nstorage systems and workflow if the company is early in its data maturity. If the\ncompany is later in its data maturity, the data engineer will probably manage a section\nof the storage system. This data engineer will also likely interact with engineers on\neither side of storage—ingestion and transformation.\nThe data engineer needs to ensure that the storage systems used by downstream users\nare securely available, contain high-quality data, have ample storage capacity, and\nperform when queries and transformations are run.\nUndercurrents\nThe undercurrents for storage are significant because storage is a critical hub for all\nstages of the data engineering lifecycle. Unlike other undercurrents for which data\nmight be in motion (ingestion) or queried and transformed, the undercurrents for\nstorage differ because storage is so ubiquitous.\nSecurity\nWhile  engineers often view security as an impediment to their work, they should\nembrace the idea that security is a key enabler. Robust security at rest and in motion\nwith fine-grained data access control allows data to be shared and consumed more\nwidely within a business. The value of data goes up significantly when this is possible.\nAs always, exercise the principle of least privilege. Don’t give full database access\nto anyone unless required. This means most data engineers don’t need full database\naccess in practice. Also, pay attention to the column, row, and cell-level access con‐\ntrols in your database. Give users only the information they need and no more.\nData Management\nData management is critical as we read and write data with storage systems.\nData catalogs and metadata management\nData is enhanced by robust metadata. Cataloging enables data scientists, analysts,\nand ML engineers by enabling data discovery. Data lineage accelerates the time to\ntrack down data problems and allows consumers to locate upstream raw sources.\nAs you build out your storage systems, invest in your metadata. Integration of a\ndata dictionary with these other tools allows users to share and record institutional\nknowledge robustly.\n228 | Chapter 6: Storage\nMetadata management also significantly enhances data governance. Beyond simply\nenabling passive data cataloging and lineage, consider implementing analytics over\nthese systems to get a clear, active picture of what’s happening with your data.\nData versioning in object storage\nMajor cloud object storage systems enable data versioning. Data versioning can help\nwith error recovery when processes fail, and data becomes corrupted. Versioning\nis also beneficial for tracking the history of datasets used to build models. Just as\ncode version control allows developers to track down commits that cause bugs,\ndata version control can aid ML engineers in tracking changes that lead to model\nperformance degradation.\nPrivacy\nGDPR and other privacy regulations have significantly impacted storage system\ndesign. Any data with privacy implications has a lifecycle that data engineers must\nmanage. Data engineers must be prepared to respond to data deletion requests and\nselectively remove data as required. In addition, engineers can accommodate privacy\nand security through anonymization and masking.\nDataOps\nDataOps  is not orthogonal to data management, and a significant area of overlap\nexists. DataOps concerns itself with traditional operational monitoring of storage\nsystems and monitoring the data itself, inseparable from metadata and quality.\nSystems monitoring\nData engineers must monitor storage in a variety of ways. This includes monitoring\ninfrastructure storage components, where they exist, but also monitoring object\nstorage and other “serverless” systems. Data engineers should take the lead on FinOps\n(cost management), security monitoring, and access monitoring.\nObserving and monitoring data\nWhile metadata systems as we’ve described are critical, good engineering must con‐\nsider the entropic nature of data by actively seeking to understand its characteristics\nand watching for major changes. Engineers can monitor data statistics, apply anom‐\naly detection methods or simple rules, and actively test and validate for logical\ninconsistencies.\nUndercurrents | 229",4415
102-Orchestration.pdf,102-Orchestration,,0
103-Software Engineering.pdf,103-Software Engineering,,0
104-Additional Resources.pdf,104-Additional Resources,"Data Architecture\nChapter 3  covers the basics of data architecture, as storage is the critical underbelly of\nthe data engineering lifecycle.\nConsider the following data architecture tips. Design for required reliability and\ndurability. Understand the upstream source systems and how that data, once inges‐\nted, will be stored and accessed. Understand the types of data models and queries that\nwill occur downstream.\nIf data is expected to grow, can you negotiate storage with your cloud provider? Take\nan active approach to FinOps, and treat it as a central part of architecture conversa‐\ntions. Don’t prematurely optimize, but prepare for scale if business opportunities\nexist in operating on large data volumes.\nLean toward fully managed systems, and understand provider SLAs. Fully managed\nsystems are generally far more robust and scalable than systems you have to babysit.\nOrchestration\nOrchestration  is highly entangled with storage. Storage allows data to flow through\npipelines, and orchestration is the pump. Orchestration also helps engineers cope\nwith the complexity of data systems, potentially combining many storage systems and\nquery engines.\nSoftware Engineering\nWe can think about software engineering in the context of storage in two ways. First,\nthe code you write should perform well with your storage system. Make sure the code\nyou write stores the data correctly and doesn’t accidentally cause data, memory leaks,\nor performance issues. Second, define your storage infrastructure as code and use\nephemeral compute resources when it’s time to process your data. Because storage\nis increasingly distinct from compute, you can automatically spin resources up and\ndown while keeping your data in object storage. This keeps your infrastructure clean\nand avoids coupling your storage and query layers.\nConclusion\nStorage is everywhere and underlays many stages of the data engineering lifecycle. In\nthis chapter, you learned about the raw ingredients, types, abstractions, and big ideas\naround storage systems. Gain deep knowledge of the inner workings and limitations\nof the storage systems you’ll use. Know the types of data, activities, and workloads\nappropriate for your storage.\n230 | Chapter 6: Storage\nAdditional Resources\n•“Column-Oriented DBMS” Wikipedia page•\n•“The Design and Implementation of Modern Column-Oriented Database Sys‐•\ntems”  by Daniel Abadi et al.\n•Designing Data-Intensive Applications  by Martin Kleppmann (O’Reilly) •\n•“Diving Into Delta Lake: Schema Enforcement and Evolution”  by Burak Y avuz •\net al.\n•“Hot Data vs. Cold Data: Why It Matters”  by Afzaal Ahmad Zeeshan •\n•IDC’s “Data Creation and Replication Will Grow at a Faster Rate than Installed •\nStorage Capacity, According to the IDC Global DataSphere and StorageSphere\nForecasts” press release\n•“Rowise vs. Columnar Database? Theory and in Practice”  by Mangat Rai Modi •\n•“Snowflake Solution Anti-Patterns: The Probable Data Scientist”  by John Aven •\n•“What Is a Vector Database?”  by Bryan Turriff •\n•“What Is Object Storage? A Definition and Overview”  by Alex Chan •\n•“The What, When, Why, and How of Incremental Loads”  by Tim Mitchell •\nAdditional Resources | 231",3226
105-Serialization and Deserialization.pdf,105-Serialization and Deserialization,"CHAPTER 7\nIngestion\nY ou’ve learned about the various source systems you’ll likely encounter as a data\nengineer and about ways to store data. Let’s now turn our attention to the patterns\nand choices that apply to ingesting data from various source systems. In this chapter,\nwe discuss data ingestion (see Figure 7-1 ), the key engineering considerations for the\ningestion phase, the major patterns for batch and streaming ingestion, technologies\nyou’ll encounter, whom you’ll work with as you develop your data ingestion pipeline,\nand how the undercurrents feature in the ingestion phase.\nFigure 7-1. To begin processing data, we must ingest it\n233\nWhat Is Data Ingestion?\nData ingestion  is the process of moving data from one place to another. Data inges‐\ntion implies data movement from source systems into storage in the data engineering\nlifecycle, with ingestion as an intermediate step ( Figure 7-2 ).\nFigure 7-2. Data from system 1 is ingested into system 2\nIt’s worth quickly contrasting data ingestion with data integration. Whereas data\ningestion  is data movement from point A to B, data integration  combines data from\ndisparate sources into a new dataset. For example, you can use data integration to\ncombine data from a CRM system, advertising analytics data, and web analytics to\ncreate a user profile, which is saved to your data warehouse. Furthermore, using\nreverse ETL, you can send this newly created user profile back  to your CRM so\nsalespeople can use the data for prioritizing leads. We describe data integration more\nfully in Chapter 8 , where we discuss data transformations; reverse ETL is covered in\nChapter 9 .\nWe also point out that data ingestion is different from internal ingestion  within a\nsystem. Data stored in a database is copied from one table to another, or data in\na stream is temporarily cached. We consider this another part of the general data\ntransformation process covered in Chapter 8 .\nData Pipelines Defined\nData pipelines begin in source systems, but ingestion is the stage where data engi‐\nneers begin actively designing data pipeline activities. In the data engineering space, a\ngood deal of ceremony occurs around data movement and processing patterns, with\nestablished patterns such as ETL, newer patterns such as ELT, and new names for\nlong-established practices (reverse ETL) and data sharing.\nAll of these concepts are encompassed in the idea of a data pipeline . It is essential\nto understand the details of these various patterns and know that a modern data\npipeline includes all of them. As the world moves away from a traditional monolithic\napproach with rigid constraints on data movement, and toward an open ecosystem of\ncloud services that are assembled like LEGO bricks to realize products, data engineers\nprioritize using the right tools to accomplish the desired outcome over adhering to a\nnarrow philosophy of data movement.\nIn general, here’s our definition of a data pipeline:\n234 | Chapter 7: Ingestion\nA data pipeline is the combination of architecture, systems, and processes that move\ndata through the stages of the data engineering lifecycle.\nOur definition is deliberately fluid—and intentionally vague—to allow data engineers\nto plug in whatever they need to accomplish the task at hand. A data pipeline could\nbe a traditional ETL system, where data is ingested from an on-premises transactional\nsystem, passed through a monolithic processor, and written into a data warehouse. Or\nit could be a cloud-based data pipeline that pulls data from 100 sources, combines it\ninto 20 wide tables, trains five other ML models, deploys them into production, and\nmonitors ongoing performance. A data pipeline should be flexible enough to fit any\nneeds along the data engineering lifecycle.\nLet’s keep this notion of data pipelines in mind as we proceed through this chapter.\nKey Engineering Considerations for the Ingestion Phase\nWhen  preparing to architect or build an ingestion system, here are some primary\nconsiderations and questions to ask yourself related to data ingestion:\n•What’s the use case for the data I’m ingesting?•\n•Can I reuse this data and avoid ingesting multiple versions of the same dataset?•\n•Where is the data going? What’s the destination?•\n•How often should the data be updated from the source?•\n•What is the expected data volume?•\n•What format is the data in? Can downstream storage and transformation accept•\nthis format?\n•Is the source data in good shape for immediate downstream use? That is, is the•\ndata of good quality? What post-processing is required to serve it? What are\ndata-quality risks (e.g., could bot traffic to a website contaminate the data)?\n•Does the data require in-flight processing for downstream ingestion if the data is•\nfrom a streaming source?\nThese questions undercut batch and streaming ingestion and apply to the underlying\narchitecture you’ll create, build, and maintain. Regardless of how often the data\nis ingested, you’ll want to consider these factors when designing your ingestion\narchitecture:\n•Bounded versus unbounded•\n•Frequency•\n•Synchronous versus asynchronous•\nKey Engineering Considerations for the Ingestion Phase | 235\n•Serialization and deserialization•\n•Throughput and scalability•\n•Reliability and durability•\n•Payload•\n•Push versus pull versus poll patterns•\nLet’s look at each of these.\nBounded Versus Unbounded Data\nAs you might recall from Chapter 3 , data comes in two forms: bounded and unboun‐\nded ( Figure 7-3 ). Unbounded data  is data as it exists in reality, as events happen,\neither sporadically or continuously, ongoing and flowing. Bounded data  is a conve‐\nnient way of bucketing data across some sort of boundary, such as time.\nFigure 7-3. Bounded versus unbounded data\nLet us adopt this mantra: All data is unbounded until it’s bounded.  Like many mantras,\nthis one is not precisely accurate 100% of the time. The grocery list that I scribbled\nthis afternoon is bounded data. I wrote it as a stream of consciousness (unbounded\ndata) onto a piece of scrap paper, where the thoughts now exist as a list of things\n(bounded data) I need to buy at the grocery store. However, the idea is correct for\n236 | Chapter 7: Ingestion\npractical purposes for the vast majority of data you’ll handle in a business context.\nFor example, an online retailer will process customer transactions 24 hours a day\nuntil the business fails, the economy grinds to a halt, or the sun explodes.\nBusiness processes have long imposed artificial bounds on data by cutting discrete\nbatches. Keep in mind the true unboundedness of your data; streaming ingestion\nsystems are simply a tool for preserving the unbounded nature of data so that\nsubsequent steps in the lifecycle can also process it continuously.\nFrequency\nOne  of the critical decisions that data engineers must make in designing data-\ningestion processes is the data-ingestion frequency. Ingestion processes can be batch,\nmicro-batch, or real-time.\nIngestion frequencies vary dramatically from slow to fast ( Figure 7-4 ). On the slow\nend, a business might ship its tax data to an accounting firm once a year. On the\nfaster side, a CDC system could retrieve new log updates from a source database once\na minute. Even faster, a system might continuously ingest events from IoT sensors\nand process these within seconds. Data-ingestion frequencies are often mixed in a\ncompany, depending on the use case and technologies.\nFigure 7-4. The spectrum batch to real-time ingestion frequencies\nWe note that “real-time” ingestion patterns are becoming increasingly common.\nWe put “real-time” in quotation marks because no ingestion system is genuinely\nreal-time. Any database, queue or pipeline has inherent latency in delivering data to a\ntarget system. It is more accurate to speak of near real-time , but we often use real-time\nfor brevity. The near real-time pattern generally does away with an explicit update\nfrequency; events are processed in the pipeline either one by one as they arrive or in\nmicro-batches (i.e., batches over concise time intervals). For this book, we will use\nreal-time  and streaming  interchangeably.\nEven with a streaming data-ingestion process, batch processing downstream is rela‐\ntively standard. At the time of this writing, ML models are typically trained on a\nbatch basis, although continuous online training is becoming more prevalent. Rarely\ndo data engineers have the option to build a purely near real-time pipeline with no\nbatch components. Instead, they choose where batch boundaries will occur—i.e., the\nKey Engineering Considerations for the Ingestion Phase | 237\ndata engineering lifecycle data will be broken into batches. Once data reaches a batch\nprocess, the batch frequency becomes a bottleneck for all downstream processing.\nIn addition, streaming systems are the best fit for many data source types. In IoT\napplications, the typical pattern is for each sensor to write events or measurements\nto streaming systems as they happen. While this data can be written directly into a\ndatabase, a streaming ingestion platform such as Amazon Kinesis or Apache Kafka\nis a better fit for the application. Software applications can adopt similar patterns\nby writing events to a message queue as they happen rather than waiting for an\nextraction process to pull events and state information from a backend database. This\npattern works exceptionally well for event-driven architectures already exchanging\nmessages through queues. And again, streaming architectures generally coexist with\nbatch processing.\nSynchronous Versus Asynchronous Ingestion\nWith  synchronous ingestion , the source, ingestion, and destination have complex\ndependencies and are tightly coupled. As you can see in Figure 7-5 , each stage of\nthe data engineering lifecycle has processes A, B, and C directly dependent upon one\nanother. If process A fails, processes B and C cannot start; if process B fails, process\nC doesn’t start. This type of synchronous workflow is common in older ETL systems,\nwhere data extracted from a source system must then be transformed before being\nloaded into a data warehouse. Processes downstream of ingestion can’t start until all\ndata in the batch has been ingested. If the ingestion or transformation process fails,\nthe entire process must be rerun.\nFigure 7-5. A synchronous ingestion process runs as discrete batch steps\nHere’s a mini case study of how not to design your data pipelines. At one company,\nthe transformation process itself was a series of dozens of tightly coupled synchro‐\nnous workflows, with the entire process taking over 24 hours to finish. If any step\nof that transformation pipeline failed, the whole transformation process had to be\nrestarted from the beginning! In this instance, we saw process after process fail, and\nbecause of nonexistent or cryptic error messages, fixing the pipeline was a game of\nwhack-a-mole that took over a week to diagnose and cure. Meanwhile, the business\ndidn’t have updated reports during that time. People weren’t happy.\n238 | Chapter 7: Ingestion",11188
106-Payload.pdf,106-Payload,"With asynchronous ingestion , dependencies can now operate at the level of individ‐\nual events, much as they would in a software backend built from microservices\n(Figure 7-6 ). Individual events become available in storage as soon as they are inges‐\nted individually. Take the example of a web application on AWS that emits events into\nAmazon Kinesis Data Streams (here acting as a buffer). The stream is read by Apache\nBeam, which parses and enriches events, and then forwards them to a second Kinesis\nstream; Kinesis Data Firehose rolls up events and writes objects to Amazon S3.\nFigure 7-6. Asynchronous processing of an event stream in AWS\nThe big idea is that rather than relying on asynchronous processing, where a batch\nprocess runs for each stage as the input batch closes and certain time conditions are\nmet, each stage of the asynchronous pipeline can process data items as they become\navailable in parallel across the Beam cluster. The processing rate depends on available\nresources. The Kinesis Data Stream acts as the shock absorber, moderating the load\nso that event rate spikes will not overwhelm downstream processing. Events will\nmove through the pipeline quickly when the event rate is low, and any backlog has\ncleared. Note that we could modify the scenario and use a Kinesis Data Stream for\nstorage, eventually extracting events to S3 before they expire out of the stream.\nSerialization and Deserialization\nMoving  data from source to destination involves serialization and deserialization. As\na reminder, serialization  means encoding the data from a source and preparing data\nstructures for transmission and intermediate storage stages.\nWhen ingesting data, ensure that your destination can deserialize the data it receives.\nWe’ve seen data ingested from a source but then sitting inert and unusable in the\ndestination because the data cannot be properly deserialized. See the more extensive\ndiscussion of serialization in Appendix A .\nThroughput and Scalability\nIn theory, your ingestion should never be a bottleneck. In practice, ingestion bottle‐\nnecks are pretty standard. Data throughput and system scalability become critical as\nyour data volumes grow and requirements change. Design your systems to scale and\nshrink to flexibly match the desired data throughput.\nWhere you’re ingesting data from matters a lot. If you’re receiving data as it’s gener‐\nated, will the upstream system have any issues that might impact your downstream\ningestion pipelines? For example, suppose a source database goes down. When it\nKey Engineering Considerations for the Ingestion Phase | 239\ncomes back online and attempts to backfill the lapsed data loads, will your ingestion\nbe able to keep up with this sudden influx of backlogged data?\nAnother thing to consider is your ability to handle  bursty data ingestion. Data gener‐\nation rarely happens at a constant rate and often ebbs and flows. Built-in buffering\nis required to collect events during rate spikes to prevent data from getting lost.\nBuffering bridges the time while the system scales and allows storage systems to\naccommodate bursts even in a dynamically scalable system.\nWhenever possible, use managed services that handle the throughput scaling for you.\nWhile you can manually accomplish these tasks by adding more servers, shards, or\nworkers, often this isn’t value-added work, and there’s a good chance you’ll miss\nsomething. Much of this heavy lifting is now automated. Don’t reinvent the data\ningestion wheel if you don’t have to.\nReliability and Durability\nReliability and durability are vital in the ingestion stages of data pipelines. Reliability\nentails high uptime and proper failover for ingestion systems. Durability  entails\nmaking sure that data isn’t lost or corrupted.\nSome data sources (e.g., IoT devices and caches) may not retain data if it is not\ncorrectly ingested. Once lost, it is gone for good. In this sense, the reliability  of\ningestion systems leads directly to the durability  of generated data. If data is ingested,\ndownstream processes can theoretically run late if they break temporarily.\nOur advice is to evaluate the risks and build an appropriate level of redundancy and\nself-healing based on the impact and cost of losing data. Reliability and durability\nhave both direct and indirect costs. For example, will your ingestion process continue\nif an AWS zone goes down? How about a whole region? How about the power grid\nor the internet? Of course, nothing is free. How much will this cost you? Y ou might\nbe able to build a highly redundant system and have a team on call 24 hours a day\nto handle outages. This also means your cloud and labor costs become prohibitive\n(direct costs), and the ongoing work takes a significant toll on your team (indirect\ncosts). There’s no single correct answer, and you need to evaluate the costs and\nbenefits of your reliability and durability decisions.\nDon’t assume that you can build a system that will reliably and durably ingest data in\nevery possible scenario. Even the nearly infinite budget of the US federal government\ncan’t guarantee this. In many extreme scenarios, ingesting data actually won’t matter.\nThere will be little to ingest if the internet goes down, even if you build multiple air-\ngapped data centers in underground bunkers with independent power. Continually\nevaluate the trade-offs and costs of reliability and durability.\n240 | Chapter 7: Ingestion\nPayload\nA payload  is the dataset you’re ingesting and has characteristics such as kind, shape,\nsize, schema and data types, and metadata. Let’s look at some of these characteristics\nto understand why this matters.\nKind\nThe kind  of data you handle directly impacts how it’s dealt with downstream in the\ndata engineering lifecycle. Kind consists of type and format. Data has a type—tabular,\nimage, video, text, etc. The type directly influences the data format or the way it\nis expressed in bytes, names, and file extensions. For example, a tabular kind of\ndata may be in formats such as CSV or Parquet, with each of these formats having\ndifferent byte patterns for serialization and deserialization. Another kind of data is an\nimage, which has a format of JPG or PNG and is inherently unstructured.\nShape\nEvery  payload has a shape  that describes its dimensions. Data shape is critical across\nthe data engineering lifecycle. For instance, an image’s pixel and red, green, blue\n(RGB) dimensions are necessary for training deep learning models. As another\nexample, if you’re trying to import a CSV file into a database table, and your CSV\nhas more columns than the database table, you’ll likely get an error during the import\nprocess. Here are some examples of the shapes of various kinds of data:\nTabular\nThe number of rows and columns in the dataset, commonly expressed as M rows\nand N columns\nSemistructured JSON\nThe key-value pairs and nesting depth occur with subelements\nUnstructured text\nNumber of words, characters, or bytes in the text body\nImages\nThe width, height, and RGB color depth (e.g., 8 bits per pixel)\nUncompressed audio\nNumber of channels (e.g., two for stereo), sample depth (e.g., 16 bits per sample),\nsample rate (e.g., 48 kHz), and length (e.g., 10,003 seconds)\nSize\nThe size of the data describes the number of bytes of a payload. A payload may range\nin size from single bytes to terabytes and larger. To reduce the size of a payload, it\nKey Engineering Considerations for the Ingestion Phase | 241\nmay be compressed into various formats such as ZIP and TAR (see the discussion of\ncompression in Appendix A ).\nA massive payload can also be split into chunks, which effectively reduces the size\nof the payload into smaller subsections. When loading a huge file into a cloud object\nstorage or data warehouse, this is a common practice as the small individual files\nare easier to transmit over a network (especially if they’re compressed). The smaller\nchunked files are sent to their destination and then reassembled after all data has\narrived.\nSchema and data types\nMany  data payloads have a schema, such as tabular and semistructured data. As\nmentioned earlier in this book, a schema describes the fields and types of data\nwithin those fields. Other data, such as unstructured text, images, and audio, will not\nhave an explicit schema or data types. However, they might come with technical file\ndescriptions on shape, data and file format, encoding, size, etc.\nAlthough you can connect to databases in various ways (such as file export, CDC,\nJDBC/ODBC), the connection is easy. The great engineering challenge is understand‐\ning the underlying schema. Applications organize data in various ways, and engineers\nneed to be intimately familiar with the organization of the data and relevant update\npatterns to make sense of it. The problem has been somewhat exacerbated by the\npopularity of object-relational mapping (ORM), which automatically generates sche‐\nmas based on object structure in languages such as Java or Python. Natural structures\nin an object-oriented language often map to something messy in an operational\ndatabase. Data engineers may need to familiarize themselves with the class structure\nof application code.\nSchema is not only for databases. As we’ve discussed, APIs present their schema\ncomplications. Many vendor APIs have friendly reporting methods that prepare data\nfor analytics. In other cases, engineers are not so lucky. The API is a thin wrapper\naround underlying systems, requiring engineers to understand application internals\nto use the data.\nMuch of the work associated with ingesting from source schemas happens in the\ndata engineering lifecycle transformation stage, which we discuss in Chapter 8 . We’ve\nplaced this discussion here because data engineers need to begin studying source\nschemas as soon they plan to ingest data from a new source.\nCommunication is critical for understanding source data, and engineers also have\nthe opportunity to reverse the flow of communication and help software engineers\nimprove data where it is produced. Later in this chapter, we’ll return to this topic in\n“Whom Y ou’ll Work With” on page 262 .\n242 | Chapter 7: Ingestion\nDetecting and handling upstream and downstream schema changes.    Changes in schema\nfrequently occur in source systems and are often well out of data engineers’ control.\nExamples of schema changes include the following:\n•Adding a new column•\n•Changing a column type•\n•Creating a new table•\n•Renaming a column•\nIt’s becoming increasingly common for ingestion tools to automate the detection of\nschema changes and even auto-update target tables. Ultimately, this is something of a\nmixed blessing. Schema changes can still break pipelines downstream of staging and\ningestion.\nEngineers must still implement strategies to respond to changes automatically and\nalert on changes that cannot be accommodated automatically. Automation is excel‐\nlent, but the analysts and data scientists who rely on this data should be informed\nof the schema changes that violate existing assumptions. Even if automation can\naccommodate a change, the new schema may adversely affect the performance of\nreports and models. Communication between those making schema changes and\nthose impacted by these changes is as important as reliable automation that checks\nfor schema changes.\nSchema registries.    In streaming data, every message has a schema, and these schemas\nmay evolve between producers and consumers. A schema registry  is a metadata\nrepository used to maintain schema and data type integrity in the face of constantly\nchanging schemas. Schema registries can also track schema versions and history. It\ndescribes the data model for messages, allowing consistent serialization and deseriali‐\nzation between producers and consumers. Schema registries are used in most major\ndata tools and clouds.\nMetadata\nIn addition to the apparent characteristics we’ve just covered, a payload often con‐\ntains metadata, which we first discussed in Chapter 2 . Metadata is data about data.\nMetadata can be as critical as the data itself. One of the significant limitations of\nthe early approach to the data lake—or data swamp, which could become a data\nsuperfund site—was a complete lack of attention to metadata. Without a detailed\ndescription of the data, it may be of little value. We’ve already discussed some types of\nmetadata (e.g., schema) and will address them many times throughout this chapter.\nKey Engineering Considerations for the Ingestion Phase | 243",12651
107-Push Versus Pull Versus Poll Patterns.pdf,107-Push Versus Pull Versus Poll Patterns,,0
108-Batch Ingestion Considerations.pdf,108-Batch Ingestion Considerations,"Push Versus Pull Versus Poll Patterns\nWe mentioned push versus pull when we introduced the data engineering lifecycle\nin Chapter 2 . A push  strategy ( Figure 7-7 ) involves a source system sending data to\na target, while a pull strategy ( Figure 7-8 ) entails a target reading data directly from\na source. As we mentioned in that discussion, the lines between these strategies are\nblurry.\nFigure 7-7. Pushing data from source to destination\nFigure 7-8. A destination pulling data from a source\nAnother pattern related to pulling is polling  for data ( Figure 7-9 ). Polling involves\nperiodically checking a data source for any changes. When changes are detected, the\ndestination pulls the data as it would in a regular pull situation.\nFigure 7-9. Polling for changes in a source system\nBatch Ingestion Considerations\nBatch ingestion, which involves processing data in bulk, is often a convenient way to\ningest data. This means that data is ingested by taking a subset of data from a source\nsystem, based either on a time interval or the size of accumulated data ( Figure 7-10 ).\n244 | Chapter 7: Ingestion\nFigure 7-10. Time-interval batch ingestion\nTime-interval batch ingestion  is widespread in traditional business ETL for data ware‐\nhousing. This pattern is often used to process data once a day, overnight during\noff-hours, to provide daily reporting, but other frequencies can also be used.\nSize-based batch ingestion  (Figure 7-11 ) is quite common when data is moved from\na streaming-based system into object storage; ultimately, you must cut the data into\ndiscrete blocks for future processing in a data lake. Some size-based ingestion systems\ncan break data into objects based on various criteria, such as the size in bytes of the\ntotal number of events.\nFigure 7-11. Size-based batch ingestion\nSome commonly used batch ingestion patterns, which we discuss in this section,\ninclude the following:\n•Snapshot or differential extraction•\n•File-based export and ingestion•\n•ETL versus ELT•\n•Inserts, updates, and batch size•\n•Data migration•\nBatch Ingestion Considerations | 245",2119
109-Snapshot or Differential Extraction.pdf,109-Snapshot or Differential Extraction,,0
110-File-Based Export and Ingestion.pdf,110-File-Based Export and Ingestion,,0
111-Inserts Updates and Batch Size.pdf,111-Inserts Updates and Batch Size,"Snapshot or Differential  Extraction\nData engineers must choose whether to capture full snapshots of a source system\nor differential (sometimes called incremental ) updates. With full snapshots , engineers\ngrab the entire current state of the source system on each update read. With  the\ndifferential  update  pattern, engineers can pull only the updates and changes since the\nlast read from the source system. While differential updates are ideal for minimizing\nnetwork traffic and target storage usage, full snapshot reads remain extremely com‐\nmon because of their simplicity.\nFile-Based Export and Ingestion\nData is quite often moved between databases and systems using files. Data is serial‐\nized into files in an exchangeable format, and these files are provided to an ingestion\nsystem. We consider file-based export to be a push-based  ingestion pattern. This is\nbecause data export and preparation work is done on the source system side.\nFile-based ingestion has several potential advantages over a direct database connec‐\ntion approach. It is often undesirable to allow direct access to backend systems\nfor security reasons. With file-based ingestion, export processes are run on the\ndata-source side, giving source system engineers complete control over what data\ngets exported and how the data is preprocessed. Once files are done, they can be\nprovided to the target system in various ways. Common file-exchange methods are\nobject storage, secure file transfer protocol (SFTP), electronic data interchange (EDI),\nor secure copy (SCP).\nETL Versus ELT\nChapter 3  introduced ETL and ELT, both extremely common ingestion, storage, and\ntransformation patterns you’ll encounter in batch workloads. The following are brief\ndefinitions of the extract and load parts of ETL and ELT:\nExtract\nThis means getting data from a source system. While extract  seems to imply\npulling  data, it can also be push based. Extraction may also require reading\nmetadata and schema changes.\nLoad\nOnce  data is extracted, it can either be transformed (ETL) before loading it into\na storage destination or simply loaded into storage for future transformation.\nWhen loading data, you should be mindful of the type of system you’re loading,\nthe schema of the data, and the performance impact of loading.\nWe cover ETL and ELT in greater detail in Chapter 8 .\n246 | Chapter 7: Ingestion",2400
112-Message and Stream Ingestion Considerations.pdf,112-Message and Stream Ingestion Considerations,"Inserts, Updates, and Batch Size\nBatch-oriented  systems often perform poorly when users attempt to perform many\nsmall-batch operations rather than a smaller number of large operations. For exam‐\nple, while it is common to insert one row at a time in a transactional database, this is\na bad pattern for many columnar databases, as it forces the creation of many small,\nsuboptimal files and forces the system to run a high number of create object  opera‐\ntions. Running many small in-place update operations is an even bigger problem\nbecause it causes the database to scan each existing column file to run the update.\nUnderstand the appropriate update patterns for the database or data store you’re\nworking with. Also, understand that certain technologies are purpose-built for high\ninsert rates. For example, Apache Druid and Apache Pinot can handle high insert\nrates. SingleStore can manage hybrid workloads that combine OLAP and OLTP\ncharacteristics. BigQuery performs poorly on a high rate of vanilla SQL single-row\ninserts but extremely well if data is fed in through its stream buffer. Know the limits\nand characteristics of your tools.\nData Migration\nMigrating data to a new database or environment is not usually trivial, and data needs\nto be moved in bulk. Sometimes this means moving data sizes that are hundreds of\nterabytes or much larger, often involving the migration of specific tables and moving\nentire databases and systems.\nData migrations probably aren’t a regular occurrence as a data engineer, but you\nshould be familiar with them. As is often the case for data ingestion, schema manage‐\nment is a crucial consideration. Suppose you’re migrating data from one database\nsystem to a different one (say, SQL Server to Snowflake). No matter how closely the\ntwo databases resemble each other, subtle differences almost always exist in the way\nthey handle schema. Fortunately, it is generally easy to test ingestion of a sample of\ndata and find schema issues before undertaking a complete table migration.\nMost data systems perform best when data is moved in bulk rather than as individual\nrows or events. File or object storage is often an excellent intermediate stage for\ntransferring data. Also, one of the biggest challenges of database migration is not the\nmovement of the data itself but the movement of data pipeline connections from the\nold system to the new one.\nBe aware that many tools are available to automate various types of data migrations.\nEspecially for large and complex migrations, we suggest looking at these options\nbefore doing this manually or writing your own migration solution.\nBatch Ingestion Considerations | 247",2690
113-Schema Evolution.pdf,113-Schema Evolution,,0
114-Late-Arriving Data.pdf,114-Late-Arriving Data,,0
115-Replay.pdf,115-Replay,"Message and Stream Ingestion Considerations\nIngesting event data is common. This section covers issues you should consider when\ningesting events, drawing on topics covered in Chapters 5 and 6.\nSchema Evolution\nSchema evolution is common when handling event data; fields may be added or\nremoved, or value types might change (say, a string to an integer). Schema evolution\ncan have unintended impacts on your data pipelines and destinations. For example,\nan IoT device gets a firmware update that adds a new field to the event it transmits,\nor a third-party API introduces changes to its event payload or countless other\nscenarios. All of these potentially impact your downstream capabilities.\nTo alleviate issues related to schema evolution, here are a few suggestions. First, if\nyour event-processing framework has a schema registry (discussed earlier in this\nchapter), use it to version your schema changes. Next, a dead-letter queue (described\nin “Error Handling and Dead-Letter Queues” on page 249) can help you investigate\nissues with events that are not properly handled. Finally, the low-fidelity route\n(and the most effective) is regularly communicating with upstream stakeholders\nabout potential schema changes and proactively addressing schema changes with the\nteams introducing these changes instead of reacting to the receiving end of breaking\nchanges.\nLate-Arriving Data\nThough  you probably prefer all event data to arrive on time, event data might arrive\nlate. A group of events might occur around the same time frame (similar event\ntimes), but some might arrive later than others (late ingestion times) because of\nvarious circumstances.\nFor example, an IoT device might be late sending a message because of internet\nlatency issues. This is common when ingesting data. Y ou should be aware of late-\narriving data and the impact on downstream systems and uses. Suppose you assume\nthat ingestion or process time is the same as the event time. Y ou may get some\nstrange results if your reports or analysis depend on an accurate portrayal of when\nevents occur. To handle late-arriving data, you need to set a cutoff time for when\nlate-arriving data will no longer be processed.\nOrdering and Multiple Delivery\nStreaming platforms are generally built out of distributed systems, which can cause\nsome complications. Specifically, messages may be delivered out of order and more\nthan once (at-least-once delivery). See the event-streaming platforms discussion in\nChapter 5  for more details.\n248 | Chapter 7: Ingestion",2560
116-Time to Live.pdf,116-Time to Live,,0
117-Message Size.pdf,117-Message Size,,0
118-Consumer Pull and Push.pdf,118-Consumer Pull and Push,"Replay\nReplay  allows  readers to request a range of messages from the history, allowing you\nto rewind your event history to a particular point in time. Replay is a key capability\nin many streaming ingestion platforms and is particularly useful when you need\nto reingest and reprocess data for a specific time range. For example, RabbitMQ\ntypically deletes messages after all subscribers consume them. Kafka, Kinesis, and\nPub/Sub all support event retention and replay.\nTime to Live\nHow long will you preserve your event record? A key parameter  is maximum message\nretention time , also known as the time to live  (TTL). TTL is usually a configuration\nyou’ll set for how long you want events to live before they are acknowledged and\ningested. Any unacknowledged event that’s not ingested after its TTL expires auto‐\nmatically disappears. This is helpful to reduce backpressure and unnecessary event\nvolume in your event-ingestion pipeline.\nFind the right balance of TTL impact on our data pipeline. An extremely short TTL\n(milliseconds or seconds) might cause most messages to disappear before processing.\nA very long TTL (several weeks or months) will create a backlog of many unpro‐\ncessed messages, resulting in long wait times.\nLet’s look at how some popular platforms handle TTL at the time of this writing.\nGoogle Cloud Pub/Sub supports retention periods of up to 7 days. Amazon Kinesis\nData Streams retention can be turned up to 365 days. Kafka can be configured for\nindefinite retention, limited by available disk space. (Kafka also supports the option\nto write older messages to cloud object storage, unlocking virtually unlimited storage\nspace and retention.)\nMessage Size\nMessage size is an easily overlooked issue: you must ensure that the streaming frame‐\nwork in question can handle the maximum expected message size. Amazon Kinesis\nsupports a maximum message size of 1 MB. Kafka defaults to this maximum size but\ncan be configured for a maximum of 20 MB or more. (Configurability may vary on\nmanaged service platforms.)\nError Handling and Dead-Letter Queues\nSometimes  events aren’t successfully ingested. Perhaps an event is sent to a nonexis‐\ntent topic or message queue, the message size may be too large, or the event has\nexpired past its TTL. Events that cannot be ingested need to be rerouted and stored in\na separate location called a dead-letter queue .\nMessage and Stream Ingestion Considerations | 249",2461
119-Location.pdf,119-Location,,0
120-Change Data Capture.pdf,120-Change Data Capture,"A dead-letter queue segregates problematic events from events that can be accepted\nby the consumer ( Figure 7-12 ). If events are not rerouted to a dead-letter queue, these\nerroneous events risk blocking other messages from being ingested. Data engineers\ncan use a dead-letter queue to diagnose why event ingestions errors occur and solve\ndata pipeline problems, and might be able to reprocess some messages in the queue\nafter fixing the underlying cause of errors.\nFigure 7-12. “Good” events are passed to the consumer, whereas “bad” events are stored\nin a dead-letter queue\nConsumer Pull and Push\nA consumer subscribing to a topic can get events in two ways: push and pull. Let’s\nlook at the ways some streaming technologies pull and push data. Kafka and Kinesis\nsupport only pull subscriptions. Subscribers read messages from a topic and confirm\nwhen they have been processed. In addition to pull subscriptions, Pub/Sub and\nRabbitMQ support push subscriptions, allowing these services to write messages to a\nlistener.\nPull subscriptions are the default choice for most data engineering applications, but\nyou may want to consider push capabilities for specialized applications. Note that\npull-only message ingestion systems can still push if you add an extra layer to handle\nthis.\nLocation\nIt is often desirable to integrate streaming across several locations for enhanced\nredundancy and to consume data close to where it is generated. As a general rule,\nthe closer your ingestion is to where data originates, the better your bandwidth and\nlatency. However, you need to balance this against the costs of moving data between\nregions to run analytics on a combined dataset. As always, data egress costs can spiral\nquickly. Do a careful evaluation of the trade-offs as you build out your architecture.\nWays to Ingest Data\nNow  that we’ve described some of the significant patterns underlying batch and\nstreaming ingestion, let’s focus on ways you can ingest data. Although we will cite\n250 | Chapter 7: Ingestion\nsome common ways, keep in mind that the universe of data ingestion practices and\ntechnologies is vast and growing daily.\nDirect Database Connection\nData  can be pulled from databases for ingestion by querying and reading over a net‐\nwork connection. Most commonly, this connection is made using ODBC or JDBC.\nODBC uses a driver hosted by a client accessing the database to translate commands\nissued to the standard ODBC API into commands issued to the database. The data‐\nbase returns query results over the wire, where the driver receives them and translates\nthem back into a standard form to be read by the client. For ingestion, the application\nutilizing the ODBC driver is an ingestion tool. The ingestion tool may pull data\nthrough many small queries or a single large query.\nJDBC is conceptually remarkably similar to ODBC. A Java driver connects to a\nremote database and serves as a translation layer between the standard JDBC API\nand the native network interface of the target database. It might seem strange to\nhave a database API dedicated to a single programming language, but there are\nstrong motivations for this. The Java Virtual Machine (JVM) is standard, portable\nacross hardware architectures and operating systems, and provides the performance\nof compiled code through a just-in-time (JIT) compiler. The JVM is an extremely\npopular compiling VM for running code in a portable manner.\nJDBC provides extraordinary database driver portability. ODBC drivers are shipped\nas OS and architecture native binaries; database vendors must maintain versions for\neach architecture/OS version that they wish to support. On the other hand, vendors\ncan ship a single JDBC driver that is compatible with any JVM language (e.g., Java,\nScala, Clojure, or Kotlin) and JVM data framework (i.e., Spark.) JDBC has become so\npopular that it is also used as an interface for non-JVM languages such as Python; the\nPython ecosystem provides translation tools that allow Python code to talk to a JDBC\ndriver running on a local JVM.\nJDBC and ODBC are used extensively for data ingestion from relational databases,\nreturning to the general concept of direct database connections. Various enhance‐\nments are used to accelerate data ingestion. Many data frameworks can parallelize\nseveral simultaneous connections and partition queries to pull data in parallel. On the\nother hand, nothing is free; using parallel connections also increases the load on the\nsource database.\nJDBC and ODBC were long the gold standards for data ingestion from databases, but\nthese connection standards are beginning to show their age for many data engineer‐\ning applications. These connection standards struggle with nested data, and they send\ndata as rows. This means that native nested data must be reencoded as string data to\nWays to Ingest Data | 251\nbe sent over the wire, and columns from columnar databases must be reserialized as\nrows.\nAs discussed in “File-Based Export and Ingestion” on page 246, many databases now\nsupport native file export that bypasses JDBC/ODBC and exports data directly in\nformats such as Parquet, ORC, and Avro. Alternatively, many cloud data warehouses\nprovide direct REST APIs.\nJDBC connections should generally be integrated with other ingestion technologies.\nFor example, we commonly use a reader process to connect to a database with JDBC,\nwrite the extracted data into multiple objects, and then orchestrate ingestion into\na downstream system (see Figure 7-13 ). The reader process can run in a wholly\nephemeral cloud instance or in an orchestration system.\nFigure 7-13. An ingestion process reads from a source database using JDBC, and then\nwrites objects into object storage. A target database (not shown) can be triggered to\ningest the data with an API call from an orchestration system.\nChange Data Capture\nChange data capture  (CDC),  introduced  in Chapter 2 , is the process of ingesting\nchanges from a source database system. For example, we might have a source Post‐\ngreSQL system that supports an application and periodically or continuously ingests\ntable changes for analytics.\nNote that our discussion here is by no means exhaustive. We introduce you to com‐\nmon patterns but suggest that you read the documentation on a particular database to\nhandle the details of CDC strategies.\nBatch-oriented CDC\nIf the database table in question has an updated_at  field containing the last time a\nrecord was written or updated, we can query the table to find all updated rows since\na specified time. We set the filter timestamp based on when we last captured changed\nrows from the tables. This process allows us to pull changes and differentially update\na target table.\nThis form of batch-oriented CDC has a key limitation: while we can easily determine\nwhich rows have changed since a point in time, we don’t necessarily obtain all\nchanges that were applied to these rows. Consider the example of running batch CDC\non a bank account table every 24 hours. This operational table shows the current\n252 | Chapter 7: Ingestion\naccount balance for each account. When money is moved in and out of accounts, the\nbanking application runs a transaction to update the balance.\nWhen we run a query to return all rows in the account table that changed in the last\n24 hours, we’ll see records for each account that recorded a transaction. Suppose that\na certain customer withdrew money five times using a debit card in the last 24 hours.\nOur query will return only the last account balance recorded in the 24 hour period;\nother records over the period won’t appear. This issue can be mitigated by utilizing\nan insert-only schema, where each account transaction is recorded as a new record in\nthe table (see “Insert-Only” on page 162 ).\nContinuous CDC\nContinuous CDC  captures all table history and can support near real-time data inges‐\ntion, either for real-time database replication or to feed real-time streaming analytics.\nRather than running periodic queries to get a batch of table changes, continuous\nCDC treats each write to the database as an event.\nWe can capture an event stream for continuous CDC in a couple of ways. One\nof the most common approaches with a transactional database such as PostgreSQL\nis log-based CDC . The database binary log records every change to the database\nsequentially (see “Database Logs” on page 161). A CDC tool can read this log and\nsend the events to a target, such as the Apache Kafka Debezium streaming platform.\nSome databases support a simplified, managed CDC paradigm. For instance, many\ncloud-hosted databases can be configured to directly trigger a serverless function\nor write to an event stream every time a change happens in the database. This com‐\npletely frees engineers from worrying about the details of how events are captured in\nthe database and forwarded.\nCDC and database replication\nCDC can be used to replicate between databases: events are buffered into a stream\nand asynchronously  written into a second database. However, many databases natively\nsupport a tightly coupled version of replication (synchronous replication) that keeps\nthe replica fully in sync with the primary database. Synchronous replication typi‐\ncally requires that the primary database and the replica are of the same type (e.g.,\nPostgreSQL to PostgreSQL). The advantage of synchronous replication is that the\nsecondary database can offload work from the primary database by acting as a read\nreplica; read queries can be redirected to the replica. The query will return the same\nresults that would be returned from the primary database.\nRead replicas are often used in batch data ingestion patterns to allow large scans\nto run without overloading the primary production database. In addition, an applica‐\ntion can be configured to fail over to the replica if the primary database becomes\nWays to Ingest Data | 253",9994
121-Moving Data with Object Storage.pdf,121-Moving Data with Object Storage,"1Karl Hughes, “The Bulk of Software Engineering Is Just Plumbing, ” Karl Hughes website, July 8, 2018,\nhttps://oreil.ly/uIuqJ .unavailable. No data will be lost in the failover because the replica is entirely in sync\nwith the primary database.\nThe advantage of asynchronous CDC replication is a loosely coupled architecture\npattern. While the replica might be slightly delayed from the primary database, this\nis often not a problem for analytics applications, and events can now be directed to\na variety of targets; we might run CDC replication while simultaneously directing\nevents to object storage and a streaming analytics processor.\nCDC considerations\nLike anything in technology, CDC is not free. CDC consumes various database\nresources, such as memory, disk bandwidth, storage, CPU time, and network band‐\nwidth. Engineers should work with production teams and run tests before turning on\nCDC on production systems to avoid operational problems. Similar considerations\napply to synchronous replication.\nFor batch CDC, be aware that running any large batch query against a transactional\nproduction system can cause excessive load. Either run such queries only at off-hours\nor use a read replica to avoid burdening the primary database.\nAPIs\nThe bulk of software engineering is just plumbing.\n—Karl Hughes1\nAs we mentioned in Chapter 5 , APIs are a data source that continues to grow in\nimportance and popularity. A typical organization may have hundreds of external\ndata sources such as SaaS platforms or partner companies. The hard reality is that\nno proper standard exists for data exchange over APIs. Data engineers can spend\na significant amount of time reading documentation, communicating with external\ndata owners, and writing and maintaining API connection code.\nThree trends are slowly changing this situation. First, many vendors provide API cli‐\nent libraries for various programming languages that remove much of the complexity\nof API access.\nSecond, numerous data connector platforms are available now as SaaS, open source,\nor managed open source. These platforms provide turnkey data connectivity to many\ndata sources; they offer frameworks for writing custom connectors for unsupported\ndata sources. See “Managed Data Connectors” on page 256 .\n254 | Chapter 7: Ingestion\nThe third trend is the emergence of data sharing (discussed in Chapter 5 )—i.e., the\nability to exchange data through a standard platform such as BigQuery, Snowflake,\nRedshift, or S3. Once data lands on one of these platforms, it is straightforward to\nstore it, process it, or move it somewhere else. Data sharing has had a large and rapid\nimpact in the data engineering space.\nDon’t reinvent the wheel when data sharing is not an option and direct API access\nis necessary. While a managed service might look like an expensive option, consider\nthe value of your time and the opportunity cost of building API connectors when you\ncould be spending your time on higher-value work.\nIn addition, many managed services now support building custom API connectors.\nThis may provide API technical specifications in a standard format or writing con‐\nnector code that runs in a serverless function framework (e.g., AWS Lambda) while\nletting the managed service handle the details of scheduling and synchronization.\nAgain, these services can be a huge time-saver for engineers, both for development\nand ongoing maintenance.\nReserve your custom connection work for APIs that aren’t well supported by existing\nframeworks; you will find that there are still plenty of these to work on. Handling\ncustom API connections has two main aspects: software development and ops. Fol‐\nlow software development best practices; you should use version control, continuous\ndelivery, and automated testing. In addition to following DevOps best practices, con‐\nsider an orchestration framework, which can dramatically streamline the operational\nburden of data ingestion.\nMessage Queues and Event-Streaming Platforms\nMessage queues and event-streaming platforms are widespread ways to ingest real-\ntime data from web and mobile applications, IoT sensors, and smart devices. As\nreal-time data becomes more ubiquitous, you’ll often find yourself either introducing\nor retrofitting ways to handle real-time data in your ingestion workflows. As such,\nit’s essential to know how to ingest real-time data. Popular real-time data ingestion\nincludes message queues or event-streaming platforms, which we covered in Chap‐\nter 5 . Though these are both source systems, they also act as ways to ingest data. In\nboth cases, you consume events from the publisher you subscribe to.\nRecall the differences between messages and streams. A message  is handled at the\nindividual event level and is meant to be transient. Once a message is consumed, it\nis acknowledged and removed from the queue. On the other hand, a stream  ingests\nevents into an ordered log. The log persists for as long as you wish, allowing events\nto be queried over various ranges, aggregated, and combined with other streams to\ncreate new transformations published to downstream consumers. In Figure 7-14 ,\nwe have two producers (producers 1 and 2) sending events to two consumers\nWays to Ingest Data | 255\n(consumers  1 and 2). These events are combined into a new dataset and sent to a\nproducer for downstream consumption.\nFigure 7-14. Two datasets are produced and consumed (producers 1 and 2) and then\ncombined, with the combined data published to a new producer (producer 3)\nThe last point is an essential difference between batch and streaming ingestion.\nWhereas batch usually involves static workflows (ingest data, store it, transform it,\nand serve it), messages and streams are fluid. Ingestion can be nonlinear, with data\nbeing published, consumed, republished, and reconsumed. When designing your\nreal-time ingestion workflows, keep in mind how data will flow.\nAnother consideration is the throughput of your real-time data pipelines. Messages\nand events should flow with as little latency as possible, meaning you should pro‐\nvision adequate partition (or shard) bandwidth and throughput. Provide sufficient\nmemory, disk, and CPU resources for event processing, and if you’re managing your\nreal-time pipelines, incorporate autoscaling to handle spikes and save money as load\ndecreases. For these reasons, managing your streaming platform can entail significant\noverhead. Consider managed services for your real-time ingestion pipelines, and\nfocus your attention on ways to get value from your real-time data.\nManaged Data Connectors\nThese  days, if you’re considering writing a data ingestion connector to a database\nor API, ask yourself: has this already been created? Furthermore, is there a service\nthat will manage the nitty-gritty details of this connection for me? “ APIs”  on page\n254 mentions the popularity of managed data connector platforms and frameworks.\nThese tools aim to provide a standard set of connectors available out of the box\nto spare data engineers building complicated plumbing to connect to a particular\nsource. Instead of creating and managing a data connector, you outsource this service\nto a third party.\nGenerally, options in the space allow users to set a target and source, ingest in various\nways (e.g., CDC, replication, truncate and reload), set permissions and credentials,\nconfigure an update frequency, and begin syncing data. The vendor or cloud behind\nthe scenes fully manages and monitors data syncs. If data synchronization fails, you’ll\nreceive an alert with logged information on the cause of the error.\n256 | Chapter 7: Ingestion",7704
122-EDI.pdf,122-EDI,,0
123-Practical Issues with Common File Formats.pdf,123-Practical Issues with Common File Formats,"We suggest using managed connector platforms instead of creating and managing\nyour connectors. Vendors and OSS projects each typically have hundreds of prebuilt\nconnector options and can easily create custom connectors. The creation and man‐\nagement of data connectors is largely undifferentiated heavy lifting these days and\nshould be outsourced whenever possible.\nMoving Data with Object Storage\nObject storage is a multitenant system in public clouds, and it supports storing\nmassive amounts of data. This makes object storage ideal for moving data in and\nout of data lakes, between teams, and transferring data between organizations. Y ou\ncan even provide short-term access to an object with a signed URL, giving a user\ntemporary permission.\nIn our view, object storage is the most optimal and secure way to handle file\nexchange. Public cloud storage implements the latest security standards, has a robust\ntrack record of scalability and reliability, accepts files of arbitrary types and sizes, and\nprovides high-performance data movement. We discussed object storage much more\nextensively in Chapter 6 .\nEDI\nAnother  practical reality for data engineers is electronic data interchange  (EDI). The\nterm is vague enough to refer to any data movement method. It usually refers to\nsomewhat archaic means of file exchange, such as by email or flash drive. Data\nengineers will find that some data sources do not support more modern means of\ndata transport, often because of archaic IT systems or human process limitations.\nEngineers can at least enhance EDI through automation. For example, they can set\nup a cloud-based email server that saves files onto company object storage as soon\nas they are received. This can trigger orchestration processes to ingest and process\ndata. This is much more robust than an employee downloading the attached file and\nmanually uploading it to an internal system, which we still frequently see.\nDatabases and File Export\nEngineers  should be aware of how the source database systems handle file export.\nExport involves large data scans that significantly load the database for many transac‐\ntional systems. Source system engineers must assess when these scans can be run\nwithout affecting application performance and might opt for a strategy to mitigate the\nload. Export queries can be broken into smaller exports by querying over key ranges\nor one partition at a time. Alternatively, a read replica can reduce load. Read replicas\nare especially appropriate if exports happen many times a day and coincide with a\nhigh source system load.\nWays to Ingest Data | 257",2633
124-SSH.pdf,124-SSH,"Major cloud data warehouses are highly optimized for direct file export. For example,\nSnowflake, BigQuery, Redshift, and others support direct export to object storage in\nvarious formats.\nPractical Issues with Common File Formats\nEngineers should also be aware of the file formats to export. CSV is still ubiquitous\nand highly error prone at the time of this writing. Namely, CSV’s default delimiter is\nalso one of the most familiar characters in the English language—the comma! But it\ngets worse.\nCSV is by no means a uniform format. Engineers must stipulate the delimiter, quote\ncharacters, and escaping to appropriately handle the export of string data. CSV also\ndoesn’t natively encode schema information or directly support nested structures.\nCSV file encoding and schema information must be configured in the target system\nto ensure appropriate ingestion. Autodetection is a convenience feature provided in\nmany cloud environments but is inappropriate for production ingestion. As a best\npractice, engineers should record CSV encoding and schema details in file metadata.\nMore robust and expressive export formats include Parquet , Avro , Arrow , and ORC\nor JSON . These formats natively encode schema information and handle arbitrary\nstring data with no particular intervention. Many of them also handle nested data\nstructures natively so that JSON fields are stored using internal nested structures\nrather than simple strings. For columnar databases, columnar formats (Parquet,\nArrow, ORC) allow more efficient data export because columns can be directly\ntranscoded between formats. These formats are also generally more optimized for\nquery engines. The Arrow file format is designed to map data directly into processing\nengine memory, providing high performance in data lake environments.\nThe disadvantage of these newer formats is that many of them are not natively\nsupported by source systems. Data engineers are often forced to work with CSV data\nand then build robust exception handling and error detection to ensure data quality\non ingestion. See Appendix A  for a more extensive discussion of file formats.\nShell\nThe shell  is an interface by which you may execute commands to ingest data. The\nshell can be used to script workflows for virtually any software tool, and shell script‐\ning is still used extensively in ingestion processes. A shell script might read data\nfrom a database, reserialize it into a different file format, upload it to object storage,\nand trigger an ingestion process in a target database. While storing data on a single\ninstance or server is not highly scalable, many of our data sources are not particularly\nlarge, and such approaches work just fine.\n258 | Chapter 7: Ingestion",2754
125-SFTP and SCP.pdf,125-SFTP and SCP,,0
126-Web Interface.pdf,126-Web Interface,"In addition, cloud vendors generally provide robust CLI-based tools. It is possible\nto run complex ingestion processes simply by issuing commands to the AWS CLI .\nAs ingestion processes grow more complicated and the SLA grows more stringent,\nengineers should consider moving to a proper orchestration system.\nSSH\nSSH is not an ingestion strategy but a protocol used with other ingestion strategies.\nWe use SSH in a few ways. First, SSH can be used for file transfer with SCP , as\nmentioned earlier. Second, SSH tunnels are used to allow secure, isolated connections\nto databases.\nApplication databases should never be directly exposed on the internet. Instead, engi‐\nneers can set up a bastion host—i.e., an intermediate host instance that can connect\nto the database in question. This host machine is exposed on the internet, although\nlocked down for minimal access from only specified IP addresses to specified ports.\nTo connect to the database, a remote machine first opens an SSH tunnel connection\nto the bastion host and then connects from the host machine to the database.\nSFTP and SCP\nAccessing  and sending data both from secure FTP (SFTP) and secure copy (SCP) are\ntechniques you should be familiar with, even if data engineers do not typically use\nthese regularly (IT or security/secOps will handle this).\nEngineers rightfully cringe at the mention of SFTP (occasionally, we even hear instan‐\nces of FTP being used in production). Regardless, SFTP is still a practical reality for\nmany businesses. They work with partner businesses that consume or provide data\nusing SFTP and are unwilling to rely on other standards. To avoid data leaks, security\nanalysis is critical in these situations.\nSCP is a file-exchange protocol that runs over an SSH connection. SCP can be\na secure file-transfer option if it is configured correctly. Again, adding additional\nnetwork access control (defense in depth) to enhance SCP security is highly\nrecommended.\nWebhooks\nWebhooks , as we discussed in Chapter 5 , are often referred to as reverse APIs . For a\ntypical REST data API, the data provider gives engineers API specifications that they\nuse to write their data ingestion code. The code makes requests and receives data in\nresponses.\nWith a webhook ( Figure 7-15 ), the data provider defines an API request specifica‐\ntion, but the data provider makes API calls  rather than receiving them; it’s the data\nconsumer’s responsibility to provide an API endpoint for the provider to call. The\nWays to Ingest Data | 259",2546
127-Data Sharing.pdf,127-Data Sharing,"consumer is responsible for ingesting each request and handling data aggregation,\nstorage, and processing.\nFigure 7-15. A basic webhook ingestion architecture built from cloud services\nWebhook-based data ingestion architectures can be brittle, difficult to maintain,\nand inefficient. Using appropriate off-the-shelf tools, data engineers can build more\nrobust webhook architectures with lower maintenance and infrastructure costs. For\nexample, a webhook pattern in AWS might use a serverless function framework\n(Lambda) to receive incoming events, a managed event-streaming platform to store\nand buffer messages (Kinesis), a stream-processing framework to handle real-time\nanalytics (Flink), and an object store for long-term storage (S3).\nY ou’ll notice that this architecture does much more than simply ingest the data. This\nunderscores ingestion’s entanglement with the other stages of the data engineering\nlifecycle; it is often impossible to define your ingestion architecture without making\ndecisions about storage and processing.\nWeb Interface\nWeb interfaces for data access remain a practical reality for data engineers. We fre‐\nquently run into situations where not all data and functionality in a SaaS platform is\nexposed through automated interfaces such as APIs and file drops. Instead, someone\nmust manually access a web interface, generate a report, and download a file to a local\nmachine. This has obvious drawbacks, such as people forgetting to run the report or\nhaving their laptop die. Where possible, choose tools and workflows that allow for\nautomated access to data.\nWeb Scraping\nWeb scraping  automatically  extracts data from web pages, often by combing the web\npage’s various HTML elements. Y ou might scrape ecommerce sites to extract product\npricing information or scrape multiple news sites for your news aggregator. Web\nscraping is widespread, and you may encounter it as a data engineer. It’s also a murky\narea where ethical and legal lines are blurry.\n260 | Chapter 7: Ingestion\nHere is some top-level advice to be aware of before undertaking any web-scraping\nproject. First, ask yourself if you should be web scraping or if data is available from\na third party. If your decision is to web scrape, be a good citizen. Don’t inadvertently\ncreate a denial-of-service (DoS) attack, and don’t get your IP address blocked. Under‐\nstand how much traffic you generate and pace your web-crawling activities appropri‐\nately. Just because you can spin up thousands of simultaneous Lambda functions to\nscrape doesn’t mean you should; excessive web scraping could lead to the disabling of\nyour AWS account.\nSecond, be aware of the legal implications of your activities. Again, generating DoS\nattacks can entail legal consequences. Actions that violate terms of service may cause\nheadaches for your employer or you personally.\nThird, web pages constantly change their HTML element structure, making it tricky\nto keep your web scraper updated. Ask yourself, is the headache of maintaining these\nsystems worth the effort?\nWeb scraping has interesting implications for the data engineering lifecycle process‐\ning stage; engineers should think about various factors at the beginning of a web-\nscraping project. What do you intend to do with the data? Are you just pulling\nrequired fields from the scraped HTML by using Python code and then writing these\nvalues to a database? Do you intend to maintain the complete HTML code of the\nscraped websites and process this data using a framework like Spark? These decisions\nmay lead to very different architectures downstream of ingestion.\nTransfer Appliances for Data Migration\nFor massive quantities of data (100 TB or more), transferring data directly over the\ninternet may be a slow and costly process. At this scale, the fastest, most efficient way\nto move data is not over the wire but by truck. Cloud vendors offer the ability to send\nyour data via a physical “box of hard drives. ” Simply order a storage device, called\na transfer appliance , load your data from your servers, and then send it back to the\ncloud vendor, which will upload your data.\nThe suggestion is to consider using a transfer appliance if your data size hovers\naround 100 TB. On the extreme end, AWS even offers Snowmobile , a transfer appli‐\nance sent to you in a semitrailer! Snowmobile is intended to lift and shift an entire\ndata center, in which data sizes are in the petabytes or greater.\nTransfer appliances are handy for creating hybrid-cloud or multicloud setups. For\nexample, Amazon’s data transfer appliance (AWS Snowball) supports import and\nexport. To migrate into a second cloud, users can export their data into a Snowball\ndevice and then import it into a second transfer appliance to move data into GCP\nor Azure. This might sound awkward, but even when it’s feasible to push data over\nWays to Ingest Data | 261",4925
128-Whom Youll Work With.pdf,128-Whom Youll Work With,,0
129-Downstream Stakeholders.pdf,129-Downstream Stakeholders,"the internet between clouds, data egress fees make this a costly proposition. Physical\ntransfer appliances are a cheaper alternative when the data volumes are significant.\nRemember that transfer appliances and data migration services are one-time data\ningestion events and are not suggested for ongoing workloads. Suppose you have\nworkloads requiring constant data movement in either a hybrid or multicloud sce‐\nnario. In that case, your data sizes are presumably batching or streaming much\nsmaller data sizes on an ongoing basis.\nData Sharing\nData sharing  is growing as a popular option for consuming data (see Chapters 5\nand 6). Data providers will offer datasets to third-party subscribers, either for free or\nat a cost. These datasets are often shared in a read-only fashion, meaning you can\nintegrate these datasets with your own data (and other third-party datasets), but you\ndo not own the shared dataset. In the strict sense, this isn’t ingestion, where you get\nphysical possession of the dataset. If the data provider decides to remove your access\nto a dataset, you’ll no longer have access to it.\nMany cloud platforms offer data sharing, allowing you to share your data and\nconsume data from various providers. Some of these platforms also provide data\nmarketplaces where companies and organizations can offer their data for sale.\nWhom You’ll Work With\nData ingestion sits at several organizational boundaries. In developing and managing\ndata ingestion pipelines, data engineers will work with both people and systems\nsitting upstream (data producers) and downstream (data consumers).\nUpstream Stakeholders\nA significant disconnect often exists between those responsible for generating data\n—typically, software engineers—and the data engineers who will prepare this data\nfor analytics and data science. Software engineers and data engineers usually sit in\nseparate organizational silos; if they think about data engineers, they typically see\nthem simply as downstream consumers of the data exhaust from their application,\nnot as stakeholders.\nWe see this current state of affairs as a problem and a significant opportunity. Data\nengineers can improve the quality of their data by inviting software engineers to be\nstakeholders in data engineering outcomes. The vast majority of software engineers\nare well aware of the value of analytics and data science but don’t necessarily have\naligned incentives to contribute to data engineering efforts directly.\n262 | Chapter 7: Ingestion",2525
130-Security.pdf,130-Security,"Simply improving communication is a significant first step. Often software engineers\nhave already identified potentially valuable data for downstream consumption. Open‐\ning a communication channel encourages software engineers to get data into shape\nfor consumers and communicate about data changes to prevent pipeline regressions.\nBeyond communication, data engineers can highlight the contributions of software\nengineers to team members, executives, and especially product managers. Involving\nproduct managers in the outcome and treating downstream data processed as part of\na product encourages them to allocate scarce software development to collaboration\nwith data engineers. Ideally, software engineers can work partially as extensions of the\ndata engineering team; this allows them to collaborate on various projects, such as\ncreating an event-driven architecture to enable real-time analytics.\nDownstream Stakeholders\nWho is the ultimate customer for data ingestion? Data engineers focus on data\npractitioners and technology leaders such as data scientists, analysts, and chief tech‐\nnical officers. They would do well also to remember their broader circle of business\nstakeholders such as marketing directors, vice presidents over the supply chain, and\nCEOs.\nToo often, we see data engineers pursuing sophisticated projects (e.g., real-time\nstreaming buses or complex data systems) while digital marketing managers next\ndoor are left downloading Google Ads reports manually. View data engineering as a\nbusiness, and recognize who your customers are. Often basic automation of ingestion\nprocesses has significant value, especially for departments like marketing that control\nmassive budgets and sit at the heart of revenue for the business. Basic ingestion work\nmay seem tedious, but delivering value to these core parts of the company will open\nup more budget and more exciting long-term data engineering opportunities.\nData engineers can also invite more executive participation in this collaborative\nprocess. For a good reason, data-driven culture is quite fashionable in business\nleadership circles. Still, it is up to data engineers and other data practitioners to\nprovide executives with guidance on the best structure for a data-driven business.\nThis means communicating the value of lowering barriers between data producers\nand data engineers while supporting executives in breaking down silos and setting up\nincentives to lead to a more unified data-driven culture.\nOnce again, communication  is the watchword. Honest communication early and often\nwith stakeholders will go a long way to ensure that your data ingestion adds value.\nUndercurrents\nVirtually  all the undercurrents touch the ingestion phase, but we’ll emphasize the\nmost salient ones here.\nUndercurrents | 263",2827
131-Data Management.pdf,131-Data Management,"Security\nMoving  data introduces security vulnerabilities because you have to transfer data\nbetween locations. The last thing you want is to capture or compromise the data\nwhile moving.\nConsider where the data lives and where it is going. Data that needs to move within\nyour VPC should use secure endpoints and never leave the confines of the VPC.\nUse a VPN or a dedicated private connection if you need to send data between the\ncloud and an on-premises network. This might cost money, but the security is a good\ninvestment. If your data traverses the public internet, ensure that the transmission is\nencrypted. It is always a good practice to encrypt data over the wire.\nData Management\nNaturally, data management begins at data ingestion. This is the starting point for\nlineage and data cataloging; from this point on, data engineers need to think about\nschema changes, ethics, privacy, and compliance.\nSchema changes\nSchema changes (such as adding, changing, or removing columns in a database table)\nremain, from our perspective, an unsettled issue in data management. The traditional\napproach is a careful command-and-control review process. Working with clients at\nlarge enterprises, we have been quoted lead times of six months for the addition of a\nsingle field. This is an unacceptable impediment to agility.\nOn the opposite end of the spectrum, any schema change in the source triggers\ntarget tables to be re-created with the new schema. This solves schema problems at\nthe ingestion stage but can still break downstream pipelines and destination storage\nsystems.\nOne possible solution, which we, the authors, have meditated on for a while, is an\napproach pioneered by Git version control. When Linus Torvalds was developing Git,\nmany of his choices were inspired by the limitations of Concurrent Versions System\n(CVS). CVS is completely centralized; it supports only one current official version of\nthe code, stored on a central project server. To make Git a truly distributed system,\nTorvalds used the notion of a tree; each developer could maintain their processed\nbranch of the code and then merge to or from other branches.\nA few years ago, such an approach to data was unthinkable. On-premises MPP\nsystems are typically operated at close to maximum storage capacity. However, stor‐\nage is cheap in big data and cloud data warehouse environments. One may quite\neasily maintain multiple versions of a table with different schemas and even different\nupstream transformations. Teams can support various “development” versions of\na table by using orchestration tools such as Airflow; schema changes, upstream\n264 | Chapter 7: Ingestion\ntransformation, and code changes can appear in development tables before official\nchanges to the main  table.\nData ethics, privacy, and compliance\nClients  often ask for our advice on encrypting sensitive data in databases, which\ngenerally leads us to ask a fundamental question: do you need the sensitive data\nyou’re trying to encrypt? As it turns out, this question often gets overlooked when\ncreating requirements and solving problems.\nData engineers should always train themselves to ask this question when setting\nup ingestion pipelines. They will inevitably encounter sensitive data; the natural\ntendency is to ingest it and forward it to the next step in the pipeline. But if this data\nis not needed, why collect it at all? Why not simply drop sensitive fields before data is\nstored? Data cannot leak if it is never collected.\nWhere it is truly necessary to keep track of sensitive identities, it is common practice\nto apply tokenization to anonymize identities in model training and analytics. But\nengineers should look at where this tokenization is used. If possible, hash data at\ningestion time.\nData engineers cannot avoid working with highly sensitive data in some cases. Some\nanalytics systems must present identifiable, sensitive information. Engineers must\nact under the highest ethical standards whenever they handle sensitive data. In\naddition, they can put in place a variety of practices to reduce the direct handling of\nsensitive data. Aim as much as possible for touchless production  where sensitive data\nis involved. This means that engineers develop and test code on simulated or cleansed\ndata in development and staging environments but automated code deployments to\nproduction.\nTouchless production is an ideal that engineers should strive for, but situations\ninevitably arise that cannot be fully solved in development and staging environments.\nSome bugs may not be reproducible without looking at the live data that is triggering\na regression. For these cases, put a broken-glass process in place: require at least two\npeople to approve access to sensitive data in the production environment. This access\nshould be tightly scoped to a particular issue and come with an expiration date.\nOur last bit of advice on sensitive data: be wary of naive technological solutions to\nhuman problems. Both encryption and tokenization are often treated like privacy\nmagic bullets. Most cloud-based storage systems and nearly all databases encrypt data\nat rest and in motion by default. Generally, we don’t see encryption problems but data\naccess problems. Is the solution to apply an extra layer of encryption to a single field\nor to control access to that field? After all, one must still tightly manage access to the\nencryption key. Legitimate use cases exist for single-field encryption, but watch out\nfor ritualistic encryption.\nUndercurrents | 265",5587
132-DataOps.pdf,132-DataOps,"On the tokenization front, use common sense and assess data access scenarios. If\nsomeone had the email of one of your customers, could they easily hash the email\nand find the customer in your data? Thoughtlessly hashing data without salting and\nother strategies may not protect privacy as well as you think.\nDataOps\nReliable  data pipelines are the cornerstone of the data engineering lifecycle. When\nthey fail, all downstream dependencies come to a screeching halt. Data warehouses\nand data lakes aren’t replenished with fresh data, and data scientists and analysts can’t\neffectively do their jobs; the business is forced to fly blind.\nEnsuring that your data pipelines are properly monitored is a crucial step toward\nreliability and effective incident response. If there’s one stage in the data engineering\nlifecycle where monitoring is critical, it’s in the ingestion stage. Weak or nonexistent\nmonitoring means the pipelines may or may not be working. Referring back to\nour earlier discussion on time, be sure to track the various aspects of time—event\ncreation, ingestion, process, and processing times. Y our data pipelines should predict‐\nably process data in batches or streams. We’ve seen countless examples of reports\nand ML models generated from stale data. In one extreme case, an ingestion pipeline\nfailure wasn’t detected for six months. (One might question the concrete utility of\nthe data in this instance, but that’s another matter.) This was very much avoidable\nthrough proper monitoring.\nWhat should you monitor? Uptime, latency, and data volumes processed are good\nplaces to start. If an ingestion job fails, how will you respond? In general, build moni‐\ntoring into your pipelines from the beginning rather than waiting for deployment.\nMonitoring is key, as is knowledge of the behavior of the upstream systems you\ndepend on and how they generate data. Y ou should be aware of the number of events\ngenerated per time interval you’re concerned with (events/minute, events/second, and\nso on) and the average size of each event. Y our data pipeline should handle both the\nfrequency and size of the events you’re ingesting.\nThis also applies to third-party services. In the case of these services, what you’ve\ngained in terms of lean operational efficiencies (reduced headcount) is replaced by\nsystems you depend on being outside of your control. If you’re using a third-party\nservice (cloud, data integration service, etc.), how will you be alerted if there’s an\noutage? What’s your response plan if a service you depend on suddenly goes offline?\nSadly, no universal response plan exists for third-party failures. If you can fail over to\nother servers, preferably in another zone or region, definitely set this up.\nIf your data ingestion processes are built internally, do you have the proper testing\nand deployment automation to ensure that the code functions in production? And if\nthe code is buggy or fails, can you roll it back to a working version?\n266 | Chapter 7: Ingestion\n2Andy Petrella, “Datastrophes, ” Medium , March 1, 2021, https://oreil.ly/h6FRW .\n3Danny Sullivan, “Dark Google: One Y ear Since Search Terms Went ‘Not Provided, ’” MarTech , October 19,\n2012, https://oreil.ly/Fp8ta .Data-quality tests\nWe often refer to data as a silent killer. If quality, valid data is the foundation of\nsuccess in today’s businesses, using bad data to make decisions is much worse than\nhaving no data. Bad data has caused untold damage to businesses; these data disasters\nare sometimes called datastrophes.2\nData is entropic; it often changes in unexpected ways without warning. One of\nthe inherent differences between DevOps and DataOps is that we expect software\nregressions only when we deploy changes, while data often presents regressions\nindependently because of events outside our control.\nDevOps engineers are typically able to detect problems by using binary conditions.\nHas the request failure rate breached a certain threshold? How about response\nlatency? In the data space, regressions often manifest as subtle statistical distortions.\nIs a change in search-term statistics a result of customer behavior? Of a spike in bot\ntraffic that has escaped the net? Of a site test tool deployed in some other part of the\ncompany?\nLike system failures in DevOps, some data regressions are immediately visible. For\nexample, in the early 2000s, Google provided search terms to websites when users\narrived from search. In 2011, Google began withholding this information in some\ncases to protect user privacy better. Analysts quickly saw “not provided” bubbling to\nthe tops of their reports.3\nThe truly dangerous data regressions are silent and can come from inside or outside a\nbusiness. Application developers may change the meaning of database fields without\nadequately communicating with data teams. Changes to data from third-party sour‐\nces may go unnoticed. In the best-case scenario, reports break in obvious ways. Often\nbusiness metrics are distorted unbeknownst to decision makers.\nWhenever possible, work with software engineers to fix data-quality issues at the\nsource. It’s surprising how many data-quality issues can be handled by respecting\nbasic best practices in software engineering, such as logs to capture the history of data\nchanges, checks (nulls, etc.), and exception handling (try, catch, etc.).\nTraditional data testing tools are generally built on simple binary logic. Are nulls\nappearing in a non-nullable field? Are new, unexpected items showing up in a\ncategorical column? Statistical data testing is a new realm, but one that is likely to\ngrow dramatically in the next five years.\nUndercurrents | 267",5721
133-Orchestration.pdf,133-Orchestration,,0
134-Software Engineering.pdf,134-Software Engineering,,0
135-Additional Resources.pdf,135-Additional Resources,"Orchestration\nIngestion  generally sits at the beginning of a large and complex data graph; since\ningestion is the first stage of the data engineering lifecycle, ingested data will flow into\nmany more data processing steps, and data from many sources will commingle in\ncomplex ways. As we’ve emphasized throughout this book, orchestration is a crucial\nprocess for coordinating these steps.\nOrganizations in an early stage of data maturity may choose to deploy ingestion\nprocesses as simple scheduled cron jobs. However, it is crucial to recognize that this\napproach is brittle and can slow the velocity of data engineering deployment and\ndevelopment.\nAs data pipeline complexity grows, true orchestration is necessary. By true orches‐\ntration, we mean a system capable of scheduling complete task graphs rather than\nindividual tasks. An orchestration can start each ingestion task at the appropriate\nscheduled time. Downstream processing and transform steps begin as ingestion tasks\nare completed. Further downstream, processing steps lead to additional processing\nsteps.\nSoftware Engineering\nThe ingestion stage of the data engineering lifecycle is engineering intensive. This\nstage sits at the edge of the data engineering domain and often interfaces with\nexternal systems, where software and data engineers have to build a variety of custom\nplumbing.\nBehind the scenes, ingestion is incredibly complicated, often with teams operating\nopen source frameworks like Kafka or Pulsar, or some of the biggest tech companies\nrunning their own forked or homegrown ingestion solutions. As discussed in this\nchapter, managed data connectors have simplified the ingestion process, such as\nFivetran, Matillion, and Airbyte. Data engineers should take advantage of the best\navailable tools—primarily, managed tools and services that do a lot of the heavy\nlifting for you—and develop high software development competency in areas where\nit matters. It pays to use proper version control and code review processes and\nimplement appropriate tests even for any ingestion-related code.\nWhen writing software, your code needs to be decoupled. Avoid writing monolithic\nsystems with tight dependencies on the source or destination systems.\nConclusion\nIn your work as a data engineer, ingestion will likely consume a significant part of\nyour energy and effort. At the heart, ingestion is plumbing, connecting pipes to other\npipes, ensuring that data flows consistently and securely to its destination. At times,\n268 | Chapter 7: Ingestion\nthe minutiae of ingestion may feel tedious, but the exciting data applications (e.g.,\nanalytics and ML) cannot happen without it.\nAs we’ve emphasized, we’re also in the midst of a sea change, moving from batch\ntoward streaming data pipelines. This is an opportunity for data engineers to discover\ninteresting applications for streaming data, communicate these to the business, and\ndeploy exciting new technologies.\nAdditional Resources\n•Airbyte’s “Connections and Sync Modes” web page •\n•Chapter 6, “Batch Is a Special Case of Streaming, ” in Introduction to Apache Flink •\nby Ellen Friedman and Kostas Tzoumas (O’Reilly)\n•“The Dataflow Model: A Practical Approach to Balancing Correctness, Latency,•\nand Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing”  by Tyler\nAkidau et al.\n•Google Cloud’s “Streaming Pipelines” web page •\n•Microsoft’s “Snapshot Window (Azure Stream Analytics)” documentation •\nAdditional Resources | 269",3510
136-The Query Optimizer.pdf,136-The Query Optimizer,"CHAPTER 8\nQueries, Modeling, and Transformation\nUp to this point, the stages of the data engineering lifecycle have primarily been about\npassing data from one place to another or storing it. In this chapter, you’ll learn\nhow to make data useful. By understanding queries, modeling, and transformations\n(see Figure 8-1 ), you’ll have the tools to turn raw data ingredients into something\nconsumable by downstream stakeholders.\nFigure 8-1. Transformations allow us to create value from data\nWe’ll first discuss queries and the significant patterns underlying them. Second, we\nwill look at the major data modeling patterns you can use to introduce business logic\ninto your data. Then, we’ll cover transformations, which take the logic of your data\nmodels and the results of queries and make them useful for more straightforward\n271\ndownstream consumption. Finally, we’ll cover whom you’ll work with and the under‐\ncurrents as they relate to this chapter.\nA variety of techniques can be used to query, model, and transform data in SQL and\nNoSQL databases. This section focuses on queries made to an OLAP system, such\nas a data warehouse or data lake. Although many languages exist for querying, for\nthe sake of convenience and familiarity, throughout most of this chapter, we’ll focus\nheavily on SQL, the most popular and universal query language. Most of the concepts\nfor OLAP databases and SQL will translate to other types of databases and query\nlanguages. This chapter assumes you have an understanding of the SQL language and\nrelated concepts like primary and foreign keys. If these ideas are unfamiliar to you,\ncountless resources are available to help you get started.\nA note on the terms used in this chapter. For convenience, we’ll use the term  database\nas a shorthand for a query engine and the storage it’s querying; this could be a\ncloud data warehouse or Apache Spark querying data stored in S3. We assume the\ndatabase has a storage engine that organizes the data under the hood. This extends to\nfile-based queries (loading a CSV file into a Python notebook) and queries against file\nformats such as Parquet.\nAlso, note that this chapter focuses mainly on the query, modeling patterns, and\ntransformations related to structured and semistructured data, which data engineers\nuse often. Many of the practices discussed can also be applied to working with\nunstructured data such as images, video, and raw text.\nBefore we get into modeling and transforming data, let’s look at queries—what they\nare, how they work, considerations for improving query performance, and queries on\nstreaming data.\nQueries\nQueries are a fundamental part of data engineering, data science, and analysis. Before\nyou learn about the underlying patterns and technologies for transformations, you\nneed to understand what queries are, how they work on various data, and techniques\nfor improving query performance.\nThis section primarily concerns itself with queries on tabular and semistructured\ndata. As a data engineer, you’ll most frequently query and transform these data\ntypes. Before we get into more complicated topics about queries, data modeling, and\ntransformations, let’s start by answering a pretty simple question: what is a query?\n272 | Chapter 8: Queries, Modeling, and Transformation\nWhat Is a Query?\nWe often run into people who know how to write SQL but are unfamiliar with how\na query works under the hood. Some of this introductory material on queries will be\nfamiliar to experienced data engineers; feel free to skip ahead if this applies to you.\nA query  allows you to retrieve and act on data. Recall our conversation in Chapter 5\nabout CRUD. When a query retrieves data, it is issuing a request to read a pattern of\nrecords. This is the R (read) in CRUD. Y ou might issue a query that gets all records\nfrom a table foo, such as SELECT * FROM foo . Or, you might apply a predicate\n(logical condition) to filter your data by retrieving only records where the id is 1,\nusing the SQL query SELECT * FROM foo WHERE id=1 .\nMany databases allow you to create, update, and delete data. These are the CUD  in\nCRUD; your query will either create, mutate, or destroy existing records. Let’s review\nsome other common acronyms you’ll run into when working with query languages.\nData definition  language\nAt a high level, you first need to create the database objects before adding data. Y ou’ll\nuse data definition  language  (DDL) commands to perform operations on database\nobjects, such as the database itself, schemas, tables, or users; DDL defines the state of\nobjects in your database.\nData engineers use common SQL DDL expressions: CREATE , DROP , and UPDATE . For\nexample, you can create a database by using the DDL expression CREATE DATABASE\nbar. After that, you can also create new tables ( CREATE table bar_table ) or delete a\ntable (DROP table bar_table ).\nData manipulation language\nAfter using DDL to define database objects, you need to add and alter data within\nthese objects, which is the primary purpose of data manipulation language  (DML).\nSome common DML commands you’ll use as a data engineer are as follows:\nSELECT\nINSERT\nUPDATE\nDELETE\nCOPY\nMERGE\nFor example, you can INSERT  new records into a database table, UPDATE  existing ones,\nand SELECT  specific records.\nQueries | 273\nData control language\nY ou most likely want to limit access to database objects and finely control who has\naccess to what . Data control language  (DCL) allows  you to control access to the\ndatabase objects or the data by using SQL commands such as GRANT , DENY , and\nREVOKE .\nLet’s walk through a brief example using DCL commands. A new data scientist\nnamed Sarah joins your company, and she needs read-only access to a database called\ndata_science_db . Y ou give Sarah access to this database by using the following DCL\ncommand:\nGRANT SELECT ON data_science_db  TO user_name  Sarah;\nIt’s a hot job market, and Sarah has worked at the company for only a few months\nbefore getting poached by a big tech company. So long, Sarah! Being a security-\nminded data engineer, you remove Sarah’s ability to read from the database:\nREVOKE SELECT ON data_science_db  TO user_name  Sarah;\nAccess-control requests and issues are common, and understanding DCL will help\nyou resolve problems if you or a team member can’t access the data they need, as well\nas prevent access to data they don’t need.\nTransaction control language\nAs its name suggests, transaction control language  (TCL) supports commands that\ncontrol the details of transactions. With TCL, we can define commit checkpoints,\nconditions when actions will be rolled back, and more. Two common TCL com‐\nmands include COMMIT  and ROLLBACK .\nThe Life of a Query\nHow  does a query work, and what happens when a query is executed? Let’s cover the\nhigh-level basics of query execution ( Figure 8-2 ), using an example of a typical SQL\nquery executing in a database.\nFigure 8-2. The life of a SQL query in a database\nWhile running a query might seem simple—write code, run it, and get results—a lot\nis going on under the hood. When you execute a SQL query, here’s a summary of\nwhat happens:\n274 | Chapter 8: Queries, Modeling, and Transformation",7289
137-Improving Query Performance.pdf,137-Improving Query Performance,"1.The database engine compiles the SQL, parsing the code to check for proper1.\nsemantics and ensuring that the database objects referenced exist and that the\ncurrent user has the appropriate access to these objects.\n2.The SQL code is converted into bytecode. This bytecode expresses the steps2.\nthat must be executed on the database engine in an efficient, machine-readable\nformat.\n3.The database’s query optimizer analyzes the bytecode to determine how to exe‐3.\ncute the query, reordering and refactoring steps to use available resources as\nefficiently as possible.\n4.The query is executed, and results are produced.4.\nThe Query Optimizer\nQueries  can have wildly different execution times, depending on how they’re exe‐\ncuted. A query optimizer’s job is to optimize query performance and minimize costs\nby breaking the query into appropriate steps in an efficient order. The optimizer will\nassess joins, indexes, data scan size, and other factors. The query optimizer attempts\nto execute the query in the least expensive manner.\nQuery optimizers are fundamental to how your query will perform. Every database\nis different and executes queries in ways that are obviously and subtly different from\neach other. Y ou won’t directly work with a query optimizer, but understanding some\nof its functionality will help you write more performant queries. Y ou’ll need to know\nhow to analyze a query’s performance, using things like an explain plan or query\nanalysis, described in the following section.\nImproving Query Performance\nIn data engineering, you’ll inevitably encounter poorly performing queries. Knowing\nhow to identify and fix these queries is invaluable. Don’t fight your database. Learn to\nwork with its strengths and augment its weaknesses. This section shows various ways\nto improve your query performance.\nOptimize your join strategy and schema\nA single dataset (such as a table or file) is rarely useful on its own; we create value\nby combining it with other datasets. Joins  are one of the most common means of\ncombining datasets and creating new ones. We assume that you’re familiar with\nthe significant types of joins (e.g., inner, outer, left, cross) and the types of join\nrelationships (e.g., one to one, one to many, many to one, and many to many).\nQueries | 275\nJoins are critical in data engineering and are well supported and performant in many\ndatabases. Even columnar databases, which in the past had a reputation for slow join\nperformance, now generally offer excellent performance.\nA common technique for improving query performance is to  prejoin  data. If you find\nthat analytics queries are joining the same data repeatedly, it often makes sense to\njoin the data in advance and have queries read from the prejoined version of the\ndata so that you’re not repeating computationally intensive work. This may mean\nchanging the schema and relaxing normalization conditions to widen tables and\nutilize newer data structures (such as arrays or structs) for replacing frequently joined\nentity relationships. Another strategy is maintaining a more normalized schema but\nprejoining tables for the most common analytics and data science use cases. We\ncan simply create prejoined tables and train users to utilize these or join inside\nmaterialized views (see “Materialized Views, Federation, and Query Virtualization”\non page 323 ).\nNext, consider the details and complexity of your join conditions. Complex join logic\nmay consume significant computational resources. We can improve performance for\ncomplex joins in a few ways.\nMany row-oriented databases allow you to index a result computed from a row.\nFor instance, PostgreSQL allows you to create an index on a string field converted\nto lowercase; when the optimizer encounters a query where the lower()  function\nappears inside a predicate, it can apply the index. Y ou can also create a new derived\ncolumn for joining, though you will need to train users to join on this column.\nRow Explosion\nAn obscure but frustrating problem is row explosion . This occurs when we have a\nlarge number of many-to-many matches, either because of repetition in join keys or\nas a consequence of join logic. Suppose the join key in table A has the value this\nrepeated five times, and the join key in table B contains this same value repeated 10\ntimes. This leads to a cross-join of these rows: every this  row from table A paired\nwith every this  row from table B. This creates 5 × 10 = 50 rows in the output. Now\nsuppose that many other repeats are in the join key. Row explosion often generates\nenough rows to consume a massive quantity of database resources or even cause a\nquery to fail.\nIt is also essential to know how your query optimizer handles joins. Some databases\ncan reorder joins and predicates, while others cannot. A row explosion in an early\nquery stage may cause the query to fail, even though a later predicate should correctly\nremove many of the repeats in the output. Predicate reordering can significantly\nreduce the computational resources required by a query.\n276 | Chapter 8: Queries, Modeling, and Transformation\nFinally, use common table expressions (CTEs) instead of nested subqueries or tem‐\nporary tables. CTEs allow users to compose complex queries together in a readable\nfashion, helping you understand the flow of your query. The importance of readabil‐\nity for complex queries cannot be understated.\nIn many cases, CTEs will also deliver better performance than a script that creates\nintermediate tables; if you have to create intermediate tables, consider creating tem‐\nporary tables. If you’ d like to learn more about CTEs, a quick web search will yield\nplenty of helpful information.\nUse the explain plan and understand your query’s performance\nAs you learned in the preceding section, the database’s query optimizer influences\nthe execution of a query. The query optimizer’s explain plan will show you how the\nquery optimizer determined its optimum lowest-cost query, the database objects used\n(tables, indexes, cache, etc.), and various resource consumption and performance\nstatistics in each query stage. Some databases provide a visual representation of query\nstages. In contrast, others make the explain plan available via SQL with the EXPLAIN\ncommand, which displays the sequence of steps the database will take to execute the\nquery.\nIn addition to using EXPLAIN  to understand how your query will run, you should\nmonitor your query’s performance, viewing metrics on database resource consump‐\ntion. The following are some areas to monitor:\n•Usage of key resources such as disk, memory, and network.•\n•Data loading time versus processing time.•\n•Query execution time, number of records, the size of the data scanned, and the•\nquantity of data shuffled.\n•Competing queries that might cause resource contention in your database.•\n•Number of concurrent connections used versus connections available. Oversub‐•\nscribed concurrent connections can have negative effects on your users who may\nnot be able to connect to the database.\nAvoid full table scans\nAll queries scan data, but not all scans are created equal. As a rule of thumb, you\nshould query only the data you need. When you run SELECT *  with no predicates,\nyou’re scanning the entire table and retrieving every row and column. This is very\ninefficient performance-wise and expensive, especially if you’re using a pay-as-you-go\ndatabase that charges you either for bytes scanned or compute resources utilized\nwhile a query is running.\nQueries | 277\nWhenever possible, use pruning  to reduce the quantity of data scanned in a query.\nColumnar and row-oriented databases require different pruning strategies. In a\ncolumn-oriented database, you should select only the columns you need. Most\ncolumn-oriented OLAP databases also provide additional tools for optimizing your\ntables for better query performance. For instance, if you have a very large table\n(several terabytes in size or greater), Snowflake and BigQuery give you the option\nto define a cluster key on a table, which orders the table’s data in a way that allows\nqueries to more efficiently access portions of very large datasets. BigQuery also allows\nyou to partition a table into smaller segments, allowing you to query only specific\npartitions instead of the entire table. (Be aware that inappropriate clustering and key\ndistribution strategies can degrade performance.)\nIn row-oriented databases, pruning usually centers around table indexes, which you\nlearned in Chapter 6 . The general strategy is to create table indexes that will improve\nperformance for your most performance-sensitive queries while not overloading the\ntable with so many indexes such that you degrade performance.\nKnow how your database handles commits\nA database commit  is a change within a database, such as creating, updating, or delet‐\ning a record, table, or other database objects. Many databases support transactions —\ni.e., a notion of committing several operations simultaneously in a way that maintains\na consistent state. Please note that the term transaction  is somewhat overloaded; see\nChapter 5 . The purpose of a transaction is to keep a consistent state of a database\nboth while it’s active and in the event of a failure. Transactions also handle isolation\nwhen multiple concurrent events might be reading, writing, and deleting from the\nsame database objects. Without transactions, users would get potentially conflicting\ninformation when querying a database.\nY ou should be intimately familiar with how your database handles commits and\ntransactions, and determine the expected consistency of query results. Does your\ndatabase handle writes and updates in an ACID-compliant manner? Without ACID\ncompliance, your query might return unexpected results. This could result from a\ndirty read, which happens when a row is read and an uncommitted transaction has\naltered the row. Are dirty reads an expected behavior of your database? If so, how\ndo you handle this? Also, be aware that during update and delete transactions, some\ndatabases create new files to represent the new state of the database and retain the old\nfiles for failure checkpoint references. In these databases, running a large number of\nsmall commits can lead to clutter and consume significant storage space that might\nneed to be vacuumed periodically.\nLet’s briefly consider three databases to understand the impact of commits (note these\nexamples are current as of the time of this writing). First, suppose we’re looking at\na PostgreSQL RDBMS and applying ACID transactions. Each transaction consists of\n278 | Chapter 8: Queries, Modeling, and Transformation\n1See, for example, Emin Gün Sirer, “NoSQL Meets Bitcoin and Brings Down Two Exchanges: The Story of\nFlexcoin and Poloniex, ” Hacking, Distributed , April 6, 2014, https://oreil.ly/RM3QX .a package of operations that will either fail or succeed as a group. We can also run\nanalytics queries across many rows; these queries will present a consistent picture of\nthe database at a point in time.\nThe disadvantage of the PostgreSQL approach is that it requires row locking  (blocking\nreads and writes to certain rows), which can degrade performance in various ways.\nPostgreSQL is not optimized for large scans or the massive amounts of data appropri‐\nate for large-scale analytics applications.\nNext, consider Google BigQuery. It utilizes a point-in-time full table commit model.\nWhen a read query is issued, BigQuery will read from the latest committed snapshot\nof the table. Whether the query runs for one second or two hours, it will read only\nfrom that snapshot and will not see any subsequent changes. BigQuery does not lock\nthe table while I read from it. Instead, subsequent write operations will create new\ncommits and new snapshots while the query continues to run on the snapshot where\nit started.\nTo prevent the inconsistent state, BigQuery allows only one write operation at a time.\nIn this sense, BigQuery provides no write concurrency whatsoever. (In the sense that\nit can write massive amounts of data in parallel inside a single write query , it is highly\nconcurrent.) If more than one client attempts to write simultaneously, write queries\nare queued in order of arrival. BigQuery’s commit model is similar to the commit\nmodels used by Snowflake, Spark, and others.\nLast, let’s consider MongoDB. We refer to MongoDB as a variable-consistency data‐\nbase . Engineers have various configurable consistency options, both for the database\nand at the level of individual queries. MongoDB is celebrated for its extraordinary\nscalability and write concurrency but is somewhat notorious for issues that arise\nwhen engineers abuse it.1\nFor instance, in certain modes, MongoDB supports ultra-high write performance.\nHowever, this comes at a cost: the database will unceremoniously and silently discard\nwrites if it gets overwhelmed with traffic. This is perfectly suitable for applications\nthat can stand to lose some data—for example, IoT applications where we simply\nwant many measurements but don’t care about capturing all measurements. It is not a\ngreat fit for applications that need to capture exact data and statistics.\nNone of this is to say these are bad databases. They’re all fantastic databases when\nthey are chosen for appropriate applications and configured correctly. The same goes\nfor virtually any database technology.\nQueries | 279\n2Some Redshift configurations  rely on object storage instead.Companies don’t hire engineers simply to hack on code in isolation. To be worthy\nof their title, engineers should develop a deep understanding of the problems they’re\ntasked with solving and the technology tools. This applies to commit and consistency\nmodels and every other aspect of technology performance. Appropriate technology\nchoices and configuration can ultimately differentiate extraordinary success and mas‐\nsive failure. Refer to Chapter 6  for a deeper discussion of consistency.\nVacuum dead records\nAs we just discussed, transactions incur the overhead of creating new records during\ncertain operations, such as updates, deletes, and index operations, while retaining the\nold records as pointers to the last state of the database. As these old records accumu‐\nlate in the database filesystem, they eventually no longer need to be referenced. Y ou\nshould remove these dead records in a process called vacuuming .\nY ou can vacuum a single table, multiple tables, or all tables in a database. No mat‐\nter how you choose to vacuum, deleting dead database records is important for\na few reasons. First, it frees up space for new records, leading to less table bloat\nand faster queries. Second, new and relevant records mean query plans are more\naccurate; outdated records can lead the query optimizer to generate suboptimal and\ninaccurate plans. Finally, vacuuming cleans up poor indexes, allowing for better index\nperformance.\nVacuum operations are handled differently depending on the type of database. For\nexample, in databases backed by object storage (BigQuery, Snowflake, Databricks),\nthe only downside of old data retention is that it uses storage space, potentially\ncosting money depending on the storage pricing model for the database. In Snow‐\nflake, users cannot directly vacuum. Instead, they control a “time-travel” interval that\ndetermines how long table snapshots are retained before they are auto vacuumed.\nBigQuery utilizes a fixed seven-day history window. Databricks generally retains data\nindefinitely until it is manually vacuumed; vacuuming is important to control direct\nS3 storage costs.\nAmazon Redshift handles its cluster disks in many configurations,2 and vacuuming\ncan impact performance and available storage. VACUUM  runs automatically behind the\nscenes, but users may sometimes want to run it manually for tuning purposes.\nVacuuming becomes even more critical for relational databases such as PostgreSQL\nand MySQL. Large numbers of transactional operations can cause a rapid accumula‐\ntion of dead records, and engineers working in these systems need to familiarize\nthemselves with the details and impact of vacuuming.\n280 | Chapter 8: Queries, Modeling, and Transformation",16374
138-Queries on Streaming Data.pdf,138-Queries on Streaming Data,"3The authors are aware of an incident involving a new analyst at a large grocery store chain running SELECT *\non a production database and bringing down a critical inventory database for three days.Leverage cached query results\nLet’s  say you have an intensive query that you often run on a database that charges\nyou for the amount of data you query. Each time a query is run, this costs you money.\nInstead of rerunning the same query on the database repeatedly and incurring mas‐\nsive charges, wouldn’t it be nice if the results of the query were stored and available\nfor instant retrieval? Thankfully, many cloud OLAP databases cache query results.\nWhen a query is initially run, it will retrieve data from various sources, filter and join\nit, and output a result. This initial query—a cold query—is similar to the notion of\ncold data we explored in Chapter 6 . For argument’s sake, let’s say this query took 40\nseconds to run. Assuming your database caches query results, rerunning the same\nquery might return results in 1 second or less. The results were cached, and the query\ndidn’t need to run cold. Whenever possible, leverage query cache results to reduce\npressure on your database and provide a better user experience for frequently run\nqueries. Note also that materialized views  provide another form of query caching (see\n“Materialized Views, Federation, and Query Virtualization” on page 323 ).\nQueries on Streaming Data\nStreaming data is constantly in flight. As you might imagine, querying streaming\ndata is different from batch data. To fully take advantage of a data stream, we must\nadapt query patterns that reflect its real-time nature. For example, systems such as\nKafka and Pulsar make it easier to query streaming data sources. Let’s look at some\ncommon ways to do this.\nBasic query patterns on streams\nRecall continuous CDC, discussed in Chapter 7 . CDC, in this form, essentially sets up\nan analytics database as a fast follower to a production database. One of the longest-\nstanding streaming query patterns simply entails querying the analytics database,\nretrieving statistical results and aggregations with a slight lag behind the production\ndatabase.\nThe fast-follower approach.    How  is this a streaming query pattern? Couldn’t we\naccomplish the same thing simply by running our queries on the production data‐\nbase? In principle, yes; in practice, no. Production databases generally aren’t equipped\nto handle production workloads and simultaneously run large analytics scans across\nsignificant quantities of data. Running such queries can slow the production applica‐\ntion or even cause it to crash.3 The basic CDC query pattern allows us to serve\nreal-time analytics with a minimal impact on the production system.\nQueries | 281\nThe fast-follower pattern can utilize a conventional transactional database as the\nfollower, but there are significant advantages to using a proper OLAP-oriented system\n(Figure 8-3 ). Both Druid and BigQuery combine a streaming buffer with long-term\ncolumnar storage in a setup somewhat similar to the Lambda architecture (see Chap‐\nter 3 ). This works extremely well for computing trailing statistics on vast historical\ndata with near real-time updates.\nFigure 8-3. CDC with a fast-follower analytics database\nThe fast-follower CDC approach has critical limitations. It doesn’t fundamentally\nrethink batch query patterns. Y ou’re still running SELECT  queries against the current\ntable state, and missing the opportunity to dynamically trigger events off changes in\nthe stream.\nThe Kappa architecture.    Next, recall the Kappa architecture we discussed in Chapter 3 .\nThe principal idea of this architecture is to handle all data like events and store these\nevents as a stream rather than a table ( Figure 8-4 ). When production application\ndatabases are the source, Kappa architecture stores events from CDC. Event streams\ncan also flow directly from an application backend, from a swarm of IoT devices,\nor any system that generates events and can push them over a network. Instead of\nsimply treating a streaming storage system as a buffer, Kappa architecture retains\nevents in storage during a more extended retention period, and data can be directly\nqueried from this storage. The retention period can be pretty long (months or years).\nNote that this is much longer than the retention period used in purely real-time\noriented systems, usually a week at most.\nFigure 8-4. The Kappa architecture is built around streaming storage and ingest systems\nThe “big idea” in Kappa architecture is to treat streaming storage as a real-time trans‐\nport layer and a database for retrieving and querying historical data. This happens\n282 | Chapter 8: Queries, Modeling, and Transformation\neither through the direct query capabilities of the streaming storage system or with\nthe help of external tools. For example, Kafka KSQL supports aggregation, statistical\ncalculations, and even sessionization. If query requirements are more complex or\ndata needs to be combined with other data sources, an external tool such as Spark\nreads a time range of data from Kafka and computes the query results. The streaming\nstorage system can also feed other applications or a stream processor such as Flink or\nBeam.\nWindows, triggers, emitted statistics, and late-arriving data\nOne fundamental limitation of traditional batch queries is that this paradigm gener‐\nally treats the query engine as an external observer. An actor external to the data\ncauses the query to run—perhaps an hourly cron job or a product manager opening a\ndashboard.\nMost widely used streaming systems, on the other hand, support the notion of com‐\nputations triggered directly from the data itself. They might emit mean and median\nstatistics every time a certain number of records are collected in the buffer or output\na summary when a user session closes.\nWindows  are an essential feature in streaming queries and processing. Windows are\nsmall batches that are processed based on dynamic triggers. Windows are generated\ndynamically over time in some ways. Let’s look at some common types of windows:\nsession, fixed-time, and sliding. We’ll also look at watermarks.\nSession window.    A session window  groups events that occur close together, and filters\nout periods of inactivity when no events occur. We might say that a user session\nis any time interval with no inactivity gap of five minutes or more. Our batch\nsystem collects data by a user ID key, orders events, determines the gaps and session\nboundaries, and calculates statistics for each session. Data engineers often sessionize\ndata retrospectively by applying time conditions to user activity on web and desktop\napps.\nIn a streaming session, this process can happen dynamically. Note that session win‐\ndows are per key; in the preceding example, each user gets their own set of windows.\nThe system accumulates data per user. If a five-minute gap with no activity occurs,\nthe system closes the window, sends its calculations, and flushes the data. If new\nevents arrive for the use, the system starts a new session window.\nSession windows may also make a provision for late-arriving data. Allowing data to\narrive up to five minutes late to account for network conditions and system latency,\nthe system will open the window if a late-arriving event indicates activity less than\nfive minutes after the last event. We will have more to say about late-arriving data\nthroughout this chapter. Figure 8-5  shows three session windows, each separated by\nfive minutes of inactivity.\nQueries | 283\nFigure 8-5. Session window with a five-minute  timeout for inactivity\nMaking  sessionization dynamic and near real-time fundamentally changes its utility.\nWith retrospective sessionization, we could automate specific actions a day or an\nhour after a user session closed (e.g., a follow-up email with a coupon for a product\nviewed by the user). With dynamic sessionization, the user could get an alert in a\nmobile app that is immediately useful based on their activity in the last 15 minutes.\nFixed-time windows.    A fixed-time  (aka tumbling ) window features fixed time periods\nthat run on a fixed schedule and processes all data since the previous window is\nclosed. For example, we might close a window every 20 seconds and process all data\narriving from the previous window to give a mean and median statistic ( Figure 8-6 ).\nStatistics would be emitted as soon as they could be calculated after the window\nclosed.\nFigure 8-6. Tumbling/fixed  window\nThis is similar to traditional batch ETL processing, where we might run a data update\njob every day or every hour. The streaming system allows us to generate windows\nmore frequently and deliver results with lower latency. As we’ll repeatedly emphasize,\nbatch is a special case of streaming.\nSliding windows.    Events in a sliding window are bucketed into windows of fixed time\nlength, where separate windows might overlap. For example, we could generate a new\n60-second window every 30 seconds ( Figure 8-7 ). Just as we did before, we can emit\nmean and median statistics.\n284 | Chapter 8: Queries, Modeling, and Transformation\nFigure 8-7. Sliding windows\nThe sliding can vary. For example, we might think of the window as truly sliding\ncontinuously but emitting statistics only when certain conditions (triggers) are met.\nSuppose we used a 30-second continuously sliding window but calculated a statistic\nonly when a user clicked a particular banner. This would lead to an extremely high\nrate of output when many users click the banner, and no calculations during a lull.\nWatermarks.    We’ve  covered various types of windows and their uses. As discussed in\nChapter 7 , data is sometimes ingested out of the order from which it originated. A\nwatermark  (Figure 8-8 ) is a threshold used by a window to determine whether data in\na window is within the established time interval or whether it’s considered late. If data\narrives that is new to the window but older than the timestamp of the watermark, it is\nconsidered to be late-arriving data.\nFigure 8-8. Watermark acting as a threshold for late-arriving data\nQueries | 285\nCombining streams with other data\nAs we’ve mentioned before, we often derive value from data by combining it with\nother data. Streaming data is no different. For instance, multiple streams can be\ncombined, or a stream can be combined with batch historical data.\nConventional table joins.    Some  tables may be fed by streams ( Figure 8-9 ). The most\nbasic approach to this problem is simply joining these two tables in a database. A\nstream can feed one or both of these tables.\nFigure 8-9. Joining two tables fed by streams\nEnrichment.    Enrichment  means  that we join a stream to other data ( Figure 8-10 ).\nTypically, this is done to provide enhanced data into another stream. For example,\nsuppose that an online retailer receives an event stream from a partner business\ncontaining product and user IDs. The retailer wishes to enhance these events with\nproduct details and demographic information on the users. The retailer feeds these\nevents to a serverless function that looks up the product and user in an in-memory\ndatabase (say, a cache), adds the required information to the event, and outputs the\nenhanced events to another stream.\nFigure 8-10. In this example, a stream is enriched with data residing in object storage,\nresulting in a new enriched dataset\nIn practice, the enrichment source could originate almost anywhere—a table in a\ncloud data warehouse or RDBMS, or a file in object storage. It’s simply a question of\nreading from the source and storing the requisite enrichment data in an appropriate\nplace for retrieval by the stream.\nStream-to-stream joining.    Increasingly, streaming systems support direct stream-to-\nstream joining. Suppose that an online retailer wishes to join its web event data with\nstreaming data from an ad platform. The company can feed both streams into Spark,\n286 | Chapter 8: Queries, Modeling, and Transformation",12166
139-Normalization.pdf,139-Normalization,"4Figure 8-11  and the example it depicts are significantly based on “Introducing Stream—Stream Joins in\nApache Spark 2.3”  by Tathagata Das and Joseph Torres (Databricks Engineering Blog , March 13, 2018).but a variety of complications arise. For instance, the streams may have significantly\ndifferent latencies for arrival at the point where the join is handled in the streaming\nsystem. The ad platform may provide its data with a five-minute delay. In addition,\ncertain events may be significantly delayed—for example, a session close event for a\nuser, or an event that happens on the phone offline and shows up in the stream only\nafter the user is back in mobile network range.\nAs such, typical streaming join architectures rely on streaming buffers. The buffer\nretention interval is configurable; a longer retention interval requires more storage\nand other resources. Events get joined with data in the buffer and are eventually\nevicted after the retention interval has passed ( Figure 8-11 ).4\nFigure 8-11. An architecture to join streams buffers  each stream and joins events if\nrelated events are found during the buffer  retention interval\nNow that we’ve covered how queries work for batch and streaming data, let’s discuss\nmaking your data useful by modeling it.\nData Modeling\nData modeling is something that we see overlooked disturbingly often. We often see\ndata teams jump into building data systems without a game plan to organize their\ndata in a way that’s useful for the business. This is a mistake. Well-constructed data\narchitectures must reflect the goals and business logic of the organization that relies\non this data. Data modeling involves deliberately choosing a coherent structure for\ndata and is a critical step to make data useful for the business.\nData modeling has been a practice for decades in one form or another. For example,\nvarious types of normalization techniques (discussed in “Normalization”  on page\n290) have been used to model data since the early days of RDBMSs; data warehousing\nmodeling techniques have been around since at least the early 1990s and arguably\nlonger. As pendulums in technology often go, data modeling became somewhat\nunfashionable in the early to mid-2010s. The rise of data lake 1.0, NoSQL, and big\nData Modeling | 287\ndata systems allowed engineers to bypass traditional data modeling, sometimes for\nlegitimate performance gains. Other times, the lack of rigorous data modeling created\ndata swamps, along with lots of redundant, mismatched, or simply wrong data.\nNowadays, the pendulum seems to be swinging back toward data modeling. The\ngrowing popularity of data management (in particular, data governance and data\nquality) is pushing the need for coherent business logic. The meteoric rise of data’s\nprominence in companies creates a growing recognition that modeling is critical for\nrealizing value at the higher levels of the Data Science Hierarchy of Needs pyramid.\nThat said, we believe that new paradigms are required to truly embrace the needs of\nstreaming data and ML. In this section, we survey current mainstream data modeling\ntechniques and briefly muse on the future of data modeling.\nWhat Is a Data Model?\nA data model  represents  the way data relates to the real world. It reflects how the\ndata must be structured and standardized to best reflect your organization’s processes,\ndefinitions, workflows, and logic. A good data model captures how communication\nand work naturally flow within your organization. In contrast, a poor data model (or\nnonexistent one) is haphazard, confusing, and incoherent.\nSome data professionals view data modeling as tedious and reserved for “big enterpri‐\nses. ” Like most good hygiene practices—such as flossing your teeth and getting a\ngood night’s sleep—data modeling is acknowledged as a good thing to do but is often\nignored in practice. Ideally, every organization should model its data if only to ensure\nthat business logic and rules are translated at the data layer.\nWhen  modeling data, it’s critical to focus on translating the model to business\noutcomes. A good data model should correlate with impactful business decisions.\nFor example, a customer  might mean different things to different departments in\na company. Is someone who’s bought from you over the last 30 days a customer?\nWhat if they haven’t bought from you in the previous six months or a year? Carefully\ndefining and modeling this customer data can have a massive impact on downstream\nreports on customer behavior or the creation of customer churn models whereby the\ntime since the last purchase is a critical variable.\nA good data model contains consistent definitions. In practice,\ndefinitions are often messy throughout a company. Can you think\nof concepts or terms in your company that might mean different\nthings to different people?\n288 | Chapter 8: Queries, Modeling, and Transformation\nOur discussion focuses mainly on batch data modeling since that’s where most\ndata modeling techniques arose. We will also look at some approaches to modeling\nstreaming data and general considerations for modeling.\nConceptual, Logical, and Physical Data Models\nWhen  modeling data, the idea is to move from abstract modeling concepts to con‐\ncrete implementation. Along this continuum ( Figure 8-12 ), three main data models\nare conceptual, logical, and physical. These models form the basis for the various\nmodeling techniques we describe in this chapter:\nConceptual\nContains  business logic and rules and describes the system’s data, such as sche‐\nmas, tables, and fields (names and types). When creating a conceptual model,\nit’s often helpful to visualize it in an entity-relationship (ER) diagram, which\nis a standard tool for visualizing the relationships among various entities in\nyour data (orders, customers, products, etc.). For example, an ER diagram might\nencode the connections among customer ID, customer name, customer address,\nand customer orders. Visualizing entity relationships is highly recommended for\ndesigning a coherent conceptual data model.\nLogical\nDetails  how the conceptual model will be implemented in practice by adding\nsignificantly more detail. For example, we would add information on the types of\ncustomer ID, customer names, and custom addresses. In addition, we would map\nout primary and foreign keys.\nPhysical\nDefines  how the logical model will be implemented in a database system. We\nwould add specific databases, schemas, and tables to our logical model, including\nconfiguration details.\nFigure 8-12. The continuum of data models: conceptual, logical, and physical\nSuccessful  data modeling involves business stakeholders at the inception of the pro‐\ncess. Engineers need to obtain definitions and business goals for the data. Modeling\ndata should be a full-contact sport whose goal is to provide the business with quality\ndata for actionable insights and intelligent automation. This is a practice that every‐\none must continuously participate in.\nData Modeling | 289\n5For more details on the DRY principle, see The Pragmatic Programmer  by David Thomas and Andrew Hunt\n(Addison-Wesley Professional, 2019).Another important consideration for data modeling is the grain  of the data, which is\nthe resolution at which data is stored and queried. The grain is typically at the level\nof a primary key in a table, such as customer ID, order ID, and product ID; it’s often\naccompanied by a date or timestamp for increased fidelity.\nFor example, suppose that a company has just begun to deploy BI reporting. The\ncompany is small enough that the same person is filling the role of data engineer\nand analyst. A request comes in for a report that summarizes daily customer orders.\nSpecifically, the report should list all customers who ordered, the number of orders\nthey placed that day, and the total amount they spent.\nThis report is inherently coarse-grained. It contains no details on spending per order\nor the items in each order. It is tempting for the data engineer/analyst to ingest data\nfrom the production orders database and boil it down to a reporting table with only\nthe basic aggregated data required for the report. However, this would entail starting\nover when a request comes in for a report with finer-grained data aggregation.\nSince the data engineer is actually quite experienced, they elect to create tables with\ndetailed data on customer orders, including each order, item, item cost, item IDs,\netc. Essentially, their tables contain all details on customer orders. The data’s grain\nis at the customer-order level. This customer-order data can be analyzed as is, or\naggregated for summary statistics on customer order activity.\nIn general, you should strive to model your data at the lowest level of grain possible.\nFrom here, it’s easy to aggregate this highly granular dataset. The reverse isn’t true,\nand it’s generally impossible to restore details that have been aggregated away.\nNormalization\nNormalization  is a database data modeling practice that enforces strict control over\nthe relationships of tables and columns within a database. The goal of normalization\nis to remove the redundancy of data within a database and ensure referential integrity.\nBasically, it’s don’t repeat yourself  (DRY) applied to data in a database.5\nNormalization is typically applied to relational databases containing tables with rows\nand columns (we use the terms column  and field interchangeably in this section). It\nwas first introduced by relational database pioneer Edgar Codd in the early 1970s.\n290 | Chapter 8: Queries, Modeling, and Transformation\n6E. F. Codd, “Further Normalization of the Data Base Relational Model, ” IBM Research Laboratory (1971),\nhttps://oreil.ly/Muajm .Codd outlined four main objectives of normalization:6\n•To free the collection of relations from undesirable insertion, update, and dele‐•\ntion dependencies\n•To reduce the need for restructuring the collection of relations, as new types of•\ndata are introduced, and thus increase the lifespan of application programs\n•To make the relational model more informative to users•\n•To make the collection of relations neutral to the query statistics, where these•\nstatistics are liable to change as time goes by\nCodd introduced the idea of normal forms . The normal forms are sequential, with\neach form incorporating the conditions of prior forms. We describe Codd’s first three\nnormal forms here:\nDenormalized\nNo normalization. Nested and redundant data is allowed.\nFirst normal form (1NF)\nEach  column is unique and has a single value. The table has a unique primary\nkey.\nSecond normal form (2NF)\nThe requirements of 1NF, plus partial dependencies are removed.\nThird  normal form (3NF)\nThe requirements of 2NF, plus each table contains only relevant fields related to\nits primary key and has no transitive dependencies.\nIt’s worth spending a moment to unpack a couple of terms we just threw at you. A\nunique primary key  is a single field or set of multiple fields that uniquely determines\nrows in the table. Each key value occurs at most once; otherwise, a value would map\nto multiple rows in the table. Thus, every other value in a row is dependent on (can\nbe determined from) the key. A partial dependency  occurs when a subset of fields in\na composite key can be used to determine a nonkey column of the table. A transitive\ndependency  occurs when a nonkey field depends on another nonkey field.\nLet’s look at stages of normalization—from denormalized to 3NF—using an ecom‐\nmerce example of customer orders ( Table 8-1 ). We’ll provide concrete explanations of\neach of the concepts introduced in the previous paragraph.\nData Modeling | 291\nTable 8-1. OrderDetail\nOrderID OrderItems CustomerID CustomerName OrderDate\n100 [{\n  ""sku"": 1,\n  ""price"": 50,\n  ""quantity"" : 1,\n  ""name:"": ""Thingamajig""\n}, {\n  ""sku"": 2,\n  ""price"": 25,\n  ""quantity"" : 2,\n  ""name:"": ""Whatchamacallit""\n}]5 Joe Reis 2022-03-01\nFirst, this denormalized OrderDetail  table contains five fields. The primary key is\nOrderID . Notice that the OrderItems  field contains a nested object with two SKUs\nalong with their price, quantity, and name.\nTo convert this data to 1NF, let’s move OrderItems  into four fields ( Table 8-2 ). Now\nwe have an OrderDetail  table in which fields do not contain repeats or nested data.\nTable 8-2. OrderDetail  without repeats or nested data\nOrderID SkuPriceQuantity ProductName CustomerID CustomerName OrderDate\n100 1501 Thingamajig 5 Joe Reis 2022-03-01\n100 2252 Whatchamacallit 5 Joe Reis 2022-03-01\nThe problem is that now we don’t have a unique primary key. That is, 100 occurs in\nthe OrderID  column in two different rows. To get a better grasp of the situation, let’s\nlook at a larger sample from our table ( Table 8-3 ).\nTable 8-3. OrderDetail  with a larger sample\nOrderID SkuPriceQuantity ProductName CustomerID CustomerName OrderDate\n100 1501 Thingamajig 5 Joe Reis 2022-03-01\n100 2252 Whatchamacallit 5 Joe Reis 2022-03-01\n101 3751 Whozeewhatzit 7 Matt Housley 2022-03-01\n102 1501 Thingamajig 7 Matt Housley 2022-03-01\nTo create a unique primary (composite) key, let’s number the lines in each order by\nadding a column called LineItemNumber  (Table 8-4 ).\n292 | Chapter 8: Queries, Modeling, and Transformation\nTable 8-4. OrderDetail  with LineItemNumber  column\nOrder\nIDLineItem\nNumberSkuPriceQuantity Product\nNameCustomer\nIDCustomer\nNameOrderDate\n100 1 1501 Thingama\njig5 Joe Reis 2022-03-01\n100 2 2252 Whatchama\ncallit5 Joe Reis 2022-03-01\n101 1 3751 Whozee\nwhatzit7 Matt \nHousley2022-03-01\n102 1 1501 Thingama\njig7 Matt \nHousley2022-03-01\nThe composite key ( OrderID , LineItemNumber ) is now a unique primary key.\nTo reach 2NF, we need to ensure that no partial dependencies exist. A partial depend‐\nency  is a nonkey column that is fully determined by a subset of the columns in\nthe unique primary (composite) key; partial dependencies can occur only when the\nprimary key is composite. In our case, the last three columns are determined by\norder number. To fix this problem, let’s split OrderDetail  into two tables: Orders  and\nOrderLineItem  (Tables 8-5 and 8-6).\nTable 8-5. Orders\nOrderID CustomerID CustomerName OrderDate\n100 5 Joe Reis 2022-03-01\n101 7 Matt Housley 2022-03-01\n102 7 Matt Housley 2022-03-01\nTable 8-6. OrderLineItem\nOrderID LineItemNumber SkuPriceQuantity ProductName\n100 1 1501 Thingamajig\n100 2 2252 Whatchamacallit\n101 1 3751 Whozeewhatzit\n102 1 1501 Thingamajig\nThe composite key ( OrderID , LineItemNumber ) is a unique primary key for Order\nLineItem , while OrderID  is a primary key for Orders .\nNotice that Sku determines ProductName  in OrderLineItem . That is, Sku depends on\nthe composite key, and ProductName  depends on Sku. This is a transitive dependency.\nLet’s break OrderLineItem  into OrderLineItem  and Skus  (Tables 8-7 and 8-8).\nData Modeling | 293",15043
140-Techniques for Modeling Batch Analytical Data.pdf,140-Techniques for Modeling Batch Analytical Data,"Table 8-7. OrderLineItem\nOrderID LineItemNumber SkuPriceQuantity\n100 1 1501\n100 2 2252\n101 1 3751\n102 1 1501\nTable 8-8. Skus\nSkuProductName\n1Thingamajig\n2Whatchamacallit\n3Whozeewhatzit\nNow, both OrderLineItem  and Skus  are in 3NF. Notice that Orders  does not satisfy\n3NF. What transitive dependencies are present? How would you fix this?\nAdditional normal forms exist (up to 6NF in the Boyce-Codd system), but these are\nmuch less common than the first three. A database is usually considered normalized\nif it’s in third normal form, and that’s the convention we use in this book.\nThe degree of normalization that you should apply to your data depends on your\nuse case. No one-size-fits-all solution exists, especially in databases where some\ndenormalization presents performance advantages. Although denormalization may\nseem like an antipattern, it’s common in many OLAP systems that store semistruc‐\ntured data. Study normalization conventions and database best practices to choose an\nappropriate strategy.\nTechniques for Modeling Batch Analytical Data\nWhen  describing data modeling for data lakes or data warehouses, you should\nassume that the raw data takes many forms (e.g., structured and semistructured),\nbut the output is a structured data model of rows and columns. However, several\napproaches to data modeling can be used in these environments. The big approaches\nyou’ll likely encounter are Kimball, Inmon, and Data Vault.\nIn practice, some of these techniques can be combined. For example, we see some\ndata teams start with Data Vault and then add a Kimball star schema alongside it.\nWe’ll also look at wide and denormalized data models and other batch data-modeling\ntechniques you should have in your arsenal. As we discuss each of these techniques,\nwe will use the example of modeling transactions occurring in an ecommerce order\nsystem.\n294 | Chapter 8: Queries, Modeling, and Transformation\n7H. W . Inmon, Building the Data Warehouse  (Hoboken: Wiley, 2005).\nOur coverage of the first three approaches—Inmon, Kimball, and\nData Vault—is cursory and hardly does justice to their respective\ncomplexity and nuance. At the end of each section, we list the\ncanonical books from their creators. For a data engineer, these\nbooks are must-reads, and we highly encourage you to read them,\nif only to understand how and why data modeling is central to\nbatch analytical data.\nInmon\nThe father of the data warehouse, Bill Inmon, created his approach to data modeling\nin 1989. Before the data warehouse, the analysis would often occur directly on the\nsource system itself, with the obvious consequence of bogging down production\ntransactional databases with long-running queries. The goal of the data warehouse\nwas to separate the source system from the analytical system.\nInmon defines a data warehouse the following way:7\nA data warehouse is a subject-oriented, integrated, nonvolatile, and time-variant col‐\nlection of data in support of management’s decisions. The data warehouse contains\ngranular corporate data. Data in the data warehouse is able to be used for many\ndifferent purposes, including sitting and waiting for future requirements which are\nunknown today.\nThe four critical parts of a data warehouse can be described as follows:\nSubject-oriented\nThe data warehouse focuses on a specific subject area, such as sales or marketing.\nIntegrated\nData from disparate sources is consolidated and normalized.\nNonvolatile\nData remains unchanged after data is stored in a data warehouse.\nTime-variant\nVarying time ranges can be queried.\nLet’s look at each of these parts to understand its influence on an Inmon data\nmodel. First, the logical model must focus on a specific area. For instance, if the\nsubject orientation  is “sales, ” then the logical model contains all details related to sales\n—business keys, relationships, attributes, etc. Next, these details are  integrated  into a\nconsolidated and highly normalized data model. Finally, the data is stored unchanged\nin a nonvolatile  and time-variant  way, meaning you can (theoretically) query the\noriginal data for as long as storage history allows. The Inmon data warehouse must\nData Modeling | 295\n8Inmon, Building the Data Warehouse .strictly adhere to all four of these critical parts in support of management’s decisions.\nThis is a subtle point, but it positions the data warehouse for analytics, not OLTP .\nHere is another key characteristic of Inmon’s data warehouse:8\nThe second salient characteristic of the data warehouse is that it is integrated. Of all\nthe aspects of a data warehouse, integration is the most important. Data is fed from\nmultiple, disparate sources into the data warehouse. As the data is fed, it is converted,\nreformatted, resequenced, summarized, etc. The result is that data—once it resides in\nthe data warehouse—has a single physical corporate image.\nWith Inmon’s data warehouse, data is integrated from across the organization in a\ngranular, highly normalized ER model, with a relentless emphasis on ETL. Because\nof the subject-oriented nature of the data warehouse, the Inmon data warehouse\nconsists of key source databases and information systems used in an organization.\nData from key business source systems is ingested and integrated into a highly\nnormalized (3NF) data warehouse that often closely resembles the normalization\nstructure of the source system itself; data is brought in incrementally, starting with\nthe highest-priority business areas. The strict normalization requirement ensures as\nlittle data duplication as possible, which leads to fewer downstream analytical errors\nbecause data won’t diverge or suffer from redundancies. The data warehouse repre‐\nsents a “single source of truth, ” which supports the overall business’s information\nrequirements. The data is presented for downstream reports and analysis via business\nand department-specific data marts, which may also be denormalized.\nLet’s look at how an Inmon data warehouse is used for ecommerce ( Figure 8-13 ).\nThe business source systems are orders, inventory, and marketing. The data from\nthese source systems are ETLed to the data warehouse and stored in 3NF. Ideally,\nthe data warehouse holistically encompasses the business’s information. To serve data\nfor department-specific information requests, ETL processes take data from the data\nwarehouse, transform the data, and place it in downstream data marts to be viewed in\nreports.\nA popular option for modeling data in a data mart is a star schema (discussed\nin the following section on Kimball), though any data model that provides easily\naccessible information is also suitable. In the preceding example, sales, marketing,\nand purchasing have their own star schema, fed upstream from the granular data in\nthe data warehouse. This allows each department to have its own data structure that’s\nunique and optimized to its specific needs.\nInmon continues to innovate in the data warehouse space, currently focusing on\ntextual ETL in the data warehouse. He’s also a prolific writer and thinker, writing over\n296 | Chapter 8: Queries, Modeling, and Transformation\n9Inmon, Building the Data Warehouse .\n10Although dimensions and facts are often associated with Kimball, they were first used at General Mills and\nDartmouth University in the 1960s and had early adoption at Nielsen and IRI, among other companies.60 books and countless articles. For further reading about Inmon’s data warehouse,\nplease refer to his books listed in “ Additional Resources” on page 335 .\nFigure 8-13. An ecommerce data warehouse\nKimball\nIf there are spectrums to data modeling, Kimball is very much on the opposite end of\nInmon. Created by Ralph Kimball in the early 1990s, this approach to data modeling\nfocuses less on normalization, and in some cases accepting denormalization. As\nInmon says about the difference between the data warehouse and data mart, “ A data\nmart is never a substitute for a data warehouse. ”9\nWhereas Inmon integrates data from across the business in the data warehouse, and\nserves department-specific analytics via data marts, the Kimball model is bottom-up,\nencouraging you to model and serve department or business analytics in the data\nwarehouse itself (Inmon argues this approach skews the definition of a data ware‐\nhouse). The Kimball approach effectively makes the data mart the data warehouse\nitself. This may enable faster iteration and modeling than Inmon, with the trade-off\nof potential looser data integration, data redundancy, and duplication.\nIn Kimball’s approach, data is modeled with two general types of tables: facts and\ndimensions. Y ou can think of a fact table  as a table of numbers, and dimension tables\nas qualitative data referencing a fact. Dimension tables surround a single fact table in\na relationship called a star schema  (Figure 8-14 ).10 Let’s look at facts, dimensions, and\nstar schemas.\nData Modeling | 297\nFigure 8-14. A Kimball star schema, with facts and dimensions\nFact tables.    The first type of table in a star schema is the fact table, which contains\nfactual , quantitative, and event-related data. The data in a fact table is immutable\nbecause facts relate to events. Therefore, fact tables don’t change and are append-only.\nFact tables are typically narrow and long, meaning they have not a lot of columns but\na lot of rows that represent events. Fact tables should be at the lowest grain possible.\nQueries against a star schema start with the fact table. Each row of a fact table should\nrepresent the grain of the data. Avoid aggregating or deriving data within a fact table.\nIf you need to perform aggregations or derivations, do so in a downstream query,\ndata mart table, or view. Finally, fact tables don’t reference other fact tables; they\nreference only dimensions.\nLet’s look at an example of an elementary fact table ( Table 8-9 ). A common question\nin your company might be, “Show me gross sales, by each customer order, by date. ”\nAgain, facts should be at the lowest grain possible—in this case, the orderID of\nthe sale, customer, date, and gross sale amount. Notice that the data types in the\nfact table are all numbers (integers and floats); there are no strings. Also, in this\nexample, CustomerKey 7  has two orders on the same day, reflecting the grain of\nthe table. Instead, the fact table has keys that reference dimension tables containing\ntheir respective attributes, such as the customer and date information. The gross sales\namount represents the total sale for the sales event .\nTable 8-9. A fact table\nOrderID CustomerKey DateKey GrossSalesAmt\n100 5 20220301 100.00\n101 7 20220301 75.00\n102 7 20220301 50.00\nDimension tables.    The second primary type of table in a Kimball data model is called\na dimension . Dimension tables provide the reference data, attributes, and relational\ncontext for the events stored in fact tables. Dimension tables are smaller than fact\n298 | Chapter 8: Queries, Modeling, and Transformation\ntables and take an opposite shape, typically wide and short. When joined to a fact\ntable, dimensions can describe the events’ what, where, and when. Dimensions are\ndenormalized, with the possibility of duplicate data. This is OK in the Kimball data\nmodel. Let’s look at the two dimensions referenced in the earlier fact table example.\nIn a Kimball data model, dates are typically stored in a date dimension, allowing you\nto reference the date key ( DateKey ) between the fact and date dimension table. With\nthe date dimension table, you can easily answer questions like, “What are my total\nsales in the first quarter of 2022?” or “How many more customers shop on Tuesday\nthan Wednesday?” Notice we have five fields in addition to the date key ( Table 8-10 ).\nThe beauty of a date dimension is that you can add as many new fields as makes sense\nto analyze your data.\nTable 8-10. A date dimension table\nDateKey Date-ISO YearQuarter MonthDay-of-week\n20220301 2022-03-01 20221 3Tuesday\n20220302 2022-03-02 20221 3Wednesday\n20220303 2022-03-03 20221 3Thursday\nTable 8-11  also references another dimension—the customer dimension—by the\nCustomerKey  field. The customer dimension contains several fields that describe the\ncustomer: first and last name, zip code, and a couple of peculiar-looking date fields.\nLet’s look at these date fields, as they illustrate another concept in the Kimball data\nmodel: a Type 2 slowly changing dimension, which we’ll describe in greater detail\nnext.\nTable 8-11. A Type 2 customer dimension table\nCustomerKey FirstName LastName ZipCode EFF_StartDate EFF_EndDate\n5 Joe Reis 84108 2019-01-04 9999-01-01\n7 Matt Housley 84101 2020-05-04 2021-09-19\n7 Matt Housley 84123 2021-09-19 9999-01-01\n11 Lana Belle 90210 2022-02-04 9999-01-01\nFor example, take a look at CustomerKey 5 , with the EFF_StartDate  (EFF_StartDate\nmeans effective  start date ) of 2019-01-04  and an EFF_EndDate  of 9999-01-01 . This\nmeans Joe Reis’s customer record was created in the customer dimension table on\n2019-01-04 and has an end date of 9999-01-01. Interesting. What does this end date\nmean? It means the customer record is active and isn’t changed.\nNow let’s look at Matt Housley’s customer record ( CustomerKey  = 7). Notice the two\nentries for Housley’s start date: 2020-05-04  and 2021-09-19 . It looks like Housley\nchanged his zip code on 2021-09-19, resulting in a change to his customer record.\nData Modeling | 299\nWhen the data is queried for the most recent customer records, you will query where\nthe end date is equal to 9999-01-01.\nA slowly changing dimension (SCD) is necessary to track changes in dimensions. The\npreceding example is a Type 2 SCD: a new record is inserted when an existing record\nchanges. Though SCDs can go up to seven levels, let’s look at the three most common\nones:\nType 1\nOverwrite existing dimension records. This is super simple and means you have\nno access to the deleted historical dimension records.\nType 2\nKeep a full history of dimension records. When a record changes, that specific\nrecord is flagged as changed, and a new dimension record is created that reflects\nthe current status of the attributes. In our example, Housley moved to a new zip\ncode, which triggered his initial record to reflect an effective end date, and a new\nrecord was created to show his new zip code.\nType 3\nA Type 3 SCD is similar to a Type 2 SCD, but instead of creating a new row, a\nchange in a Type 3 SCD creates a new field. Using the preceding example, let’s see\nwhat this looks like as a Type 3 SCD in the following tables.\nIn Table 8-12 , Housley lives in the 84101 zip code. When Housley moves to a new zip\ncode, the Type 3 SCD creates two new fields, one for his new zip code and the date of\nthe change ( Table 8-13 ). The original zip code field is also renamed to reflect that this\nis the older record.\nTable 8-12. Type 3 slowly changing dimension\nCustomerKey FirstName LastName ZipCode\n7 Matt Housley 84101\nTable 8-13. Type 3 customer dimension table\nCustomerKey FirstName LastName Original ZipCode Current ZipCode CurrentDate\n7 Matt Housley 84101 84123 2021-09-19\nOf the types of SCDs described, Type 1 is the default behavior of most data ware‐\nhouses, and Type 2 is the one we most commonly see used in practice. There’s a lot\nto know about dimensions, and we suggest using this section as a starting point to get\nfamiliar with how dimensions work and how they’re used.\n300 | Chapter 8: Queries, Modeling, and Transformation\n11The Data Vault has two versions, 1.0 and 2.0. This section focuses on Data Vault 2.0, but we’ll call it Data\nVault  for the sake of brevity.Star schema.    Now  that you have a basic understanding of facts and dimensions,\nit’s time to integrate them into a star schema. The star schema  represents the data\nmodel of the business. Unlike highly normalized approaches to data modeling, the\nstar schema is a fact table surrounded by the necessary dimensions. This results in\nfewer joins than other data models, which speeds up query performance. Another\nadvantage of a star schema is it’s arguably easier for business users to understand and\nuse.\nNote that the star schema shouldn’t reflect a particular report, though you can model\na report in a downstream data mart or directly in your BI tool. The star schema\nshould capture the facts and attributes of your business logic  and be flexible enough to\nanswer the respective critical questions.\nBecause a star schema has one fact table, sometimes you’ll have multiple star sche‐\nmas that address different facts of the business. Y ou should strive to reduce the\nnumber of dimensions whenever possible since this reference data can potentially be\nreused among different fact tables. A dimension that is reused across multiple star\nschemas, thus sharing the same fields, is called a conformed dimension . A conformed\ndimension allows you to combine multiple fact tables across multiple star schemas.\nRemember, redundant data is OK with the Kimball method, but avoid replicating the\nsame dimension tables to avoid drifting business definitions and data integrity.\nThe Kimball data model and star schema have a lot of nuance. Y ou should be aware\nthat this mode is appropriate only for batch data and not for streaming data. Because\nthe Kimball data model is popular, there’s a good chance you’ll run into it.\nData Vault\nWhereas  Kimball and Inmon focus on the structure of business logic in the data\nwarehouse, the Data Vault  offers a different approach to data modeling.11 Created\nin the 1990s by Dan Linstedt, the Data Vault methodology separates the structural\naspects of a source system’s data from its attributes. Instead of representing business\nlogic in facts, dimensions, or highly normalized tables, a Data Vault simply loads data\nfrom source systems directly into a handful of purpose-built tables in an insert-only\nmanner. Unlike the other data modeling approaches you’ve learned about, there’s no\nnotion of good, bad, or conformed data in a Data Vault.\nData moves fast these days, and data models need to be agile, flexible, and scalable;\nthe Data Vault methodology aims to meet this need. The goal of this methodology\nis to keep the data as closely aligned to the business as possible, even while the\nbusiness’s data evolves.\nData Modeling | 301\n12Kent Graziano, “Data Vault 2.0 Modeling Basics, ” Vertabelo, October 20, 2015, https://oreil.ly/iuW1U .A Data Vault model consists of three main types of tables: hubs, links, and satellites\n(Figure 8-15 ). In short, a hub stores business keys, a link maintains relationships\namong business keys, and a satellite  represents a business key’s attributes and context.\nA user will query a hub, which will link to a satellite table containing the query’s\nrelevant attributes. Let’s explore hubs, links, and satellites in more detail.\nFigure 8-15. Data Vault tables: hubs, links, and satellites connected together\nHubs.    Queries often involve searching by a business key, such as a customer ID or an\norder ID from our ecommerce example. A hub is the central entity of a Data Vault\nthat retains a record of all unique business keys loaded into the Data Vault.\nA hub always contains the following standard fields:\nHash key\nThe primary key used to join data between systems. This is a calculated hash field\n(MD5 or similar).\nLoad date\nThe date the data was loaded into the hub.\nRecord source\nThe source from which the unique record was obtained.\nBusiness key(s)\nThe key used to identify a unique record.\nIt’s important to note that a hub is insert-only, and data is not altered in a hub. Once\ndata is loaded into a hub, it’s permanent.\nWhen designing a hub, identifying the business key is critical. Ask yourself: What\nis the identifiable  business element ?12 Put another way, how do users commonly look\nfor data? Ideally, this is discovered as you build the conceptual data model of your\norganization and before you start building your Data Vault.\nUsing our ecommerce scenario, let’s look at an example of a hub for products. First,\nlet’s look at the physical design of a product hub ( Table 8-14 ).\n302 | Chapter 8: Queries, Modeling, and Transformation\nTable 8-14. A physical design for a product hub\nHubProduct\nProductHashKey\nLoadDate\nRecordSource\nProductID\nIn practice, the product hub looks like this when populated with data ( Table 8-15 ). In\nthis example, three different products are loaded into a hub from an ERP system on\ntwo separate dates.\nTable 8-15. A product hub populated with data\nProductHashKey LoadDate RecordSource ProductID\n4041fd80ab... 2020-01-02 ERP 1\nde8435530d... 2021-03-09 ERP 2\ncf27369bd8... 2021-03-09 ERP 3\nWhile we’re at it, let’s create another hub for orders ( Table 8-16 ) using the same\nschema as HubProduct , and populate it with some sample order data.\nTable 8-16. An order hub populated with data\nOrderHashKey LoadDate RecordSource OrderID\nf899139df5... 2022-03-01 Website 100\n38b3eff8ba... 2022-03-01 Website 101\nec8956637a... 2022-03-01 Website 102\nLinks.    A link table  tracks the relationships of business keys between hubs. Link tables\nconnect hubs, ideally at the lowest possible grain. Because link tables connect data\nfrom various hubs, they are many to many. The Data Vault model’s relationships are\nstraightforward and handled through changes to the links. This provides excellent\nflexibility in the inevitable event that the underlying data changes. Y ou simply create\na new link that ties business concepts (or hubs) to represent the new relationship.\nThat’s it! Now let’s look at ways to view data contextually using satellites.\nBack to our ecommerce example, we’ d like to associate orders with products. Let’s see\nwhat a link table might look like for orders and products ( Table 8-17 ).\nTable 8-17. A link table for products and orders\nLinkOrderProduct\nOrderProductHashKey\nLoadDate\nData Modeling | 303\nLinkOrderProduct\nRecordSource\nProductHashKey\nOrderHashKey\nWhen the LinkOrderProduct  table is populated, here’s what it looks like ( Table 8-18 ).\nNote that we’re using the order’s record source in this example.\nTable 8-18. A link table connecting orders and products\nOrderProductHashKey LoadDate RecordSource ProductHashKey OrderHashKey\nff64ec193d... 2022-03-01 Website 4041fd80ab... f899139df5...\nff64ec193d... 2022-03-01 Website de8435530d... f899139df5...\ne232628c25... 2022-03-01 Website cf27369bd8... 38b3eff8ba...\n26166a5871... 2022-03-01 Website 4041fd80ab... ec8956637a...\nSatellites.    We’ve  described relationships between hubs and links that involve keys,\nload dates, and record sources. How do you get a sense of what these relationships\nmean? Satellites  are descriptive attributes that give meaning and context to hubs.\nSatellites can connect to either hubs or links. The only required fields in a satellite\nare a primary key consisting of the business key of the parent hub and a load date.\nBeyond that, a satellite can contain however many attributes that make sense.\nLet’s look at an example of a satellite for the Product  hub ( Table 8-19 ). In this exam‐\nple, the SatelliteProduct  table contains additional information about the product,\nsuch as product name and price.\nTable 8-19. SatelliteProduct\nSatelliteProduct\nProductHashKey\nLoadDate\nRecordSource\nProductName\nPrice\nAnd here’s the SatelliteProduct  table with some sample data ( Table 8-20 ).\nTable 8-20. A product satellite table with sample data\nProductHashKey LoadDate RecordSource ProductName Price\n4041fd80ab... 2020-01-02 ERP Thingamajig 50\nde8435530d... 2021-03-09 ERP Whatchamacallit 25\ncf27369bd8... 2021-03-09 ERP Whozeewhatzit 75\n304 | Chapter 8: Queries, Modeling, and Transformation\nLet’s tie this all together and join the hub, product, and link tables into a Data Vault\n(Figure 8-16 ).\nFigure 8-16. The Data Vault for orders and products\nOther types of Data Vault tables exist, including point-in-time (PIT) and bridge\ntables. We don’t cover these here, but mention them because the Data Vault is quite\ncomprehensive. Our goal is to simply give you an overview of the Data Vault’s power.\nUnlike other data modeling techniques we’ve discussed, in a Data Vault, the business\nlogic is created and interpreted when the data from these tables is queried. Please be\naware that the Data Vault model can be used with other data modeling techniques. It’s\nnot unusual for a Data Vault to be the landing zone for analytical data, after which\nit’s separately modeled in a data warehouse, commonly using a star schema. The Data\nVault model also can be adapted for NoSQL and streaming data sources. The Data\nVault is a huge topic, and this section is simply meant to make you aware of its\nexistence.\nWide denormalized tables\nThe strict modeling approaches we’ve described, especially Kimball and Inmon,\nwere developed when data warehouses were expensive, on premises, and heavily\nresource-constrained with tightly coupled compute and storage. While batch data\nmodeling has traditionally been associated with these strict approaches, more relaxed\napproaches are becoming more common.\nThere are reasons for this. First, the popularity of the cloud means that storage is\ndirt cheap. It’s cheaper to store data than agonize over the optimum way to represent\nthe data in storage. Second, the popularity of nested data (JSON and similar) means\nschemas are flexible in source and analytical systems.\nY ou have the option to rigidly model your data as we’ve described, or you can choose\nto throw all of your data into a single wide table. A wide table  is just what it sounds\nlike: a highly denormalized and very wide collection of many fields, typically created\nin a columnar database. A field may be a single value or contain nested data. The data\nis organized along with one or multiple keys; these keys are closely tied to the grain  of\nthe data.\nData Modeling | 305\nA wide table can potentially have thousands of columns, whereas fewer than 100\nare typical in relational databases. Wide tables are usually sparse; the vast majority\nof entries in a given field may be null. This is extremely expensive in a traditional\nrelational database because the database allocates a fixed amount of space for each\nfield entry; nulls take up virtually no space in a columnar database. A wide schema\nin a relational database dramatically slows reading because each row must allocate all\nthe space specified by the wide schema, and the database must read the contents of\neach row in its entirety. On the other hand, a columnar database reads only columns\nselected in a query, and reading nulls is essentially free.\nWide tables generally arise through schema evolution; engineers gradually add fields\nover time. Schema evolution in a relational database is a slow and resource-heavy\nprocess. In a columnar database, adding a field is initially just a change to metadata.\nAs data is written into the new field, new files are added to the column.\nAnalytics queries on wide tables often run faster than equivalent queries on highly\nnormalized data requiring many joins. Removing joins can have a huge impact on\nscan performance. The wide table simply contains all of the data you would have\njoined in a more rigorous modeling approach. Facts and dimensions are represented\nin the same table. The lack of data model rigor also means not a lot of thought\nis involved. Load your data into a wide table and start querying it. Especially with\nschemas in source systems becoming more adaptive and flexible, this data usually\nresults from high-volume transactions, meaning there’s a lot of data. Storing this as\nnested data in your analytical storage has a lot of benefits.\nThrowing all of your data into a single table might seem like heresy for a hardcore\ndata modeler, and we’ve seen plenty of criticism. What are some of these criticisms?\nThe biggest criticism is as you blend your data, you lose the business logic in your\nanalytics. Another downside is the performance of updates to things like an element\nin an array, which can be very painful.\nLet’s look at an example of a wide table ( Table 8-21 ), using the original denormal‐\nized table from our earlier normalization example. This table can have many more\ncolumns—hundreds or more!—and we include only a handful of columns for brevity\nand ease of understanding. As you can see, this table combines various data types,\nrepresented along a grain of orders for a customer on a date.\nWe suggest using a wide table when you don’t care about data modeling, or when\nyou have a lot of data that needs more flexibility than traditional data-modeling rigor\nprovides. Wide tables also lend themselves to streaming data, which we’ll discuss\nnext. As data moves toward fast-moving schemas and streaming-first, we expect to\nsee a new wave of data modeling, perhaps something along the lines of “relaxed\nnormalization. ”\n306 | Chapter 8: Queries, Modeling, and Transformation",29168
141-Modeling Streaming Data.pdf,141-Modeling Streaming Data,"Table 8-21. An example of denormalized data\nOrderID OrderItems CustomerID Customer\nNameOrderDate Site Site\nRegion\n100 [{\n  ""sku"": 1,\n  ""price"": 50,\n  ""quantity"" : 1,\n  ""name:"": \n    ""Thingamajig""\n}, {\n  ""sku"": 2,\n  ""price"": 25,\n  ""quantity"" : 2,\n  ""name:"": \n    ""Whatchamacallit""\n}]5 Joe Reis 2022-03-01 abc.com US\nWhat If You Don’t Model Your Data?\nY ou also have the option of not modeling your data. In this case, just query data\nsources directly. This pattern is often used, especially when companies are just getting\nstarted and want to get quick insights or share analytics with their users. While it\nallows you to get answers to various questions, you should consider the following:\n•If I don’t model my data, how do I know the results of my queries are consistent?•\n•Do I have proper definitions of business logic in the source system, and will my•\nquery produce truthful answers?\n•What query load am I putting on my source systems, and how does this impact•\nusers of these systems?\nAt some point, you’ll probably gravitate toward a stricter batch data model paradigm\nand a dedicated data architecture that doesn’t rely on the source systems for the heavy\nlifting.\nModeling Streaming Data\nWhereas  many data-modeling techniques are well established for batch, this is not\nthe case for streaming data. Because of the unbounded and continuous nature of\nstreaming data, translating batch techniques like Kimball to a streaming paradigm\nis tricky, if not impossible. For example, given a stream of data, how would you\ncontinuously update a Type-2 slowly changing dimension without bringing your data\nwarehouse to its knees?\nData Modeling | 307\nThe world is evolving from batch to streaming and from on premises to the cloud.\nThe constraints of the older batch methods no longer apply. That said, big questions\nremain about how to model data to balance the need for business logic against fluid\nschema changes, fast-moving data, and self-service. What is the streaming equivalent\nof the preceding batch data model approaches? There isn’t (yet) a consensus approach\non streaming data modeling. We spoke with many experts in streaming data systems,\nmany of whom told us that traditional batch-oriented data modeling doesn’t apply to\nstreaming. A few suggested the Data Vault as an option for streaming data modeling.\nAs you may recall, two main types of streams exist: event streams and CDC. Most\nof the time, the shape of the data in these streams is semistructured, such as JSON.\nThe challenge with modeling streaming data is that the payload’s schema might\nchange on a whim. For example, suppose you have an IoT device that recently\nupgraded its firmware and introduced a new field. In that case, it’s possible that your\ndownstream destination data warehouse or processing pipeline isn’t aware of this\nchange and breaks. That’s not great. As another example, a CDC system might recast\na field as a different type—say, a string instead of an International Organization for\nStandardization (ISO) datetime format. Again, how does the destination handle this\nseemingly random change?\nThe streaming data experts we’ve talked with overwhelmingly suggest you anticipate\nchanges in the source data and keep a flexible schema. This means there’s no rigid\ndata model in the analytical database. Instead, assume the source systems are provid‐\ning the correct data with the right business definition and logic, as it exists today. And\nbecause storage is cheap, store the recent streaming and saved historical data in a way\nthey can be queried together. Optimize for comprehensive analytics against a dataset\nwith a flexible schema. Furthermore, instead of reacting to reports, why not create\nautomation that responds to anomalies and changes in the streaming data instead?\nThe world of data modeling is changing, and we believe a sea change will soon occur\nin data model paradigms. These new approaches will likely incorporate metrics and\nsemantic layers, data pipelines, and traditional analytics workflows in a streaming\nlayer that sits directly on top of the source system. Since data is being generated in\nreal time, the notion of artificially separating source and analytics systems into two\ndistinct buckets may not make as much sense as when data moved more slowly and\npredictably. Time will tell…\nWe have more to say on the future of streaming data in Chapter 11 .\n308 | Chapter 8: Queries, Modeling, and Transformation",4491
142-Batch Transformations.pdf,142-Batch Transformations,"13Bill Inmon, “ Avoiding the Horrible Task of Integrating Data, ” LinkedIn Pulse, March 24, 2022,\nhttps://oreil.ly/yLb71 .Transformations\nThe net result of transforming data is the ability to unify and integrate data. Once data\nis transformed, the data can be viewed as a single entity. But without transforming\ndata, you cannot have a unified view of data across the organization.\n—Bill Inmon13\nNow that we’ve covered queries and data modeling, you might be wondering, if I can\nmodel data, query it, and get results, why do I need to think about transformations?\nTransformations  manipulate, enhance, and save data for downstream use, increasing\nits value in a scalable, reliable, and cost-effective manner.\nImagine running a query every time you want to view results from a particular\ndataset. Y ou’ d run the same query dozens or hundreds of times a day. Imagine that\nthis query involves parsing, cleansing, joining, unioning, and aggregating across 20\ndatasets. To further exacerbate the pain, the query takes 30 minutes to run, consumes\nsignificant resources, and incurs substantial cloud charges over several repetitions.\nY ou and your stakeholders would probably go insane. Thankfully, you can save the\nresults of your query  instead, or at least run the most compute-intensive portions only\nonce, so subsequent queries are simplified.\nA transformation differs from a query. A query  retrieves the data from various\nsources based on filtering and join logic. A transformation  persists the results for\nconsumption by additional transformations or queries. These results may be stored\nephemerally or permanently.\nBesides persistence, a second aspect that differentiates transformations from queries\nis complexity. Y ou’ll likely build complex pipelines that combine data from multiple\nsources and reuse intermediate results for multiple final outputs. These complex\npipelines might normalize, model, aggregate, or featurize data. While you can build\ncomplex dataflows in single queries using common table expressions, scripts, or\nDAGs, this quickly becomes unwieldy, inconsistent, and intractable. Enter transfor‐\nmations.\nTransformations critically rely on one of the major undercurrents in this book:\norchestration. Orchestration combines many discrete operations, such as intermedi‐\nate transformations, that store data temporarily or permanently for consumption by\ndownstream transformations or serving. Increasingly, transformation pipelines span\nnot only multiple tables and datasets but also multiple systems.\nTransformations | 309\nBatch Transformations\nBatch transformations  run on discrete chunks of data, in contrast to streaming trans‐\nformations, where data is processed continuously as it arrives. Batch transformations\ncan run on a fixed schedule (e.g., daily, hourly, or every 15 minutes) to support\nongoing reporting, analytics, and ML models. In this section, you’ll learn various\nbatch transformation patterns and technologies.\nDistributed joins\nThe basic idea behind distributed joins is that we need to break a logical join  (the\njoin defined by the query logic) into much smaller node joins  that run on individual\nservers in the cluster. The basic distributed join patterns apply whether one is in\nMapReduce (discussed in “MapReduce” on page 322 ), BigQuery, Snowflake, or Spark,\nthough the details of intermediate storage between processing steps vary (on disk or\nin memory). In the best-case scenario, the data on one side of the join is small enough\nto fit on a single node ( broadcast join ). Often, a more resource-intensive shuffle  hash\njoin is required.\nBroadcast join.    A broadcast join  is generally asymmetric, with one large table dis‐\ntributed across nodes and one small table that can easily fit on a single node ( Fig‐\nure 8-17 ). The query engine “broadcasts” the small table (table A) out to all nodes,\nwhere it gets joined to the parts of the large table (table B). Broadcast joins are far less\ncompute intensive than shuffle hash joins.\nFigure 8-17. In a broadcast join, the query engine sends table A out to all nodes in the\ncluster to be joined with the various parts of table B\n310 | Chapter 8: Queries, Modeling, and Transformation\nIn practice, table A is often a down-filtered larger table that the query engine collects\nand broadcasts. One of the top priorities in query optimizers is join reordering. With\nthe early application of filters, and movement of small tables to the left (for left joins),\nit is often possible to dramatically reduce the amount of data that is processed in\neach join. Prefiltering data to create broadcast joins where possible can dramatically\nimprove performance and reduce resource consumption.\nShuffle  hash join.    If neither table is small enough to fit on a single node, the query\nengine will use a shuffle  hash join . In Figure 8-18 , the same nodes are represented\nabove and below the dotted line. The area above the dotted line represents the initial\npartitioning of tables A and B across the nodes. In general, this partitioning will have\nno relation to the join key. A hashing scheme is used to repartition data by join key.\nFigure 8-18. Shuffle  hash join\nIn this example, the hashing scheme will partition the join key into three parts, with\neach part assigned to a node. The data is then reshuffled to the appropriate node, and\nthe new partitions for tables A and B on each node are joined. Shuffle hash joins are\ngenerally more resource intensive than broadcast joins.\nETL, ELT, and data pipelines\nAs we discussed in Chapter 3 , a widespread transformation pattern dating to the early\ndays of relational databases is a batch ETL. Traditional ETL relies on an external\ntransformation system to pull, transform, and clean data while preparing it for a\ntarget schema, such as a data mart or a Kimball star schema. The transformed data\nwould then be loaded into a target system, such as a data warehouse, where business\nanalytics could be performed.\nTransformations | 311\n14Alex Woodie, “Lakehouses Prevent Data Swamps, Bill Inmon Says, ” Datanami, June 1, 2021,\nhttps://oreil.ly/XMwWc .The ETL pattern itself was driven by the limitations of both source and target\nsystems. The extract phase tended to be a major bottleneck, with the constraints\nof the source RDBMS limiting the rate at which data could be pulled. And, the\ntransformation was handled in a dedicated system because the target system was\nextremely resource-constrained in both storage and CPU capacity.\nA now-popular evolution of ETL is ELT. As data warehouse systems have grown in\nperformance and storage capacity, it has become common to simply extract raw data\nfrom a source system, import it into a data warehouse with minimal transformation,\nand then clean and transform it directly in the warehouse system. (See our discussion\nof data warehouses in Chapter 3  for a more detailed discussion of the difference\nbetween ETL and ELT.)\nA second, slightly different notion of ELT was popularized with the emergence\nof data lakes. In this version, the data is not transformed at the time it’s loaded.\nIndeed, massive quantities of data may be loaded with no preparation and no plan\nwhatsoever. The assumption is that the transformation step will happen at some\nundetermined future time. Ingesting data without a plan is a great recipe for a data\nswamp. As Inmon says:14\nI’ve always been a fan of ETL because of the fact that ETL forces you to transform data\nbefore you put it into a form where you can work with it. But some organizations want\nto simply take the data, put it into a database, then do the transformation.... I’ve seen\ntoo many cases where the organization says, oh we’ll just put the data in and transform\nit later. And guess what? Six months later, that data [has] never been touched.\nWe have also seen that the line between ETL and ELT can become somewhat blurry\nin a data lakehouse environment. With object storage as a base layer, it’s no longer\nclear what’s in the database and out of the database. The ambiguity is further exa‐\ncerbated with the emergence of data federation, virtualization, and live tables. (We\ndiscuss these topics later in this section.)\nIncreasingly, we feel that the terms ETL and ELT should be applied only at the micro\nlevel (within individual transformation pipelines) rather than at the macro level (to\ndescribe a transformation pattern for a whole organization). Organizations no longer\nneed to standardize on ETL or ELT but can instead focus on applying the proper\ntechnique on a case-by-case basis as they build data pipelines.\nSQL and code-based transformation tools\nAt this juncture, the distinction between SQL-based and non-SQL-based transforma‐\ntion systems feels somewhat synthetic. Since the introduction of Hive on the Hadoop\n312 | Chapter 8: Queries, Modeling, and Transformation\nplatform, SQL has become a first-class citizen in the big data ecosystem. For example,\nSpark SQL was an early feature of Apache Spark. Streaming-first frameworks such as\nKafka, Flink, and Beam also support SQL, with varying features and functionality.\nIt is more appropriate to think about SQL-only tools versus those that support more\npowerful, general-purpose programming paradigms. SQL-only transformation tools\nspan a wide variety of proprietary and open source options.\nSQL is declarative...but it can still build complex data workflows .    We often hear SQL dis‐\nmissed because it is “not procedural. ” This is technically correct. SQL is a declarative\nlanguage: instead of coding a data processing procedure, SQL writers stipulate the\ncharacteristics of their final data in set-theoretic language; the SQL compiler and\noptimizer determine the steps required to put data in this state.\nPeople sometimes imply that because SQL is not procedural, it cannot build out\ncomplex pipelines. This is false. SQL can effectively be used to build complex DAGs\nusing common table expressions, SQL scripts, or an orchestration tool.\nTo be clear, SQL has limits, but we often see engineers doing things in Python and\nSpark that could be more easily and efficiently done in SQL. For a better idea of the\ntrade-offs we’re talking about, let’s look at a couple of examples of Spark and SQL.\nExample: When to avoid SQL for batch transformations in Spark.    When  you’re determining\nwhether to use native Spark or PySpark code instead of Spark SQL or another SQL\nengine, ask yourself the following questions:\n1.How difficult is it to code the transformation in SQL?1.\n2.How readable and maintainable will the resulting SQL code be?2.\n3.Should some of the transformation code be pushed into a custom library for3.\nfuture reuse across the organization?\nRegarding question 1, many transformations coded in Spark could be realized in\nfairly simple SQL statements. On the other hand, if the transformation is not realiz‐\nable in SQL, or if it would be extremely awkward to implement, native Spark is a\nbetter option. For example, we might be able to implement word stemming in SQL\nby placing word suffixes in a table, joining with that table, using a parsing function\nto find suffixes in words, and then reducing the word to its stem by using a substring\nfunction. However, this sounds like an extremely complex process with numerous\nedge cases to consider. A more powerful procedural programming language is a\nbetter fit here.\nQuestion 2 is closely related. The word-stemming query will be neither readable nor\nmaintainable.\nTransformations | 313\n15We remind you to use UDFs responsibly. SQL UDFs often perform reasonably well. We’ve seen JavaScript\nUDFs increase query time from a few minutes to several hours.Regarding question 3, one of the major limitations of SQL is that it doesn’t include a\nnatural notion of libraries or reusable code. One exception is that some SQL engines\nallow you to maintain user-defined functions (UDFs) as objects inside a database.15\nHowever, these aren’t committed to a Git repository without an external CI/CD\nsystem to manage deployment. Furthermore, SQL doesn’t have a good notion of\nreusability for more complex query components. Of course, reusable libraries are\neasy to create in Spark and PySpark.\nWe will add that it is possible to recycle SQL in two ways. First, we can easily reuse\nthe results  of a SQL query by committing to a table or creating a view. This process\nis often best handled in an orchestration tool such as Airflow so that downstream\nqueries can start once the source query has finished. Second, Data Build Tool (dbt)\nfacilitates the reuse of SQL statements and offers a templating language that makes\ncustomization easier.\nExample: Optimizing Spark and other processing frameworks.    Spark acolytes often com‐\nplain that SQL doesn’t give them control over data processing. The SQL engine takes\nyour statements, optimizes them, and compiles them into its processing steps. (In\npractice, optimization may happen before or after compilation, or both.)\nThis is a fair complaint, but a corollary exists. With Spark and other code-heavy\nprocessing frameworks, the code writer becomes responsible for much of the opti‐\nmization that is handled automatically in a SQL-based engine. The Spark API is\npowerful and complex, meaning it is not so easy to identify candidates for reordering,\ncombination, or decomposition. When embracing Spark, data engineering teams\nneed to actively engage with the problems of Spark optimization, especially for\nexpensive, long-running jobs. This means building optimization expertise on the\nteam and teaching individual engineers how to optimize.\nA few top-level things to keep in mind when coding in native Spark:\n1.Filter early and often.1.\n2.Rely heavily on the core Spark API, and learn to understand the Spark native way2.\nof doing things. Try to rely on well-maintained public libraries if the native Spark\nAPI doesn’t support your use case. Good Spark code is substantially declarative.\n3.Be careful with UDFs.3.\n4.Consider intermixing SQL.4.\nRecommendation 1 applies to SQL optimization as well, with the difference being\nthat Spark may not be able to reorder something that SQL would handle for you\n314 | Chapter 8: Queries, Modeling, and Transformation\nautomatically. Spark is a big data processing framework, but the less data you have to\nprocess, the less resource-heavy and more performant your code will be.\nIf you find yourself writing extremely complex custom code, pause and determine\nwhether there’s a more native way of doing whatever you’re trying to accomplish.\nLearn to understand idiomatic Spark by reading public examples and working\nthrough tutorials. Is there something in the Spark API that can accomplish what\nyou’re trying to do? Is there a well-maintained and optimized public library that can\nhelp?\nThe third recommendation is crucial for PySpark. In general, PySpark is an API\nwrapper for Scala Spark. Y our code pushes work into native Scala code running\nin the JVM by calling the API. Running Python UDFs forces data to be passed to\nPython, where processing is less efficient. If you find yourself using Python UDFs,\nlook for a more Spark-native way to accomplish what you’re doing. Go back to the\nrecommendation: is there a way to accomplish your task by using the core API or a\nwell-maintained library? If you must use UDFs, consider rewriting them in Scala or\nJava to improve performance.\nAs for recommendation 4, using SQL allows us to take advantage of the Spark Cata‐\nlyst optimizer, which may be able to squeeze out more performance than we can with\nnative Spark code. SQL is often easier to write and maintain for simple operations.\nCombining native Spark and SQL lets us realize the best of both worlds—powerful,\ngeneral-purpose functionality combined with simplicity where applicable.\nMuch of the optimization advice in this section is fairly generic and would apply just\nas well to Apache Beam, for example. The main point is that programmable data\nprocessing APIs require a bit more optimization finesse than SQL, which is perhaps\nless powerful and easier to use.\nUpdate patterns\nSince  transformations persist data, we will often update persisted data in place.\nUpdating data is a major pain point for data engineering teams, especially as they\ntransition between data engineering technologies. We’re discussing DML in SQL,\nwhich we introduced earlier in the chapter.\nWe’ve mentioned several times throughout the book that the original data lake con‐\ncept didn’t really account for updating data. This now seems nonsensical for several\nreasons. Updating data has long been a key part of handling data transformation\nresults, even though the big data community dismissed it. It is silly to rerun signif‐\nicant amounts of work because we have no update capabilities. Thus, the data lake‐\nhouse concept now builds in updates. Also, GDPR and other data deletion standards\nnow require  organizations to delete data in a targeted fashion, even in raw datasets.\nLet’s consider several basic update patterns.\nTransformations | 315\nTruncate and reload.    Truncate  is an update pattern that doesn’t update anything. It\nsimply wipes the old data. In a truncate-and-reload update pattern, a table is cleared\nof data, and transformations are rerun and loaded into this table, effectively generat‐\ning a new table version.\nInsert only.    Insert only  inserts  new records without changing or deleting old records.\nInsert-only patterns can be used to maintain a current view of data—for example, if\nnew versions of records are inserted without deleting old records. A query or view\ncan present the current data state by finding the newest record by primary key. Note\nthat columnar databases don’t typically enforce primary keys. The primary key would\nbe a construct used by engineers to maintain a notion of the current state of the table.\nThe downside to this approach is that it can be extremely computationally expensive\nto find the latest record at query time. Alternatively, we can use a materialized view\n(covered later in the chapter), an insert-only table that maintains all records, and a\ntruncate-and-reload target table that holds the current state for serving data.\nWhen inserting data into a column-oriented OLAP database,\nthe common problem is that engineers transitioning from row-\noriented systems attempt to use  single-row inserts. This antipattern\nputs a massive load on the system. It also causes data to be written\nin many separate files; this is extremely inefficient for subsequent\nreads, and the data must be reclustered later. Instead, we recom‐\nmend loading data in a periodic micro-batch or batch fashion.\nWe’ll mention an exception to the advice not to insert frequently: the enhanced\nLambda architecture used by BigQuery and Apache Druid, which hybridizes a\nstreaming buffer with columnar storage. Deletes and in-place updates can still be\nexpensive, as we’ll discuss next.\nDelete.    Deletion  is critical when a source system deletes data and satisfies recent\nregulatory changes. In columnar systems and data lakes, deletes are more expensive\nthan inserts.\nWhen deleting data, consider whether you need to do a hard or soft delete. A hard\ndelete  permanently removes a record from a database, while a soft delete  marks the\nrecord as “deleted. ” Hard deletes are useful when you need to remove data for perfor‐\nmance reasons (say, a table is too big), or if there’s a legal or compliance reason to do\nso. Soft deletes might be used when you don’t want to delete a record permanently but\nalso want to filter it out of query results.\nA third approach to deletes is closely related to soft deletes: insert deletion  inserts a\nnew record with a deleted  flag without modifying the previous version of the record.\nThis allows us to follow an insert-only pattern but still account for deletions. Just note\n316 | Chapter 8: Queries, Modeling, and Transformation\nthat our query to get the latest table state gets a little more complicated. We must now\ndeduplicate, find the latest version of each record by key, and not show any record\nwhose latest version shows deleted .\nUpsert/merge.    Of these update patterns, the upsert and merge patterns are the\nones that consistently cause the most trouble for data engineering teams, especially\nfor people transitioning from row-based data warehouses to column-based cloud\nsystems.\nUpserting  takes  a set of source records and looks for matches against a target table by\nusing a primary key or another logical condition. (Again, it’s the responsibility of the\ndata engineering team to manage this primary key by running appropriate queries.\nMost columnar systems will not enforce uniqueness.) When a key match occurs, the\ntarget record gets updated (replaced by the new record). When no match exists, the\ndatabase inserts the new record. The merge pattern adds to this the ability to delete\nrecords.\nSo, what’s the problem? The upsert/merge pattern was originally designed for row-\nbased databases. In row-based databases, updates are a natural process: the database\nlooks up the record in question and changes it in place.\nOn the other hand, file-based systems don’t actually support in-place file updates. All\nof these systems utilize copy on write (COW). If one record in a file is changed or\ndeleted, the whole file must be rewritten with the new changes.\nThis is part of the reason that early adopters of big data and data lakes rejected\nupdates: managing files and updates seemed too complicated. So they simply used\nan insert-only pattern and assumed that data consumers would determine the cur‐\nrent state of the data at query time or in downstream transformations. In reality,\ncolumnar databases such as Vertica have long supported in-place updates by hiding\nthe complexity of COW from users. They scan files, change the relevant records,\nwrite new files, and change file pointers for the table. The major columnar cloud\ndata warehouses support updates and merges, although engineers should investigate\nupdate support if they consider adopting an exotic technology.\nThere are a few key things to understand here. Even though distributed columnar\ndata systems support native update commands, merges come at a cost: the perfor‐\nmance impact of updating or deleting a single record can be quite high. On the\nother hand, merges can be extremely performant for large update sets and may even\noutperform transactional databases.\nIn addition, it is important to understand that COW seldom entails rewriting the\nwhole table. Depending on the database system in question, COW can operate at\nvarious resolutions (partition, cluster, block). To realize performant updates, focus on\nTransformations | 317\ndeveloping an appropriate partitioning and clustering strategy based on your needs\nand the innards of the database in question.\nAs with inserts, be careful with your update or merge frequency. We’ve seen many\nengineering teams transition between database systems and try to run near real-time\nmerges from CDC just as they did on their old system. It simply doesn’t work. No\nmatter how good your CDC system is, this approach will bring most columnar data\nwarehouses to their knees. We’ve seen systems fall weeks behind on updates, where an\napproach that simply merged every hour would make much more sense.\nWe can use various approaches to bring columnar databases closer to real time.\nFor example, BigQuery allows us to stream insert new records into a table, and\nthen supports specialized materialized views that present an efficient, near real-time\ndeduplicated table view. Druid uses two-tier storage and SSDs to support ultrafast\nreal-time queries.\nSchema updates\nData  has entropy and may change without your control or consent. External data\nsources may change their schema, or application development teams may add new\nfields to the schema. One advantage of columnar systems over row-based systems is\nthat while updating the data is more difficult, updating the schema is easier. Columns\ncan typically be added, deleted, and renamed.\nIn spite of these technological improvements, practical organizational schema man‐\nagement is more challenging. Will some schema updates be automated? (This is the\napproach that Fivetran uses when replicating from sources.) As convenient as this\nsounds, there’s a risk that downstream transformations will break.\nIs there a straightforward schema update request process? Suppose a data science\nteam wants to add a column from a source that wasn’t previously ingested. What will\nthe review process look like? Will downstream processes break? (Are there queries\nthat run SELECT *  rather than using explicit column selection? This is generally bad\npractice in columnar databases.) How long will it take to implement the change? Is it\npossible to create a table fork—i.e., a new table version specific to this project?\nA new interesting option has emerged for semistructured data. Borrowing an idea\nfrom document stores, many cloud data warehouses now support data types that\nencode arbitrary JSON data. One approach stores raw JSON in a field while storing\nfrequently accessed data in adjacent flattened fields. This takes up additional storage\nspace but allows for the convenience of flattened data, with the flexibility of semi‐\nstructured data for advanced users. Frequently accessed data in the JSON field can be\nadded directly into the schema over time.\nThis approach works extremely well when data engineers must ingest data from\nan application document store with a frequently changing schema. Semistructured\n318 | Chapter 8: Queries, Modeling, and Transformation\ndata available as a first-class citizen in data warehouses is extremely flexible and\nopens new opportunities for data analysts and data scientists since data is no longer\nconstrained to rows and columns.\nData wrangling\nData wrangling  takes  messy, malformed data and turns it into useful, clean data. This\nis generally a batch transformation process.\nData wrangling has long been a major source of pain and job security for data engi‐\nneers. For example, suppose that developers receive EDI data (see Chapter 7 ) from a\npartner business regarding transactions and invoices, potentially a mix of structured\ndata and text. The typical process of wrangling this data involves first trying to ingest\nit. Often, the data is so malformed that a good deal of text preprocessing is involved.\nDevelopers may choose to ingest the data as a single text field table—an entire row\ningested as a single field. Developers then begin writing queries to parse and break\napart the data. Over time, they discover data anomalies and edge cases. Eventually,\nthey will get the data into rough shape. Only then can the process of downstream\ntransformation begin.\nData wrangling tools aim to simplify significant parts of this process. These tools\noften put off data engineers because they claim to be no code, which sounds unso‐\nphisticated. We prefer to think of data wrangling tools as integrated development\nenvironments (IDEs) for malformed data. In practice, data engineers spend way too\nmuch time parsing nasty data; automation tools allow data engineers to spend time\non more interesting tasks. Wrangling tools may also allow engineers to hand some\nparsing and ingestion work off to analysts.\nGraphical data-wrangling tools typically present a sample of data in a visual interface,\nwith inferred types, statistics including distributions, anomalous data, outliers, and\nnulls. Users can then add processing steps to fix data issues. A step might provide\ninstructions for dealing with mistyped data, splitting a text field into multiple parts,\nor joining with a lookup table.\nUsers can run the steps on a full dataset when the full job is ready. The job typically\ngets pushed to a scalable data processing system such as Spark for large datasets. After\nthe job runs, it will return errors and unhandled exceptions. The user can further\nrefine the recipe to deal with these outliers.\nWe highly recommend that both aspiring and seasoned engineers experiment with\nwrangling tools; major cloud providers sell their version of data-wrangling tools,\nand many third-party options are available. Data engineers may find that these tools\nsignificantly streamline certain parts of their jobs. Organizationally, data engineering\nteams may want to consider training specialists in data wrangling if they frequently\ningest from new, messy data sources.\nTransformations | 319\nExample: Data transformation in Spark\nLet’s  look at a practical, concrete example of data transformation. Suppose we build\na pipeline that ingests data from three API sources in JSON format. This initial\ningestion step is handled in Airflow. Each data source gets its prefix (filepath) in an S3\nbucket.\nAirflow then triggers a Spark job by calling an API. This Spark job ingests each of\nthe three sources into a dataframe, converting the data into a relational format, with\nnesting in certain columns. The Spark job combines the three sources into a single\ntable and then filters the results with a SQL statement. The results are finally written\nout to a Parquet-formatted Delta Lake table stored in S3.\nIn practice, Spark creates a DAG of steps based on the code that we write for\ningesting, joining, and writing out the data. The basic ingestion of data happens in\ncluster memory, although one of the data sources is large enough that it must spill to\ndisk during the ingestion process. (This data gets written to cluster storage; it will be\nreloaded into memory for subsequent processing steps.)\nThe join requires a shuffle operation. A key is used to redistribute data across the\ncluster; once again, a spill to disk occurs as the data is written to each node. The SQL\ntransformation filters through the rows in memory and discards the unused rows.\nFinally, Spark converts the data into Parquet format, compresses it, and writes it back\nto S3. Airflow periodically calls back to Spark to see if the job is completed. Once it\nconfirms that the job has finished, it marks the full Airflow DAG as completed. (Note\nthat we have two DAG constructs here, an Airflow DAG and a DAG specific to the\nSpark job.)\nBusiness logic and derived data\nOne  of the most common use cases for transformation is to render business logic.\nWe’ve placed this discussion under batch transformations because this is where this\ntype of transformation happens most frequently, but note that it could also happen in\na streaming pipeline.\nSuppose that a company uses multiple specialized internal profit calculations. One\nversion might look at profits before marketing costs, and another might look at a\nprofit after subtracting marketing costs. Even though this appears to be a straightfor‐\nward accounting exercise, each of these metrics is highly complex to render.\nProfit before marketing costs might need to account for fraudulent orders; determin‐\ning a reasonable profit estimate for the previous business day entails estimating what\npercentage of revenue and profit will ultimately be lost to orders canceled in the\ncoming days as the fraud team investigates suspicious orders. Is there a special flag in\nthe database that indicates an order with a high probability of fraud, or one that has\n320 | Chapter 8: Queries, Modeling, and Transformation\n16Michael Blaha, “Be Careful with Derived Data, ” Dataversity, December 5, 2016, https://oreil.ly/garoL .\n17Benn Stancil, “The Missing Piece of the Modern Data Stack, ” benn.substack , April 22, 2021,\nhttps://oreil.ly/GYf3Z .been automatically canceled? Does the business assume that a certain percentage of\norders will be canceled because of fraud even before the fraud-risk evaluation process\nhas been completed for specific orders?\nFor profits after marketing costs, we must account for all the complexities of the\nprevious metric, plus the marketing costs attributed to the specific order. Does the\ncompany have a naive attribution model—e.g., marketing costs attributed to items\nweighted by price? Marketing costs might also be attributed per department, or item\ncategory, or—in the most sophisticated organizations—per individual item based on\nuser ad clicks.\nThe business logic transformation that generates this nuanced version of profit must\nintegrate all the subtleties of attribution—i.e., a model that links orders to specific\nads and advertising costs. Is attribution data stored in the guts of ETL scripts, or is it\npulled from a table that is automatically generated from ad platform data?\nThis type of reporting data is a quintessential example of derived data —data compu‐\nted from other data stored in a data system. Derived data critics will point out that\nit is challenging for the ETL to maintain consistency in the derived metrics.16 For\nexample, if the company updates its attribution model, this change may need to be\nmerged into many ETL scripts for reporting. (ETL scripts are notorious for breaking\nthe DRY principle.) Updating these ETL scripts is a manual and labor-intensive pro‐\ncess, involving domain expertise in processing logic and previous changes. Updated\nscripts must also be validated for consistency and accuracy.\nFrom our perspective, these are legitimate criticisms but not necessarily very con‐\nstructive because the alternative to derived data in this instance is equally distasteful.\nAnalysts will need to run their reporting queries if profit data is not stored in the\ndata warehouse, including profit logic. Updating complex ETL scripts to represent\nchanges to business logic accurately is an overwhelming, labor-intensive task, but\ngetting analysts to update their reporting queries consistently is well-nigh impossible.\nOne interesting alternative is to push business logic into a metrics layer ,17 but still\nleverage the data warehouse or other tool to do the computational heavy lifting.\nA metrics layer encodes business logic and allows analysts and dashboard users to\nbuild complex analytics from a library of defined metrics. The metrics layer generates\nqueries from the metrics and sends these to the database. We discuss semantic and\nmetrics layers in more detail in Chapter 9 .\nTransformations | 321\nMapReduce\nNo discussion of batch transformation can be complete without touching on Map‐\nReduce. This isn’t because MapReduce is widely used by data engineers these days.\nMapReduce was the defining batch data transformation pattern of the big data era,\nit still influences many distributed systems data engineers use today, and it’s useful\nfor data engineers to understand at a basic level. MapReduce  was introduced by\nGoogle in a follow-up to its paper on GFS. It was initially the de facto processing\npattern of Hadoop, the open source analogue technology of GFS that we introduced\nin Chapter 6 .\nA simple MapReduce job consists of a collection of map tasks that read individual\ndata blocks scattered across the nodes, followed by a shuffle that redistributes result\ndata across the cluster and a reduce step that aggregates data on each node. For\nexample, suppose that we wanted to run the following SQL query:\nSELECT COUNT(*), user_id\nFROM user_events\nGROUP BY user_id;\nThe table data is spread across nodes in data blocks; the MapReduce job generates\none map task per block. Each map task essentially runs the query on a single block\n—i.e., it generates a count for each user ID that appears in the block. While a\nblock might contain hundreds of megabytes, the full table could be petabytes in size.\nHowever, the map portion of the job is a nearly perfect example of embarrassing\nparallelism; the data scan rate across the full cluster essentially scales linearly with the\nnumber of nodes.\nWe then need to aggregate (reduce) to gather results from the full cluster. We’re\nnot gathering results to a single node; rather, we redistribute results by key so that\neach key ends up on one and only one node. This is the shuffle step, which is often\nexecuted using a hashing algorithm on keys. Once the map results have been shuffled,\nwe sum the results for each key. The key/count pairs can be written to the local disk\non the node where they are computed. We collect the results stored across nodes to\nview the full query results.\nReal-world MapReduce jobs can be far more complex than what we describe here. A\ncomplex query that filters with a WHERE  clause joins three tables and applies a window\nfunction that would consist of many map and reduce stages.\nAfter MapReduce\nGoogle’s  original MapReduce model is extremely powerful but is now viewed as\nexcessively rigid. It utilizes numerous short-lived ephemeral tasks that read from and\nwrite to disk. In particular, no intermediate state is preserved in memory; all data is\ntransferred between tasks by storing it to disk or pushing it over the network. This\n322 | Chapter 8: Queries, Modeling, and Transformation",37180
143-Materialized Views Federation and Query Virtualization.pdf,143-Materialized Views Federation and Query Virtualization,"18“What Is the Difference Between Apache Spark and Hadoop MapReduce?, ” Knowledge Powerhouse Y ouTube\nvideo, May 20, 2017, https://oreil.ly/WN0eX .simplifies state and workflow management and minimizes memory consumption, but\nit can also drive high-disk bandwidth utilization and increase processing time.\nThe MapReduce paradigm was constructed around the idea that magnetic disk\ncapacity and bandwidth were so cheap that it made sense to simply throw a massive\namount of disk at data to realize ultra-fast queries. This worked to an extent; Map‐\nReduce repeatedly set data processing records during the early days of Hadoop.\nHowever, we have lived in a post-MapReduce world for quite some time. Post-\nMapReduce processing does not truly discard MapReduce; it still includes the ele‐\nments of map, shuffle, and reduce, but it relaxes the constraints of MapReduce to\nallow for in-memory caching.18 Recall that RAM is much faster than SSD and HDDs\nin transfer speed and seek time. Persisting even a tiny amount of judiciously chosen\ndata in memory can dramatically speed up specific data processing tasks and utterly\ncrush the performance of MapReduce.\nFor example, Spark, BigQuery, and various other data processing frameworks were\ndesigned around in-memory processing. These frameworks treat data as a distributed\nset that resides in memory. If data overflows available memory, this causes a spill to\ndisk. The disk is treated as a second-class data-storage layer for processing, though it\nis still highly valuable.\nThe cloud is one of the drivers for the broader adoption of memory caching; it\nis much more effective to lease memory during a specific processing job than to\nown it 24 hours a day. Advancements in leveraging memory for transformations will\ncontinue to yield gains for the foreseeable future.\nMaterialized Views, Federation, and Query Virtualization\nIn this section, we look at several techniques that virtualize query results by present‐\ning them as table-like objects. These techniques can become part of a transformation\npipeline or sit right before end-user data consumption.\nViews\nFirst, let’s review views to set the stage for materialized views. A view  is a database\nobject that we can select from just like any other table. In practice, a view is just a\nquery that references other tables. When we select from a view, that database creates\na new query that combines the view subquery with our query. The query optimizer\nthen optimizes and runs the full query.\nTransformations | 323\nViews play a variety of roles in a database. First, views can serve a security role.\nFor example, views can select only specific columns and filter rows, thus providing\nrestricted data access. Various views can be created for job roles depending on user\ndata access.\nSecond, a view might be used to provide a current deduplicated picture of data.\nIf we’re using an insert-only pattern, a view may be used to return a deduplicated\nversion of a table showing only the latest version of each record.\nThird, views can be used to present common data access patterns. Suppose that\nmarketing analysts must frequently run a query that joins five tables. We could create\na view that joins together these five tables into a wide table. Analysts can then write\nqueries that filter and aggregate on top of this view.\nMaterialized views\nWe mentioned materialized views in our earlier discussion of query caching. A\npotential disadvantage of (nonmaterialized) views is that they don’t do any precom‐\nputation. In the example of a view that joins five tables, this join must run every\ntime a marketing analyst runs a query on this view, and the join could be extremely\nexpensive.\nA materialized view does some or all of the view computation in advance. In our\nexample, a materialized view might save the five table join results every time a change\noccurs in the source tables. Then, when a user references the view, they’re querying\nfrom the prejoined data. A materialized view is a de facto transformation step, but the\ndatabase manages execution for convenience.\nMaterialized views may also serve a significant query optimization role depending\non the database, even for queries that don’t directly reference them. Many query\noptimizers can identify queries that “look like” a materialized view. An analyst may\nrun a query that uses a filter that appears in a materialized view. The optimizer will\nrewrite the query to select from the precomputed results.\nComposable materialized views\nIn general, materialized views do not allow for composition—that is, a materialized\nview cannot select from another materialized view. However, we’ve recently seen\nthe emergence of tools that support this capability. For example, Databricks has intro‐\nduced the notion of live tables . Each table is updated as data arrives from sources.\nData flows down to subsequent tables asynchronously.\nFederated queries\nFederated queries  are a database feature that allows an OLAP database to select from\nan external data source, such as object storage or RDBMS. For example, let’s say\n324 | Chapter 8: Queries, Modeling, and Transformation\nyou need to combine data across object storage and various tables in MySQL and\nPostgreSQL databases. Y our data warehouse can issue a federated query to these\nsources and return the combined results ( Figure 8-19 ).\nFigure 8-19. An OLAP database issues a federated query that gets data from object\nstorage, MySQL, and PostgreSQL and returns a query result with the combined data\nAs another example, Snowflake supports the notion of external tables defined on\nS3 buckets. An external data location and a file format are defined when creating\nthe table, but data is not yet ingested into the table. When the external table is\nqueried, Snowflake reads from S3 and processes the data based on the parameters\nset at the time of the table’s creation. We can even join S3 data to internal database\ntables. This makes Snowflake and similar databases more compatible with a data lake\nenvironment.\nSome OLAP systems can convert federated queries into materialized views. This gives\nus much of the performance of a native table without the need to manually ingest\ndata every time the external source changes. The materialized view gets updated\nwhenever the external data changes.\nData virtualization\nData virtualization  is closely related to federated queries, but this typically entails a\ndata processing and query system that doesn’t store data internally. Right now, Trino\n(e.g., Starburst) and Presto are examples par excellence. Any query/processing engine\nthat supports external tables can serve as a data virtualization engine. The most\nsignificant considerations with data virtualization are supported external sources and\nperformance.\nTransformations | 325",6860
144-Streaming Transformations and Processing.pdf,144-Streaming Transformations and Processing,"A closely related concept is the notion of query pushdown . Suppose I wanted to\nquery data from Snowflake, join data from a MySQL database, and filter the results.\nQuery pushdown aims to move as much work as possible to the source databases.\nThe engine might look for ways to push filtering predicates into the queries on the\nsource systems. This serves two purposes: first, it offloads computation from the\nvirtualization layer, taking advantage of the query performance of the source. Second,\nit potentially reduces the quantity of data that must push across the network, a critical\nbottleneck for virtualization performance.\nData virtualization is a good solution for organizations with data stored across vari‐\nous data sources. However, data virtualization should not be used haphazardly. For\nexample, virtualizing a production MySQL database doesn’t solve the core problem of\nanalytics queries adversely impacting the production system—because Trino does not\nstore data internally, it will pull from MySQL every time it runs a query.\nAlternatively, data virtualization can be used as a component of data ingestion and\nprocessing pipelines. For instance, Trino might be used to select from MySQL once\na day at midnight when the load on the production system is low. Results could be\nsaved into S3 for consumption by downstream transformations and daily queries,\nprotecting MySQL from direct analytics queries.\nData virtualization can be viewed as a tool that expands the data lake to many\nmore sources by abstracting away barriers used to silo data between organizational\nunits. An organization can store frequently accessed, transformed data in S3 and\nvirtualize access between various parts of the company. This fits closely with the\nnotion of a data mesh  (discussed in Chapter 3 ), wherein small teams are responsible\nfor preparing their data for analytics and sharing it with the rest of the company;\nvirtualization can serve as a critical access layer for practical sharing.\nStreaming Transformations and Processing\nWe’ve  already discussed stream processing in the context of queries. The difference\nbetween streaming transformations and streaming queries is subtle and warrants\nmore explanation.\nBasics\nStreaming queries run dynamically to present a current view of data, as dis‐\ncussed previously. Streaming transformations  aim to prepare data for downstream\nconsumption.\nFor instance, a data engineering team may have an incoming stream carrying events\nfrom an IoT source. These IoT events carry a device ID and event data. We wish to\ndynamically enrich these events with other device metadata, which is stored in a sep‐\narate database. The stream-processing engine queries a separate database containing\n326 | Chapter 8: Queries, Modeling, and Transformation\n19For a detailed application of the concept of a streaming DAG, see “Why We Moved from Apache Kafka to\nApache Pulsar” by Simba Khadder, StreamNative blog, April 21, 2020, https://oreil.ly/Rxfko .this metadata by device ID, generates new events with the added data, and passes it\non to another stream. Live queries and triggered metrics run on this enriched stream\n(see Figure 8-20 ).\nFigure 8-20. An incoming stream is carried by a streaming event platform and passed\ninto a stream processor\nTransformations and queries are a continuum\nThe line between transformations and queries is also blurry in batch processing, but\nthe differences become even more subtle in the domain of streaming. For example, if\nwe dynamically compute roll-up statistics on windows, and then send the output to a\ntarget stream, is this a transformation or a query?\nMaybe we will eventually adopt new terminology for stream processing that better\nrepresents real-world use cases. For now, we will do our best with the terminology we\nhave.\nStreaming DAGs\nOne interesting notion closely related to stream enrichment and joins is the  streaming\nDAG .19 We first talked about this idea in our discussion of orchestration in Chapter 2 .\nOrchestration is inherently a batch concept, but what if we wanted to enrich, merge,\nand split multiple streams in real time?\nLet’s take a simple example where streaming DAG would be useful. Suppose that we\nwant to combine website clickstream data with IoT data. This will allow us to get a\nunified view of user activity by combining IoT events with clicks. Furthermore, each\ndata stream needs to be preprocessed into a standard format (see Figure 8-21 ).\nTransformations | 327\nFigure 8-21. A simple streaming DAG\nThis has long been possible by combining a streaming store (e.g., Kafka) with a\nstream processor (e.g., Flink). Creating the DAG amounted to building a complex\nRube Goldberg machine, with numerous topics and processing jobs connected.\nPulsar dramatically simplifies this process by treating DAGs as a core streaming\nabstraction. Rather than managing flows across several systems, engineers can define\ntheir streaming DAGs as code inside a single system.\nMicro-batch versus true streaming\nA long-running battle has been ongoing between micro-batch and true streaming\napproaches. Fundamentally, it’s important to understand your use case, the perfor‐\nmance requirements, and the performance capabilities of the framework in question.\nMicro-batching is a way to take a batch-oriented framework and apply it in a stream‐\ning situation. A micro-batch might run anywhere from every two minutes to every\nsecond. Some micro-batch frameworks (e.g., Apache Spark Streaming) are designed\nfor this use case and will perform well with appropriately allocated resources at a\nhigh batch frequency. (In truth, DBAs and engineers have long used micro-batching\nwith more traditional databases; this often led to horrific performance and resource\nconsumption.)\nTrue streaming systems (e.g., Beam and Flink) are designed to process one event at a\ntime. However, this comes with significant overhead. Also, it’s important to note that\neven in these true streaming systems, many processes will still occur in batches. A\n328 | Chapter 8: Queries, Modeling, and Transformation",6132
145-Whom Youll Work With.pdf,145-Whom Youll Work With,,0
146-Downstream Stakeholders.pdf,146-Downstream Stakeholders,"basic enrichment process that adds data to individual events can deliver one event at\na time with low latency. However, a triggered metric on windows may run every few\nseconds, every few minutes, etc.\nWhen you’re using windows and triggers (hence, batch processing), what’s the win‐\ndow frequency? What’s the acceptable latency? If you are collecting Black Friday sales\nmetrics published every few minutes, micro-batches are probably just fine as long as\nyou set an appropriate micro-batch frequency. On the other hand, if your ops team\nis computing metrics every second to detect DDoS attacks, true streaming may be in\norder.\nWhen should you use one over the other? Frankly, there is no universal answer. The\nterm micro-batch  has often been used to dismiss competing technologies, but it may\nwork just fine for your use case and can be superior in many respects depending on\nyour needs. If your team already has expertise in Spark, you will be able to spin up a\nSpark (micro-batch) streaming solution extremely fast.\nThere’s no substitute for domain expertise and real-world testing. Talk to experts\nwho can present an even-handed opinion. Y ou can also easily test the alternatives by\nspinning up tests on cloud infrastructure. Also, watch out for spurious benchmarks\nprovided by vendors. Vendors are notorious for cherry-picking benchmarks and\nsetting up artificial examples that don’t match reality (recall our conversation on\nbenchmarks in Chapter 4 ). Frequently, vendors will show massive advantages in their\nbenchmark results but fail to deliver in the real world for your use case.\nWhom You’ll Work With\nQueries, transformations, and modeling impact all stakeholders up and down the\ndata engineering lifecycle. The data engineer is responsible for several things at this\nstage in the lifecycle. From a technical angle, the data engineer designs, builds, and\nmaintains the integrity of the systems that query and transform data. The data engi‐\nneer also implements data models within this system. This is the most “full-contact”\nstage where your focus is to add as much value as possible, both in terms of function‐\ning systems and reliable and trustworthy data.\nUpstream Stakeholders\nWhen it comes to transformations, upstream stakeholders can be broken into two\nbroad categories: those who control the business definitions and those who control\nthe systems generating data.\nWhen interfacing with upstream stakeholders about business definitions and logic,\nyou’ll need to know the data sources—what they are, how they’re used, and the\nbusiness logic and definitions involved. Y ou’ll work with the engineers in charge of\nthese source systems and the business stakeholders who oversee the complementary\nWhom You’ll Work With | 329",2776
147-Undercurrents.pdf,147-Undercurrents,,0
148-Data Architecture.pdf,148-Data Architecture,"products and apps. A data engineer might work alongside “the business” and techni‐\ncal stakeholders on a data model.\nThe data engineer needs to be involved in designing the data model and later updates\nbecause of changes in business logic or new processes. Transformations are easy\nenough to do; just write a query and plop the results into a table or view. Creating\nthem so they’re both performant and valuable to the business is another matter.\nAlways keep the requirements and expectations of the business top of mind when\ntransforming data.\nThe stakeholders of the upstream systems want to make sure your queries and trans‐\nformations minimally impact their systems. Ensure bidirectional communication\nabout changes to the data models (column and index changes, for example) in source\nsystems, as these can directly impact queries, transformations, and analytical data\nmodels. Data engineers should know about schema changes, including the addition\nor deletion of fields, data type changes, and anything else that might materially\nimpact the ability to query and transform data.\nDownstream Stakeholders\nTransformations are where data starts providing utility to downstream stakeholders.\nY our downstream stakeholders include many people, including data analysts, data\nscientists, ML engineers, and “the business. ” Collaborate with them to ensure the\ndata model and transformations you provide are performant and useful. In terms of\nperformance, queries should execute as quickly as possible in the most cost-effective\nway. What do we mean by useful ? Analysts, data scientists, and ML engineers should\nbe able to query a data source with the confidence the data is of the highest quality\nand completeness and can be integrated into their workflows and data products. The\nbusiness should be able to trust that transformed data is accurate and actionable.\nUndercurrents\nThe transformation stage is where your data mutates and morphs into something\nuseful for the business. Because there are many moving parts, the undercurrents are\nespecially critical at this stage.\nSecurity\nQueries  and transformations combine disparate datasets into new datasets. Who has\naccess to this new dataset? If someone does have access to a dataset, continue to\ncontrol who has access to a dataset’s column, row, and cell-level access.\nBe aware of attack vectors against your database at query time. Read/write privileges\nto the database must be tightly monitored and controlled. Query access to the\n330 | Chapter 8: Queries, Modeling, and Transformation\ndatabase  must be controlled in the same way as you normally control access to your\norganization’s systems and environments.\nKeep credentials hidden; avoid copying and pasting passwords, access tokens, or\nother credentials into code or unencrypted files. It’s shockingly common to see code\nin GitHub repositories with database usernames and passwords pasted directly in\nthe codebase! It goes without saying, don’t share passwords with other users. Finally,\nnever allow unsecured or unencrypted data to traverse the public internet.\nData Management\nThough data management is essential at the source system stage (and every other\nstage of the data engineering lifecycle), it’s especially critical at the transformation\nstage. Transformation inherently creates new datasets that need to be managed. As\nwith other stages of the data engineering lifecycle, it’s critical to involve all stake‐\nholders in data models and transformations and manage their expectations. Also,\nmake sure everyone agrees on naming conventions that align with the respective\nbusiness definitions of the data. Proper naming conventions should be reflected in\neasy-to-understand field names. Users can also check in a data catalog for more\nclarity on what the field means when it was created, who maintains the dataset, and\nother relevant information.\nAccounting for definitional accuracy is key at the transformation stage. Does the\ntransformation adhere to the expected business logic? Increasingly, the notion of\na semantic or metrics layer that sits independent of transformations is becoming\npopular. Instead of enforcing business logic within the transformation at runtime,\nwhy not keep these definitions as a standalone stage before your transformation\nlayer? While it’s still early days, expect to see semantic and metrics layers becoming\nmore popular and commonplace in data engineering and data management.\nBecause transformations involve mutating data, it’s critical to ensure that the data\nyou’re using is free of defects and represents ground truth. If MDM is an option at\nyour company, pursue its implementation. Conformed dimensions and other trans‐\nformations rely on MDM to preserve data’s original integrity and ground truth. If\nMDM isn’t possible, work with upstream stakeholders who control the data to ensure\nthat any data you’re transforming is correct and complies with the agreed-upon\nbusiness logic.\nData transformations make it potentially difficult to know how a dataset was derived\nalong the same lines. In Chapter 6 , we discussed data catalogs. As we transform data,\ndata lineage  tools become invaluable. Data lineage tools help both data engineers,\nwho must understand previous transformation steps as they create new transforma‐\ntions, and analysts, who need to understand where data came from as they run\nqueries and build reports.\nUndercurrents | 331\nFinally, what impact does regulatory compliance have on your data model and trans‐\nformations? Are sensitive fields data masked or obfuscated if necessary? Do you have\nthe ability to delete data in response to deletion requests? Does your data lineage\ntracking allow you to see data derived from deleted data and rerun transformations to\nremove data downstream of raw sources?\nDataOps\nWith  queries and transformations, DataOps has two areas of concern: data and\nsystems. Y ou need to monitor and be alerted for changes or anomalies in these\nareas. The field of data observability is exploding right now, with a big focus on\ndata reliability. There’s even a recent job title called data reliability engineer . This\nsection emphasizes data observability and data health, which focuses on the query\nand transformation stage.\nLet’s start with the data side of DataOps. When you query data, are the inputs and\noutputs correct? How do you know? If this query is saved to a table, is the schema\ncorrect? How about the shape of the data and related statistics such as min/max\nvalues, null counts, and more? Y ou should run data-quality tests on the input datasets\nand the transformed dataset, which will ensure that the data meets the expectations of\nupstream and downstream users. If there’s a data-quality issue in the transformation,\nyou should have the ability to flag this issue, roll back the changes, and investigate the\nroot cause.\nNow let’s look at the Ops part of DataOps. How are the systems performing? Monitor\nmetrics such as query queue length, query concurrency, memory usage, storage\nutilization, network latency, and disk I/O. Use metric data to spot bottlenecks and\npoor-performing queries that might be candidates for refactoring and tuning. If the\nquery is perfectly fine, you’ll have a good idea of where to tune the database itself\n(for instance, by clustering a table for faster lookup performance). Or, you may need\nto upgrade the database’s compute resources. Today’s cloud and SaaS databases give\nyou a ton of flexibility for quickly upgrading (and downgrading) your system. Take\na data-driven approach and use your observability metrics to pinpoint whether you\nhave a query or a systems-related issue.\nThe shift toward SaaS-based analytical databases changes the cost profile of data\nconsumption. In the days of on-premises data warehouses, the system and licenses\nwere purchased up front, with no additional usage cost. Whereas traditional data\nengineers would focus on performance optimization to squeeze the maximum utility\nout of their expensive purchases, data engineers working with cloud data warehouses\nthat charge on a consumption basis need to focus on cost management and cost\noptimization. This is the practice of FinOps  (see Chapter 4 ).\n332 | Chapter 8: Queries, Modeling, and Transformation",8331
149-Orchestration.pdf,149-Orchestration,,0
150-Additional Resources.pdf,150-Additional Resources,"Data Architecture\nThe general rules of good data architecture in Chapter 3  apply to the transformation\nstage. Build robust systems that can process and transform data without imploding.\nY our choices for ingestion and storage will directly impact your general architecture’s\nability to perform reliable queries and transformations. If the ingestion and storage\nare appropriate to your query and transformation patterns, you should be in a great\nplace. On the other hand, if your queries and transformations don’t work well with\nyour upstream systems, you’re in for a world of pain.\nFor example, we often see data teams using the wrong data pipelines and databases\nfor the job. A data team might connect a real-time data pipeline to an RDBMS or\nElasticsearch and use this as their data warehouse. These systems are not optimized\nfor high-volume aggregated OLAP queries and will implode under this workload.\nThis data team clearly didn’t understand how their architectural choices would\nimpact query performance. Take the time to understand the trade-offs inherent\nin your architecture choices; be clear about how your data model will work with\ningestion and storage systems and how queries will perform.\nOrchestration\nData teams often manage their transformation pipelines using simple time-based\nschedules—e.g., cron jobs. This works reasonably well at first but turns into a night‐\nmare as workflows grow more complicated. Use orchestration to manage complex\npipelines using a dependency-based approach. Orchestration is also the glue that\nallows us to assemble pipelines that span multiple systems.\nSoftware Engineering\nWhen  writing transformation code, you can use many languages—such as SQL,\nPython, and JVM-based languages—platforms ranging from data warehouses to dis‐\ntributed computing clusters, and everything in between. Each language and platform\nhas its strengths and quirks, so you should know the best practices of your tools. For\nexample, you might write data transformations in Python, powered by a distributed\nsystem such as Spark or Dask. When running a data transformation, are you using a\nUDF when a native function might work much better? We’ve seen cases where poorly\nwritten, sluggish UDFs were replaced by a built-in SQL command, with instant and\ndramatic improvement in performance.\nThe rise of analytics engineering brings software engineering practices to end users,\nwith the notion of analytics as code . Analytics engineering transformation tools like\ndbt have exploded in popularity, giving analysts and data scientists the ability to write\nin-database transformations using SQL, without the direct intervention of a DBA or\na data engineer. In this case, the data engineer is responsible for setting up the code\nUndercurrents | 333\nrepository and CI/CD pipeline used by the analysts and data scientists. This is a big\nchange in the role of a data engineer, who would historically build and manage the\nunderlying infrastructure and create the data transformations. As data tools lower\nthe barriers to entry and become more democratized across data teams, it will be\ninteresting to see how the workflows of data teams change.\nUsing a GUI-based low-code tool, you’ll get useful visualizations of the transforma‐\ntion workflow. Y ou still need to understand what’s going on under the hood. These\nGUI-based transformation tools will often generate SQL or some other language\nbehind the scenes. While the point of a low-code tool is to alleviate the need to\nbe involved in low-level details, understanding the code behind the scenes will help\nwith debugging and performance optimization. Blindly assuming that the tool is\ngenerating performant code is a mistake.\nWe suggest that data engineers pay particular attention to software engineering best\npractices at the query and transformation stage. While it’s tempting to simply throw\nmore processing resources at a dataset, knowing how to write clean, performant code\nis a much better approach.\nConclusion\nTransformations sit at the heart of data pipelines. It’s critical to keep in mind the\npurpose of transformations. Ultimately, engineers are not hired to play with the latest\ntechnological toys but to serve their customers. Transformations are where data adds\nvalue and ROI to the business.\nOur opinion is that it is possible to adopt exciting transformation technologies and\nserve stakeholders. Chapter 11  talks about the live data stack , essentially reconfigur‐\ning the data stack around streaming data ingestion and bringing transformation\nworkflows closer to the source system applications themselves. Engineering teams\nthat think about real-time data as the technology for the sake of technology will\nrepeat the mistakes of the big data era. But in reality, the majority of organizations\nthat we work with have a business use case that would benefit from streaming data.\nIdentifying these use cases and focusing on the value before choosing technologies\nand complex systems is key.\nAs we head into the serving stage of the data engineering lifecycle in Chapter 9 ,\nreflect on technology as a tool for realizing organizational goals. If you’re a working\ndata engineer, think about how improvements in transformation systems could help\nyou to serve your end customers better. If you’re just embarking on a path toward\ndata engineering, think about the kinds of business problems you’re interested in\nsolving with technology.\n334 | Chapter 8: Queries, Modeling, and Transformation\nAdditional Resources\n•“Building a Real-Time Data Vault in Snowflake”  by Dmytro Y aroshenko and Kent •\nGraziano\n•Building a Scalable Data Warehouse with Data Vault 2.0  (Morgan Kaufmann) by •\nDaniel Linstedt and Michael Olschimke\n•Building the Data Warehouse  (Wiley), Corporate Information Factory , and The •\nUnified  Star Schema  (Technics Publications) by W . H. (Bill) Inmon\n•“Caching in Snowflake Data Warehouse” Snowflake Community page•\n•“Data Warehouse: The Choice of Inmon vs. Kimball”  by Ian Abramson •\n•The Data Warehouse Toolkit  by Ralph Kimball and Margy Ross (Wiley) •\n•“Data Vault—An Overview”  by John Ryan •\n•“Data Vault 2.0 Modeling Basics”  by Kent Graziano •\n•“ A Detailed Guide on SQL Query Optimization” tutorial  by Megha •\n•“Difference Between Kimball and Inmon”  by manmeetjuneja5 •\n•“Eventual vs. Strong Consistency in Distributed Databases”  by Saurabh.v •\n•“The Evolution of the Corporate Information Factory”  by Bill Inmon •\n•Gavroshe USA ’s “DW 2.0” web page •\n•Google Cloud’s “Using Cached Query Results” documentation •\n•Holistics’ “Cannot Combine Fields Due to Fan-Out Issues?” FAQ page •\n•“How a SQL Database Engine Works, ”  by Dennis Pham •\n•“How Should Organizations Structure Their Data?”  by Michael Berk •\n•“Inmon or Kimball: Which Approach Is Suitable for Y our Data Warehouse?”  by •\nSansu George\n•“Introduction to Data Vault Modeling” document,  compiled by Kent Graziano •\nand Dan Linstedt\n•“Introduction to Data Warehousing” , “Introduction to Dimensional Modelling •\nfor Data Warehousing” , and “Introduction to Data Vault for Data Warehousing”\nby Simon Kitching\n•Kimball Group’s “Four-Step Dimensional Design Process” , “Conformed Dimen‐ •\nsions” , and “Dimensional Modeling Techniques”  web pages\n•“Kimball vs. Inmon vs. Vault” Reddit thread•\n•“Modeling of Real-Time Streaming Data?” Stack Exchange thread•\n•“The New ‘Unified Star Schema’ Paradigm in Analytics Data Modeling Review”•\nby Andriy Zabavskyy\nAdditional Resources | 335\n•Oracle’s “Slowly Changing Dimensions” tutorial •\n•ScienceDirect’s “Corporate Information Factory” web page •\n•“ A Simple Explanation of Symmetric Aggregates or ‘Why on Earth Does My SQL•\nLook Like That?’”  by Lloyd Tabb\n•“Streaming Event Modeling”  by Paul Stanton •\n•“Types of Data Warehousing Architecture”  by Amritha Fernando •\n•US patent for “Method and Apparatus for Functional Integration of Metadata”•\n•Zentut’s “Bill Inmon Data Warehouse” web page •\n336 | Chapter 8: Queries, Modeling, and Transformation",8080
151-General Considerations for Serving Data.pdf,151-General Considerations for Serving Data,"CHAPTER 9\nServing Data for Analytics, Machine\nLearning, and Reverse ETL\nCongratulations! Y ou’ve reached the final stage of the data engineering lifecycle—\nserving data for downstream use cases (see Figure 9-1 ). In this chapter, you’ll learn\nabout various ways to serve data for three major use cases you’ll encounter as a data\nengineer. First, you’ll serve data for analytics and BI. Y ou’ll prepare data for use in\nstatistical analysis, reporting, and dashboards. This is the most traditional area of\ndata serving. Arguably, it predates IT and databases, but it is as important as ever\nfor stakeholders to have visibility into the business, organizational, and financial\nprocesses.\nFigure 9-1. Serving delivers data for use cases\n337",748
152-Analytics.pdf,152-Analytics,"1Quoted in Benjamin Snyder, “7 Insights from Legendary Investor Warren Buffett, ” CNBC Make It , May 1,\n2017, https://oreil.ly/QEqF9 .Second, you’ll serve data for ML applications. ML is not possible without high-quality\ndata, appropriately prepared. Data engineers work with data scientists and ML engi‐\nneers to acquire, transform, and deliver the data necessary for model training.\nThird, you’ll serve data through reverse ETL. Reverse ETL  is the process of sending\ndata back to data sources. For example, we might acquire data from an ad tech\nplatform, run a statistical process on this data to determine cost-per-click bids, and\nthen feed this data back into the ad tech platform. Reverse ETL is highly entangled\nwith BI and ML.\nBefore we get into these three major ways of serving data, let’s look at some general\nconsiderations.\nGeneral Considerations for Serving Data\nBefore  we get further into serving data, we have a few big considerations. First and\nforemost is trust. People need to trust the data you’re providing. Additionally, you\nneed to understand your use cases and users, the data products that will be produced,\nhow you’ll be serving data (self-service or not), data definitions and logic, and data\nmesh. The considerations we’ll discuss here are general and apply to any of the three\nways of serving data. Understanding these considerations will help you be much\nmore effective in serving your data customers.\nTrust\nIt takes 20 years to build a reputation and five minutes to ruin it. If you think about\nthat, you’ll do things differently.\n—Warren Buffett1\nAbove  all else, trust is the root consideration in serving data; end users need to trust\nthe data they’re receiving. The fanciest, most sophisticated data architecture and serv‐\ning layer are irrelevant if end users don’t believe the data is a reliable representation of\ntheir business. A loss of trust is often a silent death knell for a data project, even if the\nproject isn’t officially canceled until months or years later. The job of a data engineer\nis to serve the best data possible, so you’ll want to make sure your data products\nalways contain high-quality and trustworthy data.\nAs you learn to serve data throughout this chapter, we’ll reinforce the idea of baking\ntrust into your data and discuss pragmatic ways to accomplish this. We see too many\ncases in which data teams are fixated on pushing out data without asking whether\nstakeholders trust it in the first place. Often, stakeholders lose trust in the data. Once\n338 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL\ntrust is gone, earning it back is insanely difficult. This inevitably leads to the business\nnot performing to its fullest potential with data and data teams losing credibility (and\npossibly being dissolved).\nTo realize data quality and build stakeholder trust, utilize data validation and data\nobservability processes, in conjunction with visually inspecting and confirming valid‐\nity with stakeholders. Data validation  is analyzing data to ensure that it accurately\nrepresents financial information, customer interactions, and sales. Data observability\nprovides an ongoing view of data and data processes. These processes must be applied\nthroughout the data engineering lifecycle  to realize a good result as we reach the end.\nWe’ll discuss these further in “Undercurrents” on page 360 .\nIn addition to building trust in data quality, it is incumbent on engineers to build\ntrust in their SLAs and SLOs with their end users and upstream stakeholders. Once\nusers come to depend on data to accomplish business processes, they will require\nthat data is consistently available and up-to-date per the commitments made by data\nengineers. High-quality data is of little value if it’s not available as expected when it’s\ntime to make a critical business decision. Note, the SLAs and SLOs may also take the\nform of data contracts  (see Chapter 5 ), formally or informally.\nWe talked about SLAs in Chapter 5 , but discussing them again here is worthwhile.\nSLAs come in a variety of forms. Regardless of its form, an SLA tells users what to\nexpect from your data product; it is a contract between you and your stakeholders.\nAn example of an SLA might be, “Data will be reliably available and of high quality. ”\nAn SLO is a key part of an SLA and describes the ways you’ll measure performance\nagainst what you’ve agreed to. For example, given the preceding example SLA, an\nSLO might be, “Our data pipelines to your dashboard or ML workflow will have 99%\nuptime, with 95% of data free of defects. ” Be sure expectations are clear and you have\nthe ability to verify you’re operating within your agreed SLA and SLO parameters.\nIt’s not enough to simply agree on an SLA. Ongoing communication is a central\nfeature of a good SLA. Have you communicated possible issues that might affect your\nSLA or SLO expectations? What’s your process for remediation and improvement?\nTrust is everything. It takes a long time to earn, and it’s easy to lose.\nWhat’s the Use Case, and Who’s the User?\nThe serving stage is about data in action. But what is a productive  use of data? Y ou\nneed to consider two things to answer this question: what’s the use case, and who’s the\nuser?\nThe use case for data goes well beyond viewing reports and dashboards. Data is at\nits best when it leads to action . Will an executive make a strategic decision from\na report? Will a user of a mobile food delivery app receive a coupon that entices\nthem to purchase in the next two minutes? The data is often used in more than one\nGeneral Considerations for Serving Data | 339\n2D. J. Patil, “Data Jujitsu: The Art of Turning Data into Product, ” O’Reilly Radar , July 17, 2012,\nhttps://oreil.ly/IYS9x .use case—e.g., to train an ML model that does lead scoring and populates a CRM\n(reverse ETL). High-quality, high-impact data will inherently attract many interesting\nuse cases. But in seeking use cases, always ask, “What action  will this data trigger, and\nwho will be performing this action?, ” with the appropriate follow-up question, “Can\nthis action be automated?”\nWhenever possible, prioritize use cases with the highest possible ROI. Data engineers\nlove to obsess over the technical implementation details of the systems they build\nwhile ignoring the basic question of purpose. Engineers want to do what they do best:\nengineer things. When engineers recognize the need to focus on value and use cases,\nthey become much more valuable and effective in their roles.\nWhen starting a new data project, working backward is helpful. While it’s tempting\nto focus on tools, we encourage you to start with the use case and the users. Here are\nsome questions to ask yourself as you get started:\n•Who will use the data, and how will they use it?•\n•What do stakeholders expect?•\n•How can I collaborate with data stakeholders (e.g., data scientists, analysts, busi‐•\nness users) to understand how the data I’m working with will be used?\nAgain, always approach data engineering from the perspective of the user and their\nuse case. By understanding their expectations and goals, you can work backward to\ncreate amazing data products more easily. Let’s take a moment to expand on our\ndiscussion of a data product.\nData Products\nA good definition of a data product is a product that facilitates an end goal through the\nuse of data.\n—D. J. Patil2\nData products aren’t created in a vacuum. Like so many other organizational pro‐\ncesses that we’ve discussed, making data products is a full-contact sport, involving\na mix of product and business alongside technology. It’s important to involve key\nstakeholders in developing a data product. In most companies, a data engineer is a\ncouple of steps removed from the end users of a data product; a good data engineer\nwill seek to fully understand outcomes for direct users such as data analysts and data\nscientists or customers external to the company.\n340 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL\n3Clayton M. Christensen et al., “Know Y our Customers’ ‘Jobs to Be Done, ’” Harvard Business Review , Septem‐\nber 2016, https://oreil.ly/3uU4j .When creating a data product, it’s useful to think of the “jobs to be done. ”3 A user\n“hires” a product for a “job to be done. ” This means you need to know what the user\nwants—i.e., their motivation for “hiring” your product. A classic engineering mistake\nis simply building without understanding the requirements, needs of the end user,\nor product/market fit. This disaster happens when you build data products nobody\nwants to use.\nA good data product has positive feedback loops. More usage of a data product\ngenerates more useful data, which is used to improve the data product. Rinse and\nrepeat.\nWhen building a data product, keep these considerations in mind:\n•When someone uses the data product, what do they hope to accomplish? All•\ntoo often, data products are made without a clear understanding of the outcome\nexpected by the user.\n•Will the data product serve internal or external users? In Chapter 2 , we discussed •\ninternal- and external-facing data engineering. When creating a data product,\nknowing whether your customer is internal or external facing will impact the way\ndata is served.\n•What are the outcomes and ROI of the data product you’re building?•\nBuilding data products that people will use and love is critical. Nothing will ruin the\nadoption of a data product more than unwanted utility and loss of trust in the data\noutputs. Pay attention to the adoption and usage of data products, and be willing to\nadjust to make users happy.\nSelf-Service or Not?\nHow  will users interface with your data product? Will a business director request a\nreport from the data team, or can this director simply build the report? Self-service\ndata products—giving the user the ability to build data products on their own—have\nbeen a common aspiration of data users for many years. What’s better than just giving\nthe end user the ability to directly create reports, analyses, and ML models?\nToday, self-service BI and data science is still mostly aspirational. While we occasion‐\nally see companies successfully doing self-service with data, this is rare. Most of\nthe time, attempts at self-service data begin with great intentions but ultimately fail;\nself-service data is tough to implement in practice. Thus, the analyst or data scientist\nGeneral Considerations for Serving Data | 341\nis left to perform the heavy lifting of providing ad hoc reports and maintaining\ndashboards.\nWhy is self-service data so hard? The answer is nuanced, but it generally involves\nunderstanding the end user. If the user is an executive who needs to understand\nhow the business is doing, that person probably just wants a predefined dashboard\nof clear and actionable metrics. The executive will likely ignore any self-serve tools\nfor creating custom data views. If reports provoke further questions, they might have\nanalysts at their disposal to pursue a deeper investigation. On the other hand, a user\nwho is an analyst might already be pursuing self-service analytics via more powerful\ntools such as SQL. Self-service analytics through a BI layer is not useful. The same\nconsiderations apply to data science. Although granting self-service ML to “citizen\ndata scientists” has been a goal of many automated ML vendors, adoption is still\nnascent for the same reasons as self-service analytics. In these two extreme cases, a\nself-service data product is a wrong tool for the job.\nSuccessful self-service data projects boil down to having the right audience. Identify\nthe self-service users and the “job” they want to do. What are they trying to accom‐\nplish by using a self-service data product versus partnering with a data analyst to\nget the job done? A group of executives with a background in data forms an ideal\naudience for self-service; they likely want to slice and dice data themselves without\nneeding to dust off their languishing SQL skills. Business leaders willing to invest the\ntime to learn data skills through a company initiative and training program could also\nrealize significant value from self-service.\nDetermine how you will provide data to this group. What are their time requirements\nfor new data? What happens if they inevitably want more data or change the scope\nof what’s required from self-service? More data often means more questions, which\nrequires more data. Y ou’ll need to anticipate the growing needs of your self-service\nusers. Y ou also need to understand the fine balance between flexibility and guardrails\nthat will help your audience find value and insights without incorrect results and\nconfusion.\nData Definitions  and Logic\nAs we’ve emphatically discussed, the utility of data in an organization is ultimately\nderived from its correctness and trustworthiness. Critically, the correctness of data\ngoes beyond faithful reproduction of event values from source systems. Data correct‐\nness also encompasses proper data definitions and logic; these must be baked into\ndata through all lifecycle stages, from source systems to data pipelines to BI tools and\nmuch more.\nData definition  refers to the meaning of data as it is understood throughout the orga‐\nnization. For example, customer  has a precise meaning within a company and across\n342 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL\ndepartments. When the definition of a customer varies, these must be documented\nand made available to everyone who uses the data.\nData logic  stipulates  formulas for deriving metrics from data—say, gross sales or\ncustomer lifetime value. Proper data logic must encode data definitions and details\nof statistical calculations. To compute customer churn metrics, we would need a\ndefinition: who is a customer? To calculate net profits, we would need a set of logical\nrules to determine which expenses to deduct from gross revenue.\nFrequently, we see data definitions and logic taken for granted, often passed around\nthe organization in the form of institutional knowledge. Institutional knowledge  takes\non a life of its own, often at the expense of anecdotes replacing data-driven insights,\ndecisions, and actions. Instead, formally declaring data definitions and logic both in a\ndata catalog and within the systems of the data engineering lifecycle goes a long way\nto ensuring data correctness, consistency, and trustworthiness.\nData definitions can be served in many ways, sometimes explicitly, but mostly implic‐\nitly. By implicit , we mean that anytime you serve data for a query, a dashboard, or\nan ML model, the data and derived metrics are presented consistently and correctly.\nWhen you write a SQL query, you’re implicitly assuming that the inputs to this query\nare correct, including upstream pipeline logic and definitions. This is where data\nmodeling (described in Chapter 8 ) is incredibly useful to capture data definitions and\nlogic in a way that’s understandable and usable by multiple end users.\nUsing a semantic layer, you consolidate business definitions and logic in a reusable\nfashion. Write once, use anywhere. This paradigm is an object-oriented approach\nto metrics, calculations, and logic. We’ll have more to say in “Semantic and Metrics\nLayers” on page 355 .\nData Mesh\nData mesh will increasingly be a consideration when serving data. Data mesh funda‐\nmentally changes the way data is served within an organization. Instead of siloed data\nteams serving their internal constituents, every domain team takes on two aspects of\ndecentralized, peer-to-peer data serving.\nFirst, teams are responsible for serving data to other teams  by preparing it for con‐\nsumption. Data must be good for use in data apps, dashboards, analytics, and BI\ntools across the organization. Second, each team potentially runs its dashboards and\nanalytics for self-service . Teams consume data from across the organization based\non the particular needs in their domain. Data consumed from other teams may\nalso make its way into the software designed by a domain team through embedded\nanalytics or an ML feature.\nGeneral Considerations for Serving Data | 343",16363
153-Business Analytics.pdf,153-Business Analytics,"This dramatically changes the details and structure of serving. We introduced the\nconcept of a data mesh in Chapter 3 . Now that we’ve covered some general considera‐\ntions for serving data, let’s look at the first major area: analytics.\nAnalytics\nThe first data-serving use case you’ll likely encounter is analytics , which is discover‐\ning, exploring, identifying, and making visible key insights and patterns within data.\nAnalytics has many aspects. As a practice, analytics is carried out using statistical\nmethods, reporting, BI tools, and more. As a data engineer, knowing the various\ntypes and techniques of analytics is key to accomplishing your work. This section\naims to show how you’ll serve data for analytics and presents some points to think\nabout to help your analysts succeed.\nBefore you even serve data for analytics, the first thing you need to do (which should\nsound familiar after reading the preceding section) is identify the end use case. Is\nthe user looking at historical trends? Should users be immediately and automatically\nnotified of an anomaly, such as a fraud alert? Is someone consuming a real-time\ndashboard on a mobile application? These examples highlight the differences between\nbusiness analytics (usually BI), operational analytics, and embedded analytics. Each\nof these analytics categories has different goals and unique serving requirements. Let’s\nlook at how you’ll serve data for these types of analytics.\nBusiness Analytics\nBusiness analytics  uses historical and current data to make strategic and actionable\ndecisions. The types of decisions tend to factor in longer-term trends and often\ninvolve a mix of statistical and trend analysis, alongside domain expertise and human\njudgment. Business analysis is as much an art as it is a science.\nBusiness analytics typically falls into a few big areas—dashboards, reports, and ad hoc\nanalysis. A business analyst might focus on one or all of these categories. Let’s quickly\nlook at the differences between these practices and related tools. Understanding an\nanalyst’s workflow will help you, the data engineer, understand how to serve data.\nA dashboard  concisely  shows decision makers how an organization is performing\nagainst a handful of core metrics, such as sales and customer retention. These core\nmetrics are presented as visualizations (e.g., charts or heatmaps), summary statistics,\nor even a single number. This is similar to a car dashboard, which gives you a\nsingle readout of the critical things you need to know while driving a vehicle. An\norganization may have more than one dashboard, with C-level executives using\nan overarching dashboard and their direct reports using dashboards with their par‐\nticular metrics, KPIs, or objectives and key results (OKRs). Analysts help create\nand maintain these dashboards. Once business stakeholders embrace and rely on a\n344 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL\ndashboard, the analyst usually responds to requests to look into a potential issue\nwith a metric or add a new metric to the dashboard. Currently, you might use BI\nplatforms to create dashboards, such as Tableau, Looker, Sisense, Power BI, or Apache\nSuperset/Preset.\nAnalysts are often tasked by business stakeholders with creating a report . The goal\nof a report is to use data to drive insights and action. An analyst working at an\nonline retail company is asked to investigate which factors are driving a higher-than-\nexpected rate of returns for women’s running shorts. The analyst runs some SQL\nqueries in the data warehouse, aggregates the return codes that customers provide\nas the reason for their return, and discovers that the fabric in the running shorts\nis of inferior quality, often wearing out within a few uses. Stakeholders such as\nmanufacturing and quality control are notified of these findings. Furthermore, the\nfindings are summarized in a report and distributed in the same BI tool where the\ndashboard resides.\nThe analyst was asked to dig into a potential issue and come back with insights. This\nrepresents an example of ad hoc analysis . Reports typically start as ad hoc requests.\nIf the results of the ad hoc analysis are impactful, they often end up in a report\nor dashboard. The technologies used for reports and ad hoc analysis are similar to\ndashboards but may include Excel, Python, R-based notebooks, SQL queries, and\nmuch more.\nGood analysts constantly engage with the business and dive into the data to answer\nquestions and uncover hidden and counterintuitive trends and insights. They also\nwork with data engineers to provide feedback on data quality, reliability issues,\nand requests for new datasets. The data engineer is responsible for addressing this\nfeedback and providing new datasets for the analyst to use.\nReturning to the running shorts example, suppose that after communicating their\nfindings, analysts learn that manufacturing can provide them with various supply-\nchain details regarding the materials used in the running shorts. Data engineers\nundertake a project to ingest this data into the data warehouse. Once the supply-\nchain data is present, analysts can correlate specific garment serial numbers with the\nsupplier of the fabric used in the item. They discover that most failures are tied to one\nof their three suppliers, and the factory stops using fabric from this supplier.\nThe data for business analytics is frequently served in batch mode from a data\nwarehouse or a data lake. This varies wildly across companies, departments, and\neven data teams within companies. New data might be available every second, every\nminute, every 30 minutes, every day, or once a week. The frequency of the batches\ncan vary for several reasons. One key thing to note is that engineers working on ana‐\nlytics problems should consider various potential applications of data—current and\nfuture. It is common to have mixed data update frequencies to serve use cases appro‐\npriately but remember that the frequency of ingestion sets a ceiling on downstream\nAnalytics | 345",6131
154-Operational Analytics.pdf,154-Operational Analytics,"frequency. If streaming applications exist for the data, it should be ingested as a\nstream even if some downstream processing and serving steps are handled in batches.\nOf course, data engineers must address various backend technical considerations in\nserving business analytics. Some BI tools store data in an internal storage layer. Other\ntools run queries on your data lake or data warehouse. This is advantageous because\nyou can take full advantage of your OLAP database’s power. As we’ve discussed in\nearlier chapters, the downside is cost, access control, and latency.\nOperational Analytics\nIf business analytics is about using data to discover actionable insights, then opera‐\ntional analytics uses data to take immediate action :\nOperational analytics versus business analytics =\nimmediate action versus actionable insights\nThe big difference between operational and business analytics is time . Data used in\nbusiness analytics takes a longer view of the question under consideration. Up-to-the-\nsecond updates are nice to know but won’t materially impact the quality or outcome.\nOperational analytics is quite the opposite, as real-time updates can be impactful in\naddressing a problem when it occurs.\nAn example of operational analytics is real-time application monitoring. Many soft‐\nware engineering teams want to know how their application is performing; if issues\narise, they want to be notified immediately. The engineering team might have a\ndashboard (see, e.g., Figure 9-2 ) that shows the key metrics such as requests per sec‐\nond, database I/O, or whatever metrics are important. Certain conditions can trigger\nscaling events, adding more capacity if servers are overloaded. If certain thresholds\nare breached, the monitoring system might also send alerts via text message, group\nchat, and email.\nBusiness and Operational Analytics\nThe line between business and operational analytics has begun to blur. As streaming\nand low-latency data become more pervasive, it is only natural to apply operational\napproaches to business analytics problems; in addition to monitoring website per‐\nformance on Black Friday, an online retailer could also analyze and present sales,\nrevenue, and the impact of advertising campaigns in real time.\nThe data architectures will change to fit into a world where you can have both your\nred hot and warm data in one place. The central question you should always ask\nyourself, and your stakeholders, is this: if you have streaming data, what are you\n346 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL\ngoing to do with it? What action should you take? Correct action creates impact and\nvalue. Real-time data without action is an unrelenting distraction.\nIn the long term, we predict that streaming will supplant batch. Data products over\nthe next 10 years will likely be streaming-first, with the ability to seamlessly blend\nhistorical data. After real-time collection, data can still be consumed and processed in\nbatches as required.\nFigure 9-2. An operational analytics dashboard showing some key metrics from Google\nCompute Engine\nLet’s return once again to our running shorts example. Using analytics to discover\nbad fabric in the supply chain was a huge success; business leaders and data engineers\nwant to find more opportunities to utilize data to improve product quality. The data\nengineers suggest deploying real-time analytics at the factory. The plant already uses\nAnalytics | 347",3507
155-Ways to Serve Data for Analytics and ML.pdf,155-Ways to Serve Data for Analytics and ML,"a variety of machines capable of streaming real-time data. In addition, the plant has\ncameras recording video on the manufacturing line. Right now, technicians watch the\nfootage in real time, look for defective items, and alert those running the line when\nthey see a high rate of snags appearing in items.\nData engineers realize that they can use an off-the-shelf cloud machine vision tool to\nidentify defects in real time automatically. Defect data is tied to specific item serial\nnumbers and streamed. From here, a real-time analytics process can tie defective\nitems to streaming events from machines further up the assembly line.\nUsing this approach, factory floor analysts discover that the quality of raw fabric\nstock varies significantly from box to box. When the monitoring system shows a high\nrate of snag defects, line workers can remove the defective box and charge it back to\nthe supplier.\nSeeing the success of this quality improvement project, the supplier decides to adopt\nsimilar quality-control processes. Data engineers from the retailer work with the\nsupplier to deploy their real-time data analytics, dramatically improving the quality of\ntheir fabric stock.\nEmbedded Analytics\nWhereas  business and operational analytics are internally focused, a recent trend is\nexternal-facing or embedded analytics. With so much data powering applications,\ncompanies increasingly provide analytics to end users. These are typically referred to\nas data applications , often with analytics dashboards embedded within the application\nitself. Also known as embedded analytics , these end-user-facing dashboards give users\nkey metrics about their relationship with the application.\nA smart thermostat has a mobile application that shows the temperature in real\ntime and up-to-date power consumption metrics, allowing the user to create a better\nenergy-efficient heating or cooling schedule. In another example, a third-party ecom‐\nmerce platform provides its sellers a real-time dashboard on sales, inventory, and\nreturns. The seller has the option to use this information to offer deals to customers\nin near real time. In both cases, an application allows users to make real-time deci‐\nsions (manually or automatically) based on data.\nThe landscape of embedded analytics is snowballing, and we expect that such data\napplications will become increasingly pervasive within the next few years. As a data\nengineer, you’re probably not creating the embedded analytics frontend, as the appli‐\ncation developers handle that. Since you’re responsible for the databases serving the\nembedded analytics, you’ll need to understand the speed and latency requirements\nfor embedded analytics.\n348 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL\nPerformance for embedded analytics encompasses three problems. First, app users\nare not as tolerant of infrequent batch processing as internal company analysts; users\nof a recruiting SaaS platform may expect to see a change in their statistics as soon\nas they upload a new resume. Users want low data latency . Second, users of data\napps expect fast query performance . When they adjust parameters in an analytics\ndashboard, they want to see refreshed results appear in seconds. Third, data apps\nmust often support extremely high query rates across many dashboards and numer‐\nous customers. High concurrency  is critical.\nGoogle and other early major players in the data apps space developed exotic\ntechnologies to cope with these challenges. For new startups, the default is to use\nconventional transactional databases for data applications. As their customer bases\nexpand, they outgrow their initial architecture. They have access to a new generation\nof databases that combine high performance—fast queries, high concurrency, and\nnear real-time updates—with relative ease of use (e.g., SQL-based analytics).\nMachine Learning\nThe second major area for serving data is machine learning. ML is increasingly\ncommon, so we’ll assume you’re at least familiar with the concept. With the rise of\nML engineering (itself almost a parallel universe to data engineering), you might ask\nyourself where a data engineer fits into the picture.\nAdmittedly, the boundary between ML, data science, data engineering, and ML\nengineering is increasingly fuzzy, and this boundary varies dramatically between\norganizations. In some organizations, ML engineers take over data processing for ML\napplications right after data collection or may even form an entirely separate and\nparallel data organization that handles the entire lifecycle for all ML applications.\nData engineers handle all data processing in other settings and then hand off data to\nML engineers for model training. Data engineers may even handle some extremely\nML-specific tasks, such as featurization of data.\nLet’s return to our example of quality for control of running shorts produced by\nan online retailer. Suppose that streaming data has been implemented in the factory\nthat makes the raw fabric stock for the shorts. Data scientists discovered that the\nquality of the manufactured fabric is susceptible to characteristics of the input raw\npolyester, temperature, humidity, and various tunable parameters of the loom that\nweaves the fabric. Data scientists develop a basic model to optimize loom parameters.\nML engineers automate model training and set up a process to automatically tune the\nloom based on input parameters. Data and ML engineers work together to design a\nfeaturization pipeline, and data engineers implement and maintain the pipeline.\nMachine Learning | 349\nWhat a Data Engineer Should Know About ML\nBefore we discuss serving data for ML, you may ask yourself how much ML you need\nto know as a data engineer. ML is an incredibly vast topic, and we won’t attempt to\nteach you the field; countless books and courses are available to learn ML.\nWhile a data engineer doesn’t need to have a deep understanding of ML, it helps\ntremendously to know the basics of how classical ML works and the fundamentals\nof deep learning. Knowing the basics of ML will go a long way in helping you work\nalongside data scientists in building data products.\nHere are some areas of ML that we think a data engineer should be familiar with:\n•The difference between supervised, unsupervised, and semisupervised learning.•\n•The difference between classification and regression techniques.•\n•The various techniques for handling time-series data. This includes time-series•\nanalysis, as well as time-series forecasting.\n•When to use the “classical” techniques (logistic regression, tree-based learning,•\nsupport vector machines) versus deep learning. We constantly see data scientists\nimmediately jump to deep learning when it’s overkill. As a data engineer, your\nbasic knowledge of ML can help you spot whether an ML technique is appropri‐\nate and scales the data you’ll need to provide.\n•When would you use automated machine learning (AutoML) versus handcraft‐•\ning an ML model? What are the trade-offs with each approach regarding the data\nbeing used?\n•What are data-wrangling techniques used for structured and unstructured data?•\n•All data that is used for ML is converted to numbers. If you’re serving structured•\nor semistructured data, ensure that the data can be properly converted during the\nfeature-engineering process.\n•How to encode categorical data and the embeddings for various types of data.•\n•The difference between batch and online learning. Which approach is appropri‐•\nate for your use case?\n•How does the data engineering lifecycle intersect with the ML lifecycle at your•\ncompany? Will you be responsible for interfacing with or supporting ML tech‐\nnologies such as feature stores or ML observability?\n•Know when it’s appropriate to train locally, on a cluster, or at the edge. When•\nwould you use a GPU over a CPU? The type of hardware you use largely depends\non the type of ML problem you’re solving, the technique you’re using, and the\nsize of your dataset.\n350 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL",8168
156-Databases.pdf,156-Databases,"•Know the difference between the applications of batch and streaming data in•\ntraining ML models. For example, batch data often fits well with offline model\ntraining, while streaming data works with online training.\n•What are data cascades , and how might they impact ML models? •\n•Are results returned in real time or in batch? For example, a batch speech•\ntranscription model might process speech samples and return text in batch after\nan API call. A product recommendation model might need to operate in real\ntime as the customer interacts with an online retail site.\n•The use of structured versus unstructured data. We might cluster tabular (struc‐•\ntured) customer data or recognize images (unstructured) by using a neural net.\nML is a vast subject area, and this book won’t teach you these topics, or even\nML generalities. If you’ d like to learn more about ML, we suggest reading Hands\non Machine Learning with Scikit-Learn, Keras, and TensorFlow  by Aurélien Géron\n(O’Reilly); countless other ML courses and books are available online. Because the\nbooks and online courses evolve so rapidly, do your research on what seems like a\ngood fit for you.\nWays to Serve Data for Analytics and ML\nAs with analytics, data engineers provide data scientists and ML engineers with the\ndata they need to do their jobs. We have placed serving for ML alongside analytics\nbecause the pipelines and processes are extremely similar. There are many ways to\nserve data for analytics and ML. Some common ways to serve this data include files,\ndatabases, query engines, and data sharing. Let’s briefly look at each.\nFile Exchange\nFile exchange is ubiquitous in data serving. We process data and generate files to pass\nto data consumers.\nKeep in mind that a file might be used for many purposes. A data scientist might\nload a text file (unstructured data) of customer messages to analyze the sentiments\nof customer complaints. A business unit might receive invoice data from a partner\ncompany as a collection of CSVs (structured data), and an analyst must perform\nsome statistical analysis on these files. Or, a data vendor might provide an online\nretailer with images of products on a competitor’s website (unstructured data) for\nautomated classification using computer vision.\nWays to Serve Data for Analytics and ML | 351\nThe way you serve files depends on several factors, such as these:\n•Use case—business analytics, operational analytics, embedded analytics•\n•The data consumer’s data-handling processes•\n•The size and number of individual files in storage•\n•Who is accessing this file•\n•Data type—structured, semistructured, or unstructured•\nThe second bullet point is one of the main considerations. It is often necessary to\nserve data through files rather than data sharing because the data consumer cannot\nuse a sharing platform.\nThe simplest file to serve is something along the lines of emailing a single Excel file.\nThis is still a common workflow even in an era when files can be collaboratively\nshared. The problem with emailing files is each recipient gets their version of the file.\nIf a recipient edits the file, these edits are specific to that user’s file. Deviations among\nfiles inevitably result. And what happens if you no longer want the recipient to have\naccess to the file? If the file is emailed, you have very little recourse to retrieve the file.\nIf you need a coherent, consistent version of a file, we suggest using a collaboration\nplatform such as Microsoft 365 or Google Docs.\nOf course, serving single files is hard to scale, and your needs will eventually outgrow\nsimple cloud file storage. Y ou’ll likely grow into an object storage bucket if you have\na handful of large files, or a data lake if you have a steady supply of files. Object\nstorage can store any type of blob file and is especially useful for semistructured or\nunstructured files.\nWe’ll note that we generally consider file exchange through object storage (data lake)\nto land under “data sharing” rather than file exchange since the process can be\nsignificantly more scalable and streamlined than ad hoc file exchange.\nDatabases\nDatabases are a critical layer in serving data for analytics and ML. For this discussion,\nwe’ll implicitly keep our focus on serving data from OLAP databases (e.g., data\nwarehouses and data lakes). In the previous chapter, you learned about querying\ndatabases. Serving data involves querying a database and then consuming those\nresults for a use case. An analyst or data scientist might query a database by using a\nSQL editor and export those results to a CSV file for consumption by a downstream\napplication, or analyze the results in a notebook (described in “Serving Data in\nNotebooks” on page 356 ).\nServing data from a database carries a variety of benefits. A database imposes\norder and structure on the data through schema; databases can offer fine-grained\n352 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL\npermission  controls at the table, column, and row level, allowing database adminis‐\ntrators to craft complex access policies for various roles; and databases can offer\nhigh serving performance for large, computationally intensive queries and high query\nconcurrency.\nBI systems usually share the data processing workload with a source database, but\nthe boundary between processing in the two systems varies. For example, a Tableau\nserver runs an initial query to pull data from a database and stores it locally. Basic\nOLAP/BI slicing and dicing (interactive filtering and aggregation) runs directly on\nthe server from the local data copy. On the other hand, Looker (and similar modern\nBI systems) relies on a computational model called query pushdown ; Looker enco‐\ndes data processing logic in a specialized language (LookML), combines this with\ndynamic user input to generate SQL queries, runs these against the source database,\nand presents the output. (See “Semantic and Metrics Layers” on page 355.) Both\nTableau and Looker have various configuration options for caching results to reduce\nthe processing burden for frequently run queries.\nA data scientist might connect to a database, extract data, and perform feature\nengineering and selection. This converted dataset is then fed into an ML model; the\noffline model is trained and produces predictive results.\nData engineers are quite often tasked with managing the database-serving layer. This\nincludes management of performance and costs. In databases that separate compute\nand storage, this is a somewhat more subtle optimization problem than in the days\nof fixed on-premises infrastructure. For example, it is now possible to spin up a\nnew Spark cluster or Snowflake warehouse for each analytical or ML workload. It is\ngenerally recommended to at least split out clusters by major use cases, such as ETL\nand serving for analytics and data science. Often data teams choose to slice more\nfinely, assigning one warehouse per major area. This makes it possible for different\nteams to budget for their query costs under the supervision of a data engineering\nteam.\nAlso, recall the three performance considerations that we discussed in “Embedded\nAnalytics”  on page 348. These are data latency, query performance, and concurrency.\nA system that can ingest directly from a stream can lower data latency. And many\ndatabase architectures rely on SSD or memory caching to enhance query perfor‐\nmance and concurrency to serve the challenging use cases inherent in embedded\nanalytics.\nIncreasingly, data platforms like Snowflake and Databricks allow analysts and data\nscientists to operate under a single environment, providing SQL editors and data\nscience notebooks under one roof. Because compute and storage are separated, the\nanalysts and data scientists can consume the underlying data in various ways without\ninterfering with each other. This will allow high throughput and faster delivery of\ndata products to stakeholders.\nWays to Serve Data for Analytics and ML | 353",8089
157-Streaming Systems.pdf,157-Streaming Systems,,0
158-Data Sharing.pdf,158-Data Sharing,"Streaming Systems\nStreaming analytics are increasingly important in the realm of serving. At a high level,\nunderstand that this type of serving may involve emitted metrics , which are different\nfrom traditional queries.\nAlso, we see operational analytics databases playing a growing role in this area\n(see “Operational Analytics”  on page 346). These databases allow queries to run\nacross a large range of historical data, encompassing up-to-the-second current data.\nEssentially, they combine aspects of OLAP databases with stream-processing systems.\nIncreasingly, you’ll work with streaming systems to serve data for analytics and ML,\nso get familiar with this paradigm.\nY ou’ve learned about streaming systems throughout the book. For an idea of where\nit’s going, read about the live data stack in Chapter 11 .\nQuery Federation\nAs you learned in Chapter 8 , query federation pulls data from multiple sources,\nsuch as data lakes, RDBMSs, and data warehouses. Federation is becoming more\npopular as distributed query virtualization engines gain recognition as ways to serve\nqueries without going through the trouble of centralizing data in an OLAP system.\nToday, you can find OSS options like Trino and Presto and managed services such\nas Starburst. Some of these offerings describe themselves as ways to enable the data\nmesh; time will tell how that unfolds.\nWhen serving data for federated queries, you should be aware that the end user\nmight be querying several systems—OLTP , OLAP , APIs, filesystems, etc. ( Figure 9-3 ).\nInstead of serving data from a single system, you’re now serving data from multiple\nsystems, each with its usage patterns, quirks, and nuances. This poses challenges for\nserving data. If federated queries touch live production source systems, you must\nensure that the federated query won’t consume excessive resources in the source.\nFigure 9-3. A federated query with three data sources\n354 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL",2019
159-Serving Data in Notebooks.pdf,159-Serving Data in Notebooks,"In our experience, federated queries are ideally suited when you want flexibility in\nanalyzing data or the source data needs to be tightly controlled. Federation allows ad\nhoc queries for performing exploratory analysis, blending data from various systems\nwithout the complexity of setting up data pipelines or ETL. This will allow you to\ndetermine whether the performance of a federated query is sufficient for ongoing\npurposes or you need to set up ingestion on some or all data sources and centralize\nthe data in an OLAP database or data lake.\nFederated queries also provide read-only access to source systems, which is great\nwhen you don’t want to serve files, database access, or data dumps. The end user\nreads only the version of the data they’re supposed to access and nothing more.\nQuery federation is a great option to explore for situations where access and compli‐\nance are critical.\nData Sharing\nChapter 5  includes  an extensive discussion of data sharing. Any data exchange\nbetween organizations or units within a larger organization can be viewed as data\nsharing. Still, we mean specifically sharing through massively multitenant storage\nsystems in a cloud environment. Data sharing generally turns data serving into a\nsecurity and access control problem.\nThe actual queries are now handled by the data consumers (analysts and data scien‐\ntists) rather than the engineers sourcing the data. Whether serving data in a data\nmesh within an organization, providing data to the public, or serving to partner\nbusinesses, data sharing is a compelling serving model. Data sharing is increasingly a\ncore feature of major data platforms like Snowflake, Redshift, and BigQuery allowing\ncompanies to share data safely and securely with each other.\nSemantic and Metrics Layers\nWhen  data engineers think about serving, they naturally tend to gravitate toward\nthe data processing and storage technologies—i.e., will you use Spark or a cloud\ndata warehouse? Is your data stored in object storage or cached in a fleet of SSDs?\nBut powerful processing engines that deliver quick query results across vast datasets\ndon’t inherently make for quality business analytics. When fed poor-quality data or\npoor-quality queries, powerful query engines quickly return bad results.\nWhere data quality focuses on characteristics of the data itself and various techni‐\nques to filter or improve bad data, query quality is a question of building a query\nwith appropriate logic that returns accurate answers to business questions. Writing\nhigh-quality ETL queries and reporting is time-intensive, detailed work. Various\ntools can help automate this process while facilitating consistency, maintenance, and\ncontinuous improvement.\nWays to Serve Data for Analytics and ML | 355\n4Benn Stancil, “The Missing Piece of the Modern Data Stack, ” benn.substack , April 22, 2021,\nhttps://oreil.ly/wQyPb .\n5Srini Kadamati, “Understanding the Superset Semantic Layer, ” Preset blog, December 21, 2021,\nhttps://oreil.ly/6smWC .Fundamentally, a metrics layer  is a tool for maintaining and computing business\nlogic.4 (A semantic layer  is extremely similar conceptually,5 and headless BI  is another\nclosely related term.) This layer can live in a BI tool or in software that builds\ntransformation queries. Two concrete examples are Looker and Data Build Tool (dbt).\nFor instance, Looker’s LookML allows users to define virtual, complex business logic.\nReports and dashboards point to specific LookML for computing metrics. Looker\nallows users to define standard metrics and reference them in many downstream\nqueries; this is meant to solve the traditional problem of repetition and inconsistency\nin traditional ETL scripts. Looker uses LookML to generate SQL queries, which are\npushed down to the database. Results can be persisted in the Looker server or in the\ndatabase itself for large result sets.\ndbt allows users to define complex SQL data flows encompassing many queries and\nstandard definitions of business metrics, much like Looker. Unlike Looker, dbt runs\nexclusively in the transform layer, although this can include pushing queries into\nviews that are computed at query time. Whereas Looker focuses on serving queries\nand reporting, dbt can serve as a robust data pipeline orchestration tool for analytics\nengineers.\nWe believe that metrics layer tools will grow more popular with wider adoption and\nmore entrants, as well as move upstream toward the application. Metrics layer tools\nhelp solve a central question in analytics that has plagued organizations since people\nhave analyzed data: “ Are these numbers correct?” Many new entrants are in the space\nbeside the ones we’ve mentioned.\nServing Data in Notebooks\nData scientists often use notebooks in their day-to-day work. Whether it’s exploring\ndata, engineering features, or training a model, the data scientist will likely use a\nnotebook. At this writing, the most popular notebook platform is Jupyter Notebook,\nalong with its next-generation iteration, JupyterLab. Jupyter is open source and can\nbe hosted locally on a laptop, on a server, or through various cloud-managed services.\nJupyter  stands for Julia, Python, and R  —the latter two are popular for data science\napplications, especially notebooks. Regardless of the language used, the first thing\nyou’ll need to consider is how data can be accessed from a notebook.\nData scientists will programmatically connect to a data source, such as an API, a\ndatabase, a data warehouse, or a data lake ( Figure 9-4 ). In a notebook, all connections\n356 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL\nare created using the appropriate built-in or imported libraries to load a file from a\nfilepath, connect to an API endpoint, or make an ODBC connection to a database.\nA remote connection may require the correct credentials and privileges to establish\na connection. Once connected, a user may need the correct access to tables (and\nrows/columns) or files stored in object storage. The data engineer will often assist\nthe data scientist in finding the right data and then ensure that they have the right\npermissions to access the rows and columns required.\nLet’s look at an incredibly common workflow for data scientists: running a local\nnotebook and loading data into a pandas dataframe. Pandas  is a prevalent Python\nlibrary used for data manipulation and analysis and is commonly used to load data\n(say, a CSV file) into a Jupyter notebook. When pandas loads a dataset, it stores this\ndataset in memory.\nFigure 9-4. A notebook can be served data from many sources, such as object storage or\na database, data warehouse, or data lake\nCredential Handling\nIncorrectly handled credentials in notebooks and data science code are a major\nsecurity risk; we constantly see credentials mishandled in this domain. It is common\nto embed credentials directly in code, where they often leak into version control\nrepos. Credentials are also frequently passed around through messages and email.\nWe encourage data engineers to audit data science security practices and work collab‐\noratively on improvements. Data scientists are highly receptive to these conversations\nif they are given alternatives. Data engineers should set standards for handling cre‐\ndentials. Credentials should never be embedded in code; ideally, data scientists use\ncredential managers or CLI tools to manage access.\nWays to Serve Data for Analytics and ML | 357",7534
160-Reverse ETL.pdf,160-Reverse ETL,"What happens when the dataset size exceeds the local machine’s available memory?\nThis inevitably happens given the limited memory of laptops and workstations: it\nstops a data science project dead in its tracks. It’s time to consider more scalable\noptions. First, move to a cloud-based notebook where the underlying storage and\nmemory for the notebook can be flexibly scaled. Upon outgrowing this option, look\nat distributed execution systems; popular Python-based options include Dask, Ray,\nand Spark. If a full-fledged cloud-managed offering seems appealing, consider setting\nup a data science workflow using Amazon SageMaker, Google Cloud Vertex AI, or\nMicrosoft Azure Machine Learning. Finally, open source end-to-end ML workflow\noptions such as Kubeflow and MLflow make it easy to scale ML workloads in Kuber‐\nnetes and Spark, respectively. The point is to get data scientists off their laptops and\ntake advantage of the cloud’s power and scalability.\nData engineers and ML engineers play a key role in facilitating the move to scalable\ncloud infrastructure. The exact division of labor depends a great deal on the details of\nyour organization. They should take the lead in setting up cloud infrastructure, over‐\nseeing the management of environments, and training data scientists on cloud-based\ntools.\nCloud environments require significant operational work, such as managing versions\nand updates, controlling access, and maintaining SLAs. As with other operational\nwork, a significant payoff can result when “data science ops” are done well.\nNotebooks may even become a part of production data science; notebooks are widely\ndeployed at Netflix. This is an interesting approach with advantages and trade-offs.\nProductionized notebooks allow data scientists to get their work into production\nmuch faster, but they are also inherently a substandard form of production. The\nalternative is to have ML and data engineers convert notebooks for production use,\nplacing a significant burden on these teams. A hybrid of these approaches may be\nideal, with notebooks used for “light” production and a full productionization process\nfor high-value projects.\nReverse ETL\nToday, reverse ETL  is a buzzword that describes serving data by loading it from an\nOLAP database back into a source system. That said, any data engineer who’s worked\nin the field for more than a few years has probably done some variation of reverse\nETL. Reverse ETL grew in popularity in the late 2010s/early 2020s and is increasingly\nrecognized as a formal data engineering responsibility.\nA data engineer might pull customers and order data from a CRM and store it in\na data warehouse. This data is used to train a lead scoring model, whose results are\nreturned to the data warehouse. Y our company’s sales team wants access to these\nscored leads to try to generate more sales. Y ou have a few options to get the results of\n358 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL\nthis lead scoring model into the hands of the sales team. Y ou can put the results in a\ndashboard for them to view. Or you might email the results to them as an Excel file.\nThe challenge with these approaches is that they are not connected to the CRM,\nwhere a salesperson does their work. Why not just put the scored leads back into the\nCRM? As we mentioned, successful data products reduce friction with the end user.\nIn this case, the end user is the sales team.\nUsing reverse ETL and loading the scored leads back into the CRM is the easiest\nand best approach for this data product. Reverse ETL takes processed data from the\noutput side of the data engineering lifecycle and feeds it back into source systems\n(Figure 9-5 ).\nInstead of reverse ETL, we, the authors, half-jokingly call it bidir‐\nectional load and transform (BLT) . The term reverse ETL  doesn’t\nquite accurately describe what’s happening in this process. Regard‐\nless, the term has stuck in the popular imagination and press, so\nwe’ll use it throughout the book. More broadly, whether the term\nreverse ETL  sticks around is anyone’s guess, but the practice of\nloading data from OLAP systems back into source systems will\nremain important.\nHow do you begin serving data with reverse ETL? While you can roll your reverse\nETL solution, many off-the-shelf reverse ETL options are available. We suggest using\nopen source, or a commercial managed service. That said, the reverse ETL space is\nchanging extremely quickly. No clear winners have emerged, and many reverse ETL\nproducts will be absorbed by major clouds or other data product vendors. Choose\ncarefully.\nFigure 9-5. Reverse ETL\nReverse ETL | 359",4700
161-Whom Youll Work With.pdf,161-Whom Youll Work With,,0
162-Data Management.pdf,162-Data Management,"We do have a few words of warning regarding reverse ETL. Reverse ETL inherently\ncreates feedback loops. For example, imagine that we download Google Ads data,\nuse a model to compute new bids, load the bids back into Google Ads, and start the\nprocess again. Suppose that because of an error in your bid model, the bids trend\never higher, and your ads get more and more clicks. Y ou can quickly waste massive\namounts of money! Be careful, and build in monitoring and guardrails.\nWhom You’ll Work With\nAs we’ve discussed, in the serving stage, a data engineer will interface with a lot of\nstakeholders. These include (but aren’t limited to) the following:\n•Data analysts•\n•Data scientists•\n•MLOps/ML engineers•\n•The business—nondata or nontechnical stakeholders, managers, and executives•\nAs a reminder, the data engineer operates in a support  role for these stakeholders\nand is not necessarily responsible for the end uses of data. For example, a data\nengineer supplies the data for a report that analysts interpret, but the data engineer\nisn’t responsible for these interpretations. Instead, the data engineer is responsible for\nproducing the highest-quality data products possible.\nA data engineer should be aware of feedback loops between the data engineering\nlifecycle and the broader use of data once it’s in the hands of stakeholders. Data is\nrarely static, and the outside world will influence the data that is ingested and served\nand reingested and re-served.\nA big consideration for data engineers in the serving stage of the lifecycle is the\nseparation of duties and concerns. If you’re at an early-stage company, the data\nengineer may also be an ML engineer or data scientist; this is not sustainable. As the\ncompany grows, you need to establish a clear division of duties with other data team\nmembers.\nAdopting a data mesh dramatically reorganizes team responsibilities, and every\ndomain team takes on aspects of serving. For a data mesh to be successful, each\nteam must work effectively on its data-serving responsibilities, and teams must also\neffectively collaborate to ensure organizational success.\nUndercurrents\nThe undercurrents come to finality with serving. Remember that the data engineering\nlifecycle is just that—a lifecycle. What goes around comes around. We see many\n360 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL\ninstances where serving data highlights something missed earlier in the lifecycle.\nAlways be on the lookout for how the undercurrents can help you spot ways to\nimprove data products.\nWe’re fond of saying, “Data is a silent killer, ” and the undercurrents come to a head in\nthe serving stage. Serving is your final chance to make sure your data is in great shape\nbefore it gets into the hands of end users.\nSecurity\nThe same security principles apply whether sharing data with people or systems. We\noften see data shared indiscriminately, with little to no access controls or thought as\nto what the data will be used for. This is a huge mistake that can have catastrophic\nresults, such as a data breach and the resulting fines, bad press, and lost jobs. Take\nsecurity seriously, especially in this stage of the lifecycle. Of all the lifecycle stages,\nserving presents the largest security surface.\nAs always, exercise the principle of least privilege both for people and systems, and\nprovide only the access required for the purpose at hand and the job to be done.\nWhat data does an executive need versus an analyst or data scientist? What about an\nML pipeline or reverse ETL process? These users and destinations all have different\ndata needs, and access should be provided accordingly. Avoid giving carte blanche\npermissions to everyone and everything.\nServing data is often read-only unless a person or process needs to update data in the\nsystem from which it is queried. People should be given read-only access to specific\ndatabases and datasets unless their role requires something more advanced like write\nor update access. This can be accomplished by combining groups of users with\ncertain IAM roles (i.e., analysts group, data scientist group) or custom IAM roles if\nthis makes sense. For systems, provide service accounts and roles in a similar fashion.\nFor both users and systems, narrow access to a dataset’s fields, rows, columns, and\ncells if this is warranted. Access controls should be as fine-grained as possible and\nrevoked when access is no longer required.\nAccess controls are critical when serving data in a multitenant environment. Make\nsure users can access only their  data and nothing more. A good approach is to\nmediate access through filtered views, thus alleviating the security risks inherent in\nsharing access to a common table. Another suggestion is to use data sharing in your\nworkflows, which allows for read-only granular controls between you and people\nconsuming your data.\nCheck how often data products are used and whether it makes sense to stop sharing\ncertain data products. It’s extremely common for an executive to urgently request\nan analyst to create a report, only to have this report very quickly go unused. If\nUndercurrents | 361",5216
163-Data Architecture.pdf,163-Data Architecture,"data products aren’t used, ask the users if they’re still needed. If not, kill off the data\nproduct. This means one less security vulnerability floating around.\nFinally, you should view access control and security not as impediments to serving\nbut as key enablers. We’re aware of many instances where complex, advanced data\nsystems were built, potentially having a significant impact on a company. Because\nsecurity was not implemented correctly, few people were allowed to access the data,\nso it languished. Fine-grained, robust access control means that more interesting data\nanalytics and ML can be done while still protecting the business and its customers.\nData Management\nY ou’ve  been incorporating data management along the data engineering lifecycle,\nand the impact of your efforts will soon become apparent as people use your data\nproducts. At the serving stage, you’re mainly concerned with ensuring that people can\naccess high-quality and trustworthy data.\nAs we mentioned at the beginning of this chapter, trust is perhaps the most critical\nvariable in data serving. If people trust their data, they will use it; untrusted data\nwill go unused. Be sure to make data trust and data improvement an active process\nby providing feedback loops. As users interact with data, they can report problems\nand request improvements. Actively communicate back to your users as changes are\nmade.\nWhat data do people need to do their jobs? Especially with regulatory and compli‐\nance concerns weighing on data teams, giving people access to the raw data—even\nwith limited fields and rows—poses a problem of tracing data back to an entity, such\nas a person or a group of people. Thankfully, advancements in data obfuscation allow\nyou to serve synthetic, scrambled, or anonymized data to end users. These “fake”\ndatasets should sufficiently allow an analyst or data scientist to get the necessary\nsignal from the data, but in a way that makes identifying protected information\ndifficult. Though this isn’t a perfect process—with enough effort, many datasets can\nbe de-anonymized or reverse-engineered—it at least reduces the risk of data leakage.\nAlso, incorporate semantic and metrics layers into your serving layer, alongside\nrigorous data modeling that properly expresses business logic and definitions. This\nprovides a single source of truth, whether for analytics, ML, reverse ETL, or other\nserving uses.\nDataOps\nThe steps you take in data management—data quality, governance, and security—are\nmonitored in DataOps. Essentially, DataOps operationalizes data management. The\nfollowing are some things to monitor:\n362 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL",2724
164-Conclusion.pdf,164-Conclusion,"•Data health and data downtime•\n•Latency of systems serving data—dashboards, databases, etc.•\n•Data quality•\n•Data and system security and access•\n•Data and model versions being served•\n•Uptime to achieve an SLO•\nA variety of new tools have sprung up to address various monitoring aspects. For\nexample, many popular data observability tools aim to minimize data downtime\nand maximize data quality. Observability tools may cross over from data to ML, sup‐\nporting monitoring of models and model performance. More conventional DevOps\nmonitoring  is also critical to DataOps—e.g., you need to monitor whether connec‐\ntions are stable among storage, transformation, and serving.\nAs in every stage of the data engineering lifecycle, version-control code and opera‐\ntionalize deployment. This applies to analytical code, data logic code, ML scripts, and\norchestration jobs. Use multiple stages of deployment (dev, test, prod) for reports and\nmodels.\nData Architecture\nServing  data should have the same architectural considerations as other data engi‐\nneering lifecycle stages. At the serving stage, feedback loops must be fast and tight.\nUsers should be able to access the data they need as quickly as possible when they\nneed it.\nData scientists are notorious for doing most development on their local machines.\nAs discussed earlier, encourage them to migrate these workflows to common sys‐\ntems in a cloud environment, where data teams can collaborate in dev, test, and\nproduction environments and create proper production architectures. Facilitate your\nanalysts and data scientists by supporting tools for publishing data insights with little\nencumbrance.\nOrchestration\nData serving is the last stage of the data engineering lifecycle. Because serving is\ndownstream of so many processes, it’s an area of extremely complex overlap. Orches‐\ntration is not simply a way of organizing and automating complex work but a means\nof coordinating data flow across teams so that data is made available to consumers at\nthe promised time.\nOwnership of orchestration is a key organizational decision. Will orchestration be\ncentralized or decentralized? A decentralized approach allows small teams to manage\nUndercurrents | 363\ntheir data flows, but it can increase the burden of cross-team coordination. Instead\nof simply managing flows within a single system, directly triggering the completion\nof DAGs or tasks belonging to other teams, teams must pass messages or queries\nbetween systems.\nA centralized approach means that work is easier to coordinate, but significant\ngatekeeping must also exist to protect a single production asset. For example, a poorly\nwritten DAG can bring Airflow to a halt. The centralized approach would mean\nbringing down data processes and serving across the whole organization. Centralized\norchestration management requires high standards, automated testing of DAGs, and\ngatekeeping.\nIf orchestration is centralized, who will own it? When a company has a DataOps\nteam, orchestration usually lands here. Often, a team involved in serving is a natural\nfit because it has a fairly holistic view of all data engineering lifecycle stages. This\ncould be the DBAs, analytics engineers, data engineers, or ML engineers. ML engi‐\nneers coordinate complex model-training processes but may or may not want to add\nthe operational complexity of managing orchestration to an already crowded docket\nof responsibilities.\nSoftware Engineering\nCompared  to a few years ago, serving data has become simpler. The need to write\ncode has been drastically simplified. Data has also become more code-first, with the\nproliferation of open source frameworks focused on simplifying the serving of data.\nMany ways exist to serve data to end users, and a data engineer’s focus should be on\nknowing how these systems work and how data is delivered.\nDespite the simplicity of serving data, if code is involved, a data engineer should\nstill understand how the main serving interfaces work. For example, a data engineer\nmay need to translate the code a data scientist is running locally on a notebook and\nconvert it into a report or a basic ML model to operate.\nAnother area where data engineers will be useful is understanding the impact of how\ncode and queries will perform against the storage systems. Analysts can generate\nSQL in various programmatic ways, including LookML, Jinja via dbt, various object-\nrelational mapping (ORM) tools, and metrics layers. When these programmatic\nlayers compile to SQL, how will this SQL perform? A data engineer can suggest\noptimizations where the SQL code might not perform as well as handwritten SQL.\nThe rise of analytics and ML IaC means the role of writing code is moving toward\nbuilding the systems that support data scientists and analysts. Data engineers might\nbe responsible for setting up the CI/CD pipelines and building processes for their\ndata team. They would also do well to train and support their data team in using\n364 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL",5105
165-Additional Resources.pdf,165-Additional Resources,"the Data/MLOps infrastructure they’ve built so that these data teams can be as\nself-sufficient as possible.\nFor embedded analytics, data engineers may need to work with application develop‐\ners to ensure that queries are returned quickly and cost-effectively. The application\ndeveloper will control the frontend code that users deal with. The data engineer is\nthere to ensure that developers receive the correct payloads as they’re requested.\nConclusion\nThe data engineering lifecycle has a logical ending at the serving stage. As with all\nlifecycles, a feedback loop occurs ( Figure 9-6 ). Y ou should view the serving stage as\na chance to learn what’s working and what can be improved. Listen to your stakehold‐\ners. If they bring up issues—and they inevitably will—try not to take offense. Instead,\nuse this as an opportunity to improve what you’ve built.\nFigure 9-6. Build, learn, improve\nA good data engineer is always open to new feedback and constantly finds ways to\nimprove their craft. Now that we’ve taken a journey through the data engineering\nlifecycle, you know how to design, architect, build, maintain, and improve your data\nengineering systems and products. Let’s turn our attention to Part III  of the book,\nwhere we’ll cover some aspects of data engineering we’re constantly asked about and\nthat, frankly, deserve more attention.\nAdditional Resources\n•“Data as a Product vs. Data Products: What Are the Differences?”  by Xavier •\nGumara Rigol\n•“Data Jujitsu: The Art of Turning Data into Product”  by D. J. Patil •\n•Data Mesh  by Zhamak Dehghani (O’Reilly) •\n•“Data Mesh Principles and Logical Architecture”  by Zhamak Dehghani •\n•“Designing Data Products”  by Seth O’Regan •\n•“The Evolution of Data Products”  and “What Is Data Science”  by Mike Loukides •\nConclusion | 365\n•Forrester’s “Self-Service Business Intelligence: Dissolving the Barriers to Creative •\nDecision-Support Solutions” blog article\n•“Fundamentals of Self-Service Machine Learning”  by Paramita (Guha) Ghosh •\n•“The Future of BI Is Headless”  by ZD •\n•“How to Build Great Data Products”  by Emily Glassberg Sands •\n•“How to Structure a Data Analytics Team”  by Niall Napier •\n•“Know Y our Customers’ ‘Jobs to Be Done’”  by Clayton M. Christensen et al. •\n•“The Missing Piece of the Modern Data Stack”  and “Why Is Self-Serve Still a •\nProblem?”  by Benn Stancil\n•“Self-Service Analytics” in the Gartner Glossary•\n•Ternary Data’s “What’s Next for Analytical Databases? w/ Jordan Tigani (Mother‐ •\nDuck)” video\n•“Understanding the Superset Semantic Layer”  by Srini Kadamati •\n•“What Do Modern Self-Service BI and Data Analytics Really Mean?”  by Harry •\nDix\n•“What Is Operational Analytics (and How Is It Changing How We Work with•\nData)?”  by Sylvain Giuliani\n•“What Is User-Facing Analytics?”  by Chinmon Soman •\n366 | Chapter 9: Serving Data for Analytics, Machine Learning, and Reverse ETL",2927
166-Part III. Security Privacy and the Future of Data Engineering.pdf,166-Part III. Security Privacy and the Future of Data Engineering,"PART III\nSecurity, Privacy, and the Future of\nData Engineering",64
167-People.pdf,167-People,"CHAPTER 10\nSecurity and Privacy\nNow that you’ve learned about the data engineering lifecycle, we’ d like to reiterate the\nimportance of security and share some straightforward practices you can incorporate\nin your day-to-day workflow. Security is vital to the practice of data engineering.\nThis should be blindingly obvious, but we’re constantly amazed at how often data\nengineers view security as an afterthought. We believe that security is the first thing a\ndata engineer needs to think about in every aspect of their job and every stage of the\ndata engineering lifecycle. Y ou deal with sensitive data, information, and access daily.\nY our organization, customers, and business partners expect these valuable assets to\nbe handled with the utmost care and concern. One security breach or a data leak can\nleave your business dead in the water; your career and reputation are ruined if it’s\nyour fault.\nSecurity is a key ingredient for privacy. Privacy has long been critical to trust in the\ncorporate information technology space; engineers directly or indirectly handle data\nrelated to people’s private lives. This includes financial information, data on private\ncommunications (emails, texts, phone calls), medical history, educational records,\nand job history. A company that leaked this information or misused it could find\nitself a pariah when the breach came to light.\nIncreasingly, privacy is a matter of significant legal importance. For example, the\nFamily Educational Rights and Privacy Act (FERPA) went into effect in the US in the\n1970s; the Health Insurance Portability and Accountability Act (HIPAA) followed in\nthe 1990s; GDPR was passed in Europe in the mid-2010s. Several US-based privacy\nbills have passed or will soon. This is just a tiny sampling of privacy-related statutes\n(and we believe just the beginning). Still, the penalties for violation of any of these\nlaws can be significant, even devastating, to a business. And because data systems are\nwoven into the fabric of education, health care, and business, data engineers handle\nsensitive data related to each of these laws.\n369",2134
168-The Power of Negative Thinking.pdf,168-The Power of Negative Thinking,,0
169-Processes.pdf,169-Processes,"A data engineer’s exact security and privacy responsibilities will vary significantly\nbetween organizations. At a small startup, a data engineer may do double duty as a\ndata security engineer. A large tech company will have armies of security engineers\nand security researchers. Even in this situation, data engineers will often be able to\nidentify security practices and technology vulnerabilities within their own teams and\nsystems that they can report and mitigate in collaboration with dedicated security\npersonnel.\nBecause security and privacy are critical to data engineering (security being an under‐\ncurrent), we want to spend some more time covering security and privacy. In this\nchapter, we lay out some things data engineers should consider around security,\nparticularly in people, processes, and technology (in that order). This isn’t a complete\nlist, but it lays out the major things we wish would improve based on our experience.\nPeople\nThe weakest link in security and privacy is you. Security is often compromised at the\nhuman level, so conduct yourself as if you’re always a target. A bot or human actor is\ntrying to infiltrate your sensitive credentials and information at any given time. This\nis our reality, and it’s not going away. Take a  defensive posture with everything you do\nonline and offline. Exercise the power of negative thinking and always be paranoid.\nThe Power of Negative Thinking\nIn a world obsessed with positive thinking, negative thinking  is distasteful. However,\nAmerican surgeon Atul Gawande wrote a 2007 op-ed in the New York Times  on\nprecisely this subject. His central thesis is that positive thinking can blind us to the\npossibility of terrorist attacks or medical emergencies and deter preparation. Negative\nthinking allows us to consider disastrous scenarios and act to prevent them.\nData engineers should actively think through the scenarios for data utilization and\ncollect sensitive data only if there is an actual need downstream. The best way to\nprotect private and sensitive data is to avoid ingesting this data in the first place.\nData engineers should think about the attack and leak scenarios with any data pipe‐\nline or storage system they utilize. When deciding on security strategies, ensure that\nyour approach delivers proper security and not just the illusion of safety.\nAlways Be Paranoid\nAlways  exercise caution when someone asks you for your credentials. When in\ndoubt—and  you should always be in extreme doubt when asked for credentials—\nhold off and get second opinions from your coworkers and friends. Confirm with\nother people that the request is indeed legitimate. A quick chat or phone call is\ncheaper than a ransomware attack triggered through an email click. Trust nobody\n370 | Chapter 10: Security and Privacy",2823
170-Security Theater Versus Security Habit.pdf,170-Security Theater Versus Security Habit,,0
171-The Principle of Least Privilege.pdf,171-The Principle of Least Privilege,"at face value when asked for credentials, sensitive data, or confidential information,\nincluding from your coworkers.\nY ou are also the first line of defense in respecting privacy and ethics. Are you\nuncomfortable with sensitive data you’ve been tasked to collect? Do you have ethical\nquestions about the way data is being handled in a project? Raise your concerns\nwith colleagues and leadership. Ensure that your work is both legally compliant and\nethical.\nProcesses\nWhen  people follow regular security processes, security becomes part of the job.\nMake security a habit, regularly practice real security, exercise the principle of least\nprivilege, and understand the shared responsibility model in the cloud.\nSecurity Theater Versus Security Habit\nWith our corporate clients, we see a pervasive focus on compliance (with internal\nrules, laws, recommendations from standards bodies), but not enough attention to\npotentially bad scenarios. Unfortunately, this creates an illusion of security but often\nleaves gaping holes that would be evident with a few minutes of reflection.\nSecurity needs to be simple and effective enough to become habitual throughout an\norganization. We’re amazed at the number of companies with security policies in the\nhundreds of pages that nobody reads, the annual security policy review that people\nimmediately forget, all in checking a box for a security audit. This is security theater,\nwhere security is done in the letter of compliance (SOC-2, ISO 27001, and related)\nwithout real commitment .\nInstead, pursue the spirit of genuine and habitual security; bake a security mindset\ninto your culture. Security doesn’t need to be complicated. For example, at our\ncompany, we run security training and policy review at least once a month to ingrain\nthis into our team’s DNA and update each other on security practices we can improve.\nSecurity must not be an afterthought for your data team. Everyone is responsible and\nhas a role to play. It must be the priority for you and everyone else you work with.\nActive Security\nReturning  to the idea of negative thinking, active security  entails thinking about and\nresearching security threats in a dynamic and changing world. Rather than simply\ndeploying scheduled simulated phishing attacks, you can take an active security\nposture by researching successful phishing attacks and thinking through your organi‐\nzational security vulnerabilities. Rather than simply adopting a standard compliance\nchecklist, you can think about internal vulnerabilities specific to your organization\nand incentives employees might have to leak or misuse private information.\nProcesses | 371",2679
172-Shared Responsibility in the Cloud.pdf,172-Shared Responsibility in the Cloud,,0
173-Technology.pdf,173-Technology,"We have more to say about active security in “Technology” on page 374 .\nThe Principle of Least Privilege\nThe principle of least privilege  means that a person or system should be given only the\nprivileges and data they need to complete the task at hand and nothing more. Often,\nwe see an antipattern in the cloud: a regular user is given administrative access to\neverything, when that person may need just a handful of IAM roles to do their work.\nGiving someone carte blanche administrative access is a huge mistake and should\nnever happen under the principle of least privilege.\nInstead, provide the user (or group they belong to) the IAM roles they need when\nthey need them. When these roles are no longer needed, take them away. The same\nrule applies to service accounts. Treat humans and machines the same way: give them\nonly the privileges and data they need to do their jobs, and only for the timespan\nwhen needed.\nOf course, the principle of least privilege is also critical to privacy. Y our users and\ncustomers expect that people will look at their sensitive data only when necessary.\nMake sure that this is the case. Implement column, row, and cell-level access controls\naround sensitive data; consider masking PII and other sensitive data and create views\nthat contain only the information the viewer needs to access. Some data must be\nretained but should be accessed only in an emergency. Put this data behind a broken\nglass process : users can access it only after going through an emergency approval\nprocess to fix a problem, query critical historical information, etc. Access is revoked\nimmediately once the work is done.\nShared Responsibility in the Cloud\nSecurity  is a shared responsibility in the cloud. The cloud vendor is responsible\nfor ensuring the physical security of its data center and hardware. At the same\ntime, you are responsible for the security of the applications and systems you build\nand maintain in the cloud. Most cloud security breaches continue to be caused by\nend users, not the cloud. Breaches occur because of unintended misconfigurations,\nmistakes, oversights, and sloppiness.\nAlways Back Up Your Data\nData disappears. Sometimes it’s a dead hard drive or server; in other cases, someone\nmight accidentally delete a database or an object storage bucket. A bad actor can\nalso lock away data. Ransomware attacks are widespread these days. Some insurance\ncompanies are reducing payouts in the event of an attack, leaving you on the hook\nboth to recover your data and pay the bad actor who’s holding it hostage. Y ou\nneed to back up your data regularly, both for disaster recovery and continuity of\n372 | Chapter 10: Security and Privacy\nbusiness operations, if a version of your data is compromised in a ransomware attack.\nAdditionally, test the restoration of your data backups on a regular basis.\nData backup doesn’t strictly fit under security and privacy practices; it goes under the\nlarger heading of disaster prevention , but it’s adjacent to security, especially in the era\nof ransomware attacks.\nAn Example Security Policy\nThis  section presents a sample security policy regarding credentials, devices, and\nsensitive information. Notice that we don’t overcomplicate things; instead, we give\npeople a short list of practical actions they can take immediately.\nExample Security Policy\nProtect Your Credentials\nProtect your credentials at all costs. Here are some ground rules for credentials:\n•Use a single-sign-on (SSO) for everything. Avoid passwords whenever possible,•\nand use SSO as the default.\n•Use multifactor authentication with SSO.•\n•Don’t share passwords or credentials. This includes client passwords and creden‐•\ntials. If in doubt, see the person you report to. If that person is in doubt, keep\ndigging until you find an answer.\n•Beware of phishing and scam calls. Don’t ever give your passwords out. (Again,•\nprioritize SSO.)\n•Disable or delete old credentials. Preferably the latter.•\n•Don’t put your credentials in code. Handle secrets as configuration and never•\ncommit them to version control. Use a secrets manager where possible.\n•Always exercise the principle of least privilege. Never give more access than is•\nrequired to do the job. This applies to all credentials and privileges in the cloud\nand on premises.\nProtect Your Devices\n•Use device management for all devices used by employees. If an employee leaves•\nthe company or your device gets lost, the device can be remotely wiped.\n•Use multifactor authentication for all devices.•\n•Sign in to your device using your company email credentials.•\n•All policies covering credentials and behavior apply to your device(s).•\nProcesses | 373",4727
174-Encryption.pdf,174-Encryption,"•Treat your device as an extension of yourself. Don’t let your assigned device(s)•\nout of your sight.\n•When screen sharing, be aware of exactly what you’re sharing to protect sensitive•\ninformation and communications. Share only single documents, browser tabs,\nor windows, and avoid sharing your full desktop. Share only what’s required to\nconvey your point.\n•Use “do not disturb” mode when on video calls; this prevents messages from•\nappearing during calls or recordings.\nSoftware Update Policy\n•Restart your web browser when you see an update alert.•\n•Run minor OS updates on company and personal devices.•\n•The company will identify critical major OS updates and provide guidance.•\n•Don’t use the beta version of an OS.•\n•Wait a week or two for new major OS version releases.•\nThese are some basic examples of how security can be simple and effective. Based on\nyour company’s security profile, you may need to add more requirements for people\nto follow. And again, always remember that people are your weakest link in security.\nTechnology\nAfter  you’ve addressed security with people and processes, it’s time to look at how\nyou leverage technology to secure your systems and data assets. The following are\nsome significant areas you should prioritize.\nPatch and Update Systems\nSoftware  gets stale, and security vulnerabilities are constantly discovered. To avoid\nexposing a security flaw in an older version of the tools you’re using, always patch and\nupdate operating systems and software as new updates become available. Thankfully,\nmany SaaS and cloud-managed services automatically perform upgrades and other\nmaintenance without your intervention. To update your own code and dependencies,\neither automate builds or set alerts on releases and vulnerabilities so you can be\nprompted to perform the updates manually.\n374 | Chapter 10: Security and Privacy",1891
175-Conclusion.pdf,175-Conclusion,"Encryption\nEncryption  is not a magic bullet. It will do little to protect you in the event of a\nhuman  security breach that grants access to credentials. Encryption is a baseline\nrequirement for any organization that respects security and privacy. It will protect\nyou from basic attacks, such as network traffic interception.\nLet’s look separately at encryption at rest and in transit.\nEncryption at rest\nBe sure your data is encrypted when it is at rest (on a storage device). Y our company\nlaptops should have full-disk encryption enabled to protect data if a device is stolen.\nImplement server-side encryption for all data stored in servers, filesystems, databases,\nand object storage in the cloud. All data backups for archival purposes should also be\nencrypted. Finally, incorporate application-level encryption where applicable.\nEncryption over the wire\nEncryption  over the wire is now the default for current protocols. For instance,\nHTTPS is generally required for modern cloud APIs. Data engineers should always\nbe aware of how keys are handled; bad key handling is a significant source of data\nleaks. In addition, HTTPS does nothing to protect data if bucket permissions are left\nopen to the public, another cause of several data scandals over the last decade.\nEngineers should also be aware of the security limitations of older protocols. For\nexample, FTP is simply not secure on a public network. While this may not appear\nto be a problem when data is already public, FTP is vulnerable to man-in-the-middle\nattacks, whereby an attacker intercepts downloaded data and changes it before it\narrives at the client. It is best to simply avoid FTP .\nMake sure everything is encrypted over the wire, even with legacy protocols. When in\ndoubt, use robust technology with encryption baked in.\nLogging, Monitoring, and Alerting\nHackers and bad actors typically don’t announce that they’re infiltrating your systems.\nMost companies don’t find out about security incidents until well after the fact. Part\nof DataOps is to observe, detect, and alert on incidents. As a data engineer, you\nshould set up automated monitoring, logging, and alerting to be aware of peculiar\nevents when they happen in your systems. If possible, set up automatic anomaly\ndetection.\nHere are some areas you should monitor:\nTechnology | 375\nAccess\nWho’s  accessing what, when, and from where? What new accesses were granted?\nAre there strange patterns with your current users that might indicate their\naccount is compromised, such as trying to access systems they don’t usually\naccess or shouldn’t have access to? Do you see new unrecognized users accessing\nyour system? Be sure to regularly comb through access logs, users, and their roles\nto ensure that everything looks OK.\nResources\nMonitor your disk, CPU, memory, and I/O for patterns that seem out of the ordi‐\nnary. Did your resources suddenly change? If so, this might indicate a security\nbreach.\nBilling\nEspecially  with SaaS and cloud-managed services, you need to oversee costs. Set\nup budget alerts to make sure your spending is within expectations. If an unex‐\npected spike occurs in your billing, this might indicate someone or something is\nutilizing your resources for malicious purposes.\nExcess permissions\nIncreasingly, vendors are providing tools that monitor for permissions that are\nnot utilized  by a user or service account over some time. These tools can often be\nconfigured to automatically alert an administrator or remove permissions after a\nspecified elapsed time.\nFor example, suppose that a particular analyst hasn’t accessed Redshift for six\nmonths. These permissions can be removed, closing a potential security hole.\nIf the analyst needs to access Redshift in the future, they can put in a ticket to\nrestore permissions.\nIt’s best to combine these areas in your monitoring to get a cross-sectional view\nof your resource, access, and billing profile. We suggest setting up a dashboard for\neveryone on the data team to view monitoring and receive alerts when something\nseems out of the ordinary. Couple this with an effective incident response plan to\nmanage security breaches when they occur, and run through the plan on a regular\nbasis so you are prepared.\nNetwork Access\nWe often see data engineers doing pretty wild things regarding network access. In\nseveral instances, we’ve seen publicly available Amazon S3 buckets housing lots of\nsensitive data. We’ve also witnessed Amazon EC2 instances with inbound SSH access\nopen to the whole world for 0.0.0.0/0 (all IPs) or databases with open access to all\ninbound requests over the public internet. These are just a few examples of terrible\nnetwork security practices.\n376 | Chapter 10: Security and Privacy\nIn principle, network security should be left to security experts at your company. (In\npractice, you may need to assume significant responsibility for network security in\na small company.) As a data engineer, you will encounter databases, object storage,\nand servers so often that you should at least be aware of simple measures you can\ntake to make sure you’re in line with good network access practices. Understand\nwhat IPs and ports are open, to whom, and why. Allow the incoming IP addresses of\nthe systems and users that will access these ports (a.k.a. whitelisting IPs) and avoid\nbroadly opening connections for any reason. When accessing the cloud or a SaaS\ntool, use an encrypted connection. For example, don’t use an unencrypted website\nfrom a coffee shop.\nAlso, while this book has focused almost entirely on running workloads in the cloud,\nwe add a brief note here about hosting on-premises servers. Recall that in Chapter 3 ,\nwe discussed the difference between a hardened perimeter and zero-trust security.\nThe cloud is generally closer to zero-trust security—every action requires authentica‐\ntion. We believe that the cloud is a more secure option for most organizations because\nit imposes zero-trust practices and allows companies to leverage the army of security\nengineers employed by the public clouds.\nHowever, sometimes hardened perimeter security still makes sense; we find some\nsolace in the knowledge that nuclear missile silos are air gapped (not connected to\nany networks). Air-gapped servers are the ultimate example of a hardened security\nperimeter. Just keep in mind that even on premises, air-gapped servers are vulnerable\nto human security failings.\nSecurity for Low-Level Data Engineering\nFor engineers who work in the guts of data storage and processing systems, it is\ncritical to consider the security implications of every element. Any software library,\nstorage system, or compute node is a potential security vulnerability. A flaw in an\nobscure logging library might allow attackers to bypass access controls or encryption.\nEven CPU architectures and microcode represent potential vulnerabilities; sensitive\ndata can be vulnerable  when it’s at rest in memory or a CPU cache. No link in the\nchain can be taken for granted.\nOf course, this book is principally about high-level data engineering—stitching\ntogether tools to handle the entire lifecycle. Thus, we’ll leave it to you to dig into\nthe gory technical details.\nInternal security research\nWe discussed the idea of active security  in “Processes”  on page 371. We also highly\nrecommend adopting an active security  approach to technology. Specifically, this\nmeans that every technology employee should think about security problems.\nTechnology | 377",7564
176-The Data Engineering Lifecycle Isnt Going Away.pdf,176-The Data Engineering Lifecycle Isnt Going Away,"Why is this important? Every technology contributor develops a domain of technical\nexpertise. Even if your company employs an army of security researchers, data engi‐\nneers will become intimately familiar with specific data systems and cloud services\nin their purview. Experts in a particular technology are well positioned to identify\nsecurity holes in this technology.\nEncourage every data engineer to be actively involved in security. When they identify\npotential security risks in their systems, they should think through mitigations and\ntake an active role in deploying these.\nConclusion\nSecurity needs to be a habit of mind and action; treat data like your wallet or\nsmartphone. Although you won’t likely be in charge of security for your company,\nknowing basic security practices and keeping security top of mind will help reduce\nthe risk of data security breaches at your organization.\nAdditional Resources\n•Building Secure and Reliable Systems  by Heather Adkins et al. (O’Reilly) •\n•Open Web Application Security Project (OW ASP) publications•\n•Practical Cloud Security  by Chris Dotson (O’Reilly) •\n378 | Chapter 10: Security and Privacy\nCHAPTER 11\nThe Future of Data Engineering\nThis book grew out of the authors’ recognition that warp speed changes in the field\nhave created a significant knowledge gap for existing data engineers, people interested\nin moving into a career in data engineering, technology managers, and executives\nwho want to better understand how data engineering fits into their companies.\nWhen we started thinking about how to organize this book, we got quite a bit of\npushback from friends who’ d ask, “How dare you write about a field that is changing\nso quickly?!” In many ways, they’re right. It certainly feels like the field of data\nengineering—and, really, all things data—is changing daily. Sifting through the noise\nand finding the signal of what’s unlikely to change  was among the most challenging\nparts of organizing and writing this book.\nIn this book, we focus on big ideas that we feel will be useful for the next several\nyears—hence the continuum of the data engineering lifecycle and its undercurrents.\nThe order of operations and names of best practices and technologies might change,\nbut the primary stages of the lifecycle will likely remain intact for many years to\ncome. We’re keenly aware that technology continues to change at an exhausting pace;\nworking in the technology sector in our present era can feel like a rollercoaster ride or\nperhaps a hall of mirrors.\nSeveral years ago, data engineering didn’t even exist as a field or job title. Now you’re\nreading a book called Fundamentals of Data Engineering ! Y ou’ve learned all about\nthe fundamentals of data engineering—its lifecycle, undercurrents, technologies, and\nbest practices. Y ou might be asking yourself, what’s next in data engineering? While\nnobody can predict the future, we have a good perspective on the past, the present,\nand current trends. We’ve been fortunate to watch the genesis and evolution of data\nengineering from a front-row seat. This final chapter presents our thoughts on the\nfuture, including observations of ongoing developments and wild future speculation.\n379",3248
177-The Cloud-Scale Data OS and Improved Interoperability.pdf,177-The Cloud-Scale Data OS and Improved Interoperability,"The Data Engineering Lifecycle Isn’t Going Away\nWhile data science has received the bulk of the attention in recent years, data\nengineering is rapidly maturing into a distinct and visible field. It’s one of the fastest-\ngrowing careers in technology, with no signs of losing momentum. As companies\nrealize they first need to build a data foundation before moving to “sexier” things like\nAI and ML, data engineering will continue growing in popularity and importance.\nThis progress centers around the data engineering lifecycle.\nSome question whether increasingly simple tools and practices will lead to the dis‐\nappearance of data engineers. This thinking is shallow, lazy, and shortsighted. As\norganizations leverage data in new ways, new foundations, systems, and workflows\nwill be needed to address these needs. Data engineers sit at the center of designing,\narchitecting, building, and maintaining these systems. If tooling becomes easier to\nuse, data engineers will move up the value chain to focus on higher-level work. The\ndata engineering lifecycle isn’t going away anytime soon.\nThe Decline of Complexity and the Rise\nof Easy-to-Use Data Tools\nSimplified, easy-to-use tools continue to lower the barrier to entry for data engineer‐\ning. This is a great thing, especially given the shortage of data engineers we’ve dis‐\ncussed. The trend toward simplicity will continue. Data engineering isn’t dependent\non a particular technology or data size. It’s also not just for large companies. In the\n2000s, deploying “big data” technologies required a large team and deep pockets. The\nascendance of SaaS-managed services has largely removed the complexity of under‐\nstanding the guts of various “big data” systems. Data engineering is now something\nthat all companies can do.\nBig data is a victim of its extraordinary success. For example, Google BigQuery, a\ndescendant of GFS and MapReduce, can query petabytes of data. Once reserved for\ninternal use at Google, this insanely powerful technology is now available to anybody\nwith a GCP account. Users simply pay for the data they store and query rather than\nhaving to build a massive infrastructure stack. Snowflake, Amazon EMR, and many\nother hyper-scalable cloud data solutions compete in the space and offer similar\ncapabilities.\nThe cloud is responsible for a significant shift in the usage of open source tools.\nEven in the early 2010s, using open source typically entailed downloading the code\nand configuring it yourself. Nowadays, many open source data tools are available\nas managed cloud services that compete directly with proprietary services. Linux is\navailable preconfigured and installed on server instances on all major clouds. Server‐\nless platforms like AWS Lambda and Google Cloud Functions allow you to deploy\n380 | Chapter 11: The Future of Data Engineering\nevent-driven applications in minutes, using mainstream languages such as Python,\nJava, and Go running atop Linux behind the scenes. Engineers wishing to use Apache\nAirflow can adopt Google’s Cloud Composer or AWS’s managed Airflow service.\nManaged Kubernetes allows us to build highly scalable microservice architectures.\nAnd so on.\nThis fundamentally changes the conversation around open source code. In many\ncases, managed open source is just as easy to use as its proprietary service com‐\npetitors. Companies with highly specialized needs can also deploy managed open\nsource, then move to self-managed open source later if they need to customize the\nunderlying code.\nAnother significant trend is the growth in popularity of off-the-shelf data connectors\n(at the time of this writing, popular ones include Fivetran and Airbyte). Data engi‐\nneers have traditionally spent a lot of time and resources building and maintaining\nplumbing to connect to external data sources. The new generation of managed\nconnectors is highly compelling, even for highly technical engineers, as they begin to\nrecognize the value of recapturing time and mental bandwidth for other projects. API\nconnectors will be an outsourced problem so that data engineers can focus on the\nunique issues that drive their businesses.\nThe intersection of red-hot competition in the data-tooling space with a growing\nnumber of data engineers means data tools will continue decreasing in complexity\nwhile adding even more functionality and features. This simplification will only grow\nthe practice of data engineering, as more and more companies find opportunities to\ndiscover value in data.\nThe Cloud-Scale Data OS and Improved Interoperability\nLet’s  briefly review some of the inner workings of (single-device) operating systems,\nthen tie this back to data and the cloud. Whether you’re utilizing a smartphone,\na laptop, an application server, or a smart thermostat, these devices rely on an\noperating system to provide essential services and orchestrate tasks and processes.\nFor example, I can see roughly 300 processes running on the MacBook Pro that I’m\ntyping on. Among other things, I see services such as WindowServer (responsible for\nproviding windows in a graphical interface) and CoreAudio (tasked with providing\nlow-level audio capabilities).\nWhen I run an application on this machine, it doesn’t directly access sound and\ngraphics hardware. Instead, it sends commands to operating system services to\ndraw windows and play sound. These commands are issued to standard APIs; a\nspecification tells software developers how to communicate with operating system\nservices. The operating system orchestrates  a boot process to provide these services,\nstarting each service in the correct order based on dependencies among them; it also\nThe Cloud-Scale Data OS and Improved Interoperability | 381\n1Benn Stancil, “The Data OS, ” benn.substack , September 3, 2021, https://oreil.ly/HetE9 .maintains services by monitoring them and restarting them in the correct order in\ncase of a failure.\nNow let’s return to data in the cloud. The simplified data services that we’ve men‐\ntioned throughout this book (e.g., Google Cloud BigQuery, Azure Blob Storage,\nSnowflake, and AWS Lambda) resemble operating system services, but at a much\nlarger scale, running across many machines rather than a single server.\nNow that these simplified services are available, the next frontier of evolution for this\nnotion of a cloud data operating system will happen at a higher level of abstraction.\nBenn Stancil called for the emergence of standardized data APIs for building data\npipelines and data applications.1 We predict that data engineering will gradually\ncoalesce around a handful of data interoperability standards. Object storage in the\ncloud will grow in importance as a batch interface layer between various data services.\nNew generation file formats (such as Parquet and Avro) are already taking over\nfor the purposes of cloud data interchange, significantly improving on the dreadful\ninteroperability of CSV and the poor performance of raw JSON.\nAnother critical ingredient of a data API ecosystem is a metadata catalog that\ndescribes schemas and data hierarchies. Currently, this role is largely filled by the\nlegacy Hive Metastore. We expect that new entrants will emerge to take its place.\nMetadata will play a crucial role in data interoperability, both across applications and\nsystems and across clouds and networks, driving automation and simplification.\nWe will also see significant improvements in the scaffolding that manages cloud\ndata services. Apache Airflow has emerged as the first truly cloud-oriented data\norchestration platform, but we are on the cusp of significant enhancement. Airflow\nwill grow in capabilities, building on its massive mindshare. New entrants such as\nDagster and Prefect will compete by rebuilding orchestration architecture from the\nground up.\nThis next generation of data orchestration platforms will feature enhanced data inte‐\ngration and data awareness. Orchestration platforms will integrate with data catalog‐\ning and lineage, becoming significantly more data-aware in the process. In addition,\norchestration platforms will build IaC capabilities (similar to Terraform) and code\ndeployment features (like GitHub Actions and Jenkins). This will allow engineers to\ncode a pipeline and then pass it to the orchestration platform to automatically build,\ntest, deploy, and monitor. Engineers will be able to write infrastructure specifications\ndirectly into their pipelines; missing infrastructure and services (e.g., Snowflake data‐\nbases, Databricks clusters, and Amazon Kinesis streams) will be deployed the first\ntime the pipeline runs.\n382 | Chapter 11: The Future of Data Engineering",8726
178-Moving Beyond the Modern Data Stack Toward the Live Data Stack.pdf,178-Moving Beyond the Modern Data Stack Toward the Live Data Stack,"2Ben Rogojan, “Three Data Engineering Experts Share Their Thoughts on Where Data Is Headed, ” Better\nProgramming , May 27, 2021, https://oreil.ly/IsY4W .We will also see significant enhancements in the domain of live data —e.g., streaming\npipelines and databases capable of ingesting and querying streaming data. In the past,\nbuilding a streaming DAG was an extremely complex process with a high ongoing\noperational burden (see Chapter 8 ). Tools like Apache Pulsar point the way toward\na future in which streaming DAGs can be deployed with complex transformations\nusing relatively simple code. We have already seen the emergence of managed stream\nprocessors (such as Amazon Kinesis Data Analytics and Google Cloud Dataflow),\nbut we will see a new generation of orchestration tools for managing these services,\nstitching them together, and monitoring them. We discuss live data in “The Live Data\nStack” on page 385 .\nWhat does this enhanced abstraction mean for data engineers? As we’ve already\nargued in this chapter, the role of the data engineer won’t go away, but it will\nevolve significantly. By comparison, more sophisticated mobile operating systems\nand frameworks have not eliminated mobile app developers. Instead, mobile app\ndevelopers can now focus on building better-quality, more sophisticated applications.\nWe expect similar developments for data engineering as the cloud-scale data OS\nparadigm increases interoperability and simplicity across various applications and\nsystems.\n“Enterprisey” Data Engineering\nThe increasing simplification of data tools and the emergence and documentation of\nbest practices means data engineering will become more “enterprisey. ”2 This will make\nmany readers violently cringe. The term enterprise , for some, conjures Kafkaesque\nnightmares of faceless committees dressed in overly starched blue shirts and khakis,\nendless red tape, and waterfall-managed development projects with constantly slip‐\nping schedules and ballooning budgets. In short, some of you read “enterprise” and\nimagine a soulless place where innovation goes to die.\nFortunately, this is not what we’re talking about; we’re referring to some of the good\nthings that larger companies do with data—management, operations, governance,\nand other “boring” stuff. We’re presently living through the golden age of “enterpri‐\nsey” data management tools. Technologies and practices once reserved for giant\norganizations are trickling downstream. The once hard parts of big data and stream‐\ning data have now largely been abstracted away, with the focus shifting to ease of use,\ninteroperability, and other refinements.\n“Enterprisey” Data Engineering | 383\nThis allows data engineers working on new tooling to find opportunities in the\nabstractions of data management, DataOps, and all the other undercurrents of data\nengineering. Data engineers will become “enterprisey. ” Speaking of which…\nTitles and Responsibilities Will Morph...\nWhile  the data engineering lifecycle isn’t going anywhere anytime soon, the bound‐\naries between software engineering, data engineering, data science, and ML engi‐\nneering are increasingly fuzzy. In fact, like the authors, many data scientists are\ntransformed into data engineers through an organic process; tasked with doing “data\nscience” but lacking the tools to do their jobs, they take on the job of designing and\nbuilding systems to serve the data engineering lifecycle.\nAs simplicity moves up the stack, data scientists will spend a smaller slice of their\ntime gathering and munging data. But this trend will extend beyond data scientists.\nSimplification also means data engineers will spend less time on low-level tasks in the\ndata engineering lifecycle (managing servers, configuration, etc.), and “enterprisey”\ndata engineering will become more prevalent.\nAs data becomes more tightly embedded in every business’s processes, new roles\nwill emerge in the realm of data and algorithms. One possibility is a role that sits\nbetween ML engineering and data engineering. As ML toolsets become easier to use\nand managed cloud ML services grow in capabilities, ML is shifting away from ad hoc\nexploration and model development to become an operational discipline.\nThis new ML-focused engineer who straddles this divide will know algorithms, ML\ntechniques, model optimization, model monitoring, and data monitoring. However,\ntheir primary role will be to create or utilize the systems that automatically train\nmodels, monitor performance, and operationalize the full ML process for model\ntypes that are well understood. They will also monitor data pipelines and quality,\noverlapping into the current realm of data engineering. ML engineers will become\nmore specialized to work on model types that are closer to research and less well\nunderstood.\nAnother area in which titles may morph is at the intersection of software engineering\nand data engineering. Data applications, which blend traditional software applica‐\ntions with analytics, will drive this trend. Software engineers will need to have a much\ndeeper understanding of data engineering. They will develop expertise in things like\nstreaming, data pipelines, data modeling, and data quality. We will move beyond the\n“throw it over the wall” approach that is now pervasive. Data engineers will be inte‐\ngrated into application development teams, and software developers will acquire data\nengineering skills. The boundaries that exist between application backend systems\nand data engineering tools will be lowered as well, with deep integration through\nstreaming and event-driven architectures.\n384 | Chapter 11: The Future of Data Engineering",5713
179-The Tight Feedback Between Applications and ML.pdf,179-The Tight Feedback Between Applications and ML,"Moving Beyond the Modern Data Stack,\nToward the Live Data Stack\nWe’ll be frank: the modern data stack (MDS) isn’t so modern. We applaud the MDS\nfor bringing a great selection of powerful data tools to the masses, lowering prices,\nand empowering data analysts to take control of their data stack. The rise of ELT,\ncloud data warehouses, and the abstraction of SaaS data pipelines certainly changed\nthe game for many companies, opening up new powers for BI, analytics, and data\nscience.\nHaving said that, the MDS is basically a repackaging of old data warehouse practices\nusing modern cloud and SaaS technologies; because the MDS is built around the\ncloud data warehouse paradigm, it has some serious limitations when compared\nto the potential of next-generation real-time data applications. From our point of\nview, the world is moving beyond the use of data-warehouse-based internal-facing\nanalytics and data science, toward powering entire businesses and applications in real\ntime with next-generation real-time databases.\nWhat’s driving this evolution? In many cases, analytics (BI and operational analytics)\nwill be replaced by automation. Presently, most dashboards and reports answer\nquestions concerning what  and when . Ask yourself, “If I’m asking a what  or when\nquestion, what action do I take next?” If the action is repetitive, it is a candidate for\nautomation. Why look at a report to determine whether to take action when you can\ninstead automate the action based on events as they occur?\nAnd it goes much further than this. Why does using a product like TikTok, Uber,\nGoogle, or DoorDash feel like magic? While it seems to you like a click of a button to\nwatch a short video, order a ride or a meal, or find a search result, a lot is happening\nunder the hood. These products are examples of true real-time data applications,\ndelivering the actions you need at the click of a button while performing extremely\nsophisticated data processing and ML behind the scenes with miniscule latency.\nPresently, this level of sophistication is locked away behind custom-built technologies\nat large technology companies, but this sophistication and power are becoming\ndemocratized, similar to the way the MDS brought cloud-scale data warehouses and\npipelines to the masses. The data world will soon go “live. ”\nThe Live Data Stack\nThis democratization of real-time technologies will lead us to the successor to the\nMDS: the live data stack  will soon be accessible and pervasive . The live data stack,\ndepicted in Figure 11-1 , will fuse real-time analytics and ML into applications by\nusing streaming technologies, covering the full data lifecycle from application source\nsystems to data processing to ML, and back.\nMoving Beyond the Modern Data Stack, Toward the Live Data Stack | 385\nFigure 11-1. In the live data stack, data and intelligence moves in real time between the\napplication and supporting systems\nJust as the MDS took advantage of the cloud and brought on-premises data ware‐\nhouse and pipeline technologies to the masses, the live data stack takes real-time data\napplication technologies used at elite tech companies and makes them available to\ncompanies of all sizes as easy-to-use cloud-based offerings. This will open up a new\nworld of possibilities for creating even better user experiences and business value.\nStreaming Pipelines and Real-Time Analytical Databases\nThe MDS limits itself to batch techniques that treat data as bounded. In contrast,\nreal-time data applications treat data as an unbounded, continuous stream. Streaming\npipelines and real-time analytical databases are the two core technologies that will\nfacilitate the move from the MDS to the live data stack. While these technologies have\nbeen around for some time, rapidly maturing managed cloud services will see them\nbe deployed much more widely.\nStreaming technologies will continue to see extreme growth for the foreseeable\nfuture. This will happen in conjunction with a clearer focus on the business utility\nof streaming data. Up to the present, streaming systems have frequently been treated\nlike an expensive novelty or a dumb pipe for getting data from A to B. In the future,\nstreaming will radically transform organizational technology and business processes;\ndata architects and engineers will take the lead in these fundamental changes.\nReal-time analytical databases enable both fast ingestion and subsecond queries on\nthis data. This data can be enriched or combined with historical datasets. When\ncombined with a streaming pipeline and automation, or dashboard that is capable\nof real-time analytics, a whole new level of possibilities opens up. No longer are\nyou constrained by slow-running ELT processes, 15-minute updates, or other slow-\nmoving parts. Data moves in a continuous flow. As streaming ingestion becomes\nmore prevalent, batch ingestion will be less and less common. Why create a batch\n386 | Chapter 11: The Future of Data Engineering\nbottleneck at the head of your data pipeline? We’ll eventually look at batch ingestion\nthe same way we now look at dial-up modems.\nIn conjunction with the rise of streams, we expect a back-to-the-future moment for\ndata transformations. We’ll shift away from ELT—in database transformations—to\nsomething that looks more like ETL. We provisionally refer to this  as stream, trans‐\nform, and load  (STL). In a streaming context, extraction is an ongoing, continuous\nprocess. Of course, batch transformations won’t entirely go away. Batch will still\nbe very useful for model training, quarterly reporting, and more. But  streaming\ntransformation will become the norm.\nWhile the data warehouse and data lake are great for housing large amounts of data\nand performing ad hoc queries, they are not so well optimized for low-latency data\ningestion or queries on rapidly moving data. The live data stack will be powered by\nOLAP databases that are purpose-built for streaming. Today, databases like Druid,\nClickHouse, Rockset, and Firebolt are leading the way in powering the backend of\nthe next generation of data applications. We expect that streaming technologies will\ncontinue to evolve rapidly and that new technologies will proliferate.\nAnother  area we think is ripe for disruption is data modeling, where there hasn’t\nbeen serious innovation since the early 2000s. The traditional batch-oriented data\nmodeling techniques you learned about in Chapter 8  aren’t suited for streaming data.\nNew data-modeling techniques will occur not within the data warehouse but in the\nsystems that generate the data. We expect data modeling will involve some notion\nof an upstream definitions layer—including semantics, metrics, lineage, and data\ndefinitions (see Chapter 9 )—beginning where data is generated in the application.\nModeling will also happen at every stage as data flows and evolves through the full\nlifecycle.\nThe Fusion of Data with Applications\nWe expect the next revolution will be the fusion of the application and data layers.\nRight now, applications sit in one area, and the MDS sits in another. To make\nmatters worse, data is created with no regard for how it will be used for analytics.\nConsequently, lots of duct tape is needed to make systems talk with one another. This\npatchwork, siloed setup is awkward and ungainly.\nSoon, application stacks will be data stacks, and vice versa. Applications will integrate\nreal-time automation and decision making, powered by the streaming pipelines and\nML. The data engineering lifecycle won’t necessarily change, but the time between\nstages of the lifecycle will drastically shorten. A lot of innovation will occur in new\ntechnologies and practices that will improve the experience of engineering the live\ndata stack. Pay attention to emerging database technologies designed to address the\nmix of OLTP and OLAP use cases; feature stores may also play a similar role for ML\nuse cases.\nMoving Beyond the Modern Data Stack, Toward the Live Data Stack | 387",8054
180-Conclusion.pdf,180-Conclusion,"The Tight Feedback Between Applications and ML\nAnother  area we’re excited about is the fusion of applications and ML. Today, appli‐\ncations and ML are disjointed systems, like applications and analytics. Software\nengineers do their thing over here, data scientists and ML engineers do their thing\nover there.\nML is well-suited for scenarios where data is generated at such a high rate and\nvolume that humans cannot feasibly process it by hand. As data sizes and velocity\ngrow, this applies to every scenario. High volumes of fast-moving data, coupled with\nsophisticated workflows and actions, are candidates for ML. As data feedback loops\nbecome shorter, we expect most applications to integrate ML. As data moves more\nquickly, the feedback loop between applications and ML will tighten. The applications\nin the live data stack are intelligent and able to adapt in real time to changes in the\ndata. This creates a cycle of ever-smarter applications and increasing business value.\nDark Matter Data and the Rise of...Spreadsheets?!\nWe’ve  talked about fast-moving data and how feedback loops will shrink as applica‐\ntions, data, and ML work more closely together. This section might seem odd, but we\nneed to address something that’s widely ignored in today’s data world, especially by\nengineers.\nWhat’s the most widely used data platform? It’s the humble spreadsheet. Depending\non the estimates you read, the user base of spreadsheets is between 700 million and\n2 billion people. Spreadsheets are the dark matter of the data world. A good deal of\ndata analytics runs in spreadsheets and never makes its way into the sophisticated\ndata systems that we describe in this book. In many organizations, spreadsheets\nhandle financial reporting, supply-chain analytics, and even CRM.\nAt heart, what is a spreadsheet? A spreadsheet  is an interactive data application that\nsupports complex analytics. Unlike purely code-based tools such as pandas (Python\nData Analysis Library), spreadsheets are accessible to a whole spectrum of users,\nranging from those who just know how to open files and look at reports to power\nusers who can script sophisticated procedural data processing. So far, BI tools have\nfailed to bring comparable interactivity to databases. Users who interact with the\nUI are typically limited to slicing and dicing data within certain guardrails, not\ngeneral-purpose programmable analytics.\nWe predict that a new class of tools will emerge that combines the interactive analyt‐\nics capabilities of a spreadsheet with the backend power of cloud OLAP systems.\nIndeed, some candidates are already in the running. The ultimate winner in this\nproduct category may continue to use spreadsheet paradigms, or may define entirely\nnew interface idioms for interacting with data.\n388 | Chapter 11: The Future of Data Engineering\nConclusion\nThank you for joining us on this journey through data engineering! We traversed\ngood architecture, the stages of the data engineering lifecycle, and security best\npractices. We’ve discussed strategies for choosing technologies at a time when our\nfield continues to change at an extraordinary pace. In this chapter, we laid out our\nwild speculation about the near and intermediate future.\nSome aspects of our prognostication sit on a relatively secure footing. The simpli‐\nfication of managed tooling and the rise of “enterprisey” data engineering have\nproceeded day by day as we’ve written this book. Other predictions are much more\nspeculative in nature; we see hints of an emerging live data stack , but this entails a\nsignificant paradigm shift for both individual engineers and the organizations that\nemploy them. Perhaps the trend toward real-time data will stall once again, with\nmost companies continuing to focus on basic batch processing. Surely, other trends\nexist that we have completely failed to identify. The evolution of technology involves\ncomplex interactions of technology and culture. Both are unpredictable.\nData engineering is a vast topic; while we could not go into any technical depth in\nindividual areas, we hope that we have succeeded in creating a kind of travel guide\nthat will help current data engineers, future data engineers, and those who work\nadjacent to the field to find their way in a domain that is in flux. We advise you to\ncontinue exploration on your own. As you discover interesting topics and ideas in\nthis book, continue the conversation as part of a community. Identify domain experts\nwho can help you to uncover the strengths and pitfalls of trendy technologies and\npractices. Read extensively from the latest books, blog posts, and papers. Participate\nin meetups and listen to talks. Ask questions and share your own expertise. Keep an\neye on vendor announcements to stay abreast of the latest developments, taking all\nclaims with a healthy grain of salt.\nThrough this process, you can choose technology. Next, you will need to adopt tech‐\nnology and develop expertise, perhaps as an individual contributor, perhaps within\nyour team as a lead, perhaps across an entire technology organization. As you do\nthis, don’t lose sight of the larger goals of data engineering. Focus on the lifecycle, on\nserving your customers—internal and external—on your business, on serving and on\nyour larger goals.\nRegarding the future, many of you will play a role in determining what comes\nnext. Technology trends are defined not only by those who create the underlying\ntechnology but also by those who adopt it and put it to good use. Successful tool\nuse is as critical as tool creation . Find opportunities to apply real-time technology\nthat will improve the user experience, create value, and define entirely new types of\napplications. It is this kind of practical application that will materialize the live data\nConclusion | 389\nstack  as a new industry standard; or perhaps some other new technology trend that\nwe failed to identify will win the day.\nFinally, we wish you an exciting career! We chose to work in data engineering, to\nconsult, and to write this book not simply because it was trendy but because it was\nfascinating. We hope that we’ve managed to convey to you a bit of the joy we’ve found\nworking in this field.\n390 | Chapter 11: The Future of Data Engineering",6327
181-Appendix A. Serialization and Compression Technical Details.pdf,181-Appendix A. Serialization and Compression Technical Details,,0
182-Serialization Formats.pdf,182-Serialization Formats,,0
183-Row-Based Serialization.pdf,183-Row-Based Serialization,"APPENDIX A\nSerialization and Compression\nTechnical Details\nData engineers working in the cloud are generally freed from the complexities of\nmanaging object storage systems. Still, they need to understand details of serialization\nand deserialization formats. As we mentioned in Chapter 6  about storage raw ingre‐\ndients, serialization and compression algorithms go hand in hand.\nSerialization Formats\nMany  serialization algorithms and formats are available to data engineers. While the\nabundance of options is a significant source of pain in data engineering, they are\nalso a massive opportunity for performance improvements. We’ve sometimes seen\njob performance improve by a factor of 100 simply by switching from CSV to Parquet\nserialization. As data moves through a pipeline, engineers will also manage reseriali‐\nzation—conversion from one format to another. Sometimes data engineers have no\nchoice but to accept data in an ancient, nasty form; they must design processes to\ndeserialize this format and handle exceptions, and then clean up and convert data for\nconsistent, fast downstream processing and consumption.\nRow-Based Serialization\nAs its name suggests, row-based serialization  organizes data by row. CSV format is\nan archetypal row-based format. For semistructured data (data objects that support\nnesting and schema variation), row-oriented serialization entails storing each object\nas a unit.\n391\nCSV: The nonstandard standard\nWe discussed CSV in Chapter 7 . CSV is a serialization format that data engineers love\nto hate. The term CSV  is essentially a catchall for delimited text, but there is flexibility\nin conventions of escaping, quote characters, delimiter, and more.\nData engineers should avoid using CSV files in pipelines because they are highly\nerror-prone and deliver poor performance. Engineers are often required to use CSV\nformat to exchange data with systems and business processes outside their control.\nCSV is a common format for data archival. If you use CSV for archival, include a\ncomplete technical description of the serialization configuration for your files so that\nfuture consumers can ingest the data.\nXML\nExtensible Markup Language (XML)  was popular when HTML and the internet were\nnew, but it is now viewed as legacy; it is generally slow to deserialize and serialize for\ndata engineering applications. XML is another standard that data engineers are often\nforced to interact with as they exchange data with legacy systems and software. JSON\nhas largely replaced XML for plain-text object serialization.\nJSON and JSONL\nJavaScript Object Notation (JSON) has emerged as the new standard for data\nexchange over APIs, and it has also become an extremely popular format for data\nstorage. In the context of databases, the popularity of JSON has grown apace with\nthe rise of MongoDB and other document stores. Databases such as Snowflake,\nBigQuery, and SQL Server also offer extensive native support, facilitating easy data\nexchange between applications, APIs, and database systems.\nJSON Lines (JSONL)  is a specialized version of JSON for storing bulk semistructured\ndata in files. JSONL stores a sequence of JSON objects, with objects delimited by\nline breaks. From our perspective, JSONL is an extremely useful format for storing\ndata right after it is ingested from API or applications. However, many columnar\nformats offer significantly better performance. Consider moving to another format\nfor intermediate pipeline stages and serving.\nAvro\nAvro  is a row-oriented data format designed for RPCs and data serialization. Avro\nencodes data into a binary format, with schema metadata specified in JSON. Avro is\npopular in the Hadoop ecosystem and is also supported by various cloud data tools.\n392 | Appendix A: Serialization and Compression Technical Details",3857
184-Columnar Serialization.pdf,184-Columnar Serialization,"Columnar Serialization\nThe serialization formats we’ve discussed so far are row-oriented. Data is encoded as\ncomplete relations (CSV) or documents (XML and JSON), and these are written into\nfiles sequentially.\nWith columnar serialization , data organization is essentially pivoted by storing each\ncolumn into its own set of files. One obvious advantage to columnar storage is that it\nallows us to read data from only a subset of fields rather than having to read full rows\nat once. This is a common scenario in analytics applications and can dramatically\nreduce the amount of data that must be scanned to execute a query.\nStoring data as columns also puts similar values next to each other, allowing us\nto encode columnar data efficiently. One common technique involves looking for\nrepeated values and tokenizing these, a simple but highly efficient compression\nmethod for columns with large numbers of repeats.\nEven when columns don’t contain large numbers of repeated values, they may mani‐\nfest high redundancy. Suppose that we organized customer support messages into a\nsingle column of data. We likely see the same themes and verbiage again and again\nacross these messages, allowing data compression algorithms to realize a high ratio.\nFor this reason, columnar storage is usually combined with compression, allowing us\nto maximize disk and network bandwidth resources.\nColumnar storage and compression come with some disadvantages too. We cannot\neasily access individual data records; we must reconstruct records by reading data\nfrom several column files. Record updates are also challenging. To change one field\nin one record, we must decompress the column file, modify it, recompress it, and\nwrite it back to storage. To avoid rewriting full columns on each update, columns\nare broken into many files, typically using partitioning and clustering strategies that\norganize data according to query and update patterns for the table. Even so, the\noverhead for updating a single row is horrendous. Columnar databases are a terrible\nfit for transactional workloads, so transactional databases generally utilize some form\nof row- or record-oriented storage.\nParquet\nParquet stores data in a columnar format and is designed to realize excellent read and\nwrite performance in a data lake environment. Parquet solves a few problems that\nfrequently bedevil data engineers. Parquet-encoded data builds in schema informa‐\ntion and natively supports nested data, unlike CSV . Furthermore, Parquet is portable;\nwhile databases such as BigQuery and Snowflake serialize data in proprietary col‐\numnar formats and offer excellent query performance on data stored internally, a\nhuge performance hit occurs when interoperating with external tools. Data must be\nSerialization and Compression Technical Details | 393\n1Dejan Simic, “ Apache Arrow: Read DataFrame with Zero Memory, ” Towards Data Science , June 25, 2020,\nhttps://oreil.ly/TDAdY .deserialized, reserialized into an exchangeable format, and exported to use data lake\ntools such as Spark and Presto. Parquet files in a data lake may be a superior option to\nproprietary cloud data warehouses in a polyglot tool environment.\nParquet format is used with various compression algorithms; speed optimized com‐\npression algorithms such as Snappy (discussed later in this appendix) are especially\npopular.\nORC\nOptimized Row Columnar (ORC) is a columnar storage format similar to Parquet.\nORC was very popular for use with Apache Hive; while still widely used, we generally\nsee it much less than Apache Parquet, and it enjoys somewhat less support in modern\ncloud ecosystem tools. For example, Snowflake and BigQuery support Parquet file\nimport and export; while they can read from ORC files, neither tool can export to\nORC.\nApache Arrow or in-memory serialization\nWhen  we introduced serialization as a storage raw ingredient at the beginning\nof this chapter, we mentioned that software could store data in complex objects\nscattered in memory and connected by pointers, or more orderly, densely packed\nstructures such as Fortran and C arrays. Generally, densely packed in-memory data\nstructures were limited to simple types (e.g., INT64) or fixed-width data structures\n(e.g., fixed-width strings). More complex structures (e.g., JSON documents) could\nnot be densely stored in memory and required serialization for storage and transfer\nbetween systems.\nThe idea of Apache Arrow  is to rethink serialization by utilizing a binary data format\nthat is suitable for both in-memory processing and export.1 This allows us to avoid\nthe overhead of serialization and deserialization; we simply use the same format for\nin-memory processing, export over the network, and long-term storage. Arrow relies\non columnar storage, where each column essentially gets its own chunks of memory.\nFor nested data, we use a technique called shredding , which maps each location in the\nschema of JSON documents into a separate column.\nThis technique means that we can store a data file on disk, swap it directly into\nprogram address space by using virtual memory, and begin running a query against\nthe data without deserialization overhead. In fact, we can swap chunks of the file into\nmemory as we scan it, and then swap them back out to avoid running out of memory\nfor large datasets.\n394 | Appendix A: Serialization and Compression Technical Details",5440
185-Hybrid Serialization.pdf,185-Hybrid Serialization,,0
186-Compression gzip bzip2 Snappy Etc..pdf,186-Compression gzip bzip2 Snappy Etc.,"One obvious headache with this approach is that different programming languages\nserialize data in different ways. To address this issue, the Arrow Project has created\nsoftware libraries for a variety of programming languages (including C, Go, Java,\nJavaScript, MATLAB, Python, R, and Rust) that allow these languages to interoperate\nwith Arrow data in memory. In some cases, these libraries use an interface between\nthe chosen language and low-level code in another language (e.g., C) to read and\nwrite from Arrow. This allows high interoperability between languages without extra\nserialization overhead. For example, a Scala program can use the Java library to write\narrow data and then pass it as a message to a Python program.\nArrow is seeing rapid uptake with a variety of popular frameworks such as Apache\nSpark. Arrow has also spanned a new data warehouse product; Dremio  is a query\nengine and data warehouse built around Arrow serialization to support fast queries.\nHybrid Serialization\nWe use the term hybrid serialization  to refer to technologies that combine multiple\nserialization techniques or integrate serialization with additional abstraction layers,\nsuch as schema management. We cite as examples Apache Hudi and Apache Iceberg.\nHudi\nHudi  stands  for Hadoop Update Delete Incremental . This table management technol‐\nogy combines multiple serialization techniques to allow columnar database perfor‐\nmance for analytics queries while also supporting atomic, transactional updates. A\ntypical Hudi application is a table that is updated from a CDC stream from a transac‐\ntional application database. The stream is captured into a row-oriented serialization\nformat, while the bulk of the table is retained in a columnar format. A query runs\nover both columnar and row-oriented files to return results for the current state of\nthe table. Periodically, a repacking process runs that combines the row and columnar\nfiles into updated columnar files to maximize query efficiency.\nIceberg\nLike Hudi, Iceberg is a table management technology. Iceberg can track all files that\nmake up a table. It can also track files in each table snapshot over time, allowing table\ntime travel in a data lake environment. Iceberg supports schema evolution and can\nreadily manage tables at a petabyte scale.\nDatabase Storage Engines\nTo round out the discussion of serialization, we briefly discuss database storage\nengines. All databases have an underlying storage engine; many don’t expose their\nstorage engines as a separate abstraction (for example, BigQuery, Snowflake). Some\n(notably, MySQL) support fully pluggable storage engines. Others (e.g., SQL Server)\nSerialization and Compression Technical Details | 395\noffer major storage engine configuration options (columnar versus row-based stor‐\nage) that dramatically affect database behavior.\nTypically, the storage engine is a separate software layer from the query engine.\nThe storage engine manages all aspects of how data is stored on a disk, including\nserialization, the physical arrangement of data, and indexes.\nStorage engines have seen significant innovation in the 2000s and 2010s. While stor‐\nage engines in the past were optimized for direct access to spinning disks, modern\nstorage engines are much better optimized to support the performance characteristics\nof SSDs. Storage engines also offer improved support for modern types and data\nstructures, such as variable-length strings, arrays, and nested data.\nAnother major change in storage engines is a shift toward columnar storage for\nanalytics and data warehouse applications. SQL Server, PostgreSQL, and MySQL offer\nrobust columnar storage support.\nCompression: gzip, bzip2, Snappy, Etc.\nThe math behind compression algorithms is complex, but the basic idea is easy to\nunderstand: compression algorithms look for redundancy and repetition in data,\nthen reencode data to reduce redundancy. When we want to read the raw data, we\ndecompress  it by reversing the algorithm and putting the redundancy back in.\nFor example, you’ve noticed that certain words appear repeatedly in reading this\nbook. Running some quick analytics on the text, you could identify the words that\noccur most frequently and create shortened tokens for these words. To compress, you\nwould replace common words with their tokens; to decompress, you would replace\nthe tokens with their respective words.\nPerhaps we could use this naive technique to realize a compression ratio of 2:1 or\nmore. Compression algorithms utilize more sophisticated mathematical techniques to\nidentify and remove redundancy; they can often realize compression ratios of 10:1 on\ntext data.\nNote that we’re talking about lossless compression algorithms . Decompressing data\nencoded with a lossless algorithm recovers a bit-for-bit exact copy of the original\ndata. Lossy compression algorithms  for audio, images, and video aim for sensory fidel‐\nity; decompression recovers something that sounds like or looks like the original but\nis not an exact copy. Data engineers might deal with lossy compression algorithms\nin media processing pipelines but not in serialization for analytics, where exact data\nfidelity is required.\n396 | Appendix A: Serialization and Compression Technical Details\nTraditional compression engines such as gzip and bzip2 compress text data extremely\nwell; they are frequently applied to JSON, JSONL, XML, CSV , and other text-based\ndata formats. Engineers have created a new generation of compression algorithms\nthat prioritize speed and CPU efficiency over compression ratio in recent years.\nMajor examples are Snappy, Zstandard, LZFSE, and LZ4. These algorithms are fre‐\nquently used to compress data in data lakes or columnar databases to optimize for\nfast query performance.\nSerialization and Compression Technical Details | 397",5890
187-Appendix B. Cloud Networking.pdf,187-Appendix B. Cloud Networking,,0
188-Cloud Network Topology.pdf,188-Cloud Network Topology,,0
189-Availability Zones.pdf,189-Availability Zones,"1Matthew Prince and Nitin Rao, “ AWS’s Egregious Egress, ” The Cloudflare  Blog , July 23, 2021,\nhttps://oreil.ly/NZqKa .APPENDIX B\nCloud Networking\nThis appendix discusses some factors data engineers should consider about network‐\ning in the cloud. Data engineers frequently encounter networking in their careers and\noften ignore it despite its importance.\nCloud Network Topology\nA cloud network topology  describes how various components in the cloud are\narranged and connected, such as cloud services, networks, locations (zones, regions),\nand more. Data engineers should always know how cloud network topology will\naffect connectivity across the data systems they build. Microsoft Azure, Google\nCloud Platform (GCP), and Amazon Web Services (AWS) all use remarkably similar\nresource hierarchies of availability zones and regions. At the time of this writing,\nGCP has added one additional layer, discussed in “GCP-Specific Networking and\nMultiregional Redundancy” on page 401 .\nData Egress Charges\nChapter 4  discusses cloud economics and how actual provider costs don’t necessarily\ndrive cloud pricing. Regarding networking, clouds allow inbound traffic for free\nbut charge for outbound traffic to the internet. Outbound traffic is not inherently\ncheaper, but clouds use this method to create a moat around their services and\nincrease the stickiness of stored data, a practice that has been widely criticized.1 Note\nthat data egress charges can also apply to data passing between availability zones and\nregions within a cloud.\n399",1557
190-GCP-Specific Networking and Multiregional Redundancy.pdf,190-GCP-Specific Networking and Multiregional Redundancy,"Availability Zones\nThe availability zone  is the smallest unit of network topology that public clouds make\nvisible to customers ( Figure B-1 ). While a zone can potentially consist of multiple\ndata centers, cloud customers cannot control resource placement at this level.\nFigure B-1. Availability zones in two separate regions\nGenerally, clouds support their highest network bandwidth and lowest latency\nbetween systems and services within a zone. High throughput data workloads should\nrun on clusters located in a single zone for performance and cost reasons. For exam‐\nple, an ephemeral Amazon EMR cluster should generally sit in a single availability\nzone.\nIn addition, network traffic sent to VMs within a zone is free, but with a significant\ncaveat: traffic must be sent to private IP addresses. The major clouds utilize virtual\nnetworks known as virtual private clouds  (VPCs). Virtual machines have private\nIP addresses within the VPC. They may also be assigned public IP addresses to\ncommunicate with the outside world and receive traffic from the internet, but com‐\nmunications using external IP addresses can incur data egress charges.\nRegions\nA region  is a collection of two or more availability zones. Data centers require many\nresources to run (electrical power, water, etc.). The resources of separate availability\nzones are independent so that a local power outage doesn’t take down multiple\navailability zones. Engineers can build highly resilient, separate infrastructure even\nwithin a single region by running servers in multiple zones or creating automated\ncross-zone failover processes.\nOffering multiple regions allows engineers to put resources close to any of their users.\nClose  means that users can realize good network performance in connecting to serv‐\nices, minimizing physical distance along the network path, and a minimal number of\n400 | Appendix B: Cloud Networking",1924
191-CDNs.pdf,191-CDNs,"hops through routers. Both physical distance and network hops can increase latency\nand decrease performance. Major cloud providers continue to add new regions.\nIn general, regions support fast, low-latency networking between zones; networking\nperformance between zones will be worse than within a single zone and incur nomi‐\nnal data egress charges between VMs. Network data movement between regions is\neven slower and may incur higher egress fees.\nIn general, object storage is a regional resource. Some data may pass between zones to\nreach a virtual machine, but this is mainly invisible to cloud customers, and there are\nno direct networking charges for this. (Of course, customers are still responsible for\nobject access costs.)\nDespite regions’ geo-redundant design, many major cloud service failures have affec‐\nted entire regions, an example of correlated failure . Engineers often deploy code and\nconfiguration to entire regions; the regional failures we’ve observed have generally\nresulted from code or configuration problems occurring at the regional level.\nGCP-Specific  Networking and Multiregional Redundancy\nGCP offers a handful of unique abstractions that engineers should be aware of if they\nwork in this cloud. The first is the multiregion , a layer in the resource hierarchy; a\nmultiregion contains multiple regions. Current multiregions are US (data centers in\nthe United States), EU (data centers in European Union member states), and ASIA.\nSeveral GCP resources support multiregions, including Cloud Storage and BigQuery.\nData is stored in multiple zones within the multiregion in a geo-redundant manner so\nthat it should remain available in the event of a regional failure. Multiregional storage\nis also designed to deliver data efficiently to users within the multiregion without\nsetting up complex replication processes between regions. In addition, there are no\ndata egress fees for VMs in a multiregion to access Cloud Storage data in the same\nmultiregion.\nCloud customers can set up multiregional infrastructure on AWS or Azure. In the\ncase of databases or object storage, this involves duplicating data between regions to\nincrease redundancy and put data closer to users.\nGoogle also essentially owns significantly more global-scale networking resources\nthan other cloud providers, something it offers to its customers as premium-tier\nnetworking . Premium-tier networking allows traffic between zones and regions to\npass entirely over Google-owned networks without traversing the public internet.\nDirect Network Connections to the Clouds\nEach major public cloud offers enhanced connectivity options, allowing customers to\nintegrate their networks with a cloud region or VPC directly. For example, Amazon\nCloud Networking | 401",2790
192-Index.pdf,192-Index,"2Mark Haranas and Steven Burke, “Oracle Bests Cloud Rivals to Win Blockbuster Cloud Deal, ” CRN, April 28,\n2020, https://oreil.ly/LkqOi .\n3Corey Quinn, “Why Zoom Chose Oracle Cloud Over AWS and Maybe Y ou Should Too, ” Last Week in AWS,\nApril 28, 2020, https://oreil.ly/Lx5uu .offers AWS Direct Connect. In addition to providing higher bandwidth and lower\nlatency, these connection options often offer dramatic discounts on data egress\ncharges. In a typical scenario in the US, AWS egress charges drop from 9 cents per\ngigabyte over the public internet to 2 cents per gigabyte over direct connect.\nCDNs\nContent delivery networks  (CDNs)  can offer dramatic performance enhancements\nand discounts for delivering data assets to the public or customers. Cloud providers\noffer CDN options and many other providers, such as Cloudflare. CDNs work best\nwhen delivering the same data repeatedly, but make sure that you read the fine\nprint. Remember that CDNs don’t work everywhere, and certain countries may block\ninternet traffic and CDN delivery.\nThe Future of Data Egress Fees\nData egress fees are a significant impediment to interoperability, data sharing, and\ndata movement to the cloud. Right now, data egress fees are a moat designed to\nprevent public cloud customers from leaving or deploying across multiple clouds.\nBut interesting signals indicate that change may be on the horizon. In particular,\nZoom’s announcement in 2020 near the beginning of the COVID-19 pandemic that it\nchose Oracle as its cloud infrastructure provider caught the attention of many cloud\nwatchers.2 How did Oracle win this significant cloud contract for critical remote work\ninfrastructure against the cloud heavyweights? AWS expert Corey Quinn offers a\nreasonably straightforward answer.3 By his back-of-the-envelope calculation, Zoom’s\nAWS monthly data egress fees would run over $11 million at list price; Oracle’s would\ncost less than $2 million.\nWe suspect that GCP , AWS, or Azure will announce significant cuts in egress fees\nin the next few years, leading to a sea change in the cloud business model. It’s\nalso entirely possible that egress fees go away, similar to how limited and expensive\ncell-phone minutes disappeared decades ago.\n402 | Appendix B: Cloud Networking\nIndex\nSymbols\n1NF (first normal form), 291\n2NF (second normal form), 291\n3NF (third normal form), 291\nA\nabstraction, 22\naccess policies, 376\naccountability, 55\naccuracy, 55\nACID (atomicity, consistency, isolation, and\ndurability) transactions, 103, 158\nactive security, 371, 377\nad hoc analysis, 345\nagile architecture, 81\nagility, 77\nAI researchers, 27\nAirflow DAG, 320\nAirflow project, 150\nalerting, 375\nAmazon Elastic Block Store (EBS), 203\nAmazon EMR, 221\nanalog data, 156\nanalytics\nas code, 333\nOLTPs and, 159\nserving data for, 344-349\nvariations of, 44\nApache Arrow, 394\nApache Beam framework, 106\nApache Druid, 222\nApache Spark, 221\nAPIs (see application program interfaces)application architecture, 72 (see also data archi‐\ntecture; monolithic architectures; technical\narchitectures)\napplication databases, 157-159\napplication programming interfaces (APIs),\n157, 174-176, 254\narchitecture tiers, 90\narchival storage, 197\nareal density, 192\nasymmetric optimization, 148\nasynchronous data ingestion, 238\natomic transactions, 157-158\natomicity, consistency, isolation, and durability\n(ACID) transactions, 103, 158\nautogenerated metadata, 52\nautomated lifecycle policies, 225\nautomatic data lifecycle management, 226\nautomation, 61\navailability, 79, 89, 221\navailability zones, 195, 400\nAvro, 392\nAWS Well-Architected Framework, 77\nB\nB-trees, 166\nbackups, 372\nbaseline architecture, 81\nbash, 20\nbasically available, soft-state, eventual consis‐\ntency (BASE), 198\nbatch data ingestion, 40, 106, 244-247\nbatch data transformations\nbroadcast joins, 310\nbusiness logic and derived data, 320\n403\ndata wrangling, 319\ndistributed joins, 310\nETL, ELT, and data pipelines, 311\nkey considerations for, 43\nMapReduce, 322\nmemory caching, 322\nschema updates, 318\nshuffle hash joins, 311\nin Spark, 320\nSQL and code-based transformation tools,\n312\nversus streaming transformations, 310\nupdate patterns, 315\nbatch logs, 161\nbatch size, 247\nbatch-oriented change data capture, 252\nbenchmarks, 147\nBezos API Mandate, 82\nbig data engineers, 8\nbig data era, 7, 148, 380\nblock storage, 202-205\nblockchain technologies, 129\nblocks (HDFS), 211\nbounded data, 236\nBoyce-Codd system, 294\nbridge tables, 305\nbroadcast joins, 310\nbrownfield projects, 96\nbudget alerts, 376\nbuild, 22\nbuild versus buy\nadvice on selecting, 138\nbenefits of buying, 132\nopen source software, 133-137\nproprietary walled gardens, 137-138\ntechnology adoption within companies, 133\nbulk data storage systems, 191\nbursty data ingestion, 240\nbusiness analytics, 346\nbusiness architecture, 72\nbusiness data warehouses, 6\nbusiness intelligence (BI), 45, 344\nbusiness logic, 43, 320\nbusiness metadata, 53, 55\nbusiness stakeholders, 111\nbusiness value, increasing, 127\n(see also cost optimization and business\nvalue)C\nC-level executives, 28\ncache hierarchy, 197\ncaching, 197, 211, 221, 281\nCAO-2s (chief algorithms officers), 30\nCAOs (chief analytics officers), 29\ncapital expenses (capex), 118\ncargo-cult engineering, 116\nCDC (see change data capture)\nCDOs (chief data officers), 29\nCEOs (chief executive officers), 28\nchange data capture (CDC), 37, 159, 252-254,\n281\nchief algorithms officers (CAO-2s), 30\nchief analytics officers (CAOs), 29\nchief data officers (CDOs), 29\nchief executive officers (CEOs), 28\nchief information officers (CIOs), 29\nchief operating officers (COOs), 29\nchief technology officers (CTOs), 29\nCIOs (chief information officers), 29\ncloning, zero-copy, 223\ncloud data warehouses, 100, 216\ncloud networking\ncloud network topology, 399-402\ncontent delivery networks, 402\nfuture of data egress fees, 402\ncloud of clouds services, 129\ncloud repatriation, 130-132\ncloud services\nadopting cloud-first approach, xiv\nadopting common components, 78\nadvice on selecting, 129\ncloud economics, 125-127\ncloud-scale data OS, 381-383\nconsiderations for choosing technologies,\n124-125\ncosts of, xv\ndecentralized, 129\ndirect network connections to, 401\nfilesystems, 201\nhybrid cloud, 127\nmanaging spending and resources, 86\nmotivation for using, 120\nmulticloud deployment, 128\nmultitenancy support, 94\nproprietary cloud offerings, 138\nresponsibility for security, 85, 372\n404 | Index\nseparation of compute from storage,\n220-223\ncloud virtualized block storage, 203\nCloudflare, 131-132\nclustering, 213\ncode-based transformation tools, 312\ncold data, 39, 224\ncold storage, 225\ncollaborative architecture, 81\ncollections, 169\ncolocation, 220\nColossus file storage system, 222\ncolumnar serialization, 213, 393-395\ncolumns, 167, 213, 290\ncomma-separated values (CSV) format, 392\ncommand-and-control architecture, 81, 85\ncommercial open source software (COSS), 135\ncommits, 278\ncommon table expressions (CTEs), 277\ncommunity-managed open source software,\n134\ncompleteness, 55\ncompliance, 226, 265\ncomponents, choosing, 78\ncomposable materialized views, 324\ncompression algorithms, 196, 396\ncompute, separating from storage, 220-223\nconceptual data models, 289\nconcurrency, 349\nconformed dimension, 301\nconsistency, 158, 166, 171, 208\nconsumers (from a stream), 163, 250\ncontainer escape, 144\ncontainer platforms, 144-145\ncontent delivery networks (CDNs), 402\ncontinuous change data capture, 253, 281\nCOOs (chief operating officers), 29\ncopy on write (COW), 317\ncorrelated failure, 212\nCOSS (commercial open source software), 135\ncost\ncloud economics, 125\ncloud repatriation arguments, 130\nof cloud services, xv\ncost structure of data, 86\ndata egress costs, 128, 131, 399, 402\nof data migration, 261\ndata storage expenses, 226\ndirect costs, 118of distributed monolith pattern, 142\nindirect costs, 118\noverseeing, 376\nof running servers, 145\nof serverless approach, 143, 147\ntotal cost of ownership, 118\ntotal opportunity cost of ownership, 119\ncost comparisons, 148\ncost optimization and business value, 118-120\nFinOps, 120\nimportance of, 118\ntotal cost of ownership, 118\ntotal opportunity cost of ownership, 119\nCOW (copy on write), 317\ncreate, read, update, and delete (CRUD), 162,\n166\ncredit default swaps, 125\nCRUD (create, read, update, and delete), 162,\n166\nCSV (comma-separated values) format, 392\nCTEs (common table expressions), 277\nCTOs (chief technology officers), 29\ncurse of familiarity, 126\nD\nDAGs (directed acyclic graphs), 64, 320, 327\nDAMA (Data Management Association Inter‐\nnational), 50\ndark data, 102\ndashboards, 344\ndata (see also generation stage; source systems)\nanalog data, 156\nbacking up, 372\nbounded versus unbounded, 236\ncold data, 39, 224\ncombining streams with other data, 286\ncost structure of, 86\ndeleting, 316\ndigital data, 156\ndurability and availability of, 221\ngrain of, 290\nhot data, 39, 224\ninternal and external, 25\nkinds of, 241\nlate-arriving data, 248\nlukewarm data, 39, 224\nprejoining data, 276\nproductive uses of, 339\nself-service data, 341\nIndex | 405\nshape of, 241\nsize of, 241\nstructured, unstructured, and semistruc‐\ntured, 157\nthird-party sources for, 177\nunbounded data, 106\nwarm data, 224\ndata access frequency, 39\ndata accountability, 55\ndata analysts, 27\ndata applications, 159, 348\ndata architects, 25, 81, 111\ndata architecture\nbenefits of good data architecture, 71\nexamples and types of, 98-110\narchitecture for IoT, 106\ndata lakes, 101\ndata mesh, 109\ndata warehouses, 98\nDataflow model and unified batch and\nstreaming, 105\nKappa architecture, 105\nLambda architecture, 104\nmodern data stacks, 103\nother examples, 110\ntrend toward convergence, 102\nimpact on data storage, 230\nimpact on queries, transformations, and\nmodeling, 333\nimpact on serving data, 363\nimpact on source systems, 185\nimpact on technology selection, 150\nmajor architecture concepts, 87-98\nbrownfield versus greenfield projects, 96\nconsiderations for data architecture, 94\ndistributed systems, scalability, and\ndesigning for failure, 88\ndomains and services, 87\nevent-driven architecture, 95\nmain goal of architectures, 87\ntight versus loose coupling, 90\nuser access, 94\nprinciples of good data architecture, 77-87\nalways be architecting, 81\narchitect for scalability, 80\narchitecture is leadership, 80\nAWS framework, 77\nbuild loosely coupled systems, 82\nchoose common components wisely, 78embrace FinOps, 85\nGoogle Cloud’s principles, 77\nmake reversible decisions, 83\npillars of, 78\nplan for failure, 79\nprioritize security, 84\nrecognizing good data architecture, 76\nrole of data engineers in, 64\nas subset of enterprise architecture, 72\nversus tools, 115\nworking definition of, 71-76\ndata block location, 222\ndata breaches, 85\nData Build Tool (dbt), 356\ndata catalogs, 218\ndata connectors, 381\ndata contracts, 182, 339\ndata control language (DCL), 274\ndata definition language (DDL), 273\ndata definitions, 342\ndata egress costs, 128, 131, 399, 402\ndata engineering\napproach to learning, xiii, xvi\ndata maturity and, 13-17\ndeclining complexity, 380\ndefinition of term, 3-4\nevolution of field, 6-11\nfuture of\nenterprise-level management tools, 383\nevolution of titles and responsibilities,\n384\nfusion of data with applications, 387\nimproved interoperability, 381-383\nmove toward live data stacks, 385-386\nrise of spreadsheets, 388\nstreaming pipelines, 386\ntight feedback between applications and\nmachine learning, 388\nlearning goals of book, xv\nobject stores for, 207\nprerequisites to learning, xv\nprimary and secondary languages used in,\n20-21\nrelationship to data science, 11-12\nsecurity for low-level, 377\nskills and activities, 13\nstorage abstractions, 215-217\ntarget audience for book, xiv\n406 | Index\ndata engineering lifecycle (see also undercur‐\nrents)\ndefinition of term, xiv\nfuture of, 379\ngeneration stage, 35-37\ningestion stage, 39-42\nrelationship to data lifecycle, 34\nserving data stage, 44-48\nstages of, 5, 19, 33\nstorage stage, 38-39\ntransformation stage, 43-44\ndata engineers\nbackground and skills of, 17\nbig data engineers, 8\nbusiness leadership and, 28-31\nbusiness responsibilities of, 18\ncargo-cult engineering, 116\ncontinuum of roles for, 21\nversus data architects, 64\ndata lifecycle engineers, 10\ndefinition of term, 4\ndesigning data architectures, 111\nevolution into data lifecycle engineers, 33\nnew architectures and developments, 110\nother management roles and, 31\nproduct managers and, 30\nproject managers and, 30\nas security engineers, 85\ntechnical responsibilities of, 19-21, 155\nwithin organizations, 22-28\ndata ethics and privacy, 59, 85, 265\ndata featurization, 44\ndata generation (see generation stage)\ndata gravity, 127\ndata ingestion (see ingestion stage)\ndata integration, 58, 234\ndata lakehouses, 103, 216\ndata lakes, 101, 216, 312\ndata latency, 349\ndata lifecycle, 34\ndata lifecycle engineers, 10\ndata lifecycle management, 58\ndata lineage, 57\ndata lineage tools, 331\ndata logic, 343\ndata management\ndata accountability, 55\ndata governance, 51\ndata integration, 58data lifecycle management, 58\ndata lineage, 57\ndata modeling and design, 57\ndata quality, 55\ndefinition of term, 50\ndiscoverability, 52\nethics and privacy, 59, 85\nfacets of, 51\nimpact on data ingestion, 264\nimpact on data storage, 228\nimpact on queries, transformations, and\nmodeling, 331\nimpact on serving data, 362\nimpact on source systems, 184\nimpact on technology selection, 149\nmaster data management, 56\nmetadata, 52-54\nData Management Association International\n(DAMA), 50\nData Management Body of Knowledge\n(DMBOK), 50, 75\nData Management Maturity (DMM), 13-17\ndata manipulation language (DML), 273\ndata marketplaces, 176\ndata marts, 101\ndata maturity, 13-17\ndata mesh, 94, 109, 326, 343\ndata migration, 247\ndata modeling\nalternatives to, 307\nbusiness outcomes and, 288\nconceptual, logical, and physical models,\n289\nconsiderations for successful, 289\ndefinition of term, 287\nderiving business insights through, 57\nexamples of, 287\nfuture of, 387\nnormalization, 290\npurpose of, 288\nstakeholders of, 329\ntechniques for batch analytical data,\n294-306\ncombining, 294\nData Vault, 301\ndimension tables, 298\nfact tables, 298\nhubs (Data Vault), 302\nInmon, 295\nIndex | 407\nKimball, 297\nlink tables (Data Vault), 303\nmodeling streaming data, 307\nsatellites (Data Vault), 304\nstar schema, 301\nwide denormalized tables, 305\ndata observability, 339\nData Observability Driven Development\n(DODD), 58, 62\ndata orchestration, 64, 68\ndata pipelines, 234, 386\ndata platforms, 103, 217, 381\ndata producers and consumers, 24\ndata products, 340\ndata quality, 55\ndata reliability engineers, 332\ndata retention, 223, 225\ndata schemas, 37\nData Science Hierarchy of Needs, 11\ndata scientists, 26\ndata security, 49\ndata sharing, 176, 219, 262, 355\ndata stacks, 10, 103, 385-386\ndata stakeholders, 182\ndata storage lifecycle (see also storage stage)\ncompliance, 226\ncost, 226\ndata retention, 225\nhot, warm, and cold data, 223\nstorage tier considerations, 224\ntime, 226\nvalue, 225\ndata swamps, 102\ndata technologies\narchitecture versus tools, 115\nconsiderations for choosing\nbenchmarking, 147\nbuild versus buy, 132-139\ncost optimization and business value,\n118-120\nimmutable versus transitory technolo‐\ngies, 120-123\nimpact of undercurrents, 149-151\ninteroperability, 117\nlocation, 123-132\nmonolith versus modular, 139-143\noverview of, 116\nserverless versus servers, 143-147\nspeed to market, 117team size and capabilities, 116\nvariety of choices available, 115\nwhen to select, 115\ndata validation, 339\ndata value, 44\nData Vaults, 301\ndata virtualization, 325\ndata warehouses, 6, 98-101, 215, 295\ndata wrangling, 319\ndata-lineage metadata, 54\ndata-quality tests, 267\ndatabase logs, 161\ndatabase management systems (DBMSs), 166\ndatabase replication, 253\ndatabase storage engines, 395\ndatabases\ncommits to, 278\nconnecting directly to, 251\ndead records in, 280\nfile export from, 257\nmajor considerations for understanding,\n166\nversus query engines, 272\nreal-time analytical databases, 386\nserving data via, 352\ntransaction support, 278\nDataflow model, 56, 106\nDataOps\nadopting cultural habits of, 60\nautomation, 61\ncore technical elements of, 61\ngoals of, 59\nas a high priority, 63\nimpact on data ingestion, 266\nimpact on data storage, 229\nimpact on queries, transformations, and\nmodeling, 332\nimpact on serving data, 362\nimpact on source systems, 184\nimpact on technology selection, 149\nincident response, 63\nobservability and monitoring, 62\nrelationship to lean manufacturing, 60\nDataportal concept, 53\ndatastrophes, 267\nDBMSs (database management systems), 166\ndbt (Data Build Tool), 356\nDCL (data control language), 274\nDDL (data definition language), 273\n408 | Index\ndead database records, 280\ndead-letter queues, 249\ndecentralized computing, 129\ndecision-making, eliminating irreversible deci‐\nsions, 83\ndecompression, 396\ndecoupling, 83, 90\ndefensive posture, 370\ndeletion, 316\ndenormalization, 213, 291, 305\nderived data, 320\ndeserialization, 239\ndevices, 106\nDevOps engineers, 26\ndifferential update pattern, 246\ndigital data, 156\ndimension tables, 297-301\ndirect costs, 118\ndirected acyclic graphs (DAGs), 64, 320, 327\ndisaster prevention, 373\ndiscoverability, 52\ndisk transfer speed, 192\ndistributed joins, 310\ndistributed monolith pattern, 142\ndistributed storage, 198\ndistributed systems, 88\nDMBOK (Data Management Body of Knowl‐\nedge), 50, 75\nDML (data manipulation language), 273\nDMM (Data Management Maturity), 13\ndocument stores, 169\ndocuments, 169\nDODD (Data Observability Driven Develop‐\nment), 58, 62\ndomain coupling, 92\ndomains, 87\ndon’t repeat yourself (DRY), 290\ndownstream stakeholders, 26\nDropbox, 131-132\ndurability, 158, 221, 240\ndynamic RAM (DRAM), 194, 221\nE\nEA (enterprise architecture), 72-75\nEABOK (Enterprise Architecture Book of\nKnowledge), 73, 81\nEBS (Amazon Elastic Block Store), 203\nedge computing, 129\nedges (in a graph), 172efficiency, 176\nelastic systems, 80, 88\nelectronic data interchange (EDI), 257\nELT (extract, load, and transform), 99, 246, 312\nembedded analytics, 45, 348\nemitted metrics, 354\nencryption, 375\nenrichment, 286\nenterprise architecture (EA), 72-75\nEnterprise Architecture Book of Knowledge\n(EABOK), 73, 81\nephemerality, 220\nerror handling, 249\nethics, 59, 85, 265\nETL (see extract, transform, load process)\nevent time, 165\nevent-based data, 174, 248-250\nevent-driven architecture, 95\nevent-driven systems, 163\nevent-streaming platforms, 164, 177, 179-181,\n255\neventual consistency, 158, 171, 198, 208\nexplain plan, 277\nExtensible Markup Language (XML), 392\nexternal data, 26\nexternal-facing data engineers, 23\nextract (ETL), 246\nextract, load, and transform (ELT), 99, 246, 312\nextract, transform, load (ETL) process\nbatch data transformations, 311\ndata warehouses and, 99\nversus ELT, 246\npush versus pull models of data ingestion,\n42\nreverse ETL, 47, 358-360\nF\nfact tables, 297\nfailure, planning for, 79, 88\nFamily Educational Rights and Privacy Act\n(FERPA), 369\nfast-follower change data capture approach, 281\nfault tolerance, 181\nfeaturization, 44\nfederated queries, 324, 354\nFERPA (Family Educational Rights and Privacy\nAct), 369\nfields, 167, 290\nFIFO (first in, first out), 179\nIndex | 409\nfile exchange, 351\nfile storage, 199-201\nFile Transfer Protocol (FTP), 375\nfile-based export, 246\nfiles and file formats, 156, 257-258, 391-395\nfilesystems, 201, 210\nfinancial management, 85\nFinOps, 85-87, 120\nfirst in, first out (FIFO), 179\nfirst normal form (1NF), 291\nFive Principles for Cloud-Native Architecture,\n77\nfixed schema, 37\nfixed-time windows, 284\nforeign keys, 167\nfrequency, 237\nFTP (File Transfer Protocol), 375\nfull snapshots, 246\nfull table scans, 277\nfull-disk encryption, 375\nG\nGartner Hype Cycle, 72\ngeneration stage\nsource systems and, 35-37\nsources of data, 156\ngolden records, 56\nGoogle Cloud Platform–specific networking,\n401\nGoogle File System (GFS), 211\ngovernance, 51\ngrain, 290\ngraph databases, 172\nGraphQL, 175\ngreenfield projects, 97\ngRPC, 176\nH\nHadoop Distributed File System (HDFS), 211,\n220\nHadoop Update Delete Incremental (Hudi), 395\nhard delete, 316\nHarvard architecture, 195\nheadless BI, 356\nHealth Insurance Portability and Accountabil‐\nity Act (HIPAA), 369\nhorizontal scaling, 89\nhot data, 39, 224\nhot storage, 224hotspotting, 181\nHTTPS (Hypertext Transfer Protocol Secure),\n375\nhubs (Data Vault), 302\nHudi (Hadoop Update Delete Incremental), 395\nhuman security breaches, 370, 375\nhuman-generated metadata, 52\nhybrid cloud, 127, 130\nhybrid columnar storage, 214\nhybrid object storage, 222\nhybrid separation, 221\nhybrid serialization, 395\nHypertext Transfer Protocol Secure (HTTPS),\n375\nI\nIaaS (infrastructure as a service), 124\nIaC (infrastructure as code), 67\nIceberg, 395\nidempotent message systems, 179\nidentifiable business element, 302\nimmutable technologies, 121, 130\nimplicit data definitions, 343\nin-memory serialization, 394\nincident response, 63\nincremental updates, 246\nindependent offerings, 137\nindexes, 166, 213\nindirect costs, 118\ninfrastructure as a service (IaaS), 124\ninfrastructure as code (IaC), 67\ningestion stage\nbatch ingestion considerations, 244-247\nbatch versus streaming, 40-42, 106\nchallenges faced in, 40\nchange data capture, 252\ndata pipelines and, 234\ndefinition of data ingestion, 234\nimpact of undercurrents on, 263-268\nIoT gateways and, 108\nkey engineering considerations for, 235-243\nbounded versus unbounded data, 236\ndata-ingestion frequency, 237\noverview of, 40, 235\npayload, 241\nreliability and durability, 240\nserialization and deserialization, 239\nsynchronous versus asynchronous inges‐\ntion, 238\n410 | Index\nthroughput and scalability, 239\nmessage and stream ingestion considera‐\ntions, 248-250\npush versus pull models, 42, 244\nstakeholders of, 262\nways to ingest data, 250-262\nAPIs, 254\ndata sharing, 262\ndatabases and file export, 257\ndirect database connection, 251\nelectronic data interchange, 257\nmanaged data connectors, 256\nmessage queues and event-streaming\nplatforms, 255\nmoving data with object storage, 257\nSFTP and SCP, 259\nshell interface, 258\nSSH protocol, 259\ntransfer appliances for data migration,\n261\nweb interfaces, 260\nweb scraping, 260\nwebhooks, 259\ningestion time, 165\nInmon data model, 295\ninsert deletion, 316\ninsert-only pattern, 162, 316\ninserts, 247\ninstance store volumes, 204\ninstitutional knowledge, 343\nintegration, 58, 234, 295\ninternal data, 25\ninternal ingestion, 234\ninternal security research, 377\ninternal-facing data engineers, 23\nInternet of Things (IoT), 106-109\ninteroperability, 58, 117\nIoT gateways, 107\nirreversible decisions, 83\nisolation (ACID), 158\nJ\nJava, 20\nJava Virtual Machine languages, 20\nJavaScript Object Notation (JSON), 392\njoin strategy, 275\njoins, 286\nJSON Lines (JSONL), 392K\nKappa architecture, 105, 282\nkey-value databases, 169\nkey-value timestamps, 180\nkeys, 180\nKimball data model, 297\nL\nlakehouses, 103\nLambda architecture, 104\nlate-arriving data, 248\nlatency, 161, 192, 349\nleadership, architecture as, 80\nlean practices, 60\nlifecycle management, 58\nlift and shift, 126\nlightweight object caching, 211\nlightweight virtual machines, 144\nlineage, 57\nlinear density (magnetic storage), 192\nlink tables (Data Vault), 303\nlive data stacks, 385-386\nlive tables, 324\nload (ETL), 246\nlocal disk storage, 200\nlocal instance volumes, 204\nlocation, 250\nlog analysis, 173\nlog-based change data captures, 253\nlog-structured merge-trees (LSMs), 166\nlogging, 375\nlogic, 343\nlogical data models, 289\nlogs, 160\nLooker, 356\nlookup data, 54, 207\nlookups, 166\nloosely coupled communication, 83\nloosely coupled systems, 82, 90\nlossless compression algorithms, 396\nlossy compression algorithms, 396\nLSMs (log-structured merge-trees), 166\nlukewarm data, 39, 224\nM\nmachine learning (ML), 46, 349-351, 388\nmachine learning engineers, 27\nmachine learning operations (MLOps), 28\nIndex | 411\nmagnetic disks, 191-193\nmanaged data connectors, 256\nMapReduce, 220, 322\nmassively parallel processing (MPP) databases,\n6, 99\nmaster data management (MDM), 56\nmaterialized views, 324\nmaximum message retention time, 249\nMDSs (modern data stacks), 10, 103, 385\nmeasurement data, 174\nmemcached, 211\nmemory caching, 322\nmemory-based storage systems, 211\nmerge pattern, 317\nmessage queues, 163, 177-179, 255\nmessages\nbasics of, 163\ndelivered out of order, 248\nerror handling and dead-letter queues, 249\nlate-arriving data, 248\nretrieving from history, 249\nsize of, 249\nversus streams, 255\nmetadata, 52-54, 243\nmetrics layers, 355\nmicro-batch approach, 328\nmicro-partitioning, 214\nmicroservices architecture, 93\nmigration, 247\nML (machine learning), 46, 349-351, 388\nMLOps (machine learning operations), 28\nmodeling and design, 57\n(see also data modeling)\nmodeling patterns, 166\nmodern data stacks (MDSs), 10, 103, 385\nmodularization, 83, 139, 140, 142\nmonitoring, 62, 78, 375\nmonolithic architectures, 92, 139-140, 142\nMPP (massively parallel processing) databases,\n6, 99\nmulticloud deployment, 128, 130\nmultiregion networking layer, 401\nmultitenancy support, 46, 94, 176\nmultitenant storage, 226\nmultitier architectures, 91\nmultitier caching, 221\nN\nn-tier architectures, 91NAS (network-attached storage), 201\nnear real-time data ingestion, 41, 237\nnegative thinking, 370\nnested subqueries, 277\nnetwork security, 376\nnetwork-attached storage (NAS), 201\nnetworking (see cloud networking)\nvon Neumann architecture, 195\nNew Technology File System (NTFS), 200\nnode joins, 310\nnodes, 172\nnonrelational (NoSQL) databases, 168-174\ndocument stores, 169\ngraph databases, 172\nhistory of, 168\nkey-value stores, 169\nversus relational databases, 168\nsearch databases, 173\ntime-series databases, 173\nwide-column databases, 171\nnonvolatile random access memory (NVRAM),\n194\nnormal forms, 291\nnormalization, 290\nnormalized schemas, 167\nNoSQL (not only SQL) (see nonrelational data‐\nbases)\nnotebooks, serving data in, 356\nNTFS (New Technology File System), 200\nNVRAM (nonvolatile random access memory),\n194\nO\nobject storage, 200, 205-211\navailability zones and, 206\nbenefits of, 206, 206\nfor data engineering applications, 207\ndefinition of term, 205\nversus file storage, 200\nversus local disk, 206\nmoving data with, 257\nobject consistency and versioning, 208\nobject lookup, 207\nobject store-backed filesystems, 210\npopularity of, 206\nscalability of, 207\nstorage classes and tiers, 210\nobservability, 62, 78, 339\noff-the-shelf data connectors, 381\n412 | Index\non-premises technology stacks, 123, 126\none-size-fits-all technology solutions, 79\none-way doors, 73, 83\nonline analytical processing (OLAP) systems,\n159\nonline transaction processing (OLTP) systems,\n157\nOpen Group Architecture Framework, The\n(TOGAF)\ndata architecture, 75\nenterprise architecture, 72\nopen source software (OSS), 133-137, 381, 388\noperational analytics, 45, 346\noperational architecture, 76\noperational expenses (opex), 119\noperational metadata, 54\nOptimized Row Columnar (ORC), 394\noptional persistence, 211\norchestration\nchoosing components wisely, 78\nimpact on data ingestion, 268\nimpact on data storage, 230\nimpact on queries, transformations, and\nmodeling, 333\nimpact on serving data, 363\nimpact on source systems, 186\nimpact on technology selection, 150\npipelines and, 68\nprocess of, 64\norganizational characteristics, 82\norganizational data warehouse architecture, 98\noverarchitecting, 120\noverhead, 118\nP\nPaaS (platform as a service), 124\nparallel processing databases, 6, 99\nparanoia, 370\nParquet, 393\npartial dependency, 291\npartition keys, 180\npartitioning, 213\npatches, 374\npayloads, 241-243\nperformance, 95\npermissions, monitoring, 376\npersistence, optional, 211\nphysical data models, 289\npipeline metadata, 54pipelines as code, 68\nPIT (point-in-time) tables, 305\nplatform as a service (PaaS), 124\nplumbing tasks, 175\npoint-in-time (PIT) tables, 305\npolyglot applications, 141\nprejoining data, 276\npremium-tier networking, 401\nprerequisites for book, xv\nprimary keys, 167\nprinciple of least privilege, 49, 372\nprivacy, 59, 85, 265, 369 (see also security)\nprocess time, 165\nprocessing engines, choosing, 78\nprocessing time, 165\nproduct managers, 30\nproject managers, 30\nproprietary cloud offerings, 138\nproprietary walled gardens, 137-138\npruning, 278\npublic access, 85\npublishers, 163\npull model of data ingestion, 42, 244, 250\npush model of data ingestion, 42, 244, 246, 250\nPython, 20\nQ\nquality, 55\nqueries\nbasics of, 273-274\nexecution of, 274\nfederated queries, 324, 354\nimproving performance of, 275-281\nstakeholders of, 329\non streaming data, 281-287\ntechniques and languages used for, 272\nversus transformations, 309\nquery engines, 272\nquery optimizers, 166, 275\nquery performance, 349\nquery pushdown, 326, 353\nR\nRAID (redundant array of independent disks),\n202\nrandom access memory (RAM), 194\nRDBMSs (relational database management sys‐\ntems), 35, 157, 167\nreal-time analytical databases, 386\nIndex | 413\nreal-time data ingestion, 41, 106, 237\nreal-time logs, 161\nrecovery point objective (RPO), 79\nrecovery time objective (RTO), 79\nreduce step, 220\nredundancy, 401\nredundant array of independent disks (RAID),\n202\nreference metadata, 54\nregions, 400\nregulations, 226\nrelational database management systems\n(RDBMSs), 35, 157, 167\nrelational schema, 219\nrelations (rows), 167, 169\nreliability, 79, 89, 240\nremote procedure calls (RPCs), 176\nrepatriation, 130\nreplay, 212, 249\nreports, 345\nrepresentational state transfer (REST) APIs, 174\nresilience, 181\nresolution, 161\nresource use, 376\nresume-driven development, 97, 115\nreverse cache, 197\nreverse ETL, 47, 358-360\nreversible decisions, 83\nrotational latency, 192\nrow explosion, 276\nrow-based serialization, 391\nrows, 213, 290\nRPO (recovery point objective), 79\nRTO (recovery time objective), 79\nS\nS3 Standard-Infrequent Access storage class,\n210\nSaaS (software as a service), 124\nSAN (storage area network) systems, 203\nsatellites (Data Vault), 304\nScala, 20\nscalability\narchitecting for, 80\nbenefits of, 88\ndatabases and, 166\ningestion stage and, 239\nmessage systems and, 179\nobject storage and, 207pay-as-you-go systems, 86\nseparation of compute from storage, 220\nscans, full table, 277\nSCDs (slowly changing dimensions), 300\nschedulers, 64\nschema changes, 264\nschema evolution, 248\nschema metadata, 54\nschema on read technique, 219\nschema on write technique, 219\nschema updates, 318\nschemaless option, 37\nschemas\nbasics of, 219\ndefinition of term, 37\ndetecting and handling changes in, 243\nin data payloads, 242\nregistries for, 243\nin relational databases, 167\nstar schema, 301\nsearch databases, 173\nsecond normal form (2NF), 291\nsecure copy (SCP), 259\nsecure FTP (SFTP), 259\nsecurity\ncredential handling in notebooks, 357\nhuman activities and, 370\nimpact on data engineering lifecycle, 49\nimpact on data ingestion, 264\nimpact on data storage, 228\nimpact on queries, transformations, and\nmodeling, 330\nimpact on serving data, 361\nimpact on source systems, 183\nprioritizing, 84\nprocesses and, 371-374\nrole in privacy, 369\nsingle versus multitenant, 95\ntechnology and, 374-378\nzero-trust models, 84\nsecurity policies, 373\nself-service data, 341\nsemantic layers, 355\nsemistructured data, 157\nsequencing plans, 81\nserialization, 195, 239\nserialization formats, 391-395\ncolumnar serialization, 393\nhybrid serialization, 395\n414 | Index\nimproving performance and, 391\nrow-based serialization, 391\nserverless cloud offerings, 143, 146\nservers, considerations when using, 145\nservice-level agreements (SLAs), 183\nservice-level objectives (SLOs), 183\nservices, 87\nserving data stage\nanalytics, 44, 344-349\nbusiness intelligence, 45, 344\nembedded analytics, 45, 348\ngeneral considerations for, 338-344\ngetting value from data, 44\nIoT devices and, 108\nmachine learning, 46, 349-351\nmultitenancy support, 46\noperational analytics, 45, 346\nreverse ETL, 47, 358-360\nstakeholders of, 360\nways to serve data, 351-358\nsession windows, 283\nsessionization, 284\nSFTP (secure FTP), 259\nshape, 241\nshared disk architectures, 92\nshared responsibility security model, 84, 372\nshared-nothing architectures, 92\nshell interface, 258\nshiny object syndrome, 115\nshuffle hash joins, 311\nsingle machine data storage, 198\nsingle-row inserts, 316\nsingle-tenant storage, 226\nsingle-tier architectures, 90\nsite-reliability engineers (SREs), 26\nsize, 241, 249\nsize-based batch ingestion, 245\nSLAs (service-level agreements), 183\nsliding windows, 284\nSLOs (service-level objectives), 183\nslowly changing dimensions (SCDs), 300\nsnapshots, 246\nSnowflake micro-partitioning, 214\nsocial capital and knowledge, 53\nsoft delete, 316\nsoftware as a service (SaaS), 124\nsoftware engineering\ncore data processing code, 66\nimpact on data ingestion, 268impact on data storage, 230\nimpact on queries, transformations, and\nmodeling, 333\nimpact on serving data, 364\nimpact on source systems, 187\nimpact on technology selection, 151\ninfrastructure as code (IaC), 67\nopen source frameworks, 66\npipelines as code, 68\nstreaming data processing, 67\ntransition from coding to dataframes, 66\nsoftware engineers, 25\nsolid-state drives (SSDs), 193\nsource systems (see also generation stage)\napplication database example, 35\navoiding breaks in pipelines and analytics,\n35\nbasics of, 156-165\nAPIs, 157\napplication databases, 157\nchange data capture method, 159\ndatabase logs, 161\nfiles and unstructured data, 156\ninsert-only pattern, 162\nlogs, 160\nmessages and streams, 163\nonline analytical processing (OLAP) sys‐\ntems, 159\ntypes of time, 164\ndefinition of term, 35\nevaluating, 36-37\nimpact of undercurrents on, 183-187\nIoT swarm and message queue example, 36\npractical details, 165-181\nAPIs, 174\ndata sharing, 176\ndatabases, 166\nmessage queues and event-streaming\nplatforms, 177\nthird-party data sources, 177\nstakeholders of, 181\nunique aspects of, 37\nSpark API, 314, 320\nspeed to market, 117\nspending and resources, managing, 85\nspill to disk, 323\nspreadsheets, 388\nSQL (see Structured Query Language)\nSREs (site-reliability engineers), 26\nIndex | 415\nSSDs (solid-state drives), 193\nSSH protocol, 259\nstakeholders\ndownstream, 26\ningestion stage and, 262\nfor queries, transformations, and modeling,\n329\nserving data and, 360\nsource systems and, 181\nupstream, 25\nworking alongside, 111\nstar schema, 301\nstate, 180\nSTL (stream, transform, and load), 387\nstorage area network (SAN) systems, 203\nstorage stage\nbig ideas and trends in storage, 218-227\ndata catalogs, 218\ndata sharing, 219\ndata storage lifecycle and data retention,\n223\nschemas, 219\nseparation of compute from storage, 220\nsingle-tenant versus multitenant storage,\n226\ncaching, 197\ndata engineering storage abstractions,\n215-217\ndata storage systems, 197-214\nblock storage, 202\ncache and memory-based storage, 211\neventual versus strong consistency, 198\nfile storage, 199\nHadoop Distributed File System, 211\nindexes, partitioning, and clustering, 213\nobject storage, 205\nsingle machine versus distributed, 198\nstreaming storage, 212\ndivision of responsibilities for, 227\nIoT devices and, 108\nkey engineering considerations, 38\nlifecycle stages encompassed by, 190\nphysical and software elements of, 190\nraw ingredients of data storage, 191-197\ncompression, 196\nmagnetic disks, 191\nnetworking and CPU, 195\nrandom access memory, 194\nserialization, 195solid-state drives, 193\nselecting storage systems, 39, 78, 189\nstorage solution basics, 38\nunderstanding data access frequency, 39\nstream partitions, 180\nstream, transform, and load (STL), 387\nstream-to-batch storage architecture, 217\nstreaming data ingestion, 40, 237, 248-250\nstreaming data modeling, 307\nstreaming data processing, 67\nstreaming data queries, 281-287\nstreaming directed acyclic graphs, 327\nstreaming pipelines, 386\nstreaming platforms, 163, 354\nstreaming queries, 326\nstreaming transformations, 326-329\nstreams\nbasics of, 164\ncombining with other data, 286\nenriching, 286\nstream-to-stream joining, 286\nstreaming buffers, 287\nstrong consistency, 198, 208\nStructured Query Language (SQL)\nbatch data transformations, 312\nbuilding complex data workflows, 313\ndevelopments based on, 6\neffectiveness of, 20-21\nfocus on, 272\noptimizing processing frameworks, 314\nwhen to avoid, 313\nsubject orientation, 295\nsynchronous data ingestion, 238\nsystems stakeholder, 182\nT\ntable joins, 286\ntable management technologies, 395\ntables, 169\ntarget architecture, 81\ntarget audience for book, xiv\nTCO (total cost of ownership), 118\nTDD (test-driven development), 62\ntechnical architecture, 72, 76\ntechnical coupling, 92\ntechnical data warehouse architecture, 98\ntechnical metadata, 53\ntechnologies (see data technologies)\ntemporary tables, 277\n416 | Index\ntest-driven development (TDD), 62\ntext search, 173\nthird normal form (3NF), 291\nthird-party data sources, 177\nthree-tier architectures, 91\nthroughput, 239\ntiers\nmultitier architectures, 91\npremium-tier networking, 401\nsingle-tier architectures, 90\nstorage classes and, 210, 224\nstructuring layers of architecture, 90\ntight coupling, 90\ntime to live (TTL), 226, 249\ntime, types of, 164\ntime-interval batch ingestion, 245\ntime-series databases, 173\ntimeliness, 56\ntimestamps, 180\nTOCO (total opportunity cost of ownership),\n119\nTOGAF (see Open Group Architecture Frame‐\nwork, The)\ntools, versus architecture, 115\n(see also technologies)\ntopics, 180\ntotal cost of ownership (TCO), 118\ntotal opportunity cost of ownership (TOCO),\n119\ntouchless production, 265\ntransaction control language (TCL), 274\ntransactional databases, 157, 253\ntransactions, 158, 278\ntransfer appliances, 261\ntransform-on-read ELT, 100\ntransformation stage, 43\ntransformations\nbatch transformations, 310-323\ndata virtualization, 325\nfederated queries, 324\nmaterialized views, 324\npurpose of, 309\nversus queries, 309\nstakeholders of, 329\nstreaming transformations and processing,\n326-329, 387\nviews, 323\ntransitive dependency, 291\ntransitory technologies, 121, 130truncate update pattern, 316\ntrust, 338\nTTL (time to live), 226\ntumbling windows, 284\ntwo-way doors, 74, 83\ntypes A and B data scientists, 22\nU\nunbounded data, 106, 236\nundercurrents\nconsiderations for choosing technologies,\n149-151\ndata architecture, 64\ndata management, 50-59\nDataOps, 59-64\nexamples of, 5, 49\nimpact on data storage, 228-230\nimpact on queries, transformations, and\nmodeling, 330-334\nimpact on serving data, 360-365\norchestration, 64, 68\nrole in data engineering, 13, 34\nsecurity, 49\nsoftware engineering, 66-68\ntechnical responsibilities and, 19\nunique primary keys, 291\nunstructured data, 157\nupdate operations, 247, 374\nupdate patterns, 315-319\nupsert pattern, 317\nupstream stakeholders, 25\nuse cases, determining, 339\nV\nvacuuming, 280\nvalidation, 339\nvalue of data, 44\nvalues, 180\nversion metadata, 209\nversion-control systems, selecting, 78\nviews, 323\nvolatility, 194\nvon Neumann architecture, 195\nW\nwarm data, 224\nwaterfall project management, 81\nwatermarks, 285\nIndex | 417\nweb interfaces, 260\nweb scraping, 260\nwebhooks, 176, 259\nwide denormalized tables, 305\nwide tables, 305\nwide-column databases, 171\nwindowing methods, 67, 106, 283-285\nwrite once, read never (WORN), 102X\nXML (Extensible Markup Language), 392\nZ\nzero-copy cloning, 223\nzero-trust security models, 84\n418 | Index",40298
193-About the Authors.pdf,193-About the Authors,,0
194-Colophon.pdf,194-Colophon,"About the Authors\nJoe Reis  is a business-minded data nerd who’s worked in the data industry for 20\nyears, with responsibilities ranging from statistical modeling, forecasting, machine\nlearning, data engineering, data architecture, and almost everything else in between.\nJoe is the CEO and cofounder of Ternary Data, a data engineering and architecture\nconsulting firm based in Salt Lake City, Utah. In addition, he volunteers with several\ntechnology groups and teaches at the University of Utah. In his spare time, Joe likes\nto rock climb, produce electronic music, and take his kids on crazy adventures.\nMatt Housley  is a data engineering consultant and cloud specialist. After some early\nprogramming experience with Logo, Basic, and 6502 assembly, he completed a PhD\nin mathematics at the University of Utah. Matt then began working in data science,\neventually specializing in cloud-based data engineering. He cofounded Ternary Data\nwith Joe Reis, where he leverages his teaching experience to train future data engi‐\nneers and advise teams on robust data architecture. Matt and Joe also pontificate on\nall things data on The Monday Morning Data Chat .\nColophon\nThe animal on the cover of Fundamentals of Data Engineering  is the white-eared\npuffbird ( Nystalus chacuru ).\nSo named for the conspicuous patch of white at their ears, as well as for their fluffy\nplumage, these small, rotund birds are found across a wide swath of central South\nAmerica, where they inhabit forest edges and savanna.\nWhite-eared puffbirds are sit-and-wait hunters, perching in open spaces for long\nperiods and feeding opportunistically on insects, lizards, and even small mammals\nthat happen to come near. They are most often found alone or in pairs and are\nrelatively quiet birds, vocalizing only rarely.\nThe International Union for Conservation of Nature has listed the white-eared\npuffbird as being of least concern , due, in part, to their extensive range and stable\npopulation. Many of the animals on O’Reilly covers are endangered; all of them are\nimportant to the world.\nThe cover illustration is by Karen Montgomery, based on an antique line engraving\nfrom Shaw’s General Zoology . The cover fonts are Gilroy Semibold and Guardian\nSans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Con‐\ndensed; and the code font is Dalton Maag’s Ubuntu Mono.\nLearn from experts.  \nBecome one yourself.\nBooks | Live online courses   \nInstant Answers | Virtual events  \nVideos | Interactive learning\nGet started at oreilly.com.  \n©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175",2656
