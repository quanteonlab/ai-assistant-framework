filename,title,text,len
01-How This Book Is Organized.pdf,01-How This Book Is Organized,"Preface\nW A R N I N G\nThe Preface has not been updated from the First Edition . This will be the last part of the\nbook to be updated for the Second Edition .\nHer e’ s the plan: when someone uses a featur e you don’ t understand,\nsimply shoot them. This is easier than learning something new , and\nbefor e too long the only living coders will be writing in an easily\nunderstood, tiny subset of Python 0.9.6 <wink>.\n— T im Peters, Legendary core developer and author of\nThe Zen of Python\n“Python is an easy to learn, powerful programming language.” Those are\nthe first words of the of ficial Python T utorial . That is true, but there is a\ncatch: because the language is easy to learn and put to use, many practicing\nPython programmers leverage only a fraction of its powerful features.\nAn experienced programmer may start writing useful Python code in a\nmatter of hours. As the first productive hours become weeks and months, a\nlot of developers go on writing Python code with a very strong accent\ncarried from languages learned before. Even if Python is your first\nlanguage, often in academia and in introductory books it is presented while\ncarefully avoiding language-specific features.\nAs a teacher introducing Python to programmers experienced in other\nlanguages, I see another problem that this book tries to address: we only\nmiss stuf f we know about. Coming from another language, anyone may\nguess that Python supports regular expressions, and look that up in the docs.\nBut if you’ve never seen tuple unpacking or descriptors before, you will\nprobably not search for them, and may end up not using those features just\nbecause they are specific to Python.1\nThis book is not an A-to-Z exhaustive reference of Python. Its emphasis is\non the language features that are either unique to Python or not found in\nmany other popular languages. This is also mostly a book about the core\nlanguage and some of its libraries. I will rarely talk about packages that are\nnot in the standard library , even though the Python package index now lists\nmore than 60,000 libraries and many of them are incredibly useful.\nW h o  T h i s  B o o k  I s  F o r\nThis book was written for practicing Python programmers who want to\nbecome proficient in Python 3. If you know Python 2 but are willing to\nmigrate to Python 3.4 or later , you should be fine. At the time of this\nwriting, the majority of professional Python programmers are using Python\n2, so I took special care to highlight Python 3 features that may be new to\nthat audience.\nHowever , Fluent Python  is about making the most of Python 3.4, and I do\nnot spell out the fixes needed to make the code work in earlier versions.\nMost examples should run in Python 2.7 with little or no changes, but in\nsome cases, backporting would require significant rewriting.\nHaving said that, I believe this book may be useful even if you must stick\nwith Python 2.7, because the core concepts are still the same. Python 3 is\nnot a new language, and most dif ferences can be learned in an afternoon.\nWhat’ s New in Python 3.0  is a good starting point. Of course, there have\nbeen changes since Python 3.0 was released in 2009, but none as important\nas those in 3.0.\nIf you are not sure whether you know enough Python to follow along,\nreview the topics of the of ficial Python T utorial . T opics covered in the\ntutorial will not be explained here, except for some features that are new in\nPython 3.\nW h o  T h i s  B o o k  I s  N o t  F o r\nIf you are just learning Python, this book is going to be hard to follow . Not\nonly that, if you read it too early in your Python journey , it may give you\nthe impression that every Python script should leverage special methods\nand metaprogramming tricks. Premature abstraction is as bad as premature\noptimization.\nH o w  T h i s  B o o k  I s  O r g a n i z e d\nThe core audience for this book should not have trouble jumping directly to\nany chapter in this book. However , each of the six parts forms a book\nwithin the book. I conceived the chapters within each part to be read in\nsequence.\nI tried to emphasize using what is available before discussing how to build\nyour own. For example, in Part II , Chapter 2  covers sequence types that are\nready to use, including some that don’ t get a lot of attention, like\ncollections.deque . Building user -defined sequences is only\naddressed in Part IV , where we also see how to leverage the abstract base\nclasses (ABCs) from collections.abc . Creating your own ABCs is\ndiscussed even later in Part IV , because I believe it’ s important to be\ncomfortable using an ABC before writing your own.\nThis approach has a few advantages. First, knowing what is ready to use\ncan save you from reinventing the wheel. W e use existing collection classes\nmore often than we implement our own, and we can give more attention to\nthe advanced usage of available tools by deferring the discussion on how to\ncreate new ones. W e are also more likely to inherit from existing ABCs than\nto create a new ABC from scratch. And finally , I believe it is easier to\nunderstand the abstractions after you’ve seen them in action.\nThe downside of this strategy are the forward references scattered\nthroughout the chapters. I hope these will be easier to tolerate now that you\nknow why I chose this path.\nHere are the main topics in each part of the book:\nPart I, Pr ologue\nA single chapter about the Python Data Model explaining how the\nspecial methods (e.g., __repr__ ) are the key to the consistent\nbehavior of objects of all types—in a language that is admired for its\nconsistency . Understanding various facets of the data model is the\nsubject of most of the rest of the book, but Chapter 1  provides a high-\nlevel overview .\nPart II, Data Structur es\nThe chapters in this part cover the use of collection types: sequences,\nmappings, and sets, as well as the str  versus bytes  split—the cause\nof much celebration among Python 3 users and much pain for Python 2\nusers who have not yet migrated their codebases. The main goals are to\nrecall what is already available and to explain some behavior that is\nsometimes surprising, like the reordering of dict  keys when we are\nnot looking, or the caveats of locale-dependent Unicode string sorting.\nT o achieve these goals, the coverage is sometimes high level and wide\n(e.g., when many variations of sequences and mappings are presented)\nand sometimes deep (e.g., when we dive into the hash tables underneath\nthe dict  and set  types).\nPart III, Functions as Objects\nHere we talk about functions as first-class objects in the language: what\nthat means, how it af fects some popular design patterns, and how to\nimplement function decorators by leveraging closures. Also covered\nhere is the general concept of callables in Python, function attributes,\nintrospection, parameter annotations, and the new nonlocal\ndeclaration in Python 3.\nPart IV , Classes and Pr otocols\nNow the focus is on building classes. In Part II , the class  declaration\nappears in few examples; Part IV  presents many classes. Like any\nobject-oriented (OO) language, Python has its particular set of features\nthat may or may not be present in the language in which you and I\nlearned class-based programming. The chapters explain how references\nwork, what mutability really means, the lifecycle of instances, how to\nbuild your own collections and ABCs, how to cope with multiple\ninheritance, and how to implement operator overloading—when that\nmakes sense.\n[Link to Come]",7597
02-Hardware Used for Timings.pdf,02-Hardware Used for Timings,"Covered in this part are the language constructs and libraries that go\nbeyond sequential control flow with conditionals, loops, and\nsubroutines. W e start with generators, then visit context managers and\ncoroutines, including the challenging but powerful new yield from\nsyntax. [Link to Come] closes with a high-level introduction to modern\nconcurrency in Python with collections.futures  (using threads\nand processes under the covers with the help of futures) and doing\nevent-oriented I/O with asyncio  (leveraging futures on top of\ncoroutines and yield from ).\n[Link to Come]\nThis part starts with a review of techniques for building classes with\nattributes created dynamically to handle semi-structured data such as\nJSON datasets. Next, we cover the familiar properties mechanism,\nbefore diving into how object attribute access works at a lower level in\nPython using descriptors. The relationship between functions, methods,\nand descriptors is explained. Throughout [Link to Come], the step-by-\nstep implementation of a field validation library uncovers subtle issues\nthat lead to the use of the advanced tools of the final chapter: class\ndecorators and metaclasses.\nH a n d s - O n  A p p r o a c h\nOften we’ll use the interactive Python console to explore the language and\nlibraries. I feel it is important to emphasize the power of this learning tool,\nparticularly for those readers who’ve had more experience with static,\ncompiled languages that don’ t provide a read-eval-print loop (REPL).\nOne of the standard Python testing packages, doctest , works by\nsimulating console sessions and verifying that the expressions evaluate to\nthe responses shown. I used doctest  to check most of the code in this\nbook, including the console listings. Y ou don’ t need to use or even know\nabout doctest  to follow along: the key feature of doctests is that they",1884
03-Python Jargon.pdf,03-Python Jargon,"look like transcripts of interactive Python console sessions, so you can\neasily try out the demonstrations yourself.\nSometimes I will explain what we want to accomplish by showing a doctest\nbefore the code that makes it pass. Firmly establishing what is to be done\nbefore thinking about how to do it helps focus our coding ef fort. W riting\ntests first is the basis of test driven development (TDD) and I’ve also found\nit helpful when teaching. If you are unfamiliar with doctest , take a look\nat its documentation  and this book’ s source code repository . Y ou’ll find that\nyou can verify the correctness of most of the code in the book by typing\npython3 -m doctest example_script.py  in the command shell\nof your OS.\nH a r d w a r e  U s e d  f o r  T i m i n g s\nThe book has some simple benchmarks and timings. Those tests were\nperformed on one or the other laptop I used to write the book: a 201 1\nMacBook Pro 13″ with a 2.7 GHz Intel Core i7 CPU, 8GB of RAM, and a\nspinning hard disk, and a 2014 MacBook Air 13″ with a 1.4 GHz Intel Core\ni5 CPU, 4GB of RAM, and a solid-state disk. The MacBook Air has a\nslower CPU and less RAM, but its RAM is faster (1600 versus 1333 MHz)\nand the SSD is much faster than the HD. In daily usage, I can’ t tell which\nmachine is faster .\nS o a p b o x :  M y  P e r s o n a l  P e r s p e c t i v e\nI have been using, teaching, and debating Python since 1998, and I enjoy\nstudying and comparing programming languages, their design, and the\ntheory behind them. At the end of some chapters, I have added “Soapbox”\nsidebars with my own perspective about Python and other languages. Feel\nfree to skip these if you are not into such discussions. Their content is\ncompletely optional.",1743
04-Python Version Covered.pdf,04-Python Version Covered,,0
05-Acknowledgments.pdf,05-Acknowledgments,"P y t h o n  J a r g o n\nI wanted this to be a book not only about Python but also about the culture\naround it. Over more than 20 years of communications, the Python\ncommunity has developed its own particular lingo and acronyms. Here\nyou’ll see that some words—like “decorator”, “descriptor”, and\n“protocol”—have special meaning among Pythonistas. Y ou’ll also get\nfluent with Python slang like “dunder”, “listcomp”, and “genexp”.\nP y t h o n  V e r s i o n  C o v e r e d\nI tested all the code in the book using Python 3.4—that is, CPython 3.4, the\nmost popular Python implementation written in C. There is only one\nexception: “Using @  as an infix operator”  shows the @  operator , which is\nonly supported by Python 3.5.\nAlmost all code in the book should work with any Python 3.x–compatible\ninterpreter , including PyPy3 2.4.0, which is compatible with Python 3.2.5.\nThe notable exceptions are the examples using yield from  and\nasyncio , which are only available in Python 3.3 or later .\nMost code should also work with Python 2.7 with minor changes, except\nthe Unicode-related examples in Chapter 4 , and the exceptions already\nnoted for Python 3 versions earlier than 3.3.\nC o n v e n t i o n s  U s e d  i n  T h i s  B o o k\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file\nextensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to\nprogram elements such as variable or function names, databases, data\ntypes, environment variables, statements, and keywords.\nNote that when a line break falls within a constant_width  term, a\nhyphen is not added—it could be misunderstood as part of the term.\nConstant width bold\nShows commands or other text that should be typed literally by the user .\nConstant width italic\nShows text that should be replaced with user -supplied values or by\nvalues determined by context.\nT I P\nThis element signifies a tip or suggestion.\nN O T E\nThis element signifies a general note.\nW A R N I N G\nThis element indicates a warning or caution.\nU s i n g  C o d e  E x a m p l e s\nEvery script and most code snippets that appear in the book are available in\nthe Fluent Python code repository  on GitHub.\nW e appreciate, but do not require, attribution. An attribution usually\nincludes the title, author , publisher , and ISBN. For example: “ Fluent Python\nby Luciano Ramalho (O’Reilly). Copyright 2015 Luciano Ramalho, 978-1-\n491-94600-8.”\nH o w  t o  C o n t a c t  U s\nPlease address comments and questions concerning this book to the\npublisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nW e have a web page for this book, where we list errata, examples, and any\nadditional information. Y ou can access this page at http://bit.ly/fluent-\npython .\nT o comment or ask technical questions about this book, send email to\nbookquestions@or eilly .com .\nFor more information about our books, courses, conferences, and news, see\nour website at http://www .or eilly .com .\nFind us on Facebook: http://facebook.com/or eilly\nFollow us on T witter: http://twitter .com/or eillymedia\nW atch us on Y ouT ube: http://www .youtube.com/or eillymedia\nA c k n o w l e d g m e n t s\nThe Bauhaus chess set by Josef Hartwig is an example of excellent design:\nbeautiful, simple, and clear . Guido van Rossum, son of an architect and\nbrother of a master font designer , created a masterpiece of language design.\nI love teaching Python because it is beautiful, simple, and clear .\nAlex Martelli and Anna Ravenscroft were the first people to see the outline\nof this book and encouraged me to submit it to O’Reilly for publication.\nTheir books taught me idiomatic Python and are models of clarity , accuracy ,\nand depth in technical writing. Alex’ s 5,000+ Stack Overflow posts  are a\nfountain of insights about the language and its proper use.\nMartelli and Ravenscroft were also technical reviewers of this book, along\nwith Lennart Regebro and Leonardo Rochael. Everyone in this outstanding\ntechnical review team has at least 15 years of Python experience, with many\ncontributions to high-impact Python projects in close contact with other\ndevelopers in the community . T ogether they sent me hundreds of\ncorrections, suggestions, questions, and opinions, adding tremendous value\nto the book. V ictor Stinner kindly reviewed Chapter 22 , bringing his\nexpertise as an asyncio  maintainer to the technical review team. It was a\ngreat privilege and a pleasure to collaborate with them over these past\nseveral months.\nEditor Meghan Blanchette was an outstanding mentor , helping me improve\nthe or ganization and flow of the book, letting me know when it was boring,\nand keeping me from delaying even more. Brian MacDonald edited\nchapters in Part III  while Meghan was away . I enjoyed working with them,\nand with everyone I’ve contacted at O’Reilly , including the Atlas\ndevelopment and support team (Atlas is the O’Reilly book publishing\nplatform, which I was fortunate to use to write this book).\nMario Domenech Goulart provided numerous, detailed suggestions starting\nwith the first Early Release. I also received valuable feedback from Dave\nPawson, Elias Dorneles, Leonardo Alexandre Ferreira Leite, Bruce Eckel, J.\nS. Bueno, Rafael Gonçalves, Alex Chiaranda, Guto Maia, Lucas V ido, and\nLucas Brunialti.\nOver the years, a number of people ur ged me to become an author , but the\nmost persuasive were Rubens Prates, Aurelio Jar gas, Rudá Moura, and\nRubens Altimari. Mauricio Bussab opened many doors for me, including\nmy first real shot at writing a book. Renzo Nuccitelli supported this writing\nproject all the way , even if that meant a slow start for our partnership at\npython.pr o.br .\nThe wonderful Brazilian Python community is knowledgeable, generous,\nand fun. The Python Brasil group  has thousands of people and our national\nconferences bring together hundreds, but the most influential in my journey\nas a Pythonista were Leonardo Rochael, Adriano Petrich, Daniel\nV ainsencher , Rodrigo RBP Pimentel, Bruno Gola, Leonardo Santagada,\nJean Ferri, Rodrigo Senra, J. S. Bueno, David Kwast, Luiz Irber , Osvaldo\nSantana, Fernando Masanori, Henrique Bastos, Gustavo Niemayer , Pedro\nW erneck, Gustavo Barbieri, Lalo Martins, Danilo Bellini, and Pedro\nKroger .\nDorneles T remea was a great friend (incredibly generous with his time and\nknowledge), an amazing hacker , and the most inspiring leader of the\nBrazilian Python Association. He left us too early .\nMy students over the years taught me a lot through their questions, insights,\nfeedback, and creative solutions to problems. Érico Andrei and Simples\nConsultoria made it possible for me to focus on being a Python teacher for\nthe first time.\nMartijn Faassen was my Grok mentor and shared invaluable insights with\nme about Python and Neanderthals. His work and that of Paul Everitt, Chris\nMcDonough, T res Seaver , Jim Fulton, Shane Hathaway , Lennart Regebro,\nAlan Runyan, Alexander Limi, Martijn Pieters, Godefroid Chapelle, and\nothers from the Zope, Plone, and Pyramid planets have been decisive in my\ncareer . Thanks to Zope and surfing the first web wave, I was able to start\nmaking a living with Python in 1998. José Octavio Castro Neves was my\npartner in the first Python-centric software house in Brazil.\nI have too many gurus in the wider Python community to list them all, but\nbesides those already mentioned, I am indebted to Steve Holden, Raymond\nHettinger , A.M. Kuchling, David Beazley , Fredrik Lundh, Doug Hellmann,\nNick Coghlan, Mark Pilgrim, Martijn Pieters, Bruce Eckel, Michele\nSimionato, W esley Chun, Brandon Craig Rhodes, Philip Guo, Daniel\nGreenfeld, Audrey Roy , and Brett Slatkin for teaching me new and better\nways to teach Python.\nMost of these pages were written in my home of fice and in two labs:\nCof feeLab and Garoa Hacker Clube. Cof feeLab  is the caf feine-geek\nheadquarters in V ila Madalena, São Paulo, Brazil. Garoa Hacker Clube  is a\nhackerspace open to all: a community lab where anyone can freely try out\nnew ideas.\nThe Garoa community provided inspiration, infrastructure, and slack. I\nthink Aleph would enjoy this book.\nMy mother , Maria Lucia, and my father , Jairo, always supported me in\nevery way . I wish he was here to see the book; I am glad I can share it with\nher .\nMy wife, Marta Mello, endured 15 months of a husband who was always\nworking, but remained supportive and coached me through some critical\nmoments in the project when I feared I might drop out of the marathon.\nThank you all, for everything.\n1  Message to the comp.lang.python Usenet group, Dec. 23, 2002: “Acrimony in c.l.p.”",8906
06-1. The Python Data Model.pdf,06-1. The Python Data Model,"Part I. Prologue\nChapter 1. The Python Data\nModel\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 1st chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nGuido’ s sense of the aesthetics of language design is amazing. I’ve met\nmany fine language designers who could build theor etically beautiful\nlanguages that no one would ever use, but Guido is one of those rar e\npeople who can build a language that is just slightly less theor etically\nbeautiful but ther eby is a joy to write pr ograms in.\n— Jim Hugunin, Creator of Jython, cocreator of AspectJ,\narchitect of the .Net DLR\nOne of the best qualities of Python is its consistency . After working with\nPython for a while, you are able to start making informed, correct guesses\nabout features that are new to you.\nHowever , if you learned another object-oriented language before Python,\nyou may find it strange to use len(collection)  instead of\ncollection.len() . This apparent oddity is the tip of an iceber g that,\nwhen properly understood, is the key to everything we call Pythonic . The1\niceber g is called the Python Data Model, and it is the API that we use to\nmake our own objects play well with the most idiomatic language features.\nY ou can think of the data model as a description of Python as a framework.\nIt formalizes the interfaces of the building blocks of the language itself,\nsuch as sequences, functions, iterators, coroutines, classes, context\nmanagers, and so on.\nWhen using a framework, we spend a lot of time coding methods that are\ncalled by the framework. The same happens when we leverage the Python\nData Model to build new classes. The Python interpreter invokes special\nmethods to perform basic object operations, often triggered by special\nsyntax. The special method names are always written with leading and\ntrailing double underscores. For example, the syntax obj[key]  is\nsupported by the __getitem__  special method. In order to evaluate\nmy_collection[key] , the interpreter calls\nmy_collection.__getitem__(key) .\nW e implement special methods when we want our objects to support and\ninteract with fundamental language constructs such as:\nCollections;\nAttribute access;\nIteration (including asynchronous iteration using async for );\nOperator overloading;\nFunction and method invocation;\nString representation and formatting;\nAsynchronous programing using await ;\nObject creation and destruction;\nManaged contexts using the with  or async with  statements.",2985
07-A Pythonic Card Deck.pdf,07-A Pythonic Card Deck,"M A G I C  A N D  D U N D E R\nThe term magic method  is slang for special method, but how do we talk about a specific\nmethod like __getitem__ ? I learned to say “dunder -getitem” from author and teacher\nSteve Holden. “Dunder” is a shortcut for “double underscore before and after”. That’ s\nwhy the special methods are also known as dunder methods . The Lexical Analysis\nchapter of The Python Language Refer ence  warns that "" Any  use of __*__ names, in any\ncontext, that does not follow explicitly documented use, is subject to breakage without\nwarning.”\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThis chapter had few changes from the first edition because it is an\nintroduction to the Python Data Model, which is quite stable. The most\nsignificant changes are:\nSpecial methods supporting asynchronous programming and other\nnew features, added to the tables in “Overview of Special\nMethods” .\nFigure 1-2  showing the use of special methods in “Collection\nAPI” , including the collections.abc.Collection  abstract\nbase class introduced in Python 3.6.\nAlso, here and throughout this Second Edition  I adopted the f-string  syntax\nintroduced in Python 3.6, which is more readable and often more\nconvenient than the older string formatting notations: the str.format()\nmethod and the %  operator .\nT I P\nOne reason to still use my_fmt.format()  is when the definition of my_fmt  must be\nin a dif ferent place in the code than where the formatting operation needs to happen. For\ninstance, when my_fmt  has multiple lines and is better defined in a constant, or when it\nmust come from a configuration file, or from the database. Those are real needs, but\ndon’ t happen very often.\nA  P y t h o n i c  C a r d  D e c k\nExample 1-1  is simple, but it demonstrates the power of implementing just\ntwo special methods, __getitem__  and __len__ .\nExample 1-1. A deck as a sequence of playing car ds\nimport collections  \n \nCard = collections .namedtuple ('Card', ['rank', 'suit']) \n \nclass FrenchDeck : \n    ranks = [str(n) for n in range(2, 11)] + list('JQKA') \n    suits = 'spades diamonds clubs hearts' .split() \n \n    def __init__ (self): \n        self._cards = [Card(rank, suit) for suit in self.suits \n                                        for rank in self.ranks] \n \n    def __len__(self): \n        return len(self._cards) \n \n    def __getitem__ (self, position ): \n        return self._cards[position ]\nThe first thing to note is the use of collections.namedtuple  to\nconstruct a simple class to represent individual cards. W e use\nnamedtuple  to build classes of objects that are just bundles of attributes\nwith no custom methods, like a database record. In the example, we use it to\nprovide a nice representation for the cards in the deck, as shown in the\nconsole session:\n>>> beer_card  = Card('7', 'diamonds' ) \n>>> beer_card  \nCard(rank='7', suit='diamonds')\nBut the point of this example is the FrenchDeck  class. It’ s short, but it\npacks a punch. First, like any standard Python collection, a deck responds to\nthe len()  function by returning the number of cards in it:\n>>> deck = FrenchDeck () \n>>> len(deck) \n52\nReading specific cards from the deck—say , the first or the last—is easy ,\nthanks to the __getitem__  method:\n>>> deck[0] \nCard(rank='2', suit='spades')  \n>>> deck[-1] \nCard(rank='A', suit='hearts')\nShould we create a method to pick a random card? No need. Python already\nhas a function to get a random item from a sequence: random.choice .\nW e can use it on a deck instance:\n>>> from random import choice \n>>> choice(deck) \nCard(rank='3', suit='hearts')  \n>>> choice(deck) \nCard(rank='K', suit='spades')  \n>>> choice(deck) \nCard(rank='2', suit='clubs')\nW e’ve just seen two advantages of using special methods to leverage the\nPython Data Model:\nUsers of your classes don’ t have to memorize arbitrary method\nnames for standard operations (“How to get the number of items?\nIs it .size() , .length() , or what?”).\nIt’ s easier to benefit from the rich Python standard library and\navoid reinventing the wheel, like the random.choice  function.\nBut it gets better .\nBecause our __getitem__  delegates to the []  operator of\nself._cards , our deck automatically supports slicing. Here’ s how we\nlook at the top three cards from a brand-new deck, and then pick just the\nAces by starting at index 12 and skipping 13 cards at a time:\n>>> deck[:3] \n[Card(rank='2', suit='spades'), Card(rank='3', suit='spades'),  \nCard(rank='4', suit='spades')]  \n>>> deck[12::13] \n[Card(rank='A', suit='spades'), Card(rank='A', suit='diamonds'),  \nCard(rank='A', suit='clubs'), Card(rank='A', suit='hearts')]\nJust by implementing the __getitem__  special method, our deck is also\niterable:\n>>> for card in deck:  # doctest: +ELLIPSIS  \n...   print(card) \nCard(rank='2', suit='spades')  \nCard(rank='3', suit='spades')  \nCard(rank='4', suit='spades')  \n...\nW e can also iterate over the deck in reverse:\n>>> for card in reversed (deck):  # doctest: +ELLIPSIS  \n...   print(card) \nCard(rank='A', suit='hearts')  \nCard(rank='K', suit='hearts')  \nCard(rank='Q', suit='hearts')  \n...\nE L L I P S I S  I N  D O C T E S T S\nWhenever possible, I extracted the Python console listings in this book from doctests  to\nensure accuracy . When the output was too long, the elided part is marked by an ellipsis\n(... ) like in the last line in the preceding code. In such cases, I used the # doctest:\n+ELLIPSIS  directive to make the doctest pass. If you are trying these examples in the\ninteractive console, you may omit the doctest comments altogether .\nIteration is often implicit. If a collection has no __contains__  method,\nthe in  operator does a sequential scan. Case in point: in  works with our\nFrenchDeck  class because it is iterable. Check it out:\n>>> Card('Q', 'hearts' ) in deck \nTrue \n>>> Card('7', 'beasts' ) in deck \nFalse\nHow about sorting? A common system of ranking cards is by rank (with\naces being highest), then by suit in the order of spades (highest), hearts,\ndiamonds, and clubs (lowest). Here is a function that ranks cards by that\nrule, returning 0  for the 2 of clubs and 51  for the ace of spades:\nsuit_values  = dict(spades=3, hearts=2, diamonds =1, clubs=0) \n \ndef spades_high (card): \n    rank_value  = FrenchDeck .ranks.index(card.rank) \n    return rank_value  * len(suit_values ) + suit_values [card.suit]\nGiven spades_high , we can now list our deck in order of increasing\nrank:\n>>> for card in sorted(deck, key=spades_high ):  # doctest:  \n+ELLIPSIS  \n...      print(card) \nCard(rank='2', suit='clubs')  \nCard(rank='2', suit='diamonds')  \nCard(rank='2', suit='hearts')  \n... (46 cards omitted) \nCard(rank='A', suit='diamonds')  \nCard(rank='A', suit='hearts')  \nCard(rank='A', suit='spades')\nAlthough FrenchDeck  implicitly inherits from the object  class, most\nof its functionality is not inherited, but comes from leveraging the data\nmodel and composition. By implementing the special methods __len__\nand __getitem__ , our FrenchDeck  behaves like a standard Python\nsequence, allowing it to benefit from core language features (e.g., iteration\nand slicing) and from the standard library , as shown by the examples using\nrandom.choice , reversed , and sorted . Thanks to composition, the\n__len__  and __getitem__  implementations can delegate all the work\nto a list  object, self._cards .",7464
08-Emulating Numeric Types.pdf,08-Emulating Numeric Types,"H O W  A B O U T  S H U F F L I N G ?\nAs implemented so far , a FrenchDeck  cannot be shuf fled, because it is immutable : the\ncards, and their positions cannot be changed, except by violating encapsulation and\nhandling the _cards  attribute directly . In Chapter 13 , we will fix that by adding a one-\nline __setitem__  method.\nH o w  S p e c i a l  M e t h o d s  A r e  U s e d\nThe first thing to know about special methods is that they are meant to be\ncalled by the Python interpreter , and not by you. Y ou don’ t write\nmy_object.__len__() . Y ou write len(my_object)  and, if\nmy_object  is an instance of a user -defined class, then Python calls the\n__len__  method you implemented.\nBut the interpreter takes a shortcut when dealing for built-in types like\nlist , str , bytearray , or extensions like the NumPy arrays. Python\nvariable-sized collections written in C include a struct  called\nPyVarObject , which has an ob_size  field holding the number of items\nin the collection. So, if my_object  is an instance of one of those built-ins,\nthen len(my_object)  retrieves the value of the ob_size  field, and\nthis is much faster than calling a method.\nMore often than not, the special method call is implicit. For example, the\nstatement for i in x:  actually causes the invocation of iter(x) ,\nwhich in turn may call x.__iter__()  if that is available, or use\nx.__getitem__() —as in the FrenchDeck  example.\nNormally , your code should not have many direct calls to special methods.\nUnless you are doing a lot of metaprogramming, you should be\nimplementing special methods more often than invoking them explicitly .\nThe only special method that is frequently called by user code directly is\n__init__ , to invoke the initializer of the superclass in your own\n__init__  implementation.2\nIf you need to invoke a special method, it is usually better to call the related\nbuilt-in function (e.g., len , iter , str , etc). These built-ins call the\ncorresponding special method, but often provide other services and—for\nbuilt-in types—are faster than method calls. See, for example, “A Closer\nLook at the iter Function”  in Chapter 17 .\nIn the next sections, we’ll see some of the most important uses of special\nmethods:\nEmulating numeric types;\nString representation of objects;\nBoolean value of an object;\nImplementing collections.\nEmulating Numeric T ypes\nSeveral special methods allow user objects to respond to operators such as\n+ . W e will cover that in more detail in Chapter 16 , but here our goal is to\nfurther illustrate the use of special methods through another simple\nexample.\nW e will implement a class to represent two-dimensional vectors—that is\nEuclidean vectors like those used in math and physics (see Figure 1-1 ).\nFigur e 1-1. Example of two-dimensional vector addition; V ector(2, 4) + V ector(2, 1) r esults in\nV ector(4, 5).\nT I P\nThe built-in complex  type can be used to represent two-dimensional vectors, but our\nclass can be extended to represent n -dimensional vectors. W e will do that in Chapter 17 .\nW e will start by designing the API for such a class by writing a simulated\nconsole session that we can use later as a doctest. The following snippet\ntests the vector addition pictured in Figure 1-1 :\n>>> v1 = Vector(2, 4) \n>>> v2 = Vector(2, 1) \n>>> v1 + v2 \nVector(4, 5)\nNote how the +  operator results in a new Vector , displayed in a friendly\nformat at the console.\nThe abs  built-in function returns the absolute value of integers and floats,\nand the magnitude of complex  numbers, so to be consistent, our API also\nuses abs  to calculate the magnitude of a vector:\n>>> v = Vector(3, 4) \n>>> abs(v) \n5.0\nW e can also implement the *  operator to perform scalar multiplication (i.e.,\nmultiplying a vector by a number to make a new vector with the same\ndirection and a multiplied magnitude):\n>>> v * 3 \nVector(9, 12)  \n>>> abs(v * 3) \n15.0\nExample 1-2  is a Vector  class implementing the operations just described,\nthrough the use of the special methods __repr__ , __abs__ , __add__\nand __mul__ .\nExample 1-2. A simple two-dimensional vector class\n"""""" \nvector2d.py: a simplistic class demonstrating some special methods  \n \nIt is simplistic for didactic reasons. It lacks proper error  \nhandling,  \nespecially in the ``__add__`` and ``__mul__`` methods.  \n \nThis example is greatly expanded later in the book.  \n \nAddition::  \n \n    >>> v1 = Vector(2, 4)  \n    >>> v2 = Vector(2, 1)  \n    >>> v1 + v2  \n    Vector(4, 5)  \n \nAbsolute value::  \n \n    >>> v = Vector(3, 4)  \n    >>> abs(v)  \n    5.0 \n \nScalar multiplication::  \n \n    >>> v * 3  \n    Vector(9, 12)  \n    >>> abs(v * 3)  \n    15.0  \n \n"""""" \n \n \nimport math \n \nclass Vector: \n \n    def __init__ (self, x=0, y=0): \n        self.x = x \n        self.y = y \n \n    def __repr__ (self): \n        return f'Vector( {self.x!r} , {self.y!r} )' \n \n    def __abs__(self): \n        return math.hypot(self.x, self.y)",5019
09-String Representation.pdf,09-String Representation,"def __bool__ (self): \n        return bool(abs(self)) \n \n    def __add__(self, other): \n        x = self.x + other.x \n        y = self.y + other.y \n        return Vector(x, y) \n \n    def __mul__(self, scalar): \n        return Vector(self.x * scalar, self.y * scalar)\nW e implemented five special methods in addition to the familiar\n__init__ . Note that none of them is directly called within the class or in\nthe typical usage of the class illustrated by the doctests. As mentioned\nbefore, the Python interpreter is the only frequent caller of most special\nmethods.\nExample 1-2  implements two operators: +  and * , to show basic usage of\n__add__  and __mul__ . In both cases, the methods create and return a\nnew instance of Vector , and do not modify either operand— self  or\nother  are merely read. This is the expected behavior of infix operators: to\ncreate new objects and not touch their operands. I will have a lot more to\nsay about that in Chapter 16 .\nW A R N I N G\nAs implemented, Example 1-2  allows multiplying a Vector  by a number , but not a\nnumber by a Vector , which violates the commutative property of scalar multiplication.\nW e will fix that with the special method __rmul__  in Chapter 16 .\nIn the following sections, we discuss the code for the other special methods\nin Vector .\nString Representation\nThe __repr__  special method is called by the repr  built-in to get the\nstring representation of the object for inspection. W ithout a custom\n__repr__ , Python’ s console would display a Vector  instance <Vector\nobject at 0x10e100070> .\nThe interactive console and debugger call repr  on the results of the\nexpressions evaluated, as does the %r  placeholder in classic formatting with\nthe %  operator , and the !r  conversion field in the new Format String Syntax\nused in f-strings  the str.format  method.\nNote that the f-string  in our __repr__ , uses !r  to get the standard\nrepresentation of the attributes to be displayed. This is good practice,\nbecause it shows the crucial dif ference between Vector(1, 2)  and\nVector('1', '2') —the latter would not work in the context of this\nexample, because the constructor ’ s ar guments should be numbers, not str .\nThe string returned by __repr__  should be unambiguous and, if possible,\nmatch the source code necessary to re-create the represented object. That is\nwhy our Vector  representation looks like calling the constructor of the\nclass (e.g., Vector(3, 4) ).\nIn contrast, __str__  is called by the str()  built-in and implicitly used\nby the print  function. It should return a string suitable for display to end\nusers.\nSometimes same string returned by __repr__  is user -friendly , and you\ndon’ t need to code __str__  because the implementation inherited from\nthe object  class calls __repr__  as a fallback. Example 5-2  is one of\nseveral examples in this book with a custom __str__ .\nT I P\nProgrammers with prior experience in languages with a toString  method tend to\nimplement __str__  and not __repr__ . If you only implement one of these special\nmethods in Python, choose __repr__ .\n“Dif ference between __str__  and __repr__  in Python”  is a Stack Overflow\nquestion with excellent contributions from Pythonistas Alex Martelli and Martijn\nPieters.",3289
10-Boolean Value of a Custom Type.pdf,10-Boolean Value of a Custom Type,,0
11-Collection API.pdf,11-Collection API,"Boolean V alue of a Custom T ype\nAlthough Python has a bool  type, it accepts any object in a boolean\ncontext, such as the expression controlling an if  or while  statement, or as\noperands to and , or , and not . T o determine whether a value x  is truthy  or\nfalsy , Python applies bool(x) , which returns either True  or False .\nBy default, instances of user -defined classes are considered truthy , unless\neither __bool__  or __len__  is implemented. Basically , bool(x)  calls\nx.__bool__()  and uses the result. If __bool__  is not implemented,\nPython tries to invoke x.__len__() , and if that returns zero, bool\nreturns False . Otherwise bool  returns True .\nOur implementation of __bool__  is conceptually simple: it returns\nFalse  if the magnitude of the vector is zero, True  otherwise. W e convert\nthe magnitude to a Boolean using bool(abs(self))  because\n__bool__  is expected to return a boolean. Outside of __bool__\nmethods, it is rarely necessary to call bool()  explicitly , because any\nobject can be used in a boolean context.\nNote how the special method __bool__  allows your objects to follow the\ntruth value testing rules defined in the “Built-in T ypes” chapter  of The\nPython Standar d Library  documentation.\nN O T E\nA faster implementation of Vector.__bool__  is this:\n    def __bool__ (self): \n        return bool(self.x or self.y)\nThis is harder to read, but avoids the trip through abs , __abs__ , the squares, and\nsquare root. The explicit conversion to bool  is needed because __bool__  must return\na boolean and or  returns either operand as is: x or y  evaluates to x  if that is truthy ,\notherwise the result is y , whatever that is.\nCollection API\nFigure 1-2  documents the interfaces of the essential collection types in the\nlanguage. All the classes in the diagram are ABCs— abstract base classes .\nABCs and the collections.abc  module are covered in Chapter 13 .\nThe goal of this brief section is to give a panoramic view of Python’ s most\nimportant collection interfaces, showing how they are built from special\nmethods.\nFigur e 1-2. UML class diagram with fundamental collection types. Method names in italic  ar e\nabstract, so they must be implemented by concr ete subclasses such as list  and dict . The\nr emaining methods have concr ete implementations, ther efor e subclasses can inherit them.\nEach of the top ABCs has a single special method. The Collection\nABC (new in Python 3.6) unifies the three essential interfaces that every\ncollection should implement:\nIterable  to support for , unpacking , and other forms of\niteration;\nSized  to support the len  built-in function;\nContains  to support the in  operator .\nPython does not require concrete classes to actually inherit from any of\nthese ABCs. Any class that implements __len__  satisfies the Sized\ninterface.\nThree very important specializations of Collection  are:\nSequence , formalizing the interface of built-ins like list  and\nstr ;\nMapping , implemented by dict ,\ncollections.defaultdict , etc.;\nSet : the interface of the set  and frozenset  built-in types.\nOnly Sequence  is Reversible , because sequences support arbitrary\nordering of their contents, while mappings and sets do not.\nN O T E\nSince Python 3.7, the dict  type is of ficially “ordered”, but that only means that the key\ninsertion order is preserved. Y ou cannot rearrange the keys in a dict  however you like.\nAll the special methods in the Set  ABC implement infix operators. For\nexample, a & b  computes the intersection of sets a  and b , and is\nimplemented in the __and__  special method.\nThe next two chapters will cover standard library sequences, mappings, and\nsets in detail.\nNow let’ s consider the major categories of special methods defined in the\nPython Data Model.",3811
12-Overview of Special Methods.pdf,12-Overview of Special Methods,"O v e r v i e w  o f  S p e c i a l  M e t h o d s\nThe “Data Model” chapter  of The Python Language Refer ence  lists more\nthan 80 special method names. More than half of them implement\narithmetic, bitwise, and comparison operators. As an overview of what is\navailable, see following tables.\nT able 1-1  shows special method names excluding those used to implement\ninfix operators or core math functions like abs . Most of these methods will\nbe covered throughout the book, including the most recent additions:\nasynchronous special methods such as __anext__  (added in Python 3.5),\nand the class customization hook, __init_subclass__  (from Python\n3.6).\n \nT\na\nb\nl\ne\n \n1\n-\n1\n.  \nS\np\ne\nc\ni\na\nl  \nm\ne\nt\nh\no\nd\n \nn\na\nm\ne\ns\n \n(\no\np\ne\nr\na\nt\no\nr\ns\n \ne\nx\nc\nl\nu\nd\ne\nd\n)\n \nCategory Method names\n \nString/bytes  \nrepresentation__repr__  __str__  __format__  __bytes__  __fspat\nh__\nConversion to number __bool__  __complex__  __int__  __float__  __hash\n__  __index__\nEmulating collections __len__  __getitem__  __setitem__  __delitem__  _\n_contains__\nIteration __iter__  __aiter__  __next__ ,  __anext__  __revers\ned__\nCallable or coroutine  \nexecution__call__  __await__\nContext management __enter__  __exit__  __aexit__  __aenter__\nInstance creation and  \ndestruction__new__  __init__  __del__\nAttribute management __getattr__  __getattribute__  __setattr__  __del\nattr__  __dir__\nAttribute descriptors __get__  __set__  __delete__  __set_name__\nAbstract base classes __instancecheck__  __subclasscheck__\nClass  \nmetaprogramming__prepare__  __init_subclass__  __class_getitem__   \n__mro_entries__\n \nInfix and numerical operators are supported by the special methods in 1-2 .\nHere the most recent names are __matmul__ , __rmatmul__ , and\n__imatmul__ , added in Python 3.5 to support the use of @  as an infix\noperator for matrix multiplication, as we’ll see in Chapter 16 .\n \nT\na\nb\nl\ne  \n1\n-\n2\n.  \nS\np\ne\nc\ni\na\nl  \nm\ne\nt\nh\no\nd  \nn\na\nm\ne\ns  \na\nn\nd  \ns\ny\nm\nb\no\nl\ns  \nf\no\nr  \no\np\ne\nr\na\nt\no\nr\ns\n \nOperator category Symbols Method names\n \nUnary numeric -  +  abs() __neg__  __pos__  __abs__\nRich comparison <  \<=  ==  !=   \n>  >=__lt__  __le__  __eq__  __ne__  __g\nt__  __ge__\nArithmetic +  -  *  /  //   \n%  @  divmod()   \nround()  **  pow\n()__add__  __sub__  __mul__  __truedi\nv__  __floordiv__  __mod__  __matmu\nl__  __divmod__  __round__  __pow__\nReversed  \narithmetic(arithmetic operators  \nwith swapped operands)__radd__  __rsub__  __rmul__  __rtr\nuediv__  __rfloordiv__  __rmod__  _\n_rmatmul__  __rdivmod__  __rpow__\nAugmented  \nassignment  \narithmetic+=  -=  *=  /=   \n//=  %=  @=  **=__iadd__  __isub__  __imul__  __itr\nuediv__  __ifloordiv__  __imod__  _\n_imatmul__  __ipow__\nBitwise &  |  ^  <<  >>   \n~__and__  __or__  __xor__  __lshift_\n_  __rshift__  __invert__",2925
13-Further Reading.pdf,13-Further Reading,"Reversed bitwise (bitwise operators with  \nswapped operands)__rand__  __ror__  __rxor__  __rlsh\nift__  __rrshift__\nAugmented  \nassignment bitwise&=  |=  ^=  <<=   \n>>=__iand__  __ior__  __ixor__  __ilsh\nift__  __irshift__\n \nN O T E\nPython calls a reversed operator special method on the second operand when the\ncorresponding special method on the first operand cannot be used. Augmented\nassignments are shortcuts combining an infix operator with variable assignment, e.g. a\n+= b .\nChapter 16  explains reversed operators and augmented assignment in detail.\nW h y  l e n  I s  N o t  a  M e t h o d\nI asked this question to core developer Raymond Hettinger in 2013 and the\nkey to his answer was a quote from The Zen of Python : “practicality beats\npurity .” In “How Special Methods Are Used” , I described how len(x)\nruns very fast when x  is an instance of a built-in type. No method is called\nfor the built-in objects of CPython: the length is simply read from a field in\na C struct. Getting the number of items in a collection is a common\noperation and must work ef ficiently for such basic and diverse types as\nstr , list , memoryview , and so on.\nIn other words, len  is not called as a method because it gets special\ntreatment as part of the Python Data Model, just like abs . But thanks to the\nspecial method __len__ , you can also make len  work with your own\ncustom objects. This is a fair compromise between the need for ef ficient\nbuilt-in objects and the consistency of the language. Also from The Zen of\nPython: “Special cases aren’ t special enough to break the rules.”\nN O T E\nIf you think of abs  and len  as unary operators, you may be more inclined to for give\ntheir functional look-and-feel, as opposed to the method call syntax one might expect in\nan OO language. In fact, the ABC language—a direct ancestor of Python that pioneered\nmany of its features—had an #  operator that was the equivalent of len  (you’d write\n#s ). When used as an infix operator , written x#s , it counted the occurrences of x  in s ,\nwhich in Python you get as s.count(x) , for any sequence s .\nC h a p t e r  S u m m a r y\nBy implementing special methods, your objects can behave like the built-in\ntypes, enabling the expressive coding style the community considers\nPythonic.\nA basic requirement for a Python object is to provide usable string\nrepresentations of itself, one used for debugging and logging, another for\npresentation to end users. That is why the special methods __repr__  and\n__str__  exist in the data model.\nEmulating sequences, as shown with the FrenchDeck  example, is one of\nthe most common uses of the special methods. For example, database\nlibraries often return query results wrapped in sequence-like collections.\nMaking the most of existing sequence types is the subject of Chapter 2 .\nImplementing your own sequences will be covered in Chapter 12 , when we\ncreate a multidimensional extension of the Vector  class.\nThanks to operator overloading, Python of fers a rich selection of numeric\ntypes, from the built-ins to decimal.Decimal  and\nfractions.Fraction , all supporting infix arithmetic operators. The\nNumPy  data science libraries support infix operators with matrices and\ntensors. Implementing operators—including reversed operators and\naugmented assignment—will be shown in Chapter 16  via enhancements of\nthe Vector  example.\nThe use and implementation of the majority of the remaining special\nmethods of the Python Data Model are covered throughout this book.\nF u r t h e r  R e a d i n g\nThe “Data Model” chapter  of The Python Language Refer ence  is the\ncanonical source for the subject of this chapter and much of this book.\nPython in a Nutshell, 3r d Edition  (O’Reilly) by Alex Martelli, Anna\nRavenscroft, and Steve Holden has excellent coverage of the data model.\nTheir description of the mechanics of attribute access is the most\nauthoritative I’ve seen apart from the actual C source code of CPython.\nMartelli is also a prolific contributor to Stack Overflow , with more than\n6,200 answers posted. See his user profile at Stack Overflow .\nDavid Beazley has two books covering the data model in detail in the\ncontext of Python 3: Python Essential Refer ence, 4th Edition  (Addison-\nW esley Professional), and Python Cookbook, 3r d Edition  (O’Reilly),\ncoauthored with Brian K. Jones.\nThe Art of the Metaobject Pr otocol  (AMOP , MIT Press) by Gregor\nKiczales, Jim des Rivieres, and Daniel G. Bobrow explains the concept of a\nmetaobject protocol, of which the Python Data Model is one example.\nS O A P B O X\nData Model or Object Model?\nWhat the Python documentation calls the “Python Data Model,” most\nauthors would say is the “Python object model.” Martelli, Ravenscroft\n& Holden’ s Python in a Nutshell 3E , and David Beazley’ s Python\nEssential Refer ence 4E  are the best books covering the “Python Data\nModel,” but they refer to it as the “object model.” On W ikipedia, the\nfirst definition of object model  is “The properties of objects in general\nin a specific computer programming language.” This is what the\n“Python Data Model” is about. In this book, I will use “data model”\nbecause the documentation favors that term when referring to the\nPython object model, and because it is the title of the chapter of The\nPython Language Refer ence  most relevant to our discussions.\nMuggle Methods\nThe The Original Hacker ’ s Dictionary  defines magic  as “as yet\nunexplained, or too complicated to explain” or “a feature not generally\npublicized which allows something otherwise impossible.”\nThe Ruby community calls their equivalent of the special methods\nmagic methods . Many in the Python community adopt that term as well.\nI believe the special methods are the opposite of magic. Python and\nRuby empower their users with a rich metaobject protocol that is fully\ndocumented, enabling muggles like you and I to emulate many of the\nfeatures available to core developers who write the interpreters for those\nlanguages.\nIn contrast, consider Go. Some objects in that language have features\nthat are magic, in the sense that we cannot emulate them in our own\nuser -defined types. For example, Go arrays, strings, and maps support\nthe use brackets for item access, as in a[i] . But there’ s no way to\nmake the []  notation work with a new collection type that you define.\nEven worse, Go has no user -level concept of an iterable interface or an\niterator object, therefore its for/range  syntax is limited to\nsupporting five “magic” built-in types, including arrays, strings and\nmaps.\nMaybe in the future, the designers of Go will enhance its metaobject\nprotocol. But currently , it is much more limited than what we have in\nPython or Ruby .\nMetaobjects\nThe Art of the Metaobject Pr otocol (AMOP)  is my favorite computer\nbook title. But I mention it because the term metaobject pr otocol  is\nuseful to think about the Python Data Model and similar features in\nother languages. The metaobject  part refers to the objects that are the\nbuilding blocks of the language itself. In this context, pr otocol  is a\nsynonym of interface . So a metaobject pr otocol  is a fancy synonym for\nobject model: an API for core language constructs.\nA rich metaobject protocol enables extending a language to support new\nprogramming paradigms. Gregor Kiczales, the first author of the AMOP\nbook, later became a pioneer in aspect-oriented programming and the\ninitial author of AspectJ, an extension of Java implementing that\nparadigm. Aspect-oriented programming is much easier to implement in\na dynamic language like Python, and some frameworks do it. The most\nimportant example is zope.interface , part of the framework on which\nthe Plone content management  system is build.\n1  Story of Jython , written as a Foreword to Jython Essentials  (O’Reilly , 2002), by Samuele\nPedroni and Noel Rappin.\n2  A C struct is a record type with named fields.",7994
14-Overview of Built-In Sequences.pdf,14-Overview of Built-In Sequences,"Part II. Data Structures\nChapter 2. An Array of\nSequences\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 2nd chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nAs you may have noticed, several of the operations mentioned work\nequally for texts, lists and tables. T exts, lists and tables together ar e\ncalled trains . […] The FOR  command also works generically on trains.\n— Geurts, Meertens, and Pemberton, ABC Programmer ’ s\nHandbook\nBefore creating Python, Guido was a contributor to the ABC language—a\n10-year research project to design a programming environment for\nbeginners. ABC introduced many ideas we now consider “Pythonic”:\ngeneric operations on dif ferent types of sequences, built-in tuple and\nmapping types, structure by indentation, strong typing without variable\ndeclarations, and more. It’ s no accident that Python is so user -friendly .\nPython inherited from ABC the uniform handling of sequences. Strings,\nlists, byte sequences, arrays, XML elements, and database results share a1\nrich set of common operations including iteration, slicing, sorting, and\nconcatenation.\nUnderstanding the variety of sequences available in Python saves us from\nreinventing the wheel, and their common interface inspires us to create\nAPIs that properly support and leverage existing and future sequence types.\nMost of the discussion in this chapter applies to sequences in general, from\nthe familiar list  to the str  and bytes  types added in Python 3. Specific\ntopics on lists, tuples, arrays, and queues are also covered here, but the\nspecifics of Unicode strings and byte sequences appear in Chapter 4 . Also,\nthe idea here is to cover sequence types that are ready to use. Creating your\nown sequence types is the subject of Chapter 12 .\nThese are the main topics this chapter will cover:\nList comprehensions and the basics of generator expressions;\nUsing tuples as records, versus using tuples as immutable lists;\nSequence unpacking and sequence patterns;\nReading from slices and writing to slices;\nSpecialized sequence types, like arrays and queues.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThe most important update in this chapter is “Pattern Matching with\nSequences” . That’ s the first time the new pattern matching feature of\nPython 3.10 appears in this Second Edition .\nOther changes are not updates but improvements over the First Edition :\nNew diagram and description of the internals of sequences,\ncontrasting containers and flat sequences.\nBrief comparison of the performance and storage characteristics of\nlist  versus tuple .\nCaveats of tuples with mutable elements, and how to detect them if\nneeded.\nI moved coverage of named tuples to “Classic Named T uples”  in Chapter 5 ,\nwhere they are compared to typing.NamedTuple  and @dataclass .\nN O T E\nT o make room for new content and keep the page count within reason, the section\nManaging Or der ed Sequences with Bisect  from the First Edition  is now a post  in the\nfluentpython.com  companion W eb site.\nO v e r v i e w  o f  B u i l t - I n  S e q u e n c e s\nThe standard library of fers a rich selection of sequence types implemented\nin C:\nContainer sequences\nCan hold items of dif ferent types, including nested containers. Some\nexamples: list , tuple , and collections.deque .\nFlat sequences\nHold items of one simple type. Some examples: str , bytes , and\narray.array .\nA container sequence  holds references to the objects it contains, which may\nbe of any type, while a flat sequence  stores the value of its contents in its\nown memory space, and not as distinct Python objects. See Figure 2-1 .\nFigur e 2-1. Simplified memory diagrams for a tuple  and an array , each with 3 items. Gray cells\nr epr esent the in-memory header of each Python object—not drawn to pr oportion. The tuple  has an\narray of r efer ences to its items. Each item is a separate Python object, possibly holding r efer ences to\nother Python objects, like that 2-item list. In contrast, the Python array  is a single object, holding a\nC language array of 3 doubles.\nThus, flat sequences are more compact, but they are limited to holding\nprimitive machine values like bytes, integers, and floats.\nN O T E\nEvery Python object in memory has a header with metadata. The simplest Python\nobject, a float , has a value field and two metadata fields:\nob_refcnt : the object’ s reference count;\nob_type : a pointer to the object’ s type;\nob_fval : a C double  holding the value of the float .\nOn a 64-bit Python build, each of those fields takes 8 bytes. That’ s why an array of\nfloats is much more compact than a tuple of floats: the array is a single object holding\nthe raw values of the floats, while the tuple consists of several objects—the tuple itself\nand each float  object contained in it.\nAnother way of grouping sequence types is by mutability:\nMutable sequences\nE.g. list , bytearray , array.array , and\ncollections.deque .\nImmutable sequences\nE.g. tuple , str , and bytes .\nFigure 2-2  helps visualize how mutable sequences inherit all methods from\nimmutable sequences, and implement several additional methods. The built-\nin concrete sequence types do not actually subclass the Sequence  and\nMutableSequence  abstract base classes (ABCs), but they are virtual\nsubclasses  registered with those ABCs—as we’ll see in Chapter 13 . Being\nvirtual subclasses, tuple  and list  pass these tests:\n>>> from collections  import abc \n>>> issubclass (tuple, abc.Sequence ) \nTrue \n>>> issubclass (list, abc.MutableSequence ) \nTrue\nFigur e 2-2. Simplified UML class diagram for some classes fr om collections.abc (super classes ar e on\nthe left; inheritance arr ows point fr om subclasses to super classes; names in italic ar e abstract\nclasses and abstract methods)",6317
15-List Comprehensions and Generator Expressions.pdf,15-List Comprehensions and Generator Expressions,,0
16-List Comprehensions and Readability.pdf,16-List Comprehensions and Readability,"Keep in mind these common traits: mutable versus immutable; container\nversus flat. They are helpful to extrapolate what you know about one\nsequence type to others.\nThe most fundamental sequence type is the list : a mutable container . I\nexpect you are very familiar with lists, so we’ll jump right into list\ncomprehensions, a powerful way of building lists that is sometimes\nunderused because the syntax may look unusual at first. Mastering list\ncomprehensions opens the door to generator expressions, which—among\nother uses—can produce elements to fill up sequences of any type. Both are\nthe subject of the next section.\nL i s t  C o m p r e h e n s i o n s  a n d  G e n e r a t o r\nE x p r e s s i o n s\nA quick way to build a sequence is using a list comprehension (if the tar get\nis a list ) or a generator expression (for other kinds of sequences). If you\nare not using these syntactic forms on a daily basis, I bet you are missing\nopportunities to write code that is more readable and often faster at the\nsame time.\nIf you doubt my claim that these constructs are “more readable,” read on.\nI’ll try to convince you.\nT I P\nFor brevity , many Python programmers refer to list comprehensions as listcomps , and\ngenerator expressions as genexps . I will use these words as well.\nList Comprehensions and Readability\nHere is a test: which do you find easier to read, Example 2-1  or Example 2-\n2 ?\nExample 2-1. Build a list of Unicode codepoints fr om a string\n>>> symbols = '$¢£¥€¤'  \n>>> codes = [] \n>>> for symbol in symbols: \n...     codes.append(ord(symbol)) \n... \n>>> codes \n[36, 162, 163, 165, 8364, 164]\nExample 2-2. Build a list of Unicode codepoints fr om a string, using a\nlistcomp\n>>> symbols = '$¢£¥€¤'  \n>>> codes = [ord(symbol) for symbol in symbols] \n>>> codes \n[36, 162, 163, 165, 8364, 164]\nAnybody who knows a little bit of Python can read Example 2-1 . However ,\nafter learning about listcomps, I find Example 2-2  more readable because\nits intent is explicit.\nA for  loop may be used to do lots of dif ferent things: scanning a sequence\nto count or pick items, computing aggregates (sums, averages), or any\nnumber of other tasks. The code in Example 2-1  is building up a list. In\ncontrast, a listcomp is more explicit. Its goal is always to build a new list.\nOf course, it is possible to abuse list comprehensions to write truly\nincomprehensible code. I’ve seen Python code with listcomps used just to\nrepeat a block of code for its side ef fects. If you are not doing something\nwith the produced list, you should not use that syntax. Also, try to keep it\nshort. If the list comprehension spans more than two lines, it is probably\nbest to break it apart or rewrite as a plain old for  loop. Use your best\njudgment: for Python as for English, there are no hard-and-fast rules for\nclear writing.\nS Y N T A X  T I P\nIn Python code, line breaks are ignored inside pairs of [] , {} , or () . So you can build\nmultiline lists, listcomps, tuples, dictionaries etc. without using the \  line continuation\nescape which doesn’ t work if you accidentally type a space after it. Also, when those\ndelimiter pairs are used to define a literal with a comma-separated series of items, a\ntrailing comma will be ignored. So, for example, when coding a multi-line list literal, it\nis thoughtful to put a comma after the last item, making it a little easier for the next\ncoder do add one more item to that list, and reducing noise when reading dif fs.\nL O C A L  S C O P E  W I T H I N  C O M P R E H E N S I O N S  A N D\nG E N E R A T O R  E X P R E S S I O N S\nIn Python 3, list comprehensions, generator expressions, and their\nsiblings set  and dict  comprehensions have a local scope to hold the\nvariables assigned in the for  clause.\nHowever , variables assigned with the “W alrus operator” :=  remain\naccessible after those comprehensions or expressions return—unlike\nlocal variables in a function. PEP 572—Assignment Expr essions  defines\nthe scope of the tar get of :=  as the enclosing function, unless there is a\nglobal  or nonlocal  declaration for that tar get.\n>>> x = 'ABC' \n>>> codes = [ord(x) for x in x] \n>>> x  \n \n'ABC' \n>>> codes \n[65, 66, 67]  \n>>> codes = [last := ord(c) for c in x] \n>>> last \n \n67 \n>>> c \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nNameError : name 'c' is not defined\nx  was not clobbered: it’ s still bound to 'ABC' ;\nlast  remains;\nc  existed only inside the listcomp.\nList comprehensions build lists from sequences or any other iterable type by\nfiltering and transforming items. The filter  and map  built-ins can be\ncomposed to do the same, but readability suf fers, as we will see next.2",4765
17-Listcomps Versus map and filter.pdf,17-Listcomps Versus map and filter,,0
18-Cartesian Products.pdf,18-Cartesian Products,"Listcomps V ersus map and filter\nListcomps do everything the map  and filter  functions do, without the\ncontortions of the functionally challenged Python lambda . Consider\nExample 2-3 .\nExample 2-3. The same list built by a listcomp and a map/filter composition\n>>> symbols = '$¢£¥€¤'  \n>>> beyond_ascii  = [ord(s) for s in symbols if ord(s) > 127] \n>>> beyond_ascii  \n[162, 163, 165, 8364, 164]  \n>>> beyond_ascii  = list(filter(lambda c: c > 127, map(ord, \nsymbols))) \n>>> beyond_ascii  \n[162, 163, 165, 8364, 164]\nI used to believe that map  and filter  were faster than the equivalent\nlistcomps, but Alex Martelli pointed out that’ s not the case—at least not in\nthe preceding examples. The 02-array-seq/listcomp_speed.py  script in the\nFluent Python  code repository  is a simple speed test comparing listcomp\nwith filter/map .\nI’ll have more to say about map  and filter  in Chapter 7 . Now we turn to\nthe use of listcomps to compute Cartesian products: a list containing tuples\nbuilt from all items from two or more lists.\nCartesian Products\nListcomps can build lists from the Cartesian product of two or more\niterables. The items that make up the Cartesian product are tuples made\nfrom items from every input iterable. The resulting list has a length equal to\nthe lengths of the input iterables multiplied. See Figure 2-3 .\nFor example, imagine you need to produce a list of T -shirts available in two\ncolors and three sizes. Example 2-4  shows how to produce that list using a\nlistcomp. The result has six items.\nExample 2-4. Cartesian pr oduct using a list compr ehension\n>>> colors = ['black', 'white'] \n>>> sizes = ['S', 'M', 'L'] \n>>> tshirts = [(color, size) for color in colors for size in sizes]  \n \n>>> tshirts \n[('black', 'S'), ('black', 'M'), ('black', 'L'), ('white', 'S'),  \n ('white', 'M'), ('white', 'L')]  \n>>> for color in colors:  \n \n...     for size in sizes: \n...         print((color, size)) \n... \n('black', 'S')  \n('black', 'M')  \n('black', 'L')  \n('white', 'S')  \n('white', 'M')  \n('white', 'L')  \n>>> tshirts = [(color, size) for size in sizes      \n \n...                          for color in colors] \n>>> tshirts \n[('black', 'S'), ('white', 'S'), ('black', 'M'), ('white', 'M'),  \n ('black', 'L'), ('white', 'L')]\nThis generates a list of tuples arranged by color , then size.\nNote how the resulting list is arranged as if the for  loops were nested\nin the same order as they appear in the listcomp.\nT o get items arranged by size, then color , just rearrange the for\nclauses; adding a line break to the listcomp makes it easier to see how\nthe result will be ordered.\nFigur e 2-3. The Cartesian pr oduct of thr ee car d ranks and four suits is a sequence of twelve pairings",2767
19-Tuples as Records.pdf,19-Tuples as Records,"In Example 1-1  ( Chapter 1 ), I used the following expression to initialize a\ncard deck with a list made of 52 cards from all 13 ranks of each of the 4\nsuits, sorted by suit then rank:\n        self._cards = [Card(rank, suit) for suit in self.suits \n                                        for rank in self.ranks]\nListcomps are a one-trick pony: they build lists. T o generate data for other\nsequence types, a genexp is the way to go. The next section is a brief look at\ngenexps in the context of building sequences that are not lists.\nGenerator Expressions\nT o initialize tuples, arrays, and other types of sequences, you could also\nstart from a listcomp, but a genexp saves memory because it yields items\none by one using the iterator protocol instead of building a whole list just to\nfeed another constructor .\nGenexps use the same syntax as listcomps, but are enclosed in parentheses\nrather than brackets.\nExample 2-5  shows basic usage of genexps to build a tuple and an array .\nExample 2-5. Initializing a tuple and an array fr om a generator expr ession\n>>> symbols = '$¢£¥€¤' \n>>> tuple(ord(symbol) for symbol in symbols)  \n \n(36, 162, 163, 165, 8364, 164)  \n>>> import array \n>>> array.array('I', (ord(symbol) for symbol in symbols))  \n \narray('I', [36, 162, 163, 165, 8364, 164])\nIf the generator expression is the single ar gument in a function call,\nthere is no need to duplicate the enclosing parentheses.\nThe array  constructor takes two ar guments, so the parentheses around\nthe generator expression are mandatory . The first ar gument of the\narray  constructor defines the storage type used for the numbers in the\narray , as we’ll see in “Arrays” .\nExample 2-6  uses a genexp with a Cartesian product to print out a roster of\nT -shirts of two colors in three sizes. In contrast with Example 2-4 , here the\nsix-item list of T -shirts is never built in memory: the generator expression\nfeeds the for  loop producing one item at a time. If the two lists used in the\nCartesian product had 1,000 items each, using a generator expression would\nsave the cost of building a list with a million items just to feed the for\nloop.\nExample 2-6. Cartesian pr oduct in a generator expr ession\n>>> colors = ['black', 'white'] \n>>> sizes = ['S', 'M', 'L'] \n>>> for tshirt in (f'{c} {s}' for c in colors for s in sizes):  \n \n...     print(tshirt) \n... \nblack S \nblack M \nblack L \nwhite S \nwhite M \nwhite L\nThe generator expression yields items one by one; a list with all six T -\nshirt variations is never produced in this example.\nN O T E\nChapter 17  is explains how generators work in detail. Here the idea was just to show the\nuse of generator expressions to initialize sequences other than lists, or to produce output\nthat you don’ t need to keep in memory .\nNow we move on to the other fundamental sequence type in Python: the\ntuple.\nT u p l e s  A r e  N o t  J u s t  I m m u t a b l e  L i s t s\nSome introductory texts about Python present tuples as “immutable lists,”\nbut that is short selling them. T uples do double duty: they can be used as\nimmutable lists and also as records with no field names. This use is\nsometimes overlooked, so we will start with that.\nT uples as Records\nT uples hold records: each item in the tuple holds the data for one field and\nthe position of the item gives its meaning.\nIf you think of a tuple just as an immutable list, the quantity and the order\nof the items may or may not be important, depending on the context. But\nwhen using a tuple as a collection of fields, the number of items is usually\nfixed and their order is always important.\nExample 2-7  shows tuples used as records. Note that in every expression,\nsorting the tuple would destroy the information because the meaning of\neach field is given by its position in the tuple.\nExample 2-7. T uples used as r ecor ds\n>>> lax_coordinates  = (33.9425, -118.408056 )  \n \n>>> city, year, pop, chg, area = ('Tokyo', 2003, 32_450, 0.66, \n8014)  \n \n>>> traveler_ids  = [('USA', '31195855 '), ('BRA', 'CE342567 '),  \n \n...     ('ESP', 'XDA205856 ')] \n>>> for passport  in sorted(traveler_ids ):  \n \n...     print('%s/%s' % passport )   \n \n... \nBRA/CE342567  \nESP/XDA205856  \nUSA/31195855  \n>>> for country, _ in traveler_ids :  \n \n...     print(country) \n... \nUSA \nBRA \nESP\nLatitude and longitude of the Los Angeles International Airport.\nData about T okyo: name, year , population (thousands), population\nchange (%), area (km²).\nA list of tuples of the form (country_code,\npassport_number) .\nAs we iterate over the list, passport  is bound to each tuple.\nThe %  formatting operator understands tuples and treats each item as a\nseparate field.\nThe for  loop knows how to retrieve the items of a tuple separately—\nthis is called “unpacking.” Here we are not interested in the second\nitem, so we assign it to _ , a dummy variable.\nT I P\nIn general, using _  as a dummy variable is just a convention. It’ s just a strange but valid\nvariable name. However , there are two contexts where _  is special:\n1 . In the Python console, the result of executing a line is assigned to _ —unless\nthe result is None .\n2 . In a match/case  statement, _  is a wildcard that matches any value but is\nnever assigned a value. See “Pattern Matching with Sequences” .\nW e often think of records as data structures with named fields. Chapter 5\npresents two ways of creating tuples with named fields.\nBut often, there’ s no need to go through the trouble of creating a class just\nto name the fields, especially if you leverage unpacking and avoid using\nindexes to access the fields. In Example 2-7 , we assigned ('Tokyo',\n2003, 32_450, 0.66, 8014)  to city, year, pop, chg,\narea  in a single statement. Then, the %  operator assigned each item in the\npassport  tuple to the corresponding slot in the format string in the\nprint  ar gument. Those are two examples of tuple unpacking .",5972
20-Tuples as Immutable Lists.pdf,20-Tuples as Immutable Lists,"N O T E\nThe term tuple unpacking  is widely used by Pythonistas, but iterable unpacking  is\ngaining traction, as in the title of PEP 3132 — Extended Iterable Unpacking .\n“Unpacking sequences and iterables”  presents a lot more about unpacking not only\ntuples, but sequences and iterables in general.\nNow let’ s consider the tuple  class as an immutable variant of the list\nclass.\nT uples as Immutable Lists\nThe Python interpreter and standard library make extensive use of tuples as\nimmutable lists, and so should you. This brings two key benefits:\n1 . Clarity: when you see a tuple  in code, you know its length will\nnever change.\n2 . Performance: a tuple  uses less memory than a list  of the same\nlength, and they allow Python to do some optimizations.\nHowever , be aware that the immutability of a tuple  only applies to the\nreferences contained in it. References in a tuple cannot be deleted or\nreplaced. But if one of those references points to a mutable object, and that\nobject is changed, then the value of the tuple  changes. The next snippet\nillustrate this point by creating two tuples— a  and b —which are initially\nequal. Figure 2-4  represents the initial layout of the b  tuple in memory .\nWhen the last item in b  is changed, b  and a  become dif ferent:\n>>> a = (10, 'alpha', [1, 2]) \n>>> b = (10, 'alpha', [1, 2]) \n>>> a == b \nTrue \n>>> b[-1].append(99) \n>>> a == b \nFalse \n>>> b \n(10, 'alpha', [1, 2, 99])\nFigur e 2-4. The content of the tuple itself is immutable, but that only means the r efer ences held by the\ntuple will always point to the same objects. However , if one of the r efer enced objects is mutable—like\na list—its content may change.\nT uples with mutable items can be a source of bugs. As we’ll see in “What is\nHashable” , an object is only hashable if its value cannot ever change. An\nunhashable tuple cannot be inserted as a dict  key , or a set  element.\nIf you want to determine explicitly if a tuple (or any object) has a fixed\nvalue, you can use the hash  built-in to create a fixed  function like this:\n>>> def fixed(o): \n...     try: \n...         hash(o) \n...     except TypeError : \n...         return False \n...     return True \n... \n>>> tf = (10, 'alpha', (1, 2)) \n>>> tm = (10, 'alpha', [1, 2]) \n>>> fixed(tf) \nTrue \n>>> fixed(tm) \nFalse\nW e explore this issue further in “The Relative Immutability of T uples” .\nDespite this caveat, tuples are widely used as immutable lists. They of fer\nsome performance advantages explained by Python core developer\nRaymond Hettinger in a StackOverflow answer to the question Are tuples\nmore ef ficient than lists in Python?  T o summarize, Hettinger wrote:\nT o evaluate a tuple literal, the Python compiler generates bytecode\nfor a tuple constant in one operation, but for a list literal, the\ngenerated bytecode pushes each element as a separate constant to\nthe data stack, and then builds the list.\nGiven a tuple t , tuple(t)  simply returns a reference to the same\nt . There’ s no need to copy . In contrast, given a list l , the\nlist(l)  constructor must create a new copy of l .",3129
21-Comparing Tuple and List Methods.pdf,21-Comparing Tuple and List Methods,"Because of its fixed length, a tuple  instance is allocated the exact\nmemory space in needs. Instances of list , on the other hand, are\nallocated with room to spare, to amortize the cost of future\nappends.\nThe references to the items in a tuple are stored in an array in the\ntuple struct, while a list holds a pointer to an array of references\nstored elsewhere. The indirection is necessary because when a list\ngrows beyond the space currently allocated, Python needs to\nrealocate the array of references to make room. The extra\nindirection makes CPU caches less ef fective.\nComparing T uple and List Methods\nWhen using a tuple as an immutable variation of list, it is good to know\nhow similar their APIs are. As you can see in T able 2-1 , tuple  supports all\nlist  methods that do not involve adding or removing items, with one\nexception—tuple lacks the __reversed__  method. However , that is just\nfor optimization; reversed(my_tuple)  works without it.\n \nT\na\nb\nl\ne  \n2\n-\n1\n.  \nM\ne\nt\nh\no\nd\ns  \na\nn\nd  \na\nt\nt\nr\ni\nb\nu\nt\ne\ns  \nf\no\nu\nn\nd  \ni\nn  \nl\ni\ns\nt  \no\nr  \nt\nu\np\nl\ne  \n(\nm\ne\nt\nh\no\nd\ns  \ni\nm\np\nl\ne\nm\ne\nn\nt\ne\nd  \nb\ny  \no\nb\nj\ne\nc\nt  \na\nr\ne  \no\nm\ni\nt\nt\ne\nd  \nf\no\nr  \nb\nr\ne\nv\ni\nt\ny\n)\n \nlist tuple  \n \ns.__add__(s2) ● ● s + s2 —concatenation\ns.__iadd__(s2) ● s += s2 —in-place  \nconcatenation\ns.append(e) ● Append one element after last\ns.clear() ● Delete all items\ns.__contains__\n(e)● ● e in s\ns.copy() ● Shallow copy of the list\ns.count(e) ● ● Count occurrences of an  \nelement\ns.__delitem__\n(p)● Remove item at position p\ns.extend(it) ● Append items from iterable it\ns.__getitem__\n(p)● ● s[p] —get item at position\ns.__getnewargs_\n_()● Support for optimized  \nserialization with pickle\ns.index(e) ● ● Find position of first occurrence  \nof e\ns.insert(p, e) ● Insert element e  before the item  \nat position p\ns.__iter__() ● ● Get iterator\ns.__len__() ● ● len(s) —number of items\ns.__mul__(n) ● ● s * n —repeated  \nconcatenation\ns.__imul__(n) ● s *= n —in-place repeated  \nconcatenation\ns.__rmul__(n) ● ● n * s —reversed repeated  \nconcatenation\ns.pop([p]) ● Remove and return last item or  \nitem at optional position p\ns.remove(e) ● Remove first occurrence of  \nelement e  by valuea",2331
22-Pattern Matching with Sequences.pdf,22-Pattern Matching with Sequences,"s.reverse() ● Reverse the order of the items in  \nplace\ns.__reversed__\n()● Get iterator to scan items from  \nlast to first\ns.__setitem__\n(p, e)● s[p] = e —put e  in position  \np , overwriting existing item\ns.sort([key],  \n[reverse])● Sort items in place with optional  \nkeyword ar guments key  and r\neverse\n \na  Reversed operators are explained in Chapter 16 .\nb  Also used to overwrite a subsequence. See “Assigning to Slices” .\nNow let’ s switch to an important subject for idiomatic Python\nprogramming: tuple, list and iterable unpacking.\nU n p a c k i n g  s e q u e n c e s  a n d  i t e r a b l e s\nUnpacking is important because it avoids unnecessary and error -prone use\nof indexes to extract elements from sequences. Also, unpacking works with\nany iterable object as the data source—including iterators which don’ t\nsupport index notation [] . The only requirement is that the iterable yields\nexactly one item per variable in the receiving end, unless you use a star ( * )\nto capture excess items as explained in “Using * to grab excess items” .\nThe most visible form of unpacking is parallel assignment ; that is,\nassigning items from an iterable to a tuple of variables, as you can see in\nthis example:\n>>> lax_coordinates  = (33.9425, -118.408056 ) \n>>> latitude , longitude  = lax_coordinates   # unpacking  \n>>> latitude  \n33.9425 \n>>> longitude  \n-118.408056b\nAn elegant application of unpacking is swapping the values of variables\nwithout using a temporary variable:\n>>> b, a = a, b\nAnother example of unpacking is prefixing an ar gument with *  when\ncalling a function:\n>>> divmod(20, 8) \n(2, 4) \n>>> t = (20, 8) \n>>> divmod(*t) \n(2, 4) \n>>> quotient , remainder  = divmod(*t) \n>>> quotient , remainder  \n(2, 4)\nThe preceding code shows another use of unpacking: allowing functions to\nreturn multiple values in a way that is convenient to the caller . As another\nexample, the os.path.split()  function builds a tuple (path,\nlast_part)  from a filesystem path:\n>>> import os \n>>> _, filename  = os.path.split('/home/luciano/.ssh/id_rsa.pub' ) \n>>> filename  \n'id_rsa.pub'\nAnother way of using just some of the items when unpacking is to use the *\nsyntax, as we’ll see right away .\nUsing * to grab excess items\nDefining function parameters with *args  to grab arbitrary excess\nar guments is a classic Python feature.\nIn Python 3, this idea was extended to apply to parallel assignment as well:\n>>> a, b, *rest = range(5) \n>>> a, b, rest \n(0, 1, [2, 3, 4])  \n>>> a, b, *rest = range(3) \n>>> a, b, rest \n(0, 1, [2])  \n>>> a, b, *rest = range(2) \n>>> a, b, rest \n(0, 1, [])\nIn the context of parallel assignment, the *  prefix can be applied to exactly\none variable, but it can appear in any position:\n>>> a, *body, c, d = range(5) \n>>> a, body, c, d \n(0, [1, 2], 3, 4)  \n>>> *head, b, c, d = range(5) \n>>> head, b, c, d \n([0, 1], 2, 3, 4)\nUnpacking with * in function calls and sequence literals\nPEP 448—Additional Unpacking Generalizations  introduced more flexible\nsyntax for iterable unpacking, best summarized in What’ s New In Python\n3.5 .\nIn function calls, we can use *  multiple times:\n>>> def fun(a, b, c, d, *rest): \n...     return a, b, c, d, rest \n... \n>>> fun(*[1, 2], 3, *range(4, 7)) \n(1, 2, 3, 4, (5, 6))\nThe *  can also be used when defining list , tuple , or set  literals, as\nshown these examples from What’ s New In Python 3.5 :\n>>> *range(4), 4 \n(0, 1, 2, 3, 4)  \n>>> [*range(4), 4] \n[0, 1, 2, 3, 4]  \n>>> {*range(4), 4, *(5, 6, 7)} \n{0, 1, 2, 3, 4, 5, 6, 7}\nPEP 448 introduced similar new syntax for ** , which we’ll see in\n“Unpacking Mappings” .\nFinally , a powerful feature of tuple unpacking is that it works with nested\nstructures.\nNested Unpacking\nThe tar get of an unpacking can use nesting, e.g. (a, b, (c, d)) .\nPython will do the right thing if the value has the same nesting structure.\nExample 2-8  shows nested unpacking in action.\nExample 2-8. Unpacking nested tuples to access the longitude\nmetro_areas  = [ \n    ('Tokyo', 'JP', 36.933, (35.689722 , 139.691667 )),  \n \n    ('Delhi NCR ', 'IN', 21.935, (28.613889 , 77.208889 )), \n    ('Mexico City ', 'MX', 20.142, (19.433333 , -99.133333 )), \n    ('New York-Newark ', 'US', 20.104, (40.808611 , -74.020386 )), \n    ('São Paulo ', 'BR', 19.649, (-23.547778 , -46.635833 )), \n] \n \ndef main(): \n    print(f'{"""":15} | {""latitude "":>9} | {""longitude "":>9}') \n    for name, _, _, (lat, lon) in metro_areas :  \n \n        if lon <= 0:  \n \n            print(f'{name:15}  | {lat:9.4f}  | {lon:9.4f} ') \n \nif __name__  == '__main__ ': \n    main()\nEach tuple holds a record with four fields, the last of which is a\ncoordinate pair .\nBy assigning the last field to a nested tuple, we unpack the coordinates.\nThe lon <= 0:  test selects only cities in the W estern hemisphere.\nThe output of Example 2-8  is:\n                |   lat.    |   lon.  \nMexico City     |   19.4333 |  -99.1333  \nNew York-Newark |   40.8086 |  -74.0204  \nSão Paulo       |  -23.5478 |  -46.6358\nThe tar get of an unpacking assignment can also be a list, but good use cases\nare rare. Here is the only one I know: if you have a database query that\nreturns a single record (e.g. the SQL code has a LIMIT 1  clause), then you\ncan unpack and at the same time make sure there’ s only one result with this\ncode:\n>>> [record] = query_returning_single_row ()\nIf the record has only one field, you can get it directly like this:\n>>> [[field]] = query_returning_single_row_with_single_field ()\nBoth of these could be written with tuples, but don’ t for get the syntax quirk\nthat single-item tuples must be written with a trailing comma. So the first\ntar get would be (record,)  and the second ((field,),) . In both\ncases you get a silent bug if you for get a comma.\nNow let’ s study pattern matching, which supports even more powerful ways\nto unpack sequences.\nP a t t e r n  M a t c h i n g  w i t h  S e q u e n c e s\nThe most visible new feature in Python 3.10 is pattern matching with the\nmatch/case  statement proposed in PEP 634—Structural Pattern\nMatching: Specification .\nN O T E\nPython core developer Carol W illing wrote an excellent quick introducion to pattern\nmatching in the Structural Pattern Matching  section of What’ s New In Python 3.10 . I\nwill assume you’ve read W illing’ s intro, and give a brief overview focusing on\nmatch/case  with sequence subjects and patterns.3\nOn the surface, match/case  may look like a the switch/case\nstatement from the C language—but that’ s only half the story .  One key\nimprovement of match  over switch  is destructuring —a more advanced\nform of unpacking. Destructuring is a new word in the Python vocabulary ,\nbut it is commonly used in the documentation of languages that support\npattern matching—like Scala and Elixir .\nAs a first example of destructuring, Example 2-9  shows part of Example 2-\n8  rewritten with match/case .\nExample 2-9. Destructuring nested tuples—r equir es Python ≥ 3.10.\nmetro_areas  = [ \n    ('Tokyo', 'JP', 36.933, (35.689722 , 139.691667 )), \n    ('Delhi NCR ', 'IN', 21.935, (28.613889 , 77.208889 )), \n    ('Mexico City ', 'MX', 20.142, (19.433333 , -99.133333 )), \n    ('New York-Newark ', 'US', 20.104, (40.808611 , -74.020386 )), \n    ('São Paulo ', 'BR', 19.649, (-23.547778 , -46.635833 )), \n] \n \ndef main(): \n    print(f'{"""":15} | {""latitude "":>9} | {""longitude "":>9}') \n    for record in metro_areas : \n        match record:  \n \n            case [name, _, _, (lat, lon)] if lon <= 0:  \n \n                print(f'{name:15}  | {lat:9.4f}  | {lon:9.4f} ')\nThe subject of this match  is record — i.e. each of the tuples in\nmetro_areas .\nA case  clause has two parts: a pattern and an optional guard with the\nif  keyword.\nIn general, a sequence pattern matches the subject if:\n1 . the subject is a sequence and ;\n2 . the subject and the pattern have the same number of items and ;\n3 . each corresponding item matches, including nested items.4\nFor example, the pattern [name, _, _, (lat, lon)]  in Example 2-\n9  matches a sequence with 4 items, and the last item must be a two-item\nsequence.\nSequence patterns may be written as tuples or lists or any combination of\nnested tuples and lists, but it makes no dif ference which syntax you use: in\na pattern, tuples and lists match any sequence. I wrote the pattern as a list\nwith a nested 2-tuple just to avoid repeating brackets or parentheses in\nExample 2-9 .\nA sequence pattern can match instances of any actual or virtual subclass of\ncollections.abc.Sequence`footnote:[A virtual\n`Sequence  subclass is any class registered by calling the\nSequence.register()  class method, as detailed in “A V irtual\nSubclass of an ABC” . T ypes implemented via Python/C API are eligible if\nthey set a specific marker bit. See Py_TPFLAGS_SEQUENCE .] except for\nstr , bytes , bytearray  which are excluded for practical reasons. In the\nstandard library , these types are compatible with sequence patterns:\nlist     memoryview    array.array  \ntuple    range         collections.deque\nUnlike unpacking, patterns don’ t destructure iterables that are not\nsequences.\nThe _  symbol is special in patterns: it matches any single item in that\nposition, but it is never bound to the value of the matched item. Also, the _\nis the only variable that can appear more than once in pattern—unless the\npattern is a combination of patterns joined by the |  operator (which also has\nspecial meaning in a case  clause).\nA sequence pattern can be more strict using type information. For example,\nthe following pattern matches the same nested sequence structure as the one\nin Example 2-9 , but the first item must be an instance of str , and both\nitems in the 2-tuple must be instances of float .\n        case [str(name), _, _, (float(lat), float(lon))]:",9923
23-Pattern Matching Sequences in an Interpreter.pdf,23-Pattern Matching Sequences in an Interpreter,"On the other hand, if we want to match any subject sequence starting with a\nstr , and ending with a nested sequence of two floats, we can write:\n        case [str(name), *_, (float(lat), float(lon))]:\nThe *_  matches any number of items, without binding them to a variable.\nUsing *extra  instead of *_  would bind the items to extra  as a list\nwith 0 or more items.\nThe optional guard clause starting with if  is evaluated only if the pattern\nmatches, and can reference variables bound in the pattern, as in Example 2-\n9 :\n        match record: \n            case [name, _, _, (lat, lon)] if lon <= 0: \n                print(f'{name:15}  | {lat:9.4f}  | {lon:9.4f} ')\nThe nested block with the print  statement runs only if the pattern\nmatches and the guard expression is truthy .\nT I P\nDesctructuring with patterns is so expressive that sometimes a match  with a single\ncase  can make code simpler . Guido van Rossum has a collection of case/match\nexamples, including one that he titled A very deep iterable and type match with\nextraction .\nExample 2-9  is not an improvement over Example 2-8 . It’ s just an example\nto contrast two ways of doing the same thing. The next example shows how\npattern matching can make some code safer , shorter , and easier to read.\nPattern Matching Sequences in an Interpreter\nPeter Norvig of Stanford University wrote lis.py : an interpreter for a subset\nof the Scheme dialect of Lisp in 132 lines of beautiful and readable Python\ncode . I took Norvig’ s MIT -licensed code and updated it to Python 3.10 to\nshowcase pattern matching. In this section, I contrast parts of Norvig’ s\ncode, using if/elif  and unpacking, with a rewrite using match/case .\nThe two main functions of lis.py  are parse  and evaluate .  The parser\ntakes Scheme parenthesized expressions and returns Python lists. For\nexample:\n>>> parse('(gcd 18 44)' ) \n['gcd', 18, 44] \n>>> parse('(define double (lambda (n) (* n 2)))' ) \n['define' , 'double' , ['lambda' , ['n'], ['*', 'n', 2]]]\nThe evaluator takes lists like those and executes them.\nOur focus here is destructuring, so I will not explain the evaluator actions.\nSee “Pattern Matching: a Case Study”  to learn more about how lis.py\nworks.\nHere is Norvig’ s evaluator with minor changes, abbreviated to show only\nthe sequence patterns:\nExample 2-10. Matching patterns without match/case .\ndef evaluate (x, env): \n    ""Evaluate an expression in an environment.""  \n    if ...:  # several lines omitted  \n        ... \n    elif x[0] == 'quote':          # (quote exp)  \n        (_, exp) = x \n        return exp \n    elif x[0] == 'if':             # (if test conseq alt)  \n        (_, test, conseq, alt) = x \n        exp = (conseq if evaluate (test, env) else alt) \n        return evaluate (exp, env) \n    elif x[0] == 'define' :         # (define var exp)  \n        (_, var, exp) = x \n        env[var] = evaluate (exp, env) \n    elif x[0] == 'lambda' :         # (lambda (var...) body...)  \n        (_, parms, *body) = x \n        return Procedure (parms, body, env) \n    # more lines omitted\nNote how the elif  blocks check the first item of the list, and then unpack\nthe list, ignoring the first item. The extensive use of unpacking suggests that5\nthat Norvig is a fan of pattern matching, but he wrote that code originally\nfor Python 2 (though it now works with any Python 3).\nUsing Python 3.10, we can refactor evaluate  like this:\nExample 2-1 1. Pattern matching with match/case —r equir es Python ≥\n3.10.\ndef evaluate (exp, env): \n    ""Evaluate an expression in an environment. "" \n    match exp: \n        case ...:  # several lines omitted  \n            ... \n        case ['quote', exp]:  \n \n            return exp \n        case ['if', test, conseq, alt]:  \n \n            exp = (conseq if evaluate (test, env) else alt) \n            return evaluate (exp, env) \n        case ['define', Symbol(var), exp]:  \n \n            env[var] = evaluate (exp, env) \n        case ['lambda', parms, *body] if len(body) >= 1:  \n \n            return Procedure (parms, body, env) \n        # more lines omitted  \n        case _: \n            raise SyntaxError (repr(exp))  \nMatch if subject is a 2-item sequence starting with 'quote' .\nMatch if subject is a 4-item sequence starting with 'if' .\nMatch if subject is a 3-item sequence starting with 'define' ,\nfollowed by an instance of Symbol .\nMatch if subject is a sequence of 3 or more items starting with\n'lambda' . The guard ensures that *body  captures at least one item.\nWhen there are multiple case  clauses, it is good practice to have a\ncatch-all case . In this example, if the exp  doesn’ t match any of the\npatterns, the expression is malformed, and I raise SyntaxError .\nW ithout a catch-all, the whole match  statement does nothing when a\nsubject doesnt’ t match any case—and this can be a silent failure.\nNorvig deliberately avoided error checking in lis.py , to keep the code easy\nto understand. W ith pattern matching, we can add more checks and still\nkeep it readable. For example: in the 'define'  pattern, the original code\ndoes not ensure that var  is an instance of Symbol —that would require an\nif  block, an isinstance  call, and more code. Example 2-1 1  is shorter\nand safer than Example 2-10 .\nW e can make the 'lambda'  pattern safer using a nested sequence pattern.\nThis is the syntax of lambda  in Scheme:\n(lambda (parms... ) body1 body2... )\nThe nested list after the lambda  keyword is where the names of the formal\nparameters for the function are declared, and it must be a list, even if it has\nonly one element. It may also be an empty list, if the function has no\nparameters—like Python’ s random.random() .\nHowever , as written in Example 2-1 1 , the case  for 'lambda'  matches\nany value in the parms  position, including the first 'x'  in this invalid\nsubject:\n['lambda' , 'x', ['*', 'x', 2]]\nT o enforce the rule that parms  must be a nested list, we can rewrite that\ncase  like this:\n        case ['lambda' , [*parms], *body] if len(body) >= 1: \n            return Procedure (parms, body, env)\nIn a sequence pattern, *  can appear only once per sequence. Here we have\ntwo sequences: the outer and the inner .\nW e only added the characters [*]  to the case , made it look more like the\nScheme syntax it handles, and implemented a new structural safety check.\nPattern matching supports declarative programming: the code describes\n“what” you want to match, instead of “how” to match it. The shape of the\ncode follows the shape of the data.\n \nT\na\nb\nl\ne  \n2\n-\n2\n.  \nS\no\nm\ne  \nS\nc\nh\ne\nm\ne  \ns\ny\nn\nt\na\nc\nt\ni\nc  \nf\no\nr\nm\ns  \na\nn\nd  \nt\nh\ne  \np\na\nt\nt\ne\nr\nn\ns  \nt\no  \nh\na\nn\nd\nl\ne  \nt\nh\ne\nm\n.\n \nScheme syntax Pattern\n \n(quote exp) ['quote', exp]\n(if test conseq alt) ['if', test, conseq, alt]\n(define var exp) ['define', Symbol(var), exp]\n(lambda (parms…) body1 bod ['lambda', [*parms], *body] if len(bod",7014
24-Slicing.pdf,24-Slicing,,0
25-Slice Objects.pdf,25-Slice Objects,"y2…) y) >= 1\n \nI hope this refactoring of Norvig’ s evaluate  with pattern matching\nconvinced you that match/case  can make some code more readable and\nsafer . Recall that this was a quick overview focusing on sequence patterns.\nW e’ll cover other pattern forms in later chapters. Carol W illing’ s\nintroduction to pattern matching  of fers more motivation, explanations and\nexamples.\nN O T E\nIf you are intrigued and want to learn more about Norvig’ s lis.py , read his wonderful\npost (How to W rite a (Lisp) Interpreter (in Python)) . For more on refactoring lis.py  with\npattern matching, see “Pattern Matching: a Case Study” .\nThis concludes our brief tour of unpacking, destructuring, and pattern\nmatching with sequences.\nEvery Python programmer knows that sequences can be sliced using the\ns[a:b]  syntax. W e now turn to some less well-known facts about slicing.\nS l i c i n g\nA common feature of list , tuple , str , and all sequence types in\nPython is the support of slicing operations, which are more powerful than\nmost people realize.\nIn this section, we describe the use  of these advanced forms of slicing.\nTheir implementation in a user -defined class will be covered in Chapter 12 ,\nin keeping with our philosophy of covering ready-to-use classes in this part\nof the book, and creating new classes in Part IV .\nWhy Slices and Range Exclude the Last Item\nThe Pythonic convention of excluding the last item in slices and ranges\nworks well with the zero-based indexing used in Python, C, and many other\nlanguages. Some convenient features of the convention are:\nIt’ s easy to see the length of a slice or range when only the stop\nposition is given: range(3)  and my_list[:3]  both produce\nthree items.\nIt’ s easy to compute the length of a slice or range when start and\nstop are given: just subtract stop - start .\nIt’ s easy to split a sequence in two parts at any index x , without\noverlapping: simply get my_list[:x]  and my_list[x:] . For\nexample:\n>>> l = [10, 20, 30, 40, 50, 60] \n>>> l[:2]  # split at 2  \n[10, 20]  \n>>> l[2:] \n[30, 40, 50, 60]  \n>>> l[:3]  # split at 3  \n[10, 20, 30]  \n>>> l[3:] \n[40, 50, 60]\nThe best ar guments for this convention were written by the Dutch computer\nscientist Edsger W . Dijkstra (see the last reference in “Further Reading” ).\nNow let’ s take a close look at how Python interprets slice notation.\nSlice Objects\nThis is no secret, but worth repeating just in case: s[a:b:c]  can be used\nto specify a stride or step c , causing the resulting slice to skip items. The\nstride can also be negative, returning items in reverse. Three examples\nmake this clear:\n>>> s = 'bicycle'  \n>>> s[::3] \n'bye' \n>>> s[::-1] \n'elcycib'  \n>>> s[::-2] \n'eccb'\nAnother example was shown in Chapter 1  when we used deck[12::13]\nto get all the aces in the unshuf fled deck:\n>>> deck[12::13] \n[Card(rank='A', suit='spades'), Card(rank='A', suit='diamonds'),  \nCard(rank='A', suit='clubs'), Card(rank='A', suit='hearts')]\nThe notation a:b:c  is only valid within []  when used as the indexing or\nsubscript operator , and it produces a slice object: slice(a, b, c) . As\nwe will see in “How Slicing W orks” , to evaluate the expression\nseq[start:stop:step] , Python calls\nseq.__getitem__(slice(start, stop, step)) . Even if you\nare not implementing your own sequence types, knowing about slice objects\nis useful because it lets you assign names to slices, just like spreadsheets\nallow naming of cell ranges.\nSuppose you need to parse flat-file data like the invoice shown in\nExample 2-12 . Instead of filling your code with hardcoded slices, you can\nname them. See how readable this makes the for  loop at the end of the\nexample.\nExample 2-12. Line items fr om a flat-file invoice\n>>> invoice = """""" \n... \n0.....6.................................40........52...55........  \n... 1909  Pimoroni PiBrella                     $17.50    3     \n$52.50 \n... 1489  6mm Tactile Switch x20                 $4.95    2      \n$9.90 \n... 1510  Panavise Jr. - PV-201                 $28.00    1     \n$28.00 \n... 1601  PiTFT Mini Kit 320x240                $34.95    1     \n$34.95 \n... """""" \n>>> SKU = slice(0, 6)",4212
26-Augmented Assignment with Sequences.pdf,26-Augmented Assignment with Sequences,">>> DESCRIPTION  = slice(6, 40) \n>>> UNIT_PRICE  = slice(40, 52) \n>>> QUANTITY  =  slice(52, 55) \n>>> ITEM_TOTAL  = slice(55, None) \n>>> line_items  = invoice.split('\n')[2:] \n>>> for item in line_items : \n...     print(item[UNIT_PRICE ], item[DESCRIPTION ]) \n... \n    $17.50   Pimoroni PiBrella  \n     $4.95   6mm Tactile Switch x20  \n    $28.00   Panavise Jr. - PV-201  \n    $34.95   PiTFT Mini Kit 320x240\nW e’ll come back to slice  objects when we discuss creating your own\ncollections in “V ector T ake #2: A Sliceable Sequence” . Meanwhile, from a\nuser perspective, slicing includes additional features such as\nmultidimensional slices and ellipsis ( ... ) notation. Read on.\nMultidimensional Slicing and Ellipsis\nThe []  operator can also take multiple indexes or slices separated by\ncommas. The __getitem__  and __setitem__  special methods that\nhandle the []  operator simply receive the indices in a[i, j]  as a tuple.\nIn other words, to evaluate a[i, j] , Python calls\na.__getitem__((i, j)) .\nThis is used, for instance, in the external NumPy package, where items of a\ntwo-dimensional numpy.ndarray  can be fetched using the syntax a[i,\nj]  and a two-dimensional slice obtained with an expression like a[m:n,\nk:l] . Example 2-21  later in this chapter shows the use of this notation.\nExcept for memoryview , the built-in sequence types in Python are one-\ndimensional, so they support only one index or slice, and not a tuple of\nthem.\nThe ellipsis—written with three full stops ( ... ) and not …  (Unicode\nU+2026)—is recognized as a token by the Python parser . It is an alias to the\nEllipsis  object, the single instance of the ellipsis  class.  As such, it\ncan be passed as an ar gument to functions and as part of a slice6\n7\nspecification, as in f(a, ..., z)  or a[i:...] . NumPy uses ...  as a\nshortcut when slicing arrays of many dimensions; for example, if x  is a\nfour -dimensional array , x[i, ...]  is a shortcut for x[i, :, :, :,] .\nSee the T entative NumPy T utorial  to learn more about this.\nAt the time of this writing, I am unaware of uses of Ellipsis  or\nmultidimensional indexes and slices in the Python standard library . If you\nspot one, let me know . These syntactic features exist to support user -defined\ntypes and extensions such as NumPy .\nSlices are not just useful to extract information from sequences; they can\nalso be used to change mutable sequences in place—that is, without\nrebuilding them from scratch.\nAssigning to Slices\nMutable sequences can be grafted, excised, and otherwise modified in place\nusing slice notation on the left-hand side of an assignment statement or as\nthe tar get of a del  statement. The next few examples give an idea of the\npower of this notation:\n>>> l = list(range(10)) \n>>> l \n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n>>> l[2:5] = [20, 30] \n>>> l \n[0, 1, 20, 30, 5, 6, 7, 8, 9]  \n>>> del l[5:7] \n>>> l \n[0, 1, 20, 30, 5, 8, 9]  \n>>> l[3::2] = [11, 22] \n>>> l \n[0, 1, 20, 11, 5, 22, 9]  \n>>> l[2:5] = 100  \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : can only assign an iterable  \n>>> l[2:5] = [100] \n>>> l \n[0, 1, 100, 22, 9]\nWhen the tar get of the assignment is a slice, the right-hand side must be\nan iterable object, even if it has just one item.\nEvery coder knows that concatenation is a common operation with\nsequences. Introductory Python tutorials explain the use of +  and *  for that\npurpose, but there are some subtle details on how they work, which we\ncover next.\nU s i n g  +  a n d  *  w i t h  S e q u e n c e s\nPython programmers expect that sequences support +  and * . Usually both\noperands of +  must be of the same sequence type, and neither of them is\nmodified but a new sequence of that same type is created as result of the\nconcatenation.\nT o concatenate multiple copies of the same sequence, multiply it by an\ninteger . Again, a new sequence is created:\n>>> l = [1, 2, 3] \n>>> l * 5 \n[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]  \n>>> 5 * 'abcd' \n'abcdabcdabcdabcdabcd'\nBoth +  and *  always create a new object, and never change their operands.\nW A R N I N G\nBeware of expressions like a * n  when a  is a sequence containing mutable items\nbecause the result may surprise you. For example, trying to initialize a list of lists as\nmy_list = [[]] * 3  will result in a list with three references to the same inner\nlist, which is probably not what you want.\nThe next section covers the pitfalls of trying to use *  to initialize a list of\nlists.\nBuilding Lists of Lists\nSometimes we need to initialize a list with a certain number of nested lists\n—for example, to distribute students in a list of teams or to represent\nsquares on a game board. The best way of doing so is with a list\ncomprehension, as in Example 2-13 .\nExample 2-13. A list with thr ee lists of length 3 can r epr esent a tic-tac-toe\nboar d\n>>> board = [['_'] * 3 for i in range(3)]  \n \n>>> board \n[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]  \n>>> board[1][2] = 'X'  \n \n>>> board \n[['_', '_', '_'], ['_', '_', 'X'], ['_', '_', '_']]\nCreate a list of three lists of three items each. Inspect the structure.\nPlace a mark in row 1, column 2, and check the result.\nA tempting but wrong shortcut is doing it like Example 2-14 .\nExample 2-14. A list with thr ee r efer ences to the same list is useless\n>>> weird_board  = [['_'] * 3] * 3  \n \n>>> weird_board  \n[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]  \n>>> weird_board [1][2] = 'O' \n \n>>> weird_board  \n[['_', '_', 'O'], ['_', '_', 'O'], ['_', '_', 'O']]\nThe outer list is made of three references to the same inner list. While it\nis unchanged, all seems right.\nPlacing a mark in row 1, column 2, reveals that all rows are aliases\nreferring to the same object.\nThe problem with Example 2-14  is that, in essence, it behaves like this\ncode:\nrow = ['_'] * 3 \nboard = [] \nfor i in range(3): \n    board.append(row)  \nThe same row  is appended three times to board .\nOn the other hand, the list comprehension from Example 2-13  is equivalent\nto this code:\n>>> board = [] \n>>> for i in range(3): \n...     row = ['_'] * 3  \n \n...     board.append(row) \n... \n>>> board \n[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']] \n>>> board[2][0] = 'X' \n>>> board  \n \n[['_', '_', '_'], ['_', '_', '_'], ['X', '_', '_']]\nEach iteration builds a new row  and appends it to board .\nOnly row 2 is changed, as expected.\nT I P\nIf either the problem or the solution in this section are not clear to you, relax. Chapter 6\nwas written to clarify the mechanics and pitfalls of references and mutable objects.\nSo far we have discussed the use of the plain +  and *  operators with\nsequences, but there are also the +=  and *=  operators, which produce very\ndif ferent results depending on the mutability of the tar get sequence. The\nfollowing section explains how that works.\nAugmented Assignment with Sequences\nThe augmented assignment operators +=  and *=  behave quite dif ferently\ndepending on the first operand. T o simplify the discussion, we will focus on\naugmented addition first ( += ), but the concepts also apply to *=  and to\nother augmented assignment operators.\nThe special method that makes +=  work is __iadd__  (for “in-place\naddition”). However , if __iadd__  is not implemented, Python falls back\nto calling __add__ . Consider this simple expression:\n>>> a += b\nIf a  implements __iadd__ , that will be called. In the case of mutable\nsequences (e.g., list , bytearray , array.array ), a  will be changed\nin place (i.e., the ef fect will be similar to a.extend(b) ). However , when\na  does not implement __iadd__ , the expression a += b  has the same\nef fect as a = a + b : the expression a + b  is evaluated first, producing\na new object, which is then bound to a . In other words, the identity of the\nobject bound to a  may or may not change, depending on the availability of\n__iadd__ .\nIn general, for mutable sequences, it is a good bet that __iadd__  is\nimplemented and that +=  happens in place. For immutable sequences,\nclearly there is no way for that to happen.\nWhat I just wrote about +=  also applies to *= , which is implemented via\n__imul__ . The __iadd__  and __imul__  special methods are\ndiscussed in Chapter 16 .\nHere is a demonstration of *=  with a mutable sequence and then an\nimmutable one:\n>>> l = [1, 2, 3] \n>>> id(l) \n4311953800  \n  \n>>> l *= 2 \n>>> l \n[1, 2, 3, 1, 2, 3]  \n>>> id(l)",8550
27-A  Assignment Puzzler.pdf,27-A  Assignment Puzzler,"4311953800  \n  \n>>> t = (1, 2, 3) \n>>> id(t) \n4312681568  \n  \n>>> t *= 2 \n>>> id(t) \n4301348296  \nID of the initial list\nAfter multiplication, the list is the same object, with new items\nappended\nID of the initial tuple\nAfter multiplication, a new tuple was created\nRepeated concatenation of immutable sequences is inef ficient, because\ninstead of just appending new items, the interpreter has to copy the whole\ntar get sequence to create a new one with the new items concatenated.\nW e’ve seen common use cases for += . The next section shows an intriguing\ncorner case that highlights what “immutable” really means in the context of\ntuples.\nA += Assignment Puzzler\nT ry to answer without using the console: what is the result of evaluating the\ntwo expressions in Example 2-15 ?\nExample 2-15. A riddle\n>>> t = (1, 2, [30, 40]) \n>>> t[2] += [50, 60]\nWhat happens next? Choose the best answer:\n1 . t  becomes (1, 2, [30, 40, 50, 60]) .8\n9\n2 . TypeError  is raised with the message 'tuple' object\ndoes not support item assignment .\n3 . Neither .\n4 . Both A and B.\nWhen I saw this, I was pretty sure the answer was B, but it’ s actually D,\n“Both A and B”! Example 2-16  is the actual output from a Python 3.9\nconsole.\nExample 2-16. The unexpected r esult: item t2 is changed and  an exception\nis raised\n>>> t = (1, 2, [30, 40]) \n>>> t[2] += [50, 60] \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : 'tuple' object does not support item assignment  \n>>> t \n(1, 2, [30, 40, 50, 60])\nOnline Python T utor  is an awesome online tool to visualize how Python\nworks in detail. Figure 2-5  is a composite of two screenshots showing the\ninitial and final states of the tuple t  from Example 2-16 .10\nFigur e 2-5. Initial and final state of the tuple assignment puzzler (diagram generated by Online\nPython T utor)\nIf you look at the bytecode Python generates for the expression s[a] +=\nb  ( Example 2-17 ), it becomes clear how that happens.\nExample 2-17. Bytecode for the expr ession s[a] += b\n>>> dis.dis('s[a] += b ') \n  1           0 LOAD_NAME                0 (s)  \n              3 LOAD_NAME                1 (a)  \n              6 DUP_TOP_TWO  \n              7 BINARY_SUBSCR                      \n  \n              8 LOAD_NAME                2 (b)  \n             11 INPLACE_ADD                        \n  \n             12 ROT_THREE  \n             13 STORE_SUBSCR                       \n  \n             14 LOAD_CONST               0 (None)  \n             17 RETURN_VALUE\nPut the value of s[a]  on TOS  (T op Of Stack).",2614
28-list.sort versus the sorted Built-In.pdf,28-list.sort versus the sorted Built-In,"Perform TOS += b . This succeeds if TOS  refers to a mutable object\n(it’ s a list, in Example 2-16 ).\nAssign s[a] = TOS . This fails if s  is immutable (the t  tuple in\nExample 2-16 ).\nThis example is quite a corner case—in 20 years using Python, I have never\nseen this strange behavior actually bite somebody .\nI take three lessons from this:\nA void putting mutable items in tuples.\nAugmented assignment is not an atomic operation—we just saw it\nthrowing an exception after doing part of its job.\nInspecting Python bytecode is not too dif ficult, and can be helpful\nto see what is going on under the hood.\nAfter witnessing the subtleties of using +  and *  for concatenation, we can\nchange the subject to another essential operation with sequences: sorting.\nl i s t . s o r t  v e r s u s  t h e  s o r t e d  B u i l t - I n\nThe list.sort  method sorts a list in-place—that is, without making a\ncopy . It returns None  to remind us that it changes the receiver  and does\nnot create a new list. This is an important Python API convention: functions\nor methods that change an object in-place should return None  to make it\nclear to the caller that the receiver was changed, and no new object was\ncreated. Similar behavior can be seen, for example, in the\nrandom.shuffle(s)  function, which shuf fles the mutable sequence s\nin-place, and returns None .11\nN O T E\nThe convention of returning None  to signal in-place changes has a drawback: we\ncannot cascade calls to those methods. In contrast, methods that return new objects (e.g.,\nall str  methods) can be cascaded in the fluent interface style. See W ikipedia’ s “Fluent\ninterface” entry  for further description of this topic.\nIn contrast, the built-in function sorted  creates a new list and returns it. It\naccepts any iterable object as an ar gument, including immutable sequences\nand generators (see Chapter 17 ). Regardless of the type of iterable given to\nsorted , it always returns a newly created list.\nBoth list.sort  and sorted  take two optional, keyword-only\nar guments:\nreverse\nIf True , the items are returned in descending order (i.e., by reversing\nthe comparison of the items). The default is False .\nkey\nA one-ar gument function that will be applied to each item to produce its\nsorting key . For example, when sorting a list of strings,\nkey=str.lower  can be used to perform a case-insensitive sort, and\nkey=len  will sort the strings by character length. The default is the\nidentity function (i.e., the items themselves are compared).\nT I P\nY ou can also use the optional keyword parameter key  with the min()  and max()\nbuilt-ins and with other functions from the standard library (e.g.,\nitertools.groupby()  and heapq.nlargest() ).\nHere are a few examples to clarify the use of these functions and keyword\nar guments. The examples also demonstrate that Python’ s sorting algorithm\nis stable (i.e., it preserves the relative ordering of items that compare\nequal):\n>>> fruits = ['grape', 'raspberry ', 'apple', 'banana'] \n>>> sorted(fruits) \n['apple', 'banana', 'grape', 'raspberry']  \n  \n>>> fruits \n['grape', 'raspberry', 'apple', 'banana']  \n  \n>>> sorted(fruits, reverse=True) \n['raspberry', 'grape', 'banana', 'apple']  \n  \n>>> sorted(fruits, key=len) \n['grape', 'apple', 'banana', 'raspberry']  \n  \n>>> sorted(fruits, key=len, reverse=True) \n['raspberry', 'banana', 'grape', 'apple']  \n  \n>>> fruits \n['grape', 'raspberry', 'apple', 'banana']  \n  \n>>> fruits.sort()                          \n  \n>>> fruits \n['apple', 'banana', 'grape', 'raspberry']  \nThis produces a new list of strings sorted alphabetically .\nInspecting the original list, we see it is unchanged.\nThis is the previous “alphabetical” ordering, reversed.\nA new list of strings, now sorted by length. Because the sorting\nalgorithm is stable, “grape” and “apple,” both of length 5, are in the\noriginal order .\nThese are the strings sorted by length in descending order . It is not the\nreverse of the previous result because the sorting is stable, so again\n“grape” appears before “apple.”\nSo far , the ordering of the original fruits  list has not changed.\nThis sorts the list in place, and returns None  (which the console omits).\nNow fruits  is sorted.12\n13",4285
29-Arrays.pdf,29-Arrays,"W A R N I N G\nBy default, Python sorts strings lexicographically by character code. That means ASCII\nuppercase letters will come before lowercase letters, and non-ASCII characters are\nunlikely to be sorted in a sensible way . “Sorting Unicode T ext”  covers proper ways of\nsorting text as humans would expect.\nOnce your sequences are sorted, they can be very ef ficiently searched. A\nbinary search algorithm is already provided in the bisect  module of the\nPython standard library . That module also includes the bisect.insort\nfunction, which you can use to make sure that your sorted sequences stay\nsorted. Y ou’ll find an illustrated introduction to the bisect  module in\nManaging Or der ed Sequences with Bisect  post in the fluentpython.com\ncompanion W eb site.\nMuch of what we have seen so far in this chapter applies to sequences in\ngeneral, not just lists or tuples. Python programmers sometimes overuse the\nlist  type because it is so handy—I know I’ve done it. For example, if you\nare processing lar ge lists of numbers, you should consider using arrays\ninstead. The remainder of the chapter is devoted to alternatives to lists and\ntuples.\nW h e n  a  L i s t  I s  N o t  t h e  A n s w e r\nThe list  type is flexible and easy to use, but depending on specific\nrequirements, there are better options. For example, an array  saves a lot\nof memory when you need to handle millions of floating-point values. On\nthe other hand, if you are constantly adding and removing items from\nopposite ends of a list, it’ s good to know that a deque  (double-ended\nqueue) is a more ef ficient FIFO  data structure.14\nT I P\nIf your code frequently checks whether an item is present in a collection (e.g., item\nin my_collection ), consider using a set  for my_collection , especially if it\nholds a lar ge number of items. Sets are optimized for fast membership checking. They\nare also iterable, but they are not sequences because the ordering of set items is\nunspecified.. W e cover them in Chapter 3 .\nFor the remainder of this chapter , we discuss mutable sequence types that\ncan replace lists in many cases, starting with arrays.\nArrays\nIf a list only contains numbers, an array.array  is a more ef ficient\nreplacement. Arrays support all mutable sequence operations (including\n.pop , .insert , and .extend ), as well as additional methods for fast\nloading and saving such as .frombytes  and .tofile .\nA Python array is as lean as a C array . As shown in Figure 2-1 , an array\nof float  values does not hold full-fledged float  instances, but only the\npacked bytes representing their machine values—similar to an array of\ndouble  in the C language. When creating an array , you provide a\ntypecode, a letter to determine the underlying C type used to store each item\nin the array . For example, b  is the typecode for what C calls a signed\nchar , an integer ranging from –128 to 127. If you create an\narray('b') , then each item will be stored in a single byte and\ninterpreted as an integer . For lar ge sequences of numbers, this saves a lot of\nmemory . And Python will not let you put any number that does not match\nthe type for the array .\nExample 2-18  shows creating, saving, and loading an array of 10 million\nfloating-point random numbers.\nExample 2-18. Cr eating, saving, and loading a lar ge array of floats\n>>> from array import array  \n \n>>> from random import random \n>>> floats = array('d', (random() for i in range(10**7)))  \n \n>>> floats[-1]  \n \n0.07802343889111107  \n>>> fp = open('floats.bin ', 'wb') \n>>> floats.tofile(fp)  \n \n>>> fp.close() \n>>> floats2 = array('d')  \n \n>>> fp = open('floats.bin ', 'rb') \n>>> floats2.fromfile (fp, 10**7)  \n \n>>> fp.close() \n>>> floats2[-1]  \n \n0.07802343889111107  \n>>> floats2 == floats  \n \nTrue\nImport the array  type.\nCreate an array of double-precision floats (typecode 'd' ) from any\niterable object—in this case, a generator expression.\nInspect the last number in the array .\nSave the array to a binary file.\nCreate an empty array of doubles.\nRead 10 million numbers from the binary file.\nInspect the last number in the array .\nV erify that the contents of the arrays match.\nAs you can see, array.tofile  and array.fromfile  are easy to use.\nIf you try the example, you’ll notice they are also very fast. A quick\nexperiment shows that it takes about 0.1s for array.fromfile  to load\n10 million double-precision floats from a binary file created with\narray.tofile . That is nearly 60 times faster than reading the numbers\nfrom a text file, which also involves parsing each line with the float\nbuilt-in. Saving with array.tofile  is about 7 times faster than writing\none float per line in a text file. In addition, the size of the binary file with 10\nmillion doubles is 80,000,000 bytes (8 bytes per double, zero overhead),\nwhile the text file has 181,515,739 bytes, for the same data.\nFor the specific case of numeric arrays representing binary data, such as\nraster images, Python has the bytes  and bytearray  types discussed in\nChapter 4 .\nW e wrap up this section on arrays with T able 2-3 , comparing the features of\nlist  and array.array .\n \nT\na\nb\nl\ne  \n2\n-\n3\n.  \nM\ne\nt\nh\no\nd\ns  \na\nn\nd  \na\nt\nt\nr\ni\nb\nu\nt\ne\ns  \nf\no\nu\nn\nd  \ni\nn  \nl\ni\ns\nt  \no\nr  \na\nr\nr\na\ny  \n(\nd\ne\np\nr\ne\nc\na\nt\ne\nd  \na\nr\nr\na\ny  \nm\ne\nt\nh\no\nd\ns  \na\nn\nd  \nt\nh\no\ns\ne  \na\nl\ns\no  \ni\nm\np\nl\ne\nm\ne\nn\nt\ne\nd  \nb\ny  \no\nb\nj\ne\nc\nt  \nw\ne\nr\ne  \no\nm\ni\nt\nt\ne\nd  \nf\no\nr  \nb\nr\ne\nv\ni\nt\ny\n)\n \nlist array  \n \ns.__add__(s2) ● ● s + s2 —concatenation\ns.__iadd__(s2) ● ● s += s2 —in-place  \nconcatenation\ns.append(e) ● ● Append one element after last\ns.byteswap() ● Swap bytes of all items in array  \nfor endianness conversion\ns.clear() ● Delete all items\ns.__contains__\n(e)● ● e in s\ns.copy() ● Shallow copy of the list\ns.__copy__() ● Support for copy.copy\ns.count(e) ● ● Count occurrences of an  \nelement\ns.__deepcopy__\n()● Optimized support for copy.d\neepcopy\ns.__delitem__\n(p)● ● Remove item at position p\ns.extend(it) ● ● Append items from iterable it\ns.frombytes(b) ● Append items from byte  \nsequence interpreted as packed  \nmachine values\ns.fromfile(f,  \nn)● Append n  items from binary file  \nf  interpreted as packed machine  \nvalues\ns.fromlist(l) ● Append items from list; if one  \ncauses TypeError , none are  \nappended\ns.__getitem__\n(p)● ● s[p] —get item or slice at  \nposition\ns.index(e) ● ● Find position of first occurrence  \nof e\ns.insert(p, e) ● ● Insert element e  before the item  \nat position p\ns.itemsize ● Length in bytes of each array  \nitem\ns.__iter__() ● ● Get iterator\ns.__len__() ● ● len(s) —number of items\ns.__mul__(n) ● ● s * n —repeated  \nconcatenation\ns.__imul__(n) ● ● s *= n —in-place repeated  \nconcatenation\ns.__rmul__(n) ● ● n * s —reversed repeated  \nconcatenation\ns.pop([p]) ● ● Remove and return item at  \nposition p  (default: last)\ns.remove(e) ● ● Remove first occurrence of  \nelement e  by value\ns.reverse() ● ● Reverse the order of the items in  \nplace\ns.__reversed__\n()● Get iterator to scan items from  \nlast to first\ns.__setitem__\n(p, e)● ● s[p] = e —put e  in position  \np , overwriting existing item or  \nslice.\ns.sort([key],  \n[reverse])● Sort items in place with optional  \nkeyword ar guments key  and r\neverse\ns.tobytes() ● Return items as packed machine  \nvalues in a bytes  object\ns.tofile(f) ● Save items as packed machine  \nvalues to binary file f\ns.tolist() ● Return items as numeric objects  \nin a list\ns.typecode ● One-character string identifying  \nthe C type of the items\n \na  Reversed operators are explained in Chapter 16 .a",7783
30-Memory Views.pdf,30-Memory Views,"T I P\nAs of Python 3.10, the array  type does not have an in-place sort  method like\nlist.sort() . If you need to sort an array , use the built-in sorted  function to\nrebuild the array:\na = array.array(a.typecode , sorted(a))\nT o keep a sorted array sorted while adding items to it, use the bisect.insort\nfunction.\nIf you do a lot of work with arrays and don’ t know about memoryview ,\nyou’re missing out. See the next topic.\nMemory V iews\nThe built-in memoryview  class is a shared-memory sequence type that\nlets you handle slices of arrays without copying bytes. It was inspired by\nthe NumPy library (which we’ll discuss shortly in “NumPy” ). T ravis\nOliphant, lead author of NumPy , answers When should a memoryview be\nused?  like this:\nA memoryview is essentially a generalized NumPy array structur e in\nPython itself (without the math). It allows you to shar e memory between\ndata-structur es (things like PIL images, SQLite databases, NumPy\narrays, etc.) without first copying. This is very important for lar ge data\nsets.\nUsing notation similar to the array  module, the memoryview.cast\nmethod lets you change the way multiple bytes are read or written as units\nwithout moving bits around. memoryview.cast  returns yet another\nmemoryview  object, always sharing the same memory .\nExample 2-19  shows how to create alternate views on the same array of 6\nbytes, to operate on it as 2×3 matrix or a 3×2 matrix:\nExample 2-19. Handling 6 bytes memory of as 1×6, 2×3, and 3×2 views\n>>> from array import array \n>>> octets = array('B', range(6))  \n \n>>> m1 = memoryview (octets)  \n \n>>> m1.tolist() \n[0, 1, 2, 3, 4, 5]  \n>>> m2 = m1.cast('B', [2, 3])  \n \n>>> m2.tolist() \n[[0, 1, 2], [3, 4, 5]]  \n>>> m3 = m1.cast('B', [3, 2])  \n \n>>> m3.tolist() \n[[0, 1], [2, 3], [4, 5]]  \n>>> m2[1,1] = 22  \n \n>>> m3[1,1] = 33  \n \n>>> octets  \n \narray('B', [0, 1, 2, 33, 22, 5])\nBuild array of 6 bytes (typecode 'B' ).\nBuild memoryview  from that array , then export it as list.\nBuild new memoryview  from that previous one, but with 2 rows and 3\ncolumns.\nY et another memoryview , now with 3 rows and 2 columns.\nOverwrite byte in m2  at row 1, column 1 with 22.\nOverwrite byte in m3  at row 1, column 1 with 33.\nDisplay original array , proving that the memory was shared among\noctets , m1 , m2 , and m3 .\nThe awesome power of memoryview  can also be used to corrupt.\nExample 2-20  shows how to change a single byte of an item in an array of\n16-bit integers.\nExample 2-20. Changing the value of an 16-bit integer array item by poking\none of its bytes\n>>> numbers = array.array('h', [-2, -1, 0, 1, 2]) \n>>> memv = memoryview (numbers)",2684
31-NumPy.pdf,31-NumPy,">>> len(memv) \n5 \n>>> memv[0]  \n \n-2 \n>>> memv_oct  = memv.cast('B')  \n \n>>> memv_oct .tolist()  \n \n[254, 255, 255, 255, 0, 0, 1, 0, 2, 0]  \n>>> memv_oct [5] = 4  \n \n>>> numbers \narray('h', [-2, -1, 1024, 1, 2])  \nBuild memoryview  from array of 5 16-bit signed integers (typecode\n'h' ).\nmemv  sees the same 5 items in the array .\nCreate memv_oct  by casting the elements of memv  to bytes (typecode\n'B' ).\nExport elements of memv_oct  as a list of 10 bytes, for inspection.\nAssign value 4 to byte of fset 5.\nNote the change to numbers : a 4 in the most significant byte of a 2-\nbyte unsigned integer is 1024.\nN O T E\nY ou’ll find an example of inspecting memoryview  with the struct  package at\nfluentpython.com : Parsing binary records with struct .\nMeanwhile, if you are doing advanced numeric processing in arrays, you\nshould be using the NumPy libraries. W e’ll take a brief look at them right\naway .\nNumPy\nThroughout this book, I make a point of highlighting what is already in the\nPython standard library so you can make the most of it. But NumPy is so\nawesome that a detour is warranted.\nFor advanced array and matrix operations, NumPy is the reason why\nPython became mainstream in scientific computing applications. NumPy\nimplements multi-dimensional, homogeneous arrays and matrix types that\nhold not only numbers but also user -defined records, and provides ef ficient\nelementwise operations.\nSciPy is a library , written on top of NumPy , of fering many scientific\ncomputing algorithms from linear algebra, numerical calculus, and\nstatistics. SciPy is fast and reliable because it leverages the widely used C\nand Fortran codebase from the Netlib Repository . In other words, SciPy\ngives scientists the best of both worlds: an interactive prompt and high-level\nPython APIs, together with industrial-strength number -crunching functions\noptimized in C and Fortran.\nAs a very brief NumPy demo, Example 2-21  shows some basic operations\nwith two-dimensional arrays.\nExample 2-21. Basic operations with r ows and columns in a numpy .ndarray\n>>> import numpy as np \n \n>>> a = np.arange(12)  \n \n>>> a \narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])  \n>>> type(a) \n<class 'numpy.ndarray'>  \n>>> a.shape  \n \n(12,) \n>>> a.shape = 3, 4  \n \n>>> a \narray([[ 0,  1,  2,  3],  \n       [ 4,  5,  6,  7],  \n       [ 8,  9, 10, 11]])  \n>>> a[2]  \n \narray([ 8,  9, 10, 11])  \n>>> a[2, 1]  \n \n9 \n>>> a[:, 1]  \n \narray([1, 5, 9])  \n>>> a.transpose ()  \n \narray([[ 0,  4,  8],  \n       [ 1,  5,  9],  \n       [ 2,  6, 10],  \n       [ 3,  7, 11]])\nImport NumPy , after installing (it’ s not in the Python standard library).\nConventionally , numpy  is imported as np .\nBuild and inspect a numpy.ndarray  with integers 0 to 1 1.\nInspect the dimensions of the array: this is a one-dimensional, 12-\nelement array .\nChange the shape of the array , adding one dimension, then inspecting\nthe result.\nGet row at index 2 .\nGet element at index 2, 1 .\nGet column at index 1 .\nCreate a new array by transposing (swapping columns with rows).\nNumPy also supports high-level operations for loading, saving, and\noperating on all elements of a numpy.ndarray :\n>>> import numpy \n>>> floats = numpy.loadtxt('floats-10M-lines.txt ')  \n \n>>> floats[-3:]  \n \narray([ 3016362.69195522,   535281.10514262,  4566560.44373946])  \n>>> floats *= .5  \n \n>>> floats[-3:] \narray([ 1508181.34597761,   267640.55257131,  2283280.22186973])  \n>>> from time import perf_counter  as pc \n \n>>> t0 = pc(); floats /= 3; pc() - t0 \n \n0.03690556302899495  \n>>> numpy.save('floats-10M ', floats)  \n \n>>> floats2 = numpy.load('floats-10M.npy ', 'r+')  \n \n>>> floats2 *= 6 \n>>> floats2[-3:]  \n \nmemmap([ 3016362.69195522,   535281.10514262,  4566560.44373946])\nLoad 10 million floating-point numbers from a text file.\nUse sequence slicing notation to inspect the last three numbers.\nMultiply every element in the floats  array by .5 and inspect the last\nthree elements again.\nImport the high-resolution performance measurement timer (available\nsince Python 3.3).\nDivide every element by 3; the elapsed time for 10 million floats is less\nthan 40 milliseconds.\nSave the array in a .npy  binary file.\nLoad the data as a memory-mapped file into another array; this allows\nef ficient processing of slices of the array even if it does not fit entirely\nin memory .\nInspect the last three elements after multiplying every element by 6.\nThis was just an appetizer .\nNumPy and SciPy are formidable libraries, and are the foundation of other\nawesome tools such as the Pandas —which implements ef ficient array types\nthat can hold nonnumeric data and provides import/export functions for\nmany dif ferent formats like .csv , .xls , SQL dumps, HDF5, etc.—and Scikit-\nlearn —currently the most widely used Machine Learning toolset. Most\nNumPy and SciPy functions are implemented in C or C++, and can\nleverage all CPU cores because they release Python’ s GIL (Global\nInterpreter Lock). The Dask  project supports parallelizing NumPy , Pandas,\nand Scikit-Learn processing across clusters of machines. These packages\ndeserve entire books about them. This is not one of those books. But no",5264
32-Deques and Other Queues.pdf,32-Deques and Other Queues,"overview of Python sequences would be complete without at least a quick\nlook at NumPy arrays.\nHaving looked at flat sequences—standard arrays and NumPy arrays—we\nnow turn to a completely dif ferent set of replacements for the plain old\nlist : queues.\nDeques and Other Queues\nThe .append  and .pop  methods make a list  usable as a stack or a\nqueue (if you use .append  and .pop(0) , you get FIFO behavior). But\ninserting and removing from the head of a list (the 0-index end) is costly\nbecause the entire list must be shifted in memory .\nThe class collections.deque  is a thread-safe double-ended queue\ndesigned for fast inserting and removing from both ends. It is also the way\nto go if you need to keep a list of “last seen items” or something of that\nnature, because a deque  can be bounded—i.e., created with a fixed\nmaximum length. If a bounded deque  is full, when you add a new item it\ndiscards an item from the opposite end. Example 2-22  shows some typical\noperations performed on a deque .\nExample 2-22. W orking with a deque\n>>> from collections  import deque \n>>> dq = deque(range(10), maxlen=10)  \n \n>>> dq \ndeque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)  \n>>> dq.rotate(3)  \n \n>>> dq \ndeque([7, 8, 9, 0, 1, 2, 3, 4, 5, 6], maxlen=10)  \n>>> dq.rotate(-4) \n>>> dq \ndeque([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], maxlen=10)  \n>>> dq.appendleft (-1)  \n \n>>> dq \ndeque([-1, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)  \n>>> dq.extend([11, 22, 33])  \n \n>>> dq \ndeque([3, 4, 5, 6, 7, 8, 9, 11, 22, 33], maxlen=10)  \n>>> dq.extendleft ([10, 20, 30, 40])  \n \n>>> dq \ndeque([40, 30, 20, 10, 3, 4, 5, 6, 7, 8], maxlen=10)\nThe optional maxlen  ar gument sets the maximum number of items\nallowed in this instance of deque ; this sets a read-only maxlen\ninstance attribute.\nRotating with n > 0  takes items from the right end and prepends them\nto the left; when n < 0  items are taken from left and appended to the\nright.\nAppending to a deque  that is full ( len(d) == d.maxlen ) discards\nitems from the other end; note in the next line that the 0  is dropped.\nAdding three items to the right pushes out the leftmost -1 , 1 , and 2 .\nNote that extendleft(iter)  works by appending each successive\nitem of the iter  ar gument to the left of the deque, therefore the final\nposition of the items is reversed.\nT able 2-4  compares the methods that are specific to list  and deque\n(removing those that also appear in object ).\nNote that deque  implements most of the list  methods, and adds a few\nthat are specific to its design, like popleft  and rotate . But there is a\nhidden cost: removing items from the middle of a deque  is not as fast. It is\nreally optimized for appending and popping from the ends.\nThe append  and popleft  operations are atomic, so deque  is safe to use\nas a FIFO queue in multithreaded applications without the need for locks.\n \nT\na\nb\nl\ne\n \n2\n-\n4\n.  \nM\ne\nt\nh\no\nd\ns  \ni\nm\np\nl\ne\nm\ne\nn\nt\ne\nd\n \ni\nn\n \nl\ni\ns\nt  \no\nr  \nd\ne\nq\nu\ne\n \n(\nt\nh\no\ns\ne\n \nt\nh\na\nt  \na\nr\ne\n \na\nl\ns\no\n \ni\nm\np\nl\ne\nm\ne\nn\nt\ne\nd\n \nb\ny\n \no\nb\nj\ne\nc\nt  \nw\ne\nr\ne\n \no\nm\ni\nt\nt\ne\nd\n \nf\no\nr  \nb\nr\ne\nv\ni\nt\ny\n)\n \nlist deque  \n \ns.__add__(s2) ● s + s2 —concatenation\ns.__iadd__(s2) ● ● s += s2 —in-place  \nconcatenation\ns.append(e) ● ● Append one element to the right  \n(after last)\ns.appendleft(e) ● Append one element to the left  \n(before first)\ns.clear() ● ● Delete all items\ns.__contains__\n(e)● e in s\ns.copy() ● Shallow copy of the list\ns.__copy__() ● Support for copy.copy  \n(shallow copy)\ns.count(e) ● ● Count occurrences of an  \nelement\ns.__delitem__\n(p)● ● Remove item at position p\ns.extend(i) ● ● Append items from iterable i  to  \nthe right\ns.extendleft(i) ● Append items from iterable i  to  \nthe left\ns.__getitem__\n(p)● ● s[p] —get item or slice at  \nposition\ns.index(e) ● Find position of first occurrence  \nof e\ns.insert(p, e) ● Insert element e  before the item  \nat position p\ns.__iter__() ● ● Get iterator\ns.__len__() ● ● len(s) —number of items\ns.__mul__(n) ● s * n —repeated  \nconcatenation\ns.__imul__(n) ● s *= n —in-place repeated  \nconcatenation\ns.__rmul__(n) ● n * s —reversed repeated  \nconcatenation\ns.pop() ● ● Remove and return last item\ns.popleft() ● Remove and return first item\ns.remove(e) ● ● Remove first occurrence of  \nelement e  by value\ns.reverse() ● ● Reverse the order of the items in  \nplace\ns.__reversed__\n()● ● Get iterator to scan items from  \nlast to first\ns.rotate(n) ● Move n  items from one end to  \nthe other\ns.__setitem__\n(p, e)● ● s[p] = e —put e  in position  \np , overwriting existing item or  \nslice.\ns.sort([key],  \n[reverse])● Sort items in place with optional  \nkeyword ar guments key  and r\neverse\n a\nb\na  Reversed operators are explained in Chapter 16 .\nb  a_list.pop(p)  allows removing from position p  but deque  does not support that option.\nBesides deque , other Python standard library packages implement queues:\nqueue\nThis provides the synchronized (i.e., thread-safe) classes\nSimpleQueue , Queue , LifoQueue , and PriorityQueue . These\ncan be used for safe communication between threads. All except\nSimpleQueue  can be bounded by providing a maxsize  ar gument\ngreater than 0 to the constructor . However , they don’ t discard items to\nmake room as deque  does. Instead, when the queue is full the insertion\nof a new item blocks—i.e., it waits until some other thread makes room\nby taking an item from the queue, which is useful to throttle the number\nof live threads.\nmultiprocessing\nImplements its own unbounded SimpleQueue  and bounded Queue ,\nvery similar to those in the queue  package, but designed for\ninterprocess communication. A specialized\nmultiprocessing.JoinableQueue  is provided for task\nmanagement.\nasyncio\nProvides Queue , LifoQueue , PriorityQueue , and\nJoinableQueue  with APIs inspired by the classes in the queue  and\nmultiprocessing  modules, but adapted for managing tasks in\nasynchronous programming.\nheapq\nIn contrast to the previous three modules, heapq  does not implement a\nqueue class, but provides functions like heappush  and heappop  that\nlet you use a mutable sequence as a heap queue or priority queue.",6323
33-Chapter Summary.pdf,33-Chapter Summary,"This ends our overview of alternatives to the list  type, and also our\nexploration of sequence types in general—except for the particulars of str\nand binary sequences, which have their own chapter ( Chapter 4 ).\nC h a p t e r  S u m m a r y\nMastering the standard library sequence types is a prerequisite for writing\nconcise, ef fective, and idiomatic Python code.\nPython sequences are often categorized as mutable or immutable, but it is\nalso useful to consider a dif ferent axis: flat sequences and container\nsequences. The former are more compact, faster , and easier to use, but are\nlimited to storing atomic data such as numbers, characters, and bytes.\nContainer sequences are more flexible, but may surprise you when they\nhold mutable objects, so you need to be careful to use them correctly with\nnested data structures.\nUnfortunately , Python has no foolproof immutable container sequence type:\neven “immutable” tuples can have their values changed, when they contain\nmutable items like lists or user -defined objects.\nList comprehensions and generator expressions are powerful notations to\nbuild and initialize sequences. If you are not yet comfortable with them,\ntake the time to master their basic usage. It is not hard, and soon you will be\nhooked.\nT uples in Python play two roles: as records with unnamed fields and as\nimmutable lists. When using a tuple as an immutable list, remember that a\ntuple value is only guaranteed to be fixed if all the items in it are also\nimmutable. Calling hash(t)  on a tuple is a quick way to assert that its\nvalue is fixed. A TypeError  will be raised if t  contains mutable items.\nWhen a tuple is used as a record, tuple unpacking is the safest, most\nreadable way of extracting the the fields of the tuple. Beyond tuples, *\nworks with lists and iterables in many contexts, and some of its use cases\nappeared in Python 3.5 with PEP 448—Additional Unpacking\nGeneralizations . Python 3.10 introduced pattern matching with\nmatch/case , supporting more powerful unpacking, known as\ndestructuring.\nSequence slicing is a favorite Python syntax feature, and it is even more\npowerful than many realize. Multidimensional slicing and ellipsis ( ... )",2219
34-Further Reading.pdf,34-Further Reading,"notation, as used in NumPy , may also be supported by user -defined\nsequences. Assigning to slices is a very expressive way of editing mutable\nsequences.\nRepeated concatenation as in seq * n  is convenient and, with care, can\nbe used to initialize lists of lists containing immutable items. Augmented\nassignment with +=  and *=  behaves dif ferently for mutable and immutable\nsequences. In the latter case, these operators necessarily build new\nsequences. But if the tar get sequence is mutable, it is usually changed in\nplace—but not always, depending on how the sequence is implemented.\nThe sort  method and the sorted  built-in function are easy to use and\nflexible, thanks to the optional key  ar gument: a function to calculate the\nordering criterion. By the way , key  can also be used with the min  and max\nbuilt-in functions.\nBeyond lists and tuples, the Python standard library provides\narray.array . Although NumPy and SciPy are not part of the standard\nlibrary , if you do any kind of numerical processing on lar ge sets of data,\nstudying even a small part of these libraries can take you a long way .\nW e closed by visiting the versatile and thread-safe\ncollections.deque , comparing its API with that of list  in T able 2-\n4  and mentioning other queue implementations in the standard library .\nF u r t h e r  R e a d i n g\nChapter 1, “Data Structures” of Python Cookbook, 3r d Edition  (O’Reilly)\nby David Beazley and Brian K. Jones has many recipes focusing on\nsequences, including “Recipe 1.1 1. Naming a Slice,” from which I learned\nthe trick of assigning slices to variables to improve readability , illustrated in\nour Example 2-12 .\nThe second edition of Python Cookbook  was written for Python 2.4, but\nmuch of its code works with Python 3, and a lot of the recipes in Chapters 5\nand 6 deal with sequences. The book was edited by Alex Martelli, Anna\nMartelli Ravenscroft, and David Ascher , and it includes contributions by\ndozens of Pythonistas. The third edition was rewritten from scratch, and\nfocuses more on the semantics of the language—particularly what has\nchanged in Python 3—while the older volume emphasizes pragmatics (i.e.,\nhow to apply the language to real-world problems). Even though some of\nthe second edition solutions are no longer the best approach, I honestly\nthink it is worthwhile to have both editions of Python Cookbook  on hand.\nThe of ficial Python Sorting HOW T O  has several examples of advanced\ntricks for using sorted  and list.sort .\nPEP 3132 — Extended Iterable Unpacking  is the canonical source to read\nabout the new use of *extra  syntax on the left hand of parallel\nassignments. If you’d like a glimpse of Python evolving, Missing *-\nunpacking generalizations  is a bug tracker issue proposing enhancements to\nthe iterable unpacking notation. PEP 448 — Additional Unpacking\nGeneralizations  resulted from the discussions in that issue.\nAs I mentioned in “Pattern Matching with Sequences” , Carol W illing’ s\nStructural Pattern Matching  section of What’ s New In Python 3.10  is a\ngreat introduction to this major new feature in about 1400 words (that’ s less\nthan 5 pages when Firefox makes a PDF from the HTML). PEP 636—\nStructural Pattern Matching: T utorial  is also good, but longer . The same\nPEP 636 includes Appendix A—Quick Intro . It is shorter than W illing’ s\nintro because it omits high level considerations about why pattern matching\nis good for you. If you need more ar guments to convince yourself or others\nthat pattern matching is good for Python, read the 22-page PEP 635—\nStructural Pattern Matching: Motivation and Rationale .\nEli Bendersky’ s blog post “Less Copies in Python with the Buf fer Protocol\nand memoryviews  includes a short tutorial on memoryview .\nThere are numerous books covering NumPy in the market, and many don’ t\nmention “NumPy” in the title. T wo examples are the open access Python\nData Science Handbook  by Jake V anderPlas, and W es McKinney’ s Python\nfor Data Analysis, 2e .\n“NumPy is all about vectorization”. That is the opening sentence of Nicolas\nP . Rougier ’ s open access book From Python to NumPy . V ectorized\noperations apply mathematical functions to all elements of an array without\nan explicit loop written in Python. They can operate in parallel, using\nspecial vector instructions in modern CPUs, leveraging multiple cores or\ndelegating to the GPU, depending on the library . The first example in\nRougier ’ s book shows a speedup of 500 times after refactoring a nice\nPythonic class using a generator method, into a lean and mean function\ncalling a couple of NumPy vector functions.\nT o learn how to use deque  (and other collections) see the examples and\npractical recipes in 8.3. collections — Container datatypes  in the Python\ndocumentation.\nThe best defense of the Python convention of excluding the last item in\nranges and slices was written by Edsger W . Dijkstra himself, in a short\nmemo titled “Why Numbering Should Start at Zero” . The subject of the\nmemo is mathematical notation, but it’ s relevant to Python because Dijkstra\nexplains with rigor and humor why a sequence like 2, 3, …, 12 should\nalways be expressed as 2 ≤ i < 13. All other reasonable conventions are\nrefuted, as is the idea of letting each user choose a convention. The title\nrefers to zero-based indexing, but the memo is really about why it is\ndesirable that 'ABCDE'[1:3]  means 'BC'  and not 'BCD'  and why it\nmakes perfect sense to write range(2, 13)  to produce 2, 3, 4, …, 12.\nBy the way , the memo is a handwritten note, but it’ s beautiful and totally\nreadable. Dijkstra’ s handwriting is so clear that someone created a font  out\nof his notes.\nS O A P B O X\nThe Natur e of T uples\nIn 2012, I presented a poster about the ABC language at PyCon US.\nBefore creating Python, Guido van Rossum had worked on the ABC\ninterpreter , so he came to see my poster . Among other things, we talked\nabout the ABC compounds , which are clearly the predecessors of\nPython tuples. Compounds also support parallel assignment and are\nused as composite keys in dictionaries (or tables , in ABC parlance).\nHowever , compounds are not sequences. They are not iterable and you\ncannot retrieve a field by index, much less slice them. Y ou either handle\nthe compound as whole or extract the individual fields using parallel\nassignment, that’ s all.\nI told Guido that these limitations make the main purpose of\ncompounds very clear: they are just records without field names. His\nresponse: “Making tuples behave as sequences was a hack.”\nThis illustrates the pragmatic approach that made Python more practical\nand more successful than ABC. From a language implementer\nperspective, making tuples behave as sequences costs little. As a result,\nthe main use case for tuples as records is not so obvious, but we gained\nimmutable lists—even if their type is not as clearly named as\nfrozenlist .\nFlat V ersus Container Sequences\nT o highlight the dif ferent memory models of the sequence types, I used\nthe terms container sequence  and flat sequence . The “container” word\nis from the Data Model documentation :\nSome objects contain r efer ences to other objects; these ar e called\ncontainers.\nI used the term “container sequence” to be specific, because there are\ncontainers in Python that are not sequences, like dict  and set .\nContainer sequences can be nested because they may contain objects of\nany type, including their own type.\nOn the other hand, flat sequences  are sequence types that cannot be\nnested because they only hold simple atomic types like integers, floats,\nor characters.\nI adopted the term flat sequence  because I needed something to contrast\nwith “container sequence.”\nUpdate:  despite the previous use of the word “containers” in the\nof ficial documentation, there is an abstract class in\ncollections.abc  called Container . That ABC has just one\nmethod, __contains__ —the special method behind the in  operator .\nThis means that strings and arrays, which are not containers in the\ntraditional sense, are virtual subclasses of Container  because they\nimplement __contains__ . This is just one more example of humans\nusing a word to mean dif ferent things. In this book I’ll write “container”\nwith lowercase letters to mean “an object that contains references to\nother objects” and Container  with capitalized initial in a single-\nspaced font to refer to collections.abc.Container .\nMixed Bag Lists\nIntroductory Python texts emphasize that lists can contain objects of\nmixed types, but in practice that feature is not very useful: we put items\nin a list to process them later , which implies that all items should\nsupport at least some operation in common (i.e., they should all\n“quack” whether or not they are genetically 100% ducks). For example,\nyou can’ t sort a list in Python 3 unless the items in it are comparable:\n>>> l = [28, 14, '28', 5, '9', '1', 0, 6, '23', 19] \n>>> sorted(l) \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : unorderable types: str() < int()\nUnlike lists, tuples often hold items of dif ferent types. That’ s natural: if\neach item in a tuple is a field, then each field may have a dif ferent type.\nKey Is Brilliant\nThe optional key  ar gument of list.sort , sorted , max , and min\nis a great idea. Other languages force you to provide a two-ar gument\ncomparison function like the deprecated cmp(a, b)  function in\nPython 2. Using key  is both simpler and more ef ficient. It’ s simpler\nbecause you just define a one-ar gument function that retrieves or\ncalculates whatever criterion you want to use to sort your objects; this is\neasier than writing a two-ar gument function to return –1, 0, 1. It is also\nmore ef ficient because the key function is invoked only once per item,\nwhile the two-ar gument comparison is called every time the sorting\nalgorithm needs to compare two items. Of course, Python also has to\ncompare the keys while sorting, but that comparison is done in\noptimized C code and not in a Python function that you wrote.\nBy the way , using key  we can sort a mixed bag of numbers and\nnumber -like strings. W e just need to decide whether we want to treat all\nitems as integers or strings:\n>>> l = [28, 14, '28', 5, '9', '1', 0, 6, '23', 19] \n>>> sorted(l, key=int) \n[0, '1', 5, 6, '9', 14, 19, '23', 28, '28']  \n>>> sorted(l, key=str) \n[0, '1', 14, 19, '23', 28, '28', 5, 6, '9']\nOracle, Google, and the T imbot Conspiracy\nThe sorting algorithm used in sorted  and list.sort  is T imsort, an\nadaptive algorithm that switches from insertion sort to mer ge sort\nstrategies, depending on how ordered the data is. This is ef ficient\nbecause real-world data tends to have runs of sorted items. There is a\nW ikipedia article  about it.\nT imsort was first deployed in CPython, in 2002. Since 2009, T imsort is\nalso used to sort arrays in both standard Java and Android, a fact that\nbecame widely known when Oracle used some of the code related to\nT imsort as evidence of Google infringement of Sun’ s intellectual\nproperty . See Oracle v . Google - Day 14 Filings .\nT imsort was invented by T im Peters, a Python core developer so\nprolific that he is believed to be an AI, the T imbot. Y ou can read about\nthat conspiracy theory in Python Humor . T im also wrote The Zen of\nPython: import this .\n1  Leo Geurts, Lambert Meertens, and Steven Pemberton, ABC Pr ogrammer ’ s Handbook , p. 8.\n2  Thanks to reader T ina Lapine for pointing this out.\n3  Thanks to tech reviewer Leonardo Rochael for this example.\n4  In my view , a sequence of if/elif/elif/.../else  blocks is a fine replacement for\nswitch/case . It doesn’ t suf fer from the fallthrough  and dangling else  problems that some\nlanguage designers irrationally copied from C—decades after they were widely known as the\ncause of countless bugs.\n5  The latter is named eval  in Norvig’ s code; I renamed it to avoid confusion with Python’ s\neval  built-in.\n6  In “Memory V iews”  we show that especially constructed memory views can have more than\none dimension.\n7  No, I did not get this backwards: the ellipsis  class name is really all lowercase and the\ninstance is a built-in named Ellipsis , just like bool  is lowercase but its instances are\nTrue  and False .\n8  str  is an exception to this description. Because string building with +=  in loops is so\ncommon in real codebases, CPython is optimized for this use case. Instances str  are allocated\nin memory with extra room, so that concatenation does not require copying the whole string\nevery time.\n9  Thanks to Leonardo Rochael and Cesar Kawakami for sharing this riddle at the 2013\nPythonBrasil Conference.\n10  Readers suggested that the operation in the example can be done with\nt[2].extend([50,60]) , without errors. I am aware of that, but my intent is to show the\nstrange behavior of the +=  operator in this case.\n11  Receiver is the tar get of a method call, the object bound to self  in the method body .\n12  Python’ s main sorting algorithm is named T imsort after its creator , T im Peters. For a bit of\nT imsort trivia, see the “Soapbox” .\n13  The words in this example are sorted alphabetically because they are 100% made of\nlowercase ASCII characters. See warning after the example.\n14  First in, first out—the default behavior of queues.",13455
35-Modern dict Syntax.pdf,35-Modern dict Syntax,"Chapter 3. Dictionaries and Sets\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 3rd chapter of the final book. Please note that the GitHub\nrepo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this chapter ,\nplease reach out to the author at fluentpython2e@ramalho.or g .\nPython is basically dicts wrapped in loads of syntactic sugar .\n— Lalo Martins, early digital nomad and Pythonista.\nW e use dictionaties in all our Python programs. If not directly in our code, then\nindirectly because the dict  type is a fundamental part of Python’ s\nimplementation. Class and instance attributes, module namespaces, and\nfunction keyword ar guments are some of the core Python constructs\nrepresented by dictionaries in memory . The __builtins__.__dict__\nstores all built-in types, objects, and functions.\nBecause of their crucial role, Python dicts are highly optimized—and continue\nto get improvements. Hash tables  are the engines behind Python’ s high-\nperformance dicts.\nOther built-in types based on hash tables are set  and frozenset . These\nof fer richer APIs and operators than the sets you may have encountered in\nother popular languages. In particular , Python sets implement all the\nfundamental operations from set theory , like union, intersection, subset tests\netc. W ith them, we can express algorithms in a more declarative way , avoiding\nlots of nested loops and conditionals.\nHere is a brief outline of this chapter:\nModern syntax to build and handle dicts  and mappings, including\nenhanced unpacking and pattern matching.\nCommon methods of mapping types.\nSpecial handling for missing keys.\nV ariations of dict  in the standard library .\nThe set  and frozenset  types.\nImplications of hash tables in the behavior of sets and dictionaries.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nMost changes in this Second Edition  cover new features related to mapping\ntypes:\n“Modern dict  Syntax”  covers enhanced unpacking syntax and\ndif ferent ways of mer ging mappings—including the |  and |=\noperators supported by dicts since Python 3.9.\n“Pattern Matching with Mappings”  illustrates handling mappings with\nmatch/case , since Python 3.10.\nSection “collections.OrderedDict ”  now focuses on the small\nbut still relevant dif ferences between dict  and OrderedDict —\nconsidering that dict  keeps the key insertion order since Python 3.6.\nNew sections on the view objects returned by dict.keys ,\ndict.items , and dict.values : “Dictionary views”  and “Set\noperations on dict  views” .\nThe underlying implementation of dict  and set  still relies on hash tables,\nbut the dict  code has two important optimizations which save memory and\npreserve the insertion order of the keys in dict . The “Practical Consequences\nof How dict W orks”  and “Practical Consequences of How Sets W ork”\nsummarize what you need to know to use them well.",3245
36-Pattern Matching with Mappings.pdf,36-Pattern Matching with Mappings,"N O T E\nAfter adding more than 200 pages in this Second Edition , I moved the optional section\nInternals of sets and dicts  to the fluentpython.com  companion W eb site. The updated and\nexpanded 18-page post  includes explanations and diagrams about:\nThe hash table algorithm and data structures, starting with its use in set , which is\nsimpler to understand.\nThe memory optimization that preserves key insertion order in dict  instances\n(since Python 3.6).\nThe key-sharing layout for dictionaries holding instance attributes—the\n__dict__  of user -defined objects (optimization implemented in Python 3.3).\nM o d e r n  dict  S y n t a x\nThe next sections decribes advanced syntax features to build, unpack, and\nprocess mappings. Some of these features are not new in the language, but but\nmay be new to you. Others require Python 3.9 (like the |  operator) or Python\n3.10 (like match/case ). Let’ s start with one of the best and oldest of these\nfeatures.\ndict Comprehensions\nSince Python 2.7, the syntax of listcomps and genexps was adapted to dict\ncomprehensions (and set  comprehensions as well, which we’ll soon visit). A\ndictcomp  builds a dict  instance by taking key:value  pairs from any\niterable. Example 3-1  shows the use of dict  comprehensions to build two\ndictionaries from the same list of tuples.\nExample 3-1. Examples of dict compr ehensions\n>>> dial_codes  = [                                                  \n  \n...     (880, 'Bangladesh '), \n...     (55,  'Brazil'), \n...     (86,  'China'), \n...     (91,  'India'), \n...     (62,  'Indonesia '), \n...     (81,  'Japan'), \n...     (234, 'Nigeria'), \n...     (92,  'Pakistan '), \n...     (7,   'Russia'), \n...     (1,   'United States '), \n... ] \n>>> country_dial  = {country: code for code, country in dial_codes }  \n \n>>> country_dial  \n{'Bangladesh': 880, 'Brazil': 55, 'China': 86, 'India': 91,  \n'Indonesia': 62,  \n'Japan': 81, 'Nigeria': 234, 'Pakistan': 92, 'Russia': 7, 'United  \nStates': 1}  \n>>> {code: country.upper()                                          \n  \n...     for country, code in sorted(country_dial .items()) \n...     if code < 70} \n{55: 'BRAZIL', 62: 'INDONESIA', 7: 'RUSSIA', 1: 'UNITED STATES'}\nAn iterable of key-value pairs like dial_codes  can be passed directly to\nthe dict  constructor , but…\n…here we swap the pairs: country  is the key , and code  is the value.\nSorting country_dial  by name, reversing the pairs again, uppercasing\nvalues, and filtering items with code < 70 .\nIf you’re used to listcomps, dictcomps are a natural next step. If you aren’ t, the\nspread of the comprehension syntax means it’ s now more profitable than ever\nto become fluent in it.\nUnpacking Mappings\nPEP 448—Additional Unpacking Generalizations  enhanced the support of\nmapping unpackings in two ways, since Python 3.5.\nFirst, we can apply **  to more than one ar gument in a function call. This\nworks when keys are all strings and unique accross all ar guments (because\nduplicate keyword ar guments are forbidden).\n>>> def dump(**kwargs): \n...     return kwargs \n... \n>>> dump(**{'x': 1}, y=2, **{'z': 3}) \n{'x': 1, 'y': 2, 'z': 3}\nSecond, **  can be used inside a dict  literal—also multiple times.\n>>> {'a': 0, **{'x': 1}, 'y': 2, **{'z': 3, 'x': 4}} \n{'a': 0, 'x': 4, 'y': 2, 'z': 3}\nIn this case, duplicate keys are allowed. Later occurrences overwrite previous\nones—see the value mapped to x  in the example.\nThis syntax can also be used to mer ge mappings, but there are other ways.\nPlease read on.\nMerging Mappings with |\nPython 3.9 supports using |  and |=  to mer ge mappings. This makes sense,\nsince these are also the set union operators.\nThe |  operator creates a new mapping:\n>>> d1 = {'a': 1, 'b': 3} \n>>> d2 = {'a': 2, 'b': 4, 'c': 6} \n>>> d1 | d2 \n{'a': 2, 'b': 4, 'c': 6}\nUsually the type of the new mapping will be the same as the type of the left\noperand—d1  in the example—but it can be the type of the second operand if\nuser -defined types are involved, according the operator overloading rules we\nexplore in Chapter 16 .\nT o update an existing mapping in-place, use |= . Continuing from the previous\nexample, d1  was not changed, but now it is:\n>>> d1 \n{'a': 1, 'b': 3}  \n>>> d1 |= d2 \n>>> d1 \n{'a': 2, 'b': 4, 'c': 6}\nT I P\nIf you need to maintain code to run on Python 3.8 or earlier , the Motivation  section of PEP\n584—Add Union Operators T o dict  provides a good summary of other ways to mer ge\nmappings.\nNow let’ s see how pattern matching applies to mappings.\nPattern Matching with Mappings\nThe match/case  statement supports subjects that are mapping objects.\nPatterns for mappings look like dict  literals, but they can match instances of\nany actual or virtual subclass of collections.abc.Mapping .\nIn Chapter 2  we focused on sequence patterns only , but dif ferent types of\npatterns can be combined and nested. Thanks to destructuring, pattern matching\nis a powerful tool to process records structured like nested mappings and\nsequences, which we often need to read from JSON APIs and databases with\nsemi-structured schemas, like MongoDB, EdgeDB, or PostgreSQL.\nExample 3-2  demonstrates that. The simple type hints in get_creators\nmake it clear that it takes a dict  and returns a list .\nExample 3-2. cr eator .py: get_cr eators() extracts names of cr eators fr om media\nr ecor ds.\ndef get_creators (record: dict) -> list: \n    match record: \n        case {'type': 'book', 'api': 2, 'authors': [*names]}:  \n \n            return names \n        case {'type': 'book', 'api': 1, 'author': name}:  \n \n            return [name] \n        case {'type': 'book'}:  \n \n            raise ValueError (f""Invalid 'book' record: {record!r} "") \n        case {'type': 'movie', 'director ': name}:  \n \n            return [name] \n        case _:  \n \n            raise ValueError (f'Invalid record: {record!r} ')\nMatch any mapping with 'type': 'book', 'api' :2  and an\n'authors'  key mapped to a sequence. Return the items in the sequence,\nas a new list .1\nMatch any mapping with 'type': 'book', 'api' :1  and an\n'author'  key mapped to any object. Return the object inside a list .\nAny other mapping with 'type': 'book'  is invalid, raise\nValueError .\nMatch any mapping with 'type': 'movie'  and a 'director'  key\nmapped to a single object. Return the object inside a list .\nAny other subject is invalid, raise ValueError .\nExample 3-2  shows some useful practices for handling semi-structured data\nsuch as JSON records:\ninclude a field describing the kind of record (e.g. 'type':\n'movie' );\ninclude a field identifying the schema version (e.g. 'api': 2' ) to\nallow for future evolution of public APIs;\nhave case  clauses to handle invalid records of a specific type (e.g.\n'book' ), as well as a catch-all.\nNow let’ s see how get_creators  handles some concrete doctests:\n>>> b1 = dict(api=1, author='Douglas Hofstadter' , \n...         type='book', title='Gödel, Escher, Bach' ) \n>>> get_creators (b1) \n['Douglas Hofstadter']  \n>>> from collections  import OrderedDict  \n>>> b2 = OrderedDict (api=2, type='book', \n...         title='Python in a Nutshell' , \n...         authors='Martelli Ravenscroft Holden' .split()) \n>>> get_creators (b2) \n['Martelli', 'Ravenscroft', 'Holden']  \n>>> get_creators ({'type': 'book', 'pages': 770}) \nTraceback (most recent call last):  \n    ... \nValueError : Invalid 'book' record: {'type': 'book', 'pages': 770}  \n>>> get_creators ('Spam, spam, spam' ) \nTraceback (most recent call last):",7586
37-Standard API of Mapping Types.pdf,37-Standard API of Mapping Types,"... \nValueError : Invalid record: 'Spam, spam, spam'\nNote that the order of the keys in the patterns is irrelevant, even if the subject is\nan OrderedDict  as b2 .\nIn contrast with sequence patterns, mapping patterns succeed on partial\nmatches. In the doctests, the b1  and b2  subjects include a 'title'  key that\ndoes not appear in any 'book'  pattern, yet they match.\nThere is no need to use **extra  to match extra key-value pairs, but if you\nwant to capture them as a dict , you can prefix one variable with ** . It must\nbe the last in the pattern, and **_  is forbidden because it would be redundant.\nA simple example:\n>>> food = dict(category ='ice cream' , flavor='vanilla' , cost=199) \n>>> match food: \n...     case {'category' : 'ice cream' , **details}: \n...         print(f'Ice cream details: {details}' ) \n... \nIce cream details: {'flavor': 'vanilla', 'cost': 199}\nIn “Automatic Handling of Missing Keys”  we’ll study defaultdict  and\nother mappings where key lookups via __getitem__  (i.e. d[key] ) succeed\nbecause missing items are created on the fly . In the context of pattern matching,\na match succeeds only if the subject already has the required keys at the top of\nthe match  statement.\nT I P\nThe automatic handling of missing keys is not triggered because pattern matching always\nuses the d.get(key, sentinel)  method—where the default sentinel  is a special\nmarker value that cannot occur in user data.\nMoving on from syntax and structure, let’ s study the API of mappings.\nS t a n d a r d  A P I  o f  M a p p i n g  T y p e s\nThe collections.abc  module provides the Mapping  and\nMutableMapping  ABCs describing the interfaces of dict  and similar\ntypes. See Figure 3-1 .\nFigur e 3-1. Simplified UML class diagram for the MutableMapping and its super classes fr om\ncollections.abc (inheritance arr ows point fr om subclasses to super classes; names in italic ar e abstract\nclasses and abstract methods)\nThe main value of the ABCs is documenting and formalizing the standard\ninterfaces for mappings, and serving as criteria for isinstance  tests in code\nthat needs to support mappings in a broad sense:\n>>> my_dict = {} \n>>> isinstance (my_dict, abc.Mapping) \nTrue \n>>> isinstance (my_dict, abc.MutableMapping ) \nTrue",2280
38-Overview of Common Mapping Methods.pdf,38-Overview of Common Mapping Methods,"T I P\nUsing isinstance  with an ABC is often better than checking whether a function\nar gument is of the concrete dict  type, because then alternative mapping types can be used.\nW e’ll discuss this in detail in Chapter 13 .\nT o implement a custom mapping, it’ s easier to extend\ncollections.UserDict , or to wrap a dict  by composition, instead of\nsubclassing these ABCs. The collections.UserDict  class and all\nconcrete mapping classes in the standard library encapsulate the basic dict  in\ntheir implementation, which in turn is built on a hash table. Therefore, they all\nshare the limitation that the keys must be hashable  (the values need not be\nhashable, only the keys). If you need a refresher , the next section explains.\nWhat is Hashable\nHere is part of the definition of hashable adapted from the Python Glossary :\nAn object is hashable if it has a hash code which never changes during its\nlifetime (it needs a __hash__()  method), and can be compar ed to other\nobjects (it needs an __eq__()  method). Hashable objects which compar e\nequal must have the same hash code.\nNumeric types and flat immutable types str  and bytes  are all hashable.\nContainer types are hashable if they are immutable and all contained objects\nare also hashable. A frozenset  is always hashable, because every element it\ncontains must be hashable by definition. A tuple  is hashable only if all its\nitems are hashable. See tuples tt , tl , and tf :\n>>> tt = (1, 2, (30, 40)) \n>>> hash(tt) \n8027212646858338501  \n>>> tl = (1, 2, [30, 40]) \n>>> hash(tl) \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : unhashable type: 'list'  \n>>> tf = (1, 2, frozenset ([30, 40])) \n>>> hash(tf) \n-41184199234445011102\nThe hash code of an object may be dif ferent depending on the version of\nPython, the machine architecture, and because of a salt  added to the hash\ncomputation for security reasons.  The hash code of a correctly implemented\nobject is guaranteed to be constant only within one Python process.\nUser -defined types are hashable by default because their hash code is their\nid()  and the __eq__()  method inherited from the object  class simply\ncompares the object ids. If an object implements a custom __eq__()  which\ntakes into account its internal state, it will be hashable only if its\n__hash__()  always returns the same hash code. In practice, this requires\nthat __eq__()  and __hash__()  only take into account instance attributes\nthat never change during the life of the object.\nNow let’ s review the API of the most commonly used mapping types in\nPython: dict , defaultdict  and OrderedDict .\nOverview of Common Mapping Methods\nThe basic API for mappings is quite rich. T able 3-1  shows the methods\nimplemented by dict  and two popular variations: defaultdict  and\nOrderedDict , both defined in the collections  module.3\n \nT a\nble  \n3-\n1.  \nMe\ntho\nds  \nof  \nthe  \nma\nppi\nng  \ntyp\nes  \ndic\nt,  \ncol\nlec\ntio\nns.\ndef\naul\ntdi\nct,  \nan\nd  \ncol\nlec\ntio\nns.\nOr\nder\ned\nDi\nct  \n(co\nm\nmo\nn  \nobj\nect  \nme\ntho\nds  \nom\nitte\nd  \nfor  \nbr e\nvit\ny);  \nopt\nion\nal  \nar\ngu\nme\nnts  \nar e  \nen\nclo\nsed  \nin  \n[…]\n \ndict defaultdict OrderedDict  \n \nd.clear() ● ● ● Remove all items\nd.__contains_\n_(k)● ● ● k in d\nd.copy() ● ● ● Shallow copy\nd.__copy__() ● Support for copy.\ncopy(d)\nd.default_fac\ntory● Callable invoked  \nby __missing__  \nto set missing  \nvalues\nd.__delitem__\n(k)● ● ● del d[k] —\nremove item with  \nkey k\nd.fromkeys(i\nt, [initial])● ● ● New mapping from  \nkeys in iterable,  \nwith optional initial  \nvalue (defaults to N\none )\nd.get(k, [def\nault])● ● ● Get item with key  \nk , return default  \nor None  if missing\nd.__getitem__\n(k)● ● ● d[k] —get item  \nwith key k\nd.items() ● ● ● Get view  over items\n—(key, valu\ne)  pairs\nd.__iter__() ● ● ● Get iterator over  \nkeys\nd.keys() ● ● ● Get view  over keys\nd.__len__() ● ● ● len(d) —number  \nof items\nd.__missing__\n(k)● Called when __ge\ntitem__  cannot  \nfind the key\nd.move_to_end\n(k, [last])● Move k  first or last  \nposition (last  is T\nrue  by default)\nd.__or__(othe\nr)● ● ● Support for d1 | \nd2  to create new da\nict  mer ging d1  \nand d2  (Python ≥  \n3.9)\nd.__ior__(oth\ner)● ● ● Support for d1 |= \nd2  to update d1  \nwith d2  (Python ≥  \n3.9)\nd.pop(k, [def\nault])● ● ● Remove and return  \nvalue at k , or defa\nult  or None  if  \nmissing\nd.popitem() ● ● ● Remove and return  \nthe last inserted  \nitem as (key, va\nlue)  \nd.__reversed_\n_()● ● ● Support for rever\nse(d) —returns  \niterator for keys  \nfrom last to first  \ninserted.\nd.__ror__(oth\ner)● ● ● Support for other \n| dd —reversed  \nunion operator  \n(Python ≥ 3.9)\nd.setdefault\n(k, [defaul\nt])● ● ● If k in d , return  \nd[k] ; else set d\n[k] = default  \nand return it\nd.__setitem__\n(k, v)● ● ● d[k] = v —put v  \nat k\nd.update(m,  \n[**kwargs])● ● ● Update d  with  \nitems from  \nmapping or iterable  \nof (key, valu\ne)  pairs\nd.values() ● ● ● Get view  over  \nvaluesb\nc",5114
39-Inserting or Updating Mutable Values.pdf,39-Inserting or Updating Mutable Values,"a  default_factory  is not a method, but a callable attribute set by the end user when a defaultdict  \nis instantiated.\nb  OrderedDict.popitem(last=False)  removes the first item inserted (FIFO). The last  keyword  \nar gument is not supported in dict  or defaultdict  as recently as Python 3.10b3.\nc  Reversed operators are explained in Chapter 16 .\nThe way d.update(m)  handles its first ar gument m  is a prime example of\nduck typing : it first checks whether m  has a keys  method and, if it does,\nassumes it is a mapping. Otherwise, update()  falls back to iterating over m ,\nassuming its items are (key, value)  pairs. The constructor for most\nPython mappings uses the logic of update()  internally , which means they\ncan be initialized from other mappings or from any iterable object producing\n(key, value)  pairs.\nA subtle mapping method is setdefault() . It avoids redundant key\nlookups when we need to update the value of an item in-place. The next section\nshows how to use it.\nInserting or Updating Mutable V alues\nIn line with Python’ s fail-fast  philosophy , dict  access with d[k]  raises an\nerror when k  is not an existing key . Pythonistas know that d.get(k,\ndefault)  is an alternative to d[k]  whenever a default value is more\nconvenient than handling KeyError . However , when you retrieve a mutable\nvalue and want to update it, there is better way .\nConsider a script to index text, producing a mapping where each key is a word\nand the value is a list of positions where that word occurs, as shown in\nExample 3-3 .\nExample 3-3. Partial output fr om Example 3-4  pr ocessing the Zen of Python;\neach line shows a wor d and a list of occurr ences coded as pairs: (line_number ,\ncolumn_number)\n$ python3 index0.py zen.txt  \na [(19, 48), (20, 53)] \nAlthough [(11, 1), (16, 1), (18, 1)] \nambiguity [(14, 16)] \nand [(15, 23)] \nare [(21, 12)] \naren [(10, 15)] \nat [(16, 38)] \nbad [(19, 50)] \nbe [(15, 14), (16, 27), (20, 50)] \nbeats [(11, 23)] \nBeautiful [(3, 1)] \nbetter [(3, 14), (4, 13), (5, 11), (6, 12), (7, 9), (8, 11), (17, 8), \n(18, 25)] \n...\nExample 3-4 , a suboptimal script written to show one case where dict.get\nis not the best way to handle a missing key . I adapted it from an example by\nAlex Martelli.\nExample 3-4. index0.py uses dict.get to fetch and update a list of wor d\noccurr ences fr om the index (a better solution is in Example 3-5 )\n""""""Build an index mapping word -> list of occurrences""""""  \n \nimport re \nimport sys \n \nWORD_RE = re.compile(r'\w+') \n \nindex = {} \nwith open(sys.argv[1], encoding ='utf-8') as fp: \n    for line_no, line in enumerate (fp, 1): \n        for match in WORD_RE.finditer (line): \n            word = match.group() \n            column_no  = match.start() + 1 \n            location  = (line_no, column_no ) \n            # this is ugly; coded like this to make a point  \n            occurrences  = index.get(word, [])  \n \n            occurrences .append(location )       \n  \n            index[word] = occurrences           \n  \n \n# display in alphabetical order  \nfor word in sorted(index, key=str.upper):  \n \n    print(word, index[word])\nGet the list of occurrences for word , or []  if not found.\nAppend new location to occurrences .\n4\nPut changed occurrences  into index  dict; this entails a second search\nthrough the index .\nIn the key=  ar gument of sorted  I am not calling str.upper , just\npassing a reference to that method so the sorted  function can use it to\nnormalize the words for sorting.\nThe three lines dealing with occurrences  in Example 3-4  can be replaced\nby a single line using dict.setdefault . Example 3-5  is closer to Alex\nMartelli’ s code.\nExample 3-5. index.py uses dict.setdefault to fetch and update a list of wor d\noccurr ences fr om the index in a single line; contrast with Example 3-4\n""""""Build an index mapping word -> list of occurrences""""""  \n \nimport re \nimport sys \n \nWORD_RE = re.compile(r'\w+') \n \nindex = {} \nwith open(sys.argv[1], encoding ='utf-8') as fp: \n    for line_no, line in enumerate (fp, 1): \n        for match in WORD_RE.finditer (line): \n            word = match.group() \n            column_no  = match.start() + 1 \n            location  = (line_no, column_no ) \n            index.setdefault (word, []).append(location )  \n \n \n# display in alphabetical order  \nfor word in sorted(index, key=str.upper): \n    print(word, index[word])\nGet the list of occurrences for word , or set it to []  if not found;\nsetdefault  returns the value, so it can be updated without requiring a\nsecond search.\nIn other words, the end result of this line…\nmy_dict.setdefault (key, []).append(new_value )5",4706
40-Automatic Handling of Missing Keys.pdf,40-Automatic Handling of Missing Keys,,0
41-defaultdict Another Take on Missing Keys.pdf,41-defaultdict Another Take on Missing Keys,"…is the same as running…\nif key not in my_dict: \n    my_dict[key] = [] \nmy_dict[key].append(new_value )\n…except that the latter code performs at least two searches for key—three if\nit’ s not found—while setdefault  does it all with a single lookup.\nA related issue, handling missing keys on any lookup (and not only when\ninserting), is the subject of the next section.\nA u t o m a t i c  H a n d l i n g  o f  M i s s i n g  K e y s\nSometimes it is convenient to have mappings that return some made-up value\nwhen a missing key is searched. There are two main approaches to this: one is\nto use a defaultdict  instead of a plain dict . The other is to subclass\ndict  or any other mapping type and add a __missing__  method. Both\nsolutions are covered next.\ndefaultdict : Another T ake on Missing Keys\nA collections.defaultdict  instance creates items with a default value\non demand whenever a missing key is searched using d[k]  syntax.\nExample 3-6  uses defaultdict  to provide another elegant solution to the\nword index task from Example 3-5 .\nHere is how it works: when instantiating a defaultdict , you provide a\ncallable to produce a default value whenever __getitem__  is passed a\nnonexistent key ar gument.\nFor example, given a defaultdict  created as dd =\ndefaultdict(list) , if 'new-key'  is not in dd , the expression\ndd['new-key']  does the following steps:\n1 . Calls list()  to create a new list.\n2 . Inserts the list into dd  using 'new-key'  as key .\n3 . Returns a reference to that list.\nThe callable that produces the default values is held in an instance attribute\nnamed default_factory .\nExample 3-6. index_default.py: using defaultdict  instead of the\nsetdefault  method\n""""""Build an index mapping word -> list of occurrences""""""  \n \nimport collections  \nimport re \nimport sys \n \nWORD_RE = re.compile(r'\w+') \n \nindex = collections .defaultdict (list)     \n \nwith open(sys.argv[1], encoding ='utf-8') as fp: \n    for line_no, line in enumerate (fp, 1): \n        for match in WORD_RE.finditer (line): \n            word = match.group() \n            column_no  = match.start() + 1 \n            location  = (line_no, column_no ) \n            index[word].append(location )  \n \n \n# display in alphabetical order  \nfor word in sorted(index, key=str.upper): \n    print(word, index[word])\nCreate a defaultdict  with the list  constructor as\ndefault_factory .\nIf word  is not initially in the index , the default_factory  is called to\nproduce the missing value, which in this case is an empty list  that is then\nassigned to index[word]  and returned, so the .append(location)\noperation always succeeds.\nIf no default_factory  is provided, the usual KeyError  is raised for\nmissing keys.",2752
42-The __missing__ Method.pdf,42-The __missing__ Method,"W A R N I N G\nThe default_factory  of a defaultdict  is only invoked to provide default values\nfor __getitem__  calls, and not for the other methods. For example, if dd  is a\ndefaultdict , and k  is a missing key , dd[k]  will call the default_factory  to\ncreate a default value, but dd.get(k)  still returns None , and k in dd  is False .\nThe mechanism that makes defaultdict  work by calling\ndefault_factory  is the __missing__  special method, a feature that we\ndiscuss next.\nThe __missing__  Method\nUnderlying the way mappings deal with missing keys is the aptly named\n__missing__  method. This method is not defined in the base dict  class,\nbut dict  is aware of it: if you subclass dict  and provide a __missing__\nmethod, the standard dict.__getitem__  will call it whenever a key is not\nfound, instead of raising KeyError .\nW A R N I N G\nThe __missing__  method is only called by __getitem__  (i.e., for the d[k]  operator).\nThe presence of a __missing__  method has no ef fect on the behavior of other methods\nthat look up keys, such as get  or __contains__  (which implements the in  operator).\nThis is why the default_factory  of defaultdict  works only with __getitem__ ,\nas noted in the warning at the end of the previous section.\nSuppose you’d like a mapping where keys are converted to str  when looked\nup. A concrete use case is a device library for IoT , where a programmable\nboard with general purpose I/O pins (e.g., a Raspberry Pi or an Arduino) is\nrepresented by a Board  class with a my_board.pins  attribute, which is a\nmapping of physical pin identifiers to pin software objects. The physical pin\nidentifier may be just a number or a string like ""A0""  or ""P9_12"" . For\nconsistency , it is desirable that all keys in board.pins  are strings, but it is\nalso convenient that looking up a pin by number , as in6\nmy_arduino.pin[13] , so that beginners are not tripped when they want to\nblink the LED on pin 13 of their Arduinos. Example 3-7  shows how such a\nmapping would work.\nExample 3-7. When sear ching for a nonstring key , StrKeyDict0 converts it to\nstr when it is not found\nTests for item retrieval  using `d[key]`  notation :: \n \n    >>> d = StrKeyDict0 ([('2', 'two'), ('4', 'four')]) \n    >>> d['2'] \n    'two' \n    >>> d[4] \n    'four' \n    >>> d[1] \n    Traceback  (most recent call last): \n      ... \n    KeyError : '1' \n \nTests for item retrieval  using `d.get(key)`  notation :: \n \n    >>> d.get('2') \n    'two' \n    >>> d.get(4) \n    'four' \n    >>> d.get(1, 'N/A') \n    'N/A' \n \n \nTests for the `in` operator :: \n \n    >>> 2 in d \n    True \n    >>> 1 in d \n    False\nExample 3-8  implements a class StrKeyDict0  that passes the preceding\ndoctests.\nT I P\nA better way to create a user -defined mapping type is to subclass\ncollections.UserDict  instead of dict  (as we’ll do in Example 3-9 ). Here we\nsubclass dict  just to show that __missing__  is supported by the built-in\ndict.__getitem__  method.\nExample 3-8. StrKeyDict0 converts nonstring keys to str on lookup (see tests in\nExample 3-7 )\nclass StrKeyDict0 (dict):  \n \n \n    def __missing__ (self, key): \n        if isinstance (key, str):  \n \n            raise KeyError (key) \n        return self[str(key)]  \n \n \n    def get(self, key, default=None): \n        try: \n            return self[key]  \n \n        except KeyError : \n            return default  \n \n \n    def __contains__ (self, key): \n        return key in self.keys() or str(key) in self.keys()  \nStrKeyDict0  inherits from dict .\nCheck whether key  is already a str . If it is, and it’ s missing, raise\nKeyError .\nBuild str  from key  and look it up.\nThe get  method delegates to __getitem__  by using the self[key]\nnotation; that gives the opportunity for our __missing__  to act.\nIf a KeyError  was raised, __missing__  already failed, so we return\nthe default .\nSearch for unmodified key (the instance may contain non- str  keys), then\nfor a str  built from the key .\nT ake a moment to consider why the test isinstance(key, str)  is\nnecessary in the __missing__  implementation.\nW ithout that test, our __missing__  method would work OK for any key k\n—str  or not str —whenever str(k)  produced an existing key . But if\nstr(k)  is not an existing key , we’d have an infinite recursion. In the last line",4361
43-Variations of dict.pdf,43-Variations of dict,,0
44-Subclassing UserDict Instead of dict.pdf,44-Subclassing UserDict Instead of dict,"of __missing__ , self[str(key)]  would call __getitem__  passing\nthat str  key , which in turn would call __missing__  again.\nThe __contains__  method is also needed for consistent behavior in this\nexample, because the operation k in d  calls it, but the method inherited from\ndict  does not fall back to invoking __missing__ . There is a subtle detail\nin our implementation of __contains__ : we do not check for the key in the\nusual Pythonic way— k  in my_dict —because str(key) in self  would\nrecursively call __contains__ . W e avoid this by explicitly looking up the\nkey in self.keys() .\nN O T E\nA search like k in my_dict.keys()  is ef ficient in Python 3 even for very lar ge\nmappings because dict.keys()  returns a view , which is similar to a set, as we’ll see in\n“Set operations on dict  views” . However , remember that k in my_dict  does the same\njob, and is faster because it avoids the attribute lookup to find the .keys  method. I had a\nspecific reason to use self.keys()  in the __contains__  method in Example 3-8 .\nThe check for the unmodified key— key in self.keys() —is necessary\nfor correctness because StrKeyDict0  does not enforce that all keys in the\ndictionary must be of type str . Our only goal with this simple example is to\nmake searching “friendlier” and not enforce types.\nSo far we have covered the dict  and defaultdict  mapping types, but the\nstandard library comes with other mapping implementations, which we discuss\nnext.\nV a r i a t i o n s  o f  d i c t\nIn this section is an overview of mapping types included in the standard library ,\nbesides defaultdict , already covered in “defaultdict : Another T ake\non Missing Keys” .\ncollections.OrderedDict\nNow that the built-in dict  also keeps the keys ordered since Python 3.6, the\nmost common reason to use OrderedDict  is writing code that is backward-\ncompatible with earlier Python versions. Having said that, Python’ s\ndocumentation lists some remaining dif ferences between dict  and\nOrderedDict , which I quote here—only reordering the items for relevance\nin daily use:\nThe equality operation for OrderedDict  checks for matching order .\nThe popitem()  method of OrderedDict  has a dif ferent\nsignature. It accepts an optional ar gument to specify which item is\npopped.\nOrderedDict  has a move_to_end()  method to ef ficiently\nreposition an element to an endpoint.\nThe regular dict  was designed to be very good at mapping\noperations. T racking insertion order was secondary .\nOrderedDict  was designed to be good at reordering operations.\nSpace ef ficiency , iteration speed, and the performance of update\noperations were secondary .\nAlgorithmically , OrderedDict  can handle frequent reordering\noperations better than dict . This makes it suitable for tracking recent\naccesses (for example in an LRU cache).\ncollections.ChainMap\nA ChainMap  instance holds a list of mappings that can be searched as one.\nThe lookup is performed on each input mapping in the order they appear in the\nconstructor call, and succeeds as soon as the key is found in one of those\nmappings. For example:\n>>> d1 = dict(a=1, b=3) \n>>> d2 = dict(a=2, b=4, c=6) \n>>> from collections  import ChainMap  \n>>> chain = ChainMap (d1, d2) \n>>> chain['a'] \n1 \n>>> chain['c'] \n6\nThe ChainMap  instance does not copy the input mappings, but holds\nreferences to them. A later update to a key in the ChainMap  will update the\nfirst input mapping where that key appears. Continuing the previous example:\n>>> chain['b'] = -1 \n>>> d1 \n{'a': 1, 'b': -1}  \n>>> d2 \n{'a': 2, 'b': 4, 'c': 6}\nChainMap  is useful to implement interpreters for languages with nested\nscopes, where each mapping represents a scope context, from the innermost\nenclosing scope to the outermost scope. The “ChainMap objects” section of the\ncollections  docs  has several examples of ChainMap  usage, including\nthis snippet inspired by the basic rules of variable lookup in Python:\nimport builtins  \npylookup  = ChainMap (locals(), globals(), vars(builtins ))\ncollections.Counter\nA mapping that holds an integer count for each key . Updating an existing key\nadds to its count. This can be used to count instances of hashable objects or as a\nmultiset (see below). Counter  implements the +  and -  operators to combine\ntallies, and other useful methods such as most_common([n]) , which\nreturns an ordered list of tuples with the n  most common items and their\ncounts; see the documentation . Here is Counter  used to count letters in\nwords:\n>>> ct = collections .Counter('abracadabra' ) \n>>> ct \nCounter({'a': 5, 'b': 2, 'r': 2, 'c': 1, 'd': 1})  \n>>> ct.update('aaaaazzz' ) \n>>> ct \nCounter({'a': 10, 'z': 3, 'b': 2, 'r': 2, 'c': 1, 'd': 1})  \n>>> ct.most_common (3) \n[('a', 10), ('z', 3), ('b', 2)]\nNote that the 'b'  and 'r'  keys are tied in third place, but\nct.most_common(3)  shows only three counts.\nT o use collections.Counter  as a multiset, pretend each key is an\nelement in the set, and the count is the number of occurrences of that element\nin the set.\nshelve.Shelf\nThe shelve  module in the standard library provides persistent storage for a\nmapping of string keys to Python objects serialized in the pickle  binary\nformat. The curious name of shelve  makes sense when you realize that\npickle jars are stored in shelves.\nThe shelve.open  module-level function returns a shelve.Shelf\ninstance—a simple key-value DBM database backed by the dbm  module, with\nthese characteristics:\nshelve.Shelf  subclasses abc.MutableMapping , so it\nprovides the essential methods we expect of a mapping type.\nIn addition, shelve.Shelf  provides a few other I/O management\nmethods, like sync  and close .\na Shelf  instance is a context manager , so you can use a with  block\nto make sure it is closed after use.\nKeys and values are saved whenever a new value is assigned to a key .\nThe keys must be strings.\nThe values must be objects that the pickle  module can serialize.\nThe documentation for the shelve , dbm , and pickle  modules provide more\ndetails and some caveats.\nW A R N I N G\nPython’ s pickle  is easy to use in the simplest cases, but has several drawbacks. Read Ned\nBatchelder ’ s Pickle’ s nine flaws  before adopting any solution involving pickle . In his\npost, Ned mentions other serialization formats to consider .\nOrderedDict , ChainMap , Counter , and Shelf  are ready to use but can\nalso be customized by subclassing. In contrast, UserDict  is intended only as\na base class to be extended.\nSubclassing UserDict  Instead of dict\nIt’ s better to create a new mapping type by extending\ncollections.UserDict  rather than dict . W e realize that when we try to\nextend our StrKeyDict0  from Example 3-8  to make sure that any keys\nadded to the mapping are stored as str .\nThe main reason why it’ s better to subclass UserDict  rather than dict  is\nthat the built-in has some implementation shortcuts that end up forcing us to\noverride methods that we can just inherit from UserDict  with no problems.\nNote that UserDict  does not inherit from dict , but uses composition: it has\nan internal dict  instance, called data , which holds the actual items. This\navoids undesired recursion when coding special methods like __setitem__ ,\nand simplifies the coding of __contains__ , compared to Example 3-8 .\nThanks to UserDict , StrKeyDict  ( Example 3-9 ) is actually shorter than\nStrKeyDict0  ( Example 3-8 ), but it does more: it stores all keys as str ,\navoiding unpleasant surprises if the instance is built or updated with data\ncontaining nonstring keys.\nExample 3-9. StrKeyDict always converts non-string keys to str—on insertion,\nupdate, and lookup\nimport collections  \n \n \nclass StrKeyDict (collections .UserDict ):  \n \n \n    def __missing__ (self, key):  \n 7\n        if isinstance (key, str): \n            raise KeyError (key) \n        return self[str(key)] \n \n    def __contains__ (self, key): \n        return str(key) in self.data  \n \n \n    def __setitem__ (self, key, item): \n        self.data[str(key)] = item   \nStrKeyDict  extends UserDict .\n__missing__  is exactly as in Example 3-8 .\n__contains__  is simpler: we can assume all stored keys are str  and\nwe can check on self.data  instead of invoking self.keys()  as we\ndid in StrKeyDict0 .\n__setitem__  converts any key  to a str . This method is easier to\noverwrite when we can delegate to the self.data  attribute.\nBecause UserDict  extends abc.MutableMapping , the remaining\nmethods that make StrKeyDict  a full-fledged mapping are inherited from\nUserDict , MutableMapping , or Mapping . The latter have several useful\nconcrete methods, in spite of being abstract base classes (ABCs). The\nfollowing methods are worth noting:\nMutableMapping.update\nThis powerful method can be called directly but is also used by __init__\nto load the instance from other mappings, from iterables of (key,\nvalue)  pairs, and keyword ar guments. Because it uses self[key] =\nvalue  to add items, it ends up calling our implementation of\n__setitem__ .\nMapping.get\nIn StrKeyDict0  ( Example 3-8 ), we had to code our own get  to return\nthe same results as __getitem__ , but in Example 3-9  we inherited",9249
45-Dictionary views.pdf,45-Dictionary views,"Mapping.get , which is implemented exactly like StrKeyDict0.get\n(see Python source code ).\nT I P\nAntoine Pitrou authored PEP 455 — Adding a key-transforming dictionary to collections  and\na patch to enhance the collections  module with a TransformDict , that is more\ngeneral than StrKeyDict  and preserves the keys as they are provided, before tha\ntransformation is applied. PEP 455 was rejected in May 2015—see Raymond Hettinger ’ s\nrejection message . T o experiment with TransformDict , I extracted Pitrou’ s patch from\nissue18986  into a standalone module ( 03-dict-set/transformdict.py  in the Fluent Python\nSecond Edition  code repository ).\nW e know there are immutable sequence types, but how about an immutable\nmapping? W ell, there isn’ t a real one in the standard library , but a stand-in is\navailable. That’ s next.\nI m m u t a b l e  M a p p i n g s\nThe mapping types provided by the standard library are all mutable, but you\nmay need to prevent users from changing a mapping by accident. A concrete\nuse case can be found, again, in a hardware programming library like Pingo ,\nmentioned in “The __missing__  Method” : the board.pins  mapping\nrepresents the physical GPIO pins on the device. As such, it’ s useful to prevent\ninadvertent updates to board.pins  because the hardware can’ t be changed\nvia software, so any change in the mapping would make it inconsistent with the\nphysical reality of the device.\nThe types  module provides a wrapper class called MappingProxyType ,\nwhich, given a mapping, returns a mappingproxy  instance that is a read-\nonly but dynamic proxy for the original mapping. This means that updates to\nthe original mapping can be seen in the mappingproxy , but changes cannot\nbe made through it. See Example 3-10  for a brief demonstration.\nExample 3-10. MappingPr oxyT ype builds a r ead-only mappingpr oxy instance\nfr om a dict\n>>> from types import MappingProxyType  \n>>> d = {1: 'A'} \n>>> d_proxy = MappingProxyType (d) \n>>> d_proxy \nmappingproxy({1: 'A'})  \n>>> d_proxy[1]  \n \n'A' \n>>> d_proxy[2] = 'x'  \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : 'mappingproxy' object does not support item assignment  \n>>> d[2] = 'B' \n>>> d_proxy  \n \nmappingproxy({1: 'A', 2: 'B'})  \n>>> d_proxy[2] \n'B' \n>>>\nItems in d  can be seen through d_proxy .\nChanges cannot be made through d_proxy .\nd_proxy  is dynamic: any change in d  is reflected.\nHere is how this could be used in practice in the hardware programming\nscenario: the constructor in a concrete Board  subclass would fill a private\nmapping with the pin objects, and expose it to clients of the API via a public\n.pins  attribute implemented as a mappingproxy . That way the clients\nwould not be able to add, remove, or change pins by accident.\nNext, we’ll cover views—which allow high-performance oparations on a\ndict , without unnecessary copying of data.\nD i c t i o n a r y  v i e w s\nThe dict  instance methods .keys() , .values() , and .items()  return\ninstances of classes called dict_keys , dict_values , and dict_items ,\nrespectively . These dictionary views are read-only projections of the internal\ndata structures used in the dict  implementation. They avoid the memory\noverhead of the equivalent Python 2 methods that returned lists duplicating\ndata already in the tar get dict , and they also replace the old methods that\nreturned iterators.\nExample 3-1 1  shows some basic operations supported by all dictionary views.\nExample 3-1 1. The .values()  method r eturns a view of the values in a\ndict .\n>>> d = dict(a=10, b=20, c=30) \n>>> values = d.values() \n>>> values \ndict_values([10, 20, 30])  \n  \n>>> len(values)  \n \n3 \n>>> list(values)  \n \n[10, 20, 30]  \n>>> reversed (values)  \n \n<dict_reversevalueiterator object at 0x10e9e7310>  \n>>> values[0] \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : 'dict_values' object is not subscriptable\nThe repr  of a view object shows its content.\nW e can query the len  of a view .\nV iews are iterable, so it’ s easy to create lists from them.\nV iews implement __reversed__ , returning a custom iterator .\nW e can’ t use []  to get individual items from a view .\nA view object is a dynamic proxy . If the source dict  is updated, you can\nimmediately see the changes through an existing view . Continuing from\nExample 3-1 1 :\n>>> d['z'] = 99 \n>>> d \n{'a': 10, 'b': 20, 'c': 30, 'z': 99}  \n>>> values \ndict_values([10, 20, 30, 99])",4565
46-Set Theory.pdf,46-Set Theory,"The classes dict_keys , dict_values , and dict_items  are internal:\nthey are not available via __builtins__  or any standard library module,\nand even if you get a reference to one of them, you can’ t use it to create a view\nfrom scratch in Python code:\n>>> values_class  = type({}.values()) \n>>> v = values_class () \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : cannot create 'dict_values' instances\nThe dict_values  class is the simplest dictionary view—it implements only\nthe __len__ , __iter__ , and __reversed__  special methods. In addition\nto these methods, dict_keys  and dict_items  implement several set\nmethods, almost as many as the frozenset  class. After we cover sets, we’ll\nhave more to say about dict_keys  and dict_items  in “Set operations on\ndict  views” .\nNow let’ s see some rules and tips informed by the way dict  is implemented\nunder the hood.\nP r a c t i c a l  C o n s e q u e n c e s  o f  H o w  d i c t  W o r k s\nThe hash table implementation of Python’ s dict  is very ef ficient, but it’ s\nimportant to understand the practical ef fects of this design.\nKeys must be hashable objects. They must implement proper\n__hash__  and __eq__  methods as described in “What is\nHashable” .\nItem access by key is very fast. A dict  may have millions of keys,\nbut Python can locate a key directly by computing the hash code of the\nkey and deriving an index of fset into the hash table, with the possible\noverhead of a small number of tries to find a matching entry .\nKey ordering is preserved as a side-ef fect of a more compact memory\nlayout for dict  in CPython 3.6, which became an of ficial language\nfeature in 3.7.\nDespite its new compact layout, dicts inevitably have a significant\nmemory overhead. The most compact internal data structure for a\ncontainer would be an array of pointers to the items.  Compared to\nthat, a hash table needs to store more data per entry , and Python needs\nto keep at least ⅓  of the hash table rows empty to remain ef ficient.\nT o save memory , avoid creating instance attributes outside of the\n__init__  method.\nThat tip about instance attributes comes from the fact that Python’ s default\nbehavior is to store instance attributes in a special __dict__  attribute which\nis a dict  attached to each instance.  Since PEP 412—Key-Sharing Dictionary\nwas implemented in Python 3.3, instances of a class can share a common hash\ntable, stored with the class. That common hash table is shared by the\n__dict__  of each new instance that has the same attributes names as the first\ninstance of that class when __init__  returns. Each instance __dict__  can\nthen hold only its own attribute values as a simple array of pointers. Adding an\ninstance attribute after __init__  forces Python to create a new hash table\njust for the __dict__  of that one instance (which was the default behavior for\nall instances before Python 3.3). According to PEP 412, this optimization\nreduces memory use by 10% to 20% for object-oriented programs.\nThe details of the compact layout and key-sharing optimizations are rather\ncomplex. For more, please read Internals of sets and dicts  at fluentpython.com .\nNow let’ s dive into sets.\nS e t  T h e o r y\nSets are not new in Python, but are still somewhat underused. The set  type\nand its immutable sibling frozenset  first appeared as modules in the Python\n2.3 standard library , and were promoted to built-ins in Python 2.6.\nN O T E\nIn this book, I use the word “set” to refer both to set  and frozenset . When talking\nspecifically about the set  class, I use constant width font: set .8\n9\nA set is a collection of unique objects. A basic use case is removing\nduplication:\n>>> l = ['spam', 'spam', 'eggs', 'spam', 'bacon', 'eggs'] \n>>> set(l) \n{'eggs', 'spam', 'bacon'}  \n>>> list(set(l)) \n['eggs', 'spam', 'bacon']\nT I P\nIf you want to remove duplicates but also preserve the order of the first occurrence of each\nitem, you can now use a plain dict  to do it, like this:\n>>> dict.fromkeys (l).keys() \ndict_keys(['spam', 'eggs', 'bacon'])  \n>>> list(dict.fromkeys (l).keys()) \n['spam', 'eggs', 'bacon']\nSet elements must be hashable. The set  type is not hashable, so you can’ t\nbuild a set  with nested set  instances. But frozenset  is hashable, so you\ncan have frozenset  elements inside a set .\nIn addition to enforcing uniqueness, the set types implement many set\noperations as infix operators, so, given two sets a  and b , a | b  returns their\nunion, a & b  computes the intersection, a - b  the dif ference, and a ^ b\nthe symmetric dif ference. Smart use of set operations can reduce both the line\ncount and the execution time of Python programs, at the same time making\ncode easier to read and reason about—by removing loops and conditional logic.\nFor example, imagine you have a lar ge set of email addresses (the haystack )\nand a smaller set of addresses (the needles ) and you need to count how\nmany needles  occur in the haystack . Thanks to set  intersection (the &\noperator) you can code that in a simple line (see Example 3-12 ).\nExample 3-12. Count occurr ences of needles in a haystack, both of type set\nfound = len(needles & haystack )",5262
47-Set Operations.pdf,47-Set Operations,"W ithout the intersection operator , you’d have write Example 3-13  to\naccomplish the same task as Example 3-12 .\nExample 3-13. Count occurr ences of needles in a haystack (same end r esult as\nExample 3-12 )\nfound = 0 \nfor n in needles: \n    if n in haystack : \n        found += 1\nExample 3-12  runs slightly faster than Example 3-13 . On the other hand,\nExample 3-13  works for any iterable objects needles  and haystack , while\nExample 3-12  requires that both be sets. But, if you don’ t have sets on hand,\nyou can always build them on the fly , as shown in Example 3-14 .\nExample 3-14. Count occurr ences of needles in a haystack; these lines work for\nany iterable types\nfound = len(set(needles) & set(haystack )) \n \n# another way:  \nfound = len(set(needles).intersection (haystack ))\nOf course, there is an extra cost involved in building the sets in Example 3-14 ,\nbut if either the needles  or the haystack  is already a set, the alternatives\nin Example 3-14  may be cheaper than Example 3-13 .\nAny one of the preceding examples are capable of searching 1,000 elements in\na haystack  of 10,000,000 items in about 0.3 milliseconds—that’ s close to\n0.3 microseconds per element.\nBesides the extremely fast membership test (thanks to the underlying hash\ntable), the set  and frozenset  built-in types provide a rich API to create\nnew sets or , in the case of set , to change existing ones. W e will discuss the\noperations shortly , but first a note about syntax.\nSet Literals\nThe syntax of set  literals—{1} , {1, 2} , etc.—looks exactly like the math\nnotation, with one important exception: there’ s no literal notation for the empty\nset , so we must remember to write set() .\nS Y N T A X  Q U I R K\nDon’ t for get: to create an empty set , you should use the constructor without an ar gument:\nset() . If you write {} , you’re creating an empty dict —this hasn’ t changed in Python 3.\nIn Python 3, the standard string representation of sets always uses the {…}\nnotation, except for the empty set:\n>>> s = {1} \n>>> type(s) \n<class 'set'>  \n>>> s \n{1} \n>>> s.pop() \n1 \n>>> s \nset()\nLiteral set  syntax like {1, 2, 3}  is both faster and more readable than\ncalling the constructor (e.g., set([1, 2, 3]) ). The latter form is slower\nbecause, to evaluate it, Python has to look up the set  name to fetch the\nconstructor , then build a list, and finally pass it to the constructor . In contrast,\nto process a literal like {1, 2, 3} , Python runs a specialized BUILD_SET\nbytecode .\nThere is no special syntax to represent frozenset  literals—they must be\ncreated by calling the constructor . The standard string representation in Python\n3 looks like a frozenset  constructor call. Note the output in the console\nsession:\n>>> frozenset (range(10)) \nfrozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9})\nSpeaking of syntax, the idea of listcomps was adapted to build sets as well.\nSet Comprehensions\nSet comprehensions ( setcomps ) were added way back in Python 2.7, together\nwith the dictcomps that we saw in “dict Comprehensions” . Example 3-1510\nshows how .\nExample 3-15. Build a set of Latin-1 characters that have the wor d “SIGN” in\ntheir Unicode names\n>>> from unicodedata  import name  \n \n>>> {chr(i) for i in range(32, 256) if 'SIGN' in name(chr(i),'')}  \n \n{'§', '=', '¢', '#', '¤', '<', '¥', 'µ', '×', '$', '¶', '£', '©',  \n'°', '+', '÷', '±', '>', '¬', '®', '%'}\nImport name  function from unicodedata  to obtain character names.\nBuild set of characters with codes from 32 to 255 that have the word\n'SIGN'  in their names.\nThe order of the output changes for each Python process, because of the salted\nhash mentioned in “What is Hashable” .\nSyntax matters aside, let’ s now consider the behavior of sets.\nP r a c t i c a l  C o n s e q u e n c e s  o f  H o w  S e t s  W o r k\nThe set  and frozenset  types are both implemented with a hash table. This\nhas these ef fects:\nSet elements must be hashable objects. They must implement proper\n__hash__  and __eq__  methods as described in “What is\nHashable” .\nMembership testing is very ef ficient. A set may have millions of\nelements, but an element can be located directly by computing its hash\ncode and deriving an index of fset, with the possible overhead of a\nsmall number of tries to find a matching element or exhaust the search.\nSets have a significant memory overhead, compared to a low-level\narray a pointers to its elements—which would be more compact but\nalso much slower to search beyond a handful of elements.\nElement ordering depends on insertion order , but not in a useful or\nreliable way . If two elements are dif ferent but have the same hash\ncode, their position depends on which element is added first.\nAdding elements to a set may change the order of existing elements.\nThat’ s because the algorithm becomes less ef ficient if the hash table is\nmore than ⅔  full, so Python may need to move and resize the table as\nit grows. When this happens, elements are reinserted and and their\nrelative ordering may change.\nSee Internals of sets and dicts  at fluentpython.com  for details.\nLet’ s now review the rich assortment of operations provided by sets.\nSet Operations\nFigure 3-2  gives an overview of the methods you can use on mutable and\nimmutable sets. Many of them are special methods that overload operators such\nas &  and >= . T able 3-2  shows the math set operators that have corresponding\noperators or methods in Python. Note that some operators and methods perform\nin-place changes on the tar get set (e.g., &= , difference_update , etc.).\nSuch operations make no sense in the ideal world of mathematical sets, and are\nnot implemented in frozenset .\nT I P\nThe infix operators in T able 3-2  require that both operands be sets, but all other methods take\none or more iterable ar guments. For example, to produce the union of four collections, a , b ,\nc , and d , you can call a.union(b, c, d) , where a  must be a set , but b , c , and d  can\nbe iterables of any type that produces hashable items. If you need to create a new set with the\nunion of for iterables, instead of updating an existing set, you can write {*a, *b, *c,\n*d}  since Python 3.5 thanks to PEP 448—Additional Unpacking Generalizations .\nFigur e 3-2. Simplified UML class diagram for MutableSet  and its super classes fr om\ncollections.abc  (names in italic ar e abstract classes and abstract methods; r everse operator\nmethods omitted for br evity)\n \nT\na\nb\nl\ne  \n3\n-\n2\n.  \nM\na\nt\nh\ne\nm\na\nt\ni\nc\na\nl  \ns\ne\nt  \no\np\ne\nr\na\nt\ni\no\nn\ns\n:  \nt\nh\ne\ns\ne  \nm\ne\nt\nh\no\nd\ns  \ne\ni\nt\nh\ne\nr  \np\nr\no\nd\nu\nc\ne  \na  \nn\ne\nw\n \ns\ne\nt  \no\nr  \nu\np\nd\na\nt\ne  \nt\nh\ne  \nt\na\nr\ng\ne\nt  \ns\ne\nt  \ni\nn  \np\nl\na\nc\ne\n,  \ni\nf  \ni\nt’\ns  \nm\nu\nt\na\nb\nl\ne\n \nMath symbol Python operator Method Description\n \nS ∩ Z s & z s.__and__(z) Intersection of s  and z\nz & s s.__rand__(z) Reversed &  operator\ns.intersection\n(it, …)Intersection of s  and all sets built  \nfrom iterables it , etc.\ns &= z s.__iand__(z) s  updated with intersection of s  and  \nz\ns.intersection_\nupdate(it, …)s  updated with intersection of s  and  \nall sets built from iterables it , etc.\nS ∪  Z s | z s.__or__(z) Union of s  and z\nz | s s.__ror__(z) Reversed |\ns.union(it, …) Union of s  and all sets built from  \niterables it , etc.\ns |= z s.__ior__(z) s  updated with union of s  and z\ns.update(it, …) s  updated with union of s  and all  \nsets built from iterables it , etc.\nS \ Z s - z s.__sub__(z) Relative complement or dif ference  \nbetween s  and z\nz - s s.__rsub__(z) Reversed -  operator\ns.difference(i\nt, …)Dif ference between s  and all sets  \nbuilt from iterables it , etc.\ns -= z s.__isub__(z) s  updated with dif ference between  \ns  and z\ns.difference_up\ndate(it, …)s  updated with dif ference between  \ns  and all sets built from iterables i\nt , etc.\nS ∆ Z s ^ z s.__xor__(z) Symmetric dif ference (the  \ncomplement of the intersection s & \nz )\nz ^ s s.__rxor__(z) Reversed ^  operator\ns.symmetric_dif\nference(it)Complement of s & set(it)\ns ^= z s.__ixor__(z) s  updated with symmetric  \ndif ference of s  and z\ns.symmetric_dif\nference_update\n(it, …)s  updated with symmetric  \ndif ference of s  and all sets built  \nfrom iterables it , etc.\n \nT able 3-3  lists set predicates: operators and methods that return True  or\nFalse .\n \nT\na\nb\nl\ne  \n3\n-\n3\n.  \nS\ne\nt  \nc\no\nm\np\na\nr\ni\ns\no\nn  \no\np\ne\nr\na\nt\no\nr\ns  \na\nn\nd  \nm\ne\nt\nh\no\nd\ns  \nt\nh\na\nt  \nr\ne\nt\nu\nr\nn  \na  \nb\no\no\nl\n \nMath symbol Python operator Method Description\n \nS ∩ Z = ∅ s.isdisjoint\n(z)s  and z  are disjoint (no elements in  \ncommon)\ne ∈  S e in s s.__contains_\n_(e)Element e  is a member of s\nS ⊆  Z s <= z s.__le__(z) s  is a subset of the z  set\ns.issubset(i\nt)s  is a subset of the set built from the  \niterable it\nS ⊂  Z s < z s.__lt__(z) s  is a proper subset of the z  set\nS ⊇  Z s >= z s.__ge__(z) s  is a superset of the z  set\ns.issuperset\n(it)s  is a superset of the set built from the  \niterable it\nS ⊃  Z s > z s.__gt__(z) s  is a proper superset of the z  set\n \nIn addition to the operators and methods derived from math set theory , the set\ntypes implement other methods of practical use, summarized in T able 3-4 .\n \nT\na\nb\nl\ne\n \n3\n-\n4\n.  \nA\nd\nd\ni\nt\ni\no\nn\na\nl  \ns\ne\nt  \nm\ne\nt\nh\no\nd\ns\n \nset frozenset  \n \ns.add(e) ● Add element e  to s",9569
48-Set operations on dict views.pdf,48-Set operations on dict views,"s.clear() ● Remove all elements of s\ns.copy() ● ● Shallow copy of s\ns.discard(e) ● Remove element e  from s  if it is  \npresent\ns.__iter__() ● ● Get iterator over s\ns.__len__() ● ● len(s)\ns.pop() ● Remove and return an element from s ,  \nraising KeyError  if s  is empty\ns.remove(e) ● Remove element e  from s , raising Key\nError  if e not in s\n \nThis completes our overview of the features of sets. As promised in\n“Dictionary views” , we’ll now see how two of the dictionary view types\nbehave very much like a frozenset .\nS e t  o p e r a t i o n s  o n  dict  v i e w s\nT able 3-5  shows that the view objects returned by the dict  methods\n.keys()  and .items()  are remarkably similar to frozenset .\n \nT\na\nb\nl\ne  \n3\n-\n5\n.  \nM\ne\nt\nh\no\nd\ns  \ni\nm\np\nl\ne\nm\ne\nn\nt\ne\nd  \nb\ny  \nf\nr\no\nz\ne\nn\ns\ne\nt ,  \nd\ni\nc\nt\n_\nk\ne\ny\ns ,  \na\nn\nd  \nd\ni\nc\nt\n_\ni\nt\ne\nm\ns .\n \nfrozenset dict_keys dict_items Description\n \ns.__and__(z) ● ● ● s & z  \n(intersection of s  \nand z )\ns.__rand__(z) ● ● ● Reversed &  \noperator\ns.__contains_\n_()● ● ● e in s\ns.copy() ● Shallow copy of s\ns.difference\n(it, …)● Dif ference between  \ns  and iterables it ,  \netc.\ns.intersectio\nn(it, …)● Intersection of s  \nand iterables it ,  \netc.\ns.isdisjoint\n(z)● ● ● s  and z  are disjoint  \n(no elements in  \ncommon)\ns.issubset(i\nt)● s  is a subset of  \niterable it\ns.issuperset\n(it)● s  is a superset of  \niterable it\ns.__iter__() ● ● ● Get iterator over s\ns.__len__() ● ● ● len(s)\ns.__or__(z) ● ● ● s | z  (union of s  \nand z )\ns.__ror__() ● ● ● Reversed |  \noperator\ns.__reversed_\n_()● ● Get iterator over s  \nin reverse order\ns.__rsub__(z) ● ● ● Reversed -  \noperator\ns.__sub__(z) ● ● ● s - z  (dif ference  \nbetween s  and z )\ns.symmetric_d\nifference(it)● Complement of s \n& set(it)\ns.union(it,  \n…)● Union of s  and  \niterables it , etc.\ns.__xor__() ● ● ● s ^ z  (symmetric  \ndif ference of s  and  \nz )\ns.__rxor__() ● ● ● Reversed ^  \noperator\n \nIn particular , dict_keys  and dict_items  implement the special methods\nto support the powerful set operators &  (intersection), |  (union), -  (dif ference)\nand ^  (symmetric dif ference).\nFor example, using &  is easy to get the keys that appear in two dictionaries:\n>>> d1 = dict(a=1, b=2, c=3, d=4) \n>>> d2 = dict(b=20, d=40, e=50) \n>>> d1.keys() & d2.keys() \n{'b', 'd'}\nNote that the return value of &  is a set . Even better: the set operators in\ndictionary views are compatible with set  instances. Check this out:\n>>> s = {'a', 'e', 'i'} \n>>> d1.keys() & s \n{'a'} \n>>> d1.keys() | s \n{'a', 'c', 'b', 'd', 'i', 'e'}\nW A R N I N G\nA dict_items  view only works as a set if all values in the dict  are hashable. Attempting\nset operations on a dict_items  view with an unhashable value raises TypeError:\nunhashable type 'T' , with T  as the type of the of fending value.\nOn the other hand, a dict_keys  view can always be used as a set, because every key is\nhashable—by definition.\nUsing set operators with views will save a lot of loops and ifs when inspecting\nthe contents of dictionaries in your code. Let Python’ s ef ficient implementation\nin C work for you!",3226
49-Chapter Summary.pdf,49-Chapter Summary,"W ith this, we can wrap up this chapter .\nC h a p t e r  S u m m a r y\nDictionaries are a keystone of Python. Over the years, the familiar {k1: v1,\nk2: v2}  literal syntax was enhanced to support unpacking with ** , pattern\nmatching—as well as dict  comprehensions.\nBeyond the basic dict , the standard library of fers handy , ready-to-use\nspecialized mappings like defaultdict , ChainMap , and Counter , all\ndefined in the collections  module. W ith the new dict  implementation,\nOrderedDict  is not as useful as before, but should remain in the standard\nlibrary for backward compatibility—and has specific characterstics that dict\ndoesn’ t have—such as taking into account key ordering in ==  comparisons.\nAlso in the collections  module is the UserDict , an easy to use base\nclass to create custom mappings.\nT wo powerful methods available in most mappings are setdefault  and\nupdate . The setdefault  method can update items holding mutable values\n—for example, in a dict  of list  values—avoiding a second search for the\nsame key . The update  method allows bulk insertion or overwriting of items\nfrom any other mapping, from iterables providing (key, value)  pairs and\nfrom keyword ar guments. Mapping constructors also use update  internally ,\nallowing instances to be initialized from mappings, iterables, or keyword\nar guments. Since Python 3.9 we can also use the |=  operator to update a\nmapping, and the |  operator to create a new one from the union of two\nmappings.\nA clever hook in the mapping API is the __missing__  method, which lets\nyou customize what happens when a key is not found when using the d[k]\nsyntax which invokes __getitem__ .\nThe collections.abc  module provides the Mapping  and\nMutableMapping  abstract base classes as standard interfaces, useful for\nrun-time type checking. The MappingProxyType  from the types  module\ncreates an immutable façade for a mapping you want to protect from accidental\nchange. There are also ABCs for Set  and MutableSet .\nDictionary views were great addition in Python 3, eliminating the memory\noverhead of the Python 2 .keys() , .values()  and .items()  methods",2158
50-Further Reading.pdf,50-Further Reading,"that built lists duplicating data in the tar get dict  instance. In addition, the\ndict_keys  and dict_items  classes support the most useful operators and\nmethods of frozenset .\nF u r t h e r  R e a d i n g\nIn The Python Standard Library documentation, 8.3. collections — Container\ndatatypes  includes examples and practical recipes with several mapping types.\nThe Python source code for the module Lib/collections/__init__.py  is a great\nreference for anyone who wants to create a new mapping type or grok the logic\nof the existing ones. Chapter 1 of Python Cookbook, Thir d edition  (O’Reilly)\nby David Beazley and Brian K. Jones has 20 handy and insightful recipes with\ndata structures—the majority using dict  in clever ways.\nGreg Gandenber ger advocates for the continued use of\ncollections.OrderedDict , on the grounds that “explicit is better than\nimplicit”, backward compatibility , and the fact that some tools and libraries\nassume the ordering of dict  keys is irrelevant—his post: Python Dictionaries\nAre Now Ordered. Keep Using OrderedDict. .\nPEP 3106 — Revamping dict.keys(), .values() and .items()  is where Guido van\nRossum presented the dictionary views feature for Python 3. In the abstract, he\nwrote the idea came from the Java Collections Framework.\nPyPy  was the first Python interpreter to implement Raymond Hettinger ’ s\nproposal of compact dicts, and they blogged about it in Faster , more memory\nef ficient and more ordered dictionaries on PyPy , acknowledging that a similar\nlayout was adopted in PHP 7, described in PHP’ s new hashtable\nimplementation . It’ s always great when creators cite prior art.\nAt PyCon 2017, Brandon Rhodes presented The Dictionary Even Mightier , a\nsequel to his classic animated presentation The Mighty Dictionary —including\nanimated hash collisions! Another up-to-date, but more in-depth video on the\ninternals of Python’ s dict  is Modern Dictionaries  by Raymond Hettinger ,\nwhere he tells that after initially failing to sell compact dicts to the CPython\ncore devs, he lobbied the PyPy team, they adopted it, the idea gained traction,\nand was finally contributed  to CPython 3.6 by INADA Naoki. For all details,\ncheck out the extensive comments in the CPython code for Objects/dictobject.c\nand Objects/dict-common.h , as well as the design document\nObjects/dictnotes.txt .\nThe rationale for adding sets to Python is documented in PEP 218 — Adding a\nBuilt-In Set Object T ype . When PEP 218 was approved, no special literal\nsyntax was adopted for sets. The set  literals were created for Python 3 and\nbackported to Python 2.7, along with dict  and set  comprehensions. At\nPyCon 2019, I presented Set Practice: learning from Python’ s set types  ( slides ),\ndescribing use cases of sets in real programs, covering their API design, and\nthe implementation of uintset , a set class for integer elements using a bit\nvector instead of a hash table, inspired by an example in chapter 6 of the\nexcellent The Go Pr ogramming Language , by Donovan & Kernighan.\nIEEE’ s Spectrum magazine has a story about Hans Peter Luhn, a prolific\ninventor who patented a punched card deck to select cocktail recipes depending\non ingredients available, among other diverse inventions including… hash\ntables! See Hans Peter Luhn and the Birth of the Hashing Algorithm .\nS O A P B O X\nSyntactic sugar\nMy friend Geraldo Cohen once remarked that Python is “simple and\ncorrect.”\nProgramming language purists like to dismiss syntax as unimportant.\nSyntactic sugar causes cancer of the semicolon.\n— Alan Perlis\nSyntax is the user interface of a programming language, so it does matter in\npractice.\nBefore finding Python, I did some W eb programming using Perl and PHP .\nThe syntax for mappings in these languages is very useful, and I badly miss\nit whenever I have to use Java or C.\nA good literal syntax for mappings is very convenient for configuration,\ntable-driven implementations, and to hold data for prototyping and testing.\nThat’ s one lesson the designers of Go learned from dynamic languages.\nThe lack of a good way to express structured data in code pushed the Java\ncommunity to adopt the verbose and overly complex XML as a data\nformat.\nJSON was proposed as “The Fat-Free Alternative to XML”  and became a\nhuge success, replacing XML in many contexts. A concise syntax for lists\nand dictionaries makes an excellent data interchange format.\nPHP and Ruby imitated the hash syntax from Perl, using =>  to link keys to\nvalues. JavaScript uses :  like Python. Why use two characters when one is\nreadable enough?\nJSON came from JavaScript, but it also happens to be an almost exact\nsubset of Python syntax. JSON is compatible with Python except for the\nspelling of the values true , false , and null .\nArmin Ronacher tweeted  that he likes to hack Python’ s global namespace\nto add JSON-compatible aliases for Python’ s True , False , and None  so\nhe can paste JSON directly in the console. The basic idea:11\n>>> true, false, null = True, False, None \n>>> fruit = { \n...     ""type"": ""banana"" , \n...     ""avg_weight"" : 123.2, \n...     ""edible_peel"" : false, \n...     ""species"" : [""acuminata"" , ""balbisiana"" , ""paradisiaca"" ], \n...     ""issues"" : null, \n... } \n>>> fruit \n{'type': 'banana', 'avg_weight': 123.2, 'edible_peel': False,  \n'species': ['acuminata', 'balbisiana', 'paradisiaca'], 'issues':  \nNone}\nThe syntax everybody now uses for exchanging data is Python’ s dict  and\nlist  syntax. Now we have the nice syntax with the convenience of\npreserved insertion order .\nSimple and correct.\n1  A virtual subclass is any class registered by calling the .register()  method of an ABC, as\nexplained in “A V irtual Subclass of an ABC” . A type implemented via Python/C API is also\neligible if a specific marker bit is set. See Py_TPFLAGS_MAPPING .\n2  The Python Glossary  entry for “hashable” uses the term “hash value” instead of hash code . I prefer\nhash code  because that is a concept often discussed in the context of mappings, where items are\nmade of keys and values, so it may be confusing to mention the hash code as a value. In this book, I\nonly use hash code .\n3  See PEP 456—Secure and interchangeable hash algorithm  to learn about the security implications\nand solutions adopted.\n4  The original script appears in slide 41 of Martelli’ s “Re-learning Python” presentation . His script\nis actually a demonstration of dict.setdefault , as shown in our Example 3-5 .\n5  This is an example of using a method as a first-class function, the subject of Chapter 7 .\n6  One such library is Pingo.io , no longer under active development.\n7  The exact problem with subclassing dict  and other built-ins is covered in “Subclassing Built-In\nT ypes Is T ricky” .\n8  That’ s how tuples are stored.\n9  Unless the class has a __slots__  attribute, as explained in “Saving Memory with\n__slots__ ” .\n10  This may be interesting, but is not super important. The speed up will happen only when a set\nliteral is evaluated, and that happens at most once per Python process—when a module is initially\ncompiled. If you’re curious, import the dis  function from the dis  module and use it to\ndisassemble the bytecodes for a set  literal—e.g. dis('{1}') —and a set  call—\ndis('set([1])')\n11  It’ s possible that Brendan Eich studied Python before he created JavaScript. I’ve heard a rumor\nthat Netscape reached out to Guido van Rossum to embed Python in their browser , before Eich\nspent 10 days  creating a language almost completely unlike Java, except for the C-like syntax and\nthe same set of reserved words. In the tale I heard, Guido told Netscape that Python was not\nsuitable. Maybe it’ s just an urban legend.",7754
51-Character Issues.pdf,51-Character Issues,"Chapter 4. T ext V ersus Bytes\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 4th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nHumans use text. Computers speak bytes.\n— Esther Nam and T ravis Fischer , Character Encoding and\nUnicode in Python\nPython 3 introduced a sharp distinction between strings of human text and\nsequences of raw bytes. Implicit conversion of byte sequences to Unicode\ntext is a thing of the past. This chapter deals with Unicode strings, binary\nsequences, and the encodings used to convert between them.\nDepending on the kind of work you do with Python, you may think that\nunderstanding Unicode is not important. That’ s unlikely , but anyway there\nis no escaping the str  versus byte  divide. As a bonus, you’ll find that the\nspecialized binary sequence types provide features that the “all-purpose”\nPython 2 str  type did not have.\nIn this chapter , we will visit the following topics:\nCharacters, code points, and byte representations1\nUnique features of binary sequences: bytes , bytearray , and\nmemoryview\nEncodings for full Unicode and legacy character sets\nA voiding and dealing with encoding errors\nBest practices when handling text files\nThe default encoding trap and standard I/O issues\nSafe Unicode text comparisons with normalization\nUtility functions for normalization, case folding, and brute-force\ndiacritic removal\nProper sorting of Unicode text with locale  and the PyUCA\nlibrary\nCharacter metadata in the Unicode database\nDual-mode APIs that handle str  and bytes\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nSupport for Unicode in Python 3 has been comprehensive and stable, so the\nmost notable addition is is “Finding characters by name” , describing a\nutility for searching the Unicode database—a great way to find circled\ndigits and smiling cats from the command-line.\nOne minor change worth mentioning is the Unicode support on W indows,\nwhich is better and simpler since Python 3.6, as we’ll see in “Beware of\nEncoding Defaults” .\nLet’ s start with the not-so-new , but fundamental concepts of characters,\ncode points, and bytes.\nN O T E\nFor the Second Edition , I expanded the section about the struct  module and\npublished it online at Parsing binary r ecor ds with struct , in the fluentpython.com\ncompanion W ebsite.\nThere you will also find Building Multi-character Emojis , describing how to make\ncountry flags, rainbow flags, people with dif ferent skin tones, and diverse family icons\nby combining Unicode characters.\nC h a r a c t e r  I s s u e s\nThe concept of “string” is simple enough: a string is a sequence of\ncharacters. The problem lies in the definition of “character .”\nIn 2021, the best definition of “character” we have is a Unicode character .\nAccordingly , the items we get out of a Python 3 str  are Unicode\ncharacters, just like the items of a unicode  object in Python 2—and not\nthe raw bytes we got from a Python 2 str .\nThe Unicode standard explicitly separates the identity of characters from\nspecific byte representations:\nThe identity of a character—its code point —is a number from 0 to\n1,1 14,1 1 1 (base 10), shown in the Unicode standard as 4 to 6 hex\ndigits with a “U+” prefix, from U+0000 to U+10FFFF . For\nexample, the code point for the letter A is U+0041, the Euro sign is\nU+20AC, and the musical symbol G clef is assigned to code point\nU+1D1 1E. About 13% of the valid code points have characters\nassigned to them in Unicode 13.0.0, the standard used in Python\n3.10.0b4.\nThe actual bytes that represent a character depend on the encoding\nin use. An encoding is an algorithm that converts code points to\nbyte sequences and vice versa. The code point for the letter A\n(U+0041) is encoded as the single byte \x41  in the UTF-8\nencoding, or as the bytes \x41\x00  in UTF-16LE encoding. As\nanother example, UTF-8 requires three bytes— \xe2\x82\xac\n—to encode the Euro sign (U+20AC) but in UTF-16LE the same\ncode point is encoded as two bytes: \xac\x20 .\nConverting from code points to bytes is encoding ; converting from bytes to\ncode points is decoding . See Example 4-1 .\nExample 4-1. Encoding and decoding\n>>> s = 'café' \n>>> len(s)  \n \n4 \n>>> b = s.encode('utf8')  \n \n>>> b \nb'caf\xc3\xa9'  \n \n>>> len(b)  \n \n5 \n>>> b.decode('utf8')  \n \n'café'\nThe str  'café'  has four Unicode characters.\nEncode str  to bytes  using UTF-8 encoding.\nbytes  literals have a b  prefix.\nbytes  b  has five bytes (the code point for “é” is encoded as two bytes\nin UTF-8).\nDecode bytes  to str  using UTF-8 encoding.\nT I P\nIf you need a memory aid to help distinguish .decode()  from .encode() ,\nconvince yourself that byte sequences can be cryptic machine core dumps while\nUnicode str  objects are “human” text. Therefore, it makes sense that we decode\nbytes  to str  to get human-readable text, and we encode  str  to bytes  for storage or\ntransmission.",5434
52-Byte Essentials.pdf,52-Byte Essentials,"Although the Python 3 str  is pretty much the Python 2 unicode  type\nwith a new name, the Python 3 bytes  is not simply the old str  renamed,\nand there is also the closely related bytearray  type. So it is worthwhile\nto take a look at the binary sequence types before advancing to\nencoding/decoding issues.\nB y t e  E s s e n t i a l s\nThe new binary sequence types are unlike the Python 2 str  in many\nregards. The first thing to know is that there are two basic built-in types for\nbinary sequences: the immutable bytes  type introduced in Python 3 and\nthe mutable bytearray , added way back in Python 2.6.  The Python\ndocumentation sometimes uses the generic term “byte string” to refer to\nboth bytes  and bytearray . I avoid that confusing term.\nEach item in bytes  or bytearray  is an integer from 0 to 255, and not a\none-character string like in the Python 2 str . However , a slice of a binary\nsequence always produces a binary sequence of the same type—including\nslices of length 1. See Example 4-2 .\nExample 4-2. A five-byte sequence as bytes and as bytearray\n>>> cafe = bytes('café', encoding ='utf_8')  \n \n>>> cafe \nb'caf\xc3\xa9'  \n>>> cafe[0]  \n \n99 \n>>> cafe[:1]  \n \nb'c' \n>>> cafe_arr  = bytearray (cafe) \n>>> cafe_arr   \n \nbytearray(b'caf\xc3\xa9')  \n>>> cafe_arr [-1:]  \n \nbytearray(b'\xa9')\nbytes  can be built from a str , given an encoding.\nEach item is an integer in range(256) .2\nSlices of bytes  are also bytes —even slices of a single byte.\nThere is no literal syntax for bytearray : they are shown as\nbytearray()  with a bytes  literal as ar gument.\nA slice of bytearray  is also a bytearray .\nW A R N I N G\nThe fact that my_bytes[0]  retrieves an int  but my_bytes[:1]  returns a bytes\nsequence of length 1 is only surprising because we are used to Python’ s str  type,\nwhere s[0] == s[:1] . For all other sequence types in Python, 1 item is not the\nsame as a slice of length 1.\nAlthough binary sequences are really sequences of integers, their literal\nnotation reflects the fact that ASCII text is often embedded in them.\nTherefore, four dif ferent displays are used, depending on each byte value:\n1 . For bytes with decimal codes 32 to 126—from space to ~  (tilde)—\nthe ASCII character itself is used.\n2 . For bytes corresponding to tab, newline, carriage return, and \ , the\nescape sequences \t , \n , \r , and \\  are used.\n3 . If both string delimiters '  and ""  appear in the byte sequence, the\nwhole sequence is delimited by '  and any '  inside are escaped as\n\' .\n4 . For other byte values, a hexadecimal escape sequence is used (e.g.,\n\x00  is the null byte).\nThat is why in Example 4-2  you see b'caf\xc3\xa9' : the first three\nbytes b'caf'  are in the printable ASCII range, the last two are not.\nBoth bytes  and bytearray  support every str  method except those that\ndo formatting ( format , format_map ) and a those that depend on3\nUnicode data, including casefold , isdecimal , isidentifier ,\nisnumeric , isprintable , and encode . This means that you can use\nfamiliar string methods like endswith , replace , strip , translate ,\nupper , and dozens of others with binary sequences—only using bytes\nand not str  ar guments. In addition, the regular expression functions in the\nre  module also work on binary sequences, if the regex is compiled from a\nbinary sequence instead of a str . Since Python 3.5, the %  operator works\nwith binary sequences again.\nBinary sequences have a class method that str  doesn’ t have, called\nfromhex , which builds a binary sequence by parsing pairs of hex digits\noptionally separated by spaces:\n>>> bytes.fromhex('31 4B CE A9' ) \nb'1K\xce\xa9'\nThe other ways of building bytes  or bytearray  instances are calling\ntheir constructors with:\nA str  and an encoding  keyword ar gument.\nAn iterable providing items with values from 0 to 255.\nAn object that implements the buf fer protocol (e.g., bytes ,\nbytearray , memoryview , array.array ); this copies the\nbytes from the source object to the newly created binary sequence.\nW A R N I N G\nUntil Python 3.5, it was also possible to call bytes  or bytearray  with a single\ninteger to create a binary sequence of that size initialized with null bytes. This signature\nwas deprecated in Python 3.5 and removed in Python 3.6. See PEP 467 — Minor API\nimprovements for binary sequences .)\nBuilding a binary sequence from a buf fer -like object is a low-level\noperation that may involve type casting. See a demonstration in Example 4-4",4532
53-Basic EncodersDecoders.pdf,53-Basic EncodersDecoders,"3 .\nExample 4-3. Initializing bytes fr om the raw data of an array\n>>> import array \n>>> numbers = array.array('h', [-2, -1, 0, 1, 2])  \n \n>>> octets = bytes(numbers)  \n \n>>> octets \nb'\xfe\xff\xff\xff\x00\x00\x01\x00\x02\x00'  \nT ypecode 'h'  creates an array  of short integers (16 bits).\noctets  holds a copy of the bytes that make up numbers .\nThese are the 10 bytes that represent the five short integers.\nCreating a bytes  or bytearray  object from any buf fer -like source will\nalways copy the bytes. In contrast, memoryview  objects let you share\nmemory between binary data structures, as we saw in “Memory V iews” .\nAfter this basic exploration of binary sequence types in Python, let’ s see\nhow they are converted to/from strings.\nB a s i c  E n c o d e r s / D e c o d e r s\nThe Python distribution bundles more than 100 codecs  (encoder/decoder)\nfor text to byte conversion and vice versa. Each codec has a name, like\n'utf_8' , and often aliases, such as 'utf8' , 'utf-8' , and 'U8' ,\nwhich you can use as the encoding  ar gument in functions like open() ,\nstr.encode() , bytes.decode() , and so on. Example 4-4  shows the\nsame text encoded as three dif ferent byte sequences.\nExample 4-4. The string “El Niño” encoded with thr ee codecs pr oducing\nvery differ ent byte sequences\n>>> for codec in ['latin_1' , 'utf_8', 'utf_16' ]: \n...     print(codec, 'El Niño' .encode(codec), sep='\t') \n... \nlatin_1 b'El Ni\xf1o'  \nutf_8   b'El Ni\xc3\xb1o'  \nutf_16  b'\xff\xfeE\x00l\x00 \x00N\x00i\x00\xf1\x00o\x00'\nFigure 4-1  demonstrates a variety of codecs generating bytes from\ncharacters like the letter “A” through the G-clef musical symbol. Note that\nthe last three encodings are variable-length, multibyte encodings.\nFigur e 4-1. T welve characters, their code points, and their byte r epr esentation (in hex) in seven\ndiffer ent encodings (asterisks indicate that the character cannot be r epr esented in that encoding)\nAll those asterisks in Figure 4-1  make clear that some encodings, like\nASCII and even the multibyte GB2312, cannot represent every Unicode\ncharacter . The UTF encodings, however , are designed to handle every\nUnicode code point.\nThe encodings shown in Figure 4-1  were chosen as a representative sample:\nlatin1  a.k.a. iso8859_1\nImportant because it is the basis for other encodings, such as cp1252\nand Unicode itself (note how the latin1  byte values appear in the\ncp1252  bytes and even in the code points).\ncp1252\nA useful latin1  superset created by Microsoft, adding useful symbols\nlike curly quotes and the € (euro); some W indows apps call it “ANSI,”\nbut it was never a real ANSI standard.\ncp437\nThe original character set of the IBM PC, with box drawing characters.\nIncompatible with latin1 , which appeared later .\ngb2312\nLegacy standard to encode the simplified Chinese ideographs used in\nmainland China; one of several widely deployed multibyte encodings\nfor Asian languages.\nutf-8\nThe most common 8-bit encoding on the W eb, by far; as of July 2021,\nW3T echs: Usage of Character Encodings for W ebsites  claims that 97%\nof sites use UTF-8, up from 81.4% when I wrote this paragraph in the\nFirst Edition  in September 2014.\nutf-16le\nOne form of the UTF 16-bit encoding scheme; all UTF-16 encodings\nsupport code points beyond U+FFFF through escape sequences called",3370
54-Understanding EncodeDecode Problems.pdf,54-Understanding EncodeDecode Problems,,0
55-Coping with UnicodeEncodeError.pdf,55-Coping with UnicodeEncodeError,"“surrogate pairs.”\nW A R N I N G\nUTF-16 superseded the original 16-bit Unicode 1.0 encoding—UCS-2—way back in\n1996. UCS-2 is still used in many systems despite being deprecated since the last\ncentury because it only supports code points up to U+FFFF . As of 2021, more than 57%\nof the allocated code points are above U+FFFF , including the all-important emojis.\nW ith this overview of common encodings now complete, we move to\nhandling issues in encoding and decoding operations.\nU n d e r s t a n d i n g  E n c o d e / D e c o d e  P r o b l e m s\nAlthough there is a generic UnicodeError  exception, the error reported\nby Python is usually more specific: either a UnicodeEncodeError\n(when converting str  to binary sequences) or a UnicodeDecodeError\n(when reading binary sequences into str ). Loading Python modules may\nalso raise SyntaxError  when the source encoding is unexpected. W e’ll\nshow how to handle all of these errors in the next sections.\nT I P\nThe first thing to note when you get a Unicode error is the exact type of the exception.\nIs it a UnicodeEncodeError , a UnicodeDecodeError , or some other error\n(e.g., SyntaxError ) that mentions an encoding problem? T o solve the problem, you\nhave to understand it first.\nCoping with UnicodeEncodeError\nMost non-UTF codecs handle only a small subset of the Unicode\ncharacters. When converting text to bytes, if a character is not defined in the\ntar get encoding, UnicodeEncodeError  will be raised, unless special\nhandling is provided by passing an errors  ar gument to the encoding\nmethod or function. The behavior of the error handlers is shown in\nExample 4-5 .\nExample 4-5. Encoding to bytes: success and err or handling\n>>> city = 'São Paulo ' \n>>> city.encode('utf_8')  \n \nb'S\xc3\xa3o Paulo'  \n>>> city.encode('utf_16') \nb'\xff\xfeS\x00\xe3\x00o\x00 \x00P\x00a\x00u\x00l\x00o\x00'  \n>>> city.encode('iso8859_1 ')  \n \nb'S\xe3o Paulo'  \n>>> city.encode('cp437')  \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \n  File ""/.../lib/python3.4/encodings/cp437.py"" , line 12, in encode \n    return codecs.charmap_encode (input,errors,encoding_map ) \nUnicodeEncodeError : 'charmap' codec can't encode character '\xe3'  \nin \nposition 1: character maps to <undefined>  \n>>> city.encode('cp437', errors='ignore')  \n \nb'So Paulo'  \n>>> city.encode('cp437', errors='replace')  \n \nb'S?o Paulo'  \n>>> city.encode('cp437', errors='xmlcharrefreplace ')  \n \nb'S&#227;o Paulo'\nThe UTF encodings handle any str .\niso8859_1  also works for the 'São Paulo'  string.\ncp437  can’ t encode the 'ã'  (“a” with tilde). The default error handler\n—'strict' —raises UnicodeEncodeError .\nThe error='ignore'  handler skips characters that cannot be\nencoded; this is usually a very bad idea, leading to silent data loss.\nWhen encoding, error='replace'  substitutes unencodable\ncharacters with '?' ; data is also lost, but users will get a clue that\nsomething is amiss.",2997
56-How to Discover the Encoding of a Byte Sequence.pdf,56-How to Discover the Encoding of a Byte Sequence,"'xmlcharrefreplace'  replaces unencodable characters with an\nXML entity . If you can’ t use UTF , and you can’ t af fort to lose data, this\nis the only option.\nN O T E\nThe codecs  error handling is extensible. Y ou may register extra strings for the\nerrors  ar gument by passing a name and an error handling function to the\ncodecs.register_error  function. See the codecs.register_error\ndocumentation .\nASCII is a common subset to all the encodings that I know about, therefore\nencoding should always work if the text is made exclusively of ASCII\ncharacters. Python 3.7 added a new boolean method str.isascii()  to\ncheck whether your Unicode text is 100% pure ASCII. If it is, you should\nbe able to encode it to bytes in any encoding without raising\nUnicodeEncodeError .\nCoping with UnicodeDecodeError\nNot every byte holds a valid ASCII character , and not every byte sequence\nis valid UTF-8 or UTF-16; therefore, when you assume one of these\nencodings while converting a binary sequence to text, you will get a\nUnicodeDecodeError  if unexpected bytes are found.\nOn the other hand, many legacy 8-bit encodings like 'cp1252' ,\n'iso8859_1' , and 'koi8_r'  are able to decode any stream of bytes,\nincluding random noise, without reporting errors. Therefore, if your\nprogram assumes the wrong 8-bit encoding, it will silently decode garbage.\nT I P\nGarbled characters are known as gremlins or mojibake ( 文字化け —Japanese for\n“transformed text”).\nExample 4-6  illustrates how using the wrong codec may produce gremlins\nor a UnicodeDecodeError .\nExample 4-6. Decoding fr om str to bytes: success and err or handling\n>>> octets = b'Montr\xe9al'  \n \n>>> octets.decode('cp1252')  \n \n'Montréal'  \n>>> octets.decode('iso8859_7 ')  \n \n'Montrιal'  \n>>> octets.decode('koi8_r')  \n \n'MontrИal'  \n>>> octets.decode('utf_8')  \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nUnicodeDecodeError : 'utf-8' codec can't decode byte 0xe9 in  \nposition 5:  \ninvalid continuation byte  \n>>> octets.decode('utf_8', errors='replace')  \n \n'Montr � al'\nThe word “Montréal” encoded as latin1 ; '\xe9'  is the byte for “é”.\nDecoding with W indows 1252 works because it is a superset of\nlatin1 .\nISO-8859-7 is intended for Greek, so the '\xe9'  byte is\nmisinterpreted, and no error is issued.\nKOI8-R is for Russian. Now '\xe9'  stands for the Cyrillic letter “И”.\nThe 'utf_8'  codec detects that octets  is not valid UTF-8, and\nraises UnicodeDecodeError .\nUsing 'replace'  error handling, the \xe9  is replaced by “ � ” (code\npoint U+FFFD), the of ficial Unicode REPLACEMENT CHARACTER\nintended to represent unknown characters.\nSyntaxError When Loading Modules with Unexpected\nEncoding\nUTF-8 is the default source encoding for Python 3, just as ASCII was the\ndefault for Python 2. If you load a .py  module containing non-UTF-8 data\nand no encoding declaration, you get a message like this:\nSyntaxError: Non-UTF-8 code starting with '\xe1' in file ola.py  \non line \n  1, but no encoding declared; see  \nhttps://python.org/dev/peps/pep-0263/  \n  for details\nBecause UTF-8 is widely deployed in GNU/Linux and MacOS systems, a\nlikely scenario is opening a .py  file created on W indows with cp1252 .\nNote that this error happens even in Python for W indows, because the\ndefault encoding for Python 3 source is UTF-8 across all platforms.\nT o fix this problem, add a magic coding  comment at the top of the file, as\nshown in Example 4-7 .\nExample 4-7. ola.py : “Hello, W orld!” in Portuguese\n# coding: cp1252  \n \nprint('Olá, Mundo!' )\nT I P\nNow that Python 3 source code is no longer limited to ASCII and defaults to the\nexcellent UTF-8 encoding, the best “fix” for source code in legacy encodings like\n'cp1252'  is to convert them to UTF-8 already , and not bother with the coding\ncomments. If your editor does not support UTF-8, it’ s time to switch.\nSuppose you have a text file, be it source code or poetry , but you don’ t\nknow its encoding. How do you detect the actual encoding? Answers in the\nnext section.\nHow to Discover the Encoding of a Byte Sequence\nHow do you find the encoding of a byte sequence? Short answer: you can’ t.\nY ou must be told.\nSome communication protocols and file formats, like HTTP and XML,\ncontain headers that explicitly tell us how the content is encoded. Y ou can\nbe sure that some byte streams are not ASCII because they contain byte\nvalues over 127, and the way UTF-8 and UTF-16 are built also limits the\npossible byte sequences.\nL E O ’ S  H A C K  F O R  G U E S S I N G  U T F - 8  D E C O D I N G\n(The next paragraphs come from a note left by tech reviewer Leonardo\nRochael in the draft of this book.)\nThe way UTF-8 was designed, it’ s almost impossible for a random\nsequence of bytes, or even a non-random sequence of bytes coming\nfrom a non-UTF-8 encoding, to be decoded accidentally as garbage in\nUTF-8, instead of raising UnicodeDecodeError .\nThe reasons for this are that UTF-8 escape sequences never use ASCII\ncharacters, and these escape sequences have bit patterns that make it\nvery hard for random data to be valid UTF-8 by accident.\nSo if you can decode some bytes containing codes > 127 as UTF-8, it’ s\nprobably UTF-8.\nIn dealing with Brazillian online services, some of which were attached\nto legacy backends, I’ve had, on occasion, to implement a decoding\nstrategy of trying to decode via UTF-8 and treat a\nUnicodeDecodeError  by decoding via cp1252 . It was ugly but\nef fective.\nHowever , considering that human languages also have their rules and\nrestrictions, once you assume that a stream of bytes is human plain text  it\nmay be possible to snif f out its encoding using heuristics and statistics. For\nexample, if b'\x00'  bytes are common, it is probably a 16- or 32-bit\nencoding, and not an 8-bit scheme, because null characters in plain text are\nbugs. When the byte sequence b'\x20\x00'  appears often, it is more",5987
57-BOM A Useful Gremlin.pdf,57-BOM A Useful Gremlin,"likely to be the space character (U+0020) in a UTF-16LE encoding, rather\nthan the obscure U+2000 EN QUAD  character—whatever that is.\nThat is how the package Chardet — The Universal Character Encoding\nDetector  works to guess one of more than 30 supported encodings. Char det\nis a Python library that you can use in your programs, but also includes a\ncommand-line utility , chardetect . Here is what it reports on the source\nfile for this chapter:\n$ chardetect 04-text-byte.asciidoc  \n04-text-byte.asciidoc: utf-8 with confidence 0.99\nAlthough binary sequences of encoded text usually don’ t carry explicit hints\nof their encoding, the UTF formats may prepend a byte order mark to the\ntextual content. That is explained next.\nBOM: A Useful Gremlin\nIn Example 4-4 , you may have noticed a couple of extra bytes at the\nbeginning of a UTF-16 encoded sequence. Here they are again:\n>>> u16 = 'El Niño' .encode('utf_16' ) \n>>> u16 \nb'\xff\xfeE\x00l\x00 \x00N\x00i\x00\xf1\x00o\x00'\nThe bytes are b'\xff\xfe' . That is a BOM —byte-order mark—\ndenoting the “little-endian” byte ordering of the Intel CPU where the\nencoding was performed.\nOn a little-endian machine, for each code point the least significant byte\ncomes first: the letter 'E' , code point U+0045 (decimal 69), is encoded in\nbyte of fsets 2 and 3 as 69 and 0:\n>>> list(u16) \n[255, 254, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]\nOn a big-endian CPU, the encoding would be reversed; 'E'  would be\nencoded as 0 and 69.\nT o avoid confusion, the UTF-16 encoding prepends the text to be encoded\nwith the special invisible character ZERO WIDTH NO-BREAK SPACE\n(U+FEFF). On a little-endian system, that is encoded as b'\xff\xfe'\n(decimal 255, 254). Because, by design, there is no U+FFFE character in\nUnicode, the byte sequence b'\xff\xfe'  must mean the ZERO WIDTH\nNO-BREAK SPACE  on a little-endian encoding, so the codec knows which\nbyte ordering to use.\nThere is a variant of UTF-16—UTF-16LE—that is explicitly little-endian,\nand another one explicitly big-endian, UTF-16BE. If you use them, a BOM\nis not generated:\n>>> u16le = 'El Niño' .encode('utf_16le' ) \n>>> list(u16le) \n[69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]  \n>>> u16be = 'El Niño' .encode('utf_16be' ) \n>>> list(u16be) \n[0, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111]\nIf present, the BOM is supposed to be filtered by the UTF-16 codec, so that\nyou only get the actual text contents of the file without the leading ZERO\nWIDTH NO-BREAK SPACE . The Unicode standard says that if a file is\nUTF-16 and has no BOM, it should be assumed to be UTF-16BE (big-\nendian). However , the Intel x86 architecture is little-endian, so there is\nplenty of little-endian UTF-16 with no BOM in the wild.\nThis whole issue of endianness only af fects encodings that use words of\nmore than one byte, like UTF-16 and UTF-32. One big advantage of UTF-8\nis that it produces the same byte sequence regardless of machine\nendianness, so no BOM is needed. Nevertheless, some W indows\napplications (notably Notepad) add the BOM to UTF-8 files anyway—and\nExcel depends on the BOM to detect a UTF-8 file, otherwise it assumes the\ncontent is encoded with a W indows code page. This UTF-8 encoding with\nBOM is called UTF-8-SIG in Python’ s codec registry . The character",3334
58-Handling Text Files.pdf,58-Handling Text Files,"U+FEFF encoded in UTF-8-SIG is the three-byte sequence\nb'\xef\xbb\xbf' . So if a file starts with those three bytes, it is likely\nto be a UTF-8 file with a BOM.\nC A L E B ’ S  T I P  A B O U T  U T F - 8 - S I G\nCaleb Hattingh—one of the tech reviewers—suggests always using the UTF-8-SIG\ncodec when reading UTF-8 files. This is harmless because UTF-8-SIG reads files with\nor without a BOM correctly , and does not return the BOM itself. When writing, I\nrecommend using UTF-8 for general interoperability . For example, Python scripts can\nbe made executable in Unix systems if they start with the comment:\n#!/usr/bin/env python3 . The first two bytes of the file must be b'#!'  for that\nto work, but the BOM breaks that convention. If you have a specific requirement to\nexport data to apps that need the BOM, use UTF-8-SIG but be aware that Python’ s\ncodecs documentation  says: “In UTF-8, the use of the BOM is discouraged and should\ngenerally be avoided.”\nW e now move on to handling text files in Python 3.\nH a n d l i n g  T e x t  F i l e s\nThe best practice for handling text I/O is the “Unicode sandwich”\n( Figure 4-2 ).  This means that bytes  should be decoded to str  as early as\npossible on input (e.g., when opening a file for reading). The “filling” of the\nsandwich is the business logic of your program, where text handling is done\nexclusively on str  objects. Y ou should never be encoding or decoding in\nthe middle of other processing. On output, the str  are encoded to bytes\nas late as possible. Most web frameworks work like that, and we rarely\ntouch bytes  when using them. In Django, for example, your views should\noutput Unicode str ; Django itself takes care of encoding the response to\nbytes , using UTF-8 by default.5\nFigur e 4-2. Unicode sandwich: curr ent best practice for text pr ocessing\nPython 3 makes it easier to follow the advice of the Unicode sandwich,\nbecause the open()  built-in does the necessary decoding when reading\nand encoding when writing files in text mode, so all you get from\nmy_file.read()  and pass to my_file.write(text)  are str\nobjects.\nTherefore, using text files is apparently simple. But if you rely on default\nencodings you will get bitten.\nConsider the console session in Example 4-8 . Can you spot the bug?\nExample 4-8. A platform encoding issue (if you try this on your machine,\nyou may or may not see the pr oblem)\n>>> open('cafe.txt' , 'w', encoding ='utf_8').write('café') \n4 \n>>> open('cafe.txt' ).read() \n'cafÃ©'\nThe bug: I specified UTF-8 encoding when writing the file but failed to do\nso when reading it, so Python assumed W indows default file encoding—\ncode page 1252—and the trailing bytes in the file were decoded as\ncharacters 'Ã©'  instead of 'é' .\nI ran Example 4-8  on Python 3.8.1, 64 bits, on W indows 10 (build 18363).\nThe same statements running on recent GNU/Linux or MacOS work\nperfectly well because their default encoding is UTF-8, giving the false\nimpression that everything is fine. If the encoding ar gument was omitted\nwhen opening the file to write, the locale default encoding would be used,\nand we’d read the file correctly using the same encoding. But then this\nscript would generate files with dif ferent byte contents depending on the\nplatform or even depending on locale settings in the same platform, creating\ncompatibility problems.\nT I P\nCode that has to run on multiple machines or on multiple occasions should never depend\non encoding defaults. Always pass an explicit encoding=  ar gument when opening\ntext files, because the default may change from one machine to the next, or from one\nday to the next.\nA curious detail in Example 4-8  is that the write  function in the first\nstatement reports that four characters were written, but in the next line five\ncharacters are read. Example 4-9  is an extended version of Example 4-8 ,\nexplaining that and other details.\nExample 4-9. Closer inspection of Example 4-8  running on W indows\nr eveals the bug and how to fix it\n>>> fp = open('cafe.txt ', 'w', encoding ='utf_8') \n>>> fp  \n \n<_io.TextIOWrapper name='cafe.txt' mode='w' encoding='utf_8'>  \n>>> fp.write('café')  \n \n4 \n>>> fp.close() \n>>> import os \n>>> os.stat('cafe.txt ').st_size  \n \n5 \n>>> fp2 = open('cafe.txt ') \n>>> fp2  \n \n<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='cp1252'>  \n>>> fp2.encoding   \n \n'cp1252'  \n>>> fp2.read() \n \n'cafÃ©' \n>>> fp3 = open('cafe.txt ', encoding ='utf_8')  \n \n>>> fp3 \n<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='utf_8'>  \n>>> fp3.read() \n \n'café' \n>>> fp4 = open('cafe.txt ', 'rb')  \n \n>>> fp4                           \n  \n<_io.BufferedReader name='cafe.txt'>  \n>>> fp4.read()  \n \nb'caf\xc3\xa9'\nBy default, open  uses text mode and returns a TextIOWrapper\nobject with a specific encoding.\nThe write  method on a TextIOWrapper  returns the number of\nUnicode characters written.\nos.stat  says the file has 5 bytes; UTF-8 encodes 'é'  as 2 bytes,\n0xc3 and 0xa9.\nOpening a text file with no explicit encoding returns a\nTextIOWrapper  with the encoding set to a default from the locale.\nA TextIOWrapper  object has an encoding attribute that you can\ninspect: cp1252  in this case.",5252
59-Beware of Encoding Defaults.pdf,59-Beware of Encoding Defaults,"In the W indows cp1252  encoding, the byte 0xc3 is an “Ã” (A with\ntilde) and 0xa9 is the copyright sign.\nOpening the same file with the correct encoding.\nThe expected result: the same four Unicode characters for 'café' .\nThe 'rb'  flag opens a file for reading in binary mode.\nThe returned object is a BufferedReader  and not a\nTextIOWrapper .\nReading that returns bytes, as expected.\nT I P\nDo not open text files in binary mode unless you need to analyze the file contents to\ndetermine the encoding—even then, you should be using Chardet instead of reinventing\nthe wheel (see “How to Discover the Encoding of a Byte Sequence” ). Ordinary code\nshould only use binary mode to open binary files, like raster images.\nThe problem in Example 4-9  has to do with relying on a default setting\nwhile opening a text file. There are several sources for such defaults, as the\nnext section shows.\nBeware of Encoding Defaults\nSeveral settings af fect the encoding defaults for I/O in Python. See the\ndefault_encodings.py  script in Example 4-10 .\nExample 4-10. Exploring encoding defaults\nimport locale \nimport sys \n \nexpressions  = """""" \n        locale.getpreferredencoding()  \n        type(my_file)  \n        my_file.encoding  \n        sys.stdout.isatty()  \n        sys.stdout.encoding  \n        sys.stdin.isatty()  \n        sys.stdin.encoding  \n        sys.stderr.isatty()  \n        sys.stderr.encoding  \n        sys.getdefaultencoding()  \n        sys.getfilesystemencoding()  \n    """""" \n \nmy_file = open('dummy', 'w') \n \nfor expression  in expressions .split(): \n    value = eval(expression ) \n    print(f'{expression:>30}  -> {value!r} ')\nThe output of Example 4-10  on GNU/Linux (Ubuntu 14.04 to 19.10) and\nMacOS (10.9 to 10.14) is identical, showing that UTF-8  is used\neverywhere in these systems:\n$ python3 default_encodings.py  \n locale.getpreferredencoding () -> 'UTF-8' \n                 type(my_file) -> <class '_io.TextIOWrapper' > \n              my_file.encoding -> 'UTF-8' \n           sys.stdout.isatty () -> True  \n           sys.stdout.encoding -> 'utf-8' \n            sys.stdin.isatty () -> True  \n            sys.stdin.encoding -> 'utf-8' \n           sys.stderr.isatty () -> True  \n           sys.stderr.encoding -> 'utf-8' \n      sys.getdefaultencoding () -> 'utf-8' \n   sys.getfilesystemencoding () -> 'utf-8'\nOn W indows, however , the output is Example 4-1 1 .\nExample 4-1 1. Default encodings on W indows 10 PowerShell (output is the\nsame on cmd.exe)\n> chcp  \n \nActive code page: 437 \n> python default_encodings.py   \n \n locale.getpreferredencoding () -> 'cp1252'   \n \n                 type(my_file) -> <class '_io.TextIOWrapper' > \n              my_file.encoding  -> 'cp1252'   \n \n           sys.stdout.isatty () -> True      \n \n           sys.stdout.encoding  -> 'utf-8'   \n \n            sys.stdin.isatty () -> True \n            sys.stdin.encoding  -> 'utf-8' \n           sys.stderr.isatty () -> True \n           sys.stderr.encoding  -> 'utf-8' \n      sys.getdefaultencoding () -> 'utf-8' \n   sys.getfilesystemencoding () -> 'utf-8'\nchcp  shows the active code page for the console: 437.\nRunning default_encodings.py  with output to console.\nlocale.getpreferredencoding()  is the most important\nsetting.\nT ext files use locale.getpreferredencoding()  by default.\nThe output is going to the console, so sys.stdout.isatty()  is\nTrue .\nNow , sys.stdout.encoding  is not the same as the console code\npage reported by chcp !\nUnicode support in W indows itself, and in Python for W indows, got better\nsince I wrote the First Edition . Example 4-1 1  used to report four dif ferent\nencodings in Python 3.4 on W indows 7. The encodings for stdout ,\nstdin , and stderr  used to be the same as the active code page reported\nby the chcp  command, but now they’re all utf-8  thanks to PEP 528:\nChange W indows console encoding to UTF-8  implemented in Python 3.6,\nand Unicode support in PowerShell in cmd.exe (since W indows 1809 from\nOctober 2018).  It’ s weird that chcp  and sys.stdout.encoding  say\ndif ferent things when stdout  is writing to the console, but it’ s great that\nnow we can print Unicode strings without encoding errors on W indows—\nunless the user redirects output to a file, as we’ll soon see. That does not\nmean all your favorite emojis will appear in the console: that also depends\non the font the console is using.6\nAnother change was PEP 529: Change W indows filesystem encoding to\nUTF-8 , also implemented in Python 3.6, which changed the file system\nencoding (used to represent names of directories and files) from Microsoft’ s\nproprietary MBCS to UTF-8.\nHowever , if the output of Example 4-10  is redirected to a file, like this:\nZ:\>python default_encodings.py > encodings.log\nThen, the value of sys.stdout.isatty()  becomes False , and\nsys.stdout.encoding  is set by\nlocale.getpreferredencoding() , 'cp1252'  in that machine—\nbut sys.stdin.encoding  and sys.stderr.encoding  remain\nutf-8 .\nT I P\nIn Example 4-12  I use the '\N{}'  escape for Unicode literals, where we write the\nof ficial name of the character inside the \N{} . It’ s rather verbose, but explicit and safe:\nPython raises SyntaxError  if the name doesn’ t exist—much better than writing an\nhex number that could be wrong but you’ll only find out much later . Y ou’d probably\nwant to write a comment explaining the character codes anyway , so the verbosity of\n\N{}  is easy to accept.\nThis means that a script like Example 4-12  works when printing to the\nconsole, but may break when output is redirected to a file.\nExample 4-12. stdout_check.py\nimport sys \nfrom unicodedata  import name \n \nprint(sys.version) \nprint() \nprint('sys.stdout.isatty():' , sys.stdout.isatty()) \nprint('sys.stdout.encoding:' , sys.stdout.encoding ) \nprint() \n \ntest_chars  = [ \n    '\N{HORIZONTAL ELLIPSIS} ',       # exists in cp1252, not in  \ncp437 \n    '\N{INFINITY} ',                  # exists in cp437, not in  \ncp1252 \n    '\N{CIRCLED NUMBER FORTY TWO} ',  # not in cp437 or in cp1252  \n] \n \nfor char in test_chars : \n    print(f'Trying to output {name(char)}:' ) \n    print(char)\nExample 4-12  displays the result of sys.stdout.isatty() , the value\nof sys.stdout.encoding , and these three characters:\n'…'  HORIZONT AL ELLIPSIS—exists in CP 1252 but not in CP\n437;\n'∞'  INFINITY—exists in CP 437 but not in CP 1252;\n'  '  CIRCLED NUMBER FOR TY TWO —doesn’ t exist in CP\n1252 or CP 437.\nWhen I run stdout_check.py  on PowerShell or cmd.exe, it works as\ncaptured in Figure 4-3 .7\nFigur e 4-3. Running stdout_check.py  on PowerShell.\nDespite chcp  reporting the active code as 437,\nsys.stdout.encoding  is UTF-8, so the HORIZONT AL ELLIPSIS\nand INFINITY both output correctly . The CIRCLED NUMBER FOR TY\nTWO is replaced by a rectangle, but no error is raised. Presumably it is\nrecognized as a valid character , but the console font doesn’ t have the glyph\nto display it.\nHowever , when I redirect the output of stdout_check.py  to a file, I get\nFigure 4-4 .\nFigur e 4-4. Running stdout_check.py  on PowerShell, r edir ecting output.\nThe first problem demonstrated by Figure 4-4  is the\nUnicodeEncodeError  mentioning character '\u221e' , because\nsys.stdout.encoding  is 'cp1252' —a code page that doesn’ t have\nthe INFINITY character .\nReading out.txt  with the type  command—or a W indows editor like\nVS Code or Sublime T ext—shows that instead of HORIZONT AL\nELLIPSIS, I got 'à'  (LA TIN SMALL LETTER A WITH GRA VE). As it\nturns out, the byte value 0x85 in CP 1252 means '…' , but in CP 437 the\nsame byte value represents 'à' . So it seems the active code page does\nmatter , not in a sensible or useful way , but as partial explanation of a bad\nUnicode experience.\nN O T E\nI used a laptop configured for the US market, running W indows 10 OEM to run these\nexperiments. W indows versions localized for other countries may have dif ferent\nencoding configurations. For example, in Brazil the W indows console uses code page\n850 by default—not 437.\nT o wrap up this maddening issue of default encodings, let’ s give a final look\nat the dif ferent encodings in Example 4-1 1 :\nIf you omit the encoding  ar gument when opening a file, the\ndefault is given by locale.getpreferredencoding()\n('cp1252'  in Example 4-1 1 ).\nThe encoding of sys.stdout|stdin|stderr  used to be set\nby the PYTHONIOENCODING  environment variable before Python\n3.6—now that variable is ignored, unless\nPYTHONLEGACYWINDOWSSTDIO  is set to a non-empty string.\nOtherwise, the encoding for standard I/O is UTF-8 for interactive\nI/O, or defined by locale.getpreferredencoding()  if\nthe output/input is redirected to/from a file.\nsys.getdefaultencoding()  is used internally by Python in\nimplicit conversions of binary data to/from str ; this happens less\noften in Python 3, but still happens.  Changing this setting is not\nsupported.\nsys.getfilesystemencoding()  is used to encode/decode\nfilenames (not file contents). It is used when open()  gets a str\nar gument for the filename; if the filename is given as a bytes\nar gument, it is passed unchanged to the OS API.8\n9",9211
60-Normalizing Unicode for Reliable Comparisons.pdf,60-Normalizing Unicode for Reliable Comparisons,"N O T E\nOn GNU/Linux and MacOS all of these encodings are set to UTF-8 by default, and have\nbeen for several years, so I/O handles all Unicode characters. On W indows, not only are\ndif ferent encodings used in the same system, but they are usually code pages like\n'cp850'  or 'cp1252'  that support only ASCII with 127 additional characters that\nare not the same from one encoding to the other . Therefore, W indows users are far more\nlikely to face encoding errors unless they are extra careful.\nT o summarize, the most important encoding setting is that returned by\nlocale.getpreferredencoding() : it is the default for opening\ntext files and for sys.stdout/stdin/stderr  when they are\nredirected to files. However , the documentation  reads (in part):\nlocale.getpreferredencoding(do_setlocale=True)\nReturn the encoding used for text data, accor ding to user pr efer ences.\nUser pr efer ences ar e expr essed differ ently on differ ent systems, and\nmight not be available pr ogrammatically on some systems, so this\nfunction only r eturns a guess. […]\nTherefore, the best advice about encoding defaults is: do not rely on them.\nY ou will avoid a lot of pain if you follow the advice of the Unicode\nsandwich and always are explicit about the encodings in your programs.\nUnfortunately , Unicode is painful even if you get your bytes  correctly\nconverted to str . The next two sections cover subjects that are simple in\nASCII-land, but get quite complex on planet Unicode: text normalization\n(i.e., converting text to a uniform representation for comparisons) and\nsorting.\nN o r m a l i z i n g  U n i c o d e  f o r  R e l i a b l e\nC o m p a r i s o n s\nString comparisons are complicated by the fact that Unicode has combining\ncharacters: diacritics and other marks that attach to the preceding character ,\nappearing as one when printed.\nFor example, the word “café” may be composed in two ways, using four or\nfive code points, but the result looks exactly the same:\n>>> s1 = 'café' \n>>> s2 = 'cafe\N{COMBINING ACUTE ACCENT} ' \n>>> s1, s2 \n('café', 'café')  \n>>> len(s1), len(s2) \n(4, 5) \n>>> s1 == s2 \nFalse\nPlacing COMBINING ACUTE ACCENT  (U+0301) after “e” renders “é”. In\nthe Unicode standard, sequences like 'é'  and 'e\u0301'  are called\n“canonical equivalents,” and applications are supposed to treat them as the\nsame. But Python sees two dif ferent sequences of code points, and\nconsiders them not equal.\nThe solution is unicodedata.normalize() . The first ar gument to\nthat function is one of four strings: 'NFC' , 'NFD' , 'NFKC' , and\n'NFKD' . Let’ s start with the first two.\nNormalization Form C (NFC) composes the code points to produce the\nshortest equivalent string, while NFD decomposes, expanding composed\ncharacters into base characters and separate combining characters. Both of\nthese normalizations make comparisons work as expected, as the next\nexample shows.\n>>> from unicodedata  import normalize  \n>>> s1 = 'café' \n>>> s2 = 'cafe\N{COMBINING ACUTE ACCENT} ' \n>>> len(s1), len(s2) \n(4, 5) \n>>> len(normalize ('NFC', s1)), len(normalize ('NFC', s2)) \n(4, 4) \n>>> len(normalize ('NFD', s1)), len(normalize ('NFD', s2)) \n(5, 5) \n>>> normalize ('NFC', s1) == normalize ('NFC', s2) \nTrue \n>>> normalize ('NFD', s1) == normalize ('NFD', s2) \nTrue\nKeyboard drivers usually generate composed characters, so text typed by\nusers will be in NFC by default. However , to be safe, it may be good to\nnormalize strings with normalize('NFC', user_text)  before\nsaving. NFC is also the normalization form recommended by the W3C in\nCharacter Model for the W orld W ide W eb: String Matching and Sear ching .\nSome single characters are normalized by NFC into another single\ncharacter . The symbol for the ohm (Ω) unit of electrical resistance is\nnormalized to the Greek uppercase omega. They are visually identical, but\nthey compare unequal so it is essential to normalize to avoid surprises:\n>>> from unicodedata  import normalize , name \n>>> ohm = '\u2126' \n>>> name(ohm) \n'OHM SIGN'  \n>>> ohm_c = normalize ('NFC', ohm) \n>>> name(ohm_c) \n'GREEK CAPITAL LETTER OMEGA'  \n>>> ohm == ohm_c \nFalse \n>>> normalize ('NFC', ohm) == normalize ('NFC', ohm_c) \nTrue\nThe other two normalization forms are NFKC and NFKD, where the letter\nK stands for “compatibility .” These are stronger forms of normalization,\naf fecting the so-called “compatibility characters.” Although one goal of\nUnicode is to have a single “canonical” code point for each character , some\ncharacters appear more than once for compatibility with preexisting\nstandards. For example, the MICRO SIGN , µ  (U+00B5 ), was added to\nUnicode to support round-trip conversion to latin1  which includes it,\neven though the same character is part of the Greek alphabet with code\npoint U+03BC  (GREEK SMALL LETTER MU ). So, the micro sign is\nconsidered a “compatibility character .”\nIn the NFKC and NFKD forms, each compatibility character is replaced by\na “compatibility decomposition” of one or more characters that are\nconsidered a “preferred” representation, even if there is some formatting\nloss—ideally , the formatting should be the responsibility of external\nmarkup, not part of Unicode. T o exemplify , the compatibility\ndecomposition of the one half fraction '½'  (U+00BD ) is the sequence of\nthree characters '1/2' , and the compatibility decomposition of the micro\nsign 'µ'  (U+00B5 ) is the lowercase mu 'μ'  (U+03BC ).\nHere is how the NFKC works in practice:\n>>> from unicodedata  import normalize , name \n>>> half = '\N{VULGAR FRACTION ONE HALF} ' \n>>> print(half) \n½ \n>>> normalize ('NFKC', half) \n'1⁄2' \n>>> for char in normalize ('NFKC', half): \n...     print(char, name(char), sep='\t') \n... \n1 DIGIT ONE  \n⁄ FRACTION SLASH  \n2 DIGIT TWO  \n>>> four_squared  = '4²' \n>>> normalize ('NFKC', four_squared ) \n'42' \n>>> micro = 'µ' \n>>> micro_kc  = normalize ('NFKC', micro) \n>>> micro, micro_kc  \n('µ', 'μ')  \n>>> ord(micro), ord(micro_kc ) \n(181, 956)  \n>>> name(micro), name(micro_kc ) \n('MICRO SIGN', 'GREEK SMALL LETTER MU')\nAlthough '1⁄2'  is a reasonable substitute for '½' , and the micro sign is\nreally a lowercase Greek mu, converting '4²'  to '42'  changes the\nmeaning. An application could store '4²'  as '4<sup>2</sup>' , but\nthe normalize  function knows nothing about formatting. Therefore,\nNFKC or NFKD may lose or distort information, but they can produce\nconvenient intermediate representations for searching and indexing.10",6544
61-Extreme Normalization Taking Out Diacritics.pdf,61-Extreme Normalization Taking Out Diacritics,"Unfortunately , with Unicode everything is always more complicated than it\nfirst seems. For the VULGAR FRACTION ONE HALF , the NFKC\nnormalization produced 1 and 2 joined by FRACTION SLASH , instead of\nSOLIDUS , a.k.a. “slash”—the familiar character with ASCII code decimal\n47. Therefore, searching for the 3-character ASCII sequence '1/2'  would\nnot find the normalized Unicode sequence.\nW A R N I N G\nNFKC and NFKD normalization cause data loss and should be applied only in special\ncases like search and indexing, and not for permanent storage of text.\nWhen preparing text for searching or indexing, another operation is useful:\ncase folding, our next subject.\nCase Folding\nCase folding is essentially converting all text to lowercase, with some\nadditional transformations. It is supported by the str.casefold()\nmethod.\nFor any string s  containing only latin1  characters, s.casefold()\nproduces the same result as s.lower() , with only two exceptions—the\nmicro sign 'µ'  is changed to the Greek lowercase mu (which looks the\nsame in most fonts) and the German Eszett or “sharp s” (ß) becomes “ss”:\n>>> micro = 'µ' \n>>> name(micro) \n'MICRO SIGN'  \n>>> micro_cf  = micro.casefold () \n>>> name(micro_cf ) \n'GREEK SMALL LETTER MU'  \n>>> micro, micro_cf  \n('µ', 'μ')  \n>>> eszett = 'ß' \n>>> name(eszett) \n'LATIN SMALL LETTER SHARP S'  \n>>> eszett_cf  = eszett.casefold () \n>>> eszett, eszett_cf  \n('ß', 'ss')\nThere are nearly 300 code points for which str.casefold()  and\nstr.lower()  return dif ferent results.\nAs usual with anything related to Unicode, case folding is a hard issue with\nplenty of linguistic special cases, but the Python core team made an ef fort to\nprovide a solution that hopefully works for most users.\nIn the next couple of sections, we’ll put our normalization knowledge to use\ndeveloping utility functions.\nUtility Functions for Normalized T ext Matching\nAs we’ve seen, NFC and NFD are safe to use and allow sensible\ncomparisons between Unicode strings. NFC is the best normalized form for\nmost applications. str.casefold()  is the way to go for case-insensitive\ncomparisons.\nIf you work with text in many languages, a pair of functions like\nnfc_equal  and fold_equal  in Example 4-13  are useful additions to\nyour toolbox.\nExample 4-13. normeq.py: normalized Unicode string comparison\n"""""" \nUtility functions for normalized Unicode string comparison.  \n \nUsing Normal Form C, case sensitive:  \n \n    >>> s1 = 'café'  \n    >>> s2 = 'cafe\u0301'  \n    >>> s1 == s2  \n    False  \n    >>> nfc_equal(s1, s2)  \n    True  \n    >>> nfc_equal('A', 'a')  \n    False  \n \nUsing Normal Form C with case folding:  \n \n    >>> s3 = 'Straße'  \n    >>> s4 = 'strasse'  \n    >>> s3 == s4  \n    False  \n    >>> nfc_equal(s3, s4)  \n    False  \n    >>> fold_equal(s3, s4)  \n    True  \n    >>> fold_equal(s1, s2)  \n    True  \n    >>> fold_equal('A', 'a')  \n    True  \n \n"""""" \n \nfrom unicodedata  import normalize  \n \ndef nfc_equal (str1, str2): \n    return normalize ('NFC', str1) == normalize ('NFC', str2) \n \ndef fold_equal (str1, str2): \n    return (normalize ('NFC', str1).casefold () == \n            normalize ('NFC', str2).casefold ())\nBeyond Unicode normalization and case folding—which are both part of\nthe Unicode standard—sometimes it makes sense to apply deeper\ntransformations, like changing 'café'  into 'cafe' . W e’ll see when and\nhow in the next section.\nExtreme “Normalization”: T aking Out Diacritics\nThe Google Search secret sauce involves many tricks, but one of them\napparently is ignoring diacritics (e.g., accents, cedillas, etc.), at least in\nsome contexts. Removing diacritics is not a proper form of normalization\nbecause it often changes the meaning of words and may produce false\npositives when searching. But it helps coping with some facts of life: people\nsometimes are lazy or ignorant about the correct use of diacritics, and\nspelling rules change over time, meaning that accents come and go in living\nlanguages.\nOutside of searching, getting rid of diacritics also makes for more readable\nURLs, at least in Latin-based languages. T ake a look at the URL for the\nW ikipedia article about the city of São Paulo:\nhttps://en.wikipedia.org/wiki/S%C3%A3o_Paulo\nThe %C3%A3  part is the URL-escaped, UTF-8 rendering of the single letter\n“ã” (“a” with tilde). The following is much easier to recognize, even if it is\nnot the right spelling:\nhttps://en.wikipedia.org/wiki/Sao_Paulo\nT o remove all diacritics from a str , you can use a function like Example 4-\n14 .\nExample 4-14. simplify .py: Function to r emove all combining marks.\nimport unicodedata  \nimport string \n \n \ndef shave_marks (txt): \n    """"""Remove all diacritic marks""""""  \n    norm_txt  = unicodedata .normalize ('NFD', txt)  \n \n    shaved = ''.join(c for c in norm_txt  \n                     if not unicodedata .combining (c))  \n \n    return unicodedata .normalize ('NFC', shaved)  \nDecompose all characters into base characters and combining marks.\nFilter out all combining marks.\nRecompose all characters.\nExample 4-15  shows a couple of uses of shave_marks .\nExample 4-15. T wo examples using shave_marks fr om Example 4-14\n>>> order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of  \naçaí.”' \n>>> shave_marks (order) \n'“Herr Voß: • ½ cup of Œtker™ caffe latte • bowl of acai.”'  \n  \n>>> Greek = 'Ζέφυρος, Zéfiro ' \n>>> shave_marks (Greek) \n'Ζεφυρος, Zefiro'  \n\nOnly the letters “è”, “ç”, and “í” were replaced.\nBoth “έ” and “é” were replaced.\nThe function shave_marks  from Example 4-14  works all right, but\nmaybe it goes too far . Often the reason to remove diacritics is to change\nLatin text to pure ASCII, but shave_marks  also changes non-Latin\ncharacters—like Greek letters—which will never become ASCII just by\nlosing their accents. So it makes sense to analyze each base character and to\nremove attached marks only if the base character is a letter from the Latin\nalphabet. This is what Example 4-16  does.\nExample 4-16. Function to r emove combining marks fr om Latin characters\n(import statements ar e omitted as this is part of the simplify .py module fr om\nExample 4-14 )\ndef shave_marks_latin (txt): \n    """"""Remove all diacritic marks from Latin base characters""""""  \n    norm_txt  = unicodedata .normalize ('NFD', txt)  \n \n    latin_base  = False \n    preserve  = [] \n    for c in norm_txt : \n        if unicodedata .combining (c) and latin_base :   \n \n            continue   # ignore diacritic on Latin base char  \n        preserve .append(c)                            \n  \n        # if it isn't a combining char, it's a new base char  \n        if not unicodedata .combining (c):              \n  \n            latin_base  = c in string.ascii_letters  \n    shaved = ''.join(preserve ) \n    return unicodedata .normalize ('NFC', shaved)   \nDecompose all characters into base characters and combining marks.\nSkip over combining marks when base character is Latin.\nOtherwise, keep current character .\nDetect new base character and determine if it’ s Latin.\nRecompose all characters.\nAn even more radical step would be to replace common symbols in W estern\ntexts (e.g., curly quotes, em dashes, bullets, etc.) into ASCII  equivalents.\nThis is what the function asciize  does in Example 4-17 .\nExample 4-17. T ransform some W estern typographical symbols into ASCII\n(this snippet is also part of simplify .py fr om Example 4-14 )\nsingle_map  = str.maketrans (""""""‚ƒ„ˆ‹‘’“”•–—˜› """""",  \n \n                           """"""'f""^<''""""---~>"""""" ) \n \nmulti_map  = str.maketrans ({  \n \n    '€': 'EUR', \n    '…': '...', \n    'Æ': 'AE', \n    'æ': 'ae', \n    'Œ': 'OE', \n    'œ': 'oe', \n    '™': '(TM)', \n    '‰': '<per mille> ', \n    '†': '**', \n    '‡': '***', \n}) \n \nmulti_map .update(single_map )  \n \n \n \ndef dewinize (txt): \n    """"""Replace Win1252 symbols with ASCII chars or sequences""""""  \n    return txt.translate (multi_map )  \n \n \n \ndef asciize(txt): \n    no_marks  = shave_marks_latin (dewinize (txt))     \n \n    no_marks  = no_marks .replace('ß', 'ss')          \n  \n    return unicodedata .normalize ('NFKC', no_marks )  \nBuild mapping table for char -to-char replacement.\nBuild mapping table for char -to-string replacement.\nMer ge mapping tables.\ndewinize  does not af fect ASCII  or latin1  text, only the Microsoft\nadditions in to latin1  in cp1252 .\nApply dewinize  and remove diacritical marks.\nReplace the Eszett with “ss” (we are not using case fold here because\nwe want to preserve the case).\nApply NFKC normalization to compose characters with their\ncompatibility code points.\nExample 4-18  shows asciize  in use.\nExample 4-18. T wo examples using asciize fr om Example 4-17\n>>> order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of  \naçaí.”' \n>>> dewinize (order) \n'""Herr Voß: - ½ cup of OEtker(TM) caffè latte - bowl of açaí.""'  \n  \n>>> asciize(order) \n'""Herr Voss: - 1⁄2 cup of OEtker(TM) caffe latte - bowl of acai.""'   \ndewinize  replaces curly quotes, bullets, and ™ (trademark symbol).\nasciize  applies dewinize , drops diacritics, and replaces the 'ß' .\nW A R N I N G\nDif ferent languages have their own rules for removing diacritics. For example, Germans\nchange the 'ü'  into 'ue' . Our asciize  function is not as refined, so it may or not be\nsuitable for your language. It works acceptably for Portuguese, though.\nT o summarize, the functions in simplify .py  go way beyond standard\nnormalization and perform deep sur gery on the text, with a good chance of\nchanging its meaning. Only you can decide whether to go so far , knowing\nthe tar get language, your users, and how the transformed text will be used.\nThis wraps up our discussion of normalizing Unicode text.\nNow let’ s sort out Unicode sorting.",9869
62-Sorting Unicode Text.pdf,62-Sorting Unicode Text,"S o r t i n g  U n i c o d e  T e x t\nPython sorts sequences of any type by comparing the items in each\nsequence one by one. For strings, this means comparing the code points.\nUnfortunately , this produces unacceptable results for anyone who uses non-\nASCII characters.\nConsider sorting a list of fruits grown in Brazil:\n>>> fruits = ['caju', 'atemoia' , 'cajá', 'açaí', 'acerola' ] \n>>> sorted(fruits) \n['acerola', 'atemoia', 'açaí', 'caju', 'cajá']\nSorting rules vary for dif ferent locales, but in Portuguese and many\nlanguages that use the Latin alphabet, accents and cedillas rarely make a\ndif ference when sorting.  So “cajá” is sorted as “caja,” and must come\nbefore “caju.”\nThe sorted fruits  list should be:\n['açaí', 'acerola' , 'atemoia' , 'cajá', 'caju']\nThe standard way to sort non-ASCII text in Python is to use the\nlocale.strxfrm  function which, according to the locale  module\ndocs , “transforms a string to one that can be used in locale-aware\ncomparisons.”\nT o enable locale.strxfrm , you must first set a suitable locale for your\napplication, and pray that the OS supports it. The sequence of commands in\nExample 4-19  may work for you.\nExample 4-19. locale_sort.py : using the locale.strxfrm function as\nsort key\nimport locale \nmy_locale  = locale.setlocale (locale.LC_COLLATE , 'pt_BR.UTF-8' ) \nprint(my_locale ) \nfruits = ['caju', 'atemoia' , 'cajá', 'açaí', 'acerola' ] \nsorted_fruits  = sorted(fruits, key=locale.strxfrm) \nprint(sorted_fruits )11\nRunning Example 4-19  on GNU/Linux (Ubuntu 19.10) with the\npt_BR.UTF-8  locale installed, I get the correct result:\n'pt_BR.UTF-8'  \n['açaí', 'acerola' , 'atemoia' , 'cajá', 'caju']\nSo you need to call setlocale(LC_COLLATE, «your_locale»)\nbefore using locale.strxfrm  as the key when sorting.\nThere are some caveats, though:\nBecause locale settings are global, calling setlocale  in a library\nis not recommended. Y our application or framework should set the\nlocale when the process starts, and should not change it afterwards.\nThe locale must be installed on the OS, otherwise setlocale\nraises a locale.Error: unsupported locale\nsetting  exception.\nY ou must know how to spell the locale name.\nThe locale must be correctly implemented by the makers of the\nOS. I was successful on Ubuntu 19.10, but not on MacOS 10.14.\nOn MacOS, the call setlocale(LC_COLLATE,\n'pt_BR.UTF-8')  returns the string 'pt_BR.UTF-8'  with no\ncomplaints. But sorted(fruits, key=locale.strxfrm)\nproduced the same incorrect result as sorted(fruits)  did. I\nalso tried the fr_FR , es_ES , and de_DE  locales on MacOS, but\nlocale.strxfrm  never did its job.\nSo the standard library solution to internationalized sorting works, but\nseems to be well supported only on GNU/Linux (perhaps also on W indows,\nif you are an expert). Even then, it depends on locale settings, creating\ndeployment headaches.\nFortunately , there is a simpler solution: the PyUCA library , available on\nPyPI .12",2982
63-The Unicode Database.pdf,63-The Unicode Database,"Sorting with the Unicode Collation Algorithm\nJames T auber , prolific Django contributor , must have felt the pain and\ncreated _PyUCA_ , a pure-Python implementation of the Unicode Collation\nAlgorithm (UCA). Example 4-20  shows how easy it is to use.\nExample 4-20. Using the pyuca.Collator .sort_key method\n>>> import pyuca \n>>> coll = pyuca.Collator () \n>>> fruits = ['caju', 'atemoia' , 'cajá', 'açaí', 'acerola' ] \n>>> sorted_fruits  = sorted(fruits, key=coll.sort_key ) \n>>> sorted_fruits  \n['açaí', 'acerola', 'atemoia', 'cajá', 'caju']\nThis is simple and works on GNU/Linux, MacOS, and W indows, at least\nwith my small sample.\nPyUCA does not take the locale into account. If you need to customize the\nsorting, you can provide the path to a custom collation table to the\nCollator()  constructor . Out of the box, it uses allkeys.txt , which\nis bundled with the project. That’ s just a copy of the Default Unicode\nCollation Element T able  from Unicode.or g.\nP Y I C U :  M I R O ’ S  R E C O M M E N D A T I O N  F O R  U N I C O D E\nS O R T I N G\n(T ech reviewer Miroslav Šedivý is a polyglot and an expert on Unicode. This is what he\nwrote about PyUCA.)\nPyUCA has one sorting algorithm that does not respect the sorting order in individual\nlanguages. For instance, Ä in German is between A and B, while in Swedish it comes\nafter Z. Have a look at PyICU  that works like locale without changing the locale of the\nprocess. It is also needed if you want to change the case of iİ/ıI in T urkish. PyICU\nincludes an extension that must be compiled, so it may be harder to install in some\nsystems than PyUCA , which is just Python.\nBy the way , that collation table is one of the many data files that comprise\nthe Unicode database, our next subject.",1778
64-Finding characters by name.pdf,64-Finding characters by name,"T h e  U n i c o d e  D a t a b a s e\nThe Unicode standard provides an entire database—in the form of several\nstructured text files—that includes not only the table mapping code points\nto character names, but also metadata about the individual characters and\nhow they are related. For example, the Unicode database records whether a\ncharacter is printable, is a letter , is a decimal digit, or is some other numeric\nsymbol. That’ s how the str  methods isalpha , isprintable ,\nisdecimal , and isnumeric  work. str.casefold  also uses\ninformation from a Unicode table.\nN O T E\nThe unicodedata.category(char)  function returns the two-letter category of\nchar  from the Unicode database. The higher level str  methods are easier to use. For\nexample, label.isalpha()  returns True  if every character in label  belongs to\none of these categories: Lm , Lt , Lu , Ll , or Lo . T o learn what those codes mean, see\nGeneral Category  in the English W ikipedia’ s Unicode character property  article.\nFinding characters by name\nThe unicodedata  module has functions to retrieve character metadata,\nincluding unicodedata.name() , which returns a character ’ s of ficial\nname in the standard. Figure 4-5  demonstrates that function.13\nFigur e 4-5. Exploring unicodedata.name() in the Python console\nY ou can use the name()  function to build apps that let users search for\ncharacters by name. Figure 4-6  demonstrates the cf.py  command-line\nscript that takes one or more words as ar guments, and lists the characters\nthat have those words in their of ficial Unicode names. The full source code\nfor cf.py  is in Example 4-21 .\nFigur e 4-6. Using cf.py to find smiling cats.\nW A R N I N G\nEmoji support varies widely across operating systems and apps. In recent years the\nMacOS terminal of fers the best support for emojis, followed by modern GNU/Linux\ngraphic terminals. W indows cmd.exe and PowerShell now support Unicode output, but\nas I write this section in January 2020, they still don’ t display emojis—at least not “out\nof the box”. T ech reviewer Leonardo Rochael told me about a new , Open Source\nW indows T erminal  by Microsoft, which may have better Unicode support than the older\nMicrosoft consoles. I did not have time to try it.\nIn Example 4-21 , note the if  statement in the find  function using the\n.issubset()  method to quickly test whether all the words in the query\nset appear in the list of words built from the character ’ s name. Thanks to\nPython’ s rich set API, we don’ t need a nested for  loop and another if  to\nimplement this check.\nExample 4-21. cf.py: the character finder utility\n#!/usr/bin/env python3  \nimport sys \nimport unicodedata  \n \nSTART, END = ord(' '), sys.maxunicode  + 1           \n  \n \ndef find(*query_words , start=START, end=END):       \n  \n    query = {w.upper() for w in query_words }        \n  \n    for code in range(start, end): \n        char = chr(code)",2948
65-Numeric meaning of characters.pdf,65-Numeric meaning of characters,"name = unicodedata .name(char, None)         \n  \n        if name and query.issubset (name.split()):   \n \n            print(f'U+{code:04X} \t{char}\t{name}')  \n \n \ndef main(words): \n    if words: \n        find(*words) \n    else: \n        print('Please provide words to find. ') \n \nif __name__  == '__main__ ': \n    main(sys.argv[1:])\nSet defaults for the range of code points to search.\nfind  accepts query_words  and optional keyword-only ar guments to\nlimit the range of the search, to facilitate testing.\nConvert query_words  into a set of uppercased strings.\nGet Unicode character for code .\nGet name of character , or None  if the code point is unassigned.\nIf there is a name, split it into a list words, then check the query  set is\na subset of that list.\nPrint out line with code point in U+9999  format, the character and its\nname.\nThe unicodedata  module has other interesting functions. Next we’ll see\na few that are related to getting information from characters that have\nnumeric meaning.\nNumeric meaning of characters\nThe unicodedata  module includes functions to check whether a\nUnicode character represents a number and, if so, its numeric value for\nhumans—as opposed to its code point number . Example 4-22  shows the use\nof unicodedata.name()  and unicodedata.numeric()  along\nwith the .isdecimal()  and .isnumeric()  methods of str .\nExample 4-22. Demo of Unicode database numerical character metadata\n(callouts describe each column in the output)\nimport unicodedata  \nimport re \n \nre_digit  = re.compile(r'\d') \n \nsample = '1\xbc\xb2\u0969\u136b\u216b\u2466\u2480\u3285' \n \nfor char in sample: \n    print(f'U+{ord(char):04x} ',                       \n  \n          char.center(6),                             \n  \n          're_dig' if re_digit .match(char) else '-',  \n \n          'isdig' if char.isdigit() else '-',         \n  \n          'isnum' if char.isnumeric () else '-',       \n  \n          f'{unicodedata.numeric(char):5.2f} ',        \n  \n          unicodedata .name(char),                     \n  \n          sep='\t')\nCode point in U+0000  format.\nCharacter centralized in a str  of length 6.\nShow re_dig  if character matches the r'\d'  regex.\nShow isdig  if char.isdigit()  is True .\nShow isnum  if char.isnumeric()  is True .\nNumeric value formatted with width 5 and 2 decimal places.\nUnicode character name.\nRunning Example 4-22  gives you Figure 4-7 , if your terminal font has all\nthose glyphs.\nFigur e 4-7. MacOS terminal showing numeric characters and metadata about them; r e_dig means\nthe character matches the r egular expr ession r’\d’\nThe sixth column of Figure 4-7  is the result of calling\nunicodedata.numeric(char)  on the character . It shows that\nUnicode knows the numeric value of symbols that represent numbers. So if\nyou want to create a spreadsheet application that supports T amil digits or\nRoman numerals, go for it!\nFigure 4-7  shows that the regular expression r'\d'  matches the digit “1”\nand the Devanagari digit 3, but not some other characters that are\nconsidered digits by the isdigit  function. The re  module is not as savvy",3159
66-Dual-Mode str and bytes APIs.pdf,66-Dual-Mode str and bytes APIs,,0
67-str Versus bytes in Regular Expressions.pdf,67-str Versus bytes in Regular Expressions,"about Unicode as it could be. The new regex  module available on PyPI\nwas designed to eventually replace re  and provides better Unicode\nsupport.  W e’ll come back to the re  module in the next section.\nThroughout this chapter we’ve used several unicodedata  functions, but\nthere are many more we did not cover . See the standard library\ndocumentation for the unicodedata  module .\nNext we’ll take a quick look at dual-mode APIs of fering functions that\naccept str  or bytes  ar guments with special handling depending on the\ntype.\nD u a l - M o d e  s t r  a n d  b y t e s  A P I s\nPython’ s standard library has functions that accept str  or bytes\nar guments and behave dif ferently depending on the type. Some examples\nare in the re  and os  modules.\nstr V ersus bytes in Regular Expressions\nIf you build a regular expression with bytes , patterns such as \d  and \w\nonly match ASCII characters; in contrast, if these patterns are given as str ,\nthey match Unicode digits or letters beyond ASCII. Example 4-23  and\nFigure 4-8  compare how letters, ASCII digits, superscripts, and T amil digits\nare matched by str  and bytes  patterns.\nExample 4-23. ramanujan.py: compar e behavior of simple str and bytes\nr egular expr essions\nimport re \n \nre_numbers_str  = re.compile(r'\d+')     \n \nre_words_str  = re.compile(r'\w+') \nre_numbers_bytes  = re.compile(rb'\d+')  \n \nre_words_bytes  = re.compile(rb'\w+') \n \ntext_str  = (""Ramanujan saw \u0be7\u0bed\u0be8\u0bef""  \n \n            "" as 1729 = 1³ + 12³ = 9³ + 10³. "")        \n  \n 14\ntext_bytes  = text_str .encode('utf_8')  \n \n \nprint(f'Text\n  {text_str!r} ') \nprint('Numbers') \nprint('  str  : ', re_numbers_str .findall(text_str ))      \n \nprint('  bytes: ', re_numbers_bytes .findall(text_bytes ))  \n \nprint('Words') \nprint('  str  : ', re_words_str .findall(text_str ))        \n  \nprint('  bytes: ', re_words_bytes .findall(text_bytes ))    \nThe first two regular expressions are of the str  type.\nThe last two are of the bytes  type.\nUnicode text to search, containing the T amil digits for 1729 (the logical\nline continues until the right parenthesis token).\nThis string is joined to the previous one at compile time (see “2.4.2.\nString literal concatenation”  in The Python Language Refer ence ).\nA bytes  string is needed to search with the bytes  regular\nexpressions.\nThe str  pattern r'\d+'  matches the T amil and ASCII digits.\nThe bytes  pattern rb'\d+'  matches only the ASCII bytes for digits.\nThe str  pattern r'\w+'  matches the letters, superscripts, T amil, and\nASCII digits.\nThe bytes  pattern rb'\w+'  matches only the ASCII bytes for letters\nand digits.",2683
68-str Versus bytes in os Functions.pdf,68-str Versus bytes in os Functions,"Figur e 4-8. Scr eenshot of running ramanujan.py fr om Example 4-23\nExample 4-23  is a trivial example to make one point: you can use regular\nexpressions on str  and bytes , but in the second case bytes outside the\nASCII range are treated as nondigits and nonword characters.\nFor str  regular expressions, there is a re.ASCII  flag that makes \w , \W ,\n\b , \B , \d , \D , \s , and \S  perform ASCII-only matching. See the\ndocumentation of the re  module  for full details.\nAnother important dual-mode module is os .\nstr V ersus bytes in os Functions\nThe GNU/Linux kernel is not Unicode savvy , so in the real world you may\nfind filenames made of byte sequences that are not valid in any sensible\nencoding scheme, and cannot be decoded to str . File servers with clients\nusing a variety of OSes are particularly prone to this problem.\nIn order to work around this issue, all os  module functions that accept\nfilenames or pathnames take ar guments as str  or bytes . If one such\nfunction is called with a str  ar gument, the ar gument will be automatically\nconverted using the codec named by\nsys.getfilesystemencoding() , and the OS response will be\ndecoded with the same codec. This is almost always what you want, in\nkeeping with the Unicode sandwich best practice.\nBut if you must deal with (and perhaps fix) filenames that cannot be\nhandled in that way , you can pass bytes  ar guments to the os  functions to\nget bytes  return values. This feature lets you deal with any file or\npathname, no matter how many gremlins you may find. See Example 4-24 .\nExample 4-24. listdir with str and bytes ar guments and r esults\n>>> os.listdir('.')  \n \n['abc.txt', 'digits-of-π.txt']  \n>>> os.listdir(b'.')  \n \n[b'abc.txt', b'digits-of-\xcf\x80.txt']\nThe second filename is “digits-of-π.txt” (with the Greek letter pi).\nGiven a byte  ar gument, listdir  returns filenames as bytes:\nb'\xcf\x80'  is the UTF-8 encoding of the Greek letter pi).\nT o help with manual handling of str  or bytes  sequences that are file or\npath names, the os  module provides special encoding and decoding\nfunctions os.fsencode(name_or_path)  and\nos.fsdecode(name_or_path) . Both of these functions accept an\nar gument of type str , bytes , or -an object implementing the\nos.PathLike  interface since Python 3.6.",2318
69-Chapter Summary.pdf,69-Chapter Summary,"Unicode is a deep rabbit hole. T ime to wrap up our exploration of str  and\nbytes .\nC h a p t e r  S u m m a r y\nW e started the chapter by dismissing the notion that 1 character ==\n1 byte . As the world adopts Unicode, we need to keep the concept of text\nstrings separated from the binary sequences that represent them in files, and\nPython 3 enforces this separation.\nAfter a brief overview of the binary sequence data types— bytes ,\nbytearray , and memoryview —we jumped into encoding and\ndecoding, with a sampling of important codecs, followed by approaches to\nprevent or deal with the infamous UnicodeEncodeError ,\nUnicodeDecodeError , and the SyntaxError  caused by wrong\nencoding in Python source files.\nW e then considered the theory and practice of encoding detection in the\nabsence of metadata: in theory , it can’ t be done, but in practice the Chardet\npackage pulls it of f pretty well for a number of popular encodings. Byte\norder marks were then presented as the only encoding hint commonly found\nin UTF-16 and UTF-32 files—sometimes in UTF-8 files as well.\nIn the next section, we demonstrated opening text files, an easy task except\nfor one pitfall: the encoding=  keyword ar gument is not mandatory when\nyou open a text file, but it should be. If you fail to specify the encoding, you\nend up with a program that manages to generate “plain text” that is\nincompatible across platforms, due to conflicting default encodings. W e\nthen exposed the dif ferent encoding settings that Python uses as defaults\nand how to detect them. A sad realization for W indows users is that these\nsettings often have distinct values within the same machine, and the values\nare mutually incompatible; GNU/Linux and MacOS users, in contrast, live\nin a happier place where UTF-8  is the default pretty much everywhere.\nUnicode provides multiple ways of representing some characters, so\nnormalizing is a prerequisite for text matching. In addition to explaining\nnormalization and case folding, we presented some utility functions that\nyou may adapt to your needs, including drastic transformations like\nremoving all accents. W e then saw how to sort Unicode text correctly by",2198
70-Further Reading.pdf,70-Further Reading,"leveraging the standard locale  module—with some caveats—and an\nalternative that does not depend on tricky locale configurations: the external\nPyUCA package.\nW e leveraged the Unicode database to program a command-line utility to\nsearch for characters by name–in 28 lines of code, thanks to the power of\nPython. W e glanced at other Unicode metadata, and had a brief overview of\ndual-mode APIs where some functions can be called with str  or bytes\nar guments, producing dif ferent results.\nF u r t h e r  R e a d i n g\nNed Batchelder ’ s 2012 PyCon US talk “Pragmatic Unicode–or–How Do I\nStop the Pain?”  was outstanding. Ned is so professional that he provides a\nfull transcript of the talk along with the slides and video.\n“Character encoding and Unicode in Python: How to ( ╯ °□°) ╯ ︵  ┻━┻  with\ndignity” ( slides , video ) was the excellent PyCon 2014 talk by Esther Nam\nand T ravis Fischer where I found this chapter ’ s pithy epigraph: “Humans\nuse text. Computers speak bytes.”\nLennart Regebro—one of the technical reviewers for the First Edition --\nshares his “Useful Mental Model of Unicode (UMMU)” in the short post\n“Unconfusing Unicode: What Is Unicode?” . Unicode is a complex standard,\nso Lennart’ s UMMU is a really useful starting point.\nThe of ficial Unicode HOWT O  in the Python docs approaches the subject\nfrom several dif ferent angles, from a good historic intro to syntax details,\ncodecs, regular expressions, filenames, and best practices for Unicode-\naware I/O (i.e., the Unicode sandwich), with plenty of additional reference\nlinks from each section. Chapter 4, “Strings” , of Mark Pilgrim’ s awesome\nbook Dive into Python 3  also provides a very good intro to Unicode support\nin Python 3. In the same book, Chapter 15  describes how the Chardet\nlibrary was ported from Python 2 to Python 3, a valuable case study given\nthat the switch from the old str  to the new bytes  is the cause of most\nmigration pains, and that is a central concern in a library designed to detect\nencodings.\nIf you know Python 2 but are new to Python 3, Guido van Rossum’ s What’ s\nNew in Python 3.0  has 15 bullet points that summarize what changed, with\nlots of links. Guido starts with the blunt statement: “Everything you thought\nyou knew about binary data and Unicode has changed.” Armin Ronacher ’ s\nblog post “The Updated Guide to Unicode on Python”  is deep and\nhighlights some of the pitfalls of Unicode in Python 3 (Armin is not a big\nfan of Python 3).\nChapter 2, “Strings and T ext,” of the Python Cookbook, Thir d Edition\n(O’Reilly), by David Beazley and Brian K. Jones, has several recipes\ndealing with Unicode normalization, sanitizing text, and performing text-\noriented operations on byte sequences. Chapter 5 covers files and I/O, and it\nincludes “Recipe 5.17. W riting Bytes to a T ext File,” showing that\nunderlying any text file there is always a binary stream that may be\naccessed directly when needed. Later in the cookbook, the struct  module\nis put to use in “Recipe 6.1 1. Reading and W riting Binary Arrays of\nStructures.”\nNick Coghlan’ s Python Notes blog has two posts very relevant to this\nchapter: “Python 3 and ASCII Compatible Binary Protocols”  and\n“Processing T ext Files in Python 3” . Highly recommended.\nA list of encodings supported by Python is available at Standard Encodings\nin the codecs  module documentation. If you need to get that list\nprogrammatically , see how it’ s done in the /T ools/unicode/listcodecs.py\nscript that comes with the CPython source code.\nThe books Unicode Explained  by Jukka K. Korpela (O’Reilly) and Unicode\nDemystified  by Richard Gillam (Addison-W esley) are not Python-specific\nbut were very helpful as I studied Unicode concepts. Pr ogramming with\nUnicode  by V ictor Stinner is a free, self-published book (Creative\nCommons BY -SA) covering Unicode in general as well as tools and APIs in\nthe context of the main operating systems and a few programming\nlanguages, including Python.\nThe W3C pages Case Folding: An Introduction  and Character Model for the\nW orld W ide W eb: String Matching and Searching  cover normalization\nconcepts, with the former being a gentle introduction and the latter a\nworking group note written in dry standard-speak—the same tone of the\nUnicode Standard Annex #15 — Unicode Normalization Forms . The\nFrequently Asked Questions / Normalization  from Unicode.or g  is more\nreadable, as is the NFC F AQ  by Mark Davis—author of several Unicode\nalgorithms and president of the Unicode Consortium at the time of this\nwriting.\nIn 2016, the Museum of Modern Art (MoMA) in New Y ork added to its\ncollection The Original Emoji , the 176 emojis designed by Shigetaka Kurita\nin 1999 for NTT DOCOMO—the Japanese mobile carrier . Going further\nback in history , Emojipedia  published Correcting the Record on the First\nEmoji Set , crediting Japan’ s SoftBank for the earliest known emoji set,\ndeployed in cell phones in 1997. SoftBank’ s set is the source of 90 emojis\nnow in Unicode, including U+1F4A9 (PILE OF POO). Matthew\nRothenber g’ s emojitracker .com  is a live dashboard showing counts of emoji\nusage on T witter , updated in real time. As I write this, F ACE WITH TEARS\nOF JOY (U+1F602) is the most popular emoji on T witter , with more than\n3,313,667,315 recorded occurrences.\nS O A P B O X\nNon-ASCII Names in Sour ce Code: Should Y ou Use Them?\nPython 3 allows non-ASCII identifiers in source code:\n>>> ação = 'PBR'  # ação = stock  \n>>> ε = 10**-6    # ε = epsilon\nSome people dislike the idea. The most common ar gument to stick with\nASCII identifiers is to make it easy for everyone to read and edit code.\nThat ar gument misses the point: you want your source code to be\nreadable and editable by its intended audience, and that may not be\n“everyone.” If the code belongs to a multinational corporation or is\nopen source and you want contributors from around the world, the\nidentifiers should be in English, and then all you need is ASCII.\nBut if you are a teacher in Brazil, your students will find it easier to\nread code that uses Portuguese variable and function names, correctly\nspelled. And they will have no dif ficulty typing the cedillas and\naccented vowels on their localized keyboards.\nNow that Python can parse Unicode names and UTF-8 is the default\nsource encoding, I see no point in coding identifiers in Portuguese\nwithout accents, as we used to do in Python 2 out of necessity—unless\nyou need the code to run on Python 2 also. If the names are in\nPortuguese, leaving out the accents won’ t make the code more readable\nto anyone.\nThis is my point of view as a Portuguese-speaking Brazilian, but I\nbelieve it applies across borders and cultures: choose the human\nlanguage that makes the code easier to read by the team, then use the\ncharacters needed for correct spelling.\nWhat Is “Plain T ext”?\nFor anyone who deals with non-English text on a daily basis, “plain\ntext” does not imply “ASCII.” The Unicode Glossary  defines plain text\nlike this:\nComputer -encoded text that consists only of a sequence of code\npoints fr om a given standar d, with no other formatting or structural\ninformation.\nThat definition starts very well, but I don’ t agree with the part after the\ncomma. HTML is a great example of a plain-text format that carries\nformatting and structural information. But it’ s still plain text because\nevery byte in such a file is there to represent a text character , usually\nusing UTF-8. There are no bytes with nontext meaning, as you can find\nin a .png  or .xls  document where most bytes represent packed binary\nvalues like RGB values and floating-point numbers. In plain text,\nnumbers are represented as sequences of digit characters.\nI am writing this book in a plain-text format called—ironically—\nAsciiDoc , which is part of the toolchain of O’Reilly’ s excellent Atlas\nbook publishing platform . AsciiDoc source files are plain text, but they\nare UTF-8, not ASCII. Otherwise, writing this chapter would have been\nreally painful. Despite the name, AsciiDoc is just great.\nThe world of Unicode is constantly expanding and, at the edges, tool\nsupport is not always there. Not all characters I wanted to show were\navailable in the fonts used to render the book. That’ s why I had to use\nimages instead of listings in several examples in this chapter . On the\nother hand, the Ubuntu and MacOS terminals display most Unicode text\nvery well—including the Japanese characters for the word “mojibake”:\n文字化け .\nHow Ar e str Repr esented in RAM?\nThe of ficial Python docs avoid the issue of how the code points of a\nstr  are stored in memory . It is really an implementation detail. In\ntheory , it doesn’ t matter: whatever the internal representation, every\nstr  must be encoded to bytes  on output.\nIn memory , Python 3 stores each str  as a sequence of code points\nusing a fixed number of bytes per code point, to allow ef ficient direct\naccess to any character or slice.\nSince Python 3.3, when creating a new str  object, the interpreter\nchecks the characters in it and chooses the most economic memory\nlayout that is suitable for that particular str : if there are only\ncharacters in the latin1  range, that str  will use just one byte per\ncode point. Otherwise, 2 or 4 bytes per code point may be used,\ndepending on the str . This is a simplification; for the full details, look\nup PEP 393 — Flexible String Representation .\nThe flexible string representation is similar to the way the int  type\nworks in Python 3: if the integer fits in a machine word, it is stored in\none machine word. Otherwise, the interpreter switches to a variable-\nlength representation like that of the Python 2 long  type. It is nice to\nsee the spread of good ideas.\nHowever , we can always count on Armin Ronacher to find problems in\nPython 3. He explained to me why that was not such as great idea in\npractice: it takes a single RA T (U+1F400) to inflate an otherwise all-\nASCII text into a memory-hogging array using 4 bytes per character ,\nwhen one 1 byte would suf fice for each character except the RA T . In\naddition, because of all the ways Unicode characters combine, the\nability to quickly retrieve an arbitrary character by position is overrated\n—and extracting arbitrary slices from Unicode text is naïve at best, and\noften wrong, producing mojibake. As emojis become more popular ,\nthese problems will only get worse.\n1  Slide 12 of PyCon 2014 talk “Character Encoding and Unicode in Python” ( slides , video ).\n2  Python 2.6 and 2.7 also had bytes , but it was just an alias to the str  type.\n3  T rivia: the ASCII “single quote” character that Python uses by default as the string delimiter\nis actually named APOSTROPHE in the Unicode standard. The real single quotes are\nasymmetric: left is U+2018 and right is U+2019\n4  It did not work in Python 3.0 to 3.4, causing much pain to developers dealing with binary\ndata. The reversal is documented in PEP 461 — Adding % formatting to bytes and bytearray .\n5  I first saw the term “Unicode sandwich” in Ned Batchelder ’ s excellent “Pragmatic Unicode”\ntalk  at US PyCon 2012.\n6  Source: W indows Command-Line: Unicode and UTF-8 Output T ext Buf fer .\n7  The CIRCLED NUMBER FOR TY TWO character is not rendering correctly in the PDF\ngenerated by O’Reilly’ s toolchain as of July , 2021. Its pictograph is a black circular outline\nwith the number 42 inside.\n8  While researching this subject, I did not find a list of situations when Python 3 internally\nconverts bytes  to str . Python core developer Antoine Pitrou says on the\ncomp.python.devel  list  that CPython internal functions that depend on such conversions\n“don’ t get a lot of use in py3k.”\n9  The Python 2 sys.setdefaultencoding  function was misused and is no longer\ndocumented in Python 3. It was intended for use by the core developers when the internal\ndefault encoding of Python was still undecided. In the same comp.python.devel  thread ,\nMarc-André Lembur g states that the sys.setdefaultencoding  must never be called by\nuser code and the only values supported by CPython are 'ascii'  in Python 2 and 'utf-8'\nin Python 3.\n10  Curiously , the micro sign is considered a “compatibility character” but the ohm symbol is not.\nThe end result is that NFC doesn’ t touch the micro sign but changes the ohm symbol to capital\nomega, while NFKC and NFKD change both the ohm and the micro into Greek characters.\n11  Diacritics af fect sorting only in the rare case when they are the only dif ference between two\nwords—in that case, the word with a diacritic is sorted after the plain word.\n12  Again, I could not find a solution, but did find other people reporting the same problem. Alex\nMartelli, one of the tech reviewers, had no problem using setlocale  and\nlocale.strxfrm  on his Mac with MacOS 10.9. In summary: your mileage may vary .\n13  That’ s an image—not a code listing—because emojis are not well supported by O’Reilly’ s\ndigital publishing toolchain as I write this.\n14  Although it was not better than re  at identifying digits in this particular sample.",13190
71-Whats new in this chapter.pdf,71-Whats new in this chapter,"Chapter 5. Data Class Builders\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 5th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nData classes ar e like childr en. They ar e okay as a starting point, but to\nparticipate as a gr ownup object, they need to take some r esponsibility .\n— Martin Fowler and Kent Beck\nPython of fers a few ways to build a simple class that is just a collection of\nfields, with little or no extra functionality . That pattern is known as a “data\nclass”—and dataclasses  is one of the packages that supports this\npattern. This chapter covers three dif ferent class builders that you may use\nas shortcuts to write data classes:\ncollections.namedtuple : the simplest way—available\nsince Python 2.6;\ntyping.NamedTuple : an alternative that requires type hints on\nthe fields—since Python 3.5, with class  syntax added in 3.6;\n@dataclasses.dataclass : a class decorator that allows\nmore customization than previous alternatives, adding lots of1",1487
72-Overview of data class builders.pdf,72-Overview of data class builders,"options and potential complexity—since Python 3.7.\nAfter covering those class builders, we will discuss why Data Class  is also\nthe name of a code smell: a coding pattern that may be a symptom of poor\nobject-oriented design.\nN O T E\ntyping.TypedDict  may seem like another data class builder . It uses similar syntax\nand is described right after typing.NamedTuple  in the typing  module\ndocumentation  for Python 3.9.\nHowever , TypedDict  does not build concrete classes that you can instantiate. It’ s just\nsyntax to write type hints for function parameters and variables that will accept mapping\nvalues used as records, with keys as field names. W e’ll see them in Chapter 15 ,\n“T ypedDict” .\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThis chapter is new in Fluent Python Second Edition . The section “Classic\nNamed T uples”  appeared in chapter 2 of the First Edition , but the rest of\nthe chapter is completely new .\nW e begin with a high level overview of the three class builders.\nO v e r v i e w  o f  d a t a  c l a s s  b u i l d e r s\nConsider a simple class to represent a geographic coordinate pair:\nExample 5-1. class/coordinates.py\nclass Coordinate : \n \n    def __init__ (self, lat, lon): \n        self.lat = lat \n        self.lon = lon\nThat Coordinate  class does the job of holding latitude and longitude\nattributes. W riting the __init__  boilerplate becomes old real fast,\nespecially if your class has more than a couple of attributes: each of them is\nmentioned three times! And that boilerplate doesn’ t buy us basic features\nwe’d expect from a Python object:\n>>> from coordinates  import Coordinate  \n>>> moscow = Coordinate (55.76, 37.62) \n>>> moscow \n<coordinates.Coordinate object at 0x107142f10>  \n  \n>>> location  = Coordinate (55.76, 37.62) \n>>> location  == moscow  \n \nFalse \n>>> (location .lat, location .lon) == (moscow.lat, moscow.lon)  \n \nTrue\n__repr__  inherited from object  is not very helpful.\nMeaningless == ; the __eq__  method inherited from object\ncompares object ids.\nComparing two coordinates requires explicit comparison of each\nattribute.\nThe data class builders covered in this chapter provide the necessary\n__init__ , __repr__ , and __eq__  methods automatically , as well as\nother useful features.\nN O T E\nNone of the class builders discussed here depend on inheritance to do their work. Both\ncollections.namedtuple  and typing.NamedTuple  build classes that are\ntuple  subclasses. @dataclass  is a class decorator that does not af fect the class\nhierarchy in any way . Each of them uses dif ferent metaprogramming techniques to inject\nmethods and data attributes into the class under construction.\nHere is a Coordinate  class built with namedtuple —a factory function\nthat builds a subclass of tuple  with the name and fields you specify:\n>>> from collections  import namedtuple  \n>>> Coordinate  = namedtuple ('Coordinate ', 'lat lon') \n>>> issubclass (Coordinate , tuple) \nTrue \n>>> moscow = Coordinate (55.756, 37.617) \n>>> moscow \nCoordinate(lat=55.756, lon=37.617)  \n  \n>>> moscow == Coordinate (lat=55.756, lon=37.617)  \n \nTrue\nUseful __repr__ .\nMeaningful __eq__ .\nThe newer typing.NamedTuple  provides the same functionality ,\nadding a type annotation to each field:\n>>> import typing \n>>> Coordinate  = typing.NamedTuple ('Coordinate' , [('lat', float), \n('lon', float)]) \n>>> issubclass (Coordinate , tuple) \nTrue \n>>> typing.get_type_hints (Coordinate ) \n{'lat': <class 'float'>, 'lon': <class 'float'>}\nT I P\nA typed named tuple can also be constructed with the fields given as keyword\nar guments, like this:\nCoordinate = typing.NamedTuple('Coordinate', lat=float,  \nlon=float)\nThis is more readable, and also lets you provide the mapping of fields and types as\n**fields_and_types .\nSince Python 3.6, typing.NamedTuple  can also be used in a class\nstatement, with type annotations written as described in PEP 526—Syntax\nfor V ariable Annotations . This is much more readable, and makes it easy to\noverride methods or add new ones. Example 5-2  is the same Coordinate\nclass, with a pair of float  attributes and a custom __str__  to display a\ncoordinate formatted like 55.8°N, 37.6°E:\nExample 5-2. typing_namedtuple/coordinates.py\nfrom typing import NamedTuple  \n \nclass Coordinate (NamedTuple ): \n    lat: float \n    lon: float \n \n    def __str__(self): \n        ns = 'N' if self.lat >= 0 else 'S' \n        we = 'E' if self.lon >= 0 else 'W' \n        return f'{abs(self.lat):.1f}°{ns}, {abs(self.lon):.1f}°\n{we}'\nW A R N I N G\nAlthough NamedTuple  appears in the class  statement as a superclass, it’ s actually\nnot. typing.NamedTuple  uses the advanced functionality of a metaclass  to\ncustomize the creation of the user ’ s class. Check this out:\n>>> issubclass (Coordinate , typing.NamedTuple ) \nFalse \n>>> issubclass (Coordinate , tuple) \nTrue\nIn the __init__  method generated by typing.NamedTuple , the\nfields appear as parameters in the same order they appear in the class\nstatement.\nLike typing.NamedTuple , the dataclass  decorator supports PEP\n526  syntax to declare instance attributes. The decorator reads the variable\nannotations and automatically generates methods for your class. For\ncomparison, check out the equivalent Coordinate  class written with the\nhelp of the dataclass  decorator:\nExample 5-3. dataclass/coordinates.py2",5429
73-Main features.pdf,73-Main features,"from dataclasses  import dataclass  \n \n@dataclass (frozen=True) \nclass Coordinate : \n    lat: float \n    lon: float \n \n    def __str__(self): \n        ns = 'N' if self.lat >= 0 else 'S' \n        we = 'E' if self.lon >= 0 else 'W' \n        return f'{abs(self.lat):.1f}°{ns}, {abs(self.lon):.1f}°\n{we}'\nNote that the body of the classes in Example 5-2  and Example 5-3  are\nidentical—the dif ference is in the class  statement itself. The\n@dataclass  decorator does not depend on inheritance or a metaclass, so\nit should not interfere with your own use of these mechanisms.  The\nCoordinate  class in Example 5-3  is a subclass of object .\nMain features\nThe dif ferent data class builders have a lot in common. T able 5-1\nsummarizes.3\n \nT\na\nb\nl\ne  \n5\n-\n1\n.  \nS\ne\nl\ne\nc\nt\ne\nd  \nf\ne\na\nt\nu\nr\ne\ns  \nc\no\nm\np\na\nr\ne\nd  \na\nc\nc\nr\no\ns\ns  \nt\nh\ne  \nt\nh\nr\ne\ne  \nd\na\nt\na  \nc\nl\na\ns\ns  \nb\nu\ni\nl\nd\ne\nr\ns\n.  \nx  \ns\nt\na\nn\nd\ns  \nf\no\nr  \na\nn  \ni\nn\ns\nt\na\nn\nc\ne  \no\nf  \na  \nd\na\nt\na  \nc\nl\na\ns\ns  \no\nf  \nt\nh\na\nt  \nk\ni\nn\nd\n.\n \nnamedtuple NamedT uple dataclass\n \nmutable instances NO NO YES\nclass statement syntax NO YES YES\nconstruct dict x._asdict() x._asdict() dataclasses.asdict(x)\nget field names x._fields x._fields [f.name for f in  \ndataclasses.fields(x)]\nget defaults x._field_defaults x._field_defaults [f.default for f in  \ndataclasses.fields(x)]\nget field types N/A x.__annotations__ x.__annotations__\nnew instance with  \nchangesx._replace(…) x._replace(…) dataclasses.replace(x, …)\nnew class at runtime namedtuple(…) NamedT uple(…) dataclasses.make_dataclass(…\n)\n \nW A R N I N G\nThe classes built by typing.NamedTuple  and @dataclass  have an\n__annotations__  attribute holding the type hints for the fields. However , the best\npractice is not to read from __annotations__  directly , but use\ntyping.get_type_hints(my_data_class)  to obtain that information. That’ s\nbecause get_type_hints  provides extra services, like resolving forward references\nin type hints. W e get back to this issue much later in the book, in “Problems with\nAnnotations at Runtime” .\nNow let’ s discuss those main features.\nMutable instances\nA key dif ference between these class builders is that\ncollections.namedtuple  and typing.NamedTuple  build\ntuple  subclasses, therefore the instances are immutable. By default,\n@dataclass  produces mutable classes. But the decorator accepts a\nkeyword ar gument frozen —shown in Example 5-3 . When\nfrozen=True , the class will raise an exception if you try to assign a\nvalue to a field after the instance is initialized.\nClass statement syntax\nOnly typing.NamedTuple  and dataclass  support the regular\nclass  statement syntax, making it easier to add methods and docstrings to\nthe class you are creating.\nConstruct dict\nBoth named tuple variants provide an instance method ( ._asdict ) to\nconstruct a dict  object from the fields in a data class instance. The\ndataclasses  module provides a function to do it:\ndataclasses.asdict .\nGet field names and default values\nAll three class builders let you get the field names and default values that\nmay be configured for them. In named tuple classes, that metadata is in the\n._fields  and ._fields_defaults  class attributes. Y ou can get the\nsame metadata from a dataclass  decorated class using the fields\nfunction from the dataclasses  module. It returns a tuple of Field\nobjects which have several attributes, including name  and default .\nGet field types\nClasses defined with the help of typing.NamedTuple  and\n@dataclass  have a mapping of field names to type annotations the\n__annotations__  class attribute. As mentioned, use the",3765
74-Classic Named Tuples.pdf,74-Classic Named Tuples,"typing.get_type_hints  function instead of readint\n__annotations__  directly .\nNew instance with changes\nGiven a named tuple instance x , the call x._replace(**kwargs)\nreturns a new instance with some attribute values replaced according to the\nkeyword ar guments given. The dataclasses.replace(x,\n**kwargs)  module-level function does the same for an instance of a\ndataclass  decorated class.\nNew class at runtime\nAlthough the class  statement syntax is more readable, it is hard-coded. A\nframework may need to build data classes on the fly , at runtime. For that,\nyou can use the default function call syntax of\ncollections.namedtuple , which is likewise supported by\ntyping.NamedTuple . The dataclasses  module provides a\nmake_dataclass  function for the same purpose.\nAfter this overview of the main features of the data class builders, let’ s\nfocus on each of them in turn, starting with the simplest.\nC l a s s i c  N a m e d  T u p l e s\nThe collections.namedtuple  function is a factory that builds\nsubclasses of tuple  enhanced with field names, a class name, and an\ninformative __repr__ . Classes built with namedtuple  can be used\nanywhere where tuples  are needed, and in fact many functions of the\nPython standard library that used to return tuples now return named tuples\nfor convenience, without af fecting user ’ s code at all.\nT I P\nEach instance of a class built by namedtuple  takes exactly the same amount of\nmemory as a tuple because the field names are stored in the class.\nExample 5-4  shows how we could define a named tuple to hold information\nabout a city .\nExample 5-4. Defining and using a named tuple type\n>>> from collections  import namedtuple  \n>>> City = namedtuple ('City', 'name country population  \ncoordinates ')  \n \n>>> tokyo = City('Tokyo', 'JP', 36.933, (35.689722 , 139.691667 ))  \n \n>>> tokyo \nCity(name='Tokyo', country='JP', population=36.933, coordinates=\n(35.689722,  \n139.691667))  \n>>> tokyo.population   \n \n36.933 \n>>> tokyo.coordinates  \n(35.689722, 139.691667)  \n>>> tokyo[1] \n'JP'\nT wo parameters are required to create a named tuple: a class name and a\nlist of field names, which can be given as an iterable of strings or as a\nsingle space-delimited string.\nField values must be passed as separate positional ar guments to the\nconstructor (in contrast, the tuple  constructor takes a single iterable).\nY ou can access the fields by name or position.\nAs a tuple  subclass, City  inherits useful methods such as __eq__  and\nthe special methods for comparison operators—including __lt__  which\nallows sorting lists of City  instances.\nA named tuple of fers a few attributes and methods in addition to those\ninherited from tuple . Example 5-5  shows the most useful: the _fields\nclass attribute, the class method _make(iterable) , and the\n_asdict()  instance method.\nExample 5-5. Named tuple attributes and methods (continued fr om the\npr evious example)\n>>> City._fields  \n \n('name', 'country', 'population', 'location')  \n>>> Coordinate  = namedtuple ('Coordinate ', 'lat lon') \n>>> delhi_data  = ('Delhi NCR ', 'IN', 21.935, Coordinate (28.613889 , \n77.208889 )) \n>>> delhi = City._make(delhi_data )  \n \n>>> delhi._asdict()  \n \n{'name': 'Delhi NCR', 'country': 'IN', 'population': 21.935,  \n'location': Coordinate(lat=28.613889, lon=77.208889)}  \n>>> import json \n>>> json.dumps(delhi._asdict())  \n \n'{""name"": ""Delhi NCR"", ""country"": ""IN"", ""population"": 21.935,  \n""location"": [28.613889, 77.208889]}'\n._fields  is a tuple with the field names of the class.\n._make()  builds City  from an iterable; City(*delhi_data)\nwould do the same.\n._asdict()  returns a dict  built from the named tuple instance.\n._asdict()  is useful to serialize the data in JSON format, for\nexample.\nW A R N I N G\nThe _asdict  method returned an OrderedDict  until Python 3.7. Since Python 3.8,\nit returns a simple dict —which is OK now that we can rely on key insertion order . If\nyou must have an OrderedDict , the _asdict  documentation  recommends building\none from the result: OrderedDict(x._asdict()) .\nSince Python 3.7, namedtuple  accepts the defaults  keyword-only\nar gument providing an iterable of N default values for each of the N\nrightmost fields of the class. Example 5-6  shows how to define a\nCoordinate  named tuple with a default value for a reference  field:\nExample 5-6. Named tuple attributes and methods, continued fr om\nExample 5-5 .\n>>> Coordinate  = namedtuple ('Coordinate' , 'lat lon reference' , \ndefaults =['WGS84']) \n>>> Coordinate (0, 0) \nCoordinate(lat=0, lon=0, reference='WGS84')  \n>>> Coordinate ._field_defaults  \n{'reference': 'WGS84'}\nIn “Class statement syntax”  I mentioned it’ s easier to code methods with the\nclass syntax supported by typing.NamedTuple  and @dataclass .\nY ou can also add methods to a namedtuple , but it’ s a hack. Skip the\nfollowing box if you’re not interested in hacks.\nH A C K I N G  A  NAMEDTUPLE  T O  I N J E C T  A  M E T H O D\nRecall how we built the Card  class in Example 1-1  in Chapter 1 :\nCard = collections .namedtuple ('Card', ['rank', 'suit'])\nLater in Chapter 1  I wrote a spades_high  function for sorting. It\nwould be nice if that logic was encapsulated in a method of Card , but\nadding spades_high  to Card  without the benefit of a class\nstatement requires a quick hack: define the function and then assign it to\na class attribute. Example 5-7  shows how .\nE x a m p l e  5 - 7 .  frenchdeck.doctest :  A d d i n g  a  c l a s s\na t t r i b u t e  a n d  a  m e t h o d  t o  Card ,  t h e  namedtuple  f r o m  “ A\nP y t h o n i c  C a r d  D e c k ”\n>>> Card.suit_values  = dict(spades=3, hearts=2, diamonds =1, \nclubs=0)  \n \n>>> def spades_high (card):                                         \n \n...     rank_value  = FrenchDeck .ranks.index(card.rank) \n...     suit_value  = card.suit_values [card.suit] \n...     return rank_value  * len(card.suit_values ) + suit_value  \n... \n>>> Card.overall_rank  = spades_high                                 \n \n>>> lowest_card  = Card('2', 'clubs') \n>>> highest_card  = Card('A', 'spades') \n>>> lowest_card .overall_rank ()                                     \n \n0 \n>>> highest_card .overall_rank () \n51\nAttach a class attribute with values for each suit.\nspades_high  will become a method; the first ar gument doesn’ t\nneed to be named self . Anyway , it will get the receiver when\ncalled as a method.",6490
75-Type hints 101.pdf,75-Type hints 101,"Attach the function to the Cards  class as a method named\noverall_rank .\nIt works!\nFor readability and future maintenance, it’ s much better to code\nmethods inside a class  statement. But it’ s good to know this hack is\npossible, because it may come in handy .\nThis was a small detour to showcase the power of a dynamic language.\nNow let’ s check out the typing.NamedTuple  variation.\nT y p e d  N a m e d  T u p l e s\nThe Coordinate  class with a default field from Example 5-6  can be\nwritten like this using typing.NamedTuple :\nExample 5-8. typing_namedtuple/coordinates2.py\nfrom typing import NamedTuple  \n \nclass Coordinate (NamedTuple ): \n    lat: float                \n  \n    lon: float \n    reference : str = 'WGS84'  \nEvery instance field must be annotated with a type.\nThe reference  instance field is annotated with a type and a default\nvalue\nClasses built by typing.NamedTuple  don’ t have any methods beyond\nthose that collections.namedtuple  also generates—and those that\nare inherited from tuple . The only dif ference is the presence of the\n__annotations__  class attribute—which Python completely ignores at\nruntime.4",1160
76-The meaning of variable annotations.pdf,76-The meaning of variable annotations,"Given that the main feature of typing.NamedTuple  are the type\nannotations, we’ll take a brief look at them before resuming our exploration\nof data class builders.\nT y p e  h i n t s  1 0 1\nT ype hints—a.k.a. type annotations—are ways to declare the expected type\nof function ar guments, return values, variables, and attributes.\nN O T E\nThis is a very brief introduction to type hints, just enough to make sense of the syntax\nand meaning of the annotations used typing.NamedTuple  and @dataclass\ndeclarations. W e will cover type hints for function signatures in Chapter 8  and more\nadvanced annotations in Chapter 15 . Here we’ll mostly see hints with simple built-in\ntypes, such as str , int , and float , which are probably the most common types used\nto annotate fields of data classes.\nThe first thing you need to know about type hints is that they are not\nenforced at all by the Python bytecode compiler and interpreter .\nNo runtime effect\nA good way to understand Python type hints is to think of them as\n“documentation that can be verified by IDEs and type checkers.”\nThat’ s because type hints have no impact on the runtime behavior of Python\nprograms. Check this out:\nExample 5-9. Python does not enfor ce type hints at runtime.\n>>> import typing \n>>> class Coordinate (typing.NamedTuple ): \n...     lat: float \n...     lon: float \n... \n>>> trash = Coordinate ('Ni!', None) \n>>> print(trash) \nCoordinate (lat='Ni!', lon=None)    \n\nI told you: no type checking at runtime!\nIf you type the code of Example 5-9  in a Python module, it will run and\ndisplay a meaningless Coordinate , with no error or warning:\n$ python3 nocheck_demo.py  \nCoordinate (lat='Ni!', lon=None)\nThe type hints are intended primarily to support third-party type checkers,\nlike Mypy  or the PyCharm IDE  built-in type checker . These are static\nanalysis tools: they check Python source code “at rest”, not running code.\nT o see the ef fect of type hints, you must run one of those tools on your code\n—like a linter . For instance, here is what Mypy has to say about the\nprevious example:\n$ mypy nocheck_demo.py  \nnocheck_demo.py:8: error: Argument 1 to ""Coordinate""  has \nincompatible type ""str""; expected ""float"" \nnocheck_demo.py:8: error: Argument 2 to ""Coordinate""  has \nincompatible type ""None""; expected ""float""\nAs you can see, given the definition of Coordinate , Mypy knows that\nboth ar guments to create an instance must be of type float , but the\nassignment to trash  uses a str  and None .\nNow let’ s talk about the syntax and meaning of type hints.\nV ariable annotation syntax\nBoth typing.NamedTuple  and @dataclass  use the syntax of\nvariable annotations defined in PEP 526 . This is a quick introduction to that\nsyntax in the context defining attributes in class  statements.\nThe basic syntax of variable annotation is:\nvar_name : some_type5\nSection Acceptable type hints  in PEP 484 explains what are acceptable\ntypes, but in the context of defining a data class, these types are more likely\nto be useful:\na concrete class, for example str  or FrenchDeck ;\na parameterized collection type, like list[int] , tuple[str,\nfloat]  etc.\ntyping.Optional , for example Optional[str] —to\ndeclare a field that can be a str  or None .\nY ou can also initialize the variable with a value. In a\ntyping.NamedTuple  or @dataclass  declaration, that value will\nbecome the default for that attribute, if the corresponding ar gument is\nomitted in the constructor call.\nvar_name : some_type  = a_value\nThe meaning of variable annotations\nW e saw in “No runtime ef fect”  that type hints have no ef fect at runtime. But\nat import time—when a module is loaded—Python does read them to build\nthe __annotations__  dictionary that typing.NamedTuple  and\n@dataclass  then use to enhance the class.\nW e’ll start this exploration with a simple class, so that we can later see what\nextra features are added by typing.NamedTuple  and @dataclass .\nExample 5-10. meaning/demo_plain.py: a plain class with type hints\nclass DemoPlainClass : \n    a: int           \n  \n    b: float = 1.1   \n \n    c = 'spam'       \na  becomes an entry in __annotations__ , but is otherwise\ndiscarded: no attribute named a  is created in the class.\nb  is saved as an annotation, and also becomes a class attribute with\nvalue 1.1 .\nc  is just a plain old class attribute, not an annotation.\nW e can verify that in the console, first reading the __annotations__  of\nthe DemoPlainClass , then trying to get its attributes named a , b , and c :\n>>> from demo_plain  import DemoPlainClass  \n>>> DemoPlainClass .__annotations__  \n{'a': <class 'int'>, 'b': <class 'float'>}  \n>>> DemoPlainClass .a \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nAttributeError : type object 'DemoPlainClass' has no attribute 'a'  \n>>> DemoPlainClass .b \n1.1 \n>>> DemoPlainClass .c \n'spam'\nNote that the __annotations__  special attribute is created by the\ninterpreter to record the type hints that appear in the source code—even in a\nplain class.\nThe a  survives only as an annotation. It doesn’ t become a class attribute\nbecause no value is bound to it.  The b  and c  are stored as class attributes\nbecause they are bound to values.\nNone of those three attributes will be in a new instance of\nDemoPlainClass . If you create an object o = DemoPlainClass() ,\no.a  will raise AttributeError , while o.b  and o.c  will retrieve the\nclass attributes with values 1.1  and 'spam' —that’ s just normal Python\nobject behavior .\nInspecting a typing.NamedTuple\nNow let’ s examine a class built with typing.NamedTuple , using the\nsame attributes and annotations as DemoPlainClass  from Example 5-6\n10 .\nExample 5-1 1. meaning/demo_nt.py: a class built with\ntyping.NamedTuple .\nimport typing \n \nclass DemoNTClass (typing.NamedTuple ): \n    a: int           \n  \n    b: float = 1.1   \n \n    c = 'spam'       \na  becomes an annotation and also an instance attribute.\nb  is another annotation, and also becomes an instance attribute with\ndefault value 1.1 .\nc  is just a plain old class attribute; no annotation will refer to it.\nInspecting the DemoNTClass , we get:\n>>> from demo_nt import DemoNTClass  \n>>> DemoNTClass .__annotations__  \n{'a': <class 'int'>, 'b': <class 'float'>}  \n>>> DemoNTClass .a \n<_collections._tuplegetter object at 0x101f0f940>  \n>>> DemoNTClass .b \n<_collections._tuplegetter object at 0x101f0f8b0>  \n>>> DemoNTClass .c \n'spam'\nHere we have the same annotations for a  and b  as we saw in Example 5-10 .\nBut typing.NamedTuple  creates a  and b  class attributes. The c\nattribute is just a plain class attribute with the value 'spam' .\nThe a  and b  class attributes are descriptors —an advanced feature covered\nin Chapter 24 . For now , think of them as similar to property getters:\nmethods that don’ t require the explicit call operator ()  to retrieve an\ninstance attribute. In practice, this means a  and b  will work as read-only\ninstance attributes—which makes sense when we recall that\nDemoNTClass  instances are just fancy tuples, and tuples are immutable.\nDemoNTClass  also gets a custom docstring:\n>>> DemoNTClass .__doc__ \n'DemoNTClass(a, b)'\nLet’ s inspect an instance of DemoNTClass :\n>>> nt = DemoNTClass (8) \n>>> nt.a \n8 \n>>> nt.b \n1.1 \n>>> nt.c \n'spam'\nT o construct nt , we need to give at least the a  ar gument to\nDemoNTClass . The constructor also takes a b  ar gument, but it has a\ndefault value of 1.1 , so it’ s optional. The nt  object has the a  and b\nattributes as expected; it doesn’ t have a c  attribute, but Python retrieves it\nfrom the class, as usual.\nIf you try to assign values to nt.a , nt.b , nt.c  or even nt.z  you’ll get\nAttributeError  exceptions, with subtly dif ferent error messages. T ry\nthat and reflect on the messages.\nInspecting a class decorated with dataclass\nNow we’ll examine Example 5-12 :\nExample 5-12. meaning/demo_dc.py: a class decorated with @dataclass\nfrom dataclasses  import dataclass  \n \n@dataclass  \nclass DemoDataClass : \n    a: int           \n  \n    b: float = 1.1   \n \n    c = 'spam'       \n\na  becomes an annotation and also an instance attribute controlled by a\ndescriptor .\nb  is another annotation, and also becomes an instance attribute with a\ndescriptor and a default value 1.1 .\nc  is just a plain old class attribute; no annotation will refer to it.\nNow let’ s check out __annotations__ , __doc__ , and the a , b , c\nattributes on DemoDataClass :\n>>> from demo_dc import DemoDataClass  \n>>> DemoDataClass .__annotations__  \n{'a': <class 'int'>, 'b': <class 'float'>}  \n>>> DemoDataClass .__doc__ \n'DemoDataClass(a: int, b: float = 1.1)'  \n>>> DemoDataClass .a \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nAttributeError : type object 'DemoDataClass' has no attribute 'a'  \n>>> DemoDataClass .b \n1.1 \n>>> DemoDataClass .c \n'spam'\nThe __annotations__  and __doc__  are not surprising. However ,\nthere is no attribute named a  in DemoDataClass —in contrast with\nDemoNTClass  from Example 5-1 1 , which has a descriptor to get a  from\nthe instances as read-only attributes (that mysterious\n<_collections._tuplegetter> ). That’ s because the a  attribute\nwill only exist in instances of DemoDataClass . It will be a public\nattribute that we can get and set, unless the class is frozen. But b  and c  exist\nas class attributes, with b  holding the default value for the b  instance\nattribute, while c  is just a class attribute that will not be bound to the\ninstances.\nNow let’ s see how a DemoDataClass  instance looks like:",9699
77-More about dataclass.pdf,77-More about dataclass,">>> dc = DemoDataClass (9) \n>>> dc.a \n9 \n>>> dc.b \n1.1 \n>>> dc.c \n'spam'\nAgain, a  and b  are instance attributes, and c  is a class attribute we get via\nthe instance.\nAs mentioned, DemoDataClass  instances are mutable—and no type\nchecking is done at runtime:\n>>> dc.a = 10 \n>>> dc.b = 'oops'\nW e can do even sillier assignments:\n>>> dc.c = 'whatever'  \n>>> dc.z = 'secret stash'\nNow the dc  instance has a c  attribute—but that does not change the c  class\nattribute. And we can add a new z  attribute. This is normal Python\nbehavior: regular instances can have their own attributes that don’ t appear\nin the class.\nM o r e  a b o u t  @dataclass\nW e’ve only seen simple examples of @dataclass  use so far . The\ndecorator accepts several keyword ar guments. This is its signature:\n@dataclass (*, init=True, repr=True, eq=True, order=False, \n              unsafe_hash =False, frozen=False)\nThe *  in the first position means the remaining parameters are keyword-\nonly . T able 5-2  describes them.7\n \nT\na\nb\nl\ne  \n5\n-\n2\n.  \nK\ne\ny\nw\no\nr\nd\n \np\na\nr\na\nm\ne\nt\ne\nr\ns  \na\nc\nc\ne\np\nt\ne\nd\n \nb\ny  \nt\nh\ne  \n@\nd\na\nt\na\nc\nl\na\ns\ns  \nd\ne\nc\no\nr\na\nt\no\nr\n \noption meaning default notes\n \ninit generate __init_\n_T rue Ignored if __init__  is  \nimplemented by user .\nrepr generate __repr_\n_T rue Ignored if __repr__  is  \nimplemented by user .\neq generate __eq__ T rue Ignored if __eq__  is implemented  \nby user .\norder generate __lt__ ,  \n__le__ , __gt__ ,  \n__ge__False If True , raises exceptions if eq=F\nalse , or if any of the comparison  \nmethods that would be generated  \nare defined or inherited.\nunsafe_hash generate __hash_\n_False Complex semantics and several  \ncaveats—see: dataclass  \ndocumentation .\nfrozen make instances  \n“immutable”False instances will be reasonably safe  \nfrom accidental change, but not  \nreally immutable.\n \na  @dataclass  emulates immutability by generating __setattr__  and __delattr__  which raise  \ndataclass.FrozenInstanceError —a subclass of AttributeError —when the user attempts  \nto set or delete a field.\nThe defaults are really the most useful settings for common use cases. The\noptions you are more likely to change from the defaults are:\nfrozen=True : to protect against accidental changes to the class\ninstances;\norder=True : to allow sorting of instances of the data class.\nGiven the dynamic nature of Python objects, it’ s not too hard for a nosy\nprogrammer to go around the protection af forded by frozen=True . But\nthe necessary tricks should be easy to spot on a code review .\nIf the eq  and frozen  ar guments are both True , @dataclass  produces\na suitable __hash__  method, so the instances will be hashable. The\ngenerated __hash__  will use data from all fields that are not individually\nexcluded using a field option we’ll see in “Field options” . If\nfrozen=False  (the default), @dataclass  will set __hash__  to\nNone , signalling that the instances are unhashable, therefore overriding\n__hash__  from any superclass.a",3088
78-Field options.pdf,78-Field options,"PEP 557—Data Classes  has this to say about unsafe_hash :\nAlthough not r ecommended, you can for ce Data Classes to cr eate a\n__hash__  method with unsafe_hash=True . This might be the case\nif your class is logically immutable but can nonetheless be mutated. This\nis a specialized use case and should be consider ed car efully .\nI will leave unsafe_hash  at that. If you feel you must use that option,\ncheck the dataclasses.dataclass  documentation .\nFurther customization of the generated data class can be done at a field\nlevel.\nField options\nW e’ve already seen the most basic field option: providing or not a default\nvalue with the type hint. The instance fields you declare will become\nparameters in the generated __init__ . Python does not allow parameters\nwithout defaults after parameters with defaults, therefore after you declare a\nfield with a default value, all remaining fields must also have default\nvalues.\nMutable default values are a common source of bugs for beginning Python\ndevelopers. In function definitions, a mutable default value is easily\ncorrupted when one invocation of the function mutates the default,\nchanging the behavior of further invocations—an issue we’ll explore in\n“Mutable T ypes as Parameter Defaults: Bad Idea”  ( Chapter 6 ). Class\nattributes are often used as default attribute values for instances, including\nin data classes. And @dataclass  uses the default values in the type hints\nto generate parameters with defaults for __init__ . T o prevent bugs,\n@dataclass  rejects the class definition in Example 5-13 .\nExample 5-13. dataclass/club_wrong.py : this class raises\nValueError\n@dataclass  \nclass ClubMember : \n    name: str \n    guests: list = []\nIf you load the module with that ClubMember  class, this is what you get:\n$ python3 club_wrong.py  \nTraceback (most recent call last ): \n  File ""club_wrong.py"" , line 4, in <module>  \n    class ClubMember:  \n  ...several lines ommitted...  \nValueError: mutable default <class 'list'> for field guests is  \nnot allowed:  \nuse default_factory\nThe ValueError  message explains the problem and suggests a solution:\nuse default_factory . This is how to correct ClubMember :\nExample 5-14. dataclass/club.py : this ClubMember  definition\nworks.\nfrom dataclasses  import dataclass , field \n \n \n@dataclass  \nclass ClubMember : \n    name: str \n    guests: list = field(default_factory =list)\nIn the guests  field of Example 5-14 , instead of a literal list, the default\nvalue is set by calling the dataclasses.field  function with\ndefault_factory=list .\nThe default_factory  parameter lets you provide a function, class, or\nany other callable, which will be invoked with zero ar guments to build a\ndefault value each time an instance of the data class is created. This way ,\neach instance of ClubMember  will have its own list —instead of all\ninstances sharing the same list  from the class, which is rarely what we\nwant and is often a bug.\nW A R N I N G\nIt’ s good that @dataclass  rejects class definitions with a list  default value in a\nfield. However , be aware that it is a partial solution that only applies to list , dict\nand set . Other mutable values used as defaults will not be flagged by @dataclass .\nIt’ s up to you to understand the problem and remember to use a default factory to set\nmutable default values.\nIf you browse the dataclasses  module documentation, you’ll see a\nlist  field defined with a novel syntax, as in Example 5-15 .\nExample 5-15. dataclass/club_generic.py : this ClubMember\ndefinition is mor e pr ecise\nfrom dataclasses  import dataclass , field \n \n@dataclass  \nclass ClubMember : \n    name: str \n    guests: list[str] = field(default_factory =list)  \nlist[str]  means “a list of str”.\nThe new syntax list[str]  is a parameterized generic type: since Python\n3.9, the list  built-in accepts that bracket notation to specify the type of\nthe list items.\nW A R N I N G\nPrior to Python 3.9, the built-in collections did not support generic type notation. As a\ntemporary workaround, there are corresponding collection types in the typing\nmodule. If you need a parameterized list  type hint in Python 3.8 or earlier , you must\nimport the List  type from typing  and use it: List[str] . For more about this\nissue, see “Legacy Support and Deprecated Collection T ypes” .\nW e’ll cover generics in Chapter 8 . For now , note that both Example 5-14\nand Example 5-15  are correct, and the Mypy type checker does not\ncomplain about either of those class definitions.\nThe dif ference is that guests: list  means that guests  can be a\nlist  of objects of any kind, while guests: list[str]  says that\nguests  must be a list  in which every item is a str . This will allow the\ntype checker to find (some) bugs in code that puts invalid items in the list,\nor that read items from it.\nThe default_factory  is likely to be the most common option of the\nfield  function, but there are several others, listed in T able 5-3 .\n \nT\na\nb\nl\ne  \n5\n-\n3\n.  \nK\ne\ny\nw\no\nr\nd\n \na\nr\ng\nu\nm\ne\nn\nt\ns  \na\nc\nc\ne\np\nt\ne\nd\n \nb\ny  \nt\nh\ne  \nf\ni\ne\nl\nd  \nf\nu\nn\nc\nt\ni\no\nn\n \noption meaning default\n \ndefault default value for field _MISSING_TYPE\ndefault_factory 0-parameter function used to produce a default _MISSING_TYPE\ninit include field in parameters to __init__ T rue\nrepr include field in __repr__ T rue\ncompare use field in comparison methods __eq__ , __lt__  etc. T rue\nhash include field in __hash__  calculation None\nmetadata mapping with user -defined data; ignored by the @datac\nlassNonea\nb",5636
79-Post-init processing.pdf,79-Post-init processing,"a  dataclass._MISSING_TYPE  is a sentinel value indicating the option was not provided. It  \nexists so we can set None  as an actual default value, a common use case.\nb  The option hash=None  means the field will be used in __hash__  only if compare=True .\nThe default  option exists because the field  call takes the place of the\ndefault value in the field annotation. If you want to create an athlete\nfield with default value of False , and also omit that field from the\n__repr__  method, you’d write this:\n@dataclass  \nclass ClubMember : \n    name: str \n    guests: list = field(default_factory =list) \n    athlete: bool = field(default=False, repr=False)\nPost-init processing\nThe __init__  method generated by @dataclass  only takes the\nar guments passed and assigns them—or their default values, if missing—to\nthe instance attributes that are instance fields. But you may need to do more\nthan that to initialize the instance. If that’ s the case, you can provide a\n__post_init__  method. When that method exists, @dataclass  will\nadd code to the generated __init__  to call __post_init__  as the last\nstep.\nCommon use cases for __post_init__  are validation and computing\nfield values based on other fields. W e’ll study a simple example that uses\n__post_init__  for both of these reasons.\nFirst, let’ s look at the expected behavior of a ClubMember  subclass\nnamed HackerClubMember , as described by doctests in Example 5-16 .\nExample 5-16. dataclass/hackerclub.py : doctests for\nHackerClubMember\n"""""" \n``HackerClubMember`` objects accept an optional ``handle``  \nargument::  \n \n    >>> anna = HackerClubMember('Anna Ravenscroft',  \nhandle='AnnaRaven')  \n    >>> anna  \n    HackerClubMember(name='Anna Ravenscroft', guests=[],  \nhandle='AnnaRaven')  \n \nIf ``handle`` is ommitted, it's set to the first part of the  \nmember's name::  \n \n    >>> leo = HackerClubMember('Leo Rochael')  \n    >>> leo  \n    HackerClubMember(name='Leo Rochael', guests=[], handle='Leo')  \n \nMembers must have a unique handle. The following ``leo2`` will not  \nbe created,  \nbecause its ``handle`` would be 'Leo', which was taken by ``leo``::  \n \n    >>> leo2 = HackerClubMember('Leo DaVinci')  \n    Traceback (most recent call last):  \n      ...  \n    ValueError: handle 'Leo' already exists.  \n \nTo fix, ``leo2`` must be created with an explicit ``handle``::  \n \n    >>> leo2 = HackerClubMember('Leo DaVinci', handle='Neo')  \n    >>> leo2  \n    HackerClubMember(name='Leo DaVinci', guests=[], handle='Neo')  \n""""""\nNote that we must provide handle  as a keyword ar gument, because\nHackerClubMember  inherits name  and guests  from ClubMember ,\nand adds the handle  field. The generated docstring for\nHackerClubMember  shows the order of the fields in the constructor call:\n>>> HackerClubMember .__doc__ \n""HackerClubMember(name: str, guests: list = <factory>, handle:  \nstr = '')""\nHere, <factory>  is a short way of saying that some callable will produce\nthe default value for guests  (in our case, the factory is the list  class).\nThe point is: to provide a handle  but no guests , we must pass handle\nas a keyword ar gument.\nThe Inheritance  section of the dataclasses  module documentation\nexplains how the order of the fields is computed when there are several\nlevels of inheritance.\nN O T E\nIn Chapter 14  we’ll talk about misusing inheritance, particularly when the superclasses\nare not abstract. Creating a hierarchy of data classes is usually a bad idea, but it served\nus well here to make Example 5-17  shorter , focusing on the handle  field declaration\nand __post_init__  validation.\nExample 5-17  is the implementation:\nExample 5-17. dataclass/hackerclub.py : code for\nHackerClubMember .\nfrom dataclasses  import dataclass  \nfrom club import ClubMember  \n \n@dataclass  \nclass HackerClubMember (ClubMember ):                         \n  \n    all_handles  = set()                                     \n  \n    handle: str = ''                                        \n  \n \n    def __post_init__ (self): \n        cls = self.__class__                                 \n  \n        if self.handle == '':                               \n  \n            self.handle = self.name.split()[0] \n        if self.handle in cls.all_handles :                  \n  \n            msg = f'handle {self.handle!r}  already exists. ' \n            raise ValueError (msg) \n        cls.all_handles .add(self.handle)                    \nHackerClubMember  extends ClubMember .\nall_handles  is a class attribute.",4570
80-dataclass Example Dublin Core Resource Record.pdf,80-dataclass Example Dublin Core Resource Record,"handle  is an instance field of type str  with empty string as its\ndefault value; this makes it optional.\nGet the class of the instance.\nIf self.handle  is the empty string, set it to the first part of name .\nIf self.handle  is in cls.all_handles , raise ValueError .\nAdd the new handle  to cls.all_handles .\nExample 5-17  works as intended, but is not satisfactory to a static type\nchecker . Next, we’ll see why , and how to fix it.\nT yped class attributes\nIf we typecheck Example 5-17  with Mypy , we are reprimanded:\n$ mypy hackerclub.py  \nhackerclub.py:37: error: Need type annotation for ""all_handles""  \n(hint: ""all_handles: Set[<type>] = ..."")  \nFound 1 error in 1 file (checked 1 source file)\nUnfortunately , the hint provided by Mypy (version 0.910 as I review this) is\nnot helpful in the context of @dataclass  usage. First, it suggests using\nSet , but I am using Python 3.9 so I can use set —and avoid importing\nSet  from typing . More importantly , if we add a type hint like set[…]\nto all_handles , @dataclass  will find that annotation and make\nall_handles  an instance field. W e saw this happening in “Inspecting a\nclass decorated with dataclass ” .\nThe workaround defined in PEP 526—Syntax for V ariable Annotations  is\nugly . T o code a class variable with a type hint`, we need to use a pseudo-\ntype named typing.ClassVar , which leverages the generics []\nnotation to set the type of the variable and also declare it a class attribute.\nT o make the type checker and @dataclass  happy , this is how we are\nsupposed to declare all_handles  in Example 5-17 :\n    all_handles : ClassVar [set[str]] = set()\nThat type hint is saying:\nall_handles  is a class attribute of type set -of-str , with an empty\nset  as its default value.\nT o code that annotation, we must import ClassVar  from the typing\nmodule.\nThe @dataclass  decorator doesn’ t care about the types in the\nannotations, except in two cases, and this is one of them: if the type is\nClassVar , an instance field will not be generated for that attribute.\nThe other case where the type of the field is relevant to @dataclass  is\nwhen declaring init-only variables , our next topic.\nInitialization variables that are not fields\nSometimes you may need to pass ar guments to __init__  that are not\ninstance fields. Such ar guments are called init-only variables  by the\ndataclasses  documentation . T o declare an ar gument like that,\ndataclasses  module provides the pseudo-type InitVar , which uses\nthe same syntax of typing.ClassVar . The example given in the\ndocumentation is a data class that has a field initialized from a database,\nand the database object must be passed to the constructor .\nThis is the code that illustrates the Init-only variables  section:\nExample 5-18. Example fr om the dataclasses  module documentation.\n@dataclass  \nclass C: \n    i: int \n    j: int = None \n    database : InitVar[DatabaseType ] = None \n \n    def __post_init__ (self, database ): \n        if self.j is None and database  is not None: \n            self.j = database .lookup('j') \n \nc = C(10, database =my_database )\nNote how the database  attribute is declared. InitVar  will prevent\n@dataclass  from treating database  as a regular field. It will not be set\nas an instance attribute, and the dataclasses.fields  function will not\nlist it. However , database  will be one of the ar guments that the generated\n__init__  will accept, and it will be also passed to __post_init__ —\nif you write that method, you must add a corresponding ar gument to the\nmethod signature, as shown in Example 5-18\nThis rather long overview of @dataclass  covered the most useful\nfeatures—some of them appeared in previous sections, like “Main features”\nwhere we covered all three data class builders in parallel. The\ndataclasses  documentation  and PEP 526 — Syntax for V ariable\nAnnotations  have all details.\nIn the next section, I present a longer example with @dataclass .\n@dataclass Example: Dublin Core Resource Record\nOften, classes built with @dataclass  will have more fields than the very\nshort examples presented so far . Dublin Core  provides the foundation for a\nmore typical @dataclass  example.\nThe Dublin Cor e Schema is a small set of vocabulary terms that can be\nused to describe digital r esour ces (video, images, web pages, etc.), as\nwell as physical r esour ces such as books or CDs, and objects like\nartworks.\n— Dublin Core on W ikipedia\nThe standard defines 15 optional fields, the Resource  class in Example 5-\n19  uses 8 of them.\nExample 5-19. dataclass/resource.py : code for Resource , a\nclass based on Dublin Cor e terms.8\nfrom dataclasses  import dataclass , field \nfrom typing import Optional  \nfrom enum import Enum, auto \nfrom datetime  import date \n \n \nclass ResourceType (Enum):  \n \n    BOOK = auto() \n    EBOOK = auto() \n    VIDEO = auto() \n \n \n@dataclass  \nclass Resource : \n    """"""Media resource description.""""""  \n    identifier : str                                    \n  \n    title: str = '<untitled> '                          \n  \n    creators : list[str] = field(default_factory =list) \n    date: Optional [date] = None                        \n  \n    type: ResourceType  = ResourceType .BOOK             \n  \n    description : str = '' \n    language : str = '' \n    subjects : list[str] = field(default_factory =list)\nThis Enum  will provide type-safe values for the Resource.type\nfield.\nidentifier  is the only required field.\ntitle  is the first field with a default. This forces all fields below to\nprovide defaults.\nThe value of date  can be a datetime.date  instance, or None .\nThe type  field default is ResourceType.BOOK .\nExample 5-20  is a doctest to demonstrate how a Resource  record appears\nin code:\nExample 5-20. dataclass/resource.py : code for Resource , a\nclass based on Dublin Cor e terms.\n    >>> description  = 'Improving the design of existing code'  \n    >>> book = Resource ('978-0-13-475759-9' , 'Refactoring, 2nd  \nEdition' , \n    ...     ['Martin Fowler' , 'Kent Beck' ], date(2018, 11, 19), \n    ...     ResourceType .BOOK, description , 'EN', \n    ...     ['computer programming' , 'OOP']) \n    >>> book  # doctest: +NORMALIZE_WHITESPACE  \n    Resource (identifier ='978-0-13-475759-9' , title='Refactoring,  \n2nd Edition' , \n    creators =['Martin Fowler' , 'Kent Beck' ], \ndate=datetime .date(2018, 11, 19), \n    type=<ResourceType .BOOK: 1>, description ='Improving the design  \nof existing code' , \n    language ='EN', subjects =['computer programming' , 'OOP'])\nThe __repr__  generated by @dataclass  is OK, but we can make it\nmore readable. This is the format we want from repr(book) :\n    >>> book  # doctest: +NORMALIZE_WHITESPACE  \n    Resource ( \n        identifier  = '978-0-13-475759-9' , \n        title = 'Refactoring, 2nd Edition' , \n        creators  = ['Martin Fowler' , 'Kent Beck' ], \n        date = datetime .date(2018, 11, 19), \n        type = <ResourceType .BOOK: 1>, \n        description  = 'Improving the design of existing code' , \n        language  = 'EN', \n        subjects  = ['computer programming' , 'OOP'], \n    )\nExample 5-21  is the code of __repr__  to produce the format above. This\nexample uses dataclass.fields  to get the names of the data class\nfields.\nExample 5-21. dataclass/resource_repr.py : code for\n__repr__  method implemented in the Resource  class fr om Example 5-\n19 .\n    def __repr__ (self): \n        cls = self.__class__  \n        cls_name  = cls.__name__  \n        indent = ' ' * 4 \n        res = [f'{cls_name} (']                            \n  \n        for f in fields(cls):                             \n  \n            value = getattr(self, f.name)",7776
81-Data class as a code smell.pdf,81-Data class as a code smell,"res.append(f'{indent} {f.name}  = {value!r} ,')  \n \n \n        res.append(')')                                   \n  \n        return '\n'.join(res)                             \nStart the res  list to build the output string with the class name and open\nparenthesis.\nFor each field f  in the class…\nGet the named attribute from the instance.\nAppend an indented line with the name of the field and repr(value)\n—that’ s what the !r  does.\nAppend closing parenthesis.\nBuild multiline string from res  and return it.\nW ith this example inspired by the soul of Dublin, Ohio, we conclude our\ntour of Python’ s data class builders.\nData classes are handy , but your project may suf fer if you overuse them.\nThe next section explains.\nD a t a  c l a s s  a s  a  c o d e  s m e l l\nWhether you implement a data class writing all the code yourself or\nleveraging one of the class builders described in this chapter , be aware that\nit may signal a problem in your design.\nIn Refactoring, Second Edition , Martin Fowler and Kent Beck present a\ncatalog of “code smells”—patterns in code that may indicate the need for\nrefactoring. The entry titled Data Class  starts like this:\nThese ar e classes that have fields, getting and setting methods for fields,\nand nothing else. Such classes ar e dumb data holders and ar e often being\nmanipulated in far too much detail by other classes.\nIn Fowler ’ s personal W eb site there’ s an illuminating post titled Code\nSmell . The post is very relevant to our discussion because he uses data\nclass  as one example of a code smell and suggests how to deal with it. Here\nis the post, reproduced in full.9\nC O D E  S M E L L\nBy Martin Fowler\nA code smell is a surface indication that usually corresponds to a deeper\nproblem in the system. The term was first coined by Kent Beck while\nhelping me with my Refactoring  book.\nThe quick definition above contains a couple of subtle points. Firstly a\nsmell is by definition something that’ s quick to spot—or snif fable as\nI’ve recently put it. A long method is a good example of this—just\nlooking at the code and my nose twitches if I see more than a dozen\nlines of Java.\nThe second is that smells don’ t always indicate a problem. Some long\nmethods are just fine. Y ou have to look deeper to see if there is an\nunderlying problem there—smells aren’ t inherently bad on their own—\nthey are often an indicator of a problem rather than the problem\nthemselves.\nThe best smells are something that’ s easy to spot and most of time lead\nyou to really interesting problems. Data classes (classes with all data\nand no behavior) are good examples of this. Y ou look at them and ask\nyourself what behavior should be in this class. Then you start\nrefactoring to move that behavior in there. Often simple questions and\ninitial refactorings can be the vital step in turning anemic objects into\nsomething that really has class.\nOne of the nice things about smells is that it’ s easy for inexperienced\npeople to spot them, even if they don’ t know enough to evaluate if\nthere’ s a real problem or to correct them. I’ve heard of lead developers\nwho will pick a “smell of the week” and ask people to look for the\nsmell and bring it up with the senior members of the team. Doing it one\nsmell at a time is a good way of gradually teaching people on the team\nto be better programmers.",3389
82-Data class as scaffolding.pdf,82-Data class as scaffolding,,0
83-Pattern Matching Class Instances.pdf,83-Pattern Matching Class Instances,"The main idea of Object Oriented Programming is to place behavior and\ndata together in the same code unit: a class. If a class is widely used but has\nno significant behavior of its own, it’ s possible that code dealing with its\ninstances is scattered (and even duplicated) in methods and functions\nthroughout the system—a recipe for maintenance headaches. That’ s why\nFowler ’ s refactorings to deal with a data class involve bringing\nresponsibilities back into it.\nT aking that into account, there are a couple of common scenarios where it\nmakes sense to have a data class with little or no behavior .\nData class as scaffolding\nIn this scenario, the data class is an initial, simplistic implementation of a\nclass to jump start a new project or module. W ith time, the class should get\nits own methods, instead of relying on methods of other classes to operate\non its instances. Scaf folding is temporary; eventually your custom class\nmay become fully independent from the builder you used to start it.\nPython is also used for quick problem solving and experimentation, and\nthen it’ s OK to leave the scaf folding in place.\nData class as intermediate representation\nA data class can be useful to build records about to be exported to JSON or\nsome other interchange format, or to hold data that was just imported,\ncrossing some system boundary . Python’ s data class builders all provide a\nmethod or function to convert an instance to a plain dict , and you can\nalways invoke the constructor with a dict  used as keyword ar guments\nexpanded with ** . Such a dict  is very close to a JSON record.\nIn this scenario, the data class instances should be handled as immutable\nobjects—even if the fields are mutable, you should not change them while\nthey are in this intermediate form. If you do, you’re losing the key benefit\nof having data and behavior close together . When importing/exporting",1915
84-Keyword Class Patterns.pdf,84-Keyword Class Patterns,"requires changing values, you should implement your own builder methods\ninstead of using the given “as dict” methods or standard constructors.\nNow we change the subject to see how to write patterns that match\ninstances of arbitrary classes, and not just the sequences and mappings\nwe’ve seen in the pattern matching sections of Chapter 2  and Chapter 3 .\nP a t t e r n  M a t c h i n g  C l a s s  I n s t a n c e s\nClass patterns are designed to match class instances by type and—\noptionally—by attributes. The subject of a class pattern can be any class\ninstance, not only instances of data classes.\nThere are three variations of class patterns: simple, keyword, and positional.\nW e’ll study them in that order .\nSimple Class Patterns\nW e’ve already seen an example with simple class patterns used as\nsubpatterns in “Pattern Matching with Sequences” :\n        case [str(name), _, _, (float(lat), float(lon))]:\nThat pattern matches a 4-item sequence where the first item must be an\ninstance of str , and the last item must be a 2-tuple with two instances of\nfloat .\nThe syntax for class patterns looks like a constructor invocation. Below is a\nclass pattern which matches float  values, without binding a variable (the\ncase body can refer to x  directly if needed):\n    match x: \n        case float(): \n            do_something_with (x)\nBut this is likely to be a bug in your code:10\n    match x: \n        case float:  # DANGER!!!  \n            do_something_with (x)\nIn the example above, case float:  matches any subject, because\nPython sees float  as a variable, which is then bound to the subject.\nThe simple pattern syntax of float()  or float(x)  is a special case that\napplies only to nine blessed built-in types, listed at the end of the Class\npatterns  section of PEP 634—Structural Pattern Matching: Specification :\nbytes   dict   float   frozenset   int   list   set   str   tuple\nIn those classes, the variable that looks like a constructor ar gument—e.g. x\nin float(x) —is bound to the whole subject instance or the part of the\nsubject that matches a subpattern, as exemplified by str(name)  in the\nsequence pattern we saw earlier:\n        case [str(name), _, _, (float(lat), float(lon))]:\nIf the class is not one of those nine blessed built-ins, then the ar gument-like\nvariables or constants represent dif ferent attributes of the class, as if they\nwere keyword ar guments or positional ar guments.\nKeyword Class Patterns\nT o understand how to use keyword class patterns, consider the following\nCity  class and five instances:\nExample 5-22. City class and a few instances.\nimport typing \n \nclass City(typing.NamedTuple ): \n    continent : str \n    name: str \n    country: str \n \n \ncities = [ \n    City('Asia', 'Tokyo', 'JP'), \n    City('Asia', 'Delhi', 'IN'), \n    City('North America' , 'Mexico City' , 'MX'), \n    City('North America' , 'New York' , 'US'), \n    City('South America' , 'São Paulo' , 'BR'), \n]\nGiven those definitions the following function would return a list of Asian\ncities:\ndef match_asian_cities (): \n    results = [] \n    for city in cities: \n        match city: \n            case City(continent ='Asia'): \n                results.append(city) \n    return results\nThe pattern City(continent='Asia')  matches any City  instance\nwhere the continent  attribute value is equal to 'Asia' , regardless of\nthe values of the other attributes.\nIf you want to collect the value of the country attribute, you could write:\ndef match_asian_countries (): \n    results = [] \n    for city in cities: \n        match city: \n            case City(continent ='Asia', country=cc): \n                results.append(cc) \n    return results\nThe pattern City(continent='Asia', country=cc)  matches the\nsame Asian cities as before, but now the cc  variable is bound to the\ncountry  attribute of the instance. This also works if the pattern variable is\ncalled country  as well:\n        match city: \n            case City(continent ='Asia', country=country): \n                results.append(country)",4095
85-Chapter Summary.pdf,85-Chapter Summary,"Keyword class patterns are very readable, and work with any class that has\npublic instance attributes, but they are somewhat verbose.\nPositional class patterns are more convenient in some cases, but they\nrequire explicit support by the class of the subject, as we’ll see next.\nPositional Class Patterns\nGiven the definitions from Example 5-22 , the following function would\nreturn a list of Asian cities, using a positional class pattern:\ndef match_asian_cities_pos (): \n    results = [] \n    for city in cities: \n        match city: \n            case City('Asia'): \n                results.append(city) \n    return results\nThe pattern City('Asia')  matches any City  instance where the first\nattribute value is 'Asia' , regardless of the values of the other attributes.\nIf you want to collect the value of the country attribute, you could write:\ndef match_asian_countries_pos (): \n    results = [] \n    for city in cities: \n        match city: \n            case City('Asia', _, country): \n                results.append(country) \n    return results\nThe pattern City('Asia', _, country)  matches the same cities as\nbefore, but now the country  variable is bound to the third attribute of the\ninstance.\nI’ve mentioned “first” or “third” attribute, but what does that really mean?\nWhat makes City  or any class work with positional patterns is the\npresence of a special class attribute named __match_args__ , which the\nclass builders in this chapter automatically create. This is value of\n__match_args__  in the City  class:\n>>> City.__match_args__  \n('continent', 'name', 'country')\nAs you can see, __match_args__  declares the names of the attributes in\nthe order they will be used in positional patterns.\nIn Chapter 1 1  we’ll write code to define __match_args__  for a class\nwe’ll create without the help of a class builder .\nT I P\nY ou can combine keyword and positional ar guments in a pattern. Some but not all of the\ninstance attributes available for matching may be listed in __match_args__ .\nTherefore, sometimes you may need to use keyword ar guments in addition to positional\nar guments in a pattern.\nT ime for a chapter summary .\nC h a p t e r  S u m m a r y\nThe main topic of this chapter were the data class builders\ncollections.namedtuple , typing.NamedTuple  and\ndataclasses.dataclass . W e saw that each of them generate data\nclasses from descriptions provided as ar guments to a factory function or\nfrom class  statements with type hints—in the case of the latter two. In\nparticular , both named tuple variants produce tuple  subclasses, adding\nonly the ability to access fields by name, and providing a _fields  class\nattribute listing the field names as a tuple of strings.\nNext we studied the main features of the three class builders side by side,\nincluding how to extract instance data as a dict , how to get the names and\ndefault values of fields, and how to make a new instance from an existing\none.\nThis prompted our first look into type hints, particularly those used to\nannotate attributes in a class  statement, using the notation introduced in\nPython 3.6 with PEP 526—Syntax for V ariable Annotations . Probably the\nmost surprising aspect of type hints in general is the fact that they have no\nef fect at all at runtime. Python remains a dynamic language. External tools,\nlike Mypy , are needed to take advantage of typing information to detect\nerrors via static analysis of the source code. After a basic overview of the\nsyntax from PEP 526, we studied the ef fect of annotations in a plain class\nand in classes built by typing.NamedTuple  and @dataclass .\nNext we covered the most commonly used features provided by\n@dataclass  and the default_factory  option of the\ndataclasses.field  function. W e also looked into the special pseudo-\ntype hints typing.ClassVar  and dataclasses.InitVar  that are\nimportant in the context of data classes. This main topic concluded with an\nexample based on the Dublin Core Schema, which illustrated how to use\ndataclasses.fields  to iterate over the attributes of a Resource\ninstance in a custom __repr__ .",4145
86-Further Reading.pdf,86-Further Reading,"“Data class as a code smell”  came after that, warning against possible abuse\nof data classes defeating a basic principle of Object Oriented Programming:\ndata and the functions that touch it should be together in the same class.\nClasses with no logic may be a sign of misplaced logic.\nIn the last section, we saw how pattern matching works with subjects that\nare instances of any class—not just classes built with the tools presented in\nthis chapter .\nF u r t h e r  R e a d i n g\nPython’ s standard documentation for the data class builders we covered is\nvery good, and has quite a few small examples.\nFor @dataclass  in particular , most of PEP 557—Data Classes  was\ncopied into the dataclasses  module documentation. But PEP 557  has a\nfew very informative sections that were not copied, including Why not just\nuse namedtuple? , Why not just use typing.NamedT uple?  and the Rationale\nsection  which concludes with this Q&A:\nWher e is it not appr opriate to use Data Classes?\nAPI compatibility with tuples or dicts is r equir ed. T ype validation beyond\nthat pr ovided by PEPs 484 and 526 is r equir ed, or value validation or\nconversion is r equir ed.\n— Eric V . Smith, PEP 557 Rationale\nOver at RealPython.com , Geir Arne Hjelle wrote a very complete Ultimate\nGuide to Data Classes in Python 3.7 .\nAt PyCon US 2018, Raymond Hettinger presented Dataclasses: The code\ngenerator to end all code generators  (video).\nFor more features and advanced functionality , including validation, the attrs\nproject  led by Hynek Schlawack appeared years before dataclasses ,\nand of fers more features, promising to “bring back the joy of writing classes\nby relieving you from the drudgery of implementing object protocols (aka\ndunder methods).” The influence of attrs  on @dataclass  is\nacknowledged by Eric V . Smith in PEP 557. This probably includes Smith’ s\nmost important API decision: the use of a class decorator instead of a base\nclass and/or a metaclass to do the job.\nGlyph—founder of the T wisted project—wrote an excellent introduction to\nattrs  in The One Python Library Everyone Needs . The attrs  documentation\nincludes a discussion of alternatives .\nBook author , instructor , and mad computer scientist Dave Beazley wrote\ncluegen , yet another data class generator . If you’ve seen any of Dave’ s\ntalks, you know he is a master of metaprograming Python from first\nprinciples. So, I found it inspiring to learn from the cluegen  README.md\nfile the concrete use case that motivated him to write an alternative to\nPython’ s @dataclass , and his philosophy of presenting an approach to\nsolve the problem, in contrast to providing a tool: the tool may be quicker to\nuse at first, but the approach is more flexible and can take you as far as you\nwant to go.\nRegarding Data Class  as a code smell, the best source I found was Martin\nFowler ’ s book Refactoring, Second Edition . This newest version is missing\nthe quote from the epigraph of this chapter , “Data classes are like\nchildren…”, but otherwise it’ s the best edition of Fowler ’ s most famous\nbook, particularly for Pythonistas because the examples are in modern\nJavaScript, which is closer to Python than Java—the language of the first\nedition.\nThe W eb site Refactoring Guru  also has a description of the Data Class\ncode smell.\nS O A P B O X\nThe entry for “Guido”  in the Jar gon file is about Guido van Rossum. It\nsays, among other things:\nMythically , Guido’ s most important attribute besides Python itself is\nGuido’ s time machine, a device he is r eputed to possess because of\nthe unnerving fr equency with which user r equests for new featur es\nhave been met with the r esponse “I just implemented that last\nnight…”\nFor the longest time, one of the missing pieces in Python’ s syntax has\nbeen a quick, standard way to declare instance attributes in a class.\nMany Object-Oriented languages have that. Here is part of a Point\nclass definition in Smalltalk:\nObject subclass: #Point  \n    instanceVariableNames: 'x y'  \n    classVariableNames: ''  \n    package: 'Kernel-BasicObjects'\nThe second line lists the names of the instance attributes x  and y . If\nthere were class attributes, they would be in the third line.\nPython has always of fered an easy way to declare class attributes, if\nthey have an initial value. But instance attributes are much more\ncommon, and Python coders have been forced to look into the\n__init__  method to find them, always afraid that there may be\ninstance attributes created elsewhere in the class—or even created by\nexternal functions or methods of other classes.\nNow we have @dataclass , yay!\nBut they bring their own problems.\nFirst: when you use @dataclass , type hints are not optional. W e’ve\nbeen promised for the last 7 years since PEP 484—T ype Hints  that they\nwould always be optional. Now we have a major new language feature\nthat requires them. If you don’ t like the whole static typing trend, you\nmay want to use attrs  instead.\nSecond: the PEP 526  syntax for annotating instance and class attributes\nreverses the established convention of class  statements: everything\ndeclared at the top-level of a class  block was a class attribute\n(methods are class attributes too). W ith PEP 526 and @dataclass ,\nany attribute declared at the top level with a type hint becomes an\ninstance attribute:\n    @dataclass  \n    class Spam: \n        repeat: int  # instance attribute\nBelow , repeat  is also an instance attribute:\n    @dataclass  \n    class Spam: \n        repeat: int = 99  # instance attribute\nBut if there are no type hints, suddenly you are back in the good old\ntimes when declarations at the top-level of the class belong to the class\nonly:\n    @dataclass  \n    class Spam: \n        repeat = 99  # class attribute!\nFinally , if you want to annotate that class attribute with a type, you\ncan’ t use regular types because then it will become an instance attribute.\nY ou must resort to that pseudo-type ClassVar  annotation:\n    @dataclass  \n    class Spam: \n        repeat: ClassVar [int] = 99  # aargh!\nHere we are talking about the exception to the exception to the rule.\nThis seems rather unpythonic to me.\nI did not take part in the discussions leading to PEP 526 or PEP 557—\nData Classes , but here is an alternative syntax that I’d like to see:\n@dataclass  \nclass HackerClubMember : \n    .name: str                                   \n  \n    .guests: list = field(default_factory =list) \n    .handle: str = '' \n \n    all_handles  = set()                          \nInstance attributes must be declared with a .  prefix.\nAny attribute name that doesn’ t have a .  prefix is a class attribute\n(as they always have been).\nThe language grammar would have to change to accept that. I find this\nquite readable, and it avoids the exception-to-the-exception issue.\nI wish I could borrow Guido’ s time machine to go back to 2017 and sell\nthis idea to the core team.\n1  From Refactoring, First Edition , chapter 3, Bad Smells in Code , Data Class  section, page 87.\n2  Metaclasses are one of the subjects covered in Chapter 25 — Class Metapr ogramming .\n3  Class decorators are covered in Chapter 25 — Class Metapr ogramming , along with\nmetaclasses. Both are ways of customizing class behavior beyond what is possible with\ninheritance.\n4  If you know Ruby , you know that injecting methods is a well-known but controversial\ntechnique among Rubyists. In Python, it’ s not as common, because it doesn’ t work with any\nbuilt-in type— str , list , etc. I consider this limitation of Python a blessing.\n5  In the context of type hints, None  is not the NoneType  singleton, but an alias for\nNoneType  itself. This is strange when we stop to think about it, but appeals to our intuition\nand makes function return annotations easier to read in the common case of functions that\nreturn None .\n6  Python has no concept of undefined , one of the silliest mistakes in the design of JavaScript.\nThank Guido!\n7  However , almost always when I see this in real code it’ s a bad idea. I once spent hours\nchasing a bug that was caused by attributes sneakily stashed in instances, like contraband\nacross module borders. Also, setting an attribute after __init__  defeats the __dict__  key-\nsharing memory optimization mentioned in “Practical Consequences of How dict W orks” .\n8  Source: Dublin Core  article in the English W ikipedia.\n9  I am fortunate to have Martin Fowler as a colleague at Thoughtworks, so it took just 20\nminutes to get his permission.\n10  I put this content here because it is the earliest chapter focusing on user -defined classes, and I\nthought pattern matching with classes was too important to wait until part III of the book. My\nphilosophy: it’ s more important to know how to use classes than to define classes.",8885
87-Variables Are Not Boxes.pdf,87-Variables Are Not Boxes,"Chapter 6. Object References,\nMutability , and Recycling\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 6th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\n‘Y ou ar e sad,’ the Knight said in an anxious tone: ‘let me sing you a song\nto comfort you. […] The name of the song is called “HADDOCKS’\nEYES”.’\n‘Oh, that’ s the name of the song, is it?’ Alice said, trying to feel\ninter ested.\n‘No, you don’ t understand,’ the Knight said, looking a little vexed. ‘That’ s\nwhat the name is CALLED. The name r eally IS “THE AGED AGED\nMAN.""’ (adapted fr om Chapter VIII. ‘It’ s my own Invention’).\n— Lewis Carroll, Through the Looking-Glass, and What\nAlice Found There\nAlice and the Knight set the tone of what we will see in this chapter . The\ntheme is the distinction between objects and their names. A name is not the\nobject; a name is a separate thing.\nW e start the chapter by presenting a metaphor for variables in Python:\nvariables are labels, not boxes. If reference variables are old news to you,\nthe analogy may still be handy if you need to explain aliasing issues to\nothers.\nW e then discuss the concepts of object identity , value, and aliasing. A\nsurprising trait of tuples is revealed: they are immutable but their values\nmay change. This leads to a discussion of shallow and deep copies.\nReferences and function parameters are our next theme: the problem with\nmutable parameter defaults and the safe handling of mutable ar guments\npassed by clients of our functions.\nThe last sections of the chapter cover garbage collection, the del\ncommand, and a selection of tricks that Python plays with immutable\nobjects.\nThis is a rather dry chapter , but its topics lie at the heart of many subtle\nbugs in real Python programs.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThe topics covered here are very fundamental and stable. There were no\nchanges worth mentioning in this Second Edition .\nI added an example of using is  to test for a sentinel object, and a warning\nabout misuses of the is  operator at the end of “Choosing Between == and\nis ” .\nThis chapter used to be in Part IV , but I decided to bring it up earlier\nbecause it works better as an ending to Part II— Data Structur es —than an\nopening to Object-Oriented Idioms .\nN O T E\nThe section on W eak Refer ences  from the First Edition  is now a post  at\nfluentpython.com .\nLet’ s start by unlearning that a variable is like a box where you store data.\nV a r i a b l e s  A r e  N o t  B o x e s\nIn 1997, I took a summer course on Java at MIT . The professor , L ynn Stein\nmade the point that the usual “variables as boxes” metaphor actually\nhinders the understanding of reference variables in OO languages. Python\nvariables are like reference variables in Java, a better metaphor is to think\nof variables as labels with names attached to objects. The next example and\nfigure will help you understand why .\nExample 6-1  is a simple interaction that the “variables as boxes” idea\ncannot explain. Figure 6-1  illustrates why the box metaphor is wrong for\nPython, while sticky notes provide a helpful picture of how variables\nactually work.\nExample 6-1. V ariables a and b hold r efer ences to the same list, not copies\nof the list\n>>> a = [1, 2, 3]  \n \n>>> b = a          \n  \n>>> a.append(4)    \n \n>>> b              \n  \n[1, 2, 3, 4]\nCreate a list [1, 2, 3]  and bind the variable a  to it\nBind the variable b  to the same value that a  is referencing.\nModify the list referenced by a , by appending another item.\nY ou can see the ef fect via the b  variable. If we think of b  as box that\nstored a copy of the [1, 2, 3]  from the a  box, this behavior is\nmakes no sense.1\nFigur e 6-1. If you imagine variables ar e like boxes, you can’ t make sense of assignment in Python;\ninstead, think of variables as sticky notes— Example 6-1  then becomes easy to explain\nTherefore, the b = a  statement does not copy the contents of box a  into\nbox b . It attaches the label b  to the object that already has the label a .\nProf. Stein also spoke about assignment in a very deliberate way . For\nexample, when talking about a seesaw object in a simulation, she would\nsay: “V ariable s  is assigned to the seesaw ,” but never “The seesaw is\nassigned to variable s .” W ith reference variables, it makes much more sense\nto say that the variable is assigned to an object, and not the other way\naround. After all, the object is created before the assignment. Example 6-2\nproves that the right-hand side of an assignment happens first.\nSince the verb “to assign” is used in contradictory ways, a useful alternative\nis “to bind”: Python’ s assignment statement x = …  binds the x  name to the\nobject created or referenced on the right-hand side. And the object must\nexist before a name can be bound to it, as Example 6-2  proves.\nExample 6-2. V ariables ar e bound to objects only after the objects ar e\ncr eated.\n>>> class Gizmo: \n...    def __init__ (self): \n...         print(f'Gizmo id: {id(self)} ') \n... \n>>> x = Gizmo() \nGizmo id: 4301489152  \n  \n>>> y = Gizmo() * 10  \n \nGizmo id: 4301489432  \n  \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : unsupported operand type(s) for *: 'Gizmo' and 'int'  \n>>> \n>>> dir()  \n \n['Gizmo', '__builtins__', '__doc__', '__loader__', '__name__',  \n'__package__', '__spec__', 'x']\nThe output Gizmo id: …  is a side ef fect of creating a Gizmo\ninstance.\nMultiplying a Gizmo  instance will raise an exception.\nHere is proof that a second Gizmo  was actually instantiated before the\nmultiplication was attempted.\nBut variable y  was never created, because the exception happened while\nthe right-hand side of the assignment was being evaluated.\nT I P\nT o understand an assignment in Python, read the right-hand side first: that’ s where the\nobject is created or retrieved. After that, the variable on the left is bound to the object,\nlike a label stuck to it. Just for get about the boxes.",6542
88-Identity Equality and Aliases.pdf,88-Identity Equality and Aliases,"Because variables are mere labels, nothing prevents an object from having\nseveral labels assigned to it. When that happens, you have aliasing , our next\ntopic.\nI d e n t i t y ,  E q u a l i t y ,  a n d  A l i a s e s\nLewis Carroll is the pen name of Prof. Charles Lutwidge Dodgson. Mr .\nCarroll is not only equal to Prof. Dodgson: they are one and the same.\nExample 6-3  expresses this idea in Python.\nExample 6-3. charles and lewis r efer to the same object\n>>> charles = {'name': 'Charles L. Dodgson ', 'born': 1832} \n>>> lewis = charles  \n \n>>> lewis is charles \nTrue \n>>> id(charles), id(lewis)  \n \n(4300473992, 4300473992)  \n>>> lewis['balance'] = 950  \n \n>>> charles \n{'name': 'Charles L. Dodgson', 'born': 1832, 'balance': 950}\nlewis  is an alias for charles .\nThe is  operator and the id  function confirm it.\nAdding an item to lewis  is the same as adding an item to charles .\nHowever , suppose an impostor—let’ s call him Dr . Alexander Pedachenko—\nclaims he is Charles L. Dodgson, born in 1832. His credentials may be the\nsame, but Dr . Pedachenko is not Prof. Dodgson. Figure 6-2  illustrates this\nscenario.\nFigur e 6-2. charles and lewis ar e bound to the same object; alex is bound to a separate object of\nequal value.\nExample 6-4  implements and tests the alex  object depicted in Figure 6-2 .\nExample 6-4. alex and charles compar e equal, but alex is not  charles\n>>> alex = {'name': 'Charles L. Dodgson ', 'born': 1832, 'balance': \n950}  \n \n>>> alex == charles  \n \nTrue \n>>> alex is not charles  \n \nTrue\nalex  refers to an object that is a replica of the object assigned to\ncharles .\nThe objects compare equal, because of the __eq__  implementation in\nthe dict  class.\nBut they are distinct objects. This is the Pythonic way of writing the\nnegative identity comparison: a is not b .\nExample 6-3  is an example of aliasing . In that code, lewis  and charles\nare aliases: two variables bound to the same object. On the other hand,\nalex  is not an alias for charles : these variables are bound to distinct\nobjects. The objects bound to alex  and charles  have the same value —\nthat’ s what ==  compares—but they have dif ferent identities.\nIn The Python Language Refer ence , “3.1. Objects, values and types”  states:\nAn object’ s identity never changes once it has been cr eated; you may\nthink of it as the object’ s addr ess in memory . The is  operator compar es\nthe identity of two objects; the id()  function r eturns an integer\nr epr esenting its identity .\nThe real meaning of an object’ s ID is implementation-dependent. In\nCPython, id()  returns the memory address of the object, but it may be\nsomething else in another Python interpreter . The key point is that the ID is\nguaranteed to be a unique integer label, and it will never change during the\nlife of the object.\nIn practice, we rarely use the id()  function while programming. Identity\nchecks are most often done with the is  operator , which compares the\nobject IDs, so our code doesn’ t need to call id()  explicitly . Next, we’ll\ntalk about is  versus == .\nT I P\nFor tech reviewer Leonardo Rochael, the most frequent use for id()  is while\ndebugging, when the repr()  of two objects look alike, but you need to understand\nwhether two references are aliases or point to separate objects. If the references are in\ndif ferent contexts—such as dif ferent stack frames—using the is  operator may not be\nviable.",3461
89-Copies Are Shallow by Default.pdf,89-Copies Are Shallow by Default,"Choosing Between == and is\nThe ==  operator compares the values of objects (the data they hold), while\nis  compares their identities.\nWhile programming, we often care more about values and than object\nidentities, so ==  appears more frequently than is  in Python code.\nHowever , if you are comparing a variable to a singleton, then it makes sense\nto use is . By far , the most common case is checking whether a variable is\nbound to None . This is the recommended way to do it:\nx is None\nAnd the proper way to write its negation is:\nx is not None\nNone  is the most common singleton we test with is . Sentinel objects are\nanother example of singletons we test with is . Here is one way to create\nand test a sentinel object:\nEND_OF_DATA  = object() \n# ... many lines  \ndef traverse (...): \n    # ... more lines  \n    if node is END_OF_DATA : \n        return \n    # etc.\nThe is  operator is faster than == , because it cannot be overloaded, so\nPython does not have to find and invoke special methods to evaluate it, and\ncomputing is as simple as comparing two integer IDs. In contrast, a == b\nis syntactic sugar for a.__eq__(b) . The __eq__  method inherited from\nobject  compares object IDs, so it produces the same result as is . But\nmost built-in types override __eq__  with more meaningful\nimplementations that actually take into account the values of the object\nattributes. Equality may involve a lot of processing—for example, when\ncomparing lar ge collections or deeply nested structures.\nW A R N I N G\nUsually we are more interested in object equality than identity . Checking for None  is\nthe only  common use case for the is  operator . Most other uses I see while reviewing\ncode are wrong. If you are not sure, use == . It’ s usually what you want, and also works\nwith None —albeit not as fast.\nT o wrap up this discussion of identity versus equality , we’ll see that the\nfamously immutable tuple  is not as unchanging as you may expect.\nThe Relative Immutability of T uples\nT uples, like most Python collections—lists, dicts, sets, etc.—are containers:\nthey hold references to objects.  If the referenced items are mutable, they\nmay change even if the tuple itself does not. In other words, the\nimmutability of tuples really refers to the physical contents of the tuple\ndata structure (i.e., the references it holds), and does not extend to the\nreferenced objects.\nExample 6-5  illustrates the situation in which the value of a tuple changes\nas result of changes to a mutable object referenced in it. What can never\nchange in a tuple is the identity of the items it contains.\nExample 6-5. t1 and t2 initially compar e equal, but changing a mutable\nitem inside tuple t1 makes it differ ent\n>>> t1 = (1, 2, [30, 40])  \n \n>>> t2 = (1, 2, [30, 40])  \n \n>>> t1 == t2  \n \nTrue \n>>> id(t1[-1])  \n \n4302515784  \n>>> t1[-1].append(99)  \n \n>>> t1 \n(1, 2, [30, 40, 99])  \n>>> id(t1[-1])  \n 2\n4302515784  \n>>> t1 == t2  \n \nFalse\nt1  is immutable, but t1[-1]  is mutable.\nBuild a tuple t2  whose items are equal to those of t1 .\nAlthough distinct objects, t1  and t2  compare equal, as expected.\nInspect the identity of the list at t1[-1] .\nModify the t1[-1]  list in place.\nThe identity of t1[-1]  has not changed, only its value.\nt1  and t2  are now dif ferent.\nThis relative immutability of tuples is behind the riddle “A += Assignment\nPuzzler” . It’ s also the reason why some tuples are unhashable, as we’ve\nseen in “What is Hashable” .\nThe distinction between equality and identity has further implications when\nyou need to copy an object. A copy is an equal object with a dif ferent ID.\nBut if an object contains other objects, should the copy also duplicate the\ninner objects, or is it OK to share them? There’ s no single answer . Read on\nfor a discussion.\nC o p i e s  A r e  S h a l l o w  b y  D e f a u l t\nThe easiest way to copy a list (or most built-in mutable collections) is to use\nthe built-in constructor for the type itself. For example:\n>>> l1 = [3, [55, 44], (7, 8, 9)] \n>>> l2 = list(l1)  \n \n>>> l2 \n[3, [55, 44], (7, 8, 9)]  \n>>> l2 == l1  \n \nTrue \n>>> l2 is l1  \n \nFalse\nlist(l1)  creates a copy of l1 .\nThe copies are equal.\nBut refer to two dif ferent objects.\nFor lists and other mutable sequences, the shortcut l2 = l1[:]  also\nmakes a copy .\nHowever , using the constructor or [:]  produces a shallow copy  (i.e., the\noutermost container is duplicated, but the copy is filled with references to\nthe same items held by the original container). This saves memory and\ncauses no problems if all the items are immutable. But if there are mutable\nitems, this may lead to unpleasant surprises.\nIn Example 6-6 , we create a shallow copy of a list containing another list\nand a tuple, and then make changes to see how they af fect the referenced\nobjects.\nT I P\nIf you have a connected computer on hand, I highly recommend watching the interactive\nanimation for Example 6-6  at the Online Python T utor . As I write this, direct linking to\na prepared example at pythontutor .com  is not working reliably , but the tool is awesome,\nso taking the time to copy and paste the code is worthwhile.\nFigur e 6-3. Pr ogram state immediately after the assignment l2 = list(l1) in Example 6-6 . l1 and l2\nr efer to distinct lists, but the lists shar e r efer ences to the same inner list object [66, 55, 44] and tuple\n(7, 8, 9). (Diagram generated by the Online Python T utor .)\nExample 6-6. Making a shallow copy of a list containing another list; copy\nand paste this code to see it animated at the Online Python T utor\nl1 = [3, [66, 55, 44], (7, 8, 9)] \nl2 = list(l1)      \n \nl1.append(100)     \n \nl1[1].remove(55)   \n \nprint('l1:', l1) \nprint('l2:', l2) \nl2[1] += [33, 22]  \n \nl2[2] += (10, 11)  \n \nprint('l1:', l1) \nprint('l2:', l2)\nl2  is a shallow copy of l1 . This state is depicted in Figure 6-3 .\nAppending 100  to l1  has no ef fect on l2 .\nHere we remove 55  from the inner list l1[1] . This af fects l2  because\nl2[1]  is bound to the same list as l1[1] .\nFor a mutable object like the list referred by l2[1] , the operator +=\nchanges the list in place. This change is visible at l1[1] , which is an\nalias for l2[1] .\n+=  on a tuple creates a new tuple and rebinds the variable l2[2]  here.\nThis is the same as doing l2[2] = l2[2] + (10, 11) . Now the\ntuples in the last position of l1  and l2  are no longer the same object.\nSee Figure 6-4 .\nThe output of Example 6-6  is Example 6-7 , and the final state of the objects\nis depicted in Figure 6-4 .\nExample 6-7. Output of Example 6-6\nl1: [3, [66, 44], (7, 8, 9), 100] \nl2: [3, [66, 44], (7, 8, 9)] \nl1: [3, [66, 44, 33, 22], (7, 8, 9), 100] \nl2: [3, [66, 44, 33, 22], (7, 8, 9, 10, 11)]\nFigur e 6-4. Final state of l1 and l2: they still shar e r efer ences to the same list object, now containing\n[66, 44, 33, 22], but the operation l2[2] += (10, 1 1) cr eated a new tuple with content (7, 8, 9, 10,\n1 1), unr elated to the tuple (7, 8, 9) r efer enced by l1[2]. (Diagram generated by the Online Python\nT utor .)",7137
90-Deep and Shallow Copies of Arbitrary Objects.pdf,90-Deep and Shallow Copies of Arbitrary Objects,"It should be clear now that shallow copies are easy to make, but they may\nor may not be what you want. How to make deep copies is our next topic.\nDeep and Shallow Copies of Arbitrary Objects\nW orking with shallow copies is not always a problem, but sometimes you\nneed to make deep copies (i.e., duplicates that do not share references of\nembedded objects). The copy  module provides the deepcopy  and copy\nfunctions that return deep and shallow copies of arbitrary objects.\nT o illustrate the use of copy()  and deepcopy() , Example 6-8  defines a\nsimple class, Bus , representing a school bus that is loaded with passengers\nand then picks up or drops of f passengers on its route.\nExample 6-8. Bus picks up and dr ops off passengers\nclass Bus: \n \n    def __init__ (self, passengers =None): \n        if passengers  is None: \n            self.passengers  = [] \n        else: \n            self.passengers  = list(passengers ) \n \n    def pick(self, name): \n        self.passengers .append(name) \n \n    def drop(self, name): \n        self.passengers .remove(name)\nNow in the interactive Example 6-9  we will create a bus  object (bus1 )\nand two clones—a shallow copy ( bus2 ) and a deep copy ( bus3 )—to\nobserve what happens as bus1  drops of f a student.\nExample 6-9. Effects of using copy versus deepcopy\n>>> import copy \n>>> bus1 = Bus(['Alice', 'Bill', 'Claire', 'David']) \n>>> bus2 = copy.copy(bus1) \n>>> bus3 = copy.deepcopy (bus1) \n>>> id(bus1), id(bus2), id(bus3) \n(4301498296, 4301499416, 4301499752)  \n  \n>>> bus1.drop('Bill') \n>>> bus2.passengers  \n['Alice', 'Claire', 'David']          \n  \n>>> id(bus1.passengers ), id(bus2.passengers ), id(bus3.passengers ) \n(4302658568, 4302658568, 4302657800)  \n  \n>>> bus3.passengers  \n['Alice', 'Bill', 'Claire', 'David']  \nUsing copy  and deepcopy , we create three distinct Bus  instances.\nAfter bus1  drops 'Bill' , he is also missing from bus2 .\nInspection of the passengers  attributes shows that bus1  and bus2\nshare the same list object, because bus2  is a shallow copy of bus1 .\nbus3  is a deep copy of bus1 , so its passengers  attribute refers to\nanother list.\nNote that making deep copies is not a simple matter in the general case.\nObjects may have cyclic references that would cause a naïve algorithm to\nenter an infinite loop. The deepcopy  function remembers the objects\nalready copied to handle cyclic references gracefully . This is demonstrated\nin Example 6-10 .\nExample 6-10. Cyclic r efer ences: b r efers to a, and then is appended to a;\ndeepcopy still manages to copy a\n>>> a = [10, 20] \n>>> b = [a, 30] \n>>> a.append(b) \n>>> a \n[10, 20, [[...], 30]]  \n>>> from copy import deepcopy  \n>>> c = deepcopy (a) \n>>> c \n[10, 20, [[...], 30]]\nAlso, a deep copy may be too deep in some cases. For example, objects\nmay refer to external resources or singletons that should not be copied. Y ou\ncan control the behavior of both copy  and deepcopy  by implementing\nthe __copy__()  and __deepcopy__()  special methods as described\nin the copy  module documentation .",3092
91-Mutable Types as Parameter Defaults Bad Idea.pdf,91-Mutable Types as Parameter Defaults Bad Idea,"The sharing of objects through aliases also explains how parameter passing\nworks in Python, and the problem of using mutable types as parameter\ndefaults. These issues will be covered next.\nF u n c t i o n  P a r a m e t e r s  a s  R e f e r e n c e s\nThe only mode of parameter passing in Python is call by sharing . That is\nthe same mode used in most object oriented languages, including\nJavaScript, Ruby , and Java (this applies to Java reference types; primitive\ntypes use call by value). Call by sharing means that each formal parameter\nof the function gets a copy of each reference in the ar guments. In other\nwords, the parameters inside the function become aliases of the actual\nar guments.\nThe result of this scheme is that a function may change any mutable object\npassed as a parameter , but it cannot change the identity of those objects\n(i.e., it cannot altogether replace an object with another). Example 6-1 1\nshows a simple function using +=  on one of its parameters. As we pass\nnumbers, lists, and tuples to the function, the actual ar guments passed are\naf fected in dif ferent ways. The next example demonstrates:\nExample 6-1 1. A function may change any mutable object it r eceives\n>>> def f(a, b): \n...     a += b \n...     return a \n... \n>>> x = 1 \n>>> y = 2 \n>>> f(x, y) \n3 \n>>> x, y  \n \n(1, 2) \n>>> a = [1, 2] \n>>> b = [3, 4] \n>>> f(a, b) \n[1, 2, 3, 4]  \n>>> a, b  \n \n([1, 2, 3, 4], [3, 4])  \n>>> t = (10, 20) \n>>> u = (30, 40) \n>>> f(t, u)  \n \n(10, 20, 30, 40)  \n>>> t, u \n((10, 20), (30, 40))\nThe number x  is unchanged.\nThe list a  is changed.\nThe tuple t  is unchanged.\nAnother issue related to function parameters is the use of mutable values for\ndefaults, as discussed next.\nMutable T ypes as Parameter Defaults: Bad Idea\nOptional parameters with default values are a great feature of Python\nfunction definitions, allowing our APIs to evolve while remaining\nbackward-compatible. However , you should avoid mutable objects as\ndefault values for parameters.\nT o illustrate this point, in Example 6-12 , we take the Bus  class from\nExample 6-8  and change its __init__  method to create HauntedBus .\nHere we tried to be clever and instead of having a default value of\npassengers=None , we have passengers=[] , thus avoiding the if\nin the previous __init__ . This “cleverness” gets us into trouble.\nExample 6-12. A simple class to illustrate the danger of a mutable default\nclass HauntedBus : \n    """"""A bus model haunted by ghost passengers""""""  \n \n    def __init__ (self, passengers =[]):  \n \n        self.passengers  = passengers   \n \n \n    def pick(self, name): \n        self.passengers .append(name)  \n \n \n    def drop(self, name): \n        self.passengers .remove(name)\nWhen the passengers  ar gument is not passed, this parameter is\nbound to the default list object, which is initially empty .\nThis assignment makes self.passengers  an alias for\npassengers , which is itself an alias for the default list, when no\npassengers  ar gument is given.\nWhen the methods .remove()  and .append()  are used with\nself.passengers  we are actually mutating the default list, which is\nan attribute of the function object.\nExample 6-13  shows the eerie behavior of the HauntedBus .\nExample 6-13. Buses haunted by ghost passengers\n>>> bus1 = HauntedBus (['Alice', 'Bill'])  \n \n>>> bus1.passengers  \n['Alice', 'Bill']  \n>>> bus1.pick('Charlie') \n>>> bus1.drop('Alice') \n>>> bus1.passengers   \n \n['Bill', 'Charlie']  \n>>> bus2 = HauntedBus ()  \n \n>>> bus2.pick('Carrie') \n>>> bus2.passengers  \n['Carrie']  \n>>> bus3 = HauntedBus ()  \n \n>>> bus3.passengers   \n \n['Carrie']  \n>>> bus3.pick('Dave') \n>>> bus2.passengers   \n \n['Carrie', 'Dave']  \n>>> bus2.passengers  is bus3.passengers   \n \nTrue \n>>> bus1.passengers   \n \n['Bill', 'Charlie']\nbus1  starts with a two-passenger list.\nSo far , so good: no surprises with bus1 .\nbus2  starts empty , so the default empty list is assigned to\nself.passengers .\nbus3  also starts empty , again the default list is assigned.\nThe default is no longer empty!\nNow Dave , picked by bus3 , appears in bus2 .\nThe problem: bus2.passengers  and bus3.passengers  refer to\nthe same list.\nBut bus1.passengers  is a distinct list.\nThe problem is that HauntedBus  instances that don’ t get an initial\npassenger list end up sharing the same passenger list among themselves.\nSuch bugs may be subtle. As Example 6-13  demonstrates, when a\nHauntedBus  is instantiated with passengers, it works as expected.\nStrange things happen only when a HauntedBus  starts empty , because\nthen self.passengers  becomes an alias for the default value of the\npassengers  parameter . The problem is that each default value is\nevaluated when the function is defined—i.e., usually when the module is\nloaded—and the default values become attributes of the function object. So\nif a default value is a mutable object, and you change it, the change will\naf fect every future call of the function.\nAfter running the lines in Example 6-13 , you can inspect the\nHauntedBus.__init__  object and see the ghost students haunting its\n__defaults__  attribute:\n>>> dir(HauntedBus .__init__ )  # doctest: +ELLIPSIS  \n['__annotations__', '__call__', ..., '__defaults__', ...]  \n>>> HauntedBus .__init__ .__defaults__  \n(['Carrie', 'Dave'],)",5381
92-Defensive Programming with Mutable Parameters.pdf,92-Defensive Programming with Mutable Parameters,"Finally , we can verify that bus2.passengers  is an alias bound to the\nfirst element of the HauntedBus.__init__.__defaults__\nattribute:\n>>> HauntedBus .__init__ .__defaults__ [0] is bus2.passengers  \nTrue\nThe issue with mutable defaults explains why None  is commonly used as\nthe default value for parameters that may receive mutable values. In\nExample 6-8 , __init__  checks whether the passengers  ar gument is\nNone . If it is, self.passengers  is bound to a new empty list. If\npassengers  is not None , the correct implementation binds a copy of that\nar gument to self.passengers . The next section explains why copying\nthe ar gument is a good practice.\nDefensive Programming with Mutable Parameters\nWhen you are coding a function that receives a mutable parameter , you\nshould carefully consider whether the caller expects the ar gument passed to\nbe changed.\nFor example, if your function receives a dict  and needs to modify it while\nprocessing it, should this side ef fect be visible outside of the function or\nnot? Actually it depends on the context. It’ s really a matter of aligning the\nexpectation of the coder of the function and that of the caller .\nThe last bus example in this chapter shows how a TwilightBus  breaks\nexpectations by sharing its passenger list with its clients. Before studying\nthe implementation, see in Example 6-14  how the TwilightBus  class\nworks from the perspective of a client of the class.\nExample 6-14. Passengers disappear when dr opped by a T wilightBus\n>>> basketball_team  = ['Sue', 'Tina', 'Maya', 'Diana', 'Pat']  \n \n>>> bus = TwilightBus (basketball_team )  \n \n>>> bus.drop('Tina')  \n \n>>> bus.drop('Pat') \n>>> basketball_team   \n \n['Sue', 'Maya', 'Diana']\nbasketball_team  holds five student names.\nA TwilightBus  is loaded with the team.\nThe bus  drops one student, then another .\nThe dropped passengers vanished from the basketball team!\nTwilightBus  violates the “Principle of least astonishment,” a best\npractice of interface design.  It surely is astonishing that when the bus\ndrops a student, her name is removed from the basketball team roster .\nExample 6-15  is the implementation TwilightBus  and an explanation of\nthe problem.\nExample 6-15. A simple class to show the perils of mutating r eceived\nar guments\nclass TwilightBus : \n    """"""A bus model that makes passengers vanish""""""  \n \n    def __init__ (self, passengers =None): \n        if passengers  is None: \n            self.passengers  = []  \n \n        else: \n            self.passengers  = passengers   \n \n \n    def pick(self, name): \n        self.passengers .append(name) \n \n    def drop(self, name): \n        self.passengers .remove(name)  \nHere we are careful to create a new empty list when passengers  is\nNone .\nHowever , this assignment makes self.passengers  an alias for\npassengers , which is itself an alias for the actual ar gument passed to\n__init__  (i.e.,basketball_team  in Example 6-14 ).3\nWhen the methods .remove()  and .append()  are used with\nself.passengers , we are actually mutating the original list\nreceived as ar gument to the constructor .\nThe problem here is that the bus is aliasing the list that is passed to the\nconstructor . Instead, it should keep its own passenger list. The fix is simple:\nin __init__ , when the passengers  parameter is provided,\nself.passengers  should be initialized with a copy of it, as we did\ncorrectly in Example 6-8 :\n    def __init__ (self, passengers =None): \n        if passengers  is None: \n            self.passengers  = [] \n        else: \n            self.passengers  = list(passengers ) \nMake a copy of the passengers  list, or convert it to a list  if it’ s\nnot one.\nNow our internal handling of the passenger list will not af fect the ar gument\nused to initialize the bus. As a bonus, this solution is more flexible: now the\nar gument passed to the passengers  parameter may be a tuple  or any\nother iterable, like a set  or even database results, because the list\nconstructor accepts any iterable. As we create our own list to manage, we\nensure that it supports the necessary .remove()  and .append()\noperations we use in the .pick()  and .drop()  methods.\nT I P\nUnless a method is explicitly intended to mutate an object received as ar gument, you\nshould think twice before aliasing the ar gument object by simply assigning it to an\ninstance variable in your class. If in doubt, make a copy . Y our clients will be happier . Of\ncourse, making a copy is not free: there is a cost in CPU and memory . However , an API\nthat causes subtle bugs is usually a bigger problem than one that is a little slower or uses\nmore resources.",4701
93-del and Garbage Collection.pdf,93-del and Garbage Collection,"Now let’ s talk about one of the most misunderstood of Python’ s statements:\ndel .\nd e l  a n d  G a r b a g e  C o l l e c t i o n\nObjects ar e never explicitly destr oyed; however , when they become\nunr eachable they may be garbage-collected.\n— Data Model, chapter of The Python Language Refer ence\nThe first strange fact about del  is that it’ s not a function: it’ s a statement.\nW e write del x , and not del(x) —although the latter also works, but\nonly because the expressions x  and (x)  usually mean the same thing in\nPython.\nThe second surprising fact is that del  deletes references, not objects.\nPython’ s garbage collector may discard an object from memory as an\nindirect result of del , if the deleted variable was the last reference to the\nobject. Rebinding a variable may also cause the number of references to an\nobject to reach zero, causing its destruction.\n>>> a = [1, 2]  \n \n>>> b = a       \n  \n>>> del a       \n  \n>>> b           \n  \n[1, 2] \n>>> b = [3]     \nCreate object [1, 2]  and bind a  to it.\nBind b  to the same [1, 2]  object.\nDelete reference a .\n[1, 2]  was not af fected, because b  still points to it.\n\nRebinding b  to a dif ferent object removes the last remaining reference\nto [1, 2] . Now the garbage collector can discard that object.\nW A R N I N G\nThere is a __del__  special method, but it does not cause the disposal of the instance,\nand should not be called by your code. __del__  is invoked by the Python interpreter\nwhen the instance is about to be destroyed to give it a chance to release external\nresources. Y ou will seldom need to implement __del__  in your own code, yet some\nPython programmers spend time coding it for no good reason. The proper use of\n__del__  is rather tricky . See the __del__  special method documentation  in the\n“Data Model” chapter of The Python Language Refer ence .\nIn CPython, the primary algorithm for garbage collection is reference\ncounting. Essentially , each object keeps count of how many references\npoint to it. As soon as that r efcount  reaches zero, the object is immediately\ndestroyed: CPython calls the __del__  method on the object (if defined)\nand then frees the memory allocated to the object. In CPython 2.0, a\ngenerational garbage collection algorithm was added to detect groups of\nobjects involved in reference cycles—which may be unreachable even with\noutstanding references to them, when all the mutual references are\ncontained within the group. Other implementations of Python have more\nsophisticated garbage collectors that do not rely on reference counting,\nwhich means the __del__  method may not be called immediately when\nthere are no more references to the object. See “PyPy , Garbage Collection,\nand a Deadlock”  by A. Jesse Jiryu Davis for discussion of improper and\nproper use of __del__ .\nT o demonstrate the end of an object’ s life, Example 6-16  uses\nweakref.finalize  to register a callback function to be called when an\nobject is destroyed.\nExample 6-16. W atching the end of an object when no mor e r efer ences point\nto it.\n>>> import weakref \n>>> s1 = {1, 2, 3} \n>>> s2 = s1         \n  \n>>> def bye():      \n \n...     print('...like tears in the rain. ') \n... \n>>> ender = weakref.finalize (s1, bye)  \n \n>>> ender.alive  \n \nTrue \n>>> del s1 \n>>> ender.alive  \n \nTrue \n>>> s2 = 'spam'  \n \n...like tears in the rain.  \n>>> ender.alive \nFalse\ns1  and s2  are aliases referring to the same set, {1, 2, 3} .\nThis function must not be a bound method of the object about to be\ndestroyed or otherwise hold a reference to it.\nRegister the bye  callback on the object referred by s1 .\nThe .alive  attribute is True  before the finalize  object is called.\nAs discussed, del  did not delete the object, just the s1  reference to it.\nRebinding the last reference, s2 , makes {1, 2, 3}  unreachable. It is\ndestroyed, the bye  callback is invoked, and ender.alive  becomes\nFalse .\nThe point of Example 6-16  is to make explicit that del  does not delete\nobjects, but objects may be deleted as a consequence of being unreachable\nafter del  is used.\nY ou may be wondering why the {1, 2, 3}  object was destroyed in\nExample 6-16 . After all, the s1  reference was passed to the finalize\nfunction, which must have held on to it in order to monitor the object and\ninvoke the callback. This works because finalize  holds a weak\nr efer ence  to {1, 2, 3} . W eak references to an object do not increase its",4497
94-Tricks Python Plays with Immutables.pdf,94-Tricks Python Plays with Immutables,"reference count. Therefore, a weak reference does not prevent the tar get\nobject from being garbage collected. W eak references are useful in caching\napplications because you don’ t want the cached objects to be kept alive just\nbecause they are referenced by the cache.\nN O T E\nW eak references is a very specialized topic. That’ s why I chose to skip it in this Second\nEdition . Instead, I published W eak References  on fluentpython.com .\nT r i c k s  P y t h o n  P l a y s  w i t h  I m m u t a b l e s\nN O T E\nThis optional section discusses some Python details that are not really important for\nusers  of Python, and that may not apply to other Python implementations or even future\nversions of CPython. Nevertheless, I’ve seen people stumble upon these corner cases\nand then start using the is  operator incorrectly , so I felt they were worth mentioning.\nI was surprised to learn that, for a tuple t , t[:]  does not make a copy , but\nreturns a reference to the same object. Y ou also get a reference to the same\ntuple if you write tuple(t) .  Example 6-17  proves it.\nExample 6-17. A tuple built fr om another is actually the same exact tuple\n>>> t1 = (1, 2, 3) \n>>> t2 = tuple(t1) \n>>> t2 is t1  \n \nTrue \n>>> t3 = t1[:] \n>>> t3 is t1  \n \nTrue\nt1  and t2  are bound to the same object.\nAnd so is t3 .4\nThe same behavior can be observed with instances of str , bytes , and\nfrozenset . Note that a frozenset  is not a sequence, so fs[:]  does\nnot work if fs  is a frozenset . But fs.copy()  has the same ef fect: it\ncheats and returns a reference to the same object, and not a copy at all, as\nExample 6-18  shows.\nExample 6-18. String literals may cr eate shar ed objects\n>>> t1 = (1, 2, 3) \n>>> t3 = (1, 2, 3)  \n \n>>> t3 is t1  \n \nFalse \n>>> s1 = 'ABC' \n>>> s2 = 'ABC'  \n \n>>> s2 is s1 \n \nTrue\nCreating a new tuple from scratch.\nt1  and t3  are equal, but not the same object.\nCreating a second str  from scratch.\nSurprise: a  and b  refer to the same str !\nThe sharing of string literals is an optimization technique called interning .\nCPython uses a similar technique with small integers to avoid unnecessary\nduplication of numbers that appear frequently in programs like 0, 1, –1, etc.\nNote that CPython does not intern all strings or integers, and the criteria it\nuses to do so is an undocumented implementation detail.\nW A R N I N G\nNever depend on str  or int  interning! Always use ==  instead of is  to compare\nstrings or integers for equality . Interning is an optimization for internal use of the Python\ninterpreter .5",2594
95-Chapter Summary.pdf,95-Chapter Summary,"The tricks discussed in this section, including the behavior of\nfrozenset.copy() , are harmless “lies” that save memory and make\nthe interpreter faster . Do not worry about them, they should not give you\nany trouble because they only apply to immutable types. Probably the best\nuse of these bits of trivia is to win bets with fellow Pythonistas.6\nC h a p t e r  S u m m a r y\nEvery Python object has an identity , a type, and a value. Only the value of\nan object may change over time.\nIf two variables refer to immutable objects that have equal values ( a == b\nis True ), in practice it rarely matters if they refer to copies or are aliases\nreferring to the same object because the value of an immutable object does\nnot change, with one exception. The exception being immutable collections\nsuch as tuples: if an immutable collection holds references to mutable\nitems, then its value may actually change when the value of a mutable item\nchanges. In practice, this scenario is not so common. What never changes in\nan immutable collection are the identities of the objects within. The\nfrozenset  class is does not suf fer from this problem because it can only\nhold hashable elements, and the value of hashable objects cannot ever\nchange, by definition.\nThe fact that variables hold references has many practical consequences in\nPython programming:\nSimple assignment does not create copies.\nAugmented assignment with +=  or *=  creates new objects if the\nleft-hand variable is bound to an immutable object, but may\nmodify a mutable object in place.\nAssigning a new value to an existing variable does not change the\nobject previously bound to it. This is called a rebinding: the\nvariable is now bound to a dif ferent object. If that variable was the\nlast reference to the previous object, that object will be garbage\ncollected.\nFunction parameters are passed as aliases, which means the\nfunction may change any mutable object received as an ar gument.\nThere is no way to prevent this, except making local copies or\nusing immutable objects (e.g., passing a tuple instead of a list).7",2110
96-Further Reading.pdf,96-Further Reading,"Using mutable objects as default values for function parameters is\ndangerous because if the parameters are changed in place, then the\ndefault is changed, af fecting every future call that relies on the\ndefault.\nIn CPython, objects are discarded as soon as the number of references to\nthem reaches zero. They may also be discarded if they form groups with\ncyclic references but no outside references.\nIn some situations, it may be useful to hold a reference to an object that will\nnot—by itself—keep an object alive. One example is a class that wants to\nkeep track of all its current instances. This can be done with weak\nreferences, a low-level mechanism underlying the more useful collections\nWeakValueDictionary , WeakKeyDictionary , WeakSet , and the\nfinalize  function from the weakref  module. For more on this, please\nsee W eak References  at fluentpython.com .\nF u r t h e r  R e a d i n g\nThe “Data Model” chapter  of The Python Language Refer ence  starts with a\nclear explanation of object identities and values.\nW esley Chun, author of the Cor e Python  series of books, made a great\npresentation about many of the topics covered in this chapter during\nOSCON 2013. Y ou can download the slides from the “Python 103: Memory\nModel & Best Practices” talk page . There is also a Y ouT ube video  of a\nlonger presentation W esley gave at EuroPython 201 1, covering not only the\ntheme of this chapter but also the use of special methods.\nDoug Hellmann wrote a long series of excellent blog posts titled Python\nModule of the W eek , which became a book, The Python Standar d Library\nby Example . His posts “copy – Duplicate Objects”  and “weakref – Garbage-\nCollectable References to Objects”  cover some of the topics we just\ndiscussed.\nMore information on the CPython generational garbage collector can be\nfound in the gc module documentation , which starts with the sentence “This\nmodule provides an interface to the optional garbage collector .” The\n“optional” qualifier here may be surprising, but the “Data Model” chapter\nalso states:\nAn implementation is allowed to postpone garbage collection or omit it\naltogether—it is a matter of implementation quality how garbage\ncollection is implemented, as long as no objects ar e collected that ar e\nstill r eachable.\nPablo Galindo wrote more in-depth treatment of Python’ s GC in Design of\nCPython’ s Garbage Collector  at the Python Developer ’ s Guide , aimed at\nnew and experienced contributors to the CPython implementation.\nThe CPython 3.4 garbage collector improved handling of objects with a\n__del__  method, as described in PEP 442 — Safe object finalization .\nW ikipedia has an article about string interning , mentioning the use of this\ntechnique in several languages, including Python.\nS O A P B O X\nEqual T r eatment to All Objects\nI learned Java before I discovered Python. The ==  operator in Java\nnever felt right for me. It is much more common for programmers to\ncare about equality than identity , but for objects (not primitive types)\nthe Java ==  compares references, and not object values. Even for\nsomething as basic as comparing strings, Java forces you to use the\n.equals  method. Even then, there is another catch: if you write\na.equals(b)  and a  is null , you get a null pointer exception. The\nJava designers felt the need to overload +  for strings, so why not go\nahead and overload ==  as well?\nPython gets this right. The ==  operator compares object values; is\ncompares references. And because Python has operator overloading, ==\nworks sensibly with all objects in the standard library , including None ,\nwhich is a proper object, unlike Java’ s null .\nAnd of course, you can define __eq__  in your own classes to decide\nwhat ==  means for your instances. If you don’ t override __eq__ , the\nmethod inherited from object  compares object IDs, so the fallback is\nthat every instance of a user -defined class is considered dif ferent.\nThese are some of the things that made me switch from Java to Python\nas soon as I finished reading the Python T utorial one afternoon in\nSeptember 1998.\nMutability\nThis chapter would not be necessary if all Python objects were\nimmutable. When you are dealing with unchanging objects, it makes no\ndif ference whether variables hold the actual objects or references to\nshared objects. If a == b  is true, and neither object can change, they\nmight as well be the same. That’ s why string interning is safe. Object\nidentity becomes important only when objects are mutable.\nIn “pure” functional programming, all data is immutable: appending to\na collection actually creates a new collection. Elixir is one easy to learn,\npractical functional language in which all built-in types are immutable,\nincluding lists.\nPython, however , is not a functional language, much less a pure one.\nInstances of user -defined classes are mutable by default in Python—as\nin most object-oriented languages. When creating your own objects,\nyou have to be extra careful to make them immutable, if that is a\nrequirement. Every attribute of the object must also be immutable,\notherwise you end up with something like the tuple : immutable as far\nas object IDs go, but the value of a tuple  may change if it holds a\nmutable object.\nMutable objects are also the main reason why programming with\nthreads is so hard to get right: threads mutating objects without proper\nsynchronization produce corrupted data. Excessive synchronization, on\nthe other hand, causes deadlocks. The Erlang language and platform—\nwhich includes Elixir—was designed to maximize uptime in highly-\nconcurrent, distributed applications such as telecommunications\nswitches. Naturally , they chose immutable data by default.\nObject Destruction and Garbage Collection\nThere is no mechanism in Python to directly destroy an object, and this\nomission is actually a great feature: if you could destroy an object at\nany time, what would happen to existing strong references pointing to\nit?\nGarbage collection in CPython is done primarily by reference counting,\nwhich is easy to implement, but is prone to memory leaking when there\nare reference cycles, so with version 2.0 (October 2000) a generational\ngarbage collector was implemented, and it is able to dispose of\nunreachable objects kept alive by reference cycles.\nBut the reference counting is still there as a baseline, and it causes the\nimmediate disposal of objects with zero references. This means that, in\nCPython—at least for now—it’ s safe to write this:\nopen('test.txt' , 'wt', encoding ='utf-8').write('1, 2, 3' )\nThat code is safe because the reference count of the file object will be\nzero after the write  method returns, and Python will immediately\nclose the file before destroying the object representing it in memory .\nHowever , the same line is not safe in Jython or IronPython that use the\ngarbage collector of their host runtimes (the Java VM and the .NET\nCLR), which are more sophisticated but do not rely on reference\ncounting and may take longer to destroy the object and close the file. In\nall cases, including CPython, the best practice is to explicitly close the\nfile, and the most reliable way of doing it is using the with  statement,\nwhich guarantees that the file will be closed even if exceptions are\nraised while it is open. Using with , the previous snippet becomes:\nwith open('test.txt' , 'wt', encoding ='utf-8') as fp: \n    fp.write('1, 2, 3' )\nIf you are into the subject of garbage collectors, you may want to read\nThomas Perl’ s paper “Python Garbage Collector Implementations:\nCPython, PyPy and GaS” , from which I learned the bit about the safety\nof the open().write()  in CPython.\nParameter Passing: Call by Sharing\nA popular way of explaining how parameter passing works in Python is\nthe phrase: “Parameters are passed by value, but the values are\nreferences.” This is not wrong, but causes confusion because the most\ncommon parameter passing modes in older languages are call by value\n(the function gets a copy of the ar gument) and call by r efer ence  (the\nfunction gets a pointer to the ar gument). In Python, the function gets a\ncopy of the ar guments, but the ar guments are always references. So the\nvalue of the referenced objects may be changed, if they are mutable, but\ntheir identity cannot. Also, because the function gets a copy of the\nreference in an ar gument, rebinding it in the function body has no ef fect\noutside of the function. I adopted the term call by sharing  after reading\nup on the subject in Pr ogramming Language Pragmatics, Thir d Edition\nby Michael L. Scott (Mor gan Kaufmann), section “8.3.1: Parameter\nModes.”\n1  L ynn Andrea Stein is award-winning computer science educator who currently teaches at Olin\nCollege of Engineering\n2  In contrast, flat sequences like str , bytes , and array.array  don’ t contain references\nbut directly hold their contents—characters, bytes, and numbers—in contiguous memory .\n3  See Principle of least astonishment  in the English W ikipedia\n4  This is clearly documented. T ype help(tuple)  in the Python console to read: “If the\nar gument is a tuple, the return value is the same object.” I thought I knew everything about\ntuples before writing this book.\n5  The harmless lie of having the copy  method not copying anything is justified by interface\ncompatibility: it makes frozenset  more compatible with set . Anyway , it makes no\ndif ference to the end user whether two identical immutable objects are the same or are copies.\n6  A terrible use for this information would be to ask about it when interviewing candidates or\nauthoring questions for “certification” exams. There are countless more important and useful\nfacts to check for Python knowledge.\n7  Actually the type of an object may be changed by merely assigning a dif ferent class to its\n__class__  attribute, but that is pure evil and I regret writing this footnote.",9998
97-Modern Replacements for map filter and reduce.pdf,97-Modern Replacements for map filter and reduce,"Part III. Functions as Objects\nChapter 7. Functions as First-\nClass Objects\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 7th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nI have never consider ed Python to be heavily influenced by functional\nlanguages, no matter what people say or think. I was much mor e familiar\nwith imperative languages such as C and Algol 68 and although I had\nmade functions first-class objects, I didn’ t view Python as a functional\npr ogramming language.\n— Guido van Rossum, Python BDFL\nFunctions in Python are first-class objects. Programming language\nresearchers define a “first-class object” as a program entity that can be:\ncreated at runtime;\nassigned to a variable or element in a data structure;\npassed as an ar gument to a function;\nreturned as the result of a function.1\nIntegers, strings, and dictionaries are other examples of first-class objects in\nPython—nothing fancy here. Having functions as first-class objects is an\nessential feature of functional languages, such as Clojure, Elixir , and\nHaskell. However , first-class functions are so useful that they’ve been\nadopted by popular languages like JavaScript, Go, and Java (since JDK 8),\nnone of which claim to be “functional languages.”\nThis chapter and most of Part III explore the practical applications of\ntreating functions as objects.\nT I P\nThe term “first-class functions” is widely used as shorthand for “functions as first-class\nobjects.” It’ s not ideal because it implies an “elite” among functions. In Python, all\nfunctions are first-class.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nSection “The Nine Flavors of Callable Objects”  was titled “The Seven\nFlavors of Callable Objects” in the First Edition . The new callables are\nnative coroutines and asynchronous generators, introduced in Python 3.5\nand 3.6, respectively . Both are covered in Chapter 22 , but they are\nmentioned here along with the other callables for completeness.\n“Positional-only parameters”  is a new section, covering a feature added in\nPython 3.8.\nI moved coverage of runtime access to function annotations to “Reading\nT ype Hints at Runtime” . When I wrote the First Edition , PEP 484—T ype\nHints —was still under consideration, and people used annotations in\ndif ferent ways. Since Python 3.5, annotations should conform to PEP 484.\nTherefore, the best place to cover them is when discussing type hints.\nN O T E\nThe First Edition  had sections about the introspection of function objects that were too\nlow-level and distracted from the main subject of this chapter . I mer ged those sections\ninto a post titled Introspection of Function Parameters  at fluentpython.com .\nNow let’ s see why Python functions are full-fledged objects.\nT r e a t i n g  a  F u n c t i o n  L i k e  a n  O b j e c t\nThe console session in Example 7-1  shows that Python functions are\nobjects. Here we create a function, call it, read its __doc__  attribute, and\ncheck that the function object itself is an instance of the function  class.\nExample 7-1. Cr eate and test a function, then r ead its __doc__ and check\nits type\n>>> def factorial (n):  \n \n...     """"""returns n!""""""  \n...     return 1 if n < 2 else n * factorial (n - 1) \n... \n>>> factorial (42) \n1405006117752879898543142606244511569936384000000000  \n>>> factorial .__doc__  \n \n'returns n!'  \n>>> type(factorial )  \n \n<class 'function'>\nThis is a console session, so we’re creating a function at “runtime.”\n__doc__  is one of several attributes of function objects.\nfactorial  is an instance of the function  class.\nThe __doc__  attribute is used to generate the help text of an object. In the\nPython console, the command help(factorial)  will display a screen\nlike Figure 7-1 .\nFigur e 7-1. Help scr een for factorial ; the text is built fr om the __doc__ attribute of the function.\nExample 7-2  shows the “first class” nature of a function object. W e can\nassign it a variable fact  and call it through that name. W e can also pass\nfactorial  as an ar gument to the map  function. Calling\nmap(function, iterable)  returns an iterable where each item is the\nresult of calling the first ar gument (a function) to successive elements of the\nsecond ar gument (an iterable), range(10)  in this example.\nExample 7-2. Use function thr ough a differ ent name, and pass function as\nar gument\n>>> fact = factorial  \n>>> fact \n<function factorial at 0x...>  \n>>> fact(5) \n120 \n>>> map(factorial , range(11)) \n<map object at 0x...>  \n>>> list(map(factorial , range(11))) \n[1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800]\nHaving first-class functions enables programming in a functional style. One\nof the hallmarks of functional programming  is the use of higher -order\nfunctions, our next topic.\nH i g h e r - O r d e r  F u n c t i o n s\nA function that takes a function as ar gument or returns a function as the\nresult is a higher -or der function . One example is map , shown in Example 7-\n2 . Another is the built-in function sorted : the optional key  ar gument lets\nyou provide a function to be applied to each item for sorting, as we saw in\n“list.sort versus the sorted Built-In” . For example, to sort a list of words by\nlength, pass the len  function as the key , as in Example 7-3 .\nExample 7-3. Sorting a list of wor ds by length\n>>> fruits = ['strawberry' , 'fig', 'apple', 'cherry' , 'raspberry' , \n'banana' ] \n>>> sorted(fruits, key=len) \n['fig', 'apple', 'cherry', 'banana', 'raspberry', 'strawberry']  \n>>>\nAny one-ar gument function can be used as the key . For example, to create a\nrhyme dictionary it might be useful to sort each word spelled backward. In\nExample 7-4 , note that the words in the list are not changed at all; only their\nreversed spelling is used as the sort criterion, so that the berries appear\ntogether .\nExample 7-4. Sorting a list of wor ds by their r eversed spelling\n>>> def reverse(word): \n...     return word[::-1] \n>>> reverse('testing' ) \n'gnitset'  \n>>> sorted(fruits, key=reverse) \n['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry']  \n>>>\nIn the functional programming paradigm, some of the best known higher -\norder functions are map , filter , reduce , and apply . The apply\nfunction was deprecated in Python 2.3 and removed in Python 3 because it’ s\nno longer necessary . If you need to call a function with a dynamic set of\nar guments, you can write fn(*args, **kwargs)  instead of\napply(fn, args, kwargs) .\nThe map , filter , and reduce  higher -order functions are still around,\nbut better alternatives are available for most of their use cases, as the next\nsection shows.\nModern Replacements for map, filter , and reduce\nFunctional languages commonly of fer the map , filter , and reduce\nhigher -order functions (sometimes with dif ferent names). The map  and\nfilter  functions are still built-ins in Python 3, but since the introduction\nof list comprehensions and generator expressions, they are not as important.\nA listcomp or a genexp does the job of map  and filter  combined, but is\nmore readable. Consider Example 7-5 .\nExample 7-5. Lists of factorials pr oduced with map and filter compar ed to\nalternatives coded as list compr ehensions\n>>> list(map(factorial , range(6)))  \n \n[1, 1, 2, 6, 24, 120]  \n>>> [factorial (n) for n in range(6)]  \n \n[1, 1, 2, 6, 24, 120]  \n>>> list(map(factorial , filter(lambda n: n % 2, range(6))))  \n \n[1, 6, 120]  \n>>> [factorial (n) for n in range(6) if n % 2]  \n \n[1, 6, 120]  \n>>>\nBuild a list of factorials from 0! to 5!.\nSame operation, with a list comprehension.\nList of factorials of odd numbers up to 5!, using both map  and\nfilter .\nList comprehension does the same job, replacing map  and filter ,\nand making lambda  unnecessary .\nIn Python 3, map  and filter  return generators—a form of iterator—so\ntheir direct substitute is now a generator expression (in Python 2, these\nfunctions returned lists, therefore their closest alternative is a listcomp).\nThe reduce  function was demoted from a built-in in Python 2 to the\nfunctools  module in Python 3. Its most common use case, summation,\nis better served by the sum  built-in available since Python 2.3 was released\nin 2003. This is a big win in terms of readability and performance (see\nExample 7-6 ).\nExample 7-6. Sum of integers up to 99 performed with r educe and sum\n>>> from functools  import reduce  \n \n>>> from operator  import add  \n \n>>> reduce(add, range(100))  \n \n4950 \n>>> sum(range(100))  \n \n4950 \n>>>\nStarting with Python 3.0, reduce  is no longer a built-in.\nImport add  to avoid creating a function just to add two numbers.\nSum integers up to 99.\nSame task with sum —no need to import and call reduce  and add .\nN O T E\nThe common idea of sum  and reduce  is to apply some operation to successive items\nin a sequence, accumulating previous results, thus reducing a sequence of values to a\nsingle value.\nOther reducing built-ins are all  and any :\nall(iterable)\nReturns True  if there are no falsy elements in the iterable; all([])\nreturns True .\nany(iterable)\nReturns True  if any element of the iterable  is truthy; any([])\nreturns False .\nI give a fuller explanation of reduce  in “V ector T ake #4: Hashing and a\nFaster ==”  where an ongoing example provides a meaningful context for",9896
98-The Nine Flavors of Callable Objects.pdf,98-The Nine Flavors of Callable Objects,"the use of this function. The reducing functions are summarized later in the\nbook when iterables are in focus, in “Iterable Reducing Functions” .\nT o use a higher -order function, sometimes it is convenient to create a small,\none-of f function. That is why anonymous functions exist. W e’ll cover them\nnext.\nA n o n y m o u s  F u n c t i o n s\nThe lambda  keyword creates an anonymous function within a Python\nexpression.\nHowever , the simple syntax of Python limits the body of lambda functions\nto be pure expressions. In other words, the body cannot contain other\nPython statements such as while , try , etc. Assignment with =  is also a\nstatement, so it cannot occur in a lambda. The new assignment expression\nsyntax using :=  can be used—but if you need it, your lambda is probably\ntoo complicated and hard to read, and it should be refactored into a regular\nfunction using def .\nThe best use of anonymous functions is in the context of an ar gument list\nfor a higher -order function. For example, Example 7-7  is the rhyme index\nexample from Example 7-4  rewritten with lambda , without defining a\nreverse  function.\nExample 7-7. Sorting a list of wor ds by their r eversed spelling using lambda\n>>> fruits = ['strawberry' , 'fig', 'apple', 'cherry' , 'raspberry' , \n'banana' ] \n>>> sorted(fruits, key=lambda word: word[::-1]) \n['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry']  \n>>>\nOutside the limited context of ar guments to higher -order functions,\nanonymous functions are rarely useful in Python. The syntactic restrictions\ntend to make nontrivial lambdas either unreadable or unworkable. If a\nlambda is hard to read, I strongly advise you follow Fredrik Lundh’ s\nrefactoring advice.\nF R E D R I K  L U N D H ’ S  L A M B D A  R E F A C T O R I N G  R E C I P E\nIf you find a piece of code hard to understand because of a lambda ,\nFredrik Lundh suggests this refactoring procedure:\n1 . W rite a comment explaining what the heck that lambda  does.\n2 . Study the comment for a while, and think of a name that\ncaptures the essence of the comment.\n3 . Convert the lambda  to a def  statement, using that name.\n4 . Remove the comment.\nThese steps are quoted from the Functional Programming HOWT O , a\nmust read.\nThe lambda  syntax is just syntactic sugar: a lambda  expression creates a\nfunction object just like the def  statement. That is just one of several kinds\nof callable objects in Python. The following section reviews all of them.\nT h e  N i n e  F l a v o r s  o f  C a l l a b l e  O b j e c t s\nThe call operator ()  may be applied to other objects beyond user -defined\nfunctions and lambdas. T o determine whether an object is callable, use the\ncallable()  built-in function. As of Python 3.9, the Data Model\ndocumentation  lists nine callable types:\nUser -defined functions\nCreated with def  statements or lambda  expressions.\nBuilt-in functions\nA function implemented in C (for CPython), like len  or\ntime.strftime .\nBuilt-in methods\nMethods implemented in C, like dict.get .\nMethods\nFunctions defined in the body of a class.\nClasses\nWhen invoked, a class runs its __new__  method to create an instance,\nthen __init__  to initialize it, and finally the instance is returned to\nthe caller . Because there is no new  operator in Python, calling a class is\nlike calling a function.\nClass instances\nIf a class defines a __call__  method, then its instances may be\ninvoked as functions—that’ s the subject of the next section.\nGenerator functions\nFunctions or methods that use the yield  keyword in their body . When\ncalled, they return a generator object.\nNative cor outine functions\nFunctions or methods defined with async def . When called, they\nreturn a coroutine object. Added in Python 3.5.\nAsynchr onous generator functions\nFunctions or methods defined with async def  that have yield  in\ntheir body . When called, they return an asynchronous generator for use\nwith async for . Added in Python 3.6.\nGenerators, native coroutines, and asynchronous generator functions are\nunlike other callables in that their return values are never application data,\nbut objects that require further processing to yield application data or\nperform useful work. Generator functions return iterators. Both are covered2",4316
99-User-Defined Callable Types.pdf,99-User-Defined Callable Types,"in Chapter 17 . Native coroutine functions and asynchronous generator\nfunctions return objects that only work with the help of an asynchronous\nprogramming framework, such as asyncio . They are the subject of\nChapter 22 .\nT I P\nGiven the variety of existing callable types in Python, the safest way to determine\nwhether an object is callable is to use the callable()  built-in:\n>>> abs, str, 'Ni!'  \n(<built-in function abs>, <class 'str'>, 'Ni!')  \n>>> [callable(obj) for obj in (abs, str, 'Ni!')]  \n[True, True, False]\nW e now move on to building class instances that work as callable objects.\nU s e r - D e f i n e d  C a l l a b l e  T y p e s\nNot only are Python functions real objects, but arbitrary Python objects may\nalso be made to behave like functions. Implementing a __call__  instance\nmethod is all it takes.\nExample 7-8  implements a BingoCage  class. An instance is built from\nany iterable, and stores an internal list  of items, in random order . Calling\nthe instance pops an item.\nExample 7-8. bingocall.py: A BingoCage does one thing: picks items fr om a\nshuffled list\nimport random \n \nclass BingoCage : \n \n    def __init__ (self, items): \n        self._items = list(items)  \n \n        random.shuffle(self._items)  \n \n 3\n    def pick(self):  \n \n        try: \n            return self._items.pop() \n        except IndexError : \n            raise LookupError ('pick from empty BingoCage ')  \n \n \n    def __call__ (self):  \n \n        return self.pick()\n__init__  accepts any iterable; building a local copy prevents\nunexpected side ef fects on any list  passed as an ar gument.\nshuffle  is guaranteed to work because self._items  is a list .\nThe main method.\nRaise exception with custom message if self._items  is empty .\nShortcut to bingo.pick() : bingo() .\nHere is a simple demo of Example 7-8 . Note how a bingo  instance can be\ninvoked as a function, and the callable()  built-in recognizes it as a\ncallable object:\n>>> bingo = BingoCage (range(3)) \n>>> bingo.pick() \n1 \n>>> bingo() \n0 \n>>> callable (bingo) \nTrue\nA class implementing __call__  is an easy way to create function-like\nobjects that have some internal state that must be kept across invocations,\nlike the remaining items in the BingoCage . Another good use case for\n__call__  is implementing decorators. Decorators must be callable, and it\nis sometimes convenient to “remember” something between calls of the\ndecorator (e.g., for memoization—caching the results of expensive",2518
100-From Positional to Keyword-Only Parameters.pdf,100-From Positional to Keyword-Only Parameters,"computations for later use) or to split a complex implementation into\nseparate methods.\nThe functional approach to creating functions with internal state is to use\nclosures. Closures, as well as decorators, are the subject of Chapter 9 .\nNow let’ s explore the powerful syntax Python of fers to declare function\nparameters and pass ar guments into them.\nF r o m  P o s i t i o n a l  t o  K e y w o r d - O n l y\nP a r a m e t e r s\nOne of the best features of Python functions is the extremely flexible\nparameter handling mechanism. Closely related are the use of *  and **  to\nunpack iterables and mappings into separate ar guments when we call a\nfunction. T o see these features in action, see the code for Example 7-9  and\ntests showing its use in Example 7-10 .\nExample 7-9. tag generates HTML elements; a keywor d-only ar gument\nclass_  is used to pass “class” attributes as a workar ound because\nclass  is a keywor d in Python\ndef tag(name, *content, class_=None, **attrs): \n    """"""Generate one or more HTML tags""""""  \n    if class_ is not None: \n        attrs['class'] = class_ \n    attr_pairs  = (f' {attr}=""{value}""'  for attr, value \n                    in sorted(attrs.items())) \n    attr_str  = ''.join(attr_pairs ) \n    if content: \n        elements  = (f'<{name}{attr_str}>{c}</{name}>'  \n                    for c in content) \n        return '\n'.join(elements ) \n    else: \n        return f'<{name}{attr_str} />'\nThe tag  function can be invoked in many ways, as Example 7-10  shows.\nExample 7-10. Some of the many ways of calling the tag function fr om\nExample 7-9\n>>> tag('br')  \n \n'<br />' \n>>> tag('p', 'hello')  \n \n'<p>hello</p> ' \n>>> print(tag('p', 'hello', 'world')) \n<p>hello</p> \n<p>world</p> \n>>> tag('p', 'hello', id=33)  \n \n'<p id=""33"">hello</p> ' \n>>> print(tag('p', 'hello', 'world', class_='sidebar'))  \n \n<p class=""sidebar"">hello</p> \n<p class=""sidebar"">world</p> \n>>> tag(content='testing', name=""img"")  \n \n'<img content= ""testing"" />' \n>>> my_tag = {'name': 'img', 'title': 'Sunset Boulevard ', \n...           'src': 'sunset.jpg ', 'class': 'framed'} \n>>> tag(**my_tag)  \n \n'<img class= ""framed"" src=""sunset.jpg "" title=""Sunset Boulevard "" />'\nA single positional ar gument produces an empty tag with that name.\nAny number of ar guments after the first are captured by *content  as\na tuple .\nKeyword ar guments not explicitly named in the tag  signature are\ncaptured by **attrs  as a dict .\nThe class_  parameter can only be passed as a keyword ar gument.\nThe first positional ar gument can also be passed as a keyword.\nPrefixing the my_tag  dict  with **  passes all its items as separate\nar guments, which are then bound to the named parameters, with the\nremaining caught by **attrs . In this case we can have a 'class'\nkey in the ar guments dict , because it is a string, and does not clash\nwith the class  reserved word.\nKeyword-only ar guments are a feature of Python 3. In Example 7-9 , the\nclass_  parameter can only be given as a keyword ar gument—it will\nnever capture unnamed positional ar guments. T o specify keyword-only",3138
101-Packages for Functional Programming.pdf,101-Packages for Functional Programming,"ar guments when defining a function, name them after the ar gument prefixed\nwith * . If you don’ t want to support variable positional ar guments but still\nwant keyword-only ar guments, put a *  by itself in the signature, like this:\n>>> def f(a, *, b): \n...     return a, b \n... \n>>> f(1, b=2) \n(1, 2) \n>>> f(1, 2) \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : f() takes 1 positional argument but 2 were given\nNote that keyword-only ar guments do not need to have a default value: they\ncan be mandatory , like b  in the preceding example.\nPositional-only parameters\nSince Python 3.8, user -defined function signatures may specify positional-\nonly parameters. This feature always existed for built-in functions, such as\ndivmod(a, b) , which can only be called with positional parameters, and\nnot as divmod(a=10, b=4) .\nT o define a function requiring positional-only parameters, use /  in the\nparameter list.\nThis example from What’ s New In Python 3.8  shows how to emulate the\ndivmod  built-in function:\ndef divmod(a, b, /): \n    return (a // b, a % b)\nAll ar guments to the left of the /  are positional-only . After the / , you may\nspecify other ar guments, which work as usual.",1253
102-The operator Module.pdf,102-The operator Module,"W A R N I N G\nThe /  in the parameter list is a syntax error in Python 3.7 or earlier .\nFor example, consider the tag  function from Example 7-9 . If we want the\nname  parameter to be positional only , we can add a /  after it in the\nfunction signature, like this:\ndef tag(name, /, *content, class_=None, **attrs): \n    ...\nY ou can find other examples of positional-only parameters in What’ s New\nIn Python 3.8  and in PEP 570 .\nAfter diving into Python’ s flexible ar gument declaration features, the\nremainder of this chapter covers the most useful packages in the standard\nlibrary for programming in a functional style.\nP a c k a g e s  f o r  F u n c t i o n a l  P r o g r a m m i n g\nAlthough Guido makes it clear that he did not design Python to be a\nfunctional programming language, a functional coding style can be used to\ngood extent, thanks to first-class functions, pattern matching, and the\nsupport of packages like operator  and functools , which we cover in\nthe next two sections.\nThe operator Module\nOften in functional programming it is convenient to use an arithmetic\noperator as a function. For example, suppose you want to multiply a\nsequence of numbers to calculate factorials without using recursion. T o\nperform summation, you can use sum , but there is no equivalent function\nfor multiplication. Y ou could use reduce —as we saw in “Modern\nReplacements for map, filter , and reduce” —but this requires a function to\nmultiply two items of the sequence. Example 7-1 1  shows how to solve this\nusing lambda .\nExample 7-1 1. Factorial implemented with r educe and an anonymous\nfunction\nfrom functools  import reduce \n \ndef factorial (n): \n    return reduce(lambda a, b: a*b, range(1, n+1))\nThe operator  module provides function equivalents for dozens of\noperators so you don’ t have to code trivial functions like lambda a, b:\na*b . W ith it, we can rewrite Example 7-1 1  as Example 7-12 .\nExample 7-12. Factorial implemented with r educe and operator .mul\nfrom functools  import reduce \nfrom operator  import mul \n \ndef factorial (n): \n    return reduce(mul, range(1, n+1))\nAnother group of one-trick lambdas that operator  replaces are functions\nto pick items from sequences or read attributes from objects: itemgetter\nand attrgetter  are factories that build custom functions to do that.\nExample 7-13  shows a common use of itemgetter : sorting a list of\ntuples by the value of one field. In the example, the cities are printed sorted\nby country code (field 1). Essentially , itemgetter(1)  creates a function\nthat, given a collection, returns the item at index 1. That’ s easier to write\nand read than lambda fields: fields[1] , which does the same:\nExample 7-13. Demo of itemgetter to sort a list of tuples (data fr om\nExample 2-8 )\n>>> metro_data  = [ \n...     ('Tokyo', 'JP', 36.933, (35.689722 , 139.691667 )), \n...     ('Delhi NCR' , 'IN', 21.935, (28.613889 , 77.208889 )), \n...     ('Mexico City' , 'MX', 20.142, (19.433333 , -99.133333 )), \n...     ('New York-Newark' , 'US', 20.104, (40.808611 , -74.020386 )), \n...     ('São Paulo' , 'BR', 19.649, (-23.547778 , -46.635833 )), \n... ] \n>>> \n>>> from operator  import itemgetter  \n>>> for city in sorted(metro_data , key=itemgetter (1)): \n...     print(city) \n... \n('São Paulo', 'BR', 19.649, (-23.547778, -46.635833))  \n('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889))  \n('Tokyo', 'JP', 36.933, (35.689722, 139.691667))  \n('Mexico City', 'MX', 20.142, (19.433333, -99.133333))  \n('New York-Newark', 'US', 20.104, (40.808611, -74.020386))\nIf you pass multiple index ar guments to itemgetter , the function it\nbuilds will return tuples with the extracted values, which is useful for\nsorting on multiple keys:\n>>> cc_name = itemgetter (1, 0) \n>>> for city in metro_data : \n...     print(cc_name(city)) \n... \n('JP', 'Tokyo')  \n('IN', 'Delhi NCR')  \n('MX', 'Mexico City')  \n('US', 'New York-Newark')  \n('BR', 'São Paulo')  \n>>>\nBecause itemgetter  uses the []  operator , it supports not only\nsequences but also mappings and any class that implements\n__getitem__ .\nA sibling of itemgetter  is attrgetter , which creates functions to\nextract object attributes by name. If you pass attrgetter  several\nattribute names as ar guments, it also returns a tuple of values. In addition, if\nany ar gument name contains a .  (dot), attrgetter  navigates through\nnested objects to retrieve the attribute. These behaviors are shown in\nExample 7-14 . This is not the shortest console session because we need to\nbuild a nested structure to showcase the handling of dotted attributes by\nattrgetter .\nExample 7-14. Demo of attr getter to pr ocess a pr eviously defined list of\nnamedtuple called metr o_data (the same list that appears in Example 7-13 )\n>>> from collections  import namedtuple  \n>>> LatLon = namedtuple ('LatLon', 'lat lon')  \n \n>>> Metropolis  = namedtuple ('Metropolis ', 'name cc pop coord ')  \n \n>>> metro_areas  = [Metropolis (name, cc, pop, LatLon(lat, lon))  \n \n...     for name, cc, pop, (lat, lon) in metro_data ] \n>>> metro_areas [0] \nMetropolis(name='Tokyo', cc='JP', pop=36.933,  \ncoord=LatLon(lat=35.689722,  \nlon=139.691667))  \n>>> metro_areas [0].coord.lat  \n \n35.689722  \n>>> from operator  import attrgetter  \n>>> name_lat  = attrgetter ('name', 'coord.lat ')  \n \n>>> \n>>> for city in sorted(metro_areas , key=attrgetter ('coord.lat ')):  \n \n...     print(name_lat (city))  \n \n... \n('São Paulo', -23.547778)  \n('Mexico City', 19.433333)  \n('Delhi NCR', 28.613889)  \n('Tokyo', 35.689722)  \n('New York-Newark', 40.808611)\nUse namedtuple  to define LatLon .\nAlso define Metropolis .\nBuild metro_areas  list with Metropolis  instances; note the\nnested tuple unpacking to extract (lat, lon)  and use them to build\nthe LatLon  for the coord  attribute of Metropolis .\nReach into element metro_areas[0]  to get its latitude.\nDefine an attrgetter  to retrieve the name  and the coord.lat\nnested attribute.\nUse attrgetter  again to sort list of cities by latitude.\nUse the attrgetter  defined in \n  to show only city name and\nlatitude.\nHere is a partial list of functions defined in operator  (names starting\nwith _  are omitted, because they are mostly implementation details):\n>>> [name for name in dir(operator ) if not name.startswith ('_')] \n['abs', 'add', 'and_', 'attrgetter', 'concat', 'contains',  \n'countOf', 'delitem', 'eq', 'floordiv', 'ge', 'getitem', 'gt',  \n'iadd', 'iand', 'iconcat', 'ifloordiv', 'ilshift', 'imatmul',  \n'imod', 'imul', 'index', 'indexOf', 'inv', 'invert', 'ior',  \n'ipow', 'irshift', 'is_', 'is_not', 'isub', 'itemgetter',  \n'itruediv', 'ixor', 'le', 'length_hint', 'lshift', 'lt',  \n'matmul',  \n'methodcaller', 'mod', 'mul', 'ne', 'neg', 'not_', 'or_', 'pos',  \n'pow', 'rshift', 'setitem', 'sub', 'truediv', 'truth', 'xor']\nMost of the 54 names listed are self-evident. The group of names prefixed\nwith i  and the name of another operator—e.g., iadd , iand , etc.—\ncorrespond to the augmented assignment operators—e.g., += , &= , etc.\nThese change their first ar gument in place, if it is mutable; if not, the\nfunction works like the one without the i  prefix: it simply returns the result\nof the operation.\nOf the remaining operator  functions, methodcaller  is the last we\nwill cover . It is somewhat similar to attrgetter  and itemgetter  in\nthat it creates a function on the fly . The function it creates calls a method by\nname on the object given as ar gument, as shown in Example 7-15 .\nExample 7-15. Demo of methodcaller: second test shows the binding of\nextra ar guments\n>>> from operator  import methodcaller  \n>>> s = 'The time has come'  \n>>> upcase = methodcaller ('upper') \n>>> upcase(s) \n'THE TIME HAS COME'  \n>>> hyphenate  = methodcaller ('replace' , ' ', '-') \n>>> hyphenate (s) \n'The-time-has-come'\nThe first test in Example 7-15  is there just to show methodcaller  at\nwork, but if you need to use the str.upper  as a function, you can just\ncall it on the str  class and pass a string as ar gument, like this:",8125
103-Freezing Arguments with functools.partial.pdf,103-Freezing Arguments with functools.partial,">>> str.upper(s) \n'THE TIME HAS COME'\nThe second test in Example 7-15  shows that methodcaller  can also do a\npartial application to freeze some ar guments, like the\nfunctools.partial  function does. That is our next subject.\nFreezing Arguments with functools.partial\nThe functools  module provides several higher -order functions. W e saw\nreduce  in “Modern Replacements for map, filter , and reduce” . Another is\npartial : given a callable, it produces a new callable with some of the\nar guments of the original callable bound to pre-determined values. This is\nuseful to adapt a function that takes one or more ar guments to an API that\nrequires a callback with fewer ar guments. Example 7-16  is a trivial\ndemonstration.\nExample 7-16. Using partial to use a two-ar gument function wher e a one-\nar gument callable is r equir ed\n>>> from operator  import mul \n>>> from functools  import partial \n>>> triple = partial(mul, 3)  \n \n>>> triple(7)  \n \n21 \n>>> list(map(triple, range(1, 10)))  \n \n[3, 6, 9, 12, 15, 18, 21, 24, 27]\nCreate new triple  function from mul , binding first positional\nar gument to 3 .\nT est it.\nUse triple  with map ; mul  would not work with map  in this\nexample.\nA more useful example involves the unicode.normalize  function that\nwe saw in “Normalizing Unicode for Reliable Comparisons” . If you work\nwith text from many languages, you may want to apply\nunicode.normalize('NFC', s)  to any string s  before comparing\nor storing it. If you do that often, it’ s handy to have an nfc  function to do\nso, as in Example 7-17 .\nExample 7-17. Building a convenient Unicode normalizing function with\npartial\n>>> import unicodedata , functools  \n>>> nfc = functools .partial(unicodedata .normalize , 'NFC') \n>>> s1 = 'café' \n>>> s2 = 'cafe\u0301' \n>>> s1, s2 \n('café', 'café')  \n>>> s1 == s2 \nFalse \n>>> nfc(s1) == nfc(s2) \nTrue\npartial  takes a callable as first ar gument, followed by an arbitrary\nnumber of positional and keyword ar guments to bind.\nExample 7-18  shows the use of partial  with the tag  function from\nExample 7-9 , to freeze one positional ar gument and one keyword ar gument.\nExample 7-18. Demo of partial  applied to the function tag fr om\nExample 7-9\n>>> from tagger import tag \n>>> tag \n<function tag at 0x10206d1e0>  \n  \n>>> from functools  import partial \n>>> picture = partial(tag, 'img', class_='pic-frame ')  \n \n>>> picture(src='wumpus.jpeg ') \n'<img class=""pic-frame"" src=""wumpus.jpeg"" />'  \n  \n>>> picture \nfunctools.partial(<function tag at 0x10206d1e0>, 'img',  \nclass_='pic-frame')  \n  \n>>> picture.func  \n \n<function tag at 0x10206d1e0>  \n>>> picture.args \n('img',)  \n>>> picture.keywords  \n{'class_': 'pic-frame'}",2740
104-Further Reading.pdf,104-Further Reading,"Import tag  from Example 7-9  and show its ID.\nCreate picture  function from tag  by fixing the first positional\nar gument with 'img'  and the class_  keyword ar gument with\n'pic-frame' .\npicture  works as expected.\npartial()  returns a functools.partial  object.\nA functools.partial  object has attributes providing access to the\noriginal function and the fixed ar guments.\nThe functools.partialmethod  function does the same job as\npartial , but is designed to work with methods.\nThe functools  module also include higher -order functions designed to\nbe used as function decorators, such as cache  and singledispatch ,\namong others. Those functions are the covered in Chapter 9 , which also\nexplains how to implement custom decorators.4\nC h a p t e r  S u m m a r y\nThe goal of this chapter was to explore the first-class nature of functions in\nPython. The main ideas are that you can assign functions to variables, pass\nthem to other functions, store them in data structures, and access function\nattributes, allowing frameworks and tools to act on that information.\nHigher -order functions, a staple of functional programming, are common in\nPython. The sorted , min , and max  built-ins, and\nfunctools.partial  are examples of commonly used higher -order\nfunctions in the language. Using map , filter , and reduce  is not as\ncommon as it used to be—thanks to list comprehensions (and similar\nconstructs like generator expressions) and the addition of reducing built-ins\nlike sum , all , and any .\nCallables come in nine dif ferent flavors since Python 3.6, from the simple\nfunctions created with lambda  to instances of classes implementing\n__call__ . Generators and coroutines are also callable, although their\nbehavior is very dif ferent from other callables. All callables can be detected\nby the callable()  built-in. Callables of fer rich syntax for declaring\nformal parameters, including keyword-only parameters, positional-only\nparamenters, and annotations.\nLastly , we covered some functions from the operator  module and\nfunctools.partial , which facilitate functional programming by\nminimizing the need for the functionally challenged lambda  syntax.\nF u r t h e r  R e a d i n g\nThe next chapters continue our exploration of programming with function\nobjects. Chapter 8  is devoted to type hints in function parameters and return\nvalues. Chapter 9  dives into function decorators—a special kind of higher -\norder function—and the closure mechanism that makes them work.\nChapter 10  shows how first-class functions can simplify some classic\nobject-oriented design patterns.\nIn The Python Language Refer ence , “3.2. The standard type hierarchy”\npresents the nine callable types, along with all the other built-in types.\nChapter 7 of the Python Cookbook, Thir d Edition  (O’Reilly), by David\nBeazley and Brian K. Jones, is an excellent complement to the current\nchapter as well as Chapter 9  of this book, covering mostly the same\nconcepts with a dif ferent approach.\nSee PEP 3102 — Keyword-Only Ar guments  if you are interested in the\nrationale and use cases for that feature.\nA great introduction to functional programming in Python is A. M.\nKuchling’ s Python Functional Programming HOWT O . The main focus of\nthat text, however , is the use of iterators and generators, which are the\nsubject of Chapter 17 .\nThe StackOverflow question “Python: Why is functools.partial necessary?”\nhas a highly informative (and funny) reply by Alex Martelli, co-author of\nthe classic Python in a Nutshell .\nReflecting on the question “Is Python a functional language?”, I created one\nof my favorite talks: Beyond Paradigms , which I presented at PyCaribbean,\nPyBay and PyConDE. See the slides  and video  from the Berlin presentation\n—where I met Miroslav Šedivý and Jür gen Gmach, two of the technical\nreviewers of this book.\nS O A P B O X\nIs Python a Functional Language?\nSometime in the year 2000 I attended a Zope workshop at Zope\nCorporation in the United States when Guido van Rossum dropped by\nthe classroom (he was not the instructor). In the Q&A that followed,\nsomebody asked him which features of Python were borrowed from\nother languages. Guido’ s answer: “Everything that is good in Python\nwas stolen from other languages.”\nShriram Krishnamurthi, professor of Computer Science at Brown\nUniversity , starts his “T eaching Programming Languages in a Post-\nLinnaean Age” paper  with this:\nPr ogramming language “paradigms” ar e a moribund and tedious\nlegacy of a bygone age. Modern language designers pay them no\nr espect, so why do our courses slavishly adher e to them?\nIn that paper , Python is mentioned by name in this passage:\nWhat else to make of a language like Python, Ruby , or Perl? Their\ndesigners have no patience for the niceties of these Linnaean\nhierar chies; they borr ow featur es as they wish, cr eating melanges\nthat utterly defy characterization.\nKrishnamurthi ar gues that instead of trying to classify languages in\nsome taxonomy , it’ s more useful to consider them as aggregations of\nfeatures. His ideas inspired my talk Beyond Paradigms , mentioned at\nthe end of “Further Reading” .\nEven if it was not Guido’ s goal, endowing Python with first-class\nfunctions opened the door to functional programming. In his post\n“Origins of Python’ s Functional  Features” , he says that map , filter ,\nand reduce  were the motivation for adding lambda  to Python in the\nfirst place. All of these features were contributed together by Amrit\nPrem for Python 1.0 in 1994 (according to Misc/HIST OR Y  in the\nCPython source code).\nFunctions like map , filter , and reduce  first appeared in Lisp, the\noriginal functional language. However , Lisp does not limit what can be\ndone inside a lambda , because everything in Lisp is an expression.\nPython uses a statement-oriented syntax in which expressions cannot\ncontain statements, and many language constructs are statements—\nincluding try/catch , which is what I miss most often when writing\nlambdas . This is the price to pay for Python’ s highly readable\nsyntax.  Lisp has many strengths, but readability is not one of them.\nIronically , stealing the list comprehension syntax from another\nfunctional language—Haskell—significantly diminished the need for\nmap  and filter , and also for lambda .\nBesides the limited anonymous function syntax, the biggest obstacle to\nwider adoption of functional programming idioms in Python is the lack\nof tail-call elimination, an optimization that allows memory-ef ficient\ncomputation of a function that makes a recursive call at the “tail” of its\nbody . In another blog post, “T ail Recursion Elimination” , Guido gives\nseveral reasons why such optimization is not a good fit for Python. That\npost is a great read for the technical ar guments, but even more so\nbecause the first three and most important reasons given are usability\nissues. It is no accident that Python is a pleasure to use, learn, and\nteach. Guido made it so.\nSo there you have it: Python is not, by design, a functional language—\nwhatever that means. Python just borrows a few good ideas from\nfunctional languages.\nThe Pr oblem with Anonymous Functions\nBeyond the Python-specific syntax constraints, anonymous functions\nhave a serious drawback in any language: they have no name.\nI am only half joking here. Stack traces are easier to read when\nfunctions have names. Anonymous functions are a handy shortcut,\npeople have fun coding with them, but sometimes they get carried away\n—especially if the language and environment encourage deep nesting of5\nanonymous functions, like JavaScript on Node.js do. Lots of nested\nanonymous functions make debugging and error handling hard.\nAsynchronous programming in Python is more structured, perhaps\nbecause the limited lambda  syntax prevents its abuse and forces a\nmore explicit approach. Promises, futures, and deferreds are concepts\nused in modern asynchronous APIs. Along with coroutines, they\nprovide an escape from the so-called “callback hell.” I promise to write\nmore about asynchronous programming in the future, but this subject\nmust be deferred to Chapter 22 .\n1  “Origins of Python’ s Functional  Features” , from Guido’ s The History of Python blog.\n2  Calling a class usually creates an instance of that same class, but other behaviors are possible\nby overriding __new__ . W e’ll see an example of this in “Flexible Object Creation with\n__new__” .\n3  Why build a BingoCage  when we already have random.choice ? The choice  function\nmay return the same item multiple times, because the picked item is not removed from the\ncollection given. Calling BingoCage  never returns duplicate results—as long as the instance\nis filled with unique values.\n4  The source code  for functools.py  reveals that the functools.partial  class is\nimplemented in C and is used by default. If that is not available, a pure-Python implementation\nof partial  is available since Python 3.4.\n5  There is also the problem of lost indentation when pasting code to W eb forums, but I digress.",9135
105-A Default Parameter Value.pdf,105-A Default Parameter Value,"Chapter 8. T ype Hints in\nFunctions\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 8th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nIt should also be emphasized that Python will remain a dynamically\ntyped language, and the authors have no desire to ever make type hints\nmandatory , even by convention .\n— Guido van Rossum, Jukka Lehtosalo, and Łukasz\nLanga, PEP 484—T ype Hints\nT ype hints are the biggest change in the history of Python since the\nunification of types and classes  in Python 2.2, released in 2001. However ,\ntype hints do not benefit all Python users equally . That’ s why they should\nalways be optional.\nPEP 484—T ype Hints  introduced syntax and semantics for explicit type\ndeclarations in function ar guments, return values, and variables. The goal is\nto help developer tools find bugs in Python codebases via static analysis,\ni.e. without actually running the code through tests.1\nThe main beneficiaries are professional software engineers using IDEs\n(Integrated Development Environments) and CI (Continuous Integration).\nThe cost-benefit analysis that makes type hints attractive to that group does\nnot apply to all users of Python.\nPython’ s user base is much wider than that. It includes scientists, traders,\njournalists, artists, makers, analysts and students in many fields—among\nothers. For most of them, the cost of learning type hints is higher—unless\nthey already know a language with static types, subtyping, and generics.\nThe cost is likely to be higher and the benefits will be lower for many of\nthose users, given how they interact with Python, and the smaller size of\ntheir code bases and teams—often, “teams” of one. Python’ s default\ndynamic typing is simpler and more expressive when writing code for\nexploring data and ideas, as in data science, creative computing, and\nlearning,\nThis chapter focuses on Python’ s type hints in function signatures.\nChapter 15  explores type hints in the context of classes, and other typing\nmodule features.\nThe major topics in this chapter are:\nA hands-on introduction to gradual typing with Mypy .\nThe complementary perspectives of duck typing and nominal\ntyping.\nOverview of the main categories of types that can appear in\nannotations—this is about 60% of the chapter .\nT ype hinting variadic parameters ( *args , **kwargs ).\nLimitations and downsides of type hints and static typing.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThis chapter is completely new . T ype hints appeared in Python 3.5 after I\nwrapped up the first edition of Fluent Python .\nGiven the limitations of a static type system, the best idea of PEP 484 was\nto propose a gratual type system . Let’ s begin by defining what that means.\nA b o u t  g r a d u a l  t y p i n g\nPEP 484 introduced a gradual type system  to Python. Other languages with\ngradual type systems are Microsoft’ s T ypeScript, Dart (the language of the\nFlutter SDK, created by Google), and Hack (a dialect of PHP supported by\nFacebook’ s HHVM virtual machine). The Mypy type checker itself started\nas a language: a gradually typed dialect of Python with its own interpreter .\nGuido van Rossum convinced the creator of Mypy , Jukka Lehtosalo, to\nmake it a tool for checking annotated Python code.\nA gradual type system:\nIs optional.\nBy default, the type checker should not emit warnings for code that has\nno type hints. Instead, the type checker assumes the Any  type when it\ncannot determine the type of an object. The Any  type is considered\ncompatible with all other types.\nDoes not catch type err ors at runtime.\nT ype hints are used by static type checkers, linters, and IDEs to raise\nwarnings. They do not prevent inconsistent values to be passed to\nfunctions or assigned to variables at runtime.\nDoes not enhance performance.\nT ype annotations provide data that could, in theory , allow optimizations\nin the generated byte code, but such optimizations are not implemented\nin any Python runtime that I am aware in July 2021.\nThe best usability feature of gradual typing is that annotations are always\noptional.2\nW ith static type systems, most type constraints are easy to express, many\nare cumbersome, some are hard, and a few are impossible.  Y ou may very\nwell write an excellent piece of Python code, with good test coverage and\npassing tests, but still be unable to add type hints that satisfy a type checker .\nThat’ s ok, just leave out the problematic type hints and ship it!\nT ype hints are optional at all levels: you can have entire packages with no\ntype hints, you can silence the type checker when you import one of those\npackages into a module where you use type hints, and you can add special\ncomments to make the type checker ignore specific lines in your code.\nT I P\nSeeking 100% coverage of type hints is likely to stimulate type hinting without proper\nthought, only to satisfy the metric. It will also prevent teams from making the most of\nthe power and flexibility of Python. Code without type hints should naturally be\naccepted when annotations would make an API less user -friendly , or unduly complicate\nits implementation.\nG r a d u a l  t y p i n g  i n  p r a c t i c e\nLet’ s see how gradual typing works in practice, starting with a simple\nfunction and gradually adding type hints to it, guided by Mypy .\nN O T E\nThere are several Python type checkers compatible with PEP 484, including Google’ s\npytype , Microsoft’ s Pyright , Facebook’ s Pyre —in addition to type checkers embedded\nin IDEs such as PyCharm. I picked Mypy  for the examples because it’ s the best known.\nHowever , one of the others may be a better fit for some projects or teams. Pytype, for\nexample, is designed to handle codebases with no type hints and still provide useful\nadvice. It is more lenient than Mypy , and can also generate annotations for your code.\nW e will annotate a show_count  function that returns a string with a count\nand a singular or plural word, depending on the count:3\n>>> show_count (99, 'bird') \n'99 birds'  \n>>> show_count (1, 'bird') \n'1 bird'  \n>>> show_count (0, 'bird') \n'no birds'\nExample 8-1  shows the source code of show_count , without annotations.\nExample 8-1. show_count  fr om messages.py  without type hints.\ndef show_count (count, word): \n    if count == 1: \n        return f'1 {word}'  \n    count_str  = str(count) if count else 'no' \n    return f'{count_str} {word}s'\nStarting with Mypy\nT o begin type checking, I run the mypy  command on the messages.py\nmodule:\n…/no_hints/ $ pip install mypy  \n[lots of messages omitted...]  \n…/no_hints/ $ mypy messages.py  \nSuccess: no issues found in 1 source file\nMypy with default settings finds no problem with Example 8-2 :\nW A R N I N G\nI am using Mypy 0.910, the most recent release as I review this in July 2021. The Mypy\nIntroduction  warns it “is of ficially beta software. There will be occasional changes that\nbreak backward compatibility .” Mypy is giving me at least one report that is not the\nsame I got when I wrote this chapter in April 2020. By the time you read this, you may\nget dif ferent results than shown here.\nIf a function signature has no annotations, Mypy ignores it by default.\nThat’ s the spirit of gradual typing.\nFor this example, I also have pytest  unit tests. This is the code in\nmessages_test.py .\nExample 8-2. messages_test.py  without type hints.\nfrom pytest import mark \n \nfrom messages  import show_count  \n \n@mark.parametrize ('qty, expected' , [ \n    (1, '1 part' ), \n    (2, '2 parts' ), \n]) \ndef test_show_count (qty, expected ): \n    got = show_count (qty, 'part') \n    assert got == expected  \n \ndef test_show_count_zero (): \n    got = show_count (0, 'part') \n    assert got == 'no parts'\nNow let’ s add type hints, guided by Mypy .\nMaking Mypy More Strict\nThe command-line option --disallow-untyped-defs  makes Mypy\nflag any function definition that does not have type hints for all its\nparameters and for its return value.\nUsing --disallow-untyped-defs  on the test file produces three\nerrors and a note:\n…/no_hints/ $ mypy --disallow-untyped-defs messages_test.py  \nmessages.py:14: error: Function is missing a type annotation  \nmessages_test.py:10: error: Function is missing a type annotation  \nmessages_test.py:15: error: Function is missing a return type  \nannotation  \nmessages_test.py:15: note: Use ""-> None"" if function does not  \nreturn a value  \nFound 3 errors in 2 files (checked 1 source file)\nFor the first steps with gradual typing, I prefer to use another option: --\ndisallow-incomplete-defs . Initially , it tells me nothing:\n…/no_hints/ $ mypy --disallow-incomplete-defs messages_test.py  \nSuccess: no issues found in 1 source file\nNow I can add just the return type to show_count  in messages.py :\ndef show_count(count, word) -> str:\nThis is enough to make Mypy look at it. Using the same command line as\nbefore to check messages_test.py , will lead Mypy to look at\nmessages.py  again:\n…/no_hints/ $ mypy --disallow-incomplete-defs messages_test.py  \nmessages.py:14: error: Function is missing a type annotation for  \none or more arguments  \nFound 1 error in 1 file (checked 1 source file)\nNow I can gradually add type hints function by function, without getting\nwarnings about functions that I haven’ t annotated. This is a fully annotated\nsignature that satisfies Mypy:\ndef show_count (count: int, word: str) -> str:\nT I P\nInstead of typing command line options like --disallow-incomplete-defs ,\nyou can save your favorite as described in the Mypy configuration file  documentation.\nY ou can have global settings and per -module settings. Here is a simple mypy.ini  to\nget started:\n[mypy] \npython_version = 3.9  \nwarn_unused_configs = True  \ndisallow_incomplete_defs = True\nA Default Parameter V alue\nThe show_count  function in Example 8-2  only works with regular nouns.\nIf the plural can’ t be spelled by appending an 's' , we should let the user\nprovide the plural form, like this:\n>>> show_count (3, 'mouse', 'mice') \n'3 mice'\nLet’ s do a little “type driven development.” First we add a test that uses that\nthird ar gument. Don’ t for get to add the return type hint to the test function,\notherwise Mypy will not check it.\ndef test_irregular () -> None: \n    got = show_count (2, 'child', 'children' ) \n    assert got == '2 children'\nMypy detects the error:\n…/hints_2/ $ mypy messages_test .py \nmessages_test .py:22: error: Too many arguments  for ""show_count""  \nFound 1 error in 1 file (checked 1 source file)\nNow I edit show_count , adding the optional plural  parameter:\nExample 8-3. showcount  fr om hints_2/messages.py  with an\noptional parameter .\ndef show_count (count: int, singular : str, plural: str = '') -> str: \n    if count == 1: \n        return f'1 {singular}'  \n    count_str  = str(count) if count else 'no' \n    if not plural: \n        plural = singular  + 's' \n    return f'{count_str} {plural}'\nNow Mypy reports “Success.”\nW A R N I N G\nHere is one typing mistake that Python does not catch. Can you spot it?\ndef hex2rgb(color=str) -> tuple[int, int, int]:\nMypy’ s error report is not very helpful:\ncolors.py:24: error: Function is missing a type  \n    annotation for one or more arguments\nThe type hint for the color  ar gument should be color: str . I wrote color=str ,\nwhich is not an annotation: it sets the default value of color  to str .\nIn my experience, it’ s a common mistake and easy to overlook, especially in\ncomplicated type hints.\nThe following details are considered good style for type hints:\nThere should be no space between the parameter name and the : ,\nand one space after the : .\nThere should be spaces on both sides of the =  that precedes a\ndefault parameter value.\nOn the other hand, PEP 8 says there should be no spaces around the =  if\nthere is no type hint for that particular parameter .",12455
106-Types are defined by supported operations.pdf,106-Types are defined by supported operations,"C O D E  S T Y L E :  U S E  F L A K E 8  A N D  B L U E\nInstead of memorizing such silly rules, use tools like flake8  and blue .\nflake8  reports on code styling, among many other issues, and blue\nrewrites source code according to (most) rules embeded in the black\ncode formatting tool.\nGiven the goal of enforcing a “standard” coding style, blue  is better\nthan black  because it follows Python’ s own style of using single quotes\nby default, double quotes as an alternative:\n>>> ""I prefer single quotes""  \n'I prefer single quotes'\nThe preference for single quotes is embedded in repr() , among other\nplaces in CPython. The doctest  module depends on repr()  using\nsingle quotes by default.\nIf you you must use black , use the black -S  option. Then it will\nleave your quotes as they are.\nN O T E\nOne of the authors of blue  is Barry W arsaw , co-author of PEP 8, Python core\ndeveloper since 1994, and member of Python’ s Steering Council from 2019 to\npresent (July , 2021). W e are in very good company when we choose single quotes\nby default.\nUsing None  as a default\nIn Example 8-3  the parameter plural  is annotated as str , and the default\nvalue is '' , so there is no type conflict.\nI like that solution, but in other contexts None  is a better default. If the\noptional parameter expects a mutable type, then None  is the only sensible\ndefault—as we saw in “Mutable T ypes as Parameter Defaults: Bad Idea” .\nT o have None  as the default for the plural  parameter , here is how the\nsignature would look like:\nfrom typing import Optional  \n \ndef show_count (count: int, singular : str, plural: Optional [str] = \nNone) -> str:\nLet’ s unpack that:\nOptional[str]  means plural  may be a str  or None .\nY ou must explicitly provide the default value = None .\nIf you don’ t assign a default value to plural , the Python runtime will treat\nit as a required parameter . Remember: at runtime, type hints are ignored.\nNote that we need to import Optional  from the typing  module. When\nimporting types, it’ s good practice to use the syntax from typing\nimport X , to reduce the length of the function signatures.\nW A R N I N G\nOptional  is not a great name, because that annotation does not make the parameter\noptional. What makes it optional is assigning a default value to the parameter .\nOptional[str]  just means: the type of this parameter may be str  or NoneType .\nIn the Haskell and Elm languages, a similar type is named Maybe .\nNow that we’ve had a first practical view of gradual typing, let’ s consider\nwhat the concept of type  means in practice.\nT y p e s  a r e  d e f i n e d  b y  s u p p o r t e d  o p e r a t i o n s\nTher e ar e many definitions of the concept of type in the literatur e. Her e\nwe assume that type is a set of values and a set of functions that one can\napply to these values.\n— PEP 483: The Theory of T ype Hints\nIn practice, it’ s more useful to consider the set of supported operations as\nthe defining characteristic of a type.\nFor example, from the point of view of applicable operations, what are the\nvalid types for x  in the following function?\ndef double(x): \n    return x * 2\nThe x  parameter type may be numeric ( int , complex , Fraction ,\nnumpy.uint32  etc.) but it may also be a sequence ( str , tuple , list ,\narray ), an N-dimensional numpy.array  or any other type that\nimplements or inherits a __mul__  method that accepts an int  ar gument.\nHowever , consider this annotated double . Please ignore the missing return\ntype for now , let’ s focus on the parameter type:\nfrom collections  import abc \n \ndef double(x: abc.Sequence ): \n    return x * 2\nA type checker will reject that code. If you tell Mypy that x  is of type\nabc.Sequence , it will flag x * 2  as an error because the Sequence\nABC  does not implement or inherit the __mul__  method. At runtime, that\ncode will work with concrete sequences such as str , tuple , list ,\narray  etc.—as well as numbers, because at runtime the type hints are\nignored. But the type checker only cares about what is explicitly declared,\nand abc.Sequence  has no __mul__ .\nThat’ s why the title of this section is “T ypes are defined by supported\noperations”. The Python runtime accepts any object as the x  ar gument for\nboth versions of the double  function. The computation x * 2  may work,4\nor it may raise TypeError  if the operation is not supported by x . In\ncontrast, Mypy will declare x * 2  as wrong while analyzing the annotated\ndouble  source code, because it’ s an unsupported operation for the\ndeclared type: x: abc.Sequence .\nIn a gradual type system, we have the interplay of two dif ferent views of\ntypes:\nDuck typing\nThe view adopted by Smalltalk—the pioneering OO language—as well\nas Python, JavaScript, and Ruby . Objects have types, but variables\n(including parameters) are untyped. In practice, it doesn’ t matter what is\nthe declared type of the object, only what operations it actually\nsupports. If I can invoke birdie.quack() , then birdie  is a duck\nin this context. By definition, duck typing is only enforced at runtime,\nwhen operations on objects are attempted. This is more flexible than\nnominal typing , at the cost of allowing more errors at runtime.\nNominal typing\nThe view adopted by C++, Java, and C#, supported by annotated\nPython. Objects and variables have types. But objects only exist at\nruntime, and the type checker only cares about the source code where\nvariables (including parameters) are annotated with type hints. If Duck\nis a subclass of Bird , you can assign a Duck  instance to a parameter\nannotated as birdie: Bird . But in the body of the function, the type\nchecker considers the call birdie.quack()  illegal, because\nbirdie  is nominally a Bird , and that class does not provide the\n.quack()  method. It doesn’ t matter if the actual ar gument at runtime\nis a Duck , because nominal typing is enforced statically . The type\nchecker doesn’ t run any part of the program, it only reads the source\ncode. This is more rigid than duck typing , with the advantage of\ncatching some bugs earlier in a build pipeline, or even as the code is\ntyped in an IDE.5\nHere is a silly example that contrasts duck typing and nominal typing, as\nwell as static type checking and runtime behavior :\nExample 8-4. birds.py\nclass Bird: \n    pass \n \nclass Duck(Bird):  \n \n    def quack(self): \n        print('Quack!') \n \ndef alert(birdie):  \n \n    birdie.quack() \n \ndef alert_duck (birdie: Duck) -> None:  \n \n    birdie.quack() \n \ndef alert_bird (birdie: Bird) -> None:  \n \n    birdie.quack()\nDuck  is a subclass of Bird .\nalert  has no type hints, so the type checker ignores it.\nalert_duck  takes one ar gument of type Duck .\nalert_bird  takes one ar gument of type Bird .\nT ype checking birds.py  with Mypy , we see a problem:\n…/birds/ $ mypy birds.py  \nbirds.py:16: error: ""Bird"" has no attribute ""quack""  \nFound 1 error in 1 file (checked 1 source file)\nJust by analyzing the source code, Mypy sees that alert_bird  is\nproblematic: the type hint declares the birdie  parameter with type Bird ,\nbut the body of the function calls birdie.quack() —and the Bird\nclass has no such method.\nNow let’ s try to use the birds  module in daffy.py :6\nExample 8-5. daffy.py\nfrom birds import * \n \ndaffy = Duck() \nalert(daffy)       \n  \nalert_duck (daffy)  \n \nalert_bird (daffy)  \nV alid call, because alert  has no type hints.\nV alid call, because alert_duck  takes a Duck  ar gument, and daffy\nis a Duck .\nV alid call, because alert_bird  takes a Bird  ar gument, and daffy\nis a also a Bird —the superclass of Duck .\nRunning Mypy on daffy.py  raises the same error about the quack  call\nin the alert_bird  function defined in birds.py :\n…/birds/ $ mypy daffy.py  \nbirds.py:16: error: ""Bird"" has no attribute ""quack""  \nFound 1 error in 1 file (checked 1 source file)\nBut Mypy sees no problem with daffy.py  itself: the three function calls\nare OK.\nNow , if you run daffy.py , this is what you get:\n…/birds/ $ python3 daffy.py  \nQuack! \nQuack! \nQuack!\nEverything works! Duck typing FTW!\nAt runtime, Python doesn’ t care about declared types. It uses duck typing\nonly . Mypy flagged an error in alert_bird , but calling it with daffy\nworks fine at runtime. This may surprise many Pythonistas at first: a static\ntype checker will sometimes find errors in programs that we know will\nexecute.\nHowever , if months from now you are tasked with extending the silly bird\nexample, you may be grateful for Mypy . Consider this woody.py  module\nwhich also uses birds :\nExample 8-6. woody.py\nfrom birds import * \n \nwoody = Bird() \nalert(woody) \nalert_duck (woody) \nalert_bird (woody)\nMypy finds two errors while checking woody.py :\n…/birds/ $ mypy woody.py  \nbirds.py:16: error: ""Bird"" has no attribute ""quack""  \nwoody.py:5: error: Argument 1 to ""alert_duck"" has incompatible  \ntype ""Bird""; expected ""Duck""  \nFound 2 errors in 2 files (checked 1 source file)\nThe first error is in birds.py : the birdie.quack()  call in\nalert_bird , which we’ve seen before. The second error is in\nwoody.py : woody  is an instance of Bird , so the call\nalert_duck(woody)  is invalid because that function requires a Duck .\nEvery Duck  is a Bird , but not every Bird  is a Duck .\nAt runtime, none of the calls in woody.py  succeed. The succession of\nfailures is best illustrated in a console session with callouts:\nExample 8-7. Runtime err ors and how Mypy could have helped.\n>>> from birds import * \n>>> woody = Bird() \n>>> alert(woody)  \n \nTraceback  (most recent call last): \n  ... \nAttributeError : 'Bird' object has no attribute  'quack' \n>>> \n>>> alert_duck (woody) \n \nTraceback  (most recent call last): \n  ... \nAttributeError : 'Bird' object has no attribute  'quack' \n>>> \n>>> alert_bird (woody)  \n \nTraceback  (most recent call last): \n  ... \nAttributeError : 'Bird' object has no attribute  'quack'\nMypy could not detect this error because there are no type hints in\nalert .\nMypy reported the problem: Argument 1 to ""alert_duck""\nhas incompatible type ""Bird""; expected ""Duck"" .\nMypy has been telling us since Example 8-4  that the body of the\nalert_bird  function is wrong: ""Bird"" has no attribute\n""quack"" .\nThis little experiment shows that duck typing is easier to get started and is\nmore flexible, but allows unsupported operations to cause errors at runtime.\nNominal typing detects errors before runtime, but sometimes can reject\ncode that actually runs—such as the call alert_bird(daffy)  in\nExample 8-5 . Even if it sometimes works, the alert_bird  function is\nmisnamed: its body does require an object that supports the .quack()\nmethod, which Bird  doesn’ t have.\nIn this silly example, the functions are one-liners. But in real code they\ncould be longer , they could pass the birdie  ar gument to more functions,\nand the origin of the birdie  ar gument could be many function calls away ,\nmaking it hard to pinpoint the cause of a runtime error . The type checker\nprevents many such errors from ever happening at runtime.",11153
107-The Any type.pdf,107-The Any type,"N O T E\nThe value of type hints is questionable in the tiny examples that fit in a book. The\nbenefits grow with the size of the codebase. That’ s why companies with millions of lines\nof Python code—like Dropbox, Google, and Facebook—invested in teams and tools to\nsupport the company-wide adoption of type hints, and have significant and increasing\nportions of their Python codebases type checked in their CI pipelines.\nIn this section we explored the relationship of types and operations in duck\ntyping and nominal typing, starting with the simple double()  function—\nwhich we left without proper type hints. Now we will tour the most\nimportant types used for annotating functions. W e’ll see a good way to add\ntype hints to double()  when we reach “Static Protocols” . But before we\nget to that, there are more fundamental types to know .\nT y p e s  u s a b l e  i n  a n n o t a t i o n s\nPretty much any Python type can be used in type hints, but there are\nrestrictions and recommendations. In addition, the typing  module\nintroduced special constructs with semantics that are sometimes surprising.\nThis section covers all the major types you can use with annotations:\ntyping.Any ;\nSimple types and classes;\ntyping.Optional  and typing.Union ;\nGeneric collections, including tuples and mappings;\nAbstract Base Classes;\nGeneric iterables;\nParameterized generics and TypeVar ;\ntyping.Protocols —the key to static duck typing ;\ntyping.Callable ;\ntyping.NoReturn —a good way to end this list.\nW e’ll cover each of these in turn, starting with a type that is strange,\napparently useless, but crucially important.\nThe Any  type\nThe keystone of any gradual type system is the Any  type, also known as the\ndynamic type . When a type checker sees an untyped function like this:\ndef double(x): \n    return x * 2\nIt assumes this:\ndef double(x: Any) -> Any: \n    return x * 2\nThat means the x  ar gument and the return value can be of any type,\nincluding dif ferent types. Any  is assumed to support every possible\noperation.\nContrast Any  with object . Consider this signature:\ndef double(x: object) -> object:\nThis function also accepts ar guments of every type, because every type is a\nsubtype-of  object .\nHowever , a type checker will reject this function:\ndef double(x: object) -> object: \n    return x * 2\nThe problem is that object  does not support the __mul__  operation.\nThis is what Mypy reports:\n…/birds/ $ mypy double_object.py  \ndouble_object.py:2: error: Unsupported operand types for *  \n(""object"" and ""int"")  \nFound 1 error in 1 file (checked 1 source file)\nMore general types have narrower interfaces, i.e. they support less\noperations. The object  class implements fewer operations than\nabc.Sequence , which implements fewer operations than\nabc.MutableSequence , which implements fewer operations than\nlist .\nBut Any  is a magic type that sits at the top and the bottom of the type\nhierarchy . It’ s simultaneously the most general type—so that an ar gument\nn: Any  accepts values of every type—and the most specialized type,\nsupporting every possible operation. At least, that’ s how the type checker\nunderstands Any .\nOf course, no type can support every possible operation, so using Any\nprevents the type checker from fulfilling its core mission: detecting\npotentially illegal operations before your program crashes with a runtime\nexception.\nSubtype-of versus Consistent-with\nT raditional object-oriented nominal type systems rely on the is subtype-of\nrelationship. Given a class T1  and a subclass T2 , then T2  is subtype-of  T1 .\nConsider this code:\nclass T1: \n    ... \n \nclass T2(T1): \n    ... \n \ndef f1(p: T1) -> None: \n    ... \n \no2 = T2() \n \nf1(o2)  # OK\nThe call f1(o2)  is an application of the Liskov Substitution Principle—\nLSP . Barbara Liskov  actually defined is-sub-type-of  in terms of supported\noperations: if an object of type T2  substitutes an object of type T1  and the\nprogram still behaves correctly , then T2  is subtype-of  T1 .\nContinuing from the previous code, this shows a violation of the LSP:\ndef f2(p: T2) -> None: \n    ... \n \no1 = T1() \n \nf2(o1)  # type error\nFrom the point of view of supported operations, this makes perfect sense: as\na subclass, T2  inherits and must support all operations that T1  does. So an\ninstance of T2  can be used anywhere a instance of T1  is expected. But the\nreverse is not necessarily true: T2  may implement additional methods, so an\ninstance of T1  may not be used everywhere an instance of T2  is expected.\nThis focus on supported operations is reflected in the name behavioral\nsubtyping , also used to refer to the LSP .\nIn a gradual type system, there is another relationship: consistent-with ,\nwhich applies wherever subtype-of  applies, with special provisions for type\nAny .\nThe rules for consistent-with  are:\n1 . Given T1  and a subtype T2 , then T2  is consistent-with  T1  (Liskov\nsubstitution).\n2 . Every type is consistent-with  Any : you can pass objects of every\ntype to an ar gument declared of type Any .\n3 . Any  is consistent-with  every type: you can always pass an object\nof type Any  where an ar gument of another type is expected.\nConsidering the previous definitions of the objects o1  and o2 , here are\nexamples of valid code, illustrating rules #2 and #3:7",5376
108-Optional and Union types.pdf,108-Optional and Union types,"def f3(p: Any) -> None: \n    ... \n \no0 = object() \no1 = T1() \no2 = T2() \n \nf3(o0)  # \nf3(o1)  #  all OK: rule #2  \nf3(o2)  # \n \ndef f4():  # implicit return type: `Any`  \n    ... \n \no4 = f4()  # inferred type: `Any`  \n \nf1(o4)  # \nf2(o4)  #  all OK: rule #3  \nf3(o4)  #\nEvery gradual type system needs a wildcard type like Any .\nT I P\nThe verb “to infer” is a fancy synomym for “to guess”, used in the context of type\nanalysis. Modern type checkers in Python and other languages don’ t require type\nannotations everywhere because they can infer the type of many expressions. For\nexample, if I write x = len(s) * 10 , the type checker doesn’ t need an explicit\nlocal declaration to know that x  is an int , as long as it can find type hints for the len\nbuilt-in.\nNow we can explore the rest of the types used in annotations.\nSimple types and classes\nSimple types like int , float , str , bytes  may be used directly in type\nhints. Concrete classes from the standard library , external packages, or user\ndefined—FrenchDeck , Vector2d , and Duck —may also be used in\ntype hints.\nAbstract Base Classes are also useful in type hints. W e’ll get back to them\nas we study collection types, and in “Abstract Base Classes” .\nAmong classes, is consistent-with  is defined like is subtype-of : a subclass is\nconsistent-with  all its superclasses.\nHowever , “practicality beats purity” so there is an important exception:\nINT  I S  C O N S I S T E N T - W I T H  COMPLEX\nThere is no nominal subtype relationship between the built-in types int , float  and\ncomplex : they are direct subclasses of object . But PEP 484 declares  that int  is\nconsistent-with  float , and float  is consistent-with  complex . It makes sense in\npractice: int  implements all operations that float  does, and int  implements\nadditional ones as well—bitwise operations like & , | , <<  etc. The end result is: int  is\nconsistent-with  complex . For i = 3 , i.real  is 3 , and i.imag  is 0 .\nOptional  and Union  types\nW e saw the Optional  special type in “Using None  as a default” . It solves\nthe problem of having None  as a default, as in this example from that\nsection:\nfrom typing import Optional  \n \ndef show_count (count: int, singular : str, plural: Optional [str] = \nNone) -> str:\nThe construct Optional[str]  is actually a shortcut for Union[str,\nNone]  which means the type of plural  may be str  or None .\nThe ord  built-in function’ s signature is a simple example of Union —it\naccepts str  or bytes , and returns an int :\ndef ord(c: Union[str, bytes]) -> int: ...8\nT I P\nIn “Dual-Mode str and bytes APIs”  we saw functions that accept either str  or bytes\nar guments but return str  if the ar gument was str  or bytes  if the ar guments was\nbytes . In those cases, the return type is determined by the input type, so Union  is not\nan accurate solution. T o properly annotate such functions, we need a type variable—\npresented in “Parameterized generics and TypeVar ” —or overloading, which we’ll see\nin “Overloaded signatures” .\nHere is an example of a function that takes a str , but may return a str  or\na float :\nfrom typing import Union \n \ndef parse_token (token: str) -> Union[str, float]: \n    try: \n        return float(token) \n    except ValueError : \n        return token\nIf possible, avoid creating functions that return Union  types, as they put an\nextra burden on the user—forcing them to check the type of the returned\nvalue at runtime to know what to do with it. But the parse_token  above\nis a reasonable use case in the context of a simple expression evaluator .\nUnion[]  requires at least two types. Nested Union  types have the same\nef fect as a flattened Union . So this type hint:\nUnion[A, B, Union[C, D, E]]\nis the same as:\nUnion[A, B, C, D, E]\nUnion  is more useful with types that are not consistent among themselves.\nFor example: Union[int, float]  is redundant because int  is",3965
109-Generic collections.pdf,109-Generic collections,"consistent-with  float . If you just use float  to annotate the parameter , it\nwill accept int  values as well.\nB E T T E R  S Y N T A X  F O R  U N I O N  I N  P Y T H O N  3 . 1 0\nIn Python 3.10 we can write str | float  instead of Union[str, float] . It’ s\nshorter , more readable, and doesn’ t require importing typing.Union . For more, see\nPEP 604—Complementary syntax for Union[] .\nGeneric collections\nMost Python collections are heterogeneous. For example, you can put any\nmixture of dif ferent types in a list . However , in practice that’ s not very\nuseful: if you put objects in a collection, you are likely to want to operate\non them later , and usually this means they must share at least one common\nmethod.\nGeneric types can be declared with type parameters to specify the type of\nthe items they can handle.\nFor example, a list  can be parameterized to constrain the type of the\nelements in it:\nExample 8-8. tokenize  with type hints for Python ≥ 3.9\ndef tokenize (text: str) -> list[str]: \n    return text.upper().split()\nIn Python ≥ 3.9, that means tokenize  returns a list  where every item\nis of type str .\nThe annotations stuff: list  and stuff: list[Any]  mean the\nsame thing: stuff  is a list of objects of any type.9\nT I P\nIf you are using Python 3.8 or earlier the concept is the same, but you need more code to\nmake it work—as explained the optional box “Legacy Support and Deprecated\nCollection T ypes” .\nPEP 585—T ype Hinting Generics In Standard Collections  lists collections\nfrom the standard library accepting generic type hints. The following list\nshows only those collections that use the simplest form of generic type hint:\ncontainer[item] .\nlist        collections.deque        abc.Sequence    \nabc.MutableSequence  \nset         abc.Container            abc.Set         \nabc.MutableSet  \nfrozenset   abc.Collection\nThe tuple  and mapping types support more complex type hints, as we’ll\nsee in their respective sections.\nAs of Python 3.10, there is no good way to annotate array.array  taking\ninto account the typecode  constructor ar gument which determines\nwhether integers or floats are stored in the array . An even harder problem is\nhow to typecheck integer ranges to prevent OverflowError  at runtime\nwhen adding elements to arrays. For example, an array  with\ntypecode='B'  can only hold int  values from 0 to 255. Currently ,\nPython’ s static type system is not up to this challenge.\nL E G A C Y  S U P P O R T  A N D  D E P R E C A T E D  C O L L E C T I O N\nT Y P E S\n(Y ou may skip this box if you only use Python 3.9 or later .)\nFor Python 3.7 and 3.8, you need a __future__  import to make the\n[]  notation work with built-in collections such as list :\nE x a m p l e  8 - 9 .  tokenize  w i t h  t y p e  h i n t s  f o r  P y t h o n  ≥  3 . 7\nfrom __future__  import annotations  \n \ndef tokenize (text: str) -> list[str]: \n    return text.upper().split()\nThat __future__  import does not work with Python 3.6 or earlier .\nThis is how to annotate tokenize  in a way that works with Python ≥\n3.5:\nE x a m p l e  8 - 1 0 .  tokenize  w i t h  t y p e  h i n t s  f o r  P y t h o n  ≥  3 . 5\nfrom typing import List \n \ndef tokenize (text: str) -> List[str]: \n    return text.upper().split()\nT o provide the initial support for generic type hints, the authors of PEP\n484 created dozens of generic types in the typing  module. T able 8-1\nshows some of them. For the full list, visit the typing  documentation.\n \nT\na\nb\nl\ne  \n8\n-\n1\n.  \nS\no\nm\ne  \nc\no\nl\nl\ne\nc\nt\ni\no\nn  \nt\ny\np\ne\ns  \na\nn\nd  \nt\nh\ne\ni\nr  \nt\ny\np\ne  \nh\ni\nn\nt  \ne\nq\nu\ni\nv\na\nl\ne\nn\nt\ns\n \ncollection type hint equivalent\n \nlist typing.List\nset typing.Set\nfrozenset typing.FrozenSet\ncollections.deque typing.Deque\ncollections.abc.MutableSequence typing.MutableSequence\ncollections.abc.Sequence typing.Sequence\ncollections.abc.Set typing.AbstractSet",3973
110-Tuple types.pdf,110-Tuple types,"collections.abc.MutableSet typing.MutableSet\n \nPEP 585—T ype Hinting Generics In Standard Collections  started a\nmulti-year process to improve the usability of generic type hits. W e can\nsummarize that process in 4 steps:\n1 . Introduce from __future__ import annotations\nin Python 3.7 to enable the use of standard library classes as\ngenerics with list[str]  notation.\n2 . Make that behavior the default in Python 3.9: list[str]\nnow works without the future  import.\n3 . Deprecate all the redundant generic types from the typing\nmodule.  Deprecation warnings will not be issued by the\nPython interpreter because type checkers should flag the\ndeprecated types when the checked program tar gets Python 3.9\nor newer .\n4 . Remove those redundant generic types in the first version of\nPython released 5 years after Python 3.9. At the current\ncadence, that could be Python 3.14, a.k.a as Python Pi.\nNow let’ s see how to annotate generic tuples.\nT uple types\nThere are three ways to annotate tuple types:\n1 . tuples as records;\n2 . tuples as records with named fields;\n3 . tuples as immutable sequences.10\nT uples as records\nIf you’re using a tuple  as a record, use the tuple  built-in and declare the\ntypes of the fields within [] .\nFor example, the type hint would be tuple[str, float, str]  to\naccept a tuple with city name, population and country: ('Shanghai',\n24.28, 'China') .\nConsider a function that takes a pair of geographic coordinates and returns a\nGeohash , used like this:\n>>> shanghai  = 31.2304, 121.4737  \n>>> geohash(shanghai ) \n'wtw3sjq6q'\nThis is how geohash  is defined, using the geolib  package from PyPI:\nExample 8-1 1. coordinates.py  with the geohash  function.\nfrom geolib import geohash as gh  # type: ignore  \n  \n \nPRECISION  = 9 \n \ndef geohash(lat_lon: tuple[float, float]) -> str:  \n \n    return gh.encode(*lat_lon, PRECISION )\nThis comment stops Mypy from reporting that the geolib  package\ndoesn’ t have type hints.\nlat_lon  parameter annotated as a tuple  with two float  fields.\nT I P\nFor Python < 3.9, import and use typing.Tuple  in type hints. It is deprecated but\nwill remain in the standard library at least until 2024.\nT uples as records with named fields\nT o annotate a tuple with many fields, or specific types of tuple your code\nuses in many places, I highly recommend using typing.NamedTuple —\nas seen in Chapter 5 . Here is a variation of Example 8-1 1  with\nNamedTuple :\nExample 8-12. coordinates_named.py  with the NamedTuple\nCoordinates  and the geohash  function.\nfrom typing import NamedTuple  \n \nfrom geolib import geohash as gh  # type: ignore  \n \nPRECISION  = 9 \n \nclass Coordinate (NamedTuple ): \n    lat: float \n    lon: float \n \ndef geohash(lat_lon: Coordinate ) -> str: \n    return gh.encode(*lat_lon, PRECISION )\nAs explained in “Overview of data class builders” ,\ntyping.NamedTuple  is a factory for tuple  subclasses, so\nCoordinate  is consistent-with  tuple[float, float]  but the\nreverse is not true—after all, Coordinate  has extra methods added by\nNamedTuple , like ._asdict() , and could also have user -defined\nmethods.\nIn practice, this means that it is typesafe to pass a Coordinate  instance\nto the display  function defined below .\ndef display(lat_lon: tuple[float, float]) -> str: \n    lat, lon = lat_lon \n    ns = 'N' if lat >= 0 else 'S' \n    ew = 'E' if lon >= 0 else 'W' \n    return f'{abs(lat):0.1f}°{ns}, {abs(lon):0.1f}°{ew}'\nT uples as immutable sequences\nT o annotate tuples of unspecified length that are used as immutable lists you\nmust specify a single type, followed by a comma and ...  (that’ s Python’ s\nellipsis token, made of three periods, not Unicode U+2026 —\nHORIZONTAL ELLIPSIS ).\nFor example, tuple[int, ...]  is a tuple with int  items.\nThe ellipsis indicates that any number of elements >= 1 is acceptable. There\nis no way to specify fields of dif ferent types for tuples of arbitrary length.\nThe annotations stuff: tuple[Any, ...]  and stuff: tuple\nmean the same thing: stuff  is a tuple of unspecified length with objects of\nany type.\nHere is a columnize  function that transforms a sequence into a table of\nrows and cells in the form of list of tuples with unspecified lengths. This is\nuseful to display items in columns, like this:\n>>> animals = 'drake fawn heron ibex koala lynx tahr xerus yak  \nzapus'.split() \n>>> table = columnize (animals) \n>>> table \n[('drake', 'koala', 'yak'), ('fawn', 'lynx', 'zapus'), ('heron',  \n'tahr'),  \n ('ibex', 'xerus')]  \n>>> for row in table: \n...     print(''.join(f'{word:10}'  for word in row)) \n... \ndrake     koala     yak  \nfawn      lynx      zapus  \nheron     tahr  \nibex      xerus\nExample 8-13  shows the implementation of columnize . Note the return\ntype:\n`list[tuple[str, ...]]` .\nExample 8-13. columnize.py  r eturns a list of tuples of strings.\nfrom collections.abc  import Sequence  \n \ndef columnize (sequence : Sequence [str], num_columns : int = 0) -> \nlist[tuple[str, ...]]: \n    if num_columns  == 0:",5069
111-Abstract Base Classes.pdf,111-Abstract Base Classes,"num_columns  = round(len(sequence ) ** .5) \n    num_rows , reminder  = divmod(len(sequence ), num_columns ) \n    num_rows  += bool(reminder ) \n    return [tuple(sequence [i::num_rows ]) for i in range(num_rows )]\nGeneric mappings\nGeneric mapping types are annotated as MappingType[KeyType,\nValueType] . The built-in dict  and the mapping types in\ncollections  and collections.abc  accept that notation in Python\n≥ 3.9. For earlier versions, you must use typing.Dict  and other\nmapping types from the typing  module, as described in “Legacy Support\nand Deprecated Collection T ypes” .\nExample 8-14  shows a practical use of a function returning an inverted\nindex  to search Unicode characters by name—a variation of Example 4-21\nmore suitable for server -side code that we’ll study in Chapter 22 .\nGiven starting and ending Unicode character codes, name_index  returns\na dict[str, set[str]]  which is an inverted index mapping each\nword to a set of characters that have that word in their names. For example,\nafter indexing ASCII characters from 32 to 64, here are the sets of\ncharacters mapped to the words 'SIGN'  and 'DIGIT' , and how to find\nthe character named 'DIGIT EIGHT' :\n>>> index = name_index (32, 65) \n>>> index['SIGN'] \n{'$', '>', '=', '+', '<', '%', '#'}  \n>>> index['DIGIT'] \n{'8', '5', '6', '2', '3', '0', '1', '4', '7', '9'}  \n>>> index['DIGIT'] & index['EIGHT'] \n{'8'}\nBelow is the source code for charindex.py  with the name_index\nfunction. Besides a dict[]  type hint, this example has three features\nappearing for the first time in the book.\nExample 8-14. charindex.py\nimport sys \nimport re \nimport unicodedata  \nfrom collections.abc  import Iterator  \n \nRE_WORD = re.compile(r'\w+') \nSTOP_CODE  = sys.maxunicode  + 1 \n \ndef tokenize (text: str) -> Iterator [str]:  \n \n    """"""return iterable of uppercased words""""""  \n    for match in RE_WORD.finditer (text): \n        yield match.group().upper() \n \ndef name_index (start: int = 32, end: int = STOP_CODE ) -> dict[str, \nset[str]]: \n    index: dict[str, set[str]] = {}  \n \n    for char in (chr(i) for i in range(start, end)): \n        if name := unicodedata .name(char, ''):  \n \n            for word in tokenize (name): \n                index.setdefault (word, set()).add(char) \n    return index\ntokenize  is a generator function. Chapter 17  is about generators.\nThe local variable index  is annotated. W ithout the hint, Mypy says:\nNeed type annotation for 'index' (hint: ""index:\ndict[<type>, <type>] = ..."") .\nI used the walrus operator :=  in the if  condition. It assigns the result\nof the unicodedata.name()  call to name , and the whole\nexpression evaluates to that result. When the result is '' , that’ s falsy\nand the index  is not updated.\nN O T E\nWhen using a dict  as a record, it is common to have all keys of the str  type, with\nvalues of dif ferent types depending on the keys. That is covered in “T ypedDict” ,\nChapter 15 .\nAbstract Base Classes11\nBe conservative in what you send, be liberal in what you accept.\n— Postel’ s law , a.k.a. the Robustness Principle\nT able 8-1  list several abstract classes from collections.abc . Ideally , a\nfunction should accept ar guments of those abstract types—or their typing\nequivalents before Python 3.9—and not concrete types. This gives more\nflexibility to the caller .\nConsider this function signature:\nfrom collections.abc  import Mapping \n \ndef name2hex (name: str, color_map : Mapping[str, int]) -> str:\nUsing abc.Mapping  allows the caller to provide an instance of dict ,\ndefaultdict , ChainMap , a UserDict  subclass or any other type that\nis a subtype-of  Mapping .\nIn contrast, consider this signature:\ndef name2hex (name: str, color_map : dict[str, int]) -> str:\nNow color_map  must be a dict  or one of its subtypes such as\ndefaultDict  or OrderedDict . In particular , a subclass of\ncollections.UserDict  would not pass the type check for\ncolor_map , despite being the recommended way to create user -defined\nmappings, as we saw in “Subclassing UserDict  Instead of dict ” . Mypy\nwould reject a UserDict  or an instance of a class derived from it, because\nUserDict  is not a subclass of dict ; they are siblings. Both are\nsubclasses of abc.MutableMapping .\nTherefore, in general it’ s better to use abc.Mapping  or\nabc.MutableMapping  in parameter type hints, instead of dict  (or\ntyping.Dict  in legacy code). If the name2hex  function doesn’ t need\nto mutate the given color_map , the most accurate type hint for\ncolor_map  is abc.Mapping . That way , the caller doesn’ t need to\nprovide an object that implements methods like setdefault , pop  and12\nupdate  which are part of the MutableMapping  interface, but not of\nMapping . This has to do with the second part of Postel’ s law: “be liberal\nin what you accept.”\nPostel’ s law also tells us to be conservative in what we send. The return\nvalue of a function is always a concrete object, so the return type hint\nshould be a concrete type, as in the example from “Generic collections” —\nwhich uses list[str] .\ndef tokenize (text: str) -> list[str]: \n    return text.upper().split()\nUnder the entry of typing.List , the Python documentation says:\nGeneric version of list . Useful for annotating r eturn types. T o annotate\nar guments it is pr eferr ed to use an abstract collection type such as\nSequence  or Iterable .\nA similar comment appears in the entries for typing.Dict  and\ntyping.Set .\nRemember that most ABCs from collections.abc  and other concrete\nclasses from collections , as well as built-in collections, support\ngeneric type hint notation like collections.deque[str]  starting\nwith Python 3.9. The corresponding typing  collections are only needed to\nsupport code written in Python 3.8 or earlier . The full list of classes that\nbecame generic appears in section Implementation  of PEP 585—T ype\nHinting Generics In Standard Collections .\nT o wrap up our discussion of ABCs in type hints, we need to talk about the\nnumbers  ABCs.\nThe Fall of the Numeric T ower\nSince Python 2.6, the numbers  module defines a hierarchy of ABCs with\nNumber  at the top, then Complex , Real , Rational , and Integral .\nThose ABCs allow isinstance  checks independent of concrete numeric\ntypes. For example, isinstance(x, numbers.Real)  is True  for x",6362
112-Iterable.pdf,112-Iterable,"of type float , but also for NumPy types like float32 , longdouble\netc.\nThose ABCs work perfectly well for runtime type checking, but not for\nstatic type checking, for one main reason: the Number  ABC defines no\nmethod, therefore a typechecker would not let your code do anything with a\nvalue declared or inferred to be of the Number  type, which makes it\nuseless. As of July 2021, Mypy does not support the use of the numbers\nABCs in type hints. See Mypy issue int is not a Number? .\nSection The Numeric T ower  of PEP 484 rejects the numbers  ABCs and\ndictates that the built-in types complex , float , and int  should be\ntreated as special cases, as explained in “int is consistent-with complex” .\nT I P\nT o annotate numeric parameters without hard coding concrete types, use numeric\nprotocol types, covered in “Runtime checkable static protocols” . “Static Protocols”  is a\npre-requisite for that section.\nIterable\nThe typing.List  documentation I just quoted recommends Sequence\nand Iterable  for function parameter type hints.\nOne example of Iterable  ar gument appears the math.fsum  function\nfrom the standard library:\ndef fsum(__seq: Iterable [float]) -> float:\nS T U B  F I L E S  A N D  T H E  T Y P E S H E D  P R O J E C T .\nAs of Python 3.10, the standard library has no annotations but Mypy , PyCharm etc. can\nfind the necessary type hints in the T ypeshed  project, in the form of stub files : special\nsource files with a .pyi  extension that have annotated function and method signatures,\nwithout the implementation—much like header files in C.\nThe signature for math.fsum  is in /stdlib/2and3/math.pyi . The leading\nunderscores in __seq  are a PEP 484 convention for positional-only parameters,\nexplained in “Annotating positional-only and variadic parameters” .\nExample 8-15  is another example using an Iterable  parameter that\nproduces items that are tuple[str, str] . Here is how the function is\nused:\n>>> l33t = [('a', '4'), ('e', '3'), ('i', '1'), ('o', '0')] \n>>> text = 'mad skilled noob powned leet'  \n>>> from replacer  import zip_replace  \n>>> zip_replace (text, l33t) \n'm4d sk1ll3d n00b p0wn3d l33t'\nAnd here is how it’ s implemented:\nExample 8-15. replacer.py\nfrom collections.abc  import Iterable  \n \nFromTo = tuple[str, str]  \n \n \ndef zip_replace (text: str, changes: Iterable [FromTo]) -> str:  \n \n    for from_, to in changes: \n        text = text.replace(from_, to) \n    return text\nFromTo  is a type alias : I assigned tuple[str, str]  to FromTo ,\nto make the signature of zip_replace  more readable.\nchanges  needs to be an Iterable[FromTo] ; that’ s the same as\nIterable[tuple[str, str]] , but shorter and easier to read.",2712
113-Parameterized generics and TypeVar.pdf,113-Parameterized generics and TypeVar,"E X P L I C I T  TYPEALIAS  I N  P Y T H O N  3 . 1 0\nPEP 613—Explicit T ype Aliases  introduced a special type, TypeAlias , to make the\nassignments that create type aliases more visible and easier to typecheck. Starting with\nPython 3.10, this is the preferred way to create type aliases:\nfrom typing import TypeAlias  \n \nFromTo: TypeAlias  = tuple[str, str]\nabc.Iterable  versus abc.Sequence\nBoth math.fsum  and replacer.zip_replace  must iterate over the\nentire Iterable  ar guments to return a result. Given an endless iterable\nsuch as the itertools.cycle  generator as input, these functions would\nconsume all memory and crash the Python process. Despite this potential\ndanger , it is fairly common in modern Python to of fer functions that accept\nan Iterable  input even if they must process it completely to return a\nresult. That gives the caller the option of providing input data as a generator\ninstead of a pre-built sequence, potentially saving a lot of memory if the\nnumber of input items is lar ge.\nOn the other hand, the columnize  function from Example 8-13  needs a\nSequence  parameter , and not an Iterable , because it must get the\nlen()  of the input to compute the number of rows up front.\nLike Sequence , Iterable  is best used as a parameter type. It’ s too\nvague as a return type. A function should be more precise about the\nconcrete type it returns.\nClosely related to Iterable  is the Iterator  type, used as a return type\nin Example 8-14 . W e’ll get back to it in Chapter 17  which is about\ngenerators and classic iterators.\nParameterized generics and TypeVar\nA parameterized generic is a generic type, written as list[T]  where T  is\na type variable that will be bound to a specific type with each usage. This\nallows a parameter type to be reflected on the result type.\nExample 8-16  defines sample , a function that takes two ar guments: a\nSequence  of elements of type T , and an int . It returns a list  of\nelements of the same type T , picked at random from the first ar gument.\nThis is the implementation:\nExample 8-16. sample.py\nfrom collections.abc  import Sequence  \nfrom random import shuffle \nfrom typing import TypeVar \n \nT = TypeVar('T') \n \ndef sample(population : Sequence [T], size: int) -> list[T]: \n    if size < 1: \n        raise ValueError ('size must be >= 1' ) \n    result = list(population ) \n    shuffle(result) \n    return result[:size]\nHere are two examples why I used a type variable in sample :\n1 . If called with a tuple of type tuple[int, ...] —which is\nconsistent-with  Sequence[int] —then the type parameter is\nint , so the return type is list[int] ;\n2 . If called with a str —which is consistent-with  Sequence[str]\n—then the type parameter is str , so the return type is\nlist[str] .\nW H Y  I S  TYPEVAR  N E E D E D ?\nThe authors of PEP 484 wanted to introduce type hints by adding the typing  module\nand not changing anything else in the language. W ith clever metaprogramming they\ncould make the []  operator work on classes like Sequence[T] . But the name of the\nT  variable inside the brackets must be defined somewhere—otherwise the Python\ninterpreter would need deep changes to support generic type notation as special use of\n[] . That’ s why the typing.TypeVar  constructor is needed: to introduce the variable\nname in the current namespace. Languages such as Java, C#, and T ypeScript don’ t\nrequire the name of type variable to be declared beforehand, so they have no equivalent\nof Python’ s TypeVar  class.\nAnother example is the statistics.mode  function from the standard\nlibrary , which returns the most common data point from a series.\nHere is one usage example from the documentation :\n>>> mode([1, 1, 2, 3, 3, 3, 3, 4]) \n3\nW ithout using a TypeVar , mode  could have this signature:\nExample 8-17. mode_float.py : mode  that operates on float  and\nsubtypes.\nfrom collections  import Counter \nfrom collections.abc  import Iterable  \n \ndef mode(data: Iterable [float]) -> float: \n    pairs = Counter(data).most_common (1) \n    if len(pairs) == 0: \n        raise ValueError ('no mode for empty data' ) \n    return pairs[0][0]\nMany uses of mode  involve int  or float  values, but Python has other\nnumerical types, and it is desirable that the return type follows the element\ntype of the given Iterable . W e can improve that signature using\nTypeVar . Let’ s start with a simple but wrong parameterized signature:\nfrom collections.abc  import Iterable  \nfrom typing import TypeVar 13\n \nT = TypeVar('T') \n \ndef mode(data: Iterable [T]) -> T:\nWhen it first appears in the signature, the type parameter T  can be any type.\nThe second time it appears, it will mean the same type as the first.\nTherefore, every iterable is consistent-with  Iterable[T] , including\niterables of unhashable types that collections.Counter  cannot\nhandle. W e need to restrict the possible types assigned to T . W e’ll see two\nways of doing that in the next two sections.\nRestricted TypeVar\nTypeVar  accepts extra positional ar guments to restrict the type parameter .\nW e can improve the signature of mode  to accept specific number types like\nthis:\nfrom collections.abc  import Iterable  \nfrom decimal import Decimal \nfrom fractions  import Fraction  \nfrom typing import TypeVar \n \nNumberT = TypeVar('NumberT' , float, Decimal, Fraction ) \n \ndef mode(data: Iterable [NumberT]) -> NumberT:\nThat’ s better than before, and it was the signature for mode  in the\nstatistics.pyi  stub file on typeshed  on May 25, 2020.\nHowever , the statistics.mode  documentation includes this example:\n>>> mode([""red"", ""blue"", ""blue"", ""red"", ""green"", ""red"", ""red""]) \n'red'\nIn a hurry , we could just add str  to the NumberT  definition:\nNumberT = TypeVar('NumberT' , float, Decimal, Fraction , str)\nThat certainly works, but NumberT  is badly misnamed if it accepts str .\nMore importantly , we can’ t keep listing types forever as we realize mode\ncan deal with them. W e can do better with another feature of TypeVar ,\nintroduced next.\nBounded TypeVar\nLooking at the body of mode  in Example 8-17 , we see that the Counter\nclass is used for ranking. Counter is based on dict , therefore the element\ntype of the data  iterable must be hashable.\nAt first, this signature may seem to work:\nfrom collections.abc  import Iterable , Hashable  \n \ndef mode(data: Iterable [Hashable ]) -> Hashable :\nNow the problem is that the type of the returned item is Hashable : an\nABC that implements only the __hash__  method. So the type checker\nwill not let us do anything with the return value except call hash()  on it.\nNot very useful.\nThe solution is another optional parameter of TypeVar : the bound\nkeyword parameter . It sets an upper boundary for the acceptable types. In\nExample 8-18 , we have bound=Hashable , which means the type\nparameter may be Hashable  or any subtype-of  it.\nExample 8-18. mode_hashable.py : same as Example 8-17 , with a mor e\nflexible signatur e.\nfrom collections  import Counter \nfrom collections.abc  import Iterable , Hashable  \nfrom typing import TypeVar \n \nHashableT  = TypeVar('HashableT' , bound=Hashable ) \n \ndef mode(data: Iterable [HashableT ]) -> HashableT : \n    pairs = Counter(data).most_common (1) \n    if len(pairs) == 0: \n        raise ValueError ('no mode for empty data' ) \n    return pairs[0][0]14\nT o summarize:\nA restricted type variable will be set to one of the types named in\nthe TypeVar  declaration.\nA bounded type variable will be set to the inferred type of the\nexpression—as long as the inferred type is consistent-with  the\nboundary declared in the bound=  keyword ar gument of\nTypeVar .\nN O T E\nIt is unfortunate that the keyword ar gument to declare a bounded TypeVar  is named\nbound= , because the verb “to bind” is commonly used to mean setting the value of a\nvariable, which in the reference semantics of Python is best described as binding a name\nto the value. It would have been less confusing if the keyword ar gument was named\nboundary= .\nThe typing.TypeVar  constructor has other optional parameters—\ncovariant  and contravariant —that we’ll cover in Chapter 15 ,\n“V ariance” .\nLet’ s conclude this introduction to TypeVar  with AnyStr .\nThe AnyStr  predefined type variable\nThe typing  module includes a predefined TypeVar  named AnyStr . It’ s\ndefined like this:\nAnyStr = TypeVar('AnyStr' , bytes, str)\nAnyStr  is used in many functions that accept either bytes  or str , and\nreturn values of the given type.\nNow , on to typing.Protocol , a new feature of Python 3.8 that can\nsupport more Pythonic use of type hints.",8676
114-Static Protocols.pdf,114-Static Protocols,"Static Protocols\nN O T E\nIn Object-Oriented programming, the concept of a “protocol” as an informal interface is\nas old as Smalltalk, and is an essential part of Python from the beginning. However , in\nthe context of type hints, a protocol is a typing.Protocol  subclass defining an\ninterface that a type checker can verify . Both kinds of protocols are covered in\nChapter 13 . This is just a brief introduction in the context of function annotations.\nThe Protocol  type as presented in PEP 544—Protocols: Structural\nsubtyping (static duck typing)  is similar to interfaces in Go: a protocol type\nis defined by specifying one or more methods, and the type checker verifies\nthat those methods are implemented where that protocol type is required.\nIn Python, a protocol definition is written as a typing.Protocol\nsubclass. However , classes that implement  a protocol don’ t need to inherit,\nregister or declare any relationship with the class that defines  the protocol.\nIt’ s up to the type checker to find the available protocol types and enforce\ntheir usage.\nHere is a problem that can be solved with the help of Protocol  and\nTypeVar . Suppose you want to create a function top(it, n)  that\nreturns the lar gest n  elements of the iterable it :\n>>> top([4, 1, 5, 2, 6, 7, 3], 3) \n[7, 6, 5]  \n>>> l = 'mango pear apple kiwi banana' .split() \n>>> top(l, 3) \n['pear', 'mango', 'kiwi']  \n>>> \n>>> l2 = [(len(s), s) for s in l] \n>>> l2 \n[(5, 'mango'), (4, 'pear'), (5, 'apple'), (4, 'kiwi'), (6,  \n'banana')]  \n>>> top(l2, 3) \n[(6, 'banana'), (5, 'mango'), (5, 'apple')]\nA parameterized generic top  would look like this:\nExample 8-19. top  function with an undefined T  type parameter .\ndef top(series: Iterable [T], length: int) -> list[T]: \n    ordered = sorted(series, reverse=True) \n    return ordered[:length]\nThe problem is how to constrain T ? It cannot be Any  or object , because\nthe series  must work with sorted . The sorted  built-in actually\naccepts Iterable[Any] , but that’ s because the optional parameter key\ntakes a function that computes an arbitrary sort key from each element.\nWhat happens if you give sorted  a list of plain objects but don’ t provide\na key  ar gument? Let’ s try that:\n>>> l = [object() for _ in range(4)] \n>>> l \n[<object object at 0x10fc2fca0>, <object object at 0x10fc2fbb0>,  \n<object object at 0x10fc2fbc0>, <object object at 0x10fc2fbd0>]  \n>>> sorted(l) \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : '<' not supported between instances of 'object' and  \n'object'\nThe error message shows that sorted  uses the <  operator on the elements\nof the iterable. Is this all it takes? Let’ s do another quick experiment:\n>>> class Spam: \n...     def __init__ (self, n): self.n = n \n...     def __lt__(self, other): return self.n < other.n \n...     def __repr__ (self): return f'Spam({self.n})'  \n... \n>>> l = [Spam(n) for n in range(5, 0, -1)] \n>>> l \n[Spam(5), Spam(4), Spam(3), Spam(2), Spam(1)]  \n>>> sorted(l) \n[Spam(1), Spam(2), Spam(3), Spam(4), Spam(5)]\nThat confirms it: I can sort  a list of Spam  because Spam  implements\n__lt__ —the special method that supports the <  operator .\nSo the T  type parameter in Example 8-19  should be limited to types that\nimplement __lt__ . In Example 8-18  we needed a type parameter that15\nimplemented __hash__ , so we were able to use typing.Hashable  as\nthe upper bound for the type parameter . But now there is no suitable type in\ntyping  or abc  to use, so we need to create it.\nHere is the new SupportsLessThan  type, a Protocol :\nExample 8-20. comparable.py : definition of a SupportsLessThan\nProtocol  type:\nfrom typing import Protocol , Any \n \nclass SupportsLessThan (Protocol ):  \n \n    def __lt__(self, other: Any) -> bool: ...  \nA protocol is a subclass of typing.Protocol .\nThe body of the protocol has one or more method definitions, with ...\nin their bodies.\nA type T  is consistent-with  a protocol P  if T  implements all the methods\ndefined in P , with matching type signatures.\nGiven SupportsLessThan , we can now define this working version of\ntop :\nExample 8-21. top.py : definition of the top  function using a TypeVar\nwith bound=SupportsLessThan :\nfrom collections.abc  import Iterable  \nfrom typing import TypeVar \n \nfrom comparable  import SupportsLessThan  \n \nLT = TypeVar('LT', bound=SupportsLessThan ) \n \ndef top(series: Iterable [LT], length: int) -> list[LT]: \n    ordered = sorted(series, reverse=True) \n    return ordered[:length]\nLet’ s test-drive top . Example 8-22  shows part of a test suite for use with\npytest . It tries calling top  first with a generator expression that yields\ntuple[int, str] , and then with a list of object . W ith the list of\nobject , we expect to get a TypeError  exception.\nExample 8-22. top_test.py : partial listing of the test suite for top\nfrom collections.abc  import Iterator  \nfrom typing import TYPE_CHECKING   \n \n \nimport pytest \n \nfrom top import top \n \n# several lines omitted  \n \ndef test_top_tuples () -> None: \n    fruit = 'mango pear apple kiwi banana '.split() \n    series: Iterator [tuple[int, str]] = (  \n \n        (len(s), s) for s in fruit) \n    length = 3 \n    expected  = [(6, 'banana'), (5, 'mango'), (5, 'apple')] \n    result = top(series, length) \n    if TYPE_CHECKING :  \n \n        reveal_type (series)  \n \n        reveal_type (expected ) \n        reveal_type (result) \n    assert result == expected  \n \n# intentional type error  \ndef test_top_objects_error () -> None: \n    series = [object() for _ in range(4)] \n    if TYPE_CHECKING : \n        reveal_type (series) \n    with pytest.raises(TypeError ) as excinfo: \n        top(series, 3)  \n \n    assert ""'<' not supported "" in str(excinfo.value)\nThe typing.TYPE_CHECKING  constant is always False  at\nruntime, but type checkers pretend it is True  when they are type\nchecking.\nExplicit type declaration for the series  variable, to make the Mypy\noutput easier to read. ], as we’ll see in “Generic Iterable T ypes” .]\nThis if  prevents the next three lines from executing when the test runs.16\nreveal_type()  cannot be called at runtime, because it is not a\nregular function but a Mypy debugging facility—that’ s why there is no\nimport  for it. Mypy will output one debugging message for each\nreveal_type()  pseudo function call, showing the inferred type of\nthe ar gument.\nThis line will be flagged as an error by Mypy .\nThe above tests pass—but they would pass anyway , with or without type\nhints in top.py . More to the point, if I check that test file with Mypy , I see\nthat the TypeVar  is working as intended. See the mypy  command output\nin Example 8-23 .\nW A R N I N G\nAs of Mypy 0.910 (July 2021), the output of reveal_type  does not show precisely\nthe types I declared in some cases, but compatible types instead. For example, I did not\nuse typing.Iterator  but abc.Iterator . Please ignore this detail. The Mypy\noutput is still useful. I will pretend this issue of Mypy is fixed when discussing the\noutput.\nExample 8-23. Output of mypy top_test.py  (lines split for\nr eadability)\n…/comparable/ $ mypy top_test.py  \ntop_test.py:32: note:  \n    Revealed type is ""typing.Iterator[Tuple[builtins.int,  \nbuiltins.str]]"" \n  \ntop_test.py:33: note:  \n    Revealed type is ""builtins.list[Tuple[builtins.int,  \nbuiltins.str]]""  \ntop_test.py:34: note:  \n    Revealed type is ""builtins.list[Tuple[builtins.int,  \nbuiltins.str]]"" \n  \ntop_test.py:41: note:  \n    Revealed type is ""builtins.list[builtins.object*]"" \n  \ntop_test.py:43: error:  \n    Value of type variable ""LT"" of ""top"" cannot be ""object""  \n  \nFound 1 error in 1 file (checked 1 source file)\nIn test_top_tuples , reveal_type(series)  shows it is an\nIterator[tuple[int, str]] —which I explicitly declared.\nreveal_type(result)  confirms that the type returned by the top\ncall is what I wanted: given the type of series , the result  is\nlist[tuple[int, str]] .\nIn test_top_objects_error , reveal_type(series)  shows\nit is list[object*] . Mypy puts a *  after any type that was inferred:\nI did not annotate the type of series  in this test.\nMypy flags the error that this test intentionally triggers: the element\ntype of the Iterable  series  cannot be object  (it must be of type\nSupportsLessThan ).\nA key advantage of a protocol type over ABCs is that a type doesn’ t need\nany special declaration to consistent-with  a protocol type. This allows a\nprotocol to be created leveraging pre-existing types, or types implemented\nin code that we do not control. I don’ t need to derive or register str ,\ntuple , float , set , etc. with SupportsLessThan  to use them where\na SupportsLessThan  parameter is expected. They only need to\nimplement __lt__ . And the type checker will still be able do its job,\nbecause SupportsLessThan  is explicitly defined as a Protocol —in\ncontrast with the implicit protocols that are common with duck typing,\nwhich are invisible to the type checker .\nThe special Protocol  class was introduced in PEP 544—Protocols:\nStructural subtyping (static duck typing) . Example 8-21  demonstrates why\nthis feature is known as static duck typing : the solution to annotate the\nseries  parameter of top  was to say “The nominal type of series\ndoesn’ t matter , as long as it implements the __lt__  method”. Python’ s\nduck typing always allowed us to say that implicitly , leaving static type\ncheckers clueless. A type checker can’ t read CPython’ s source code in C, or",9613
115-Callable.pdf,115-Callable,"perform console experiments to find out that sorted  only requires that the\nelements support < .\nNow we can make duck typing explicit for static type checkers. That’ s why\nit makes sense to say that typing.Protocol  gives us static duck\ntyping .\nThere’ s more to see about typing.Protocol . W e’ll come back to it in\nPart IV , where Chapter 13  contrasts structural typing, duck typing, and\nABCs—another approach to formalizing protocols. In addition,\n“Overloaded signatures”  ( Chapter 15 ) explains how to declare overloaded\nfunction signatures with @typing.overload , and includes an extensive\nexample using typing.Protocol  and a bounded TypeVar .\nN O T E\ntyping.Protocol  makes it possible to annotate the double  function presented in\n“T ypes are defined by supported operations”  without losing functionality . The key is to\ndefine a protocol class with the __mul__  method. I invite you to do that as an exercise.\nThe solution appears in “The typed double function”  ( Chapter 13 ).\nCallable\nT o annotate callback parameters or function objects returned by higher -\norder functions, the typing  module provides the Callable  type, which\nis parameterized like this:\nCallable [[ParamType1 , ParamType2 ], ReturnType ]\nThe parameter list— [ParamType1, ParamType2] —can have 0 or\nmore types.\nHere is an example in context:\ndef repl(input_fn : Callable [[Any], str] = input) -> None:\nThe repl  function is part of a simple interactive interpreter .17\n18\nDuring normal usage, the repl  function uses Python’ s input  built-in to\nread expressions from the user . However , for automated testing or for\nintegration with other input sources, repl  accepts an optional input_fn\nparameter: a Callable  with the same parameter and return types as\ninput .\nThe built-in input()  has this signature on typeshed:\ndef input(__prompt : Any = ...) -> str: ...\nThat function is consistent-with  this Callable  type hint:\nCallable [[Any], str]\nAs another example, in Chapter 10 , the Order.__init__  method in\nExample 10-3  uses this signature:\nclass Order: \n \n    def __init__ ( \n        self,  \n \n        customer : Customer , \n        cart: Sequence [LineItem ], \n        promotion : Optional [Callable [['Order'], float]] = None,  \n \n    ) -> None:  \nself  rarely needs a type hint. .\npromotion  may be None , or Callable[[Order], float] : a\nfunction that takes an Order  and returns float .\n__init__  always returns None , but I recommend adding the return\ntype hint for it anyway .\nNote that the Order  type appears as the string 'Order'  in the\nCallable  type hint, otherwise Python would raise NameError: name19\n20",2664
116-Flawed Typing and Strong Testing.pdf,116-Flawed Typing and Strong Testing,"'Order' is not defined —because the Order  class is not defined\nuntil Python reads the whole body of the class—an issue we’ll discuss in\nChapter 25 : Class Metapr ogramming .\nT I P\nPEP 563—Postponed Evaluation of Annotations  was implemented in Python 3.7 to\nsupport forward references in annotations, avoiding the need to write Order  as string in\nthe previous example. However , that feature is only enabled when from\n__future__ import annotations  is used at the top of the module, to avoid\nbreaking code that depend on reading annotations at runtime, like the pydantic  and\nFastAPI  packages—to name just two examples. The PEP 563 behavior was planned to\nbecome default in Python 3.10 but this has been postponed—pun intended—while a\ncompromise is reached between those who care about using annotations at runtime and\nthose who don’ t. See this message from Python’ s Steering Council for more: PEP 563\nand Python 3.10 .\nThere is no syntax to annotate optional or keyword ar guments in\nCallable[] . The documentation  says “such function types are rarely\nused as callback types”. If you need a type hint to match a function with a\ndynamic signature, replace the whole parameter list with ... , like this:\nCallable[..., ReturnType] .\nNoReturn\nThis is a special type used only to annotate the return type of functions that\nnever return. Usually , they exist to raise exceptions. There are dozens of\nsuch functions in the standard library .\nFor example: sys.exit()  raises SystemExit , to terminate the Python\nprocess.\nIts signature in typeshed  is:\ndef exit(__status : object = ...) -> NoReturn : ...\nThe __status  parameter is positional-only , and it has a default value.\nStub files don’ t spell out the default values: they use ...  instead. The type\nof __status  is object  which means it may also be None , therefore it\nwould be redundant to mark it Optional[object] .\nIn Chapter 25 , Example 25-6  uses NoReturn  in the\n__flag_unknown_attrs , a method designed to produce a user friendly\nand comprehensive error message, and then raise AttributeError .\nThe last section in this epic chapter is about positional and variadic\nparameters.\nA n n o t a t i n g  p o s i t i o n a l - o n l y  a n d  v a r i a d i c\np a r a m e t e r s\nRecall the tag  function from Example 7-9 . The last time we saw its\nsignature was in section “Positional-only parameters” :\ndef tag(name, /, *content, class_=None, **attrs):\nHere is tag , fully annotated, written in several lines—a common\nconvention for long signatures, with line breaks the way the blue  formatter\nwould do it:\nfrom typing import Optional  \n \ndef tag( \n    name: str, \n    /, \n    *content: str, \n    class_: Optional [str] = None, \n    **attrs: str, \n) -> str:\nNote the type hint *content: str  for the arbitrary positional\nparameters: this means all those ar guments must be of type str . The type\nof the content  local variable in the function body will be tuple[str,\n...] .\nThe type hint for the arbitrary keyword ar guments is **attrs: str  in\nthis example, therefore the type of attrs  inside the function will be\ndict[str, str] . For a type hint like **attrs: float , the type of\nattrs  in the functin would be dict[str, float] .\nIf the attrs  parameter must accept values of dif ferent types, you’ll need\nto use a Union[]  or Any : **attrs: Any .\nThe /  notation for positional-only parameters is only available in Python ≥\n3.8. In Python 3.7 or earlier , that’ s a syntax error . The PEP 484 convention\nis to prefix each positional-only parameter name with two underscores.\nHere is the tag  signature again, now in two lines, using the PEP 484\nconvention:\nfrom typing import Optional  \n \ndef tag(__name: str, *content: str, class_: Optional [str] = None, \n        **attrs: str) -> str:\nMypy understands and enforces both ways of declaring positional-only\nparameters.\nT o close this chapter , let’ s briefly consider the limits of type hints and the\nstatic type system they support.\nF l a w e d  T y p i n g  a n d  S t r o n g  T e s t i n g\nMaintainers of lar ge corporate codebases report that many bugs are found\nby static type checkers and fixed more cheaply than if the bugs were\ndiscovered only after the code is running in production.\nHowever , it’ s essential to note that automated testing was standard practice\nand widely adopted long before static typing was introduced in the\ncompanies that I know about.\nEven in the contexts where they are most beneficial, static typing cannot be\ntrusted as the ultimate arbiter of correctness. It’ s not hard to discover:\nFalse positives: tools report type errors on code that is correct.\nFalse negatives: tools don’ t report type errors on code that is\nincorrect.\nAlso, if we are forced to type check everything, we lose some of the\nexpressive power of Python:\nSome handy features can’ t be statically checked. For example:\nar gument unpacking like config(**settings) .\nAdvanced features like properties, descriptors, metaclasses, and\nmetaprogramming in general are poorly supported or beyond\ncomprehension for type checkers.\nT ype checkers lag behind Python releases, rejecting or even\ncrashing while analysing code with new language features—for\nmore than a year in some cases.\nCommon data constraints cannot be expressed in the type system—even\nsimple ones. For example: type hints are unable to ensure “quantity must be\nan integer > 0” or “label must be a string with 6 to 12 ASCII letters.” In\ngeneral, type hints are not helpful to catch errors in business logic.\nGiven those caveats, type hints cannot be the mainstay of software quality ,\nand making them mandatory without exception would amplify its\ndownsides.\nConsider a static type checker as one of the tools in a modern CI pipeline,\nalong with test runners, linters, etc. The point of a CI pipeline is to reduce\nsofware failures, and automated tests catch many bugs that are beyond the\nreach of type hints. Any code you can write in Python, you can test in\nPython—with or without type hints.",6079
117-Chapter summary.pdf,117-Chapter summary,"N O T E\nThe title and conclusion of this section were inspired by Bruce Eckel’ s article Strong\nT yping vs. Strong T esting , also published in the anthology The Best Software W riting I\nedited by Joel Spolky . Bruce is a fan of Python and author of books about C++, Java,\nScala, and Kotlin. In that post, he tells how he was a static typing advocate until he\nlearned Python and concluded: “If a Python program has adequate unit tests, it can be as\nrobust as a C++, Java, or C# program with adequate unit tests (although the tests in\nPython will be faster to write).”\nThis wraps up our coverage of Python’ s type hints for now . They are also\nthe main focus of Chapter 15 , which covers generic classes, variance,\noverloaded signatures, type casting, and more. Meanwhile, type hints will\nmake guest appearances in several examples throughout the book.\nC h a p t e r  s u m m a r y\nW e started with a brief introduction to the concept of gradual typing and\nthen switched to a hands-on approach. It’ s hard to see how gradual typing\nworks without a tool that actually reads the type hints, so we developed an\nannotated function guided by Mypy error reports. That section ended with\nanother practical matter: how to annotate code that must run under Python\n2.7 and 3.x.\nBack to the idea of gradual typing, we explored how it is a hybrid of\nPython’ s traditional duck typing and the nominal typing more familiar to\nusers of Java, C++ and other statically typed languages.\nMost of the chapter was devoted to presenting the major groups of types\nused in annotations. Many of the types we covered are related to familiar\nPython object types, such as collections, tuples, and callables—extended to\nsupport generic notation like Sequence[float] . Many of those types\nare temporary surrogates implemented in the typing  module before the\nstandard types were changed to support generics in Python 3.9.\nSome of the types are special entities. Any , Optional , Union , and\nNoReturn  have nothing to do with actual objects in memory , but exist\nonly in the abstract domain of the type system.\nW e studied parameterized generics and type variables, which bring more\nflexibility to type hints without sacrificing type safety .\nParameterized generics become even more expressive with the use of\nProtocol . Because it appeared only in Python 3.8, Protocol  is not\nwidely used yet—but it is hugely important. Protocol  enables static duck\ntyping: the essential bridge between Python’ s duck typed core and the\nnominal typing that allows static type checkers to catch bugs.\nWhile covering some of these types we experimented with Mypy to see\ntype checking errors and inferred types with the help of Mypy’ s magic\nreveal_type()  function.",2757
118-Further Reading.pdf,118-Further Reading,"The final section covered how to annotate positional-only and variadic\nparameters.\nT ype hints are a complex and evolving topic. Fortunately , they are an\noptional feature. Let us keep Python accessible to the widest user base and\nstop preaching that all Python code should have type hints—as I’ve seen in\npublic sermons by typing evangelists.\nOur BDFL emeritus led this push towards type hints in Python, so it’ s only\nfair that this chapter starts and ends with his words:\nI wouldn’ t like a version of Python wher e I was morally obligated to add\ntype hints all the time. I r eally do think that type hints have their place\nbut ther e ar e also plenty of times that it’ s not worth it, and it’ s so\nwonderful that you can choose to use them.\n— Guido van Rossum\nF u r t h e r  R e a d i n g\nBernát Gábor wrote in his excellent post The state of type hints in Python :\nT ype hints should be used whenever unit tests ar e worth writing.\nI am a big fan of testing, but I also do a lot exploratory coding. When I am\nexploring, tests and type hints are not helpful. They are a drag.\nGábor ’ s post is one of the best introductions to Python’ s type hints that I\nfound, along with Geir Arne Hjelle’ s Python T ype Checking (Guide) .\nHypermodern Python Chapter 4: T yping  by Claudio Jolowicz is a shorter\nintroduction that also covers runtime type checking validation.\nFor deeper coverage, the Mypy documentation  is the best source. It is\nvaluable regardless of the type checker you are using, because it has tutorial\nand reference pages about Python typing in general—not just about the\nMypy tool itself. There you will also find a handy cheat sheets  and a very\nuseful page about Common issues and solutions .21\nThe typing  module documentation is a good quick reference, but it\ndoesn’ t go into much detail. The ultimate references are the PEP documents\nrelated to typing. There are more than 20 of them already . The intended\naudience of PEPs are Python core developers and Python’ s Steering\nCouncil, so they assume a lot of prior knowledge and are certainly not light\nreading.\nAs mentioned, Chapter 15  covers more typing topics, and “Further\nReading”  provides additional references, including T able 15-1 , listing\ntyping PEPs approved or under discussion as of March 2021.\nA wesome Python T yping  is a valuable collection of links to tools and\nreferences.\nS O A P B O X\nJust ride\nFor get the ultralight, uncomfortable bikes, flashy jerseys, clunky\nshoes that clip onto tiny pedals, the grinding out of endless miles.\nInstead, ride like you did when you wer e a kid—just get on your bike\nand discover the pur e joy of riding it.\n— Grant Petersen, Just Ride: A Radically Practical\nGuide to Riding Y our Bike\nIf coding is not your whole profession, but a useful tool in you\nprofession, or something you do to learn, tinker and enjoy , you probably\ndon’ t need type hints anymore than most bikers need shoes with stif f\nsoles and metal cleats.\nJust code.\nThe cognitive effect of typing\nI worry about the ef fect type hints will have on Python coding style.\nI agree that users of most APIs benefit from type hints. But Python\nattracted me—among other reasons—because it provides functions that\nare so powerful that they replace entire APIs, and we can write\nsimilarly powerful functions ourselves. Consider the max()  built-in.\nIt’ s powerful yet easy to understand. But I will show in “Max\nOverload”  that it takes 14 lines of type hints to properly annotate it—\nnot counting a typing.Protocol  and a few TypeVar  definitions\nto support those type hints.\nI am concerned that strict enforcement of type hints in libraries will\ndiscourage programmers from even considering writing such functions\nin the future.\nAccording to the English W ikipedia, Linguistic Relativity —a.k.a. the\nSapir–Whorf hypothesis— is a “principle claiming that the structure of\na language af fects its speakers’ world view or cognition. W ikipedia\nfurther explains:\nThe str ong  version says that language determines  thought and\nthat linguistic categories limit and determine cognitive\ncategories.\nThe weak  version says that linguistic categories and usage only\ninfluence  thought and decisions.\nLinguists generally agree the strong version is false, but there is\nempirical evidence supporting the weak version.\nI am not aware of specific studies with programming languages, but in\nmy experience they’ve had a big impact on how I approach problems.\nThe first programming language I used professionally was Applesoft\nBASIC in the age of 8-bit computers. Recursion was not directly\nsupported by BASIC—you had to roll your own call stack to use it. So I\nnever considered using recursive algorithms or data structures. I knew\nat some conceptual level such things existed, but they weren’ t part of\nmy problem-solving toolbox.\nDecades later when I started with Elixir , I enjoyed solving problems\nwith recursion and overused it—until I discovered that many of my\nsolutions would be simpler if I used existing functions from the Elixir\nEnum  and Stream  modules. I learned that idiomatic Elixir\napplication-level code rarely has explicit recursive calls, but use enums\nand streams that implement recursion under the hood.\nLinguistic Relativity could explain the widespread idea (also unproven)\nthat learning dif ferent programming languages makes you a better\nprogrammer , particularly when the languages support dif ferent\nprogramming paradigms. Practicing Elixir made me more likely to\napply functional patterns when I write Python or Go code.\nNow , back to Earth.\nThe requests  package would probably have a very dif ferent API if\nKenneth Reitz was determined (or told by his boss) to annotate all its\nfunctions. His goal was to write an API that was easy to use, flexible,\nand powerful. He succeeded, given the amazing popularity of\nrequests —in May 2020, it’ s #4 on PyPI Stats , with 2.6 million\ndownloads a day . #1 is urllib3 , a dependency of requests .\nIn 2017, the requests  maintainers decided  not to spend their time\nwriting type hints. One of the maintainers, Cory Benfield, had written\nan e-mail  stating:\nI think that libraries with Pythonic  APIs ar e the least likely to take up\nthis typing system because it will pr ovide the least value to them.\nIn that message, Benfield gave this extreme example of a tentative type\ndefinition for the files  keyword ar gument of\nrequests.request() :\nOptional[  \n  Union[  \n    Mapping[  \n      basestring,  \n      Union[  \n        Tuple[basestring, Optional[Union[basestring, file]]],  \n        Tuple[basestring, Optional[Union[basestring, file]],  \n              Optional[basestring]],  \n        Tuple[basestring, Optional[Union[basestring, file]],  \n              Optional[basestring], Optional[Headers]]  \n      ] \n    ], \n    Iterable[  \n      Tuple[  \n        basestring,  \n        Union[  \n          Tuple[basestring, Optional[Union[basestring,  \nfile]]],  \n          Tuple[basestring, Optional[Union[basestring, file]],  \n                Optional[basestring]],  \n          Tuple[basestring, Optional[Union[basestring, file]],  \n                Optional[basestring], Optional[Headers]]  \n      ] \n    ] \n  ] \n]\nAnd that assumes this definition:\nHeaders = Union[  \n  Mapping[basestring, basestring],  \n  Iterable[Tuple[basestring, basestring]],  \n]\nDo you think requests  would be the way it is if the maintainers\ninsisted on 100% type hint coverage? SQLAlchemy is another\nimportant package that doesn’ t play well with type hints.\nWhat makes these libraries great is embracing the dynamic nature of\nPython.\nWhile there are benefits to type hints, there is also a price to pay .\nFirst, there is the significant investment of understanding how the type\nsystem works. That’ s a one-time cost.\nBut there is also a recurring cost, forever .\nW e lose some of the expressive power of Python if we insist on type\nchecking everything. Beautiful features like ar gument unpacking—e.g.\nconfig(**settings) —are beyond comprehension for type\ncheckers.\nIf you want to have a call like config(**settings)  type checked,\nyou must spell every ar gument out. That brings me memories of T urbo\nPascal code I wrote 35 years ago.\nLibraries that use metaprogramming are hard or impossible to annotate.\nSurely metaprogramming can be abused, but it’ s also what makes many\nPython packages a joy to use.\nIf type hints are mandated top down without exceptions in lar ge\ncompanies, I bet soon we’ll see people using code generation to reduce\nboilerplate in Python source-code—a common practice with less\ndynamic languages.\nFor some projects and contexts, type hints just don’ t make sense. Even\nin contexts where they mostly make sense, they don’ t make sense all\nthe time. Any reasonable policy about the use of type hints must have\nexceptions.\nAlan Kay—the T uring A ward laureate who pioneered Object Oriented\nProgramming—once said:\nSome people ar e completely r eligious about type systems and as a\nmathematician I love the idea of type systems, but nobody has ever\ncome up with one that has enough scope.\nThank Guido for optional typing. Let’ s use it as intended, and not aim\nto annotate everything into strict conformity to a coding style that looks\nlike Java 1.5.\nDuck typing FTW\nDuck typing fits my brain, and static duck typing is a good compromise\nallowing static type checking without losing a lot of flexibility that\nsome nominal type systems only provide with a lot of complexity—if\never .\nBefore PEP 544, this whole idea of type hints seemed utterly\nunpythonic to me. I was very glad to see typing.Protocol  land in\nPython. It brings balance to the force.\nGenerics or specifics?\nFrom a Python perspective, the typing usage of the term “generic” is\nbackwards. Common meanings of “generic” are “applicable to an entire\nclass or group” or “without a brand name.”\nConsider list  versus list[str] . The first is generic: it accepts any\nobject. The second is specific: it only accepts str .22\nThe term makes sense in Java, though. Before Java 1.5, all Java\ncollections (except the magic array ) were “specific”: they could only\nhold Object  references, so we had to cast the items that came out of a\ncollection to use them. W ith Java 1.5, collections got type parameters,\nand became “generic.”\n1  PEP 484—T ype Hints , section Rationale and Goals ; bold emphasis retained from the original.\n2  A just-in-time compiler like the one in PyPy has much better data than type hints: it monitors\nthe Python program as it runs, detects the concrete types in use, and generates optimized\nmachine code for those concrete types.\n3  For example, recursive types are not supported as of July 2021—see typing  module issue\n#182 Define a JSON type  and Mypy issue #731 Support recursive types\n4  Python doesn’ t provide syntax to control the set of possible values for a type—except in\nEnum  types. For example, using type hints you can’ t define Quantity  as an integer between\n1 and 1000, or AirportCode  as a 3-letter combination. NumPy of fers uint8 , int16  and\nother machine-oriented numeric types, but in the Python standard library we only have types\nwith very small sets of values ( NoneType , bool ) or extremely lar ge sets (float , int ,\nstr , all possible tuples etc.).\n5  Duck typing is a weaker form of structural typing , which Python ≥ 3.8 also supports with the\nintroduction of typing.Protocol . This is covered later in this chapter—in “Static\nProtocols” —with more details in Chapter 13 .\n6  Inheritance is often overused and hard to justify in examples that are realistic yet simple, so\nplease accept this animal example as a quick illustration of subtyping.\n7  MIT Professor , programming language designer , and T uring A ward recipient. W ikipedia:\nBarbara Liskov .\n8  T o be more precise, ord  only accepts str  or bytes  with len(s) == 1 . But the type\nsystem currently can’ t express this constraint.\n9  In ABC—the language that most influenced the initial design of Python—each list was\nconstrained to accept values of a single type: the type of the first item you put into it.\n10  One of my contributions to the typing  module documentation was to add dozens of\ndeprecation warnings as I reor ganized the entries below Module Contents  into subsections,\nunder the supervision of Guido van Rossum.\n11  I use :=  when it makes sense in a few examples, but I don’ t cover it in the book. Please see\nPEP 572—Assignment Expressions  for all the gory details.\n12  Actually , dict  is a virtual subclass of abc.MutableMapping . The concept of a virtual\nsubclass is explained in Chapter 13 . For now , know that issubclass(dict,\nabc.MutableMapping)  is True , despite the fact that dict  is implemented in C and does\nnot inherit anything from abc.MutableMapping , but only from object .\n13  The implementation here is simpler than the one in the Python standard library statistics\nmodule.\n14  I contributed this solution to typeshed, and that’ s how mode  is annotated on\nstatistics.pyi  as of May 26, 2020.\n15  How wonderful it is to open an interactive console and rely on duck typing to explore\nlanguage features like I just did. I badly miss this kind of exploration when I use languages that\ndon’ t support it.\n16  W ithout this type hint, Mypy would infer the type of series  as\nGenerator[Tuple[builtins.int, builtins.str*], None, None] , which is\nverbose but consistent-with  Iterator[tuple[int, str\n17  I don’ t know who invented the term static duck typing , but it became more popular with the\nsuccess of the Go language, which has interface semantics that are more like Python’ s\nprotocols than the nominal interfaces of Java.\n18  REPL stands for read-eval-print-loop, the common code pattern in interactive interpreters.\n19  W e’ll see cases where self  is annotated in Chapter 15 , “Implementing a generic class”\n20  As special case for __init__ , if at least one parameter has a type hint, Mypy does not\ncomplain about the missing return type, by default. But if you for get this rule, and __init__\nis completely untyped, then it will not be type checked.\n21  From Y ouT ube video of T ype Hints by Guido van Rossum (Mar ch 2015) . Quote starts at\n13’40” . I did some light editing for clarity .\n22  Source: A Conversation with Alan Kay .",14422
119-Decorators 101.pdf,119-Decorators 101,"Chapter 9. Decorators and\nClosures\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 9th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nTher e’ s been a number of complaints about the choice of the name\n“decorator” for this featur e. The major one is that the name is not\nconsistent with its use in the GoF book.  The name decorator  pr obably\nowes mor e to its use in the compiler ar ea—a syntax tr ee is walked and\nannotated.\n— PEP 318 — Decorators for Functions and Methods\nFunction decorators let us “mark” functions in the source code to enhance\ntheir behavior in some way . This is powerful stuf f, but mastering it requires\nunderstanding closures—which is what happens when functions capture\nvariables defined outside of their bodies.\nThe most obscure reserved keyword in Python is nonlocal , introduced in\nPython 3.0. Y ou can have a profitable life as a Python programmer without\never using it if you adhere to a strict regimen of class-centered object\norientation. However , if you want to implement your own function1\ndecorators, you must understand closures, and then the need for nonlocal\nbecomes obvious.\nAside from their application in decorators, closures are also essential for\nany type of programming using callbacks, and for coding in a functional\nstyle when it makes sense.\nThe end goal of this chapter is to explain exactly how function decorators\nwork, from the simplest registration decorators to the rather more\ncomplicated parameterized ones. However , before we reach that goal we\nneed to cover:\nHow Python evaluates decorator syntax\nHow Python decides whether a variable is local\nWhy closures exist and how they work\nWhat problem is solved by nonlocal\nW ith this grounding, we can tackle further decorator topics:\nImplementing a well-behaved decorator\nPowerful decorators in the standard library: @cache ,\n@lru_cache , and @singledispatch\nImplementing a parameterized decorator\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThe caching decorator functools.cache —new in Python 3.9—is\nsimpler than the traditional functools.lru_cache , so I present it first.\nThe latter is covered in “Using lru_cache” , including the simplified form\nadded in Python 3.8.\nSection “Single Dispatch Generic Functions”  was expanded and now uses\ntype hints, the preferred way to use functools.singledispatch\nsince Python 3.7.\n“Parameterized Decorators”  now includes a class-based example,\nExample 9-27 .\nI moved Chapter 10 — Design Patterns with First-Class Functions —to the\nend of part III to improve the flow of the book. Section “Decorator -\nEnhanced Strategy Pattern”  is now in that chapter , along with other\nvariations of the Strategy design pattern using callables.\nW e start with a very gentle introduction to decorators, and then proceed\nwith the rest of the items listed in the chapter opening.\nD e c o r a t o r s  1 0 1\nA decorator is a callable that takes another function as ar gument (the\ndecorated function).\nA decorator may perform some processing with the decorated function, and\nreturns it or replaces it with another function or callable object.\nIn other words, assuming an existing decorator named decorate , this\ncode:\n@decorate  \ndef target(): \n    print('running target()' )\nHas the same ef fect as writing this:\ndef target(): \n    print('running target()' ) \n \ntarget = decorate (target)\nThe end result is the same: at the end of either of these snippets, the\ntarget  name is bound to whatever function is returned by\ndecorate(target) —which may be the function initially named\ntarget , or may be a dif ferent function.2\nT o confirm that the decorated function is replaced, see the console session\nin Example 9-1 .\nExample 9-1. A decorator usually r eplaces a function with a differ ent one\n>>> def deco(func): \n...     def inner(): \n...         print('running inner() ') \n...     return inner  \n \n... \n>>> @deco \n... def target():  \n \n...     print('running target() ') \n... \n>>> target()  \n \nrunning inner()  \n>>> target  \n \n<function deco.<locals>.inner at 0x10063b598>\ndeco  returns its inner  function object.\ntarget  is decorated by deco .\nInvoking the decorated target  actually runs inner .\nInspection reveals that target  is a now a reference to inner .\nStrictly speaking, decorators are just syntactic sugar . As we just saw , you\ncan always simply call a decorator like any regular callable, passing another\nfunction. Sometimes that is actually convenient, especially when doing\nmetapr ogramming —changing program behavior at runtime.\nThree essential facts make a good summary of decorators:\n1 . A decorator is a function or another callable.\n2 . A decorator may replace the decorated function with a dif ferent\none.\n3 . Decorators are executed immediately when a module is loaded.\nNow let’ s focus on the third point.",5374
120-When Python Executes Decorators.pdf,120-When Python Executes Decorators,"W h e n  P y t h o n  E x e c u t e s  D e c o r a t o r s\nA key feature of decorators is that they run right after the decorated\nfunction is defined. That is usually at import time  (i.e., when a module is\nloaded by Python). Consider r egistration.py  in Example 9-2 .\nExample 9-2. The r egistration.py module\nregistry  = []  \n \n \ndef register (func):  \n \n    print(f'running register({func}) ')  \n \n    registry .append(func)  \n \n    return func  \n \n \n@register   \n \ndef f1(): \n    print('running f1() ') \n \n@register  \ndef f2(): \n    print('running f2() ') \n \ndef f3():  \n \n    print('running f3() ') \n \ndef main():  \n \n    print('running main() ') \n    print('registry -> ', registry ) \n    f1() \n    f2() \n    f3() \n \nif __name__  == '__main__ ': \n    main()  \nregistry  will hold references to functions decorated by\n@register .\nregister  takes a function as ar gument.\nDisplay what function is being decorated, for demonstration.\nInclude func  in registry .\nReturn func : we must return a function; here we return the same\nreceived as ar gument.\nf1  and f2  are decorated by @register .\nf3  is not decorated.\nmain  displays the registry , then calls f1() , f2() , and f3() .\nmain()  is only invoked if r egistration.py  runs as a script.\nThe output of running r egistration.py  as a script looks like this:\n$ python3 registration.py  \nrunning register (<function  f1 at 0x100631bf8> ) \nrunning register (<function  f2 at 0x100631c80> ) \nrunning main () \nregistry -> [<function  f1 at 0x100631bf8>, < function  f2 at \n0x100631c80> ] \nrunning f1 () \nrunning f2 () \nrunning f3 ()\nNote that register  runs (twice) before any other function in the module.\nWhen register  is called, it receives the decorated function object as an\nar gument—for example, <function f1 at 0x100631bf8> .\nAfter the module is loaded, the registry  list holds references to the two\ndecorated functions: f1  and f2 . These functions, as well as f3 , are only\nexecuted when explicitly called by main .\nIf r egistration.py  is imported (and not run as a script), the output is this:\n>>> import registration  \nrunning register(<function f1 at 0x10063b1e0>)  \nrunning register(<function f2 at 0x10063b268>)\nAt this time, if you inspect registry , this is what you see:",2313
121-Variable Scope Rules.pdf,121-Variable Scope Rules,">>> registration .registry  \n[<function f1 at 0x10063b1e0>, <function f2 at 0x10063b268>]\nThe main point of Example 9-2  is to emphasize that function decorators are\nexecuted as soon as the module is imported, but the decorated functions\nonly run when they are explicitly invoked. This highlights the dif ference\nbetween what Pythonistas call import time  and runtime .\nR e g i s t r a t i o n  d e c o r a t o r s\nConsidering how decorators are commonly employed in real code,\nExample 9-2  is unusual in two ways:\nThe decorator function is defined in the same module as the\ndecorated functions. A real decorator is usually defined in one\nmodule and applied to functions in other modules.\nThe register  decorator returns the same function passed as\nar gument. In practice, most decorators define an inner function and\nreturn it.\nEven though the register  decorator in Example 9-2  returns the\ndecorated function unchanged, that technique is not useless. Similar\ndecorators are used in many Python frameworks to add functions to some\ncentral registry—for example, a registry mapping URL patterns to functions\nthat generate HTTP responses. Such registration decorators may or may not\nchange the decorated function.\nW e will see a registration decorator applied in “Decorator -Enhanced\nStrategy Pattern”  ( Chapter 10 ).\nMost decorators do change the decorated function. They usually do it by\ndefining an inner function and returning it to replace the decorated function.\nCode that uses inner functions almost always depends on closures to\noperate correctly . T o understand closures, we need to take a step back and\nreview how variable scopes work in Python.\nV a r i a b l e  S c o p e  R u l e s\nIn Example 9-3 , we define and test a function that reads two variables: a\nlocal variable a —defined as function parameter—and variable b  that is not\ndefined anywhere in the function.\nExample 9-3. Function r eading a local and a global variable\n>>> def f1(a): \n...     print(a) \n...     print(b) \n... \n>>> f1(3) \n3 \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \n  File ""<stdin>"" , line 3, in f1 \nNameError : global name 'b' is not defined\nThe error we got is not surprising. Continuing from Example 9-3 , if we\nassign a value to a global b  and then call f1 , it works:\n>>> b = 6 \n>>> f1(3) \n3 \n6\nNow , let’ s see an example that may surprise you.\nT ake a look at the f2  function in Example 9-4 . Its first two lines are the\nsame as f1  in Example 9-3 , then it makes an assignment to b . But it fails at\nthe second print , before the assignment is made.\nExample 9-4. V ariable b is local, because it is assigned a value in the body\nof the function\n>>> b = 6 \n>>> def f2(a): \n...     print(a) \n...     print(b) \n...     b = 9 \n... \n>>> f2(3) \n3 \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \n  File ""<stdin>"" , line 3, in f2 \nUnboundLocalError : local variable 'b' referenced before assignment\nNote that the output starts with 3 , which proves that the print(a)\nstatement was executed. But the second one, print(b) , never runs. When\nI first saw this I was surprised, thinking that 6  should be printed, because\nthere is a global variable b  and the assignment to the local b  is made after\nprint(b) .\nBut the fact is, when Python compiles the body of the function, it decides\nthat b  is a local variable because it is assigned within the function. The\ngenerated bytecode reflects this decision and will try to fetch b  from the\nlocal scope. Later , when the call f2(3)  is made, the body of f2  fetches\nand prints the value of the local variable a , but when trying to fetch the\nvalue of local variable b  it discovers that b  is unbound.\nThis is not a bug, but a design choice: Python does not require you to\ndeclare variables, but assumes that a variable assigned in the body of a\nfunction is local. This is much better than the behavior of JavaScript, which\ndoes not require variable declarations either , but if you do for get to declare\nthat a variable is local (with var ), you may clobber a global variable\nwithout knowing.\nIf we want the interpreter to treat b  as a global variable and still assign a\nnew value to it within the function, we use the global  declaration:\n>>> b = 6 \n>>> def f3(a): \n...     global b \n...     print(a) \n...     print(b) \n...     b = 9 \n... \n>>> f3(3) \n3 \n6 \n>>> b \n9\nIn the examples above we can see two scopes in action:\n1 . A module global scope, made of names assigned to values outside\nof any class or function block.\n2 . Function local scopes, made of names assigned to values as\nparameters, or directly in the body of the function.\nThere is one other scope where variables can come from, which we call\nnonlocal  and is fundamental for closres; we’ll see it in a bit.\nAfter this closer look at how variable scopes work in Python, we can tackle\nclosures in the next section, “Closures” . If you are curious about the\nbytecode dif ferences between the functions in Examples 9-3  and 9-4 , see\nthe following sidebar .\nC O M P A R I N G  B Y T E C O D E S\nThe dis  module provides an easy way to disassemble the bytecode of\nPython functions. Read Examples 9-5  and 9-6  to see the bytecodes for\nf1  and f2  from Examples 9-3  and 9-4 .\nE x a m p l e  9 - 5 .  D i s a s s e m b l y  o f  t h e  f 1  f u n c t i o n  f r o m  E x a m p l e  9 - 3\n>>> from dis import dis \n>>> dis(f1) \n  2           0 LOAD_GLOBAL               0 (print)  \n \n              3 LOAD_FAST                 0 (a)  \n \n              6 CALL_FUNCTION             1 (1 positional , 0 \nkeyword pair) \n              9 POP_TOP \n \n  3          10 LOAD_GLOBAL               0 (print) \n             13 LOAD_GLOBAL               1 (b)  \n \n             16 CALL_FUNCTION             1 (1 positional , 0 \nkeyword pair) \n             19 POP_TOP \n             20 LOAD_CONST                0 (None) \n             23 RETURN_VALUE\nLoad global name print .\nLoad local name a .\nLoad global name b .\nContrast the bytecode for f1  shown in Example 9-5  with the bytecode\nfor f2  in Example 9-6 .\nE x a m p l e  9 - 6 .  D i s a s s e m b l y  o f  t h e  f 2  f u n c t i o n  f r o m  E x a m p l e  9 - 4\n>>> dis(f2) \n  2           0 LOAD_GLOBAL               0 (print) \n              3 LOAD_FAST                 0 (a) \n              6 CALL_FUNCTION             1 (1 positional , 0 \nkeyword pair) \n              9 POP_TOP \n \n  3          10 LOAD_GLOBAL               0 (print)",6577
122-Closures.pdf,122-Closures,"13 LOAD_FAST                 1 (b)  \n \n             16 CALL_FUNCTION             1 (1 positional , 0 \nkeyword pair) \n             19 POP_TOP \n \n  4          20 LOAD_CONST                1 (9) \n             23 STORE_FAST                1 (b) \n             26 LOAD_CONST                0 (None) \n             29 RETURN_VALUE\nLoad local  name b . This shows that the compiler considers b  a local\nvariable, even if the assignment to b  occurs later , because the nature\nof the variable—whether it is local or not—cannot change in the\nbody of the function.\nThe CPython VM that runs the bytecode is a stack machine, so LOAD\nand POP  operations refer to the stack. It is beyond the scope of this\nbook to further describe the Python opcodes, but they are documented\nalong with the dis  module in dis — Disassembler for Python\nbytecode .\nC l o s u r e s\nIn the blogosphere, closures are sometimes confused with anonymous\nfunctions. Many confuse them because of the parallel history of those\nfeatures: defining functions inside functions is not so common or\nconvenient, until you have anonymous functions. And closures only matter\nwhen you have nested functions. So a lot of people learn both concepts at\nthe same time.\nActually , a closure is a function—let’ s call it f —with an extended scope\nthat encompasses variables referenced in the body of f  that are not global\nvariables nor local variables of f . Such variables must come from the local\nscope of an outer function which encompasses f .\nIt does not matter whether the function is anonymous or not; what matters is\nthat it can access nonglobal variables that are defined outside of its body .\nThis is a challenging concept to grasp, and is better approached through an\nexample.\nConsider an avg  function to compute the mean of an ever -growing series\nof values; for example, the average closing price of a commodity over its\nentire history . Every day a new price is added, and the average is computed\ntaking into account all prices so far .\nStarting with a clean slate, this is how avg  could be used:\n>>> avg(10) \n10.0 \n>>> avg(11) \n10.5 \n>>> avg(12) \n11.0\nWhere does avg  come from, and where does it keep the history of previous\nvalues?\nFor starters, Example 9-7  is a class-based implementation.\nExample 9-7. average_oo.py: A class to calculate a running average\nclass Averager (): \n \n    def __init__ (self): \n        self.series = [] \n \n    def __call__ (self, new_value ): \n        self.series.append(new_value ) \n        total = sum(self.series) \n        return total / len(self.series)\nThe Averager  class creates instances that are callable:\n>>> avg = Averager () \n>>> avg(10) \n10.0 \n>>> avg(11) \n10.5 \n>>> avg(12) \n11.0\nNow , Example 9-8  is a functional implementation, using the higher -order\nfunction make_averager .\nExample 9-8. average.py: A higher -or der function to calculate a running\naverage\ndef make_averager (): \n    series = [] \n \n    def averager (new_value ): \n        series.append(new_value ) \n        total = sum(series) \n        return total / len(series) \n \n    return averager\nWhen invoked, make_averager  returns an averager  function object.\nEach time an averager  is called, it appends the passed ar gument to the\nseries, and computes the current average, as shown in Example 9-9 .\nExample 9-9. T esting Example 9-8\n>>> avg = make_averager () \n>>> avg(10) \n10.0 \n>>> avg(11) \n10.5 \n>>> avg(12) \n11.0\nNote the similarities of the examples: we call Averager()  or\nmake_averager()  to get a callable object avg  that will update the\nhistorical series and calculate the current mean. In Example 9-7 , avg  is an\ninstance of Averager , and in Example 9-8  it is the inner function,\naverager . Either way , we just call avg(n)  to include n  in the series and\nget the updated mean.\nIt’ s obvious where the avg  of the Averager  class keeps the history: the\nself.series  instance attribute. But where does the avg  function in the\nsecond example find the series ?\nNote that series  is a local variable of make_averager  because the\nassignment series = []  happens in the body of that function. But when\navg(10)  is called, make_averager  has already returned, and its local\nscope is long gone.\nW ithin averager , series  is a fr ee variable . This is a technical term\nmeaning a variable that is not bound in the local scope. See Figure 9-1 .\nFigur e 9-1. The closur e for averager extends the scope of that function to include the binding for the\nfr ee variable series.\nInspecting the returned averager  object shows how Python keeps the\nnames of local and free variables in the __code__  attribute that represents\nthe compiled body of the function. Example 9-10  demonstrates.\nExample 9-10. Inspecting the function cr eated by make_averager in\nExample 9-8",4847
123-The nonlocal Declaration.pdf,123-The nonlocal Declaration,">>> avg.__code__ .co_varnames  \n('new_value', 'total')  \n>>> avg.__code__ .co_freevars  \n('series',)\nThe value for series  is kept in the __closure__  attribute of the\nreturned function avg . Each item in avg.__closure__  corresponds to\na name in avg.__code__.co_freevars . These items are cells , and\nthey have an attribute called cell_contents  where the actual value can\nbe found. Example 9-1 1  shows these attributes.\nExample 9-1 1. Continuing fr om Example 9-9\n>>> avg.__code__ .co_freevars  \n('series',)  \n>>> avg.__closure__  \n(<cell at 0x107a44f78: list object at 0x107a91a48>,)  \n>>> avg.__closure__ [0].cell_contents  \n[10, 11, 12]\nT o summarize: a closure is a function that retains the bindings of the free\nvariables that exist when the function is defined, so that they can be used\nlater when the function is invoked and the defining scope is no longer\navailable.\nNote that the only situation in which a function may need to deal with\nexternal variables that are nonglobal is when it is nested in another function\nand those variables are part of the local scope of the outer function.\nT h e  nonlocal  D e c l a r a t i o n\nOur previous implementation of make_averager  was not ef ficient. In\nExample 9-8 , we stored all the values in the historical series and computed\ntheir sum  every time averager  was called. A better implementation\nwould only store the total and the number of items so far , and compute the\nmean from these two numbers.\nExample 9-12  is a broken implementation, just to make a point. Can you\nsee where it breaks?\nExample 9-12. A br oken higher -or der function to calculate a running\naverage without keeping all history\ndef make_averager (): \n    count = 0 \n    total = 0 \n \n    def averager (new_value ): \n        count += 1 \n        total += new_value  \n        return total / count \n \n    return averager\nIf you try Example 9-12 , here is what you get:\n>>> avg = make_averager () \n>>> avg(10) \nTraceback (most recent call last):  \n  ... \nUnboundLocalError : local variable 'count' referenced before  \nassignment  \n>>>\nThe problem is that the statement count += 1  actually means the same\nas count = count + 1 , when count  is a number or any immutable\ntype. So we are actually assigning to count  in the body of averager ,\nand that makes it a local variable. The same problem af fects the total\nvariable.\nW e did not have this problem in Example 9-8  because we never assigned to\nthe series  name; we only called series.append  and invoked sum\nand len  on it. So we took advantage of the fact that lists are mutable.\nBut with immutable types like numbers, strings, tuples, etc., all you can do\nis read, never update. If you try to rebind them, as in count = count +\n1 , then you are implicitly creating a local variable count . It is no longer a\nfree variable, and therefore it is not saved in the closure.\nT o work around this, the nonlocal  keyword was introduced in Python 3.\nIt lets you declare a variable as a free variable even when it is assigned\nwithin the function. If a new value is assigned to a nonlocal  variable, the\nbinding stored in the closure is changed. A correct implementation of our\nnewest make_averager  looks like Example 9-13 .\nExample 9-13. Calculate a running average without keeping all history\n(fixed with the use of nonlocal)\ndef make_averager (): \n    count = 0 \n    total = 0 \n \n    def averager (new_value ): \n        nonlocal  count, total \n        count += 1 \n        total += new_value  \n        return total / count \n \n    return averager\nAfter studing the use of nonlocal , let’ s summarize how Python’ s variable\nlookup works.\nThe Python bytecode compiler determines when the function is defined\nhow to fetch a variable x  that appears in it, based on these rules:\nIf there is a global x  declaration, x  comes from and is assigned\nto the x  global variable the module.\nIf there is a nonlocal x  declaration, x  comes from and is\nassigned to the x  local variable of the nearest surrounding function\nwhere x  is defined.\nIf x  is a parameter or is assigned a value in the function body , then\nx  is local variable.\nIf x  is referenced but is not assigned and is not a parameter:\nx  will be looked up in the local scopes of the surrounding function\nbodies (nonlocal scopes);\nIf not found in sorrounding scopes, it will be read from the module\nglobal scope;3\n4",4434
124-How It Works.pdf,124-How It Works,"If not found in the global scope, it will be read from\n__builtins__.__dict__ .\nNow that we have Python closures covered, we can ef fectively implement\ndecorators with nested functions.\nI m p l e m e n t i n g  a  S i m p l e  D e c o r a t o r\nExample 9-14  is a decorator that clocks every invocation of the decorated\nfunction and displays the elapsed time, the ar guments passed, and the result\nof the call.\nExample 9-14. clockdeco0.py : simple decorator to show the running\ntime of functions\nimport time \n \n \ndef clock(func): \n    def clocked(*args):  \n \n        t0 = time.perf_counter () \n        result = func(*args)  \n \n        elapsed = time.perf_counter () - t0 \n        name = func.__name__  \n        arg_str = ', '.join(repr(arg) for arg in args) \n        print(f'[{elapsed:0.8f} s] {name}({arg_str} ) -> {result!r} ') \n        return result \n    return clocked  \nDefine inner function clocked  to accept any number of positional\nar guments.\nThis line only works because the closure for clocked  encompasses\nthe func  free variable.\nReturn the inner function to replace the decorated function.\nExample 9-15  demonstrates the use of the clock  decorator .\nExample 9-15. Using the clock decorator\nimport time \nfrom clockdeco0  import clock \n \n@clock \ndef snooze(seconds): \n    time.sleep(seconds) \n \n@clock \ndef factorial (n): \n    return 1 if n < 2 else n*factorial (n-1) \n \nif __name__  == '__main__' : \n    print('*' * 40, 'Calling snooze(.123)' ) \n    snooze(.123) \n    print('*' * 40, 'Calling factorial(6)' ) \n    print('6! =', factorial (6))\nThe output of running Example 9-15  looks like this:\n$ python3 clockdeco_demo.py  \n**************************************** Calling snooze (.123) \n[0.12363791s ] snooze(0.123) -> None  \n**************************************** Calling factorial (6) \n[0.00000095s ] factorial (1) -> 1 \n[0.00002408s ] factorial (2) -> 2 \n[0.00003934s ] factorial (3) -> 6 \n[0.00005221s ] factorial (4) -> 24 \n[0.00006390s ] factorial (5) -> 120 \n[0.00008297s ] factorial (6) -> 720 \n6! = 720\nHow It W orks\nRemember that this code:\n@clock \ndef factorial (n): \n    return 1 if n < 2 else n*factorial (n-1)\nActually does this:\ndef factorial (n): \n    return 1 if n < 2 else n*factorial (n-1) \n \nfactorial  = clock(factorial )\nSo, in both examples, clock  gets the factorial  function as its func\nar gument (see Example 9-14 ). It then creates and returns the clocked\nfunction, which the Python interpreter assigns to factorial  (behind the\nscenes, in the first example). In fact, if you import the clockdeco_demo\nmodule and check the __name__  of factorial , this is what you get:\n>>> import clockdeco_demo  \n>>> clockdeco_demo .factorial .__name__  \n'clocked'  \n>>>\nSo factorial  now actually holds a reference to the clocked  function.\nFrom now on, each time factorial(n)  is called, clocked(n)  gets\nexecuted. In essence, clocked  does the following:\n1 . Records the initial time t0 .\n2 . Calls the original factorial  function, saving the result.\n3 . Computes the elapsed time.\n4 . Formats and displays the collected data.\n5 . Returns the result saved in step 2.\nThis is the typical behavior of a decorator: it replaces the decorated function\nwith a new function that accepts the same ar guments and (usually) returns\nwhatever the decorated function was supposed to return, while also doing\nsome extra processing.",3443
125-Memoization with functools.cache.pdf,125-Memoization with functools.cache,"T I P\nIn Design Patterns  by Gamma et al., the short description of the Decorator pattern starts\nwith: “Attach additional responsibilities to an object dynamically .” Function decorators\nfit that description. But at the implementation level, Python decorators bear little\nresemblance to the classic Decorator described in the original Design Patterns  work.\n“Soapbox”  has more on this subject.\nThe clock  decorator implemented in Example 9-14  has a few\nshortcomings: it does not support keyword ar guments, and it masks the\n__name__  and __doc__  of the decorated function. Example 9-16  uses\nthe functools.wraps  decorator to copy the relevant attributes from\nfunc  to clocked . Also, in this new version, keyword ar guments are\ncorrectly handled.\nExample 9-16. clockdeco.py : an impr oved clock decorator\nimport time \nimport functools  \n \n \ndef clock(func): \n    @functools .wraps(func) \n    def clocked(*args, **kwargs): \n        t0 = time.perf_counter () \n        result = func(*args, **kwargs) \n        elapsed = time.perf_counter () - t0 \n        name = func.__name__  \n        arg_lst = [repr(arg) for arg in args] \n        arg_lst.extend(f'{k}={v!r}' for k, v in kwargs.items()) \n        arg_str = ', '.join(arg_lst) \n        print(f'[{elapsed:0.8f} s] {name}({arg_str} ) -> {result!r} ') \n        return result \n    return clocked\nfunctools.wraps  is just one of the ready-to-use decorators in the\nstandard library . In the next section, we’ll meet the most impressive\ndecorator that functools  provides: cache .\nD e c o r a t o r s  i n  t h e  S t a n d a r d  L i b r a r y\nPython has three built-in functions that are designed to decorate methods:\nproperty , classmethod , and staticmethod . W e will discuss\nproperty  in “Using a Property for Attribute V alidation”  and the others in\n“classmethod  V ersus staticmethod ” .\nIn Example 9-16  we saw another important decorator:\nfunctools.wraps , a helper for building well-behaved decorators. Some\nof the most interesting decorators in the standard library are cache ,\nlru_cache , and singledispatch —all from the functools\nmodule. W e’ll cover them next.\nMemoization with functools.cache\nThe functools.cache  decorator implements memoization:  an\noptimization technique that works by saving the results of previous\ninvocations of an expensive function, avoiding repeat computations on\npreviously used ar guments.\nT I P\nfunctools.cache  was added in Python 3.9. If you need to run these examples in\nPython 3.8, replace @cache  with @lru_cache . For prior versions of Python, you\nmust invoke the decorator , writing @lru_cache() , as explained in “Using lru_cache”\nA good demonstration is to apply @cache  to the painfully slow recursive\nfunction to generate the n th number in the Fibonacci sequence, as shown in\nExample 9-17 .\nExample 9-17. The very costly r ecursive way to compute the nth number in\nthe Fibonacci series\nfrom clockdeco  import clock \n \n \n@clock \ndef fibonacci (n): \n    if n < 2: \n        return n 5\n    return fibonacci (n - 2) + fibonacci (n - 1) \n \n \nif __name__  == '__main__' : \n    print(fibonacci (6))\nHere is the result of running fibo_demo.py . Except for the last line, all\noutput is generated by the clock  decorator:\n$ python3 fibo_demo.py  \n[0.00000042s] fibonacci(0) -> 0  \n[0.00000049s] fibonacci(1) -> 1  \n[0.00006115s] fibonacci(2) -> 1  \n[0.00000031s] fibonacci(1) -> 1  \n[0.00000035s] fibonacci(0) -> 0  \n[0.00000030s] fibonacci(1) -> 1  \n[0.00001084s] fibonacci(2) -> 1  \n[0.00002074s] fibonacci(3) -> 2  \n[0.00009189s] fibonacci(4) -> 3  \n[0.00000029s] fibonacci(1) -> 1  \n[0.00000027s] fibonacci(0) -> 0  \n[0.00000029s] fibonacci(1) -> 1  \n[0.00000959s] fibonacci(2) -> 1  \n[0.00001905s] fibonacci(3) -> 2  \n[0.00000026s] fibonacci(0) -> 0  \n[0.00000029s] fibonacci(1) -> 1  \n[0.00000997s] fibonacci(2) -> 1  \n[0.00000028s] fibonacci(1) -> 1  \n[0.00000030s] fibonacci(0) -> 0  \n[0.00000031s] fibonacci(1) -> 1  \n[0.00001019s] fibonacci(2) -> 1  \n[0.00001967s] fibonacci(3) -> 2  \n[0.00003876s] fibonacci(4) -> 3  \n[0.00006670s] fibonacci(5) -> 5  \n[0.00016852s] fibonacci(6) -> 8  \n8\nThe waste is obvious: fibonacci(1)  is called eight times,\nfibonacci(2)  five times, etc. But adding just two lines to use cache ,\nperformance is much improved. See Example 9-18 .\nExample 9-18. Faster implementation using caching\nimport functools  \n \nfrom clockdeco  import clock \n \n \n@functools .cache  \n \n@clock  \n \ndef fibonacci (n): \n    if n < 2: \n        return n \n    return fibonacci (n - 2) + fibonacci (n - 1) \n \n \nif __name__  == '__main__ ': \n    print(fibonacci (6))\nThis line works with Python 3.9 or later . See “Using lru_cache”  for\nalternatives supporting earlier versions of Python.\nThis is an example of stacked decorators: @cache  is applied on the\nfunction returned by @clock .\nS T A C K E D  D E C O R A T O R S\nT o make sense of stacked decorators, recall that the @  is syntax sugar for applying the\ndecorator function to the function below it. If there’ s more than one decorator , they\nbehave like nested function calls. This:\n@alpha \n@beta \ndef my_fn(): \n    ...\nIs the same as this:\nmy_fn = alpha(beta(my_fn))\nIn other words, the beta  decorator is applied first, and the function it returns is then\npassed to alpha .\nUsing cache  in Example 9-18 , the fibonacci  function is called only\nonce for each value of n :",5469
126-Using lru_cache.pdf,126-Using lru_cache,"$ python3 fibo_demo_lru.py  \n[0.00000043s] fibonacci(0) -> 0  \n[0.00000054s] fibonacci(1) -> 1  \n[0.00006179s] fibonacci(2) -> 1  \n[0.00000070s] fibonacci(3) -> 2  \n[0.00007366s] fibonacci(4) -> 3  \n[0.00000057s] fibonacci(5) -> 5  \n[0.00008479s] fibonacci(6) -> 8  \n8\nIn another test, to compute fibonacci(30) , Example 9-18  made the 31\ncalls needed in 0.00017s—total time–while the uncached Example 9-17\ntook 12.09s on an Intel Core i7 notebook, because it called\nfibonacci(1)  832,040 times, in a total of 2,692,537 calls.\nAll the ar guments taken by the decorated function must be hashable ,\nbecause the underlying lru_cache  uses a dict  to store the results, and\nthe keys are made from the positional and keyword ar guments used in the\ncalls.\nBesides making silly recursive algorithms viable, @cache  really shines in\napplications that need to fetch information from remote APIs.\nW A R N I N G\nfunctools.cache  can consume all available memory if there is a very lar ge number\nof cache entries. I consider it more suitable for use in short lived command-line scripts.\nIn long running processes, I recommend using functools.lru_cache  with a\nsuitable maxsize  parameter , as explained in the next section.\nUsing lru_cache\nThe functools.cache  decorator is actually a simple wrapper around\nthe older functools.lru_cache  function, which is more flexible and\ncompatible with Python 3.8 and earlier versions.\nThe main advantage of @lru_cache  is that its memory usage is bounded\nby the maxsize  parameter , which has a rather conservative default value\nof 128—which means the cache will hold at most 128 entries at any time.\nThe acronym LRU stands for Least Recently Used, meaning that older\nentries that have not been read for a while are discarded to make room for\nnew ones.\nSince Python 3.8 lru_cache  can be applied in two ways. This is how to\nuse it in the simplest way:\n@lru_cache  \ndef costly_function (a, b): \n    ...\nThe other way—available since Python 3.2—is to invoke it as a function—\nwith () :\n@lru_cache () \ndef costly_function (a, b): \n    ...\nIn both cases above, the default parameters would be used. They are:\nmaxsize=128\nSets the maximum number of entries to be stored. After the cache is full,\nthe least recently used entry is discarded to make room for each new\nentry . For optimal performance, maxsize  should be a power of 2. If\nyou pass maxsize=None  the LRU logic is disabled, so the cache\nworks faster but entries are never discarded, which may consume too\nmuch memory . That’ s what @functools.cache  does.\ntyped=False\nDetermines whether results of dif ferent ar gument types are stored\nseparately . For example, in the default setting, float and integer\nar guments that are considered equal are stored only once, so there\nwould be a single entry for the calls f(1)  and f(1.0) . If\ntyped=True , those ar guments would produce dif ferent entries,\npossibly storing distinct results.",2970
127-Single Dispatch Generic Functions.pdf,127-Single Dispatch Generic Functions,"Here is an example invoking @lru_cache  with non-default parameters:\n@lru_cache (maxsize=2**20, typed=True) \ndef costly_function (a, b): \n    ...\nNow let’ s study another powerful decorator:\nfunctools.singledispatch .\nSingle Dispatch Generic Functions\nImagine we are creating a tool to debug web applications. W e want to\ngenerate HTML displays for dif ferent types of Python objects.\nW e could start with a function like this:\nimport html \n \ndef htmlize(obj): \n    content = html.escape(repr(obj)) \n    return f'<pre>{content} </pre>'\nThat will work for any Python type, but now we want to extend it to\ngenerate custom displays for some types. Some examples:\nstr : replace embedded newline characters with '<br/>\n'  and\nuse <p>  tags instead of <pre> .\nint : show the number in decimal and hexadecimal (with a special\ncase for bool ).\nlist : output an HTML list, formatting each item according to its\ntype.\nfloat  and Decimal : output the value as usual, but also in the\nform of a fraction (why not?).\nThe behavior we want is shown in Example 9-19 .\nExample 9-19. htmlize()  generates HTML tailor ed to differ ent object\ntypes\n>>> htmlize({1, 2, 3})  \n \n'<pre>{1, 2, 3}</pre>'  \n>>> htmlize(abs) \n'<pre>&lt;built-in function abs&gt;</pre>'  \n>>> htmlize('Heimlich & Co. \n- a game ')  \n \n'<p>Heimlich &amp; Co.<br/>\n- a game</p>'  \n>>> htmlize(42)  \n \n'<pre>42 (0x2a)</pre>'  \n>>> print(htmlize(['alpha', 66, {3, 2, 1}]))  \n \n<ul> \n<li><p>alpha</p></li>  \n<li><pre>66 (0x42)</pre></li>  \n<li><pre>{1, 2, 3}</pre></li>  \n</ul> \n>>> htmlize(True)  \n \n'<pre>True</pre>'  \n>>> htmlize(fractions .Fraction (2, 3))  \n \n'<pre>2/3</pre>'  \n>>> htmlize(2/3)   \n \n'<pre>0.6666666666666666 (2/3)</pre>'  \n>>> htmlize(decimal.Decimal('0.02380952 ')) \n'<pre>0.02380952 (1/42)</pre>'\nThe original function is registered for object , so it serves as a catch-\nall to handle ar gument types that don’ t match the other\nimplementations.\nstr  objects are also HTML-escaped but wrapped in <p></p>  with\n<br/>  line breaks inserted before each '\n' .\nAn int  is shown in decimal and hexadecimal, inside <pre></pre> .\nEach list item is formatted according to its type, and the whole sequence\nrendered as an HTML list.\nAlthough bool  is an int  subtype, it gets special treatment.\nShow Fraction  as a fraction.\nShow float  and Decimal  with an approximate fractional equivalent.\nFunction singledispatch\nBecause we don’ t have Java-style method overloading in Python, we can’ t\nsimply create variations of htmlize  with dif ferent signatures for each data\ntype we want to handle dif ferently . A possible solution in Python would be\nto turn htmlize  into a dispatch function, with a chain of if/elif/…  or\nmatch/case/…  calling specialized functions like htmlize_str ,\nhtmlize_int , etc. This is not extensible by users of our module, and is\nunwieldy: over time, the htmlize  dispatcher would become too big, and\nthe coupling between it and the specialized functions would be very tight.\nThe functools.singledispatch  decorator allows dif ferent modules\nto contribute to the overall solution, and lets you easily provide a\nspecialized functions even for types that belong to third party packages that\nyou can’ t edit. If you decorate a plain function with @singledispatch ,\nit becomes the entry point for a generic function : a group of functions to\nperform the same operation in dif ferent ways, depending on the type of the\nfirst ar gument. This is what is meant by the term single-dispatch. If more\nar guments were used to select the specific functions, we’d have multiple-\ndispatch. Example 9-20  shows how .\nW A R N I N G\nfunctools.singledispatch  exists since Python 3.4, but it only supports type\nhints since Python 3.7. The last two functions in Example 9-20  illustrate the syntax that\nworks in all versions of Python since 3.4.\nExample 9-20. @singledispatch  cr eates a custom\n@htmlize.register  to bundle several functions into a generic\nfunction\nfrom functools  import singledispatch  \nfrom collections  import abc \nimport fractions  \nimport decimal \nimport html \nimport numbers \n \n@singledispatch   \n \ndef htmlize(obj: object) -> str: \n    content = html.escape(repr(obj)) \n    return f'<pre>{content}</pre> ' \n \n@htmlize.register   \n \ndef _(text: str) -> str:  \n \n    content = html.escape(text).replace('\n', '<br/>\n') \n    return f'<p>{content}</p> ' \n \n@htmlize.register   \n \ndef _(seq: abc.Sequence ) -> str: \n    inner = '</li>\n<li>'.join(htmlize(item) for item in seq) \n    return '<ul>\n<li>' + inner + '</li>\n</ul>' \n \n@htmlize.register   \n \ndef _(n: numbers.Integral ) -> str: \n    return f'<pre>{n} (0x{n:x})</pre> ' \n \n@htmlize.register   \n \ndef _(n: bool) -> str: \n    return f'<pre>{n}</pre> ' \n \n@htmlize.register (fractions .Fraction )  \n \ndef _(x) -> str: \n    frac = fractions .Fraction (x) \n    return f'<pre>{frac.numerator}/{frac.denominator}</pre> ' \n \n@htmlize.register (decimal.Decimal)  \n \n@htmlize.register (float) \ndef _(x) -> str: \n    frac = fractions .Fraction (x).limit_denominator () \n    return f'<pre>{x} ({frac.numerator}/{frac.denominator})</pre> '\n@singledispatch  marks the base function that handles the\nobject  type.\nEach specialized function is decorated with @«base».register\nThe type of the first ar gument given at runtime determines when this\nparticular function definition will be used. The name of the specialized\nfunctions is irrelevant; _  is a good choice to make this clear .\nFor each additional type to get special treatment, register a new function\nwith a matching type hint in the first parameter .\nThe numbers  ABCs are useful for use with singledispatch .\nbool  is a subtype-of  numbers.Integral , but the\nsingledispatch  logic seeks the implementation with the most\nspecific matching type, regardless of the order they appear in the code.\nIf you don’ t want to, or cannot, add type hints to the decorated function,\nyou can pass a type to the @«base».register  decorator . This\nsyntax works in Python 3.4 or later .\nThe @«base».register  decorator returns the undecorated function,\nso it’ s possible to stack them to register two or more types on the same\nimplementation.\nWhen possible, register the specialized functions to handle ABCs (abstract\nclasses) such as numbers.Integral  and abc.MutableSequence\ninstead of concrete implementations like int  and list . This allows your\ncode to support a greater variety of compatible types. For example, a\nPython extension can provide alternatives to the int  type with fixed bit\nlengths as subclasses of numbers.Integral .\nT I P\nUsing ABCs or typing.Protocol  with @singledispatch  allows your code to\nsupport existing or future classes that are actual or virtual subclasses of those ABCs, or\nthat implement those protocols. The use of ABCs and the concept of a virtual subclass\nare subjects of Chapter 13 .6\n7\n8\n9",6982
128-A Parameterized Registration Decorator.pdf,128-A Parameterized Registration Decorator,"A notable quality of the singledispatch  mechanism is that you can\nregister specialized functions anywhere in the system, in any module. If you\nlater add a module with a new user -defined type, you can easily provide a\nnew custom function to handle that type. And you can write custom\nfunctions for classes that you did not write and can’ t change.\nsingledispatch  is a well-thought-out addition to the standard library ,\nand it of fers more features than I can describe here. PEP 443 — Single-\ndispatch generic functions  is a good reference—but it doesn’ t mention the\nuse of type hints, which were added later . The functools  module\ndocumentation has improved and more up-to-date coverage with several\nexamples in its singledispatch  entry .\nN O T E\n@singledispatch  is not designed to bring Java-style method overloading to\nPython. A single class with many overloaded variations of a method is better than a\nsingle function with a lengthy stretch of if/elif/elif/elif  blocks. But both\nsolutions are flawed because they concentrate too much responsibility in a single code\nunit—the class or the function. The advantage of @singledispatch  is supporting\nmodular extension: each module can register a specialized function for each type it\nsupports. In a realistic use case, you would not have all the implementations of generic\nfunction in the same module as in Example 9-20 .\nW e’ve seen some decorators that take ar guments, for example,\n@lru_cache()  and htmlize.register(float)  created by\n@singledispatch  in Example 9-20 . The next section shows how to\nbuild decorators that accept parameters.\nP a r a m e t e r i z e d  D e c o r a t o r s\nWhen parsing a decorator in source code, Python takes the decorated\nfunction and passes it as the first ar gument to the decorator function. So\nhow do you make a decorator accept other ar guments? The answer is: make\na decorator factory that takes those ar guments and returns a decorator ,\nwhich is then applied to the function to be decorated. Confusing? Sure.\nLet’ s start with an example based on the simplest decorator we’ve seen:\nregister  in Example 9-21 .\nExample 9-21. Abridged r egistration.py module fr om Example 9-2 , r epeated\nher e for convenience\nregistry  = [] \n \ndef register (func): \n    print(f'running register({func})' ) \n    registry .append(func) \n    return func \n \n@register  \ndef f1(): \n    print('running f1()' ) \n \nprint('running main()' ) \nprint('registry ->' , registry ) \nf1()\nA Parameterized Registration Decorator\nIn order to make it easy to enable or disable the function registration\nperformed by register , we’ll make it accept an optional active\nparameter which, if False , skips registering the decorated function.\nExample 9-22  shows how . Conceptually , the new register  function is\nnot a decorator but a decorator factory . When called, it returns the actual\ndecorator that will be applied to the tar get function.\nExample 9-22. T o accept parameters, the new r egister decorator must be\ncalled as a function\nregistry  = set()  \n \n \ndef register (active=True):  \n \n    def decorate (func):  \n \n        print('running register ' \n              f'(active={active})->decorate({func}) ') \n        if active:   \n \n            registry .add(func) \n        else: \n            registry .discard(func)  \n \n \n        return func  \n \n    return decorate   \n \n \n@register (active=False)  \n \ndef f1(): \n    print('running f1() ') \n \n@register ()  \n \ndef f2(): \n    print('running f2() ') \n \ndef f3(): \n    print('running f3() ')\nregistry  is now a set , so adding and removing functions is faster .\nregister  takes an optional keyword ar gument.\nThe decorate  inner function is the actual decorator; note how it takes\na function as ar gument.\nRegister func  only if the active  ar gument (retrieved from the\nclosure) is True .\nIf not active  and func in registry , remove it.\nBecause decorate  is a decorator , it must return a function.\nregister  is our decorator factory , so it returns decorate .\nThe @register  factory must be invoked as a function, with the\ndesired parameters.\nIf no parameters are passed, register  must still be called as a\nfunction—@register() —i.e., to return the actual decorator ,\ndecorate .\nThe main point is that register()  returns decorate , which is then\napplied to the decorated function.\nThe code in Example 9-22  is in a r egistration_param.py  module. If we\nimport it, this is what we get:\n>>> import registration_param  \nrunning register(active=False)->decorate(<function f1 at  \n0x10063c1e0>)  \nrunning register(active=True)->decorate(<function f2 at  \n0x10063c268>)  \n>>> registration_param .registry  \n[<function f2 at 0x10063c268>]\nNote how only the f2  function appears in the registry ; f1  does not\nappear because active=False  was passed to the register  decorator\nfactory , so the decorate  that was applied to f1  did not add it to the\nregistry .\nIf, instead of using the @  syntax, we used register  as a regular function,\nthe syntax needed to decorate a function f  would be register()(f)  to\nadd f  to the registry , or register(active=False)(f)  to not\nadd it (or remove it). See Example 9-23  for a demo of adding and removing\nfunctions to the registry .\nExample 9-23. Using the r egistration_param module listed in Example 9-22\n>>> from registration_param  import * \nrunning register(active=False)->decorate(<function f1 at  \n0x10073c1e0>)  \nrunning register(active=True)->decorate(<function f2 at  \n0x10073c268>)  \n>>> registry   \n \n{<function f2 at 0x10073c268>}  \n>>> register ()(f3)  \n \nrunning register(active=True)->decorate(<function f3 at  \n0x10073c158>)  \n<function f3 at 0x10073c158>  \n>>> registry   \n \n{<function f3 at 0x10073c158>, <function f2 at 0x10073c268>}  \n>>> register (active=False)(f2)  \n \nrunning register(active=False)->decorate(<function f2 at  \n0x10073c268>)",5963
129-The Parameterized Clock Decorator.pdf,129-The Parameterized Clock Decorator,"<function f2 at 0x10073c268>  \n>>> registry   \n \n{<function f3 at 0x10073c158>}\nWhen the module is imported, f2  is in the registry .\nThe register()  expression returns decorate , which is then\napplied to f3 .\nThe previous line added f3  to the registry .\nThis call removes f2  from the registry .\nConfirm that only f3  remains in the registry .\nThe workings of parameterized decorators are fairly involved, and the one\nwe’ve just discussed is simpler than most. Parameterized decorators usually\nreplace the decorated function, and their construction requires yet another\nlevel of nesting. Now we will explore the architecture of one such function\npyramid.\nThe Parameterized Clock Decorator\nIn this section, we’ll revisit the clock  decorator , adding a feature: users\nmay pass a format string to control the output of the clocked function\nreport. See Example 9-24 .\nN O T E\nFor simplicity , Example 9-24  is based on the initial clock  implementation from\nExample 9-14 , and not the improved one from Example 9-16  that uses\n@functools.wraps , adding yet another function layer .\nExample 9-24. Module clockdeco_param.py: the parameterized clock\ndecorator\nimport time \n \nDEFAULT_FMT  = '[{elapsed:0.8f}s] {name}({args}) -> {result} ' \n \ndef clock(fmt=DEFAULT_FMT ):  \n \n    def decorate (func):      \n \n        def clocked(*_args): \n \n            t0 = time.perf_counter () \n            _result = func(*_args)  \n \n            elapsed = time.perf_counter () - t0 \n            name = func.__name__  \n            args = ', '.join(repr(arg) for arg in _args)  \n \n            result = repr(_result)  \n \n            print(fmt.format(**locals()))  \n \n            return _result  \n \n        return clocked  \n \n    return decorate   \n \n \nif __name__  == '__main__ ': \n \n    @clock()  \n \n    def snooze(seconds): \n        time.sleep(seconds) \n \n    for i in range(3): \n        snooze(.123)\nclock  is our parameterized decorator factory .\ndecorate  is the actual decorator .\nclocked  wraps the decorated function.\n_result  is the actual result of the decorated function.\n_args  holds the actual ar guments of clocked , while args  is str\nused for display .\nresult  is the str  representation of _result , for display .\nUsing **locals()  here allows any local variable of clocked  to be\nreferenced in the fmt .10\nclocked  will replace the decorated function, so it should return\nwhatever that function returns.\ndecorate  returns clocked .\nclock  returns decorate .\nIn this self test, clock()  is called without ar guments, so the decorator\napplied will use the default format str .\nIf you run Example 9-24  from the shell, this is what you get:\n$ python3 clockdeco_param.py  \n[0.12412500s ] snooze(0.123) -> None  \n[0.12411904s ] snooze(0.123) -> None  \n[0.12410498s ] snooze(0.123) -> None\nT o exercise the new functionality , let’ s have a look at Examples 9-25  and 9-\n26 , which are two other modules using clockdeco_param , and the\noutputs they generate.\nExample 9-25. clockdeco_param_demo1.py\nimport time \nfrom clockdeco_param  import clock \n \n@clock('{name}: {elapsed} s') \ndef snooze(seconds): \n    time.sleep(seconds) \n \nfor i in range(3): \n    snooze(.123)\nOutput of Example 9-25 :\n$ python3 clockdeco_param_demo1.py  \nsnooze: 0.12414693832397461s  \nsnooze: 0.1241159439086914s  \nsnooze: 0.12412118911743164s",3404
130-Further Reading.pdf,130-Further Reading,"Example 9-26. clockdeco_param_demo2.py\nimport time \nfrom clockdeco_param  import clock \n \n@clock('{name}({args}) dt={elapsed:0.3f} s') \ndef snooze(seconds): \n    time.sleep(seconds) \n \nfor i in range(3): \n    snooze(.123)\nOutput of Example 9-26 :\n$ python3 clockdeco_param_demo2.py  \nsnooze(0.123) dt=0.124s \nsnooze(0.123) dt=0.124s \nsnooze(0.123) dt=0.124s\nN O T E\nGraham Dumpleton and Lennart Regebro—technical reviewer of the First Edition —\nar gue that decorators are best coded as classes implementing __call__ , and not as\nfunctions like the examples in this chapter . I agree that approach is better for non-trivial\ndecorators, but to explain the basic idea of this language feature, functions are easier to\nunderstand. See “Further Reading” , in particular Graham Dumpleton’ s blog and wrapt\nmodule for industrial-strength techniques when building decorators.\nThe next section shows an example in the style recommended by\nDumpleton and Regebro.\nA class-based clock  decorator\nAs a final example, Example 9-27  lists the implementation of a\nparameterized clock  decorator implemented as a class with __call__ .\nContrast Example 9-24  with Example 9-27 . Which one do you prefer?\nExample 9-27. Module clockdeco_cls.py: parameterized clock decorator\nimplemented as class\nimport time \n \nDEFAULT_FMT  = '[{elapsed:0.8f}s] {name}({args}) -> {result} ' \n \nclass clock:  \n \n \n    def __init__ (self, fmt=DEFAULT_FMT ):  \n \n        self.fmt = fmt \n \n    def __call__ (self, func):  \n \n        def clocked(*_args): \n            t0 = time.perf_counter () \n            _result = func(*_args)  \n \n            elapsed = time.perf_counter () - t0 \n            name = func.__name__  \n            args = ', '.join(repr(arg) for arg in _args) \n            result = repr(_result) \n            print(self.fmt.format(**locals())) \n            return _result \n        return clocked\nInstead of a clock  outer function, the clock  class is our\nparameterized decorator factory . I named it with a lowercase c  to make\nclear that this implementation is a drop-in replacement for the one in\nExample 9-24 .\nThe ar gument passed in the clock(my_format)  is assigned to the\nfmt  parameter here. The class constructor returns an instance of\nclock , with my_format  stored in self.fmt .\n__call__  makes the clock  instance callable. When invoked, the\ninstance replaces the decorated function with clocked\nclocked  wraps the decorated function.\nThis ends our exploration of function decorators. W e’ll see class decorators\nin Chapter 25 .\nC h a p t e r  S u m m a r y\nW e covered some dif ficult terrain in this chapter . I tried to make the journey\nas smooth as possible, but we definitely entered the realm of\nmetaprogramming.\nW e started with a simple @register  decorator without an inner function,\nand finished with a parameterized @clock()  involving two levels of\nnested functions.\nRegistration decorators, though simple in essence, have real applications in\nPython frameworks. W e will apply the registration idea in one\nimplementation of the Strategy design pattern in Chapter 10 .\nUnderstanding how decorators actually work required covering the\ndif ference between import time  and runtime , then diving into variable\nscoping, closures, and the new nonlocal  declaration. Mastering closures\nand nonlocal  is valuable not only to build decorators, but also to code\nevent-oriented programs for GUIs or asynchronous I/O with callbacks, and\nto adopt a functional style when it makes sense.\nParameterized decorators almost always involve at least two nested\nfunctions, maybe more if you want to use @functools.wraps  to\nproduce a decorator that provides better support for more advanced\ntechniques. One such technique is stacked decorators, which we saw in\nExample 9-18 . For more sophisticated decorators, a class-based\nimplementation may be easier to read and maintain.\nAs examples of parametrized decorators in the standard library , we visited\nthe powerful @cache  and @singledispatch  from the functools\nmodule.\nF u r t h e r  R e a d i n g\nItem #26 of Brett Slatkin’ s Effective Python, Second Edition  (Addison-\nW esley , 2019) covers best practices for function decorators and\nrecommends always using functools.wraps —which we saw in\nExample 9-16 .\nGraham Dumpleton has a series of in-depth blog posts  about techniques for\nimplementing well-behaved decorators, starting with “How Y ou\nImplemented Y our Python Decorator is W rong” . His deep expertise in this\nmatter is also nicely packaged in the wrapt  module he wrote to simplify the\nimplementation of decorators and dynamic function wrappers, which\nsupport introspection and behave correctly when further decorated, when\napplied to methods and when used as attribute descriptors. Chapter 24  in\nPart VI  is about descriptors.\nChapter 9, Metapr ogramming  of the Python Cookbook, Thir d Edition  by\nDavid Beazley and Brian K. Jones (O’Reilly), has several recipes from\nelementary decorators to very sophisticated ones, including one that can be\ncalled as a regular decorator or as a decorator factory , e.g., @clock  or\n@clock() . That’ s “Recipe 9.6. Defining a Decorator That T akes an\nOptional Ar gument” in that cookbook.\nMichele Simionato authored a package aiming to “simplify the usage of\ndecorators for the average programmer , and to popularize decorators by\nshowing various non-trivial examples,” according to the docs. It’ s available\non PyPI as the decorator package .\nCreated when decorators were still a new feature in Python, the Python\nDecorator Library wiki page  has dozens of examples. Because that page\nstarted years ago, some of the techniques shown have been superseded, but\nthe page is still an excellent source of inspiration.\n“Closures in Python”  is a short blog post by Fredrik Lundh that explains the\nterminology of closures.\nPEP 3104 — Access to Names in Outer Scopes  describes the introduction\nof the nonlocal  declaration to allow rebinding of names that are neither\nlocal nor global. It also includes an excellent overview of how this issue is\nresolved in other dynamic languages (Perl, Ruby , JavaScript, etc.) and the\npros and cons of the design options available to Python.11\nOn a more theoretical level, PEP 227 — Statically Nested Scopes\ndocuments the introduction of lexical scoping as an option in Python 2.1\nand as a standard in Python 2.2, explaining the rationale and design choices\nfor the implementation of closures in Python.\nPEP 443  provides the rationale and a detailed description of the single-\ndispatch generic functions’ facility . An old (March 2005) blog post by\nGuido van Rossum, “Five-Minute Multimethods in Python” , walks through\nan implementation of generic functions (a.k.a. multimethods) using\ndecorators. His code supports multiple-dispatch (i.e., dispatch based on\nmore than one positional ar gument). Guido’ s multimethods code is\ninteresting, but it’ s a didactic example. For a modern, production-ready\nimplementation of multiple-dispatch generic functions, check out Reg  by\nMartijn Faassen—author of the model-driven and REST -savvy Morepath\nweb framework.\nS O A P B O X\nThe designer of any language with first-class functions faces this issue:\nbeing first-class objects, functions are defined in a certain scope but\nmay be invoked in other scopes. The question is: how to evaluate the\nfree variables? The first and simplest answer is “dynamic scope.” This\nmeans that free variables are evaluated by looking into the environment\nwhere the function is invoked.\nIf Python had dynamic scope and no closures, we could improvise avg\n—similar to Example 9-8 —like this:\n>>> ### this is not a real Python console session! ###  \n>>> avg = make_averager () \n>>> series = []  \n \n>>> avg(10) \n10.0 \n>>> avg(11)  \n \n10.5 \n>>> avg(12) \n11.0 \n>>> series = [1]  \n \n>>> avg(5) \n3.0\nBefore using avg , we have to define series = []  ourselves, so\nwe must know that averager  (inside make_averager ) refers\nto a list named series .\nBehind the scenes, series  accumulates the values to be averaged.\nWhen series = [1]  is executed, the previous list is lost. This\ncould happen by accident, when handling two independent running\naverages at the same time.\nFunctions should be black boxes, with their implementation hidden\nfrom users. But with dynamic scope, if a function uses free variables,\nthe programmer has to know its internals to set up an environment\nwhere it works correctly . After years of struggling with the LaT eX\ndocument preparation language, the excellent Practical LaT eX  book by\nGeor ge Grätzer taught me that LaT eX variables use dynamic scope.\nThat’ s why they were so confusing to me!\nEmacs Lisp also uses dynamic scope, at least by default. See Dynamic\nBinding  in the Emacs Lisp manual for a short explanation.\nDynamic scope is easier to implement, which is probably why it was\nthe path taken by John McCarthy when he created Lisp, the first\nlanguage to have first-class functions. Paul Graham’ s article “The Roots\nof Lisp”  is an accessible explanation of John McCarthy’ s original paper\nabout the Lisp language: “Recursive Functions of Symbolic\nExpressions and Their Computation by Machine, Part I” . McCarthy’ s\npaper is a masterpiece as great as Beethoven’ s 9th Symphony . Paul\nGraham translated it for the rest of us, from mathematics to English and\nrunning code.\nPaul Graham’ s commentary explains how tricky dynamic scoping is.\nQuoting from “The Roots of Lisp”:\nIt’ s an eloquent testimony to the dangers of dynamic scope that even\nthe very first example of higher -or der Lisp functions was br oken\nbecause of it. It may be that McCarthy was not fully awar e of the\nimplications of dynamic scope in 1960. Dynamic scope r emained in\nLisp implementations for a surprisingly long time—until Sussman\nand Steele developed Scheme in 1975. Lexical scope does not\ncomplicate the definition of eval  very much, but it may make\ncompilers har der to write.\nT oday , lexical scope is the norm: free variables are evaluated\nconsidering the environment where the function is defined. Lexical\nscope complicates the implementation of languages with first-class\nfunctions, because it requires the support of closures. On the other\nhand, lexical scope makes source code easier to read. Most languages\ninvented since Algol have lexical scope.\nFor many years, Python lambdas  did not provide closures,\ncontributing to the bad name of this feature among functional-\nprogramming geeks in the blogosphere. This was fixed in Python 2.2\n(December 2001), but the blogosphere has a long memory . Since then,\nlambda  is embarrassing only because of its limited syntax.\nPython Decorators and the Decorator Design Pattern\nPython function decorators fit the general description of Decorator\ngiven by Gamma et al. in Design Patterns : “Attach additional\nresponsibilities to an object dynamically . Decorators provide a flexible\nalternative to subclassing for extending functionality .”\nAt the implementation level, Python decorators do not resemble the\nclassic Decorator design pattern, but an analogy can be made.\nIn the design pattern, Decorator  and Component  are abstract\nclasses. An instance of a concrete decorator wraps an instance of a\nconcrete component in order to add behaviors to it. Quoting from\nDesign Patterns :\nThe decorator conforms to the interface of the component it\ndecorates so that its pr esence is transpar ent to the component’ s\nclients. The decorator forwar ds r equests to the component and may\nperform additional actions (such as drawing a bor der) befor e or after\nforwar ding. T ranspar ency lets you nest decorators r ecursively ,\nther eby allowing an unlimited number of added r esponsibilities.” (p.\n175)\nIn Python, the decorator function plays the role of a concrete\nDecorator  subclass, and the inner function it returns is a decorator\ninstance. The returned function wraps the function to be decorated,\nwhich is analogous to the component in the design pattern. The returned\nfunction is transparent because it conforms to the interface of the\ncomponent by accepting the same ar guments. It forwards calls to the\ncomponent and may perform additional actions either before or after it.\nBorrowing from the previous citation, we can adapt the last sentence to\nsay that “T ransparency lets you stack decorators, thereby allowing an\nunlimited number of added behaviors.”\nNote that I am not suggesting that function decorators should be used to\nimplement the Decorator pattern in Python programs. Although this can\nbe done in specific situations, in general the Decorator pattern is best\nimplemented with classes to represent the Decorator and the\ncomponents it will wrap.\n1  That’ s the 1995 Design Patterns  book by the so-called Gang of Four .\n2  If you replace “function” with “class” in the previous sentence, you have a brief description of\nwhat a class decorator does. Class decorators are covered in Chapter 25 .\n3  Thanks to tech reviewer Leonardo Rochael suggesting this summary .\n4  Python does not have a program global scope, only module global scopes.\n5  T o clarify , this is not a typo: “memoization”  is a computer science term vaguely related to\n“memorization”, but not the same.\n6  Unfortunately , Mypy 0.770 complains when it sees multiple functions with the same name…\n7  Despite the warning in “The Fall of the Numeric T ower” , the number  ABC are not\ndeprecated and you find them in Python 3 code.\n8  Maybe one day you’ll also be able to express this with single unparameterized\n@htmlize.register  and type hint using Union , but when I tried, Python raised a\nTypeError  with a message saying that Union  is not a class. So, although PEP 484 syntax  is\nsupported by @singledispatch , the semantics  are not there yet.\n9  NumPy , for example, implements several machine-oriented integer and floating-point  types.\n10  T ech reviewer Miroslav Šedivý noted: “It also means that code linters will complain about\nunused variables since they tend to ignore uses of locals() .” Y es, that’ s yet another\nexample of how static checking tools discourage the use of the dynamic features that attracted\nme and countless programmers to Python in the first place. T o make the linter happy , I could\nspell out each local variable twice in the call: .format(elapsed=elapsed,\nname=name, args=args, result=result) . I’d rather not. If you use static checking\ntools, it’ s very important to know when to ignore them.\n11  I wanted to make the code as simple as possible, so I did not follow Slatkin’ s excellent advice\nin all examples.",14732
131-Case Study Refactoring Strategy.pdf,131-Case Study Refactoring Strategy,"Chapter 10. Design Patterns\nwith First-Class Functions\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 10th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nConformity to patterns is not a measur e of goodness.\n— Ralph Johnson, Coauthor of the Design Patterns classic\nIn software engineering, a design pattern  is a general recipe for solving a\ncommon design problem. Y ou don’ t need to know design patterns to follow\nthis chapter . I will explain the patterns used in the examples.\nThe use of design patterns in programming was popularized by the\nlandmark book Design Patterns: Elements of Reusable Object-Oriented\nSoftwar e  (Addison-W esley , 1995) by Erich Gamma, Richard Helm, Ralph\nJohnson & John Vlissides—a.k.a. “the Gang of Four .” The book is a catalog\nof 23 patterns consisting of arrangements of classes exemplified with code\nin C++, but assumed to be useful in other Object-Oriented languages as\nwell.1\nAlthough design patterns are language-independent, that does not mean\nevery pattern applies to every language. For example, Chapter 17  will show\nthat it doesn’ t make sense to emulate the recipe of the Iterator  pattern in\nPython, because the pattern is embedded in the language and ready to use in\nin the form of generators—which don’ t need classes to work, and require\nless code than the classic recipe.\nThe authors of Design Patterns  acknowledge in their Intr oduction  that the\nimplementation language determines which patterns are relevant:\nThe choice of pr ogramming language is important because it influences\none’ s point of view . Our patterns assume Smalltalk/C++-level language\nfeatur es, and that choice determines what can and cannot be\nimplemented easily . If we assumed pr ocedural languages, we might have\nincluded design patterns called “Inheritance,” “Encapsulation,” and\n“Polymorphism.” Similarly , some of our patterns ar e supported dir ectly\nby the less common object-oriented languages. CLOS has multi-methods,\nfor example, which lessen the need for a pattern such as V isitor .\nIn his 1996 presentation, “Design Patterns in Dynamic Languages” , Peter\nNorvig states that 16 out of the 23 patterns in the original Design Patterns\nbook become either “invisible or simpler” in a dynamic language (slide 9).\nHe was talking about the Lisp and Dylan languages, but many of the\nrelevant dynamic features are also present in Python. In particular , in the\ncontext of languages with first-class functions, Norvig suggests rethinking\nthe classic patterns known as Strategy , Command, T emplate Method, and\nV isitor .\nThe goal of this chapter is to show how—in some cases—functions can do\nthe same work as classes, with code that is shorter and easier to read. W e\nwill refactor an implementation of Strategy using functions as objects,\nremoving a lot of boilerplate code. W e’ll also discuss a similar approach to\nsimplifying the Command pattern.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r2",3478
132-Classic Strategy.pdf,132-Classic Strategy,"I moved this chapter to the end of Part III so I could apply a registration\ndecorator in “Decorator -Enhanced Strategy Pattern”  and also use type hints\nin the examples. Most type hints used in this chapter are not complicated,\nand they do help with readability .\nC a s e  S t u d y :  R e f a c t o r i n g  S t r a t e g y\nStrategy is a good example of a design pattern that can be simpler in Python\nif you leverage functions as first-class objects. In the following section, we\ndescribe and implement Strategy using the “classic” structure described in\nDesign Patterns . If you are familiar with the classic pattern, you can skip to\n“Function-Oriented Strategy”  where we refactor the code using functions,\nsignificantly reducing the line count.\nClassic Strategy\nThe UML class diagram in Figure 10-1  depicts an arrangement of classes\nthat exemplifies the Strategy pattern.\nFigur e 10-1. UML class diagram for or der discount pr ocessing implemented with the Strategy design\npattern\nThe Strategy pattern is summarized like this in Design Patterns :\nDefine a family of algorithms, encapsulate each one, and make them\ninter changeable. Strategy lets the algorithm vary independently fr om\nclients that use it.\nA clear example of Strategy applied in the ecommerce domain is computing\ndiscounts to orders according to the attributes of the customer or inspection\nof the ordered items.\nConsider an online store with these discount rules:\nCustomers with 1,000 or more fidelity points get a global 5%\ndiscount per order .\nA 10% discount is applied to each line item with 20 or more units\nin the same order .\nOrders with at least 10 distinct items get a 7% global discount.\nFor brevity , let’ s assume that only one discount may be applied to an order .\nThe UML class diagram for the Strategy pattern is depicted in Figure 10-1 .\nIts participants are:\nContext\nProvides a service by delegating some computation to interchangeable\ncomponents that implement alternative algorithms. In the ecommerce\nexample, the context is an Order , which is configured to apply a\npromotional discount according to one of several algorithms.\nStrategy\nThe interface common to the components that implement the dif ferent\nalgorithms. In our example, this role is played by an abstract class\ncalled Promotion .\nConcr ete Strategy\nOne of the concrete subclasses of Strategy . FidelityPromo ,\nBulkPromo , and LargeOrderPromo  are the three concrete\nstrategies implemented.\nThe code in Example 10-1  follows the blueprint in Figure 10-1 . As\ndescribed in Design Patterns , the concrete strategy is chosen by the client\nof the context class. In our example, before instantiating an order , the\nsystem would somehow select a promotional discount strategy and pass it to\nthe Order  constructor . The selection of the strategy is outside the scope of\nthe pattern.\nExample 10-1. Implementation of the Order  class with pluggable discount\nstrategies.\nfrom abc import ABC, abstractmethod  \nfrom collections.abc  import Sequence  \nfrom decimal import Decimal \nfrom typing import NamedTuple , Optional  \n \n \nclass Customer (NamedTuple ): \n    name: str \n    fidelity : int \n \n \nclass LineItem (NamedTuple ): \n    product: str \n    quantity : int \n    price: Decimal \n \n    def total(self) -> Decimal: \n        return self.price * self.quantity  \n \n \nclass Order(NamedTuple ):  # the Context  \n    customer : Customer  \n    cart: Sequence [LineItem ] \n    promotion : Optional ['Promotion' ] = None \n \n    def total(self) -> Decimal: \n        totals = (item.total() for item in self.cart) \n        return sum(totals, start=Decimal(0)) \n \n    def due(self) -> Decimal: \n        if self.promotion  is None: \n            discount  = Decimal(0) \n        else: \n            discount  = self.promotion .discount (self) \n        return self.total() - discount  \n \n    def __repr__ (self): \n        return f'<Order total: {self.total():.2f} due:  \n{self.due():.2f}>'  \n \n \nclass Promotion (ABC):  # the Strategy: an abstract base class  \n    @abstractmethod  \n    def discount (self, order: Order) -> Decimal: \n        """"""Return discount as a positive dollar amount""""""  \n \n \nclass FidelityPromo (Promotion ):  # first Concrete Strategy  \n    """"""5% discount for customers with 1000 or more fidelity  \npoints""""""  \n \n    def discount (self, order: Order) -> Decimal: \n        rate = Decimal('0.05') \n        if order.customer .fidelity  >= 1000: \n            return order.total() * rate \n        return Decimal(0) \n \n \nclass BulkItemPromo (Promotion ):  # second Concrete Strategy  \n    """"""10% discount for each LineItem with 20 or more units""""""  \n \n    def discount (self, order: Order) -> Decimal: \n        discount  = Decimal(0) \n        for item in order.cart: \n            if item.quantity  >= 20: \n                discount  += item.total() * Decimal('0.1') \n        return discount  \n \n \nclass LargeOrderPromo (Promotion ):  # third Concrete Strategy  \n    """"""7% discount for orders with 10 or more distinct items""""""  \n \n    def discount (self, order: Order) -> Decimal: \n        distinct_items  = {item.product for item in order.cart} \n        if len(distinct_items ) >= 10: \n            return order.total() * Decimal('0.07') \n        return Decimal(0)\nNote that in Example 10-1 , I coded Promotion  as an abstract base class\n(ABC), to use the @abstractmethod  decorator and make the pattern\nmore explicit.\nExample 10-2  shows doctests used to demonstrate and verify the operation\nof a module implementing the rules described earlier .\nExample 10-2. Sample usage of Order  class with differ ent pr omotions\napplied.\n    >>> joe = Customer('John Doe', 0)  \n  \n    >>> ann = Customer('Ann Smith', 1100)  \n    >>> cart = (LineItem('banana', 4, Decimal('.5')),  \n  \n    ...         LineItem('apple', 10, Decimal('1.5')),  \n    ...         LineItem('watermelon', 5, Decimal(5)))  \n    >>> Order(joe, cart, FidelityPromo())  \n  \n    <Order total: 42.00 due: 42.00>  \n    >>> Order(ann, cart, FidelityPromo())  \n  \n    <Order total: 42.00 due: 39.90>  \n    >>> banana_cart = (LineItem('banana', 30, Decimal('.5')),  \n  \n    ...                LineItem('apple', 10, Decimal('1.5')))  \n    >>> Order(joe, banana_cart, BulkItemPromo())  \n  \n    <Order total: 30.00 due: 28.50>  \n    >>> long_cart = tuple(LineItem(str(sku), 1, Decimal(1)) \n  \n    ...                  for sku in range(10))  \n    >>> Order(joe, long_cart, LargeOrderPromo())  \n  \n    <Order total: 10.00 due: 9.30>  \n    >>> Order(joe, cart, LargeOrderPromo())  \n    <Order total: 42.00 due: 42.00>\nT wo customers: joe  has 0 fidelity points, ann  has 1,100.\nOne shopping cart with three line items.\nThe FidelityPromo  promotion gives no discount to joe .\nann  gets a 5% discount because she has at least 1,000 points.\nThe banana_cart  has 30 units of the ""banana""  product and 10\napples.",6970
133-Function-Oriented Strategy.pdf,133-Function-Oriented Strategy,"Thanks to the BulkItemPromo , joe  gets a $1.50 discount on the\nbananas.\nlong_cart  has 10 dif ferent items at $1.00 each.\njoe  gets a 7% discount on the whole order because of\nLargerOrderPromo .\nExample 10-1  works perfectly well, but the same functionality can be\nimplemented with less code in Python by using functions as objects. The\nnext section shows how .\nFunction-Oriented Strategy\nEach concrete strategy in Example 10-1  is a class with a single method,\ndiscount . Furthermore, the strategy instances have no state (no instance\nattributes). Y ou could say they look a lot like plain functions, and you\nwould be right. Example 10-3  is a refactoring of Example 10-1 , replacing\nthe concrete strategies with simple functions and removing the Promo\nabstract class. Only small adjustments are needed in the Order  class.\nExample 10-3. Order  class with discount strategies implemented as\nfunctions.\nfrom collections.abc  import Sequence  \nfrom dataclasses  import dataclass  \nfrom decimal import Decimal \nfrom typing import Optional , Callable , NamedTuple  \n \n \nclass Customer (NamedTuple ): \n    name: str \n    fidelity : int \n \n \nclass LineItem (NamedTuple ): \n    product: str \n    quantity : int \n    price: Decimal \n 3\n    def total(self): \n        return self.price * self.quantity  \n \n@dataclass (frozen=True) \nclass Order:  # the Context  \n    customer : Customer  \n    cart: Sequence [LineItem ] \n    promotion : Optional [Callable [['Order'], Decimal]] = None  \n \n \n    def total(self) -> Decimal: \n        totals = (item.total() for item in self.cart) \n        return sum(totals, start=Decimal(0)) \n \n    def due(self) -> Decimal: \n        if self.promotion  is None: \n            discount  = Decimal(0) \n        else: \n            discount  = self.promotion (self)  \n \n        return self.total() - discount  \n \n    def __repr__ (self): \n        return f'<Order total: {self.total():.2f} due:  \n{self.due():.2f}> ' \n \n \n \n \n \ndef fidelity_promo (order: Order) -> Decimal:  \n \n    """"""5% discount for customers with 1000 or more fidelity  \npoints""""""  \n    if order.customer .fidelity  >= 1000: \n        return order.total() * Decimal('0.05') \n    return Decimal(0) \n \n \ndef bulk_item_promo (order: Order) -> Decimal: \n    """"""10% discount for each LineItem with 20 or more units""""""  \n    discount  = Decimal(0) \n    for item in order.cart: \n        if item.quantity  >= 20: \n            discount  += item.total() * Decimal('0.1') \n    return discount  \n \n \ndef large_order_promo (order: Order) -> Decimal: \n    """"""7% discount for orders with 10 or more distinct items""""""  \n    distinct_items  = {item.product for item in order.cart} \n    if len(distinct_items ) >= 10: \n        return order.total() * Decimal('0.07') \n    return Decimal(0)\nThis type hint says: promotion  may be None , or it may be a callable\nthat takes an Order  ar gument and returns a Decimal .\nT o compute a discount, call the self.promotion  callable, passing\nself  as an ar gument. See below for the reason.\nNo abstract class.\nEach strategy is a function.\nW H Y  SELF.PROMOTION(SELF)\nIn the Order  class, promotion  is not a method. It’ s an instance attribute that happens\nto be callable. So the first part of the expression, self.promotion , retrieves that\ncallable. T o invoke it, we must provide an instance of Order , which in this case is\nself . That’ s why self  appears twice in that expression.\n“Methods Are Descriptors”  will explain the mechanism that binds methods to instances\nautomatically . It does not apply to promotion  because it is not a method.\nThe code in Example 10-3  is shorter than Example 10-1 . Using the new\nOrder  is also a bit simpler , as shown in the Example 10-4  doctests.\nExample 10-4. Sample usage of Or der class with pr omotions as functions\n    >>> joe = Customer('John Doe', 0)  \n  \n    >>> ann = Customer('Ann Smith', 1100)  \n    >>> cart = [LineItem('banana', 4, Decimal('.5')),  \n    ...         LineItem('apple', 10, Decimal('1.5')),  \n    ...         LineItem('watermelon', 5, Decimal(5))]  \n    >>> Order(joe, cart, fidelity_promo)  \n  \n    <Order total: 42.00 due: 42.00>  \n    >>> Order(ann, cart, fidelity_promo)  \n    <Order total: 42.00 due: 39.90>  \n    >>> banana_cart = [LineItem('banana', 30, Decimal('.5')),  \n    ...                LineItem('apple', 10, Decimal('1.5'))]  \n    >>> Order(joe, banana_cart, bulk_item_promo)  \n  \n    <Order total: 30.00 due: 28.50>  \n    >>> long_cart = [LineItem(str(item_code), 1, Decimal(1))  \n    ...               for item_code in range(10)]  \n    >>> Order(joe, long_cart, large_order_promo)  \n    <Order total: 10.00 due: 9.30>  \n    >>> Order(joe, cart, large_order_promo)  \n    <Order total: 42.00 due: 42.00>\nSame test fixtures as Example 10-1 .\nT o apply a discount strategy to an Order , just pass the promotion\nfunction as an ar gument.\nA dif ferent promotion function is used here and in the next test.\nNote the callouts in Example 10-4 : there is no need to instantiate a new\npromotion object with each new order: the functions are ready to use.\nIt is interesting to note that in Design Patterns  the authors suggest:\n“Strategy objects often make good flyweights.”  A definition of the\nFlyweight in another part of that work states: “A flyweight is a shared\nobject that can be used in multiple contexts simultaneously .”  The sharing\nis recommended to reduce the cost of creating a new concrete strategy\nobject when the same strategy is applied over and over again with every\nnew context—with every new Order  instance, in our example. So, to\novercome a drawback of the Strategy pattern—its runtime cost—the authors\nrecommend applying yet another pattern. Meanwhile, the line count and\nmaintenance cost of your code are piling up.\nA thornier use case, with complex concrete strategies holding internal state,\nmay require all the pieces of the Strategy and Flyweight design patterns\ncombined. But often concrete strategies have no internal state; they only\ndeal with data from the context. If that is the case, then by all means use\nplain old functions instead of coding single-method classes implementing a\nsingle-method interface declared in yet another class. A function is more\nlightweight than an instance of a user -defined class, and there is no need for\nFlyweight because each strategy function is created just once per Python4\n5",6477
134-Finding Strategies in a Module.pdf,134-Finding Strategies in a Module,"process when it loads the module. A plain function is also “a shared object\nthat can be used in multiple contexts simultaneously .”\nNow that we have implemented the Strategy pattern with functions, other\npossibilities emer ge. Suppose you want to create a “meta-strategy” that\nselects the best available discount for a given Order . In the following\nsections we study additional refactorings that implement this requirement\nusing a variety of approaches that leverage functions and modules as\nobjects.\nChoosing the Best Strategy: Simple Approach\nGiven the same customers and shopping carts from the tests in Example 10-\n4 , we now add three additional tests in Example 10-5 .\nExample 10-5. The best_pr omo function applies all discounts and r eturns\nthe lar gest\n    >>> Order(joe, long_cart , best_promo )  \n \n    <Order total: 10.00 due: 9.30> \n    >>> Order(joe, banana_cart , best_promo )  \n \n    <Order total: 30.00 due: 28.50> \n    >>> Order(ann, cart, best_promo )  \n \n    <Order total: 42.00 due: 39.90>\nbest_promo  selected the larger_order_promo  for customer\njoe .\nHere joe  got the discount from bulk_item_promo  for ordering lots\nof bananas.\nChecking out with a simple cart, best_promo  gave loyal customer\nann  the discount for the fidelity_promo .\nThe implementation of best_promo  is very simple. See Example 10-6 .\nExample 10-6. best_pr omo finds the maximum discount iterating over a list\nof functions\npromos = [fidelity_promo , bulk_item_promo , large_order_promo ]  \n \n \n \ndef best_promo (order: Order) -> Decimal:  \n \n    """"""Compute the best discount available""""""  \n    return max(promo(order) for promo in promos)  \npromos : list of the strategies implemented as functions.\nbest_promo  takes an instance of Order  as ar gument, as do the other\n*_promo  functions.\nUsing a generator expression, we apply each of the functions from\npromos  to the order , and return the maximum discount computed.\nExample 10-6  is straightforward: promos  is a list  of functions. Once\nyou get used to the idea that functions are first-class objects, it naturally\nfollows that building data structures holding functions often makes sense.\nAlthough Example 10-6  works and is easy to read, there is some duplication\nthat could lead to a subtle bug: to add a new promotion strategy , we need to\ncode the function and remember to add it to the promos  list, or else the\nnew promotion will work when explicitly passed as an ar gument to Order ,\nbut will not be considered by best_promotion .\nRead on for a couple of solutions to this issue.\nFinding Strategies in a Module\nModules in Python are also first-class objects, and the standard library\nprovides several functions to handle them. The built-in globals  is\ndescribed as follows in the Python docs:\nglobals()\nReturn a dictionary representing the current global symbol table. This is\nalways the dictionary of the current module (inside a function or\nmethod, this is the module where it is defined, not the module from\nwhich it is called).\nExample 10-7  is a somewhat hackish way of using globals  to help\nbest_promo  automatically find the other available *_promo  functions.\nExample 10-7. The promos  list is built by intr ospection of the module\nglobal namespace\nfrom decimal import Decimal \nfrom strategy  import Order \nfrom strategy  import ( \n    fidelity_promo , bulk_item_promo , large_order_promo   \n \n) \n \npromos = [promo for name, promo in globals().items()  \n \n                if name.endswith ('_promo') and        \n  \n                   name != 'best_promo '               \n  \n] \n \n \ndef best_promo (order: Order) -> Decimal:              \n  \n    """"""Compute the best discount available""""""  \n    return max(promo(order) for promo in promos)\nImport the promotion functions so they are available in the global\nnamespace.\nIterate over each item in the dict  returned by globals() .\nSelect only values where the name ends with the _promo  suf fix and…\nFilter out best_promo  itself, to avoid an infinite recursion when\nbest_promo  is called.\nNo changes in best_promo .\nAnother way of collecting the available promotions would be to create a\nmodule and put all the strategy functions there, except for best_promo .6\nIn Example 10-8 , the only significant change is that the list of strategy\nfunctions is built by introspection of a separate module called\npromotions . Note that Example 10-8  depends on importing the\npromotions  module as well as inspect , which provides high-level\nintrospection functions.\nExample 10-8. The pr omos list is built by intr ospection of a new pr omotions\nmodule\nfrom decimal import Decimal \nimport inspect \n \nfrom strategy  import Order \nimport promotions  \n \n \npromos = [func for _, func in inspect.getmembers (promotions , \ninspect.isfunction )] \n \n \ndef best_promo (order: Order) -> Decimal: \n    """"""Compute the best discount available""""""  \n    return max(promo(order) for promo in promos)\nThe function inspect.getmembers  returns the attributes of an object\n—in this case, the promotions  module—optionally filtered by a\npredicate (a boolean function). W e use inspect.isfunction  to get\nonly the functions from the module.\nExample 10-8  works regardless of the names given to the functions; all that\nmatters is that the promotions  module contains only functions that\ncalculate discounts given orders. Of course, this is an implicit assumption of\nthe code. If someone were to create a function with a dif ferent signature in\nthe promotions  module, then best_promo  would break while trying\nto apply it to an order .\nW e could add more stringent tests to filter the functions, by inspecting their\nar guments for instance. The point of Example 10-8  is not to of fer a\ncomplete solution, but to highlight one possible use of module\nintrospection.",5871
135-Decorator-Enhanced Strategy Pattern.pdf,135-Decorator-Enhanced Strategy Pattern,"A more explicit alternative to dynamically collecting promotional discount\nfunctions would be to use a simple decorator . That’ s next.\nD e c o r a t o r - E n h a n c e d  S t r a t e g y  P a t t e r n\nRecall that our main issue with Example 10-6  is the repetition of the\nfunction names in their definitions and then in the promos  list used by the\nbest_promo  function to determine the highest discount applicable. The\nrepetition is problematic because someone may add a new promotional\nstrategy function and for get to manually add it to the promos  list—in\nwhich case, best_promo  will silently ignore the new strategy ,\nintroducing a subtle bug in the system. Example 10-9  solves this problem\nwith the technique covered in “Registration decorators” .\nExample 10-9. The pr omos list is filled by the pr omotion decorator\nPromotion  = Callable [[Order], Decimal] \n \npromos: list[Promotion ] = []  \n \n \n \ndef promotion (promo: Promotion ) -> Promotion :  \n \n    promos.append(promo) \n    return promo \n \n \ndef best_promo (order: Order) -> Decimal: \n    """"""Compute the best discount available""""""  \n    return max(promo(order) for promo in promos)  \n \n \n \n@promotion   \n \ndef fidelity (order: Order) -> Decimal: \n    """"""5% discount for customers with 1000 or more fidelity  \npoints""""""  \n    if order.customer .fidelity  >= 1000: \n        return order.total() * Decimal('0.05') \n    return Decimal(0) \n \n \n@promotion  \ndef bulk_item (order: Order) -> Decimal: \n    """"""10% discount for each LineItem with 20 or more units""""""  \n    discount  = Decimal(0) \n    for item in order.cart: \n        if item.quantity  >= 20: \n            discount  += item.total() * Decimal('0.1') \n    return discount  \n \n \n@promotion  \ndef large_order (order: Order) -> Decimal: \n    """"""7% discount for orders with 10 or more distinct items""""""  \n    distinct_items  = {item.product for item in order.cart} \n    if len(distinct_items ) >= 10: \n        return order.total() * Decimal('0.07') \n    return Decimal(0)\nThe promos  list is a module global, and starts empty .\npromotion  is a registration decorator: it returns the promo  function\nunchanged, after appending it to the promos  list.\nNo changes needed to best_promo , because it relies on the promos\nlist.\nAny function decorated by @promotion  will be added to promos .\nThis solution has several advantages over the others presented before:\nThe promotion strategy functions don’ t have to use special names\n—no need for the _promo  suf fix.\nThe @promotion  decorator highlights the purpose of the\ndecorated function, and also makes it easy to temporarily disable a\npromotion: just comment out the decorator .\nPromotional discount strategies may be defined in other modules,\nanywhere in the system, as long as the @promotion  decorator is\napplied to them.\nIn the next section, we discuss Command—another design pattern that is\nsometimes implemented via single-method classes when plain functions",3002
136-The Command Pattern.pdf,136-The Command Pattern,"would do.\nT h e  C o m m a n d  P a t t e r n\nCommand is another design pattern that can be simplified by the use of\nfunctions passed as ar guments. Figure 10-2  shows the arrangement of\nclasses in the Command pattern.\nFigur e 10-2. UML class diagram for menu-driven text editor implemented with the Command design\npattern. Each command may have a differ ent r eceiver: the object that implements the action. For\nPasteCommand, the r eceiver is the Document. For OpenCommand, the r eceiver is the application.\nThe goal of Command is to decouple an object that invokes an operation\n(the Invoker) from the provider object that implements it (the Receiver). In\nthe example from Design Patterns , each invoker is a menu item in a\ngraphical application, and the receivers are the document being edited or\nthe application itself.\nThe idea is to put a Command  object between the two, implementing an\ninterface with a single method, execute , which calls some method in the\nReceiver to perform the desired operation. That way the Invoker does not\nneed to know the interface of the Receiver , and dif ferent receivers can be\nadapted through dif ferent Command  subclasses. The Invoker is configured\nwith a concrete command and calls its execute  method to operate it. Note\nin Figure 10-2  that MacroCommand  may store a sequence of commands;\nits execute()  method calls the same method in each command stored.\nQuoting from Gamma et al., “Commands are an object-oriented\nreplacement for callbacks.” The question is: do we need an object-oriented\nreplacement for callbacks? Sometimes yes, but not always.\nInstead of giving the Invoker a Command  instance, we can simply give it a\nfunction. Instead of calling command.execute() , the Invoker can just\ncall command() . The MacroCommand  can be implemented with a class\nimplementing __call__ . Instances of MacroCommand  would be\ncallables, each holding a list of functions for future invocation, as\nimplemented in Example 10-10 .\nExample 10-10. Each instance of Macr oCommand has an internal list of\ncommands\nclass MacroCommand : \n    """"""A command that executes a list of commands""""""  \n \n    def __init__ (self, commands ): \n        self.commands  = list(commands )  \n \n \n    def __call__ (self):",2272
137-Further Reading.pdf,137-Further Reading,"for command in self.commands :  \n \n            command()\nBuilding a list from the commands  ar guments ensures that it is iterable\nand keeps a local copy of the command references in each\nMacroCommand  instance.\nWhen an instance of MacroCommand  is invoked, each command in\nself.commands  is called in sequence.\nMore advanced uses of the Command pattern—to support undo, for\nexample—may require more than a simple callback function. Even then,\nPython provides a couple of alternatives that deserve consideration:\nA callable instance like MacroCommand  in Example 10-10  can\nkeep whatever state is necessary , and provide extra methods in\naddition to __call__ .\nA closure can be used to hold the internal state of a function\nbetween calls.\nThis concludes our rethinking of the Command pattern with first-class\nfunctions. At a high level, the approach here was similar to the one we\napplied to Strategy: replacing with callables the instances of a participant\nclass that implemented a single-method interface. After all, every Python\ncallable implements a single-method interface and that method is named\n__call__ .\nC h a p t e r  S u m m a r y\nAs Peter Norvig pointed out a couple of years after the classic Design\nPatterns  book appeared, “16 of 23 patterns have qualitatively simpler\nimplementation in Lisp or Dylan than in C++ for at least some uses of each\npattern” (slide 9 of Norvig’ s “Design Patterns in Dynamic Languages”\npresentation ). Python shares some of the dynamic features of the Lisp and\nDylan languages, in particular first-class functions, our focus in this part of\nthe book.\nFrom the same talk quoted at the start of this chapter , in reflecting on the\n20th anniversary of Design Patterns: Elements of Reusable Object-Oriented\nSoftwar e , Ralph Johnson has stated that one of the failings of the book is\n“T oo much emphasis on patterns as end-points instead of steps in the design\nprocess.”  In this chapter , we used the Strategy pattern as a starting point: a\nworking solution that we could simplify using first-class functions.\nIn many cases, functions or callable objects provide a more natural way of\nimplementing callbacks in Python than mimicking the Strategy or the\nCommand patterns as described by Gamma, Helm, Johnson & Vlissides.\nThe refactoring of Strategy and the discussion of Command in this chapter\nare examples of a more general insight: sometimes you may encounter a\ndesign pattern or an API that requires that components implement an\ninterface with a single method, and that method has a generic-sounding\nname such as “execute”, “run”, or “do_it”. Such patterns or APIs often can\nbe implemented with less boilerplate code in Python using functions as\nfirst-class objects.\nF u r t h e r  R e a d i n g\n“Recipe 8.21. Implementing the V isitor Pattern,” in the Python Cookbook,\nThir d Edition  (O’Reilly), by David Beazley and Brian K. Jones, presents an\nelegant implementation of the V isitor pattern in which a NodeVisitor\nclass handles methods as first-class objects.7\nOn the general topic of design patterns, the choice of readings for the\nPython programmer is not as broad as what is available to other language\ncommunities.\nLearning Python Design Patterns , by Gennadiy Zlobin (Packt), is the only\nbook that I have seen entirely devoted to patterns in Python. But Zlobin’ s\nwork is quite short (100 pages) and covers eight of the original 23 design\npatterns.\nExpert Python Pr ogramming  by T arek Ziadé (Packt) is one of the best\nintermediate-level Python books in the market, and its final chapter , “Useful\nDesign Patterns,” presents several of the classic patterns from a Pythonic\nperspective.\nAlex Martelli has given several talks about Python Design Patterns. There\nis a video of his EuroPython 201 1 presentation  and a set of slides on his\npersonal website . I’ve found dif ferent slide decks and videos over the years,\nof varying lengths, so it is worthwhile to do a thorough search for his name\nwith the words “Python Design Patterns.” A publisher told me Martelli is\nworking on a book about this subject. I will certainly get it when it comes\nout.\nThere are many books about design patterns in the context of Java, but\namong them the one I like most is Head First Design Patterns, Second\nEdition  by Eric Freeman & Elisabeth Robson (O’Reilly). It explains 16 of\nthe 23 classic patterns. If you like the wacky style of the Head First  series\nand need an introduction to this topic, you will love that work. It is Java-\ncentric, but the Second Edition  was uptaded to reflect the addition of first-\nclass functions in Java, making some of the examples closer to code we’d\nwrite in Python.\nFor a fresh look at patterns from the point of view of a dynamic language\nwith duck typing and first-class functions, Design Patterns in Ruby  by Russ\nOlsen (Addison-W esley) has many insights that are also applicable to\nPython. In spite of their many syntactic dif ferences, at the semantic level\nPython and Ruby are closer to each other than to Java or C++.\nIn Design Patterns in Dynamic Languages  (slides), Peter Norvig shows how\nfirst-class functions (and other dynamic features) make several of the\noriginal design patterns either simpler or unnecessary .\nThe Intr oduction  of the original Design Patterns  book by Gamma et al. is\nworth the price of the book—more than the catalog of 23 patterns which\nincludes recipes ranging from very important to rarely useful. The widely\nquoted design principles “Program to an interface, not an implementation”\nand “Favor object composition over class inheritance” both come from that\nIntr oduction .\nThe application of patterns to design originated with the architect\nChristopher Alexander , presented in the book A Pattern Language  ( Oxford\nUniversity Press, 1977). Alexander ’ s idea is to create a standard vocabulary\nallowing teams to share common design decisions while designing\nbuildings. M. J. Dominus wrote “Design Patterns” Ar en’ t : an intriguing\nslide deck and postscript text ar guing that Alexander ’ s original vision of\npatterns is more profound, more human, and also applicable to software\nengineering.\nS O A P B O X\nPython has first-class functions and first-class types, features that\nNorvig claims af fect 10 of the 23 patterns (slide 10 of Design Patterns\nin Dynamic Languages ). In the Chapter 9 , we saw that Python also has\ngeneric functions ( “Single Dispatch Generic Functions” ), a limited form\nof the CLOS multimethods that Gamma et al. suggest as a simpler way\nto implement the classic V isitor pattern. Norvig, on the other hand, says\nthat multimethods simplify the Builder pattern (slide 10). Matching\ndesign patterns to language features is not an exact science.\nIn classrooms around the world, design patterns are frequently taught\nusing Java examples. I’ve heard more than one student claim that they\nwere led to believe that the original design patterns are useful in any\nimplementation language. It turns out that the “classic” 23 patterns from\nthe Gamma et al. book apply to “classic” Java very well in spite of\nbeing originally presented mostly in the context of C++—a few have\nSmalltalk examples in the book. But that does not mean every one of\nthose patterns applies equally well in any language. The authors are\nexplicit right at the beginning of their book that “some of our patterns\nare supported directly by the less common object-oriented languages”\n(recall full quote on first page of this chapter).\nThe Python bibliography about design patterns is very thin, compared\nto that of Java, C++, or Ruby . In “Further Reading”  I mentioned\nLearning Python Design Patterns  by Gennadiy Zlobin, which was\npublished as recently as November 2013. In contrast, Russ Olsen’ s\nDesign Patterns in Ruby  was published in 2007 and has 384 pages—\n284 more than Zlobin’ s work.\nNow that Python is becoming increasingly popular in academia, let’ s\nhope more will be written about design patterns in the context of this\nlanguage. Also, Java 8 introduced method references and anonymous\nfunctions, and those highly anticipated features are likely to prompt\nfresh approaches to patterns in Java—recognizing that as languages\nevolve, so must our understanding of how to apply the classic design\npatterns.\nThe __call__  of the wild\nAs we collaborated to put the final touches to this book, tech reviewer\nLeonardo Rochael wondered…\nIf functions have a __call__  method, and methods are also callable,\ndo __call__  methods also have a __call__  method?\nI don’ t know if his discovery is useful, but it is a fun fact:\n>>> def turtle(): \n...     return 'eggs' \n... \n>>> turtle() \n'eggs' \n>>> turtle.__call__ () \n'eggs' \n>>> turtle.__call__ .__call__ () \n'eggs' \n>>> turtle.__call__ .__call__ .__call__ () \n'eggs' \n>>> turtle.__call__ .__call__ .__call__ .__call__ () \n'eggs' \n>>> turtle.__call__ .__call__ .__call__ .__call__ .__call__ () \n'eggs' \n>>> \nturtle.__call__ .__call__ .__call__ .__call__ .__call__ .__call__ () \n'eggs' \n>>> \nturtle.__call__ .__call__ .__call__ .__call__ .__call__ .__call__ ._\n_call__() \n'eggs'\nT urtles all the way down!\n1  From a slide in the talk “Root Cause Analysis of Some Faults in Design Patterns,” presented\nby Ralph Johnson at IME/CCSL, Universidade de São Paulo, Nov . 15, 2014.\n2  Quoted from page 4 of Design Patterns  (Addison-W esley , 1995).\n3  I had to reimplement Order  with @dataclass  due to a bug in Mypy . Y ou may ignore this\ndetail, because this class works with NamedTuple  as well, just like in Example 10-1 . If\nOrder  is a NamedTuple , Mypy 0.910 crashes when checking the type hint for\npromotion . I tried adding # type ignore  to that specific line, but Mypy crashed\nanyway . Mypy handles the same type hint correctly if Order  is built with @dataclass .\nIssue #9397  is unresolved as of July 19, 2021. Hopefully it will be fixed by the time you read\nthis.\n4  See page 323 of Design Patterns .\n5  idem , p. 196\n6  flake8  and VS Code both complain that these names are imported but not used. By definition,\nstatic analysis tools cannot understand the dynamic nature of Python. If we heed every advice\nfrom such tools, we’ll soon be writing grim and verbose Java-like code with Python syntax.\n7  “Root Cause Analysis of Some Faults in Design Patterns”, presented by Johnson at IME-USP ,\nNovember 15, 2014.",10447
138-Vector Class Redux.pdf,138-Vector Class Redux,"Part IV . Classes and Protocols\nChapter 11. A Pythonic Object\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 1 1th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nFor a library or framework to be Pythonic is to make it as easy and\nnatural as possible for a Python pr ogrammer to pick up how to perform a\ntask. .\n— Martijn Faassen, creator of Python and JavaScript\nframeworks\nThanks to the Python Data Model, your user -defined types can behave as\nnaturally as the built-in types. And this can be accomplished without\ninheritance, in the spirit of duck typing : you just implement the methods\nneeded for your objects to behave as expected.\nIn previous chapters, we studied the behavior of many built-in objects. W e\nwill now build user -defined classes that behave as real Python objects. Y our\napplication classes probably don’ t need and should not implement as many\nspecial methods as the examples in this chapter . But if you are writing a\nlibrary or a framework, the programmers who will use your classes may\nexpect them to behave like the classes that Python provides. Fulfilling that\nexpectation is one way of being “Pythonic.”1\nThis chapter starts where Chapter 1  ended, by showing how to implement\nseveral special methods that are commonly seen in Python objects of many\ndif ferent types.\nIn this chapter , we will see how to:\nSupport the built-in functions that convert objects to other types\n(e.g., repr() , bytes() , complex() , etc).\nImplement an alternative constructor as a class method.\nExtend the format mini-language used by f-strings, the format()\nbuilt-in, and the str.format()  method.\nProvide read-only access to attributes.\nMake an object hashable for use in sets and as dict  keys.\nSave memory with the use of __slots__ .\nW e’ll do all that as we develop a simple two-dimensional Euclidean vector\ntype, Vector2d . This code will be the foundation of an N-dimensional\nvector class in Chapter 12 .\nThe evolution of the example will be paused to discuss two conceptual\ntopics:\nHow and when to use the @classmethod  and\n@staticmethod  decorators.\nPrivate and protected attributes in Python: usage, conventions, and\nlimitations.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nI added a new epigraph and a few words in the second paragraph of the\nchapter to address the concept of “Pythonic”—which was only discussed at\nthe very end in the first edition.\n“Formatted Displays”  was updated to mention f-strings, introduced in\nPython 3.6. It’ s a small change because f-strings support the same\nformatting mini-language as the format()  built-in and the\nstr.format()  method, so any previously implemented __format__\nmethods simply work with f-strings.\nThe rest of the chapter barely changed—the special methods are mostly the\nsame since Python 3.0, and the core ideas appeared in Python 2.2.\nLet’ s get started with the object representation methods.\nO b j e c t  R e p r e s e n t a t i o n s\nEvery object-oriented language has at least one standard way of getting a\nstring representation from any object. Python has two:\nrepr()\nReturn a string representing the object as the developer wants to see it.\nIt’ s what you get when the Python console or a debugger shows an\nobject.\nstr()\nReturn a string representing the object as the user wants to see it. It’ s\nwhat you get when you print()  an object.\nThe special methods __repr__  and __str__  support repr()  and\nstr() , as we saw in Chapter 1 .\nThere are two additional special methods to support alternative\nrepresentations of objects: __bytes__  and __format__ . The\n__bytes__  method is analogous to __str__ : it’ s called by bytes()  to\nget the object represented as a byte sequence. Regarding __format__ , it\nis used by f-strings, by the built-in function format() , and by the\nstr.format()  method. They call\nobj.__format__(format_spec)  to get string displays of objects\nusing special formatting codes. W e’ll cover __bytes__  in the next\nexample, and __format__  after that.\nW A R N I N G\nIf you’re coming from Python 2, remember that in Python 3 __repr__ , __str__ ,\nand __format__  must always return Unicode strings (type str ). Only __bytes__\nis supposed to return a byte sequence (type bytes ).\nV e c t o r  C l a s s  R e d u x\nIn order to demonstrate the many methods used to generate object\nrepresentations, we’ll use a Vector2d  class similar to the one we saw in\nChapter 1 . W e will build on it in this and future sections. Example 1 1-1\nillustrates the basic behavior we expect from a Vector2d  instance.\nExample 1 1-1. V ector2d instances have several r epr esentations\n    >>> v1 = Vector2d (3, 4) \n    >>> print(v1.x, v1.y)  \n \n    3.0 4.0 \n    >>> x, y = v1  \n \n    >>> x, y \n    (3.0, 4.0) \n    >>> v1  \n \n    Vector2d (3.0, 4.0) \n    >>> v1_clone  = eval(repr(v1))  \n \n    >>> v1 == v1_clone   \n \n    True \n    >>> print(v1)  \n \n    (3.0, 4.0) \n    >>> octets = bytes(v1)  \n \n    >>> octets \n    \nb'd\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x\n00\\x10@' \n    >>> abs(v1)  \n \n    5.0 \n    >>> bool(v1), bool(Vector2d (0, 0))  \n \n    (True, False)\nThe components of a Vector2d  can be accessed directly as attributes\n(no getter method calls).\nA Vector2d  can be unpacked to a tuple of variables.\nThe repr  of a Vector2d  emulates the source code for constructing\nthe instance.\nUsing eval  here shows that the repr  of a Vector2d  is a faithful\nrepresentation of its constructor call.\nVector2d  supports comparison with == ; this is useful for testing.\nprint  calls str , which for Vector2d  produces an ordered pair\ndisplay .\nbytes  uses the __bytes__  method to produce a binary\nrepresentation.\nabs  uses the __abs__  method to return the magnitude of the\nVector2d .\nbool  uses the __bool__  method to return False  for a Vector2d\nof zero magnitude or True  otherwise.\nVector2d  from Example 1 1-1  is implemented in vector2d_v0.py\n( Example 1 1-2 ). The code is based on Example 1-2 , except for the methods\nfor the +  and *  operations, which we’ll see later in Chapter 16 . W e’ll add\nthe method for ==  since it’ s useful for testing. At this point, Vector2d\nuses several special methods to provide operations that a Pythonista expects\nin a well-designed object.\nExample 1 1-2. vector2d_v0.py: methods so far ar e all special methods\nfrom array import array \nimport math \n 2\n \nclass Vector2d : \n    typecode  = 'd'  \n \n \n    def __init__ (self, x, y): \n        self.x = float(x)    \n \n        self.y = float(y) \n \n    def __iter__ (self): \n        return (i for i in (self.x, self.y))  \n \n \n    def __repr__ (self): \n        class_name  = type(self).__name__  \n        return '{}({!r}, {!r}) '.format(class_name , *self)  \n \n \n    def __str__(self): \n        return str(tuple(self))  \n \n \n    def __bytes__ (self): \n        return (bytes([ord(self.typecode )]) +  \n \n                bytes(array(self.typecode , self)))  \n \n \n    def __eq__(self, other): \n        return tuple(self) == tuple(other)  \n \n \n    def __abs__(self): \n        return math.hypot(self.x, self.y)  \n \n \n    def __bool__ (self): \n        return bool(abs(self))  \ntypecode  is a class attribute we’ll use when converting Vector2d\ninstances to/from bytes .\nConverting x  and y  to float  in __init__  catches errors early ,\nwhich is helpful in case Vector2d  is called with unsuitable\nar guments.\n__iter__  makes a Vector2d  iterable; this is what makes unpacking\nwork (e.g, x, y = my_vector ). W e implement it simply by using a\ngenerator expression to yield the components one after the other .\n3\n__repr__  builds a string by interpolating the components with {!r}\nto get their repr ; because Vector2d  is iterable, *self  feeds the x\nand y  components to format .\nFrom an iterable Vector2d , it’ s easy to build a tuple  for display as\nan ordered pair .\nT o generate bytes , we convert the typecode to bytes  and\nconcatenate…\n…bytes  converted from an array  built by iterating over the instance.\nT o quickly compare all components, build tuples out of the operands.\nThis works for operands that are instances of Vector2d , but has\nissues. See the following warning.\nThe magnitude is the length of the hypotenuse of the right triangle\nformed by the x  and y  components.\n__bool__  uses abs(self)  to compute the magnitude, then converts\nit to bool , so 0.0  becomes False , nonzero is True .\nW A R N I N G\nMethod __eq__  in Example 1 1-2  works for Vector2d  operands but also returns\nTrue  when comparing Vector2d  instances to other iterables holding the same\nnumeric values (e.g., Vector(3, 4) == [3, 4] ). This may be considered a\nfeature or a bug. Further discussion needs to wait until Chapter 16 , when we cover\noperator overloading.\nW e have a fairly complete set of basic methods, but we still need a way to\nrebuild a Vector2d  from the binary representation produced by\nbytes() .",9501
139-Formatted Displays.pdf,139-Formatted Displays,"A n  A l t e r n a t i v e  C o n s t r u c t o r\nSince we can export a Vector2d  as bytes, naturally we need a method\nthat imports a Vector2d  from a binary sequence. Looking at the standard\nlibrary for inspiration, we find that array.array  has a class method\nnamed .frombytes  that suits our purpose—we saw it in “Arrays” . W e\nadopt its name and use its functionality in a class method for Vector2d  in\nvector2d_v1.py  ( Example 1 1-3 ).\nExample 1 1-3. Part of vector2d_v1.py: this snippet shows only the\nfr ombytes class method, added to the V ector2d definition in vector2d_v0.py\n( Example 1 1-2 )\n    @classmethod   \n \n    def frombytes (cls, octets):  \n \n        typecode  = chr(octets[0])  \n \n        memv = memoryview (octets[1:]).cast(typecode )  \n \n        return cls(*memv)  \nThe classmethod  decorator modifies a method so it can be called\ndirectly on a class.\nNo self  ar gument; instead, the class itself is passed as the first\nar gument—conventionally named cls .\nRead the typecode  from the first byte.\nCreate a memoryview  from the octets  binary sequence and use the\ntypecode  to cast it.\nUnpack the memoryview  resulting from the cast into the pair of\nar guments needed for the constructor .\nI just used a classmethod  decorator and it is very Python-specific, so\nlet’ s have a word about it.4\nclassmethod  V e r s u s  staticmethod\nThe classmethod  decorator is not mentioned in the Python tutorial, and\nneither is staticmethod . Anyone who has learned OO in Java may\nwonder why Python has both of these decorators and not just one of them.\nLet’ s start with classmethod . Example 1 1-3  shows its use: to define a\nmethod that operates on the class and not on instances. classmethod\nchanges the way the method is called, so it receives the class itself as the\nfirst ar gument, instead of an instance. Its most common use is for\nalternative constructors, like frombytes  in Example 1 1-3 . Note how the\nlast line of frombytes  actually uses the cls  ar gument by invoking it to\nbuild a new instance: cls(*memv) .\nIn contrast, the staticmethod  decorator changes a method so that it\nreceives no special first ar gument. In essence, a static method is just like a\nplain function that happens to live in a class body , instead of being defined\nat the module level. Example 1 1-4  contrasts the operation of\nclassmethod  and staticmethod .\nExample 1 1-4. Comparing behaviors of classmethod and staticmethod\n>>> class Demo: \n...     @classmethod  \n...     def klassmeth (*args): \n...         return args  \n \n...     @staticmethod  \n...     def statmeth (*args): \n...         return args  \n \n... \n>>> Demo.klassmeth ()  \n \n(<class '__main__.Demo'>,)  \n>>> Demo.klassmeth ('spam') \n(<class '__main__.Demo'>, 'spam')  \n>>> Demo.statmeth ()   \n \n() \n>>> Demo.statmeth ('spam') \n('spam',)\nklassmeth  just returns all positional ar guments.\nstatmeth  does the same.\nNo matter how you invoke it, Demo.klassmeth  receives the Demo\nclass as the first ar gument.\nDemo.statmeth  behaves just like a plain old function.\nN O T E\nThe classmethod  decorator is clearly useful, but I’ve never seen a compelling use\ncase for staticmethod . If you want to define a function that does not interact with\nthe class, just define it in the module. Maybe the function is closely related even if it\nnever touches the class, so you may want to place it nearby in the code. Even so,\ndefining the function right before or after the class in the same module is close enough\nfor all practical purposes.\nNow that we’ve seen what classmethod  is good for (and that\nstaticmethod  is not very useful), let’ s go back to the issue of object\nrepresentation and see how to support formatted output.\nF o r m a t t e d  D i s p l a y s\nThe f-strings, the format()  built-in function, and the str.format()\nmethod delegate the actual formatting to each type by calling their\n.__format__(format_spec)  method. The format_spec  is a\nformatting specifier , which is either:\nThe second ar gument in format(my_obj, format_spec) ,\nor\nWhatever appears after the colon in a replacement field delimited\nwith {}  inside an f-string or the fmt  in fmt.str.format()\nFor example:5\n>>> brl = 1 / 4.82  # BRL to USD currency conversion rate  \n>>> brl \n0.20746887966804978  \n>>> format(brl, '0.4f')  \n \n'0.2075'  \n>>> '1 BRL = {rate:0.2f} USD '.format(rate=brl)  \n \n'1 BRL = 0.21 USD'  \n>>> f'1 USD = {1 / brl:0.2f} BRL '  \n \n'1 USD = 4.82 BRL'\nFormatting specifier is '0.4f' .\nFormatting specifier is '0.2f' . The rate  part in the replacement\nfield is not part of the formatting specifier . It determines which keyword\nar gument of .format()  goes into that replacement field.\nAgain, the formatting specifier is '0.2f' . The 1 / brl  expression\nis not part of it.\nThe second and third callouts make an important point: a format string such\nas '{0.mass:5.3e}'  actually uses two separate notations. The\n'0.mass'  to the left of the colon is the field_name  part of the\nreplacement field syntax, and it can be an arbitrary expression in an f-string.\nThe '5.3e'  after the colon is the formatting specifier . The notation used\nin the formatting specifier is called the Format Specification Mini-\nLanguage .\nT I P\nIf f-strings, format()  and str.format()  are new to you, classroom experience\ntells me it’ s best to study the format()  built-in function first, which uses just the\nFormat Specification Mini-Language . After you get the gist of that, read Formatted\nstring literals  and Format String Syntax  to learn about the {:}  replacement field\nnotation, used in f-strings and str.format()  method (including the !s , !r , and !a\nconversion flags). F-strings don’ t make str.format()  obsolete: most of the time f-\nstrings solve the problem, but sometimes it’ s better to specify the formatting string\nelsewhere, and not where it will be rendered.\nA few built-in types have their own presentation codes in the Format\nSpecification Mini-Language. For example—among several other codes—\nthe int  type supports b  and x  for base 2 and base 16 output, respectively ,\nwhile float  implements f  for a fixed-point display and %  for a percentage\ndisplay:\n>>> format(42, 'b') \n'101010'  \n>>> format(2 / 3, '.1%') \n'66.7%'\nThe Format Specification Mini-Language is extensible because each class\ngets to interpret the format_spec  ar gument as it likes. For instance, the\nclasses in the datetime  module use the same format codes in the\nstrftime()  functions and in their __format__  methods. Here are a\ncouple examples using the format()  built-in and the str.format()\nmethod:\n>>> from datetime  import datetime  \n>>> now = datetime .now() \n>>> format(now, '%H:%M:%S' ) \n'18:49:05'  \n>>> ""It's now {:%I:%M %p}"" .format(now) \n""It's now 06:49 PM""\nIf a class has no __format__ , the method inherited from object  returns\nstr(my_object) . Because Vector2d  has a __str__ , this works:\n>>> v1 = Vector2d (3, 4) \n>>> format(v1) \n'(3.0, 4.0)'\nHowever , if you pass a format specifier , object.__format__  raises\nTypeError :\n>>> format(v1, '.3f') \nTraceback (most recent call last):  \n  ... \nTypeError : non-empty format string passed to object.__format__\nW e will fix that by implementing our own format mini-language. The first\nstep will be to assume the format specifier provided by the user is intended\nto format each float  component of the vector . This is the result we want:\n>>> v1 = Vector2d (3, 4) \n>>> format(v1) \n'(3.0, 4.0)'  \n>>> format(v1, '.2f') \n'(3.00, 4.00)'  \n>>> format(v1, '.3e') \n'(3.000e+00, 4.000e+00)'\nExample 1 1-5  implements __format__  to produce the displays just\nshown.\nExample 1 1-5. V ector2d. format  method, take #1\n    # inside the Vector2d class  \n \n    def __format__ (self, fmt_spec =''): \n        components  = (format(c, fmt_spec ) for c in self)  \n \n        return '({}, {})'.format(*components )  \nUse the format  built-in to apply the fmt_spec  to each vector\ncomponent, building an iterable of formatted strings.\nPlug the formatted strings in the formula '(x, y)' .\nNow let’ s add a custom formatting code to our mini-language: if the format\nspecifier ends with a 'p' , we’ll display the vector in polar coordinates:\n<r, θ> , where r  is the magnitude and θ (theta) is the angle in radians.\nThe rest of the format specifier (whatever comes before the 'p' ) will be\nused as before.\nT I P\nWhen choosing the letter for the custom format code I avoided overlapping with codes\nused by other types. In Format Specification Mini-Language  we see that integers use the\ncodes 'bcdoxXn' , floats use 'eEfFgGn%' , and strings use 's' . So I picked 'p'  for\npolar coordinates. Because each class interprets these codes independently , reusing a\ncode letter in a custom format for a new type is not an error , but may be confusing to\nusers.\nT o generate polar coordinates we already have the __abs__  method for\nthe magnitude, and we’ll code a simple angle  method using the\nmath.atan2()  function to get the angle. This is the code:\n    # inside the Vector2d class  \n \n    def angle(self): \n        return math.atan2(self.y, self.x)\nW ith that, we can enhance our __format__  to produce polar coordinates.\nSee Example 1 1-6 .\nExample 1 1-6. V ector2d. format  method, take #2, now with polar\ncoor dinates\n    def __format__ (self, fmt_spec =''): \n        if fmt_spec .endswith ('p'):  \n \n            fmt_spec  = fmt_spec [:-1]  \n \n            coords = (abs(self), self.angle())  \n \n            outer_fmt  = '<{}, {}> '  \n \n        else: \n            coords = self  \n \n            outer_fmt  = '({}, {}) '  \n \n        components  = (format(c, fmt_spec ) for c in coords)  \n \n        return outer_fmt .format(*components )  \nFormat ends with 'p' : use polar coordinates.\nRemove 'p'  suf fix from fmt_spec .\nBuild tuple  of polar coordinates: (magnitude, angle) .",9963
140-A Hashable Vector2d.pdf,140-A Hashable Vector2d,"Configure outer format with angle brackets.\nOtherwise, use x, y  components of self  for rectangular coordinates.\nConfigure outer format with parentheses.\nGenerate iterable with components as formatted strings.\nPlug formatted strings into outer format.\nW ith Example 1 1-6 , we get results similar to these:\n>>> format(Vector2d (1, 1), 'p') \n'<1.4142135623730951, 0.7853981633974483>'  \n>>> format(Vector2d (1, 1), '.3ep') \n'<1.414e+00, 7.854e-01>'  \n>>> format(Vector2d (1, 1), '0.5fp') \n'<1.41421, 0.78540>'\nAs this section shows, it’ s not hard to extend the format specification mini-\nlanguage to support user -defined types.\nNow let’ s move to a subject that’ s not just about appearances: we will make\nour Vector2d  hashable, so we can build sets of vectors, or use them as\ndict  keys.\nA  H a s h a b l e  V e c t o r 2 d\nAs defined, so far our Vector2d  instances are unhashable, so we can’ t put\nthem in a set :\n>>> v1 = Vector2d (3, 4) \n>>> hash(v1) \nTraceback (most recent call last):  \n  ... \nTypeError : unhashable type: 'Vector2d'  \n>>> set([v1]) \nTraceback (most recent call last):  \n  ... \nTypeError : unhashable type: 'Vector2d'\nT o make a Vector2d  hashable, we must implement __hash__\n(__eq__  is also required, and we already have it). W e also need to make\nvector instances immutable, as we’ve seen in “What is Hashable” .\nRight now , anyone can do v1.x = 7  and there is nothing in the code to\nsuggest that changing a Vector2d  is forbidden. This is the behavior we\nwant:\n>>> v1.x, v1.y \n(3.0, 4.0)  \n>>> v1.x = 7 \nTraceback (most recent call last):  \n  ... \nAttributeError : can't set attribute\nW e’ll do that by making the x  and y  components read-only properties in\nExample 1 1-7 .\nExample 1 1-7. vector2d_v3.py: only the changes needed to make V ector2d\nimmutable ar e shown her e; see full listing in Example 1 1-1 1\nclass Vector2d : \n    typecode  = 'd' \n \n    def __init__ (self, x, y): \n        self.__x = float(x)  \n \n        self.__y = float(y) \n \n    @property   \n \n    def x(self):  \n \n        return self.__x  \n \n \n    @property   \n \n    def y(self): \n        return self.__y \n \n    def __iter__ (self): \n        return (i for i in (self.x, self.y))  \n \n \n    # remaining methods: same as previous Vector2d\nUse exactly two leading underscores (with zero or one trailing\nunderscore) to make an attribute private.\nThe @property  decorator marks the getter method of a property .\nThe getter method is named after the public property it exposes: x .\nJust return self.__x .\nRepeat same formula for y  property .\nEvery method that just reads the x , y  components can stay as they were,\nreading the public properties via self.x  and self.y  instead of the\nprivate attribute, so this listing omits the rest of the code for the class.\nN O T E\nVector.x  and Vector.y  are examples of read-only properties. Read/write\nproperties will be covered in Chapter 23 , where we dive deeper into @property .\nNow that our vectors are reasonably safe from accidental mutation, we can\nimplement the __hash__  method. It should return an int  and ideally\ntake into account the hashes of the object attributes that are also used in the\n__eq__  method, because objects that compare equal should have the same\nhash. The __hash__  special method documentation  suggests using the\nbitwise XOR operator ( ^ ) to mix the hashes of the components, so that’ s\nwhat we do. The code for our Vector2d.__hash__  method is really\nsimple, as shown in Example 1 1-8 .\nExample 1 1-8. vector2d_v3.py: implementation of hash\n    # inside class Vector2d:  \n \n    def __hash__ (self): \n        return hash(self.x) ^ hash(self.y)6",3714
141-Supporting Positional Patterns.pdf,141-Supporting Positional Patterns,"W ith the addition of the __hash__  method, we now have hashable\nvectors:\n>>> v1 = Vector2d (3, 4) \n>>> v2 = Vector2d (3.1, 4.2) \n>>> hash(v1), hash(v2) \n(7, 384307168202284039)  \n>>> set([v1, v2]) \n{Vector2d(3.1, 4.2), Vector2d(3.0, 4.0)}\nT I P\nIt’ s not strictly necessary to implement properties or otherwise protect the instance\nattributes to create a hashable type. Implementing __hash__  and __eq__  correctly is\nall it takes. But the value of a hashable object is never supposed to change, so this\nprovided an excellent opportunity to talk about read-only properties.\nIf you are creating a type that has a sensible scalar numeric value, you may\nalso implement the __int__  and __float__  methods, invoked by the\nint()  and float()  constructors—which are used for type coercion in\nsome contexts. There’ s also a __complex__  method to support the\ncomplex()  built-in constructor . Perhaps Vector2d  should provide\n__complex__ , but I’ll leave that as an exercise for you.\nS u p p o r t i n g  P o s i t i o n a l  P a t t e r n s\nSo far , Vector2d  instances are compatible with keyword class patterns—\ncovered in “Keyword Class Patterns” .\nFor example, all of these keyword patterns work as expected:\nExample 1 1-9. Keywor d patterns for Vector2d  subjects—r equir es Python\n3.10.\ndef keyword_pattern_demo (v: Vector2d ) -> None: \n    match v: \n        case Vector2d (x=0, y=0): \n            print(f'{v!r} is null' ) \n        case Vector2d (x=0): \n            print(f'{v!r} is vertical' ) \n        case Vector2d (y=0): \n            print(f'{v!r} is horizontal' ) \n        case Vector2d (x=x, y=y) if x==y: \n            print(f'{v!r} is diagonal' ) \n        case _: \n            print(f'{v!r} is awesome' )\nHowever , if you try to use a positional pattern like this:\n        case Vector2d (_, 0): \n            print(f'{v!r} is horizontal' )\nY ou get:\nTypeError: Vector2d() accepts 0 positional sub-patterns (1 given)\nT o make Vector2d  work with positional patterns, we need to add a class\nattribute named __match_args__  , listing the instance attributes in the\norder they will be used for positional pattern matching:\nclass Vector2d : \n    __match_args__  = ('x', 'y') \n \n    # etc...\nNow we can save a few keystrokes when writing patterns to match\nVector2d  subjects:\nExample 1 1-10. Positional patterns for Vector2d  subjects—r equir es\nPython 3.10.\ndef positional_pattern_demo (v: Vector2d ) -> None: \n    match v: \n        case Vector2d (0, 0): \n            print(f'{v!r} is null' ) \n        case Vector2d (0): \n            print(f'{v!r} is vertical' ) \n        case Vector2d (_, 0): \n            print(f'{v!r} is horizontal' ) \n        case Vector2d (x, y) if x==y: \n            print(f'{v!r} is diagonal' )",2783
142-Complete Listing of Vector2d version 3.pdf,142-Complete Listing of Vector2d version 3,"case _: \n            print(f'{v!r} is awesome' )\nThe __match_args__  class attribute does not need to include all public\ninstance attributes. In particular , if the class __init__  has required and\noptional ar guments that are assigned to instance attributes, it may be\nreasonable to name the required ar guments in __match_args__ , but not\nthe optional ones.\nLet’ s step back and review what we’ve coded so far in Vector2d .\nC o m p l e t e  L i s t i n g  o f  Vector2d ,  v e r s i o n  3\nW e have been working on Vector2d  for a while, showing just snippets,\nso Example 1 1-1 1  is a consolidated, full listing of vector2d_v3.py , including\nthe doctests I used when developing it.\nExample 1 1-1 1. vector2d_v3.py: the full monty\n"""""" \nA two-dimensional vector class  \n \n    >>> v1 = Vector2d(3, 4)  \n    >>> print(v1.x, v1.y)  \n    3.0 4.0  \n    >>> x, y = v1  \n    >>> x, y  \n    (3.0, 4.0)  \n    >>> v1  \n    Vector2d(3.0, 4.0)  \n    >>> v1_clone = eval(repr(v1))  \n    >>> v1 == v1_clone  \n    True  \n    >>> print(v1)  \n    (3.0, 4.0)  \n    >>> octets = bytes(v1)  \n    >>> octets  \n    \nb'd\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x\n00\\x10@'  \n    >>> abs(v1)  \n    5.0 \n    >>> bool(v1), bool(Vector2d(0, 0))  \n    (True, False)  \n \n \nTest of ``.frombytes()`` class method:  \n \n    >>> v1_clone = Vector2d.frombytes(bytes(v1))  \n    >>> v1_clone  \n    Vector2d(3.0, 4.0)  \n    >>> v1 == v1_clone  \n    True  \n \n \nTests of ``format()`` with Cartesian coordinates:  \n \n    >>> format(v1)  \n    '(3.0, 4.0)'  \n    >>> format(v1, '.2f')  \n    '(3.00, 4.00)'  \n    >>> format(v1, '.3e')  \n    '(3.000e+00, 4.000e+00)'  \n \n \nTests of the ``angle`` method::  \n \n    >>> Vector2d(0, 0).angle()  \n    0.0 \n    >>> Vector2d(1, 0).angle()  \n    0.0 \n    >>> epsilon = 10**-8  \n    >>> abs(Vector2d(0, 1).angle() - math.pi/2) < epsilon  \n    True  \n    >>> abs(Vector2d(1, 1).angle() - math.pi/4) < epsilon  \n    True  \n \n \nTests of ``format()`` with polar coordinates:  \n \n    >>> format(Vector2d(1, 1), 'p')  # doctest:+ELLIPSIS  \n    '<1.414213..., 0.785398...>'  \n    >>> format(Vector2d(1, 1), '.3ep')  \n    '<1.414e+00, 7.854e-01>'  \n    >>> format(Vector2d(1, 1), '0.5fp')  \n    '<1.41421, 0.78540>'  \n \n \nTests of `x` and `y` read-only properties:  \n \n    >>> v1.x, v1.y  \n    (3.0, 4.0)  \n    >>> v1.x = 123  \n    Traceback (most recent call last):  \n      ...  \n    AttributeError: can't set attribute 'x'  \n \n \nTests of hashing:  \n \n    >>> v1 = Vector2d(3, 4)  \n    >>> v2 = Vector2d(3.1, 4.2)  \n    >>> hash(v1), hash(v2)  \n    (7, 384307168202284039)  \n    >>> len({v1, v2})  \n    2 \n \n"""""" \n \nfrom array import array \nimport math \n \nclass Vector2d : \n    __match_args__  = ('x', 'y') \n \n    typecode  = 'd' \n \n    def __init__ (self, x, y): \n        self.__x = float(x) \n        self.__y = float(y) \n \n    @property  \n    def x(self): \n        return self.__x \n \n    @property  \n    def y(self): \n        return self.__y \n \n    def __iter__ (self): \n        return (i for i in (self.x, self.y)) \n \n    def __repr__ (self): \n        class_name  = type(self).__name__  \n        return '{}({!r}, {!r})'.format(class_name , *self) \n \n    def __str__(self): \n        return str(tuple(self)) \n \n    def __bytes__ (self): \n        return (bytes([ord(self.typecode )]) + \n                bytes(array(self.typecode , self))) \n \n    def __eq__(self, other): \n        return tuple(self) == tuple(other) \n \n    def __hash__ (self): \n        return hash(self.x) ^ hash(self.y) \n \n    def __abs__(self): \n        return math.hypot(self.x, self.y) \n \n    def __bool__ (self): \n        return bool(abs(self)) \n \n    def angle(self): \n        return math.atan2(self.y, self.x) \n \n    def __format__ (self, fmt_spec =''): \n        if fmt_spec .endswith ('p'): \n            fmt_spec  = fmt_spec [:-1] \n            coords = (abs(self), self.angle()) \n            outer_fmt  = '<{}, {}>' \n        else: \n            coords = self \n            outer_fmt  = '({}, {})' \n        components  = (format(c, fmt_spec ) for c in coords) \n        return outer_fmt .format(*components ) \n \n    @classmethod  \n    def frombytes (cls, octets): \n        typecode  = chr(octets[0]) \n        memv = memoryview (octets[1:]).cast(typecode ) \n        return cls(*memv)\nT o recap, in this and the previous sections, we saw some essential special\nmethods that you may want to implement to have a full-fledged object.",4578
143-Private and Protected Attributes in Python.pdf,143-Private and Protected Attributes in Python,"N O T E\nY ou should only implement these special methods if your application needs them. End\nusers don’ t care if the objects that make up the application are “Pythonic” or not.\nOn the other hand, if your classes are part of a library for other Python programmers to\nuse, you can’ t really guess what they will do with your objects, and they may expect\nmore of the “Pythonic” behaviors we are describing.\nAs coded in Example 1 1-1 1 , Vector2d  is a didactic example with a\nlaundry list of special methods related to object representation, not a\ntemplate for every user -defined class.\nIn the next section, we’ll take a break from Vector2d  to discuss the\ndesign and drawbacks of the private attribute mechanism in Python—the\ndouble-underscore prefix in self.__x .\nP r i v a t e  a n d  “ P r o t e c t e d ”  A t t r i b u t e s  i n  P y t h o n\nIn Python, there is no way to create private variables like there is with the\nprivate  modifier in Java. What we have in Python is a simple\nmechanism to prevent accidental overwriting of a “private” attribute in a\nsubclass.\nConsider this scenario: someone wrote a class named Dog  that uses a mood\ninstance attribute internally , without exposing it. Y ou need to subclass Dog\nas Beagle . If you create your own mood  instance attribute without being\naware of the name clash, you will clobber the mood  attribute used by the\nmethods inherited from Dog . This would be a pain to debug.\nT o prevent this, if you name an instance attribute in the form __mood  (two\nleading underscores and zero or at most one trailing underscore), Python\nstores the name in the instance __dict__  prefixed with a leading\nunderscore and the class name, so in the Dog  class, __mood  becomes\n_Dog__mood , and in Beagle  it’ s _Beagle__mood . This language\nfeature goes by the lovely name of name mangling .\nExample 1 1-12  shows the result in the Vector2d  class from Example 1 1-\n7 .\nExample 1 1-12. Private attribute names ar e “mangled” by pr efixing the _\nand the class name\n>>> v1 = Vector2d (3, 4) \n>>> v1.__dict__  \n{'_Vector2d__y': 4.0, '_Vector2d__x': 3.0}  \n>>> v1._Vector2d__x  \n3.0\nName mangling is about safety , not security: it’ s designed to prevent\naccidental access and not malicious prying. Figure 1 1-1  illustrates another\nsafety device.\nFigur e 1 1-1. A cover on a switch is a safety  device, not a security  one: it pr events accidents, not\nsabotage.\nAnyone who knows how private names are mangled can read the private\nattribute directly , as the last line of Example 1 1-12  shows—that’ s actually\nuseful for debugging and serialization. They can also directly assign a value\nto a private component of a Vector2d  by writing v1._Vector2d__x\n= 7 . But if you are doing that in production code, you can’ t complain if\nsomething blows up.\nThe name mangling functionality is not loved by all Pythonistas, and\nneither is the skewed look of names written as self.__x . Some prefer to\navoid this syntax and use just one underscore prefix to “protect” attributes\nby convention (e.g., self._x ). Critics of the automatic double-underscore\nmangling suggest that concerns about accidental attribute clobbering should\nbe addressed by naming conventions. Ian Bicking—creator of pip,\nvirtualenv , and other projects—wrote:\nNever , ever use two leading underscor es. This is annoyingly private. If\nname clashes ar e a concern, use explicit name mangling instead (e.g.,\n_MyThing_blahblah ). This is essentially the same thing as double-\nunderscor e, only it’ s transpar ent wher e double underscor e obscur es.\nThe single underscore prefix has no special meaning to the Python\ninterpreter when used in attribute names, but it’ s a very strong convention\namong Python programmers that you should not access such attributes from\noutside the class.  It’ s easy to respect the privacy of an object that marks its\nattributes with a single _ , just as it’ s easy respect the convention that\nvariables in ALL_CAPS  should be treated as constants.\nAttributes with a single _  prefix are called “protected” in some corners of\nthe Python documentation.  The practice of “protecting” attributes by\nconvention with the form self._x  is widespread, but calling that a\n“protected” attribute is not so common. Some even call that a “private”\nattribute.\nT o conclude: the Vector2d  components are “private” and our Vector2d\ninstances are “immutable”—with scare quotes—because there is no way to\nmake them really private and immutable.\nW e’ll now come back to our Vector2d  class. In the next section, we\ncover a special attribute (not a method) that af fects the internal storage of an\nobject, with potentially huge impact on the use of memory but little ef fect\non its public interface: __slots__ .7\n8\n9\n10",4804
144-Saving Memory with __slots__.pdf,144-Saving Memory with __slots__,"S a v i n g  M e m o r y  w i t h  __slots__\nBy default, Python stores the attributes of each instance in a dict  named\n__dict__ . As we saw in “Practical Consequences of How dict W orks” , a\ndict  has a signficant memory overhead—even with the optimizations\nmentioned in that section. But if you define a class attribute named\n__slots__  holding sequence of attribute names, Python uses an\nalternative storage model for the instance attributes: the attributes named in\n__slots__  are stored in a hidden array or references that uses less\nmemory than a dict . Let’ s see how that works through simple examples.\nExample 1 1-13. The Pixel  class uses ` slots .\n>>> class Pixel: \n...     __slots__  = ('x', 'y')  \n \n... \n>>> p = Pixel()  \n \n>>> p.__dict__   \n \nTraceback (most recent call last):  \n  ... \nAttributeError : 'Pixel' object has no attribute '__dict__'  \n>>> p.x = 10  \n \n>>> p.y = 20 \n>>> p.color = 'red'  \n \nTraceback (most recent call last):  \n  ... \nAttributeError : 'Pixel' object has no attribute 'color'\n__slots__  must be present when the class is created; adding or\nchanging it later has no ef fect. The attribute names may be in a tuple\nor list , but I prefer a tuple  to make it clear there’ s no point in\nchanging it.\nCreate an instance of Pixel , because we see the ef fects of\n__slots__  on the instances.\nFirst ef fect: instances of Pixel  have no __dict__ .\nSet the p.x  and p.y  attributes normally .\nSecond ef fect: trying to set an attribute not listed in __slots__  raises\nAttributeError .\nSo far , so good. Now let’ s create a subclass of Pixel  to see the\ncounterintuitive side of __slots__ :\nExample 1 1-14. The OpenPixel  is a subclass of Pixel .\n>>> class OpenPixel (Pixel):  \n \n...     pass \n... \n>>> op = OpenPixel () \n>>> op.__dict__   \n \n{} \n>>> op.x = 8  \n \n>>> op.__dict__   \n \n{} \n>>> op.x  \n \n8 \n>>> op.color = 'green'  \n \n>>> op.__dict__   \n \n{'color': 'green'}\nOpenPixel  declares no attributes of its own.\nSurprise: instances of OpenPixel  have a __dict__ .\nIf you set attribute x  (named in the __slots__  of the base class\nPixel )…\n…it is not stored in the instance __dict__ …\n…but it is stored in the hidden array of references in the instance.\nIf you set an attribute not named in the __slots__ …\n…it is stored in the instance __dict__ .\nExample 1 1-14  shows that the ef fect of __slots__  is only partially\ninherited by a subclass. T o make sure that instances of a subclass have no\n__dict__ , you must declare __slots__  again in the subclass.\nIf you declare __slots__ = ()  (an empty tuple), then the instances of\nthe subclass will have no __dict__  and will only accept the attributes\nnamed in the __slots__  of the base class.\nIf you want a subclass to have additional attributes, name them in\n__slots__ :\nExample 1 1-15. The ColorPixel , another subclass of Pixel .\n>>> class ColorPixel (Pixel): \n...    __slots__  = ('color',)  \n \n>>> cp = ColorPixel () \n>>> cp.__dict__   \n \nTraceback (most recent call last):  \n  ... \nAttributeError : 'ColorPixel' object has no attribute '__dict__'  \n>>> cp.x = 2 \n>>> cp.color = 'blue'  \n \n>>> cp.flavor = 'banana' \nTraceback (most recent call last):  \n  ... \nAttributeError : 'ColorPixel' object has no attribute 'flavor'\nEssentially , __slots__  of the superclasses are added to the\n__slots__  of the current class. Don’ t for get that single item tuples\nmust have a trailing comma.\nColorPixel  instances have no __dict__ .\nY ou can set the attributes declared in the __slots__  of this class and\nsuperclasses, but no other .\nIt’ s possible to “save memory and eat it too”: if you add the '__dict__'\nname to the __slots__  list, your instances will keep attributes named in\n__slots__  in the per -instance array of references, but will also support\ndynamically created attributes, which will be stored in the usual",3915
145-Simple Measure of __slot__ Savings.pdf,145-Simple Measure of __slot__ Savings,"__dict__ . This is necessary if you want to use the\n@cached_property  decorator , (covered in “Step 5: Caching Properties\nwith functools” ).\nOf course, having '__dict__'  in __slots__  may entirely defeat its\npurpose, depending on the number of static and dynamic attributes in each\ninstance and how they are used. Careless optimization is worse than\npremature optimization: you add complexity but may not get any benefit.\nAnother special per -instance attribute that you may want to keep is\n__weakref__ , necessary for an object to support weak references\n(mentioned briefly in “del and Garbage Collection” ). That attribute exists\nby default in instances of user -defined classes. However , if the class defines\n__slots__ , and you need the instances to be tar gets of weak references,\nthen you need to include '__weakref__'  among the attributes named in\n__slots__ .\nNow let’ s see the ef fect of adding __slots__  to Vector2d .\nSimple Measure of __slot__  Savings\nExample 1 1-16  shows the implementation of __slots__ in\n`Vector2d .\nExample 1 1-16. vector2d_v3_slots.py: the slots  attribute is the only addition\nto V ector2d\nclass Vector2d : \n    __match_args__  = ('x', 'y')  \n \n    __slots__  = ('__x', '__y')  \n \n \n    typecode  = 'd' \n    # methods are the same as previous version\n__match_args__  lists the public attribute names for positional\npattern matching.\nIn contrast, __slots__  lists the names of the instance attributes,\nwhich in this case are private attributes.\nT o measure the memory savings, I wrote the mem_test.py  script. It takes the\nname of a module with a Vector2d  class variant as command-line\nar gument, and uses a list comprehension to build a list  with 10,000,000\ninstances of Vector2d . In the first run shown in Example 1 1-17 , I use\nvector2d_v3.Vector2d  (from Example 1 1-7 ); in the second run, I\nused the version with __slots__  from Example 1 1-16 .\nExample 1 1-17. mem_test.py cr eates 10 million Vector2d  instances using\nthe class defined in the named module.\n$ time python3 mem_test.py vector2d_v3  \nSelected Vector2d type: vector2d_v3.Vector2d  \nCreating 10,000,000 Vector2d instances  \nInitial RAM usage:      6,983,680  \n  Final RAM usage:  1,666,535,424  \n \nreal 0m11.990s  \nuser 0m10.861s  \nsys 0m0.978s  \n(.py310b4 ) TW-LR-MBP:11-pythonic-obj luciano $ time python3 \nmem_test.py vector2d_v3_slots  \nSelected Vector2d type: vector2d_v3_slots.Vector2d  \nCreating 10,000,000 Vector2d instances  \nInitial RAM usage:      6,995,968  \n  Final RAM usage:    577,839,104  \n \nreal 0m8.381s  \nuser 0m8.006s  \nsys 0m0.352s\nAs Example 1 1-17  reveals, the RAM footprint of the script grows to 1.55\nGiB when instance __dict__  is used in each of the 10 million\nVector2d  instances, but that is reduced to 551 MiB when Vector2d  has\na __slots__  attribute. The __slots__  version is also faster . The\nmem_test.py  script in this test basically deals with loading a module,\nchecking memory usage, and formatting results. Y ou can find its source\ncode in the fluentpython/example-code-2e  repository .",3104
146-Summarizing The Issues with __slots__.pdf,146-Summarizing The Issues with __slots__,,0
147-Overriding Class Attributes.pdf,147-Overriding Class Attributes,"T I P\nIf you are handling millions of objects with numeric data, you should really be using\nNumPy arrays (see “NumPy” ), which are not only memory-ef ficient but have highly\noptimized functions for numeric processing, many of which operate on the entire array\nat once. I designed the Vector2d  class just to provide context when discussing special\nmethods, because I try to avoid vague foo  and bar  examples when I can.\nSummarizing The Issues with __slots__\nThe __slots__  class attribute may provide significant memory savings if\nproperly used, but there are a few caveats:\nY ou must remember to redeclare __slots__  in each subclass to\nprevent their instances to have __dict__ .\nInstances will only be able to have the attributes listed in\n__slots__ , unless you include '__dict__'  in __slots__\n(but doing so may negate the memory savings).\nClasses using __slots__  cannot use the @cached_property\ndecorator , unless they explicitly name '__dict__'  in\n__slots__ .\nInstances cannot be tar gets of weak references unless you add\n'__weakref__'  in __slots__ .\nThe last topic in this chapter has to do with overriding a class attribute in\ninstances and subclasses.\nO v e r r i d i n g  C l a s s  A t t r i b u t e s\nA distinctive feature of Python is how class attributes can be used as default\nvalues for instance attributes. In Vector2d  there is the typecode  class\nattribute. It’ s used twice in the __bytes__  method, but we read it as\nself.typecode  by design. Because Vector2d  instances are created\nwithout a typecode  attribute of their own, self.typecode  will get\nthe Vector2d.typecode  class attribute by default.\nBut if you write to an instance attribute that does not exist, you create a new\ninstance attribute—e.g., a typecode  instance attribute—and the class\nattribute by the same name is untouched. However , from then on, whenever\nthe code handling that instance reads self.typecode , the instance\ntypecode  will be retrieved, ef fectively shadowing the class attribute by\nthe same name. This opens the possibility of customizing an individual\ninstance with a dif ferent typecode .\nThe default Vector2d.typecode  is 'd' , meaning each vector\ncomponent will be represented as an 8-byte double precision float when\nexporting to bytes . If we set the typecode  of a Vector2d  instance to\n'f'  prior to exporting, each component will be exported as a 4-byte single\nprecision float. Example 1 1-18  demonstrates.\nN O T E\nW e are discussing adding a custom instance attribute, therefore Example 1 1-18  uses the\nVector2d  implementation without __slots__  as listed in Example 1 1-1 1 .\nExample 1 1-18. Customizing an instance by setting the typecode attribute\nthat was formerly inherited fr om the class\n>>> from vector2d_v3  import Vector2d  \n>>> v1 = Vector2d (1.1, 2.2) \n>>> dumpd = bytes(v1) \n>>> dumpd \nb'd\x9a\x99\x99\x99\x99\x99\xf1?\x9a\x99\x99\x99\x99\x99\x01@'  \n>>> len(dumpd)  \n \n17 \n>>> v1.typecode  = 'f'  \n \n>>> dumpf = bytes(v1) \n>>> dumpf \nb'f\xcd\xcc\x8c?\xcd\xcc\x0c@'  \n>>> len(dumpf)  \n \n9 \n>>> Vector2d .typecode   \n \n'd'\nDefault bytes  representation is 17 bytes long.\nSet typecode  to 'f'  in the v1  instance.\nNow the bytes  dump is 9 bytes long.\nVector2d.typecode  is unchanged; only the v1  instance uses\ntypecode  'f' .\nNow it should be clear why the bytes  export of a Vector2d  is prefixed\nby the typecode : we wanted to support dif ferent export formats.\nIf you want to change a class attribute you must set it on the class directly ,\nnot through an instance. Y ou could change the default typecode  for all\ninstances (that don’ t have their own typecode ) by doing this:\n>>> Vector2d .typecode  = 'f'\nHowever , there is an idiomatic Python way of achieving a more permanent\nef fect, and being more explicit about the change. Because class attributes\nare public, they are inherited by subclasses, so it’ s common practice to\nsubclass just to customize a class data attribute. The Django class-based\nviews use this technique extensively . Example 1 1-19  shows how .\nExample 1 1-19. The ShortV ector2d is a subclass of V ector2d, which only\noverwrites the default typecode\n>>> from vector2d_v3  import Vector2d  \n>>> class ShortVector2d (Vector2d ):  \n \n...     typecode  = 'f' \n... \n>>> sv = ShortVector2d (1/11, 1/27)  \n \n>>> sv \nShortVector2d(0.09090909090909091, 0.037037037037037035)  \n  \n>>> len(bytes(sv))  \n \n9",4458
148-Chapter Summary.pdf,148-Chapter Summary,"Create ShortVector2d  as a Vector2d  subclass just to overwrite\nthe typecode  class attribute.\nBuild ShortVector2d  instance sv  for demonstration.\nInspect the repr  of sv .\nCheck that the length of the exported bytes is 9, not 17 as before.\nThis example also explains why I did not hardcode the class_name  in\nVector2d.__repr__ , but instead got it from\ntype(self).__name__ , like this:\n    # inside class Vector2d:  \n \n    def __repr__ (self): \n        class_name  = type(self).__name__  \n        return '{}({!r}, {!r})'.format(class_name , *self)\nIf I had hardcoded the class_name , subclasses of Vector2d  like\nShortVector2d  would have to overwrite __repr__  just to change the\nclass_name . By reading the name from the type  of the instance, I made\n__repr__  safer to inherit.\nThis ends our coverage of building a simple class that leverages the data\nmodel to play well with the rest of Python—of fering dif ferent object\nrepresentations, providing a custom formatting code, exposing read-only\nattributes, and supporting hash()  to integrate with sets and mappings.\nC h a p t e r  S u m m a r y\nThe aim of this chapter was to demonstrate the use of special methods and\nconventions in the construction of a well-behaved Pythonic class.\nIs vector2d_v3.py  ( Example 1 1-1 1 ) more Pythonic than vector2d_v0.py\n( Example 1 1-2 )? The Vector2d  class in vector2d_v3.py  certainly exhibits\nmore Python features. But whether the first or the last Vector2d\nimplementation is suitable depends on the context where it would be used.\nT im Peter ’ s Zen of Python says:\nSimple is better than complex.\nAn object should be as simple as the requirements dictate—and not a\nparade of language features. If the code is for an application, then it should\nfocus on what is needed to support the end users, not more. If the code is\nfor a library for other programmers to use, then it’ s reasonable to implement\nspecial methods supporting behaviors that Pythonistas expect. For example,\n__eq__  may not be necessary to support a business requirement, but it\nmakes it makes the class easier to test.\nMy goal in expanding the Vector2d  code was to provide context for\ndiscussing Python special methods and coding conventions. The examples\nin this chapter have demonstrated several of the special methods we first\nsaw in T able 1-1  ( Chapter 1 ):\nString/bytes representation methods: __repr__ , __str__ ,\n__format__ , and __bytes__ .\nMethods for reducing an object to a number: __abs__ ,\n__bool__ , __hash__ .\nThe __eq__  operator , to support testing and hashing (along with\n__hash__ ).\nWhile supporting conversion to bytes  we also implemented an alternative\nconstructor , Vector2d.frombytes() , which provided the context for\ndiscussing the decorators @classmethod  (very handy) and",2817
149-Further Reading.pdf,149-Further Reading,"@staticmethod  (not so useful, module-level functions are simpler). The\nfrombytes  method was inspired by its namesake in the array.array\nclass.\nW e saw that the Format Specification Mini-Language  is extensible by\nimplementing a __format__  method that parses a format_spec\nprovided to the format(obj, format_spec)  built-in or within\nreplacement fields '{:«format_spec»}'  in f-strings or strings used\nwith the str.format()  method.\nIn preparation to make Vector2d  instances hashable, we made an ef fort\nto make them immutable, at least preventing accidental changes by coding\nthe x  and y  attributes as private, and exposing them as read-only properties.\nW e then implemented __hash__  using the recommended technique of\nxor -ing the hashes of the instance attributes.\nW e then discussed the memory savings and the caveats of declaring a\n__slots__  attribute in Vector2d . Because using __slots__  has side\nef fects, it really makes sense only when handling a very lar ge number of\ninstances—think millions of instances, not just thousands. In many such\ncases, using pandas  may be the best option.\nThe last topic we covered was the overriding of a class attribute accessed\nvia the instances (e.g., self.typecode ). W e did that first by creating an\ninstance attribute, and then by subclassing and overwriting at the class\nlevel.\nThroughout the chapter , I mentioned how design choices in the examples\nwere informed by studying the API of standard Python objects. If this\nchapter can be summarized in one sentence, this is it:\nT o build Pythonic objects, observe how r eal Python objects behave.\n— Ancient Chinese proverb\nF u r t h e r  R e a d i n g\nThis chapter covered several special methods of the data model, so\nnaturally the primary references are the same as the ones provided in\nChapter 1 , which gave a high-level view of the same topic. For\nconvenience, I’ll repeat those four earlier recommendations here, and add a\nfew other ones:\n“Data Model” chapter  of The Python Language Refer ence\nMost of the methods we used in this chapter are documented in “3.3.1.\nBasic customization” .\nPython in a Nutshell, 3r d Edition  by Alex Martelli, Anna Ravenscroft, and\nSteve Holden covers the special methods in depth.\nPython Cookbook, 3r d Edition , by David Beazley and Brian K. Jones\nModern Python practices demonstrated through recipes. Chapter 8,\n“Classes and Objects” in particular has several solutions related to\ndiscussions in this chapter .\nPython Essential Refer ence, 4th Edition , by David Beazley\nCovers the data model in detail. Even if only Python 2.6 and 3.0 is\ncovered (in the fourth edition). The fundamental concepts are all the\nsame and most of the Data Model APIs haven’ t changed at all since\nPython 2.2, when built-in types and user -defined classes were unified.\nIn 2015—the year when I finished Fluent Python, First Edition —Hynek\nSchlawack started the attrs  package. From the attrs  documentation:\nattrs  is the Python package that will bring back the joy  of writing\nclasses  by r elieving you fr om the drudgery of implementing object\npr otocols (aka dunder methods).\nI mentioned attrs  as a more powerful alternative to @dataclass  in\n“Further Reading” . The data class builders from Chapter 5  as well as\nattrs  automatically equip your classes with several special methods. But\nknowing how to code those special methods yourself is still essential to\nunderstand what those packages do, to decide whether you really need\nthem, and to override the methods they generate—when necessary .\nIn this chapter , we saw every special method related to object\nrepresentation, except __index__  and __fspath__ . W e’ll discuss\n__index__  in Chapter 12 , “A Slice-A ware __getitem__” . I will not cover\n__fspath__ . T o learn about it, see PEP 519—Adding a file system path\nprotocol .\nAn early realization of the need for distinct string representations for\nobjects appeared in Smalltalk. The 1996 article “How to Display an Object\nas a String: printString and displayString”  by Bobby W oolf discusses the\nimplementation of the printString  and displayString  methods in\nthat language. From that article, I borrowed the pithy descriptions “the way\nthe developer wants to see it” and “the way the user wants to see it” when\ndefining repr()  and str()  in “Object Representations” .\nS O A P B O X\nPr operties Help Reduce Upfr ont Costs\nIn the initial versions of Vector2d , the x  and y  attributes were public,\nas are all Python instance and class attributes by default. Naturally ,\nusers of vectors need to access its components. Although our vectors\nare iterable and can be unpacked into a pair of variables, it’ s also\ndesirable to write my_vector.x  and my_vector.y  to get each\ncomponent.\nWhen we felt the need to avoid accidental updates to the x  and y\nattributes, we implemented properties, but nothing changed elsewhere\nin the code and in the public interface of Vector2d , as verified by the\ndoctests. W e are still able to access my_vector.x  and\nmy_vector.y .\nThis shows that we can always start our classes in the simplest possible\nway , with public attributes, because when (or if) we later need to\nimpose more control with getters and setters, these can be implemented\nthrough properties without changing any of the code that already\ninteracts with our objects through the names (e.g., x  and y ) that were\ninitially simple public attributes.\nThis approach is the opposite of that encouraged by the Java language:\na Java programmer cannot start with simple public attributes and only\nlater , if needed, implement properties, because they don’ t exist in the\nlanguage. Therefore, writing getters and setters is the norm in Java—\neven when those methods do nothing useful—because the API cannot\nevolve from simple public attributes to getters and setters without\nbreaking all code that uses those attributes.\nIn addition, as Martelli, Ravenscroft & Holden point out in Python in a\nNutshell, 3r d Edition , typing getter/setter calls everywhere is goofy .\nY ou have to write stuf f like:\n>>> my_object .set_foo(my_object .get_foo() + 1)\nJust to do this:\n>>> my_object .foo += 1\nW ard Cunningham, inventor of the wiki and an Extreme Programming\npioneer , recommends asking “What’ s the simplest thing that could\npossibly work?” The idea is to focus on the goal.  Implementing\nsetters and getters up front is a distraction from the goal. In Python, we\ncan simply use public attributes knowing we can change them to\nproperties later , if the need arises.\nSafety V ersus Security in Private Attributes\nPerl doesn’ t have an infatuation with enfor ced privacy . It would\npr efer that you stayed out of its living r oom because you wer en’ t\ninvited, not because it has a shotgun.\n— Larry W all, Creator of Perl\nPython and Perl are polar opposites in many regards, but Guido and\nLarry seem to agree on object privacy .\nHaving taught Python to many Java programmers over the years, I’ve\nfound a lot of them put too much faith in the privacy guarantees that\nJava of fers. As it turns out, the Java private  and protected\nmodifiers normally provide protection against accidents only (i.e.,\nsafety). They only of fer security against malicious intent if the\napplication is specially configured and deployed on top of a Java\nSecurityManager , and that seldom happens in practice, even in security\nconscious corporate settings.\nT o prove my point, I like to show this Java class ( Example 1 1-20 ).\nE x a m p l e  1 1 - 2 0 .  C o n f i d e n t i a l . j a v a :  a  J a v a  c l a s s  w i t h  a  p r i v a t e\nf i e l d  n a m e d  s e c r e t\npublic class Confidential  { \n \n    private String secret = """"; \n \n    public Confidential (String text) { 11\n        this.secret = text.toUpperCase (); \n    } \n}\nIn Example 1 1-20 , I store the text  in the secret  field after\nconverting it to uppercase, just to make it obvious that whatever is in\nthat field will be in all caps.\nThe actual demonstration consists of running expose.py  with Jython.\nThat script uses introspection (“reflection” in Java parlance) to get the\nvalue of a private field. The code is in Example 1 1-21 .\nE x a m p l e  1 1 - 2 1 .  e x p o s e . p y :  J y t h o n  c o d e  t o  r e a d  t h e  c o n t e n t  o f  a\np r i v a t e  f i e l d  i n  a n o t h e r  c l a s s\n#!/usr/bin/env jython  \n# NOTE: Jython is still Python 2.7 in late2020  \n \nimport Confidential  \n \nmessage = Confidential ('top secret text' ) \nsecret_field  = Confidential .getDeclaredField ('secret' ) \nsecret_field .setAccessible (True)  # break the lock!  \nprint 'message.secret =' , secret_field .get(message)\nIf you run Example 1 1-21 , this is what you get:\n$ jython expose.py  \nmessage.secret = TOP SECRET TEXT\nThe string 'TOP SECRET TEXT'  was read from the secret  private\nfield of the Confidential  class.\nThere is no black magic here: expose.py  uses the Java reflection API to\nget a reference to the private field named 'secret' , and then calls\n'secret_field.setAccessible(True)'  to make it readable.\nThe same thing can be done with Java code, of course (but it takes more\nthan three times as many lines to do it; see the file Expose.java  in\nthe Fluent Python, Second Edition  code repository ).\nThe crucial call .setAccessible(True)  will fail only if the\nJython script or the Java main program (e.g., Expose.class ) is\nrunning under the supervision of a SecurityManager . But in the real\nworld, Java applications are rarely deployed with a SecurityManager—\nexcept for Java applets when they were still supported by browsers.\nMy point is: in Java too, access control modifiers are mostly about\nsafety and not security , at least in practice. So relax and enjoy the power\nPython gives you. Use it responsibly .\n1  From Faassen’ s blog post What is Pythonic?\n2  I used eval  to clone the object here just to make a point about repr ; to clone an instance,\nthe copy.copy  function is safer and faster .\n3  This line could also be written as yield self.x; yield.self.y . I have a lot more to\nsay about the __iter__  special method, generator expressions, and the yield  keyword in\nChapter 17 .\n4  W e had a brief introduction to memoryview , explaining its .cast  method in “Memory\nV iews” .\n5  Leonardo Rochael, one of the technical reviewers of this book disagrees with my low opinion\nof staticmethod , and recommends the blog post “The Definitive Guide on How to Use\nStatic, Class or Abstract Methods in Python”  by Julien Danjou as a counter -ar gument.\nDanjou’ s post is very good; I do recommend it. But it wasn’ t enough to change my mind about\nstaticmethod . Y ou’ll have to decide for yourself.\n6  The pros and cons of private attributes are the subject of the upcoming “Private and\n“Protected” Attributes in Python” .\n7  From the Paste Style Guide .\n8  In modules, a single _  in front of a top-level name does have an ef fect: if you write from\nmymod import *  the names with a _  prefix are not imported from mymod . However , you\ncan still write from mymod import _privatefunc . This is explained in the Python\nT utorial, section 6.1. More on Modules .\n9  One example is in the gettext module docs .\n10  If this state of af fairs depresses you, and makes you wish Python was more like Java in this\nregard, don’ t read my discussion of the relative strength of the Java private  modifier in\n“Soapbox” .\n11  See “Simplest Thing that Could Possibly W ork: A Conversation with W ard Cunningham, Part\nV” .",11595
150-Whats new in this chapter.pdf,150-Whats new in this chapter,"Chapter 12. W riting Special\nMethods for Sequences\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 12th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nDon’ t check whether it is -a duck: check whether it quacks -like-a duck,\nwalks -like-a duck, etc, etc, depending on exactly what subset of duck-like\nbehavior you need to play your language-games with.\n(comp.lang.python , Jul. 26, 2000)\n— Alex Martelli\nIn this chapter , we will create a class to represent a multidimensional\nVector  class—a significant step up from the two-dimensional Vector2d\nof Chapter 1 1 . Vector  will behave like a standard Python immutable flat\nsequence. Its elements will be floats, and it will support the following by\nthe end of this chapter:\nBasic sequence protocol: __len__  and __getitem__ .\nSafe representation of instances with many items.",1346
151-Vector Take 1 Vector2d Compatible.pdf,151-Vector Take 1 Vector2d Compatible,"Proper slicing support, producing new Vector  instances.\nAggregate hashing taking into account every contained element\nvalue.\nCustom formatting language extension.\nW e’ll also implement dynamic attribute access with __getattr__  as a\nway of replacing the read-only properties we used in Vector2d —\nalthough this is not typical of sequence types.\nThe code-intensive presentation will be interrupted by a conceptual\ndiscussion about the idea of protocols as an informal interface. W e’ll talk\nabout how protocols and duck typing  are related, and its practical\nimplications when you create your own types.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThere are no major changes in this chapter . There is a new , brief discussion\nof the typing.Protocol  in a tip box near the end of “Protocols and\nDuck T yping” .\nIn “A Slice-A ware __getitem__” , the implementation of __getitem__  in\nExample 12-6  is shorter and more robust than the example in the first\nedition, thanks to duck typing and operator.index . This change\ncarried over to later implementations of Vector  in this chapter and in\nChapter 16 .\nLet’ s get started.\nV e c t o r :  A  U s e r - D e f i n e d  S e q u e n c e  T y p e\nOur strategy to implement Vector  will be to use composition, not\ninheritance. W e’ll store the components in an array  of floats, and will\nimplement the methods needed for our Vector  to behave like an\nimmutable flat sequence.\nBut before we implement the sequence methods, let’ s make sure we have a\nbaseline implementation of Vector  that is compatible with our earlier\nVector2d  class—except where such compatibility would not make sense.\nV E C T O R  A P P L I C A T I O N S  B E Y O N D  T H R E E  D I M E N S I O N S\nWho needs a vector with 1,000 dimensions? N-dimensional vectors\n(with lar ge values of N) are widely used in information retrieval, where\ndocuments and text queries are represented as vectors, with one\ndimension per word. This is called the V ector space model . In this\nmodel, a key relevance metric is the cosine similarity (i.e., the cosine of\nthe angle between a the vector representing the query and the vector\nrepresenting the document). As the angle decreases, the cosine\napproaches the maximum value of 1, and so does the relevance of the\ndocument to the query .\nHaving said that, the Vector  class in this chapter is a didactic example\nand we’ll not do much math here. Our goal is just to demonstrate some\nPython special methods in the context of a sequence type.\nNumPy and SciPy are the tools you need for real-world vector math.\nThe PyPI package gensim , by Radim Řehůřek, implements vector space\nmodeling for natural language processing and information retrieval,\nusing NumPy and SciPy .\nV e c t o r  T a k e  # 1 :  V e c t o r 2 d  C o m p a t i b l e\nThe first version of Vector  should be as compatible as possible with our\nearlier Vector2d  class.\nHowever , by design, the Vector  constructor is not compatible with the\nVector2d  constructor . W e could make Vector(3, 4)  and\nVector(3, 4, 5)  work, by taking arbitrary ar guments with *args  in\n__init__ , but the best practice for a sequence constructor is to take the\ndata as an iterable ar gument in the constructor , like all built-in sequence\ntypes do. Example 12-1  shows some ways of instantiating our new\nVector  objects.\nExample 12-1. T ests of V ector .__init__ and V ector .__r epr__\n>>> Vector([3.1, 4.2]) \nVector([3.1, 4.2])  \n>>> Vector((3, 4, 5)) \nVector([3.0, 4.0, 5.0])  \n>>> Vector(range(10)) \nVector([0.0, 1.0, 2.0, 3.0, 4.0, ...])\nApart from new constructor signature, I made sure every test I did with\nVector2d  (e.g., Vector2d(3, 4) ) passed and produced the same\nresult with a two-component Vector([3, 4]) .\nW A R N I N G\nWhen a Vector  has more than six components, the string produced by repr()  is\nabbreviated with ...  as seen in the last line of Example 12-1 . This is crucial in any\ncollection type that may contain a lar ge number of items, because repr  is used for\ndebugging—and you don’ t want a single lar ge object to span thousands of lines in your\nconsole or log. Use the reprlib  module to produce limited-length representations, as\nin Example 12-2 . The reprlib  module was named repr  in Python 2.7.\nExample 12-2  lists the implementation of our first version of Vector  (this\nexample builds on the code shown in Examples 1 1-2  and 1 1-3 ).\nExample 12-2. vector_v1.py: derived fr om vector2d_v1.py\nfrom array import array \nimport reprlib \nimport math \n \n \nclass Vector: \n    typecode  = 'd' \n \n    def __init__ (self, components ): \n        self._components  = array(self.typecode , components )  \n \n \n    def __iter__ (self): \n        return iter(self._components )  \n \n \n    def __repr__ (self): \n        components  = reprlib.repr(self._components )  \n \n        components  = components [components .find('['):-1]  \n \n        return f'Vector({components}) ' \n \n    def __str__(self): \n        return str(tuple(self)) \n \n    def __bytes__ (self): \n        return (bytes([ord(self.typecode )]) + \n                bytes(self._components ))  \n \n \n    def __eq__(self, other): \n        return tuple(self) == tuple(other) \n \n    def __abs__(self): \n        return math.hypot(*self)  \n \n \n    def __bool__ (self): \n        return bool(abs(self)) \n \n    @classmethod  \n    def frombytes (cls, octets): \n        typecode  = chr(octets[0]) \n        memv = memoryview (octets[1:]).cast(typecode ) \n        return cls(memv)  \nThe self._components  instance “protected” attribute will hold an\narray  with the Vector  components.\nT o allow iteration, we return an iterator over self._components .\nUse reprlib.repr()  to get a limited-length representation of\nself._components  (e.g., array('d', [0.0, 1.0, 2.0,\n3.0, 4.0, ...]) ).\nRemove the array('d',  prefix and the trailing )  before plugging the\nstring into a Vector  constructor call.\nBuild a bytes  object directly from self._components .1\nSince Python 3.8, math.hypot  accepts n-dimensional points. I used\nthis expression before: math.sqrt(sum(x * x for x in\nself)) .\nThe only change needed from the earlier frombytes  is in the last line:\nwe pass the memoryview  directly to the constructor , without\nunpacking with *  as we did before.\nThe way I used reprlib.repr  deserves some elaboration. That function\nproduces safe representations of lar ge or recursive structures by limiting the\nlength of the output string and marking the cut with '...' . I wanted the\nrepr  of a Vector  to look like Vector([3.0, 4.0, 5.0])  and not\nVector(array('d', [3.0, 4.0, 5.0])) , because the fact that\nthere is an array  inside a Vector  is an implementation detail. Because\nthese constructor calls build identical Vector  objects, I prefer the simpler\nsyntax using a list  ar gument.\nWhen coding __repr__ , I could have produced the simplified\ncomponents  display with this expression:\nreprlib.repr(list(self._components)) . However , this would\nbe wasteful, as I’d be copying every item from self._components  to a\nlist  just to use the list  repr . Instead, I decided to apply\nreprlib.repr  to the self._components  array directly , and then\nchop of f the characters outside of the [] . That’ s what the second line of\n__repr__  does in Example 12-2 .\nT I P\nBecause of its role in debugging, calling repr()  on an object should never raise an\nexception. If something goes wrong inside your implementation of __repr__ , you\nmust deal with the issue and do your best to produce some serviceable output that gives\nthe user a chance of identifying the tar get object.",7682
152-Protocols and Duck Typing.pdf,152-Protocols and Duck Typing,"Note that the __str__ , __eq__ , and __bool__  methods are unchanged\nfrom Vector2d , and only one character was changed in frombytes  (a *\nwas removed in the last line). This is one of the benefits of making the\noriginal Vector2d  iterable.\nBy the way , we could have subclassed Vector  from Vector2d , but I\nchose not to do it for two reasons. First, the incompatible constructors really\nmake subclassing not advisable. I could work around that with some clever\nparameter handling in __init__ , but the second reason is more\nimportant: I want Vector  to be a standalone example of a class\nimplementing the sequence protocol. That’ s what we’ll do next, after a\ndiscussion of the term pr otocol .\nP r o t o c o l s  a n d  D u c k  T y p i n g\nAs early as Chapter 1 , we saw that you don’ t need to inherit from any\nspecial class to create a fully functional sequence type in Python; you just\nneed to implement the methods that fulfill the sequence protocol. But what\nkind of protocol are we talking about?\nIn the context of Object-Oriented programming, a protocol is an informal\ninterface, defined only in documentation and not in code. For example, the\nsequence protocol in Python entails just the __len__  and __getitem__\nmethods. Any class Spam  that implements those methods with the standard\nsignature and semantics can be used anywhere a sequence is expected.\nWhether Spam  is a subclass of this or that is irrelevant; all that matters is\nthat it provides the necessary methods. W e saw that in Example 1-1 ,\nreproduced here in Example 12-3 .\nExample 12-3. Code fr om Example 1-1 , r epr oduced her e for convenience\nimport collections  \n \nCard = collections .namedtuple ('Card', ['rank', 'suit']) \n \nclass FrenchDeck : \n    ranks = [str(n) for n in range(2, 11)] + list('JQKA') \n    suits = 'spades diamonds clubs hearts' .split() \n \n    def __init__ (self): \n        self._cards = [Card(rank, suit) for suit in self.suits \n                                        for rank in self.ranks] \n \n    def __len__(self): \n        return len(self._cards) \n \n    def __getitem__ (self, position ): \n        return self._cards[position ]\nThe FrenchDeck  class in Example 12-3  takes advantage of many Python\nfacilities because it implements the sequence protocol, even if that is not\ndeclared anywhere in the code. An experienced Python coder will look at it\nand understand that it is  a sequence, even if it subclasses object . W e say\nit is  a sequence because it behaves  like one, and that is what matters.\nThis became known as duck typing , after Alex Martelli’ s post quoted at the\nbeginning of this chapter .\nBecause protocols are informal and unenforced, you can often get away\nwith implementing just part of a protocol, if you know the specific context\nwhere a class will be used. For example, to support iteration, only\n__getitem__  is required; there is no need to provide __len__ .\nT I P\nW ith PEP 544—Protocols: Structural subtyping (static duck typing) , Python 3.8\nsupports pr otocol classes : typing  constructs which we studied in “Static Protocols” .\nThis new use of the word protocol in Python has a related but dif ferent meaning. When I\nneed to dif ferentiate them, I write static pr otocol  to refer to the protocols formalized in\nprotocol classes, and dynamic pr otocol  for the traditional sense. One key dif ference is\nthat static protocol implementations must provide all methods defined in the protocol\nclass. “T wo kinds of protocols”  in Chapter 13  has more details.\nW e’ll now implement the sequence protocol in Vector , initially without\nproper support for slicing, but later adding that.",3680
153-How Slicing Works.pdf,153-How Slicing Works,"V e c t o r  T a k e  # 2 :  A  S l i c e a b l e  S e q u e n c e\nAs we saw with the FrenchDeck  example, supporting the sequence\nprotocol is really easy if you can delegate to a sequence attribute in your\nobject, like our self._components  array . These __len__  and\n__getitem__  one-liners are a good start:\nclass Vector: \n    # many lines omitted  \n    # ... \n \n    def __len__(self): \n        return len(self._components ) \n \n    def __getitem__ (self, index): \n        return self._components [index]\nW ith these additions, all of these operations now work:\n>>> v1 = Vector([3, 4, 5]) \n>>> len(v1) \n3 \n>>> v1[0], v1[-1] \n(3.0, 5.0)  \n>>> v7 = Vector(range(7)) \n>>> v7[1:4] \narray('d', [1.0, 2.0, 3.0])\nAs you can see, even slicing is supported—but not very well. It would be\nbetter if a slice of a Vector  was also a Vector  instance and not an\narray . The old FrenchDeck  class has a similar problem: when you slice\nit, you get a list . In the case of Vector , a lot of functionality is lost\nwhen slicing produces plain arrays.\nConsider the built-in sequence types: every one of them, when sliced,\nproduces a new instance of its own type, and not of some other type.\nT o make Vector  produce slices as Vector  instances, we can’ t just\ndelegate the slicing to array . W e need to analyze the ar guments we get in\n__getitem__  and do the right thing.\nNow , let’ s see how Python turns the syntax my_seq[1:3]  into ar guments\nfor my_seq.__getitem__(...) .\nHow Slicing W orks\nA demo is worth a thousand words, so take a look at Example 12-4 .\nExample 12-4. Checking out the behavior of __getitem__ and slices\n>>> class MySeq: \n...     def __getitem__ (self, index): \n...         return index  \n \n... \n>>> s = MySeq() \n>>> s[1]  \n \n1 \n>>> s[1:4]  \n \nslice(1, 4, None)  \n>>> s[1:4:2]  \n \nslice(1, 4, 2)  \n>>> s[1:4:2, 9]  \n \n(slice(1, 4, 2), 9)  \n>>> s[1:4:2, 7:9]  \n \n(slice(1, 4, 2), slice(7, 9, None))\nFor this demonstration, __getitem__  merely returns whatever is\npassed to it.\nA single index, nothing new .\nThe notation 1:4  becomes slice(1, 4, None) .\nslice(1, 4, 2)  means start at 1, stop at 4, step by 2.\nSurprise: the presence of commas inside the []  means __getitem__\nreceives a tuple.\nThe tuple may even hold several slice objects.\nNow let’ s take a closer look at slice  itself in Example 12-5 .\nExample 12-5. Inspecting the attributes of the slice class\n>>> slice  \n \n<class 'slice'>  \n>>> dir(slice) \n \n['__class__', '__delattr__', '__dir__', '__doc__', '__eq__',  \n '__format__', '__ge__', '__getattribute__', '__gt__',  \n '__hash__', '__init__', '__le__', '__lt__', '__ne__',  \n '__new__', '__reduce__', '__reduce_ex__', '__repr__',  \n '__setattr__', '__sizeof__', '__str__', '__subclasshook__',  \n 'indices', 'start', 'step', 'stop']\nslice  is a built-in type (we saw it first in “Slice Objects” ).\nInspecting a slice  we find the data attributes start , stop , and\nstep , and an indices  method.\nIn Example 12-5 , calling dir(slice)  reveals an indices  attribute,\nwhich turns out to be a very interesting but little-known method. Here is\nwhat help(slice.indices)  reveals:\nS.indices(len) -> (start, stop, stride)\nAssuming a sequence of length len , calculate the start  and stop\nindices, and the stride  length of the extended slice described by S .\nOut of bounds indices are clipped just like they are in a normal slice.\nIn other words, indices  exposes the tricky logic that’ s implemented in\nthe built-in sequences to gracefully handle missing or negative indices and\nslices that are longer than the original sequence. This method produces\n“normalized” tuples of nonnegative start , stop , and stride  integers\ntailored to sequence of the given length.\nHere are a couple of examples, considering a sequence of len == 5 , e.g.,\n'ABCDE' :\n>>> slice(None, 10, 2).indices(5)  \n \n(0, 5, 2)  \n>>> slice(-3, None, None).indices(5)  \n \n(2, 5, 1)",3966
154-A Slice-Aware __getitem__.pdf,154-A Slice-Aware __getitem__,"'ABCDE'[:10:2]  is the same as 'ABCDE'[0:5:2]\n'ABCDE'[-3:]  is the same as 'ABCDE'[2:5:1]\nIn our Vector  code, we’ll not need the slice.indices()  method\nbecause when we get a slice ar gument we’ll delegate its handling to the\n_components  array . But if you can’ t count on the services of an\nunderlying sequence, this method can be a huge time saver .\nNow that we know how to handle slices, let’ s take a look at the improved\nVector.__getitem__  implementation.\nA Slice-A ware __getitem__\nExample 12-6  lists the two methods needed to make Vector  behave as a\nsequence: __len__  and __getitem__  (the latter now implemented to\nhandle slicing correctly).\nExample 12-6. Part of vector_v2.py: __len__ and __getitem__ methods\nadded to V ector class fr om vector_v1.py (see Example 12-2 )\n    def __len__(self): \n        return len(self._components ) \n \n    def __getitem__ (self, key): \n        if isinstance (key, slice):  \n \n            cls = type(self)  \n \n            return cls(self._components [key])  \n \n        index = operator .index(key)  \n \n        return self._components [index]  \nIf the key  ar gument is a slice …\n…get the class of the instance (i.e., Vector ) and…\n…invoke the class to build another Vector  instance from a slice of the\n_components  array .\nIf we can get an index  from key …\n…return the specific item from _components .\nThe operator.index()  function calls the __index__  special\nmethod. The function and the special method were defined in PEP 357—\nAllowing Any Object to be Used for Slicing , proposed by T ravis Oliphant\nto allow any of the numerous types of integers in NumPy to be used as\nindexes and slice ar guments. The key dif ference between\noperator.index()  and int()  is that the former is intended for this\nspecific purpose. For example, int(3.14)  returns 3, but\noperator.index(3.14)  raises TypeError  because a float  should\nnot be used as an index.\nN O T E\nExcessive use of isinstance  may be a sign of bad OO design, but handling slices in\n__getitem__  is a justified use case. In the first edition, I also used an isinstance\ntest on key  to test if it was an integer . Using operator.index  avoids this test, and\nraises TypeError  with a very informative message if we can’ t get the index  from\nkey . See the last error message from Example 12-7  below .\nOnce the code in Example 12-6  is added to the Vector  class, we have\nproper slicing behavior , as Example 12-7  demonstrates.\nExample 12-7. T ests of enhanced V ector . getitem  fr om Example 12-6\n    >>> v7 = Vector(range(7))  \n    >>> v7[-1]  \n  \n    6.0 \n    >>> v7[1:4]  \n  \n    Vector([1.0, 2.0, 3.0])  \n    >>> v7[-1:]  \n  \n    Vector([6.0])  \n    >>> v7[1,2]  \n  \n    Traceback (most recent call last):  \n      ...  \n    TypeError: 'tuple' object cannot be interpreted as an integer\nAn integer index retrieves just one component value as a float .",2928
155-Vector Take 3 Dynamic Attribute Access.pdf,155-Vector Take 3 Dynamic Attribute Access,"A slice index creates a new Vector .\nA slice of len == 1  also creates a Vector .\nVector  does not support multidimensional indexing, so a tuple of\nindices or slices raises an error .\nV e c t o r  T a k e  # 3 :  D y n a m i c  A t t r i b u t e  A c c e s s\nIn the evolution from Vector2d  to Vector , we lost the ability to access\nvector components by name (e.g., v.x , v.y ). W e are now dealing with\nvectors that may have a lar ge number of components. Still, it may be\nconvenient to access the first few components with shortcut letters such as\nx , y , z  instead of v[0] , v[1]  and v[2] .\nHere is the alternative syntax we want to provide for reading the first four\ncomponents of a vector:\n>>> v = Vector(range(10)) \n>>> v.x \n0.0 \n>>> v.y, v.z, v.t \n(1.0, 2.0, 3.0)\nIn Vector2d , we provided read-only access to x  and y  using the\n@property  decorator ( Example 1 1-7 ). W e could write four properties in\nVector , but it would be tedious. The __getattr__  special method\nprovides a better way .\nThe __getattr__  method is invoked by the interpreter when attribute\nlookup fails. In simple terms, given the expression my_obj.x , Python\nchecks if the my_obj  instance has an attribute named x ; if not, the search\ngoes to the class ( my_obj.__class__ ), and then up the inheritance\ngraph.  If the x  attribute is not found, then the __getattr__  method\ndefined in the class of my_obj  is called with self  and the name of the\nattribute as a string (e.g., 'x' ).2\nExample 12-8  lists our __getattr__  method. Essentially it checks\nwhether the attribute being sought is one of the letters xyzt  and if so,\nreturns the corresponding vector component.\nExample 12-8. Part of vector_v3.py : __getattr__ method added to Vector\nclass.\n    __match_args__  = ('x', 'y', 'z', 't')  \n \n \n    def __getattr__ (self, name): \n        cls = type(self)  \n \n        try: \n            pos = cls.__match_args__ .index(name)  \n \n        except ValueError :  \n \n            pos = -1 \n        if 0 <= pos < len(self._components ):  \n \n            return self._components [pos] \n        msg = f'{cls.__name__!r} object has no attribute {name!r} '  \n \n        raise AttributeError (msg)\nSet __match_args__  to allow pattern matching on the dynamic\nattributes supported by __getattr__ .\nGet the Vector  class for later use.\nT ry to get the position of name  in __match_args__ .\n.index(name)  raises ValueError  when name  is not found; set\npos  to -1 (I’d rather use a method like str.find  here, but tuple\ndoesn’ t implement it.)\nIf the pos  is within range of the available components, return the\ncomponent.\nIf we get this far , raise AttributeError  with a standard message\ntext.3\nIt’ s not hard to implement __getattr__ , but in this case it’ s not enough.\nConsider the bizarre interaction in Example 12-9 .\nExample 12-9. Inappr opriate behavior: assigning to v .x raises no err or , but\nintr oduces an inconsistency\n>>> v = Vector(range(5)) \n>>> v \nVector([0.0, 1.0, 2.0, 3.0, 4.0])  \n>>> v.x  \n \n0.0 \n>>> v.x = 10  \n \n>>> v.x  \n \n10 \n>>> v \nVector([0.0, 1.0, 2.0, 3.0, 4.0])  \nAccess element v[0]  as v.x .\nAssign new value to v.x . This should raise an exception.\nReading v.x  shows the new value, 10 .\nHowever , the vector components did not change.\nCan you explain what is happening? In particular , why the second time v.x\nreturns 10  if that value is not in the vector components array? If you don’ t\nknow right of f the bat, study the explanation of __getattr__  given right\nbefore Example 12-8 . It’ s a bit subtle, but a very important foundation to\nunderstand a lot of what comes later in the book.\nAfter you’ve given it some thought, proceed and we’ll explain exactly what\nhappened.\nThe inconsistency in Example 12-9  was introduced because of the way\n__getattr__  works: Python only calls that method as a fall back, when\nthe object does not have the named attribute. However , after we assign v.x\n= 10 , the v  object now has an x  attribute, so __getattr__  will no\nlonger be called to retrieve v.x : the interpreter will just return the value 10\nthat is bound to v.x . On the other hand, our implementation of\n__getattr__  pays no attention to instance attributes other than\nself._components , from where it retrieves the values of the “virtual\nattributes” listed in __match_args__ .\nW e need to customize the logic for setting attributes in our Vector  class in\norder to avoid this inconsistency .\nRecall that in the latest Vector2d  examples from Chapter 1 1 , trying to\nassign to the .x  or .y  instance attributes raised AttributeError . In\nVector  we want the same exception with any attempt at assigning to all\nsingle-letter lowercase attribute names, just to avoid confusion. T o do that,\nwe’ll implement __setattr__  as listed in Example 12-10 .\nExample 12-10. Part of vector_v3.py: __setattr__ method in V ector class\n    def __setattr__ (self, name, value): \n        cls = type(self) \n        if len(name) == 1:  \n \n            if name in cls.__match_args__ :  \n \n                error = 'readonly attribute {attr_name!r} ' \n            elif name.islower():  \n \n                error = ""can't set attributes 'a' to 'z' in \n{cls_name!r} "" \n            else: \n                error = ''  \n \n            if error:  \n \n                msg = error.format(cls_name =cls.__name__ , \nattr_name =name) \n                raise AttributeError (msg) \n        super().__setattr__ (name, value)  \nSpecial handling for single-character attribute names.\nIf name  is one of __match_args__ , set specific error message.\nIf name  is lowercase, set error message about all single-letter names.\nOtherwise, set blank error message.\nIf there is a nonblank error message, raise AttributeError .\nDefault case: call __setattr__  on superclass for standard behavior .\nT I P\nThe super()  function provides a way to access methods of superclasses dynamically ,\na necessity in a dynamic language supporting multiple inheritance like Python. It’ s used\nto delegate some task from a method in a subclass to a suitable method in a superclass,\nas seen in Example 12-10 . There is more about super  in “Multiple Inheritance and\nMethod Resolution Order” .\nWhile choosing the error message to display with AttributeError , my\nfirst check was the behavior of the built-in complex  type, because they are\nimmutable and have a pair of data attributes real  and imag . T rying to\nchange either of those in a complex  instance raises AttributeError\nwith the message ""can't set attribute"" . On the other hand, trying\nto set a read-only attribute protected by a property as we did in “A Hashable\nV ector2d”  produces the message ""read-only attribute"" . I drew\ninspiration from both wordings to set the error  string in __setitem__ ,\nbut was more explicit about the forbidden attributes.\nNote that we are not disallowing setting all attributes, only single-letter ,\nlowercase ones, to avoid confusion with the supported read-only attributes\nx , y , z , and t .\nW A R N I N G\nKnowing that declaring __slots__  at the class level prevents setting new instance\nattributes, it’ s tempting to use that feature instead of implementing __setattr__  as\nwe did. However , because of all the caveats discussed in “Summarizing The Issues with\n__slots__” , using __slots__  just to prevent instance attribute creation is not\nrecommended. __slots__  should be used only to save memory , and only if that is a\nreal issue.",7528
156-Vector Take 4 Hashing and a Faster.pdf,156-Vector Take 4 Hashing and a Faster,"Even without supporting writing to the Vector  components, here is an\nimportant takeaway from this example: very often when you implement\n__getattr__  you need to code __setattr__  as well, to avoid\ninconsistent behavior in your objects.\nIf we wanted to allow changing components, we could implement\n__setitem__  to enable v[0] = 1.1  and/or __setattr__  to make\nv.x = 1.1  work. But Vector  will remain immutable because we want\nto make it hashable in the coming section.\nV e c t o r  T a k e  # 4 :  H a s h i n g  a n d  a  F a s t e r  = =\nOnce more we get to implement a __hash__  method. T ogether with the\nexisting __eq__ , this will make Vector  instances hashable.\nThe __hash__  in Example 1 1-8  simply computed hash(self.x) ^\nhash(self.y) . W e now would like to apply the ^  (xor) operator to the\nhashes of every component, in succession, like this: v[0] ^ v[1] ^\nv[2] …. That is what the functools.reduce  function is for .\nPreviously I said that reduce  is not as popular as before,  but computing\nthe hash of all vector components is a perfect job for it. Figure 12-1  depicts\nthe general idea of the reduce  function.4\nFigur e 12-1. Reducing functions—r educe, sum, any , all—pr oduce a single aggr egate r esult fr om a\nsequence or fr om any finite iterable object.\nSo far we’ve seen that functools.reduce()  can be replaced by\nsum() , but now let’ s properly explain how it works. The key idea is to\nreduce a series of values to a single value. The first ar gument to\nreduce()  is a two-ar gument function, and the second ar gument is an\niterable. Let’ s say we have a two-ar gument function fn  and a list lst .\nWhen you call reduce(fn, lst) , fn  will be applied to the first pair of\nelements— fn(lst[0], lst[1]) —producing a first result, r1 . Then\nfn  is applied to r1  and the next element— fn(r1, lst[2]) —\nproducing a second result, r2 . Now fn(r2, lst[3])  is called to\nproduce r3  … and so on until the last element, when a single result, rN , is\nreturned.\nHere is how you could use reduce  to compute 5! (the factorial of 5):\n>>> 2 * 3 * 4 * 5  # the result we want: 5! == 120  \n120 \n>>> import functools  \n>>> functools .reduce(lambda a,b: a*b, range(1, 6)) \n120\nBack to our hashing problem, Example 12-1 1  shows the idea of computing\nthe aggregate xor by doing it in three ways: with a for  loop and two\nreduce  calls.\nExample 12-1 1. Thr ee ways of calculating the accumulated xor of integers\nfr om 0 to 5\n>>> n = 0 \n>>> for i in range(1, 6):  \n \n...     n ^= i \n... \n>>> n \n1 \n>>> import functools  \n>>> functools .reduce(lambda a, b: a^b, range(6))  \n \n1 \n>>> import operator  \n>>> functools .reduce(operator .xor, range(6))  \n \n1\nAggregate xor with a for  loop and an accumulator variable.\nfunctools.reduce  using an anonymous function.\nfunctools.reduce  replacing custom lambda  with\noperator.xor .\nFrom the alternatives in Example 12-1 1 , the last one is my favorite, and the\nfor  loop comes second. What is your preference?\nAs seen in “The operator Module” , operator  provides the functionality\nof all Python infix operators in function form, lessening the need for\nlambda .\nT o code Vector.__hash__  in my preferred style, we need to import the\nfunctools  and operator  modules. Example 12-12  shows the relevant\nchanges.\nExample 12-12. Part of vector_v4.py: two imports and __hash__ method\nadded to V ector class fr om vector_v3.py\nfrom array import array \nimport reprlib \nimport math \nimport functools   \n \nimport operator   \n \n \n \nclass Vector: \n    typecode  = 'd' \n \n    # many lines omitted in book listing...  \n \n    def __eq__(self, other):  \n \n        return tuple(self) == tuple(other) \n \n    def __hash__ (self): \n        hashes = (hash(x) for x in self._components )  \n \n        return functools .reduce(operator .xor, hashes, 0)  \n \n \n    # more lines omitted...\nImport functools  to use reduce .\nImport operator  to use xor .\nNo change to __eq__ ; I listed it here because it’ s good practice to keep\n__eq__  and __hash__  close in source code, because they need to\nwork together .\nCreate a generator expression to lazily compute the hash of each\ncomponent.\nFeed hashes  to reduce  with the xor  function to compute the\naggregate hash code; the third ar gument, 0 , is the initializer (see next\nwarning).\nW A R N I N G\nWhen using reduce , it’ s good practice to provide the third ar gument,\nreduce(function, iterable, initializer) , to prevent this exception:\nTypeError: reduce() of empty sequence with no initial value\n(excellent message: explains the problem and how to fix it). The initializer  is the\nvalue returned if the sequence is empty and is used as the first ar gument in the reducing\nloop, so it should be the identity value of the operation. As examples, for + , | , ^  the\ninitializer  should be 0 , but for * , &  it should be 1 .\nAs implemented, the __hash__  method in Example 12-12  is a perfect\nexample of a map-reduce computation ( Figure 12-2 ).\nFigur e 12-2. Map-r educe: apply function to each item to generate a new series (map), then compute\naggr egate (r educe)\nThe mapping step produces one hash for each component, and the reduce\nstep aggregates all hashes with the xor  operator . Using map  instead of a\ngenexp  makes the mapping step even more visible:\n    def __hash__ (self): \n        hashes = map(hash, self._components ) \n        return functools .reduce(operator .xor, hashes)\nT I P\nThe solution with map  would be less ef ficient in Python 2, where the map  function\nbuilds a new list  with the results. But in Python 3, map  is lazy: it creates a generator\nthat yields the results on demand, thus saving memory—just like the generator\nexpression we used in the __hash__  method of Example 12-8 .\nWhile we are on the topic of reducing functions, we can replace our quick\nimplementation of __eq__  with another one that will be cheaper in terms\nof processing and memory , at least for lar ge vectors. As introduced in\nExample 1 1-2 , we have this very concise implementation of __eq__ :\n    def __eq__(self, other): \n        return tuple(self) == tuple(other)\nThis works for Vector2d  and for Vector —it even considers\nVector([1, 2])  equal to (1, 2) , which may be a problem, but we’ll\noverlook that for now .  But for Vector  instances that may have thousands\nof components, it’ s very inef ficient. It builds two tuples copying the entire\ncontents of the operands just to use the __eq__  of the tuple  type. For\nVector2d  (with only two components), it’ s a good shortcut, but not for\nthe lar ge multidimensional vectors. A better way of comparing one\nVector  to another Vector  or iterable would be Example 12-13 .\nExample 12-13. The Vector.__eq__  implementation using zip  in a\nfor  loop for mor e efficient comparison\n    def __eq__(self, other): \n        if len(self) != len(other):  \n \n            return False \n        for a, b in zip(self, other):  \n \n            if a != b:  \n \n                return False \n        return True  \nIf the len  of the objects are dif ferent, they are not equal.\n5\nzip  produces a generator of tuples made from the items in each iterable\nar gument. See “The A wesome zip”  if zip  is new to you. The len\ncomparison above is needed because zip  stops producing values\nwithout warning as soon as one of the inputs is exhausted.\nAs soon as two components are dif ferent, exit returning False .\nOtherwise, the objects are equal.\nT I P\nThe zip  function is named after the zipper fastener because the physical device works\nby interlocking pairs of teeth taken from both zipper sides, a good visual analogy for\nwhat zip(left, right)  does. No relation with compressed files.\nExample 12-13  is ef ficient, but the all  function can produce the same\naggregate computation of the for  loop in one line: if all comparisons\nbetween corresponding components in the operands are True , the result is\nTrue . As soon as one comparison is False , all  returns False .\nExample 12-14  shows how __eq__  looks using all .\nExample 12-14. The Vector.__eq__  implementation using zip  and\nall : same logic as Example 12-13\n    def __eq__(self, other): \n        return len(self) == len(other) and all(a == b for a, b in \nzip(self, other))\nNote that we first check that the operands have equal length, because zip\nwill stop at the shortest operand.\nExample 12-14  is the implementation we choose for __eq__  in\nvector_v4.py .\nT H E  A W E S O M E  Z I P\nHaving a for  loop that iterates over items without fiddling with index\nvariables is great and prevents lots of bugs, but demands some special\nutility functions. One of them is the zip  built-in, which makes it easy\nto iterate in parallel over two or more iterables by returning tuples that\nyou can unpack into variables, one for each item in the parallel inputs.\nSee Example 12-15 .\nE x a m p l e  1 2 - 1 5 .  T h e  z i p  b u i l t - i n  a t  w o r k\n>>> zip(range(3), 'ABC')  \n \n<zip object at 0x10063ae48>  \n>>> list(zip(range(3), 'ABC'))  \n \n[(0, 'A'), (1, 'B'), (2, 'C')]  \n>>> list(zip(range(3), 'ABC', [0.0, 1.1, 2.2, 3.3]))  \n \n[(0, 'A', 0.0), (1, 'B', 1.1), (2, 'C', 2.2)]  \n>>> from itertools  import zip_longest   \n \n>>> list(zip_longest (range(3), 'ABC', [0.0, 1.1, 2.2, 3.3], \nfillvalue =-1)) \n[(0, 'A', 0.0), (1, 'B', 1.1), (2, 'C', 2.2), (-1, -1, 3.3)]\nzip  returns a generator that produces tuples on demand.\nBuild a list  just for display; usually we iterate over the generator .\nzip  stops without warning when one of the iterables is exhausted.\nThe itertools.zip_longest  function behaves dif ferently: it\nuses an optional fillvalue  (None  by default) to complete\nmissing values so it can generate tuples until the last iterable is\nexhausted.",9838
157-Vector Take 5 Formatting.pdf,157-Vector Take 5 Formatting,"N E W  ZIP()  O P T I O N  I N  P Y T H O N  3 . 1 0 .\nI wrote in the First Edition  that zip  silently stopping at the shortest iterable was\nsurprising—not a good trait for an API. Silently ignoring part of the input can\ncause subtle bugs. Instead, zip  should raise ValueError  if the iterables are\nnot all of the same length, which is what happens when unpacking an iterable to a\ntuple of variables of dif ferent length—in line with Python’ s fail fast  policy . PEP\n618—Add Optional Length-Checking T o zip  added an optional strict\nar gument to zip  to make it behave in that way . It is implemented in Python 3.10.\nThe zip  function can also be used to transpose a matrix represented as\nnested iterables. For example:\n>>> a = [(1, 2, 3), \n...      (4, 5, 6)] \n>>> list(zip(*a)) \n[(1, 4), (2, 5), (3, 6)]  \n>>> b = [(1, 2), \n...      (3, 4), \n...      (5, 6)] \n>>> list(zip(*b)) \n[(1, 3, 5), (2, 4, 6)]\nIf you want to grok zip , spend some time figuring out how these\nexamples work.\nThe enumerate  built-in is another generator function often used in\nfor  loops to avoid direct handling of index variables. If you are not\nfamiliar with enumerate , you should definitely check it out in the\n“Built-in functions” documentation . The zip  and enumerate  built-\nins, along with several other generator functions in the standard library ,\nare covered in “Generator Functions in the Standard Library” .\nW e wrap up this chapter by bringing back the __format__  method from\nVector2d  to Vector .\nV e c t o r  T a k e  # 5 :  F o r m a t t i n g\nThe __format__  method of Vector  will resemble that of Vector2d ,\nbut instead of providing a custom display in polar coordinates, Vector\nwill use spherical coordinates—also known as “hyperspherical”\ncoordinates, because now we support n  dimensions, and spheres are\n“hyperspheres” in 4D and beyond.  Accordingly , we’ll change the custom\nformat suf fix from 'p'  to 'h' .\nT I P\nAs we saw in “Formatted Displays” , when extending the Format Specification Mini-\nLanguage  it’ s best to avoid reusing format codes supported by built-in types. In\nparticular , our extended mini-language also uses the float formatting codes\n'eEfFgGn%'  in their original meaning, so we definitely must avoid these. Integers use\n'bcdoxXn'  and strings use 's' . I picked 'p'  for Vector2d  polar coordinates.\nCode 'h'  for hyperspherical coordinates is a good choice.\nFor example, given a Vector  object in 4D space ( len(v) == 4 ), the\n'h'  code will produce a display like <r, Φ ₁, Φ ₂ , Φ ₃ >  where r  is the\nmagnitude ( abs(v) ) and the remaining numbers are the angular\ncomponents Φ ₁ , Φ ₂ , Φ ₃ .\nHere are some samples of the spherical coordinate format in 4D, taken from\nthe doctests of vector_v5.py  (see Example 12-16 ):\n>>> format(Vector([-1, -1, -1, -1]), 'h') \n'<2.0, 2.0943951023931957, 2.186276035465284,  \n3.9269908169872414>'  \n>>> format(Vector([2, 2, 2, 2]), '.3eh') \n'<4.000e+00, 1.047e+00, 9.553e-01, 7.854e-01>'  \n>>> format(Vector([0, 1, 0, 0]), '0.5fh') \n'<1.00000, 1.57080, 0.00000, 0.00000>'\nBefore we can implement the minor changes required in __format__ , we\nneed to code a pair of support methods: angle(n)  to compute one of the\nangular coordinates (e.g., Φ ₁ ), and angles()  to return an iterable of all\nangular coordinates. I will not describe the math here; if you’re curious,\nW ikipedia’ s "" n -sphere” entry  has the formulas I used to calculate the6\nspherical coordinates from the Cartesian coordinates in the Vector\ncomponents array .\nExample 12-16  is a full listing of vector_v5.py  consolidating all we’ve\nimplemented since “V ector T ake #1: V ector2d Compatible”  and introducing\ncustom formatting.\nExample 12-16. vector_v5.py: doctests and all code for final V ector class;\ncallouts highlight additions needed to support __format__\n"""""" \nA multidimensional ``Vector`` class, take 5  \n \nA ``Vector`` is built from an iterable of numbers::  \n \n    >>> Vector([3.1, 4.2])  \n    Vector([3.1, 4.2])  \n    >>> Vector((3, 4, 5))  \n    Vector([3.0, 4.0, 5.0])  \n    >>> Vector(range(10))  \n    Vector([0.0, 1.0, 2.0, 3.0, 4.0, ...])  \n \n \nTests with two dimensions (same results as ``vector2d_v1.py``)::  \n \n    >>> v1 = Vector([3, 4])  \n    >>> x, y = v1  \n    >>> x, y  \n    (3.0, 4.0)  \n    >>> v1  \n    Vector([3.0, 4.0])  \n    >>> v1_clone = eval(repr(v1))  \n    >>> v1 == v1_clone  \n    True  \n    >>> print(v1)  \n    (3.0, 4.0)  \n    >>> octets = bytes(v1)  \n    >>> octets  \n    \nb'd\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x\n00\\x10@'  \n    >>> abs(v1)  \n    5.0 \n    >>> bool(v1), bool(Vector([0, 0]))  \n    (True, False)  \n \n \nTest of ``.frombytes()`` class method:  \n \n    >>> v1_clone = Vector.frombytes(bytes(v1))  \n    >>> v1_clone  \n    Vector([3.0, 4.0])  \n    >>> v1 == v1_clone  \n    True  \n \n \nTests with three dimensions::  \n \n    >>> v1 = Vector([3, 4, 5])  \n    >>> x, y, z = v1  \n    >>> x, y, z  \n    (3.0, 4.0, 5.0)  \n    >>> v1  \n    Vector([3.0, 4.0, 5.0])  \n    >>> v1_clone = eval(repr(v1))  \n    >>> v1 == v1_clone  \n    True  \n    >>> print(v1)  \n    (3.0, 4.0, 5.0)  \n    >>> abs(v1)  # doctest:+ELLIPSIS  \n    7.071067811...  \n    >>> bool(v1), bool(Vector([0, 0, 0]))  \n    (True, False)  \n \n \nTests with many dimensions::  \n \n    >>> v7 = Vector(range(7))  \n    >>> v7  \n    Vector([0.0, 1.0, 2.0, 3.0, 4.0, ...])  \n    >>> abs(v7)  # doctest:+ELLIPSIS  \n    9.53939201...  \n \n \nTest of ``.__bytes__`` and ``.frombytes()`` methods::  \n \n    >>> v1 = Vector([3, 4, 5])  \n    >>> v1_clone = Vector.frombytes(bytes(v1))  \n    >>> v1_clone  \n    Vector([3.0, 4.0, 5.0])  \n    >>> v1 == v1_clone  \n    True  \n \n \nTests of sequence behavior::  \n \n    >>> v1 = Vector([3, 4, 5])  \n    >>> len(v1)  \n    3 \n    >>> v1[0], v1[len(v1)-1], v1[-1]  \n    (3.0, 5.0, 5.0)  \n \n \nTest of slicing::  \n \n    >>> v7 = Vector(range(7))  \n    >>> v7[-1]  \n    6.0 \n    >>> v7[1:4]  \n    Vector([1.0, 2.0, 3.0])  \n    >>> v7[-1:]  \n    Vector([6.0])  \n    >>> v7[1,2]  \n    Traceback (most recent call last):  \n      ...  \n    TypeError: 'tuple' object cannot be interpreted as an integer  \n \n \nTests of dynamic attribute access::  \n \n    >>> v7 = Vector(range(10))  \n    >>> v7.x  \n    0.0 \n    >>> v7.y, v7.z, v7.t  \n    (1.0, 2.0, 3.0)  \n \nDynamic attribute lookup failures::  \n \n    >>> v7.k  \n    Traceback (most recent call last):  \n      ...  \n    AttributeError: 'Vector' object has no attribute 'k'  \n    >>> v3 = Vector(range(3))  \n    >>> v3.t  \n    Traceback (most recent call last):  \n      ...  \n    AttributeError: 'Vector' object has no attribute 't'  \n    >>> v3.spam  \n    Traceback (most recent call last):  \n      ...  \n    AttributeError: 'Vector' object has no attribute 'spam'  \n \n \nTests of hashing::  \n \n    >>> v1 = Vector([3, 4])  \n    >>> v2 = Vector([3.1, 4.2])  \n    >>> v3 = Vector([3, 4, 5])  \n    >>> v6 = Vector(range(6))  \n    >>> hash(v1), hash(v3), hash(v6)  \n    (7, 2, 1)  \n \n \nMost hash codes of non-integers vary from a 32-bit to 64-bit  \nCPython build::  \n \n    >>> import sys  \n    >>> hash(v2) == (384307168202284039 if sys.maxsize > 2**32 else  \n357915986)  \n    True  \n \n \nTests of ``format()`` with Cartesian coordinates in 2D::  \n \n    >>> v1 = Vector([3, 4])  \n    >>> format(v1)  \n    '(3.0, 4.0)'  \n    >>> format(v1, '.2f')  \n    '(3.00, 4.00)'  \n    >>> format(v1, '.3e')  \n    '(3.000e+00, 4.000e+00)'  \n \n \nTests of ``format()`` with Cartesian coordinates in 3D and 7D::  \n \n    >>> v3 = Vector([3, 4, 5])  \n    >>> format(v3)  \n    '(3.0, 4.0, 5.0)'  \n    >>> format(Vector(range(7)))  \n    '(0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0)'  \n \n \nTests of ``format()`` with spherical coordinates in 2D, 3D and 4D::  \n \n    >>> format(Vector([1, 1]), 'h')  # doctest:+ELLIPSIS  \n    '<1.414213..., 0.785398...>'  \n    >>> format(Vector([1, 1]), '.3eh')  \n    '<1.414e+00, 7.854e-01>'  \n    >>> format(Vector([1, 1]), '0.5fh')  \n    '<1.41421, 0.78540>'  \n    >>> format(Vector([1, 1, 1]), 'h')  # doctest:+ELLIPSIS  \n    '<1.73205..., 0.95531..., 0.78539...>'  \n    >>> format(Vector([2, 2, 2]), '.3eh')  \n    '<3.464e+00, 9.553e-01, 7.854e-01>'  \n    >>> format(Vector([0, 0, 0]), '0.5fh')  \n    '<0.00000, 0.00000, 0.00000>'  \n    >>> format(Vector([-1, -1, -1, -1]), 'h')  # doctest:+ELLIPSIS  \n    '<2.0, 2.09439..., 2.18627..., 3.92699...>'  \n    >>> format(Vector([2, 2, 2, 2]), '.3eh')  \n    '<4.000e+00, 1.047e+00, 9.553e-01, 7.854e-01>'  \n    >>> format(Vector([0, 1, 0, 0]), '0.5fh')  \n    '<1.00000, 1.57080, 0.00000, 0.00000>'  \n"""""" \n \nfrom array import array \nimport reprlib \nimport math \nimport functools  \nimport operator  \nimport itertools   \n \n \n \nclass Vector: \n    typecode  = 'd' \n \n    def __init__ (self, components ): \n        self._components  = array(self.typecode , components ) \n \n    def __iter__ (self): \n        return iter(self._components ) \n \n    def __repr__ (self): \n        components  = reprlib.repr(self._components ) \n        components  = components [components .find('['):-1] \n        return f'Vector({components}) ' \n \n    def __str__(self): \n        return str(tuple(self)) \n \n    def __bytes__ (self): \n        return (bytes([ord(self.typecode )]) + \n                bytes(self._components )) \n \n    def __eq__(self, other): \n        return (len(self) == len(other) and \n                all(a == b for a, b in zip(self, other))) \n \n    def __hash__ (self): \n        hashes = (hash(x) for x in self) \n        return functools .reduce(operator .xor, hashes, 0) \n \n    def __abs__(self): \n        return math.hypot(*self) \n \n    def __bool__ (self): \n        return bool(abs(self)) \n \n    def __len__(self): \n        return len(self._components ) \n \n    def __getitem__ (self, key): \n        if isinstance (key, slice): \n            cls = type(self) \n            return cls(self._components [key]) \n        index = operator .index(key) \n        return self._components [index] \n \n    __match_args__  = ('x', 'y', 'z', 't') \n \n    def __getattr__ (self, name): \n        cls = type(self) \n        try: \n            pos = cls.__match_args__ .index(name) \n        except ValueError : \n            pos = -1 \n        if 0 <= pos < len(self._components ): \n            return self._components [pos] \n        msg = f'{cls.__name__!r} object has no attribute {name!r} ' \n        raise AttributeError (msg) \n \n    def angle(self, n):  \n \n        r = math.hypot(*self[n:]) \n        a = math.atan2(r, self[n-1]) \n        if (n == len(self) - 1) and (self[-1] < 0): \n            return math.pi * 2 - a \n        else: \n            return a \n \n    def angles(self):  \n \n        return (self.angle(n) for n in range(1, len(self))) \n \n    def __format__ (self, fmt_spec =''): \n        if fmt_spec .endswith ('h'):  # hyperspherical coordinates  \n            fmt_spec  = fmt_spec [:-1] \n            coords = itertools .chain([abs(self)], \n                                     self.angles())  \n \n            outer_fmt  = '<{}>'  \n \n        else: \n            coords = self \n            outer_fmt  = '({})'  \n \n        components  = (format(c, fmt_spec ) for c in coords)  \n \n        return outer_fmt .format(', '.join(components ))  \n \n \n    @classmethod  \n    def frombytes (cls, octets): \n        typecode  = chr(octets[0]) \n        memv = memoryview (octets[1:]).cast(typecode ) \n        return cls(memv)\nImport itertools  to use chain  function in __format__ .\nCompute one of the angular coordinates, using formulas adapted from\nthe n -sphere article .\nCreate generator expression to compute all angular coordinates on\ndemand.\nUse itertools.chain  to produce genexp  to iterate seamlessly over\nthe magnitude and the angular coordinates.\nConfigure spherical coordinate display with angular brackets.\nConfigure Cartesian coordinate display with parentheses.\nCreate generator expression to format each coordinate item on demand.\nPlug formatted components separated by commas inside brackets or\nparentheses.",12242
158-Chapter Summary.pdf,158-Chapter Summary,"N O T E\nW e are making heavy use of generator expressions in __format__ , angle , and\nangles  but our focus here is in providing __format__  to bring Vector  to the same\nimplementation level as Vector2d . When we cover generators in Chapter 17  we’ll use\nsome of the code in Vector  as examples, and then the generator tricks will be\nexplained in detail.\nThis concludes our mission for this chapter . The Vector  class will be\nenhanced with infix operators in Chapter 16 , but our goal here was to\nexplore techniques for coding special methods that are useful in a wide\nvariety of collection classes.\nC h a p t e r  S u m m a r y\nThe Vector  example in this chapter was designed to be compatible with\nVector2d , except for the use of a dif ferent constructor signature\naccepting a single iterable ar gument, just like the built-in sequence types\ndo. The fact that Vector  behaves as a sequence just by implementing\n__getitem__  and __len__  prompted a discussion of protocols, the\ninformal interfaces used in duck-typed languages.\nW e then looked at how the my_seq[a:b:c]  syntax works behind the\nscenes, by creating a slice(a, b, c)  object and handing it to\n__getitem__ . Armed with this knowledge, we made Vector  respond\ncorrectly to slicing, by returning new Vector  instances, just like a\nPythonic sequence is expected to do.\nThe next step was to provide read-only access to the first few Vector\ncomponents using notation such as my_vec.x . W e did it by implementing\n__getattr__ . Doing that opened the possibility of tempting the user to\nassign to those special components by writing my_vec.x = 7 , revealing\na potential bug. W e fixed it by implementing __setattr__  as well, to\nforbid assigning values to single-letter attributes. V ery often, when you\ncode a __getattr__  you need to add __setattr__  too, in order to\navoid inconsistent behavior .\nImplementing the __hash__  function provided the perfect context for\nusing functools.reduce , because we needed to apply the xor operator\n^  in succession to the hashes of all Vector  components to produce an\naggregate hash code for the whole Vector . After applying reduce  in\n__hash__ , we used the all  reducing built-in to create a more ef ficient\n__eq__  method.\nThe last enhancement to Vector  was to reimplement the __format__\nmethod from Vector2d  by supporting spherical coordinates as an\nalternative to the default Cartesian coordinates. W e used quite a bit of math\nand several generators to code __format__  and its auxiliary functions,\nbut these are implementation details—and we’ll come back to the",2611
159-Further Reading.pdf,159-Further Reading,"generators in Chapter 17 . The goal of that last section was to support a\ncustom format, thus fulfilling the promise of a Vector  that could do\neverything a Vector2d  did, and more.\nAs we did in Chapter 1 1 , here we often looked at how standard Python\nobjects behave, to emulate them and provide a “Pythonic” look-and-feel to\nVector .\nIn Chapter 16 , we will implement several infix operators on Vector . The\nmath will be much simpler than that in the angle()  method here, but\nexploring how infix operators work in Python is a great lesson in OO\ndesign. But before we get to operator overloading, we’ll step back from\nworking on one class and look at or ganizing multiple classes with interfaces\nand inheritance, the subjects of Chapters 13  and 14 .\nF u r t h e r  R e a d i n g\nMost special methods covered in the Vector  example also appear in the\nVector2d  example from Chapter 1 1 , so the references in “Further\nReading”  are all relevant here.\nThe powerful reduce  higher -order function is also known as fold,\naccumulate, aggregate, compress, and inject. For more information, see\nW ikipedia’ s “Fold (higher -order function)” article , which presents\napplications of that higher -order function with emphasis on functional\nprogramming with recursive data structures. The article also includes a\ntable listing fold-like functions in dozens of programming languages.\nWhat’ s New in Python 2.5  has a short explanation of __index__ ,\ndesigned to support __getitem__  methods, as we saw in “A Slice-A ware\n__getitem__” . PEP 357—Allowing Any Object to be Used for Slicing\ndetails the need for it from the perspective of an implementor of a C-\nextension—T ravis Oliphant, the primary creator of NumPy . Oliphant’ s\nmany contributions to Python made it a leading scientific computing\nlanguage, which then positioned it to lead the way in machine learning\napplications.\nS O A P B O X\nPr otocols as Informal Interfaces\nProtocols are not an invention of Python. The Smalltalk team, who also\ncoined the expression “object oriented,” used “protocol” as a synonym\nfor what we now call interfaces. Some Smalltalk programming\nenvironments allowed programmers to tag a group of methods as a\nprotocol, but that was merely a documentation and navigation aid, and\nnot enforced by the language. That’ s why I believe “informal interface”\nis a reasonable short explanation for “protocol” when I speak to an\naudience that is more familiar with formal (and compiler enforced)\ninterfaces.\nEstablished protocols naturally evolve in any language that uses\ndynamic typing, that is, when type-checking is done at runtime because\nthere is no static type information in method signatures and variables.\nRuby is another important OO language that has dynamic typing and\nuses protocols.\nIn the Python documentation, you can often tell when a protocol is\nbeing discussed when you see language like “a file-like object.” This is\na quick way of saying “something that behaves suf ficiently like a file,\nby implementing the parts of the file interface that are relevant in the\ncontext.”\nY ou may think that implementing only part of a protocol is sloppy , but\nit has the advantage of keeping things simple. Section 3.3  of the “Data\nModel” chapter suggests:\nWhen implementing a class that emulates any built-in type, it is\nimportant that the emulation only be implemented to the degr ee that\nit makes sense for the object being modeled. For example, some\nsequences may work well with r etrieval of individual elements, but\nextracting a slice may not make sense.\nWhen we don’ t need to code nonsense methods just to fulfill some\nover -designed interface contract and keep the compiler happy , it\nbecomes easier to follow the KISS principle .\nOn the other hand, if you want to use a type checker to verify your\nprotocol implementations, then a stricter definition of protocol is\nrequired. That’ s what typing.Protocol  provides.\nI’ll have more to say about protocols and interfaces in Chapter 13 ,\nwhere they are the main focus.\nOrigins of Duck T yping\nI believe the Ruby community , more than any other , helped popularize\nthe term “duck typing,” as they preached to the Java masses. But the\nexpression has been used in Python discussions before either Ruby or\nPython were “popular .” According to W ikipedia, an early example of\nthe duck analogy in object-oriented programming is a message to the\nPython-list by Alex Martelli from July 26, 2000: polymorphism (was\nRe: T ype checking in python?) . That’ s where the quote at the beginning\nof this chapter came from. If you are curious about the literary origins\nof the “duck typing” term, and the applications of this OO concept in\nmany languages, check out W ikipedia’ s “Duck typing” entry .\nA safe format , with Enhanced Usability\nWhile implementing __format__ , I did not take any precautions\nregarding Vector  instances with a very lar ge number of components,\nas we did in __repr__  using reprlib . The reasoning is that\nrepr()  is for debugging and logging, so it must always generate some\nserviceable output, while __format__  is used to display output to\nend users who presumably want to see the entire Vector . If you think\nthis is dangerous, then it would be cool to implement a further\nextension to the format specifier mini-language.\nHere is how I’d do it: by default, any formatted Vector  would display\na reasonable but limited number of components, say 30. If there are\nmore elements than that, the default behavior would be similar to what\nthe reprlib  does: chop the excess and put ...  in its place. However ,\nif the format specifier ended with the special *  code, meaning “all,”\nthen the size limitation would be disabled. So a user who’ s unaware of\nthe problem of very long displays will not be bitten by it by accident.\nBut if the default limitation becomes a nuisance, then the presence of\nthe ...  could lead the user to search the documentation and discover\nthe *  formatting code.\nThe Sear ch for a Pythonic Sum\nThere’ s no single answer to “What is Pythonic?” just as there’ s no\nsingle answer to “What is beautiful?” Saying, as I often do, that it\nmeans using “idiomatic Python” is not 100% satisfactory , because what\nmay be “idiomatic” for you may not be for me. One thing I know:\n“idiomatic” does not mean using the most obscure language features.\nIn the Python-list , there’ s a thread from April 2003 titled “Pythonic\nW ay to Sum n-th List Element?” . It’ s relevant to our discussion of\nreduce  in this chapter .\nThe original poster , Guy Middleton, asked for an improvement on this\nsolution, stating he did not like to use lambda :\n>>> my_list = [[1, 2, 3], [40, 50, 60], [9, 8, 7]] \n>>> import functools  \n>>> functools .reduce(lambda a, b: a+b, [sub[1] for sub in \nmy_list]) \n60\nThat code uses lots of idioms: lambda , reduce , and a list\ncomprehension. It would probably come last in a popularity contest,\nbecause it of fends people who hate lambda  and those who despise list\ncomprehensions—pretty much both sides of a divide.\nIf you’re going to use lambda , there’ s probably no reason to use a list\ncomprehension—except for filtering, which is not the case here.\nHere is a solution of my own that will please the lambda  lovers:\n>>> functools .reduce(lambda a, b: a + b[1], my_list, 0) \n607\nI did not take part in the original thread, and I wouldn’ t use that in real\ncode, because I don’ t like lambda  too much myself, but I wanted to\nshow an example without a list comprehension.\nThe first answer came from Fernando Perez, creator of IPython,\nhighlighting that NumPy supports n -dimensional arrays and n -\ndimensional slicing:\n>>> import numpy as np \n>>> my_array  = np.array(my_list) \n>>> np.sum(my_array [:, 1]) \n60\nI think Perez’ s solution is cool, but Guy Middleton praised this next\nsolution, by Paul Rubin and Skip Montanaro:\n>>> import operator  \n>>> functools .reduce(operator .add, [sub[1] for sub in \nmy_list], 0) \n60\nThen Evan Simpson asked, “What’ s wrong with this?”:\n>>> total = 0 \n>>> for sub in my_list: \n...     total += sub[1] \n... \n>>> total \n60\nLots of people agreed that was quite Pythonic. Alex Martelli went as far\nas saying that’ s probably how Guido would code it.\nI like Evan Simpson’ s code but I also like David Eppstein’ s comment\non it:\nIf you want the sum of a list of items, you should write it in a way that\nlooks like “the sum of a list of items”, not in a way that looks like\n“loop over these items, maintain another variable t, perform a\nsequence of additions”. Why do we have high level languages if not\nto expr ess our intentions at a higher level and let the language worry\nabout what low-level operations ar e needed to implement it?\nThen Alex Martelli comes back to suggest:\n“The sum” is so fr equently needed that I wouldn’ t mind at all if\nPython singled it out as a built-in. But “r educe(operator .add, …” just\nisn’ t a gr eat way to expr ess it, in my opinion (and yet as an old\nAPL ’er , and FP-liker , I should  like it—but I don’ t).\nAlex goes on to suggest a sum()  function, which he contributed. It\nbecame a built-in in Python 2.3, released only three months after that\nconversation took place. So Alex’ s preferred syntax became the norm:\n>>> sum([sub[1] for sub in my_list]) \n60\nBy the end of the next year (November 2004), Python 2.4 was launched\nwith generator expressions, providing what is now in my opinion the\nmost Pythonic answer to Guy Middleton’ s original question:\n>>> sum(sub[1] for sub in my_list) \n60\nThis is not only more readable than reduce  but also avoids the trap of\nthe empty sequence: sum([])  is 0 , simple as that.\nIn the same conversation, Alex Martelli suggests the reduce  built-in\nin Python 2 was more trouble than it was worth, because it encouraged\ncoding idioms that were hard to explain. He was most convincing: the\nfunction was demoted to the functools  module in Python 3.\nStill, functools.reduce  has its place. It solved the problem of our\nVector.__hash__  in a way that I would call Pythonic.\n1  The iter()  function is covered in Chapter 17 , along with the __iter__  method.\n2  Attribute lookup is more complicated than this; we’ll see the gory details in [Link to Come].\nFor now , this simplified explanation will do.\n3  Although __match_args__  exists to support pattern matching in Python 3.10, setting this\nattribute is harmless in previous versions of Python. In the First Edition , I named it\nshortcut_names . W ith the new name it does double duty: it supports positional patterns in\ncase  clauses, and it holds the names of the dynamic attributes supported by special logic in\n__getattr__  and __setattr__ .\n4  The sum , any , and all  cover the most common uses of reduce . See the discussion in\n“Modern Replacements for map, filter , and reduce” .\n5  W e’ll seriously consider the matter of Vector([1, 2]) == (1, 2)  in “Operator\nOverloading 101” .\n6  The W olfram Mathworld site has an article on Hypersphere ; on W ikipedia, “hypersphere”\nredirects to the "" n -sphere” entry .\n7  I adapted the code for this presentation: in 2003, reduce  was a built-in, but in Python 3 we\nneed to import it; also, I replaced the names x  and y  with my_list  and sub , for sub-list.",11337
160-The Typing Map.pdf,160-The Typing Map,"Chapter 13. Interfaces,\nProtocols, and ABCs\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 13th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nPr ogram to an interface, not an implementation.\n— Gamma, Helm, Johnson, Vlissides, First Principle of\nObject-Oriented Design\nObject-oriented programming is all about interfaces. The best approach to\nunderstanding a type in Python is knowing the methods it provides—its\ninterface—as discussed in “T ypes are defined by supported operations”\n( Chapter 8 ).\nDepending on the programming language, we have one or more ways of\ndefining and using interfaces. Since Python 3.8, we have four ways. They\nare depicted in the T yping Map  ( Figure 13-1 ). W e can summarize them like\nthis:\nduck typing1\nPython’ s default approach to typing from the beginning. W e’ve been\nstudying duck typing since Chapter 1 .\ngoose typing\nThe approach supported by Abstract Base Classes (ABCs) since Python\n2.6, which relies on runtime checks of objects against ABCs. Goose\ntyping  is a major subject in this chapter .\nstatic typing\nT raditional approach of statically-typed languages like C and Java;\nsupported since Python 3.5 by the typing  module, and enforced by\nexternal type checkers compliant with PEP 484—T ype Hints . This is\nnot the theme of this chapter . Most of Chapter 8  and the upcoming\nChapter 15  are about static typing.\nstatic duck typing\nAn approach made popular by the Go language; supported by\nsubclasses of typing.Protocol —new in Python 3.8—also\nenforced by external type checkers. W e first saw this in “Static\nProtocols”  ( Chapter 8 ).\nT h e  T y p i n g  M a p",2149
161-Two kinds of protocols.pdf,161-Two kinds of protocols,"Figur e 13-1. The top half describes runtime type checking appr oaches using just the Python\ninterpr eter; the bottom r equir es an external static type checker such as MyPy or an IDE like\nPyCharm. The left quadrants cover typing based on the object’ s structur e— i.e. the methods pr ovided\nby the object, r egar dless of the name of its class or super classes; the right quadrants depend on\nobjects having explicitly named types: the name of the object’ s class, or the name of its super classes.\nThese four typing approaches are complementary: they have dif ferent pros\nand cons. It doesn’ t make sense to dismiss any of them.\nEach of these four approaches rely on interfaces to work, but static typing\ncan be done—poorly—using only concrete types instead of interface\nabstractions like protocols and Abstract Base Classes. This chapter is about\nduck typing, goose typing and static duck typing—typing disciplines that\nrevolve around interfaces.\nThis chapter is split in four top sections, addressing three of the four\nquadrants in the T yping Map  ( Figure 13-1 ):\n“T wo kinds of protocols”  compares the two forms of structural\ntyping with protocols—i.e. the left-hand side of the T yping Map .\n“Programming ducks”  dives deeper into Python’ s usual duck\ntyping, including how to make it safer while preserving its major\nstrength: flexibility .\n“Goose typing”  explains the use of ABCs for stricter runtime type\nchecking. This is the longest section, not because it’ s more\nimportant, but because there are more sections about duck typing ,\nstatic duck typing , and static typing  elsewhere in the book.\n“Static protocols”  covers usage, implementation and design of\ntyping.Protocol  subclasses—useful for static and runtime\ntype checking.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThis chapter was heavily edited and is about 24% longer than the\ncorresponding Chapter 1 1  in Fluent Python, First Edition . Although some\nsections and many paragraphs are the same, there’ s a lot of new content.\nThese are the highlights:\nThe chapter introduction and the T yping Map  ( Figure 13-1 ) are\nnew . That’ s the key to most new content in this chapter—and all\nother chapters related to typing in Python ≥ 3.8.\n“T wo kinds of protocols”  explains the similarities and dif ferences\nbetween dynamic and static protocols.\n“Defensive programming and “fail fast””  mostly reproduces\ncontent from the First Edition , but was updated and now has a\nsection title to highlight its importance.\n“Static protocols”  is all new . It builds on the initial presentation in\n“Static Protocols”  ( Chapter 8 ).\nI updated the UML class diagrams of collections.abc  in\nFigure 13-2 , Figure 13-3 , and Figure 13-4  to include the\nCollection  ABC added in Python 3.6.\nFluent Python, First Edition  had a section encouraging use of the\nnumbers  ABCs for goose typing. In “The numbers ABCs and numeric\nprotocols” , I explain why you should use numeric static protocols from the\ntyping  module instead, if you plan to use static type checkers as well as\nruntime checks in the style of goose typing.\nT w o  k i n d s  o f  p r o t o c o l s\nThe word pr otocol  has dif ferent meanings in computer science depending\non context. A network protocol such as HTTP specifies commands that a\nclient can send to a server , such as GET , PUT , and HEAD . W e saw in\n“Protocols and Duck T yping”  that an object protocol specifies methods\nwhich an object must provide to fulfill a role. The FrenchDeck  example\nin Chapter 1  was demonstrated one object protocol, the sequence protocol:\nthe methods that allow a Python object to behave as a sequence.\nImplementing a full protocol may require several methods, but often it is\nOK to implement only part of it. Consider this Vowels  class:\nExample 13-1. Partial sequence pr otocol implementation with __getitem__.\n>>> class Vowels: \n...     def __getitem__ (self, i): \n...         return 'AEIOU'[i] \n... \n>>> v = Vowels() \n>>> v[0] \n'A' \n>>> v[-1] \n'U' \n>>> for c in v: print(c) \n... \nA \nE \nI \nO \nU \n>>> 'E' in v \nTrue \n>>> 'Z' in v \nFalse\nImplementing __getitem__  is enough to allow retrieving items by\nindex, and also to support iteration and the in  operator . The\n__getitem__  special method is really the key to the sequence protocol.\nT ake a look at this entry from the Python/C API Reference Manual , section\nSequence Protocol :\nint PySequence_Check(PyObject *o)\nReturn 1  if the object provides sequence protocol, and 0  otherwise.\nNote that it returns 1  for Python classes with a __getitem__()\nmethod unless they are dict  subclasses […]\nW e expect a sequence to also support len() , by implementing __len__ .\nVowels  has no __len__  method, but it still behaves as a sequence in\nsome contexts. And that may be enough for our purposes. That is why I like\nto say that a protocol is an “informal interface”. That is also how protocols\nare understood in Smalltalk, the first Object-Oriented programming\nenvironment to use that term.\nExcept in pages about network programming, most uses of the word\n“protocol” in the Python documentation refer to these informal interfaces.\nNow , with the adoption of PEP 544—Protocols: Structural subtyping (static\nduck typing)  in Python 3.8, the word “protocol” has another meaning in\nPython—closely related, but dif ferent. As we saw in “Static Protocols”\n( Chapter 8 ), PEP 544 allows us to create subclasses of\ntyping.Protocol  to define one or more methods that a class must\nimplement (or inherit) to satisfy a static type checker .\nWhen I need to be specific, I will adopt these terms:\ndynamic pr otocol\nThe informal protocols Python always had. Dynamic protocols are\nimplicit, defined by convention and described in the documentation.\nPython’ s most important dynamic protocols are supported by the\ninterpreter itself, and are documented in the “Data Model” chapter  of\nThe Python Language Refer ence .\nstatic pr otocol\nA protocol as defined by PEP 544—Protocols: Structural subtyping\n(static duck typing) , since Python 3.8. A static protocol has an explicit\ndefinition: a typing.Protocol  subclass.\nThere are two key dif ferences between them:\n1 . An object may implement only part of a dynamic protocol and still\nbe useful; but to fulfill a static protocol, the object must provide\nevery method declared in the protocol class, even if your program\ndoesn’ t need them all.\n2 . Static protocols can be verified by static type checkers, but\ndynamic protocols can’ t.",6549
162-Programming ducks.pdf,162-Programming ducks,,0
163-Python Digs Sequences.pdf,163-Python Digs Sequences,"Both kinds of protocols share the essential characteristic that a class never\nneeds to declare that it supports a protocol by name, i.e. by inheritance.\nIn addition to static protocols, Python provides another way of defining an\nexplicit interface in code: an Abstract Base Class (ABC).\nThe rest of this chapter covers dynamic and static protocols, as well as\nABCs.\nP r o g r a m m i n g  d u c k s\nLet’ s start our discussion of dynamic protocols with two of the most\nimportant in Python: the sequence and iterable protocols. The interpreter\ngoes out of its way to handle objects that provide even a minimal\nimplementation of those protocols, as the next section explains.\nPython Digs Sequences\nThe philosophy of the Python Data Model is to cooperate with essential\ndynamic protocols as much as possible. When it comes to sequences,\nPython tries hard to work with even the simplest implementations.\nFigure 13-2  shows how the Sequence  interface is formalized as an ABC.\nThe Python interpreter and built-in sequences like list , str  etc. do not\nrely on that ABC at all. I am using it only to describe what a full-fledged\nSequence  is expected to support.\nFigur e 13-2. UML class diagram for the Sequence ABC and r elated abstract classes fr om\ncollections.abc. Inheritance arr ows point fr om subclass to its super classes. Names in italic ar e\nabstract methods. Befor e Python 3.6, ther e was no Collection  ABC—Sequence  was a dir ect\nsubclass of Container , Iterable , and Sized .\nT I P\nMost ABCs in the collections.abc  module exist to formalize interfaces that are\nimplemented by built-in objects and are implicitly supported by the interpreter—both of\nwhich predate the ABCs themselves. The ABCs are useful as starting points for new\nclasses, and to support explicit type checking at runtime (a.k.a. goose typing ) as well as\ntype hints for static type checkers.\nStudying Figure 13-2 , we see that a correct subclass of Sequence  must\nimplement __getitem__  and __len__  (from Sized ). All the other\nmethods in Sequence  are concrete, so subclasses can inherit their\nimplementations—or provide better ones.\nNow , recall the Vowels  class in Example 13-1 . It does not inherit from\nabc.Sequence  and it only implements __getitem__ .\nThere is no __iter__  method, yet Vowels  instances are iterable because\n—as a fallback—if Python finds a __getitem__  method, it tries to\niterate over the object by calling that method with integer indexes starting\nwith 0 . Because Python is smart enough to iterate over Vowels  instances,\nit can also make the in  operator work even when the __contains__\nmethod is missing: it does a sequential scan to check if an item is present.\nIn summary , given the importance of sequence-like data structures, Python\nmanages to make iteration and the in  operator work by invoking\n__getitem__  when __iter__  and __contains__  are unavailable.\nThe original FrenchDeck  from Chapter 1  does not subclass\nabc.Sequence  either , but it does implement both methods of the\nsequence protocol: __getitem__  and __len__ . See Example 13-2 .\nExample 13-2. A deck as a sequence of car ds (same as Example 1-1 )\nimport collections  \n \nCard = collections .namedtuple ('Card', ['rank', 'suit']) \n \nclass FrenchDeck : \n    ranks = [str(n) for n in range(2, 11)] + list('JQKA') \n    suits = 'spades diamonds clubs hearts' .split() \n \n    def __init__ (self): \n        self._cards = [Card(rank, suit) for suit in self.suits \n                                        for rank in self.ranks] \n \n    def __len__(self): \n        return len(self._cards) \n \n    def __getitem__ (self, position ): \n        return self._cards[position ]\nSeveral of the examples in Chapter 1  work because of the special treatment\nPython gives to anything vaguely resembling a sequence. The iterable",3855
164-Monkey-Patching Implementing a Protocol at Runtime.pdf,164-Monkey-Patching Implementing a Protocol at Runtime,"protocol in Python represents an extreme form of duck typing: the\ninterpreter tries two dif ferent methods to iterate over objects.\nT o be clear: the behaviors I described in this section are implemented in the\ninterpreter itself, mostly in C. They do not depend on methods from the\nSequence  ABC. For example, the concrete methods __iter__  and\n__contains__  in the Sequence  class emulate the built-in behaviors of\nthe Python interpreter . If you are curious, check source code of these\nmethods in Lib/_collections_abc.py .\nNow let’ s study another example emphasizing the dynamic nature of\nprotocols—and why static type checkers have no chance of dealing with\nthem.\nMonkey-Patching: Implementing a Protocol at Runtime\nN O T E\nMonkey patching is dynamically changing a module, class, or function at runtime, to\nadd features or fix bugs.  Because it does not change the source code like a regular\npatch, a monkey patch only af fects the currently running instance of the program. The\ngevent  networking library monkey patches parts of Python’ s standard library to allow\nlightweight concurrency without threads or async/await . Be aware that monkey\npatches depend on implementation details of the patched code, so they can easily break\nwhen libraries are updated.\nThe FrenchDeck  class from Example 13-2  is missing an essential\nfeature: it cannot be shuf fled. Y ears ago when I first wrote the\nFrenchDeck  example I did implement a shuffle  method. Later I had a\nPythonic insight: if a FrenchDeck  acts like a sequence, then it doesn’ t\nneed its own shuffle  method because there is already\nrandom.shuffle , documented  as “Shuf fle the sequence x  in place.”\nThe standard random.shuffle  function is used like this:\n>>> from random import shuffle \n>>> l = list(range(10)) 2\n>>> shuffle(l) \n>>> l \n[5, 2, 9, 7, 8, 3, 1, 4, 0, 6]\nT I P\nWhen you follow established protocols, you improve your chances of leveraging\nexisting standard library and third-party code, thanks to duck typing.\nHowever , if we try to shuf fle a FrenchDeck  instance, we get an\nexception, as in Example 13-3 .\nExample 13-3. random.shuffle cannot handle Fr enchDeck\n>>> from random import shuffle \n>>> from frenchdeck  import FrenchDeck  \n>>> deck = FrenchDeck () \n>>> shuffle(deck) \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \n  File "".../random.py"" , line 265, in shuffle \n    x[i], x[j] = x[j], x[i] \nTypeError : 'FrenchDeck' object does not support item assignment\nThe error message is clear: “ 'FrenchDeck'  object does not support item\nassignment.” The problem is that shuffle  operates in place , by swapping\nitems inside the collection, and FrenchDeck  only implements the\nimmutable  sequence protocol. Mutable sequences must also provide a\n__setitem__  method.\nBecause Python is dynamic, we can fix this at runtime, even at the\ninteractive console. Example 13-4  shows how to do it.\nExample 13-4. Monkey patching Fr enchDeck to make it mutable and\ncompatible with random.shuffle (continuing fr om Example 13-3 )\n>>> def set_card (deck, position , card):  \n \n...     deck._cards[position ] = card \n... \n>>> FrenchDeck .__setitem__  = set_card   \n \n>>> shuffle(deck)  \n \n>>> deck[:5] \n[Card(rank='3', suit='hearts'), Card(rank='4', suit='diamonds'),  \nCard(rank='4',  \nsuit='clubs'), Card(rank='7', suit='hearts'), Card(rank='9',  \nsuit='spades')]\nCreate a function that takes deck , position , and card  as\nar guments.\nAssign that function to an attribute named __setitem__  in the\nFrenchDeck  class.\ndeck  can now be shuf fled because I added the necessary method of the\nmutable sequence protocol.\nThe signature of the __setitem__  special method is defined in The\nPython Language Refer ence  in “3.3.6. Emulating container types” . Here I\nnamed the ar guments deck, position, card —and not self,\nkey, value  as in the language reference—to show that every Python\nmethod starts life as a plain function, and naming the first ar gument self\nis merely a convention. This is OK in a console session, but in a Python\nsource file it’ s much better to use self , key , and value  as documented.\nThe trick is that set_card  knows that the deck  object has an attribute\nnamed _cards , and _cards  must be a mutable sequence. The\nset_card  function is then attached to the FrenchDeck  class as the\n__setitem__  special method. This is an example of monkey patching :\nchanging a class or module at runtime, without touching the source code.\nMonkey patching is powerful, but the code that does the actual patching is\nvery tightly coupled with the program to be patched, often handling private\nand undocumented attributes.\nBesides being an example of monkey patching, Example 13-4  highlights the\ndynamic nature of protocols in dynamic duck typing: random.shuffle\ndoesn’ t care about the class of the ar gument, it only needs the object to\nimplement methods from the mutable sequence protocol. It doesn’ t even\nmatter if the object was “born” with the necessary methods or if they were\nsomehow acquired later .",5110
165-Defensive programming and fail fast.pdf,165-Defensive programming and fail fast,"Duck typing doesn’ t need to be wildly unsafe or hard to debug. The next\nsection shows some useful code patterns to detect dynamic protocols\nwithout resorting to explicit checks.\nDefensive programming and “fail fast”\nDefensive programming is like defensive driving: a set of practices to\nenhance safety even when faced with careless programmers—or drivers.\nMany bugs cannot be caught except at runtime—even in mainstream\nstatically typed languages.  In a dynamically typed language, “fail fast” is\nexcellent advice for safer and easier to maintain programs. Failing fast\nmeans raising runtime errors as soon as possible, for example, rejecting\ninvalid ar guments right a the beginning of a function body .\nHere is one example: when you write code that accepts a sequence of items\nto process internally as a list , don’ t enforce a list  ar gument by type\nchecking. Instead, take the ar gument and immediately build a list  from it.\nOne example of this code pattern is the __init__  method in Example 13-\n10 , later in this chapter:\n    def __init__ (self, iterable ): \n        self._balls = list(iterable )\nThat way you make your code more flexible, because the list()\nconstructor handles any iterable that fits in memory . If the ar gument is not\niterable, the call will fail fast with a very clear TypeError  exception,\nright when the object is initialized. If you want to be more explict, you can\nwrap the list()  call with try/except  to customize the error message\n—but I’d use that extra code only on an external API, because the problem\nwould be easy to see for maintainers of the codebase. Either way , the\nof fending call will appear near the end of the traceback, making it\nstraightforward to fix. If you don’ t catch the invalid ar gument in the class\nconstructor , the program will blow up later , when some other method of the\nclass needs to operate on self._balls  and it is not a list . Then the\nroot cause will be harder to find.3\nOf course, calling list()  on the ar gument would be bad if the data\nshouldn’ t be copied, either because it’ s too lar ge or because the function, by\ndesign, needs to change it in place for the benefit of the caller , like\nrandom.shuffle  does. In that case, a runtime check like\nisinstance(x, abc.MutableSequence)  would be the way to go.\nIf you are afraid to get an infinite generator—not a common issue—you can\nbegin by calling len()  on the ar gument. This would reject iterators, while\nsafely dealing with tuples, arrays, and other existing or future classes that\nfully implement the Sequence  interface. Calling len()  is usually very\ncheap and an invalid ar gument will raise an error immediately .\nOn the other hand, if any iterable is acceptable, then call iter(x)  as soon\nas possible to obtain an iterator , as we’ll see in “Why Sequences Are\nIterable: The iter  Function” . Again, if x  is not iterable this will fail fast\nwith an easy to debug exception.\nIn the cases I just described, a type hint could catch some problems earlier ,\nbut not all problems. Recall that the type Any  is consistent-with  every other\ntype. T ype inference may cause a variable to be tagged with the Any  type.\nWhen that happens, the type checker is in the dark. In addition, type hints\nare not enforced at runtime. Fail fast is the last line of defense.\nDefensive code leveraging duck types can also include logic to handle\ndif ferent types without using isinstance()  or hasattr()  tests.\nOne example is how we might emulate the handling of the field_names\nar gument in collections.namedtuple : field_names  accepts a\nsingle string with identifiers separated by spaces or commas, or a sequence\nof identifiers. Example 13-5  shows how I’d do it using duck typing.\nExample 13-5. Duck typing to handle a string or an iterable of strings\n    try:  \n \n        field_names  = field_names .replace(',', ' ').split()  \n \n    except AttributeError :  \n \n        pass  \n \n    field_names  = tuple(field_names )  \n \n    if not all(s.isidentifier () for s in field_names ):  \n \n        raise ValueError ('field_names must all be valid  \nidentifiers ')\nAssume it’ s a string (EAFP = it’ s easier to ask for giveness than\npermission).\nConvert commas to spaces and split the result into a list of names.\nSorry , field_names  doesn’ t quack like a str : it has no .replace ,\nor it returns something we can’ t .split .\nIf AttributeError  was raised, then field_names  is not a str\nand we assume it was already an iterable of names.\nT o make sure it’ s an iterable and to keep our own copy , create a tuple\nout of what we have. A tuple  is more compact than list , and it also\nprevents my code from changing the names by mistake.\nUse str.isidentifier  to ensure every name is a valid.\nExample 13-5  shows one situation where duck typing is more expressive\nthan static type hints. There is no way to spell a type hint that says\n""field_names  must be a string of identifiers separated by spaces or\ncommas”. This is the relevant part of the namedtuple  signature on\ntypeshed: (see full source at stdlib/3/collections/ init .pyi ):\n    def namedtuple ( \n        typename : str, \n        field_names : Union[str, Iterable [str]], \n        *, \n        # rest of signature omitted\nAs you can see, field_names  is annotated as Union[str,\nIterable[str]]  which is OK as far as it goes, but is not enough to\ncatch all possible problems.",5443
166-Goose typing.pdf,166-Goose typing,"After reviewing dynamic protocols, we move to a more explicit form of\nruntime type checking: goose typing.\nG o o s e  t y p i n g\nAn abstract class r epr esents an interface.\n— Bjarne Stroustrup, Creator of C++\nPython doesn’ t have an interface  keyword. W e use Abstract Base\nClasses (ABCs) to define explicit interfaces.\nThe Python Glossary  entry for abstract base class  has a good explanation of\nthe value they bring to duck-typed languages:\nabstract base class\nAbstract base classes complement duck-typing by providing a way to\ndefine interfaces when other techniques like hasattr()  would be\nclumsy or subtly wrong (for example with magic methods). ABCs\nintroduce virtual subclasses, which are classes that don’ t inherit from a\nclass but are still recognized by isinstance()  and\nissubclass() ; see the abc  module documentation.\nGoose typing is a runtime type checking approach that leverages ABCs. I\nwill let Alex Martelli explain in “W aterfowl and ABCs” .\nN O T E\nI am very grateful to my friends Alex Martelli and Anna Ravenscroft. I showed them the\nfirst outline of Fluent Python  at OSCON 2013 and they encouraged me to submit it for\npublication with O’Reilly . Both later contributed with thorough tech reviews. Alex was\nalready the most cited person in this book, and then he of fered to write this essay . T ake\nit away , Alex!4\n5\nW A T E R F O W L  A N D  A B C S\nBy Alex Martelli\nI’ve been credited on W ikipedia  for helping spread the helpful meme\nand sound-bite “ duck typing ” (i.e, ignoring an object’ s actual type,\nfocusing instead on ensuring that the object implements the method\nnames, signatures, and semantics required for its intended use).\nIn Python, this mostly boils down to avoiding the use of isinstance\nto check the object’ s type (not to mention the even worse approach of\nchecking, for example, whether type(foo) is bar —which is\nrightly anathema as it inhibits even the simplest forms of inheritance!).\nThe overall duck typing  approach remains quite useful in many contexts\n—and yet, in many others, an often preferable one has evolved over\ntime. And herein lies a tale…\nIn recent generations, the taxonomy of genus and species (including but\nnot limited to the family of waterfowl known as Anatidae) has mostly\nbeen driven by phenetics —an approach focused on similarities of\nmorphology and behavior… chiefly , observable  traits. The analogy to\n“duck typing” was strong.\nHowever , parallel evolution can often produce similar traits, both\nmorphological and behavioral ones, among species that are actually\nunrelated, but just happened to evolve in similar , though separate,\necological niches. Similar “accidental similarities” happen in\nprogramming, too—for example, consider the classic OOP example:\nclass Artist: \n    def draw(self): ... \n \nclass Gunslinger : \n    def draw(self): ... \n \nclass Lottery: \n    def draw(self): ...\nClearly , the mere existence of a method named draw , callable without\nar guments, is far from suf ficient to assure us that two objects x  and y\nsuch that x.draw()  and y.draw()  can be called are in any way\nexchangeable or abstractly equivalent—nothing about the similarity of\nthe semantics resulting from such calls can be inferred. Rather , we need\na knowledgeable programmer to somehow positively assert  that such an\nequivalence holds at some level!\nIn biology (and other disciplines) this issue has led to the emer gence\n(and, on many facets, the dominance) of an approach that’ s an\nalternative to phenetics, known as cladistics —focusing taxonomical\nchoices on characteristics that are inherited from common ancestors,\nrather than ones that are independently evolved. (Cheap and rapid DNA\nsequencing can make cladistics highly practical in many more cases, in\nrecent years.)\nFor example, sheldgeese (once classified as being closer to other geese)\nand shelducks (once classified as being closer to other ducks) are now\ngrouped together within the subfamily T adornidae (implying they’re\ncloser to each other than to any other Anatidae, as they share a closer\ncommon ancestor). Furthermore, DNA analysis has shown, in\nparticular , that the white-winged wood duck is not as close to the\nMuscovy duck (the latter being a shelduck) as similarity in looks and\nbehavior had long suggested—so the wood duck was reclassified into\nits own genus, and entirely out of the subfamily!\nDoes this matter? It depends on the context! For such purposes as\ndeciding how best to cook a waterfowl once you’ve bagged it, for\nexample, specific observable traits (not all of them—plumage, for\nexample, is de minimis in such a context), mostly texture and flavor\n(old-fashioned phenetics!), may be far more relevant than cladistics.\nBut for other issues, such as susceptibility to dif ferent pathogens\n(whether you’re trying to raise waterfowl in captivity , or preserve them\nin the wild), DNA closeness can matter much more…\nSo, by very loose analogy with these taxonomic revolutions in the\nworld of waterfowls, I’m recommending supplementing (not entirely\nreplacing—in certain contexts it shall still serve) good old duck typing\nwith… goose typing !\nWhat goose typing  means is: isinstance(obj, cls)  is now just\nfine… as long as cls  is an abstract base class—in other words, cls ’ s\nmetaclass is abc.ABCMeta .\nY ou can find many useful existing abstract classes in\ncollections.abc  (and additional ones in the numbers  module of\nThe Python Standar d Library ).\nAmong the many conceptual advantages of ABCs over concrete classes\n(e.g., Scott Meyer ’ s “all non-leaf classes should be abstract”—see Item\n33  in his book, Mor e Effective C++ ), Python’ s ABCs add one major\npractical advantage: the register  class method, which lets end-user\ncode “declare” that a certain class becomes a “virtual” subclass of an\nABC (for this purpose the registered class must meet the ABC’ s method\nname and signature requirements, and more importantly the underlying\nsemantic contract—but it need not have been developed with any\nawareness of the ABC, and in particular need not inherit from it!). This\ngoes a long way toward breaking the rigidity and strong coupling that\nmake inheritance something to use with much more caution than\ntypically practiced by most OOP programmers…\nSometimes you don’ t even need to register a class for an ABC to\nrecognize it as a subclass!\nThat’ s the case for the ABCs whose essence boils down to a few special\nmethods. For example:\n>>> class Struggle : \n...     def __len__(self): return 23 \n... \n>>> from collections  import abc \n>>> isinstance (Struggle (), abc.Sized) \nTrue6\nAs you see, abc.Sized  recognizes Struggle  as “a subclass,” with\nno need for registration, as implementing the special method named\n__len__  is all it takes (it’ s supposed to be implemented with the\nproper syntax—callable without ar guments—and semantics—returning\na nonnegative integer denoting an object’ s “length”; any code that\nimplements a specially named method, such as __len__ , with\narbitrary , non-compliant syntax and semantics has much worse\nproblems anyway).\nSo, here’ s my valediction: whenever you’re implementing a class\nembodying any of the concepts represented in the ABCs in numbers ,\ncollections.abc , or other framework you may be using, be sure\n(if needed) to subclass it from, or register it into, the corresponding\nABC. At the start of your programs using some library or framework\ndefining classes which have omitted to do that, perform the registrations\nyourself; then, when you must check for (most typically) an ar gument\nbeing, e.g, “a sequence,” check whether:\nisinstance (the_arg, collections .abc.Sequence )\nAnd, don’ t  define custom ABCs (or metaclasses) in production code…\nif you feel the ur ge to do so, I’d bet it’ s likely to be a case of “all\nproblems look like a nail”-syndrome for somebody who just got a shiny\nnew hammer—you (and future maintainers of your code) will be much\nhappier sticking with straightforward and simple code, eschewing such\ndepths. V alē!\nT o summarize, goose typing  entails:\nSubclassing from ABCs to make it explict that you are\nimplementing a previously defined interface.\nRuntime type checking using ABCs instead of concrete classes as\nthe second ar gument for isinstance  and issubclass .\nAlex makes the point that inheriting from an ABC is more than\nimplementing the required methods: it’ s also a clear declaration of intent by\nthe developer . That intent can also be made explicit through registering a\nvirtual subclass.\nN O T E\nDetails of using register  are covered in “A V irtual Subclass of an ABC” , later in this\nchapter . For now , here is a brief example: given the FrenchDeck  class, if I want it to\npass a check like issubclass(FrenchDeck, Sequence) , I can make it a virtual\nsubclass  of the Sequence  ABC with these lines:\nfrom collections.abc  import Sequence  \nSequence .register (FrenchDeck )\nThe use of isinstance  and issubclass  becomes more acceptable if\nyou are checking against ABCs instead of concrete classes. If used with\nconcrete classes, type checks limit polymorphism—an essential feature of\nobject oriented programming. But with ABCs these tests are more flexible.\nAfter all, if a component does not implement an ABC by subclassing—but\ndoes implement the required methods— it can always be registered after the\nfact so it passes those explicit type checks.\nHowever , even with ABCs, you should beware that excessive use of\nisinstance  checks may be a code smell —a symptom of bad OO design.\nIt’ s usually not  OK to have a chain of if/elif/elif  with\nisinstance  checks performing dif ferent actions depending on the type\nof an object: you should be using polymorphism for that—i.e., design your\nclasses so that the interpreter dispatches calls to the proper methods, instead\nof you hardcoding the dispatch logic in if/elif/elif  blocks.\nOn the other hand, it’ s OK to perform an isinstance  check against an\nABC if you must enforce an API contract: “Dude, you have to implement\nthis if you want to call me,” as technical reviewer Lennart Regebro put it.",10193
167-Subclassing an ABC.pdf,167-Subclassing an ABC,"That’ s particularly useful in systems that have a plug-in architecture.\nOutside of frameworks, duck typing is often simpler and more flexible than\ntype checks.\nFinally , in his essay , Alex reinforces more than once the need for restraint in\nthe creation of ABCs. Excessive use of ABCs would impose ceremony in a\nlanguage that became popular because it is practical and pragmatic. During\nthe Fluent Python  review process, Alex wrote in an e-mail:\nABCs ar e meant to encapsulate very general concepts, abstractions,\nintr oduced by a framework—things like “a sequence” and “an exact\nnumber .” [Readers] most likely don’ t need to write any new ABCs, just\nuse existing ones corr ectly , to get 99.9% of the benefits without serious\nrisk of misdesign.\nNow let’ s see goose typing in practice.\nSubclassing an ABC\nFollowing Martelli’ s advice, we’ll leverage an existing ABC,\ncollections.MutableSequence , before daring to invent our own.\nIn Example 13-6 , FrenchDeck2  is explicitly declared a subclass of\ncollections.MutableSequence .\nExample 13-6. fr enchdeck2.py: Fr enchDeck2, a subclass of\ncollections.MutableSequence\nimport collections  \nfrom collections .abc import MutableSequence  \n \nCard = collections .namedtuple ('Card', ['rank', 'suit']) \n \nclass FrenchDeck2 (MutableSequence ): \n    ranks = [str(n) for n in range(2, 11)] + list('JQKA') \n    suits = 'spades diamonds clubs hearts '.split() \n \n    def __init__ (self): \n        self._cards = [Card(rank, suit) for suit in self.suits \n                                        for rank in self.ranks] \n \n    def __len__(self): \n        return len(self._cards) \n \n    def __getitem__ (self, position ): \n        return self._cards[position ] \n \n    def __setitem__ (self, position , value):  \n \n        self._cards[position ] = value \n \n    def __delitem__ (self, position ):  \n \n        del self._cards[position ] \n \n    def insert(self, position , value):  \n \n        self._cards.insert(position , value)\n__setitem__  is all we need to enable shuf fling…\nBut subclassing MutableSequence  forces us to implement\n__delitem__ , an abstract method of that ABC.\nW e are also required to implement insert , the third abstract method\nof MutableSequence .\nPython does not check for the implementation of the abstract methods at\nimport time (when the fr enchdeck2.py  module is loaded and compiled), but\nonly at runtime when we actually try to instantiate FrenchDeck2 . Then,\nif we fail to implement any of the abstract methods, we get a TypeError\nexception with a message such as ""Can't instantiate abstract\nclass FrenchDeck2 with abstract methods\n__delitem__, insert"" . That’ s why we must implement\n__delitem__  and insert , even if our FrenchDeck2  examples do\nnot need those behaviors: the MutableSequence  ABC demands them.\nAs Figure 13-3  shows, not all methods of the Sequence  and\nMutableSequence  ABCs are abstract.\nFigur e 13-3. UML class diagram for the MutableSequence ABC and its super classes fr om\ncollections.abc (inheritance arr ows point fr om subclasses to ancestors; names in italic ar e abstract\nclasses and abstract methods)\nT o write FrenchDeck2  as a subclass of MutableSequence , I had to\npay the price of implementing __delitem__  and insert , which my\nexamples did not require. In return, FrenchDeck2  inherits five concrete\nmethods from Sequence : __contains__ , __iter__ ,\n__reversed__ , index , and count . From MutableSequence , it\ngets another six methods: append , reverse , extend , pop , remove ,",3555
168-ABCs in the Standard Library.pdf,168-ABCs in the Standard Library,"and __iadd__ —which supports the +=  operator for in-place\nconcatenation.\nThe concrete methods in each collections.abc  ABC are implemented\nin terms of the public interface of the class, so they work without any\nknowledge of the internal structure of instances.\nT I P\nAs the coder of a concrete subclass, you may be able to override methods inherited from\nABCs with more ef ficient implementations. For example, __contains__  works by\ndoing a sequential scan of the sequence, but if your concrete sequence keeps its items\nsorted, you can write a faster __contains__  that does a binary search using bisect\nfunction (see [Link to Come]).\nT o use ABCs well, you need to know what’ s available. W e’ll review the\ncollections ABCs next.\nABCs in the Standard Library\nSince Python 2.6, the standard library provides several ABCs. Most are\ndefined in the collections.abc  module, but there are others. Y ou can\nfind ABCs in the io  and numbers  packages, for example. But the most\nwidely used are in collections.abc .\nT I P\nThere are two modules named abc  in the standard library . Here we are talking about\ncollections.abc . T o reduce loading time, since Python 3.4 that module is\nimplemented outside of the collections  package—in Lib/_collections_abc.py —so\nit’ s imported separately from collections . The other abc  module is just abc  (i.e.,\nLib/abc.py ) where the abc.ABC  class is defined. Every ABC depends on the abc\nmodule, but we don’ t need to import it ourselves except to create a brand-new ABC.\nFigure 13-4  is a summary UML class diagram (without attribute names) of\n17 ABCs defined in collections.abc . The documentation of\ncollections.abc  has a nice table  summarizing the ABCs, their\nrelationships, and their abstract and concrete methods (called “mixin\nmethods”). There is plenty of multiple inheritance going on in Figure 13-4 .\nW e’ll devote most of Chapter 14  to multiple inheritance, but for now it’ s\nenough to say that it is usually not a problem when ABCs are concerned.7\nFigur e 13-4. UML class diagram for ABCs in collections.abc\nLet’ s review the clusters in Figure 13-4 :\nIterable , Container , Sized\nEvery collection should either inherit from these ABCs or implement\ncompatible protocols. Iterable  supports iteration with __iter__ ,\nContainer  supports the in  operator with __contains__ , and\nSized  supports len()  with __len__ .\nCollection\nThis ABC has no methods of its own, but was added in Python 3.6 to\nmake it easier to subclass from Iterable , Container , and Sized .\nSequence , Mapping , Set\nThese are the main immutable collection types, and each has a mutable\nsubclass. A detailed diagram for MutableSequence  is in Figure 13-\n3 ; for MutableMapping  and MutableSet , there are diagrams in\nChapter 3  (Figures 3-1  and 3-2 ).\nMappingView\nIn Python 3, the objects returned from the mapping methods\n.items() , .keys() , and .values()  implement the interfaces\ndefined in ItemsView , KeysView , and ValuesView , respectively .\nThe first two also implement the rich interface of Set , with all the\noperators we saw in “Set Operations” .\nIterator\nNote that iterator subclasses Iterable . W e discuss this further in\nChapter 17 .\nAfter looking at some existing ABCs, let’ s practice goose typing by\nimplementing an ABC from scratch and putting it to use. The goal here is\nnot to encourage everyone to start creating ABCs left and right, but to learn\nhow to read the source code of the ABCs you’ll find in the standard library\nand other packages.\nCallable , Hashable\nThese are not collections, but collections.abc  was the first\npackage to define ABCs in the standard library , and these two were\ndeemed important enough to be included. They support type checking\nobjects that must be callable or hashable.\nFor callable detection, the callable(obj)  built-in function is more\nconvenient than insinstance(obj, Callable) .\nIf insinstance(obj, Hashable)  returns False , you can be\ncertain that obj  is not hashable. But if the return is True , it may be a false\npositive. The next box explains.",4099
169-Defining and Using an ABC.pdf,169-Defining and Using an ABC,"ISINSTANCE  W I T H  HASHABLE  A N D  ITERABLE  C A N  B E\nM I S L E A D I N G\nIt’ s easy to misinterpret the results of the isinstance  and\nissubclass  tests against the Hashable  and Iterable  ABCs.\nIf isinstance(obj, Hashable)  returns True , that only means\nthat the class of obj  implements or inherits __hash__ . But if obj  is\na tuple  containing unhashable items, then obj  is not hashable,\ndespite the positive result of the isinstance  check. T ech reviewer\nJür gen Gmach pointed out that duck typing provides the most accurate\nway to determine if an instance is hashable: call hash(obj) . That call\nwill raise TypeError  if obj  is not hashable.\nOn the other hand, even when isinstance(obj, Iterable)\nreturns False , Python may still be able to iterate over obj  using\n__getitem__  with 0-based indices, as we saw in Chapter 1  and\n“Python Digs Sequences” . The documentation for\ncollections.abc.Iterable  states:\nThe only r eliable way to determine whether an object is iterable is to\ncall iter(obj) .\nDefining and Using an ABC\nT I P\nThis warning appeared in the Interfaces  chapter of Fluent Python, First Edition :\nABCs, like descriptors and metaclasses, ar e tools for building frameworks.\nTher efor e, only a small minority of Python developers can cr eate ABCs without\nimposing unr easonable limitations and needless work on fellow pr ogrammers.\nNow ABCs have more potential use cases in type hints to support static typing. As\ndiscussed in “Abstract Base Classes” , using ABCs instead of concrete types in function\nar gument type hints gives more flexibility to the caller .\nT o justify creating an ABC, we need to come up with a context for using it\nas an extension point in a framework. So here is our context: imagine you\nneed to display advertisements on a website or a mobile app in random\norder , but without repeating an ad before the full inventory of ads is shown.\nNow let’ s assume we are building an ad management framework called\nADAM . One of its requirements is to support user -provided non-repeating\nrandom-picking classes.  T o make it clear to ADAM  users what is expected\nof a “non-repeating random-picking” component, we’ll define an ABC.\nIn the literature about data structures, “stack” and “queue” describe abstract\ninterfaces in terms of physical arrangements of objects. I will follow suit\nand use a real-world metaphor to name our ABC: bingo cages and lottery\nblowers are machines designed to pick items at random from a finite set,\nwithout repeating, until the set is exhausted.\nThe ABC will be named Tombola , after the Italian name of bingo and the\ntumbling container that mixes the numbers.\nThe Tombola  ABC has four methods. The two abstract methods are:\n.load(…)\nput items into the container .\n.pick()\nremove one item at random from the container , returning it.\nThe concrete methods are:\n.loaded()\nreturn True  if there is at least one item in the container .\n.inspect()\nreturn a tuple  built from the items currently in the container , without\nchanging its contents (the internal ordering is not preserved).8\nFigure 13-5  shows the Tombola  ABC and three concrete implementations.\nFigur e 13-5. UML diagram for an ABC and thr ee subclasses. The name of the T ombola ABC and its\nabstract methods ar e written in italics , per UML conventions. The dashed arr ow is used for interface\nimplementation—her e I am using it to show that T omboList not only implements the T ombola\ninterface, but is also r egister ed as virtual subclass  of T ombola—as we will see later in this chapter .\nExample 13-7  shows the definition of the Tombola  ABC.\nExample 13-7. tombola.py: T ombola is an ABC with two abstract methods\nand two concr ete methods\nimport abc \n \nclass Tombola(abc.ABC):  \n \n \n    @abc.abstractmethod  \n    def load(self, iterable ):  \n \n        """"""Add items from an iterable.""""""  \n \n    @abc.abstractmethod  \n    def pick(self):  \n \n        """"""Remove item at random, returning it.  \n \n        This method should raise `LookupError` when the instance is  \nempty. \n        """"""  \n \n    def loaded(self):  \n \n        """"""Return `True` if there's at least 1 item, `False`  \notherwise.""""""  \n        return bool(self.inspect())  \n \n \n    def inspect(self): \n        """"""Return a sorted tuple with the items currently  \ninside.""""""  \n        items = [] \n        while True:  \n \n            try: \n                items.append(self.pick()) \n            except LookupError : \n                break \n        self.load(items)  \n \n        return tuple(items)\nT o define an ABC, subclass abc.ABC .\n9\nAn abstract method is marked with the @abstractmethod  decorator ,\nand often its body is empty except for a docstring.\nThe docstring instructs implementers to raise LookupError  if there\nare no items to pick.\nAn ABC may include concrete methods.\nConcrete methods in an ABC must rely only on the interface defined by\nthe ABC (i.e., other concrete or abstract methods or properties of the\nABC).\nW e can’ t know how concrete subclasses will store the items, but we can\nbuild the inspect  result by emptying the Tombola  with successive\ncalls to .pick() …\n…then use .load(…)  to put everything back.\nT I P\nAn abstract method can actually have an implementation. Even if it does, subclasses\nwill still be forced to override it, but they will be able to invoke the abstract method with\nsuper() , adding functionality to it instead of implementing from scratch. See the abc\nmodule documentation  for details on @abstractmethod  usage.\nThe code for the .inspect()  method in Example 13-7  is silly but it\nshows that we can rely on .pick()  and .load(…)  to inspect what’ s\ninside the Tombola  by picking all items and loading them back—without\nknowing how the items are actually stored. The point of this example is to\nhighlight that it’ s OK to provide concrete methods in ABCs, as long as they\nonly depend on other methods in the interface. Being aware of their internal\ndata structures, concrete subclasses of Tombola  may always override\n.inspect()  with a smarter implementation, but they don’ t have to.10\nThe .loaded()  method in Example 13-7  has one line, but it’ s expensive:\nit calls .inspect()  to build the tuple  just to apply bool()  on it. This\nworks, but a concrete subclass can do much better , as we’ll see.\nNote that our roundabout implementation of .inspect()  requires that\nwe catch a LookupError  thrown by self.pick() . The fact that\nself.pick()  may raise LookupError  is also part of its interface, but\nthere is no way to make this explicit in Python, except in the documentation\n(see the docstring for the abstract pick  method in Example 13-7 .)\nI chose the LookupError  exception because of its place in the Python\nhierarchy of exceptions in relation to IndexError  and KeyError , the\nmost likely exceptions to be raised by the data structures used to implement\na concrete Tombola . Therefore, implementations can raise\nLookupError , IndexError , KeyError , or a custom subclass of\nLookupError  to comply . See Figure 13-6 .\nFigur e 13-6. Part of the Exception  class hierar chy .\n➊  LookupError  is the exception we handle in Tombola.inspect ;\n➋  IndexError  is the LookupError  subclass raised when we try to get\nan item from a sequence with an index beyond the last position;\n➌  KeyError  is raised when we use a nonexistent key to get an item from\na mapping.\nW e now have our very own Tombola  ABC. T o witness the interface\nchecking performed by an ABC, let’ s try to fool Tombola  with a defective\nimplementation in Example 13-8 .\nExample 13-8. A fake T ombola doesn’ t go undetected\n>>> from tombola import Tombola \n>>> class Fake(Tombola):  \n \n...     def pick(self): \n...         return 13 \n... \n>>> Fake  \n \n<class '__main__.Fake'>  \n>>> f = Fake()  \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : Can't instantiate abstract class Fake with abstract  \nmethod load\nDeclare Fake  as a subclass of Tombola .\nThe class was created, no errors so far .\nTypeError  is raised when we try to instantiate Fake . The message is\nvery clear: Fake  is considered abstract because it failed to implement\nload , one of the abstract methods declared in the Tombola  ABC.\nSo we have our first ABC defined, and we put it to work validating a class.\nW e’ll soon subclass the Tombola  ABC, but first we must cover some ABC\ncoding rules.1 1",8503
170-ABC Syntax Details.pdf,170-ABC Syntax Details,,0
171-Subclassing an ABC.pdf,171-Subclassing an ABC,"ABC Syntax Details\nThe best way to declare an ABC is to subclass abc.ABC  or any other\nABC. abc.ABC  is actually an instance of abc.ABCMeta —a special class\nfactory , a.k.a. a “metaclass”. W e’ll explain metaclasses in Chapter 25 . For\nnow , let’ s accept that metaclasses are used to build classes that are special in\nsome way , and agree that an ABC is a special kind of class. For example,\n“regular” classes don’ t verify their subclasses for compliance to its\ninterface, so this is a special behavior of ABCs.\nBesides the @abstractmethod , the abc  module defines the\n@abstractclassmethod , @abstractstaticmethod , and\n@abstractproperty  decorators. However , these last three were\ndeprecated in Python 3.3, when it became possible to stack decorators on\ntop of @abstractmethod , making the others redundant. For example,\nthe preferred way to declare an abstract class method is:\nclass MyABC(abc.ABC): \n    @classmethod  \n    @abc.abstractmethod  \n    def an_abstract_classmethod (cls, ...): \n        pass\nW A R N I N G\nThe order of stacked function decorators matters, and in the case of\n@abstractmethod , the documentation is explicit:\nWhen abstractmethod() is applied in combination with other method descriptors, it\nshould be applied as the innermost decorator , …\nIn other words, no other decorator may appear between @abstractmethod  and the\ndef  statement.\nNow that we got these ABC syntax issues covered, let’ s put Tombola  to\nuse by implementing two concrete descendants of it.\nSubclassing an ABC1 2\nGiven the Tombola  ABC, we’ll now develop two concrete subclasses that\nsatisfy its interface. These classes were pictured in Figure 13-5 , along with\nthe virtual subclass to be discussed in the next section.\nThe BingoCage  class in Example 13-9  is a variation of Example 7-8\nusing a better randomizer . This BingoCage  implements the required\nabstract methods load  and pick .\nExample 13-9. bingo.py: BingoCage is a concr ete subclass of T ombola\nimport random \n \nfrom tombola import Tombola \n \n \nclass BingoCage (Tombola):  \n \n \n    def __init__ (self, items): \n        self._randomizer  = random.SystemRandom ()  \n \n        self._items = [] \n        self.load(items)  \n \n \n    def load(self, items): \n        self._items.extend(items) \n        self._randomizer .shuffle(self._items)  \n \n \n    def pick(self):  \n \n        try: \n            return self._items.pop() \n        except IndexError : \n            raise LookupError ('pick from empty BingoCage ') \n \n    def __call__ (self):  \n \n        self.pick()\nThis BingoCage  class explicitly extends Tombola .\nPretend we’ll use this for online gaming. random.SystemRandom\nimplements the random  API on top of the os.urandom(…)\nfunction, which provides random bytes “suitable for cryptographic use”\naccording to the os  module docs .\nDelegate initial loading to the .load(…)  method.\nInstead of the plain random.shuffle()  function, we use the\n.shuffle()  method of our SystemRandom  instance.\npick  is implemented as in Example 7-8 .\n__call__  is also from Example 7-8 . It’ s not needed to satisfy the\nTombola  interface, but there’ s no harm in adding extra methods.\nBingoCage  inherits the expensive loaded  and the silly inspect\nmethods from Tombola . Both could be overridden with much faster one-\nliners, as in Example 13-10 . The point is: we can be lazy and just inherit the\nsuboptimal concrete methods from an ABC. The methods inherited from\nTombola  are not as fast as they could be for BingoCage , but they do\nprovide correct results for any Tombola  subclass that correctly\nimplements pick  and load .\nExample 13-10  shows a very dif ferent but equally valid implementation of\nthe Tombola  interface. Instead of shuf fling the “balls” and popping the\nlast, LottoBlower  pops from a random position.\nExample 13-10. lotto.py: LottoBlower is a concr ete subclass that overrides\nthe inspect and loaded methods fr om T ombola\nimport random \n \nfrom tombola import Tombola \n \n \nclass LottoBlower (Tombola): \n \n    def __init__ (self, iterable ): \n        self._balls = list(iterable )  \n \n \n    def load(self, iterable ): \n        self._balls.extend(iterable ) \n \n    def pick(self): \n        try: \n            position  = random.randrange (len(self._balls))  \n \n        except ValueError : \n            raise LookupError ('pick from empty LottoBlower ') \n        return self._balls.pop(position )  \n \n \n    def loaded(self):  \n \n        return bool(self._balls) \n \n    def inspect(self):  \n \n        return tuple(self._balls)\nThe initializer accepts any iterable: the ar gument is used to build a list.\nThe random.randrange(…)  function raises ValueError  if the\nrange is empty , so we catch that and throw LookupError  instead, to\nbe compatible with Tombola .\nOtherwise the randomly selected item is popped from self._balls .\nOverride loaded  to avoid calling inspect  (as Tombola.loaded\ndoes in Example 13-7 ). W e can make it faster by working with\nself._balls  directly—no need to build a whole new tuple .\nOverride inspect  with one-liner .\nExample 13-10  illustrates an idiom worth mentioning: in __init__ ,\nself._balls  stores list(iterable)  and not just a reference to\niterable  (i.e., we did not merely assign self._balls =\niterable , aliasing the ar gument). As mentioned in “Defensive\nprogramming and “fail fast”” , this makes our LottoBlower  flexible\nbecause the iterable  ar gument may be any iterable type. At the same\ntime, we make sure to store its items in a list  so we can pop  items. And\neven if we always get lists as the iterable  ar gument,\nlist(iterable)  produces a copy of the ar gument, which is a good\npractice considering we will be removing items from it and the client might\nnot expect that the provided list will be changed.\nW e now come to the crucial dynamic feature of goose typing: declaring\nvirtual subclasses with the register  method.13",5988
172-A Virtual Subclass of an ABC.pdf,172-A Virtual Subclass of an ABC,"A V irtual Subclass of an ABC\nAn essential characteristic of goose typing—and one reason why it deserves\na waterfowl name—is the ability to register a class as a virtual subclass  of\nan ABC, even if it does not inherit from it. When doing so, we promise that\nthe class faithfully implements the interface defined in the ABC—and\nPython will believe us without checking. If we lie, we’ll be caught by the\nusual runtime exceptions.\nThis is done by calling a register  class method on the ABC. The\nregistered class then becomes a virtual subclass of the ABC, and will be\nrecognized as such by issubclass , but it does not inherit any methods or\nattributes from the ABC.\nW A R N I N G\nV irtual subclasses do not inherit from their registered ABCs, and are not checked for\nconformance to the ABC interface at any time, not even when they are instantiated.\nAlso, static type checkers can’ t handle virtual subclasses at this time. For details, see\nMypy issue 2922—ABCMeta.register support .\nThe register  method is usually invoked as a plain function (see “Usage\nof register in Practice” ), but it can also be used as a decorator . In\nExample 13-1 1 , we use the decorator syntax and implement TomboList , a\nvirtual subclass of Tombola  depicted in Figure 13-7 .\nFigur e 13-7. UML class diagram for the T omboList, a r eal subclass of list and a virtual subclass of\nT ombola\nExample 13-1 1. tombolist.py: class T omboList is a virtual subclass of\nT ombola\nfrom random import randrange  \n \nfrom tombola import Tombola \n \n@Tombola .register   \n \nclass TomboList (list):  \n \n \n    def pick(self): \n        if self:  \n \n            position  = randrange (len(self)) \n            return self.pop(position )  \n \n        else: \n            raise LookupError ('pop from empty TomboList ') \n \n    load = list.extend  \n \n \n    def loaded(self): \n        return bool(self)  \n \n \n    def inspect(self): \n        return tuple(self) \n \n# Tombola.register(TomboList)  \nTombolist  is registered as a virtual subclass of Tombola .\nTombolist  extends list .\nTombolist  inherits its boolean behavior from list , and that returns\nTrue  if the list is not empty .\nOur pick  calls self.pop , inherited from list , passing a random\nitem index.\nTombolist.load  is the same as list.extend .\nloaded  delegates to bool .14",2348
173-Structural typing with ABCs.pdf,173-Structural typing with ABCs,"It’ s always possible to call register  in this way , and it’ s useful to do\nso when you need to register a class that you do not maintain, but which\ndoes fulfill the interface.\nNote that because of the registration, the functions issubclass  and\nisinstance  act as if TomboList  is a subclass of Tombola :\n>>> from tombola import Tombola \n>>> from tombolist  import TomboList  \n>>> issubclass (TomboList , Tombola) \nTrue \n>>> t = TomboList (range(100)) \n>>> isinstance (t, Tombola) \nTrue\nHowever , inheritance is guided by a special class attribute named\n__mro__ —the Method Resolution Order . It basically lists the class and its\nsuperclasses in the order Python uses to search for methods.  If you\ninspect the __mro__  of TomboList , you’ll see that it lists only the “real”\nsuperclasses— list  and object :\n>>> TomboList .__mro__ \n(<class 'tombolist.TomboList'>, <class 'list'>, <class 'object'>)\nTombola  is not in Tombolist.__mro__ , so Tombolist  does not\ninherit any methods from Tombola .\nThis concludes our Tombola  ABC case study . In the next section, we’ll\naddress how the register  ABC function is used in the wild.\nUsage of register in Practice\nIn Example 13-1 1 , we used Tombola.register  as a class decorator .\nPrior to Python 3.3, register  could not be used like that—it had to be\ncalled as a plain function after the class definition, as suggested by the\ncomment at the end of Example 13-1 1 . However , even now , it’ s more15\nwidely deployed as a function to register classes defined elsewhere. For\nexample, in the source code  for the collections.abc  module, the\nbuilt-in types tuple , str , range , and memoryview  are registered as\nvirtual subclasses of Sequence  like this:\nSequence .register (tuple) \nSequence .register (str) \nSequence .register (range) \nSequence .register (memoryview )\nSeveral other built-in types are registered to ABCs in _collections_abc.py .\nThose registrations happen only when that module is imported, which is\nOK because you’ll have to import it anyway to get the ABCs. For example,\nyou need to import MutableMapping  from collections.abc  to\nperform a check like isinstance(my_dict, MutableMapping) .\nSubclassing an ABC or registering with an ABC are both explicit ways of\nmaking our classes pass issubclass  checks—as well as isinstance\nchecks, which also rely on issubclass . But some ABCs support\nstructural typing as well. The next section explains.\nStructural typing with ABCs\nABCs are mostly used with nominal typing. When a class Sub  explicitly\ninherits from AnABC , or is registered with AnABC , the name of AnABC  is\nlinked to the Sub  class—and that’ s how at runtime,\nissubclass(AnABC, Sub)  returns True .\nIn contrast, structural typing is about looking at the structure of an object’ s\npublic interface to determine its type: an object is consistent-with  a type if it\nimplements the methods defined in the type.  Dynamic and static duck\ntyping are two approaches to structural typing.\nIt turns out that some ABCs also support structural typing. In his\n“W aterfowl and ABCs”  essay , Alex shows that a class can be recognized as\na subclass of an ABC even without registration. Here is his example again,\nwith an added test using issubclass :16\n>>> class Struggle : \n...     def __len__(self): return 23 \n... \n>>> from collections  import abc \n>>> isinstance (Struggle (), abc.Sized) \nTrue \n>>> issubclass (Struggle , abc.Sized) \nTrue\nClass Struggle  is considered a subclass of abc.Sized  by the\nissubclass  function (and, consequently , by isinstance  as well)\nbecause abc.Sized  implements a special class method named\n__subclasshook__ .\nThe __subclasshook__  for Sized  checks whether the class ar gument\nhas an attribute named __len__ . If it does, then it is considered a virtual\nsubclass of Sized . See Example 13-12 .\nExample 13-12. Definition of Sized  fr om the sour ce code of\nLib/_collections_abc.py .\nclass Sized(metaclass =ABCMeta): \n \n    __slots__  = () \n \n    @abstractmethod  \n    def __len__(self): \n        return 0 \n \n    @classmethod  \n    def __subclasshook__ (cls, C): \n        if cls is Sized: \n            if any(""__len__"" in B.__dict__  for B in C.__mro__):  \n \n                return True  \n \n        return NotImplemented   \nIf there is an attribute named __len__  in the __dict__  of any class\nlisted in C.__mro__  (i.e., C  and its superclasses)…\n…return True , signaling that C  is a virtual subclass of Sized .\nOtherwise return NotImplemented  to let the subclass check proceed.\nN O T E\nIf you are interested in the details of the subclass check, see the source code for the\nABCMeta.__subclasscheck__  method in Python 3.6: Lib/abc.py . Beware: it has\nlots of ifs and two recursive calls. In Python 3.7, Ivan Levkivskyi and INADA Naoki\nrewrote in C most of the logic for the abc  module, for better performance. See Python\nissue #31333 . The current implementation of ABCMeta.__subclasscheck__\nsimply calls _abc_subclasscheck . The relevant C source code is in\ncpython/Modules/_abc.c#L605 .\nThat’ s how __subclasshook__  allows ABCs to support structural\ntyping. Y ou can formalize an interface with an ABC, you can make\nisinstance  checks against that ABC, and still have a completely\nunrelated class pass an issubclass  check because it implements a\ncertain method (or because it does whatever it takes to convince a\n__subclasshook__  to vouch for it).\nIs it a good idea to implement __subclasshook__  in our own ABCs?\nProbably not. All the implementations of __subclasshook__  I’ve seen\nin the Python source code are in ABCs like Sized  that declare just one\nspecial method, and they simply check for that special method name. Given\ntheir “special” status, you can be pretty sure that any method named\n__len__  does what you expect. But even in the realm of special methods\nand fundamental ABCs, it can be risky to make such assumptions. For\nexample, mappings implement __len__ , __getitem__ , and\n__iter__ , but they are rightly not considered subtypes of Sequence ,\nbecause you can’ t retrieve items using integer of fsets or slices. That’ s why\nthe abc.Sequence  class does not implement __subclasshook__ .\nFor ABCs that you and I may write, a __subclasshook__  would be\neven less dependable. I am not ready to believe that any class named Spam\nthat implements or inherits load , pick , inspect , and loaded  is\nguaranteed to behave as a Tombola . It’ s better to let the programmer\naf firm it by subclassing Spam  from Tombola , or registering it with\nTombola.register(Spam) . Of course, your __subclasshook__",6643
174-Static protocols.pdf,174-Static protocols,,0
175-The typed double function.pdf,175-The typed double function,"could also check method signatures and other features, but I just don’ t think\nit’ s worthwhile.\nS t a t i c  p r o t o c o l s\nN O T E\nStatic protocols were introduced in “Static Protocols”  ( Chapter 8 ). I considered delaying\nall coverage of protocols until the present Chapter 13 , but decided that the initial\npresentation of type hints in functions had to include protocols because duck typing is\nan essential part of Python, and static type checking without protocols doesn’ t handle\nPythonic APIs very well.\nW e will wrap up this chapter illustrating static protocols with two simple\nexamples, and a discussion of numeric ABCs and protocols. Let’ s start by\nshowing how a static protocol makes it possible to annotate and type check\nthe double()  function we first saw in “T ypes are defined by supported\noperations” .\nThe typed double function\nWhen introducing Python to programmers more used to statically typed\nlanguages, one of my favorite examples is this simple double  function.\n>>> def double(x): \n...     return x * 2 \n... \n>>> double(1.5) \n3.0 \n>>> double('A') \n'AA' \n>>> double([10, 20, 30]) \n[10, 20, 30, 10, 20, 30]  \n>>> from fractions  import Fraction  \n>>> double(Fraction (2, 5)) \nFraction(4, 5)\nBefore static protocols were introduced, there was no practical way to add\ntype hints to double  without limiting its possible uses.\nThanks to duck typing, double  works even with types from the future,\nsuch as the enhanced Vector  class that we’ll see in “Overloading * for\nScalar Multiplication”  ( Chapter 16 ).\n>>> from vector_v7  import Vector \n>>> double(Vector([11.0, 12.0, 13.0])) \nVector([22.0, 24.0, 26.0])\nThe initial implementation of type hints in Python was a nominal type\nsystem: the name of a type in an annotation had to match the name of the\ntype of the actual ar guments—or the name of one of its superclasses. Since\nit’ s impossible to name all types that implement a protocol by supporting\nthe required operations, duck typing could not be described by type hints\nbefore Python 3.8.\nNow , with typing.Protocol  we can tell Mypy that double  takes an\nar gument x  that supports x * 2 . Here is how:\nExample 13-13. double_protocol.py : definition of double  using a\nProtocol .\nfrom typing import TypeVar, Protocol  \n \nT = TypeVar('T')  \n \n \nclass Repeatable (Protocol ): \n    def __mul__(self: T, repeat_count : int) -> T: ...  \n \n \nRT = TypeVar('RT', bound=Repeatable )  \n \n \ndef double(x: RT) -> RT:  \n \n    return x * 2\nW e’ll use this T  in the __mul__  signature.\n__mul__  is the essence of the Repeatable  protocol. The self\nparameter is usually not annotated—its type is assumed to be the class.17",2708
176-Runtime checkable static protocols.pdf,176-Runtime checkable static protocols,"Here we use T  to make sure the result type is the same as the type of\nself . Also, note that repeat_count  is limited to int  in this\nprotocol.\nThe RT  type variable is bounded by the Repeatable  protocol: the\ntype checker will require that the actual type implements\nRepeatable .\nNow the type checker is able to verify that the x  parameter is an object\nthat can be multiplied by an integer , and the return value has the same\ntype as x .\nThis example shows why PEP 544  is titled “Protocols: Structural subtyping\n(static duck typing)”. The nominal type of the actual ar gument x  given to\ndouble  is irrelevant as long as it quacks—that is, as long as it implements\n__mul__ .\nRuntime checkable static protocols\nIn the T yping Map  ( Figure 13-1 ), typing.Protocol  appears in the\nstatic checking area—the bottom half of the diagram. However , when\ndefining a typing.Protocol  subclass, you can use the\n@runtime_checkable  decorator to make that protocol support\nisinstance/issubclass  checks at runtime. This works because\ntyping.Protocol  is an ABC, therefore it supports the\n__subclasshook__  we saw in “Structural typing with ABCs” .\nAs of Python 3.9, the typing  module includes seven ready-to-use\nprotocols that are runtime checkable. Here are two of them, quoted directly\nfrom the typing  documentation :\nclass typing.SupportsComplex\nAn ABC with one abstract method __complex__ .\nclass typing.SupportsFloat\nAn ABC with one abstract method __float__ .\nThese protocols are designed to check numeric types for “convertibility”: if\nan object o  implements __complex__ , then you should be able to get a\ncomplex  by invoking complex(o) —because the __complex__\nspecial method exists to support the complex()  built-in function.\nThis is the source code  for the typing.SupportsComplex  protocol:\nExample 13-14.\n@runtime_checkable  \nclass SupportsComplex (Protocol ): \n    """"""An ABC with one abstract method __complex__.""""""  \n    __slots__  = () \n \n    @abstractmethod  \n    def __complex__ (self) -> complex: \n        pass\nThe key is the __complex__  abstract method.  During static type\nchecking, an object will be considered consistent-with  the\nSupportsComplex  protocol if it implements a __complex__  method\nthat takes only self  and returns a complex .\nThanks to the @runtime_checkable  class decorator applied to\nSupportsComplex , that protocol can also be used with isinstance\nchecks:\nExample 13-15. Using SupportsComplex  at runtime.\n>>> from typing import SupportsComplex  \n>>> import numpy as np \n>>> c64 = np.complex64 (3+4j)  \n \n>>> isinstance (c64, complex)   \n \nFalse \n>>> isinstance (c64, SupportsComplex )  \n \nTrue \n>>> c = complex(c64)  \n \n>>> c \n(3+4j) \n>>> isinstance (c, SupportsComplex ) \n \nFalse 18\n>>> complex(c) \n(3+4j)\ncomplex64  is one of five complex number types provided by NumPy .\nNone of the NumPy complex types subclass the built-in complex .\nBut NumPy’ s complex types implement __complex__  so they\ncomply with the SupportsComplex  protocol.\nTherefore, you can create built-in complex  objects from them.\nSadly , the complex  built-in type does not implement __complex__\nalthough complex(c)  works fine if c  is a complex .\nAs a result of that last point, if you want to test whether an object c  is a\ncomplex  or SupportsComplex  you can provide a tuple of types as the\nsecond ar gument to isinstance , like this:\nisinstance (c, (complex, SupportsComplex ))\nAn alternative would be to use the Complex  ABC, defined in the\nnumbers  module. The built-in complex  type and the NumPy\ncomplex64  and complex128  types are all registered as virtual\nsubclasses of numbers.Complex , therefore this works:\n>>> import numbers \n>>> isinstance (c, numbers.Complex) \nTrue \n>>> isinstance (c64, numbers.Complex) \nTrue\nI recommended using the numbers  ABCs in Fluent Python, First Edition\nbut now that’ s no longer good advice, because those ABCs are not\nrecognized by the static type checkers, as we’ll see in “The numbers ABCs\nand numeric protocols” .\nIn this section I wanted to demonstrate that a runtime checkable protocol\nworks with isinstance , but it turns out this is example not a particularly\ngood use case of isinstance , as the sidebar “Duck typing is your\nfriend”  explains.\nT I P\nIf you’re using an external type checker , there is one advantage of explict\nisinstance  checks: when you write an if  statement where the condition is\nisinstance(o, MyType) , then Mypy can infer that inside the if  block the type\nof the o  object is consistent-with  MyType .\nD U C K  T Y P I N G  I S  Y O U R  F R I E N D\nV ery often at runtime, duck typing is the best approach for type\nchecking: instead of calling isinstance  or hasattr , just try the\noperations you need to do on the object, and handle exceptions as\nneeded. Here is a concrete example.\nContinuing the previous discussion—given an object o  that I need to\nuse as a complex number , this would be one approach:\nif isinstance (o, (complex, SupportsComplex )): \n    # do something that requires `o` to be convertible to  \ncomplex \nelse: \n    raise TypeError ('o must be convertible to complex' )\nThe goose typing  approach would be to use the numbers.Complex\nABC:\nif isinstance (o, numbers.Complex): \n    # do something with `o`, an instance of `Complex`  \nelse: \n    raise TypeError ('o must be an instance of Complex' )\nHowever , I prefer to leverage duck typing and do this, using the EAFP\nprinciple—it’ s easier to ask for giveness than permission:\ntry: \n    c = complex(o) \nexcept TypeError  as exc: \n    raise TypeError ('o must be convertible to complex' ) from \nexc\nAnd, if all you’re going to do is raise a TypeError  anyway , then I’d\nomit the try/except/raise  statements and just write this:\nc = complex(o)",5836
177-Supporting a static protocol.pdf,177-Supporting a static protocol,"In this last case, if o  is not an acceptable type, Python will raise an\nexception with a very clear message: For example, this is what I get if o\nis a tuple :\nTypeError: complex() first argument must be a string or a  \nnumber, not 'tuple'\nI find the duck typing approach much better in this case.\nNow that we’ve seen how to use static protocols at runtime with preexisting\ntypes like complex  and numpy.complex64 , let’ s see how to use them\nwith a user -defined class.\nSupporting a static protocol\nRecall the Vector2d  class we built in Chapter 1 1 . Given that a complex\nnumber and a Vector2d  instance both consist of a pair of floats, it makes\nsense to support conversion from Vector2d  to complex .\nExample 13-16  shows the implementation of the __complex__  method\nto enhance the last version of Vector2d  we saw in Example 1 1-1 1 . For\ncompleteness, we can support the inverse operation with a fromcomplex\nclass method to build a Vector2d  from a complex .\nExample 13-16. vector2d_v4.py : methods for converting to and fr om\ncomplex .\n    def __complex__ (self): \n        return complex(self.x, self.y) \n \n    @classmethod  \n    def fromcomplex (cls, datum): \n        return cls(datum.real, datum.imag)  \nThis assumes that datum  has .real  and .imag  attributes. W e’ll see a\nbetter implementation in Example 13-17 .\nGiven the code above, and the __abs__  method the Vector2d  already\nhad in Example 1 1-1 1 , we get these features:\n>>> from typing import SupportsComplex , SupportsAbs  \n>>> from vector2d_v4  import Vector2d  \n>>> v = Vector2d (3, 4) \n>>> isinstance (v, SupportsComplex ) \nTrue \n>>> isinstance (v, SupportsAbs ) \nTrue \n>>> complex(v) \n(3+4j) \n>>> abs(v) \n5.0 \n>>> Vector2d .fromcomplex (3+4j) \nVector2d(3.0, 4.0)\nFor runtime type checking, Example 13-16  is fine, but for better static\ncoverage and error reporting with Mypy , the __abs__ , __complex__\nand fromcomplex  methods should get type hints as shown in\nExample 13-17 .\nExample 13-17. vector2d_v5.py : adding annotations to the methods\nunder study .\n    def __abs__(self) -> float:  \n \n        return math.hypot(self.x, self.y) \n \n    def __complex__ (self) -> complex:  \n \n        return complex(self.x, self.y) \n \n    @classmethod  \n    def fromcomplex (cls, datum: SupportsComplex ) -> Vector2d :  \n \n        c = complex(datum)  \n \n        return cls(c.real, c.imag)\nThe float  return annotation is needed, otherwise Mypy infers Any ,\nand doesn’ t check the body of the method.\nEven without the annotation, Mypy was able to infer that this returns a\ncomplex . The annotation prevents a warning, depending on your\nMypy configuration.\nHere SupportsComplex  ensures the datum  is convertible.\nThis explicit conversion is necessary , because the\nSupportsComplex  type does not declare .real  and .imag\nattributes, used in the next line. For example, Vector2d  doesn’ t have\nthose attributes, but implements __complex__ .\nThe return type of fromcomplex  can be Vector2d  if from\n__future__ import annotations  appears at the top of the\nmodule. That import causes type hints to be stored as strings, without being\nevaluated at import time—when functions definitions are evaluated.\nW ithout the __future__  import of annotations , Vector2d  is an\ninvalid reference at this point (the class is not fully defined yet) and should\nbe written as a string: 'Vector2d' —as if it were a forward reference.\nThis __future__  import was introduced by PEP 563—Postponed\nEvaluation of Annotations , implemented in Python 3.7. That behavior was\nscheduled to become default in 3.10, but the change was delayed to a later\nversion.  When that happens, the import will be redundant but harmless.19",3743
178-Designing a static protocol.pdf,178-Designing a static protocol,"T Y P E  H I N T S  A R E  I G N O R E D  A T  R U N T I M E\nT ype hints are ignored at runtime, including for isinstance  or issubclass\nchecks against static protocols. For example, this means that any class with a\n__float__  method is considered—at runtime—a virtual subclass of\nSupportsFloat , even if the __float__  method exists only to raise a clearly\nworded exception :\n>>> from typing import SupportsFloat  \n>>> c = 3+4j \n>>> isinstance (c, SupportsFloat ) \nTrue \n>>> c.__float__  \n<method-wrapper '__float__' of complex object at  \n0x1065dc370>  \n>>> float(c) \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : can't convert complex to float\nNext, let’ s see how to create—and later , extend—a new static protocol.\nDesigning a static protocol\nWhile studying goose typing, we saw the Tombola  ABC in “Defining and\nUsing an ABC” . Here we’ll see how to define a similar interface using a\nstatic protocol.\nThe Tombola  ABC specifies two methods: pick  and load . W e could\ndefine a static protocol with these two methods as well, but I learned from\nthe Go community that single-method protocols make static duck typing\nmore useful and flexible. The Go standard library has several interfaces like\nReader —an interface for I/O that requires just a read  method. After a\nwhile, if you realize a more complete protocol is required, you can combine\ntwo or more protocols to define a new one.\nUsing a container that picks items at random may or may not require\nreloading the container , but it certainly needs a method to do the actual\npick, so that’ s the method I will choose for the minimal RandomPicker20\nprotocol. The code for that protocol is in Example 13-18  and its use is\ndemonstrated by tests in Example 13-19 .\nExample 13-18. randompick.py : definition of RandomPicker .\nfrom typing import Protocol , runtime_checkable , Any \n \n@runtime_checkable  \nclass RandomPicker (Protocol ): \n    def pick(self) -> Any: ...\nN O T E\nThe pick  method returns Any . In “Implementing a generic static protocol”  we will see\nhow to make RandomPicker  a generic type with a parameter to let users of the\nprotocol to specify the return type of the pick  method.\nExample 13-19. randompick_test.py : RandomPicker  in use.\nimport random \nfrom typing import Any, Iterable , TYPE_CHECKING  \n \nfrom randompick  import RandomPicker   \n \n \nclass SimplePicker :  \n \n    def __init__ (self, items: Iterable ) -> None: \n        self._items = list(items) \n        random.shuffle(self._items) \n \n    def pick(self) -> Any:  \n \n        return self._items.pop() \n \ndef test_isinstance () -> None:  \n \n    popper: RandomPicker  = SimplePicker ([1])  \n \n    assert isinstance (popper, RandomPicker )  \n \n \ndef test_item_type () -> None:  \n \n    items = [1, 2] \n    popper = SimplePicker (items) \n    item = popper.pick() \n    assert item in items \n    if TYPE_CHECKING : \n        reveal_type (item)  \n \n    assert isinstance (item, int)\nIt’ s not necessary to import the static protocol to define a class that\nimplements it. Here I imported RandomPicker  only to use it\ntest_isintance  below .\nSimplePicker  implements RandomPicker —but it does not\nsubclass it. This is static duck typing in action.\nAny  is the default return type, so this annotation is not strictly\nnecessary , but it does make it more clear that we are implementing the\nRandomPicker  protocol as defined in Example 13-18 .\nDon’ t for get to add -> None  hints to your tests, if you want Mypy to\nlook at them.\nI added a type hint for the popper  variable to show that Mypy\nunderstands that SimplePicker  is consistent-with .\nThis test proves that an instance of SimplePicker  is also an instance\nof RandomPicker . This works because of the\n@runtime_checkable  decorator applied to RandomPicker , and\nbecause SimplePicker  has a pick  method as required.\nThis test invokes the pick  method from a SimplePicker , verifies\nthat it returns one of the items given to SimplePicker , and then does\nstatic and runtime checks on the returned item.\nThis line generates a note in the output of Mypy .\nAs we saw in Example 8-22 , reveal_type  is a “magic” function\nrecognized by Mypy—that’ s why it is not imported and we can only call it\ninside if  blocks protected by typing.TYPE_CHECKING  which is only\nTrue  in the eyes of a static type checker , but is False  at runtime.\nBoth tests in Example 13-19  pass. Mypy does not see any errors in that\ncode either , and shows the result of the reveal_type  on the item",4593
179-Protocol naming conventions.pdf,179-Protocol naming conventions,"returned by pick :\n$ mypy randompick_test.py  \nrandompick_test.py:24: note: Revealed type is 'Any'\nNext, we’ll see how to extend a protocol, adding a method.\nExtending a protocol\nAs I mentioned at the start of the previous section, Go developers advocate\nto err on the side of minimalism when defining interfaces—their name for\nstatic protocols. Many of the most widely used Go interfaces have a single\nmethod.\nWhen practice reveals that a protocol with more methods is useful, instead\nof adding methods to the original protocol it’ s better to derive a new\nprotocol from it. Extending a static protocol in Python has a few caveats, as\nExample 13-20  shows.\nExample 13-20. randompickload.py : extending RandomPicker .\nfrom typing import Protocol , runtime_checkable  \nfrom randompick  import RandomPicker  \n \n@runtime_checkable   \n \nclass LoadableRandomPicker (RandomPicker , Protocol ):  \n \n    def load(self, Iterable ) -> None: ...  \nIf you want the derived protocol to be runtime checkable, you must\napply the decorator again—its behavior is not inherited.\nEvery protocol must explicitly name typing.Protocol  as one of its\nbase classes—in addition to the protocol we are extending. This is\ndif ferent from the way inheritance works in Python.\nBack to “regular” OOP: we only need to declare the method that is new\nin this derived protocol. The pick  method declaration is inherited from\nRandomPicker .21\n22",1440
180-The numbers ABCs and numeric protocols.pdf,180-The numbers ABCs and numeric protocols,"This concludes the final example of defining and using a static protocol in\nthis chapter . Naming is considered  one of the hardest things in computer\nscience, so let’ s talk about naming conventions for static protocols.\nProtocol naming conventions\nThe page Contributing to typeshed  recommends this naming convention for\nstatic protocols:\nUse plain names for protocols that represent a clear concept (e.g.\nIterator , Container ).\nUse SupportsX  for protocols that provide callable methods (e.g.\nSupportsInt , SupportsRead , SupportsReadSeek ).\nUse HasX  for protocols that have readable and/or writable\nattributes or getter/setter methods (e.g. HasItems , HasFileno ).\nThe Go standard library has a naming convention that is also useful: for\nsingle method protocols, if the method name is a verb, append “-er” or “-or”\nto make it a noun. Examples: Formatter , Animator , Scanner . For\ninspiration, see Go (Golang) Standar d Library Interfaces (Selected)  by\nAsuka Kenji.\nT o close this chapter , we’ll go over numeric ABCs and their possible\nreplacement with numeric protocols.\nThe numbers ABCs and numeric protocols\nW A R N I N G\nAs I review this in July 2021, the numbers  package is not supported by PEP 484 or the\nMypy type checker . Since 2017 there is an open issue in the Mypy project titled “int is\nnot a Number?” . This is not a Mypy bug; it reflects a shortcoming of the numbers\npackage, which I explain below .\nThe numbers  package defines the so-called numeric tower  described in\nPEP 3141—A T ype Hierarchy for Numbers . The tower is linear hierarchy\nof ABCs, where Number  is the topmost ABC, Complex  is its immediate\nsubclass, and so on, down to Integral :\nNumber\nComplex\nReal\nRational\nIntegral\nSo if you need to check for an integer , you can use isinstance(x,\nnumbers.Integral)  to accept int , bool  (which subclasses int ) or\nother integer types that are provided by external libraries that register their\ntypes as virtual subclasses of the numbers  ABCs. For example, NumPy\nhas 21 integer types —as well as several variations of floating point types\nregistered as numbers.Real , and complex numbers with various bit\nwidths registered as numbers.Complex .\nT I P\nSomewhat surprisingly , decimal.Decimal  is not registered as a virtual subclass of\nnumbers.Real . The reason is that, if you need the precision of Decimal  in your\nprogram, then you want to be protected from accidental mixing of decimals with other\nless precise numeric types, particularly floating point numbers.\nSadly , the numeric tower was not designed for static type checking. The\nroot ABC— numbers.Number —has no methods, so if you declare x:\nNumber  then type checkers will not let you do arithmetic or call any\nmethods on x .\nT o be frank, we don’ t often need to implement type safe functions that can\nhandle various types of floating point numbers, or integers of varying bit\nwidths. When needed, a possible workaround is to use the numeric\nprotocols provided by the typing  module, which we discussed in\n“Runtime checkable static protocols” .\nUnfortunately , at runtime, the numeric protocols may let you down. As\nmentioned in “T ype Hints Are Ignored at Runtime” , Python’ s complex\ntype implements __float__ , but the method exists only to raise\nTypeError  with an explicit message: “can’ t convert complex to float.” It\nimplements __int__  as well, for the same reason. The presence of those\nmethods make isinstance  return misleading results. However ,\nNumPy’ s complex types implement __float__  and __int__  methods\nthat work, only issuing a warning when each of them is used for the first\ntime:\n>>> import numpy as np \n>>> cd = np.cdouble(3+4j) \n>>> cd \n(3+4j) \n>>> float(cd) \n<stdin>:1: ComplexWarning: Casting complex values to real  \ndiscards the imaginary part  \n3.0\nThe opposite problem also happens: built-ins complex , float  and int ,\nand also numpy.float16 , numpy.uint8  don’ t have a __complex__\nmethod, so isinstance(x, SupportsComplex)  returns False  for\nthem. . The NumPy complex types, such as np.complex64  do\nimplement __complex__  to convert to a built-in complex .\nHowever , in practice, the complex()  built-in constructor handles\ninstances of all these types with no errors or warnings:\n>>> import numpy as np \n>>> from typing import SupportsComplex  \n>>> sample = [1+0j, np.complex64 (1+0j), 1.0, np.float16(1.0), 1, \nnp.uint8(1)] \n>>> [isinstance (x, SupportsComplex ) for x in sample] \n[False, True, False, False, False, False]  23\n>>> [complex(x) for x in sample] \n[(1+0j), (1+0j), (1+0j), (1+0j), (1+0j), (1+0j)]\nThis shows that isinstance  checks against SupportsComplex\nsuggest those conversions to complex  would fail, but they all succeed. In\nthe typing-sig mailing list, Guido pointed out that the built-in complex\naccepts a single ar gument, and that’ s why those conversions work.\nOn the other hand, Mypy accepts ar guments of all those six types in a call\nto a to_complex()  function defined like this:\ndef to_complex (n: SupportsComplex ) -> complex: \n    return complex(n)\nAs I write this, NumPy has no type hints, so its number types are all Any .\nOn the other hand, Mypy is somehow “aware” that the built-in int  and\nfloat  can be converted to complex , even though on typeshed only the\nbuilt-in complex  class has a __complex__  method.\nIn conclusion, although numeric types should not be hard to type check, the\ncurrent situation is this: the type hints PEP-484 eschews  the numeric tower\nand implicitly recommends that type checkers hard code the subtype\nrelationships between built-in complex , float  and int . Mypy does\nthat, and also pragmatically accepts that int  and float  are consistent-\nwith  SupportsComplex , even though they don’ t implement\n__complex__ .\nT I P\nI only found unexpected results when using isinstance  checks with numeric\nSupports*  protocols while experimenting with conversions to or from complex . If\nyou don’ t use complex numbers, you can rely on those protocols instead of the\nnumbers  ABCs.\nThe main takeaways for this section are:24\n25",6129
181-Chapter Summary.pdf,181-Chapter Summary,"The numbers  ABCs are fine for goose typing, but unsuitable for\nstatic typing.\nThe numeric static protocols SupportsComplex ,\nSupportsFloat , etc. work well for static typing, but are\nunreliable for goose typing when complex numbers are involved.\nW e are now ready for a quick review of what we saw in this chapter .\nC h a p t e r  S u m m a r y\nThe T yping Map  ( Figure 13-1 ) is the key to making sense of this chapter .\nAfter a brief introduction to the four approaches to typing, we contrasted\ndynamic and static protocols, which respectively support duck typing  and\nstatic duck typing . Both kinds of protocols share the essential characteristic\nthat a class is never required to explicitly declare support for any specific\nprotocol. A class supports a protocol simply by implementing the necessary\nmethods.\nThe next major section was “Programming ducks” , where we explored the\nlengths to which the Python interpreter goes to make the sequence and\niterable dynamic protocols work, including partial implementations of both.\nW e then saw how a class can be made to implement a protocol at runtime\nthrough the addition of extra methods via monkey-patching. The duck\ntyping section ended with hints for defensive programming, including\ndetection of structural types without explicit isinstance  or hasattr\nchecks using try/except  and failing fast.\nAfter Alex Martelli introduced goose typing  in “W aterfowl and ABCs” , we\nsaw how to subclass existing ABCs, surveyed important ABCs in the\nstandard library , and created an ABC from scratch, which we then\nimplemented by traditional subclassing and by registration. T o close this\nsection, we saw how the __subclasshook__  special method enables\nABCs to support structural typing by recognizing unrelated classes that\nprovide methods fulfilling the interface defined in the ABC.\nThe last major section was “Static protocols” , where we resumed coverage\nof static duck typing  which started in Chapter 8 , section “Static Protocols” .\nW e saw how the @runtime_checkable  decorator also leverages\n__subclasshook__  to support structural typing at runtime—even\nthough the best use of static protocols is with static type checkers which can\ntake into account type hints to make structural typing more reliable. Next\nwe talked about the design and coding of a static protocol and how to\nextend it. The chapter ended with “The numbers ABCs and numeric\nprotocols”  which tells the sad story of the derelict state of the numeric",2512
182-Further Reading.pdf,182-Further Reading,"tower and a few existing shortcomings of the proposed alternative: the\nnumeric static protocols such as SupportsFloat  and others added to the\ntyping  module in Python 3.8.\nThe main message of this chapter is that we have four complementary ways\nof programming with interfaces in modern Python, each with dif ferent\nadvantages and drawbacks. Y ou are likely to find suitable use cases for each\ntyping scheme in any modern Python codebase of significant size. Rejecting\nany one of these approaches will make your work as a Python programmer\nharder than it needs to be.\nHaving said that, Python achieved widespread popularity while supporting\nonly duck typing . Other popular languages such as JavaScript, PHP , and\nRuby , as well as Lisp, Smalltalk, Erlang, and Clojure—not popular but very\ninfluential—are all languages that had and still have tremendous impact by\nleveraging the power and simplicity of duck typing .\nF u r t h e r  R e a d i n g\nGreat books about Python have—almost by definition—great coverage of\nduck typing. T wo of my favorite Python books had updates released after\nFluent Python, First Edition : The Quick Python Book 3r d Edition\n(Manning, 2018), by Naomi Ceder; and Python in a Nutshell, 3r d Edition\n(O’Reilly , 2017) by Alex Martelli, Anna Ravenscroft, and Steve Holden.\nFor a discussion of the pros and cons of dynamic typing, see Guido van\nRossum’ s interview to Bill V enners in “Contracts in Python: A\nConversation with Guido van Rossum, Part IV” .\nThe Mypy documentation is often the best source of information for\nanything related to static typing in Python, including static duck typing,\naddressed in their Protocols and structural subtyping  chapter .\nThe remaining references are all about goose typing . Beazley and Jones’ s\nPython Cookbook, 3r d Edition  (O’Reilly) has a section about defining an\nABC (Recipe 8.12). The book was written before Python 3.4, so they don’ t\nuse the now preferred syntax of declaring ABCs by subclassing from\nabc.ABC  (instead, they use the metaclass  keyword, which we’ll only\nreally need in Chapter 25 ). Apart from this small detail, the recipe covers\nthe major ABC features very well.\nThe Python Standar d Library by Example  by Doug Hellmann (Addison-\nW esley), has a chapter about the abc  module. It’ s also available on the W eb\nin Doug’ s excellent PyMOTW—Python Module of the W eek . Hellmann\nalso uses the old style of ABC declaration:\nPluginBase(metaclass=abc.ABCMeta)  instead of the simpler\nPluginBase(abc.ABC)  available since Python 3.4.\nWhen using ABCs, multiple inheritance is not only common but practically\ninevitable, because each of the fundamental collection ABCs— Sequence ,\nMapping , and Set —extends Collection , which in turn extends\nmultiple ABCs (see Figure 13-4 ). Therefore, Chapter 14  is an important\nfollow-up to this one.\nPEP 31 19 — Introducing Abstract Base Classes  gives the rationale for\nABCs. PEP 3141 - A T ype Hierarchy for Numbers  presents the ABCs of\nthe numbers  module , but the discussion in the Mypy issue #3186—int is\nnot a Number?  includes some ar guments about why the numeric tower is\nunsuitable for static type checking.\nS O A P B O X\nThe MVP Journey of Python Static T yping\nI work for Thoughtworks, a worldwide leader in agile software\ndevelopment. At Thoughtworks, we often recommend that our clients\nshould aim to create and deploy MVPs: minimal viable products— “a\nsimple version of a product that is given to users in order to validate the\nkey business assumptions” as defined by my colleague Paulo Caroli in\nLean Inception , a post in Martin Fowler ’ s collective blog .\nGuido van Rossum and the other core developers who designed and\nimplemented static typing have followed an MVP strategy since 2006.\nFirst, PEP 3107—Function Annotations  was implemented in Python 3.0\nwith very limited semantics: just syntax to attach annotations to\nfunction ar guments and returns. This was done explicitly to allow for\nexperimentation and collect feedback—key benefits of an MVP .\nEight years later , PEP 484—T ype Hints  was proposed and approved. Its\nimplementation in Python 3.5 required no changes in the language or\nstandard library—except the addition of the typing  module, on which\nno other part of the standard library depended. PEP 484 supported only\nnominal types with generics—similar to Java—but with the actual static\nchecking done by external tools. Important features—like variable\nannotations, generic built-in types, and static protocols—were missing.\nDespite those limitations, this typing MVP was valuable enough to\nattract investment and adoption by companies with very lar ge Python\ncodebases, like Dropbox, Google, and Facebook—as well as support\nfrom professional IDEs like PyCharm , W ing , and VS Code .\nPEP 526—Syntax for V ariable Annotations  was the first evolutionary\nstep that required changes to the interpreter , in Python 3.6. Further\nchanges to the interpreter were made in Python 3.7 to support PEP 563\n—Postponed Evaluation of Annotations  and PEP 560—Core support\nfor typing module and generic types —which in turn allowed built-in\nand standard library collections to accept generic type hints out of the\nbox in Python 3.9, thanks to PEP 585—T ype Hinting Generics In\nStandard Collections .\nDuring those years, some Python users—including me—were\nunderwhelmed by the typing support. After I learned Go, the lack of\nstatic duck typing in Python’ s type hints was incomprehensible, in a\nlanguage where duck typing had always been a core strength.\nBut that is the nature of MVPs: they may not satisfy all potential users,\nbut they can be implemented with less ef fort, and guide further\ndevelopment with feedback from actual usage in the field.\nIf there is one thing we all learned from Python 3, is that incremental\nprogress is safer than big-bang releases. I am glad we did not have to\nwait for Python 4—if it ever comes—to make Python more attractive to\nlar ge enterprises, where the benefits of static typing outweigh the added\ncomplexity .\nT yping Appr oaches in Popular Languages\nFigur e 13-8. Four appr oaches to type checking and languages that support them.\nFigure 13-8  is a variation of the T yping Map  ( Figure 13-1 ) with the\nnames of a few popular languages that support each of the typing\napproaches.\nT ypeScript and Python ≥ 3.8 are the only languages in my small and\narbitrary sample that support all four approaches.\nGo is clearly a statically typed language in the Pascal tradition, but it\npioneered static duck typing —at least among languages that are widely\nused today . I also put Go in the goose typing  quadrant because of its\ntype assertions, which allow checking and adapting to dif ferent types at\nruntime.\nIf I had to draw a similar diagram in the year 2000, only the duck typing\nand the static typing  quadrants would have languages in them. I am not\naware of languages that supported static duck typing  or goose typing  20\nyears ago. The fact that each of the four quadrants have at least three\npopular languages suggests that a lot of people see value in each of the\nfour approaches to typing.\nMonkey Patching\nMonkey patching has a bad reputation. If abused, it can lead to systems\nthat are hard to understand and maintain. The patch is usually tightly\ncoupled with its tar get, making it brittle. Another problem is that two\nlibraries that apply monkey-patches may step on each other ’ s toes, with\nthe second library to run destroying patches of the first.\nBut monkey patching can also be useful, for example, to make a class\nimplement a protocol at runtime. The adapter design pattern solves the\nsame problem by implementing a whole new class.\nIt’ s easy to monkey-patch Python code, but there are limitations. Unlike\nRuby and JavaScript, Python does not let you monkey-patch the built-in\ntypes. I actually consider this an advantage, because you can be certain\nthat a str  object will always have those same methods. This limitation\nreduces the chance that external libraries apply conflicting patches.\nMetaphors and Idioms in Interfaces\nA metaphor fosters understanding by making constraints and\naf fordances clear . That’ s the value of the words “stack” and “queue” in\ndescribing those fundamental data structures: they make clear which\noperations ara allowed, i.e. how items can be added or removed. On the\nother hand, Alan Cooper writes in About Face, 4E  (W iley):\nStrict adher ence to metaphors ties interfaces unnecessarily tightly to\nthe workings of the physical world.\nHe’ s referring to user interfaces, but the admonition applies to APIs as\nwell. But Cooper does grant that when a “truly appropriate” metaphor\n“falls on our lap,” we can use it (he writes “falls on our lap” because it’ s\nso hard to find fitting metaphors that you should not spend time actively\nlooking for them). I believe the bingo machine imagery I used in this\nchapter is appropriate and I stand by it.\nAbout Face  is by far the best book about UI design I’ve read—and I’ve\nread a few . Letting go of metaphors as a design paradigm, and replacing\nit with “idiomatic interfaces” was the most valuable thing I learned\nfrom Cooper ’ s work.\nIn About Face , Cooper does not deal with APIs, but the more I think\nabout his ideas, the more I see how they apply to Python. The\nfundamental protocols of the language are what Cooper calls “idioms.”\nOnce we learn what a “sequence” is we can apply that knowledge in\ndif ferent contexts. This is a main theme of Fluent Python : highlighting\nthe fundamental idioms of the language, so your code is concise,\nef fective, and readable—for a fluent Pythonista.\n1  Design Patterns: Elements of Reusable Object-Oriented Software, Introduction, p. 18.\n2  The Monkey patch  article on W ikipedia has a funny example in Python.\n3  That’ s why automated testing is necessary .\n4  Bjarne Stroustrup, The Design and Evolution of C++  (Addison-W esley , 1994), p. 278.\n5  Retrieved October 18, 2020.\n6  Y ou can also, of course, define your own ABCs—but I would discourage all but the most\nadvanced Pythonistas from going that route, just as I would discourage them from defining\ntheir own custom metaclasses… and even for said “most advanced Pythonistas,” those of us\nsporting deep mastery of every fold and crease in the language, these are not tools for frequent\nuse: such “deep metaprogramming,” if ever appropriate, is intended for authors of broad\nframeworks meant to be independently extended by vast numbers of separate development\nteams… less than 1% of “most advanced Pythonistas” may ever need that! — A.M.\n7  Multiple inheritance was consider ed harmful  and excluded from Java, except for interfaces:\nJava interfaces can extend multiple interfaces, and Java classes can implement multiple\ninterfaces.\n8  Perhaps the client needs to audit the randomizer; or the agency wants to provide a rigged one.\nY ou never know…\n9  «registered» and «virtual subclass» are not standard UML terms. I am using them to represent\na class relationship that is specific to Python.\n10  Before ABCs existed, abstract methods would raise NotImplementedError  to signal that\nsubclasses were responsible for their implementation. In Smalltalk-80, abstract method bodies\nwould invoke subclassResponsibility , a method inherited from object  that would\nproduce an error with the message “My subclass should have overridden one of my messages.”\n11  The complete tree appears in section “5.4. Exception hierarchy” of The Python Standar d\nLibrary  documentation.\n12  @abc.abstractmethod  entry in the abc  module documentation .\n13  “Defensive Programming with Mutable Parameters”  in Chapter 6  was devoted to the aliasing\nissue we just avoided here.\n14  The same trick I used with load()  doesn’ t work with loaded() , because the list  type\ndoes not implement __bool__ , the method I’d have to bind to loaded . The bool()  built-\nin doesn’ t need __bool__  to work because it can also use __len__ . See “4.1. T ruth V alue\nT esting”  in the “Built-in T ypes” chapter of the Python documentation.\n15  There is a whole section explaining the __mro__  class attribute in “Multiple Inheritance and\nMethod Resolution Order” . Right now , this quick explanation will do.\n16  The concept of type consistency was explained in “Subtype-of versus Consistent-with” .\n17  OK, double()  is not very useful, except as an example. But the Python standard library has\nmany functions that could not be properly annotated before static protocols were added in\nPython 3.8. I helped fixing a couple of bugs in typeshed by adding type hints using protocols.\nFor example, the pull request that fixed Should Mypy warn about potential invalid ar guments\nto max ?  leveraged a _SupportsLessThan  protocol, which I used to enhance the\nannotations for max , min , sorted , and list.sort .\n18  The __slots__  attribute is irrelevant to the current discussion—it’ s an optimization we\ncovered in “Saving Memory with __slots__ ” .\n19  Read the Python Steering Council decision  on python-dev .\n20  Thanks to Guido van Rossum for telling me the reason why the complex.__float__\nmethod exists and to Ivan Levkivskyi for pointing out that inspecting type hints at runtime\nwould have an unacceptable performance cost. T ype checking is not just a matter of checking\nwhether the type of x  is T : it’ s about determining that the type of x  is consistent-with  T , which\nmay be expensive.\n21  For details and rationale, please see the section about @runtime_checkable  in PEP 544—\nProtocols: Structural subtyping (static duck typing)\n22  Again, please read Mer ging and extending protocols  in PEP 544 for details and rationale.\n23  I did not test all the other float and integer variants NumPy of fers\n24  The NumPy number types are all registered against the appropriate numbers  ABCs, but\nMypy ignores that fact.\n25  That’ s a well-meaning lie on the part of typeshed: as of Python 3.9, the built-in complex\ntype does not actually have a __complex__  method.",14121
183-Whats new in this chapter.pdf,183-Whats new in this chapter,"Chapter 14. Inheritance: For\nGood or For W orse\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 14th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\n— Alan Kay , The Early History of Smalltalk\nThis chapter is about inheritance and subclassing, with emphasis on two\nparticulars that are very specific to Python:\nThe pitfalls of subclassing from built-in types\nMultiple inheritance and the method resolution order\nMany consider multiple inheritance more trouble than it’ s worth. The lack\nof it certainly did not hurt Java; it probably fueled its widespread adoption\nafter many were traumatized by the excessive use of multiple inheritance in\nC++.\nHowever , the amazing success and influence of Java means that a lot of\nprogrammers come to Python without having seen multiple inheritance in\npractice. This is why , instead of toy examples, our coverage of multiple",1383
184-Subclassing Built-In Types Is Tricky.pdf,184-Subclassing Built-In Types Is Tricky,"inheritance will be illustrated by two important Python projects: the Tkinter\nGUI toolkit and the Django W eb framework.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThis chapter has only minor changes. As the title suggests, the caveats of\ninheritance have always been one of the main themes here. But more and\nmore software engineers consider it so problematic that I’ve added a couple\nof paragraphs about avoiding inheritance altogether to the end of “Chapter\nSummary”  and “Further Reading” .\nW e’ll start with the issue of subclassing built-ins. The rest of the chapter\nwill cover multiple inheritance with our case studies and discuss good and\nbad practices when building class hierarchies.\nS u b c l a s s i n g  B u i l t - I n  T y p e s  I s  T r i c k y\nBefore Python 2.2, it was not possible to subclass built-in types such as\nlist  or dict . Since then, it can be done but there is a major caveat: the\ncode of the built-ins (written in C) does not call special methods overridden\nby user -defined classes.\nA good short description of the problem is in the documentation for PyPy ,\nin “Dif ferences between PyPy and CPython”, section Subclasses of built-in\ntypes :\nOfficially , CPython has no rule at all for when exactly overridden method\nof subclasses of built-in types get implicitly called or not. As an\nappr oximation, these methods ar e never called by other built-in methods\nof the same object. For example, an overridden __getitem__()  in a\nsubclass of dict  will not be called by e.g. the built-in get()  method.\nExample 14-1  illustrates the problem.\nExample 14-1. Our __setitem__ override is ignor ed by the __init__ and\n__update__ methods of the built-in dict\n>>> class DoppelDict (dict): \n...     def __setitem__ (self, key, value): \n...         super().__setitem__ (key, [value] * 2)  \n \n... \n>>> dd = DoppelDict (one=1)  \n \n>>> dd \n{'one': 1}  \n>>> dd['two'] = 2  \n \n>>> dd \n{'one': 1, 'two': [2, 2]}  \n>>> dd.update(three=3)  \n \n>>> dd \n{'three': 3, 'one': 1, 'two': [2, 2]}\nDoppelDict.__setitem__  duplicates values when storing (for no\ngood reason, just to have a visible ef fect). It works by delegating to the\nsuperclass.\nThe __init__  method inherited from dict  clearly ignored that\n__setitem__  was overridden: the value of 'one'  is not duplicated.\nThe []  operator calls our __setitem__  and works as expected:\n'two'  maps to the duplicated value [2, 2] .\nThe update  method from dict  does not use our version of\n__setitem__  either: the value of 'three'  was not duplicated.\nThis built-in behavior is a violation of a basic rule of object-oriented\nprogramming: the search for methods should always start from the class of\nthe tar get instance ( self ), even when the call happens inside a method\nimplemented in a superclass. In this sad state of af fairs, the __missing__\nmethod—which we saw in “The __missing__  Method” —works as\ndocumented only because it’ s handled as a special case.\nThe problem is not limited to calls within an instance—whether\nself.get()  calls self.__getitem__() )—but also happens with\noverridden methods of other classes that should be called by the built-in\nmethods. Example 14-2  is an example adapted from the PyPy\ndocumentation .\nExample 14-2. The __getitem__ of AnswerDict is bypassed by dict.update\n>>> class AnswerDict (dict): \n...     def __getitem__ (self, key):  \n \n...         return 42 \n... \n>>> ad = AnswerDict (a='foo')  \n \n>>> ad['a']  \n \n42 \n>>> d = {} \n>>> d.update(ad)  \n \n>>> d['a']  \n \n'foo' \n>>> d \n{'a': 'foo'}\nAnswerDict.__getitem__  always returns 42 , no matter what the\nkey .\nad  is an AnswerDict  loaded with the key-value pair ('a',\n'foo') .\nad['a']  returns 42 , as expected.\nd  is an instance of plain dict , which we update with ad .\nThe dict.update  method ignored our\nAnswerDict.__getitem__ .\nW A R N I N G\nSubclassing built-in types like dict  or list  or str  directly is error -prone because the\nbuilt-in methods mostly ignore user -defined overrides. Instead of subclassing the built-\nins, derive your classes from the collections  module using UserDict ,\nUserList , and UserString , which are designed to be easily extended.\nIf you subclass collections.UserDict  instead of dict , the issues\nexposed in Examples 14-1  and 14-2  are both fixed. See Example 14-3 .\nExample 14-3. DoppelDict2 and AnswerDict2 work as expected because\nthey extend UserDict and not dict\n>>> import collections  \n>>> \n>>> class DoppelDict2 (collections .UserDict ): \n...     def __setitem__ (self, key, value): \n...         super().__setitem__ (key, [value] * 2) \n... \n>>> dd = DoppelDict2 (one=1) \n>>> dd \n{'one': [1, 1]}  \n>>> dd['two'] = 2 \n>>> dd \n{'two': [2, 2], 'one': [1, 1]}  \n>>> dd.update(three=3) \n>>> dd \n{'two': [2, 2], 'three': [3, 3], 'one': [1, 1]}  \n>>> \n>>> class AnswerDict2 (collections .UserDict ): \n...     def __getitem__ (self, key): \n...         return 42 \n... \n>>> ad = AnswerDict2 (a='foo') \n>>> ad['a'] \n42 \n>>> d = {} \n>>> d.update(ad) \n>>> d['a'] \n42 \n>>> d \n{'a': 42}\nAs an experiment to measure the extra work required to subclass a built-in,\nI rewrote the StrKeyDict  class from Example 3-9 . The original version\ninherited from collections.UserDict , and implemented just three\nmethods: __missing__ , __contains__ , and __setitem__ . The\nexperimental StrKeyDict  subclassed dict  directly , and implemented\nthe same three methods with minor tweaks due to the way the data was\nstored.",5535
185-Multiple Inheritance and Method Resolution Order.pdf,185-Multiple Inheritance and Method Resolution Order,"But in order to make it pass the same suite of tests, I had to implement\n__init__ , get , and update  because the versions inherited from dict\nrefused to cooperate with the overridden __missing__ ,\n__contains__ , and __setitem__ . The UserDict  subclass from\nExample 3-9  has 16 lines, while the experimental dict  subclass ended up\nwith 37 lines.\nT o summarize: the problem described in this section applies only to method\ndelegation within the C language implementation of the built-in types, and\nonly af fects user -defined classes derived directly from those types. If you\nsubclass from a class coded in Python, such as UserDict  or\nMutableMapping , you will not be troubled by this.\nAnother matter related to inheritance, particularly of multiple inheritance,\nis: how does Python decide which attribute to use if superclasses from\nparallel branches define attributes with the same name? The answer is next.\nM u l t i p l e  I n h e r i t a n c e  a n d  M e t h o d  R e s o l u t i o n\nO r d e r1\n2\nFigur e 14-1. Left : UML class diagram illustrating the “diamond pr oblem.” Right : Dashed arr ows\ndepict Python MRO (method r esolution or der) for Example 14-4 .\nAny language implementing multiple inheritance needs to deal with\npotential naming conflicts when unrelated ancestor classes implement a\nmethod by the same name. This is called the “diamond problem,” and is\nillustrated in Figure 14-1  and Example 14-4 .\nExample 14-4. diamond.py: classes A, B, C, and D form the graph in\nFigur e 14-1\nclass A: \n    def ping(self): \n        print('ping:', self) \n \n \nclass B(A): \n    def pong(self): \n        print('pong:', self) \n \n \nclass C(A): \n    def pong(self): \n        print('PONG:', self) \n \n \nclass D(B, C): \n \n    def ping(self): \n        super().ping() \n        print('post-ping:' , self) \n \n    def pingpong (self): \n        self.ping() \n        super().ping() \n        self.pong() \n        super().pong() \n        C.pong(self)\nNote that both classes B  and C  implement a pong  method. The only\ndif ference is that C.pong  outputs the word PONG  in uppercase.\nIf you call d.pong()  on an instance of D , which pong  method actually\nruns? In C++, the programmer must qualify method calls with class names\nto resolve this ambiguity . This can be done in Python as well. T ake a look at\nExample 14-5 .\nExample 14-5. T wo ways of invoking method pong on an instance of class D\n>>> from diamond import * \n>>> d = D() \n>>> d.pong()  \n \npong: <diamond.D object at 0x10066c278>  \n>>> C.pong(d)  \n \nPONG: <diamond.D object at 0x10066c278>\nSimply calling d.pong()  causes the B  version to run.\nY ou can always call a method on a superclass directly , passing the\ninstance as an explicit ar gument.\nThe ambiguity of a call like d.pong()  is resolved because Python follows\na specific order when traversing the inheritance graph. That order is called\nMRO: Method Resolution Order . Classes have an attribute called __mro__\nholding a tuple of references to the superclasses in MRO order , from the\ncurrent class all the way to the object  class. For the D  class, this is the\n__mro__  (see Figure 14-1 ):\n>>> D.__mro__ \n(<class 'diamond.D'>, <class 'diamond.B'>, <class 'diamond.C'>,  \n<class 'diamond.A'>, <class 'object'>)\nThe recommended way to delegate method calls to superclasses is the\nsuper()  built-in function, which became easier to use in Python 3, as\nmethod pingpong  of class D  in Example 14-4  illustrates.  However , it’ s\nalso possible, and sometimes convenient, to bypass the MRO and invoke a\nmethod on a superclass directly . For example, the D.ping  method could\nbe written as:\n    def ping(self): \n        A.ping(self)  # instead of super().ping()  \n        print('post-ping:' , self)3\nNote that when calling an instance method directly on a class, you must\npass self  explicitly , because you are accessing an unbound method .\nHowever , it’ s safest and more future-proof to use super() , especially\nwhen calling methods on a framework, or any class hierarchies you do not\ncontrol. Example 14-6  shows that super()  follows the MRO when\ninvoking a method.\nExample 14-6. Using super() to call ping (sour ce code in Example 14-4 )\n>>> from diamond import D \n>>> d = D() \n>>> d.ping()  \n \nping: <diamond.D object at 0x10cc40630>  \n  \npost-ping: <diamond.D object at 0x10cc40630>  \nThe ping  of D  makes two calls.\nThe first call is super().ping() ; the super  delegates the ping\ncall to class A ; A.ping  outputs this line.\nThe second call is print('post-ping:', self) , which outputs\nthis line.\nNow let’ s see what happens when pingpong  is called on an instance of D .\nSee Example 14-7 .\nExample 14-7. The five calls made by pingpong (sour ce code in\nExample 14-4 )\n>>> from diamond import D \n>>> d = D() \n>>> d.pingpong () \nping: <diamond.D object at 0x10bf235c0>  \n  \npost-ping: <diamond.D object at 0x10bf235c0>  \nping: <diamond.D object at 0x10bf235c0>  \n  \npong: <diamond.D object at 0x10bf235c0>  \n  \npong: <diamond.D object at 0x10bf235c0>  \n  \nPONG: <diamond.D object at 0x10bf235c0>  \n\nCall #1 is self.ping() , which runs the ping  method of D , which\noutputs this line and the next one.\nCall #2 is super().ping() , which bypasses the ping  in D  and\nfinds the ping  method in A .\nCall #3 is self.pong() , which finds the B  implementation of pong ,\naccording to the __mro__ .\nCall #4 is super().pong() , which finds the same B.pong\nimplementation, also following the __mro__ .\nCall #5 is C.pong(self) , which finds the C.pong  implementation,\nignoring the __mro__ .\nThe MRO takes into account not only the inheritance graph but also the\norder in which superclasses are listed in a subclass declaration. In other\nwords, if in diamond.py  ( Example 14-4 ) the D  class was declared as class\nD(C, B): , the __mro__  of class D  would be dif ferent: C  would be\nsearched before B .\nI often check the __mro__  of classes interactively when I am studying\nthem. Example 14-8  has some examples using familiar classes.\nExample 14-8. Inspecting the __mr o__ attribute in several classes\n>>> bool.__mro__  \n \n(<class 'bool'>, <class 'int'>, <class 'object'>)  \n>>> def print_mro (cls):  \n \n...     print(', '.join(c.__name__  for c in cls.__mro__)) \n... \n>>> print_mro (bool) \nbool, int, object  \n>>> from frenchdeck2  import FrenchDeck2  \n>>> print_mro (FrenchDeck2 )  \n \nFrenchDeck2, MutableSequence, Sequence, Sized, Iterable, Container,  \nobject \n>>> import numbers \n>>> print_mro (numbers.Integral )  \n \nIntegral, Rational, Real, Complex, Number, object  \n>>> import io  \n \n>>> print_mro (io.BytesIO) \nBytesIO, _BufferedIOBase, _IOBase, object  \n>>> print_mro (io.TextIOWrapper ) \nTextIOWrapper, _TextIOBase, _IOBase, object\nbool  inherits methods and attributes from int  and object .\nprint_mro  produces more compact displays of the MRO.\nThe ancestors of FrenchDeck2  include several ABCs from the\ncollections.abc  module.\nThese are the numeric ABCs provided by the numbers  module.\nThe io  module includes ABCs (those with the …Base  suf fix) and\nconcrete classes like BytesIO  and TextIOWrapper , which are the\ntypes of binary and text file objects returned by open() , depending on\nthe mode ar gument.\nN O T E\nThe MRO is computed using an algorithm called C3. The canonical paper on the Python\nMRO explaining C3 is Michele Simionato’ s “The Python 2.3 Method Resolution\nOrder” . If you are interested in the subtleties of the MRO, “Further Reading”  has other\npointers. But don’ t fret too much about this, the algorithm is sensible; as Simionato\nwrites:\n[…] unless you make str ong use of multiple inheritance and you have non-trivial\nhierar chies, you don’ t need to understand the C3 algorithm, and you can easily skip\nthis paper .\nT o wrap up this discussion of the MRO, Figure 14-2  illustrates part of the\ncomplex multiple inheritance graph of the Tkinter GUI toolkit from the\nPython standard library . T o study the picture, start at the Text  class at the\nbottom. The Text  class implements a full featured, multiline editable text\nwidget. It has rich functionality of its own, but also inherits many methods\nfrom other classes. The left-hand side shows a plain UML class diagram.\nOn the right, it’ s decorated with arrows showing the MRO, as listed here\nwith the help of the print_mro  convenience function defined in\nExample 14-8 :\n>>> import tkinter \n>>> print_mro (tkinter.Text) \nText, Widget, BaseWidget, Misc, Pack, Place, Grid, XView, YView,  \nobject\nIn the next section, we’ll discuss the pros and cons of multiple inheritance,\nwith examples from real frameworks that use it.",8725
186-Multiple Inheritance in the Real World.pdf,186-Multiple Inheritance in the Real World,"Figur e 14-2. Left : UML class diagram of the Tkinter T ext widget class and its super classes. Right :\nDashed arr ows depict T ext.__mr o__.\nM u l t i p l e  I n h e r i t a n c e  i n  t h e  R e a l  W o r l d\nIt is possible to put multiple inheritance to good use. The Adapter pattern in\nthe Design Patterns  book uses multiple inheritance, so it can’ t be\ncompletely wrong to do it (the remaining 22 patterns in the book use single\ninheritance only , so multiple inheritance is clearly not a cure-all).\nIn the Python standard library , the most visible use of multiple inheritance\nis the collections.abc  package. That is not controversial: after all,\neven Java supports multiple inheritance of interfaces, and ABCs are\ninterface declarations that may optionally provide concrete method\nimplementations.\nAn extreme example of multiple inheritance in the standard library is the\nTkinter GUI toolkit ( module tkinter : Python interface to T cl/Tk ). I used\npart of the Tkinter widget hierarchy to illustrate the MRO in Figure 14-2 ,\nbut Figure 14-3  shows all the widget classes in the tkinter  base package\n(there are more widgets in the tkinter.ttk  sub-package ).4\nFigur e 14-3. Summary UML diagram for the Tkinter GUI class hierar chy; classes tagged «mixin»\nar e designed to pr ovide concr ete methods to other classes via multiple inheritance\nTkinter is 20 years old as I write this, and is not an example of current best\npractices. But it shows how multiple inheritance was used when coders did\nnot appreciate its drawbacks. And it will serve as a counter -example when\nwe cover some good practices in the next section.\nConsider these classes from Figure 14-3 :\n➊  Toplevel : The class of a top-level window in a Tkinter application.\n➋  Widget : The superclass of every visible object that can be placed on a\nwindow .\n➌  Button : A plain button widget.\n➍  Entry : A single-line editable text field.\n➎  Text : A multiline editable text field.\nHere are the MROs of those classes, displayed by the print_mro\nfunction from Example 14-8 :\n>>> import tkinter \n>>> print_mro (tkinter.Toplevel ) \nToplevel, BaseWidget, Misc, Wm, object  \n>>> print_mro (tkinter.Widget) \nWidget, BaseWidget, Misc, Pack, Place, Grid, object  \n>>> print_mro (tkinter.Button) \nButton, Widget, BaseWidget, Misc, Pack, Place, Grid, object  \n>>> print_mro (tkinter.Entry) \nEntry, Widget, BaseWidget, Misc, Pack, Place, Grid, XView, object  \n>>> print_mro (tkinter.Text) \nText, Widget, BaseWidget, Misc, Pack, Place, Grid, XView, YView,  \nobject\nThings to note about how these classes relate to others:\nToplevel  is the only graphical class that does not inherit from\nWidget , because it is the top-level window and does not behave\nlike a widget—for example, it cannot be attached to a window or",2819
187-1. Distinguish Interface Inheritance from Implementation Inheritance.pdf,187-1. Distinguish Interface Inheritance from Implementation Inheritance,"frame. Toplevel  inherits from Wm , which provides direct access\nfunctions of the host window manager , like setting the window title\nand configuring its borders.\nWidget  inherits directly from BaseWidget  and from Pack ,\nPlace , and Grid . These last three classes are geometry\nmanagers: they are responsible for arranging widgets inside a\nwindow or frame. Each encapsulates a dif ferent layout strategy and\nwidget placement API.\nButton , like most widgets, descends only from Widget , but\nindirectly from Misc , which provides dozens of methods to every\nwidget.\nEntry  subclasses Widget  and XView , the class that implements\nhorizontal scrolling.\nText  subclasses from Widget , XView , and YView , which\nprovides vertical scrolling functionality .\nW e’ll now discuss some good practices of multiple inheritance and see\nwhether Tkinter goes along with them.\nC o p i n g  w i t h  M u l t i p l e  I n h e r i t a n c e\n[…] we needed a better theory about inheritance entir ely (and still do).\nFor example, inheritance and instancing (which is a kind of inheritance)\nmuddles both pragmatics (such as factoring code to save space) and\nsemantics (used for way too many tasks such as: specialization,\ngeneralization, speciation, etc.).\n— Alan Kay , The Early History of Smalltalk\nAs Alan Kay wrote, inheritance is used for dif ferent reasons, and multiple\ninheritance adds alternatives and complexity . It’ s easy to create\nincomprehensible and brittle designs using multiple inheritance. Because",1520
188-2. Make Interfaces Explicit with ABCs.pdf,188-2. Make Interfaces Explicit with ABCs,,0
189-4. Make Mixins Explicit by Naming.pdf,189-4. Make Mixins Explicit by Naming,"we don’ t have a comprehensive theory , here are a few tips to avoid\nspaghetti class graphs.\n1. Distinguish Interface Inheritance from Implementation\nInheritance\nWhen dealing with multiple inheritance, it’ s useful to keep straight the\nreasons why subclassing is done in the first place. The main reasons are:\nInheritance of interface creates a subtype, implying an “is-a”\nrelationship.\nInheritance of implementation avoids code duplication by reuse.\nIn practice, both uses are often simultaneous, but whenever you can make\nthe intent clear , do it. Inheritance for code reuse is an implementation detail,\nand it can often be replaced by composition and delegation. On the other\nhand, interface inheritance is the backbone of a framework.\n2. Make Interfaces Explicit with ABCs\nIn modern Python, if a class is designed to define an interface, it should be\nan explicit ABC. In Python ≥ 3.4, this means: subclass abc.ABC  or\nanother ABC (see “ABC Syntax Details”  if you need to support older\nPython versions).\n3. Use Mixins for Code Reuse\nIf a class is designed to provide method implementations for reuse by\nmultiple unrelated subclasses, without implying an “is-a” relationship, it\nshould be an explicit mixin class . Conceptually , a mixin does not define a\nnew type; it merely bundles methods for reuse. A mixin should never be\ninstantiated, and concrete classes should not inherit only from a mixin. Each\nmixin should provide a single specific behavior , implementing few and very\nclosely related methods.",1533
190-5. An ABC May Also Be a Mixin The Reverse Is Not True.pdf,190-5. An ABC May Also Be a Mixin The Reverse Is Not True,,0
191-6. Dont Subclass from More Than One Concrete Class.pdf,191-6. Dont Subclass from More Than One Concrete Class,,0
192-A Modern Example Mixins in Django Generic Views.pdf,192-A Modern Example Mixins in Django Generic Views,"4. Make Mixins Explicit by Naming\nThere is no formal way in Python to state that a class is a mixin, so it is\nhighly recommended that they are named with a …Mixin  suf fix. Tkinter\ndoes not follow this advice, but if it did, XView  would be XViewMixin ,\nPack  would be PackMixin , and so on with all the classes where I put the\n«mixin» tag in Figure 14-3 .\n5. An ABC May Also Be a Mixin; The Reverse Is Not T rue\nBecause an ABC can implement concrete methods, it works as a mixin as\nwell. An ABC also defines a type, which a mixin does not. And an ABC\ncan be the sole base class of any other class, while a mixin should never be\nsubclassed alone except by another , more specialized mixin—not a\ncommon arrangement in real code.\nOne restriction applies to ABCs and not to mixins: the concrete methods\nimplemented in an ABC should only collaborate with methods of the same\nABC and its superclasses. This implies that concrete methods in an ABC\nare always for convenience, because everything they do, a user of the class\ncan also do by calling other methods of the ABC.\n6. Don’t Subclass from More Than One Concrete Class\nConcrete classes should have zero or at most one concrete superclass.  In\nother words, all but one of the superclasses of a concrete class should be\nABCs or mixins. For example, in the following code, if Alpha  is a\nconcrete class, then Beta  and Gamma  must be ABCs or mixins:\nclass MyConcreteClass (Alpha, Beta, Gamma): \n    """"""This is a concrete class: it can be instantiated.""""""  \n    # ... more code ...\n7. Provide Aggregate Classes to Users\nIf some combination of ABCs or mixins is particularly useful to client code,\nprovide a class that brings them together in a sensible way . Grady Booch5\ncalls this an aggr egate class .\nFor example, here is the complete source code  for tkinter.Widget :\nclass Widget(BaseWidget , Pack, Place, Grid): \n    """"""Internal class.  \n \n    Base class for a widget which can be positioned with the  \n    geometry managers Pack, Place or Grid.""""""  \n    pass\nThe body of Widget  is empty , but the class provides a useful service: it\nbrings together four superclasses so that anyone who needs to create a new\nwidget does not need to remember all those mixins, or wonder if they need\nto be declared in a certain order in a class  statement. A better example of\nthis is the Django ListView  class, which we’ll discuss shortly , in “A\nModern Example: Mixins in Django Generic V iews” .\n8. “Favor Object Composition Over Class Inheritance.”\nThe title of this section is the second principle of object-oriented design\nfrom the Design Patterns  book,  and is the best advice I can of fer here.\nOnce you get comfortable with inheritance, it’ s too easy to overuse it.\nPlacing objects in a neat hierarchy appeals to our sense of order;\nprogrammers do it just for fun.\nHowever , favoring composition leads to more flexible designs. For\nexample, in the case of the tkinter.Widget  class, instead of inheriting\nthe methods from all geometry managers, widget instances could hold a\nreference to a geometry manager , and invoke its methods. After all, a\nWidget  should not “be” a geometry manager , but could use the services of\none via delegation. Then you could add a new geometry manager without\ntouching the widget class hierarchy and without worrying about name\nclashes. Even with single inheritance, this principle enhances flexibility ,\nbecause subclassing is a form of tight coupling, and tall inheritance trees\ntend to be brittle.6\n7\nComposition and delegation can replace the use of mixins to make\nbehaviors available to dif ferent classes, but cannot replace the use of\ninterface inheritance to define a hierarchy of types.\nW e will now analyze Tkinter from the point of view of these\nrecommendations.\nTkinter: The Good, the Bad, and the Ugly\nN O T E\nKeep in mind that Tkinter has been part of the standard library since Python 1.1 was\nreleased in 1994. Tkinter is a layer on top of the excellent Tk GUI toolkit of the T cl\nlanguage. The T cl/Tk combo is not originally object oriented, so the Tk API is basically\na vast catalog of functions. However , the toolkit is very object oriented in its concepts, if\nnot in its implementation.\nMost advice in the previous section is not followed by Tkinter , with #7\nbeing a notable exception. Even then, it’ s not a great example, because\ncomposition would probably work better for integrating the geometry\nmanagers into Widget , as discussed in #8.\nThe docstring of tkinter.Widget  starts with the words “Internal class.”\nThis suggests that Widget  should probably be an ABC. Although\nWidget  has no methods of its own, it does define an interface. Its message\nis: “Y ou can count on every Tkinter widget providing basic widget methods\n(__init__ , destroy , and dozens of Tk API functions), in addition to\nthe methods of all three geometry managers.” W e can agree that this is not a\ngreat interface definition (it’ s just too broad), but it is an interface, and\nWidget  “defines” it as the union of the interfaces of its superclasses.\nThe Tk  class, which encapsulates the GUI application logic, inherits from\nWm  and Misc , neither of which are abstract or mixin ( Wm  is not proper\nmixin because TopLevel  subclasses only from it). The name of the Misc\nclass is—by itself—a very strong code smell . Misc  has more than 100\nmethods, and all widgets inherit from it. Why is it necessary that every\nsingle widget has methods for clipboard handling, text selection, timer\nmanagement, and the like? Y ou can’ t really paste into a button or select text\nfrom a scrollbar . Misc  should be split into several specialized mixin\nclasses, and not all widgets should inherit from every one of those mixins.\nT o be fair , as a Tkinter user , you don’ t need to know or use multiple\ninheritance at all. It’ s an implementation detail hidden behind the widget\nclasses that you will instantiate or subclass in your own code. But you will\nsuf fer the consequences of excessive multiple inheritance when you type\ndir(tkinter.Button)  and try to find the method you need among the\n214 attributes listed.\nDespite the problems, Tkinter is stable, flexible, and not necessarily ugly .\nThe legacy (and default) Tk widgets are not themed to match modern user\ninterfaces, but the tkinter.ttk  package provides pretty , native-looking\nwidgets, making professional GUI development viable since Python 3.1\n(2009). Also, some of the legacy widgets, like Canvas  and Text , are\nincredibly powerful. W ith just a little coding, you can turn a Canvas\nobject into a simple drag-and-drop drawing application. Tkinter and T cl/Tk\nare definitely worth a look if you are interested in GUI programming.\nHowever , our theme here is not GUI programming, but the practice of\nmultiple inheritance. A more up-to-date example with explicit mixin classes\ncan be found in Django.\nA  M o d e r n  E x a m p l e :  M i x i n s  i n  D j a n g o\nG e n e r i c  V i e w s\nN O T E\nY ou don’ t need to know Django to follow this section. I am just using a small part of the\nframework as a practical example of multiple inheritance, and I will try to give all the\nnecessary background, assuming you have some experience with server -side web\ndevelopment in another language or framework.\nIn Django, a view is a callable object that takes, as ar gument, an object\nrepresenting an HTTP request and returns an object representing an HTTP\nresponse. The dif ferent responses are what interests us in this discussion.\nThey can be as simple as a redirect response, with no content body , or as\ncomplex as a catalog page in an online store, rendered from an HTML\ntemplate and listing multiple merchandise with buttons for buying and links\nto detail pages.\nOriginally , Django provided a set of functions, called generic views, that\nimplemented some common use cases. For example, many sites need to\nshow search results that include information from numerous items, with the\nlisting spanning multiple pages, and for each item a link to a page with\ndetailed information about it. In Django, a list view and a detail view are\ndesigned to work together to solve this problem: a list view renders search\nresults, and a detail view produces pages for individual items.\nHowever , the original generic views were functions, so they were not\nextensible. If you needed to do something similar but not exactly like a\ngeneric list view , you’d have to start from scratch.\nIn Django 1.3, the concept of class-based views was introduced, along with\na set of generic view classes or ganized as base classes, mixins, and ready-\nto-use concrete classes. The base classes and mixins are in the base\nmodule of the django.views.generic  package, pictured in\nFigure 14-4 . At the top of the diagram we see two classes that take care of\nvery distinct responsibilities: View  and TemplateResponseMixin .\nT I P\nA great resource to study these classes is the Classy Class-Based V iews  website, where\nyou can easily navigate through them, see all methods in each class (inherited,\noverridden, and added methods), view diagrams, browse their documentation, and jump\nto their source code on GitHub .\nView  is the base class of all views (it could be an ABC), and it provides\ncore functionality like the dispatch  method, which delegates to\n“handler” methods like get , head , post , etc., implemented by concrete\nsubclasses to handle the dif ferent HTTP verbs.  The RedirectView\nclass inherits only from View , and you can see that it implements get ,\nhead , post , etc.\nConcrete subclasses of View  are supposed to implement the handler\nmethods, so why aren’ t they part of the View  interface? The reason:\nsubclasses are free to implement just the handlers they want to support. A\nTemplateView  is used only to display content, so it only implements\nget . If an HTTP POST  request is sent to a TemplateView , the inherited\nView.dispatch  method checks that there is no post  handler , and\nproduces an HTTP 405 Method Not Allowed  response.8\n9\nFigur e 14-4. UML class diagram for the django.views.generic.base module\nThe TemplateResponseMixin  provides functionality that is of interest\nonly to views that need to use a template. A RedirectView , for example,\nhas no content body , so it has no need of a template and it does not inherit\nfrom this mixin. TemplateResponseMixin  provides behaviors to\nTemplateView  and other template-rendering views, such as ListView ,\nDetailView , etc., defined in other modules of the\ndjango.views.generic  package. Figure 14-5  depicts the\ndjango.views.generic.list  module and part of the base  module.\nFigur e 14-5. UML class diagram for the django.views.generic.list module. Her e the thr ee classes of\nthe base module ar e collapsed (see Figur e 14-4 ). The ListV iew class has no methods or attributes: it’ s\nan aggr egate class.\nFor Django users, the most important class in Figure 14-5  is ListView ,\nwhich is an aggregate class, with no code at all (its body is just a docstring).\nWhen instantiated, a ListView  has an object_list  instance attribute\nthrough which the template can iterate to show the page contents, usually\nthe result of a database query returning multiple objects. All the\nfunctionality related to generating this iterable of objects comes from the\nMultipleObjectMixin . That mixin also provides the complex\npagination logic—to display part of the results in one page and links to\nmore pages.\nSuppose you want to create a view that will not render a template, but will\nproduce a list of objects in JSON format. That’ s why the BaseListView\nexists. It provides an easy-to-use extension point that brings together View\nand MultipleObjectMixin  functionality , without the overhead of the\ntemplate machinery .\nThe Django class-based views API is a better example of multiple\ninheritance than Tkinter . In particular , it is easy to make sense of its mixin\nclasses: each has a well-defined purpose, and they are all named with the …\nMixin  suf fix.\nClass-based views were not universally embraced by Django users. Many\ndo use them in a limited way , as black boxes, but when it’ s necessary to\ncreate something new , a lot of Django coders continue writing monolithic\nview functions that take care of all those responsibilities, instead of trying\nto reuse the base views and mixins.\nIt does take some time to learn how to leverage class-based views and how\nto extend them to fulfill specific application needs, but I found that it was\nworthwhile to study them: they eliminate a lot of boilerplate code, make it\neasier to reuse solutions, and even improve team communication—for\nexample, by defining standard names to templates, and to the variables\npassed to template contexts. Class-based views are Django views “on rails.”",12894
193-Chapter Summary.pdf,193-Chapter Summary,"This concludes our tour of multiple inheritance and mixin classes.\nC h a p t e r  S u m m a r y\nW e started our coverage of inheritance explaining the problem with\nsubclassing built-in types: their native methods implemented in C do not\ncall overridden methods in subclasses, except in very few special cases.\nThat’ s why , when we need a custom list , dict , or str  type, it’ s easier\nto subclass UserList , UserDict , or UserString —all defined in the\ncollections  module , which actually wraps the built-in types and\ndelegate operations to them—three examples of favoring composition over\ninheritance in the standard library . If the desired behavior is very dif ferent\nfrom what the built-ins of fer , it may be easier to subclass the appropriate\nABC from collections.abc  and write your own implementation.\nThe rest of the chapter was devoted to the double-edged sword of multiple\ninheritance. First we saw how the method resolution order , encoded in the\n__mro__  class attribute, addresses the problem of potential naming\nconflicts in inherited methods. W e also saw how the super()  built-in\nfollows the __mro__  to call a method on a superclass. W e then studied\nhow multiple inheritance is used in the Tkinter GUI toolkit that comes with\nthe Python standard library . Tkinter is not an example of current best\npractices, so we discussed some ways of coping with multiple inheritance,\nincluding careful use of mixin classes and avoiding multiple inheritance\naltogether by using composition instead. After considering how multiple\ninheritance is abused in Tkinter , we wrapped up by studying the core parts\nof the Django class-based views hierarchy , which I consider a better\nexample of mixin usage.\nLennart Regebro—a very experienced Pythonista and one of first edition’ s\ntechnical reviewers—finds the design of Django’ s mixin views hierarchy\nconfusing. But he also wrote:\nThe dangers and badness of multiple inheritance ar e gr eatly overblown.\nI’ve actually never had a r eal big pr oblem with it.\nIn the end, each of us may have dif ferent opinions about how to use\nmultiple inheritance, or whether to use it at all in our own projects.",2183
194-Further Reading.pdf,194-Further Reading,"Meanwhile, rejecting inheritance—even single inheritance—is a growing\ntrend. One of the most successful languages created in the 21st century is\nGo. It doesn’ t have a construct called “class”, but you can build types that\nare structs of encapsulated fields and you can attach methods to those\nstructs. Go allows the definition of interfaces that are checked by the\ncompiler using structural typing, a.k.a. static duck typing —very similar to\nwhat we now have with protocol types since Python 3.8. Go has special\nsyntax for building types and interfaces by composition, but it does not\nsupport inheritance—not even among interfaces.\nSo perhaps the best advice about inheritance is: avoid it if you can. But\noften, we don’ t have a choice: the frameworks we use impose their own\ndesign choices.\nF u r t h e r  R e a d i n g\nWhen using ABCs, multiple inheritance is not only common but practically\ninevitable, because each of the most fundamental collection ABCs\n(Sequence , Mapping , and Set ) extend multiple ABCs. The source code\nfor collections.abc  ( Lib/_collections_abc.py ) is a good example of\nmultiple inheritance with ABCs—many of which are also mixin classes.\nRaymond Hettinger ’ s post Python’ s super() considered super!  explains the\nworkings of super  and multiple inheritance in Python from a positive\nperspective. It was written in response to Python’ s Super is nifty , but you\ncan’ t use it (a.k.a. Python’ s Super Considered Harmful)  by James Knight.\nDespite the titles of those posts, the problem is not really the super  built-\nin—which in Python 3 is not as ugly as it was in Python 2. The real issue is\nmultiple inheritance, which is inherently complicated and tricky . Michele\nSimionato goes beyond criticizing and actually of fers a solution in his\nSetting Multiple Inheritance Straight : he implements traits, a constrained\nform of mixins that originated in the Self language. Simionato has a long\nseries of illuminating blog posts about multiple inheritance in Python,\nincluding The wonders of cooperative inheritance, or using super in Python\n3 ; Mixins considered harmful, part 1  and part 2 ; and Things to Know About\nPython Super , part 1 , part 2  and part 3 . The oldest posts use the Python 2\nsuper  syntax, but are still relevant.\nI read the first edition of Grady Booch’ s Object-Oriented Analysis and\nDesign, 3E  (Addison-W esley , 2007), and highly recommend it as a general\nprimer on object-oriented thinking, independent of programming language.\nIt is a rare book that covers multiple inheritance without prejudice.\nIn 2021, it’ s more fashionable than ever to avoid inheritance, so here are\ntwo references about how to do that. Brandon Rhodes wrote The\nComposition Over Inheritance Principle , part of his excellent Python\nDesign Patterns  guide on the W eb. Augie Fackler and Nathaniel Manista\npresented The End Of Object Inheritance & The Beginning Of A New\nModularity  at PyCon 2013—that was before I wrote the first edition, but I\nonly found it in 2019. Fackler and Manista talk about or ganizing systems\naround interfaces and functions that handle objects implementing those\ninterfaces, avoiding the tight coupling and failure modes of classes and\ninheritance. That reminds me a lot of the Go way , but they advocate it for\nPython.\nS O A P B O X\nThink About the Classes Y ou Really Need\nThe vast majority of programmers write applications, not frameworks.\nEven those who do write frameworks are likely to spend a lot (if not\nmost) of their time writing applications. When we write applications,\nwe normally don’ t need to code class hierarchies. At most, we write\nclasses that subclass from ABCs or other classes provided by the\nframework. As application developers, it’ s very rare that we need to\nwrite a class that will act as the superclass of another . The classes we\ncode are almost always leaf classes (i.e., leaves of the inheritance tree).\nIf, while working as an application developer , you find yourself\nbuilding multilevel class hierarchies, it’ s likely that one or more of the\nfollowing applies:\nY ou are reinventing the wheel. Go look for a framework or\nlibrary that provides components you can reuse in your\napplication.\nY ou are using a badly designed framework. Go look for an\nalternative.\nY ou are overengineering. Remember the KISS principle .\nY ou became bored coding applications and decided to start a\nnew framework. Congratulations and good luck!\nIt’ s also possible that all of the above apply to your situation: you\nbecame bored and decided to reinvent the wheel by building your own\noverengineered and badly designed framework, which is forcing you to\ncode class after class to solve trivial problems. Hopefully you are\nhaving fun, or at least getting paid for it.\nMisbehaving Built-ins: Bug or Featur e?\nThe built-in dict , list , and str  types are essential building blocks\nof Python itself, so they must be fast—any performance issues in them\nwould severely impact pretty much everything else. That’ s why\nCPython adopted the shortcuts that cause their built-in methods to\nmisbehave by not cooperating with methods overridden by subclasses.\nA possible way out of this dilemma would be to of fer two\nimplementations for each of those types: one “internal,” optimized for\nuse by the interpreter and an external, easily extensible one.\nBut wait, this is what we have: UserDict , UserList , and\nUserString  are not as fast as the built-ins but are easily extensible.\nThe pragmatic approach taken by CPython means we also get to use, in\nour own applications, the highly optimized implementations that are\nhard to subclass. Which makes sense, considering that it’ s not so often\nthat we need a custom mapping, list, or string, but we use dict , list\nand str  every day . W e just need to be aware of the trade-of fs involved.\nInheritance Acr oss Languages\nAlan Kay coined the term “object oriented,” and Smalltalk had only\nsingle inheritance, although there are forks with various forms of\nmultiple inheritance support, including the modern Squeak and Pharo\nSmalltalk dialects that support traits—a language construct that fulfills\nthe role of a mixin class, while avoiding some of the issues with\nmultiple inheritance.\nThe first popular language to implement multiple inheritance was C++,\nand the feature was abused enough that Java—intended as a C++\nreplacement—was designed without support for multiple inheritance of\nimplementation (i.e., no mixin classes). That is, until Java 8 introduced\ndefault methods that make interfaces very similar to the abstract classes\nused to define interfaces in C++ and in Python. Except that Java\ninterfaces cannot have state—a key distinction. After Java, probably the\nmost widely deployed JVM language is Scala, and it implements traits.\nOther languages supporting traits are the latest stable versions of PHP\nand Groovy , and the under -construction languages Rust and Perl 6—so\nit’ s fair to say that traits are trendy as I write this.\nRuby of fers an original take on multiple inheritance: it does not support\nit, but introduces mixins as a language feature. A Ruby class can\ninclude a module in its body , so the methods defined in the module\nbecome part of the class implementation. This is a “pure” form of\nmixin, with no inheritance involved, and it’ s clear that a Ruby mixin\nhas no influence on the type of the class where it’ s used. This provides\nthe benefits of mixins, while avoiding many of its usual problems.\nT wo new object-oriented languages that are getting a lot of attention\nseverely limit inheritance: Go and Julia. Both are about programming\n“objects”, but they avoid the term “class”. Go has no inheritance at all.\nJulia has a type hierarchy but subtypes cannot inherit structure, only\nbehaviors, and only abstract types can be subtyped. In addition, Julia\nmethods are implemented using multiple dispatch—a more advanced\nform of the mechanism we saw in “Single Dispatch Generic Functions” .\n1  If you are curious, the experiment is in the strkeydict_dictsub.py  file in the Fluent Python\ncode repository .\n2  By the way , in this regard, PyPy behaves more “correctly” than CPython, at the expense of\nintroducing a minor incompatibility . See “Dif ferences between PyPy and CPython”  for details.\n3  In Python 2, the second line of D.pingpong  would be written as super(D,\nself).ping()  rather than super().ping() .\n4  As previously mentioned, Java 8 allows interfaces to provide method implementations as\nwell. The new feature is called Default Methods  in the of ficial Java T utorial.\n5  In “W aterfowl and ABCs” , Alex Martelli quotes Scott Meyer ’ s Mor e Effective C++ , which\ngoes even further: “all non-leaf classes should be abstract” (i.e., concrete classes should not\nhave concrete superclasses at all).\n6  “A class that is constructed primarily by inheriting from mixins and does not add its own\nstructure or behavior is called an aggr egate class .”, Grady Booch et al., Object Oriented\nAnalysis and Design, 3E  (Addison-W esley , 2007), p. 109.\n7  Erich Gamma, Richard Helm, Ralph Johnson and John Vlissides, Design Patterns: Elements\nof Reusable Object-Oriented Softwar e , Intr oduction , p. 20.\n8  Django programmers know that the as_view  class method is the most visible part of the\nView  interface, but it’ s not relevant to us here.\n9  If you are into design patterns, you’ll notice that the Django dispatch mechanism is a dynamic\nvariation of the T emplate Method pattern . It’ s dynamic because the View  class does not force\nsubclasses to implement all handlers, but dispatch  checks at runtime if a concrete handler is\navailable for the specific request.",9756
195-Whats new in this chapter.pdf,195-Whats new in this chapter,"Chapter 15. More About T ype\nHints\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 15th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nI learned a painful lesson that for small pr ograms, dynamic typing is\ngr eat. For lar ge pr ograms you need a mor e disciplined appr oach. And it\nhelps if the language gives you that discipline rather than telling you\n“W ell, you can do whatever you want”.\n— Guido van Rossum, a fan of Monty Python\nThis chapter is a sequel to Chapter 8 , covering more of Python’ s gradual\ntype system. The main topics are:\nOverloaded function signatures;\ntyping.TypedDict  for type hinting dicts  used as records;\nT ype casting;\nRuntime access to type hints;1",1201
196-Overloaded signatures.pdf,196-Overloaded signatures,"Generic types:\nDeclaring a generic class;\nV ariance: invariant, covariant, and contravariant types;\nGeneric static protocols.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThis chapter is new in Fluent Python, Second Edition .\nLet’ s start with a subject that really belonged in Chapter 8 , but I moved it\nhere because that was already the longest chapter in the book.\nO v e r l o a d e d  s i g n a t u r e s\nSome Python functions accept dif ferent combinations of ar guments. The\n@typing.overload  allows annotating each dif ferent combination. This\nis particularly important when the return type of the function depends on\nthe type of two or more parameters.\nConsider the sum  built-in function. This is the text of help(sum) :\n>>> help(sum)  \nsum(iterable, /, start=0)  \n    Return the sum of a 'start' value (default: 0) plus an  \niterable of numbers  \n \n    When the iterable is empty, return the start value.  \n    This function is intended specifically for use with numeric  \nvalues and may  \n    reject non-numeric types.\nThe sum  built-in is written in C, but typeshed  has overloaded type hints for\nit, in builtins.pyi :\n@overload  \ndef sum(__iterable : Iterable [_T]) -> Union[_T, int]: ... \n@overload  \ndef sum(__iterable : Iterable [_T], start: _S) -> Union[_T, _S]: \n...\nFirst let’ s look at the overall syntax of overloads. On a stub file ( .pyi ),\nthat’ s all there would be about sum —the implementation would be in a\ndif ferent file.\nThe type checker tries to match the given ar guments with each overloaded\nsignature, in order . The call sum(range(100), 1000)  doesn’ t match\nthe first overload, but matches the second.\nY ou can also use @overload  in a regular Python module, by writing the\noverloaded signatures right before the function’ s actual signature and\nimplementation. Example 15-1  shows how sum  would appear annotated\nand implemented in a Python module.\nExample 15-1. mysum.py : definition of the sum  function with overloaded\nsignatur es:\nimport functools  \nimport operator  \nfrom collections .abc import Iterable  \nfrom typing import overload , Union, TypeVar \n \nT = TypeVar('T') \nS = TypeVar('S')  \n \n \n@overload  \ndef sum(it: Iterable [T]) -> Union[T, int]: ...  \n \n@overload  \ndef sum(it: Iterable [T], /, start: S) -> Union[T, S]: ...  \n \ndef sum(it, /, start=0):  \n \n    return functools .reduce(operator .add, it, start)\nW e need this second TypeVar  in the second overload.\nThis signature is for the simple case: sum(my_iterable) . The result\ntype may be T —the type of the elements that my_iterable  yields—\nor it may be int  if the iterable is empty , because the default value of\nthe start  parameter is 0 .",2719
197-Max Overload.pdf,197-Max Overload,"When start  is given, it can be of any type S , so the result type is\nUnion[T, S] . This is why we need S . If we reused T  then the type of\nstart  would have to be the same type as the elements of\nIterable[T] .\nThe signature of the actual function implementation has no type hints.\nThat’ s a lot of lines to annotate a one-line function. Probably overkill, I\nknow . At least it wasn’ t a foo  function.\nIf you want to learn about @overload  by reading code, typeshed  has\nhundreds of examples. On typeshed , the stub file  for Python’ s built-ins has\n186 overloads as I write this—more than any other in the standard library .\nT A K E  A D V A N T A G E  O F  G R A D U A L  T Y P I N G\nAiming for 100% of annotated code may lead to type hints that add lots of noise but\nlittle value. Refactoring to simplify type hinting can lead to cumbersome APIs.\nSometimes it’ s better to be pragmatic and leave a piece of code without type hints.\nThe handy APIs we call Pythonic are often hard to annotate. In the next\nsection we’ll see example of this: six overloads are needed to properly\nannotate the flexible max  built-in function.\nMax Overload\nIt is dif ficult to add type hints to functions that leverage the powerful\ndynamic features of Python.\nWhile studying typeshed , I found bug report ( #4051 ): Mypy failed to warn\nthat it is illegal to pass None  as one of the ar guments to the built-in max()\nfunction, or to pass an iterable that at some point yields None . In either\ncase, you get a runtime exception like this one:\nTypeError: '>' not supported between instances of 'int' and  \n'NoneType'\nThe documentation of max  starts with this sentence:\nReturn the lar gest item in an iterable or the lar gest of two or mor e\nar guments.\nT o me, that’ s a very intuitive description.\nBut if I must annotate a function described in those terms, I have to ask:\nwhich is it? An iterable or two or more ar guments?\nThe reality is more complicated because max  also takes two optional\nkeyword ar guments: key  and default .\nI coded max  in Python to make it easier to test (the original max  is in C).\ndef max(first, *args, key=None, default=MISSING): \n    if args: \n        series = args \n        candidate  = first \n    else: \n        series = iter(first) \n        try: \n            candidate  = next(series) \n        except StopIteration : \n            if default is not MISSING: \n                return default \n            raise ValueError (EMPTY_MSG ) from None \n    if key is None: \n        for current in series: \n            if candidate  < current: \n                candidate  = current \n    else: \n        candidate_key  = key(candidate ) \n        for current in series: \n            current_key  = key(current) \n            if candidate_key  < current_key : \n                candidate  = current \n                candidate_key  = current_key  \n    return candidate\nT o fix issue #4051 , I wrote the code in Example 15-2 .2\nExample 15-2.\nfrom typing import Protocol , Any, TypeVar, overload , Callable , \nIterable , Union \n \nclass SupportsLessThan (Protocol ): \n    def __lt__(self, other: Any) -> bool: ... \n \nT = TypeVar('T') \nLT = TypeVar('LT', bound=SupportsLessThan ) \nDT = TypeVar('DT') \n \nMISSING = object() \nEMPTY_MSG  = 'max() arg is an empty sequence'  \n \n@overload  \ndef max(__arg1: LT, __arg2: LT, *args: LT, key: None = ...) -> LT: \n    ... \n@overload  \ndef max(__arg1: T, __arg2: T, *args: T, key: Callable [[T], LT]) -> \nT: \n    ... \n@overload  \ndef max(__iterable : Iterable [LT], *, key: None = ...) -> LT: \n    ... \n@overload  \ndef max(__iterable : Iterable [T], *, key: Callable [[T], LT]) -> T: \n    ... \n@overload  \ndef max(__iterable : Iterable [LT], *, key: None = ..., \n        default: DT) -> Union[LT, DT]: \n    ... \n@overload  \ndef max(__iterable : Iterable [T], *, key: Callable [[T], LT], \n        default: DT) -> Union[T, DT]: \n    ...\nMy Python implementation of max  is about the same length as all those\ntyping imports and declarations. Thanks to duck typing, my code has no\nisinstance  checks, and provides the same error checking as those type\nhints—but only at runtime, of course.\nThe double underscore prefix in some ar guments is a convention used on\ntypeshed  for positional-only ar guments. That means you can call max(10,\n20) , but not max(__arg1=10, __arg2=20) .\nA key benefit of @overload  making the return type as precise as possible,\naccording to the types of the ar guments given. Let’ s study the overloads for\nmax  in groups.\nInputs implementing SupportsLessThan , no default=\n@overload  \ndef max(__arg1: LT, __arg2: LT, *_args: LT, key: None = ...) -> \nLT: \n    ... \n# ... lines omitted ...  \n@overload  \ndef max(__iterable : Iterable [LT], *, key: None = ...) -> LT: \n    ...\nIn these cases the inputs are either separate ar guments of type LT\nimplementing SupportsLessThan , or an Iterable  of such items.\nThe return type of max  is the same as the actual ar guments or items, as\ndescribed in [Link to Come].\nSample calls that match these overloads:\nmax(1, 2, -3)  # returns 2  \nmax(['Go', 'Python' , 'Rust'])  # returns 'Rust'\nkey=  provided, no default=\n@overload  \ndef max(__arg1: T, __arg2: T, *_args: T, key: Callable [[T], LT]) \n-> T: \n    ... \n# ... lines omitted ...  \n@overload  \ndef max(__iterable : Iterable [T], *, key: Callable [[T], LT]) -> T: \n    ...\nThe inputs can be separate items of any type T  or a single Iterable[T] ,\nand key=  must be a callable that takes an ar gument of the same type T , and\nreturns a value that implements SupportsLessThan . The return type of\nmax  is the same as the actual ar guments.\nSample calls that match these overloads:\nmax(1, 2, -3, key=abs)  # returns -3  \nmax(['Go', 'Python' , 'Rust'], key=len)  # returns 'Python'\ndefault=  provided, no key=\n@overload  \ndef max(__iterable : Iterable [LT], *, key: None = ..., \n        default: DT) -> Union[LT, DT]: \n    ...\nThe input is an iterable of items of type LT  implementing\nSupportsLessThan . The default=  ar gument is the return value\nwhen the Iterable  is empty . Therefore the return type of max  must be a\nUnion  of type LT  or the type of the default  ar gument.\nSample calls that match these overloads:\nmax([1, 2, -3], default=0)  # returns 2  \nmax([], default=None)  # returns None\nkey=  and default=  provided\n@overload  \ndef max(__iterable : Iterable [T], *, key: Callable [[T], LT], \n        default: DT) -> Union[T, DT]: \n    ...\nThe inputs are:\nan Iterable  of items of any type T ;\ncallable that takes an ar gument of type T  and returns a value of\ntype LT  that implements SupportsLessThan ;\na default value of any type DT .",6753
198-Takeaways from Overloading max.pdf,198-Takeaways from Overloading max,,0
199-TypedDict.pdf,199-TypedDict,"The return type of max  must be a Union  of type T  or the type of the\ndefault  ar gument.\nmax([1, 2, -3], key=abs, default=None)  # returns -3  \nmax([], key=abs, default=None)  # returns None\nT akeaways from Overloading max\nT ype hints allow Mypy to flag a call like max([None, None])  with this\nerror message:\nmymax_demo.py:109: error: Value of type variable ""_LT"" of ""max""  \n  cannot be ""None""\nOn the other hand, having to write so many lines to support the type\nchecker may discourage people from writing convenient and flexible\nfunctions like max . If I had to reinvent the min  function as well, I could\nrefactor and reuse most of the implementation of max . But I’d have to copy\n& paste all overloaded declarations—even though they would be identical\nfor min , except for the function name.\nMy friend João S. O. Bueno—one of the smartest Python devs I know—\ntweeted this :\nAlthough it is this har d to expr ess the signatur e of max —it fits in one’ s\nmind quite easily . My understanding is that the expr essiveness of\nannotation markings is very limited, compar ed to that of Python.\nNow let’ s study the TypedDict  typing construct. It is not as useful as I\nimagined at first, but has its uses. Experimenting with TypedDict\ndemonstrates the limitations of static typing for handling dynamic structures\nsuch as JSON data.\nT y p e d D i c t\nW A R N I N G\nIt’ s tempting to use TypedDict  to protect against errors while handling dynamic data\nstructures like JSON API responses. But the examples here make clear that correct\nhandling of JSON must be done at runtime, and not with static type checking. For\nruntime checking of JSON-like structures using type hints, check out the pydantic\npackage on PyPI.\nPython dictionaries are sometimes used as records, with the keys used as\nfield names and field values of dif ferent types.\nFor example, consider a record describing a book in JSON or Python:\n{""isbn"": ""0134757599"" , \n ""title"": ""Refactoring, 2e"" , \n ""authors"" : [""Martin Fowler"" , ""Kent Beck"" ], \n ""pagecount"" : 478}\nBefore Python 3.8, there was no good way to annotate a record like that,\nbecause the mapping types we saw in “Generic mappings”  limit all values\nto have the same type.\nHere are two lame attempts to annotate a record like the JSON object\nabove:\nDict[str, Any]\nThe values may be of any type.\nDict[str, Union[str, int, List[str]]]\nHard to read, and doesn’ t preserve the relationship between field names\nand their respective field types: title  is supposed to be a str , it can’ t\nbe an int  or a List[str] .\nPEP 589—T ypedDict: T ype Hints for Dictionaries with a Fixed Set of Keys\naddressed that problem. Here is a simple TypedDict :\nExample 15-3. books.py : the BookDict  definition.\nfrom typing import TypedDict  \nimport json \n \nclass BookDict (TypedDict ): \n    isbn: str \n    title: str \n    authors: list[str] \n    pagecount : int\nAt first glance, typing.TypedDict  may seem like a data class builder ,\nsimilar to typing.NamedTuple —covered in Chapter 5 .\nThe syntactic similarity is misleading. TypedDict  is very dif ferent. It\nexists only for the benefit of type checkers, and has no runtime ef fect.\nTypedDict  provides two things:\n1 . Class-like syntax to annotate a dict  with type hints for the value\nof each “field”.\n2 . A constructor that tells the type checker to expect a dict  with the\nkeys and values as specified.\nAt runtime, a TypedDict  constructor such as BookDict  is placebo: it\nhas the same ef fect as calling the dict  constructor with the same\nar guments.\nThe fact that BookDict  creates a plain dict  also means that:\nThe “fields” in the pseudo-class definition don’ t create instance\nattributes.\nY ou can’ t write initializers with default values for the “fields”.\nMethod definitions are not allowed.\nLet’ s explore the behavior of a BookDict  at runtime.\nExample 15-4. Using a BookDict , but not quite as intended.\n>>> from books import BookDict  \n>>> pp = BookDict (title='Programming Pearls ',  \n \n...               authors='Jon Bentley ',  \n \n...               isbn='0201657880 ', \n...               pagecount =256) \n>>> pp  \n \n{'title': 'Programming Pearls', 'authors': 'Jon Bentley', 'isbn':  \n'0201657880',  \n 'pagecount': 256}  \n>>> type(pp) \n<class 'dict'>  \n>>> pp.title  \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nAttributeError : 'dict' object has no attribute 'title'  \n>>> pp['title'] \n'Programming Pearls'  \n>>> BookDict .__annotations__   \n \n{'isbn': <class 'str'>, 'title': <class 'str'>, 'authors':  \ntyping.List[str],  \n 'pagecount': <class 'int'>}\nY ou can call BookDict  like a dict  constructor with keyword\nar guments, or passing a dict  ar gument—including a dict  literal.\nOoops… I for got authors  takes a list. But gradual typing means no\ntype checking at runtime.\nThe result of calling BookDict  is a plain dict …\n… therefore you can’ t read the data using object.field  notation.\nThe type hints are in BookDict.__annotations__ , and not in\npp .\nW ithout a type checker , TypedDict  is as useful as comments: it may help\npeople read the code, but that’ s it. In contrast, the class builders from\nChapter 5  are useful even if you don’ t use a type checker because at runtime\nthey generate or enhance a custom class that you can instantiate. They also\nprovide several useful methods or functions listed in T able 5-1 .\nExample 15-5  builds a valid BookDict  and tries some operations on it.\nThis shows how TypedDict  enables Mypy to catch errors, shown in\nExample 15-6 .\nExample 15-5. demo_books.py : legal and ilegal operations on a\nBookDict .\nfrom books import BookDict  \nfrom typing import TYPE_CHECKING  \n \ndef demo() -> None:  \n \n    book = BookDict (  \n \n        isbn='0134757599 ', \n        title='Refactoring, 2e ', \n        authors=['Martin Fowler ', 'Kent Beck '], \n        pagecount =478 \n    ) \n    authors = book['authors'] \n \n    if TYPE_CHECKING :  \n \n        reveal_type (authors)  \n \n    authors = 'Bob'  \n \n    book['weight'] = 4.2 \n    del book['title'] \n \n \nif __name__  == '__main__ ': \n    demo()\nRemember to add a return type, so that Mypy doesn’ t ignore the\nfunction.\nThis is a valid BookDict : all the keys are present, with values of the\ncorrect types.\nMypy will infer the type of authors  from the annotation for the\n'authors'  key in BookDict .\ntyping.TYPE_CHECKING  is only True  when the program is being\ntype checked. At runtime, it’ s always false.\nThe previous if  statement prevents reveal_type(authors)  from\nbeing called at runtime. reveal_type  is not a runtime Python\nfunction, but a debugging facility provided by Mypy . That’ s why there\nis no import  for it. See its output in Example 15-6 .\nThe last three lines of the demo  function are illegal. They will cause\nerror messages in Example 15-6 .\nT ype checking demo_books.py  from Example 15-5 , this is what we get:\nExample 15-6. T ype checking demo_books.py .\n…/typeddict/ $ mypy demo_books.py  \ndemo_books.py:13: note: Revealed type is 'built-ins.list[built-\nins.str]'  \n  \ndemo_books.py:14: error: Incompatible types in assignment  \n                  (expression has type ""str"", variable has type  \n""List[str]"")  \n  \ndemo_books.py:15: error: TypedDict ""BookDict"" has no key 'weight'   \n \ndemo_books.py:16: error: Key 'title' of TypedDict ""BookDict"" cannot  \nbe deleted  \n  \nFound 3 errors in 1 file (checked 1 source file)\nThis note is the result of reveal_type(authors) .\nThe type of the authors  variable was inferred from the type of the\nbook['authors']  expression that initialized it. Y ou can’ t assign a\nstr  to a variable of type List[str] . T ype checkers usually don’ t\nallow the type of a variable to change.\nCannot assign to a key that is not part of the BookDict  definition.\nCannot delete a key that is part of the BookDict  definition.\nNow let’ s see BookDict  used in function signatures, to type check\nfunction calls.\nImagine you need to generate XML from book records, similar to this:\n<BOOK> \n  <ISBN>0134757599 </ISBN> \n  <TITLE>Refactoring, 2e </TITLE>  \n  <AUTHOR> Martin Fowler </AUTHOR>  \n  <AUTHOR> Kent Beck </AUTHOR>  3\n  <PAGECOUNT> 478</PAGECOUNT>  \n</BOOK>\nIf you were writing MicroPython code to embed in a tiny microcontroller ,\nyou might write a function like this:\nExample 15-7. books.py : to_xml  function.\nAUTHOR_EL  = '<AUTHOR>{}</AUTHOR> ' \n \ndef to_xml(book: BookDict ) -> str:  \n \n    elements : list[str] = []  \n \n    for key, value in book.items(): \n        if isinstance (value, list):  \n \n            elements .extend( \n                AUTHOR_EL .format(n) for n in value)  \n \n        else: \n            tag = key.upper() \n            elements .append(f'<{tag}>{value}</{tag}> ') \n    xml = '\n\t'.join(elements ) \n    return f'<BOOK>\n\t{xml}\n</BOOK>'\nThe whole point of the example: using BookDict  in the function\nsignature.\nIt’ s often necessary to annotate collections that start empty , otherwise\nMypy can’ t infer the type of the elements.\nMypy understands isinstance  checks, and treats value  as a list\nin this block.\nWhen I used key == 'authors'  as the condition for the if\nguarding this block, Mypy found an error in this line: ""object"" has\nno attribute ""__iter__"" , because it inferred the type of\nvalue  returned from book.items()  as object , which doesn’ t\nsupport the __iter__  method required by the generator expression.\nW ith the isinstance  check, this works because Mypy knows that\nvalue  is a list  in this block.\nHere is a function that parses a JSON str  and returns a BookDict :4\n5\nExample 15-8. books_any.py : from_json  function.\ndef from_json (data: str) -> BookDict : \n    whatever  = json.loads(data)  \n \n    return whatever   \nThe return type of json.loads()  is Any .\nI can return whatever —of type Any —because Any  is consistent-with\nevery type, including the declared return type, BookDict .\nThe second point of Example 15-8  is very important to keep in mind: Mypy\nwill not flag any problem in this code, but at runtime the value in\nwhatever  may not conform to the BookDict  structure—in fact, it may\nnot be a dict  at all!\nIf you run Mypy with --disallow-any-expr  it will complain about\nthe two lines in the body of from_json :\n…/typeddict/ $ mypy books_any.py --disallow-any-expr  \nbooks_any.py:30: error: Expression has type ""Any""  \nbooks_any.py:31: error: Expression has type ""Any""  \nFound 2 errors in 1 file (checked 1 source file)\nIn this case, the type error can be silenced by adding a type hint to the\ninitialization of the whatever  variable, as in Example 15-9 :\nExample 15-9. books.py : from_json  function with variable\nannotation.\ndef from_json (data: str) -> BookDict : \n    whatever : BookDict  = json.loads(data)  \n \n    return whatever   \n--disallow-any-expr  does not cause errors when an expression\nof type Any  is immediately assigned to a variable with a type hint.\nNow whatever  is of type BookDict , the declared return type.6\nW A R N I N G\nDon’ t be lulled into a false sense of type safety by Example 15-9 ! Looking at the code at\nrest, the type checker cannot predict that json.loads()  will return anything that\nresembles a BookDict . Only runtime validation can guarantee that.\nStatic type checking is unable to prevent errors with code that is inherently\ndynamic, such as json.loads() , which builds a Python objects of\ndif ferent types at runtime. Example 15-10 , Example 15-1 1 , and\nExample 15-12  demonstrate.\nExample 15-10. demo_not_book.py : from_json  r eturns an invalid\nBookDict , and to_xml  accepts it.\nfrom books import to_xml, from_json  \nfrom typing import TYPE_CHECKING  \n \ndef demo() -> None: \n    NOT_BOOK_JSON  = """""" \n        { ""title"": ""Andromeda Strain "", \n         ""flavor"": ""pistachio "", \n         ""authors"": true} \n    """""" \n    not_book  = from_json (NOT_BOOK_JSON )  \n \n    if TYPE_CHECKING :  \n \n        reveal_type (not_book ) \n        reveal_type (not_book ['authors']) \n \n    print(not_book )  \n \n    print(not_book ['flavor'])  \n \n \n    xml = to_xml(not_book )  \n \n    print(xml)  \n \n \n \nif __name__  == '__main__ ': \n    demo()\nThis line does not produce a valid BookDict —see the content of\nNOT_BOOK_JSON .\nLet’ s have Mypy reveal a couple of types.\nThis should not be a problem: print  can handle object  and every\nother type.\nBookDict  has no 'flavor'  key , but the JSON source does… what\nwill happen?\nRemember the signature: def to_xml(book: BookDict) ->\nstr:\nHow will the XML output look like?\nChecking demo_not_book.py  with Mypy:\nExample 15-1 1. Mypy r eport for demo_not_book.py , r eformatted for\nclarity .\n…/typeddict/ $ mypy demo_not_book.py  \ndemo_not_book.py:12: note: Revealed type is  \n   'TypedDict('books.BookDict', {'isbn': built-ins.str,  \n                                 'title': built-ins.str,  \n                                 'authors': built-ins.list[built-\nins.str],  \n                                 'pagecount': built-ins.int})'  \n  \ndemo_not_book.py:13: note: Revealed type is 'built-ins.list[built-\nins.str]'  \n  \ndemo_not_book.py:16: error: TypedDict ""BookDict"" has no key  \n'flavor'  \n  \nFound 1 error in 1 file (checked 1 source file)\nThe revealed type is the nominal type, not the runtime content of\nnot_book .\nAgain, this is the nominal type of not_book['authors'] , as\ndefined in BookDict . Not the runtime type.\nThis error is for line print(not_book['flavor']) : that key\ndoes not exist in the nominal type.\nNow let’ s run demo_not_book.py .\nExample 15-12. Output of running demo_not_book.py .\n…/typeddict/ $ python3 demo_not_book.py  \n{'title': 'Andromeda Strain', 'flavor': 'pistachio', 'authors':  \nTrue}  \n  \npistachio  \n  \n<BOOK>  \n  \n        <TITLE>Andromeda Strain</TITLE>  \n        <FLAVOR>pistachio</FLAVOR>  \n        <AUTHORS>True</AUTHORS>  \n</BOOK>\nThis is not really a BookDict .\nThe value of not_book['flavor'] .\nto_xml  takes a BookDict  ar gument, but there is no runtime\nchecking: garbage in, garbage out.\nExample 15-12  shows that demo_not_book.py  outputs nonsense, but\nhas no runtime errors. Using a TypedDict  while handling JSON data did\nnot provide much type safety .\nIf you look at the code for to_xml  in Example 15-7  through the lens of\nduck typing, the ar gument book  must provide an .items()  method that\nreturns an iterable of tuples like (key, value)  where:\nkey  must have an .upper()  method;\nvalue  can be anything.\nThe point of this demonstration: when handling data with a dynamic\nstructure, such as JSON or XML, TypedDict  is absolutely not a\nreplacement for data validation at runtime. For that, use pydantic .\nTypedDict  has more features, including support for optional keys, a\nlimited form of inheritance, and an alternative declaration syntax. If you\nwant to know more about it, please review PEP 589_T ypedDict: T ype Hints\nfor Dictionaries with a Fixed Set of Keys .",15078
200-Type Casting.pdf,200-Type Casting,"Now let’ s turn our attention to a function that is best avoided, but\nsometimes is unavoidable: typing.cast .\nT y p e  C a s t i n g\nNo type system is perfect, and neither are the static type checkers, the type\nhints in the typeshed  project, or the type hints in the third-party packages\nthat have them.\nThe typing.cast()  special function provides one way to handle type\nchecking malfunctions or incorrect type hints in code we can’ t fix. The\nMypy documentation  explains:\nCasts ar e used to silence spurious type checker warnings and give the\ntype checker a little help when it can’ t quite understand what is going on.\nAt runtime, typing.cast  does absolutely nothing. This is its\nimplementation :\ndef cast(typ, val): \n    """"""Cast a value to a type.  \n    This returns the value unchanged.  To the type checker this  \n    signals that the return value has the designated type, but at  \n    runtime we intentionally don't check anything (we want this  \n    to be as fast as possible).  \n    """""" \n    return val\nPEP 484 requires type checkers to “blindly believe” the type stated in the\ncast . The Casts  section of PEP 484 gives an example where the type\nchecker needs the guidance of cast ::\nfrom typing import cast \n \ndef find_first_str (a: list[object]) -> str: \n    index = next(i for i, x in enumerate (a) if isinstance (x, \nstr)) \n    # We only get here if there's at least one string in a  \n    return cast(str, a[index])\nThe next()  call on the generator expression will either return the index of\na str  item or raise StopIteration . Therefore, find_first_str\nwill always return a str  if no exception is raised, and str  is the declared\nreturn type.\nBut if the last line were just return a[index] , Mypy would infer the\nreturn type as object  because the a  ar gument is declared as\nlist[object] . So the cast()  is required to guide Mypy .\nHere is another example with cast , this time to correct an outdated type\nhint for Python’ s standard library . In Example 22-12 , I create an asyncio\nServer  object and I want to get the address the server is listening to. I\ncoded this line:\naddr = server.sockets[0].getsockname ()\nBut Mypy reported this error:\nValue of type ""Optional[List[socket]]"" is not indexable\nThe type hint for Server.sockets  on typeshed  in May 2021 is valid for\nPython 3.6, where the sockets  attribute could be None.  But in Python\n3.7 sockets  became a property with a getter that always returns a list\n—which may be empty if the server has no sockets. And since Python 3.8\nthe getter returns a tuple  (used as an immutable sequence).\nSince I can’ t fix typeshed  right now  I added a cast , like this:\nfrom asyncio.trsock  import TransportSocket  \nfrom typing import cast \n \n# ... many lines omitted ...  \n \n    socket_list  = cast(tuple[TransportSocket , ...], \nserver.sockets) \n    addr = socket_list [0].getsockname ()7\n8\nUsing cast  in this case required a couple of hours to understand the\nproblem and read asyncio  source code to find the correct type of the\nsockets: the TransportSocket  class from the undocumented\nasyncio.trsock  module. I also had to add two import  statements and\nanother line of code for readability .  But the code is safer .\nThe careful reader may note that sockets[0]  could raise IndexError\nif sockets  is empty . However , as far as I understand asyncio , that\ncannot happen in Example 22-12  because the server  is ready to accept\nconnections by the time I read its sockets  attribute, therefore it will not\nbe empty . Anyway , IndexError  is a runtime error . Mypy can’ t spot the\nproblem even in a trivial case like print([][0]) .\nW A R N I N G\nDon’ t get too comfortable using cast  to silence Mypy , because Mypy is usually right\nwhen it reports an error . If you are using cast  very often, that’ s a code smell . Y our\nteam may be misusing type hints, or you may have low quality dependencies in your\ncodebase.\nDespite the downsides, there are valid uses for cast . Here is something\nGuido van Rossum wrote about it:\nWhat’ s wr ong with the occasional cast()  call or # type: ignore\ncomment?\nIt is unwise to completely ban the use of cast , especially because the other\nworkarounds are worse:\n# type: ignore  is less informative; .\nUsing Any  is contagious: since Any  is consistent-with  all types,\nabusing it may produce cascading ef fects through type inference,\nundermining the type checker ’ s ability to detect errors in other\nparts of the code.9\n10\n11",4524
201-Reading Type Hints at Runtime.pdf,201-Reading Type Hints at Runtime,,0
202-Problems with Annotations at Runtime.pdf,202-Problems with Annotations at Runtime,"Of course, not all typing mishaps can be fixed with cast . Sometimes we\nneed # type: ignore , the occasional Any , or even leaving a function\nwithout type hints.\nNext, let’ s talk about using annotations at runtime.\nR e a d i n g  T y p e  H i n t s  a t  R u n t i m e\nAt import time, Python reads the type hints in functions, classes and\nmodules and stores them in attributes named __annotations__ . For\nexample, Example 15-13  is an annotated signature of [Link to Come].\nExample 15-13. Annotated clip function\ndef clip(text: str, max_len: int = 80) -> str:\nThe type hints are stored as a dict  in the __annotations__  attribute\nof the function:\n>>> from clip_annot  import clip \n>>> clip.__annotations__  \n{'text': <class 'str'>, 'max_len': <class 'int'>, 'return':  \n<class 'str'>}\nThe 'return'  key maps to the return type hint after the ->  symbol in\nExample 15-13 .\nNote that the annotations are evaluated by the interpreter . That’ s why the\nvalues in the annotations are the Python classes str  and int , and not the\nstrings 'str'  and 'int' . The import time evaluation of annotations is\nthe standard in Python 3.9 and even in Python 3.10 (unreleased as of May ,\n2021), and it is the behavior described in PEP 3107  when the syntax for\nannotations was introduced way back in 2006.\nProblems with Annotations at Runtime\nThe increased use of type hints raised two problems:\nImporting modules uses more CPU and memory when many type\nhints are used.\nReferring to types not yet defined requires using strings instead of\nactual types.\nBoth issues are relevant. The first is self-explanatory at a high level. The\nroot causes at a lower level are beyond the scope of this book. Let’ s focus\non the second issue.\nThe second issue is often described as the “forward reference” problem, but\none of its common manifestations in source code doesn’ t look like a\nforward reference at all: that’ s when a method returns a new object of the\nsame class. Since the class object is not defined until Python completely\nevaluates the class body , type hints must use the name of the class as a\nstring. Here is an example:\nclass Rectangle : \n    # ... lines omitted ...  \n    def stretch(self, factor: float) -> 'Rectangle'  \n        return Rectangle (width=self.width * factor)\nW riting forward referencing type hints as strings is the standard and\nrequired practice as of Python 3.10. Static type checkers were designed to\ndeal with that issue from the beginning.\nBut at runtime, if you write code to read the return  annotation for\nstretch , you will get a string 'Rectangle'  instead of a reference to\nthe actual type, the Rectangle  class. Now your code needs to figure out\nwhat that string means.\nThe typing  module includes three functions and a class categorized as\nIntrospection helpers , the most important being\ntyping.get_type_hints . Part of its documentation states:\nget_type_hints(obj, globals=None, locals=None,\ninclude_extras=False)\n[…] This is often the same as obj.__annotations__ . In addition,\nforward references encoded as string literals are handled by evaluating\nthem in globals  and locals  namespaces. […]\nThat sounds great, but get_type_hints  can’ t handle all cases, as we’ll\nsee.\nPEP 563—Postponed Evaluation of Annotations  was approved to make it\nunnecessary to write annotations as strings, and to reduce the runtime costs\nof type hints. Its main idea is described in these two periods of the Abstract :\nThis PEP pr oposes changing function annotations and variable\nannotations so that they ar e no longer evaluated at function definition\ntime. Instead, they ar e pr eserved in annotations  in string form.\nBeginning with Python 3.7, that’ s how annotations are handled in any\nmodule that starts with this import  statement:\nfrom __future__  import annotations\nT o demonstrate its ef fect, I put a copy of the same clip  function\nmentioned before in a clip_annot_post.py  module with that __future__\nimport at the top.\nAt the console, here’ s what I get when you import that module and read the\nannotations from clip :\n>>> from clip_annot_post  import clip \n>>> clip.__annotations__  \n{'text': 'str', 'max_len': 'int', 'return': 'str'}\nAs you can see, all the type hints are now plain strings, despite the fact they\nare not written as quoted strings in the definition of clip  ( Example 15-13 ).\nThe typing.get_type_hints  function is able to resolve many type\nhints, including those in clip :\n>>> from clip_annot_post  import clip \n>>> from typing import get_type_hints  \n>>> get_type_hints (clip) \n{'text': <class 'str'>, 'max_len': <class 'int'>, 'return':  \n<class 'str'>}\nCalling get_type_hints  gives us the real types—even in some cases\nwhere the original type hint is written as a quoted string. That’ s the\nrecommended way to read type hints at runtime.\nThe PEP 563 behavior was scheduled to become default in Python 3.10—\nwith no __future__  import needed. However , the maintainers of\nFastAPI  and pydantic  raised the alarm that the change would break their\ncode which relies on type hints at runtime, and cannot use\nget_type_hints  reliably .\nIn the ensuing discussion on the python-dev  mailing list, Łukasz Langa—\nthe author of PEP 563—described some limitations of that function:\n[…] it turned out that typing.get_type_hints()  has limits that\nmake its use in general costly at runtime, and mor e importantly\ninsufficient to r esolve all types. The most common example deals with\nnon-global context in which types ar e generated (e.g. inner classes,\nclasses within functions, etc.). But one of the cr own examples of forwar d\nr efer ences: classes with methods accepting or r eturning objects of their\nown type, also isn’ t pr operly handled by\ntyping.get_type_hints()  if a class generator is used. Ther e’ s\nsome trickery we can do to connect the dots but in general it’ s not\ngr eat.\nPython’ s Steering Council decided to postpone making PEP 563 the default\nbehavior until Python 3.1 1 or later , giving more time to developers to come\nup with a solution that addresses the issues PEP 563 tried to solve, without\nbreaking widespread uses of type hints at runtime. PEP 649—Deferred\nEvaluation Of Annotations Using Descriptors  is under consideration as a\npossible solution, but a dif ferent compromise may be reached.12",6369
203-Dealing with the Problem.pdf,203-Dealing with the Problem,,0
204-Implementing a generic class.pdf,204-Implementing a generic class,"T o summarize: reading type hints at runtime is not 100% reliable as of\nPython 3.10 and is likely to change in 2022.\nDealing with the Problem\nGiving the present situation, I recommend:\n1 . A void reading __annotations__  directly; use\ntyping.get_type_hints  instead.\n2 . W rap any calls to typing.get_type_hints  in a function of\nyour own, so that future changes that may be required are\nlocalized.\nT o demonstrate the second point, here are the first lines of the Checked\nclass defined in Example 25-5 , which we’ll study in Chapter 25 .\nclass Checked: \n    @classmethod  \n    def _fields(cls) -> dict[str, type]: \n        return get_type_hints (cls) \n    # ... more lines ...\nThe Checked._fields  class method protects other parts of the module\nfrom depending directly on typing.get_type_hints . If\nget_type_hints  changes in the future, I can add logic to\nChecked._fields  to work around eventual issues, hopefully avoiding\nchanges elsewhere in my code.\nThe remaining sections of this chapter cover generics, starting with how to\ndefine a generic class that can be parameterized by its users.\nI m p l e m e n t i n g  a  g e n e r i c  c l a s s\nIn Example 13-7  we defined the Tombola  ABC: an interface for classes\nthat work like a bingo cage. The LottoBlower  class from Example 13-10\nis a concrete implementation. Now we’ll study a generic version of\nLottoBlower  used like this:\nExample 15-14. generic_lotto_demo.py: using a generic lottery blower\nclass\nfrom generic_lotto  import LottoBlower  \n \nmachine = LottoBlower [int](range(1, 11))  \n \n \nfirst = machine.pick()  \n \nremain = machine.inspect()  \nT o instantiate a generic class we give it a actual type parameter , like\nint  here.\nMypy will correctly infer that first  is an int …\n… and that remain  is a tuple  of integers.\nIn addition, Mypy reports violations of the parameterized type with helpful\nmessages, such as these:\nExample 15-15. generic_lotto_err ors.py: err ors r eported by Mypy\nfrom generic_lotto  import LottoBlower  \n \nmachine = LottoBlower [int]([1, .2]) \n## error: List item 1 has incompatible type ""float"";  \n  \n##        expected ""int""  \n \nmachine = LottoBlower [int](range(1, 11)) \n \nmachine.load('ABC') \n## error: Argument 1 to ""load"" of ""LottoBlower""  \n  \n##        has incompatible type ""str"";  \n##        expected ""Iterable[int]""  \n## note:  Following member(s) of ""str"" have conflicts:  \n## note:      Expected:  \n## note:          def __iter__(self) -> Iterator[int]  \n## note:      Got:  \n## note:          def __iter__(self) -> Iterator[str]\nUpon instantiation of LottoBlower[int] , Mypy flags the float .\nWhen calling .load('ABC') , Mypy explains why a str  won’ t do:\nstr.__iter__  returns an Iterator[str] , but\nLottoBlower[int]  requires an Iterator[int] .\nExample 15-16  is the implementation.\nExample 15-16. generic_lotto.py: a generic lottery blower class\nimport random \n \nfrom collections.abc  import Iterable  \nfrom typing import TypeVar, Generic \n \nfrom tombola import Tombola \n \nT = TypeVar('T') \n \nclass LottoBlower (Tombola, Generic[T]):  \n \n \n    def __init__ (self, items: Iterable [T]) -> None:  \n \n        self._balls = list[T](items) \n \n    def load(self, items: Iterable [T]) -> None:  \n \n        self._balls.extend(items) \n \n    def pick(self) -> T:  \n \n        try: \n            position  = random.randrange (len(self._balls)) \n        except ValueError : \n            raise LookupError ('pick from empty LottoBlower ') \n        return self._balls.pop(position ) \n \n    def loaded(self) -> bool:  \n \n        return bool(self._balls) \n \n    def inspect(self) -> tuple[T, ...]:  \n \n        return tuple(self._balls)\nGeneric class declarations often use multiple inheritance, because we\nneed to subclass Generic  to declare the formal type parameters—in\nthis case, T .",3886
205-Variance.pdf,205-Variance,"The items  ar gument in __init__  is of type Iterable[T] , which\nbecomes Iterable[int]  when an instance is declared as\nLottoBlower[int] .\nThe load  method is likewise constrained.\nThe return type of T  now becomes int  in a LottoBlower[int] .\nNo type variable here.\nFinally , T  sets the type of the items in the returned tuple .\nT I P\nThe User -defined generic types  section of the typing  module documentation is short,\npresents good examples, and provides a few more details that I do not cover here.\nNow that we’ve seen how to implement a generic class, let’ s define the\nterminology to talk about generics.\nBasic Jargon for Generic T ypes\nHere are a few definitions that I found useful when studying generics.\nGeneric type\nA type declared with one or more type variables.  \nExamples: LottoBlower[T] , abc.Mapping[KT, VT] .\nFormal type parameter\nThe type variables that appear in a generic type declaration.  \nExample: T , KT , and VT  in the generic type examples above.\nParameterized type13",1018
206-An Invariant Dispenser.pdf,206-An Invariant Dispenser,"A type declared with actual type parameters.  \nExamples: list[int] , abc.Mapping[str, float] .\nActual type parameter\nThe actual types given as parameters when a parameterized type is\ndeclared.  \nExample: the int  in LottoBlower[int] .\nThe next topic is about how to make generic types more flexible,\nintroducing the concepts of covariance, contravariance, and invariance.\nV a r i a n c e\nThe interaction of generics and a type hierarchy introduces a new typing\nconcept: variance. W e will approach this abstract concept through an\nanalogy . Imagine that a school cafeteria has a rule that only juice dispensers\ncan be installed. General beverage dispensers are not allowed because they\nmay serve sodas, which are banned by the school board.\nAn Invariant Dispenser\nLet’ s try to model the cafeteria scenario with a generic\nBeverageDispenser  class that can be parameterized on the type of\nbeverage. See Example 15-17 .\nExample 15-17. invariant.py : type definitions and install  function.\nfrom typing import TypeVar, Generic \n \nclass Beverage :  \n \n    """"""Any beverage.""""""  \n \nclass Juice(Beverage ): \n    """"""Any fruit juice.""""""  \n \nclass OrangeJuice (Juice): \n    """"""Delicious juice from Brazilian oranges.""""""  \n 14\nT = TypeVar('T')  \n \n \nclass BeverageDispenser (Generic[T]):  \n \n    """"""A dispenser parameterized on the beverage type.""""""  \n    def __init__ (self, beverage : T) -> None: \n        self.beverage  = beverage  \n \n    def dispense (self) -> T: \n        return self.beverage  \n \ndef install(dispenser : BeverageDispenser [Juice]) -> None:  \n \n    """"""Install a fruit juice dispenser.""""""\nBeverage , Juice  and OrangeJuice  form a type hierarchy .\nSimple TypeVar  declaration.\nBeverageDispenser  is parameterized on the type of beverage.\ninstall  is a module-global function. Its type hint enforces the rule\nthat only a juice dispenser is acceptable.\nGiven the definitions in Example 15-17 , the following code is legal:\njuice_dispenser  = BeverageDispenser (Juice()) \ninstall(juice_dispenser )\nHowever , this is not legal:\nbeverage_dispenser  = BeverageDispenser (Beverage ()) \ninstall(beverage_dispenser ) \n## mypy: Argument 1 to ""install"" has  \n## incompatible type ""BeverageDispenser[Beverage]""  \n##          expected ""BeverageDispenser[Juice]""\nA dispenser that serves any Beverage  is not acceptable because the\ncafeteria requires a dispenser that is specialized for Juice .\nSomewhat surprisingly , this code is also illegal:",2501
207-A Contravariant Trash Can.pdf,207-A Contravariant Trash Can,"orange_juice_dispenser  = BeverageDispenser (OrangeJuice ()) \ninstall(orange_juice_dispenser ) \n## mypy: Argument 1 to ""install"" has  \n## incompatible type ""BeverageDispenser[OrangeJuice]""  \n##          expected ""BeverageDispenser[Juice]""\nA dispenser specialized for OrangeJuice  is not allowed either . Only\nBeverageDispenser[Juice]  will do. In the typing jar gon, this\nmeans that the BeverageDispenser  generic class is invariant.\nPython mutable collection types—such as list  and set —are invariant.\nThe LottoBlower  class from Example 15-16  is also invariant.\nA Covariant Dispenser\nIf we want to be more flexible and model dispensers as a generic class that\ncan accept some beverage type and also its subtypes, we must make it\ncovariant. This is how we’d declare BeverageDispenser :\nExample 15-18. covariant.py : type definitions and install  function.\nT_co = TypeVar('T_co', covariant =True)  \n \n \n \nclass BeverageDispenser (Generic[T_co]):  \n \n    def __init__ (self, beverage : T_co) -> None: \n        self.beverage  = beverage  \n \n    def dispense (self) -> T_co: \n        return self.beverage  \n \ndef install(dispenser : BeverageDispenser [Juice]) -> None:  \n \n    """"""Install a fruit juice dispenser.""""""\nSet covariant=True  when declaring the type variable; _co  is a\nconventional suf fix for covariant type parameters on typeshed .\nUse T_co  to parameterize the Generic  special class.\nT ype hints for install  are the same as in Example 15-17 .\nThe following code works because now both the Juice  dispenser and the\nOrangeJuice  dispenser are valid in a covariant\nBeverageDispenser .\njuice_dispenser  = BeverageDispenser (Juice()) \ninstall(juice_dispenser ) \n \norange_juice_dispenser  = BeverageDispenser (OrangeJuice ()) \ninstall(orange_juice_dispenser )\nBut a dispenser for any Beverage  is not acceptable:\nbeverage_dispenser  = BeverageDispenser (Beverage ()) \ninstall(beverage_dispenser ) \n## mypy: Argument 1 to ""install"" has  \n## incompatible type ""BeverageDispenser[Beverage]""  \n##          expected ""BeverageDispenser[Juice]""\nThat’ s covariance: the subtype relationship of the parameterized dispensers\nvaries in the same direction of the subtype relationship of the type\nparameters.\nA Contravariant T rash Can\nNow we’ll model the cafeteria rule for deploying a trash can. Let’ s assume\nfood and drinks are served in biodegradable packages, and leftovers as well\nas single-use utensils are also biodegradable. The trash cans must be\nsuitable for biodegradable refuse.\nThis code models the cafeteria trash can rule:\nExample 15-19. contravariant.py : type definitions and install\nfunction.\nfrom typing import TypeVar, Generic \n \nclass Refuse:  \n \n    """"""Any refuse.""""""  \n \nclass Biodegradable (Refuse): \n    """"""Biodegradable refuse.""""""  \n \nclass Compostable (Biodegradable ): \n    """"""Compostable refuse.""""""  \n \nT_contra  = TypeVar('T_contra ', contravariant =True)  \n \n \nclass TrashCan (Generic[T_contra ]):  \n \n    def put(self, refuse: T_contra ) -> None: \n        """"""Store trash until dumped.""""""  \n \ndef deploy(trash_can : TrashCan [Biodegradable ]): \n    """"""Deploy a trash can for biodegradable refuse.""""""\nA type hierarchy for refuse: Refuse  is the most general type,\nCompostable  is the most specific.\nT_contra  is a conventional name for a contravariant type variable.\nTrashCan  is contravariant on the type of refuse.\nGiven those definitions, these types of trash cans are acceptable:\nbio_can: TrashCan [Biodegradable ] = TrashCan () \ndeploy(bio_can) \n \ntrash_can : TrashCan [Refuse] = TrashCan () \ndeploy(trash_can )\nThe more general TrashCan[Refuse]  is acceptable because it can take\nany kind of refuse, including Biodegradable  and Compostable .\nHowever , a TrashCan[Compostable]  won’ t do, because it is cannot\ntake Biodegradable  or general Trash :\ncompost_can : TrashCan [Compostable ] = TrashCan () \ndeploy(compost_can ) \n## mypy: Argument 1 to ""deploy"" has  \n## incompatible type ""TrashCan[Compostable]""  \n##          expected ""TrashCan[Biodegradable]""\nLet’ s summarize the concepts we just saw .",4133
208-Variance Review.pdf,208-Variance Review,"V ariance Review\nInvariant T ypes\nA generic type L  is invariant when there is no supertype or subtype\nrelationship between two parameterized types, regardless of the relationship\nthat may exist between the actual parameters. In other words, if L  is\ninvariant, then L[A]  is not a supertype or a subtype of L[B] . They are\ninconsistent in both ways.\nAs mentioned, Python’ s mutable collections are invariant by default. The\nlist  type is a good example: list[int]  is not consistent-with\nlist[float]  and vice-versa.\nIn general, if a formal type parameter appears in type hints of method\nar guments and the same parameter appears in method return types, that\nparameter must be invariant to ensure type safety when updating and\nreading from the collection.\nFor example, here is part of the type hints for the list  built-in on\ntypeshed :\nclass list(MutableSequence [_T], Generic[_T]): \n    @overload  \n    def __init__ (self) -> None: ... \n    @overload  \n    def __init__ (self, iterable : Iterable [_T]) -> None: ... \n    # ... lines omitted ...  \n    def append(self, __object : _T) -> None: ... \n    def extend(self, __iterable : Iterable [_T]) -> None: ... \n    def pop(self, __index: int = ...) -> _T: ... \n    # etc...\nNote that _T  appears in the ar guments of __init__ , append , and\nextend  and as the return type of pop . There is no way to make such a\nclass type safe if it is covariant or contravariant in _T .\nCovariant T ypes\nConsider two types A  and B  where B  is consistent-with  A , and neither of\nthem is Any . Some authors use the <:  and :>  symbols to denote type\nrelationships like this:\nA :> B\nA  is a supertype or the same as B .\nB <: A\nB  is a subtype or the same as A .\nGiven A :> B , a generic type C  is covariant when C[A] :> C[B] .\nNote the direction of the :>  symbol is the same in both cases where A  is to\nthe left of B . Covariant generic types follow the subtype relationship of the\nactual type parameters.\nImmutable containers can be covariant. For example, this how the\ntyping.FrozenSet  class is documented  as a covariant with a type\nvariable using the conventional name T_co :\nclass FrozenSet (frozenset , AbstractSet [T_co]):\nApplying the :>  notation to parameterized types, we have:\n           float :> int  \nfrozenset[float] :> frozenset[int]\nIterators are another example of covariant generics: they are not read-only\ncollections like a frozenset , but they only produce output. Any code\nexpecting an abc.Iterator[float]  yielding floats can safely use an\nabc.Iterator[int]  yielding integers.\nContravariant T ypes\nGiven A :> B , a generic type K  is contravariant if K[A] <: K[B] .\nContravariant generic types reverse the subtype relationship of the actual\ntype parameters.\nThe TrashCan  class exemplifies this:\n          Refuse :> Biodegradable  \nTrashCan[Refuse] <: TrashCan[Biodegradable]\nA contravariant container is usually a write-only data structure, also known\nas a “sink”.\nThere are no examples of contravariant generics with a single formal type\nparameter in the Python 3.9 standard library . But Generator ,\nCoroutine , and AsyncGenerator  all have multiple formal type\nparameters, and each of them has one contravariant formal parameter .\nThose three generic types are all related to generator -like constructs used as\ncoroutines—as opposed to simple iterators. The Generator  type appears\nin Chapter 19 ; Coroutine  and AsyncGenerator , in Chapter 22 .\nFor the present discussion about variance, the main point is that the\ncontravariant formal parameter defines the type of the only ar gument used\nto send data to the object, while a dif ferent covariant formal parameter\ndefines the type of outputs produced by the object—the yield type. The\nprecise meanings of “send” and “yield” are explained in Chapter 19 .\nW e can derive useful guidelines from these observations of covariant\noutputs and contravariant inputs.\nV ariance Rules of Thumb\n1 . If a formal type parameter defines a type for data that comes out of\nthe object, it can be covariant.\n2 . If a formal type parameter defines a type for data that goes into the\nobject after its initial construction, it can be contravariant.\n3 . If a formal type parameter defines a type for data that comes out of\nthe object and the same parameter defines a type for data that goes",4369
209-Implementing a generic static protocol.pdf,209-Implementing a generic static protocol,"into it, it must be invariant.\n4 . T o err on the safe side, make formal parameters invariant.\nBy default, TypeVar  creates formal parameters that are invariant, and\nthat’ s how the mutable collections in the standard library are annotated.\nThe generic typing.Generator  is a great example of rules #1 and #2,\nas long as you understand how classic coroutines work—because that’ s\nwhat that type describes. After Chapter 19  covers classic coroutines in\ndepth, “Generic T ype Hints for Classic Coroutines”  continues the present\ndiscussion about variance.\nNext, let’ s see how to define generic static protocols, applying the idea of\ncovariance to a couple of new examples.\nI m p l e m e n t i n g  a  g e n e r i c  s t a t i c  p r o t o c o l\nThe Python 3.9 standard library provides a couple of generic static\nprotocols. One of them is SupportsAbs , implemented like this in the\ntyping  module :\n@runtime_checkable  \nclass SupportsAbs (Protocol [T_co]): \n    """"""An ABC with one abstract method __abs__ that is covariant  \nin its return type.""""""  \n    __slots__  = () \n \n    @abstractmethod  \n    def __abs__(self) -> T_co: \n        pass\nT_co  is declared according to the naming convention:\nT_co = TypeVar('T_co', covariant =True)\nThanks to SupportsAbs , Mypy recognizes this code as valid:\nExample 15-20. abs_demo.py : use of the generic SupportsAbs\npr otocol.\n#!/usr/bin/env python3  \n \nimport math \nfrom typing import NamedTuple , SupportsAbs  \n \nclass Vector2d (NamedTuple ): \n    x: float \n    y: float \n \n    def __abs__(self) -> float:  \n \n        return math.hypot(self.x, self.y) \n \ndef is_unit(v: SupportsAbs [float]) -> bool:  \n \n    """"""'True' if the magnitude of 'v' is close to 1.""""""  \n    return math.isclose(abs(v), 1.0)  \n \n \nassert issubclass (Vector2d , SupportsAbs )  \n \n \nv0 = Vector2d (0, 1)  \n \nsqrt2 = math.sqrt(2) \nv1 = Vector2d (sqrt2 / 2, sqrt2 / 2) \nv2 = Vector2d (1, 1) \nv3 = complex(.5, math.sqrt(3) / 2) \nv4 = 1  \n \n \nassert is_unit(v0) \nassert is_unit(v1) \nassert not is_unit(v2) \nassert is_unit(v3) \nassert is_unit(v4) \n \nprint('OK')\nDefining __abs__  makes Vector2d  consistent-with\nSupportsAbs .\nParameterizing SupportsAbs  with float  ensures…\n…that Mypy accepts abs(v)  as the first ar gument for\nmath.isclose .\nThanks to @runtime_checkable  in the definition of\nSupportsAbs , this is a valid runtime assertion.\nThe remaining code all passes Mypy checks and runtime assertions.\nThe int  type is also consistent-with  SupportsAbs . According to\ntypeshed , int.__abs__  returns an int , which is consistent-with  the\nfloat  type parameter declared in the is_unit  type hint for the v\nar gument.\nSimilarly , we can write a generic version of the RandomPicker  protocol\npresented in Example 13-18 , which was defined with a single method pick\nreturning Any .\nExample 15-21  shows how to make a generic RandomPicker  covariant\non the return type of pick .\nExample 15-21. generic_randompick.py : definition of generic\nRandomPicker .\nfrom typing import Protocol , runtime_checkable , TypeVar \n \nT_co = TypeVar('T_co', covariant =True)  \n \n \n@runtime_checkable  \nclass RandomPicker (Protocol [T_co]):  \n \n    def pick(self) -> T_co: ...  \nDeclare T_co  as covariant .\nThis makes RandomPicker  generic with a covariant formal type\nparameter .\nUse T_co  as the return type.\nThe generic RandomPicker  protocol can be covariant because its only\nformal parameter is used in a return type.",3514
210-Chapter summary.pdf,210-Chapter summary,"W ith this, we can call it a chapter .\nC h a p t e r  s u m m a r y\nThe chapter started with a simple example of using @overload , followed\nby much more complex example that we studied in detail: the overloaded\nsignatures required to correctly annotate the max  built-in function.\nThe typing.TypedDict  special construct came next. I chose to cover it\nhere, and not in Chapter 5  where we saw typing.NamedTuple , because\nTypedDict  is not a class builder: it’ s merely a way to add type hints to\nvariable or ar gument that requires a dict  with a specific set of string keys,\nand specific types for each key—which happens when we use a dict  as a\nrecord, often in the context of handling with JSON data. That section was a\nbit long because using TypedDict  can give a false sense of security , and I\nwanted to show how runtime checks and error handling are really inevitable\nwhen trying to make statically structured records out of mappings that are\ndynamic in nature.\nNext we talked about typing.cast , a function designed to let us guide\nwork of the type checker . It’ s important to carefully consider when to use\ncast , because overusing it hinders the type checker .\nRuntime access to type hints came next. The key point was to use\ntyping.get_type_hints  instead of reading the\n__annotations__  attribute directly . However , we also discussed how\nthat function may be unreliable with some annotations, and we saw that\nPython core developers are still working on a way to make type hints usable\nat runtime, while reducing their impact on CPU and memory usage.\nThe final sections were about generics, starting with the LottoBlower\ngeneric class—which we later learn is an invariant generic class. That\nexample was followed by definitions of four basic terms: generic type ,\nformal type parameter , parameterized type , and actual type parameter .\nThe major topic of variance was presented next, using cafeteria beverage\ndispensers and trash cans as “real life” examples of invariant, covariant and\ncontravariant generic types. Next we reviewed, formalized and further\napplied those concepts to examples in Python’ s standard library .",2169
211-Further Reading.pdf,211-Further Reading,"Lastly , we saw how a generic static protocol is defined, first considering the\ntyping.SupportsAbs  protocol, and then applying the same idea to the\nRandomPicker  example making it more strict than the original protocol\nfrom Chapter 13 .\nN O T E\nPython’ s type system is a huge and rapidly evolving subject. This chapter is not\ncomprehensive. I chose to focus on topics that are either widely applicable, particularly\nchallenging, or conceptually important.\nF u r t h e r  R e a d i n g\nPython’ s static type system was complex as initially designed, and is getting\nmore complex with each passing year . T able 15-1  lists all the PEPs that I\nam aware of as of May 2021. Python’ s of ficial documentation hardly keeps\nup with all that, so Mypy’ s documentation  is an essential reference. Robust\nPython  by Patrick V iafore (O’Reilly , 2021) is the only book that I know\nabout focusing on Python’ s static type system.\n \nT\na\nb\nl\ne  \n1\n5\n-\n1\n.  \nP\nE\nP\ns  \na\nb\no\nu\nt  \nt\ny\np\ne  \nh\ni\nn\nt\ns\n,  \nw\nit\nh  \nli\nn\nk\ns  \ni\nn  \nt\nh\ne  \nti\ntl\ne\ns\n.  \nP\nE\nP\n \nw\nit\nh  \nn\nu\nm\nb\ne\nr\ns  \nm\na\nr\nk\ne\nd  \nw\nit\nh  \n*  \na\nr\ne  \ni\nm\np\no\nr\nt\na\nn\nt  \ne\nn\no\nu\ng\nh  \nt\no  \nb\ne  \nm\ne\nn\nti\no\nn\ne\nd  \ni\nn  \nt\nh\ne  \no\np\ne\nn\ni\nn\ng  \np\na\nr\na\ng\nr\na\np\nh  \no\nf  \nt\nh\ne  \nt\ny\np\ni\nn\ng  \nd\no\nc\nu\nm\ne\nn\nt\na\nti\no\nn\n.  \nQ\nu\ne\ns\nti\no\nn  \nm\na\nr\nk\ns  \ni\nn  \nt\nh\ne  \nP\ny\nt\nh\no\nn  \nc\no\nl\nu\nm\nn  \ni\nn\nd\ni\nc\na\nt\ne  \nP\nE\nP\ns  \nu\nn\nd\ne\nr  \nd\ni\ns\nc\nu\ns\ns\ni\no\nn  \no\nr  \nn\no\nt  \ny\ne\nt  \ni\nm\np\nl\ne\nm\ne\nn\nt\ne\nd\n.\n \nPEP T itle Python Y ear\n \n3107 Function Annotations 3.0 2006\n483* The Theory of T ype Hints n/a 2014\n484* T ype Hints 3.5 2014\n482 Literature Overview for T ype Hints n/a 2015\n526* Syntax for V ariable Annotations 3.6 2016\n544* Protocols: Structural subtyping  \n(static duck typing)3.8 2017\n557 Data Classes 3.7 2017\n560 Core support for typing module  \nand generic types3.7 2017\n561 Distributing and Packaging T ype  \nInformation3.7 2017\n563 Postponed Evaluation of  \nAnnotations3.7 2017\n586* Literal T ypes 3.8 2018\n585 T ype Hinting Generics In Standard  \nCollections3.9 2019\n589* T ypedDict: T ype Hints for  \nDictionaries with a Fixed Set of  3.8 2019\nKeys\n591* Adding a final qualifier to typing 3.8 2019\n593 Flexible function and variable  \nannotations? 2019\n604 Allow writing union types as X | Y 3.10 2019\n612 Parameter Specification V ariables 3.10 2019\n613 Explicit T ype Aliases 3.10 2020\n645 Allow writing optional types as x? ? 2020\n646 V ariadic Generics ? 2020\n647 User -Defined T ype Guards 3.10 2021\n649 Deferred Evaluation Of  \nAnnotations Using Descriptors? 2021\n655 Marking individual T ypedDict  \nitems as required or potentially-\nmissing? 2021\n \nThe subtle topic of variance has its own section  in PEP 484, and is also\ncovered in the Generics  page of Mypy , as well as in their invaluable\nCommon Issues  page.\nPEP 362—Function Signatur e Object  is worth reading if you intend to use\nthe inspect  module that complements the\ntyping.get_type_hints  function.\nIf you are interested in the history of Python, you may like to know that\nGuido van Rossum posted Adding Optional Static T yping to Python  on\nDecember 23, 2004.\nPython 3 types in the wild: a tale of two type systems  is a research paper by\nIngkarat Rak-amnouykit and others from the Rensselaer Polytechnic\nInstitute and IBM TJ W atson Research Center . The paper surveys the use of\ntype hints in open source projects on GitHub, showing that most projects\ndon’ t use them, and also that most projects that have type hints apparently\ndon’ t use a type checker . I found most interesting the discussion of the\ndif ferent semantics of Mypy  and Google’ s pytype , which they conclude are\n“essentially two dif ferent type systems”.\nGilad Bracha’ s seminal paper Pluggable T ypes , submits that one of the\nadvantages of gradual typing is to allow multiple type systems for the same\nlanguage:\nOnce our runtime is independent of the type system, we can choose to\ntr eat type systems as plug-ins. W e can have zer o, one or many type\nsystems, suited to differing purposes, all at the same time. Ther e ar e\nstatic type systems that deal with aliasing, ownership, with information\nflow , as well as traditional types systems. Indeed, a very wide range of\nstatic analyses can be cast as type systems.\nAnother seminal paper about gradual typing is Static T yping Wher e\nPossible, Dynamic T yping When Needed: The End of the Cold W ar Between\nPr ogramming Languages  by Eric Meijer and Peter Drayton.\nI learned a lot reading the relevant parts of a some books about other\nlanguages that implement some of the same ideas:\nAtomic Kotlin —Bruce Eckel and Svetlana Isakova (Leanpub,\n2020)\nEffective Java, 3r d Edition —Joshua Bloch (Addison-W esley , 2017)\nPr ogramming with T ypes: T ypeScript Examples —Vlad Riscutia\n(Manning, 2019)\nPr ogramming T ypeScript —Boris Cherny (O’Reilly , 2019)\nThe Dart Pr ogramming Language —Gilad Bracha (Addison-\nW esley , 2016).\nFor some critical views on type systems, I recommend V ictor Y oudaiken’ s\nposts Bad ideas in type theory  and T ypes consider ed harmful II ,15\n16\nFinally , I was surprised to find Generics Considered Harmful  by Ken\nArnold, a core contributor to Java from the beginning, as well as co-author\nof the first four editions of the of ficially branded The Java Pr ogramming\nLanguage  book—in collaboration with James Gosling, the lead designer of\nJava.\nSadly , Arnold’ s criticism of Java’ s type system applies to Python’ s as well.\nWhile reading the many rules and special cases of the typing PEPs, I was\nconstantly reminded of this passage from Gosling’ s post:\nWhich brings up the pr oblem that I always cite for C++: I call it the “N\nor der exception to the exception rule.” It sounds like this: “Y ou can do x,\nexcept in case y , unless y does z, in which case you can if …”\nFortunately , Python has a key advantage over Java and C++: we have a\ngradual type system. W e can completely or partially omit type hints when\nthe complexity they add is not worthwhile.th\nS O A P B O X\nT yping Rabbit Holes\nWhen using a type checker , we are sometimes forced to discover and\nimport classes we did not need to know about, and our code has no need\nto reference—except to write type hints. Such classes are\nundocumented, probably because they are considered implementation\ndetails by the authors of the packages. Here are two examples from the\nstandard library .\nT o use cast()  in the server.sockets  example in “T ype\nCasting” , I had to scour the vast asyncio  documentation and then\nbrowse the source code of several modules in that package to discover\nthe undocumented TransportSocket  class in the equally\nundocumented asyncio.trsock  module. Using socket.socket\ninstead of TransportSocket  would be incorrect, because the latter\nis explicitly not a subtype of the former , according to a docstring  in the\nsource code.\nI fell into a similar rabbit hole when I added type hints to Example 20-\n13 , a simple demonstration of multiprocessing . That example\nuses SimpleQueue  objects, which you get by calling\nmultiprocessing.SimpleQueue() . However , I could not use\nthat name in a type hint, because it turns out that\nmultiprocessing.SimpleQueue  is not a class! It’ s a bound\nmethod of the undocumented multiprocessing.BaseContext\nclass, which builds and returns an instance of the SimpleQueue  class\ndefined in the undocumented multiprocessing.queues  module.\nIn each of those cases I had to spend a couple of hours to find the right\nundocumented class to import, just to write a single type hint. This kind\nof research is part of the job when writing a book. But when writing\napplication code, I’d probably avoid such scavenger hunts for a single\nof fending line and just write # type: ignore . Sometimes that’ s the\nonly cost-ef fective solution.\nV ariance notation in other languages\nV ariance is a dif ficult topic and Python’ s type hints syntax is not as\ngood as it could be. This is evidenced by this direct quote from PEP\n484:\nCovariance or contravariance is not a pr operty of a type variable,\nbut a pr operty of a generic class defined using this variable.\nIf that is the case, why are covariance and contravariance declared with\nTypeVar  and not on the generic class?\nThe authors of PEP 484 worked under the severe self-imposed\nconstraint that type hints should be supported without making any\nchange to the interpreter . This required the introduction of TypeVar  to\ndefine type variables, and also the abuse of []  to provide Klass[T]\nsyntax for generics—instead of the Klass<T>  notation used in other\npopular languages, including C#, Java, Kotlin, and T ypeScript. None of\nthese languages require type variables to be declared before use.\nIn addition, the syntax of Kotlin and C# makes it clear whether the type\nparameter is covariant, contravariant or invariant exactly where it\nmakes sense: in the class or interface declaration.\nIn Kotlin, we could declare the BeverageDispenser  like this:\nclass BeverageDispenser <out T> { \n    // etc...  \n}\nThe out  modifier in the formal type parameter means T  is an “output”\ntype, therefore BeverageDispenser  is covariant.\nY ou can probably guess how TrashCan  would be declared:\nclass TrashCan <in T> { \n    // etc...  \n}17\nGiven T  as an “input” formal type parameter , then TrashCan  is\ncontravariant.\nIf neither in  nor out  appear , then the class is invariant on the\nparameter .\nIt’ s easy to recall the “V ariance Rules of Thumb”  when out  and in  are\nused in the formal type parameters.\nThis suggests that a good naming convention for covariant and\ncontravariant type variables in Python would be:\nT_out = TypeVar('T_out', covariant =True) \nT_in = TypeVar('T_in', contravariant =True)\nThen we could define the classes like this:\nclass BeverageDispenser (Generic[T_out]): \n    ... \n \nclass TrashCan (Generic[T_in]): \n    ...\nIs it too late to change the naming convention established in PEP 484?\nFalse Positives 147 × False Negatives 19\nMany typeshed  bugs are tagged false positive  or false negative .\nIt’ s a false positive  when the type hints are too restrictive and make type\ncheckers report false errors. That was the case with the\nstatistics.mode  type hints which accepted only numbers, while\nthe function can handle any hashable, as discussed in “Restricted\nTypeVar ” .\nThe max  issue #4051  discussed before is a false negative : the type hints\nwere not strict enough, so type checkers were unable to catch some\ninvalid ar guments.\nOn May 27, 2020, I counted 147 false positive  issues (41 open) and 19\nfalse negatives  (8 open) on typeshed . That’ s a ratio of 7.7 false positive\nfor each false negative .\nIn the typeshed  sample, type hints are strongly biased to raise false\nalarms. I don’ t know what causes this. It may be because it’ s easier to\nwrite type hints that are overly restrictive, either due to limitations in\nPython’ s type system or due to our collective experience with\ntraditional nominally typed languages that provide less flexible APIs\nthan Python allows.\nThe Python type hinting PEPs and tools were developed by teams\nworking on some of the lar gest Python-powered systems in the world.\nSo this false positive  bias may be intentional: in lar ge systems the cost\nof detecting and fixing a bug in production may be very high, so it’ s\nbetter for them to err on the side of caution. I wonder if the bias is as\ngood for every Python user as it is for the W eb-scale companies that\nsponsored most of the work on typeshed  and the static type checkers.\n1  From Y ouT ube video of A Language Cr eators’ Conversation: Guido van Rossum, James\nGosling, Larry W all & Anders Hejlsber g , streamed live on April 2, 2019. Quote starts at\n1:32:05 , edited for brevity . Full transcript available at\nhttps://github.com/fluentpython/language-cr eators .\n2  I am grateful to Jelle Zijlstra—a typeshed  maintainer—who taught me several things,\nincluding how to reduce my original 9 overloads to 6.\n3  As of May 2020, pytype allows it. But its F AQ  says it will be disallowed in the future. See\nquestion “Why didn’ t pytype catch that I changed the type of an annotated variable?” in the\npytype F AQ .\n4  I prefer to use the lxml  package to generate and parse XML: it’ s easy to get started, full-\nfeatured, and fast. Unfortunately , lxml and Python’ s own ElementT ree  don’ t fit the limited\nRAM of my hypothetical microcontroller .\n5  The Mypy documentation discusses this in its Common issues and solutions  page, section\nT ypes of empty collections .\n6  Brett Cannon, Guido van Rossum, and others have been discussing how to type hint\njson.loads()  since 2016 in Mypy issue #182: Define a JSON type .\n7  The use of enumerate  in the example is intended to confuse the type checker . A simpler\nimplementation yielding strings directly instead of going through the enumerate  index is\ncorrectly analysed by Mypy , and the cast()  is not needed.\n8  I reported typeshed  issue #5535  “W rong type hint for asyncio.base_events.Server sockets\nattribute.” and it was quickly fixed by Sebastian Rittau. However , I decided to keep the\nexample because it illustrates a common use case for cast , and the cast  I wrote is harmless.\n9  T o be honest, I originally appended a # type: ignore  comment to the line with\nserver.sockets[0]  because after a little research I found similar lines the asyncio\ndocumentation  and in a test case , so I suspected the problem was not in my code.\n10  19 May 2020 message  to the typing-sig mailing list.\n11  The syntax # type: ignore[code]  allows you to specify which Mypy error code is\nbeing silenced, but the codes are not always easy to interpret. See err or codes  in the Mypy\ndocumentation\n12  Message PEP 563 in light of PEP 649 , posted April 16, 2021.\n13  The terms are from Joshua Bloch’ s classic book Effective Java, Thir d Edition  (Addison\nW esley , 2017). The definitions and examples are mine.\n14  I first saw the cafeteria analogy for variance in Erik Meijer ’ s For ewor d  in The Dart\nPr ogramming Language  book by Gilad Bracha (Addison-W esley , 2016).\n15  As a reader of footnotes, so you may recall that I credited Erik Meijer for the cafeteria\nanalogy to explain variance.\n16  That book was written for Dart 1. There are significant changes in Dart 2—including in the\ntype system. Nevertheless, Bracha is an important resarcher in the field of programming\nlanguage design, and I found the book valuable for his perspective on the design of Dart.\n17  Last paragraph of section Covariance and Contravariance  in PEP 484.",14921
212-Whats new in this chapter.pdf,212-Whats new in this chapter,"Chapter 16. Operator\nOverloading: Doing It Right\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 16th chapter of the final book. Please note that the GitHub\nrepo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this chapter ,\nplease reach out to the author at fluentpython2e@ramalho.or g .\nTher e ar e some things that I kind of feel torn about, like operator\noverloading. I left out operator overloading as a fairly personal choice\nbecause I had seen too many people abuse it in C++.\n— James Gosling, Creator of Java\nOperator overloading allows user -defined objects to interoperate with infix\noperators such as +  and |  or unary operators like -  and ~ . More generally ,\nfunction invocation ( () ), attribute access ( . ), and item access/slicing ( [] ) are\nalso operators in Python, but this chapter covers unary and infix operators.\nIn “Emulating Numeric T ypes”  ( Chapter 1 ) we saw some trivial\nimplementations of operators in a bare bones Vector  class. The __add__\nand __mul__  methods in Example 1-2  were written to show how special\nmethods support operator overloading, but there are subtle problems in their\nimplementations that we overlooked. Also, in Example 1 1-2 , we noted that the\nVector2d.__eq__  method considers this to be True : Vector(3, 4)1",1650
213-Unary Operators.pdf,213-Unary Operators,"== [3, 4] —which may or not make sense. W e will address those matters in\nthis chapter , as well as:\nHow an infix operator method should signal it cannot handle an\noperand\nUsing duck typing or goose typing to deal with operands of various\ntypes\nThe special behavior of the rich comparison operators (e.g., == , > , <= ,\netc.)\nThe default handling of augmented assignment operators such as += ,\nand how to overload them\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nGoose typing is a key part of Python, but the numbers  ABCs are not\nsupported in static typing, so I changed Example 16-1 1  to use duck typing\ninstead of an explicit isinstance  check against numbers.Real .\nI covered the @  matrix multiplication operator Fluent Python, First Edition  as\nan upcoming change when 3.5 was still in alpha. Accordingly , “Using @  as an\ninfix operator”  is no longer a sidebar , but is integrated in the flow of the\nchapter . I leveraged goose typing to make the implementation of\n__matmul__  in that section safer than the one in the first edition, without\ncompromising on flexibility .\n“Further Reading”  now has a couple of new references—including a blog post\nby Guido van Rossum. I also added mentions of two libraries that showcase\nef fective use of operator overloading outside the domain of mathematics:\npathlib  and Scapy .\nO p e r a t o r  O v e r l o a d i n g  1 0 1\nOperator overloading has a bad name in some circles. It is a language feature\nthat can be (and has been) abused, resulting in programmer confusion, bugs,\nand unexpected performance bottlenecks. But if well used, it leads to2\npleasurable APIs and readable code. Python strikes a good balance between\nflexibility , usability , and safety by imposing some limitations:\nW e cannot overload operators for the built-in types.\nW e cannot create new operators, only overload existing ones.\nA few operators can’ t be overloaded: is , and , or , not  (but the\nbitwise & , | , ~ , can).\nIn Chapter 12 , we already had one infix operator in Vector : == , supported by\nthe __eq__  method. In this chapter , we’ll improve the implementation of\n__eq__  to better handle operands of types other than Vector . However , the\nrich comparison operators ( == , != , > , < , >= , <= ) are special cases in operator\noverloading, so we’ll start by overloading four arithmetic operators in\nVector : the unary -  and + , followed by the infix +  and * .\nLet’ s start with the easiest topic: unary operators.\nU n a r y  O p e r a t o r s\nIn The Python Language Refer ence , “6.5. Unary arithmetic and bitwise\noperations”  lists three unary operators, shown here with their associated special\nmethods:\n-  (__neg__ )\nArithmetic unary negation. If x  is -2  then -x == 2 .\n+  (__pos__ )\nArithmetic unary plus. Usually x == +x , but there are a few cases when\nthat’ s not true. See “When x and +x Are Not Equal”  if you’re curious.\n~  (__invert__ )\nBitwise inverse of an integer , defined as ~x == -(x+1) . If x  is 2  then\n~x == -3 .\nThe Data Model” chapter  of The Python Language Refer ence  also lists the\nabs(…)  built-in function as a unary operator . The associated special method is\n__abs__ , as we’ve seen before, starting with “Emulating Numeric T ypes” .\nIt’ s easy to support the unary operators. Simply implement the appropriate\nspecial method, which will receive just one ar gument: self . Use whatever\nlogic makes sense in your class, but stick to the fundamental rule of operators:\nalways return a new object. In other words, do not modify self , but create\nand return a new instance of a suitable type.\nIn the case of -  and + , the result will probably be an instance of the same class\nas self ; for + , returning a copy of self  is the best approach most of the\ntime. For abs(…) , the result should be a scalar number . As for ~ , it’ s dif ficult\nto say what would be a sensible result if you’re not dealing with bits in an\ninteger , but in an ORM  it could make sense to return the negation of an SQL\nWHERE  clause, for example.\nAs promised before, we’ll implement several new operators on the Vector\nclass from Chapter 12 . Example 16-1  shows the __abs__  method we already\nhad in Example 12-16 , and the newly added __neg__  and __pos__  unary\noperator method.\nExample 16-1. vector_v6.py: unary operators - and + added to Example 12-16\n    def __abs__(self): \n        return math.hypot(*self) \n \n    def __neg__(self): \n        return Vector(-x for x in self)  \n \n \n    def __pos__(self): \n        return Vector(self)  \nT o compute -v , build a new Vector  with every component of self\nnegated.\nT o compute +v , build a new Vector  with every component of self .\nRecall that Vector  instances are iterable, and the Vector.__init__  takes\nan iterable ar gument, so the implementations of __neg__  and __pos__  are\nshort and sweet.\nW e’ll not implement __invert__ , so if the user tries ~v  on a Vector\ninstance, Python will raise TypeError  with a clear message: “bad operand\ntype for unary ~: 'Vector' .”\nThe following sidebar covers a curiosity that may help you win a bet about\nunary +  someday . The next important topic is “Overloading + for V ector\nAddition” .\nW H E N  X  A N D  + X  A R E  N O T  E Q U A L\nEverybody expects that x == +x , and that is true almost all the time in\nPython, but I found two cases in the standard library where x != +x .\nThe first case involves the decimal.Decimal  class. Y ou can have x\n!= +x  if x  is a Decimal  instance created in an arithmetic context and +x\nis then evaluated in a context with dif ferent settings. For example, x  is\ncalculated in a context with a certain precision, but the precision of the\ncontext is changed and then +x  is evaluated. See Example 16-2  for a\ndemonstration.\nE x a m p l e  1 6 - 2 .  A  c h a n g e  i n  t h e  a r i t h m e t i c  c o n t e x t  p r e c i s i o n  m a y\nc a u s e  x  t o  d i f f e r  f r o m  + x\n>>> import decimal \n>>> ctx = decimal.getcontext ()  \n \n>>> ctx.prec = 40  \n \n>>> one_third  = decimal.Decimal('1') / decimal.Decimal('3')  \n \n>>> one_third   \n \nDecimal('0.3333333333333333333333333333333333333333 ') \n>>> one_third  == +one_third   \n \nTrue \n>>> ctx.prec = 28  \n \n>>> one_third  == +one_third   \n \nFalse \n>>> +one_third   \n \nDecimal('0.3333333333333333333333333333 ')\nGet a reference to the current global arithmetic context.\nSet the precision of the arithmetic context to 40.\nCompute 1/3 using the current precision.\nInspect the result; there are 40 digits after the decimal point.\none_third == +one_third  is True .\nLower precision to 28—the default for Decimal  arithmetic in Python\n3.4.",6703
214-Overloading  for Vector Addition.pdf,214-Overloading  for Vector Addition,"Now one_third == +one_third  is False .\nInspect +one_third ; there are 28 digits after the '.'  here.\nThe fact is that each occurrence of the expression +one_third  produces\na new Decimal  instance from the value of one_third , but using the\nprecision of the current arithmetic context.\nThe second case where x != +x  you can find in the\ncollections.Counter  documentation . The Counter  class\nimplements several arithmetic operators, including infix +  to add the tallies\nfrom two Counter  instances. However , for practical reasons, Counter\naddition discards from the result any item with a negative or zero count.\nAnd the prefix +  is a shortcut for adding an empty Counter , therefore it\nproduces a new Counter  preserving only the tallies that are greater than\nzero. See Example 16-3 .\nE x a m p l e  1 6 - 3 .  U n a r y  +  p r o d u c e s  a  n e w  C o u n t e r  w i t h o u t  z e r o e d  o r\nn e g a t i v e  t a l l i e s\n>>> ct = Counter('abracadabra' ) \n>>> ct \nCounter({'a': 5, 'r': 2, 'b': 2, 'd': 1, 'c': 1})  \n>>> ct['r'] = -3 \n>>> ct['d'] = 0 \n>>> ct \nCounter({'a': 5, 'b': 2, 'c': 1, 'd': 0, 'r': -3})  \n>>> +ct \nCounter({'a': 5, 'b': 2, 'c': 1})\nNow , back to our regularly scheduled programming.\nO v e r l o a d i n g  +  f o r  V e c t o r  A d d i t i o n\nN O T E\nThe Vector  class is a sequence type, and the section “3.3.6. Emulating container types”  in\nthe “Data Model” chapter says sequences should support the +  operator for concatenation\nand *  for repetition. However , here we will implement +  and *  as mathematical vector\noperations, which are a bit harder but more meaningful for a Vector  type.\nAdding two Euclidean vectors results in a new vector in which the components\nare the pairwise additions of the components of the addends. T o illustrate:\n>>> v1 = Vector([3, 4, 5]) \n>>> v2 = Vector([6, 7, 8]) \n>>> v1 + v2 \nVector([9.0, 11.0, 13.0])  \n>>> v1 + v2 == Vector([3 + 6, 4 + 7, 5 + 8]) \nTrue\nWhat happens if we try to add two Vector  instances of dif ferent lengths? W e\ncould raise an error , but considering practical applications (such as information\nretrieval), it’ s better to fill out the shortest Vector  with zeros. This is the\nresult we want:\n>>> v1 = Vector([3, 4, 5, 6]) \n>>> v3 = Vector([1, 2]) \n>>> v1 + v3 \nVector([4.0, 6.0, 5.0, 6.0])\nGiven these basic requirements, the implementation of __add__  is short and\nsweet, as shown in Example 16-4 .\nExample 16-4. V ector . add  method, take #1\n    # inside the Vector class  \n \n    def __add__(self, other): \n        pairs = itertools .zip_longest (self, other, fillvalue =0.0)  \n \n        return Vector(a + b for a, b in pairs)  \npairs  is a generator that will produce tuples (a, b)  where a  is from\nself , and b  is from other . If self  and other  have dif ferent lengths,\nfillvalue  supplies the missing values for the shortest iterable.\nA new Vector  is built from a generator expression producing one sum for\neach item in pairs .\nNote how __add__  returns a new Vector  instance, and does not af fect\nself  or other .\nW A R N I N G\nSpecial methods implementing unary or infix operators should never change their operands.\nExpressions with such operators are expected to produce results by creating new objects.\nOnly augmented assignment operators may change the first operand ( self ), as discussed in\n“Augmented Assignment Operators” .\nExample 16-4  allows adding Vector  to a Vector2d , and Vector  to a tuple\nor to any iterable that produces numbers, as Example 16-5  proves.\nExample 16-5. V ector .__add__ take #1 supports non- Vector  objects, too\n>>> v1 = Vector([3, 4, 5]) \n>>> v1 + (10, 20, 30) \nVector([13.0, 24.0, 35.0])  \n>>> from vector2d_v3  import Vector2d  \n>>> v2d = Vector2d (1, 2) \n>>> v1 + v2d \nVector([4.0, 6.0, 5.0])\nBoth additions in Example 16-5  work because __add__  uses\nzip_longest(…) , which can consume any iterable, and the generator\nexpression to build the new Vector  merely performs a + b  with the pairs\nproduced by zip_longest(…) , so an iterable producing any number items\nwill do.\nHowever , if we swap the operands ( Example 16-6 ), the mixed-type additions\nfail..\nExample 16-6. V ector .__add__ take #1 fails with non- Vector  left operands\n>>> v1 = Vector([3, 4, 5]) \n>>> (10, 20, 30) + v1 \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : can only concatenate tuple (not ""Vector"") to tuple  \n>>> from vector2d_v3  import Vector2d  \n>>> v2d = Vector2d (1, 2) \n>>> v2d + v1 \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : unsupported operand type(s) for +: 'Vector2d' and 'Vector'\nT o support operations involving objects of dif ferent types, Python implements a\nspecial dispatching mechanism for the infix operator special methods. Given an\nexpression a + b , the interpreter will perform these steps (also see Figure 16-\n1 ):\n1 . If a  has __add__ , call a.__add__(b)  and return result unless it’ s\nNotImplemented .\n2 . If a  doesn’ t have __add__ , or calling it returns NotImplemented ,\ncheck if b  has __radd__ , then call b.__radd__(a)  and return\nresult unless it’ s NotImplemented .\n3 . If b  doesn’ t have __radd__ , or calling it returns\nNotImplemented , raise TypeError  with an unsupported\noperand types  message.\nThe __radd__  method is called the “reflected” or “reversed” version of\n__add__ . I prefer to call them “reversed” special methods.  Three of this\nbook’ s technical reviewers—Alex, Anna, and Leo—told me they like to think\nof them as the “right” special methods, because they are called on the right-\nhand operand. Whatever “r”-word you prefer , that’ s what the “r” prefix stands\nfor in __radd__ , __rsub__ , and the like.3\nFigur e 16-1. Flowchart for computing a + b with __add__ and __radd__\nTherefore, to make the mixed-type additions in Example 16-6  work, we need to\nimplement the Vector.__radd__  method, which Python will invoke as a\nfall back if the left operand does not implement __add__  or if it does but\nreturns NotImplemented  to signal that it doesn’ t know how to handle the\nright operand.\nW A R N I N G\nDo not confuse NotImplemented  with NotImplementedError . The first,\nNotImplemented , is a special singleton value that an infix operator special method\nshould return  to tell the interpreter it cannot handle a given operand. In contrast,\nNotImplementedError  is an exception that stub methods in abstract classes may\nraise  to warn that subclasses must implement them.\nThe simplest possible __radd__  that works is shown in Example 16-7 .\nExample 16-7. V ector .__add__ and __radd__ methods\n    # inside the Vector class  \n \n    def __add__(self, other):  \n \n        pairs = itertools .zip_longest (self, other, fillvalue =0.0) \n        return Vector(a + b for a, b in pairs) \n \n    def __radd__ (self, other):  \n \n        return self + other\nNo changes to __add__  from Example 16-4 ; listed here because\n__radd__  uses it.\n__radd__  just delegates to __add__ .\nOften, __radd__  can be as simple as that: just invoke the proper operator ,\ntherefore delegating to __add__  in this case. This applies to any commutative\noperator; +  is commutative when dealing with numbers or our vectors, but it’ s\nnot commutative when concatenating sequences in Python.\nThe methods in Example 16-4  work with Vector  objects, or any iterable with\nnumeric items, such as a Vector2d , a tuple  of integers, or an array  of\nfloats. But if provided with a noniterable object, __add__  fails with a\nmessage that is not very helpful, as in Example 16-8 .\nExample 16-8. V ector .__add__ method needs an iterable operand\n>>> v1 + 1 \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \n  File ""vector_v6.py"" , line 328, in __add__ \n    pairs = itertools .zip_longest (self, other, fillvalue =0.0) \nTypeError : zip_longest argument #2 must support iteration\nAnother unhelpful message is given if an operand is iterable but its items\ncannot be added to the float  items in the Vector . See Example 16-9 .\nExample 16-9. V ector .__add__ method needs an iterable with numeric items\n>>> v1 + 'ABC' \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \n  File ""vector_v6.py"" , line 329, in __add__ \n    return Vector(a + b for a, b in pairs) \n  File ""vector_v6.py"" , line 243, in __init__  \n    self._components  = array(self.typecode , components ) \n  File ""vector_v6.py"" , line 329, in <genexpr>  \n    return Vector(a + b for a, b in pairs) \nTypeError : unsupported operand type(s) for +: 'float' and 'str'\nThe problems in Examples 16-8  and 16-9  actually go deeper than obscure error\nmessages: if an operator special method cannot return a valid result because of\ntype incompatibility , it should return NotImplemented  and not raise\nTypeError . By returning NotImplemented , you leave the door open for\nthe implementer of the other operand type to perform the operation when\nPython tries the reversed method call.\nIn the spirit of duck typing, we will refrain from testing the type of the other\noperand, or the type of its elements. W e’ll catch the exceptions and return\nNotImplemented . If the interpreter has not yet reversed the operands, it will\ntry that. If the reverse method call returns NotImplemented , then Python\nwill raise TypeError  with a standard error message like “unsupported\noperand type(s) for +: V ector  and str .”\nThe final implementation of the special methods for Vector  addition are in\nExample 16-10 .",9623
215-Overloading  for Scalar Multiplication.pdf,215-Overloading  for Scalar Multiplication,"Example 16-10. vector_v6.py: operator + methods added to vector_v5.py\n( Example 12-16 )\n    def __add__(self, other): \n        try: \n            pairs = itertools .zip_longest (self, other, fillvalue =0.0) \n            return Vector(a + b for a, b in pairs) \n        except TypeError : \n            return NotImplemented  \n \n    def __radd__ (self, other): \n        return self + other\nW A R N I N G\nIf an infix operator method raises an exception, it aborts the operator dispatch algorithm. In\nthe particular case of TypeError , it is often better to catch it and return\nNotImplemented . This allows the interpreter to try calling the reversed operator method,\nwhich may correctly handle the computation with the swapped operands, if they are of\ndif ferent types.\nAt this point, we have safely overloaded the +  operator by writing __add__\nand __radd__ . W e will now tackle another infix operator: * .\nO v e r l o a d i n g  *  f o r  S c a l a r  M u l t i p l i c a t i o n\nWhat does Vector([1, 2, 3]) * x  mean? If x  is a number , that would\nbe a scalar product, and the result would be a new Vector  with each\ncomponent multiplied by x —also known as an elementwise multiplication:\n>>> v1 = Vector([1, 2, 3]) \n>>> v1 * 10 \nVector([10.0, 20.0, 30.0])  \n>>> 11 * v1 \nVector([11.0, 22.0, 33.0])\nN O T E\nAnother kind of product involving Vector  operands would be the dot product of two\nvectors—or matrix multiplication, if you take one vector as a 1 × N matrix and the other as\nan N × 1 matrix. W e will implement that operator in our Vector  class in “Using @  as an\ninfix operator” .\nBack to our scalar product, again we start with the simplest __mul__  and\n__rmul__  methods that could possibly work:\n    # inside the Vector class  \n \n    def __mul__(self, scalar): \n        return Vector(n * scalar for n in self) \n \n    def __rmul__ (self, scalar): \n        return self * scalar\nThose methods do work, except when provided with incompatible operands.\nThe scalar  ar gument has to be a number that when multiplied by a float\nproduces another float  (because our Vector  class uses an array  of floats\ninternally). So a complex  number will not do, but the scalar can be an int , a\nbool  (because bool  is a subclass of int ), or even a\nfractions.Fraction  instance. In Example 16-1 1 , the __mul__  method\ndoes not make an explicit type check on scalar , but instead converts it into a\nfloat , and returns NotImplemented  if that fails. Y et another example of\nduck typing.\nN O T E\nIn Fluent Python, First Edition , I used goose typing in Example 16-1 1 : testing the second\noperand with isinstance(scalar, numbers.Real) . Currently I avoid using the\nnumbers  ABCs because they are not supported by PEP 484, and using types at runtime that\ncannot also be statically checked seems a bad idea to me. I hope one day those ABCs can be\nfixed so we can use them with goose typing as well as static typing. On the other hand,\n__matmul__  in Example 16-12  provides a good example of goose typing, new in this\nedition.\nExample 16-1 1. vector_v7.py: operator * methods added\nclass Vector: \n    typecode  = 'd' \n \n    def __init__ (self, components ): \n        self._components  = array(self.typecode , components ) \n \n    # many methods omitted in book listing, see vector_v7.py  \n    # in https://github.com/fluentpython/example-code-2e ...  \n \n    def __mul__(self, scalar): \n        try: \n            factor = float(scalar) \n        except TypeError :  \n \n            return NotImplemented   \n \n        return Vector(n * factor for n in self) \n \n    def __rmul__ (self, scalar): \n        return self * scalar  \nIf scalar  cannot be converted to float …\n…return NotImplemented , to let Python try __rmul__  on the\nscalar  operand.\nIn this example, __rmul__  works fine by just performing self *\nscalar , delegating to the __mul__  method.\nW ith Example 16-1 1 , we can multiply Vectors  by scalar values of the usual\nand not so usual numeric types:\n>>> v1 = Vector([1.0, 2.0, 3.0]) \n>>> 14 * v1 \nVector([14.0, 28.0, 42.0])  \n>>> v1 * True \nVector([1.0, 2.0, 3.0])  \n>>> from fractions  import Fraction  \n>>> v1 * Fraction (1, 3) \nVector([0.3333333333333333, 0.6666666666666666, 1.0])\nNow that we can multiply Vector  by scalars, let’ s see how to implement\nVector  by Vector  products.",4382
216-Wrapping-up arithmetic operators.pdf,216-Wrapping-up arithmetic operators,"U s i n g  @  a s  a n  i n f i x  o p e r a t o r\nThe @  sign is well-known as the prefix of function decorators, but since 2015, it\ncan also be used as an infix operator . For years, the dot product was written as\nnumpy.dot(a, b)  in NumPy . The function call notation makes longer\nformulas harder to translate from mathematical notation to Python,  so the\nnumerical computing community lobbied for PEP 465—A dedicated infix\noperator for matrix multiplication  which was implemented in Python 3.5.\nT oday you can write a @ b  to compute the dot product of two NumPy arrays.\nThe @  operator is supported by the special methods __matmul__ ,\n__rmatmul__ , and __imatmul__ , named for “matrix multiplication.”\nThese methods are not used anywhere in the standard library at this time, but\nare recognized by the interpreter since Python 3.5, so the NumPy team—and\nthe rest of us—can support the @  operator in user -defined types. The parser was\nalso changed to handle the new operator ( a @ b  was a syntax error in Python\n3.4).\nThese simple tests show how @  should work with Vector  instances:\n>>> va = Vector([1, 2, 3]) \n>>> vz = Vector([5, 6, 7]) \n>>> va @ vz == 38.0  # 1*5 + 2*6 + 3*7  \nTrue \n>>> [10, 20, 30] @ vz \n380.0 \n>>> va @ 3 \nTraceback (most recent call last):  \n... \nTypeError : unsupported operand type(s) for @: 'Vector' and 'int'\nHere is the code of the relevant special methods:\nExample 16-12. vector_v7.py: operator @  methods\nclass Vector: \n    # many methods omitted in book listing  \n \n    def __matmul__ (self, other): \n        if (isinstance (other, abc.Sized) and \n \n            isinstance (other, abc.Iterable )): \n            if len(self) == len(other): \n \n                return sum(a * b for a, b in zip(self, other)) \n 4\n            else: \n                raise ValueError ('@ requires vectors of equal  \nlength.') \n        else: \n            return NotImplemented  \n \n    def __rmatmul__ (self, other): \n        return self @ other\nBoth operands must implement __len__  and __iter__ …\n…and have the same length to allow…\n…a beautiful application of sum , zip  and generator expression.\nExample 16-12  is a good example of goose typing  in practice. If we tested the\nother  operand against Vector , we’d deny users the flexibility of using lists\nor arrays as operands to @ . As long as one operand is a Vector , our @\nimplementation supports other operands that are instances of abc.Sized  and\nabc.Iterable . Both of these ABCs implement the __subclasshook__ ,\ntherefore any object providing __len__  and __iter__  satisfies our test—\nno need to actually subclass those ABCs, as explained in “Structural typing\nwith ABCs” . In particular , our Vector  class does not subclass either\nabc.Sized  or abc.Iterable , but it does pass the isinstance  checks\nagainst those ABCs because it has the necessary methods.\nLet’ s review the arithmetic operators supported by Python, before diving into\nthe special category of “Rich Comparison Operators” .\nW r a p p i n g - u p  a r i t h m e t i c  o p e r a t o r s\nImplementing + , * , and @  we saw the most common patterns for coding infix\noperators. The techniques we described are applicable to all operators listed in\nT able 16-1  (the in-place operators will be covered in “Augmented Assignment\nOperators” ).\n \nT\na\nbl\ne  \n1\n6\n-\n1.  \nI\nnf\nix  \no\np\ne\nr\nat\no\nr  \nm\net\nh\no\nd  \nn\na\nm\ne\ns  \n(t\nh\ne  \nin\n-\npl\na\nc\ne  \no\np\ne\nr\nat\no\nrs  \na\nr e  \nu\ns\ne\nd  \nfo\nr  \na\nu\ng\nm\ne\nnt\ne\nd  \na\nss\nig\nn\nm\ne\nnt\n;  \nc\no\nm\np\na\nri\ns\no\nn  \no\np\ne\nr\nat\no\nrs  \na\nr e  \nin  \nT\na\nbl\ne \n1\n6\n-\n2\n)\n \nOperator Forward Reverse In-place Description\n \n+ __add__ __radd__ __iadd__ Addition or  \nconcatenation\n- __sub__ __rsub__ __isub__ Subtraction\n* __mul__ __rmul__ __imul__ Multiplication or  \nrepetition\n/ __truediv__ __rtruediv__ __itruediv__ T rue division\n// __floordiv__ __rfloordiv__ __ifloordiv__ Floor division\n% __mod__ __rmod__ __imod__ Modulo",4066
217-Rich Comparison Operators.pdf,217-Rich Comparison Operators,"divmod() __divmod__ __rdivmod__ __idivmod__ Returns tuple of  \nfloor division  \nquotient and  \nmodulo\n** , pow() __pow__ __rpow__ __ipow__ Exponentiation\n@ __matmul__ __rmatmul__ __imatmul__ Matrix  \nmultiplication\n& __and__ __rand__ __iand__ Bitwise and\n| __or__ __ror__ __ior__ Bitwise or\n^ __xor__ __rxor__ __ixor__ Bitwise xor\n<< __lshift__ __rlshift__ __ilshift__ Bitwise shift left\n>> __rshift__ __rrshift__ __irshift__ Bitwise shift right\n \na  pow  takes an optional third ar gument, modulo : pow(a, b, modulo) , also supported by the special  \nmethods when invoked directly (e.g., a.__pow__(b, modulo) ).\nThe rich comparison operators use a dif ferent set of rules. W e cover them next.\nR i c h  C o m p a r i s o n  O p e r a t o r s\nThe handling of the rich comparison operators == , != , > , < , >= , <=  by the\nPython interpreter is similar to what we just saw , but dif fers in two important\naspects:\nThe same set of methods are used in forward and reverse operator\ncalls. The rules are summarized in T able 16-2 . For example, in the\ncase of == , both the forward and reverse calls invoke __eq__ , only\nswapping ar guments; and a forward call to __gt__  is followed by a\nreverse call to __lt__  with the swapped ar guments.\nIn the case of ==  and != , if the reverse call fails, Python compares the\nobject IDs instead of raising TypeError .a\n \nT\na\nb\nl\ne\n \n1\n6\n-\n2\n.  \nR\ni\nc\nh\n \nc\no\nm\np\na\nr\ni\ns\no\nn\n \no\np\ne\nr\na\nt\no\nr\ns\n:  \nr\ne\nv\ne\nr\ns\ne\n \nm\ne\nt\nh\no\nd\ns  \ni\nn\nv\no\nk\ne\nd\n \nw\nh\ne\nn\n \nt\nh\ne\n \ni\nn\ni\nt\ni\na\nl  \nm\ne\nt\nh\no\nd\n \nc\na\nl\nl  \nr\ne\nt\nu\nr\nn\ns  \nN\no\nt\nI\nm\np\nl\ne\nm\ne\nn\nt\ne\nd\n \nGroup Infix operatorForward method  \ncallReverse method  \ncall Fall back Group Infix operatorForward method  \ncallReverse method  \ncall Fall back\n \nEquality a == b a.__eq__(b) b.__eq__(a) Return id(a) ==  \nid(b)\na != b a.__ne__(b) b.__ne__(a) Return not (a =\n= b)\nOrdering a > b a.__gt__(b) b.__lt__(a) Raise TypeError\na < b a.__lt__(b) b.__gt__(a) Raise TypeError\na >= b a.__ge__(b) b.__le__(a) Raise TypeError\na <= b a.__le__(b) b.__ge__(a) Raise TypeError\n \nN E W  B E H A V I O R  I N  P Y T H O N  3\nThe fallback step for all comparison operators changed from Python 2. For __ne__ , Python\n3 now returns the negated result of __eq__ . For the ordering comparison operators, Python\n3 raises TypeError  with a message like 'unorderable types: int() <\ntuple()' . In Python 2, those comparisons produced weird results taking into account\nobject types and IDs in some arbitrary way . However , it really makes no sense to compare an\nint  to a tuple , for example, so raising TypeError  in such cases is a real improvement\nin the language.\nGiven these rules, let’ s review and improve the behavior of the\nVector.__eq__  method, which was coded as follows in vector_v5.py\n( Example 12-16 ):\nclass Vector: \n    # many lines omitted  \n \n    def __eq__(self, other): \n        return (len(self) == len(other) and \n                all(a == b for a, b in zip(self, other)))\nThat method produces the results in Example 16-13 .\nExample 16-13. Comparing a V ector to a V ector , a V ector2d, and a tuple\n>>> va = Vector([1.0, 2.0, 3.0]) \n>>> vb = Vector(range(1, 4)) \n>>> va == vb  \n \nTrue \n>>> vc = Vector([1, 2]) \n>>> from vector2d_v3  import Vector2d  \n>>> v2d = Vector2d (1, 2) \n>>> vc == v2d  \n \nTrue \n>>> t3 = (1, 2, 3) \n>>> va == t3  \n \nTrue\nT wo Vector  instances with equal numeric components compare equal.\nA Vector  and a Vector2d  are also equal if their components are equal.\nA Vector  is also considered equal to a tuple  or any iterable with\nnumeric items of equal value.\nThe last one of the results in Example 16-13  is probably not desirable. Do we\nreally want a Vector  to be considered equal to a tuple  containing the same\nnumbers? I have no hard rule about this; it depends on the application context.\nThe Zen of Python says:\nIn the face of ambiguity , r efuse the temptation to guess.\nExcessive liberality in the evaluation of operands may lead to surprising\nresults, and programmers hate surprises.\nT aking a clue from Python itself, we can see that [1,2] == (1, 2)  is\nFalse . Therefore, let’ s be conservative and do some type checking. If the\nsecond operand is a Vector  instance (or an instance of a Vector  subclass),\nthen use the same logic as the current __eq__ . Otherwise, return\nNotImplemented  and let Python handle that. See Example 16-14 .\nExample 16-14. vector_v8.py: impr oved __eq__ in the V ector class\n    def __eq__(self, other): \n        if isinstance (other, Vector):  \n \n            return (len(self) == len(other) and \n                    all(a == b for a, b in zip(self, other))) \n        else: \n            return NotImplemented   \nIf the other  operand is an instance of Vector  (or of a Vector\nsubclass), perform the comparison as before.\nOtherwise, return NotImplemented .\nIf you run the tests in Example 16-13  with the new Vector.__eq__  from\nExample 16-14 , what you get now is shown in Example 16-15 .\nExample 16-15. Same comparisons as Example 16-13 : last r esult changed\n>>> va = Vector([1.0, 2.0, 3.0]) \n>>> vb = Vector(range(1, 4)) \n>>> va == vb  \n \nTrue \n>>> vc = Vector([1, 2]) \n>>> from vector2d_v3  import Vector2d  \n>>> v2d = Vector2d (1, 2) \n>>> vc == v2d  \n \nTrue \n>>> t3 = (1, 2, 3) \n>>> va == t3  \n \nFalse\nSame result as before, as expected.\nSame result as before, but why? Explanation coming up.\nDif ferent result; this is what we wanted. But why does it work? Read on…\nAmong the three results in Example 16-15 , the first one is no news, but the last\ntwo were caused by __eq__  returning NotImplemented  in Example 16-\n14 . Here is what happens in the example with a Vector  and a Vector2d ,\nstep by step:\n1 . T o evaluate vc == v2d , Python calls Vector.__eq__(vc,\nv2d) .\n2 . Vector.__eq__(vc, v2d)  verifies that v2d  is not a Vector\nand returns NotImplemented .\n3 . Python gets NotImplemented  result, so it tries\nVector2d.__eq__(v2d, vc) .\n4 . Vector2d.__eq__(v2d, vc)  turns both operands into tuples an\ncompares them: the result is True  (the code for\nVector2d.__eq__  is in Example 1 1-1 1 ).\nAs for the comparison between Vector  and tuple  in Example 16-15 , the\nactual steps are:\n1 . T o evaluate va == t3 , Python calls Vector.__eq__(va, t3) .\n2 . Vector.__eq__(va, t3)  verifies that t3  is not a Vector  and\nreturns NotImplemented .\n3 . Python gets NotImplemented  result, so it tries\ntuple.__eq__(t3, va) .\n4 . tuple.__eq__(t3, va)  has no idea what a Vector  is, so it\nreturns NotImplemented .\n5 . In the special case of == , if the reversed call returns\nNotImplemented , Python compares object IDs as a last resort.\nHow about != ? W e don’ t need to implement it because the fallback behavior of\nthe __ne__  inherited from object  suits us: when __eq__  is defined and\ndoes not return NotImplemented , __ne__  returns that result negated.\nIn other words, given the same objects we used in Example 16-15 , the results\nfor !=  are consistent:\n>>> va != vb \nFalse \n>>> vc != v2d \nFalse \n>>> va != (1, 2, 3) \nTrue\nThe __ne__  inherited from object  works like the following code—except\nthat the original is written in C:5",7351
218-Augmented Assignment Operators.pdf,218-Augmented Assignment Operators,"def __ne__(self, other): \n        eq_result  = self == other \n        if eq_result  is NotImplemented : \n            return NotImplemented  \n        else: \n            return not eq_result\nAfter covering the essentials of infix operator overloading, let’ s turn to a\ndif ferent class of operators: the augmented assignment operators.\nA u g m e n t e d  A s s i g n m e n t  O p e r a t o r s\nOur Vector  class already supports the augmented assignment operators +=\nand *= . Example 16-16  shows them in action.\nExample 16-16. Augmented assignment works with immutable tar gets by\ncr eating new instances and r ebinding\n>>> v1 = Vector([1, 2, 3]) \n>>> v1_alias  = v1  \n \n>>> id(v1)  \n \n4302860128  \n>>> v1 += Vector([4, 5, 6])  \n \n>>> v1  \n \nVector([5.0, 7.0, 9.0])  \n>>> id(v1)  \n \n4302859904  \n>>> v1_alias   \n \nVector([1.0, 2.0, 3.0])  \n>>> v1 *= 11  \n \n>>> v1  \n \nVector([55.0, 77.0, 99.0])  \n>>> id(v1) \n4302858336\nCreate alias so we can inspect the Vector([1, 2, 3])  object later .\nRemember the ID of the initial Vector  bound to v1 .\nPerform augmented addition.\nThe expected result…\n\n…but a new Vector  was created.\nInspect v1_alias  to confirm the original Vector  was not altered.\nPerform augmented multiplication.\nAgain, the expected result, but a new Vector  was created.\nIf a class does not implement the in-place operators listed in T able 16-1 , the\naugmented assignment operators are just syntactic sugar: a += b  is evaluated\nexactly as a = a + b . That’ s the expected behavior for immutable types, and\nif you have __add__  then +=  will work with no additional code.\nHowever , if you do implement an in-place operator method such as\n__iadd__ , that method is called to compute the result of a += b . As the\nname says, those operators are expected to change the left-hand operand in\nplace, and not create a new object as the result.\nW A R N I N G\nThe in-place special methods should never be implemented for immutable types like our\nVector  class. This is fairly obvious, but worth stating anyway .\nT o show the code of an in-place operator , we will extend the BingoCage\nclass from Example 13-9  to implement __add__  and __iadd__ .\nW e’ll call the subclass AddableBingoCage . Example 16-17  is the behavior\nwe want for the +  operator .\nExample 16-17. A new AddableBingoCage instance can be cr eated with  \n    >>> vowels = 'AEIOU' \n    >>> globe = AddableBingoCage (vowels)  \n \n    >>> globe.inspect() \n    ('A', 'E', 'I', 'O', 'U') \n    >>> globe.pick() in vowels  \n \n    True \n    >>> len(globe.inspect())  \n \n    4 \n    >>> globe2 = AddableBingoCage ('XYZ')  \n \n    >>> globe3 = globe + globe2 \n    >>> len(globe3.inspect())  \n \n    7 \n    >>> void = globe + [10, 20]  \n \n    Traceback  (most recent call last): \n      ... \n    TypeError : unsupported  operand type(s) for +: 'AddableBingoCage ' \nand 'list'\nCreate a globe  instance with five items (each of the vowels ).\nPop one of the items, and verify it is one the vowels .\nConfirm that the globe  is down to four items.\nCreate a second instance, with three items.\nCreate a third instance by adding the previous two. This instance has seven\nitems.\nAttempting to add an AddableBingoCage  to a list  fails with\nTypeError . That error message is produced by the Python interpreter\nwhen our __add__  method returns NotImplemented .\nBecause an AddableBingoCage  is mutable, Example 16-18  shows how it\nwill work when we implement __iadd__ .\nExample 16-18. An existing AddableBingoCage can be loaded with +=\n(continuing fr om Example 16-17 )\n    >>> globe_orig  = globe  \n \n    >>> len(globe.inspect())  \n \n    4 \n    >>> globe += globe2  \n \n    >>> len(globe.inspect()) \n    7 \n    >>> globe += ['M', 'N']  \n \n    >>> len(globe.inspect()) \n    9 \n    >>> globe is globe_orig   \n \n    True \n    >>> globe += 1  \n \n    Traceback  (most recent call last): \n      ... \n    TypeError : right operand in += must be 'AddableBingoCage ' or an \niterable\nCreate an alias so we can check the identity of the object later .\nglobe  has four items here.\nAn AddableBingoCage  instance can receive items from another\ninstance of the same class.\nThe right-hand operand of +=  can also be any iterable.\nThroughout this example, globe  has always referred to the globe_orig\nobject.\nT rying to add a noniterable to an AddableBingoCage  fails with a\nproper error message.\nNote that the +=  operator is more liberal than +  with regard to the second\noperand. W ith + , we want both operands to be of the same type\n(AddableBingoCage , in this case), because if we accepted dif ferent types\nthis might cause confusion as to the type of the result. W ith the += , the\nsituation is clearer: the left-hand object is updated in place, so there’ s no doubt\nabout the type of the result.\nT I P\nI validated the contrasting behavior of +  and +=  by observing how the list  built-in type\nworks. W riting my_list + x , you can only concatenate one list  to another list , but\nif you write my_list += x , you can extend the left-hand list  with items from any\niterable x  on the right-hand side. This how the list.extend()  method works: it accepts\nany iterable ar gument.\nNow that we are clear on the desired behavior for AddableBingoCage , we\ncan look at its implementation in Example 16-19 .\nExample 16-19. bingoaddable.py: AddableBingoCage extends BingoCage to\nsupport + and +=\nfrom tombola import Tombola \nfrom bingo import BingoCage  \n \n \nclass AddableBingoCage (BingoCage ):  \n \n \n    def __add__(self, other): \n        if isinstance (other, Tombola):  \n \n            return AddableBingoCage (self.inspect() + other.inspect()) \n        else: \n            return NotImplemented  \n \n    def __iadd__ (self, other): \n        if isinstance (other, Tombola): \n            other_iterable  = other.inspect()  \n \n        else: \n            try: \n                other_iterable  = iter(other)  \n \n            except TypeError :  \n \n                self_cls  = type(self).__name__  \n                msg = ""right operand in += must be {!r} or an  \niterable "" \n                raise TypeError (msg.format(self_cls )) \n        self.load(other_iterable )  \n \n        return self  \nAddableBingoCage  extends BingoCage .\nOur __add__  will only work with an instance of Tombola  as the second\noperand.\nRetrieve items from other , if it is an instance of Tombola .\nOtherwise, try to obtain an iterator over other .\nIf that fails, raise an exception explaining what the user should do. When\npossible, error messages should explicitly guide the user to the solution.\nIf we got this far , we can load the other_iterable  into self .\nV ery important: augmented assignment special methods must return self .6",6814
219-Chapter Summary.pdf,219-Chapter Summary,"W e can summarize the whole idea of in-place operators by contrasting the\nreturn  statements that produce results in __add__  and __iadd__  in\nExample 16-19 :\n__add__\nThe result is produced by calling the constructor AddableBingoCage  to\nbuild a new instance.\n__iadd__\nThe result is produced by returning self , after it has been modified.\nT o wrap up this example, a final observation on Example 16-19 : by design, no\n__radd__  was coded in AddableBingoCage , because there is no need for\nit. The forward method __add__  will only deal with right-hand operands of\nthe same type, so if Python is trying to compute a + b  where a  is an\nAddableBingoCage  and b  is not, we return NotImplemented —maybe\nthe class of b  can make it work. But if the expression is b + a  and b  is not an\nAddableBingoCage , and it returns NotImplemented , then it’ s better to\nlet Python give up and raise TypeError  because we cannot handle b .\nT I P\nIn general, if a forward infix operator method (e.g., __mul__ ) is designed to work only\nwith operands of the same type as self , it’ s useless to implement the corresponding reverse\nmethod (e.g., __rmul__ ) because that, by definition, will only be invoked when dealing\nwith an operand of a dif ferent type.\nThis concludes our exploration of operator overloading in Python.\nC h a p t e r  S u m m a r y\nW e started this chapter by reviewing some restrictions Python imposes on\noperator overloading: no overloading of operators in built-in types, and\noverloading limited to existing operators, except for a few ones ( is , and , or ,\nnot ).\nW e got down to business with the unary operators, implementing __neg__\nand __pos__ . Next came the infix operators, starting with + , supported by the\n__add__  method. W e saw that unary and infix operators are supposed to\nproduce results by creating new objects, and should never change their\noperands. T o support operations with other types, we return the\nNotImplemented  special value—not an exception—allowing the interpreter\nto try again by swapping the operands and calling the reverse special method\nfor that operator (e.g., __radd__ ). The algorithm Python uses to handle infix\noperators is summarized in the flowchart in Figure 16-1 .\nMixing operand types requires detecting operands we can’ t handle. In this\nchapter , we did this in two ways: in the duck typing way , we just went ahead\nand tried the operation, catching a TypeError  exception if it happened; later ,\nin __mul__  and __matmul__ , we did it with an explicit isinstance  test.\nThere are pros and cons to these approaches: duck typing is more flexible, but\nexplicit type checking is more predictable.\nIn general, libraries should leverage duck typing—opening the door for objects\nregardless of their types, as long as they support the necessary operations.\nHowever , Python’ s operator dispatch algorithm may produce misleading error\nmessages or unexpected results when combined with duck typing. For this\nreason, the discipline of type checking using isinstance  calls against ABCs\nis often useful when writing special methods for operator overloading. That’ s\nthe technique dubbed goose typing  by Alex Martelli—which we saw in “Goose\ntyping” . Goose typing is a good compromise between flexibility and safety ,\nbecause existing or future user -defined types can be declared as actual or\nvirtual subclasses of an ABC. In addition, if an ABC implements the\n__subclasshook__ , then objects pass isinstance  checks against that\nABC by providing the required methods—no subclassing or registration\nrequired.",3608
220-Further Reading.pdf,220-Further Reading,"The next topic we covered was the rich comparison operators. W e implemented\n==  with __eq__  and discovered that Python provides a handy implementation\nof !=  in the __ne__  inherited from the object  base class. The way Python\nevaluates these operators along with > , < , >= , and <=  is slightly dif ferent, with\nspecial logic for choosing the reverse method, and fallback handling for ==  and\n!=  which never generate errors because Python compares the object IDs as a\nlast resort.\nIn the last section, we focused on augmented assignment operators. W e saw\nthat Python handles them by default as a combination of plain operator\nfollowed by assignment, that is: a += b  is evaluated exactly as a = a + b .\nThat always creates a new object, so it works for mutable or immutable types.\nFor mutable objects, we can implement in-place special methods such as\n__iadd__  for += , and alter the value of the left-hand operand. T o show this\nat work, we left behind the immutable Vector  class and worked on\nimplementing a BingoCage  subclass to support +=  for adding items to the\nrandom pool, similar to the way the list  built-in supports +=  as a shortcut for\nthe list.extend()  method. While doing this, we discussed how +  tends to\nbe stricter than +=  regarding the types it accepts. For sequence types, +  usually\nrequires that both operands are of the same type, while +=  often accepts any\niterable as the right-hand operand.\nF u r t h e r  R e a d i n g\nGuido van Rossum wrote a good defense of operator overloading in Why\noperators are useful . T rey Hunner blogged T uple ordering and deep\ncomparisons in Python  ar guing that the rich comparisons operators in Python\nare more flexible and powerful than programmers may realize when coming\nfrom other languages.\nOperator overloading is one area of Python programming where isinstance\ntests are common. The best practice around such tests is goose typing , covered\nin “Goose typing” . If you skipped that, make sure to read it.\nThe main reference for the operator special methods is the “Data Model”\nchapter . Another relevant reading in the Python documentation is “9.1.2.2.\nImplementing the arithmetic operations”  in the numbers  module of The\nPython Standard Library .\nA clever example of operator overloading appeared in the pathlib  package,\nadded in Python 3.4. Its Path  class overloads the /  operator to build\nfilesystem paths from strings, as shown in this example from the\ndocumentation:\n>>> p = Path('/etc') \n>>> q = p / 'init.d'  / 'reboot'  \n>>> q \nPosixPath('/etc/init.d/reboot')\nAnother non-arithmetic example of operator overloading is in the Scapy\nlibrary , used to “send, snif f, dissect and for ge network packets”. In Scapy , the /\noperator builds packets by stacking fields from dif ferent network layers. See\nStacking layers  for details.\nIf you are about to implement comparison operators, study\nfunctools.total_ordering . That is class decorator that automatically\ngenerates methods for all rich comparison operators in any class that defines at\nleast a couple of them. See the functools module docs .\nIf you are curious about operator method dispatching in languages with\ndynamic typing, two seminal readings are “A Simple T echnique for Handling\nMultiple Polymorphism”  by Dan Ingalls (member of the original Smalltalk\nteam) and “Arithmetic and Double Dispatching in Smalltalk-80”  by Kurt J.\nHebel and Ralph Johnson (Johnson became famous as one of the authors of the\noriginal Design Patterns  book). Both papers provide deep insight into the\npower of polymorphism in languages with dynamic typing, like Smalltalk,\nPython, and Ruby . Python does not use double dispatching for handling\noperators as described in those articles. The Python algorithm using forward\nand reverse operators is easier for user -defined classes to support than double\ndispatching, but requires special handling by the interpreter . In contrast, classic\ndouble dispatching is a general technique you can use in Python or any OO\nlanguage beyond the specific context of infix operators, and in fact Ingalls,\nHebel, and Johnson use very dif ferent examples to describe it.\nThe article “The C Family of Languages: Interview with Dennis Ritchie,\nBjarne Stroustrup, and James Gosling”  from which I quoted the epigraph in\nthis chapter appeared in Java Report , 5(7), July 2000 and C++ Report , 12(7),\nJuly/August 2000, along with two other snippets I used in the Soapbox  (next).\nIf you are into programming language design, do yourself a favor and read that\ninterview .\nS O A P B O X\nOperator Overloading: Pr os and Cons\nJames Gosling, quoted at the start of this chapter , made the conscious\ndecision to leave operator overloading out when he designed Java. In that\nsame interview ( “The C Family of Languages: Interview with Dennis\nRitchie, Bjarne Stroustrup, and James Gosling” ) he says:\nPr obably about 20 to 30 per cent of the population think of operator\noverloading as the spawn of the devil; somebody has done something\nwith operator overloading that has just r eally ticked them off, because\nthey’ve used like + for list insertion and it makes life r eally , r eally\nconfusing. A lot of that pr oblem stems fr om the fact that ther e ar e only\nabout half a dozen operators you can sensibly overload, and yet ther e\nar e thousands or millions of operators that people would like to define—\nso you have to pick, and often the choices conflict with your sense of\nintuition.\nGuido van Rossum picked the middle way in supporting operator\noverloading: he did not leave the door open for users creating new arbitrary\noperators like <=>  or :-) , which prevents a T ower of Babel of custom\noperators, and allows the Python parser to be simple. Python also does not\nlet you overload the operators of the built-in types, another limitation that\npromotes readability and predictable performance.\nGosling goes on to say:\nThen ther e’ s a community of about 10 per cent that have actually used\noperator overloading appr opriately and who r eally car e about it, and for\nwhom it’ s actually r eally important; this is almost exclusively people who\ndo numerical work, wher e the notation is very important to appealing to\npeople’ s intuition, because they come into it with an intuition about what\nthe + means, and the ability to say “a + b” wher e a and b ar e complex\nnumbers or matrices or something r eally does make sense.\nThe notation side of the issue cannot be underestimated. Here is an\nillustrative example from the realm of finances. In Python, you can\ncompute compound interest using a formula written like this:\ninterest  = principal  * ((1 + rate) ** periods - 1)\nThat same notation works regardless of the numeric types involved. Thus,\nif you are doing serious financial work, you can make sure that periods\nis an int , while rate , interest , and principal  are exact numbers\n—instances of the Python decimal.Decimal  class — and that formula\nwill work exactly as written.\nBut in Java, if you switch from float  to BigDecimal  to get arbitrary\nprecision, you can’ t use infix operators anymore, because they only work\nwith the primitive types. This is the same formula coded to work with\nBigDecimal  numbers in Java:\nBigDecimal  interest  = principal .multiply (BigDecimal .ONE.add(rate) \n                        .pow(periods).subtract (BigDecimal .ONE));\nIt’ s clear that infix operators make formulas more readable, at least for\nmost of us.  And operator overloading is necessary to support non-\nprimitive types with infix operator notation. Having operator overloading\nin a high-level, easy-to-use language was probably a key reason for the\namazing penetration of Python in scientific computing in recent years.\nOf course, there are benefits to disallowing operator overloading in a\nlanguage. It is ar guably a sound decision for lower -level systems languages\nwhere performance and safety are paramount. The much newer Go\nlanguage followed the lead of Java in this regard and does not support\noperator overloading.\nBut overloaded operators, when used sensibly , do make code easier to read\nand write. It’ s a great feature to have in a modern high-level language.\nA Glimpse at Lazy Evaluation\nIf you look closely at the traceback in Example 16-9 , you’ll see evidence of\nthe lazy  evaluation of generator expressions. Example 16-20  is that same\ntraceback, now with callouts.\nE x a m p l e  1 6 - 2 0 .  S a m e  a s  E x a m p l e  1 6 - 9\n>>> v1 + 'ABC' \nTraceback (most recent call last):  7\n  File ""<stdin>"" , line 1, in <module>  \n  File ""vector_v6.py"" , line 329, in __add__ \n    return Vector(a + b for a, b in pairs)  \n \n  File ""vector_v6.py"" , line 243, in __init__  \n    self._components  = array(self.typecode , components )  \n \n  File ""vector_v6.py"" , line 329, in <genexpr>  \n    return Vector(a + b for a, b in pairs)  \n \nTypeError : unsupported operand type(s) for +: 'float' and 'str'\nThe Vector  call gets a generator expression as its components\nar gument. No problem at this stage.\nThe components  genexp is passed to the array  constructor . W ithin\nthe array  constructor , Python tries to iterate over the genexp, causing\nthe evaluation of the first item a + b . That’ s when the TypeError\noccurs.\nThe exception propagates to the Vector  constructor call, where it is\nreported.\nThis shows how the generator expression is evaluated at the latest possible\nmoment, and not where it is defined in the source code.\nIn contrast, if the Vector  constructor was invoked as Vector([a + b\nfor a, b in pairs]) , then the exception would happen right there,\nbecause the list comprehension tried to build a list  to be passed as the\nar gument to the Vector()  call. The body of Vector.__init__\nwould not be reached at all.\nChapter 17  will cover generator expressions in detail, but I did not want to\nlet this accidental demonstration of their lazy nature go unnoticed.\n1  Source: “The C Family of Languages: Interview with Dennis Ritchie, Bjarne Stroustrup, and\nJames Gosling” .\n2  The remaining ABCs in Python’ s standard library are still valuable for goose typing and static\ntyping. The issue with the numbers  ABCs is explained in “The numbers ABCs and numeric\nprotocols” .\n3  The Python documentation uses both terms. The “Data Model” chapter  uses “reflected,” but\n“9.1.2.2. Implementing the arithmetic operations”  in the numbers  module docs mention “forward”\nand “reverse” methods, and I find this terminology better , because “forward” and “reversed” clearly\nname each of the directions, while “reflected” doesn’ t have an obvious opposite.\n4  See “Soapbox”  for an discussion of the problem.\n5  The logic for object.__eq__  and object.__ne__  is in function object_richcompare\nin Objects/typeobject.c  in the CPython source code.\n6  The iter  built-in function will be covered in the next chapter . Here I could have used\ntuple(other) , and it would work, but at the cost of building a new tuple  when all the\n.load(…)  method needs is to iterate over its ar gument.\n7  My friend Mario Domenech Goulart, a core developer of the CHICKEN Scheme compiler , will\nprobably disagree with this.",11255
221-17. Iterables Iterators and Generators.pdf,221-17. Iterables Iterators and Generators,"Chapter 17. Iterables, Iterators,\nand Generators\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 17th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nWhen I see patterns in my pr ograms, I consider it a sign of tr ouble. The\nshape of a pr ogram should r eflect only the pr oblem it needs to solve. Any\nother r egularity in the code is a sign, to me at least, that I’m using\nabstractions that ar en’ t powerful enough—often that I’m generating by\nhand the expansions of some macr o that I need to write.\n— Paul Graham, Lisp hacker and venture capitalist\nIteration is fundamental to data processing: programs mostly apply\ncomputations to data series, from pixels to nucleotides. If the data doesn’ t\nfit in memory , we need to fetch the items lazily — one at a time and on\ndemand. That’ s what an iterator does. This chapter shows how the Iterator\npattern is built into the Python language so you never need to code it by\nhand.\nPython does not have macros like Lisp (Paul Graham’ s favorite language),\nso abstracting away the Iterator pattern required changing the language: the1\nyield  keyword was added in Python 2.2 (2001).  The yield  keyword\nallows the construction of generator functions, which return iterators.\nPython 3 uses generators in many places. Even the range()  built-in now\nreturns a generator -like object instead of full-blown lists like before. If you\nmust build a list  from range , you have to be explicit (e.g.,\nlist(range(100)) ).\nEvery collection in Python is iterable , and iterators are used internally to\nsupport:\nfor  loops\nCollection types construction and extension\nLooping over text files line by line\nList, dict, and set comprehensions\nT uple unpacking\nUnpacking actual parameters with *  in function calls\nThis chapter covers the following topics:\nHow the iter(…)  built-in function is used internally to handle\niterable objects\nHow to implement the classic Iterator pattern in Python\nHow a generator function works in detail, with line-by-line\ndescriptions\nHow the classic Iterator can be replaced by a generator function or\ngenerator expression\nLeveraging the general-purpose generator functions in the standard\nlibrary\nUsing the new yield from  statement to combine generators2",2760
222-Whats new in this chapter.pdf,222-Whats new in this chapter,,0
223-A Sequence of Words.pdf,223-A Sequence of Words,"A case study: using generator functions in a database conversion\nutility designed to work with lar ge datasets\nWhy generators and coroutines look alike but are actually very\ndif ferent and should not be mixed\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThe one major change was the introductory section on yield from ,\nwhich grew from 1 to 6 pages. “Subgenerators with yield from”  now\nincludes both simpler experiments demonstrating the behavior of generators\nwith yield from , and a practical application of that syntax to traverse a\ntree data structure, developed step-by-step.\nW e’ll get started studying how the iter(…)  built-in function makes\nsequences iterable.\nA  S e q u e n c e  o f  W o r d s\nW e’ll start our exploration of iterables by implementing a Sentence\nclass: you give its constructor a string with some text, and then you can\niterate word by word. The first version will implement the sequence\nprotocol, and it’ s iterable because all sequences are iterable—as we’ve seen\nsince Chapter 1 . Now we’ll see exactly why .\nExample 17-1  shows a Sentence  class that extracts words from a text by\nindex.\nExample 17-1. sentence.py: A Sentence as a sequence of wor ds\nimport re \nimport reprlib \n \nRE_WORD = re.compile(r'\w+') \n \n \nclass Sentence : \n \n    def __init__ (self, text): \n        self.text = text \n        self.words = RE_WORD.findall(text)  \n \n \n    def __getitem__ (self, index): \n        return self.words[index]  \n \n \n    def __len__(self):  \n \n        return len(self.words) \n \n    def __repr__ (self): \n        return 'Sentence( %s)' % reprlib.repr(self.text)  \nre.findall  returns a list with all nonoverlapping matches of the\nregular expression, as a list of strings.\nself.words  holds the result of .findall , so we simply return the\nword at the given index.\nT o complete the sequence protocol, we implement __len__ —but it is\nnot needed to make an iterable object.\nreprlib.repr  is a utility function to generate abbreviated string\nrepresentations of data structures that can be very lar ge.\nBy default, reprlib.repr  limits the generated string to 30 characters.\nSee the console session in Example 17-2  to see how Sentence  is used.\nExample 17-2. T esting iteration on a Sentence instance\n>>> s = Sentence ('""The time has come, "" the Walrus said, ')  \n \n>>> s \nSentence('""The time ha... Walrus said,')  \n  \n>>> for word in s:  \n \n...     print(word) \nThe \ntime \nhas \ncome \nthe \nWalrus \nsaid \n>>> list(s)  \n \n['The', 'time', 'has', 'come', 'the', 'Walrus', 'said']3",2577
224-Why Sequences Are Iterable The iter Function.pdf,224-Why Sequences Are Iterable The iter Function,"A sentence is created from a string.\nNote the output of __repr__  using ...  generated by\nreprlib.repr .\nSentence  instances are iterable; we’ll see why in a moment.\nBeing iterable, Sentence  objects can be used as input to build lists\nand other iterable types.\nIn the following pages, we’ll develop other Sentence  classes that pass the\ntests in Example 17-2 . However , the implementation in Example 17-1  is\ndif ferent from all the others because it’ s also a sequence, so you can get\nwords by index:\n>>> s[0] \n'The' \n>>> s[5] \n'Walrus'  \n>>> s[-1] \n'said'\nEvery Python programmer knows that sequences are iterable. Now we’ll\nsee precisely why .\nW h y  S e q u e n c e s  A r e  I t e r a b l e :  T h e  iter\nF u n c t i o n\nWhenever the interpreter needs to iterate over an object x , it automatically\ncalls iter(x) .\nThe iter  built-in function:\n1 . Checks whether the object implements __iter__ , and calls that\nto obtain an iterator .\n2 . If __iter__  is not implemented, but __getitem__  is\nimplemented, Python creates an iterator that attempts to fetch items\nin order , starting from index 0 (zero).\n3 . If that fails, Python raises TypeError , usually saying “ C  object\nis not iterable,” where C  is the class of the tar get object.\nThat is why any Python sequence is iterable: they all implement\n__getitem__ . In fact, the standard sequences also implement\n__iter__ , and yours should too, because the special handling of\n__getitem__  exists for backward compatibility reasons and may be\ngone in the future (although it is not deprecated as I write this).\nAs mentioned in “Python Digs Sequences” , this is an extreme form of duck\ntyping: an object is considered iterable not only when it implements the\nspecial method __iter__ , but also when it implements __getitem__ ,\nas long as __getitem__  accepts int  keys starting from 0 .\nIn the goose-typing approach, the definition for an iterable is simpler but\nnot as flexible: an object is considered iterable if it implements the\n__iter__  method. No subclassing or registration is required, because\nabc.Iterable  implements the __subclasshook__ , as seen in\n“Structural typing with ABCs” . Here is a demonstration:\n>>> class Foo: \n...     def __iter__ (self): \n...         pass \n... \n>>> from collections  import abc \n>>> issubclass (Foo, abc.Iterable ) \nTrue \n>>> f = Foo() \n>>> isinstance (f, abc.Iterable ) \nTrue\nHowever , note that our initial Sentence  class does not pass the\nissubclass(Sentence, abc.Iterable)  test, even though it is\niterable in practice.",2581
225-Iterables Versus Iterators.pdf,225-Iterables Versus Iterators,"T I P\nAs of Python 3.9, the most accurate way to check whether an object x  is iterable is to\ncall iter(x)  and handle a TypeError  exception if it isn’ t. This is more accurate\nthan using isinstance(x, abc.Iterable) , because iter(x)  also considers\nthe legacy __getitem__  method, while the Iterable  ABC does not.\nExplicitly checking whether an object is iterable may not be worthwhile if\nright after the check you are going to iterate over the object. After all, when\nthe iteration is attempted on a noniterable, the exception Python raises is\nclear enough: TypeError: 'C' object is not iterable  . If\nyou can do better than just raising TypeError , then do so in a\ntry/except  block instead of doing an explicit check. The explicit check\nmay make sense if you are holding on to the object to iterate over it later; in\nthis case, catching the error early may be useful.\nThe next section makes explicit the relationship between iterables and\niterators.\nI t e r a b l e s  V e r s u s  I t e r a t o r s\nFrom the explanation in “Why Sequences Are Iterable: The iter\nFunction”  we can extrapolate a definition:\niterable\nAny object from which the iter  built-in function can obtain an\niterator . Objects implementing an __iter__  method returning an\niterator  are iterable. Sequences are always iterable; as are objects\nimplementing a __getitem__  method that takes 0-based indexes.\nIt’ s important to be clear about the relationship between iterables and\niterators: Python obtains iterators from iterables.\nHere is a simple for  loop iterating over a str . The str  'ABC'  is the\niterable here. Y ou don’ t see it, but there is an iterator behind the curtain:\n>>> s = 'ABC' \n>>> for char in s: \n...     print(char) \n... \nA \nB \nC\nIf there was no for  statement and we had to emulate the for  machinery\nby hand with a while  loop, this is what we’d have to write:\n>>> s = 'ABC' \n>>> it = iter(s)  \n \n>>> while True: \n...     try: \n...         print(next(it))  \n \n...     except StopIteration :  \n \n...         del it  \n \n...         break  \n \n... \nA \nB \nC\nBuild an iterator it  from the iterable.\nRepeatedly call next  on the iterator to obtain the next item.\nThe iterator raises StopIteration  when there are no further items.\nRelease reference to it —the iterator object is discarded.\nExit the loop.\nStopIteration  signals that the iterator is exhausted. This exception is\nhandled internally in for  loops and other iteration contexts like list\ncomprehensions, tuple unpacking, etc.\nThe standard interface for an iterator has two methods:\n__next__\nReturns the next available item, raising StopIteration  when there\nare no more items.\n__iter__\nReturns self ; this allows iterators to be used where an iterable is\nexpected, for example, in a for  loop.\nThis is formalized in the collections.abc.Iterator  ABC, which\ndefines the __next__  abstract method, and subclasses Iterable —\nwhere the abstract __iter__  method is defined. See Figure 17-1 .\nFigur e 17-1. The Iterable and Iterator ABCs. Methods in italic  ar e abstract. A concr ete\nIterable.__iter__ should r eturn a new Iterator instance. A concr ete Iterator must implement\n__next__. The Iterator .__iter__ method just r eturns the instance itself.\nThe Iterator  ABC implements __iter__  by doing return self .\nThis allows an iterator to be used wherever an iterable is required. The\nsource code for abc.Iterator  is in Example 17-3 .\nExample 17-3. abc.Iterator class; extracted fr om Lib/_collections_abc.py\nclass Iterator (Iterable ): \n \n    __slots__  = () \n \n    @abstractmethod  \n    def __next__ (self): \n        'Return the next item from the iterator. When exhausted,  \nraise StopIteration'  \n        raise StopIteration  \n \n    def __iter__ (self): \n        return self \n \n    @classmethod  \n    def __subclasshook__ (cls, C): \n        if cls is Iterator : \n            if (any(""__next__""  in B.__dict__  for B in C.__mro__) \nand \n                any(""__iter__""  in B.__dict__  for B in C.__mro__)): \n                return True \n        return NotImplemented\nW A R N I N G\nThe Iterator  ABC abstract method is it.__next__()  in Python 3 and\nit.next()  in Python 2. As usual, you should avoid calling special methods directly .\nJust use the next(it) : this built-in function does the right thing in Python 2 and 3.\nThe Lib/types.py  module source code in Python 3.9 has a comment that\nsays:\n# Iterators in Python aren't a matter of type but of protocol.  A  \nlarge \n# and changing number of builtin types implement *some* flavor of  \n# iterator.  Don't check the type!  Use hasattr to check for both  \n# ""__iter__"" and ""__next__"" attributes instead.\nIn fact, that’ s exactly what the __subclasshook__  method of the\nabc.Iterator  ABC does (see Example 17-3 ).\nT I P\nT aking into account the advice from Lib/types.py  and the logic implemented in\nLib/_collections_abc.py , the best way to check if an object x  is an iterator is to call\nisinstance(x, abc.Iterator) . Thanks to\nIterator.__subclasshook__ , this test works even if the class of x  is not a real\nor virtual subclass of Iterator .\nBack to our Sentence  class from Example 17-1 , you can clearly see how\nthe iterator is built by iter(…)  and consumed by next(…)  using the\nPython console:\n>>> s3 = Sentence ('Pig and Pepper ')  \n \n>>> it = iter(s3)  \n \n>>> it  # doctest: +ELLIPSIS  \n<iterator object at 0x...>  \n>>> next(it)  \n \n'Pig' \n>>> next(it) \n'and' \n>>> next(it) \n'Pepper'  \n>>> next(it)  \n \nTraceback (most recent call last):  \n  ... \nStopIteration  \n>>> list(it)  \n \n[] \n>>> list(iter(s3))  \n \n['Pig', 'and', 'Pepper']\nCreate a sentence s3  with three words.\nObtain an iterator from s3 .\nnext(it)  fetches the next word.\nThere are no more words, so the iterator raises a StopIteration\nexception.\nOnce exhausted, an iterator becomes useless.\nT o go over the sentence again, a new iterator must be built.\nBecause the only methods required of an iterator are __next__  and\n__iter__ , there is no way to check whether there are remaining items,\nother than to call next()  and catch StopIteration . Also, it’ s not",6200
226-Sentence classes with __iter__.pdf,226-Sentence classes with __iter__,,0
227-Sentence Take 2 A Classic Iterator.pdf,227-Sentence Take 2 A Classic Iterator,"possible to “reset” an iterator . If you need to start over , you need to call\niter(…)  on the iterable that built the iterator in the first place. Calling\niter(…)  on the iterator itself won’ t help, because—as mentioned—\nIterator.__iter__  is implemented by returning self , so this will\nnot reset a depleted iterator .\nT o wrap up this section, here is a definition for iterator :\niterator\nAny object that implements the __next__  no-ar gument method that\nreturns the next item in a series or raises StopIteration  when there\nare no more items. Python iterators also implement the __iter__\nmethod so they are iterable  as well.\nThe first version of Sentence  from Example 17-1  was iterable thanks to\nthe special treatment the iter(…)  built-in gives to sequences. Next, we\nwill implement Sentence  variations that implement __iter__  to return\niterators.\nS e n t e n c e  c l a s s e s  w i t h  __iter__\nThe first variation of Sentence  implements the standard iterable protocol.\nSentence T ake #2: A Classic Iterator\nThe following Sentence  class is built according to the classic Iterator\ndesign pattern according to the blueprint in the GoF book. Note that this is\nnot idiomatic Python, as the next refactorings will make very clear . But it\nserves to make explicit the relationship between the iterable collection and\nthe iterator object.\nExample 17-4  shows an implementation of a Sentence  that is iterable\nbecause it implements the __iter__  special method, which builds and\nreturns a SentenceIterator . This is how the Iterator design pattern is\ndescribed in the original Design Patterns  book.\nW e are doing it this way here just to make clear the crucial distinction\nbetween an iterable and an iterator and how they are connected.\nExample 17-4. sentence_iter .py: Sentence implemented using the Iterator\npattern\nimport re \nimport reprlib \n \nRE_WORD = re.compile(r'\w+') \n \n \nclass Sentence : \n \n    def __init__ (self, text): \n        self.text = text \n        self.words = RE_WORD.findall(text) \n \n    def __repr__ (self): \n        return f'Sentence({reprlib.repr(self.text)}) ' \n \n    def __iter__ (self):  \n \n        return SentenceIterator (self.words)  \n \n \n \nclass SentenceIterator : \n \n    def __init__ (self, words): \n        self.words = words  \n \n        self.index = 0  \n \n \n    def __next__ (self): \n        try: \n            word = self.words[self.index]  \n \n        except IndexError : \n            raise StopIteration ()  \n \n        self.index += 1  \n \n        return word  \n \n \n    def __iter__ (self):  \n \n        return self\nThe __iter__  method is the only addition to the previous\nSentence  implementation. This version has no __getitem__ , to\nmake it clear that the class is iterable because it implements\n__iter__ .\n__iter__  fulfills the iterable protocol by instantiating and returning\nan iterator .\nSentenceIterator  holds a reference to the list of words.\nself.index  determines the next word to fetch.\nGet the word at self.index .\nIf there is no word at self.index , raise StopIteration .\nIncrement self.index .\nReturn the word.\nImplement self.__iter__ .\nThe code in Example 17-4  passes the tests in Example 17-2 .\nNote that implementing __iter__  in SentenceIterator  is not\nactually needed for this example to work, but the it’ s the right thing to do:\niterators are supposed to implement both __next__  and __iter__ , and\ndoing so makes our iterator pass the\nissubclass(SentenceIterator, abc.Iterator)  test. If we\nhad subclassed SentenceIterator  from abc.Iterator , we’d\ninherit the concrete abc.Iterator.__iter__  method.\nThat is a lot of work (for us lazy Python programmers, anyway). Note how\nmost code in SentenceIterator  deals with managing the internal state\nof the iterator . Soon we’ll see how to make it shorter . But first, a brief\ndetour to address an implementation shortcut that may be tempting, but is\njust wrong.",3976
228-How a Generator Works.pdf,228-How a Generator Works,"Don’t make the iterable an iterator for itself\nA common cause of errors in building iterables and iterators is to confuse\nthe two. T o be clear: iterables have an __iter__  method that instantiates\na new iterator every time. Iterators implement a __next__  method that\nreturns individual items, and an __iter__  method that returns self .\nTherefore, iterators are also iterable, but iterables are not iterators.\nIt may be tempting to implement __next__  in addition to __iter__  in\nthe Sentence  class, making each Sentence  instance at the same time\nan iterable and iterator over itself. But this is a terrible idea. It’ s also a\ncommon anti-pattern, according to Alex Martelli who has a lot of\nexperience with Python code reviews.\nThe “Applicability” section  of the Iterator design pattern in the GoF book\nsays:\nUse the Iterator pattern\nto access an aggr egate object’ s contents without exposing its\ninternal r epr esentation.\nto support multiple traversals of aggr egate objects.\nto pr ovide a uniform interface for traversing differ ent aggr egate\nstructur es (that is, to support polymorphic iteration).\nT o “support multiple traversals” it must be possible to obtain multiple\nindependent iterators from the same iterable instance, and each iterator must\nkeep its own internal state, so a proper implementation of the pattern\nrequires each call to iter(my_iterable)  to create a new , independent,\niterator . That is why we need the SentenceIterator  class in this\nexample.4\nW A R N I N G\nA void making an iterable act as an iterator over itself. In other words, iterables must\nimplement __iter__ , but should not implement __next__ .\nOn the other hand, iterators should always be iterable. An iterator ’ s __iter__  should\njust return self .\nNow that the classic Iterator pattern is properly demonstrated, we can let it\ngo. The next section presents a more idiomatic implementation of\nSentence .\nSentence T ake #3: A Generator Function\nA Pythonic implementation of the same functionality uses a generator ,\navoiding all the work to implement the SentenceIterator  class. A\nproper explanation of the generator comes right after Example 17-5 .\nExample 17-5. sentence_gen.py: Sentence implemented using a generator\nimport re \nimport reprlib \n \nRE_WORD = re.compile(r'\w+') \n \n \nclass Sentence : \n \n    def __init__ (self, text): \n        self.text = text \n        self.words = RE_WORD.findall(text) \n \n    def __repr__ (self): \n        return 'Sentence( %s)' % reprlib.repr(self.text) \n \n    def __iter__ (self): \n        for word in self.words:  \n \n            yield word  \n \n        return  \n \n \n# done! \n\nIterate over self.words .\nY ield the current word .\nThis return  is not needed; the function can just “fall-through” and\nreturn automatically . Either way , a generator function doesn’ t raise\nStopIteration : it simply exits when it’ s done producing values.\nNo need for a separate iterator class!\nHere again we have a dif ferent implementation of Sentence  that passes\nthe tests in Example 17-2 .\nBack in the Sentence  code in Example 17-4 , __iter__  called the\nSentenceIterator  constructor to build an iterator and return it. Now\nthe iterator in Example 17-5  is in fact a generator object, built automatically\nwhen the __iter__  method is called, because __iter__  here is a\ngenerator function.\nA full explanation of generators follows.\nHow a Generator W orks\nAny Python function that has the yield  keyword in its body is a generator\nfunction: a function which, when called, returns a generator object. In other\nwords, a generator function is a generator factory .\nT I P\nThe only syntax distinguishing a plain function from a generator function is the fact that\nthe latter has a yield  keyword somewhere in its body . Some ar gued that a new\nkeyword like gen  should be used for generator functions instead of def , but Guido did\nnot agree. His ar guments are in PEP 255 — Simple Generators .5\n6\nHere is the simplest function useful to demonstrate the behavior of a\ngenerator:\n>>> def gen_123():  \n \n...     yield 1  \n \n...     yield 2 \n...     yield 3 \n... \n>>> gen_123  # doctest: +ELLIPSIS  \n<function gen_123 at 0x...>  \n  \n>>> gen_123()   # doctest: +ELLIPSIS  \n<generator object gen_123 at 0x...>  \n  \n>>> for i in gen_123():  \n \n...     print(i) \n1 \n2 \n3 \n>>> g = gen_123()  \n \n>>> next(g)  \n \n1 \n>>> next(g) \n2 \n>>> next(g) \n3 \n>>> next(g)  \n \nTraceback (most recent call last):  \n  ... \nStopIteration\nAny Python function that contains the yield  keyword is a generator\nfunction.\nUsually the body of a generator function has loop, but not necessarily;\nhere I just repeat yield  three times.\nLooking closely , we see gen_123  is a function object.\nBut when invoked, gen_123()  returns a generator object.\nGenerators are iterators that produce the values of the expressions\npassed to yield .7\nFor closer inspection, we assign the generator object to g .\nBecause g  is an iterator , calling next(g)  fetches the next item\nproduced by yield .\nWhen the body of the function completes, the generator object raises a\nStopIteration .\nA generator function builds a generator object that wraps the body of the\nfunction. When we invoke next(…)  on the generator object, execution\nadvances to the next yield  in the function body , and the next(…)  call\nevaluates to the value yielded when the function body is suspended. Finally ,\nwhen the function body returns, the enclosing generator object raises\nStopIteration , in accordance with the Iterator  protocol.\nT I P\nI find it helpful to be strict when talking about the results obtained from a generator: I\nsay that a generator yields  or pr oduces  values. But it’ s confusing to say a generator\n“returns” values. Functions return values. Calling a generator function returns a\ngenerator . A generator yields or produces values. A generator doesn’ t “return” values in\nthe usual way: the return  statement in the body of a generator function causes\nStopIteration  to be raised by the generator object.\nExample 17-6  makes the interaction between a for  loop and the body of\nthe function more explicit.\nExample 17-6. A generator function that prints messages when it runs\n>>> def gen_AB():  \n \n...     print('start') \n...     yield 'A'       \n  \n...     print('continue ') \n...     yield 'B'       \n  \n...     print('end.')   \n \n... \n>>> for c in gen_AB():  \n \n...     print('-->', c)  \n 8\n... \nstart    \n  \n--> A   \n  \ncontinue \n  \n--> B   \n  \nend.     \n  \n>>>  \nThe generator function is defined like any function, but uses yield .\nThe first implicit call to next()  in the for  loop at \n  will print\n'start'  and stop at the first yield , producing the value 'A' .\nThe second implicit call to next()  in the for  loop will print\n'continue'  and stop at the second yield , producing the value\n'B' .\nThe third call to next()  will print 'end.'  and fall through the end of\nthe function body , causing the generator object to raise\nStopIteration .\nT o iterate, the for  machinery does the equivalent of g =\niter(gen_AB())  to get a generator object, and then next(g)  at\neach iteration.\nThe loop block prints -->  and the value returned by next(g) . But\nthis output will be seen only after the output of the print  calls inside\nthe generator function.\nThe string 'start'  appears as a result of print('start')  in the\ngenerator function body .\nyield 'A'  in the generator function body produces the value A\nconsumed by the for  loop, which gets assigned to the c  variable and\nresults in the output --> A .",7670
229-Sentence Take 5 Lazy Generator Expression.pdf,229-Sentence Take 5 Lazy Generator Expression,"Iteration continues with a second call next(g) , advancing the\ngenerator function body from yield 'A'  to yield 'B' . The text\ncontinue  is output because of the second print  in the generator\nfunction body .\nyield 'B'  produces the value B  consumed by the for  loop, which\ngets assigned to the c  loop variable, so the loop prints --> B .\nIteration continues with a third call next(it) , advancing to the end of\nthe body of the function. The text end.  appears in the output because\nof the third print  in the generator function body .\nWhen the generator function body runs to the end, the generator object\nraises StopIteration . The for  loop machinery catches that\nexception, and the loop terminates cleanly .\nNow hopefully it’ s clear how Sentence.__iter__  in Example 17-5\nworks: __iter__  is a generator function which, when called, builds a\ngenerator object that implements the iterator interface, so the\nSentenceIterator  class is no longer needed.\nThis second version of Sentence  is much shorter than the first, but it’ s\nnot as lazy as it could be. Nowadays, laziness is considered a good trait, at\nleast in programming languages and APIs. A lazy implementation\npostpones producing values to the last possible moment. This saves\nmemory and may avoid useless processing as well.\nW e’ll build lazy Sentence  classes next.\nL a z y  s e n t e n c e s\nThe final variations of Sentence  are lazy , taking advantage of a lazy\nfunction from the re  module.\nSentence T ake #4: Lazy Generator\nThe Iterator  interface is designed to be lazy: next(my_iterator)\nproduces one item at a time. The opposite of lazy is eager: lazy evaluation\nand eager evaluation are actual technical terms in programming language\ntheory .\nOur Sentence  implementations so far have not been lazy because the\n__init__  eagerly builds a list of all words in the text, binding it to the\nself.words  attribute. This will entail processing the entire text, and the\nlist may use as much memory as the text itself (probably more; it depends\non how many nonword characters are in the text). Most of this work will be\nin vain if the user only iterates over the first couple words.\nWhenever you are using Python 3 and start wondering “Is there a lazy way\nof doing this?”, often the answer is “Y es.”\nThe re.finditer  function is a lazy version of re.findall  which,\ninstead of a list, returns a generator producing re.MatchObject\ninstances on demand. If there are many matches, re.finditer  saves a\nlot of memory . Using it, our third version of Sentence  is now lazy: it\nonly produces the next word when it is needed. The code is in Example 17-\n7 .\nExample 17-7. sentence_gen2.py: Sentence implemented using a generator\nfunction calling the r e.finditer generator function\nimport re \nimport reprlib \n \nRE_WORD = re.compile(r'\w+') \n \n \nclass Sentence : \n \n    def __init__ (self, text): \n        self.text = text  \n \n \n    def __repr__ (self): \n        return f'Sentence({reprlib.repr(self.text)}) ' \n \n    def __iter__ (self): \n        for match in RE_WORD.finditer (self.text):  \n \n            yield match.group()  \nNo need to have a words  list.\nfinditer  builds an iterator over the matches of RE_WORD  on\nself.text , yielding MatchObject  instances.\nmatch.group()  extracts the actual matched text from the\nMatchObject  instance.\nGenerators are an awesome shortcut, but the code can be made even shorter\nwith a generator expression.\nSentence T ake #5: Lazy Generator Expression\nSimple generator functions like the one in the previous Sentence  class\n( Example 17-7 ) can be replaced by a generator expression.\nA generator expression can be understood as a lazy version of a list\ncomprehension: it does not eagerly build a list, but returns a generator that\nwill lazily produce the items on demand. In other words, if a list\ncomprehension is a factory of lists, a generator expression is a factory of\ngenerators.\nExample 17-8  is a quick demo of a generator expression, comparing it to a\nlist comprehension.\nExample 17-8. The gen_AB generator function is used by a list\ncompr ehension, then by a generator expr ession\n>>> def gen_AB():  \n \n...     print('start') \n...     yield 'A' \n...     print('continue ') \n...     yield 'B' \n...     print('end.') \n... \n>>> res1 = [x*3 for x in gen_AB()]  \n \nstart \ncontinue  \nend. \n>>> for i in res1:  \n \n...     print('-->', i) \n... \n--> AAA \n--> BBB \n>>> res2 = (x*3 for x in gen_AB())  \n \n>>> res2  \n \n<generator object <genexpr> at 0x10063c240>  \n>>> for i in res2:  \n \n...     print('-->', i) \n... \nstart \n--> AAA \ncontinue  \n--> BBB \nend.\nThis is the same gen_AB  function from Example 17-6 .\nThe list comprehension eagerly iterates over the items yielded by the\ngenerator object produced by calling gen_AB() : 'A'  and 'B' . Note\nthe output in the next lines: start , continue , end.\nThis for  loop is iterating over the res1  list  produced by the list\ncomprehension.\nThe generator expression returns res2 . The call to gen_AB()  is\nmade, but that call returns a generator , which is not consumed here.\nres2  is a generator object.\nOnly when the for  loop iterates over res2 , the body of gen_AB\nactually executes. Each iteration of the for  loop implicitly calls\nnext(res2) , advancing gen_AB  to the next yield . Note the output\nof gen_AB  with the output of the print  in the for  loop.",5438
230-Another Example Arithmetic Progression Generator.pdf,230-Another Example Arithmetic Progression Generator,"So, a generator expression produces a generator , and we can use it to further\nreduce the code in the Sentence  class. See Example 17-9 .\nExample 17-9. sentence_genexp.py: Sentence implemented using a\ngenerator expr ession\nimport re \nimport reprlib \n \nRE_WORD = re.compile(r'\w+') \n \n \nclass Sentence : \n \n    def __init__ (self, text): \n        self.text = text \n \n    def __repr__ (self): \n        return f'Sentence({reprlib.repr(self.text)})'  \n \n    def __iter__ (self): \n        return (match.group() for match in \nRE_WORD.finditer (self.text))\nThe only dif ference from Example 17-7  is the __iter__  method, which\nhere is not a generator function (it has no yield ) but uses a generator\nexpression to build a generator and then returns it. The end result is the\nsame: the caller of __iter__  gets a generator object.\nGenerator expressions are syntactic sugar: they can always be replaced by\ngenerator functions, but sometimes are more convenient. The next section is\nabout generator expression usage.\nG e n e r a t o r  E x p r e s s i o n s :  W h e n  t o  U s e  T h e m\nI used several generator expressions when implementing the Vector  class\nin Example 12-16 . Each of the methods __eq__ , __hash__ , __abs__ ,\nangle , angles , format , __add__ , and __mul__  has a generator\nexpression. In all those methods, a list comprehension would also work, at\nthe cost of using more memory to store the intermediate list values.\nIn Example 17-9 , we saw that a generator expression is a syntactic shortcut\nto create a generator without defining and calling a function. On the other\nhand, generator functions are much more flexible: you can code complex\nlogic with multiple statements, and can even use them as cor outines  (see\nChapter 19 ).\nFor the simpler cases, a generator expression will do, and it’ s easier to read\nat a glance, as the Vector  example shows.\nMy rule of thumb in choosing the syntax to use is simple: if the generator\nexpression spans more than a couple of lines, I prefer to code a generator\nfunction for the sake of readability .\nS Y N T A X  T I P\nWhen a generator expression is passed as the single ar gument to a function or\nconstructor , you don’ t need to write a set of parentheses for the function call and another\nto enclose the generator expression. A single pair will do, like in the Vector  call from\nthe __mul__  method in Example 12-16 , reproduced here. However , if there are more\nfunction ar guments after the generator expression, you need to enclose it in parentheses\nto avoid a SyntaxError :\ndef __mul__(self, scalar): \n    if isinstance (scalar, numbers.Real): \n        return Vector(n * scalar for n in self) \n    else: \n        return NotImplemented\nThe Sentence  examples we’ve seen exemplify the use of generators\nplaying the role of classic iterators: retrieving items from a collection. But\ngenerators can also be used to produce values independent of a data source.\nThe next section shows an example of that.\nA n o t h e r  E x a m p l e :  A r i t h m e t i c  P r o g r e s s i o n\nG e n e r a t o r\nThe classic Iterator pattern is all about traversal: navigating some data\nstructure. But a standard interface based on a method to fetch the next item\nin a series is also useful when the items are produced on the fly , instead of\nretrieved from a collection. For example, the range  built-in generates a\nbounded arithmetic progression (AP) of integers, and the\nitertools.count  function generates a boundless AP .\nW e’ll cover itertools.count  in the next section, but what if you need\nto generate a bounded AP of numbers of any type?\nExample 17-10  shows a few console tests of an\nArithmeticProgression  class we will see in a moment. The\nsignature of the constructor in Example 17-10  is\nArithmeticProgression(begin, step[, end]) . The\nrange()  function is similar to the ArithmeticProgression  here,\nbut its full signature is range(start, stop[, step]) . I chose to\nimplement a dif ferent signature because for an arithmetic progression the\nstep  is mandatory but end  is optional. I also changed the ar gument names\nfrom start/stop  to begin/end  to make it very clear that I opted for a\ndif ferent signature. In each test in Example 17-10  I call list()  on the\nresult to inspect the generated values.\nExample 17-10. Demonstration of an ArithmeticPr ogr ession class\n    >>> ap = ArithmeticProgression (0, 1, 3) \n    >>> list(ap) \n    [0, 1, 2] \n    >>> ap = ArithmeticProgression (1, .5, 3) \n    >>> list(ap) \n    [1.0, 1.5, 2.0, 2.5] \n    >>> ap = ArithmeticProgression (0, 1/3, 1) \n    >>> list(ap) \n    [0.0, 0.3333333333333333 , 0.6666666666666666 ] \n    >>> from fractions  import Fraction  \n    >>> ap = ArithmeticProgression (0, Fraction (1, 3), 1) \n    >>> list(ap) \n    [Fraction (0, 1), Fraction (1, 3), Fraction (2, 3)] \n    >>> from decimal import Decimal \n    >>> ap = ArithmeticProgression (0, Decimal('.1'), .3) \n    >>> list(ap) \n    [Decimal('0.0'), Decimal('0.1'), Decimal('0.2')]\nNote that type of the numbers in the resulting arithmetic progression\nfollows the type of begin  or step , according to the numeric coercion\nrules of Python arithmetic. In Example 17-10 , you see lists of int , float ,\nFraction , and Decimal  numbers.\nExample 17-1 1  lists the implementation of the\nArithmeticProgression  class.\nExample 17-1 1. The ArithmeticPr ogr ession class\nclass ArithmeticProgression : \n \n    def __init__ (self, begin, step, end=None):       \n  \n        self.begin = begin \n        self.step = step \n        self.end = end  # None -> ""infinite"" series  \n \n    def __iter__ (self): \n        result_type  = type(self.begin + self.step)   \n \n        result = result_type (self.begin)             \n  \n        forever = self.end is None                   \n  \n        index = 0 \n        while forever or result < self.end:          \n  \n            yield result                             \n  \n            index += 1 \n            result = self.begin + self.step * index  \n__init__  requires two ar guments: begin  and step . end  is\noptional, if it’ s None , the series will be unbounded.\nGet the type of adding self.begin  and self.step . For example,\nif one is int  and the other is float , result_type  will be float .\nThis line produces a result  value equal to self.begin , but\ncoerced to the type of the subsequent additions.\nFor readability , the forever  flag will be True  if the self.end\nattribute is None , resulting in an unbounded series.\nThis loop runs forever  or until the result matches or exceeds\nself.end . When this loop exits, so does the function.9\nThe current result  is produced.\nThe next potential result is calculated. It may never be yielded, because\nthe while  loop may terminate.\nIn the last line of Example 17-1 1 , instead of simply incrementing the\nresult  with self.step  iteratively , I opted to use an index  variable\nand calculate each result  by adding self.begin  to self.step\nmultiplied by index  to reduce the cumulative ef fect of errors when\nworking with floats.\nThe ArithmeticProgression  class from Example 17-1 1  works as\nintended, and is a clear example of the use of a generator function to\nimplement the __iter__  special method. However , if the whole point of\na class is to build a generator by implementing __iter__ , the class can be\nreduced to a generator function. A generator function is, after all, a\ngenerator factory .\nExample 17-12  shows a generator function called aritprog_gen  that\ndoes the same job as ArithmeticProgression  but with less code.\nThe tests in Example 17-10  all pass if you just call aritprog_gen\ninstead of ArithmeticProgression .\nExample 17-12. The aritpr og_gen generator function\ndef aritprog_gen (begin, step, end=None): \n    result = type(begin + step)(begin) \n    forever = end is None \n    index = 0 \n    while forever or result < end: \n        yield result \n        index += 1 \n        result = begin + step * index\nExample 17-12  is pretty cool, but always remember: there are plenty of\nready-to-use generators in the standard library , and the next section will\nshow an even cooler implementation using the itertools  module.10",8264
231-Generator Functions in the Standard Library.pdf,231-Generator Functions in the Standard Library,"Arithmetic Progression with itertools\nThe itertools  module in Python 3.9 has 19 generator functions that can\nbe combined in a variety of interesting ways.\nFor example, the itertools.count  function returns a generator that\nproduces numbers. W ithout ar guments, it produces a series of integers\nstarting with 0 . But you can provide optional start  and step  values to\nachieve a result very similar to our aritprog_gen  functions:\n>>> import itertools  \n>>> gen = itertools .count(1, .5) \n>>> next(gen) \n1 \n>>> next(gen) \n1.5 \n>>> next(gen) \n2.0 \n>>> next(gen) \n2.5\nHowever , itertools.count  never stops, so if you call\nlist(count()) , Python will try to build a list  lar ger than available\nmemory and your machine will be very grumpy long before the call fails.\nOn the other hand, there is the itertools.takewhile  function: it\nproduces a generator that consumes another generator and stops when a\ngiven predicate evaluates to False . So we can combine the two and write\nthis:\n>>> gen = itertools .takewhile (lambda n: n < 3, itertools .count(1, \n.5)) \n>>> list(gen) \n[1, 1.5, 2.0, 2.5]\nLeveraging takewhile  and count , Example 17-13  is sweet and short.\nExample 17-13. aritpr og_v3.py: this works like the pr evious aritpr og_gen\nfunctions\nimport itertools  \n \n \ndef aritprog_gen (begin, step, end=None): \n    first = type(begin + step)(begin) \n    ap_gen = itertools .count(first, step) \n    if end is not None: \n        ap_gen = itertools .takewhile (lambda n: n < end, ap_gen) \n    return ap_gen\nNote that aritprog_gen  is not a generator function in Example 17-13 : it\nhas no yield  in its body . But it returns a generator , so it operates as a\ngenerator factory , just as a generator function does.\nThe point of Example 17-13  is: when implementing generators, know what\nis available in the standard library , otherwise there’ s a good chance you’ll\nreinvent the wheel. That’ s why the next section covers several ready-to-use\ngenerator functions.\nG e n e r a t o r  F u n c t i o n s  i n  t h e  S t a n d a r d  L i b r a r y\nThe standard library provides many generators, from plain-text file objects\nproviding line-by-line iteration, to the awesome os.walk  function, which\nyields filenames while traversing a directory tree, making recursive\nfilesystem searches as simple as a for  loop.\nThe os.walk  generator function is impressive, but in this section I want to\nfocus on general-purpose functions that take arbitrary iterables as\nar guments and return generators that produce selected, computed, or\nrearranged items. In the following tables, I summarize two dozen of them,\nfrom the built-in, itertools , and functools  modules. For\nconvenience, I grouped them by high-level functionality , regardless of\nwhere they are defined.\nN O T E\nPerhaps you know all the functions mentioned in this section, but some of them are\nunderused, so a quick overview may be good to recall what’ s already available.\nThe first group are filtering generator functions: they yield a subset of items\nproduced by the input iterable, without changing the items themselves. W e\nused itertools.takewhile  previously in this chapter , in “Arithmetic\nProgression with itertools” . Like takewhile , most functions listed in\nT able 17-1  take a predicate , which is a one-ar gument Boolean function\nthat will be applied to each item in the input to determine whether the item\nis included in the output.\n \nT\na\nb\nl\ne\n \n1\n7\n-\n1\n.  \nF\ni\nl\nt\ne\nr\ni\nn\ng\n \ng\ne\nn\ne\nr\na\nt\no\nr\n \nf\nu\nn\nc\nt\ni\no\nn\ns\n \nModule Function Description\n \nitertools compress(it, sel\nector_it)Consumes two iterables in parallel; yields items  \nfrom it  whenever the corresponding item in sele\nctor_it  is truthy\nitertools dropwhile(predic\nate, it)Consumes it  skipping items while predicate  \ncomputes truthy , then yields every remaining item  \n(no further checks are made)\n(built-in) filter(predicat\ne, it)Applies predicate  to each item of iterable ,  \nyielding the item if predicate(item)  is truthy;  \nif predicate  is None , only truthy items are  \nyielded\nitertools filterfalse(pred\nicate, it)Same as filter , with the predicate  logic  \nnegated: yields items whenever predicate  \ncomputes falsy\nitertools islice(it, stop)  \nor islice(it, st\nart, stop, step=\n1)Y ields items from a slice of it , similar to s[:sto\np]  or s[start:stop:step]  except it  can be  \nany iterable, and the operation is lazy\nitertools takewhile(predic\nate, it)Y ields items while predicate  computes truthy ,  \nthen stops and no further checks are made\n \nThe console listing in Example 17-14  shows the use of all functions in\nT able 17-1 .\nExample 17-14. Filtering generator functions examples\n>>> def vowel(c): \n...     return c.lower() in 'aeiou' \n... \n>>> list(filter(vowel, 'Aardvark' )) \n['A', 'a', 'a']  \n>>> import itertools  \n>>> list(itertools .filterfalse (vowel, 'Aardvark' )) \n['r', 'd', 'v', 'r', 'k']  \n>>> list(itertools .dropwhile (vowel, 'Aardvark' )) \n['r', 'd', 'v', 'a', 'r', 'k']  \n>>> list(itertools .takewhile (vowel, 'Aardvark' )) \n['A', 'a']  \n>>> list(itertools .compress ('Aardvark' , (1,0,1,1,0,1))) \n['A', 'r', 'd', 'a']  \n>>> list(itertools .islice('Aardvark' , 4)) \n['A', 'a', 'r', 'd']  \n>>> list(itertools .islice('Aardvark' , 4, 7)) \n['v', 'a', 'r']  \n>>> list(itertools .islice('Aardvark' , 1, 7, 2)) \n['a', 'd', 'a']\nThe next group are the mapping generators: they yield items computed from\neach individual item in the input iterable—or iterables, in the case of map\nand starmap .  The generators in T able 17-2  yield one result per item in\nthe input iterables. If the input comes from more than one iterable, the\noutput stops as soon as the first input iterable is exhausted.11\n \nT\na\nb\nl\ne\n \n1\n7\n-\n2\n.  \nM\na\np\np\ni\nn\ng\n \ng\ne\nn\ne\nr\na\nt\no\nr\n \nf\nu\nn\nc\nt\ni\no\nn\ns\n \nModule Function Description\n \nitertools accumulate(i\nt, [func])Y ields accumulated sums; if func  is provided, yields  \nthe result of applying it to the first pair of items, then to  \nthe first result and next item, etc.\n(built-in) enumerate(ite\nrable, start=\n0)Y ields 2-tuples of the form (index, item) , where i\nndex  is counted from start , and item  is taken from  \nthe iterable\n(built-in) map(func, it\n1, [it2, …, i\ntN])Applies func  to each item of it , yielding the result; if  \nN iterables are given, func  must take N ar guments and  \nthe iterables will be consumed in parallel\nitertools starmap(func,  \nit)Applies func  to each item of it , yielding the result;  \nthe input iterable should yield iterable items iit , and f\nunc  is applied as func(*iit)\n \nExample 17-15  demonstrates some uses of itertools.accumulate .\nExample 17-15. itertools.accumulate generator function examples\n>>> sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1] \n>>> import itertools  \n>>> list(itertools .accumulate (sample))  \n \n[5, 9, 11, 19, 26, 32, 35, 35, 44, 45]  \n>>> list(itertools .accumulate (sample, min))  \n \n[5, 4, 2, 2, 2, 2, 2, 0, 0, 0]  \n>>> list(itertools .accumulate (sample, max))  \n \n[5, 5, 5, 8, 8, 8, 8, 8, 9, 9]  \n>>> import operator  \n>>> list(itertools .accumulate (sample, operator .mul))  \n \n[5, 20, 40, 320, 2240, 13440, 40320, 0, 0, 0]  \n>>> list(itertools .accumulate (range(1, 11), operator .mul)) \n[1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800]  \n\nRunning sum.\nRunning minimum.\nRunning maximum.\nRunning product.\nFactorials from 1!  to 10! .\nThe remaining functions of T able 17-2  are shown in Example 17-16 .\nExample 17-16. Mapping generator function examples\n>>> list(enumerate ('albatroz ', 1))  \n \n[(1, 'a'), (2, 'l'), (3, 'b'), (4, 'a'), (5, 't'), (6, 'r'), (7,  \n'o'), (8, 'z')]  \n>>> import operator  \n>>> list(map(operator .mul, range(11), range(11)))  \n \n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]  \n>>> list(map(operator .mul, range(11), [2, 4, 8]))  \n \n[0, 4, 16]  \n>>> list(map(lambda a, b: (a, b), range(11), [2, 4, 8]))  \n \n[(0, 2), (1, 4), (2, 8)]  \n>>> import itertools  \n>>> list(itertools .starmap(operator .mul, enumerate ('albatroz ', 1)))  \n \n['a', 'll', 'bbb', 'aaaa', 'ttttt', 'rrrrrr', 'ooooooo',  \n'zzzzzzzz']  \n>>> sample = [5, 4, 2, 8, 7, 6, 3, 0, 9, 1] \n>>> list(itertools .starmap(lambda a, b: b/a, \n...     enumerate (itertools .accumulate (sample), 1)))  \n \n[5.0, 4.5, 3.6666666666666665, 4.75, 5.2, 5.333333333333333,  \n5.0, 4.375, 4.888888888888889, 4.5]\nNumber the letters in the word, starting from 1 .\nSquares of integers from 0  to 10 .\nMultiplying numbers from two iterables in parallel: results stop when\nthe shortest iterable ends.\nThis is what the zip  built-in function does.\nRepeat each letter in the word according to its place in it, starting from\n1 .\nRunning average.\nNext, we have the group of mer ging generators—all of these yield items\nfrom multiple input iterables. chain  and chain.from_iterable\nconsume the input iterables sequentially (one after the other), while\nproduct , zip , and zip_longest  consume the input iterables in\nparallel. See T able 17-3 .\n \nT\na\nb\nl\ne\n \n1\n7\n-\n3\n.  \nG\ne\nn\ne\nr\na\nt\no\nr\n \nf\nu\nn\nc\nt\ni\no\nn\ns\n \nt\nh\na\nt  \nm\ne\nr\ng\ne\n \nm\nu\nl\nt\ni\np\nl\ne\n \ni\nn\np\nu\nt  \ni\nt\ne\nr\na\nb\nl\ne\ns\n \nModule Function Description\n \nitertools chain(it1, …,  \nitN)Y ield all items from it1 , then from it2  etc.,  \nseamlessly\nitertools chain.from_it\nerable(it)Y ield all items from each iterable produced by it , one  \nafter the other , seamlessly; it  should yield iterable  \nitems, for example, a list of iterables\nitertools product(it1,  \n…, itN, repea\nt=1)Cartesian product: yields N-tuples made by combining  \nitems from each input iterable like nested for  loops  \ncould produce; repeat  allows the input iterables to be  \nconsumed more than once\n(built-in) zip(it1, …, i\ntN)Y ields N-tuples built from items taken from the iterables  \nin parallel, silently stopping when the first iterable is  \nexhausted\nitertools zip_longest(i\nt1, …, itN, f\nillvalue=Non\ne)Y ields N-tuples built from items taken from the iterables  \nin parallel, stopping only when the last iterable is  \nexhausted, filling the blanks with the fillvalue\n \nExample 17-17  shows the use of the itertools.chain  and zip\ngenerator functions and their siblings. Recall that the zip  function is\nnamed after the zip fastener or zipper (no relation with compression). Both\nzip  and itertools.zip_longest  were introduced in “The A wesome\nzip” .\nExample 17-17. Mer ging generator function examples\n>>> list(itertools .chain('ABC', range(2)))  \n \n['A', 'B', 'C', 0, 1]  \n>>> list(itertools .chain(enumerate ('ABC')))  \n \n[(0, 'A'), (1, 'B'), (2, 'C')]  \n>>> list(itertools .chain.from_iterable (enumerate ('ABC')))  \n \n[0, 'A', 1, 'B', 2, 'C']  \n>>> list(zip('ABC', range(5)))  \n \n[('A', 0), ('B', 1), ('C', 2)]  \n>>> list(zip('ABC', range(5), [10, 20, 30, 40]))  \n \n[('A', 0, 10), ('B', 1, 20), ('C', 2, 30)]  \n>>> list(itertools .zip_longest ('ABC', range(5)))  \n \n[('A', 0), ('B', 1), ('C', 2), (None, 3), (None, 4)]  \n>>> list(itertools .zip_longest ('ABC', range(5), fillvalue ='?'))  \n \n[('A', 0), ('B', 1), ('C', 2), ('?', 3), ('?', 4)]\nchain  is usually called with two or more iterables.\nchain  does nothing useful when called with a single iterable.\nBut chain.from_iterable  takes each item from the iterable, and\nchains them in sequence, as long as each item is itself iterable.\nzip  is commonly used to mer ge two iterables into a series of two-\ntuples.\nAny number of iterables can be consumed by zip  in parallel, but the\ngenerator stops as soon as the first iterable ends.\nitertools.zip_longest  works like zip , except it consumes all\ninput iterables to the end, padding output tuples with None  as needed.\nThe fillvalue  keyword ar gument specifies a custom padding value.\nThe itertools.product  generator is a lazy way of computing\nCartesian products, which we built using list comprehensions with more\nthan one for  clause in “Cartesian Products” . Generator expressions with\nmultiple for  clauses can also be used to produce Cartesian products lazily .\nExample 17-18  demonstrates itertools.product .\nExample 17-18. itertools.pr oduct generator function examples\n>>> list(itertools .product('ABC', range(2)))  \n \n[('A', 0), ('A', 1), ('B', 0), ('B', 1), ('C', 0), ('C', 1)]  \n>>> suits = 'spades hearts diamonds clubs '.split() \n>>> list(itertools .product('AK', suits))  \n \n[('A', 'spades'), ('A', 'hearts'), ('A', 'diamonds'), ('A',  \n'clubs'),  \n('K', 'spades'), ('K', 'hearts'), ('K', 'diamonds'), ('K',  \n'clubs')]  \n>>> list(itertools .product('ABC'))  \n \n[('A',), ('B',), ('C',)]  \n>>> list(itertools .product('ABC', repeat=2))  \n \n[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'),  \n('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')]  \n>>> list(itertools .product(range(2), repeat=3)) \n[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0),  \n(1, 0, 1), (1, 1, 0), (1, 1, 1)]  \n>>> rows = itertools .product('AB', range(2), repeat=2) \n>>> for row in rows: print(row) \n... \n('A', 0, 'A', 0)  \n('A', 0, 'A', 1)  \n('A', 0, 'B', 0)  \n('A', 0, 'B', 1)  \n('A', 1, 'A', 0)  \n('A', 1, 'A', 1)  \n('A', 1, 'B', 0)  \n('A', 1, 'B', 1)  \n('B', 0, 'A', 0)  \n('B', 0, 'A', 1)  \n('B', 0, 'B', 0)  \n('B', 0, 'B', 1)  \n('B', 1, 'A', 0)  \n('B', 1, 'A', 1)  \n('B', 1, 'B', 0)  \n('B', 1, 'B', 1)\nThe Cartesian product of a str  with three characters and a range  with\ntwo integers yields six tuples (because 3 * 2  is 6 ).\nThe product of two card ranks ( 'AK' ), and four suits is a series of eight\ntuples.\nGiven a single iterable, product  yields a series of one-tuples, not very\nuseful.\nThe repeat=N  keyword ar gument tells product to consume each input\niterable N  times.\nSome generator functions expand the input by yielding more than one value\nper input item. They are listed in T able 17-4 .\n \nT\na\nb\nl\ne\n \n1\n7\n-\n4\n.  \nG\ne\nn\ne\nr\na\nt\no\nr  \nf\nu\nn\nc\nt\ni\no\nn\ns  \nt\nh\na\nt  \ne\nx\np\na\nn\nd\n \ne\na\nc\nh\n \ni\nn\np\nu\nt  \ni\nt\ne\nm\n \ni\nn\nt\no\n \nm\nu\nl\nt\ni\np\nl\ne\n \no\nu\nt\np\nu\nt  \ni\nt\ne\nm\ns\n \nModule Function Description\n \nitertools combinations(it, o\nut_len)Y ield combinations of out_len  items from the  \nitems yielded by it\nitertools combinations_with_\nreplacement(it, ou\nt_len)Y ield combinations of out_len  items from the  \nitems yielded by it , including combinations  \nwith repeated items\nitertools count(start=0, ste\np=1)Y ields numbers starting at start , incremented  \nby step , indefinitely\nitertools cycle(it) Y ields items from it  storing a copy of each, then  \nyields the entire sequence repeatedly , indefinitely\nitertools permutations(it, o\nut_len=None)Y ield permutations of out_len  items from the  \nitems yielded by it ; by default, out_len  is le\nn(list(it))\nitertools repeat(item, [time\ns])Y ield the given item repeatedly , indefinitely  \nunless a number of times  is given\n \nThe count  and repeat  functions from itertools  return generators\nthat conjure items out of nothing: neither of them takes an iterable as input.\nW e saw itertools.count  in “Arithmetic Progression with itertools” .\nThe cycle  generator makes a backup of the input iterable and yields its\nitems repeatedly . Example 17-19  illustrates the use of count , repeat ,\nand cycle .\nExample 17-19. count, cycle, and r epeat\n>>> ct = itertools .count()  \n \n>>> next(ct)  \n \n0 \n>>> next(ct), next(ct), next(ct)  \n \n(1, 2, 3)  \n>>> list(itertools .islice(itertools .count(1, .3), 3))  \n \n[1, 1.3, 1.6]  \n>>> cy = itertools .cycle('ABC')  \n \n>>> next(cy) \n'A' \n>>> list(itertools .islice(cy, 7))  \n \n['B', 'C', 'A', 'B', 'C', 'A', 'B']  \n>>> rp = itertools .repeat(7)  \n \n>>> next(rp), next(rp) \n(7, 7) \n>>> list(itertools .repeat(8, 4))  \n \n[8, 8, 8, 8]  \n>>> list(map(operator .mul, range(11), itertools .repeat(5)))  \n \n[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\nBuild a count  generator ct .\nRetrieve the first item from ct .\nI can’ t build a list  from ct , because ct  never stops, so I fetch the\nnext three items.\nI can build a list  from a count  generator if it is limited by islice\nor takewhile .\nBuild a cycle  generator from 'ABC'  and fetch its first item, 'A' .\nA list  can only be built if limited by islice ; the next seven items\nare retrieved here.\nBuild a repeat  generator that will yield the number 7  forever .\nA repeat  generator can be limited by passing the times  ar gument:\nhere the number 8  will be produced 4  times.\nA common use of repeat : providing a fixed ar gument in map ; here it\nprovides the 5  multiplier .\nThe combinations , combinations_with_replacement , and\npermutations  generator functions—together with product —are\ncalled the combinatorics generators  in the itertools  documentation\npage . There is a close relationship between itertools.product  and\nthe remaining combinatoric  functions as well, as Example 17-20  shows.\nExample 17-20. Combinatoric generator functions yield multiple values per\ninput item\n>>> list(itertools .combinations ('ABC', 2))  \n \n[('A', 'B'), ('A', 'C'), ('B', 'C')]  \n>>> list(itertools .combinations_with_replacement ('ABC', 2))  \n \n[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C',  \n'C')] \n>>> list(itertools .permutations ('ABC', 2))  \n \n[('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C',  \n'B')] \n>>> list(itertools .product('ABC', repeat=2))  \n \n[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B',  \n'C'), \n('C', 'A'), ('C', 'B'), ('C', 'C')]\nAll combinations of len()==2  from the items in 'ABC' ; item\nordering in the generated tuples is irrelevant (they could be sets).\nAll combinations of len()==2  from the items in 'ABC' , including\ncombinations with repeated items.\nAll permutations of len()==2  from the items in 'ABC' ; item\nordering in the generated tuples is relevant.\nCartesian product from 'ABC'  and 'ABC'  (that’ s the ef fect of\nrepeat=2 ).\nThe last group of generator functions we’ll cover in this section are\ndesigned to yield all items in the input iterables, but rearranged in some\nway . Here are two functions that return multiple generators:\nitertools.groupby  and itertools.tee . The other generator\nfunction in this group, the reversed  built-in, is the only one covered in\nthis section that does not accept any iterable as input, but only sequences.\nThis makes sense: because reversed  will yield the items from last to\nfirst, it only works with a sequence with a known length. But it avoids the\ncost of making a reversed copy of the sequence by yielding each item as\nneeded. I put the itertools.product  function together with the\nmer ging  generators in T able 17-3  because they all consume more than one\niterable, while the generators in T able 17-5  all accept at most one input\niterable.\n \nT\na\nb\nl\ne\n \n1\n7\n-\n5\n.  \nR\ne\na\nr\nr\na\nn\ng\ni\nn\ng\n \ng\ne\nn\ne\nr\na\nt\no\nr\n \nf\nu\nn\nc\nt\ni\no\nn\ns\n \nModule Function Description\n \nitertools groupby(it, k\ney=None)Y ields 2-tuples of the form (key, group) , where ke\ny  is the grouping criterion and group  is a generator  \nyielding the items in the group\n(built-in) reversed(seq) Y ields items from seq  in reverse order , from last to  \nfirst; seq  must be a sequence or implement the __rev\nersed__  special method\nitertools tee(it, n=2) Y ields a tuple of n  generators, each yielding the items of  \nthe input iterable independently\n \nExample 17-21  demonstrates the use of itertools.groupby  and the\nreversed  built-in. Note that itertools.groupby  assumes that the\ninput iterable is sorted by the grouping criterion, or at least that the items\nare clustered by that criterion—even if not sorted.\nExample 17-21. itertools.gr oupby\n>>> list(itertools .groupby('LLLLAAGGG '))  \n \n[('L', <itertools._grouper object at 0x102227cc0>),  \n('A', <itertools._grouper object at 0x102227b38>),  \n('G', <itertools._grouper object at 0x102227b70>)]  \n>>> for char, group in itertools .groupby('LLLLAAAGG '):  \n \n...     print(char, '->', list(group)) \n... \nL -> ['L', 'L', 'L', 'L']  \nA -> ['A', 'A',]  \nG -> ['G', 'G', 'G']  \n>>> animals = ['duck', 'eagle', 'rat', 'giraffe', 'bear', \n...            'bat', 'dolphin', 'shark', 'lion'] \n>>> animals.sort(key=len)  \n \n>>> animals \n['rat', 'bat', 'duck', 'bear', 'lion', 'eagle', 'shark',  \n'giraffe', 'dolphin']  \n>>> for length, group in itertools .groupby(animals, len):  \n \n...     print(length, '->', list(group)) \n... \n3 -> ['rat', 'bat']  \n4 -> ['duck', 'bear', 'lion']  \n5 -> ['eagle', 'shark']  \n7 -> ['giraffe', 'dolphin']  \n>>> for length, group in itertools .groupby(reversed (animals), len): \n \n...     print(length, '->', list(group)) \n... \n7 -> ['dolphin', 'giraffe']  \n5 -> ['shark', 'eagle']  \n4 -> ['lion', 'bear', 'duck']  \n3 -> ['bat', 'rat']  \n>>>\ngroupby  yields tuples of (key, group_generator) .\nHandling groupby  generators involves nested iteration: in this case,\nthe outer for  loop and the inner list  constructor .\nT o use groupby , the input should be sorted; here the words are sorted\nby length.\nAgain, loop over the key  and group  pair , to display the key  and\nexpand the group  into a list .\nHere the reverse  generator iterates over animals  from right to left.\nThe last of the generator functions in this group is iterator.tee , which\nhas a unique behavior: it yields multiple generators from a single input\niterable, each yielding every item from the input. Those generators can be\nconsumed independently , as shown in Example 17-22 .",21830
232-Subgenerators with yield from.pdf,232-Subgenerators with yield from,"Example 17-22. itertools.tee yields multiple generators, each yielding every\nitem of the input generator\n>>> list(itertools .tee('ABC')) \n[<itertools._tee object at 0x10222abc8>, <itertools._tee object at  \n0x10222ac08>]  \n>>> g1, g2 = itertools .tee('ABC') \n>>> next(g1) \n'A' \n>>> next(g2) \n'A' \n>>> next(g2) \n'B' \n>>> list(g1) \n['B', 'C']  \n>>> list(g2) \n['C'] \n>>> list(zip(*itertools .tee('ABC'))) \n[('A', 'A'), ('B', 'B'), ('C', 'C')]\nNote that several examples in this section used combinations of generator\nfunctions. This is a great feature of these functions: because they take\ngenerators as ar guments and return generators, they can be combined in\nmany dif ferent ways.\nThe yield from  syntax provides a new way of combining generators.\nThat’ s next.\nS u b g e n e r a t o r s  w i t h  y i e l d  f r o m\nThe yield from  expression syntax was introduced in Python 3.3 to\nallow a generator to delegate work to a subgenerator .\nExample 17-23  is a simple experiment with yield from :\nExample 17-23. T est driving yield from .\n>>> def sub_gen(): \n...     yield 1.1 \n...     yield 1.2 \n... \n>>> def gen(): \n...     yield 1 \n...     yield from  sub_gen() \n...     yield 2 \n... \n>>> for x in gen(): \n...     print(x) \n... \n1 \n1.1 \n1.2 \n2\nIn Example 17-23 , the for  loop is the client code , gen  is the delegating\ngenerator  and sub_gen  is the subgenerator . Note that yield from\npauses gen , then executes sub_gen . The values yielded by sub_gen\npass through gen  directly to the client for  loop. Meanwhile, gen  is\nsuspended and cannot see the values passing through it. When sub_gen  is\ndone, gen  resumes.\nWhen used in an expression, the value of yield from  is the return value\nof the subgenerator . Example 17-24  demonstrates.\nExample 17-24. yield from  gets the r eturn value of the subgenerator .\n>>> def sub_gen(): \n...     yield 1.1 \n...     yield 1.2 \n...     return 'Done!' \n... \n>>> def gen(): \n...     yield 1 \n...     result = yield from  sub_gen() \n...     print('<--', result) \n...     yield 2 \n... \n>>> for x in gen(): \n...     print(x) \n... \n1 \n1.1 \n1.2 \n<-- Done!  \n2\nNow that we’ve seen the basics of yield from , let’ s study a couple of\nsimple but practical examples of its use.",2285
233-Reinventing chain..pdf,233-Reinventing chain.,,0
234-Traversing a tree.pdf,234-Traversing a tree,"Reinventing chain .\nBefore yield from  was introduced, when a generator needed to yield\nvalues produced from another generator , nested for  loops were the only\nway .\nHere is an example: the itertools  module of the Python standard library\nhas a chain  generator that yields items from several iterables, iterating\nover the first, then the second and so on up to the last.  This is a\nhomemade implementation of chain  in Python, using nested for  loops:\n>>> def chain(*iterables ): \n...     for it in iterables : \n...         for i in it: \n...             yield i \n... \n>>> s = 'ABC' \n>>> t = tuple(range(3)) \n>>> list(chain(s, t)) \n['A', 'B', 'C', 0, 1, 2]\nThe chain  generator above is delegating to each iterable it  in turn, by\ndriving each it  in the inner for  loop. That inner loop can be replaced with\na yield from  expression, as shown in the next console listing:\n>>> def chain(*iterables ): \n...     for i in iterables : \n...         yield from  i \n... \n>>> list(chain(s, t)) \n['A', 'B', 'C', 0, 1, 2]\nThe use of yield from  in this example is correct, and the code reads\nbetter , but it seems like mere syntactic sugar . Now let’ s develop a more\ninteresting example.\nT raversing a tree12\nIn this section we’ll use yield from  in a script to traverse a tree\nstructure. W e will build it in baby steps.\nThe tree structure for this example is Python’ s exception hierarchy . But the\ncode can be easily adapted to show a directory tree or any other tree\nstructure.\nStarting from BaseException  at level zero, the exception hierarchy is 5\nlevels deep (as of Python 3.9). Our first baby step is to show level zero.\nGiven a root class, the tree  generator in Example 17-25  yields its name\nand stops:\nExample 17-25. tr ee/step0/tr ee.py: yield the name of r oot class and stop.\ndef tree(cls): \n    yield cls.__name__  \n \n \ndef display(cls): \n    for cls_name  in tree(cls): \n        print(cls_name ) \n \n \nif __name__  == '__main__' : \n    display(BaseException )\nThe output of Example 17-25  is just one line:\nBaseException\nThe next baby step takes us to level 1. The tree  generator will yield the\nname of the root class and the names of each direct subclass. The names of\nthe subclasses are indented to reveal the hierarchy . This is the output we\nwant:\n$ python3 tree.py  \nBaseException  \n    Exception  \n    GeneratorExit  \n    SystemExit  \n    KeyboardInterrupt\nExample 17-26  produces that output.\nExample 17-26. tr ee/step1/tr ee.py: yield the name of r oot class and dir ect\nsubclasses.\ndef tree(cls): \n    yield cls.__name__ , 0                        \n  \n    for sub_cls in cls.__subclasses__ ():         \n  \n        yield sub_cls.__name__ , 1                \n  \n \n \ndef display(cls): \n    for cls_name , level in tree(cls): \n        indent = ' ' * 4 * level                 \n  \n        print(f'{indent}{cls_name} ') \n \n \nif __name__  == '__main__ ': \n    display(BaseException )\nT o support the indented output, yield the name of the class and its level\nin the hierarchy .\nUse the __subclasses__  special method to get list of subclasses.\nY ield name of subclass and level 1.\nBuild indentation string of 4 spaces times level . At level zero, this\nwill be an empty string.\nIn Example 17-27  we refactor to separate the special case of the root class\nfrom the subclasses, which are now handled in the sub_tree  generator .\nAt yield from , the tree  generator is suspended and sub_tree  takes\nover yielding values.\nExample 17-27. tr ee/step2/tr ee.py: tree  yields r oot class name, then\ndelegates to sub_tree .\ndef tree(cls): \n    yield cls.__name__ , 0 \n    yield from  sub_tree (cls)              \n  \n 13\n \ndef sub_tree (cls): \n    for sub_cls in cls.__subclasses__ (): \n        yield sub_cls.__name__ , 1         \n  \n \n \ndef display(cls): \n    for cls_name , level in tree(cls): \n        indent = ' ' * 4 * level \n        print(f'{indent}{cls_name} ') \n \n \nif __name__  == '__main__ ': \n    display(BaseException )\nDelegate to sub_tree  to yield the names of the subclasses.\nY ield name of subclass and level 1, directly to the printing for  loop\ndriving tree .\nIn keeping with our baby steps method, we’ll write the simplest code we\ncan imagine to reach level 2. For depth-first tree traversal, after yielding\neach node in level 1, we want to yield the children of that node in level 2,\nbefore resuming level 1. W e can code this with a nested for  loop, as in\nExample 17-28 .\nExample 17-28. tr ee/step3/tr ee.py: sub_tree  traverses levels 1 and 2\ndepth-first.\ndef tree(cls): \n    yield cls.__name__ , 0 \n    yield from  sub_tree (cls) \n \n \ndef sub_tree (cls): \n    for sub_cls in cls.__subclasses__ (): \n        yield sub_cls.__name__ , 1 \n        for sub_sub_cls  in sub_cls.__subclasses__ (): \n            yield sub_sub_cls .__name__ , 2 \n \n \ndef display(cls): \n    for cls_name , level in tree(cls): \n        indent = ' ' * 4 * level \n        print(f'{indent}{cls_name}' ) \n \n \nif __name__  == '__main__' : \n    display(BaseException )\nThis is the result of running step3/tree.py  from Example 17-28 :\n$ python3 tree.py  \nBaseException  \n    Exception  \n        TypeError  \n        StopAsyncIteration  \n        StopIteration  \n        ImportError  \n        OSError  \n        EOFError  \n        RuntimeError  \n        NameError  \n        AttributeError  \n        SyntaxError  \n        LookupError  \n        ValueError  \n        AssertionError  \n        ArithmeticError  \n        SystemError  \n        ReferenceError  \n        MemoryError  \n        BufferError  \n        Warning  \n    GeneratorExit  \n    SystemExit  \n    KeyboardInterrupt\nY ou may already know where this is going, but I will stick to baby steps\none more time: let’ s reach level 3 by adding yet another nested for  loop.\nThe rest of the program is unchanged, so Example 17-29  shows only the\nsub_tree  generator .\nExample 17-29. sub_tree  generator fr om tree/step4/tree.py .\ndef sub_tree (cls): \n    for sub_cls in cls.__subclasses__ (): \n        yield sub_cls.__name__ , 1 \n        for sub_sub_cls  in sub_cls.__subclasses__ (): \n            yield sub_sub_cls .__name__ , 2 \n            for sub_sub_sub_cls  in sub_sub_cls .__subclasses__ (): \n                yield sub_sub_sub_cls .__name__ , 3\nThere is a clear pattern in Example 17-29 . W e do a for  loop to get the\nsubclasses of level N. Each time around the loop we yield a subclass and\nlevel N, then start another for loop to visit level N+1.\nIn “Reinventing chain .”  we saw how we can replace a nested for  loop\ndriving a generator with yield from  on the same generator . W e can\napply that idea here, if we make sub_tree  accept a level  parameter ,\nand yield from  it recursively , passing the current subclass as the new\nroot class with the next level number . See Example 17-30 .\nExample 17-30. tr ee/step5/tr ee.py: r ecursive sub_tree  goes as far as\nmemory allows.\ndef tree(cls): \n    yield cls.__name__ , 0 \n    yield from  sub_tree (cls, 1) \n \n \ndef sub_tree (cls, level): \n    for sub_cls in cls.__subclasses__ (): \n        yield sub_cls.__name__ , level \n        yield from  sub_tree (sub_cls, level+1) \n \n \ndef display(cls): \n    for cls_name , level in tree(cls): \n        indent = ' ' * 4 * level \n        print(f'{indent}{cls_name}' ) \n \n \nif __name__  == '__main__' : \n    display(BaseException )\nExample 17-30  can traverse trees of any depth, limited only by Python’ s\nrecursion limit. The default limit allows 1000 pending functions.\nAny good tutorial about recursion will stress the importance of a base case\nto avoid infinite recursion. The body of a recursive function often has an if\nwith one branch that does not make a recursive call—that’ s the base case. In\nExample 17-30 , sub_tree  has no if , but there is an implicit conditional\nin the for  loop: if cls.__subclasses__()  returns an empty list, the\nbody of the loop is not executed, therefore no recursive call happens. The\nbase case is when the current class has no subclasses. In that case,\nsub_tree  does yields nothing. It just returns.\nExample 17-30  works as intended, but we can make it more elegant by\nrecalling the pattern we observed in when we reached level 3 ( Example 17-\n29 ): we yield a subclass with level N, then start a nested for loop to visit\nlevel N+1. In Example 17-30  we replaced that nested loop with yield\nfrom . Now we can mer ge tree  and sub_tree  into a single generator .\nExample 17-31  is the last step for this example.\nExample 17-31. tr ee/step6/tr ee.py: r ecursive calls of tree  pass an\nincr emented level  ar gument.\ndef tree(cls, level=0): \n    yield cls.__name__ , level \n    for sub_cls in cls.__subclasses__ (): \n        yield from  tree(sub_cls, level+1) \n \n \ndef display(cls): \n    for cls_name , level in tree(cls): \n        indent = ' ' * 4 * level \n        print(f'{indent}{cls_name}' ) \n \n \nif __name__  == '__main__' : \n    display(BaseException )\nAt the start of “Subgenerators with yield from”  we saw how yield from\nconnects the subgenerator directly to the client code, bypassing the\ndelegating generator . That connection becomes really important when\ngenerators are used as coroutines and not only produce but also consume\nvalues from the client code. Chapter 19  dives into coroutines, and has\nseveral pages explaining why yield from  is much more than syntactic\nsugar .\nAfter this first encounter with yield from , we’ll go back to our review\nof iterable-savvy functions in the standard library .",9640
235-Iterable Reducing Functions.pdf,235-Iterable Reducing Functions,"I t e r a b l e  R e d u c i n g  F u n c t i o n s\nThe functions in T able 17-6  all take an iterable and return a single result.\nThey are known as “reducing,” “folding,” or “accumulating” functions.\nActually , every one of the built-ins listed here can be implemented with\nfunctools.reduce , but they exist as built-ins because they address\nsome common use cases more easily . Also, in the case of all  and any ,\nthere is an important optimization that can’ t be done with reduce : these\nfunctions short-circuit (i.e., they stop consuming the iterator as soon as the\nresult is determined). See the last test with any  in Example 17-32 .\n \nT\na\nb\nl\ne  \n1\n7\n-\n6\n.  \nB\nu\ni\nl\nt\n-\ni\nn  \nf\nu\nn\nc\nt\ni\no\nn\ns  \nt\nh\na\nt  \nr\ne\na\nd  \ni\nt\ne\nr\na\nb\nl\ne\ns  \na\nn\nd  \nr\ne\nt\nu\nr\nn  \ns\ni\nn\ng\nl\ne  \nv\na\nl\nu\ne\ns\n \nModule Function Description\n \n(built-in) all(it) Returns True  if all items in it  are truthy , otherwise Fa\nlse ; all([])  returns True\n(built-in) any(it) Returns True  if any item in it  is truthy , otherwise Fal\nse ; any([])  returns False\n(built-in) max(it, [key\n=,] [default\n=])Returns the maximum value of the items in it ;  key  is  \nan ordering function, as in sorted ; default  is  \nreturned if the iterable is empty\n(built-in) min(it, [key\n=,] [default\n=])Returns the minimum value of the items in it .  key  is  \nan ordering function, as in sorted ; default  is  \nreturned if the iterable is empty\nfunctools reduce(func,  \nit, [initia\nl])Returns the result of applying func  to the first pair of  \nitems, then to that result and the third item and so on; if  \ngiven, initial  forms the initial pair with the first item\n(built-in) sum(it, start\n=0)The sum of all items in it , with the optional start  \nvalue added (use math.fsum  for better precision when  \nadding floats)\n \na  May also be called as max(arg1, arg2, …, [key=?]) , in which case the maximum among  \nthe ar guments is returned.\nb  May also be called as min(arg1, arg2, …, [key=?]) , in which case the minimum among  \nthe ar guments is returned.\nThe operation of all  and any  is exemplified in Example 17-32 .\nExample 17-32. Results of all and any for some sequences\n>>> all([1, 2, 3]) \nTrue \n>>> all([1, 0, 3]) \nFalse \n>>> all([]) \nTrue \n>>> any([1, 2, 3]) \nTrue \n>>> any([1, 0, 3]) \nTrue \n>>> any([0, 0.0]) \nFalse \n>>> any([]) a\nb",2421
236-A Closer Look at the iter Function.pdf,236-A Closer Look at the iter Function,"False \n>>> g = (n for n in [0, 0.0, 7, 8]) \n>>> any(g) \nTrue \n>>> next(g) \n8\nA longer explanation about functools.reduce  appeared in “V ector\nT ake #4: Hashing and a Faster ==” .\nAnother built-in that takes an iterable and returns something else is\nsorted . Unlike reversed , which is a generator function, sorted\nbuilds and returns an actual list. After all, every single item of the input\niterable must be read so they can be sorted, and the sorting happens in a\nlist , therefore sorted  just returns that list  after it’ s done. I mention\nsorted  here because it does consume an arbitrary iterable.\nOf course, sorted  and the reducing functions only work with iterables\nthat eventually stop. Otherwise, they will keep on collecting items and\nnever return a result.\nW e’ll now go back to the iter()  built-in: it has a little-known feature that\nwe haven’ t covered yet.\nA  C l o s e r  L o o k  a t  t h e  i t e r  F u n c t i o n\nAs we’ve seen, Python calls iter(x)  when it needs to iterate over an\nobject x .\nBut iter  has another trick: it can be called with two ar guments to create\nan iterator from a regular function or any callable object. In this usage, the\nfirst ar gument must be a callable to be invoked repeatedly (with no\nar guments) to yield values, and the second ar gument is a sentinel: a marker\nvalue which, when returned by the callable, causes the iterator to raise\nStopIteration  instead of yielding the sentinel.\nThe following example shows how to use iter  to roll a six-sided dice until\na 1  is rolled:\n>>> def d6(): \n...     return randint(1, 6) \n... \n>>> d6_iter = iter(d6, 1) \n>>> d6_iter \n<callable_iterator object at 0x00000000029BE6A0>  \n>>> for roll in d6_iter: \n...     print(roll) \n... \n4 \n3 \n6 \n3\nNote that the iter  function here returns a callable_iterator . The\nfor  loop in the example may run for a very long time, but it will never\ndisplay 1 , because that is the sentinel value. As usual with iterators, the\nd6_iter  object in the example becomes useless once exhausted. T o start\nover , you must rebuild the iterator by invoking iter(…)  again.\nA simple example used to be found in the iter  built-in function\ndocumentation : this snippet reads lines from a file until a blank line\nterminated with \n  is found.\nwith open('mydata.txt' ) as fp: \n    for line in iter(fp.readline , '\n'): \n        process_line (line)\nHowever , that example is problematic in practice. If no blank line with a\nsingle \n  is present, the for  loop will run forever because\nfp.readline()  returns an empty string ''  when the end of file is\nreached.\nSince I wrote Fluent Python, First Edition , that example was replaced in the\niter  entry  with this new one, a block reader . The documentation explains:\nOne useful application of the second form of iter()  is to build a block-\nr eader . For example, r eading fixed-width blocks fr om a binary database\nfile until the end of file is r eached:",2981
237-Case Study Generators in a Database Conversion Utility.pdf,237-Case Study Generators in a Database Conversion Utility,"from functools  import partial \n \nwith open('mydata.db' , 'rb') as f: \n    read64 = partial(f.read, 64) \n    for block in iter(read64, b''): \n        process_block (block)\nFor clarity , I’ve added the read64  assignment, which is not in the current\nexample .\nT o close this chapter , I present a practical example of using generators to\nhandle a lar ge volume of data ef ficiently .\nC a s e  S t u d y :  G e n e r a t o r s  i n  a  D a t a b a s e\nC o n v e r s i o n  U t i l i t y\nY ears ago I worked at BIREME, a digital library run by P AHO/WHO (Pan-\nAmerican Health Or ganization/W orld Health Or ganization) in São Paulo,\nBrazil. Among the bibliographic datasets created by BIREME are LILACS\n(Latin American and Caribbean Health Sciences index) and SciELO\n(Scientific Electronic Library Online), two comprehensive databases\nindexing the scientific and technical literature produced in the region.\nSince the late 1980s, the database system used to manage LILACS is\nCDS/ISIS, a non-relational, document database created by UNESCO and\neventually rewritten in C by BIREME to run on GNU/Linux servers. One of\nmy jobs was to research alternatives for a possible migration of LILACS—\nand eventually the much lar ger SciELO—to a modern, open source,\ndocument database such as CouchDB or MongoDB.\nAs part of that research, I wrote a Python script, isis2json.py , that reads a\nCDS/ISIS file and writes a JSON file suitable for importing to CouchDB or\nMongoDB. Initially , the script read files in the ISO-2709 format exported\nby CDS/ISIS. The reading and writing had to be done incrementally\nbecause the full datasets were much bigger than main memory . That was\neasy enough: each iteration of the main for  loop read one record from the\n.iso  file, massaged it, and wrote it to the .json  output.\nHowever , for operational reasons, it was deemed necessary that isis2json.py\nsupported another CDS/ISIS data format: the binary .mst  files used in\nproduction at BIREME—to avoid the costly export to ISO-2709.\nNow I had a problem: the libraries used to read ISO-2709 and .mst  files had\nvery dif ferent APIs. And the JSON writing loop was already complicated\nbecause the script accepted a variety of command-line options to restructure\neach output record. Reading data using two dif ferent APIs in the same for\nloop where the JSON was produced would be unwieldy .\nThe solution was to isolate the reading logic into a pair of generator\nfunctions: one for each supported input format. In the end, the isis2json.py\nscript was split into four functions. Y ou can see the main Python 2 script in\n[Link to Come], but the full source code with dependencies is in\nfluentpython/isis2json  on GitHub.\nHere is a high-level overview of how the script is structured:\nmain\nThe main  function uses argparse  to read command-line options that\nconfigure the structure of the output records. Based on the input\nfilename extension, a suitable generator function is selected to read the\ndata and yield the records, one by one.\niter_iso_records\nThis generator function reads .iso  files (assumed to be in the ISO-2709\nformat). It takes two ar guments: the filename and isis_json_type ,\none of the options related to the record structure. Each iteration of its\nfor  loop reads one record, creates an empty dict , populates it with\nfield data, and yields the dict .\niter_mst_records\nThis other generator functions reads .mst  files.  If you look at the\nsource code for isis2json.py , you’ll see that it’ s not as simple as\niter_iso_records , but its interface and overall structure is the14",3621
238-Generators as Coroutines.pdf,238-Generators as Coroutines,"same: it takes a filename and an isis_json_type  ar gument and\nenters a for  loop, which builds and yields one dict  per iteration,\nrepresenting a single record.\nwrite_json\nThis function performs the actual writing of the JSON records, one at a\ntime. It takes numerous ar guments, but the first one— input_gen —is\na reference to a generator function: either iter_iso_records  or\niter_mst_records . The main for  loop in write_json  iterates\nover the dictionaries yielded by the selected input_gen  generator ,\nmassages it in several ways as determined by the command-line options,\nand appends the JSON record to the output file.\nBy leveraging generator functions, I was able to decouple the reading logic\nfrom the writing logic. Of course, the simplest way to decouple them would\nbe to read all records to memory , then write them to disk. But that was not a\nviable option because of the size of the datasets. Using generators, the\nreading and writing is interleaved, so the script can process files of any size.\nNow if isis2json.py  needs to support an additional input format—say ,\nMARCXML, a DTD used by the U.S. Library of Congress to represent\nISO-2709 data—it will be easy to add a third generator function to\nimplement the reading logic, without changing anything in the complicated\nwrite_json  function.\nThis is not rocket science, but it’ s a real example where generators provided\na flexible solution to processing databases as a stream of records, keeping\nmemory usage low regardless of the amount of data. Anyone who manages\nlar ge datasets finds many opportunities for using generators in practice.\nThe next section addresses an aspect of generators that we’ll actually skip\nfor now . Read on to understand why .\nG e n e r a t o r s  a s  C o r o u t i n e s\nAbout five years after generator functions with the yield  keyword were\nintroduced in Python 2.2, PEP 342 — Coroutines via Enhanced Generators\nwas implemented in Python 2.5. This proposal added extra methods and\nfunctionality to generator objects, most notably the .send()  method.\nLike .__next__() , .send()  causes the generator to advance to the\nnext yield , but it also allows the client using the generator to send data\ninto it: whatever ar gument is passed to .send()  becomes the value of the\ncorresponding yield  expression inside the generator function body . In\nother words, .send()  allows two-way data exchange between the client\ncode and the generator—in contrast with .__next__() , which only lets\nthe client receive data from the generator .\nThis is such a major “enhancement” that it actually changes the nature of\ngenerators: when used in this way , they become cor outines . David Beazley\n—probably the most prolific writer and speaker about coroutines in the\nPython community—warned in a famous PyCon US 2009 tutorial :\nGenerators pr oduce data for iteration\nCor outines ar e consumers of data\nT o keep your brain fr om exploding, you don’ t mix the two\nconcepts together\nCor outines ar e not r elated to iteration\nNote: Ther e is a use of having yield pr oduce a value in a\ncor outine, but it’ s not tied to iteration.\n— David Beazley , A Curious Course on Coroutines and\nConcurrency\nI will follow Dave’ s advice and close this chapter—which is really about\niteration techniques—without touching send  and the other features that\nmake generators usable as coroutines. Coroutines will be covered in\nChapter 19 .15",3459
239-Generic Iterable Types.pdf,239-Generic Iterable Types,,0
240-Further Reading.pdf,240-Further Reading,"G e n e r i c  I t e r a b l e  T y p e s\nXXX\nclass typing.Iterable (Generic[T_co]): \n    ... \nclass typing.Iterator (Iterable [T_co]): \n    ...\nC h a p t e r  S u m m a r y\nIteration is so deeply embedded in the language that I like to say that\nPython groks iterators.  The integration of the Iterator pattern in the\nsemantics of Python is a prime example of how design patterns are not\nequally applicable in all programming languages. In Python, a classic\niterator implemented “by hand” as in Example 17-4  has no practical use,\nexcept as a didactic example.\nIn this chapter , we built a few versions of a class to iterate over individual\nwords in text files that may be very long. Thanks to the use of generators,\nthe successive refactorings of the Sentence  class become shorter and\neasier to read—when you know how they work.\nW e then coded a generator of arithmetic progressions and showed how to\nleverage the itertools  module to make it simpler . An overview of 24\ngeneral-purpose generator functions in the standard library followed.\nFollowing that, we looked at the iter  built-in function: first, to see how it\nreturns an iterator when called as iter(o) , and then to study how it builds\nan iterator from any function when called as iter(func, sentinel) .\nFor practical context, I described the implementation of a database\nconversion utility using generator functions to decouple the reading to the\nwriting logic, enabling ef ficient handling of lar ge datasets and making it\neasy to support more than one data input format.\nAlso mentioned in this chapter were the yield from  syntax, new in\nPython 3.3, and coroutines. Both topics were just introduced here; they get\nmore coverage later in the book.\nF u r t h e r  R e a d i n g\nA detailed technical explanation of generators appears in The Python\nLanguage Reference in 6.2.9. Y ield expressions . The PEP where generator\nfunctions were defined is PEP 255 — Simple Generators .16\nThe itertools  module documentation  is excellent because of all the\nexamples included. Although the functions in that module are implemented\nin C, the documentation shows how many of them would be written in\nPython, often by leveraging other functions in the module. The usage\nexamples are also great: for instance, there is a snippet showing how to use\nthe accumulate  function to amortize a loan with interest, given a list of\npayments over time. There is also an Itertools Recipes  section with\nadditional high-performance functions that use the itertools  functions\nas building blocks.\nBeyond Python’ s standard library , I recommend the More Itertools  package,\nwhich follows the fine itertools  tradition in providing powerful\ngenerators with plenty of examples and some useful recipes.\nChapter 4, “Iterators and Generators,” of Python Cookbook, 3E  (O’Reilly),\nby David Beazley and Brian K. Jones, has 16 recipes covering this subject\nfrom many dif ferent angles, focusing on practical applications. It includes\nsome illuminating recipes with yield from .\nSebastian Rittau—a top contributor of typeshed —explains why iterators\nshould be interable, as he noted in 2006 that Java: Iterators are not Iterable .\nThe yield from  syntax is explained with examples in What’ s New in\nPython 3.3 , section PEP 380: Syntax for Delegating to a Subgenerator .\nW e’ll also cover it in detail in “Using yield from”  and “The Meaning of\nyield from”  in Chapter 19 .\nIf you are interested in document databases and would like to learn more\nabout the context of “Case Study: Generators in a Database Conversion\nUtility” , the Code4Lib Journal—which covers the intersection between\nlibraries and technology—published my paper “From ISIS to CouchDB:\nDatabases and Data Models for Bibliographic Records” . One section of the\npaper describes the isis2json.py  script. The rest of it explains why and how\nthe semi-structured data model implemented by document databases like\nCouchDB and MongoDB are more suitable for cooperative bibliographic\ndata collection than the relational model.\nS O A P B O X\nGenerator Function Syntax: Mor e Sugar W ould Be Nice\nDesigners need to ensur e that contr ols and displays for differ ent\npurposes ar e significantly differ ent fr om one another .\n— Donald Norman, The Design of Everyday Things\nSource code plays the role of “controls and displays” in programming\nlanguages. I think Python is exceptionally well designed; its source\ncode is often as readable as pseudocode. But nothing is perfect. Guido\nvan Rossum should have followed Donald Norman’ s advice (previously\nquoted) and introduced another keyword for defining generator\nexpressions, instead of reusing def . The “BDFL Pronouncements”\nsection of PEP 255 — Simple Generators  actually ar gues:\nA “yield” statement buried in the body is not enough warning that\nthe semantics ar e so differ ent.\nBut Guido avoids introducing new keywords, because they may break\nexisting code. The Python 3 breakage was a one-of f event—I believe\nthat because I started using Python 1.5 and when Python 2 came along,\nmost programs did not break. Anyway , Guido did not find that\nar gument convincing, and I don’ t anticipate major breaking changes in\nfuture versions of Python, so we are stuck with def  doing double-duty\nfor functions and generators.\nReusing the function syntax for generators has other bad consequences.\nIn the paper and experimental work “Python, the Full Monty: A T ested\nSemantics for the Python Programming Language,” Politz  et al. show\nthis trivial example of a generator function (section 4.1 of the paper):\ndef f(): x=0 \n    while True: \n        x += 1 \n        yield x17\nThe authors then make the point that we can’ t abstract the process of\nyielding with a function call ( Example 17-33 ).\nE x a m p l e  1 7 - 3 3 .  “ [ T h i s ]  s e e m s  t o  p e r f o r m  a  s i m p l e  a b s t r a c t i o n\no v e r  t h e  p r o c e s s  o f  y i e l d i n g ”  ( P o l i t z  e t  a l . )\ndef f(): \n    def do_yield (n): \n        yield n \n    x = 0 \n    while True: \n        x += 1 \n        do_yield (x)\nIf we call f()  in Example 17-33 , we get an infinite loop, and not a\ngenerator , because the yield  keyword only makes the immediately\nenclosing function a generator function. The call do_yield(x)\nreturns a generator object which is immediately discarded, and the body\nof do_yield  never runs.\nAlthough generator functions look like functions, we cannot delegate to\nanother generator function with a simple function call. As a point of\ncomparison, the Lua language does not impose this limitation. A Lua\ncoroutine can call other functions and any of them can yield to the\noriginal caller .\nThe new yield from  syntax was introduced to allow a Python\ngenerator or coroutine to delegate work to another , without requiring\nthe workaround of an inner for  loop. Example 17-33  can be “fixed” by\nprefixing the function call with yield from , as in Example 17-34 .\nE x a m p l e  1 7 - 3 4 .  T h i s  a c t u a l l y  a b s t r a c t s  o v e r  t h e  p r o c e s s  o f\ny i e l d i n g\ndef f(): \n    def do_yield (n): \n        yield n \n    x = 0 \n    while True: \n        x += 1 \n        yield from  do_yield (x)\nReusing def  for declaring generators was a usability mistake, and the\nproblem was compounded in Python 2.5 with coroutines, which are also\ncoded as functions with yield . In the case of coroutines, the yield\njust happens to appear—usually—on the right-hand side of an\nassignment, because it receives the ar gument of the .send()  call from\nthe client. As David Beazley says:\nDespite some similarities, generators and cor outines ar e basically\ntwo differ ent concepts.\nFortunately , when Guido accepted PEP 492  by Y ury Selivanov , the\nasync  and await  keywords were introduced to support coroutines,\nwhich are now declared with async def . I celebrated that decision,\nbut it did cause breakage: when I wrote the first edition of Fluent\nPython , the asyncio  package had a very important function named\nasync . It was renamed to ensure_future , breaking many of the\nasyncio  examples in the book. The asyncio  API was provisional at\nthe time, so I can’ t blame them. And I really like the new keywords.\nW e’ll cover them in Chapter 19  and Chapter 22 .\nHowever , PEP 492 did not fix the issue of using a plain def  to declare\ngenerators. It can be ar gued that, because those features were made to\nwork with little additional syntax, extra syntax would be merely\n“syntactic sugar .” I happen to like syntactic sugar when it makes\nfeatures that are dif ferent look dif ferent. The lack of syntactic sugar is\nthe main reason why Lisp code is hard to read: every language construct\nin Lisp looks like a function call.\nT erminology matters\nOver the years, Python’ s of ficial documentation has been inconsistent\nabout the words “generator” and “iterator”, using them as near\nsynonyms in some places, but meaning dif ferent things in other places.\nSometime after I wrote the first edition of Fluent Python , Python’ s\nGlossary  was updated with new , clearly distinct definitions for those\nwords and related terms. As I write this in February 2020, this is the\ndefinition of generator  from the Python Glossary:18\ngenerator\nA function which r eturns a generator iterator . It looks like a normal\nfunction except that it contains yield  expr essions for pr oducing a\nseries of values usable in a for  loop or that can be r etrieved one at\na time with the next()  function.\nUsually r efers to a generator function, but may r efer to a generator\niterator in some contexts. In cases wher e the intended meaning isn’ t\nclear , using the full terms avoids ambiguity .\n— Python Glossary\nI like that definition. The next Glossary entry is good as well:\ngenerator iterator\nAn object cr eated by a generator function.\nEach yield temporarily suspends pr ocessing, r emembering the\nlocation execution state (including local variables and pending try-\nstatements). When the generator iterator r esumes, it picks up wher e it\nleft off (in contrast to functions which start fr esh on every\ninvocation).\n— Python Glossary\nGenerator iterator would be a good term to describe the object returned\nby a generator , but the Python runtime has not changed to adopt this\nnew term:\n>>> def gen(): \n...     yield 1 \n... \n>>> gen() \n<generator  object gen at 0x10bb3d120 >\nAs long as Python itself uses the term generator object  in that way , I am\nafraid generator iterator  will not catch on, and the terminology will\nremain inconsistent. I did not adopt generator iterator  in this second\nedition. I am sticking with generator object . But the Glossary has\nencouraged me to use the unqualified word generator  when writing\nabout generator functions.\nAfter generator iterator , the next definition in the Glossary is for\ngenerator expr ession . It says:\ngenerator iterator\nAn expr ession that r eturns an iterator . It looks like a normal\nexpr ession followed by a for  clause defining a loop variable, range,\nand an optional if  clause. The combined expr ession generates\nvalues for an enclosing function:\n>>> sum(i * i for i in range(10))  # sum of squares 0, 1,  \n4, ... 81  \n285\n— Python Glossary\nThat definition uses the word iterator  to describe the object returned by\na generator expression. I don’ t see the point of naming such objects\ndif ferently from those returned by generator functions. Python agrees\nwith me: it also calls that a generator object .\n>>> (i*i for i in range(10)) \n<generator  object <genexpr> at 0x10bb3d190 >\nFinally , the definition for iterator  in the Glossary starts with these\nwords:\niterator\nAn object r epr esenting a str eam of data. Repeated calls to the\niterator ’ s __next__()  method (or passing it to the built-in\nfunction next() ) r eturn successive items in the str eam. When no\nmor e data ar e available a StopIteration  exception is raised\ninstead. […]\n— Python Glossary\nThe entry is longer than that, but that’ s the most important part. I like it.\nThis definition encompasses classic iterators with a user -defined\n__next__  method as well as generator objects returned by generator\nfunctions or generator expressions. The main point is: generator objects\nare iterators that Python builds for you.\nThe Minimalistic Iterator Interface in Python\nIn the “Implementation” section of the Iterator pattern,  the Gang of\nFour  wrote:\nThe minimal interface to Iterator consists of the operations First,\nNext, IsDone, and Curr entItem.\nHowever , that very sentence has a footnote which reads:\nW e can make this interface even smaller by mer ging Next, IsDone,\nand Curr entItem into a single operation that advances to the next\nobject and r eturns it. If the traversal is finished, then this operation\nr eturns a special value (0, for instance) that marks the end of the\niteration.\nThis is close to what we have in Python: the single method __next__\ndoes the job. But instead of using a sentinel, which could be overlooked\nby mistake, the StopIteration  exception signals the end of the\niteration. Simple and correct: that’ s the Python way .\n1  From “Revenge of the Nerds” , a blog post.\n2  Python 2.2 users could use yield  with the directive from __future__ import\ngenerators ; yield  became available by default in Python 2.3.\n3  W e first used reprlib  in “V ector T ake #1: V ector2d Compatible” .\n4  Gamma et. al., Design Patterns: Elements of Reusable Object-Oriented Softwar e , p. 259.\n5  When reviewing this code, Alex Martelli suggested the body of this method could simply be\nreturn iter(self.words) . He is correct, of course: the result of calling __iter__\nwould also be an iterator , as it should be. However , I used a for  loop with yield  here to\nintroduce the syntax of a generator function, which will be covered in detail in the next section.19\n6  Sometimes I add a gen  prefix or suf fix when naming generator functions, but this is not a\ncommon practice. And you can’ t do that if you’re implementing an iterable, of course: the\nnecessary special method must be named __iter__ .\n7  Thanks to David Kwast for suggesting this example.\n8  Prior to Python 3.3, it was an error to provide a value with the return  statement in a\ngenerator function. Now that is legal, but the return  still causes a StopIteration\nexception to be raised. The caller can retrieve the return value from the exception object.\nHowever , this is only relevant when using a generator function as a coroutine, as we’ll see in\n“Returning a V alue from a Coroutine” .\n9  In Python 2, there was a coerce()  built-in function but it’ s gone in Python 3, deemed\nunnecessary because the numeric coercion rules are implicit in the arithmetic operator\nmethods. So the best way I could think of to coerce the initial value to be of the same type as\nthe rest of the series was to perform the addition and use its type to convert the result. I asked\nabout this in the Python-list and got an excellent response from Steven D’Aprano .\n10  The 14-it-generator/  directory in the Fluent Python  code repository  includes doctests and a\nscript, aritpr og_runner .py , which runs the tests against all variations of the aritpr og*.py  scripts.\n11  Here the term “mapping” is unrelated to dictionaries, but has to do with the map  built-in.\n12  The itertools.chain  from the standard library is written in C.\n13  W e saw this method earlier in [Link to Come], Chapter 13 .\n14  The library used to read the complex .mst  binary is actually written in Java, so this\nfunctionality is only available when isis2json.py  is executed with the Jython interpreter ,\nversion 2.5 or newer . For further details, see the README.rst  file in the repository . The\ndependencies are imported inside the generator functions that need them, so the script can run\neven if only one of the external libraries is available.\n15  Slide 33, “Keeping It Straight,” in “A Curious Course on Coroutines and Concurrency” .\n16  According to the Jar gon file , to gr ok  is not merely to learn something, but to absorb it so “it\nbecomes part of you, part of your identity .”\n17  Joe Gibbs Politz, Alejandro Martinez, Matthew Milano, Sumner W arren, Daniel Patterson,\nJunsong Li, Anand Chitipothu, and Shriram Krishnamurthi, “Python: The Full Monty ,”\nSIGPLAN Not. 48, 10 (October 2013), 217-232.\n18  Slide 31, “A Curious Course on Coroutines and Concurrency” .\n19  Gamma et. al., Design Patterns: Elements of Reusable Object-Oriented Softwar e , p. 261.",16638
241-Whats new in this chapter.pdf,241-Whats new in this chapter,"Chapter 18. Context Managers\nand else Blocks\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 18th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nContext managers may end up being almost as important as the\nsubr outine itself. W e’ve only scratched the surface with them. […] Basic\nhas a with  statement, ther e ar e with  statements in lots of languages.\nBut they don’ t do the same thing, they all do something very shallow , they\nsave you fr om r epeated dotted [attribute] lookups, they don’ t do setup\nand tear down. Just because it’ s the same name don’ t think it’ s the same\nthing. The with  statement is a very big deal.\n— Raymond Hettinger , Eloquent Python evangelist\nIn this chapter , we will discuss control flow features that are not so common\nin other languages, and for this reason tend to be overlooked or underused\nin Python. They are:\nThe with  statement and context managers.\nThe else  clause in for , while , and try  statements.1",1473
242-Do This Then That else Blocks Beyond if.pdf,242-Do This Then That else Blocks Beyond if,"Pattern matching with match/case .\nThe with  statement sets up a temporary context and reliably tears it down,\nunder the control of a context manager object. This prevents errors and\nreduces boilerplate code, making APIs at the same time safer and easier to\nuse. Python programmers are finding lots of uses for with  blocks beyond\nautomatic file closing.\nXXX\nThe else  clause is completely unrelated to with . But this is [Link to\nCome]—Control Flow . I couldn’ t find another place for covering else ,\nand I wouldn’ t have a one-page chapter about it, so here it is.\nPattern matching appeared in several previous chapters, but here you’ll see\na more extensive example in “Pattern Matching: a Case Study” .\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThe only updates are in “The contextlib Utilities” , mentioning features of\nthe contextlib  module added since Python 3.5.\nLet’ s review the smaller topic to get to the real substance of this chapter .\nD o  T h i s ,  T h e n  T h a t :  e l s e  B l o c k s  B e y o n d  i f\nThis is no secret, but it is an underappreciated language feature: the else\nclause can be used not only in if  statements but also in for , while , and\ntry  statements.\nThe semantics of for/else , while/else , and try/else  are closely\nrelated, but very dif ferent from if/else . Initially the word else  actually\nhindered my understanding of these features, but eventually I got used to it.\nHere are the rules:\nfor\nThe else  block will run only if and when the for  loop runs to\ncompletion (i.e., not if the for  is aborted with a break ).\nwhile\nThe else  block will run only if and when the while  loop exits\nbecause the condition became falsy  (i.e., not if the while  is aborted\nwith a break ).\ntry\nThe else  block will only run if no exception is raised in the try\nblock. The of ficial docs  also state: “Exceptions in the else  clause are\nnot handled by the preceding except  clauses.”\nIn all cases, the else  clause is also skipped if an exception or a return ,\nbreak , or continue  statement causes control to jump out of the main\nblock of the compound statement.\nN O T E\nI think else  is a very poor choice for the keyword in all cases except if . It implies an\nexcluding alternative, like “Run this loop, otherwise do that,” but the semantics for\nelse  in loops is the opposite: “Run this loop, then do that.” This suggests then  as a\nbetter keyword—which would also make sense in the try  context: “T ry this, then do\nthat.” However , adding a new keyword is a breaking change to the language—not an\neasy decision to make.\nUsing else  with these statements often makes the code easier to read and\nsaves the trouble of setting up control flags or coding extra if  statements.\nThe use of else  in loops generally follows the pattern of this snippet:\nfor item in my_list: \n    if item.flavor == 'banana' : \n        break \nelse: \n    raise ValueError ('No banana flavor found!' )\nIn the case of try/except  blocks, else  may seem redundant at first.\nAfter all, the after_call()  in the following snippet will run only if the\ndangerous_call()  does not raise an exception, correct?\ntry: \n    dangerous_call () \n    after_call () \nexcept OSError: \n    log('OSError...' )\nHowever , doing so puts the after_call()  inside the try  block for no\ngood reason. For clarity and correctness, the body of a try  block should\nonly have the statements that may generate the expected exceptions. This is\nmuch better:\ntry: \n    dangerous_call () \nexcept OSError: \n    log('OSError...' ) \nelse: \n    after_call ()\nNow it’ s clear that the try  block is guarding against possible errors in\ndangerous_call()  and not in after_call() . It’ s also more\nobvious that after_call()  will only execute if no exceptions are raised\nin the try  block.\nIn Python, try/except  is commonly used for control flow , and not just\nfor error handling. There’ s even an acronym/slogan for that documented in\nthe of ficial Python glossary :",4019
243-Context Managers and with Blocks.pdf,243-Context Managers and with Blocks,"EAFP\nEasier to ask for for giveness than permission. This common Python\ncoding style assumes the existence of valid keys or attributes and\ncatches exceptions if the assumption pr oves false. This clean and fast\nstyle is characterized by the pr esence of many try and except\nstatements. The technique contrasts with the LBYL  style common to\nmany other languages such as C.\nThe glossary then defines LBYL:\nLBYL\nLook befor e you leap. This coding style explicitly tests for pr e-\nconditions befor e making calls or lookups. This style contrasts with\nthe EAFP  appr oach and is characterized by the pr esence of many if\nstatements. In a multi-thr eaded envir onment, the LBYL appr oach can\nrisk intr oducing a race condition between “the looking” and “the\nleaping”. For example, the code, if key in mapping: r eturn\nmapping[key] can fail if another thr ead r emoves key fr om mapping\nafter the test, but befor e the lookup. This issue can be solved with\nlocks or by using the EAFP appr oach.\nGiven the EAFP style, it makes even more sense to know and use well\nelse  blocks in try/except  statements.\nNow let’ s address the main topic of this chapter: the powerful with\nstatement.\nC o n t e x t  M a n a g e r s  a n d  w i t h  B l o c k s\nContext manager objects exist to control a with  statement, just like\niterators exist to control a for  statement.\nThe with  statement was designed to simplify the try/finally  pattern,\nwhich guarantees that some operation is performed after a block of code,\neven if the block is aborted because of an exception, a return  or\nsys.exit()  call. The code in the finally  clause usually releases a\ncritical resource or restores some previous state that was temporarily\nchanged.\nThe context manager interface consists of the __enter__  and\n__exit__  methods. At the start of the with , __enter__  is invoked on\nthe context manager object. The role of the finally  clause is played by a\ncall to __exit__  on the context manager object at the end of the with\nblock.\nThe most common example is making sure a file object is closed. See\nExample 18-1  for a detailed demonstration of using with  to close a file.\nExample 18-1. Demonstration of a file object as a context manager\n>>> with open('mirror.py ') as fp:  \n \n...     src = fp.read(60)  \n \n... \n>>> len(src) \n60 \n>>> fp  \n \n<_io.TextIOWrapper name='mirror.py' mode='r' encoding='UTF-8'>  \n>>> fp.closed, fp.encoding   \n \n(True, 'UTF-8')  \n>>> fp.read(60)  \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nValueError : I/O operation on closed file.\nfp  is bound to the opened file because the file’ s __enter__  method\nreturns self .\nRead some data from fp .\nThe fp  variable is still available.\nY ou can read the attributes of the fp  object.\nBut you can’ t perform I/O with fp  because at the end of the with\nblock, the TextIOWrapper.__exit__  method is called and closes2\nthe file.\nThe first callout in Example 18-1  makes a subtle but crucial point: the\ncontext manager object is the result of evaluating the expression after\nwith , but the value bound to the tar get variable (in the as  clause) is the\nresult of calling __enter__  on the context manager object.\nIt just happens that in Example 18-1 , the open()  function returns an\ninstance of TextIOWrapper , and its __enter__  method returns self .\nBut the __enter__  method may also return some other object instead of\nthe context manager .\nWhen control flow exits the with  block in any way , the __exit__\nmethod is invoked on the context manager object, not on whatever is\nreturned by __enter__ .\nThe as  clause of the with  statement is optional. In the case of open ,\nyou’ll always need it to get a reference to the file, but some context\nmanagers return None  because they have no useful object to give back to\nthe user .\nExample 18-2  shows the operation of a perfectly frivolous context manager\ndesigned to highlight the distinction between the context manager and the\nobject returned by its __enter__  method.\nExample 18-2. T est driving the LookingGlass context manager class\n    >>> from mirror import LookingGlass  \n    >>> with LookingGlass () as what:  \n \n    ...      print('Alice, Kitty and Snowdrop ')  \n \n    ...      print(what) \n    ... \n    pordwonS  dna yttiK ,ecilA  \n \n    YKCOWREBBAJ  \n    >>> what  \n \n    'JABBERWOCKY ' \n    >>> print('Back to normal. ')  \n \n    Back to normal.\n\nThe context manager is an instance of LookingGlass ; Python calls\n__enter__  on the context manager and the result is bound to what .\nPrint a str , then the value of the tar get variable what .\nThe output of each print  comes out backward.\nNow the with  block is over . W e can see that the value returned by\n__enter__ , held in what , is the string 'JABBERWOCKY' .\nProgram output is no longer backward.\nExample 18-3  shows the implementation of LookingGlass .\nExample 18-3. mirr or .py: code for the LookingGlass context manager class\nclass LookingGlass : \n \n    def __enter__ (self):  \n \n        import sys \n        self.original_write  = sys.stdout.write  \n \n        sys.stdout.write = self.reverse_write   \n \n        return 'JABBERWOCKY '  \n \n \n    def reverse_write (self, text):  \n \n        self.original_write (text[::-1]) \n \n    def __exit__ (self, exc_type , exc_value , traceback ):  \n \n        import sys  \n \n        sys.stdout.write = self.original_write   \n \n        if exc_type  is ZeroDivisionError :  \n \n            print('Please DO NOT divide by zero! ') \n            return True  \n \n        \nPython invokes __enter__  with no ar guments besides self .\nHold the original sys.stdout.write  method in an instance\nattribute for later use.\n\nMonkey-patch sys.stdout.write , replacing it with our own\nmethod.\nReturn the 'JABBERWOCKY'  string just so we have something to put\nin the tar get variable what .\nOur replacement to sys.stdout.write  reverses the text\nar gument and calls the original implementation.\nPython calls __exit__  with None, None, None  if all went well;\nif an exception is raised, the three ar guments get the exception data, as\ndescribed next.\nIt’ s cheap to import modules again because Python caches them.\nRestore the original method to sys.stdout.write .\nIf the exception is not None  and its type is ZeroDivisionError ,\nprint a message…\n…and return True  to tell the interpreter that the exception was handled.\nIf __exit__  returns None  or anything but True , any exception\nraised in the with  block will be propagated.\nT I P\nWhen real applications take over standard output, they often want to replace\nsys.stdout  with another file-like object for a while, then switch back to the original.\nThe contextlib.redirect_stdout  context manager does exactly that: just pass\nit the file-like object that will stand in for sys.stdout .\nThe interpreter calls the __enter__  method with no ar guments—beyond\nthe implicit self . The three ar guments passed to __exit__  are:\nexc_type\nThe exception class (e.g., ZeroDivisionError ).\nexc_value\nThe exception instance. Sometimes, parameters passed to the exception\nconstructor—such as the error message—can be found in\nexc_value.args .\ntraceback\nA traceback  object.\nFor a detailed look at how a context manager works, see Example 18-4 ,\nwhere LookingGlass  is used outside of a with  block, so we can\nmanually call its __enter__  and __exit__  methods.\nExample 18-4. Exer cising LookingGlass without a with block\n    >>> from mirror import LookingGlass  \n    >>> manager = LookingGlass ()  \n \n    >>> manager \n    <mirror.LookingGlass  object at 0x2a578ac > \n    >>> monster = manager.__enter__ ()  \n \n    >>> monster == 'JABBERWOCKY '  \n \n    eurT \n    >>> monster \n    'YKCOWREBBAJ ' \n    >>> manager \n    >ca875a2x0  ta tcejbo ssalGgnikooL .rorrim< \n    >>> manager.__exit__ (None, None, None)  \n \n    >>> monster \n    'JABBERWOCKY '\nInstantiate and inspect the manager  instance.\nCall the context manager __enter__()  method and store result in\nmonster .\nMonster is the string 'JABBERWOCKY' . The True  identifier appears\nreversed because all output via stdout  goes through the write\nmethod we patched in __enter__ .3",8279
244-The contextlib Utilities.pdf,244-The contextlib Utilities,"Call manager.__exit__  to restore previous stdout.write .\nContext managers are a fairly novel feature and slowly but surely the\nPython community is finding new , creative uses for them. Some examples\nfrom the standard library are:\nManaging transactions in the sqlite3  module; see “12.6.7.3.\nUsing the connection as a context manager” .\nHolding locks, conditions, and semaphores in threading  code;\nsee “17.1.10. Using locks, conditions, and semaphores in the with\nstatement” .\nSetting up environments for arithmetic operations with Decimal\nobjects; see the decimal.localcontext  documentation .\nApplying temporary patches to objects for testing; see the\nunittest.mock.patch  function .\nThe standard library also includes the contextlib  utilities, covered next.\nT h e  c o n t e x t l i b  U t i l i t i e s\nBefore rolling your own context manager classes, take a look at\n""contextlib  — Utilities for with -statement contexts”  in The Python\nStandar d Library . Maybe what you are about to build already exists, or\nthere is a class or some callable that will make your job easier .\nBesides the redirect_stdout  context manager mentioned in\nExample 18-3 , redirect_stderr  was added in Python 3.5—it does the\nsame as the former , but for output directed to stderr .\nThe contextlib  package also includes:\nclosing\nA function to build context managers out of objects that provide a\nclose()  method but don’ t implement the __enter__/__exit__\ninterface.\nsuppress\nA context manager to temporarily ignore exceptions given as\nar guments.\nnullcontext\nA context manager wrapper that does nothing, to simplify conditional\nlogic around objects that may or may not implement a suitable context\nmanager (since Python 3.7).\nThe contextlib  module provides classes and a decorator that are more\nwidely applicable than those above:\n@contextmanager\nA decorator that lets you build a context manager from a simple\ngenerator function, instead of creating a class and implementing the\ninterface. See “Using @contextmanager” .\nAbstractContextManager\nAn ABC that formalizes the context manager interface, and makes it a\nbit easier to create context manager classes by subclassing (since\nPython 3.6).\nContextDecorator\nA base class for defining class-based context managers that can also be\nused as function decorators, running the entire function within a\nmanaged context.\nExitStack",2408
245-Using contextmanager.pdf,245-Using contextmanager,"A context manager that lets you enter a variable number of context\nmanagers. When the with  block ends, ExitStack  calls the stacked\ncontext managers’ __exit__  methods in LIFO order (last entered,\nfirst exited). Use this class when you don’ t know beforehand how many\ncontext managers you need to enter in your with  block; for example,\nwhen opening all files from an arbitrary list of files at the same time.\nW ith Python 3.7, contextlib  added\nAbstractAsyncContextManager , @asynccontextmanager ,\nand AsyncExitStack . They are similar to the equivalent utilities\nwithout the async  part of the name, but designed for use with the new\nasync with  statement, covered in Chapter 22 .\nThe most widely used of these utilities is surely the @contextmanager\ndecorator , so it deserves more attention. That decorator is also intriguing\nbecause it shows a use for the yield  statement unrelated to iteration. This\npaves the way to the concept of a coroutine, the theme of the next chapter .\nU s i n g  @ c o n t e x t m a n a g e r\nThe @contextmanager  decorator reduces the boilerplate of creating a\ncontext manager: instead of writing a whole class with\n__enter__/__exit__  methods, you just implement a generator with a\nsingle yield  that should produce whatever you want the __enter__\nmethod to return.\nIn a generator decorated with @contextmanager , yield  splits the\nbody of the function in two parts: everything before the yield  will be\nexecuted at the beginning of the with  block when the interpreter calls\n__enter__ ; the code after yield  will run when __exit__  is called at\nthe end of the block.\nHere is an example. Example 18-5  replaces the LookingGlass  class\nfrom Example 18-3  with a generator function.\nExample 18-5. mirr or_gen.py: a context manager implemented with a\ngenerator\nimport contextlib  \n \n \n@contextlib.contextmanager   \n \ndef looking_glass (): \n    import sys \n    original_write  = sys.stdout.write  \n \n \n    def reverse_write (text):  \n \n        original_write (text[::-1]) \n \n    sys.stdout.write = reverse_write   \n \n    yield 'JABBERWOCKY '  \n \n    sys.stdout.write = original_write   \nApply the contextmanager  decorator .\nPreserve original sys.stdout.write  method.\nDefine custom reverse_write  function; original_write  will\nbe available in the closure.\nReplace sys.stdout.write  with reverse_write .\nY ield the value that will be bound to the tar get variable in the as  clause\nof the with  statement. This function pauses at this point while the body\nof the with  executes.\nWhen control exits the with  block in any way , execution continues\nafter the yield ; here the original sys.stdout.write  is restored.\nExample 18-6  shows the looking_glass  function in operation.\nExample 18-6. T est driving the looking_glass context manager function\n    >>> from mirror_gen  import looking_glass  \n    >>> with looking_glass () as what:  \n \n    ...      print('Alice, Kitty and Snowdrop ') \n    ...      print(what) \n    ... \n    pordwonS  dna yttiK ,ecilA \n    YKCOWREBBAJ  \n    >>> what \n    'JABBERWOCKY '\nThe only dif ference from Example 18-2  is the name of the context\nmanager: looking_glass  instead of LookingGlass .\nEssentially the contextlib.contextmanager  decorator wraps the\nfunction in a class that implements the __enter__  and __exit__\nmethods.\nThe __enter__  method of that class:\n1 . Invokes the generator function and holds on to the generator object\n—let’ s call it gen .\n2 . Calls next(gen)  to make it run to the yield  keyword.\n3 . Returns the value yielded by next(gen) , so it can be bound to a\ntar get variable in the with/as form.\nWhen the with  block terminates, the __exit__  method:\n1 . Checks an exception was passed as exc_type ; if so,\ngen.throw(exception)  is invoked, causing the exception to\nbe raised in the yield  line inside the generator function body .\n2 . Otherwise, next(gen)  is called, resuming the execution of the\ngenerator function body after the yield .\nExample 18-5  has a serious flaw: if an exception is raised in the body of the\nwith  block, the Python interpreter will catch it and raise it again in the\nyield  expression inside looking_glass . But there is no error handling\nthere, so the looking_glass  function will abort without ever restoring\nthe original sys.stdout.write  method, leaving the system in an\ninvalid state.4\nExample 18-7  adds special handling of the ZeroDivisionError\nexception, making it functionally equivalent to the class-based Example 18-\n3 .\nExample 18-7. mirr or_gen_exc.py: generator -based context manager\nimplementing exception handling—same external behavior as Example 18-\n3\nimport contextlib  \n \n \n@contextlib.contextmanager  \ndef looking_glass (): \n    import sys \n    original_write  = sys.stdout.write \n \n    def reverse_write (text): \n        original_write (text[::-1]) \n \n    sys.stdout.write = reverse_write  \n    msg = ''  \n \n    try: \n        yield 'JABBERWOCKY ' \n    except ZeroDivisionError :  \n \n        msg = 'Please DO NOT divide by zero! ' \n    finally: \n        sys.stdout.write = original_write   \n \n        if msg: \n            print(msg)  \nCreate a variable for a possible error message; this is the first change in\nrelation to Example 18-5 .\nHandle ZeroDivisionError  by setting an error message.\nUndo monkey-patching of sys.stdout.write .\nDisplay error message, if it was set.\nRecall that the __exit__  method tells the interpreter that it has handled\nthe exception by returning True ; in that case, the interpreter suppresses the\nexception. On the other hand, if __exit__  does not explicitly return a\nvalue, the interpreter gets the usual None , and propagates the exception.\nW ith @contextmanager , the default behavior is inverted: the\n__exit__  method provided by the decorator assumes any exception sent\ninto the generator is handled and should be suppressed.  Y ou must\nexplicitly re-raise an exception in the decorated function if you don’ t want\n@contextmanager  to suppress it.\nT I P\nHaving a try/finally  (or a with  block) around the yield  is an unavoidable price\nof using @contextmanager , because you never know what the users of your context\nmanager are going to do inside their with  block.\nAn interesting real-life example of @contextmanager  outside of the\nstandard library is Martijn Pieters’ in-place file rewriting context manager .\nExample 18-8  shows how it’ s used.\nExample 18-8. A context manager for r ewriting files in place\nimport csv \n \nwith inplace(csvfilename , 'r', newline='') as (infh, outfh): \n    reader = csv.reader(infh) \n    writer = csv.writer(outfh) \n \n    for row in reader: \n        row += ['new', 'columns' ] \n        writer.writerow (row)\nThe inplace  function is a context manager that gives you two handles—\ninfh  and outfh  in the example—to the same file, allowing your code to\nread and write to it at the same time. It’ s easier to use than the standard\nlibrary’ s fileinput.input  function  (which also provides a context\nmanager , by the way).\nIf you want to study Martijn’ s inplace  source code (listed in the post ),\nfind the yield  keyword: everything before it deals with setting up the\ncontext, which entails creating a backup file, then opening and yielding5\n6\n7",7327
246-Pattern Matching a Case Study.pdf,246-Pattern Matching a Case Study,,0
247-An Expression Evaluator.pdf,247-An Expression Evaluator,"references to the readable and writable file handles that will be returned by\nthe __enter__  call. The __exit__  processing after the yield  closes\nthe file handles and restores the file from the backup if something went\nwrong.\nNote that the use of yield  in a generator used with the\n@contextmanager  decorator has nothing to do with iteration. In the\nexamples shown in this section, the generator function is operating more\nlike a coroutine: a procedure that runs up to a point, then suspends to let the\nclient code run until the client wants the coroutine to proceed with its job.\nChapter 19  is all about coroutines.\nP a t t e r n  M a t c h i n g :  a  C a s e  S t u d y\nXXX missing introduction\nBefore looking at the Python code, let’ s learn the bare minimum of Scheme\nso you can make sense of this case study—in case you haven’ t studied\nScheme or Lisp before.\nScheme Syntax\nEverything in Scheme is an expression—there is no distinction between\nexpressions and statements, like we have in Python.\nScheme has no infix operators. Expressions with arithmetic and logic\noperators all use prefix notation like (+ x 13) . The same syntax is used\nfor function calls—e.g. (gcd x 13) —and special forms—e.g. (define\nx 13) , which we’d write as x = 13  in Python.\nHere is a simple example in Scheme:\nExample 18-9. Gr eatest common divisor in Scheme. The last r esult of this\ncode is 9, the GCD of 18 and 45.\n(define (mod m n) \n    (- m (* n (// m n)))) \n \n(define (gcd m n) 8\n    (if (= n 0) \n        m \n        (gcd n (mod m n)))) \n \n(gcd 18 45)\nExample 18-9  shows two function definitions— mod  and gcd —and a call\nto gcd . Here is the same code in Python (quicker than an English\nexplanation):\nExample 18-10. Same as Example 18-9 , written in Python.\ndef mod(m, n): \n    return m - (m // n * n) \n \ndef gcd(m, n): \n    if n == 0: \n        return m \n    else: \n        return gcd(m, mod(m, n)) \n \ngcd(18, 45)  # returns 9\nAt its core, Scheme has no iterative control flow forms like while  or for .\nIteration is always implemented with recursion, as you saw in Example 18-\n9 . Scheme implementations are required to implement tail call optimization\n(TCO) to make iteration through recursion ef ficient and practical. Norvig’ s\nlispy .py  interpreter has TCO, but his simpler lis.py  does not.\nThe Parser\nThe first part of Norvig’ s code is a parser that reads a string of Scheme\nsource code, splits it into syntactic tokens, and returns a Python object\nrepresenting the code.\nHere are some examples from a doctest:\nExample 18-1 1. parse  takes a string and r eturns numbers, symbols,\nand/or lists.\n>>> parse('1.5')  \n \n1.5 \n>>> parse('set!')  \n \n'set!' \n>>> parse('(gcd 18 44) ')  \n \n['gcd', 18, 44] \n>>> parse('(- m (* n (// m n))) ')  \n \n['-', 'm', ['*', 'n', ['//', 'm', 'n']]]\nA token that looks like a number is parsed as a number— float  or\nint .\nAnything else that doesn’ t start with '('  is parsed as a symbol —a str\nto be used as an identifier .\nExpressions inside '('  and ')'  are parsed as lists of numbers or\nsymbols or…\n…nested lists that may contain numbers, symbols, and more nested lists.\nThe simplest tokens—numbers and symbols—are called atoms . Using\nPython terminology , the output of parse  is an AST (Abstract Syntax\nT ree): the nested lists form a tree-like structure, where the outermost list is\nthe trunk, the inner lists are the branches, and the atoms are the leaves.\nAn Expression Evaluator\nNow we are ready to see the beauty of pattern matching applied to\ninterpreting Scheme expressions. The evaluate  function in Example 18-\n12  is the most important part of the interpreter .\nExample 18-12. evaluate  takes an expr ession fr om parse  and\ncomputes its value.\ndef evaluate (exp: Expression , env: Environment ) -> Any: \n    ""Evaluate an expression in an environment.""  \n    match exp: \n        case int(x) | float(x): \n            return x \n        case Symbol(var): \n            return env[var] \n        case []: \n            return [] \n        case ['quote', exp]: \n            return exp \n        case ['if', test, consequence , alternative ]: \n            if evaluate (test, env): \n                return evaluate (consequence , env) \n            else: \n                return evaluate (alternative , env) \n        case ['define' , Symbol(var), value_exp ]: \n            env[var] = evaluate (value_exp , env) \n        case ['define' , [Symbol(name), *parms], *body]: \n            env[name] = Procedure (parms, body, env) \n        case ['lambda' , [*parms], *body]: \n            return Procedure (parms, body, env) \n        case [op, *args]: \n            proc = evaluate (op, env) \n            values = [evaluate (arg, env) for arg in args] \n            return proc(*values) \n        case _: \n            raise SyntaxError (repr(exp))\nThe two ar guments of evaluate  are:\nexp\nnumbers, symbols or lists returned by parse ;\nenv\nan envirnoment—a mapping of names to values.\nWhen the interpreter makes the initial call to evaluate , env  gets a dict\nwith dozens of names mapped to Python functions. This is a small sample\nof items in the initial environment:\n{ \n    '+': op.add, \n    '-': op.sub, \n    'abs': abs, \n    'append' : lambda *args: list(itertools .chain(*args)), \n    'length' : len, \n    'number?' : lambda x: isinstance (x, (int, float)), \n}\nThe body of evaluate  is a single match  statement with an expression\nexp  as the subject. The 10 case  patterns express the syntax and semantics\nof Scheme with amazing clarity .\nLet’ s study each case in turn. On top of each case , I added a sample of\nScheme code that would produce a subject exp  matching that pattern, and a\nPython object that could be the value of that expression.\n        # 1.5 \n        case int(x) | float(x):  \n \n            return x\nIf subject is an int  or float , just return it.\n        # count \n        case Symbol(var):  \n \n            return env[var]\nIf subject is a Symbol  (a str  used as an identifier), get its value from\nenv  and return it.\nNow , the sequence patterns:\n        # () \n        case []:  \n \n            return []\nIf subject is an empty list, return it.\n        # (quote (1.1 is not 1))  \n        case ['quote', exp]:  \n \n            return exp\nIf subject is a list starting with 'quote' , followed by one exp , then\nreturn exp  without evaluating it. Given the Scheme code in the\ncomment, the Python object returned would be [1.1, 'is',\n'not', 1] .\n        # (if (> n 0) n (- 0 n))  \n        case ['if', test, consequence , alternative ]:  \n \n            if evaluate (test, env): \n                return evaluate (consequence , env) \n            else: \n                return evaluate (alternative , env)\nIf subject is a list starting with 'if'  followed by three expressions,\nthen evaluate test ; if true, evaluate consequence  and return it;\notherwise, evaluete alternative  and return it.\n        # (define half (/ 1 2))  \n        case ['define', Symbol(var), value_exp ]:  \n \n            env[var] = evaluate (value_exp , env)\nIf subject is a list starting with 'define' , followed by a symbol var\nand an expression, then evaluate the expression and add its value to\nenv , using the var  as key .\nThe next case also matches a sequence starting with define , but with a\ndif ferent structure.\n        # (define (double x) (* x 2))  \n        case ['define', [Symbol(name), *parms], body]:  \n \n            env[name] = Procedure (parms, body, env)\nIf subject is a list starting with 'define'  and two other items, the first\nbeing a list starting with a symbol name , followed by 0 or more\nparameter names, the second being an expression body , then create a\nnew Procedure  with those parameters, body , and the current\nenvironment, and add it to the env  using name  as the key .",7907
248-OR-patterns.pdf,248-OR-patterns,"The previous case is a named function definition. The next is an anonymous\nfunction definition.\n        # (lambda (a b) (* (/ a b) 100))  \n        case ['lambda', [*parms], body]:  \n \n            return Procedure (parms, body, env)\nIf subject is a list starting with 'lambda'  and two other items, the first\nbeing a list of parameter names, the second being an expression body ,\nthen create a new Procedure  with those parameters, body , and the\ncurrent environment, and return it.\nNow we get to a function call.\n        # (gcd 210 84)  \n        case [op, *args]:  \n \n            proc = evaluate (op, env) \n            values = [evaluate (arg, env) for arg in args] \n            return proc(*values)\nIf subject is a list with one or more items, then evaluate the first to\nobtain a function proc , evaluate each of the remaining items to build a\nlist of ar gument values, then call proc  with the values as separate\nar guments.\n        case _:  \n \n            raise SyntaxError (repr(exp))\nIf subject did not match any previous pattern, it matches the wildcard _ .\nRaise SyntaxError .\nT o wrapt up the coverage of pattern matching in this chapter , let’ s talk about\nOR-patterns.\nOR-patterns\nN O T E\nAn OR-pattern can be built from any other patterns, not only class patterns.\nIn Example 2-1 1  we saw part of Peter Norvig’ s lis.py  evaluate  function\nrefactored to use match/case . Here are the first case  clauses of that\nfunction, which I previously ommitted:\nExample 18-13. Pattern matching with match/case —r equir es Python ≥\n3.10.\ndef evaluate (exp, env): \n    ""Evaluate an expression in an environment. "" \n    match exp: \n        case int(x) | float(x):  \n \n            return x \n        case Symbol(var):  \n \n            return env[var] \n        case ...:  # sequence patterns omitted  \n            ... \n        case _: \n            raise SyntaxError (repr(exp))\nMatch if subject is an instance of int  or float .\nMatch is subject is an instance of Symbol —which is an alias for str\nin lis.py .\nA series of patterns separated by |  is an OR-pattern : it succeeds if any of\nthe subpatterns succeed. All subpatterns must use the same variables. This\nrestriction is necessary to ensure that the case  body can rely on all the\nvariables if there is a match.\nW A R N I N G\nIn the context of a case  clause, the |  operator has a special meaning. It does not trigger\nthe __or__  special method which handles expressions like a | b  in other contexts,\nwhere it is overloaded to perform operations such as set union or integer bitwise-or .",2599
249-Further Reading.pdf,249-Further Reading,"Example 18-13  illustrates the simplest form of class pattern, exemplified by\nint(x) , which matches if isinstance(x, int)  returns True .\nXXX\nC h a p t e r  S u m m a r y\nThis chapter started easily enough with discussion of else  blocks in for ,\nwhile , and try  statements. Once you get used to the peculiar meaning of\nthe else  clause in these statements, I believe else  can clarify your\nintentions.\nW e then covered context managers and the meaning of the with  statement,\nquickly moving beyond its common use to automatically close opened files.\nW e implemented a custom context manager: the LookingGlass  class\nwith the __enter__/__exit__  methods, and saw how to handle\nexceptions in the __exit__  method. A key point that Raymond Hettinger\nmade in his PyCon US 2013 keynote is that with  is not just for resource\nmanagement, but it’ s a tool for factoring out common setup and teardown\ncode, or any pair of operations that need to be done before and after another\nprocedure ( slide 21, What Makes Python A wesome? ).\nFinally , we reviewed functions in the contextlib  standard library\nmodule. One of them, the @contextmanager  decorator , makes it\npossible to implement a context manager using a simple generator with one\nyield —a leaner solution than coding a class with at least two methods.\nW e reimplemented the LookingGlass  as a looking_glass  generator\nfunction, and discussed how to do exception handling when using\n@contextmanager .\nThe @contextmanager  decorator is an elegant and practical tool that\nbrings together three distinctive Python features: a function decorator , a\ngenerator , and the with  statement.\nF u r t h e r  R e a d i n g\nChapter 8, “Compound Statements,”  in The Python Language Refer ence\nsays pretty much everything there is to say about else  clauses in if , for ,\nwhile , and try  statements. Regarding Pythonic usage of try/except ,\nwith or without else , Raymond Hettinger has a brilliant answer to the\nquestion “Is it a good practice to use try-except-else in Python?”  in\nStackOverflow . Alex Martelli’ s Python in a Nutshell, 2E  (O’Reilly), has a\nchapter about exceptions with an excellent discussion of the EAFP style,\ncrediting computing pioneer Grace Hopper for coining the phrase “It’ s\neasier to ask for giveness than permission.”\nThe Python Standar d Library , Chapter 4, “Built-in T ypes,” has a section\ndevoted to Context Manager T ypes . The __enter__/__exit__  special\nmethods are also documented in The Python Language Refer ence  in “3.3.8.\nW ith Statement Context Managers” . Context managers were introduced in\nPEP 343 — The “with” Statement . This PEP is not easy reading because it\nspends a lot of time covering corner cases and ar guing against alternative\nproposals. That’ s the nature of PEPs.\nRaymond Hettinger highlighted the with  statement as a “winning language\nfeature” in his PyCon US 2013 keynote . He also showed some interesting\napplications of context managers in his talk “T ransforming Code into\nBeautiful, Idiomatic Python”  at the same conference.\nJef f Preshing’ blog post “The Python with  Statement by Example”  is\ninteresting for the examples using context managers with the pycairo\ngraphics library .\nBeazley and Jones devised context managers for very dif ferent purposes in\ntheir Python Cookbook, 3E  (O’Reilly). “Recipe 8.3. Making Objects\nSupport the Context-Management Protocol” implements a\nLazyConnection  class whose instances are context managers that open\nand close network connections automatically in with  blocks. “Recipe 9.22.\nDefining Context Managers the Easy W ay” introduces a context manager\nfor timing code, and another for making transactional changes to a list\nobject: within the with  block, a working copy of the list  instance is\nmade, and all changes are applied to that working copy . Only when the\nwith  block completes without an exception, the working copy replaces the\noriginal list. Simple and ingenious.\nS O A P B O X\nFactoring Out the Br ead\nIn his PyCon US 2013 keynote, “What Makes Python A wesome,”\nRaymond Hettinger says when he first saw the with  statement\nproposal he thought it was “a little bit arcane.” Initially , I had a similar\nreaction. PEPs are often hard to read, and PEP 343 is typical in that\nregard.\nThen—Hettinger told us—he had an insight: subroutines are the most\nimportant invention in the history of computer languages. If you have\nsequences of operations like A;B;C and P;B;Q, you can factor out B in\na subroutine. It’ s like factoring out the filling in a sandwich: using tuna\nwith dif ferent breads. But what if you want to factor out the bread, to\nmake sandwiches with wheat bread, using a dif ferent filling each time?\nThat’ s what the with  statement of fers. It’ s the complement of the\nsubroutine. Hettinger went on to say:\nThe with  statement is a very big deal. I encourage you to go out and\ntake this tip of the iceber g and drill deeper . Y ou can pr obably do\npr ofound things with the with  statement. The best uses of it have not\nbeen discover ed yet. I expect that if you make good use of it, it will be\ncopied into other languages and all futur e languages will have it. Y ou\ncan be part of discovering something almost as pr ofound as the\ninvention of the subr outine itself.\nHettinger admits he is overselling the with  statement. Nevertheless, it\nis a very useful feature. When he used the sandwich analogy to explain\nhow with  is the complement to the subroutine, many possibilities\nopened up in my mind.\nIf you need to convince anyone that Python is awesome, you should\nwatch Hettinger ’ s keynote. The bit about context managers is from\n23:00 to 26:15. But the entire keynote is excellent.\n1  PyCon US 2013 keynote: “What Makes Python A wesome” ; the part about with  starts at\n23:00 and ends at 26:15.\n2  with  blocks don’ t define a new scope, as functions and modules do.\n3  The three ar guments received by self  are exactly what you get if you call\nsys.exc_info()  in the finally  block of a try/finally  statement. This makes\nsense, considering that the with  statement is meant to replace most uses of try/finally ,\nand calling sys.exc_info()  was often necessary to determine what clean-up action would\nbe required.\n4  The actual class is named _GeneratorContextManager . If you want to see exactly how\nit works, read its source code  in Lib/contextlib.py  in the Python 3.4 distribution.\n5  The exception is sent into the generator using the throw  method, covered in “Coroutine\nT ermination and Exception Handling” .\n6  This convention was adopted because when context managers were created, generators could\nnot return  values, only yield . They now can, as explained in “Returning a V alue from a\nCoroutine” . As you’ll see, returning a value from a generator does involve an exception.\n7  This tip is quoted literally from a comment by Leonardo Rochael, one of the tech reviewers\nfor this book. Nicely said, Leo!\n8  People complain about the overuse of parenthesis, but the main readability problem of Lisp\nand its dialects is using the same (foo ...)  syntax for function calls and special forms like\n(define ...) , (if ...) , and macros that don’ t behave at all like function calls.",7280
250-19. Classic Coroutines.pdf,250-19. Classic Coroutines,"Chapter 19. Classic Coroutines\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 19th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nTher e ar e many implementations of cor outines; even in Python ther e ar e\nseveral. […] Starting in Python 3.5, cor outines ar e a native featur e of the\nlanguage itself; however , understanding cor outines as they wer e first\nimplemented in Python 3.4, using pr e-existing language facilities, is the\nfoundation to tackle Python 3.5’ s native cor outines.\n— A. Jesse Jiryu Davis and Guido van Rossum, A W eb\nCrawler W ith asyncio Coroutines\nW e find two main senses for the verb “to yield” in dictionaries: to produce\nor to give way . Both senses apply in Python when we use the yield\nkeyword in a generator . A line such as yield item  produces a value that\nis received by the caller of next(…) , and it also gives way , suspending the\nexecution of the generator so that the caller may proceed until it’ s ready to\nconsume another value by invoking next()  again. The caller pulls values\nfrom the generator .\nA Python coroutine is essentially a generator driven by calls to its\n.send(…)  method. In a coroutine, the essential meaning of “to yield” is to1\ngive way—to hand control to some other part of the program, and wait until\nnotified to resume. The caller invokes my_coroutine.send(datum)\nto push data into the coroutine. The coroutine then resumes and gets datum\nas the value of the yield  expression where it was suspended. In normal\nusage, a caller repeatedly pushes data into the coroutine in that way . In\ncontrast with generators, coroutines are usually data consumers, not\nproducers.\nRegardless of the flow of data, yield  is a control flow device that can be\nused to implement cooperative multitasking: each coroutine yields control\nto a central scheduler so that other coroutines can be activated.\nSince version 3.5, Python has three kinds of coroutines:\nclassic cor outines\nA generator function that consumes data sent to it via\nmy_coro.send(data)  calls, and reads that data by using yield  in\nan expression. Classic coroutines can delegate to other classic\ncoroutines using yield from .\ngenerator -based cor outines\nA generator function decorated with @types.coroutine , which\nmakes it compatible with the new await  keyword, introduced in\nPython 3.5.\nnative cor outines\nA coroutine defined with async def . Y ou can delegate from a native\ncoroutine to another native coroutine or to a generator -based coroutine\nusing the await  keyword, similar to how classic coroutines use\nyield from .\nNative coroutines and generator -based coroutines are intended specifically\nfor asynchronous I/O programming. As such, we’ll get back to them in\nChapter 22 —Basic Asyncio.",3265
251-Basic Behavior of a Generator Used as a Coroutine.pdf,251-Basic Behavior of a Generator Used as a Coroutine,"This chapter is about classic cor outines . Although native coroutines evolved\nfrom classic coroutines, they don’ t replace them completely . Classic\ncoroutines have some useful behaviors that native coroutines can’ t emulate\n—and vice-versa, native coroutines have features that are missing in classic\ncoroutines.\nClassic coroutines are the product of a series of enhancements to the\nsimpler generator functions we’ve seen so far in the book. Following the\nevolution of coroutines in Python helps understand their features in stages\nof increasing functionality and complexity .\nAfter a brief overview of how generators were enhanced to act as\ncoroutines, we jump to the core of the chapter . Then we’ll see:\nThe behavior and states of a generator operating as a coroutine.\nPriming a coroutine automatically with a decorator .\nHow the caller can control a coroutine through the .close()  and\n.throw(…)  methods of the generator object.\nHow coroutines can return values upon termination.\nUsage and semantics of the new yield from  syntax.\nA use case: coroutines for managing concurrent activities in a\nsimulation.\nN O T E\nIn this chapter I often use the word “coroutine” to refer to classic coroutines—a.k.a.\ngenerator -based coroutines— except when I am contrasting them with native coroutines.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nSince 2012 when yield from  was implemented in Python 3.3, classic\ncoroutines did not under go major changes. Native coroutines—created with\nasync def —are similar , but not a full replacement of classic coroutines.\nThey are covered in Chapter 22 .\nTherefore, this chapter has no significant changes except for the occasional\ncomparisons of classic versus native coroutines, as well as yield from\nversus await .\nH o w  C o r o u t i n e s  E v o l v e d  f r o m  G e n e r a t o r s\nA classic coroutine is syntactically like a generator: just a function with the\nyield  keyword in its body . However , in a coroutine, yield  usually\nappears on the right-hand side of an expression (e.g., datum = yield ),\nand it may or may not produce a value—if there is no expression after the\nyield  keyword, the generator yields None . The coroutine may receive\ndata from the caller , which uses coro.send(datum)  instead of\nnext(coro)  to drive the coroutine. Usually , the caller pushes values into\nthe coroutine. It is even possible that no data goes in or out through the\nyield  keyword. When you start thinking of yield  primarily in terms of\ncontrol flow , you have the mindset to understand why coroutines are useful\nfor concurrent programming.\nThe infrastructure for coroutines appeared in PEP 342 — Coroutines via\nEnhanced Generators , implemented in Python 2.5 (2006): since then, the\nyield  keyword can be used in an expression, and the .send(value)\nmethod was added to the generator API. This allows a generator to be used\nas a coroutine: a procedure that collaborates with the caller , yielding and\nreceiving values from the caller .\nIn addition to .send(…) , PEP 342 also added .throw(…)  and\n.close()  methods that respectively allow the caller to throw an\nexception to be handled inside the generator , and to terminate it. These\nfeatures are covered in the next section and in “Coroutine T ermination and\nException Handling” .\nThe latest evolutionary step for classic coroutines came with PEP 380 -\nSyntax for Delegating to a Subgenerator , implemented in Python 3.3\n(2012). PEP 380 made two syntax changes to generator functions, to make\nthem more useful as coroutines:\nA generator can now return  a value; previously , providing a\nvalue to the return  statement inside a generator raised a\nSyntaxError .\nThe yield from  syntax enables complex generators to be\nrefactored into smaller , nested generators while avoiding a lot of\nboilerplate code previously required for a generator to delegate to\nsubgenerators.\nThese changes will be addressed in “Returning a V alue from a Coroutine”\nand “Using yield from” .\nAfter PEP 380 there have been no major changes to classic coroutines. PEP\n492 introduced native coroutines, but that’ s a story for Chapter 22 .\nLet’ s follow the established tradition of Fluent Python  and start with some\nvery basic facts and examples, then move into increasingly mind-bending\nfeatures.\nB a s i c  B e h a v i o r  o f  a  G e n e r a t o r  U s e d  a s  a\nC o r o u t i n e\nExample 19-1  illustrates the behavior of a coroutine.\nExample 19-1. Simplest possible demonstration of cor outine in action\n>>> def simple_coroutine ():  \n \n...     print('-> coroutine started ') \n...     x = yield  \n \n...     print('-> coroutine received: ', x) \n... \n>>> my_coro = simple_coroutine () \n>>> my_coro  \n \n<generator object simple_coroutine at 0x100c2be10>  \n>>> next(my_coro)  \n \n-> coroutine started  \n>>> my_coro.send(42)  \n \n-> coroutine received: 42  \nTraceback (most recent call last):  \n  \n  ... \nStopIteration\nA coroutine is defined as a generator function: with yield  in its body .\nyield  is used in an expression; when the coroutine is designed just to\nreceive data from the client it yields None —this is implicit because\nthere is no expression to the right of the yield  keyword.\nAs usual with generators, you call the function to get a generator object\nback.\nThe first call is next(…)  because the generator hasn’ t started so it’ s\nnot waiting in a yield  and we can’ t send it any data initially .\nThis call makes the yield  in the coroutine body evaluate to 42 ; now\nthe coroutine resumes and runs until the next yield  or termination.\nIn this case, control flows of f the end of the coroutine body , which\nprompts the generator machinery to raise StopIteration , as usual.\nA coroutine can be in one of four states. Y ou can determine the current state\nusing the inspect.getgeneratorstate(…)  function, which returns\none of these strings:\n'GEN_CREATED'\nW aiting to start execution.\n'GEN_RUNNING'\nCurrently being executed by the interpreter .\n'GEN_SUSPENDED'\nCurrently suspended at a yield  expression.2\n'GEN_CLOSED'\nExecution has completed.\nBecause the ar gument to the send  method will become the value of the\npending yield  expression, it follows that you can only make a call like\nmy_coro.send(42)  if the coroutine is currently suspended. But that’ s\nnot the case if the coroutine has never been activated—when its state is\n'GEN_CREATED' . That’ s why the first activation of a coroutine is always\ndone with next(my_coro) —you can also call\nmy_coro.send(None) , and the ef fect is the same.\nIf you create a coroutine object and immediately try to send it a value that is\nnot None , this is what happens:\n>>> my_coro = simple_coroutine () \n>>> my_coro.send(1729) \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nTypeError : can't send non-None value to a just-started generator\nNote the error message: it’ s quite clear .\nThe initial call next(my_coro)  is often described as “priming” the\ncoroutine (i.e., advancing it to the first yield  to make it ready for use as a\nlive coroutine).\nT o get a better feel for the behavior of a coroutine, an example that yields\nmore than once is useful. See Example 19-2 .\nExample 19-2. A cor outine that yields twice\n>>> def simple_coro2 (a): \n...     print('-> Started: a = ', a) \n...     b = yield a \n...     print('-> Received: b = ', b) \n...     c = yield a + b \n...     print('-> Received: c = ', c) \n... \n>>> my_coro2  = simple_coro2 (14) \n>>> from inspect import getgeneratorstate  \n>>> getgeneratorstate (my_coro2 )  \n \n'GEN_CREATED'  \n>>> next(my_coro2 )  \n \n-> Started: a = 14  \n14 \n>>> getgeneratorstate (my_coro2 )  \n \n'GEN_SUSPENDED'  \n>>> my_coro2 .send(28)  \n \n-> Received: b = 28  \n42 \n>>> my_coro2 .send(99)  \n \n-> Received: c = 99  \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nStopIteration  \n>>> getgeneratorstate (my_coro2 )  \n \n'GEN_CLOSED'\ninspect.getgeneratorstate  reports GEN_CREATED  (i.e., the\ncoroutine has not started).\nAdvance coroutine to first yield , printing -> Started: a = 14\nmessage then yielding value of a  and suspending to wait for value to be\nassigned to b .\ngetgeneratorstate  reports GEN_SUSPENDED  (i.e., the coroutine\nis paused at a yield  expression).\nSend number 28  to suspended coroutine; the yield  expression\nevaluates to 28  and that number is bound to b . The -> Received:\nb = 28  message is displayed, the value of a + b  is yielded ( 42 ), and\nthe coroutine is suspended waiting for the value to be assigned to c .\nSend number 99  to suspended coroutine; the yield  expression\nevaluates to 99  the number is bound to c . The -> Received: c =\n99  message is displayed, then the coroutine terminates, causing the\ngenerator object to raise StopIteration .\n\ngetgeneratorstate  reports GEN_CLOSED  (i.e., the coroutine\nexecution has completed).\nIt’ s crucial to understand that the execution of the coroutine is suspended\nexactly at the yield  keyword. As mentioned before, in an assignment\nstatement, the code to the right of the =  is evaluated before the actual\nassignment happens. This means that in a line like b = yield a , the\nvalue of b  will only be set when the coroutine is activated later by the client\ncode. It takes some ef fort to get used to this fact, but understanding it is\nessential to make sense of the use of yield  in asynchronous programming,\nas we’ll see later .\nExecution of the simple_coro2  coroutine can be split in three phases, as\nshown in Figure 19-1 :\n1 . next(my_coro2)  prints first message and runs to yield a ,\nyielding number 14 .\n2 . my_coro2.send(28)  assigns 28  to b , prints second message,\nand runs to yield a + b , yielding number 42 .\n3 . my_coro2.send(99)  assigns 99  to c , prints third message,\nand the coroutine terminates.",9921
252-Example Coroutine to Compute a Running Average.pdf,252-Example Coroutine to Compute a Running Average,"Figur e 19-1. Thr ee phases in the execution of the simple_cor o2 cor outine (note that each phase ends\nin a yield expr ession, and the next phase starts in the very same line, when the value of the yield\nexpr ession is assigned to a variable)\nNow let’ s consider a slightly more involved coroutine example.\nE x a m p l e :  C o r o u t i n e  t o  C o m p u t e  a  R u n n i n g\nA v e r a g e\nWhile discussing closures in Chapter 9 , we studied objects to compute a\nrunning average: Example 9-7  shows a plain class and Example 9-13\npresents a higher -order function producing a closure to keep the total  and\ncount  variables across invocations. Example 19-3  shows how to do the\nsame with a coroutine.\nExample 19-3. cor oaverager0.py: code for a running average cor outine\ndef averager (): \n    total = 0.0 \n    count = 0 \n    average = None \n    while True:  \n \n        term = yield average  \n \n        total += term \n        count += 1 \n        average = total/count\nThis infinite loop means this coroutine will keep on accepting values\nand producing results as long as the caller sends them. This coroutine\nwill only terminate when the caller calls .close()  on it, or when it’ s\ngarbage collected because there are no more references to it.\nThe yield  statement here suspends the coroutine, produces a result to\nthe caller , and—later—gets a value sent by the caller to the coroutine,\nwhich resumes its infinite loop.\nThe advantage of using a coroutine is that total  and count  can be\nsimple local variables: no instance attributes or closures are needed to keep\nthe context between calls. Example 19-4  are doctests to show the\naverager  coroutine in operation.\nExample 19-4. cor oaverager0.py: doctest for the running average cor outine\nin Example 19-3\n    >>> coro_avg  = averager ()  \n \n    >>> next(coro_avg )  \n \n    >>> coro_avg .send(10)  \n \n    10.0 \n    >>> coro_avg .send(30) \n    20.0 \n    >>> coro_avg .send(5) \n    15.03",1989
253-Decorators for Coroutine Priming.pdf,253-Decorators for Coroutine Priming,"Create the coroutine object.\nPrime it by calling next .\nNow we are in business: each call to .send(…)  yields the current\naverage.\nIn the doctest ( Example 19-4 ), the call next(coro_avg)  makes the\ncoroutine advance to the yield , yielding the initial value for average ,\nwhich is None , so it does not appear on the console. At this point, the\ncoroutine is suspended at the yield , waiting for a value to be sent. The\nline coro_avg.send(10)  provides that value, causing the coroutine to\nactivate, assigning it to term , updating the total , count , and average\nvariables, and then starting another iteration in the while  loop, which\nyields the average  and waits for another term.\nThe attentive reader may be anxious to know how the execution of an\naverager  instance (e.g., coro_avg ) may be terminated, because its\nbody is an infinite loop. W e’ll cover that in “Coroutine T ermination and\nException Handling” .\nBut before discussing coroutine termination, let’ s talk about getting them\nstarted. Priming a coroutine before use is a necessary but easy-to-for get\nchore. T o avoid it, a special decorator can be applied to the coroutine. One\nsuch decorator is presented next.\nD e c o r a t o r s  f o r  C o r o u t i n e  P r i m i n g\nY ou can’ t do much with a coroutine without priming it: we must always\nremember to call next(my_coro)  before my_coro.send(x) . T o\nmake coroutine usage more convenient, a priming decorator is sometimes\nused. The coroutine  decorator in Example 19-5  is an example.\nExample 19-5. cor outil.py: decorator for priming cor outine4\nfrom functools  import wraps \n \ndef coroutine (func): \n    """"""Decorator: primes `func` by advancing to first `yield`""""""  \n    @wraps(func) \n    def primer(*args, **kwargs):  \n \n        gen = func(*args, **kwargs)  \n \n        next(gen)  \n \n        return gen  \n \n    return primer\nThe decorated generator function is replaced by this primer  function\nwhich, when invoked, returns the primed generator .\nCall the decorated function to get a generator object.\nPrime the generator .\nReturn it.\nExample 19-6  shows the @coroutine  decorator in use. Contrast with\nExample 19-3 .\nExample 19-6. cor oaverager1.py: doctest and code for a running average\ncor outine using the @cor outine decorator fr om Example 19-5\n"""""" \nA coroutine to compute a running average  \n \n    >>> coro_avg = averager()  \n  \n    >>> from inspect import getgeneratorstate  \n    >>> getgeneratorstate(coro_avg)  \n  \n    'GEN_SUSPENDED'  \n    >>> coro_avg.send(10)  \n  \n    10.0  \n    >>> coro_avg.send(30)  \n    20.0  \n    >>> coro_avg.send(5)  \n    15.0  \n \n"""""" \n \nfrom coroutil  import coroutine   \n \n \n@coroutine   \n \ndef averager ():  \n \n    total = 0.0 \n    count = 0 \n    average = None \n    while True: \n        term = yield average \n        total += term \n        count += 1 \n        average = total/count\nCall averager() , creating a generator object that is primed inside the\nprimer  function of the coroutine  decorator .\ngetgeneratorstate  reports GEN_SUSPENDED , meaning that the\ncoroutine is ready to receive a value.\nY ou can immediately start sending values to coro_avg : that’ s the\npoint of the decorator .\nImport the coroutine  decorator .\nApply it to the averager  function.\nThe body of the function is exactly the same as Example 19-3 .\nSeveral frameworks provide special decorators designed to work with\ncoroutines. Not all of them actually prime the coroutine—some provide\nother services, such as hooking it to an event loop. One example from the\nT ornado asynchronous networking library is the tornado.gen  decorator .\nThe yield from  syntax we’ll see in “Using yield from”  automatically\nprimes the coroutine called by it, making it incompatible with decorators\nsuch as @coroutine  from Example 19-5 . The asyncio.coroutine\ndecorator from the Python 3.4 standard library is designed to work with\nyield from  so it does not prime the coroutine. W e’ll cover it in\nChapter 22 .",4037
254-Coroutine Termination and Exception Handling.pdf,254-Coroutine Termination and Exception Handling,"W e’ll now focus on essential features of coroutines: the methods used to\nterminate and throw exceptions into them.\nC o r o u t i n e  T e r m i n a t i o n  a n d  E x c e p t i o n\nH a n d l i n g\nAn unhandled exception within a coroutine propagates to the caller of the\nnext  or send  that triggered it. Example 19-7  is an example using the\ndecorated averager  coroutine from Example 19-6 .\nExample 19-7. How an unhandled exception kills a cor outine\n>>> from coroaverager1  import averager  \n>>> coro_avg  = averager () \n>>> coro_avg .send(40)  \n \n40.0 \n>>> coro_avg .send(50) \n45.0 \n>>> coro_avg .send('spam')  \n \nTraceback (most recent call last):  \n  ... \nTypeError : unsupported operand type(s) for +=: 'float' and 'str'  \n>>> coro_avg .send(60)  \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nStopIteration\nUsing the @coroutine  decorated averager  we can immediately\nstart sending values.\nSending a nonnumeric value causes an exception inside the coroutine.\nBecause the exception was not handled in the coroutine, it terminated.\nAny attempt to reactivate it will raise StopIteration .\nThe cause of the error was the sending of a value 'spam'  that could not be\nadded to the total  variable in the coroutine.\nExample 19-7  suggests one way of terminating coroutines: you can use\nsend  with some sentinel value that tells the coroutine to exit. Constant\nbuilt-in singletons like None  and Ellipsis  are convenient sentinel\nvalues. Ellipsis  has the advantage of being quite unusual in data\nstreams. Another sentinel value I’ve seen used is StopIteration —the\nclass itself, not an instance of it (and not raising it). In other words, using it\nlike: my_coro.send(StopIteration) .\nSince Python 2.5, generator objects have two methods that allow the client\nto explicitly send exceptions into the coroutine— throw  and close :\ngenerator.throw(exc_type[, exc_value[,\ntraceback]])\nCauses the yield  expression where the generator was paused to raise\nthe exception given. If the exception is handled by the generator , flow\nadvances to the next yield , and the value yielded becomes the value\nof the generator.throw  call. If the exception is not handled by the\ngenerator , it propagates to the context of the caller .\ngenerator.close()\nCauses the yield  expression where the generator was paused to raise a\nGeneratorExit  exception. No error is reported to the caller if the\ngenerator does not handle that exception or raises StopIteration —\nusually by running to completion. When receiving a\nGeneratorExit , the generator must not yield a value, otherwise a\nRuntimeError  is raised. If any other exception is raised by the\ngenerator , it propagates to the caller .\nT I P\nThe of ficial documentation of the generator object methods is buried deep in The Python\nLanguage Refer ence , (see Generator -iterator methods ).\nLet’ s see how close  and throw  control a coroutine. Example 19-8  lists\nthe demo_exc_handling  function used in the following examples.\nExample 19-8. cor o_exc_demo.py: test code for studying exception\nhandling in a cor outine\nclass DemoException (Exception ): \n    """"""An exception type for the demonstration.""""""  \n \ndef demo_exc_handling (): \n    print('-> coroutine started ') \n    while True: \n        try: \n            x = yield \n        except DemoException :  \n \n            print('*** DemoException handled. Continuing... ') \n        else:  \n \n            print(f'-> coroutine received: {x!r} ') \n    raise RuntimeError ('This line should never run. ')  \nSpecial handling for DemoException .\nIf no exception, display received value.\nThis line will never be executed.\nThe last line in Example 19-8  is unreachable because the infinite loop can\nonly be aborted with an unhandled exception, and that terminates the\ncoroutine immediately .\nNormal operation of demo_exc_handling  is shown in Example 19-9 .\nExample 19-9. Activating and closing demo_exc_handling without an\nexception\n    >>> exc_coro  = demo_exc_handling () \n    >>> next(exc_coro ) \n    -> coroutine  started \n    >>> exc_coro .send(11) \n    -> coroutine  received : 11 \n    >>> exc_coro .send(22) \n    -> coroutine  received : 22 \n    >>> exc_coro .close() \n    >>> from inspect import getgeneratorstate  \n    >>> getgeneratorstate (exc_coro ) \n    'GEN_CLOSED'\nIf the DemoException  is thrown into the coroutine, it’ s handled and the\ndemo_exc_handling  coroutine continues, as in Example 19-10 .\nExample 19-10. Thr owing DemoException into demo_exc_handling does\nnot br eak it\n    >>> exc_coro  = demo_exc_handling () \n    >>> next(exc_coro ) \n    -> coroutine  started \n    >>> exc_coro .send(11) \n    -> coroutine  received : 11 \n    >>> exc_coro .throw(DemoException ) \n    *** DemoException  handled. Continuing ... \n    >>> getgeneratorstate (exc_coro ) \n    'GEN_SUSPENDED'\nOn the other hand, if an unhandled exception is thrown into the coroutine, it\nstops—its state becomes 'GEN_CLOSED' . Example 19-1 1  demonstrates it.\nExample 19-1 1. Cor outine terminates if it can’ t handle an exception thr own\ninto it\n    >>> exc_coro  = demo_exc_handling () \n    >>> next(exc_coro ) \n    -> coroutine  started \n    >>> exc_coro .send(11) \n    -> coroutine  received : 11 \n    >>> exc_coro .throw(ZeroDivisionError ) \n    Traceback  (most recent call last): \n      ... \n    ZeroDivisionError  \n    >>> getgeneratorstate (exc_coro ) \n    'GEN_CLOSED'\nIf it’ s necessary that some cleanup code is run no matter how the coroutine\nends, you need to wrap the relevant part of the coroutine body in a\ntry/finally  block, as in Example 19-12 .\nExample 19-12. cor o_finally_demo.py: use of try/finally to perform actions\non cor outine termination\nclass DemoException (Exception ): \n    """"""An exception type for the demonstration.""""""",5884
255-Returning a Value from a Coroutine.pdf,255-Returning a Value from a Coroutine,"def demo_finally (): \n    print('-> coroutine started' ) \n    try: \n        while True: \n            try: \n                x = yield \n            except DemoException : \n                print('*** DemoException handled. Continuing...' ) \n            else: \n                print(f'-> coroutine received: {x!r}' ) \n    finally: \n        print('-> coroutine ending' )\nOne of the main reasons why the yield from  construct was added to\nPython 3.3 has to do with throwing exceptions into nested coroutines. The\nother reason was to enable coroutines to return values more conveniently .\nRead on to see how .\nR e t u r n i n g  a  V a l u e  f r o m  a  C o r o u t i n e\nExample 19-13  shows a variation of the averager  coroutine that returns a\nresult. For didactic reasons, it does not yield the running average with each\nactivation. This is to emphasize that some coroutines do not yield anything\ninteresting, but are designed to return a value at the end, often the result of\nsome accumulation.\nThe result returned by averager  in Example 19-13  is a namedtuple\nwith the number of terms averaged ( count ) and the average . I could\nhave returned just the average  value, but returning a tuple exposes\nanother interesting piece of data that was accumulated: the count  of terms.\nExample 19-13. cor oaverager2.py: code for an averager cor outine that\nr eturns a r esult\nfrom collections  import namedtuple  \n \nResult = namedtuple ('Result', 'count average ') \n \n \ndef averager (): \n    total = 0.0 \n    count = 0 \n    average = None \n    while True: \n        term = yield \n        if term is None: \n            break  \n \n        total += term \n        count += 1 \n        average = total/count \n    return Result(count, average)  \nIn order to return a value, a coroutine must terminate normally; this is\nwhy this version of averager  has a condition to break out of its\naccumulating loop.\nReturn a namedtuple  with the count  and average . Before Python\n3.3, it was a syntax error to return a value in a generator function.\nT o see how this new averager  works, we can drive it from the console,\nas in Example 19-14 .\nExample 19-14. cor oaverager2.py: doctest showing the behavior of\naverager\n    >>> coro_avg  = averager () \n    >>> next(coro_avg ) \n    >>> coro_avg .send(10)  \n \n    >>> coro_avg .send(30) \n    >>> coro_avg .send(6.5) \n    >>> coro_avg .send(None)  \n \n    Traceback  (most recent call last): \n       ... \n    StopIteration : Result(count=3, average=15.5)\nThis version does not yield values.\nSending None  terminates the loop, causing the coroutine to end by\nreturning the result. As usual, the generator object raises\nStopIteration . The value  attribute of the exception carries the\nvalue returned.\nNote that the value of the return  expression is smuggled to the caller as\nan attribute of the StopIteration  exception. This is a bit of a hack, but\nit preserves the existing behavior of generator objects: raising\nStopIteration  when exhausted.\nExample 19-15  shows how to retrieve the value returned by the coroutine.\nExample 19-15. Catching StopIteration lets us get the value r eturned by\naverager\n    >>> coro_avg  = averager () \n    >>> next(coro_avg ) \n    >>> coro_avg .send(10) \n    >>> coro_avg .send(30) \n    >>> coro_avg .send(6.5) \n    >>> try: \n    ...     coro_avg .send(None) \n    ... except StopIteration  as exc: \n    ...     result = exc.value \n    ... \n    >>> result \n    Result(count=3, average=15.5)\nThis roundabout way of getting the return value from a coroutine makes\nmore sense when we realize it was defined as part of PEP 380, and the\nyield from  construct handles it automatically by catching\nStopIteration  internally . This is analogous to the use of\nStopIteration  in for  loops: the exception is handled by the loop\nmachinery in a way that is transparent to the user . In the case of yield\nfrom , the interpreter not only consumes the StopIteration , but its\nvalue  attribute becomes the value of the yield from  expression itself.\nUnfortunately we can’ t test this interactively in the console, because it’ s a\nsyntax error to use yield from —or yield , for that matter—outside of\na function.\nThe next section has an example where the averager  coroutine is used\nwith yield from  to produce a result, as intended in PEP 380. So let’ s\ntackle yield from .5",4410
256-Using yield from.pdf,256-Using yield from,"U s i n g  y i e l d  f r o m\nThe first thing to know about yield from  is that it is a completely new\nlanguage construct. It does a lot more than yield . The newer await\nkeyword is very similar to yield from , and its name conveys a crucial\npoint: when a generator gen  calls yield from subgen() , the\nsubgen  takes over and will yield values to the caller of gen ; the caller\nwill in ef fect drive subgen  directly . Meanwhile gen  will be blocked,\nwaiting until subgen  terminates.\nHowever , await  does not completely replace yield from . Each of\nthem has its own use cases, and await  is more strict about its context and\ntar get: await  can only be used inside a native coroutine, and its tar get\nmust be an awaitable  object, which we will cover in Chapter 22 . In\ncontrast, yield from  can be used in any function—which then becomes\na generator—and its tar get can be any iterable. This super simple yield\nfrom  example cannot be written with async/await  syntax:\n>>> def gen123(): \n...     yield from  [1, 2, 3] \n... \n>>> tuple(gen123()) \n(1, 2, 3)\nW e’ve seen in Chapter 17  that yield from  can be used as a shortcut to\nyield  in a for  loop. For example, this:\n>>> def gen(): \n...     for c in 'AB': \n...         yield c \n...     for i in range(1, 3): \n...         yield i \n... \n>>> list(gen()) \n['A', 'B', 1, 2]\nCan be written as:\n>>> def gen(): \n...     yield from  'AB' \n...     yield from  range(1, 3) \n... \n>>> list(gen()) \n['A', 'B', 1, 2]\nWhen we first mentioned yield from  in “Subgenerators with yield\nfrom” , the code from Example 19-16  demonstrates a practical use for it—\nalthough the itertools  module already provides an optimized chain\ngenerator written in C.\nExample 19-16. Chaining iterables with yield fr om\n>>> def chain(*iterables ): \n...     for it in iterables : \n...         yield from  it \n... \n>>> s = 'ABC' \n>>> t = tuple(range(3)) \n>>> list(chain(s, t)) \n['A', 'B', 'C', 0, 1, 2]\nT wo slightly more complicated—but more useful—examples of yield\nfrom  are the code in “T raversing a tree” , and “Recipe 4.14. Flattening a\nNested Sequence” in Beazley and Jones’ s Python Cookbook, 3E  (source\ncode available on GitHub ).\nThe first thing the yield from x  expression does with the x  object is to\ncall iter(x)  to obtain an iterator from it. This means that x  can be any\niterable.\nHowever , if replacing nested for  loops yielding values was the only\ncontribution of yield from , this language addition wouldn’ t have had a\ngood chance of being accepted. The real nature of yield from  cannot be\ndemonstrated with simple iterables; it requires the mind-expanding use of\nnested generators. That’ s why PEP 380, which introduced yield from ,\nis titled Syntax for Delegating to a Subgenerator .\nThe main feature of yield from  is to open a bidirectional channel from\nthe outermost caller to the innermost subgenerator , so that values can be\nsent and yielded back and forth directly from them, and exceptions can be\nthrown all the way in without adding a lot of exception handling boilerplate\ncode in the intermediate coroutines. This is what enables coroutine\ndelegation in a way that was not possible before.\nThe use of yield from  requires a nontrivial arrangement of code. T o talk\nabout the required moving parts, PEP 380 uses some terms in a very\nspecific way:\ndelegating generator\nThe generator function that contains the yield from <iterable>\nexpression.\nsubgenerator\nThe generator obtained from the <iterable>  part of the yield\nfrom  expression. This is the “subgenerator” mentioned in the title of\nPEP 380: “Syntax for Delegating to a Subgenerator .”\ncaller\nPEP 380 uses the term “caller” to refer to the client code that calls the\ndelegating generator . Depending on context, I use “client” instead of\n“caller ,” to distinguish from the delegating generator , which is also a\n“caller” (it calls the subgenerator).\nT I P\nPEP 380 often uses the word “iterator” to refer to the subgenerator . That’ s confusing\nbecause the delegating generator is also an iterator . So I prefer to use the term\nsubgenerator , in line with the title of the PEP—“Syntax for Delegating to a\nSubgenerator .” However , the subgenerator can be a simple iterator implementing only\n__next__ , and yield from  can handle that too, although it was created to support\ngenerators implementing __next__ , send , close , and throw .\nExample 19-17  provides more context to see yield from  at work, and\nFigure 19-2  identifies the relevant parts of the example.6\nFigur e 19-2. While the delegating generator is suspended at yield fr om, the caller sends data dir ectly\nto the subgenerator , which yields data back to the caller . The delegating generator r esumes when the\nsubgenerator r eturns and the interpr eter raises StopIteration with the r eturned value attached.\nThe cor oaverager3.py  script reads a dict  with weights and heights from\ngirls and boys in an imaginary seventh grade class. For example, the key\n'boys;m'  maps to the heights of 9 boys, in meters; 'girls;kg'  are the\nweights of 10 girls in kilograms. The script feeds the data for each group\ninto the averager  coroutine we’ve seen before, and produces a report like\nthis one:\n$ python3 coroaverager3.py  \n 9 boys  averaging 40.42kg  \n 9 boys  averaging 1.39m  \n10 girls averaging 42.04kg  \n10 girls averaging 1.43m\nThe code in Example 19-17  is certainly not the most straightforward\nsolution to the problem, but it serves to show yield from  in action. This\nexample is inspired by the one given in What’ s New in Python 3.3 .\nExample 19-17. cor oaverager3.py: using yield fr om to drive averager and\nr eport statistics\nfrom collections  import namedtuple  \n \nResult = namedtuple ('Result', 'count average ') \n \n \n# the subgenerator  \ndef averager ():  \n \n    total = 0.0 \n    count = 0 \n    average = None \n    while True: \n        term = yield  \n \n        if term is None:  \n \n            break \n        total += term \n        count += 1 \n        average = total/count \n    return Result(count, average)  \n \n \n \n# the delegating generator  \ndef grouper(results, key):  \n \n    while True:  \n \n        results[key] = yield from  averager ()  \n \n \n \n# the client code, a.k.a. the caller  \ndef main(data):  \n \n    results = {} \n    for key, values in data.items(): \n        group = grouper(results, key)  \n \n        next(group)  \n \n        for value in values: \n            group.send(value)  \n \n        group.send(None)  # important! \n  \n \n    # print(results)  # uncomment to debug  \n    report(results) \n \n \n# output report  \ndef report(results): \n    for key, result in sorted(results.items()): \n        group, unit = key.split(';') \n        print(f'{result.count:2} {group:5} ', \n              f'averaging {result.average:.2f}{unit} ') \n \n \ndata = { \n    'girls;kg ': \n        [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, \n44.5], \n    'girls;m': \n        [1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43], \n    'boys;kg': \n        [39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3], \n    'boys;m': \n        [1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46], \n} \n \n \nif __name__  == '__main__ ': \n    main(data)\nSame averager  coroutine from Example 19-13 . Here it is the\nsubgenerator .\nEach value sent by the client code in main  will be bound to term  here.\nThe crucial terminating condition. W ithout it, a yield from  calling\nthis coroutine will block forever .\nThe returned Result  will be the value of the yield from\nexpression in grouper .\ngrouper  is the delegating generator .\nEach iteration in this loop creates a new instance of averager ; each is\na generator object operating as a coroutine.\n\nWhenever grouper  is sent a value, it’ s piped into the averager\ninstance by the yield from . grouper  will be suspended here as\nlong as the averager  instance is consuming values sent by the client.\nWhen an averager  instance runs to the end, the value it returns is\nbound to results[key] . The while  loop then proceeds to create\nanother averager  instance to consume more values.\nmain  is the client code, or “caller” in PEP 380 parlance. This is the\nfunction that drives everything.\ngroup  is a generator object resulting from calling grouper  with the\nresults  dict  to collect the results, and a particular key . It will\noperate as a coroutine.\nPrime the coroutine.\nSend each value  into the grouper . That value ends up in the term\n= yield  line of averager ; grouper  never has a chance to see it.\nSending None  into grouper  causes the current averager  instance\nto terminate, and allows grouper  to run again, which creates another\naverager  for the next group of values.\nThe last callout in Example 19-17  with the comment "" important! ""\nhighlights a crucial line of code: group.send(None) , which terminates\none averager  and starts the next. If you comment out that line, the script\nproduces no output. Uncommenting the print(results)  line near the\nend of main  reveals that the results  dict  ends up empty .\nN O T E\nIf you want to figure out for yourself why no results are collected, it will be a great way\nto exercise your understanding of how yield from  works. The code for\ncor oaverager3.py  is in the Fluent Python code repository . The explanation is next.\nHere is an overview of how Example 19-17  works, explaining what would\nhappen if we omitted the call group.send(None)  marked “important!”\nin main :\nEach iteration of the outer for  loop creates a new grouper\ninstance named group ; this is the delegating generator .\nThe call next(group)  primes the grouper  delegating\ngenerator , which enters its while True  loop and suspends at the\nyield from , after calling the subgenerator averager .\nThe inner for  loop calls group.send(value) ; this feeds the\nsubgenerator averager  directly . Meanwhile, the current group\ninstance of grouper  is suspended at the yield from .\nWhen the inner for  loop ends, the group  instance is still\nsuspended at the yield from , so the assignment to\nresults[key]  in the body of grouper  has not happened yet.\nW ithout the last group.send(None)  in the outer for  loop, the\naverager  subgenerator never terminates, the delegating\ngenerator group  is never reactivated, and the assignment to\nresults[key]  never happens.\nWhen execution loops back to the top of the outer for  loop, a new\ngrouper  instance is created and bound to group . The previous\ngrouper  instance is garbage collected (together with its own\nunfinished averager  subgenerator instance).\nW A R N I N G\nThe key takeaway from this experiment is: if a subgenerator never terminates, the\ndelegating generator will be suspended forever at the yield from . This will not\nprevent your program from making progress because the yield from  (like the simple\nyield ) transfers control to the client code (i.e., the caller of the delegating generator).\nBut it does mean that some task will be left unfinished.",11098
257-Pipelines of coroutines.pdf,257-Pipelines of coroutines,,0
258-Basic behavior of yield from.pdf,258-Basic behavior of yield from,"Pipelines of coroutines\nExample 19-17  demonstrates the simplest arrangement of yield from ,\nwith only one delegating generator and one subgenerator . Because the\ndelegating generator works as a pipe, you can connect any number of them\nin a pipeline: one delegating generator uses yield from  to call a\nsubgenerator , which itself is a delegating generator calling another\nsubgenerator with yield from , and so on. Eventually this chain must\nend in a simple generator that uses just yield , but it may also end in any\niterable object, as in Example 19-16 .\nEvery yield from  chain must be driven by a client that calls next(…)\nor .send(…)  on the outermost delegating generator . This call may be\nimplicit, such as a for  loop.\nNow let’ s review the formal description of the yield from  construct, as\npresented in PEP 380.\nT h e  M e a n i n g  o f  y i e l d  f r o m\nN O T E\nThis is one of the most challenging sections in the book. Y ou may wonder whether it is\nworth your attention, given that most uses of yield from  are in legacy asynchronous\nprogramming code, where it is now preferable to use await  instead. But if you really\nwant to understand how async/wait  works under the hood, you need to understand\nyield from . The underlying machinery is the same. PEP 492–Coroutines with async\nand await syntax  states that await  “uses the yield from  implementation with an\nextra step of validating its ar gument”.  PEP 492 also assumes understanding of PEP\n380, and does not go into the same level of detail about the behavior of yield from\nor await .\nWhile developing PEP 380, Greg Ewing—the PEP author—was questioned\nabout the complexity of the proposed semantics. One of his answers was\n“For humans, almost all the important information is contained in one7\nparagraph near the top.” He then quoted part of the draft of PEP 380 which\nat the time read as follows:\n“When the iterator is another generator , the effect is the same as if the\nbody of the subgenerator wer e inlined at the point of the yield from\nexpr ession. Furthermor e, the subgenerator is allowed to execute a\nreturn  statement with a value, and that value becomes the value of the\nyield from  expr ession.”\nThose soothing words are no longer part of the PEP—because they don’ t\ncover all the corner cases. But they are OK as a first approximation.\nW e will tackle our study of yield from  in two steps: first, its basic\nbehavior , which covers many use cases. After that, we’ll see what happens\nwhen the subgenerator is terminated before it runs to completion, as well as\nother exceptional execution paths.\nBasic behavior of yield from\nThe approved version of PEP 380 explains the behavior of yield from\nin six points in the Proposal section . Here I reproduce them almost exactly ,\nexcept that I replaced every occurrence of the ambiguous word “iterator”\nwith “subgenerator” and added a few clarifications. Let’ s start with the four\npoints that are illustrated by coroaverager3.py  in Example 19-17 :\nAny values that the subgenerator yields are passed directly to the\ncaller of the delegating generator (i.e., the client code).\nAny values sent to the delegating generator using send()  are\npassed directly to the subgenerator . If the sent value is None , the\nsubgenerator ’ s __next__()  method is called. If the sent value is\nnot None , the subgenerator ’ s send()  method is called. If the call\nraises StopIteration , the delegating generator is resumed.\nAny other exception is propagated to the delegating generator .\nreturn expr  in a generator (or subgenerator) causes\nStopIteration(expr)  to be raised upon exit from the8\ngenerator .\nThe value of the yield from  expression is the first ar gument to\nthe StopIteration  exception raised by the subgenerator when\nit terminates.\nThe other two points about yield from  in PEP 380 have to do with\nexceptions and termination. W e’ll see them in “Exception handling in yield\nfrom” . For now , let’ s study on the behaviour of yield from  under\n“normal” operating conditions.\nThe detailed semantics of yield from  are subtle. Greg Ewing did a great\njob putting them to words in English in PEP 380.\nEwing also documented the behavior of yield from  using pseudocode\n(with Python syntax). I personally found it useful to spend some time\nstudying the pseudocode in PEP 380. However , the pseudocode is 40 lines\nlong and not easy to grasp at first.\nA good way to approach that pseudocode is to simplify it to handle only the\nmost basic and common uses of yield from .\nConsider that yield from  appears in a delegating generator . The client\ncode drives the delegating generator , which drives the subgenerator . So, to\nsimplify the logic involved, let’ s assume the client doesn’ t ever call\n.throw(…)  or .close()  on the delegating generator . Let’ s also assume\nthe subgenerator never raises an exception until it terminates, when\nStopIteration  is raised by the interpreter .\nThe coroaverager3.py  script in Example 19-17  is an example where\nthose simplifying assumptions hold. In fact, often the delegating generator\nis expected to run to completion. So let’ s see how yield from  works in\nthis happier , simpler world.\nT ake a look at Example 19-18 , which is an expansion of this single\nstatement, in the body of the delegating generator:\nRESULT = yield from  EXPR\nT ry to follow the logic in Example 19-18 .\nExample 19-18. Simplified pseudocode equivalent to the statement RESUL T\n= yield fr om EXPR in the delegating generator (this covers the simplest\ncase: .thr ow(…) and .close() ar e not supported; the only exception handled\nis StopIteration)\n_i = iter(EXPR)  \n \ntry: \n    _y = next(_i)  \n \nexcept StopIteration  as _e: \n    _r = _e.value  \n \nelse: \n    while 1:  \n \n        _s = yield _y  \n \n        try: \n            _y = _i.send(_s)  \n \n        except StopIteration  as _e:  \n \n            _r = _e.value \n            break \n \nRESULT = _r  \nThe EXPR  can be any iterable, because iter()  is applied to get an\niterator _i  (this is the subgenerator).\nThe subgenerator is primed; the result is stored to be the first yielded\nvalue _y .\nIf StopIteration  was raised, extract the value  attribute from the\nexception and assign it to _r : this is the RESULT  in the simplest case.\nWhile this loop is running, the delegating generator is blocked,\noperating just as a channel between the caller and the subgenerator .\nY ield the current item yielded from the subgenerator; wait for a value\n_s  sent by the caller . Note that this is the only yield  in this listing.\nT ry to advance the subgenerator , forwarding the _s  sent by the caller .\n\nIf the subgenerator raised StopIteration , get the value , assign to\n_r , and exit the loop, resuming the delegating generator .\n_r  is the RESULT : the value of the whole yield from  expression.\nIn this simplified pseudocode, I preserved the variable names used in the\npseudocode published in PEP 380. The variables are:\n_i  (iterator)\nThe subgenerator\n_y  (yielded)\nA value yielded from the subgenerator\n_r  (r esult)\nThe eventual result (i.e., the value of the yield from  expression\nwhen the subgenerator ends)\n_s  (sent)\nA value sent by the caller to the delegating generator , which is\nforwarded to the subgenerator\n_e  (exception)\nAn exception (always an instance of StopIteration  in this\nsimplified pseudocode)\nBesides not handling .throw(…)  and .close() , the simplified\npseudocode always uses .send(…)  to forward next()  or .send(…)\ncalls by the client to the subgenerator . Don’ t worry about these fine\ndistinctions on a first reading. As mentioned, coroaverager3.py  in\nExample 19-17  would run perfectly well if the yield from  did only\nwhat is shown in the simplified pseudocode in Example 19-18 .",7838
259-Exception handling in yield from.pdf,259-Exception handling in yield from,"The next section covers the behavior of yield from  when the\nsubgenerator ends prematurely , either because the client cancels it, or an\nunhandled exception is raised.\nException handling in yield from\nIn “Basic behavior of yield from”  we saw the first four points about yield\nfrom  behavior from PEP 380, and pseudo-code describing that behavior .\nBut the reality is more complicated, because of the need to handle\n.throw(…)  and .close()  calls from the client, which must be passed\ninto the subgenerator . Here are the other points of the PEP 380 Proposal\nsection , slightly edited:\nExceptions other than GeneratorExit  thrown into the\ndelegating generator are passed to the throw()  method of the\nsubgenerator . If the call raises StopIteration , the delegating\ngenerator is resumed. Any other exception is propagated to the\ndelegating generator .\nIf a GeneratorExit  exception is thrown into the delegating\ngenerator , or the close()  method of the delegating generator is\ncalled, then the close()  method of the subgenerator is called if it\nhas one. If this call results in an exception, it is propagated to the\ndelegating generator . Otherwise, GeneratorExit  is raised in\nthe delegating generator .\nAlso, the subgenerator may be a plain iterator that does not support\n.throw(…)  or .close() , so this must be handled by the yield from\nlogic. If the subgenerator does implement those methods, inside the\nsubgenerator both methods cause exceptions to be raised, which must be\nhandled by the yield from  machinery as well. The subgenerator may\nalso throw exceptions of its own, unprovoked by the caller , and this must\nalso be dealt with in the yield from  implementation. Finally , as an\noptimization, if the caller calls next(…)  or .send(None) , both are\nforwarded as a next(…)  call on the subgenerator; only if the caller sends a\nnon-None  value, the .send(…)  method of the subgenerator is used.\nFor your convenience, I present here the complete pseudocode of the\nyield from  expansion from PEP 380, with numbered annotations.\nExample 19-19  was copied verbatim; I only added the callout numbers.\nMost of the logic of the yield from  pseudocode is implemented in six\ntry/except  blocks nested up to four levels deep, so it’ s a bit hard to\nread. The only other control flow keywords used are one while , one if ,\nand one yield . Find the while , the yield , the next(…) , and the\n.send(…)  calls: they will help you get an idea of how the whole structure\nworks.\nRemember that all the code shown in Example 19-19  is an expansion of this\nsingle statement, in the body of a delegating generator:\nRESULT = yield from  EXPR\nExample 19-19. Pseudocode equivalent to the statement RESUL T = yield\nfr om EXPR in the delegating generator\n_i = iter(EXPR)  \n \ntry: \n    _y = next(_i)  \n \nexcept StopIteration  as _e: \n    _r = _e.value  \n \nelse: \n    while 1:  \n \n        try: \n            _s = yield _y  \n \n        except GeneratorExit  as _e:  \n \n            try: \n                _m = _i.close \n            except AttributeError : \n                pass \n            else: \n                _m() \n            raise _e \n        except BaseException  as _e:  \n \n            _x = sys.exc_info () \n            try: \n                _m = _i.throw \n            except AttributeError : \n                raise _e \n            else:  \n \n                try: \n                    _y = _m(*_x) \n                except StopIteration  as _e: \n                    _r = _e.value \n                    break \n        else:  \n \n            try:  \n \n                if _s is None:  \n \n                    _y = next(_i) \n                else: \n                    _y = _i.send(_s) \n            except StopIteration  as _e:  \n \n                _r = _e.value \n                break \n \nRESULT = _r  \nThe EXPR  can be any iterable, because iter()  is applied to get an\niterator _i  (this is the subgenerator).\nThe subgenerator is primed; the result is stored to be the first yielded\nvalue _y .\nIf StopIteration  was raised, extract the value  attribute from the\nexception and assign it to _r : this is the RESULT  in the simplest case.\nWhile this loop is running, the delegating generator is blocked,\noperating just as a channel between the caller and the subgenerator .\nY ield the current item yielded from the subgenerator; wait for a value\n_s  sent by the caller . This is the only yield  in this listing.\nThis deals with closing the delegating generator and the subgenerator .\nBecause the subgenerator can be any iterator , it may not have a close\nmethod.\n\nThis deals with exceptions thrown in by the caller using .throw(…) .\nAgain, the subgenerator may be an iterator with no throw  method to\nbe called—in which case the exception is raised in the delegating\ngenerator .\nIf the subgenerator has a throw  method, call it with the exception\npassed from the caller . The subgenerator may handle the exception (and\nthe loop continues); it may raise StopIteration  (the _r  result is\nextracted from it, and the loop ends); or it may raise the same or another\nexception, which is not handled here and propagates to the delegating\ngenerator .\nIf no exception was received when yielding…\nT ry to advance the subgenerator…\nCall next  on the subgenerator if the last value received from the caller\nwas None , otherwise call send .\nIf the subgenerator raised StopIteration , get the value , assign to\n_r , and exit the loop, resuming the delegating generator .\n_r  is the RESULT : the value of the whole yield from  expression.\nRight at the top of Example 19-19 , one important detail revealed by the\npseudocode is that the subgenerator is primed (second callout in\nExample 19-19 ).  This means that auto-priming decorators such as that in\n“Decorators for Coroutine Priming”  are incompatible with yield from .\nIn the same message  I quoted in the opening of this section, Greg Ewing\nhas this to say about the pseudocode expansion of yield from :\nY ou’r e not meant to learn about it by r eading the expansion—that’ s only\nther e to pin down all the details for language lawyers.9",6186
260-The Taxi Fleet Simulation.pdf,260-The Taxi Fleet Simulation,"Focusing on the details of the pseudocode expansion may not be helpful—\ndepending on your learning style. Studying real code that uses yield\nfrom  is certainly more profitable than poring over the pseudocode of its\nimplementation. However , almost all the yield from  examples I’ve seen\nare tied to asynchronous programming with the asyncio  module, so they\ndepend on an active event loop to work—and most such code now uses\nawait  instead of yield from . There are a few links in “Further\nReading”  to interesting code using yield from  without an event loop.\nW e’ll now move on to a classic example of coroutine usage: programming\nsimulations. This example does not showcase yield from , but it does\nreveal how coroutines are used to manage concurrent activities on a single\nthread.\nU s e  C a s e :  C o r o u t i n e s  f o r  D i s c r e t e  E v e n t\nS i m u l a t i o n\nCor outines ar e a natural way of expr essing many algorithms, such as\nsimulations, games, asynchr onous I/O, and other forms of event-driven\npr ogramming or co-operative multitasking.\n— Guido van Rossum and Phillip J. Eby , PEP 342—\nCoroutines via Enhanced Generators\nIn this section, I will describe a very simple simulation implemented using\njust coroutines and standard library objects. Simulation is a classic\napplication of coroutines in the computer science literature. Simula, the first\nOO language, introduced the concept of coroutines precisely to support\nsimulations.10\nN O T E\nThe motivation for the following simulation example is not academic. Coroutines are\nthe fundamental building block of the asyncio  package. A simulation shows how to\nimplement concurrent activities using coroutines instead of threads—and this will\ngreatly help when we tackle asyncio  in Chapter 22 .\nBefore going into the example, a word about simulations.\nAbout Discrete Event Simulations\nA discrete event simulation (DES) is a type of simulation where a system is\nmodeled as a sequence of events. In a DES, the simulation “clock” does not\nadvance by fixed increments, but advances directly to the simulated time of\nthe next modeled event. For example, if we are simulating the operation of\na taxi cab from a high-level perspective, one event is picking up a\npassenger , the next is dropping the passenger of f. It doesn’ t matter if a trip\ntakes 5 or 50 minutes: when the drop of f event happens, the clock is\nupdated to the end time of the trip in a single operation. In a DES, we can\nsimulate a year of cab trips in less than a second. This is in contrast to a\ncontinuous simulation where the clock advances continuously by a fixed—\nand usually small—increment.\nIntuitively , turn-based games are examples of discrete event simulations:\nthe state of the game only changes when a player moves, and while a player\nis deciding the next move, the simulation clock is frozen. Real-time games,\non the other hand, are continuous simulations where the simulation clock is\nrunning all the time, the state of the game is updated many times per\nsecond, and slow players are at a real disadvantage.\nBoth types of simulations can be written with multiple threads or a single\nthread using event-oriented programming techniques such as callbacks or\ncoroutines driven by an event loop. It’ s ar guably more natural to implement\na continuous simulation using threads to account for actions happening in\nparallel in real time. On the other hand, coroutines of fer exactly the right\nabstraction for writing a DES. SimPy  is a DES package for Python that\nuses one coroutine to represent each process in the simulation.\nT I P\nIn the field of simulation, the term pr ocess  refers to the activities of an entity in the\nmodel, and not to an OS process. A simulation process may be implemented as an OS\nprocess, but usually a thread or a coroutine is used for that purpose.\nIf you are interested in simulations, SimPy is well worth studying.\nHowever , in this section, I will describe a very simple DES implemented\nusing only standard library features. My goal is to help you develop an\nintuition about programming concurrent actions with coroutines.\nUnderstanding the next section will require careful study , but the reward\nwill come as insights on how libraries such as asyncio , T wisted, and\nT ornado can manage many concurrent activities using a single thread of\nexecution.\nThe T axi Fleet Simulation\nIn our simulation program, taxi_sim.py , a number of taxi cabs are created.\nEach will make a fixed number of trips and then go home. A taxi leaves the\ngarage and starts “prowling”—looking for a passenger . This lasts until a\npassenger is picked up, and a trip starts. When the passenger is dropped of f,\nthe taxi goes back to prowling.\nThe time elapsed during prowls and trips is generated using an exponential\ndistribution. For a cleaner display , times are in whole minutes, but the\nsimulation would work as well using float  intervals.  Each change of\nstate in each cab is reported as an event. Figure 19-3  shows a sample run of\nthe program.\nThe most important thing to note in Figure 19-3  is the interleaving of the\ntrips by the three taxis. I manually added the arrows to make it easier to see\nthe taxi trips: each arrow starts when a passenger is picked up and ends11\n12\nwhen the passenger is dropped of f. Intuitively , this demonstrates how\ncoroutines can be used for managing concurrent activities.\nOther things to note about Figure 19-3 :\nEach taxi leaves the garage 5 minutes after the other .\nIt took 2 minutes for taxi 0 to pick up the first passenger at\ntime=2 ; 3 minutes for taxi 1 ( time=8 ), and 5 minutes for taxi 2\n(time=15 ).\nThe cabbie in taxi 0 only makes two trips (purple arrows): the first\nstarts at time=2  and ends at time=18 ; the second starts at\ntime=28  and ends at time=65 —the longest trip in this\nsimulation run.\nT axi 1 makes four trips (green arrows) then goes home at\ntime=110 .\nT axi 2 makes six trips (red arrows) then goes home at time=109 .\nHis last trip lasts only one minute, starting at time=97 .\nWhile taxi 1 is making her first trip, starting at time=8 , taxi 2\nleaves the garage at time=10  and completes two trips (short red\narrows).\nIn this sample run, all scheduled events completed in the default\nsimulation time of 180 minutes; last event was at time=110 .13\nFigur e 19-3. Sample run of taxi_sim.py with thr ee taxis. The -s 3 ar gument sets the random generator\nseed so pr ogram runs can be r epr oduced for debugging and demonstration. Color ed arr ows highlight\ntaxi trips.\nThe simulation may also end with pending events. When that happens, the\nfinal message reads like this:\n*** end of simulation time: 3 events pending ***\nThe full listing of taxi_sim.py  is at [Link to Come]. In this chapter , we’ll\nshow only the parts that are relevant to our study of coroutines. The really\nimportant functions are only two: taxi_process  (a coroutine), and the\nSimulator.run  method where the main loop of the simulation is\nexecuted.\nExample 19-20  shows the code for taxi_process . This coroutine uses\ntwo objects defined elsewhere: the compute_delay  function, which\nreturns a time interval in minutes, and the Event  class, a named tuple\ndefined like this:\nEvent = collections .namedtuple ('Event', 'time proc action' )\nIn an Event  instance, time  is the simulation time when the event will\noccur , proc  is the identifier of the taxi process instance, and action  is a\nstring describing the activity .\nLet’ s review taxi_process  play by play in Example 19-20 .\nExample 19-20. taxi_sim.py: taxi_pr ocess cor outine that implements the\nactivities of each taxi\ndef taxi_process (ident, trips, start_time =0):  \n \n    """"""Yield to simulator issuing event at each state change""""""  \n    time = yield Event(start_time , ident, 'leave garage ')  \n \n    for i in range(trips):  \n \n        time = yield Event(time, ident, 'pick up passenger ')  \n \n        time = yield Event(time, ident, 'drop off passenger ')  \n \n \n    yield Event(time, ident, 'going home ')  \n \n    # end of taxi process \n\ntaxi_process  will be called once per taxi, creating a generator\nobject to represent its operations. ident  is the number of the taxi (e.g.,\n0, 1, 2 in the sample run); trips  is the number of trips this taxi will\nmake before going home; start_time  is when the taxi leaves the\ngarage.\nThe first Event  yielded is 'leave garage' . This suspends the\ncoroutine, and lets the simulation main loop proceed to the next\nscheduled event. When it’ s time to reactivate this process, the main loop\nwill send  the current simulation time, which is assigned to time .\nThis block will be repeated once for each trip.\nAn Event  signaling passenger pick up is yielded. The coroutine pauses\nhere. When the time comes to reactivate this coroutine, the main loop\nwill again send  the current time.\nAn Event  signaling passenger drop of f is yielded. The coroutine is\nsuspended again, waiting for the main loop to send it the time of when\nit’ s reactivated.\nThe for  loop ends after the given number of trips, and a final 'going\nhome'  event is yielded. The coroutine will suspend for the last time.\nWhen reactivated, it will be sent the time from the simulation main\nloop, but here I don’ t assign it to any variable because it will not be\nused.\nWhen the coroutine falls of f the end, the generator object raises\nStopIteration .\nY ou can “drive” a taxi yourself by calling taxi_process  in the Python\nconsole.  Example 19-21  shows how .\nExample 19-21. Driving the taxi_pr ocess cor outine14\n>>> from taxi_sim  import taxi_process  \n>>> taxi = taxi_process (ident=13, trips=2, start_time =0)  \n \n>>> next(taxi)  \n \nEvent(time=0, proc=13, action='leave garage')  \n>>> taxi.send(_.time + 7)  \n \nEvent(time=7, proc=13, action='pick up passenger')  \n  \n>>> taxi.send(_.time + 23)  \n \nEvent(time=30, proc=13, action='drop off passenger')  \n>>> taxi.send(_.time + 5)  \n \nEvent(time=35, proc=13, action='pick up passenger')  \n>>> taxi.send(_.time + 48)  \n \nEvent(time=83, proc=13, action='drop off passenger')  \n>>> taxi.send(_.time + 1) \nEvent(time=84, proc=13, action='going home')  \n  \n>>> taxi.send(_.time + 10)  \n \nTraceback (most recent call last):  \n  File ""<stdin>"" , line 1, in <module>  \nStopIteration\nCreate a generator object to represent a taxi with ident=13  that will\nmake two trips and start working at t=0 .\nPrime the coroutine; it yields the initial event.\nW e can now send  it the current time. In the console, the _  variable is\nbound to the last result; here I add 7  to the time, which means the taxi\nwill spend 7 minutes searching for the first passenger .\nThis is yielded by the for  loop at the start of the first trip.\nSending _.time + 23  means the trip with the first passenger will\nlast 23 minutes.\nThen the taxi  will prowl for 5 minutes.\nThe last trip will take 48 minutes.\nAfter two complete trips, the loop ends and the 'going home'  event\nis yielded.\nThe next attempt to send  to the coroutine causes it to fall through the\nend. When it returns, the interpreter raises StopIteration .\nNote that in Example 19-21  I am using the console to emulate the\nsimulation main loop. I get the .time  attribute of an Event  yielded by\nthe taxi  coroutine, add an arbitrary number , and use the sum in the next\ntaxi.send  call to reactivate it. In the simulation, the taxi coroutines are\ndriven by the main loop in the Simulator.run  method. The simulation\n“clock” is held in the sim_time  variable, and is updated by the time of\neach event yielded.\nT o instantiate the Simulator  class, the main  function of taxi_sim.py\nbuilds a taxis  dictionary like this:\n    taxis = {i: taxi_process (i, (i + 1) * 2, i * \nDEPARTURE_INTERVAL ) \n             for i in range(num_taxis )} \n    sim = Simulator (taxis)\nDEPARTURE_INTERVAL  is 5; if num_taxis  is 3  as in the sample run,\nthe preceding lines will do the same as:\n    taxis = {0: taxi_process (ident=0, trips=2, start_time =0), \n             1: taxi_process (ident=1, trips=4, start_time =5), \n             2: taxi_process (ident=2, trips=6, start_time =10)} \n    sim = Simulator (taxis)\nTherefore, the values of the taxis  dictionary will be three distinct\ngenerator objects with dif ferent parameters. For instance, taxi 1 will make 4\ntrips and begin looking for passengers at start_time=5 . This dict  is\nthe only ar gument required to build a Simulator  instance.\nThe Simulator.__init__  method is shown in Example 19-22 . The\nmain data structures of Simulator  are:\nself.events\nA PriorityQueue  to hold Event  instances. A PriorityQueue\nlets you put  items, then get  them ordered by item[0] ; i.e., the\ntime  attribute in the case of our Event  namedtuple  objects.\nself.procs\nA dict  mapping each process number to an active process in the\nsimulation—a generator object representing one taxi. This will be\nbound to a copy of taxis  dict  shown earlier .\nExample 19-22. taxi_sim.py: Simulator class initializer\nclass Simulator : \n \n    def __init__ (self, procs_map ): \n        self.events = queue.PriorityQueue ()  \n \n        self.procs = dict(procs_map )  \nThe PriorityQueue  to hold the scheduled events, ordered by\nincreasing time.\nW e get the procs_map  ar gument as a dict  (or any mapping), but\nbuild a dict  from it, to have a local copy because when the simulation\nruns, each taxi that goes home is removed from self.procs , and we\ndon’ t want to change the object passed by the user .\nPriority queues are a fundamental building block of discrete event\nsimulations: events are created in any order , placed in the queue, and later\nretrieved in order according to the scheduled time of each one. For\nexample, the first two events placed in the queue may be:\nEvent(time=14, proc=0, action='pick up passenger' ) \nEvent(time=11, proc=1, action='pick up passenger' )\nThis means that taxi 0 will take 14 minutes to pick up the first passenger ,\nwhile taxi 1—starting at time=10 —will take 1 minute and pick up a\npassenger at time=11 . If those two events are in the queue, the first event\nthe main loop gets from the priority queue will be Event(time=11,\nproc=1, action='pick up passenger') .\nNow let’ s study the main algorithm of the simulation, the\nSimulator.run  method. It’ s invoked by the main  function right after\nthe Simulator  is instantiated, like this:\n    sim = Simulator (taxis) \n    sim.run(end_time )\nThe listing with callouts for the Simulator  class is in Example 19-23 ,\nbut here is a high-level view of the algorithm implemented in\nSimulator.run :\n1 . Loop over processes representing taxis.\na . Prime the coroutine for each taxi by calling next()  on it.\nThis will yield the first Event  for each taxi.\nb . Put each event in the self.events  queue of the\nSimulator .\n2 . Run the main loop of the simulation while sim_time  <\nend_time .\na . Check if self.events  is empty; if so, break from the\nloop.\nb . Get the current_event  from self.events . This\nwill be the Event  object with the lowest time in the\nPriorityQueue .\nc . Display the Event .\nd . Update the simulation time with the time  attribute of the\ncurrent_event .\ne . Send the time to the coroutine identified by the proc\nattribute of the current_event . The coroutine will\nyield the next_event .\nf . Schedule next_event  by adding it to the\nself.events  queue.\nThe complete Simulator  class is Example 19-23 .\nExample 19-23. taxi_sim.py: Simulator , a bar e-bones discr ete event\nsimulation class; focus on the run method\nclass Simulator : \n \n    def __init__ (self, procs_map ): \n        self.events = queue.PriorityQueue () \n        self.procs = dict(procs_map ) \n \n    def run(self, end_time ):  \n \n        """"""Schedule and display events until time is up""""""  \n        # schedule the first event for each cab  \n        for _, proc in sorted(self.procs.items()):  \n \n            first_event  = next(proc)  \n \n            self.events.put(first_event )  \n \n \n        # main loop of the simulation  \n        sim_time  = 0  \n \n        while sim_time  < end_time :  \n \n            if self.events.empty():  \n \n                print('*** end of events *** ') \n                break \n \n            current_event  = self.events.get()  \n \n            sim_time , proc_id, previous_action  = current_event   \n \n            print('taxi:', proc_id, proc_id * '   ', current_event )  \n \n            active_proc  = self.procs[proc_id]  \n \n            next_time  = sim_time  + \ncompute_duration (previous_action )  \n \n            try: \n                next_event  = active_proc .send(next_time )  \n \n            except StopIteration : \n                del self.procs[proc_id]  \n \n            else: \n                self.events.put(next_event )  \n \n        else:  \n \n            msg = '*** end of simulation time: {} events pending  \n***' \n            print(msg.format(self.events.qsize()))\nThe simulation end_time  is the only required ar gument for run .\nUse sorted  to retrieve the self.procs  items ordered by the key;\nwe don’ t care about the key , so assign it to _ .\nnext(proc)  primes each coroutine by advancing it to the first yield,\nso it’ s ready to be sent data. An Event  is yielded.\nAdd each event to the self.events  PriorityQueue . The first\nevent for each taxi is 'leave garage' , as seen in the sample run\n( Example 19-20 ).\nZero sim_time , the simulation clock.\nMain loop of the simulation: run while sim_time  is less than the\nend_time .\nThe main loop may also exit if there are no pending events in the queue.\nGet Event  with the smallest time  in the priority queue; this is the\ncurrent_event .\nUnpack the Event  data. This line updates the simulation clock,\nsim_time , to reflect the time when the event happened.\nDisplay the Event , identifying the taxi and adding indentation\naccording to the taxi ID.\nRetrieve the coroutine for the active taxi from the self.procs\ndictionary .\n15\nCompute the next activation time by adding the sim_time  and the\nresult of calling compute_duration(…)  with the previous action\n(e.g., 'pick up passenger' , 'drop off passenger' , etc.)\nSend the time  to the taxi coroutine. The coroutine will yield the\nnext_event  or raise StopIteration  when it’ s finished.\nIf StopIteration  is raised, delete the coroutine from the\nself.procs  dictionary .\nOtherwise, put the next_event  in the queue.\nIf the loop exits because the simulation time passed, display the number\nof events pending (which may be zero by coincidence, sometimes).\nLinking back to Chapter 18 , note that the Simulator.run  method in\nExample 19-23  uses else  blocks in two places that are not if  statements:\nThe main while  loop has an else  statement to report that the\nsimulation ended because the end_time  was reached—and not\nbecause there were no more events to process.\nThe try  statement at the bottom of the while  loop tries to get a\nnext_event  by sending the next_time  to the current taxi\nprocess, and if that is successful the else  block puts the\nnext_event  into the self.events  queue.\nI believe the code in Simulator.run  would be a bit harder to read\nwithout those else  blocks.\nThe point of this example was to show a main loop processing events and\ndriving coroutines by sending data to them. This is the basic idea behind\nasyncio , which we’ll study in Chapter 22 .\nBefore closing the chapter , let’ s discuss generic coroutine types.",19583
261-Generic Type Hints for Classic Coroutines.pdf,261-Generic Type Hints for Classic Coroutines,"N O T E\nFeel free to skip the next section if coroutines, generic types and variance are too much\nfor you right now . I personally found the combination a bit hard to digest.\nG e n e r i c  T y p e  H i n t s  f o r  C l a s s i c  C o r o u t i n e s\nBack in “Contravariant T ypes” , I mentioned typing.Generator  as one\nof the few standard library types with a contravariant type parameter . Now\nthat we’ve studied classic coroutines, we are ready to make sense of this\ngeneric type.\nFor generators that only yield values and are never sent any value other than\nNone , the recommended type for annotations is Iterator[T_co] .\nDespite its name, typing.Generator  is really used to annotate classic\ncoroutines which not only yield values, but also accept values via\n.send()  and also return values through the StopIteration(value)\nhack.\nHere is how typing.Generator  was declared  in the typing.py  module\nof Python 3.6:\nT_co = TypeVar('T_co', covariant =True) \nV_co = TypeVar('V_co', covariant =True) \nT_contra  = TypeVar('T_contra' , contravariant =True) \n \n# many lines omitted  \n \nclass Generator (Iterator [T_co], Generic[T_co, T_contra , V_co], \n                extra=_G_base):\nThat generic type declaration means that a Generator  type hint requires\nthree type parameters, as in this example:\nmy_coro : Generator [YieldType , SendType , ReturnType ]16\nFrom the type variables in the formal parameters, we see that YieldType\nand ReturnType  are covariant, but SendType  is contravariant.\nT o understand why , consider that YieldType  and ReturnType  are\n“output” types. Both describe data that comes out of the coroutine object—\ni.e. the generator object when used as a coroutine object.\nIt makes sense that these are covariant, because any code expecting a\ncoroutine that yields floats can accept a coroutine that yields integers.\nThat’ s why Generator  is covariant on its YieldType  parameter . The\nsame reasoning applies to the ReturnType  parameter—also covariant.\nUsing the notation introduced in “Covariant T ypes” , the covariance of the\nfirst and third parameters is expressed by the parallel :>  symbols:\n                       float :> int  \nGenerator[float, Any, float] :> Generator[int, Any, int]\nYieldType  and ReturnType  are examples of the first rule of “V ariance\nRules of Thumb” .\nOn the other hand, SendType  is an “input” parameter: it is the type of the\nar gument for the send  method of the coroutine object. Code that wants to\nsend floats to a coroutine cannot use a coroutine with int  as the\nSendType  because int  is not a supertype of float . In other words,\nfloat  is not consistent-with  int . But it can use a coroutine with\ncomplex  as the SendType , because complex  is a supertype of float ,\ntherefore float  is consistent-with  complex .\nThe :>  notation makes the contravariance of the second parameter visible:\n                     float :> int  \nGenerator[Any, float, Any] <: Generator[Any, int, Any]\nThis is an example of the second V ariance Rule of Thumb .\nW ith this merry discussion of variance, we are ready to wrap this chapter—\none of the hardest in the book.",3165
262-Further Reading.pdf,262-Further Reading,"C h a p t e r  S u m m a r y\nGuido van Rossum wrote there are three dif ferent styles of code you can\nwrite using generators:\nTher e’ s the traditional “pull” style (iterators), “push” style (like the\naveraging example), and then ther e ar e “tasks” (Have you r ead Dave\nBeazley’ s cor outines tutorial yet?…).\nChapter 17  was devoted to iterators; this chapter introduced classic\ncoroutines used in “push style” and also as very simple “tasks”—the taxi\nprocesses in the simulation example. Chapter 22  will be about native\ncoroutines and asynchronous generators, which evolved from the generators\nand classic coroutines as described here.\nThe running average example demonstrated a common use for a classic\ncoroutine: as an accumulator processing items sent to it. W e saw how a\ndecorator can be applied to prime a coroutine, making it more convenient to\nuse in some cases. But keep in mind that priming decorators are not\ncompatible with some uses of coroutines. In particular , yield from\nsubgenerator()  assumes the subgenerator  is not primed, and\nprimes it automatically .\nAccumulator coroutines can yield back partial results with each send\nmethod call, but they become more useful when they can return values, a\nfeature that was added in Python 3.3 with PEP 380. W e saw how the\nstatement return the_result  in a generator now raises\nStopIteration(the_result) , allowing the caller to retrieve\nthe_result  from the value  attribute of the exception. This is a rather\ncumbersome way to retrieve coroutine results, but it’ s handled\nautomatically by the yield from  syntax introduced in PEP 380.\nThe coverage of yield from  started with trivial examples using simple\niterables, then moved to an example highlighting the three main\ncomponents of any significant use of yield from : the delegating\ngenerator (defined by the use of yield from  in its body), the\nsubgenerator activated by yield from , and the client code that actually17\ndrives the whole setup by sending values to the subgenerator through the\npass-through channel established by yield from  in the delegating\ngenerator . This section was wrapped up with a look at the formal definition\nof yield from  behavior as described in PEP 380 using English and\nPython-like pseudocode.\nW e closed the chapter with the discrete event simulation example, showing\nhow generators can be used as an alternative to threads and callbacks to\nsupport concurrency . Although simple, the taxi simulation gives a first\nglimpse at how event-driven frameworks like T ornado and asyncio  use a\nmain loop to drive coroutines executing concurrent activities with a single\nthread of execution. In event-oriented programming with coroutines, each\nconcurrent activity is carried out by a coroutine that repeatedly yields\ncontrol back to the main loop, allowing other coroutines to be activated and\nmove forward. This is a form of cooperative multitasking: coroutines\nvoluntarily and explicitly yield control to the central scheduler . In contrast,\nthreads implement preemptive multitasking. The scheduler can suspend\nthreads at any time—even halfway through a statement—to give way to\nother threads.\nF u r t h e r  R e a d i n g\nDavid Beazley is the ultimate authority on Python generators and\ncoroutines. The Python Cookbook, 3E  (O’Reilly) he coauthored with Brian\nJones has numerous recipes with coroutines. Beazley’ s PyCon tutorials on\nthe subject are famous for their depth and breadth. The first was at PyCon\nUS 2008: “Generator T ricks for Systems Programmers” . PyCon US 2009\nsaw the legendary “A Curious Course on Coroutines and Concurrency”\n(hard-to-find video links for all three parts: part 1 , part 2 , part 3 ). His\ntutorial from PyCon 2014 in Montréal was “Generators: The Final\nFrontier ,”  in which he tackles more concurrency examples—so it’ s really\nmore about topics in Chapter 22  of Fluent Python . Dave can’ t resist making\nbrains explode in his classes, so in the last part of “The Final Frontier ,”\ncoroutines replace the classic V isitor pattern in an arithmetic expression\nevaluator .\nCoroutines allow new ways of or ganizing code, and just as recursion or\npolymorphism (dynamic dispatch), it takes some time getting used to their\npossibilities. An interesting example of classic algorithm rewritten with\ncoroutines is in the post “Greedy algorithm with coroutines,”  by James\nPowell. Y ou may also want to browse “Popular recipes tagged cor outine ""  in\nthe ActiveState Code recipes database .\nPaul Sokolovsky implemented yield from  in Damien Geor ge’ s super\nlean MicroPython  interpreter designed to run on microcontrollers. As he\nstudied the feature, he created a great, detailed diagram  to explain how\nyield from  works, and shared it in the python-tulip mailing list.\nSokolovsky was kind enough to allow me to copy the PDF to this book’ s\nsite, where it has a more permanent URL .\nAs I write this, the vast majority of uses of yield from  to be found are\nin asyncio  itself or code that uses it. I spent a lot of time looking for\nexamples of yield from  that did not depend on asyncio . Greg Ewing\n—who penned PEP 380 and implemented yield from  in CPython—\npublished a few examples  of its use: a BinaryTree  class, a simple XML\nparser , and a task scheduler .\nBrett Slatkin’ s Effective Python, First Edition  (Addison-W esley) has an\nexcellent short chapter titled “Consider Coroutines to Run Many Functions\nConcurrently”. That chapter is not in Effective Python, Second Edition , but\nfortunately it is still available online as a sample chapter . Slatkin presents\nthe best example of driving coroutines with yield from  I’ve seen: an\nimplementation of John Conway’ s Game of Life  in which coroutines are\nused to manage the state of each cell as the game runs. I refactored the code\nfor the Game of Life example—separating the functions and classes that\nimplement the game from the testing snippets used in Slatkin’ s book\noriginal code. I also rewrote the tests as doctests, so you can see the output\nof the various coroutines and classes without running the script. The\nrefactored example  is posted as a GitHub gist .\nOther interesting examples of yield from  without asyncio  appear in a\nmessage to the Python T utor list, “Comparing two CSV files using Python”\nby Peter Otten, and a Rock-Paper -Scissors game in Ian W ard’ s “Iterables,\nIterators, and Generators”  tutorial published as an iPython notebook.\nGuido van Rossum sent a long message to the python-tulip Google Group\ntitled “The dif ference between yield  and yield-from ""  that is worth\nreading. Nick Coghlan posted a heavily commented version of the yield\nfrom  expansion to Python-Dev on March 21, 2009 ; in the same message,\nhe wrote:\nWhether or not differ ent people will find code using yield from\ndifficult to understand or not will have mor e to do with their grasp of the\nconcepts of cooperative multitasking in general mor e so than the\nunderlying trickery involved in allowing truly nested generators.\nExperimenting with discrete event simulations is a great way to become\ncomfortable with cooperative multitasking. W ikipedia’ s “Discrete event\nsimulation” article  is a good place to start.  A short tutorial about writing\ndiscrete event simulations by hand (no special libraries) is Ashish Gupta’ s\n“W riting a Discrete Event Simulation: T en Easy Lessons.”  The code is in\nJava so it’ s class-based and uses no coroutines, but can easily be ported to\nPython. Regardless of the code, the tutorial is a good short introduction to\nthe terminology and components of a discrete event simulation. Converting\nGupta’ s examples to Python classes and then to classes leveraging\ncoroutines is a good exercise.\nFor a ready-to-use library in Python, using coroutines, there is SimPy . Its\nonline documentation  explains:\nSimPy is a pr ocess-based discr ete-event simulation framework based on\nstandar d Python. Its event dispatcher is based on Python’ s generators\nand can also be used for asynchr onous networking or to implement\nmulti-agent systems (with both simulated and r eal communication).\nCoroutines are not so new in Python but they were pretty much tied to niche\napplication domains before asynchronous networking frameworks started18\nsupporting them, starting with T ornado. The addition of yield from  in\nPython 3.3 and asyncio  in Python 3.4 raised awareness about classic\ncoroutines until their main use case—asynchronous programming—was\ntaken over by native coroutines in Python 3.5. Classic coroutines are not\nobsolete, but they are now back to niche applications. Dif ferences between\nclassic coroutines and native coroutines are the subject of Python native\ncoroutines and send()  on StackOverflow .\nI still believe classic coroutines and yield from  are worth studying if\nyou want to understand how native coroutines and await  actually support\nconcurrency under the hood. Also, if you want to develop asynchronous\nlibraries—as opposed to applications—you’ll discover that the functions\nthat do the actual work of I/O are generators and classic coroutines, even in\nasyncio . Unfortunately , once you watch David Beazley’ s tutorials and\nread his cookbook examples on the subject, there isn’ t a whole lot of\ncontent out there that goes deep into programming with classic coroutines.\nS O A P B O X\nRaise fr om lambda\nIn programming languages, keywords establish the basic rules of\ncontrol flow and expression evaluation.\nA keyword in a language is like a piece in a board game. In the\nlanguage of Chess, the keywords are ♔ , ♕ , ♖ , ♗ , ♘ , and ♙ . In the\ngame of Go, it’ s ●.\nChess players have six dif ferent types of pieces to implement their\nplans, whereas Go players seem to have only one type of piece.\nHowever , in the semantics of Go, adjacent pieces form lar ger , solid\npieces of many dif ferent shapes, with emer ging properties. Some\narrangements of Go pieces are indestructible. Go is more expressive\nthan Chess. In Go there are 361 possible opening moves, and an\nestimated 10<sup>172</sup> legal positions; for Chess, the numbers\nare 20 opening moves and 10<sup>50</sup> positions.\nAdding a new piece to Chess would be a radical change. Adding a new\nkeyword in a programming language is also a radical change. So it\nmakes sense for language designers to be wary of introducing\nkeywords.\n \nT\na\nb\nl\ne\n \n1\n9\n-\n1\n.  \nN\nu\nm\nb\ne\nr\n \no\nf  \nk\ne\ny\nw\no\nr\nd\ns\n \ni\nn\n \np\nr\no\ng\nr\na\nm\nm\ni\nn\ng\n \nl\na\nn\ng\nu\na\ng\ne\ns\n \nKeywords Language Comment\n \n5 Smalltalk-80 Famous for its minimalist syntax.\n25 Go The language, not the game.\n32 C That’ s ANSI C. C99 has 37 keywords, C1 1 has 44.\n35 Python Python 2.7 had 31 keywords; Python 1.5 had 28.\n41 Ruby Keywords may be used as identifiers (e.g., class  \nis also a method name).\n49 Java As in C, the names of the primitive types ( char , f\nloat , etc.) are reserved.\n60 JavaScript Includes all keywords from Java 1.0, many of  \nwhich are unused .\n65 PHP Since PHP 5.3 , seven keywords were introduced,  \nincluding goto , trait , and yield .\n85 C++ According to cppreference.com , C++1 1 added 10  \nkeywords to the existing 75.\n555 COBOL I did not make this up. See this IBM ILE COBOL  \nmanual .\n∞ Scheme Anyone can define new keywords.\n \nPython 3.0 added nonlocal , promoted None , True , and False  to\nkeyword status, and dropped print  and exec . It’ s very uncommon\nfor a language to drop keywords as it evolves. T able 19-1  lists some\nlanguages, ordered by number of keywords.\nScheme inherited from Lisp a syntactic macro facility that allows\nanyone to create special forms adding new control structures and\nevaluation rules to the language. The user -defined identifiers of those\nforms are called “syntactic keywords.” The Scheme R5RS standard\nstates “There are no reserved identifiers” (page 45 of the standard ), but\na typical implementation such as MIT/GNU Scheme  comes with 34\nsyntactic keywords predefined, such as if , lambda , and define-\nsyntax —the keyword that lets you conjure new keywords.\nI now enjoy Elixir as much as I enjoy Python. Elixir has syntactic\nmacros, on top of a basic syntax that is more readable than the s-\nexpressions of Lisp and Scheme. Elixir frameworks and libraries such\nas Phoenix and Ecto extend the language syntax to build domain-\nspecific languages. For example, this is a database query written in\nElixir using an Ecto macro:\nquery = from u in User, \n          where: u.age > 18 or is_nil(u.email), \n          select: u\n“The V alue Of Syntax?”  is an interesting discussion about extensible\nsyntax and programming language usability . The forum, Lambda the\nUltimate , is a watering hole for programming language geeks.\nPython is like Chess. Scheme and Elixir are like Go (the game).\nNow , back to Python syntax. Guido used to be rather conservative with\nkeywords. It’ s nice to have a small set of them, and adding new\nkeywords potentially breaks a lot of code. But the use of else  in loops\nreveals a recurring problem: the overloading of existing keywords when\na new one would be a better choice. In the context of for , while , and\ntry , a new then  keyword would be preferable to abusing else .\nIn Fluent Python, First Edition  I wrote: “The introduction of yield\nfrom  is particularly worrying. Once again, I believe Python users\nwould be best served by a new keyword.” As I write this five years\nlater , I got so used to yield from  that I don’ t see any problem with it\nany more. Now we have await  too, which works in a similar way but\nis used in dif ferent contexts.\nI am glad Guido approved PEP 492 introducing not only await , but\nalso async  combined to existing keywords to add three new\nstatements to the language: async def , async for  and async\nwith —all of which we will see in Chapter 22 . Using async def  to\ndeclare native coroutines interrupted the long history of overloading of\ndef : it’ s still used to define functions, generators, and classic\ncoroutines—objects that are too dif ferent to share the same declaration\nsyntax. I highly recommended “What Color Is Y our Function?”  by Bob\nNystrom, a post related to this discussion in the context of JavaScript,\nPython, and other languages.\nChaining existing keywords to create new syntax—instead of adding\nsensible, descriptive keywords—avoids breaking code, but has its\ndownsides. I fear one day we may be poring over the meaning of\nraise from lambda .\n1  500 Lines or Less , edited by Michael DiBernardo, chapter A W eb Crawler W ith asyncio\nCoroutines  by A. Jesse Jiryu Davis and Guido van Rossum.\n2  Y ou’ll only see this state in a multithreaded application—or if the generator object calls\ngetgeneratorstate  on itself, which is not useful.\n3  This example is inspired by a snippet from Jacob Holm in the Python-ideas list, message\ntitled “Y ield-From: Finalization guarantees.”  Some variations appear later in the thread, and\nHolm further explains his thinking in message 003912 .\n4  There are several similar decorators published on the W eb. This one is adapted from the\nActiveState recipe Pipeline made of coroutines  by Chaobin T ang, who in turn credits David\nBeazley .\n5  There is an iPython extension called ipython-yf  that enables evaluating yield from\ndirectly in the iPython console. It’ s used to test asynchronous code and works with asyncio .\nIt was submitted as a patch to Python 3.5 but was not accepted. See Issue #22412: T owards an\nasyncio-enabled command line  in the Python bug tracker .\n6  The picture in Figure 19-2  was inspired by a diagram  by Paul Sokolovsky .\n7  From PEP 492, section A wait Expression\n8  Message to Python-Dev: “PEP 380 (yield from a subgenerator) comments”  (March 21, 2009).\n9  In a message to Python-ideas on April 5, 2009 , Nick Coghlan questioned whether the implicit\npriming done by yield from  was a good idea.\n10  Opening sentence of the “Motivation” section in PEP 342 .\n11  See the of ficial documentation for SimPy —not to be confused with the well-known but\nunrelated SymPy , a library for symbolic mathematics.\n12  I am not an expert in taxi fleet operations, so don’ t take my numbers seriously . Exponential\ndistributions are commonly used in DES. Y ou’ll see some very short trips. Just pretend it’ s a\nrainy day and some passengers are taking cabs just to go around the block—in an ideal city\nwhere there are cabs when it rains.\n13  I was the passenger . I realized I for got my wallet.\n14  The verb “to drive” is commonly used to describe the operation of a coroutine: the client code\ndrives the coroutine by sending it values. In Example 19-21 , the client code is what you type in\nthe console.\n15  This is typical of a discrete event simulation: the simulation clock is not incremented by a\nfixed amount on each loop, but advances according to the duration of each event completed.\n16  Since Python 3.7, typing.Generator  and other types that correspond to ABCs in\ncollections.abc  were refactored with a wrapper around the corresponding ABC, so their\ngeneric parameters aren’ t visible in the typing.py  source file. That’ s why I refer to Python 3.6\nsource code here.\n17  Message to thread “Y ield-From: Finalization guarantees”  in the Python-ideas mailing list. The\nDavid Beazley tutorial Guido refers to is “A Curious Course on Coroutines and Concurrency” .\n18  Nowadays even tenured professors agree that W ikipedia is a good place to start studying\npretty much any subject in computer science. Not true about other subjects, but for computer\nscience, W ikipedia rocks.",17705
263-Processes threads and Pythons Infamous GIL.pdf,263-Processes threads and Pythons Infamous GIL,"Chapter 20. Concurrency\nModels in Python\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 20th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nConcurr ency is about dealing with lots of things at once.\nParallelism is about doing lots of things at once.\nNot the same, but r elated.\nOne is about structur e, one is about execution.\nConcurr ency pr ovides a way to structur e a solution to solve a pr oblem\nthat may (but not necessarily) be parallelizable.\n— Rob Pike, Co-inventor of the Go language\nThis chapter is about how to make Python deal with “lots of things at once.”\nThis may involve concurrent or parallel programming—even academics\nwho are keen on jar gon disagree on how to use those terms.  I will adopt\nRob Pike’ s informal definitions quoted above, but note that I’ve found1\n2\nacademic papers and books that claim to be about parallel computing but\nmostly covered concurrency .\nParallelism is a special case of concurrency , in Pike’ s view . All parallel\nsystems are concurrent, but not all concurrent systems are parallel. In the\nearly 2000’ s we used single core machines that handled 100 processes\nconcurrently on GNU Linux. A modern laptop with 4 CPU cores is\nroutinely running more than 200 processes at any given time under normal,\ncasual use. T o execute 200 tasks in parallel you’d need 200 cores. So, in\npractice, most computing is concurrent and not parallel. The OS manages\nhundreds of processes, making sure each has an opportunity to make\nprogress, even if the CPU itself can’ t do more than 4 things at once. That’ s\nwhy Rob Pike titled that talk “Concurrency Is Not Parallelism (It’ s Better).”\nThis chapter assumes no prior knowledge of concurrent or parallel\nprogramming. After a brief conceptual introduction, we will study simple\nexamples to introduce and compare Python’ s core packages for concurrent\nprogramming: threading , multiprocessing , and asyncio .\nThe last 30% of the chapter is a high-level overview of third-party tools,\nlibraries, application servers, and distributed task queues—all of which can\nenhance the performance and scalability of Python applications. These are\nall important topics, but beyond the scope of a book focused on core Python\nlanguage features. Nevertheless, I felt it was important to address these\nthemes in this second edition of Fluent Python , because Python’ s fitness for\nconcurrent and parallel computing is not limited to what the standard library\nprovides. That’ s why Y ouT ube, DropBox, Instagram, Reddit, and others\nwere able to achieve W eb scale when they started, using Python as their\nprimary language.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThis chapter is new in Fluent Python, Second Edition . The spinner\nexamples in “A Concurrent Hello W orld”  previously were in the chapter\nabout asyncio . Here they are improved, and provide the first illustration\nof Python’ s three approaches to concurrency: threads, processes, and native\ncoroutines.\nThe remaining content is new—except for a few paragraphs that originally\nappeared in the chapters on concurrent.futures  and asyncio .\n“The Big Picture”  is dif ferent from the rest of the book: there are no code\nexamples. The goal is to mention important tools that you may want to\nstudy to achieve high-performance concurrency and parallelism beyond\nwhat’ s possible with Python’ s standard library .\nA  B i t  o f  J a r g o n\nLet’ s make sure we are on the same page regarding some core concepts.\nHere are some terms I will use for the rest of this chapter and the next two.\nconcurr ency\nThe ability to handle multiple pending tasks, making progress one at a\ntime or in parallel (not necessarily) so that they all eventually succeed\nor fail. A single-core CPU is capable of concurrency if it runs an OS\nscheduler that interleaves the execution of the pending tasks. Also\nknown as multitasking.\nparallelism\nThe ability to execute multiple computations at the same time. This\nrequires a multi-core CPU, a GPU, or multiple computers in cluster .\npr ocess\nAn instance of a computer program while it is running, using memory\nand a slice of the CPU time. Modern operating systems are able to\nmanage multiple processes concurrently , with each process isolated in\nits own private memory space. Processes communicate via pipes,\nsockets, or memory mapped files—all of which can only carry raw\nbytes, not live Python objects. A process can spawn sub-processes, each\ncalled a child process. These are also isolated from each other and from\nthe parent.\nthr ead\nAn execution path within a single process. When a process starts, it uses\na single thread: the main thread. Using operating system APIs, a process\ncan create more threads that operate concurrently thanks to the\noperating system scheduler . Threads share the memory space of the\nprocess, which holds live Python objects. This allows easy\ncommunication between threads, but can also lead to corrupted data\nwhen more than one thread updates the same object concurrently .\ncontention\nDispute over a limited asset. Resource contention happens when\nmultiple processes or threads try to access a shared resource—such as a\nlock or storage. There’ s also CPU contention, when compute-intensive\nprocesses or threads must wait for their share of CPU time.\nlock\nAn object that threads can use to coordinate and synchronize their\nactions and avoid corrupting data. While updating a shared data\nstructure, a thread should hold an associated lock. This makes other\nwell-behaved threads wait until the lock is released before accessing the\nsame data structure. The simplest type of lock is also known as a mutex\n(for mutual exclusion).\nNow let’ s use some of that jar gon to understand concurrency support in\nPython.\nProcesses, threads, and Python’ s Infamous GIL\nHere is how the concepts we just saw apply to Python programming, in ten\npoints.\n1 . Each instance of the Python interpreter is a process. Y ou can start\nadditional Python processes using the multiprocessing  or\nconcurrent.futures  libraries. Python’ s subprocess\nlibrary is designed to launch processes to run external programs,\nregardless of the languages used to write them.\n2 . The Python interpreter uses a single thread to run the user ’ s\nprogram and the memory garbage collector . Y ou can start\nadditional Python threads using the threading  or\nconcurrent.futures  libraries.\n3 . Access to reference counts and other internal interpreter state is\ncontrolled by a lock, the Global Interpreter Lock (GIL). Only one\nPython thread can hold the GIL at any time. This means that only\none Python thread can execute at any time, regardless of the\nnumber of CPU cores.\n4 . T o prevent a Python thread from holding the GIL indefinitely ,\nPython’ s bytecode interpreter pauses the current Python thread\nevery 5ms by default , releasing the GIL. The thread can then try\nto reacquire the GIL, but if there are other threads waiting for it,\nthe OS scheduler may pick one of them to proceed.\n5 . When we write Python code, we have no control over the GIL. But\na built-in function or an extension written in C—or any language\nthat interfaces at the Python/C API level—can release the GIL\nwhile running time-consuming tasks.\n6 . Every Python standard library function that makes a syscall\nreleases the GIL. This includes all functions that perform disk I/O,\nnetwork I/O, and time.sleep() . Many CPU-intensive\nfunctions in the NumPy/SciPy libraries, as well as the\ncompressing/decompressing functions from the zlib  and bz2\nmodules also release the GIL.\n7 . Extensions that integrate at the Python/C level can also launch\nother non-Python threads that are not af fected by the GIL. Such3\n4\n5\nGIL-free threads generally cannot change Python objects, but they\ncan read from and write to the memory underlying\narray.array  or NumPy arrays, which support the buf fer\nprotocol .\n8 . The ef fect of the GIL on network programming with Python\nthreads is relatively small, because the I/O functions release the\nGIL, and reading or writing to the network always implies high\nlatency—compared to reading and writing to memory .\nConsequently , each individual thread spends a lot of time waiting\nanyway , so their execution can be interleaved without major\nimpact on the overall throughput. That’ s why David Beazley says:\n“Python threads are great at doing nothing.”\n9 . Contention over the GIL slows down compute-intensive Python\nthreads. Sequential, single-threaded code is simpler and faster for\nsuch tasks.\n10 . T o run CPU-intensive Python code on multiple cores, you must use\nmultiple Python processes.\nHere is a good summary from the documentation of the threading\nmodule:\nCPython implementation detail : In CPython, due to the Global\nInterpr eter Lock, only one thr ead can execute Python code at once (even\nthough certain performance-oriented libraries might over come this\nlimitation). If you want your application to make better use of the\ncomputational r esour ces of multi-cor e machines, you ar e advised to use\nmultipr ocessing or concurr ent.futur es.Pr ocessPoolExecutor . However ,\nthr eading is still an appr opriate model if you want to run multiple I/O-\nbound tasks simultaneously .\nThe previous paragraph starts with “CPython implementation detail”\nbecause the GIL is not part of the Python language definition. The Jython\nimplementation does not have a GIL. Unfortunately , Jython is lagging\nbehind—it’ s still tracking Python 2.7. The highly performant PyPy6\n7",10075
264-A Concurrent Hello World.pdf,264-A Concurrent Hello World,,0
265-Spinner with threading.pdf,265-Spinner with threading,"interpreter  also has a GIL in its 2.7 and 3.7 versions—the latest as of June,\n2021.\nEnough concepts for now . Let’ s see some code.\nA  C o n c u r r e n t  H e l l o  W o r l d\nDuring a discussion about threads and how to avoid the GIL, Python\ncontributor Michele Simionato posted an example  that is like a concurrent\nHello W orld : the simplest program to demonstrate how Python can “walk\nand chew gum.”\nSimionato’ s program uses multiprocessing , but I adapted it to\nintroduce threading  and asyncio  as well. Let’ s start with the\nthreading  version, which may look familiar if you’ve studied threads in\nJava or C.\nSpinner with threading\nThe idea of the next few examples is simple: start a function that blocks for\n3 seconds while animating characters in the terminal to let the user know\nthat the program is “thinking” and not stalled.\nAn animated spinner is built by displaying each character in the string\n""\|/-""  in the same screen position.  When the slow computation finishes,\nthe line with the spinner is cleared and the result is shown: Answer: 42 .\nFigure 20-1  shows the output of two versions of the spinning example: first\nwith threads, then with coroutines. If you’re away from the computer ,\nimagine the \  in the last line is spinning.8\nFigur e 20-1. The scripts spinner_thr ead.py and spinner_async.py pr oduce similar output: the r epr of\na spinner object and the text “Answer: 42”. In the scr eenshot, spinner_async.py is still running, and\nthe animated message “/ thinking!” is shown; that line will be r eplaced by “Answer: 42” after 3\nseconds.\nLet’ s review the spinner_thr ead.py  script first. Example 20-1  lists the first\ntwo functions in the script, and Example 20-2  shows the rest.\nExample 20-1. spinner_thr ead.py: the spin and slow functions.\nimport itertools  \nimport time \nfrom threading  import Thread, Event \n \ndef spin(msg: str, done: Event) -> None:  \n \n    for char in itertools .cycle(r'\|/-'):  \n \n        status = f'\r{char} {msg} '  \n \n        print(status, end='', flush=True) \n        if done.wait(.1):  \n \n            break  \n \n    blanks = ' ' * len(status) \n    print(f'\r{blanks} \r', end='')  \n \n \ndef slow() -> int: \n    time.sleep(3)  \n \n    return 42\nThis function will run in a separate thread. The done  ar gument is an\ninstance of threading.Event , a simple way to synchronize threads.\nThis is an infinite loop because itertools.cycle  yields one\ncharacter at a time, cycling through the string forever .\nThe trick for text-mode animation: move the cursor back to the start of\nthe line with the carriage return ASCII control character ( '\r' ).\nThe Event.wait(timeout=None)  method returns True  when the\nevent is set by another thread; if the timeout  elapses, it returns\nFalse . The .1s timeout sets the “frame rate” of the animation to\n10FPS. If you want the spinner to go faster , use a smaller timeout.\nExit the infinite loop.\nClear the status line by overwriting with spaces and moving the cursor\nback to the beginning.\nslow()  will be called by the main thread. Imagine this is a slow API\ncall over the network. Calling sleep  blocks the main thread, but the\nGIL is released so the spinner thread can proceed.\nT I P\nThe first important insight of this example is that time.sleep()  blocks the calling\nthread but releases the GIL, allowing other Python threads to run.\nThe spin  and slow  functions will execute concurrently . The main thread\n—the only thread when the program starts—will start a new thread to run\nspin  and then call slow . By design, there is no API for terminating a\nthread in Python. Y ou must send it a message to shut down.\nThe threading.Event  class is Python’ s simplest signalling mechanism\nto coordinate threads. An Event  instance has an internal boolean flag\nwhich starts as False . Calling Event.set()  sets the flag to True .\nWhile the flag is false, if a thread calls Event.wait() , it is blocked until\nanother thread calls Event.set() , at which time Event.wait()\nreturns True . If a timeout in seconds is given to Event.wait(s) , this\ncall returns False  when the timeout elapses, or returns True  as soon as\nEvent.set()  is called by another thread.\nThe supervisor  function, listed in Example 20-2 , uses an Event  to\nsignal the spin  function to exit.\nExample 20-2. spinner_thr ead.py: the supervisor and main functions.\ndef supervisor () -> int:  \n \n    done = Event()  \n \n    spinner = Thread(target=spin, args=('thinking! ', done))  \n \n    print(f'spinner object: {spinner} ')  \n \n    spinner.start()  \n \n    result = slow()  \n \n    done.set()  \n \n    spinner.join()  \n \n    return result \n \ndef main() -> None: \n    result = supervisor ()  \n \n    print(f'Answer: {result} ') \n \nif __name__  == '__main__ ': \n    main()\nsupervisor  will return the result of slow .\nThe threading.Event  instance is the key to coordinate the\nactivities of the main thread and the spinner thread, as explained below .\nT o create a new Thread , provide a function as the target  keyword\nar gument, and positional ar guments to the target  as a tuple passed\nvia args .",5169
266-Spinner with multiprocessing.pdf,266-Spinner with multiprocessing,"Display the spinner  object. The output is <Thread(Thread-1,\ninitial)> , where initial  is the state of the thread—meaning it\nhas not started.\nStart the spinner  thread.\nCall slow , which blocks the main thread. Meanwhile, the secondary\nthread is running the spinner animation.\nSet the Event  flag to True ; this will terminate the for  loop inside the\nspin  function.\nW ait until the spinner  thread finishes.\nRun the supervisor  function. I wrote separate main  and\nsupervisor  functions to make this example look more like the\nasyncio  version in Example 20-4 .\nWhen the main thread sets the done  event, the spinner  thread will\neventually notice and exit cleanly .\nNow let’ s take a look at a similar example using the multiprocessing\npackage.\nSpinner with multiprocessing\nThe multiprocessing  package supports running concurrent tasks in\nseparate Python processes instead of threads. When you create a\nmultiprocessing.Process  instance, a whole new Python interpreter\nis started as a child process in the background. Since each Python process\nhas its own GIL, this allows your program to use all available CPU cores.\nW e’ll see practical ef fects in “A Homegrown Process Pool” , but for this\nsimple program it makes no real dif ference.\nThe point of this section is to introduce multiprocessing  and show\nthat its API emulates the threading  API, making it easy to convert\nsimple programs from threads to processes, as shown in spinner_pr oc.py\n( Example 20-3 ).\nExample 20-3. spinner_pr oc.py: only the changed parts ar e shown.\nEverything else is the same as spinner_thr ead.py .\nimport itertools  \nimport time \nfrom multiprocessing  import Process, Event  \n \nfrom multiprocessing  import synchronize      \n \n \ndef spin(msg: str, done: synchronize .Event) -> None:  \n \n \n# [snip] the rest of spin and slow functions are unchanged from  \nspinner_thread.py  \n \ndef supervisor () -> int: \n    done = Event() \n    spinner = Process(target=spin,               \n  \n                      args=('thinking! ', done)) \n    print(f'spinner object: {spinner} ')          \n  \n    spinner.start() \n    result = slow() \n    done.set() \n    spinner.join() \n    return result \n \n# [snip] main function is unchanged as well\nThe basic multiprocessing  API imitates the threading  API,\nbut type hints and mypy  expose this dif ference:\nmultiprocessing.Event  is a function (not a class like\nthreading.Event ) which returns a synchronize.Event\ninstance…\n…forcing us to import multiprocessing.synchronize …\n…to write this type hint.\nBasic usage of the Process  class is similar to Thread .",2639
267-Spinner with asyncio.pdf,267-Spinner with asyncio,"The spinner  object is displayed as <Process name='Process-\n1' parent=14868 initial> , were 14868  is the process id of\nthe Python instance running spinner_proc.py .\nThe basic API of threading  and multiprocessing  are similar , but\ntheir implementation is very dif ferent and multiprocessing  has a\nmuch lar ger API to handle the added complexity of multi-process\nprogramming. For example, one challenge when converting from threads to\nprocesses is how to communicate between processes that are isolated by the\noperating system and can’ t share Python objects. This means that objects\ncrossing process boundaries have to be serialized and deserialized, which\ncreates overhead. In Example 20-3  the only data that crosses the process\nboundary is the Event  state, which is implemented with a low-level OS\nsemaphore in the C code underlying the multiprocessing  module.\nN O T E\nThe semaphore is a fundamental building block that can be used to implement other\nsynchronization mechanisms. Python provides dif ferent semaphore classes for use with\nthreads, processes and coroutines. W e’ll see asyncio.Semaphore  in “Using\nasyncio.as_completed and a semaphore”  ( Chapter 22 ).\nNow let’ s see how the same behavior can be achieved with coroutines\ninstead of threads or processes.\nSpinner with asyncio\nN O T E\nChapter 22  is entirely devoted to asynchronous programming with coroutines. This is\njust a high-level introduction to contrast this approach with the traditional threading and\nmultiprocessing concurrency models. As such, we will overlook many details.\nIt is the job of OS schedulers to allocate CPU time to drive threads and\nprocesses. In contrast, coroutines are driven by an application-level event\nloop that manages a queue of pending coroutines, drives them one by one,\nmonitors events triggered by I/O operations initiated by coroutines, and\npasses control back to the corresponding coroutine when each event\nhappens. The event loop and the library coroutines and the user coroutines\nall execute in a single thread. Therefore, any time spent in a coroutine slows\ndown the event loop—and all other coroutines.\nN O T E\nIn the taxi simulator of Example 19-23 , the taxi_process  classic coroutines were\ndriven by a main loop in the Simulator.run  method. That main loop was an event\nloop, except that it handled simulation events like “drop of f passenger” instead of\nsystem events triggered by I/O and timers. The event loop of asyncio  is more\ncomplex than that simulation loop, but the idea is the same. So if you want to\nunderstand how concurrency with coroutines works, studying taxi_sim.py  may be a\ngood starting point.\nThe coroutine version of the spinner program is easier to understand if we\nstart from the main  function, then study the supervisor . That’ s what\nExample 20-4  shows.\nExample 20-4. spinner_async.py: the main function and supervisor\ncor outine\ndef main() -> None:  \n \n    result = asyncio.run(supervisor ())  \n \n    print(f'Answer: {result} ') \n \nasync def supervisor () -> int:  \n \n    spinner = asyncio.create_task (spin('thinking! '))  \n \n    print(f'spinner object: {spinner} ')  \n \n    result = await slow()  \n \n    spinner.cancel()  \n \n    return result \n \nif __name__  == '__main__ ': \n    main()\nmain  is the only regular function in this program—the others are\ncoroutines.\nThe asyncio.run  function starts the event loop to drive the\ncoroutine that will eventually set the other coroutines in motion. The\nmain  function will stay blocked until supervisor  returns. The\nreturn value of supervisor  will be the return value of\nasyncio.run .\nNative coroutines are defined with async def .\nasyncio.create_task  schedules the eventual execution of spin ,\nimmediately returning an instance of asyncio.Task .\nThe repr  of the spinner  object looks like <Task pending\nname='Task-2' coro=<spin() running at\n/path/to/spinner_async.py:11>> .\nThe await  keyword calls slow , blocking supervisor  until slow\nreturns. The return value of slow  will be assigned to result .\nThe Task.cancel  method raises a CancelledError  exception\ninside the spin  coroutine, as we’ll see in Example 20-5 .\nExample 20-4  demonstrates the three main ways of running a coroutine:\nasyncio.run(coro())\nCalled from a regular function to drive a coroutine object which usually\nis the entry point for all the asynchronous code in the program, like the\nsupervisor  in this example. This call blocks until the body of coro\nreturns. The return value of the run()  call is whatever the body of\ncoro  returns.\nasyncio.create_task(coro())\nCalled from a coroutine to schedule another coroutine to execute\neventually . This call does not suspend the current coroutine. It returns a\nTask  instance, an object that wraps the coroutine object and provides\nmethods to control and query its state.\nawait coro()\nCalled from a coroutine to transfer control to the coroutine object\nreturned by coro() . This suspends the current coroutine until the body\nof coro  returns. The value of the await expression is whatever body of\ncoro  returns.\nN O T E\nRemember: invoking a coroutine as coro()  immediately returns a coroutine object,\nbut does not run the body of the coro  function. Driving the body of coroutines is the\njob of the event loop, which invokes the .send()  method on the coroutine objects,\njust like we drove classic coroutines built from generators in Chapter 19 .\nNow let’ s study the spin  and slow  coroutines in Example 20-5 .\nExample 20-5. spinner_async.py: the spin and slow cor outines\nimport asyncio \nimport itertools  \n \nasync def spin(msg: str) -> None:  \n \n    for char in itertools .cycle(r'\|/-'): \n        status = f'\r{char} {msg} ' \n        print(status, flush=True, end='') \n        try: \n            await asyncio.sleep(.1)  \n \n        except asyncio.CancelledError :  \n \n            break \n    blanks = ' ' * len(status) \n    print(f'\r{blanks} \r', end='') \n \nasync def slow() -> int: \n    await asyncio.sleep(3)  \n \n    return 42\nW e don’ t need the Event  ar gument that was used to signal that slow\nhad completed its job in spinner_thr ead.py  ( Example 20-1 ).\nUse await asyncio.sleep(.1)  instead of time.sleep(.1) ,\nto pause without blocking other coroutines. See explanation after this\nexample.\nasyncio.CancelledError  is raised when the cancel  method is\ncalled on the Task  controlling this coroutine. T ime to exit the loop.\nThe slow  coroutine also uses await asyncio.sleep  instead of\ntime.sleep .\nExperiment: Break the Spinner for an Insight\nHere is an experiment I recommend to understand how spinner_async.py\nworks. Import the time  module, then go to the slow  coroutine and replace\nthe line await asyncio.sleep(3)  with a call to time.sleep(3) ,\nlike this:\nExample 20-6. spinner_async.py: r eplacing await\nasyncio.sleep(3)  with time.sleep(3)\nasync def slow() -> int: \n    time.sleep(3) \n    return 42\nW atching the behavior is more memorable than reading about it. Go ahead,\nI’ll wait.\nWhen you run the experiment, this is what you see:\n1 . The spinner object is shown, similar to this: <Task pending\nname='Task-2' coro=<spin() running at\n/path/to/spinner_async.py:12>> .\n2 . The spinner never appears. The program hangs for 3 seconds.\n3 . ""Answer: 42 "" is displayed and the program ends.\nT o understand what is happening, recall that Python code using asyncio\nhas only one flow of execution, unless you’ve explicitly started additional\nthreads or processes. That means only one coroutine executes at any point\nin time. Concurrency is achieved by control passing from one coroutine to\nanother . Let’ s focus on what happens in the supervisor  and slow\ncoroutines during the proposed experiment:\nExample 20-7. spinner_async_experiment.py: the supervisor and slow\ncor outines\nasync def slow() -> int: \n    time.sleep(3)  \n \n    return 42 \n \nasync def supervisor () -> int: \n    spinner = asyncio.create_task (spin('thinking! '))  \n \n    print(f'spinner object: {spinner} ')  \n \n    result = await slow()  \n \n    spinner.cancel()  \n \n    return result\nThe spinner  task is created, to eventually drive the execution of\nspin .\nThe display shows the Task  is “pending”.\nThe await  expression transfers control to the slow  coroutine.\ntime.sleep(3)  blocks for 3 seconds; nothing else can happen in the\nprogram, because the main thread is blocked—and it is the only thread.\nThe operating system will continue with other activities. After 3\nseconds, sleep  unblocks, and slow  returns.\nRight after slow  returns, the spinner  task is cancelled. The flow of\ncontrol never reached the body of the spin  coroutine.\nThe spinner_async_experiment.py  teaches an important lesson:",8796
268-Supervisors Side-by-side.pdf,268-Supervisors Side-by-side,"W A R N I N G\nNever use time.sleep(…)  in asyncio  coroutines unless you want to pause your\nwhole program. If a coroutine needs to spend some time doing nothing, it should\nawait asyncio.sleep(DELAY) . This yields control back to the asyncio  event\nloop, which can drive other pending coroutines.\nSupervisors Side-by-side\nThe line count of spinner_thr ead.py  and spinner_async.py  is nearly the\nsame. The supervisor  functions are the heart of these examples. Let’ s\ncompare them in detail. Example 20-8  lists only the supervisor  from\nExample 20-2 .\nExample 20-8. spinner_thr ead.py: the thr eaded supervisor function\ndef supervisor () -> int: \n    done = Event() \n    spinner = Thread(target=spin, \n                     args=('thinking!' , done)) \n    print('spinner object:' , spinner) \n    spinner.start() \n    result = slow() \n    done.set() \n    spinner.join() \n    return result\nFor comparison, Example 20-9  shows the supervisor  coroutine from\nExample 20-4 .\nExample 20-9. spinner_async.py: the asynchr onous supervisor cor outine\nasync def supervisor () -> int: \n    spinner = asyncio.create_task (spin('thinking!' )) \n    print('spinner object:' , spinner) \n    result = await slow() \n    spinner.cancel() \n    return result\nHere is a summary of the dif ferences and similarities to note between the\ntwo supervisor  implementations:\nAn asyncio.Task  is roughly the equivalent of a\nthreading.Thread .\nA Task  drives a coroutine object, and a Thread  invokes a\ncallable.\nA coroutine yields control explicitly with the await  keyword.\nY ou don’ t instantiate Task  objects yourself, you get them by\npassing a coroutine to asyncio.create_task(…) .\nWhen asyncio.create_task(…)  returns a Task  object, it is\nalready scheduled to run, but a Thread  instance must be\nexplicitly told to run by calling its start  method.\nIn the threaded supervisor , slow  is a plain function and is\ndirectly invoked by the main thread. In the asynchronous\nsupervisor , slow  is a coroutine driven by await .\nThere’ s no API to terminate a thread from the outside; instead, you\nmust send a signal—like setting the done  Event  object. For\ntasks, there is the Task.cancel()  instance method, which\nraises CancelledError  at the await  expression where the\ncoroutine body is currently suspended.\nThe supervisor  coroutine must be started with asyncio.run\nin the main  function.\nThis comparison should help you understand how concurrent jobs are\norchestrated with asyncio , in contrast to how it’ s done with the\nThreading  module which may be more familiar to you.\nOne final point related to threads versus coroutines: if you’ve done any\nnontrivial programming with threads, you know how challenging it is to\nreason about the program because the scheduler can interrupt a thread at\nany time. Y ou must remember to hold locks to protect the critical sections\nof your program, to avoid getting interrupted in the middle of a multistep\noperation—which could leave data in an invalid state.",3031
269-Quick Quiz.pdf,269-Quick Quiz,"W ith coroutines, your code is protected against interruption by default. Y ou\nmust explicitly await  to let the rest of the program run. Instead of holding\nlocks to synchronize the operations of multiple threads, coroutines are\n“synchronized” by definition: only one of them is running at any time.\nWhen you want to give up control, you use await  to yield control back to\nthe scheduler . That’ s why it is possible to safely cancel a coroutine: by\ndefinition, a coroutine can only be cancelled when it’ s suspended at an\nawait  expression, so you can perform cleanup by handling the\nCancelledError  exception.\nThe time.sleep()  call blocks but does nothing. Now we’ll experiment\nwith a CPU-intensive call to get a better understanding of the GIL, as well\nas the ef fect of CPU-intensive functions in asynchronous code.\nT h e  R e a l  I m p a c t  o f  t h e  G I L\nIn the threading code ( Example 20-1 ), you can replace the\ntime.sleep(3)  call in the slow  function with an HTTP client request\nfrom your favorite library , and the spinner will keep spinning. That’ s\nbecause a well-designed network library will release the GIL while waiting\nfor the network.\nY ou can also replace the asyncio.sleep(3)  expression in the slow\ncoroutine to await  for a response from a well-designed asynchronous\nnetwork library , because such libraries provide coroutines that yield control\nback to the event loop while waiting for the network. Meanwhile, the\nspinner will keep spinning.\nW ith CPU intensive code, the story is dif ferent. Consider the function\nis_prime  in Example 20-10 , which returns True  if the ar gument is a\nprime number , False  if it’ s not.\nExample 20-10. primes.py: an easy to r ead primality check, fr om Python’ s\nProcessPoolExecutor  example .\ndef is_prime (n: int) -> bool: \n    if n < 2: \n        return False \n    if n == 2: \n        return True \n    if n % 2 == 0: \n        return False \n \n    root = math.isqrt(n) \n    for i in range(3, root + 1, 2): \n        if n % i == 0: \n            return False \n    return True\nThe call is_prime(5_000_111_000_222_021)  takes about 3.3s on\nthe company laptop I am using now .\nQuick Quiz\nGiven what we’ve seen so far , please take the time to consider the following\nthree-part question. One part of the answer is tricky (at least it was for me).\nWhat would happen to the spinner animation if made the following\nchanges, assuming that n = 5_000_111_000_222_021 —that\nprime which my machine takes 3.3s to verify:\n1 . In spinner_pr oc.py , r eplace time.sleep(3)  with a call to\nis_prime(n) ?\n2 . In spinner_thr ead.py , r eplace time.sleep(3)  with a call to\nis_prime(n) ?\n3 . In spinner_async.py , r eplace await asyncio.sleep(3)\nwith a call to is_prime(n) ?\nBefore you run the code or read on, I recommend figuring out the answers\non your own. Then, you may want to copy and modify the spinner *.py_\nexamples as suggested.\nNow the answers, from easier to hardest.\n1. Answer for multiprocessing9\nThe spinner is controlled by a child process, so it continues spinning while\nthe primality test is computed by the parent process.\n2. Answer for threading\nThe spinner is controlled by a secondary thread, so it continues spinning\nwhile the primality test is computed by the main thread.\nI did not get this answer right at first: I was expecting the spinner to freeze\nbecause I overestimated the impact of the GIL .\nIn this particular example the spinner keeps spinning because Python\nsuspends the running thread every 5ms (by default), making the GIL\navailable to other pending threads. Therefore, the main thread running\nis_prime  is interrupted every 5ms, allowing the secondary thread to\nwake up and iterate once through the for  loop, until it calls the wait\nmethod of the done  event, at which time it will release the GIL. The main\nthread will then grab the GIL, and the is_prime  computation will\nproceed for another 5ms.\nThis does not have a visible impact on the running time of this specific\nexample because the spin  function quickly iterates once and releases the\nGIL as it waits for the done  event, so there is not much contention for the\nGIL. The main thread running is_prime  will have the GIL most of the\ntime.\nW e got away with a compute intensive task using threading in this simple\nexperiment because there are only two threads: one hogging the CPU, and\nthe other waking up only 10 times per second to update the spinner .\nBut you if you have two ore more threads vying for a lot of CPU time, your\nprogram will be slower than sequential code.\n3. Answer for asyncio\nIf you call is_prime(5_000_111_000_222_021)  in the slow\ncoroutine of the spinner_async.py  example, the spinner will never appear .\nThe ef fect would be the same we had in Example 20-6 , when we replaced10\nawait asyncio.sleep(3)  with time.sleep(3) : no spinning at\nall. The flow of control will pass from supervisor  to slow , and then to\nis_prime . When is_prime  returns, slow  returns as well, and\nsupervisor  resumes, cancelling the spinner  task before it is executed\neven once. The program appears frozen for about 3s, then shows the\nanswer .\nP O W E R  N A P P I N G  W I T H  SLEEP(0)\nOne way to keep the spinner alive is to rewrite is_prime  as a\ncoroutine, and periodically call asyncio.sleep(0)  in an await\nexpression to yield control back to the event loop, like this:\nE x a m p l e  2 0 - 1 1 .  s p i n n e r _ a s y n c _ n a p . p y :  is_prime  i s  n o w  a\nc o r o u t i n e\nasync def is_prime (n): \n    if n < 2: \n        return False \n    if n == 2: \n        return True \n    if n % 2 == 0: \n        return False \n \n    root = math.isqrt(n) \n    for i in range(3, root + 1, 2): \n        if n % i == 0: \n            return False \n        if i % 100_000 == 1:  \n \n            await asyncio.sleep(0) \n \n    return True\nMicro-optimization: bind sleep  to asyncio.sleep  to avoid\nthe attribute lookup inside the loop.\nawait sleep(0)  once every 100,000 iterations.\nIssue #284  in the asyncio  repository has an informative discussion\nabout the use of asyncio.sleep(0) .\nHowever , be aware this will slow down is_prime , and—more\nimportantly—will still slow down the event loop and your whole\nprogram with it. When I used await asyncio.sleep(0)  every\n100,000 iterations, the spinner was smooth but the program ran in 4.9s\non my machine, almost 50% longer than the original\nprimes.is_prime  function by itself with the same ar gument\n(5_000_1 1 1_000_222_021).",6548
270-A Homegrown Process Pool.pdf,270-A Homegrown Process Pool,"Using await asyncio.sleep(0)  should be considered a stopgap\nmeasure before you refactor your asynchronous code to delegate CPU-\nintensive computations to another process. W e’ll see one way of doing\nthat with asyncio.loop.run_in_executor , covered in\nChapter 22 . Another option would be a task queue, which we’ll briefly\ndiscuss in “Distributed task queues” .\nSo far , we’ve only experimented with a single call to a CPU-intensive\nfunction. The next section presents concurrent execution of multiple CPU-\nintensive calls.\nA  H o m e g r o w n  P r o c e s s  P o o l\nW A R N I N G\nI wrote this section to demonstrate the ef fect of multiple processes for CPU intensive\ntasks, and the common pattern of using queues to distribute tasks and collect results.\nChapter 21  will show a simpler way of distributing tasks to processes: a\nProcessPoolExecutor  from the concurrent.futures  package, which uses\nqueues internally .\nIn this section we’ll write programs to compute the primality of a sample of\n20 integers, from 2 to 9,999,999,999,999,999—i.e. 10 -1, or more than\n2 . The sample includes small and lar ge primes, as well as composite\nnumbers with small and lar ge prime factors.\nThe sequential.py  program provides the performance baseline. Here is a\nsample run:\n$ python3 sequential.py  \n               2  P  0.000001s  \n 142702110479723   P  0.568328s  \n 299593572317531   P  0.796773s  \n3333333333333301   P  2.648625s  \n3333333333333333      0.000007s  \n3333335652092209      2.672323s  16\n53\n4444444444444423   P  3.052667s  \n4444444444444444      0.000001s  \n4444444488888889      3.061083s  \n5555553133149889      3.451833s  \n5555555555555503   P  3.556867s  \n5555555555555555      0.000007s  \n6666666666666666      0.000001s  \n6666666666666719   P  3.781064s  \n6666667141414921      3.778166s  \n7777777536340681      4.120069s  \n7777777777777753   P  4.141530s  \n7777777777777777      0.000007s  \n9999999999999917   P  4.678164s  \n9999999999999999      0.000007s  \nTotal time: 40.31\nThe results are shown in three columns:\n1 . the number to be checked;\n2 . P  if it’ s a prime number , blank if not;\n3 . elapsed time for checking the primality for that specific number .\nIn this example, the total time is approximately the sum of the times for\neach check—but it is computed separately , as you can see in Example 20-\n12 .\nExample 20-12. sequential.py: sequential primality check for a small\ndataset\n#!/usr/bin/env python3  \n \n"""""" \nsequential.py: baseline for comparing sequential, multiprocessing,  \nand threading code for CPU-intensive work.  \n"""""" \n \nfrom time import perf_counter  \nfrom typing import NamedTuple  \n \nfrom primes import is_prime , NUMBERS \n \nclass Result(NamedTuple ):  \n \n    prime: bool \n    elapsed: float",2808
271-Code for the Multi-core Prime Checker.pdf,271-Code for the Multi-core Prime Checker,"def check(n: int) -> Result:  \n \n    t0 = perf_counter () \n    prime = is_prime (n) \n    return Result(prime, perf_counter () - t0) \n \ndef main() -> None: \n    print(f'Checking {len(NUMBERS)} numbers sequentially: ') \n    t0 = perf_counter () \n    for n in NUMBERS:  \n \n        prime, elapsed = check(n) \n        label = 'P' if prime else ' ' \n        print(f'{n:16}  {label} {elapsed:9.6f}s ') \n \n    elapsed = perf_counter () - t0  \n \n    print(f'Total time: {elapsed:.2f}s ') \n \nif __name__  == '__main__ ': \n    main()\nThe check  function (below) returns a Result  tuple with the boolean\nvalue of the is_prime  call and the elapsed time.\ncheck(n)  calls is_prime(n)  and computes the elapsed time to\nreturn a Result .\nFor each number in the sample, we call check  and display the result.\nCompute and display the total elapsed time.\nProcess-based Solution\nThe next example, pr ocs.py , shows the use of multiple processes to\ndistribute the primality checks across multiple CPU cores. These are the\ntimes I get with pr ocs.py :\n$ python3 procs.py  \nChecking 20 numbers with 12 processes:  \n               2  P  0.000002s  \n3333333333333333      0.000021s  \n4444444444444444      0.000002s  \n5555555555555555      0.000018s  \n6666666666666666      0.000002s  \n 142702110479723   P  1.350982s  \n7777777777777777      0.000009s  \n 299593572317531   P  1.981411s  \n9999999999999999      0.000008s  \n3333333333333301   P  6.328173s  \n3333335652092209      6.419249s  \n4444444488888889      7.051267s  \n4444444444444423   P  7.122004s  \n5555553133149889      7.412735s  \n5555555555555503   P  7.603327s  \n6666666666666719   P  7.934670s  \n6666667141414921      8.017599s  \n7777777536340681      8.339623s  \n7777777777777753   P  8.388859s  \n9999999999999917   P  8.117313s  \nTotal time: 9.58s\nThe last line of the output shows that pr ocs.py  was 4.2 times faster than\nsequential.py .\nUnderstanding the Elapsed T imes\nNote that the elapsed time in the first column is for checking that specific\nnumber . For example, is_prime(7777777777777753)  took almost\n8.4s to return True . Meanwhile, other processes were checking other\nnumbers in parallel.\nThere were 20 numbers to check. I wrote pr ocs.py  to start a number of\nworker processes equal to the number of CPU cores, as determined by\nmultiprocessing.cpu_count() .\nThe total time in this case is much less than sum of the elapsed time for the\nindividual checks. There is some overhead in spinning up processes and in\ninter -process communication, so the end result is that the multiprocess\nversion is only about 4.2 times faster than the sequential. That’ s good, but a\nlittle disappointing considering the code launches 12 processes to use all\ncores on this laptop.\nN O T E\nThe multiprocessing.cpu_count()  function returns 12  on the MacBook Pro\nI’m using to write this chapter . It’ s actually a 6-CPU Core-i7, but the OS reports 12\nCPUs because of hyper -threading, an Intel technology which executes 2 threads per\ncore. However , hyper -threading works better when one of the threads is not working as\nhard as the other thread in the same core—perhaps the first is stalled waiting for data\nafter a cache miss, and the other is crunching numbers. Anyway , there’ s no free lunch:\nthis laptop performs like a 6-CPU machine for compute-intensive work that doesn’ t use\na lot of memory—like that simple primality test. I’m not complaining, just saying.\nCode for the Multi-core Prime Checker\nWhen we delegate computing to threads or processes, our code does not call\nthe worker function directly , so we can’ t simply get a return value. Instead,\nthe worker is driven by the thread or process library , and it eventually\nproduces a result which needs to be stored somewhere. Coordinating\nworkers and collecting results are common uses of queues in concurrent\nprogramming (and also in distributed systems, by the way).\nQueues are data structures that—usually—enforce FIFO ordering: first in,\nfirst out. Queues need to be implemented according to the underlying\nconcurrency model: the queue  package in Python’ s standard library\nprovides queue classes to support threads, while the multiprocessing\nand asyncio  packages implement their own queue classes. The queue\nand asyncio  packages also include queues that are not FIFO:\nLifoQueue  and PriorityQueue .\nMuch of the new code in pr ocs.py  has to do with setting up and using\nqueues. The top of the file is in Example 20-13 .\nW A R N I N G\nSimpleQueue  was added to multiprocessing  in Python 3.9. If you’re using an\nearlier version of Python, you can replace SimpleQueue  with Queue  in this example.\nExample 20-13. pr ocs.py: multipr ocess primality check; imports, types and\nfunctions\nimport sys \nfrom time import perf_counter  \nfrom typing import NamedTuple  \nfrom multiprocessing  import Process, SimpleQueue , cpu_count   \n \nfrom multiprocessing  import queues  \n \n \nfrom primes import is_prime , NUMBERS \n \nclass PrimeResult (NamedTuple ):  \n \n    n: int \n    prime: bool \n    elapsed: float \n \nJobQueue  = queues.SimpleQueue [int]  \n \nResultQueue  = queues.SimpleQueue [PrimeResult ]  \n \n \ndef check(n: int) -> PrimeResult :  \n \n    t0 = perf_counter () \n    res = is_prime (n) \n    return PrimeResult (n, res, perf_counter () - t0) \n \ndef worker(jobs: JobQueue , results: ResultQueue ) -> None:  \n \n    while n := jobs.get():  \n \n        results.put(check(n))  \nT rying to emulate threading , multiprocessing  provides\nmultiprocessing.SimpleQueue , but this is a method bound to a\npre-defined instance of a lower -level BaseContext  class. W e must\ncall this SimpleQueue  to build a queue, but it can’ t be used in type\nhints.\nmultiprocessing.queues  includes the SimpleQueue  class we\nneed for type hints.\nPrimeResult  includes the number checked for primality . Keeping n\ntogether with the other result fields simplifies displaying results later .\nW e’ll use a SimpleQueue  to send numbers to the processes that will\ndo the work.\nA second SimpleQueue  will collect the results. The values in the\nqueue will be tuples made of the number to be tested for primality , and a\nResult  tuple.\nThis is similar to sequential.py .\nworker  gets a queue with the numbers to be checked, and another to\nput results.\nIn this code, I use the number 0  as a sentinel: a signal for the worker to\nfinish. If n  is not 0 , proceed with the loop.\nInvoke the primality check and enqueue PrimeResult .\nW H A T ’ S  A  G O O D  P O I S O N  P I L L ?\nThe worker  function in Example 20-13  follows a common pattern in concurrent\nprogramming: looping indefinitely while taking items from a queue and processing each\nwith a function that does the actual work. The loop ends when the queue produces a\nsentinel. In this pattern, the sentinel that kills the worker is sometimes called a “poison\npill”.\nBesides None , calling object()  is a common way to get a unique value to use as\nsentinel. However , this does not work across processes, because when you\npickle.dump  and pickle.load  an instance of object , the unpickled instance is\ndistinct from the original and doesn’ t compare equal. If None  can occur in the stream, a\ngood alternative is ... , the Ellipsis  built-in object, which survives serialization\nwithout losing its identity .\nNow let’ s study the main  function of pr ocs.py  in Example 20-14 .\nExample 20-14. pr ocs.py: multipr ocess primality check; main function\ndef main() -> None: \n    if len(sys.argv) < 2:  \n \n        workers = cpu_count () \n    else: \n        workers = int(sys.argv[1]) \n \n    print(f'Checking {len(NUMBERS)} numbers with {workers}  11\n12\nprocesses: ') \n \n    jobs: JobQueue  = SimpleQueue () \n \n    results: ResultQueue  = SimpleQueue () \n    t0 = perf_counter () \n \n    for n in NUMBERS:  \n \n        jobs.put(n) \n \n    for _ in range(workers): \n        proc = Process(target=worker, args=(jobs, results))  \n \n        proc.start()  \n \n        jobs.put(0)  \n \n \n    while True: \n        n, prime, elapsed = results.get()  \n \n        label = 'P' if prime else ' ' \n        print(f'{n:16}  {label} {elapsed:9.6f}s ')  \n \n        if jobs.empty():  \n \n            break \n \n    elapsed = perf_counter () - t0 \n    print(f'Total time: {elapsed:.2f}s ') \n \nif __name__  == '__main__ ': \n    main()\nIf no command line ar gument is given, set the number of workers to the\nnumber of CPU cores; otherwise, create as many workers as given in\nthe first ar gument.\njobs  and results  are the queues described in Example 20-13 .\nEnqueue the numbers to be checked in jobs .\nFork a child process for each worker . Each child will run the loop inside\nits own instance of the worker  function, until it fetches a 0  from the\njobs  queue.\nStart the child process.\nEnqueue one 0  for each worker as a sentinel.\nGet the checked number n  and the Result . Calling .get()  on a\nqueue blocks until there is an item in the queue. It’ s also possible to\nmake this unblocking, or set a timeout. See the SimpleQueue.get\ndocumentation for details.\nThe results will not come back in the same order we submitted the jobs,\nso we needed to put n  in each PrimeResult  tuple to make this\nprint  call. Otherwise, we’d have no way to know which result\nbelonged to each number .\nExit the loop when the jobs  queue is empty .\nIn this example, it’ s safe to exit the last loop when the jobs  queue is empty\nbecause the last item put in that queue is a sentinel. Therefore, when a\nworker gets that sentinel, all the other workers got their sentinels as well,\nand no more inter -process communication will happen. If the last item in\njobs  were a big prime, it could happen that jobs  is empty but a worker is\nstill running.\nN O T E\nIf the main process exits before all workers are done, you may see confusing tracebacks\non FileNotFoundError  exceptions caused by an internal lock in\nmultiprocessing . Debugging concurrent code is always hard, and debugging\nmultiprocessing  is even harder , because of all the complexity behind the thread-\nlike façade. Fortunately , the ProcessPoolExecutor  we’ll meet in Chapter 21  is\nsimpler and more robust than this example.\nExperimenting with More or Less W orkers\nY ou may want try running pr ocs.py  passing ar guments to set the number of\nworker processes. For example, this command…\n$ python3 procs.py 2\n…will launch two worker processes, producing results almost twice as fast\nas sequential.py —if your machine has at least two cores and is not too busy\nrunning other programs.\nI ran pr ocs.py  12 times with 1 to 20 processes, totalling 240 runs. Then I\ncomputed the median time for all runs with the same number of processes,\nand plotted Figure 20-2 .\nFigur e 20-2. Median run times for each number of pr ocesses fr om 1 to 20. Highest median time was\n40.81s, with 1 pr ocess. Lowest median was 10.39, with 6 pr ocesses, indicated by the dotted line.\nIn this 6-core laptop, the lowest median time was with 6 processes: 10.39s\n—marked by the dotted line in Figure 20-2 . I expected the run time to\nincrease after 6 processes due to CPU contention, and it reaches a local\nmaximum of 12.51s at 10 processes. I did not expect and I can’ t explain\nwhy the performance improves at 1 1 processes and stays almost flat from\n13 to 20 processes, with median times only slightly higher than the lowest\nmedian time at 6 processes.",11507
272-Thread-based Non-solution.pdf,272-Thread-based Non-solution,,0
273-Server-side WebMobile Development.pdf,273-Server-side WebMobile Development,"Thread-based Non-solution\nI also wrote thr eads.py , a version of pr ocs.py  using threading  instead of\nmultiprocessing . The code is very similar—as is usually the case\nwhen converting simple examples between these two APIs. . Due to the\nGIL and the compute-intensive nature of is_prime , the threaded version\nis slower than the sequential code, and it gets slower as the number of\nthreads increase, because of CPU contention and the cost of context\nswitching: to switch to a new thread, the OS needs to save CPU registers\nand update the program counter and stack pointer—triggering expensive\nside-ef fects like invalidating caches and swapping memory pages.\nThe next two chapters will cover more about concurrent programming in\nPython, using the high-level concurrent.futures  library to manage\nthreads and processes ( Chapter 21 ) and the asyncio  library for\nasynchronous programming ( Chapter 22 ).\nThe remaining sections in this chapter aim to answer the question:\nGiven the limitations discussed so far , how is Python thriving in a multi-\ncor e world?\nT h e  B i g  P i c t u r e\nConsider this citation from the widely quoted article The Fr ee Lunch Is\nOver  by Herb Sutter:\nThe major pr ocessor manufactur ers and ar chitectur es, fr om Intel and\nAMD to Spar c and PowerPC, have run out of r oom with most of their\ntraditional appr oaches to boosting CPU performance. Instead of driving\nclock speeds and straight-line instruction thr oughput ever higher , they\nar e instead turning en masse to hyperthr eading and multicor e\nar chitectur es.\nWhat Sutter calls the “free lunch” was the trend of software getting faster\nwith no additional developer ef fort because CPUs were executing\nsequential code faster , year after year . Since 2004, that is no longer true:13\n14\n15\nclock speeds and execution optimizations reached a plateau, and now any\nsignificant increase in performance must come from leveraging multiple\ncores or hyper -threading, advances that only benefit code that is written for\nconcurrent execution.\nPython’ s story started in the early 1990’ s, when CPUs were still getting\nexponentially faster at sequential code execution. No talk about multi-core\nCPUs except in supercomputers back then. At the time, the decision to have\na GIL was a no-brainer . The GIL makes the interpreter faster when running\non a single core, and its implementation simpler .  The GIL also makes it\neasier to write simple extensions through the Python/C API.\nN O T E\nI just wrote “simple extensions” because an extension does not need to deal with the\nGIL at all. A function written in C or Fortran may be hundreds of times faster than the\nsame in Python.  Therefore the added complexity of releasing the GIL to leverage\nmulti-core CPUs may not be needed in many cases. So we can thank the GIL for many\nextensions available for Python—and that is certainly one of the key reasons why the\nlanguage is so popular today .\nDespite the GIL, Python is thriving in applications that require concurrent\nor parallel execution, thanks to libraries and software architectures that\nwork around the limitations of CPython.\nNow let’ s discuss how Python is used in system administration, data\nscience, and server -side application development in the multi-core,\ndistributed computing world of 2021.\nSystem Administration\nPython is widely used to manage lar ge fleets of servers, routers, load\nbalancers, network-attached storage (NAS). It’ s also a leading option in\nsoftware-defined networking (SDN) and ethical hacking. Major cloud\nservice providers support Python through libraries and tutorials authored by\nthe providers themselves or by their lar ge communities of Python users.16\n17\nIn this domain, Python scripts automate configuration tasks by issuing\ncommands to be carried out by the remote machines, so rarely there are\nCPU-bound operations to be done. Threads or coroutines are well suited for\nsuch jobs. In particular , the concurrent.futures  package we’ll see in\nChapter 21  can be used to perform the same operations on many remote\nmachines at the same time without a lot of complexity .\nThere is also a growing number of libraries for system administration\nsupporting coroutines and asyncio . In 2016, Facebook’ s Production\nEngineering team reported : “W e are increasingly relying on AsyncIO,\nwhich was introduced in Python 3.4, and seeing huge performance gains as\nwe move codebases away from Python 2.”\nData Science\nData Science—including Artificial Intelligence—and scientific computing\nare very well served by Python. Applications in these fields are compute-\nintensive, but Python users benefit from a vast ecosystem of numeric\ncomputing libraries written in C, C++, Fortran, Cython, etc.—many of\nwhich are able to leverage multi-core machines, GPUs, and/or distributed\nparallel computing in heterogeneous clusters.\nAs of 2021, Python’ s data science ecosystem includes impressive tools such\nas:\nPr oject Jupyter\nT wo browser based interfaces—Jupyter Notebook and JupyterLab—that\nallow users to run and document analytics code potentially running\nacross the network on remote machines. Both are hybrid\nPython/JavaScript applications, supporting computing kernels written in\ndif ferent languages, all integrated via ZeroMQ—an asynchronous\nmessaging library for distributed applications. The name Jupyter\nactually comes from Julia, Python, and R, the first three languages\nsupported by the Notebook. The rich ecosystem built on top of the\nJupyter tools include Bokeh , a powerful interactive visualization library\nthat lets users navigate and interact with lar ge datasets or continuously\nupdated streaming data, thanks to the performance of modern JavaScript\nengines and browsers.\nT ensorFlow  and PyT or ch\nThese are the top two deep learning frameworks, according to O’Reilly\nMedia’ s January 2021 report  on usage of their learning resources during\n2020. Both projects are written in C++, and are able to leverage\nmultiple cores, GPUs, and clusters. They support other languages as\nwell, but Python is their main focus and is used by the majority of their\nusers. T ensorFlow was created and is used internally by Google;\nPyT orch by Facebook.\nDask\nA parallel computing library that can farm out work to local processes\nor clusters of machines, “tested on some of the lar gest supercomputers\nin the world”—as their home page  states. Dask of fers APIs that closely\nemulate NumPy , Pandas, and Scikit-Learn—the most popular libraries\nin data science and machine learning today . Dask can be used from\nJupyterLab or Jupyter Notebook, and leverages Bokeh not only for data\nvisualization but also for an interactive dashboard showing the flow of\ndata and computations across the processes/machines in near real-time.\nDask is so impressive that I recommend watching a video such as this\n15-minute demo  in which Matthew Rocklin—a maintainer of the\nproject—shows Dask crunching data on 64 cores distributed in 8 EC2\nmachines on A WS.\nThese are only some examples to illustrate how the data science community\nis creating solutions that leverage the best of Python and overcome the\nlimitations of the CPython runtime.\nServer-side W eb/Mobile Development\nPython is widely used in W eb applications and for the back-end APIs\nsupporting mobile applications. How is it that Google, Y ouT ube, Dropbox,\nInstagram, Quora, and Reddit—among others—managed to build Python\nserver -side applications serving hundreds of millions of users 24x7? Again,\nthe answer goes way beyond what Python provides “out of the box.”\nBefore we discuss tools to support Python at scale, I must quote an\nadmonition from the Thoughtworks T echnology Radar:\nHigh performance envy/web scale envy\nW e see many teams run into tr ouble because they have chosen complex\ntools, frameworks or ar chitectur es because they “might need to scale”.\nCompanies such as T witter and Netflix need to support extr eme loads and\nso need these ar chitectur es, but they also have extr emely skilled\ndevelopment teams able to handle the complexity . Most situations do not\nr equir e these kinds of engineering feats; teams should keep their web\nscale envy  in check in favor of simpler solutions that still get the job\ndone.\nAt W eb scale , the key is an architecture that allows horizontal scaling. At\nthat point, all systems are distributed systems, and no single programming\nlanguage is likely to be the right choice for every part of solution.\nDistributed systems is a field of academic research, but fortunately some\npractitioners have written accessible books anchored on solid research and\npractical experience. One of them is Martin Kleppmann, the author of\nDesigning Data-Intensive Applications  (O’Reilly , 2017).\nConsider Figure 20-3 , the first of many architecture diagrams in\nKleppmann’ s book. Here are some components I’ve seen in Python\nengagements that I worked on or have firsthand knowledge:\napplication caches : memcached , V arnish , Redis ;\nrelational databases: Postgr eSQL , MySQL ;\ndocument databases: Apache CouchDB , MongoDB ;\nfull-text indexes: Elasticsear ch , Apache Solr ;\nmessage queues: RabbitMQ , Redis .18\n19",9247
274-WSGI Application servers.pdf,274-WSGI Application servers,"Figur e 20-3. One possible ar chitectur e for a data system that combines several components.\nThere are other industrial-strength Open Source products in each of those\ncategories. Major cloud providers also of fer their own proprietary\nalternatives.\nKleppmann’ s diagram is general and language-independent—as is his book.\nFor Python server -side applications, two specific components are often\ndeployed:\nAn application server to distribute the load among several\ninstances of the Python application. The application server would\nappear near the top in Figure 20-3 , handling client requests before\nthey reached the application code.\nA task queue built around the message queue on the right-hand\nside of Figure 20-3 , providing a higher level, easier to use API to\ndistribute tasks to workers running on other machines.\nThe next two sections explore these components that are recommended best\npractices in Python server -side deployments.\nWSGI Application servers\nWSGI—the W eb Server Gateway Interface —is a standard API for a Python\nframework or application to receive requests from a HTTP server and send\nresponses to it.  The WSGI API is implemented by application servers\nmanage one or more Python processes running your application,\nmaximizing the use of the available CPUs. Figure 20-4  illustrates a typical\ndeployment.2 0\n21\nFigur e 20-4. Clients connect to a HTTP server that delivers static files and r outes other r equests to\nthe application server , which forks several Python pr ocesses to run the application code, maximizing\nthe use of the CPU cor es.\nThe best known application servers in Python W eb projects are:\nmod_wsgi ;\nuWSGI ;\ngunicorn ;\nNGINX Unit .\nFor users Apache HTTP , mod_wsgi  is the best option. It’ s as old as WSGI\nitself, but is actively maintained, and now provides a command-line\nlauncher called mod_wsgi-express  that makes it easier to configure\nand more suitable for use in Docker containers.\nuWSGI  and gunicorn  are the top choices in recent projects I know about.\nBoth are often used with the NGINX  HTTP server . uWSGI  of fers a lot of\nextra functionality , including an application cache, a task queue, cron-like\nperiodic tasks, and many other features. On the flip side, uWSGI  is much\nharder to configure properly than gunicorn .\nReleased in 2018, NGINX Unit  is a new product from the makers of the\nwell known NGINX  HTTP server and reverse proxy .\nmod_wsgi  and gunicorn  support Python W eb apps only , while uWSGI  and\nNGINX Unit  work with other languages as well. Please browse their docs to\nlearn more.\nThe main point: all of these application servers leverage all CPU cores on\nthe server by forking multiple Python processes to run traditional W eb apps\nwritten in good old sequential code in Django , Flask , Pyramid  etc. This\nexplains why it’ s been possible to earn a living as a Python W eb developer\nwithout ever studying the threading , multiprocessing , or\nasyncio  modules: the application server handles concurrency\ntransparently .22\n23",3057
275-Chapter Summary.pdf,275-Chapter Summary,"A S G I — A S Y N C H R O N O U S  S E R V E R  G A T E W A Y\nI N T E R F A C E\nWSGI is a synchronous API. It doesn’ t support coroutines with async/await —the\nmost ef ficient way to implement W ebSockets or HTTP long polling in Python. The\nASGI specification  is a successor to WSGI designed for asynchronous Python W eb\nframeworks such as aiohttp , Sanic , FastAPI  etc. as well as Django  and Flask , which are\ngradually adding asynchronous functionality .\nNow let’ s turn to another way of bypassing the GIL to achieve higher\nperformance with server -side Python applications.\nDistributed task queues\nWhen the application server delivers a request to one of the Python\nprocesses running your code, your app needs to respond quickly: you want\nthe process to be available to handle the next request as soon as possible.\nHowever , some requests demand actions that may take longer—for\nexample, sending e-mail or generating a PDF . That’ s the problem that\ndistributed task queues are designed to solve.\nCelery  and RQ  are the best known Open Source task queues with Python\nAPIs. Cloud providers also of fer their own proprietary task queues.\nThese products wrap a message queue and of fer a high-level API for\ndelegating tasks to workers, possibly running on dif ferent machines.\nN O T E\nIn the context of task queues, the words pr oducer  and consumer  are used instead of\ntraditional client/server terminology . For example, a Django  view handler pr oduces  job\nrequests which are put in the queue to be consumed  by one or more PDF rendering\nprocesses.\nQuoting directly from Celery ’ s F AQ , here are some typical use cases:\nRunning something in the backgr ound. For example, to finish\nthe web r equest as soon as possible, then update the users page\nincr ementally . This gives the user the impr ession of good\nperformance and “snappiness”, even though the r eal work\nmight actually take some time.\nRunning something after the web r equest has finished.\nMaking sur e something is done, by executing it asynchr onously\nand using r etries.\nScheduling periodic work.\nBesides solving these immediate problems, task queues support horizontal\nscalability . Producers and consumers are decoupled: a producer doesn’ t call\na consumer , it puts a request in a queue. Consumers don’ t need to know\nanything about the producers (but the request may include information\nabout the producer , if an acknowledgement is required). Crucially , you can\neasily add more workers to consume tasks as demand grows. That’ s why\nCelery  and RQ  are called distributed task queues.\nRecall that our simple pr ocs.py  ( Example 20-13 ) used two queues: one for\njob requests, the other for collecting results. The distributed architecture of\nCelery  and RQ  uses a similar pattern. Both support using the Redis  NoSQL\ndatabase as a message queue and result storage. Celery  also supports other\nmessage queues like RabbitMQ  or Amazon SQS , as well other databases for\nresult storage.\nThis wraps up our introduction to concurrency in Python. The next two\nchapters will continue this theme, focusing on the\nconcurrent.futures  and asyncio  packages of the standard library .\nC h a p t e r  S u m m a r y\nAfter a bit of theory , this chapter presented the spinner scripts implemented\nin each of Python’ s three native concurrency programming models:\nThreads, with the threading  package;\nProcesses, with multiprocessing ;\nAsynchronous coroutines with asyncio .\nW e then explored the real impact of the GIL with an experiment: changing\nthe spinner examples to compute the primality of a lar ge integer and\nobserve the resulting behavior . This demonstrated graphically that CPU-\nintensive functions must be avoided in asyncio , as they block the event\nloop. The threaded version of the experiment worked—despite the GIL—\nbecause Python periodically interrupts threads, and the example used only\ntwo threads: one doing compute-intensive work, and the other driving the\nanimation only 10 times per second. The multiprocessing  variant\nworked around the GIL, starting a new process just for the animation while\nthe main process did the primality check.\nThe next example, computing several primes, highlighted the dif ference\nbetween multiprocessing  and threading , proving that only\nprocesses allow Python to benefit from multicore CPUs. Python’ s GIL\nmakes threads worse than sequential code for heavy computations.\nThe GIL dominates discussions about concurrent and parallel computing in\nPython, but we should not overestimate its impact. That was the point of\n“The Big Picture” . For example, the GIL doesn’ t af fect many use cases of\nPython in systems administration. On the other hand, the data science and\nserver -side development communities have worked around the GIL with\nindustrial-strength solutions tailored to their specific needs. The last two\nsections mentioned two common elements to support Python server -side\napplications at scale: WSGI application servers and distributed task queues.",5057
276-Further Reading.pdf,276-Further Reading,,0
277-Concurrency with threads and processes.pdf,277-Concurrency with threads and processes,"F u r t h e r  R e a d i n g\nThis chapter has an extensive reading list, so I split it in subsections.\nConcurrency with threads and processes\nThe concurrent.futures  library covered in Chapter 21  uses threads,\nprocesses, locks, and queues under the hood, but you won’ t see individual\ninstances of them; they’re bundled and managed by the higher -level\nabstractions of a ThreadPoolExecutor  and a\nProcessPoolExecutor . If you want to learn more about the practice of\nconcurrent programming with those low-level objects, An Intro to\nThreading in Python  by Jim Anderson is a good first read. Doug Hellmann\nhas a chapter titled Concurr ency with Pr ocesses, Thr eads, and Cor outines\nin his site  and book: Python 3 Standard Library by Example  (Addison-\nW esley , 2017).\nBrett Slatkin’ s Effective Python, Second Edition  (Addison-W esley , 2019),\nDavid Beazley’ s Python Essential Refer ence, 4th Edition  (Addison-W esley\nProfessional, 2009), and Martelli, Ravenscroft & Holden’ s Python in a\nNutshell , 3E (O’Reilly) are other general Python references with significant\ncoverage of threading  and multiprocessing . The vast\nmultiprocessing  of ficial documentation includes useful advice in its\nProgramming guidelines  section.\nJesse Noller and Richard Oudkerk contributed the multiprocessing\npackage, introduced in PEP 371 — Addition of the multiprocessing package\nto the standard library . The of ficial documentation for the package  is a 93\nKB .rst  file—that’ s about 63 pages—making it one of the longest chapters\nin the Python standard library .\nIn High Performance Python, 2nd Edition  (O’Reilly , 2020), authors Micha\nGorelick and Ian Ozsvald includes a chapter about multiprocessing\nwith an example about checking for primes with a dif ferent strategy than\nour pr ocs.py  example: for each number , they split the range of possible\nfactors—from 2 to sqrt(n) —into sub-ranges, and make each worker\niterate over one of the sub-ranges. Their divide-and-conquer approach is\ntypical of scientific computing applications where the data sets are huge,\nand workstations (or clusters) have more CPU cores than users. On a\nserver -side system handling requests from many users, it is simpler and\nmore ef ficient to let each process work on one computation from start to\nfinish—reducing the overhead of communication and coordination among\nprocesses. Besides multiprocessing , Gorelick & Ozsvald present\nmany other ways of developing and deploying high performance data\nscience applications leveraging multiple cores, GPUs, clusters, profilers,\nand compilers like Cython and Numba. Their last chapter , Lessons fr om the\nField , is a valuable collection of short case studies contributed by other\npractitioners of high-performance computing in Python.\nIn Advanced Python Development  (Apress, 2020), author Matthew W ilkes\nis a rare book that includes short examples to explain concepts, while also\nshowing how to build a realistic application ready for production: a data\naggregator , similar to DevOps monitoring systems or IoT data collectors for\ndistributed sensors. T wo chapters in Advanced Python Development  cover\nconcurrent programming with threading  and asyncio .\nJan Palach’ s Parallel Pr ogramming with Python  (Packt, 2014), explains the\ncore concepts behind concurrency and parallelism, covering Python’ s\nstandard library as well as Celery .\nThe T ruth About Thr eads  is the title of chapter 2 in Using Asyncio in Python\nby Caleb Hattingh (O’Reilly , 2020).  The chapter covers the benefits and\ndrawbacks of threading—with compelling quotes from several authoritative\nsources—making it clear that the fundamental challenges of threads have\nnothing to do with Python or the GIL. Quoting verbatim from page 14 of\nUsing Asyncio in Python :\nThese themes r epeat thr oughout:\nThr eading makes code har d to r eason about.\nThr eading is an inefficient model for lar ge-scale concurr ency\n(thousands of concurr ent tasks).24",3997
278-The GIL.pdf,278-The GIL,,0
279-Concurrency beyond the standard library.pdf,279-Concurrency beyond the standard library,"If you want to learn the hard way how dif ficult it is to reason about threads\nand locks—without risking your job—try the exercises in Alen Downey’ s\nworkbook The Little Book of Semaphor es . The exercises in Downey’ s book\nrange from easy to very hard to unsolvable, but even the easy ones are eye\nopening.\nThe GIL\nIf you are intrigued about the GIL, start with the Python Library and\nExtension F AQ  ( “Can’ t we get rid of the Global Interpreter Lock?” ). Also\nworth reading are posts by Guido van Rossum and Jesse Noller (contributor\nof the multiprocessing  package): “It isn’ t Easy to Remove the GIL”\nand “Python Threads and the Global Interpreter Lock.”\nCPython Internals  by Anthony Shaw explains the implementation of the\nCPython 3 interpreter at the C programming level. Shaw’ s longest chapter\nis Parallelism and Concurr ency : a deep dive into Python’ s native support\nfor threads and processes, including managing the GIL from extensions\nusing the C/Python API.\nFinally , David Beazley has a detailed exploration on the inner workings of\nthe GIL: “Understanding the Python GIL.”  In slide #54 of the\npresentation , Beazley reports some alarming results, including a 20×\nincrease in processing time for a particular benchmark with the new GIL\nalgorithm introduced in Python 3.2. However , Beazley apparently used an\nempty while True: pass  to simulate CPU-bound work, and that is not\nrealistic. The issue is not significant with real workloads, according to a\ncomment  by Antoine Pitrou—who implemented the new GIL algorithm—\nin the bug report submitted by Beazley .\nConcurrency beyond the standard library\nI’ve already mentioned two books that cover concurrency using Python’ s\nstandard library , which also include significant coverage of third-party\nlibraries and tools: High Performance Python, 2nd Edition  and Parallel\nPr ogramming with Python .25\nFrancesco Pierfederici’ s Distributed Computing with Python  (Packt, 2016),\nwhich also addresses using cloud providers and HPC (High-Performance\nComputing) clusters.\nPython, Performance, and GPUs  by Matthew Rocklin is “a status update for\nusing GPU accelerators from Python”, posted in June 2019.\n“Instagram currently features the world’ s lar gest deployment of the Django\nweb framework, which is written entirely in Python.” That’ s the opening\nsentence of the blog post W eb Service Ef ficiency at Instagram with Python\nwritten by Min Ni—a software engineer at Instagram. The post describes\nmetrics and tools Instagram uses to optimize the ef ficiency of their Python\ncodebase, as well as detect and diagnose performance regressions as they\ndeploy their back end “30-50 times a day .”\nAr chitectur e Patterns with Python: Enabling T est-Driven Development,\nDomain-Driven Design, and Event-Driven Micr oservices  by Harry Percival\n& Bob Gregory (O’Reilly , 2020) presents architectural patterns for Python\nserver -side applications. The authors also made the book freely available\nonline at cosmicpython.com .\nT wo elegant and easy to use libraries for parallelizing tasks over processes\nare lelo  by João S. O. Bueno and python-parallelize  by Nat Pryce. The lelo\npackage defines a @parallel  decorator that you can apply to any\nfunction to magically make it unblocking: when you call the decorated\nfunction, its execution is started in another process. Nat Pryce’ s python-\nparallelize  package provides a parallelize  generator that distributes\nthe execution of a for  loop over multiple CPUs. Both packages are built\non the multiprocessing  library .\nPython core developer Eric Snow maintains a Multi-core Python  wiki, with\nnotes about his and other people’ s ef forts to improve Python’ s support for\nparallel execution. Snow is the author of PEP 554—Multiple Interpreters in\nthe Stdlib . If approved and implemented, PEP 554 lays the groundwork for\nfuture enhancements that may eventually allow Python to use multiple\ncores without the overheads of multiprocessing . One of the biggest",4021
280-Concurrency and scalability beyond Python.pdf,280-Concurrency and scalability beyond Python,"blockers is the complex interaction between multiple active subinterpreters\nand extensions that assume a single interpreter .\nMark Shannon—also a Python maintainer—created a useful table\ncomparing concurrent models in Python\nhttps://gist.github.com/markshannon/79cace3656b40e21b7021504daee950\nc  referenced in a discussion about subinterpreters between him, Eric Snow ,\nand other developers on the python-dev  mailing list. In Shannon’ s table, the\n“Ideal CSP” column refers to the theoretical Communicating sequential\nprocesses  model proposed by T ony Hoare in 1978. Go also allows shared\nobjects, violating an essential constraint of CSP: execution units should\ncommunicate through message passing through channels.\nThe actor model of concurrent programming underlies the highly scalable\nErlang and Elixir languages, as well as the Akka framework for Scala and\nJava. If you want to try out the actor model in Python, check out the\nThespian  and Pykka  libraries.\nMy remaining recommendations have few or zero mentions of Python, but\nare nevertheless relevant to readers interested in the theme of this chapter .\nConcurrency and scalability beyond Python\nRabbitMQ in Action  by Alvaro V idela and Jason J. W . W illiams (Manning,\n2012) is a very well written introduction to RabbitMQ  and the Advanced\nMessage Queuing Protocol (AMQP) standard, with examples in Python,\nPHP , and Ruby . Regardless of the rest of your tech stack, and even if you\nplan to use Celery  with RabbitMQ  under the hood, I recommend this book\nfor its coverage of concepts, motivation, and patterns for distributed\nmessage queues, as well as operating and tuning RabbitMQ  at scale.\nI learned a lot reading Seven Concurr ency Models in Seven W eeks , by Paul\nButcher (Pragmatic Bookshelf, 2014)—with the eloquent subtitle When\nThr eads Unravel . Chapter 1 of the book presents the core concepts and\nchallenges of programming with threads and locks in Java.  The remaining\nsix chapters the book are devoted to what the author considers better\nalternatives for concurrent and parallel programming, as supported by26\ndif ferent languages, tools and libraries. The examples use Java, Clojure,\nElixir , and C (for the chapter about parallel programming with the OpenCL\nframework ). The CSP model is exemplified with Clojure code, although the\nGo language deserves credit for popularizing that approach. Elixir is the\nlanguage of the examples illustrating the actor model. A freely available,\nalternative bonus chapter  about actors uses Scala and the Akka framework.\nUnless you already know Scala, Elixir is a more accessible language to\nlearn and experiment with the actor model and the Erlang/OTP distributed\nsystems platform.\nUnmesh Joshi of Thoughtworks has contributed several pages documenting\nPatterns of Distributed Systems  to Martin Fowler ’ s blog . The opening page\nis a great introduction the topic, with links to individual patterns. Joshi is\nadding patterns incrementally , but what’ s already there distills years of\nhard-earned experience in mission-critical systems.\nMartin Kleppmann’ s Designing Data-Intensive Applications  (O’Reilly ,\n2017) is a rare book written by a practitioner with deep industry experience\nand advanced academic background. The author worked with lar ge-scale\ndata infrastructure at LinkedIn and two startups, before becoming a\nresearcher of distributed systems at the University of Cambridge. Each\nchapter in Kleppmann’ s book ends with an extensive list of references\nincluding recent research results. The book also includes numerous\nilluminating diagrams and beautiful concept maps.\nI was fortunate to be in the audience for Francesco Cesarini’ s outstanding\nworkshop on the architecture of reliable distributed systems at OSCON\n2016: Designing and ar chitecting for scalability with Erlang/OTP  ( video  at\nthe O’Reilly Learning Platform). Despite the title, 9:35 into the video\nCesarini explains:\nV ery little of what I am going to say will be Erlang-specific […]. The fact\nr emains that Erlang will r emove a lot of accidental difficulties to making\nsystems which ar e r esilient and which never fail, and ar e scalable. So it\nwill be much easier if you do use Erlang, or a language running on the\nErlang virtual machine.\nThat workshop was based on the last four chapters of Designing for\nScalability with Erlang/OTP  by Francesco Cesarini and Steve V inoski\n(O’Reilly , 2016).\nProgramming distributed systems is challenging and exciting, but beware of\nW eb-scale envy . The KISS principle  remains solid engineering advice.\nCheck out the paper Scalability! But at what COST?  by Frank McSherry ,\nMichael Isard & Derek G. Murray . The authors identified parallel graph-\nprocessing systems presented in academic symposia that require hundreds\nof cores to outperform a “competent single-threaded implementation.” They\nalso found systems that “underperform one thread for all of their reported\nconfigurations.”\nThose findings remind me of a classic hacker quip:\nMy Perl script is faster than your Hadoop cluster .\nS O A P B O X\nConcurr ency in the competition\nMRI—the reference implementation of Ruby—also has a GIL, so its\nthreads are under the same limitations as Python’ s. Meanwhile,\nJavaScript interpreters don’ t support user -level threads at all;\nasynchronous programming is their only path to concurrency . I mention\nthis because Ruby and JavaScript are the closest direct competitors to\nPython as general-purpose, dynamic programming languages.\nLooking at languages born in the 21st century , Go and Elixir are\nprobably the ones best positioned to eat Python’ s lunch when\nconcurrency matters. Both were designed from day 0 to allow highly\nef ficient and reliable concurrent programming. Elixir , Go, and Python\nare my favorite languages today—in alphabetical order .\nT o manage complexity , we need constraints\nI learned to program on a TI-58 calculator . Its “language” was similar to\nassembly . At that level, all “variables” are globals, and you don’ t have\nthe luxury of structured flow control statements. Y ou have conditional\njumps: instructions that take the execution directly to an arbitrary\nlocation—ahead or behind the current spot—depending on the value of\na CPU register or flag.\nBasically you can do anything in assembly , and that’ s the challenge:\nthere are very few constraints to keep you from making mistakes, and to\nhelp maintainers understand the code when changes are needed.\nThe second language I learned the was the unstructured BASIC that\ncame with 8-bit computers—nothing like V isualBasic, which appeared\nmuch later . There were FOR , GOSUB  and RETURN  statements, but still\nno concept of local variables. GOSUB  did not support parameter\npassing: it was just a fancy GOTO  that put a return line number in a\nstack so that RETURN  had a tar get to jump to. Subroutines could help\nthemselves to the global data, and put results there too. W e had to\nimprovise other forms of flow control with combinations of IF  and\nGOTO —which, again, allowed you to jump to any line of the program.\nAfter a few years of programming with jumps and global variables, I\nremember the struggle to rewire my brain for “structured programming”\nwhen I learned Pascal. Now I had to use flow control statements around\nblocks of code that have a single entry point. I couldn’ t jump to any\ninstruction I liked. Global variables were unavoidable in BASIC, but\nnow they were taboo. I needed to rethink the flow of data and explicitly\npass ar guments to functions.\nThe next challenge for me was learning Object Oriented Programming.\nAt core, OOP is structured programming with more constraints and\npolymorphism. Information hiding forces yet another rethink of where\ndata lives. I remember being frustrated more than once because I had to\nrefactor my code so that a method I was writing could get information\nthat was encapsulated in an object that my method could not reach.\nFunctional programming languages add other constraints, but\nimmutability is the hardest to swallow after decades of imperative\nprogramming and OOP .\nAfter we get used to these constraints, we see them as blessings. They\nmake reasoning about the code much easier .\nLack of constraints is the main problem with the threads-and-locks\nmodel of concurrent programming.\nWhen summarizing chapter 1 of Seven Concurr ency Models in Seven\nW eeks , Paul Butcher wrote:\nThe gr eatest weakness of the appr oach, however , is that thr eads-and-\nlocks pr ogramming is har d . It may be easy for a language designer to\nadd them to a language, but they pr ovide us, the poor pr ogrammers,\nwith very little help.\nSome examples of unconstrained behavior in that model:\nThreads can share access to arbitrary mutable data structures.\nThe scheduler can interrupt a thread at almost any point,\nincluding in the middle of a simple operation like a += 1 .\nV ery few operations are atomic at the level of source code\nexpressions.\nLocks are usually advisory . That’ s a technical term meaning\nthat you must remember to explicitly hold a lock before\nupdating a shared data structure. If you for get to get the lock,\nnothing prevents your code from messing up the data while\nanother thread dutifully holds the lock and is updating the\nsame data.\nIn contrast, consider some constraints enforced by the actor model,\nwhere the unit of execution is called “actor” instead of thread.\nAn actor can have internal state, but cannot share state with\nother actors.\nActors can only communicate by sending and receiving\nmessages.\nMessages only hold copies of data, not references to mutable\ndata.\nAn actor only handles one message at a time. There is no\nconcurrent execution inside a single actor .\nOf course, you can adopt an actor style  of coding in any language, by\nfollowing these rules. Y ou can also use OOP idioms in C, and even\nstructured programming patterns in assembly . But doing any of that\nrequires a lot of agreement and discipline among everyone who touches\nthe code.\nManaging locks is unnecessary in the actor model, as implemented by\nErlang and Elixir , where all data types are immutable.\nThreads-and-locks are not going away . I just don’ t think dealing with\nsuch low-level entities is a good use of my time as I write applications27\n—as opposed to kernel modules or databases.\nI reserve the right to change my mind, always. But right now , I am\nconvinced that the actor model is the most sensible, general purpose\nconcurrent programming model available. CSP (Communicating\nSequential Process) is also sensible, but its implementation in Go leaves\nout some constraints.\nOld habits die hard\nLike the actor model, CSP also advocates a form of message passing.\nThe best practice is that goroutines—the unit of execution in Go—\ncommunicate through channels which are essentially queues with\nblocking puts and gets. Rob Pike—co-creator of Go—is known for his\n“proverbs”, one of which says:\nDon’ t communicate by sharing memory , shar e memory by\ncommunicating.\nIn the name of performance, I’ve seen Go developers advocate sharing\nmemory as standard practice, instead of an optimization technique to be\nconsidered in extreme cases, mostly in libraries, and rarely in\napplication code. Go’ s standard library provides locks, enabling a\ncoroutine-and-locks style that contradicts the essence of CSP .\nIn the name of performance, why don’ t we go back to assembly?\nW e need constraints to keep our thinking straight and our systems\nrunning.\n1  Slide 5 of the talk “Concurrency Is Not Parallelism (It’ s Better)” .\n2  I studied and worked with Prof. Imre Simon who liked to say there are two major sins in\nscience: using dif ferent words to mean the same thing and using one word to mean dif ferent\nthings. Imre Simon (1943–2009) was a pioneer of computer science in Brazil who made\nseminal contributions to Automata Theory and started the field of T ropical Mathematics. He\nwas also an advocate of free software and free culture.\n3  Y ou can see the configured interval by calling sys.getswitchinterval()  and change\nit via sys.setswitchinterval(s) .\n4  A syscall is a call from user code to a function of the operating system kernel. I/O, timers, and\nlocks are some of the kernel services available through syscalls. T o learn more, read the\nW ikipedia System call  article.\n5  The zlib  and bz2  modules are specifically mentioned in a python-dev message  by Antoine\nPitrou, who contributed the time-slicing GIL logic to Python 3.2.\n6  Source: slide 106 of “Generators: The Final Frontier” .\n7  Source: last paragraph of the Thread objects  section\n8  Unicode has lots of characters useful for simple animations, like the Braille patterns  for\nexample. I used the ASCII ""\|/-""  to keep the examples simple.\n9  It’ s a 15″ MacBook Pro 2018 with a 6-core, 2.2 GHz Intel Core i7 CPU.\n10  This is true today because you are probably using a modern OS with pr eemptive multi-\ntasking . W indows before the NT era and MacOS before the OSX era were not “preemptive”,\ntherefore any process could take over 100% of the CPU and freeze the whole system. W e are\nnot completely free of this kind of problem today but trust this gray beard: this troubled every\nuser in the 1990s, and a hard reset was the only cure.\n11  In this example, 0  is a convenient sentinel. None  is also commonly used for that. Using 0\nkeeps the type hints for PrimeResult  and PrimeResult —and the code for worker —as\nsimple as possible.\n12  Surviving serialization without losing our identity is a pretty good life goal.\n13  Look for primes/thr eads.py  the Fluent Python 2e  code repository  if you are curious.\n14  T o learn more, see Context switch  in the English W ikipedia.\n15  The Fr ee Lunch Is Over: A Fundamental T urn T owar d Concurr ency in Softwar e — Dr . Dobb’ s\nJournal , March 2005. A vailable online .\n16  These are probably the same reasons that prompted the creator of the Ruby language,\nY ukihiro Matsumoto, to use a GIL in his interpreter as well.\n17  As an exercise in college, I had to implemented the LZW compression algorithm in C. But\nfirst I wrote it in Python, to check my understanding of the spec. The C version was about\n900× faster .\n18  Source: Thoughtworks T echnology Advisory Board, T echnology Radar—November 2015 .\n19  Contrast application caches—used directly by your application code—with HTTP caches,\nwhich would be placed on the top edge of Figure 20-3  to serve static assets like images, CSS,\nand JS files. Content Delivery Networks (CDN) of fer another type of HTTP cache, deployed in\ndata centers closer to the end users of your application.\n20  Diagram and caption from Figure 1-1 Designing Data-Intensive Applications  (O’Reilly , 2017)\n21  Some speakers spell out the WSGI acronym, while others pronounce it as one word rhyming\nwith “whisky”.\n22  uWSGI  is spelled with a lowercase “u”, but that is pronounced as the Greek letter “µ”, so the\nwhole name sounds like “micro-whisky” with a “g” instead of the “k”.\n23  Bloomber g engineers Peter Sperl and Ben Green wrote Configuring uWSGI for Production\nDeployment , explaining how many of the default settings in uWSGI  are not suitable for many\ncommon deployment scenarios. Sperl presented a summary of their recommendations at\nEuroPython 2019 . Highly recommended for users of uWSGI .\n24  Caleb is one of the tech reviewers of Fluent Python, Second Edition\n25  Thanks to Lucas Brunialti for sending me a link to this talk.\n26  Python’ s threading  and concurrent.futures  APIs are heavily influenced by the\nJava standard library .\n27  The Erlang community uses the term “process” for actors. In Erlang, each process is a\nfunction in its own loop, so they are very lightweight and it’ s feasible to have millions of them\nactive at once in a single machine—no relation to the heavyweight OS processes we’ve been\ntalking about elsewhere in this chapter . So here we have examples of the two sins described by\nProf. Simon: using dif ferent words to mean the same thing, and using one word to mean\ndif ferent things.",16202
281-Whats new in this chapter.pdf,281-Whats new in this chapter,"Chapter 21. Concurrency with\nFutures\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 21st chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nThe people bashing thr eads ar e typically system pr ogrammers which\nhave in mind use cases that the typical application pr ogrammer will\nnever encounter in her life. […] In 99% of the use cases an application\npr ogrammer is likely to run into, the simple pattern of spawning a bunch\nof independent thr eads and collecting the r esults in a queue is everything\none needs to know .\n— Michele Simionato, Python deep thinker\nThis chapter focuses on the concurrent.futures  library that\nencapsulates the pattern of “spawning a bunch of independent threads and\ncollecting the results in a queue” described by Michele Simionato, making\nit almost trivial to use. The package also supports processes, useful for\ncompute-intensive tasks.\nHere I also introduce the concept of “futures”—objects representing the\nasynchronous execution of an operation, similar to JavaScript promises.1",1534
282-Concurrent Web Downloads.pdf,282-Concurrent Web Downloads,"This primitive idea is the foundation not only of concurrent.futures\nbut also of the asyncio  package, the subject of Chapter 22 .\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThis chapter had few important changes from the first edition, because the\nconcurrent.futures  API is stable, with minor changes since its\nintroduction in Python 3.2.\nExample 21-3  ( flags_thr eadpool.py ) is a bit simpler after I removed some\ncode to set up the number of workers, now that the\nThreadPoolExecutor  in Python 3.8 got smarter: it doesn’ t start\nunnecessary threads, and its logic for automatically setting the number of\nworkers was updated. I added a few paragraphs explaining the new logic at\nthe end of “Downloading with concurrent.futures” .\nI was able to greatly simplify the setup for the experiments in “Downloads\nwith Progress Display and Error Handling”  thanks to the multi-threaded\nserver added to the http.server  package in Python 3.7. Previously , that\npackage of fered only the single-threaded BaseHttpServer  which was\nno good for experimenting with concurrent clients, so I had to resort to\nexternal tools in the First Edition .\nIn “Launching Processes with concurrent.futures” , I replaced the previous\nexamples using ProcessPoolExecutor  with a new version of the\nprimality checker , showing how that class simplifies the code we saw in\n“Code for the Multi-core Prime Checker” .\nFinally , I moved some conceptual content to the new Chapter 20 –\nConcurr ency Models in Python .\nC o n c u r r e n t  W e b  D o w n l o a d s\nConcurrency is essential to ef ficient network I/O: instead of wasting CPU\ncycles waiting for remote machines, the application should do something\nelse until a response comes back over the wire.\nT o make this last point with code, I wrote three simple programs to\ndownload images of 20 country flags from the W eb. The first one, flags.py ,\nruns sequentially: it only requests the next image when the previous one is\ndownloaded and saved locally . The other two scripts make concurrent\ndownloads: they request several images practically at the same time, and\nsave them as they arrive. The flags_thr eadpool.py  script uses the\nconcurrent.futures  package, while flags_asyncio.py  uses\nasyncio .\nExample 21-1  shows the result of running the three scripts, three times\neach. I also posted a 73s video on Y ouT ube  so you can watch them running\nwhile a MacOS Finder window displays the flags as they are saved. The\nscripts are downloading images from fluentpython.com , which is behind a\nCDN, so you may see slower results in the first runs. The results in\nExample 21-1  were obtained after several runs, so the CDN cache was\nwarm.\nExample 21-1. Thr ee typical runs of the scripts flags.py ,\nflags_thr eadpool.py , and flags_asyncio.py\n$ python3 flags.py  \nBD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN  \n  \n20 flags downloaded in 7.26s  \n  \n$ python3 flags.py  \nBD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN  \n20 flags downloaded in 7.20s  \n$ python3 flags.py  \nBD BR CD CN DE EG ET FR ID IN IR JP MX NG PH PK RU TR US VN  \n20 flags downloaded in 7.09s  \n$ python3 flags_threadpool.py  \nDE BD CN JP ID EG NG BR RU CD IR MX US PH FR PK VN IN ET TR  \n20 flags downloaded in 1.37s  \n  \n$ python3 flags_threadpool.py  \nEG BR FR IN BD JP DE RU PK PH CD MX ID US NG TR CN VN ET IR  \n20 flags downloaded in 1.60s  \n$ python3 flags_threadpool.py  \nBD DE EG CN ID RU IN VN ET MX FR CD NG US JP TR PK BR IR PH  \n20 flags downloaded in 1.22s  \n$ python3 flags_asyncio.py  \n  \nBD BR IN ID TR DE CN US IR PK PH FR RU NG VN ET MX EG JP CD  \n20 flags downloaded in 1.36s  \n$ python3 flags_asyncio.py  \nRU CN BR IN FR BD TR EG VN IR PH CD ET ID NG DE JP PK MX US  \n20 flags downloaded in 1.27s  \n$ python3 flags_asyncio.py  \nRU IN ID DE BR VN PK MX US IR ET EG NG BD FR CN JP PH CD TR  \n  \n20 flags downloaded in 1.42s\nThe output for each run starts with the country codes of the flags as they\nare downloaded, and ends with a message stating the elapsed time.\nIt took flags.py  an average 7.18s to download 20 images.\nThe average for flags_thr eadpool.py  was 1.40s.\nFor flags_asyncio.py , 1.35 was the average time.\nNote the order of the country codes: the downloads happened in a\ndif ferent order every time with the concurrent scripts.\nThe dif ference in performance between the concurrent scripts is not\nsignificant, but they are both more than five times faster than the sequential\nscript—and this is just for the small task of downloading 20 files of a few\nkilobytes each. If you scale the task to hundreds of downloads, the\nconcurrent scripts can outpace the sequential code by a factor or 20 or\nmore.\nW A R N I N G\nWhile testing concurrent HTTP clients against public W eb servers you may\ninadvertently launch a denial-of-service (DoS) attack, or be suspected of doing so. In the\ncase of Example 21-1 , it’ s OK to do it because those scripts are hardcoded to make only\n20 requests. W e’ll use Python’ s http.server  package to run tests later in this\nchapter .\nNow let’ s study the implementations of two of the scripts tested in\nExample 21-1 : flags.py  and flags_thr eadpool.py . I will leave the third",5257
283-A Sequential Download Script.pdf,283-A Sequential Download Script,"script, flags_asyncio.py , for Chapter 22 , but I wanted to demonstrate all\nthree together to make two points:\n1 . Regardless of the concurrency constructs you use—threads or\ncoroutines—you’ll see vastly improved throughput over sequential\ncode in network I/O operations, if you code it properly .\n2 . For HTTP clients that can control how many requests they make,\nthere is no significant dif ference in performance between threads\nand coroutines.\nOn to the code.\nA Sequential Download Script\nExample 21-2  is not very interesting, but we’ll reuse most of its code and\nsettings to implement the concurrent scripts, so it deserves some attention.\nN O T E\nFor clarity , there is no error handling in Example 21-2 . W e will deal with exceptions\nlater , but here we want to focus on the basic structure of the code, to make it easier to\ncontrast this script with the concurrent ones.\nExample 21-2. flags.py: sequential download script; some functions will be\nr eused by the other scripts\nimport time \nfrom pathlib import Path \nfrom typing import Callable  \n \nimport requests   \n \n \nPOP20_CC  = ('CN IN US ID BR PK NG BD RU JP ' \n            'MX PH VN ET EG DE IR TR CD FR ').split()  \n \n \nBASE_URL  = 'http://fluentpython.com/data/flags '       \n  \nDEST_DIR  = Path('downloaded ')                         \n  \n \ndef save_flag (img: bytes, filename : str) -> None:     \n 2\n    (DEST_DIR  / filename ).write_bytes (img) \n \ndef get_flag (cc: str) -> bytes:  \n \n    url = f'{BASE_URL}/{cc}/{cc}.gif '.lower() \n    resp = requests .get(url) \n    return resp.content \n \ndef download_many (cc_list: list[str]) -> int:  \n \n    for cc in sorted(cc_list):                 \n  \n        image = get_flag (cc) \n        save_flag (image, f'{cc}.gif ') \n        print(cc, end=' ', flush=True)         \n  \n    return len(cc_list) \n \ndef main(downloader : Callable [[list[str]], int]) -> None:  \n \n    t0 = time.perf_counter ()                               \n  \n    count = downloader (POP20_CC ) \n    elapsed = time.perf_counter () - t0 \n    print(f'\n{count} downloads in {elapsed:.2f}s ') \n \nif __name__  == '__main__ ': \n    main(download_many )     \nImport the requests  library; it’ s not part of the standard library , so by\nconvention we import it after the standard library modules os , time ,\nand sys , and insert a blank line to separate them.\nList of the ISO 3166 country codes for the 20 most populous countries\nin order of decreasing population.\nThe directory with the flag images.\nLocal directory where the images are saved.\nSave the img  bytes to filename  in the DEST_DIR .\nGiven a country code, build the URL and download the image using\nrequests , returning the binary contents of the response.\ndownload_many  is the key function to compare with the concurrent\nimplementations.3",2850
284-Downloading with concurrent.futures.pdf,284-Downloading with concurrent.futures,"Loop over the list of country codes in alphabetical order , to make it easy\nto see that the ordering is preserved in the output; return the number of\ncountry codes downloaded.\nDisplay a country code and flush sys.stdout  so we can see progress\nas each download happens; flushing is needed because, otherwise,\nPython waits for a line break to output the stdout  buf fer .\nmain  must be called with the function that will make the downloads;\nthat way , we can use main  as library function with other\nimplementations of download_many  in the threadpool  and\nascyncio  examples.\nmain  records and reports the elapsed time after running the\ndownloader  function.\nCall main  with the download_many  function.\nT I P\nThe r equests  library is more powerful and easier to use than the urllib.request\nmodule from the Python 3 standard library . In fact, requests  is considered a model\nPythonic API.\nThere’ s really nothing new to flags.py . It serves as a baseline for comparing\nthe other scripts and I used it as a library to avoid redundant code when\nimplementing them. Now let’ s see a reimplementation using\nconcurrent.futures .\nDownloading with concurrent.futures\nThe main features of the concurrent.futures  package are the\nThreadPoolExecutor  and ProcessPoolExecutor  classes, which\nimplement an API for to submitting callables for execution in dif ferent\nthreads or processes, respectively . The classes transparently manage a pool\nof worker threads or processes, and queues to distribute jobs and collect\nresults. But the interface is very high level, and we don’ t need to know\nabout any of those details for a simple use case like our flag downloads.\nExample 21-3  shows the easiest way to implement the downloads\nconcurrently , using the ThreadPoolExecutor.map  method.\nExample 21-3. flags_thr eadpool.py: thr eaded download script using\nfutur es.Thr eadPoolExecutor\nfrom concurrent  import futures \n \nfrom flags import save_flag , get_flag , main  \n \n \ndef download_one (cc: str):  \n \n    image = get_flag (cc) \n    save_flag (image, f'{cc}.gif ') \n    print(cc, end=' ', flush=True) \n    return cc \n \ndef download_many (cc_list: list[str]) -> int: \n    with futures.ThreadPoolExecutor () as executor :         \n  \n        res = executor .map(download_one , sorted(cc_list))  \n \n \n    return len(list(res))                                  \n  \n \nif __name__  == '__main__ ': \n    main(download_many )  \nReuse some functions from the flags  module ( Example 21-2 ).\nFunction to download a single image; this is what each worker will\nexecute.\nInstantiate the ThreadPoolExecutor  as a context manager; the\nexecutor.__exit__  method will call\nexecutor.shutdown(wait=True) , which will block until all\nthreads are done.\n\nThe map  method is similar to the map  built-in, except that the\ndownload_one  function will be called concurrently from multiple\nthreads; it returns a generator that you can iterate to retrieve the value\nreturned by each function call—in this case, each call to\ndownload_one  will return a country code.\nReturn the number of results obtained; if any of the threaded calls raises\nan exception, that exception is raised here when the implicit next()\ncall inside the list  constructor tries to retrieve the corresponding\nreturn value from the iterator .\nCall the main  function from the flags  module, passing the concurrent\nversion of download_many .\nNote that the download_one  function from Example 21-3  is essentially\nthe body of the for  loop in the download_many  function from\nExample 21-2 . This is a common refactoring when writing concurrent code:\nturning the body of a sequential for  loop into a function to be called\nconcurrently .\nT I P\nExample 21-3  is very short because I was able to reuse most functions from the\nsequential _flags.py_  script. One of the best features of concurrent.futures\nis to make it simple to add concurrent execution on top of legacy sequential code.\nThe ThreadPoolExecutor  constructor takes several ar guments not\nshown, but the first and most important one is max_workers , setting the\nmaximum number of worker threads to be executed. Until Python 3.4,\nmax_workers  was required. In 3.5, max_workers  became optional,\nwith a default of None . When max_workers  is None, the\nThreadPoolExecutor  decides its value using the following expression\n—since Python 3.8:",4415
285-Where Are the Futures.pdf,285-Where Are the Futures,"max_workers  = min(32, os.cpu_count () + 4)\nThe rationale is well explained in the ThreadPoolExecutor\ndocumentation :\nThis default value pr eserves at least 5 workers for I/O bound tasks. It\nutilizes at most 32 CPU cor es for CPU bound tasks which r elease the\nGIL. And it avoids using very lar ge r esour ces implicitly on many-cor e\nmachines.\nThreadPoolExecutor  now r euses idle worker thr eads befor e\nstarting max_workers  worker thr eads too.\nT o conclude: the computed default for max_workers  is sensible, and\nThreadPoolExecutor  avoids starting new workers unnecessarily .\nUnderstanding the logic behind max_workers  may help you decide when\nand how to set it yourself.\nThe library is called concurrency.futures  yet there are no futures to\nbe seen in Example 21-3 , so you may be wondering where they are. The\nnext section explains.\nWhere Are the Futures?\nFutures are essential components in the internals of\nconcurrent.futures  and of asyncio , but as users of these libraries\nwe sometimes don’ t see them. Example 21-3  leverages futures behind the\nscenes, but the code I wrote does not touch them directly . This section is an\noverview of futures, with an example that shows them in action.\nSince Python 3.4, there are two classes named Future  in the standard\nlibrary: concurrent.futures.Future  and asyncio.Future .\nThey serve the same purpose: an instance of either Future  class\nrepresents a deferred computation that may or may not have completed.\nThis is similar to the Deferred  class in T wisted, the Future  class in\nT ornado, and Promise  in modern JavaScript.\nFutures encapsulate pending operations so that they can be put in queues,\ntheir state of completion can be queried, and their results (or exceptions)\ncan be retrieved when available.\nAn important thing to know about futures is that you and I should not create\nthem: they are meant to be instantiated exclusively by the concurrency\nframework, be it concurrent.futures  or asyncio . Here is why: a\nFuture  represents something that will eventually run, and the only way to\nbe sure that something will run is to schedule its execution. In particular ,\nconcurrent.futures.Future  instances are created only as the\nresult of scheduling a callable for execution with a\nconcurrent.futures.Executor  subclass. For example, the\nExecutor.submit()  method takes a callable, schedules it to run, and\nreturns a Future .\nClient code is not supposed to change the state of a future: the concurrency\nframework changes the state of a future when the computation it represents\nis done, and we can’ t control when that happens.\nBoth types of Future  have a .done()  method that is nonblocking and\nreturns a Boolean that tells you whether the callable wrapped that future has\nexecuted or not. However , instead of repeatedly asking whether a future is\ndone, client code usually asks to be notified. That’ s why both Future\nclasses have an .add_done_callback()  method: you give it a\ncallable, and the callable will be invoked with the future as the single\nar gument when the future is done.\nThere is also a .result()  method, which works the same in both classes\nwhen the future is done: it returns the result of the callable, or re-raises\nwhatever exception might have been thrown when the callable was\nexecuted. However , when the future is not done, the behavior of the\nresult  method is very dif ferent between the two flavors of Future . In a\nconcurrency.futures.Future  instance, invoking f.result()\nwill block the caller ’ s thread until the result is ready . An optional timeout\nar gument can be passed, and if the future is not done in the specified time,\nthe result  method raises TimeoutError . In [Link to Come], we’ll see\nthat the asyncio.Future.result  method does not support timeout,\nand the preferred way to get the result of futures in that library is to use\nawait —which doesn’ t work with concurrency.futures.Future\ninstances.\nSeveral functions in both libraries return futures; others use them in their\nimplementation in a way that is transparent to the user . An example of the\nlatter is the Executor.map  we saw in Example 21-3 : it returns an iterator\nin which __next__  calls the result  method of each future, so we get\nthe results of the futures, and not the futures themselves.\nT o get a practical look at futures, we can rewrite Example 21-3  to use the\nconcurrent.futures.as_completed  function, which takes an\niterable of futures and returns an iterator that yields futures as they are\ndone.\nUsing futures.as_completed  requires changes to the\ndownload_many  function only . The higher -level executor.map  call is\nreplaced by two for  loops: one to create and schedule the futures, the other\nto retrieve their results. While we are at it, we’ll add a few print  calls to\ndisplay each future before and after it’ s done Example 21-4  shows the code\nfor a new download_many  function. The code for download_many\ngrew from 5 to 17 lines, but now we get to inspect the mysterious futures.\nThe remaining functions are the same as in Example 21-3 .\nExample 21-4. flags_thr eadpool_futur es.py: r eplacing executor .map with\nexecutor .submit and futur es.as_completed in the download_many function\ndef download_many (cc_list: list[str]) -> int: \n    cc_list = cc_list[:5]  \n \n    with futures.ThreadPoolExecutor (max_workers =3) as executor :  \n \n        to_do: list[futures.Future] = [] \n        for cc in sorted(cc_list):  \n \n            future = executor .submit(download_one , cc)  \n \n            to_do.append(future)  \n \n            print(f'Scheduled for {cc}: {future} ')  \n \n \n        for count, future in enumerate (futures.as_completed (to_do), \n1):  \n \n            res: str = future.result()  \n \n            print(f'{future} result: {res!r} ')  \n \n \n    return count\nFor this demonstration, use only the top five most populous countries.\nSet max_workers  to 3  so we can see pending futures in the output.\nIterate over country codes alphabetically , to make it clear that results\nwill arrive out of order .\nexecutor.submit  schedules the callable to be executed, and returns\na future  representing this pending operation.\nStore each future  so we can later retrieve them with\nas_completed .\nDisplay a message with the country code and the respective future .\nas_completed  yields futures as they are completed.\nGet the result of this future .\nDisplay the future  and its result.\nNote that the future.result()  call will never block in this example\nbecause the future  is coming out of as_completed . Example 21-5\nshows the output of one run of Example 21-4 .\nExample 21-5. Output of flags_thr eadpool_futur es.py\n$ python3 flags_threadpool_futures.py  \nScheduled for BR: <Future at 0x100791518 state=running>  \n  \nScheduled for CN: <Future at 0x100791710 state=running>  \nScheduled for ID: <Future at 0x100791a90 state=running>  \nScheduled for IN: <Future at 0x101807080 state=pending>  \n  \nScheduled for US: <Future at 0x101807128 state=pending>  \nCN <Future at 0x100791710 state=finished returned str> result: 'CN'   \n \nBR ID <Future at 0x100791518 state=finished returned str> result:  \n'BR'  \n \n<Future at 0x100791a90 state=finished returned str> result: 'ID'  \nIN <Future at 0x101807080 state=finished returned str> result: 'IN'  \nUS <Future at 0x101807128 state=finished returned str> result: 'US'  \n \n5 downloads in 0.70s\nThe futures are scheduled in alphabetical order; the repr()  of a future\nshows its state: the first three are running , because there are three\nworker threads.\nThe last two futures are pending , waiting for worker threads.\nThe first CN  here is the output of download_one  in a worker thread;\nthe rest of the line is the output of download_many .\nHere two threads output codes before download_many  in the main\nthread can display the result of the first thread.\nT I P\nI recommend experimenting with flags_thr eadpool_futur es.py . If you run it several\ntimes, you’ll see the order of the results varying. Increasing max_workers  to 5  will\nincrease the variation in the order of the results. Decreasing it to 1  will make this script\nrun sequentially , and the order of the results will always be the order of the submit\ncalls.\nW e saw two variants of the download script using\nconcurrent.futures : Example 21-3  with\nThreadPoolExecutor.map  and Example 21-4  with\nfutures.as_completed . If you are curious about the code for\nflags_asyncio.py , you may peek at Example 22-3  in Chapter 22 , where it is\nexplained.\nNow let’ s take a brief look at a simple way to work around the GIL for\nCPU-bound jobs using concurrent.futures .",8717
286-Launching Processes with concurrent.futures.pdf,286-Launching Processes with concurrent.futures,,0
287-Multi-core Prime Checker Redux.pdf,287-Multi-core Prime Checker Redux,"L a u n c h i n g  P r o c e s s e s  w i t h\nc o n c u r r e n t . f u t u r e s\nThe concurrent.futures  documentation page  is subtitled “Launching\nparallel tasks”. The package enables parallel computation on multi-core\nmachines because it supports distributing work among multiple Python\nprocesses using the ProcessPoolExecutor  class.\nBoth ProcessPoolExecutor  and ThreadPoolExecutor\nimplement the Executor  interface, so it’ s easy to switch from a thread-\nbased to a process-based solution using concurrent.futures .\nThere is no advantage in using a ProcessPoolExecutor  for the flags\ndownload example or any I/O-bound job. It’ s easy to verify this; just change\nthese lines in Example 21-3 :\ndef download_many (cc_list: list[str]) -> int: \n    with futures.ThreadPoolExecutor () as executor :\nT o this:\ndef download_many (cc_list: list[str]) -> int: \n    with futures.ProcessPoolExecutor () as executor :\nThe constructor for ProcessPoolExecutor  also has a max_workers\nparameter which defaults to None . In that case, the executor limits the\nnumber of workers to the number returned by os.cpu_count() .\nProcesses use more memory and take longer to start than threads, so the real\nvalue of ProcessPoolExecutor  is in CPU-intensive jobs. Let’ s go\nback to the primality test example of “A Homegrown Process Pool” ,\nrewriting it with concurrent.futures .\nMulti-core Prime Checker Redux\nIn “Code for the Multi-core Prime Checker”  we studied pr ocs.py , a script\nthat checked the primality of some lar ge numbers using\nmultiprocessing . In Example 21-6  we solve the same problem in the\npr oc_pool.py  program using a ProcessPoolExecutor . From the first\nimport to the main()  call at the end, pr ocs.py  has 43 non-blank lines of\ncode, and pr oc_pool.py  has 31—28% shorter .\nExample 21-6. pr oc_pool.py: pr ocs.py  r ewritten with\nProcessPoolExecutor\nimport sys \nfrom concurrent  import futures  \n \nfrom time import perf_counter  \nfrom typing import NamedTuple  \n \nfrom primes import is_prime , NUMBERS \n \nclass PrimeResult (NamedTuple ):  \n \n    n: int \n    flag: bool \n    elapsed: float \n \ndef check(n: int) -> PrimeResult : \n    t0 = perf_counter () \n    res = is_prime (n) \n    return PrimeResult (n, res, perf_counter () - t0) \n \ndef main() -> None: \n    if len(sys.argv) < 2: \n        workers = None      \n \n    else: \n        workers = int(sys.argv[1]) \n \n    executor  = futures.ProcessPoolExecutor (workers)  \n \n    actual_workers  = executor ._max_workers   # type: ignore  \n  \n \n    print(f'Checking {len(NUMBERS)} numbers with {actual_workers}  \nprocesses: ') \n \n    t0 = perf_counter () \n \n    numbers = sorted(NUMBERS, reverse=True)  \n \n    with executor :  \n \n        for n, prime, elapsed in executor .map(check, numbers):  \n \n            label = 'P' if prime else ' ' \n            print(f'{n:16}  {label} {elapsed:9.6f}s ') \n \n    time = perf_counter () - t0 \n    print(f'Total time: {time:.2f}s ') \n \nif __name__  == '__main__ ': \n    main()\nNo need to import multiprocessing , SimpleQueue  etc.;\nconcurrent.futures  hides all that.\nThe PrimeResult  tuple and the check  function are the same we\nsaw in pr ocs.py , but we don’ t need the queues and the worker  function\nanymore.\nInstead of deciding ourselves how many workers to use if no command-\nline ar gument was given, we set workers  to None  and let the\nProcessPoolExecutor  decide.\nHere I build the ProcessPoolExecutor  before the with  block in\n➐  so that I can display the actual number of workers in the next line.\n_max_workers  is an undocumented instance attribute of a\nProcessPoolExecutor . I decided to use it to show the number of\nworkers when the workers  variable is None ; mypy  correctly\ncomplains when I access it, so I put the type: ignore  comment to\nsilence it.\nSort the numbers to be checked in descending order . This will expose a\ndif ference in the behavior of pr oc_pool.py  when compared with\npr ocs.py . See below .\nUse the executor  as a context manager , as usual.\nThe executor.map  call will return the PrimeResult  instances\nreturned by check  in the same order as the numbers  ar guments.\nIf you run Example 21-6 , you’ll see the results appearing in strict\ndescending order , as shown in Example 21-7 . In contrast, the ordering of\nthe output of pr ocs.py  (shown in “Process-based Solution” ) is heavily\ninfluenced by the dif ficulty in checking whether each number is a prime.\nFor example, pr ocs.py  shows the result for 7777777777777777 near the top,\nbecause it has a low divisor , 7, so is_prime  quickly determines it’ s not a\nprime. In contrast, 7777777536340681 is 88191709  so is_prime  will\ntake much longer to determine that it’ s a composite number , end even\nlonger to find out that 7777777777777753 is prime—therefore both of these\nnumbers appear near the end of the output of pr ocs.py .\nRunning pr oc_pool.py  you’ll observe not only the descending order of the\nresults, but also that the program will appear to be stuck after showing the\nresult for 9999999999999999.\nExample 21-7. Output of pr oc_pool.py\n$ ./proc_pool.py  \nChecking 20 numbers with 12 processes:  \n9999999999999999     0.000024s  \n  \n9999999999999917  P  9.500677s  \n  \n7777777777777777     0.000022s  \n  \n7777777777777753  P  8.976933s  \n7777777536340681     8.896149s  \n6666667141414921     8.537621s  \n6666666666666719  P  8.548641s  \n6666666666666666     0.000002s  \n5555555555555555     0.000017s  \n5555555555555503  P  8.214086s  \n5555553133149889     8.067247s  \n4444444488888889     7.546234s  \n4444444444444444     0.000002s  \n4444444444444423  P  7.622370s  \n3333335652092209     6.724649s  \n3333333333333333     0.000018s  \n3333333333333301  P  6.655039s  \n 299593572317531  P  2.072723s  \n 142702110479723  P  1.461840s  \n               2  P  0.000001s  \nTotal time: 9.65s\nThis line appears very quickly .\nThis line takes more than 9.5s to show up.2\nAll the remaining lines appear almost immediately .\nHere is why pr oc_pool.py  behaves in that way:\nAs mentioned before, executor.map(check, numbers)\nreturns the result in the same order as the numbers  are given.\nBy default, pr oc_pool.py  uses as many workers as there are CPUs\n—it’ s what ProcessPoolExecutor  does when\nmax_workers  is None . That’ s 12 processes in this laptop.\nBecause we are submitting numbers  in descending order , the first\nis 9999999999999999, with 9 as a divisor it returns quickly .\nThe second number is 9999999999999917, the lar gest prime in the\nsample. This will take longer than all the others to check.\nMeanwhile, the remaining 1 1 processes will be checking other\nnumbers which are either primes or composites with lar ge factors,\nor composites with very small factors.\nWhen the worker in char ge of 9999999999999917 finally\ndetermines that’ s a prime, all the other processes have completed\ntheir last jobs, so the results appear immediately after .\nN O T E\nAlthough the progress of pr oc_pool.py  is not as visible as that of pr ocs.py , the overall\nexecution time is practically the same as depicted in Figure 20-2 , for the same number\nof workers and CPU cores.\nUnderstanding how concurrent programs behave is not straightforward, so\nhere’ s is a second experiment that may help you visualize the operation of\nExecutor.map .",7389
288-Experimenting with Executor.map.pdf,288-Experimenting with Executor.map,"E x p e r i m e n t i n g  w i t h  E x e c u t o r . m a p\nLet’ s investigate Executor .map`, now using a ThreadPoolExecutor\nwith three workers running five callables that output timestamped\nmessages. The code is in Example 21-8 , the out put in Example 21-9 .\nExample 21-8. demo_executor_map.py: Simple demonstration of the map\nmethod of Thr eadPoolExecutor\nfrom time import sleep, strftime  \nfrom concurrent  import futures \n \ndef display(*args):  \n \n    print(strftime ('[%H:%M:%S]'), end=' ') \n    print(*args) \n \ndef loiter(n):  \n \n    msg = '{}loiter({}): doing nothing for {}s... ' \n    display(msg.format('\t'*n, n, n)) \n    sleep(n) \n    msg = '{}loiter({}): done. ' \n    display(msg.format('\t'*n, n)) \n    return n * 10  \n \n \ndef main(): \n    display('Script starting. ') \n    executor  = futures.ThreadPoolExecutor (max_workers =3)  \n \n    results = executor .map(loiter, range(5))  \n \n    display('results: ', results)  \n \n    display('Waiting for individual results: ') \n    for i, result in enumerate (results):  \n \n        display(f'result {i}: {result} ') \n \nif __name__  == '__main__ ': \n    main()\nThis function simply prints whatever ar guments it gets, preceded by a\ntimestamp in the format [HH:MM:SS] .\nloiter  does nothing except display a message when it starts, sleep for\nn  seconds, then display a message when it ends; tabs are used to indent\nthe messages according to the value of n .\nloiter  returns n * 10  so we can see how to collect results.\nCreate a ThreadPoolExecutor  with three threads.\nSubmit five tasks to the executor . Since there are only three threads,\nonly three of those tasks will start immediately: the calls loiter(0) ,\nloiter(1) , and loiter(2) ); this is a nonblocking call.\nImmediately display the results  of invoking executor.map : it’ s a\ngenerator , as the output in Example 21-9  shows.\nThe enumerate  call in the for  loop will implicitly invoke\nnext(results) , which in turn will invoke _f.result()  on the\n(internal) _f  future representing the first call, loiter(0) . The\nresult  method will block until the future is done, therefore each\niteration in this loop will have to wait for the next result to be ready .\nI encourage you to run Example 21-8  and see the display being updated\nincrementally . While you’re at it, play with the max_workers  ar gument\nfor the ThreadPoolExecutor  and with the range  function that\nproduces the ar guments for the executor.map  call—or replace it with\nlists of handpicked values to create dif ferent delays.\nExample 21-9  shows a sample run of Example 21-8 .\nExample 21-9. Sample run of demo_executor_map.py fr om Example 21-8\n$ python3 demo_executor_map.py  \n[15:56:50 ] Script starting.   \n \n[15:56:50 ] loiter(0): doing nothing for 0s...  \n \n[15:56:50 ] loiter(0): done. \n[15:56:50 ]      loiter(1): doing nothing for 1s...  \n \n[15:56:50 ]              loiter(2): doing nothing for 2s... \n[15:56:50 ] results:  <generator  object result_iterator  at \n0x106517168>   \n \n[15:56:50 ]                      loiter(3): doing nothing for 3s...  \n \n[15:56:50 ] Waiting for individual  results:  \n[15:56:50 ] result 0: 0  \n \n[15:56:51 ]      loiter(1): done. \n \n[15:56:51 ]                              loiter(4): doing nothing \nfor 4s... \n[15:56:51 ] result 1: 10  \n \n[15:56:52 ]              loiter(2): done.  \n \n[15:56:52 ] result 2: 20 \n[15:56:53 ]                      loiter(3): done. \n[15:56:53 ] result 3: 30 \n[15:56:55 ]                              loiter(4): done.  \n \n[15:56:55 ] result 4: 40\nThis run started at 15:56:50.\nThe first thread executes loiter(0) , so it will sleep for 0s and return\neven before the second thread has a chance to start, but YMMV .\nloiter(1)  and loiter(2)  start immediately (because the thread\npool has three workers, it can run three functions concurrently).\nThis shows that the results  returned by executor.map  is a\ngenerator; nothing so far would block, regardless of the number of tasks\nand the max_workers  setting.\nBecause loiter(0)  is done, the first worker is now available to start\nthe fourth thread for loiter(3) .\nThis is where execution may block, depending on the parameters given\nto the loiter  calls: the __next__  method of the results\ngenerator must wait until the first future is complete. In this case, it\nwon’ t block because the call to loiter(0)  finished before this loop\nstarted. Note that everything up to this point happened within the same\nsecond: 15:56:50.\nloiter(1)  is done one second later , at 15:56:51. The thread is freed\nto start loiter(4) .\nThe result of loiter(1)  is shown: 10. Now the for  loop will block\nwaiting for the result of loiter(2) .4",4734
289-Downloads with Progress Display and Error Handling.pdf,289-Downloads with Progress Display and Error Handling,"The pattern repeats: loiter(2)  is done, its result is shown; same with\nloiter(3) .\nThere is a 2s delay until loiter(4)  is done, because it started at\n15:56:51 and did nothing for 4s.\nThe Executor.map  function is easy to use, but often it’ s preferable to get\nthe results as they are ready , regardless of the order they were submitted. T o\ndo that, we need a combination of the Executor.submit  method and\nthe futures.as_completed  function, as we saw in Example 21-4 .\nW e’ll come back to this technique in “Using futures.as_completed” .\nT I P\nThe combination of executor.submit  and futures.as_completed  is more\nflexible than executor.map  because you can submit  dif ferent callables and\nar guments, while executor.map  is designed to run the same callable on the dif ferent\nar guments. In addition, the set of futures you pass to futures.as_completed  may\ncome from more than one executor—perhaps some were created by a\nThreadPoolExecutor  instance while others are from a\nProcessPoolExecutor .\nIn the next section, we will resume the flag download examples with new\nrequirements that will force us to iterate over the results of\nfutures.as_completed  instead of using executor.map .\nD o w n l o a d s  w i t h  P r o g r e s s  D i s p l a y  a n d  E r r o r\nH a n d l i n g\nAs mentioned, the scripts in “Concurrent W eb Downloads”  have no error\nhandling to make them easier to read and to contrast the structure of the\nthree approaches: sequential, threaded, and asynchronous.\nIn order to test the handling of a variety of error conditions, I created the\nflags2  examples:\nflags2_common.py\nThis module contains common functions and settings used by all\nflags2  examples, including a main  function, which takes care of\ncommand-line parsing, timing, and reporting results. This is really\nsupport code, not directly relevant to the subject of this chapter , so I will\nnot list the source code here, but you can find it the Fluent Python 2e\ncode repository .\nflags2_sequential.py\nA sequential HTTP client with proper error handling and progress bar\ndisplay . Its download_one  function is also used by\nflags2_threadpool.py .\nflags2_thr eadpool.py\nConcurrent HTTP client based on\nfutures.ThreadPoolExecutor  to demonstrate error handling\nand integration of the progress bar .\nflags2_asyncio.py\nSame functionality as previous example but implemented with\nasyncio  and aiohttp . This will be covered in “Enhancing the\nasyncio downloader” , in Chapter 22 .\nB E  C A R E F U L  W H E N  T E S T I N G  C O N C U R R E N T\nC L I E N T S\nWhen testing concurrent HTTP clients on public W eb servers, you may generate many\nrequests per second, and that’ s how denial-of-service (DoS) attacks are made. Carefully\nthrottle your clients when hitting public servers. For high-concurrency experiments, set\nup a local HTTP server for testing. The ThreadingHTTPServer  that comes with\nPython is OK for testing , and it can serve files in the current directory if you run it\nwith:\npython -m http.server\nAppend the -h  option to the command above for more options.\nThe most visible feature of the flags2  examples is that they have an\nanimated, text-mode progress bar implemented with the TQDM package . I\nposted a 108s video on Y ouT ube  to show the progress bar and contrast the\nspeed of the three flags2  scripts. In the video, I start with the sequential\ndownload, but I interrupt it after 32s because it was going to take more than\n5 minutes to hit on 676 URLs and get 194 flags; I then run the threaded and\nasyncio  scripts three times each, and every time they complete the job in\n6s or less (i.e., more than 60 times faster). Figure 21-1  shows two\nscreenshots: during and after running flags2_thr eadpool.py .5\nFigur e 21-1. T op-left: flags2_thr eadpool.py running with live pr ogr ess bar generated by tqdm;\nbottom-right: same terminal window after the script is finished.\nTQDM is very easy to use, the simplest example appears in an animated .gif\nin the project’ s README.md . If you type the following code in the Python\nconsole after installing the tqdm  package, you’ll see an animated progress\nbar were the comment is:\n>>> import time \n>>> from tqdm import tqdm \n>>> for i in tqdm(range(1000)): \n...     time.sleep(.01) \n... \n>>> # -> progress bar will appear here <-\nBesides the neat ef fect, the tqdm  function is also interesting conceptually:\nit consumes any iterable and produces an iterator which, while it’ s\nconsumed, displays the progress bar and estimates the remaining time to\ncomplete all iterations. T o compute that estimate, tqdm  needs to get an\niterable that has a len , or receive as a second ar gument the expected\nnumber of items. Integrating TQDM with our flags2  examples provides\nan opportunity to look deeper into how the concurrent scripts actually work,\nby forcing us to use the futures.as_completed  and the\nasyncio.as_completed  functions so that tqdm  can display progress\nas each future is completed.\nThe other feature of the flags2  example is a command-line interface. All\nthree scripts accept the same options, and you can see them by running any\nof the scripts with the -h  option. Example 21-10  shows the help text.\nExample 21-10. Help scr een for the scripts in the flags2 series\n$ python3 flags2_threadpool.py -h  \nusage: flags2_threadpool.py [-h] [-a] [-e] [-l N] [-m CONCURRENT]  \n[-s LABEL]  \n                            [-v]  \n                            [CC [CC ...]]  \n \nDownload flags for country codes. Default: top 20 countries by  \npopulation.  \n \npositional arguments:  \n  CC                    country code or 1st letter (eg. B for  \nBA...BZ)  \n \noptional arguments:  \n  -h, --help            show this help message and exit  \n  -a, --all             get all available flags (AD to ZW)  \n  -e, --every           get flags for every possible code (AA...ZZ)  \n  -l N, --limit N       limit to N first codes  \n  -m CONCURRENT, --max_req CONCURRENT  \n                        maximum concurrent requests (default=30)  \n  -s LABEL, --server LABEL  \n                        Server to hit; one of DELAY, ERROR, LOCAL,  \nREMOTE \n                        (default=LOCAL)  \n  -v, --verbose         output detailed progress info\nAll ar guments are optional. The most important ar guments are discussed\nnext.\nOne option you can’ t ignore is -s/--server : it lets you choose which\nHTTP server and base URL will be used in the test. Y ou can pass one of\nfour strings to determine where the script will look for the flags (the strings\nare case insensitive):\nLOCAL\nUse http://localhost:8000/flags ; this is the default. Y ou\nshould configure a local HTTP server to answer at port 8000. See\n“Setting up test servers”  for instructions.\nREMOTE\nUse http://fluentpython.com/data/flags ; that is a public\nwebsite owned by me, hosted on a shared server . Please do not pound it\nwith too many concurrent requests. The fluentpython.com  domain\nis handled by the Cloudflare  CDN (Content Delivery Network) so you\nmay notice that the first downloads are slower , but they get faster when\nthe CDN cache warms up.\nDELAY\nUse http://localhost:8001/flags ; a server delaying HTTP\nresponses should be listening to port 8001. I wrote slow_server .py  to\nmake it easier to experiment. Y ou’ll find it in the 21-futur es/getflags/\ndirectory of the Fluent Python 2e  code repository . See “Setting up test\nservers”  for instructions.\nERROR\nUse http://localhost:8002/flags ; a server introducing\nHTTP errors and delaying responses should be installed at port 8002.\nRunning slow_server .py  is an easy way to do it. See “Setting up test\nservers” .6\nS E T T I N G  U P  T E S T  S E R V E R S\nIf you don’ t already have a local HTTP server for testing, here are the\nsteps for an easy way to do it:\n1 . Clone or download the Fluent Python 2e  code repository .\n2 . Open your shell and go to the 21-futur es/getflags/  directory of\nyour local copy of the repository .\n3 . Unzip the flags.zip  file, creating a flags  directory at 21-\nfutur es/getflags/flags/ .\n4 . Open a second shell, go to the 21-futur es/getflags/  directory\nand run python3 -m http.server . This will start a\nThreadingHTTPServer  listening to port 8000, serving the\nlocal files. If you open the URL http://localhost:8000/flags/\nwith your browser , you’ll see a long list of directories named\nwith two-letter country codes from ad/  to zw/ .\n5 . Now you can go back to the first shell and run the flags2*.py\nexamples with the default --server LOCAL  option.\n6 . T o test with the --server DELAY  option, go to 21-\nfutur es/getflags/  and run python3 slow_server.py\n8001 . This will add a .5s delay before each response.\n7 . T o test with the --server ERROR  option, go to 21-\nfutur es/getflags/  and run python3 slow_server.py\n8002 --error-rate .25 . Each request will have a 25%\nprobability of getting a 418 I’m a teapot  response, and all\nresponses will be delayed .5s.\nI wrote slow_server .py  reusing code from Python’ s http.server\nstandard library module, which “is not recommended for production”—\naccording to the documentation . T o set up a more reliable testing\nenvironment, I recommend configuring Nginx  and toxiproxy  with\nequivalent parameters.\nBy default, each flags2*.py  script will fetch the flags of the 20 most\npopulous countries from the LOCAL  server\n(http://localhost:8000/flags ) using a default number of\nconcurrent connections, which varies from script to script. Example 21-1 1\nshows a sample run of the flags2_sequential.py  script using all defaults.\nExample 21-1 1. Running flags2_sequential.py with all defaults: LOCAL\nsite, top-20 flags, 1 concurr ent connection\n$ python3 flags2_sequential.py  \nLOCAL site: http://localhost:8000/flags  \nSearching for 20 flags: from BD to VN  \n1 concurrent connection will be used.  \n--------------------  \n20 flags downloaded.  \nElapsed time: 0.10s\nY ou can select which flags will be downloaded in several ways.\nExample 21-12  shows how to download all flags with country codes\nstarting with the letters A, B, or C.\nExample 21-12. Run flags2_thr eadpool.py to fetch all flags with country\ncodes pr efixes A, B, or C fr om DELA Y server\n$ python3 flags2_threadpool.py -s DELAY a b c  \nDELAY site: http://localhost:8001/flags  \nSearching for 78 flags: from AA to CZ  \n30 concurrent connections will be used.  \n--------------------  \n43 flags downloaded.  \n35 not found.  \nElapsed time: 1.72s\nRegardless of how the country codes are selected, the number of flags to\nfetch can be limited with the -l/--limit  option. Example 21-13\ndemonstrates how to run exactly 100 requests, combining the -a  option to\nget all flags with -l 100 .",10785
290-Error Handling in the flags2 Examples.pdf,290-Error Handling in the flags2 Examples,"Example 21-13. Run flags2_asyncio.py to get 100 flags (-al 100) fr om the\nERROR server , using 100 concurr ent r equests (-m 100)\n$ python3 flags2_asyncio.py -s ERROR -al 100 -m 100  \nERROR site: http://localhost:8002/flags  \nSearching for 100 flags: from AD to LK  \n100 concurrent connections will be used.  \n--------------------  \n73 flags downloaded.  \n27 errors.  \nElapsed time: 0.64s\nThat’ s the user interface of the flags2  examples. Let’ s see how they are\nimplemented.\nError Handling in the flags2 Examples\nThe common strategy in all three examples to deal with HTTP errors is that\n404 errors (not found) are handled by the function in char ge of downloading\na single file ( download_one ). Any other exception propagates to be\nhandled by the download_many  function or the supervisor  coroutine\n—in the asyncio  example.\nOnce more, we’ll start by studying the sequential code, which is easier to\nfollow—and mostly reused by the thread pool script. Example 21-14  shows\nthe functions that perform the actual downloads in the flags2_sequential.py\nand flags2_thr eadpool.py  scripts.\nExample 21-14. flags2_sequential.py: basic functions in char ge of\ndownloading; both ar e r eused in flags2_thr eadpool.py\ndef get_flag (base_url : str, cc: str) -> bytes: \n    url = f'{base_url}/{cc}/{cc}.gif '.lower() \n    resp = requests .get(url) \n    if resp.status_code  != 200:  \n \n        resp.raise_for_status () \n    return resp.content \n \ndef download_one (cc: str, base_url : str, verbose: bool = False): \n    try: \n        image = get_flag (base_url , cc) \n    except requests .exceptions .HTTPError  as exc:  \n \n        res = exc.response  \n        if res.status_code  == 404: \n            status = HTTPStatus .not_found   \n \n            msg = 'not found ' \n        else:  \n \n            raise \n    else: \n        save_flag (image, f'{cc}.gif ') \n        status = HTTPStatus .ok \n        msg = 'OK' \n \n    if verbose:  \n \n        print(cc, msg) \n \n    return Result(status, cc)  \nget_flag  uses requests.Response.raise_for_status  to\nraise an exception for any HTTP code other than 200.\ndownload_one  catches requests.exceptions.HTTPError\nto handle HTTP code 404 specifically…\n…by setting its local status  to HTTPStatus.not_found ;\nHTTPStatus  is an Enum  imported from flags2_common.py .\nAny other HTTPError  exception is re-raised; other exceptions will\njust propagate to the caller .\nIf the -v/--verbose  command-line option is set, the country code\nand status message will be displayed; this how you’ll see progress in the\nverbose mode.\nThe Result  tuple returned by download_one  will have a status\nfield with a value of HTTPStatus.not_found  or\nHTTPStatus.ok .\nExample 21-15  lists the sequential version of the download_many\nfunction. This code is straightforward, but its worth studying to contrast\nwith the concurrent versions coming up. Focus on how it reports progress,\nhandles errors, and tallies downloads.\nExample 21-15. flags2_sequential.py: the sequential implementation of\ndownload_many\ndef download_many (cc_list: list[str], \n                  base_url : str, \n                  verbose: bool, \n                  _unused_concur_req : int) -> Counter[int]: \n    counter: Counter[int] = Counter()  \n \n    cc_iter = sorted(cc_list)  \n \n    if not verbose: \n        cc_iter = tqdm.tqdm(cc_iter)  \n \n    for cc in cc_iter:  \n \n        try: \n            res = download_one (cc, base_url , verbose)  \n \n        except requests .exceptions .HTTPError  as exc:  \n \n            error_msg  = 'HTTP error {res.status_code} -  \n{res.reason} ' \n            error_msg  = error_msg .format(res=exc.response ) \n        except requests .exceptions .ConnectionError :  \n \n            error_msg  = 'Connection error ' \n        else:  \n \n            error_msg  = '' \n            status = res.status \n \n        if error_msg : \n            status = HTTPStatus .error  \n \n        counter[status] += 1           \n  \n        if verbose and error_msg :      \n \n            print(f'*** Error for {cc}: {error_msg} ') \n \n    return counter  \nThis Counter  will tally the dif ferent download outcomes:\nHTTPStatus.ok , HTTPStatus.not_found , or\nHTTPStatus.error .\ncc_iter  holds the list of the country codes received as ar guments,\nordered alphabetically .\nIf not running in verbose mode, cc_iter  is passed to the tqdm\nfunction, which will return an iterator that yields the items in cc_iter",4506
291-Using futures.as_completed.pdf,291-Using futures.as_completed,"while also displaying the animated progress bar .\nThis for  loop iterates over cc_iter  and…\n…performs the download by successive calls to download_one .\nHTTP-related exceptions raised by get_flag  and not handled by\ndownload_one  are handled here.\nOther network-related exceptions are handled here. Any other exception\nwill abort the script, because the flags2_common.main  function\nthat calls download_many  has no try/except .\nIf no exception escaped download_one , then the status  is\nretrieved from the HTTPStatus  namedtuple  returned by\ndownload_one .\nIf there was an error , set the local status  accordingly .\nIncrement the counter by using the value of the HTTPStatus  Enum  as\nkey .\nIf running in verbose mode, display the error message for the current\ncountry code, if any .\nReturn the counter  so that the main  function can display the\nnumbers in its final report.\nW e’ll now study the refactored thread pool example, flags2_thr eadpool.py .\nUsing futures.as_completed\nIn order to integrate the TQDM progress bar and handle errors on each\nrequest, the flags2_thr eadpool.py  script uses\nfutures.ThreadPoolExecutor  with the\nfutures.as_completed  function we’ve already seen. Example 21-16\nis the full listing of flags2_thr eadpool.py . Only the download_many\nfunction is implemented; the other functions are reused from\nflags2_common.py  and flags2_sequential.py .\nExample 21-16. flags2_thr eadpool.py: full listing\nfrom collections  import Counter \nfrom concurrent  import futures \n \nimport requests  \nimport tqdm  # type: ignore  \n  \n \nfrom flags2_common  import main, HTTPStatus   \n \nfrom flags2_sequential  import download_one   \n \n \nDEFAULT_CONCUR_REQ  = 30  \n \nMAX_CONCUR_REQ  = 1000  \n \n \n \ndef download_many (cc_list: list[str], \n                  base_url : str, \n                  verbose: bool, \n                  concur_req : int) -> Counter[int]: \n    counter: Counter[int] = Counter() \n    with futures.ThreadPoolExecutor (max_workers =concur_req ) as \nexecutor :  \n \n        to_do_map  = {}  \n \n        for cc in sorted(cc_list):  \n \n            future = executor .submit(download_one , cc, \n                                     base_url , verbose)  \n \n            to_do_map [future] = cc  \n \n        done_iter  = futures.as_completed (to_do_map )  \n \n        if not verbose: \n            done_iter  = tqdm.tqdm(done_iter , total=len(cc_list))  \n \n        for future in done_iter :  \n \n            try: \n                res = future.result()  \n \n            except requests .exceptions .HTTPError  as exc:  \n \n                error_fmt  = 'HTTP {res.status_code} - {res.reason} ' \n                error_msg  = error_fmt .format(res=exc.response ) \n            except requests .exceptions .ConnectionError : \n                error_msg  = 'Connection error ' \n            else: \n                error_msg  = '' \n                status = res.status \n \n            if error_msg : \n                status = HTTPStatus .error \n            counter[status] += 1 \n            if verbose and error_msg : \n                cc = to_do_map [future]  \n \n                print(f'*** Error for {cc}: {error_msg} ') \n \n    return counter \n \n \nif __name__  == '__main__ ': \n    main(download_many , DEFAULT_CONCUR_REQ , MAX_CONCUR_REQ )\nImport the progress-bar display library , and tell mypy  to skip checking\nit.\nImport one function and one Enum  from the flags2_common\nmodule.\nReuse the download_one  from flags2_sequential\n( Example 21-14 ).\nIf the -m/--max_req  command-line option is not given, this will be\nthe maximum number of concurrent requests, implemented as the size\nof the thread pool; the actual number may be smaller , if the number of\nflags to download is smaller .\nMAX_CONCUR_REQ  caps the maximum number of concurrent requests\nregardless of the number of flags to download or the -m/--max_req\ncommand-line option; it’ s a safety precaution.\nCreate the executor  with max_workers  set to concur_req ,\ncomputed by the main  function as the smaller of: MAX_CONCUR_REQ ,\nthe length of cc_list , and the value of the -m/--max_req\ncommand-line option. This avoids creating more threads than necessary .\nThis dict  will map each Future  instance—representing one\ndownload—with the respective country code for error reporting.\nIterate over the list of country codes in alphabetical order . The order of\nthe results will depend on the timing of the HTTP responses more than\nanything, but if the size of the thread pool (given by concur_req ) is\nmuch smaller than len(cc_list) , you may notice the downloads\nbatched alphabetically .\nEach call to executor.submit  schedules the execution of one\ncallable and returns a Future  instance. The first ar gument is the\ncallable, the rest are the ar guments it will receive.\nStore the future  and the country code in the dict .\nfutures.as_completed  returns an iterator that yields futures as\nthey are done.\nIf not in verbose mode, wrap the result of as_completed  with the\ntqdm  function to display the progress bar; because done_iter  has no\nlen , we must tell tqdm  what is the expected number of items as the\ntotal=  ar gument, so tqdm  can estimate the work remaining.\nIterate over the futures as they are completed.\nCalling the result  method on a future either returns the value\nreturned by the callable, or raises whatever exception was caught when\nthe callable was executed. This method may block waiting for a\nresolution, but not in this example because as_completed  only\nreturns futures that are done.\nHandle the potential exceptions; the rest of this function is identical to\nthe sequential version of download_many  ( Example 21-15 ), except\nfor the next callout.\nT o provide context for the error message, retrieve the country code from\nthe to_do_map  using the current future  as key . This was not\nnecessary in the sequential version because we were iterating over the",5988
292-Further Reading.pdf,292-Further Reading,"list of country codes, so we had the current cc ; here we are iterating\nover the futures.\nT I P\nExample 21-16  uses an idiom that’ s very useful with futures.as_completed :\nbuilding a dict  to map each future to other data that may be useful when the future is\ncompleted. Here the to_do_map  maps each future to the country code assigned to it.\nThis makes it easy to do follow-up processing with the result of the futures, despite the\nfact that they are produced out of order .\nPython threads are well suited for I/O-intensive applications, and the\nconcurrent.futures  package makes them trivially simple to use for\ncertain use cases. W ith ProcessPoolExecutor , you can also solve\nCPU-intensive problems on multiple cores—if the computations are\n“embarrassingly parallel” . This concludes our basic introduction to\nconcurrent.futures .\nC h a p t e r  S u m m a r y\nW e started the chapter by comparing two concurrent HTTP clients with a\nsequential one, demonstrating significant performance gains over the\nsequential script.\nAfter studying the first example based on concurrent.futures , we\ntook a closer look at future objects, either instances of\nconcurrent.futures.Future , or asyncio.Future ,\nemphasizing what these classes have in common (their dif ferences will be\nemphasized in Chapter 22 ). W e saw how to create futures by calling\nExecutor.submit , and iterate over completed futures with\nconcurrent.futures.as_completed .\nW e then discussed the use of multiple processes with the\nconcurrent.futures.ProcessPoolExecutor  class, to go\naround the GIL and use multiple CPU cores to simplify the multicore prime\nchecker we first saw in Chapter 20 .\nIn the following section, we took a close look at how the\nconcurrent.futures.ThreadPoolExecutor  works, with a\ndidactic example launching tasks that did nothing for a few seconds, except\ndisplaying their status with a timestamp.\nNext we went back to the flag downloading examples. Enhancing them\nwith a progress bar and proper error handling prompted further exploration\nof the future.as_completed  generator function showing a common\npattern: storing futures in a dict  to link further information to them when\nsubmitting, so that we can use that information when the future comes out\nof the as_completed  iterator .\nF u r t h e r  R e a d i n g\nThe concurrent.futures  package was contributed by Brian Quinlan,\nwho presented it in a great talk titled “The Future Is Soon!”  at PyCon\nAustralia 2010. Quinlan’ s talk has no slides; he shows what the library does\nby typing code directly in the Python console. As a motivating example, the\npresentation features a short video with XKCD cartoonist/programmer\nRandall Munroe making an unintended DOS attack on Google Maps to\nbuild a colored map of driving times around his city . The formal\nintroduction to the library is PEP 3148 - futures  - execute computations\nasynchronously . In the PEP , Quinlan wrote that the\nconcurrent.futures  library was “heavily influenced by the Java\njava.util.concurrent  package.”\nFor additional resources covering concurrent.futures , please see\n“Further Reading”  ( Chapter 20 ). All the references that cover Python’ s\nthreading  and multiprocessing  in “Concurrency with threads and\nprocesses”  also cover concurrent.futures .\nS O A P B O X\nThr ead avoidance\nConcurr ency: one of the most difficult topics in computer science\n(usually best avoided).\n— David Beazley , Python coach and mad scientist\nI agree with the apparently contradictory quotes by David Beazley ,\nabove, and Michele Simionato at the start of this chapter .\nI attended a course about concurrency at the university . All we did was\nPOSIX threads  programming. What I learned: I don’ t want to manage\nthreads and locks myself, for the same reason that I don’ t want to\nmanage memory allocation and deallocation. Those jobs are best carried\nout by the systems programmers who have the know-how , the\ninclination, and the time to get them right—hopefully . I am paid to\ndevelop applications, not operating systems. I don’ t need all the fine\ngrained control of threads, locks, malloc , and free —see C dynamic\nmemory allocation .\nThat’ s why I think the concurrent.futures  package is\ninteresting: it treats threads, processes, and queues as infrastructure at\nyour service, not something you have to deal with directly . Of course,\nit’ s designed with simple jobs in mind, the so-called embarrassingly\nparallel problems. But that’ s a lar ge slice of the concurrency problems\nwe face when writing applications—as opposed to operating systems or\ndatabase servers, as Simionato points out in that quote.\nFor “nonembarrassing” concurrency problems, threads and locks are\nnot the answer either . Threads will never disappear at the OS level, but\nevery programming language I’ve found exciting in the last several\nyears provides higher -level, concurrency abstractions that are easier to\nuse correctly , as the Seven Concurr ency Models in Seven W eeks  book\ndemonstrates. Go, Elixir , and Clojure are among them. Erlang—the\nimplementation language of Elixir—is a prime example of a language\ndesigned from the ground up with concurrency in mind. It doesn’ t7\nexcite me for a simple reason: I find its syntax ugly . Python spoiled me\nthat way .\nJosé V alim, previously a Ruby on Rails core contributor , designed\nElixir with a pleasant, modern syntax. Like Lisp and Clojure, Elixir\nimplements syntactic macros. That’ s a double-edged sword. Syntactic\nmacros enable powerful DSLs, but the proliferation of sublanguages\ncan lead to incompatible codebases and community fragmentation. Lisp\ndrowned in a flood of macros, with each Lisp shop using its own arcane\ndialect. Standardizing around Common Lisp resulted in a bloated\nlanguage. I hope José V alim can inspire the Elixir community to avoid a\nsimilar outcome. So far , it’ s looking good. The Ecto  database wrapper\nand query generator is a joy to use: a great example of using macros to\ncreate a flexible yet user -friendly DSL—Domain Specific Language—\nfor interacting with relational and non-relational databases.\nLike Elixir , Go is a modern language with fresh ideas. But, in some\nregards, it’ s a conservative language, compared to Elixir . Go doesn’ t\nhave macros, and its syntax is simpler than Python’ s. Go doesn’ t\nsupport inheritance or operator overloading, and it of fers fewer\nopportunities for metaprogramming than Python. These limitations are\nconsidered features. They lead to more predictable behavior and\nperformance. That’ s a big plus in the highly concurrent, mission-critical\nsettings where Go aims to replace C++, Java, and Python.\nWhile Elixir and Go are direct competitors in the high-concurrency\nspace, their design philosophies appeal to dif ferent crowds. Both are\nlikely to thrive. But in the history of programming languages, the\nconservative ones tend to attract more coders. After I finish writing this\nbook, I will devote more time to become fluent in Go, Elixir , and the\nErlang/OTP platform.\n1  From Michele Simionato’ s post Threads, processes and concurrency in Python: some\nthoughts , subtitled “Removing the hype around the multicore (non) revolution and some\n(hopefully) sensible comment about threads and other forms of concurrency .”\n2  For servers which may be hit by many clients, there is a dif ference: coroutines scale better\nbecause they use much less memory than threads, and also reduce the cost of context\nswitching, which I mentioned in “Thread-based Non-solution” .\n3  The images are originally from the CIA W orld Factbook , a public-domain, U.S. government\npublication. I copied them to my site to avoid the risk of launching a DOS attack on CIA.gov .\n4  Y our mileage may vary: with threads, you never know the exact sequencing of events that\nshould happen practically at the same time; it’ s possible that, in another machine, you see\nloiter(1)  starting before loiter(0)  finishes, particularly because sleep  always\nreleases the GIL so Python may switch to another thread even if you sleep for 0s.\n5  In my testing, about 1% of the requests I make to ThreadingHTTPServer  fail. The docs\nwarn that it’ s not intended for production, and for testing purposes it’ s good that not all\nrequests work.\n6  Before configuring Cloudflare, I got HTTP 503 errors—Service T emporarily Unavailable—\nwhen testing the scripts with a few dozen concurrent requests on my inexpensive shared host\naccount. Now those errors are gone.\n7  Slide #9 from “A Curious Course on Coroutines and Concurrency ,”  tutorial presented at\nPyCon 2009.",8660
293-Example Probing Domains.pdf,293-Example Probing Domains,"Chapter 22. Asynchronous\nProgramming\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 22nd chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nThe pr oblem with normal appr oaches to asynchr onous pr ogramming as\nthat they’r e all-or -nothing pr opositions. Y ou r ewrite all your code so\nnone of it blocks or you’r e just wasting your time.\n— Alvaro V idela & Jason J. W . W illiams, RabbitMQ in\nAction\nThis chapter addresses three major topics that are closely related:\nPython’ s async def , await , async with , and async for\nconstructs;\nObjects supporting those constructs: native coroutines and\nasynchronous variants of context managers, iterables, generators,\nand comprehensions;\nasyncio  and other asynchronous libraries.1\nThat’ s a lot, so we’ll only scratch the surface asyncio  and the other\nlibraries. The other topics build on ideas we’ve seen before: iterables and\ngenerators ( Chapter 17 ), context managers ( Chapter 18 ), and coroutines\n( Chapter 19 ).\nAlso covered here:\nHow to avoid blocking the event loop by delegating slow\noperations to a thread or process pool;\nSimple network programs using asyncio , aiohttp , FastAPI ,\nand Curio ;\nAdvantages and pitfalls of asynchronous programming.\nT I P\nThe asyncio  documentation  is much better after Y uri Selivanov  reor ganized it,\nseparating the few functions useful to application developers from the low-level API for\ncreators of packages like W eb frameworks and database drivers.\nFor book-length coverage of asyncio , I recommend Using Asyncio in Python  by\nCaleb Hattingh (O’Reilly , 2020). Full disclosure: he is one of the tech reviewers of this\nbook.\nW h a t ’ s  N e w  i n  t h i s  C h a p t e r\nWhen I wrote Fluent Python, First Edition , the asyncio  library was\nprovisional and the async/await  keywords did not exist. Therefore, I\nhad to update all examples in this chapter . I also created new examples:\ndomain probing scripts, a FastAPI  W eb service, and experiments with\nPython’ s new asynchronous console mode.\nNew sections cover language features that did not exist at the time, such as\nnative coroutines, async with , async for  and the objects that support\nthose constructs.2\nThe ideas in “How Async W orks and How It Doesn’ t”  reflect hard earned\nlessons that I consider essential reading for anyone using asynchronous\nprogramming. They may save you a lot of trouble—whether you’re using\nPython or Node.js .\nFinally , I removed several paragraphs about asyncio.Futures , which\nis now considered part of the low-level asyncio  APIs.\nA  f e w  d e f i n i t i o n s\nAt the start of Chapter 19 , we saw that Python 3.5 and later of fer three\nkinds of coroutines:\nnative cor outines\nA coroutine defined with async def . Y ou can delegate from a native\ncoroutine to another native coroutine using the await  keyword, similar\nto how classic coroutines use yield from . The async def\nstatement always defines a native coroutine, even if the await\nkeyword is not used in its body . The await  keyword cannot be used\noutside of a native coroutine.\nclassic cor outines\nA generator function that consumes data sent to it via\nmy_coro.send(data)  calls, and reads that data by using yield  in\nan expression. Classic coroutines can delegate to other classic\ncoroutines using yield from . Classic coroutines cannot be driven by\nawait , and are no longer supported by asyncio .\ngenerator -based cor outines\nA generator function decorated with @types.coroutine —\nintroduced in Python 3.5. That decorator makes the generator\ncompatible with the new await  keyword.3\nIn this chapter , we focus on native coroutines.\n@ASYNCIO.COROUTINE  H A S  N O  F U T U R E\nThe @asyncio.coroutine  decorator for classic coroutines and generator -based\ncoroutines was deprecated in Python 3.8 and is scheduled for removal in Python 3.1 1,\naccording to issue43216 . In contrast, @types.coroutine  should remain, per\nissue36921 . It is no longer supported by asyncio , but is used in low-level code in the\nCurio  and T rio  asynchronous frameworks.\nE x a m p l e :  P r o b i n g  D o m a i n s\nImagine you are about to start a new blog on Python, and you plan to\nregister a domain using a Python keyword and the .DEV suf fix—for\nexample: A W AIT .DEV . Example 22-1  is a script using asyncio  to check\nseveral domains concurrently . This is the output it produces:\n$ python3 blogdom.py  \n  with.dev  \n+ elif.dev  \n+ def.dev  \n  from.dev  \n  else.dev  \n  or.dev  \n  if.dev  \n  del.dev  \n+ as.dev  \n  none.dev  \n  pass.dev  \n  true.dev  \n+ in.dev  \n+ for.dev  \n+ is.dev  \n+ and.dev  \n+ try.dev  \n+ not.dev\nNote that the domains appear unordered. If you run the script, you’ll see\nthem displayed one after the other , with varying delays. The +  sign4\nindicates your machine was able to resolve the domain via DNS. Otherwise,\nthe domain did not resolve and may be available.\nIn blogdom.py , the DNS probing is done via native coroutine objects.\nBecause the asynchronous operations are interleaved, the time needed to\ncheck the 18 domains is much less than checking them sequentially . In fact,\nthe total time is practically the same as the time for the single slowest DNS\nresponse, instead of the sum of the times of all responses.\nHere is the code for blogdom.py :\nExample 22-1. blogdom.py: sear ch for domains for a Python blog\n#!/usr/bin/env python3  \nimport asyncio \nimport socket \nfrom keyword import kwlist \n \nMAX_KEYWORD_LEN  = 4  \n \n \n \nasync def probe(domain: str) -> tuple[str, bool]:  \n \n    loop = asyncio.get_running_loop ()  \n \n    try: \n        await loop.getaddrinfo (domain, None)  \n \n    except socket.gaierror : \n        return (domain, False) \n    return (domain, True) \n \n \nasync def main() -> None:  \n \n    names = (kw for kw in kwlist if len(kw) <= MAX_KEYWORD_LEN )  \n \n    domains = (f'{name}.dev'.lower() for name in names)  \n \n    coros = [probe(domain) for domain in domains]  \n \n    for coro in asyncio.as_completed (coros):  \n \n        domain, found = await coro  \n \n        mark = '+' if found else ' ' \n        print(f'{mark} {domain} ') \n \n \nif __name__  == '__main__ ': \n    asyncio.run(main())  \nSet maximum length of keyword for domains, because shorter is better .5\nprobe  returns a tuple with the domain name and a boolean; True\nmeans the domain resolved. Returning the domain name will make it\neasier to display the results.\nGet a reference to the asyncio  event loop, so we can use it next.\nThe loop.getaddrinfo(…)  coroutine-method returns a five-part\ntuple of parameters  to connect to the given address using a socket. In\nthis example, we don’ t need the result. If we got it, the domain resolves;\notherwise, it doesn’ t.\nmain  must be a coroutine, so that we can use await  in it.\nGenerator to yield Python keywords with length up to\nMAX_KEYWORD_LEN .\nGenerator to yield domain names with the .dev  suf fix.\nBuild a list of coroutine objects by invoking the probe  coroutine with\neach domain  ar gument.\nasyncio.as_completed  is a generator that yields the coroutines in\nthe order they are completed—not the order they were submitted. It’ s\nsimilar to futures.as_completed , which we saw in Chapter 21 ,\nExample 21-4 .\nAt this point, we know the coroutine is done because that’ s how\nas_completed  works. Therefore, the await  expression will not\nblock but we need it to get the result from coro . If coro  raised an\nunhandled exception, it would be re-raised here.\nThis is a common pattern for scripts that use asyncio : implement\nmain  as a coroutine, and drive it here with asyncio.run .",8163
294-Downloading with asyncio and aiohttp.pdf,294-Downloading with asyncio and aiohttp,"T I P\nThe asyncio.get_running_loop  function was added in Python 3.7 for use inside\ncoroutines as shown in probe . Its implementation is simpler and faster than\nasyncio.get_event_loop  (which may start an event loop if necessary). If there’ s\nno running loop, asyncio.get_running_loop  raises RuntimeError .\nGuido’ s trick to read asynchronous code\nThere are a lot of new concepts to grasp in asyncio  but the overall logic\nof Example 22-1  is easy to follow if you employ a trick suggested by Guido\nvan Rossum himself: squint and pretend the async  and await  keywords\nare not there. If you do that, you’ll realize that coroutines read like plain old\nsequential functions.\nFor example, imagine that the body of this coroutine…\nasync def probe(domain: str) -> tuple[str, bool]: \n    loop = asyncio.get_running_loop () \n    try: \n        await loop.getaddrinfo (domain, None) \n    except socket.gaierror : \n        return (domain, False) \n    return (domain, True)\n…works like the following function, except that it magically never blocks:\ndef probe(domain: str) -> tuple[str, bool]:  # no async  \n    loop = asyncio.get_running_loop () \n    try: \n        loop.getaddrinfo (domain, None)  # no await  \n    except socket.gaierror : \n        return (domain, False) \n    return (domain, True)\nUsing the syntax await loop.getaddrinfo(…)  avoids blocking\nbecause await  suspends the current coroutine object—for example,\nprobe('if.dev') . A new coroutine object is created,\ngetaddrinfo('if.dev', None) , it starts the low-level addrinfo\nquery and yields control back to the event loop, which can drive other\npending coroutine objects, such as probe('or.dev') . When the event\nloop gets a response for the getaddrinfo('if.dev', None)  query ,\nthat specific coroutine object resumes and returns control back to the\nprobe('if.dev') —which was suspended at await —and can now\nhandle a possible exception and return the result tuple.\nSo far , we’ve only seen asyncio.as_completed  and await  applied\nto coroutines. But they handle any awaitable  object. That concept is\nexplained next.\nN e w  c o n c e p t :  a w a i t a b l e\nThe for  keyword works with iterables . The await  keyword works with\nawaitables .\nAs an end user of asyncio , these are the awaitables you will see on a\ndaily basis:\nA native cor outine object , which you get by calling a native\ncor outine function .\nAn asyncio.Task , which usually you get by passing a\ncoroutine object to asyncio.create_task() .\nHowever , end-user code does not always need to await  on a Task . W e\nuse asyncio.create_task(one_coro())  to schedule one_coro\nfor concurrent execution, without waiting for its return. That’ s what we did\nwith the spinner  coroutine in spinner_async.py  ( Example 20-4 ). If you\ndon’ t expect to cancel the task or wait for it, there is no need to keep the\nTask  object returned from create_task . Creating the task is enough to\nschedule the coroutine to run.\nIn contrast, we use await other_coro()  to run other_coro  right\nnow and wait for its completion because we need its result before we can\nproceed. In spinner_async.py , the supervisor  coroutine did res =\nawait slow()  to execute slow  and get its result.\nWhen implementing asynchronous libraries or contributing to asyncio\nitself, you may also deal with these lower -level awaitables:\nAn object with an __await__  method that returns an iterator; for\nexample, an asyncio.Future  instance (asyncio.Task  is a\nsubclass of asyncio.Future ).\nObjects written in other languages using the Python/C API with a\ntp_as_async.am_await  function, returning an iterator\n(similar to __await__  method).\nExisting codebases may also have one additional kind of awaitable:\ngenerator -based cor outine objects —which are in the process of being\ndeprecated.\nN O T E\nPEP 492 states  that the await  expression “uses the yield from  implementation\nwith an extra step of validating its ar gument” and "" await  only accepts an awaitable.”\nThe PEP does not explain that implementation in detail, but refers to PEP 380 , which\nintroduced yield from . In this book there is a detailed explanation in “The Meaning\nof yield from” .\nNow let’ s study the asyncio  version of a script that downloads a fixed set\nof flag images.\nD o w n l o a d i n g  w i t h  a s y n c i o  a n d  a i o h t t p\nThe flags_asyncio.py  script downloads a fixed set of 20 flags from\nfluentpython.com . W e first mentioned it “Concurrent W eb Downloads” , but\nnow we’ll study it in detail, applying the concepts we just saw .\nAs of Python 3.9, asyncio  only supports TCP and UDP directly , and\nthere are no asynchronous HTTP client or server packages in the standard\nlibrary . I am using aiohttp  3.7.4  in the HTTP client examples.\nW e’ll explore flags_asyncio.py  from the bottom up—that is, looking first at\nthe functions that set up the action in Example 22-2 .\nW A R N I N G\nT o make the code easier to read, flags_asyncio.py  has no error handling. As we\nintroduce async/await , it’ s useful to focus on the “happy path” initially , to\nunderstand how regular functions and coroutines are arranged in a program.\nStarting with “Enhancing the asyncio downloader” , the examples include error handling\nand more features.\nExample 22-2. flags_asyncio.py: startup functions\ndef download_many (cc_list: list[str]) -> int:     \n \n    return asyncio.run(supervisor (cc_list))       \n  \n \nasync def supervisor (cc_list: list[str]) -> int: \n    async with ClientSession () as session:        \n  \n        to_do = [download_one (session, cc)        \n  \n                 for cc in sorted(cc_list)] \n        res = await asyncio.gather(*to_do)        \n  \n \n    return len(res)                               \n  \n \nif __name__  == '__main__ ': \n    main(download_many )\nThis needs to be a plain function—not a coroutine—so it can be passed\nto and called by the main  function from the flags.py  module\n( Example 21-2 ).\nExecute the event loop driving the supervisor(cc_list)\ncoroutine object until it returns. This will block while the event loop\nruns. The result of this line is whatever supervisor  returns.\nHTTP client operations in aiohttp  are methods of\nClientSession , which is also an asynchronous context manager: a\ncontext manager with asynchronous set-up and tear -down methods\n(more about this in “Asynchronous Context Managers” ). All HTTP\nrequests in aiohttp  must execute in the context of an active\nClientSession .\nBuild a list of coroutine objects by calling the download_one\ncoroutine once for each flag to be retrieved.\nW ait for the asynctio.gather  coroutine, which accepts one or\nmore awaitable ar guments and waits for all of them to complete,\nreturning a list of results for the given awaitables in the order they were\nsubmitted.\nsupervisor  returns the length the list returned by\nasyncio.gather .\nNow let’ s review the top of flags_asyncio.py . I reor ganized the coroutines\nso we can read them in the order they are started by the event loop.\nExample 22-3. flags_asyncio.py: imports and download functions\nimport asyncio \n \nfrom aiohttp import ClientSession   \n \n \nfrom flags import BASE_URL , save_flag , main  \n \n \nasync def download_one (session: ClientSession , cc: str):  \n \n    image = await get_flag (session, cc) \n    save_flag (image, f'{cc}.gif ') \n    print(cc, end=' ', flush=True) \n    return cc \n \nasync def get_flag (session: ClientSession , cc: str) -> bytes:  \n \n    url = f'{BASE_URL}/{cc}/{cc}.gif '.lower() \n    async with session.get(url) as resp:  \n \n        return await resp.read()",7654
295-The Secret of Native Coroutines Humble Generators.pdf,295-The Secret of Native Coroutines Humble Generators,"aiohttp  must be installed—it’ s not in the standard library .\nReuse code from flags.py  ( Example 21-2 ).\ndownload_one  must be a native coroutine, so it can await  on\nget_flag —which does the HTTP request. Then it displays the code\nof the downloaded flag, and saves the image.\nget_flag  needs to receive the ClientSession  to make the\nrequest.\nThe get  method of an aiohttp.ClientSession  instance returns\na ClientResponse  object which is also an asynchronous context\nmanager .\nNetwork I/O operations are implemented as coroutine-methods, so they\nare driven asynchronously by the asyncio  event loop.\nN O T E\nFor better performance, the save_flag  call inside get_flag  should be\nasynchronous, but asyncio  does not provide an asynchronous filesystem API at this\ntime—as Node.js  does. If profiling reveals that is a bottleneck in your application, you\ncan use the loop.run_in_executor  function  to run save_flag  in a thread pool.\nExample 22-8  will show how .\nY our code delegates to the aiohttp  coroutines explicitly through await\nor implicitly through the special methods of the asynchronous context\nmanagers, such as ClientSession  and ClientResponse —as we’ll\nsee in “Asynchronous Context Managers” .\nThe Secret of Native Coroutines: Humble Generators\nA key dif ference between the classic coroutine examples we saw in\nChapter 19  and flags_asyncio.py  is that there are no visible .send()  calls\nor yield  expressions in the latter . Y our code sits between the asyncio\nlibrary and the asynchronous libraries you are using, such as aiohttp .\nThis is illustrated in Figure 22-1 .",1617
296-Asynchronous Context Managers.pdf,296-Asynchronous Context Managers,"Figur e 22-1. In an asynchr onous pr ogram, a user ’ s function starts the event loop, scheduling an\ninitial cor outine with asyncio.run . Each user ’ s cor outine drives the next with an await\nexpr ession, forming a channel that enables communication between a library such as aiohttp  and\nthe event loop. Compar e this with Figur e 19-2 .\nUnder the hood, the asyncio  event loop makes the .send  calls that drive\nyour coroutines, and your coroutines await  on other coroutines, including\nlibrary coroutines. As mentioned, await  borrows most of its\nimplementation from yield from , which also makes .send  calls to\ndrive coroutines.\nThe await  chain eventually reaches a low-level awaitable, which returns a\nplain generator that the event loop can drive in response to events such as\ntimers or network I/O. The low-level awaitables and generators at the end\nof these await  chains are implemented deep into the libraries, are not part\nof their APIs, and may be written in C.\nUsing functions like asyncio.gather  and asyncio.create_task ,\nyou can start multiple concurrent await  channels, enabling concurrent\nexecution of multiple I/O operations driven by a single event loop, in a\nsingle thread.\nThe all-or-nothing problem\nNote that in Example 22-3  I could not reuse the get_flag  function from\nflags.py  ( Example 21-2 ) because it uses the requests  library , which\nperforms blocking I/O: it would block the event loop. T o leverage\nasyncio , we must replace every function that hits the network with an\nasynchronous version that is activated with await  or\nasyncio.create_task, so that control is given back\nto the event loop. Using `await  in get_flag  means that it\nmust be driven as a coroutine.\nIf you can’ t rewrite a blocking function as a native coroutine, you should\nrun it in a separate thread or process, as we’ll see in “Using an Executor to\nA void Blocking the Event Loop” .\nThis is why I chose the epigraph for this chapter , which says: “Y ou rewrite\nall your code so none of it blocks or you’re just wasting your time.”\nFor the same reason, I could not reuse the download_one  function from\nflags_thr eadpool.py  ( Example 21-3 ) either . The code in Example 22-3\ndrives get_flag  with await , so download_one  must also be a\ncoroutine. For each request, a download_one  coroutine object is created\nin supervisor , and they are all driven by the asyncio.gather\ncoroutine.\nNow let’ s study the async with  statement that appeared in\nsupervisor  ( Example 22-2 ) and get_flag  ( Example 22-3 ).\nA s y n c h r o n o u s  C o n t e x t  M a n a g e r s\nIn “Context Managers and with Blocks”  we saw how an object can be used\nto run code before and after the body of a with  block, if its class provides\nthe __enter__  and __exit__  methods.\nNow , consider this Example 22-4 , from the asyncpg  asyncio-compatible\nPostgreSQL driver documentation on transactions :\nExample 22-4. Sample code fr om the documentation of the asyncpg\nPostgr eSQL driver .\ntr = connection .transaction () \nawait tr.start() \ntry: \n    await connection .execute(""INSERT INTO mytable VALUES (1, 2,  \n3)"") \nexcept: \n    await tr.rollback () \n    raise \nelse: \n    await tr.commit()\nA database transaction is a natural fit for the context manager protocol: the\ntransaction has to be started, data is changed with\nconnection.execute , and then a rollback or commit must happen,\ndepending on the outcome of the changes.\nIn an asynchronous driver like asyncpg , the set up and wrap up need to be\ncoroutines—so that other operations can happen concurrently . However , the\nimplementation of the classic with  statement doesn’ t support coroutines\ndoing the work of __enter__  or __exit__ .\nThat’ s why PEP 492—Coroutines with async and await syntax  introduced\nthe async with  statement, which works with asynchronous context\nmanagers: objects implementing the __aenter__  and __aexit__\nmethods as coroutines.\nW ith async with , Example 22-4  can be written like this other snippet\nfrom the asyncpg  documentation :\nasync with connection .transaction (): \n    await connection .execute(""INSERT INTO mytable VALUES (1, 2,  \n3)"")\nIn the asyncpg  Transaction  class , the __aenter__  coroutine method\ndoes await self.start()  and the __aexit__  coroutine awaits on\nprivate __rollback  or __commit  coroutine methods, depending on\nwhether an exception occurred or not. The use of coroutines to implement\nTransaction  as an asynchronous context manager allows asyncpg  to\nhandle many transactions concurrently .\nBack to flags_asyncio.py , the ClientSession  and ClientResponse\nclasses of aiohttp  are both asynchronous context managers to be able to\nuse awaitables their __aenter__  and __aexit__  special coroutine\nmethods. The aiohttp  documentation has a high-level explanation about\nthese asynchronous context managers titled Why is aiohttp client API that\nway?",4931
297-Using asyncio.as_completed and a semaphore.pdf,297-Using asyncio.as_completed and a semaphore,"N O T E\n“Asynchronous Generators as Context Managers”  shows how to use Python’ s\ncontextlib  to create an asynchronous context manager without having to write a\nclass. That explanation comes later in this chapter because of a pre-requisite topic:\n“Asynchronous Generator Functions” .\nW e’ll now enhance the asyncio  flag download example with a progress\nbar , which will lead us to explore a bit more of the asyncio  API.\nE n h a n c i n g  t h e  a s y n c i o  d o w n l o a d e r\nRecall from “Downloads with Progress Display and Error Handling”  that\nthe flags2  set of examples share the same command-line interface, and they\ndisplay a progress bar while the downloads are happening. They also\ninclude error handling.\nT I P\nI encourage you to play with the flags2  examples to develop an intuition of how\nconcurrent HTTP clients perform. Use the -h  option to see the help screen in\nExample 21-10 . Use the -a , -e , and -l  command-line options to control the number of\ndownloads, and the -m  option to set the number of concurrent downloads. Run tests\nagainst the LOCAL , REMOTE , DELAY , and ERROR  servers. Discover the optimum\nnumber of concurrent downloads to maximize throughput against each server . T weak\nthe options for the test servers as described in “Setting up test servers” .\nFor instance, Example 22-5  shows how to get 100 flags ( -al 100 ) from\nthe ERROR  server , using 100 concurrent requests ( -m 100 ).\nExample 22-5. Running flags2_asyncio.py\n$ python3 flags2_asyncio.py -s ERROR -al 100 -m 100  \nERROR site: http://localhost:8002/flags  \nSearching for 100 flags: from AD to LK  \n100 concurrent connections will be used.  \n--------------------  \n73 flags downloaded.  \n27 errors.  \nElapsed time: 0.64s\nA C T  R E S P O N S I B L Y  W H E N  T E S T I N G\nC O N C U R R E N T  C L I E N T S\nEven if the overall download time is not much dif ferent between the threaded and\nasyncio  HTTP clients, asyncio  can send requests faster , so it’ s more likely that the\nserver will suspect a DOS attack. T o really exercise these concurrent clients at full\nthrottle, please set up local HTTP servers for testing as explained in “Setting up test\nservers” .\nNow let’ s see how flags2_asyncio.py  is implemented.\nUsing asyncio.as_completed and a semaphore\nIn Example 22-3 , we passed several coroutines to asyncio.gather ,\nwhich returns a list with results of the coroutines in the order they were\nsubmitted. This means that asyncio.gather  can only return when all\nthe awaitables are done. However , to update a progress bar we need to get\nresults as they are done.\nFortunately , there is an asyncio  equivalent of the as_completed\ngenerator function we used in the thread pool example with the progress bar\n( Example 21-16 ).\nExample 22-6  shows the top of the flags2_asyncio.py  script where the\nget_flag  and download_one  coroutines are defined. Example 22-7\nlists the rest of the source, with supervisor  and download_many .\nThis script is longer than flags_asyncio.py  because of error handling.\nExample 22-6. flags2_asyncio.py: T op portion of the script; r emaining code\nis in Example 22-7\nimport asyncio \nfrom collections  import Counter \n \nimport aiohttp \nimport tqdm  # type: ignore  \n \nfrom flags2_common  import main, HTTPStatus , Result, save_flag  \n \n# default set low to avoid errors from remote site, such as  \n# 503 - Service Temporarily Unavailable  \nDEFAULT_CONCUR_REQ  = 5 \nMAX_CONCUR_REQ  = 1000 \n \n \nclass FetchError (Exception ):  \n \n    def __init__ (self, country_code : str): \n        self.country_code  = country_code  \n \n \nasync def get_flag (session: aiohttp.ClientSession ,  \n \n                   base_url : str, \n                   cc: str) -> bytes: \n    url = f'{base_url}/{cc}/{cc}.gif '.lower() \n    async with session.get(url) as resp: \n        if resp.status == 200: \n            return await resp.read() \n        else: \n            resp.raise_for_status ()  \n \n            return bytes() \n \nasync def download_one (session: aiohttp.ClientSession ,  \n \n                       cc: str, \n                       base_url : str, \n                       semaphore : asyncio.Semaphore , \n                       verbose: bool) -> Result: \n    try: \n        async with semaphore :  \n \n            image = await get_flag (session, base_url , cc) \n    except aiohttp.ClientResponseError  as exc: \n        if exc.status == 404:               \n  \n            status = HTTPStatus .not_found  \n            msg = 'not found ' \n        else: \n            raise FetchError (cc) from exc  \n \n    else: \n        save_flag (image, f'{cc}.gif ') \n        status = HTTPStatus .ok \n        msg = 'OK' \n    if verbose and msg: \n        print(cc, msg) \n    return Result(status, cc)\n\nW e’ll use this custom exception to wrap other HTTP or network\nexceptions and carry the country_code  for error reporting.\nget_flag  will either return the bytes of the image downloaded, raise\nweb.HTTPNotFound  if the HTTP response status is 404, or raise an\naiohttp.HttpProcessingError  for other HTTP status codes.\nThis raises an exception for codes >= 400. If that’ s not the case, return 0\nbytes in the next line.\nThe semaphore  ar gument is an instance of asyncio.Semaphore ,\na synchronization device that limits the number of concurrent requests.\nThe semaphore  is used as an asynchronous context manager so that\nthe system as whole is not blocked: only this coroutine is suspended\nwhen the semaphore counter is zero. More about this in “About\nSemaphores” .\nIf the HTTP status was 404—not found—save it to add to the Result\nto be returned, and set an appropriate message for verbose mode\nreporting.\nW rap any other aiohttp.ClientResponseError  as a\nFetchError  with the country code and the original exception chained\nusing the raise X from Y  syntax introduced in PEP 3134 — \nException Chaining and Embedded T racebacks .\nNetwork client code of the sort we are studying should always use some\nthrottling mechanism to avoid pounding the server with too many\nconcurrent requests. In flags2_thr eadpool.py  ( Example 21-16 ), the\nthrottling was done by instantiating the ThreadPoolExecutor  with the\nrequired max_workers  ar gument set to concur_req  in the\ndownload_many  function. In flags2_asyncio.py  I used an\nasyncio.Semaphore  created by the supervisor  function (shown\nnext, in Example 22-7 ) and passed as the semaphore  ar gument to\ndownload_one  in Example 22-6 .\nA B O U T  S E M A P H O R E S\nThe semaphore is a simple but flexible synchronization primitive\ninvented by computer scientist Edsger W . Dijkstra in the early 1960’ s.\nOther synchronization objects—such as locks and barriers—can be\nbuilt on top of semaphores.\nThere are three Semaphore  classes in Python’ s standard library: one\nin threading , another in multiprocessing , and a third one in\nasyncio . Here we’ll discuss the latter .\nAn asyncio.Semaphore  has an internal counter that is\ndecremented whenever we drive the .acquire()  coroutine method,\nand incremented when we call the .release()  method—which is\nnot a coroutine because it never blocks.\nThe initial value of the counter is set when the Semaphore  is\ninstantiated, as in this line of supervisor :\n    semaphore  = asyncio.Semaphore (concur_req )\nCalling .acquire()  does not block when the counter is greater than\nzero, but if the counter is zero, .acquire()  will suspend the calling\ncoroutine until some other coroutine calls .release()  on the same\nSemaphore , thus incrementing the counter . In Example 22-6 , I don’ t\nuse .acquire()  or .release()  directly , but use the semaphore\nas an asynchronous context manager in this block of code inside\ndownload_one :\n        async with semaphore : \n            image = await get_flag (session, base_url , cc)\nThe Semaphore.__aenter__  coroutine method awaits for\n.acquire() , and its __aexit__  coroutine method calls\n.release() .\nThat snippet guarantees that no more than concur_req  instances of\nget_flags  coroutines will be active at any time.\nEach of the Semaphore  classes in the standard library has a\nBoundedSemaphore  subclass that enforces an additional constraint:\nthe internal counter can never become lar ger than the initial value when\nthere are more .release()  than .acquire()  operations.\nNow let’ s take a look at the rest of the script in Example 22-7 .\nExample 22-7. flags2_asyncio.py: Script continued fr om Example 22-6\nasync def supervisor (cc_list: list[str], \n                     base_url : str, \n                     verbose: bool, \n                     concur_req : int) -> Counter[HTTPStatus ]:  \n \n    counter: Counter[HTTPStatus ] = Counter() \n    semaphore  = asyncio.Semaphore (concur_req )  \n \n    async with aiohttp.ClientSession () as session: \n        to_do = [download_one (session, cc, base_url , semaphore , \nverbose) \n                 for cc in sorted(cc_list)]  \n \n        to_do_iter  = asyncio.as_completed (to_do)  \n \n        if not verbose: \n            to_do_iter  = tqdm.tqdm(to_do_iter , total=len(cc_list))  \n \n        for coro in to_do_iter :  \n \n            try: \n                res = await coro  \n \n            except FetchError  as exc:  \n \n                country_code  = exc.country_code   \n \n                try: \n                    error_msg  = exc.__cause__ .message  # type: \nignore  \n  \n                except AttributeError : \n                    error_msg  = 'Unknown cause '  \n \n                if verbose and error_msg : \n                    print(f'*** Error for {country_code}:  \n{error_msg} ') \n                status = HTTPStatus .error \n            else: \n                status = res.status \n            counter[status] += 1  \n \n    return counter  \n \n 6\ndef download_many (cc_list: list[str], \n                  base_url : str, \n                  verbose: bool, \n                  concur_req : int) -> Counter[HTTPStatus ]: \n    coro = supervisor (cc_list, base_url , verbose, concur_req ) \n    counts = asyncio.run(coro)  \n \n \n    return counts \n \nif __name__  == '__main__ ': \n    main(download_many , DEFAULT_CONCUR_REQ , MAX_CONCUR_REQ )\nsupervisor  takes the same ar guments as the download_many\nfunction, but it cannot be invoked directly from main  precisely because\nit’ s a coroutine and not a plain function like download_many .\nCreate an asyncio.Semaphore  that will allow at most\nconcur_req  active coroutines among those using this semaphore.\nThe value of concur_req  is computed by the main  function from\nflags2_common.py , based on command-line options and constants set in\neach example.\nCreate a list of coroutine objects, one per call to the download_one\ncoroutine.\nGet an iterator that will return coroutine objects as they are done. I did\nnot place this call to as_completed  directly in the for  loop below\nbecause I may need to wrap it with the tqdm  iterator for the progress\nbar , depending on the user ’ s choice for verbosity .\nW rap the as_completed  iterator with the tqdm  generator function to\ndisplay progress.\nIterate over the completed coroutine objects; this loop is similar to the\none in download_many  in Example 21-16 ; most changes have to do\nwith exception handling because of dif ferences in the HTTP libraries\n(requests  versus aiohttp ).\nawait  on the coroutine to get its result. This will not block because\nas_completed  only produces coroutines that are done.\nEvery exception in download_one  is wrapped in a FetchError\nwith the original exception chained.\nGet the country code where the error occurred from the FetchError\nexception.\nT ry to retrieve the error message from the original exception. Despite\nbeing protected by try/except AttributeError , Mypy reports\ntwo missing attribute errors in this line. Fortunately , we can silence it.\nThank Guido for optional typing.\nIf the error message cannot be found in the original exception, use the\nname of the chained exception class as the error message.\nT ally outcomes.\nReturn the counter , as in the other scripts.\ndownload_many  instantiates the supervisor  coroutine object and\npasses it to the event loop with asyncio.run .\nIn Example 22-7 , we could not use the mapping of futures to country codes\nwe saw in Example 21-16  because the awaitables returned by\nasyncio.as_completed  are not necessarily the same awaitables we\npass into the as_completed  call. Internally , the asyncio  machinery\nmay replace the awaitables we provide with others that will, in the end,\nproduce the same results.\nBecause I could not use the awaitables as keys to retrieve the country code\nfrom a dict  in case of failure, I implemented the custom FetchError\nexception (shown in Example 22-6 ). FetchError  wraps a network\nexception and holds the country code associated with it, so the country code7",12941
298-Using an Executor to Avoid Blocking the Event Loop.pdf,298-Using an Executor to Avoid Blocking the Event Loop,"can be reported with the error in verbose mode. If there is no error , the\ncountry code is available as the result of the await coro  expression at\nthe top of the for  loop.\nThis wraps up the discussion of an asyncio  example functionally\nequivalent to the flags2_thr eadpool.py  we saw earlier .\nWhile discussing Example 22-3 , I noted that save_flag  performs file I/O\nand should be executed asynchronously for better performance. The\nfollowing section shows how .\nUsing an Executor to A void Blocking the Event Loop\nIn the Python community , we tend to overlook the fact that local filesystem\naccess is blocking, rationalizing that it doesn’ t suf fer from the higher\nlatency of network access—which is also dangerously unpredictable. In\ncontrast, Node.js  programmers are constantly reminded that all filesystem\nfunctions are blocking because their signatures require a callback. Each\ntime event loop is blocked because of any I/O, you are wasting millions of\nCPU cycles. This may have a significant impact on the overall performance\nof the application.\nIn Example 22-6 , the blocking function is save_flag . In the threaded\nversion of the script ( Example 21-16 ), save_flag  blocks the thread that’ s\nrunning the download_one  function, but that’ s only one of several\nworker threads. Behind the scenes, the blocking I/O call releases the GIL,\nso another thread can proceed. But in flags2_asyncio.py , save_flag\nblocks the single thread our code shares with the asyncio  event loop,\ntherefore the whole application freezes while the file is being saved. The\nsolution to this problem is the run_in_executor  method of the event\nloop object.\nThe asyncio  event loop provides a thread pool executor , and you can\nsend callables to be executed by it with loop.run_in_executor . This\nallows potentially blocking code to run in other threads, without blocking\nthe event loop in the main thread or our program. Of course, the main\nthread and the thread pool will still share the same GIL, but that should not\nbe a problem if the thread pool is used for I/O.\nT o use this feature in our example, we only need to change a few lines in\nthe download_one  coroutine, as shown in Example 22-8 .\nExample 22-8. flags2_asyncio_executor .py: Using the default thr ead pool\nexecutor to run save_flag\nasync def download_one (session: aiohttp.ClientSession , \n                       cc: str, \n                       base_url : str, \n                       semaphore : asyncio.Semaphore , \n                       verbose: bool) -> Result: \n    try: \n        async with semaphore : \n            image = await get_flag (session, base_url , cc) \n    except aiohttp.ClientResponseError  as exc: \n        if exc.status == 404: \n            status = HTTPStatus .not_found  \n            msg = 'not found ' \n        else: \n            raise FetchError (cc) from exc \n    else: \n        loop = asyncio.get_running_loop ()   \n \n        loop.run_in_executor (None,          \n  \n            save_flag , image, f'{cc}.gif ')  \n \n        status = HTTPStatus .ok \n        msg = 'OK' \n    if verbose and msg: \n        print(cc, msg) \n    return Result(status, cc)\nGet a reference to the event loop object.\nThe first ar gument to run_in_executor  is an\nconcurrent.futures.Executor  instance; if None , the default\nthread pool executor provided by the event loop is used.\nThe remaining ar guments are the callable and its positional ar guments.\nWhen I tested Example 22-8 , there was no noticeable change in\nperformance for using run_in_executor  to save the flag images\nbecause they are small (13 KB each, on average). But you’ll see an ef fect if\nyou edit the save_flag  function in flags2_common.py  to save 10 times\nas many bytes on each file—just by coding fp.write(img * 10)\ninstead of fp.write(img) . W ith an average download size of 130 KB,\nthe advantage of using run_in_executor  becomes clear . If you’re\ndownloading megapixel images, the speedup will be significant.\nThe implementation of asyncio  itself uses run_in_executor  under\nthe hood in a few places. For example the, loop.getaddrinfo(…)\ncoroutine we saw in Example 22-1  is implemented by calling the\ngetaddrinfo  function from the socket  module—which is a blocking\nfunction that may take seconds to return, as it depends on DNS resolution.\nT I P\nA common pattern in asynchronous APIs is to wrap blocking calls that are\nimplementation details in coroutines using run_in_executor  internally . That way ,\nyou provide a consistent interface of coroutines to be driven with await , and hide the\nthreads you need to use for pragmatic reasons. The Motor  asynchronous driver for\nMongoDB  has an API compatible with async/await  that is really a façade around a\nthreaded core which talks to the database server . A. Jesse Jiryu Davis, the lead developer\nof Motor , explains his reasoning in Response to “Asynchr onous Python and\nDatabases” .\nThe main reason to pass an explict Executor  to\nloop.run_in_executor  is to employ a ProcessPoolExecutor  if\nthe function to execute is CPU intensive, so that it runs in a dif ferent Python\nprocess, avoiding contention for the GIL. Because of the high start-up cost,\nit would be better to start the ProcessPoolExecutor  in the\nsupervisor , and pass it to the coroutines that need to use it.\nThe next example demonstrates the simple pattern of executing one\nasynchronous task after the other using coroutines. This deserves our\nattention because anyone with previous experience with JavaScript knows\nthat running one asynchronous function after the other was the reason for\nthe nested coding pattern known as pyramids of doom . The await",5710
299-Making Multiple Requests for Each Download.pdf,299-Making Multiple Requests for Each Download,"keyword makes that curse go away . That’ s why we now have it in Python\nand JavaScript.\nMaking Multiple Requests for Each Download\nSuppose you want to save each country flag with the name of the country\nand the country code, instead of just the country code. Now you need to\nmake two HTTP requests per flag: one to get the flag image itself, the other\nto get the metadata.json  file in the same directory as the image: that’ s where\nthe name of the country is recorded.\nArticulating multiple requests in the same task is easy in the threaded script:\njust make one request then the other , blocking the thread twice, and keeping\nboth pieces of data (country code and name) in local variables, ready to use\nwhen saving the files. If you needed to do the same in an asynchronous\nscript with callbacks, you needed nested functions so that the country code\nand name were available in their closures until you could save the file\nbecause each callback runs in a dif ferent local scope. The await  keyword\nprovides relief from that, allowing you to drive the asynchronous requests\none after the other from the local scope of a coroutine.\nThe third variation of the asyncio  flag downloading script has a couple of\nchanges:\nget_country\nThis new coroutine fetches the metadata.json  file for the country code,\nand gets the name of the country from it.\ndownload_one\nThis coroutine now uses await  to delegate to get_flag  and the new\nget_country  coroutine, using the result of the latter to build the\nname of the file to save.\nLet’ s start with the code for get_country . Note that it is very similar to\nget_flag  from Example 22-6 .\nExample 22-9. flags3_asyncio.py: get_country cor outine\nasync def get_country (session: aiohttp.ClientSession ,  \n \n                      base_url : str, \n                      cc: str) -> str: \n    url = f'{base_url}/{cc}/metadata.json ' \n    async with session.get(url) as resp: \n        if resp.status == 200: \n            metadata  = await resp.json()  \n \n            return metadata .get('country', 'no name')  \n \n        else: \n            resp.raise_for_status () \n            return ''\nThis coroutine returns a string with the country name—if all goes well.\nmetadata  will get a Python dict  built from the JSON contents of the\nresponse.\nGet the country name or 'no name'  if it is missing.\nNow the modified download_one , which has only a few lines changed\nfrom the same coroutine in Example 22-6\nExample 22-10. flags3_asyncio.py: download_one cor outine\nasync def download_one (session: aiohttp.ClientSession , \n                       cc: str, \n                       base_url : str, \n                       semaphore : asyncio.Semaphore , \n                       verbose: bool) -> Result: \n    try: \n        async with semaphore : \n            image = await get_flag (session, base_url , cc)  \n \n        async with semaphore : \n            country = await get_country (session, base_url , cc)  \n \n    except aiohttp.ClientResponseError  as exc: \n        if exc.status == 404: \n            status = HTTPStatus .not_found  \n            msg = 'not found ' \n        else: \n            raise FetchError (cc) from exc \n    else: \n        filename  = country.replace(' ', '_')  \n \n        filename  = f'{filename}.gif ' \n        loop = asyncio.get_running_loop () \n        loop.run_in_executor (None, \n                             save_flag , image, filename ) \n        status = HTTPStatus .ok \n        msg = 'OK' \n    if verbose and msg: \n        print(cc, msg) \n    return Result(status, cc)\nGet the flag image…\n…then the country name.\nUse the country name to create a filename. As a command-line user , I\ndon’ t like to see spaces in filenames.\nMuch better than nested callbacks!\nI could schedule both get_flag  and get_country  in parallel using\nasyncio.gather , but if get_flag  raises an exception there is no\nimage to save, so it’ s pointless to run get_country . But there are cases\nwhere it makes sense to use asyncio.gather  to hit several APIs at the\nsame time instead of waiting for one response before making the next\nrequest.\nI put the calls to get_flag  and get_country  in separate with  blocks\ncontrolled by the semaphore  because it’ s good practice to hold\nsemaphores and locks for the shortest possible time.\nIn flags3_asyncio.py , the await  syntax appears six times, and async\nwith  five times. Hopefully , you should be getting the hang of\nasynchronous programming in Python. One challenge is to know when you\nhave to use await  and when you can’ t use it. The answer in principle is\neasy , you await  coroutines and other awaitables, such as\nasyncio.Task  instances. But some APIs are tricky , mixing coroutines\nand plain functions in seemingly arbitrary ways, like the StreamWriter\nclass we’ll use in Example 22-14 .",4871
300-Writing asyncio Servers.pdf,300-Writing asyncio Servers,"Example 22-10  wraps up the flags  set of examples. W e’ll now go from\nclient scripts to writing servers with asyncio .\nW r i t i n g  a s y n c i o  S e r v e r s\nThe classic toy example of a TCP server is an echo server . W e’ll build\nslightly more interesting toys: server -side Unicode character search utilities,\nfirst using HTTP with FastAPI , then using plain TCP with asyncio  only .\nThese servers let users query for Unicode characters based on words in their\nstandard names from the unicodedata  module we discussed in “The\nUnicode Database” . Figure 22-2  shows a session with the\nweb_mojifinder .py  server .\nFigur e 22-2. Br owser window displaying sear ch r esults for “mountain” fr om the web_mojifinder .py\nservice.\nThe Unicode search logic in these examples is in the InvertedIndex\nclass in the charindex.py  module in the Fluent Python 2e  code repository .\nThere’ s nothing concurrent in that small module, so I’ll only give a brief\noverview in the optional box below . Y ou can skip to the HTTP server\nimplementation in “A FastAPI W eb Service” .\nW H A T  I S  A N  I N V E R T E D  I N D E X\nAn inverted index usually maps words to documents in which they\noccur . In the mojifinder  examples, the “documents” are characters. The\ncharindex.InvertedIndex  class indexes each word that appears\nin each character name in the Unicode database, and creates an inverted\nindex stored in a defaultdict . For example, to index character\nU+0037—DIGIT SEVEN—the InvertedIndex  initializer appends\nthe character '7'  to the entries under the keys 'DIGIT'  and\n'SEVEN' . After indexing the Unicode 13.0.0 data bundled with Python\n3.9.1, 'DIGIT'  maps to 868 characters, and 'SEVEN'  maps to 143,\nincluding U+1F556—CLOCK F ACE SEVEN OCLOCK and U+2790\n—DINGBA T NEGA TIVE CIRCLED SANS-SERIF DIGIT SEVEN\n(which appears in many code listings in this book).\nSee Figure 22-3  for a demonstration using the entries for 'CAT'  and\n'FACE' .\nFigur e 22-3. Python console exploring InvertedIndex  attribute entries  and search\nmethod.\nThe InvertedIndex.search  method breaks the query into words,\nand returns the intersection of the entries for each word. That’ s why",2198
301-A FastAPI Web Service.pdf,301-A FastAPI Web Service,"searching for “face” finds 171 results, “cat” finds 14, but “cat face”\nonly 10.\nThat’ s the beautiful idea behind an inverted index: a fundamental\nbuilding block in information retrieval—the theory behind search\nengines. See the English W ikipedia article Inverted Index  to learn more.\nA FastAPI W eb Service\nI wrote the next example— web_mojifinder .py —using FastAPI : one of the\nPython ASGI W eb frameworks mentioned in “ASGI—Asynchronous\nServer Gateway Interface” . Figure 22-2  is a screenshot of the front-end. It’ s\na super simple SP A (Single Page Application): after the initial HTML\ndownload, the UI is updated by client-side JavaScript communicating with\nthe server .\nFastAPI  is designed to implement back-ends for SP A and mobile apps,\nwhich mostly consist of W eb API end points returning JSON responses\ninstead of server -rendered HTML. FastAPI  leverages decorators, type hints,\nand code introspection to eliminate a lot of the boilerplate code for W eb\nAPIs, and also automatically publishes interactive OpenAPI—a.k.a.\nSwagger —documentation for the APIs we create. Figure 22-4  shows the\nauto-generated /docs  page for web_mojifinder .py .\nFigur e 22-4. Auto-generated OpenAPI schema for the /search  endpoint.\nExample 22-1 1  is the code for web_mojifinder .py , but that’ s just the back-\nend code. When you hit the root URL / , the server sends the form.html  file\nwhich has 81 lines of code, including 54 lines of JavaScript to communicate\nwith the server and fill a table with the results. If you’re interested in\nreading plain framework-less JavaScript, please find 22-\nasync/mojifinder/static/form.html  in the Fluent Python 2e  code repository\nT o run web_mojifinder .py , you need to install two packages and their\ndependencies: FastAPI  and uvicorn . .\nThis is the command to run Example 22-1 1  with uvicorn  in development\nmode:\n$ uvicorn web_mojifinder:app --reload\nThe parameters are:\nweb_mojifinder:app\nThe package name, a colon, and the name of the ASGI application\ndefined in it— app  is the conventional name.\n--reload\nMake uvicorn  monitor changes to application source files and\nautomatically reload them. Useful only during development.\nNow let’ s study the source code for web_mojifinder .py .\nExample 22-1 1. web_mojifinder .py: complete sour ce\nfrom pathlib import Path \nfrom unicodedata  import name \n \nfrom fastapi import FastAPI \nfrom fastapi.responses  import HTMLResponse  \nfrom pydantic  import BaseModel  \n \nfrom charindex  import InvertedIndex  \n \napp = FastAPI(  \n \n    title='Mojifinder Web ', \n    description ='Search for Unicode characters by name. ', \n) \n 8\nclass CharName (BaseModel ):  \n \n    char: str \n    name: str \n \ndef init(app):  \n \n    app.state.index = InvertedIndex () \n    static = Path(__file__ ).parent.absolute () / 'static'  \n \n    app.state.form = (static / 'form.html ').read_text () \n \ninit(app)  \n \n \n@app.get ('/search', response_model =list[CharName ])  \n \nasync def search(q: str):  \n \n    chars = app.state.index.search(q) \n    return ({'char': c, 'name': name(c)} for c in chars)  \n \n \n@app.get ('/', response_class =HTMLResponse , include_in_schema =False) \ndef form():  \n \n    return app.state.form \n \n# no main funcion  \nThis line defines the ASGI app. It could be as simple as app =\nFastAPI() . The parameters shown are metadata for the auto-\ngenerated documentation.\nA pydantic  schema for a JSON response with char  and name  fields.\nBuild the index  and load the static HTML form, attaching both to the\napp.state  for later use.\nUnrelated to the theme of this chapter , but worth noting: the elegant use\nof the overloaded /  operator by pathlib .\nRun init  when this module is loaded by the ASGI server .\nRoute for the /search  endpoint; response_model  uses that\nCharName  pydantic  model to describe the response format.\nFastAPI  assumes that any ar guments that appear in the function or\ncoroutine signature that are not in the route path will be passed in the9\n10\nHTTP query string, e.g. /search?q=cat . Since q  has no default,\nFastAPI  will return a 422 (Unprocessable Entity) status if q  is missing\nfrom the query string.\nReturning an iterable of dicts  compatible with the\nresponse_model  schema allows FastAPI  to build the JSON\nresponse according to the response_model  in the @app.get\ndecorator .\nRegular functions can also be used to produce responses.\nThis module has no main function. It is loaded and driven by the ASGI\nserver— uvicorn  in this example.\nExample 22-1 1  has no direct calls to asyncio . FastAPI  is built on the\nStarlette  ASGI toolkit, which in turn uses asyncio .\nAlso note that the body of search  doesn’ t use await , async with  or\nasync for , therefore it could be a plain function. I defined search  as a\ncoroutine just to show that FastAPI  knows how to handle it. In a real app,\nmost endpoints will query databases or hit other remote servers, so it is a\ncritical advantage of FastAPI —and ASGI frameworks in general—to\nsupport coroutines that can take advantage of asynchronous libraries for\nnetwork I/O.\nT I P\nThe init  and form  functions I wrote to load and serve the static HTML form are a\nhack to make the example short and easy to run. The recommended best practice is to\nhave a proxy/load-balancer in front of the ASGI server to handle all static assets, and\nalso use a CDN (Content Delivery Network) when possible. One such proxy/load-\nbalancer is T raefik , a self-described “edge router” that “receives requests on behalf of\nyour system and finds out which components are responsible for handling them.”\nFastAPI  has project generation  scripts that prepare your code to do that.",5745
302-An asyncio TCP Server.pdf,302-An asyncio TCP Server,"The typing enthusiast may have noticed that there are no return type hints in\nsearch  and form . Instead, FastAPI  relies on the response_model=\nkeyword ar gument in the route decorators. The Response Model  page in the\nFastAPI  documentation explains:\nThe r esponse model is declar ed in this parameter instead of as a function\nr eturn type annotation, because the path function may not actually r eturn\nthat r esponse model but rather r eturn a dict, database object or some\nother model, and then use the response_model  to perform the field\nlimiting and serialization.\nFor example, in search  I returned a generator of dict  items, not a list of\nCharName  objects, but that’ s good enough for FastAPI  and pydantic  to\nvalidate my data and build the appropriate JSON response compatible with\nresponse_model=list[CharName] .\nW e’ll now focus on the tcp_mojifinder .py  script that is answering the\nqueries in Figure 22-5 .\nAn asyncio TCP Server\nThe tcp_mojifinder .py  program uses plain TCP to communicate with a\nclient like T elnet or Netcat, so I could write it using asyncio  without\nexternal dependencies—and without reinventing HTTP . Figure 22-5  shows\ntext-based UI.\nFigur e 22-5. T elnet session with the tcp_mojifinder .py server: querying for “cat face” then “fir e”.\nThis program is twice as long as web_mojifinder .py , so I split the\npresentation into three parts: Example 22-12 , Example 22-14 , and\nExample 22-15 . The top of tcp_mojifinder .py —including the import\nstatements—is in Example 22-14 , but I will start describing the\nsupervisor  coroutine and the main  function that drives the program.\nExample 22-12. tcp_mojifinder .py: a simple TCP server; continues in\nExample 22-14 .\nasync def supervisor (index: InvertedIndex , host: str, port: int): \n    server = await asyncio.start_server (    \n \n        functools .partial(finder, index),   \n \n        host, port)                         \n  \n \n    socket_list  = cast(tuple[TransportSocket , ...], server.sockets)  \n \n    addr = socket_list [0].getsockname () \n    print(f'Serving on {addr}. Hit CTRL-C to stop. ')  \n \n    await server.serve_forever ()  \n \n \ndef main(host: str = '127.0.0.1 ', port_arg : str = '2323'): \n    port = int(port_arg ) \n    print('Building index. ') \n    index = InvertedIndex ()                         \n  \n    try: \n        asyncio.run(supervisor (index, host, port))  \n \n    except KeyboardInterrupt :                       \n  \n        print('\nServer shut down. ') \n \nif __name__  == '__main__ ': \n    main(*sys.argv[1:])\nThis await  quickly gets an instance of asyncio.Server , a TCP\nsocket server . By default, start_server  creates and starts the\nserver , so it’ s ready to receive connections.\nThe first ar gument to start_server  is client_connected_cb ,\na callback to run when a new client connection starts. The callback can\nbe a function or a coroutine, but it must accept exactly two ar guments:\nan asyncio.StreamReader  and an asyncio.StreamWriter .\nHowever , my finder  coroutine also needs to get an index , so I used\nfunctools.partial  to bind that parameter and obtain a callable\nwhich takes the reader and writer . Adapting user functions to callback\nAPIs is the most common use case for functools.partial .\nhost  and port  are the second and third ar guments to\nstart_server . See the full signature in the asyncio\ndocumentation .\nThis cast  is needed because typeshed  has an outdated type hint for the\nsockets  property of the Server  class—as of May 2021. See issue\n#5535  on typeshed .\nDisplay address and port of the first socket of the server .\nAlthough start_server  already started the server as a concurrent\ntask, I need to await  on the server_forever  method so that my\nsupervisor  is suspended here. W ithout this line, supervisor\nwould return immediately , ending the loop started with\nasyncio.run(supervisor(…)) , and exiting the program. The\ndocumentation for Server.serve_forever  says: “This method\ncan be called if the server is already accepting connections.”\nBuild the inverted index.\nStart the event loop running supervisor .\nCatch the KeyboardInterrupt  to avoid a distracting traceback\nwhen I stop the server with CTRL-C  on the terminal running it.\nY ou may find it easier to understand how control flows in tcp_mojifinder .py\nif you study the output it generates on the server console, listed in\nExample 22-13 .\nExample 22-13. tcp_mojifinder .py: this is the server side of the session\ndepicted in Figur e 22-5\n$ python3 tcp_mojifinder.py  \nBuilding index.  \n  11\nServing on ('127.0.0.1', 2323). Hit CTRL-C to stop.  \n  \n From ('127.0.0.1', 58192): 'cat face'   \n  \n   To ('127.0.0.1', 58192): 10 results.  \n From ('127.0.0.1', 58192): 'fire'       \n  \n   To ('127.0.0.1', 58192): 11 results.  \n From ('127.0.0.1', 58192): '\x00'       \n  \nClose ('127.0.0.1', 58192).              \n  \n^C  \n \nServer shut down.  \n  \n$\nOutput by main . Before the next line appears, I see a 0.6s delay on my\nmachine while the index is built.\nOutput by supervisor .\nFirst iteration of a while  loop in finder . The TCP/IP stack assigned\nport 58192 to my T elnet client. If you connect several clients to the\nserver , you’ll see their various ports in the output.\nSecond iteration of the while  loop in finder .\nI hit CTRL-C  on the client terminal; the while  loop in finder  exits.\nThe finder  coroutine displays this message then exits. Meanwhile the\nserver is still running, ready to service another client.\nI hit CTRL-C  on the server terminal; server.serve_forever  is\ncancelled, ending supervisor  and the event loop.\nOutput by main .\nAfter main  builds the index and starts the event loop, supervisor\nquickly displays the Serving on…  message and is suspended at the\nawait server.serve_forever()  line. At that point, control flows\ninto the event loop and stays there, occasionally coming back to the\nfinder  coroutine, which yields control back to the event loop whenever it\nneeds to wait for the network to send or receive data.\nWhile the event loop is alive, a new instance of the finder  coroutine will\nbe started for each client that connects to the server . In this way , many\nclients can be handled concurrently by this simple server . This continues\nuntil a KeyboardInterrupt  occurs on the server or its process is killed\nby the OS.\nNow let’ s see the top of tcp_mojifinder .py , with the finder  coroutine.\nExample 22-14. tcp_mojifinder .py: continued fr om Example 22-12 .\nimport asyncio \nimport functools  \nimport sys \nfrom asyncio.trsock  import TransportSocket  \nfrom typing import cast \n \nfrom charindex  import InvertedIndex , format_results   \n \n \nCRLF = b'\r\n' \nPROMPT = b'?> ' \n \nasync def finder(index: InvertedIndex ,          \n  \n                 reader: asyncio.StreamReader , \n                 writer: asyncio.StreamWriter ): \n    client = writer.get_extra_info ('peername ')  \n \n    while True:  \n \n        writer.write(PROMPT)  # can't await!  \n  \n        await writer.drain()  # must await!  \n  \n        data = await reader.readline ()  \n \n        try: \n            query = data.decode().strip()  \n \n        except UnicodeDecodeError :  \n \n            query = '\x00' \n        print(f' From {client}: {query!r} ')  \n \n        if query: \n            if ord(query[:1]) < 32:  \n \n                break \n            results = await search(query, index, writer)  \n \n            print(f'   To {client}: {results} results. ')  \n \n \n    writer.close()  \n \n    await writer.wait_closed ()  \n \n    print(f'Close {client}. ')  \n\nformat_results  is useful to display the results of\nInvertedIndex.search  in a text-based UI such as the command\nline or a T elnet session.\nT o pass finder  to asyncio.start_server  I wrapped it with\nfunctools.partial , because the server expects a coroutine or\nfunction that takes only the reader  and writer  ar guments.\nGet the remote client address to which the socket is connected.\nThis loop handles a dialog that lasts until a control character is received\nfrom the client.\nThe StreamWriter.write  method is not a coroutine, just a plain\nfunction; this line sends the ?>  prompt.\nStreamWriter.drain  flushes the writer  buf fer; it is a coroutine,\nso it must be driven with await .\nStreamWriter.readline  is a coroutine that returns bytes .\nDecode the bytes  to str , using the default UTF-8 encoding.\nA UnicodeDecodeError  may happen when the user hits CTRL-C\nand the T elnet client sends control bytes; if that happens, replace the\nquery with a null character , for simplicity .\nLog the query to the server console.\nExit the loop if a control or null character was received.\nDo the actual search ; code presented next.\nLog the response to the server console.\nClose the StreamWriter .\nW ait for the StreamWriter  to close. This is recommended in the\n.close()  method documentation .\nLog the end of this client’ s session to the server console.\nThe last piece of this example is the search  coroutine:\nExample 22-15. tcp_mojifinder .py: sear ch cor outine.\nasync def search(query: str,  \n \n                 index: InvertedIndex , \n                 writer: asyncio.StreamWriter ) -> int: \n    chars = index.search(query)  \n \n    lines = (line.encode() + CRLF for line  \n \n                in format_results (chars)) \n    writer.writelines (lines)  \n \n    await writer.drain()      \n \n    status_line  = f'{""─"" * 66} {len(chars)} found '  \n \n    writer.write(status_line .encode() + CRLF) \n    await writer.drain() \n    return len(chars)\nsearch  must be a coroutine because it writes to a StreamWriter\nand must use its .drain()  coroutine method.\nQuery the inverted index.\nThis generator expression will yield byte strings encoded in UTF-8 with\nthe Unicode codepoint, the actual character , its name and a CRLF\nsequence—e.g. b'U+0039\t9\tDIGIT NINE\r\n' ).\nSend the lines . Surprisingly , writer.writelines  is not a\ncoroutine.\nBut writer.drain()  is a coroutine. Don’ t for get the await !\nBuild a status line, then send it.",10151
303-Asynchronous Generator Functions.pdf,303-Asynchronous Generator Functions,"Note that all network I/O in tcp_mojifinder .py  is in bytes : we need to\ndecode the bytes  received from the network, and encode strings before\nsending them out. In Python 3, the default encoding is UTF-8, and that’ s\nwhat I used implicitly in all encode  and decode  calls in this example.\nW A R N I N G\nNote that some of the I/O methods are coroutines and must be driven with await ,\nwhile others are simple functions. For example, StreamWriter.write  is a plain\nfunction, because it writes to a buf fer . On the other hand, StreamWriter.drain —\nwhich flushes the buf fer and performs the network I/O—is a coroutine, as is\nStreamReader.readline —but not StreamWriter.writelines ! While I\nwas writing the first edition of this book, the asyncio  API docs were improved with\nclear labeling of coroutines as such .\nThe tcp_mojifinder .py  code leverages the high-level asyncio  Streams API\nthat provides a ready-to-use server so you only need to implement a handler\nfunction, which can be a plain callback or a coroutine. There is also a\nlower -level T ransports and Protocols API , inspired by the transport and\nprotocols abstractions in the T wisted  framework. Refer to the asyncio\ndocumentation for more information, including TCP and UDP echo servers\nand clients  implemented with that lower -level API.\nOur next topic is async for  and the objects that make it work.\nA s y n c h r o n o u s  i t e r a t i o n  a n d  a s y n c h r o n o u s\ni t e r a b l e s\nW e saw in “Asynchronous Context Managers”  how async with  works\nwith objects implementing the __aenter__  and __aexit__  methods\nreturning awaitables—usually in the form of coroutine objects.\nSimilarly , async for  works with asynchr onous iterables : objects that\nimplement __aiter__ . However , __aiter__  must be a regular method\n—not a coroutine-method—and it must return an asynchr onous iterator .\nAn asynchronous iterator provides an __anext__  coroutine-method that\nreturns an awaitable—often a coroutine object. They are also expected to\nimplement __aiter__ , which usually returns self . This mirrors the\nimportant distinction of iterables and iterators we discussed in “Don’ t make\nthe iterable an iterator for itself” .\nThe aiopg  asynchronous PostgreSQL driver documentation  has an example\nthat illustrates the use of async for  to iterate over the rows of a database\ncursor:\nasync def go(): \n    pool = await aiopg.create_pool (dsn) \n    async with pool.acquire() as conn: \n        async with conn.cursor() as cur: \n            await cur.execute(""SELECT 1"" ) \n            ret = [] \n            async for row in cur: \n                ret.append(row) \n            assert ret == [(1,)]\nIn this example the query will return a single row , but in a realistic scenario\nyou may have thousands of rows in response to a SELECT  query . For lar ge\nresponses, the cursor will not be loaded with all the rows in a single batch.\nTherefore it is important that async for row in cur:  does not block\nthe event loop while the cursor may be waiting for additional rows. By\nimplementing the cursor as an asynchronous iterator , aiopg  may yield to the\nevent loop at each __anext__  call, and resume later when more rows\narrive from PostgreSQL.\nAsynchronous Generator Functions\nY ou can implement an asynchronous iterator by writing a class with\n__anext__  and __aiter__ , but there is a simpler way: write a function\ndeclared with async def  and use yield  in its body . This parallels how\ngenerator functions simplify the classic iterator pattern.\nLet’ s study a simple example using async for  and implementing an\nasynchronous generator . In Example 22-1  we saw blogdom.py , a script that\nprobed domain names. Now suppose we find other uses for the probe\ncoroutine we defined there, and decide to put it into a new module—\ndomainlib.py —together with a new multi_probe  asynchronous\ngenerator that takes a list of domain names and yields results as they are\nprobed.\nW e’ll look at the implementation of domainlib.py  soon, but first let’ s see\nhow it is used with Python’ s new asynchronous console.\nExperimenting with Python’ s Async Console\nSince Python 3.8  you can run the interpreter with the -m asyncio\ncommand-line option to get an “async REPL”: a Python console that\nimports asyncio , provides a running event loop, and accepts await ,\nasync for  and async with  at the top level prompt—which otherwise\nare syntax errors when used outside of native coroutines.\nT o experiment with domainlib.py , go to the 22-async/domains/asyncio/\ndirectory in your local copy of the Fluent Python 2e  code repository . Then\nrun:\n$ python -m asyncio\nY ou’ll see the console start, similar to this:\nasyncio REPL 3.9.1 (v3.9.1:1e5d33e9b9, Dec  7 2020, 12:10:52 ) \n[Clang 6.0 (clang-600.0.57 )] on darwin  \nUse ""await"" directly instead of ""asyncio.run()"" . \nType ""help"", ""copyright"" , ""credits""  or ""license""  for more \ninformation.  \n>>> import asyncio  \n>>>\nNote how it says you can use await  instead of asyncio.run() —to\ndrive coroutines and other awaitables. The asyncio  module is\nautomatically imported.\nNow let’ s import domainlib.py  and play with its two coroutines: probe\nand multi_probe .12\nExample 22-16. Experimenting with domainlib.py  after running python3\n-m asyncio .\n>>> await asyncio.sleep(3, 'Rise and shine! ')  \n \n'Rise and shine!'  \n>>> from domainlib  import * \n>>> await probe('python.org ')  \n \nResult(domain='python.org', found=True)  \n  \n>>> names = 'python.org rust-lang.org golang.org  \nn05uch1an9.org '.split()  \n \n>>> async for result in multi_probe (names):  \n \n...      print(*result, sep='\t') \n... \ngolang.org      True    \n  \nn05uch1an9.org  False  \npython.org      True  \nrust-lang.org   True  \n>>>\nT ry a simple await  to see the asynchronous console in action. Fun\nfact: asyncio.sleep()  takes an optional second ar gument that is\nreturned when you await  it.\nDrive the probe  coroutine.\nThe domainlib  version of probe  returns a Result  named tuple.\nMake a list of domains.\nIterate with async for  over the multi_probe  asynchronous\ngenerator to display the results.\nNote that the results are not in the order the domains were given to\nmultiprobe . They appear as each DNS response comes back.\nExample 22-16  shows that multi_probe  is an asynchronous generator\nbecause it is compatible with async for . Now let’ s do a few more\nexperiments, continuing from that example.\nExample 22-17. Mor e experiments, continuing fr om Example 22-16 .\n>>> probe('python.org ')  \n \n<coroutine object probe at 0x10e313740>  \n>>> multi_probe (names)  \n \n<async_generator object multi_probe at 0x10e246b80>  \n>>> for r in multi_probe (names):  \n \n...    print(r) \n... \nTraceback (most recent call last):  \n   ... \nTypeError : 'async_generator' object is not iterable\nCalling a native coroutine gives you a coroutine object.\nCalling an asynchronous generator gives you an async_generator\nobject.\nW e can’ t use a regular for  loop with asynchronous generators because\nthey implement __aiter__  instead of __iter__ .\nAsynchronous generators are driven by async for , which can be a block\nstatement (as seen in Example 22-16 ), and it also appears in asynchronous\ncomprehensions, which we’ll cover soon.\nImplementing an Asynchronous Generator\nNow let’ s study the code for domainlib.py , with the multi_probe\nasynchronous generator .\nExample 22-18. domainlib.py: functions for pr obing domains\nimport asyncio \nimport socket \nfrom collections .abc import Iterable , AsyncIterator  \nfrom typing import NamedTuple , Optional  \n \n \nclass Result(NamedTuple ):  \n \n    domain: str \n    found: bool \n \n \nOptionalLoop  = Optional [asyncio.AbstractEventLoop ]  \n \n \n \nasync def probe(domain: str, loop: OptionalLoop  = None) -> Result:  \n \n    if loop is None: \n        loop = asyncio.get_running_loop () \n    try: \n        await loop.getaddrinfo (domain, None) \n    except socket.gaierror : \n        return Result(domain, False) \n    return Result(domain, True) \n \n \nasync def multi_probe (domains: Iterable [str]) -> \nAsyncIterator [Result]:  \n \n    loop = asyncio.get_running_loop () \n    coros = [probe(domain, loop) for domain in domains]  \n \n    for coro in asyncio.as_completed (coros):  \n \n        result = await coro  \n \n        yield result  \nNamedTuple  makes the result from probe  easier to read and debug.\nThis type alias is to avoid making the next line too long for a book\nlisting.\nprobe  now gets an optional loop  ar gument, to save repeated calls to\nget_running_loop  when this coroutine is driven by\nmulti_probe .\nAn asynchronous generator function produces an asynchronous\ngenerator object, which can be annotated as\nAsyncIterator[SomeType] .\nBuild list of probe  coroutine objects, each with a dif ferent domain\nbut all with the same loop .\nNote that this is not async for  because\nasyncio.as_completed  is a classic generator .\nA wait on the coroutine object to retrieve the result.\nY ield result . This is the line that makes multi_probe  an\nasynchronous generator .\nN O T E\nThe for  loop in Example 22-18  could be shorter:\n    for coro in asyncio.as_completed (coros): \n        yield await coro\nPython parses that as yield (await coro) , so it works. But I thought it could be\nconfusing to use that shortcut in the first asynchronous generator example in the book,\nso I split it in two lines.\nGiven domainlib.py , we can demonstrate the use of the multi_probe\nasynchronous generator in domaincheck.py : a script that takes a domain\nsuf fix and searches for domais made from short Python keywords. Here is a\nsample output of domaincheck.py :\n$ ./domaincheck.py net  \nFOUND           NOT FOUND  \n=====           =========  \nin.net \ndel.net \ntrue.net  \nfor.net \nis.net \n                none.net  \ntry.net \n                from.net  \nand.net \nor.net \nelse.net  \nwith.net  \nif.net \nas.net \n                elif.net  \n                pass.net  \n                not.net  \n                def.net\nThanks to domainlib , the code for domaincheck.py  is straightforward.\nExample 22-19. domaincheck.py: utility for pr obing domains using\ndomainlib\n#!/usr/bin/env python3  \nimport asyncio \nimport sys \nfrom keyword import kwlist \n \nfrom domainlib  import multi_probe  \n \n \nasync def main(tld: str) -> None: \n    tld = tld.strip('.') \n    names = (kw for kw in kwlist if len(kw) <= 4)  \n \n    domains = (f'{name}.{tld}'.lower() for name in names)  \n \n    print('FOUND\t\tNOT FOUND ')  \n \n    print('=====\t\t========= ') \n    async for domain, found in multi_probe (domains):  \n \n        indent = '' if found else '\t\t'  \n \n        print(f'{indent} {domain} ') \n \n \nif __name__  == '__main__ ': \n    if len(sys.argv) == 2: \n        asyncio.run(main(sys.argv[1]))  \n \n    else: \n        print('Please provide a TLD. ', f'Example: {sys.argv[0]}  \nCOM.BR')\nGenerate keywords with length up to 4.\nGenerate domain names with the given suf fix as TLD.\nFormat a header for the tabular output.\nAsynchronously iterate over multi_probe(domains) .\nSet indent  to zero or two tabs to put the result in the proper column.\nRun the main  coroutine with the given command-line ar gument.\nGenerators have one extra use unrelated to iteration: they can be made into\ncontext managers. This also applies to asynchronous generators.\nAsynchronous Generators as Context Managers\nW riting our own asynchronous context managers is not a frequent\nprogramming task, but if you need to write one, consider using the\n@asynccontextmanager  decorator added to the contextlib\nmodule in Python 3.7. That’ s very similar to the @contextmanager\ndecorator we studied in “Using @contextmanager” .\nAn interesting example combining @asynccontextmanager  with\nloop.run_in_executor  appears in Caleb Hattingh’ s book Using\nAsyncio in Python . Example 22-20  is Caleb’ s code—with a single change\nand added callouts.\nExample 22-20. Example using @asynccontextmanager  and\nloop.run_in_executor\nfrom contextlib  import asynccontextmanager  \n \n@asynccontextmanager  \nasync def web_page (url):  \n \n    loop = asyncio.get_running_loop ()   \n \n    data = await loop.run_in_executor (  \n \n        None, download_webpage , url) \n    yield data                          \n  \n    await loop.run_in_executor (None, update_stats , url)  \n \n \nasync with web_page ('google.com ') as data:  \n \n    process(data)\nThe decorated function must be an asynchronous generator .\nMinor update to Caleb’ s code: use the lightweight\nget_running_loop  instead of get_event_loop .\nSuppose download_webpage  is a blocking function using the\nr equests  library; we run it in a separate thread to avoid blocking the\nevent loop.\nAll lines before this yield  expression will become the __aenter__\ncoroutine-method of the asynchronous context manager built by the\ndecorator . The value of data  will be bound to the data  variable after\nthe as  clause in the async with  statement below .\nLines after the yield  will become the __aexit__  coroutine-method.\nHere another blocking call is delegated to the thread executor .\nUse web_page  with async with .\nThis is very similar to the sequential @contextmanager  decorator .\nPlease see “Using @contextmanager”  for more details, including error\nhandling at the yield  line. For another example of\n@asynccontextmanager , see the contextlib  documentation .\nNow let’ s wrap up our coverage of asynchronous generator functions by\ncontrasting them with native coroutines.\nAsynchronous Generators V ersus Native Coroutines\nHere are some key similarities and dif ferences between a native coroutine\nand an asynchronous generator functions:\nBoth are declared with async def .\nAn asynchronous generator always has a yield  expression in its\nbody—that’ s what makes it a generator . A native coroutine never\nhas yield .\nA native coroutine may return  some value other than None . An\nasynchronous generator can only use empty return  statements.\nNative coroutines are awaitable: they can be driven by await\nexpressions or passed to one of the many asyncio  functions that\ntake awaitable ar guments, such as create_task . Asynchronous",14318
304-Async Comprehensions and Async Generator Expressions.pdf,304-Async Comprehensions and Async Generator Expressions,"generators are not awaitable. They are asynchronous iterables,\ndriven by async for  or by asynchronous comprehensions.\nT ime to talk about asynchronous comprehensions.\nAsync Comprehensions and Async Generator\nExpressions\nPEP 530—Asynchronous Comprehensions  introduced the use of async\nfor  and await  in the syntax of comprehensions and generator\nexpressions, starting with Python 3.6.\nThe only construct defined by PEP 530 that can appear outside an async\ndef  body is an asynchronous generator expression.\nDefining and Using an Asynchronous Generator Expression\nGiven the multi_probe  asynchronous generator from Example 22-18 ,\nwe could write another asynchronous generator returning only the names of\nthe domains found. Here is how—again using the asynchronous console\nlaunched with -m asyncio :\nExample 22-21. domaincheck.py: utility for pr obing domains using\ndomainlib\n>>> import asyncio \n>>> from domainlib  import multi_probe  \n>>> names = 'python.org rust-lang.org golang.org  \nn05uch1an9.org '.split() \n>>> gen_found  = (domain async for domain, found in \nmulti_probe (names) if found)  \n \n>>> gen_found  \n<async_generator object <genexpr> at 0x10a8f9700>  \n  \n>>> async for name in gen_found :  \n \n...     print(name) \n... \ngolang.org  \npython.org  \nrust-lang.org\n\nThe use of async for  makes this an asynchronous generator\nexpression. It can be defined anywhere in a Python module.\nThe asynchronous generator expression builds an async_generator\nobject—exactly the same type of object returned by an asynchronous\ngenerator function like multi_probe .\nThe asynchronous generator object is driven by the async for\nstatement—which in turn can only appear inside an async def  body\n—or in the magic asynchronous console I used in this example.\nT o summarize: an asynchronous generator expression can be defined\nanywhere in your program, but it can only be used inside a native coroutine\nor asynchronous generator function.\nThe remaining constructs introduced by PEP 530 can only be defined and\nused inside native coroutines or asynchronous generator functions.\nAsynchronous Comprehensions\nY uri Selivanov—the author of PEP 530—justifies the need for\nasynchronous comprehensions with three short code snippets reproduced\nnext.\nW e can all agree that we should be able to rewrite this code:\nresult = [] \nasync for i in aiter(): \n    if i % 2: \n        result.append(i)\nLike this:\nresult = [i async for i in aiter() if i % 2]\nIn addition, given a native coroutine fun , we should be able to write this:\nresult = [await fun() for fun in funcs]\nUsing await  in a list comprehension does the same job as\nasyncio.gather . Back to the magic asynchronous console:\n>>> names = 'python.org rust-lang.org golang.org  \nn05uch1an9.org' .split() \n>>> names = sorted(names) \n>>> coros = [probe(name) for name in names] \n>>> await asyncio.gather(*coros) \n[Result(domain='golang.org', found=True),  \nResult(domain='n05uch1an9.org', found=False),  \nResult(domain='python.org', found=True), Result(domain='rust-\nlang.org', found=True)]  \n>>> [await probe(name) for name in names] \n[Result(domain='golang.org', found=True),  \nResult(domain='n05uch1an9.org', found=False),  \nResult(domain='python.org', found=True), Result(domain='rust-\nlang.org', found=True)]  \n>>>\nNote that I sorted the list of names to show that the results come out in the\norder they were submitted, in both cases.\nPEP 530 allows the use of async for  and await  in list comprehensions\nas well as in dict  and set  comprehensions. For example, here is a dict\ncomprehension to store the results of multi_probe —in the\nasynchronous console:\n>>> {name: found async for name, found in multi_probe (names)} \n{'golang.org': True, 'python.org': True, 'n05uch1an9.org': False,  \n'rust-lang.org': True}\nW e can use the await  keyword in the expression before the for  or\nasync for  clause, and also in the expression after the if  clause. Here is\na set comprehension in the asynchronous console, collecting only the\ndomains that were found:\n>>> {name for name in names if (await probe(name)).found} \n{'rust-lang.org', 'python.org', 'golang.org'}",4188
305-Async beyond asyncio Curio.pdf,305-Async beyond asyncio Curio,"I had to put extra parenthesis around the await  expression due to the\nhigher precedence of the __getattr__  operator .  (dot).\nAgain, all of these comprehensions can only appear inside an async def\nbody or in the enchanted asynchronous console.\nNow let’ s briefly discuss type hints for asynchronous types.\nG e n e r i c  A s y n c h r o n o u s  T y p e s\nThe following types were introduced in Python 3.5 and 3.6 to annotate\nasynchronous objects:\nclass typing.AsyncContextManager (Generic[T_co]): \n    ... \nclass typing.AsyncIterable (Generic[T_co]): \n    ... \nclass typing.AsyncIterator (AsyncIterable [T_co]): \n    ... \nclass typing.AsyncGenerator (AsyncIterator [T_co], Generic[T_co, \nT_contra ]): \n    ... \nclass typing.Awaitable (Generic[T_co]): \n    ... \nclass typing.Coroutine (Awaitable [V_co], Generic[T_co, T_contra , \nV_co]): \n    ...\nW ith Python 3.9, we should use the collections.abc  equivalents of\nthe above.\nI want to highlight three aspects of those generic types.\nFirst: they are all covariant on the first type parameter , which is the type of\nthe items yielded from these objects. Recall rule #1 of “V ariance Rules of\nThumb” :\nIf a formal type parameter defines a type for data that comes out of the\nobject, it can be covariant.\nSecond: AsyncGenerator  and Coroutine  are contravariant on the\nsecond to last parameter . That’ s the type of the ar gument of the low-level\n.send()  method that the event loop calls to drive asynchronous\ngenerators and coroutines. As such, it is an “input” type. Therefore, it can\nbe contravariant, per V ariance Rule of Thumb #2 :\nIf a formal type parameter defines a type for data that goes into the\nobject after its initial construction, it can be contravariant.\nThird: AsyncGenerator  has no return type, in contrast with\ntyping.Generator  which we saw in “Generic T ype Hints for Classic\nCoroutines” . Returning a value by raising StopIteration(value)\nwas one of the hacks that enabled generators to operate as coroutines and\nsupport yield from , as we saw in Chapter 19 . There is no such overlap\namong the asynchronous objects: AsyncGenerators  objects don’ t\nreturn values, and are completely separate from native coroutine objects,\nwhich are annotated with typing.Coroutine .\nNow let’ s talk about a very important feature of the async  statements,\nasync  expressions, and the objects they create: they are often used with\nasyncio  but, they are actually library-independent.\nA s y n c  b e y o n d  a s y n c i o :  C u r i o\nPython’ s async/await  language constructs are not tied to any specific\nevent loop or library .  Thanks to the hackable API provided by special\nmethods, anyone suf ficiently motivated can write their own asynchronous\nruntime environment and framework to drive native coroutines,\nasynchronous generators etc.\nThat’ s what David Beazley did in his Curio  project. He was interested in\nrethinking how these new language features could be used in a framework\nbuilt from scratch. Recall that asyncio  was released in Python 3.4, and it\nused yield from  instead of await , so its API could not leverage\nasynchronous context managers, asynchronous iterators, and everything13\nelse that the async/await  keywords made possible. As a result, Curio\nhas a cleaner API and a simpler implementation, compared to asyncio .\nExample 22-22  shows the blogdom.py  script ( Example 22-1 ) rewritten to\nuse Curio .\nExample 22-22. blogdom.py: Example 22-1 , now using Curio .\n#!/usr/bin/env python3  \nfrom curio import run, TaskGroup  \nimport curio.socket as socket \nfrom keyword import kwlist \n \nMAX_KEYWORD_LEN  = 4 \n \n \nasync def probe(domain: str) -> tuple[str, bool]:  \n \n    try: \n        await socket.getaddrinfo (domain, None)  \n \n    except socket.gaierror : \n        return (domain, False) \n    return (domain, True) \n \nasync def main() -> None: \n    names = (kw for kw in kwlist if len(kw) <= MAX_KEYWORD_LEN ) \n    domains = (f'{name}.dev'.lower() for name in names) \n    async with TaskGroup () as group:  \n \n        for domain in domains: \n            await group.spawn(probe, domain)  \n \n        async for task in group:  \n \n            domain, found = task.result \n            mark = '+' if found else ' ' \n            print(f'{mark} {domain} ') \n \nif __name__  == '__main__ ': \n    run(main())  \nprobe  doesn’ t need to get the event loop, because…\ngetaddrinfo  is a top-level function of curio.socket , not a\nmethod of a loop  object—as it is in asyncio .\nA TaskGroup  is a core concept in Curio , to monitor and control\nseveral coroutines, and to make sure they are all executed and cleaned\nup.\nTaskGroup.spawn  is how you start a coroutine, managed by a\nspecific TaskGroup  instance. The coroutine is wrapped by a Task .\nIterating with async for  over a TaskGroup  yields Task  instances\nas each is completed. This corresponds to the line in Example 22-1\nusing for … as_completed(…): .\nCurio  pioneered this sensible way to start an asynchronous program in\nPython.\nT o expand on the last point: if you look at the asyncio  code examples for\nFluent Python, First Edition  you’ll see lines like these, repeated over and\nover:\n    loop = asyncio.get_event_loop () \n    loop.run_until_complete (main()) \n    loop.close()\nA Curio  TaskGroup  is an asynchronous context manager that replaces\nseveral ad-hoc APIs and coding patterns in asyncio . W e just saw how\niterating over a TaskGroup  makes the asyncio.as_completed(…)\nfunction unnecessary . Another example: instead of a special gather\nfunction, this snippet from the T ask Gr oups  docs  collects the results of all\ntasks in the group:\nasync with TaskGroup (wait=all) as g: \n    await g.spawn(coro1) \n    await g.spawn(coro2) \n    await g.spawn(coro3) \nprint('Results:' , g.results)\nT ask groups support structur ed concurr ency : a form of concurrent\nprogramming that constrains all the activity of a group of asynchronous\ntasks to a single entry and exit point. This is analogous to structured\nprogramming, which eschewed the GOT O command and introduced block\nstatements to limit the entry and exit points of loops and subroutines. When\nused as an asynchronous context manager , a TaskGroup  ensures that all\ntasks spawned inside are completed or cancelled, and any exceptions raised,\nupon exiting the enclosed block.\nN O T E\nStructured concurrency will probably be adopted by asyncio  in upcoming Python\nreleases. A strong indication appears in PEP 654–Exception Groups and except* , which\nis under consideration for Python 3.10—as of March 2021. The Motivation  section\nmentions T rio’ s  “nurseries”, their name for task groups: “Implementing a better task\nspawning API in asyncio, inspired by T rio nurseries, was the main motivation for this\nPEP .”\nAnother important feature of Curio  is better support for programming with\ncoroutines and threads in the same codebase—a necessity in most non-\ntrivial asynchronous programs. Starting a thread with await\nspawn_thread(func, …)  returns an AsyncThread  object with a\nTask -like interface. Threads can call coroutines thanks to a special\nAWAIT(coro)  function—named in all-caps because await  is now a\nkeyword.\nCurio  also provides a UniversalQueue  that can be used to coordinate\nthe work among threads, Curio  coroutines, and asyncio  coroutines.\nThat’ s right, Curio  has features that allow it to run in a thread along with\nasyncio  in another thread, in the same process, communicating via\nUniversalQueue  and UniversalEvent . The API for these\n“universal” classes is the same inside and outside of coroutines, but in a\ncoroutine you need to prefix calls with await .\nAs I write this in March 2021, there are no asynchronous HTTP or database\nlibraries compatible with Curio , so its usage “out of the box” is limited to\nlow-level network programming. In the Curio  repository there is an\nimpressive set network programming examples , including one using",8013
306-How Async Works and How It Doesnt.pdf,306-How Async Works and How It Doesnt,,0
307-Running Circles Around Blocking Calls.pdf,307-Running Circles Around Blocking Calls,"W ebSocket , and another implementing the RFC 8305—Happy Eyeballs\nconcurrent algorithm for connecting to IPv6 endpoints with fast fallback to\nIPv4 if needed.\nThe design of Curio  has been influential. The T rio  framework started by\nNathaniel J. Smith was heavily inspired by Curio . Curio  may also have\nprompted Python contributors to improve the usability of the asyncio\nAPI. For example, in its earliest releases, asyncio  users very often had to\nget and pass around a loop  object because some essential functions were\neither loop  methods or required a loop  ar gument. As of Python 3.9,\ndirect access to the loop is not needed as often, and in fact several functions\nthat accepted an optional loop  are now deprecating that ar gument.\nNow let’ s talk about the advantages and challenges of asynchronous\nprogramming.\nH o w  A s y n c  W o r k s  a n d  H o w  I t  D o e s n ’ t\nThe sections closing this chapter discuss high-level ideas around\nasynchronous programming, regardless of the language or library you are\nusing.\nLet’ s begin by explaining the #1 reason why asynchronous programming is\nappealing, followed by a popular myth, and how to deal with it.\nRunning Circles Around Blocking Calls\nR yan Dahl, the inventor of Node.js , introduces the philosophy of his project\nby saying “W e’re doing I/O completely wrong. "" He defines a blocking\nfunction  as one that does file or network I/O, and ar gues that we can’ t treat\nthem as we treat nonblocking functions. T o explain why , he presents the\nnumbers in the second column of T able 22-1 .14\n \nT\na\nb\nl\ne  \n2\n2\n-\n1\n.  \nM\no\nd\ne\nr\nn  \nc\no\nm\np\nu\nt\ne\nr  \nl\na\nt\ne\nn\nc\ny  \nf\no\nr  \nr\ne\na\nd\ni\nn\ng  \nd\na\nt\na  \nf\nr\no\nm\n \nd\ni\nf\nf\ne\nr\ne\nn\nt  \nd\ne\nv\ni\nc\ne\ns\n;  \nt\nh\ni\nr\nd  \nc\no\nl\nu\nm\nn  \ns\nh\no\nw\ns  \np\nr\no\np\no\nr\nt\ni\no\nn\na\nl  \nt\ni\nm\ne\ns  \ni\nn  \na  \ns\nc\na\nl\ne  \ne\na\ns\ni\ne\nr  \nt\no  \nu\nn\nd\ne\nr\ns\nt\na\nn\nd  \nf\no\nr  \nu\ns  \ns\nl\no\nw\n \nh\nu\nm\na\nn\ns",2055
308-Further Reading.pdf,308-Further Reading,"Device CPU cycles Proportional “human” scale\n \nL1 cache 3 3 seconds\nL2 cache 14 14 seconds\nRAM 250 250 seconds\ndisk 41,000,000 1.3 years\nnetwork 240,000,000 7.6 years\n \nT o make sense of T able 22-1 , bear in mind that modern CPUs with GHz\nclocks run billions of cycles per second. Let’ s say that a CPU runs exactly 1\nbillion cycles per second. That CPU can make more than 333 million L1\ncache reads in one second, or 4 (four!) network reads in the same time. The\nthird column of T able 22-1  puts those numbers in perspective by\nmultiplying the second column by a constant factor . So, in an alternate\nuniverse, if one read from L1 cache took 3 seconds, then a network read\nwould take 7.6 years!\nT able 22-1  explains why a disciplined approach to asynchronous\nprogramming can lead to high performance servers. The challenge is\nachieving that discipline. The first step is to recognize that “I/O bound\nsystem” is a fantasy .\nThe Myth of I/O Bound Systems\nA commonly repeated meme is that asynchronous programming is good for\n“I/O bound systems”. I learned the hard way that there are no “I/O bound\nsystems”. Y ou may have I/O bound functions . Perhaps the vast majority of\nthe functions in your system are I/O bound, i.e. they spend more time\nwaiting for I/O than crunching data. While waiting, they cede control to the\nevent loop which can then drive some other pending task. But inevitably ,\nany non-trivial system will have some parts that are CPU-bound. Even\ntrivial systems reveal that, under stress. In “Soapbox”  I tell the story of two\nasynchronous programs that struggled with CPU-bound functions slowing\ndown the event loop with severe impact on performance.\nGiven that any non-trivial system will have CPU-bound functions, dealing\nwith them is the key to success in asynchronous programming.\nA voiding CPU-bound T raps\nIf you’re using Python at scale, you should have some automated tests\ndesigned specifically to detect performance regressions as soon as they\nappear . This is critically important with asynchronous code, but also\nrelevant to threaded Python code—because of the GIL. If you wait until the\nslowdown starts bothering the development team, it’ s too late. The fix will\nprobably require some major make over .\nHere are some options for when you identify a CPU-hogging bottleneck:\ndelegate the task to a Python process pool;\ndelegate the task to an external task queue;\nrewrite the relevant code in Cython, C, Rust or some other\nlanguage that compiles to machine code and interfaces with the\nPython/C API, preferably releasing the GIL;\ndecide that you can af ford the performance hit and do nothing—but\nrecord the decision to make it easier to revert it later .\nThe external task queue should be chosen and integrated as soon as possible\nat the start of the project, so that nobody in the team hesitates to use it when\nneeded.\nThe last option—do nothing—falls in the category of technical debt .\nConcurrent programming is a fascinating topic, and I would like to write a\nlot more about it. But it is not the main focus of the book, and this is already\none of the longest chapters, so let’ s wrap it up.\nC h a p t e r  S u m m a r y\nThe pr oblem with normal appr oaches to asynchr onous pr ogramming as\nthat they’r e all-or -nothing pr opositions. Y ou r ewrite all your code so\nnone of it blocks or you’r e just wasting your time.\n— Alvaro V idela & Jason J. W . W illiams, RabbitMQ in\nAction\nI chose that epigraph for this chapter for two reasons. At a high level, it\nreminds us to avoid blocking the event loop by delegating slow tasks to a\ndif ferent processing unit, from a simple thread all the way to a distributed\ntask queue. At a lower level, it is also a warning: once you write your first\nasync def , your program is inevitably going to have more and more\nasync def , await , async with  and async for . And using non-\nasynchronous libraries suddenly becomes a challenge.\nAfter the simple spinner  examples in Chapter 20 , here we really focused on\nasynchronous programing with native coroutines, starting with the\nblogdom.py  DNS probing example, followed by the concept of awaitables .\nWhile reading the source code of flags_asyncio.py , we found the first\nexample of an asynchr onous context manager .\nThe more advanced variations of the flag downloading program introduced\ntwo powerful functions: the asyncio.as_completed  generator and the\nloop.run_in_executor  coroutine. W e also saw the concept and\napplication of a semaphore to limit the number of concurrent downloads—\nas expected from well-behaved HTTP clients.\nServer -side asynchronous programming was presented through the\nmojifinder  examples: a FastAPI  W eb service and tcp_mojifinder .py —the\nlatter using just asyncio  and the TCP protocol.\nAsynchronous iteration and asynchronous iterables were the next major\ntopic, with sections on async for , Python’ s async console, asynchronous\ngenerators, asynchronous generator expressions, and asynchronous\ncomprehensions.\nThe last example in the chapter was blogdom.py  rewritten with the Curio\nframework, to demonstrate how Python’ s asynchronous features are not tied\nto the asyncio  package. Curio  also showcases the concept of structur ed\nconcurr ency  which may have an industry-wide impact, bringing more\nclarity to concurrent code.\nFinally , the sections under “How Async W orks and How It Doesn’ t”  discuss\nthe main appeal of asynchronous programming, the misconception of “I/O\nbound systems”, and dealing with the inevitable CPU-bound parts of your\nprogram.\nF u r t h e r  R e a d i n g\nDavid Beazley’ s PyOhio 2016 keynote Fear and A waiting in Async  is a\nfantastic, live coded introduction to the potential of the language features\nmade possible by Y uri Selivanov’ s contribution of the async/await\nkeywords in Python 3.5. At one point, Beazley complains that await  can’ t\nbe used in list comprehensions, but that was fixed by Selivanov in PEP 530\n—Asynchr onous Compr ehensions , implemented in Python 3.6 later in that\nsame year . Apart from that, everything else in Beazley’ s keynote is timeless,\nas he demonstrates how the asynchronous objects we saw in this chapter\nwork, without the help of any framework—just a simple run  function\nusing .send(None)  to drive coroutines. Only at the very end Beazley\nshows Curio , which he started that year as an experiment to see how far can\nyou go doing asynchronous programming without a foundation of callbacks\nor futures, just coroutines. As it turns out, you can go very far—as\ndemonstrated by the evolution of Curio  and the later creation of T rio  by\nNathaniel J. Smith. Curio’ s  documentation has links  to more talks by\nBeazley on the subject.\nBesides starting T rio , Nathaniel J. Smith wrote two deep blog posts that I\nhighly recommend: Some thoughts on asynchr onous API design in a post-\nasync/await world —contrasting the design of Curio  with that of asyncio —\nand Notes on structur ed concurr ency , or: Go statement consider ed harmful\n—about structured concurrency . Smith also gave a long and informative\nanswer to the question What is the cor e differ ence between asyncio and\ntrio?  on StackOverflow .\nT o learn more about the asyncio  package, I’ve mentioned the best written\nresources I know at the start of this chapter: the of ficial documentation  after\nthe outstanding overhaul  started by Y uri Selivanov in 2018, and Caleb\nHattingh’ s book Using Asyncio in Python  (O’Reilly , 2020). In the of ficial\ndocumentation, make sure to read Developing with asyncio : documenting\nthe asyncio  debug mode, and also discussing “common mistakes and traps”\nand “how to avoid them”.\nFor a very accessible, 30-minute introduction to asynchronous\nprogramming in general and also asyncio , watch Miguel Grinber g’ s\nAsynchr onous Python for the Complete Beginner , presented at PyCon 2017.\nAnother great introduction is Demystifying Python’ s Async and A wait\nKeywor ds  presented by Michael Kennedy—where among other things I\nlearned about the unsync  library that provides a decorator to delegate the\nexecution of coroutines, I/O bound functions and CPU-bound functions to\nasyncio , threading  or multiprocessing  as needed.\nAt EuroPython 2019, L ynn Root—a global leader of PyLadies —presented\nthe excellent Advanced asyncio: Solving Real-world Pr oduction Pr oblems ,\ninformed by her experience using Python as a Staf f Engineer at Spotify .\nIn 2020, Łukasz Langa recorded a series of great videos about asyncio ,\nstarting with Learn Python’ s AsyncIO #1 - The Async Ecosystem . Langa\nalso made the super cool video AsyncIO + Music  for PyCon 2020 that not\nonly shows asyncio  applied in a very concrete of event-oriented domain,\nbut also explains it from the ground up.\nAnother area dominated by event-oriented programming is embedded\nsystems. That’ s why Damien Geor ge added support for async/await  in\nhis Micr oPython  interpreter for microcontrollers. At PyCon Australia 2018,\nMatt T rentini demonstrated the uasyncio  library , a subset of asyncio  that is\npart of Micr oPython’ s  standard library .\nFor higher level thinking about async programming in Python, read the blog\npost Python async frameworks—Beyond developer tribalism  by T om\nChristie.\nFinally , I highly recommend What Color Is Y our Function?  by Bob\nNystrom, discussing the incompatible execution models of plain functions\nversus async functions—a.k.a. coroutines—in JavaScript, Python, C#, and\nother languages. Spoiler alert—Nystrom’ s conclusion is: the language that\ngot this right is Go, where all functions are the same color . I like that about\nGo. But I also think Nathaniel J. Smith has a point when he wrote Go\nstatement consider ed harmful . Nothing is perfect, and concurrent\nprogramming is always complicated.\nS O A P B O X\nHow a Slow Function Almost Spoiled The uvloop  Benchmarks\nIn 2016, Y uri Selivanov released uvloop , “a fast, drop-in replacement of\nthe built-in asyncio  event loop”. The benchmarks presented in\nSelivanov’ s blog post  announcing the library in 2016 are very\nimpressive. He wrote: “it is at least 2x faster than nodejs, gevent, as\nwell as any other Python asynchronous framework. The performance of\nuvloop-based asyncio is close to that of Go programs.”\nHowever , the post reveals that uvloop  is able to match the performance\nof Go under two conditions:\n1 . Go is configured to use a single thread. That makes the Go\nruntime behave similarly to asyncio : concurrency is\nachieved via multiple coroutines driven by an event loop, all in\na single thread.\n2 . The Python 3.5 code uses httptools  in addition to uvloop  itself.\nSelivanov explains that he wrote httptools  after benchmarking uvloop\nwith aiohttp —one of the first full-featured HTTP libraries built on\nasyncio :\nHowever , the performance bottleneck in aiohttp turned out to be its\nHTTP parser , which is so slow , that it matters very little how fast the\nunderlying I/O library is. T o make things mor e inter esting, we\ncr eated a Python binding for http-parser (nodejs HTTP parser C\nlibrary , originally developed for Nginx). The library is called\nhttptools, and is available on Github and PyPI.\nNow think about that: Selivanov’ s HTTP performance tests consisted of\na simple echo server written in the dif ferent languages/libraries,\npounded by the wrk  benchmarking tool. Most developers would\nconsider a simple echo server an “I/O bound system”, right? But it\nturned out that parsing HTTP headers is CPU-bound, and it had a slow15\nPython implementation in aiohttp  in when Selivanov did the\nbenchmarks in 2016.  Whenever a Python function was parsing\nheaders in Python, the event loop was blocked. The impact was so\nsignificant that Selivanov went to the extra trouble of writing httptools .\nW ithout optimizing the CPU-bound code, the performance gains of a\nfaster event loop were lost.\nDeath by a Thousand Cuts\nInstead of a simple echo server , imagine a complex and evolving\nPython system with tens of thousands of lines of asynchronous code,\ninterfacing with many external libraries. Y ears ago I was asked to help\ndiagnose performance problems in a system like that. It was written in\nPython 2.7 with the T wisted  framework—a solid library and in many\nways a precursor to asyncio  itself.\nPython was used to build a façade for the W eb UI, integrating\nfunctionality provided by pre-existing libraries and command-line tools\nwritten in other languages—but not designed for concurrent execution.\nThe project was ambitious, it had been in development for more than a\nyear already , but it was not in production yet.  Over time, the\ndevelopers noticed that the performance of the whole system was\ndecreasing, and they were having a hard time finding the bottlenecks.\nWhat was happening: with each added feature, more CPU-bound code\nwas slowing down T wisted ’ s event loop. Python’ s role as a glue\nlanguage meant there was a lot of data parsing and conversion between\ndata formats. There wasn’ t a single bottleneck: the problem was spread\nover countless little functions added over months of development.\nFixing that would require rethinking the architecture of the system,\nrewriting a lot of code, probably leveraging a task queue, perhaps using\nmicroservices or custom libraries written in languages better suited for\nCPU-intensive concurrent processing. The stakeholders were not\nprepared to make that additional investment, and the project was\ncancelled shortly afterwards.16\n17\nWhen I told this story to Glyph Lefkowitz—founder the T wisted  project\n—he said that one of his priorities at the start of an asynchronous\nprogramming project is to decide which tools he will use to farm-out\nthe CPU-intensive tasks. This conversation with Glyph was the\ninspiration for “A voiding CPU-bound T raps” .\nSmarter Clients for Better Concurr ency\nDealing with slow clients is a major challenge for server -side\nprogrammers. Asynchronous programming is a good general strategy to\ndeal with slow clients precisely because it is much cheaper to have a\ncoroutine than a thread waiting for each client, therefore you can handle\nmany more slow clients.\nBut you can also help your server -side system handle more clients if\nthey are smarter . For example, in web_mojifinder .py , there is no\npagination. If you search for “CJK”, you’ll get more than 90,000\nChinese, Japanese, and Korean characters (that’ s what CJK stands for).\nNobody will read more than a few dozen lines, so it is a waste of\ncomputing power and bandwidth to send so many results. Implementing\npagination or “infinite scroll” can drastically reduce this waste, but it\ndoes require more code on the client and the server .\nPagination is just one example. The main point is: consider how to split\nthe task of the server in smaller chunks, so that it can handle more\nclients at one time. If you’re used to the full-page-at-time style of W eb\ndevelopment, this requires a new mindset, a lot more front-end code,\nand—sometimes—the use of new technology such as W ebSockets ,\nwhich an asynchronous server -side framework is better prepared to\nhandle. That’ s the reason why the ASGI specification was started by\nDjango  developers, and they are adding asynchronous features with\nevery new release since Django 3.0 .\n1  V idela & W illiams, RabbitMQ in Action (Manning, 2012) , Chapter 4, Solving Pr oblems with\nRabbit: coding and patterns , p. 61\n2  Selivanov implemented async/await  in Python, and wrote the related PEPs 492 , 525 , and\n530 .\n3  There is one exception to this rule: if you run Python with the -m asyncio  option you can\nuse await  directly at the >>>  prompt to drive a native coroutine. This is explained in\n“Experimenting with Python’ s Async Console” .\n4  Sorry , I could not resist it.\n5  true.dev  is available for USD 360/year as I write this. I see that for.dev  is registered,\nbut has no DNS configured.\n6  Thanks to Guto Maia who noted that the concept of a semaphore was not explained when he\nread the first edition draft for this chapter .\n7  A detailed discussion about this can be found in a thread I started in the python-tulip group,\ntitled “Which other futures my come out of asyncio.as_completed?” . Guido responds, and\ngives insight on the implementation of as_completed  as well as the close relationship\nbetween futures and coroutines in asyncio .\n8  Instead of uvicorn , you may use another ASGI server , such as hyper corn  or Daphne . See the\nof ficial ASGI documentation page about implementations  for more\n9  As mentioned in Chapter 8 , pydantic  enforces type hints at runtime, for data validation.\n10  Thanks for tech reviewer Miroslav Šedivý for highlighting good places to use pathlib  in\ncode examples.\n11  T ech reviewer Leonardo Rochael pointed out that building the index could be delegated to\nanother thread using loop.run_with_executor()  in the supervisor  coroutine, so\nthe server would be ready to take requests immediately while the index is built. That’ s true, but\nquerying the index is the only thing this server does, so it would not be a big win in this\nexample.\n12  This is great for experimentation, like the Node.js  console. Thanks Y uri Selivanov for yet\nanother excellent contribution to asynchronous Python.\n13  That’ s in contrast with JavaScript, where async/await  is hardwired to the built-in event\nloop and runtime environment, i.e. a browser , Node.js , or Deno .\n14  V ideo: Introduction to Node.js  at 4:55.\n15  Using a single thread was the default setting until Go 1.5 was released. Y ears before, Go had\nalready earned a well deserved reputation for enabling highly concurrent networked systems.\nOne more evidence that concurrency doesn’ t require multiple threads or CPU cores.\n16  Maybe that part of aiohttp  has been optimized since then; I haven’ t checked.\n17  Regardless of technical choices, this was probably the biggest mistake in this project: the\nstakeholders did not go for an MVP approach—delivering a Minimum V iable Product as soon\nas possible, and then adding features at a steady pace.",18227
309-Whats new in this chapter.pdf,309-Whats new in this chapter,"Chapter 23. Dynamic Attributes\nand Properties\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 23rd chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nThe crucial importance of pr operties is that their existence makes it\nperfectly safe and indeed advisable for you to expose public data\nattributes as part of your class’ s public interface.\n— Martelli, Ravenscroft & Holden, Why properties are\nimportant\nData attributes and methods are collectively known as attributes  in Python.\nA method is an attribute that is callable . Besides data attributes and\nmethods, we can also create properties, which replace a public data attribute\nwith accessor methods  (i.e., getter/setter), without changing the class\ninterface. This follows Bertrand Meyer ’ s Uniform access principle :\nAll services offer ed by a module should be available thr ough a uniform\nnotation, which does not betray whether they ar e implemented thr ough\nstorage or thr ough computation .1\n2",1474
310-Data Wrangling with Dynamic Attributes.pdf,310-Data Wrangling with Dynamic Attributes,"Besides the property  decorator , Python provides a rich API for\ncontrolling attribute access and implementing dynamic attributes. The\ninterpreter calls the __getattr__  and __setattr__  special methods\nto handle attribute access or assignment using dot notation (e.g.,\nobj.attr ) or via the built-in functions getattr  and setattr . A user -\ndefined class implementing __getattr__  can implement “virtual\nattributes” by computing values on the fly whenever somebody tries to read\na nonexistent attribute like obj.no_such_attr .\nCoding dynamic attributes is the kind of metaprogramming that framework\nauthors do. However , in Python the basic techniques are straightforward, so\nwe can use them in everyday data wrangling tasks. That’ s how we’ll start\nthis chapter .\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nMost updates to this chapter were motivated by a discussion of\n@functools.cached_property  (introduced in Python 3.8), as well\nas the combined use @property  with @functools.cache  (new in\n3.9). This af fected the code for the Record  and Event  classes that appear\nin “Computed Properties” . I also added a refactoring to leverage the PEP\n412—Key-Sharing Dictionary  optimization.\nT o highlight more relevant features while keeping the examples readable, I\nremoved some nonessential code—mer ging the old DbRecord  class into\nRecord , replacing shelve.Shelve  with a dict , and deleting the logic\nto download the OSCON dataset—which the examples now read from a\nlocal file included in the Fluent Python, Second Edition  code repository .\nD a t a  W r a n g l i n g  w i t h  D y n a m i c  A t t r i b u t e s\nIn the next few examples, we’ll leverage dynamic attributes to work with a\nJSON dataset published by O’Reilly for the OSCON 2014 conference.\nExample 23-1  shows four records from that dataset.3\nExample 23-1. Sample r ecor ds fr om osconfeed.json; some field contents\nabbr eviated\n{ ""Schedule"" : \n  { ""conferences"" : [{""serial"" : 115 }], \n    ""events"" : [ \n      { ""serial"" : 34505, \n        ""name"": ""Why Schools Don´t Use Open Source to Teach  \nProgramming"" , \n        ""event_type"" : ""40-minute conference session"" , \n        ""time_start"" : ""2014-07-23 11:30:00"" , \n        ""time_stop"" : ""2014-07-23 12:10:00"" , \n        ""venue_serial"" : 1462, \n        ""description"" : ""Aside from the fact that high school  \nprogramming..."" , \n        ""website_url"" : \n""http://oscon.com/oscon2014/public/schedule/detail/34505"" , \n        ""speakers"" : [157509], \n        ""categories"" : [""Education"" ] } \n    ], \n    ""speakers"" : [ \n      { ""serial"" : 157509, \n        ""name"": ""Robert Lefkowitz"" , \n        ""photo"": null, \n        ""url"": ""http://sharewave.com/"" , \n        ""position"" : ""CTO"", \n        ""affiliation"" : ""Sharewave"" , \n        ""twitter"" : ""sharewaveteam"" , \n        ""bio"": ""Robert ´r0ml´ Lefkowitz is the CTO at Sharewave, a  \nstartup...""  } \n    ], \n    ""venues"" : [ \n      { ""serial"" : 1462, \n        ""name"": ""F151"", \n        ""category"" : ""Conference Venues""  } \n    ] \n  } \n}\nExample 23-1  shows 4 of the 895 records in the JSON file. The entire\ndataset is a single JSON object with the key ""Schedule"" , and its value is\nanother mapping with four keys: ""conferences"" , ""events"" ,\n""speakers"" , and ""venues"" . Each of those four keys maps to a list of\nrecords. In the full dataset the ""events"" , ""speakers"" , and ""venues""\nlists have dozens or hundreds of records, while ""conferences""  has only\nthat one record shown in Example 23-1 . Every record has a ""serial""\nfield, which is a unique identifier for the record within the list.\nI used Python’ s console to explore the dataset, as shown in Example 23-2 .\nExample 23-2. Interactive exploration of osconfeed.json\n>>> import json \n>>> with open('data/osconfeed.json ') as fp: \n...     feed = json.load(fp)  \n \n>>> sorted(feed['Schedule '].keys())  \n \n['conferences', 'events', 'speakers', 'venues']  \n>>> for key, value in sorted(feed['Schedule '].items()): \n...     print(f'{len(value):3} {key} ')  \n \n... \n  1 conferences  \n484 events  \n357 speakers  \n 53 venues  \n>>> feed['Schedule ']['speakers '][-1]['name']  \n \n'Carina C. Zona'  \n>>> feed['Schedule ']['speakers '][-1]['serial']  \n \n141590 \n>>> feed['Schedule ']['events'][40]['name'] \n'There *Will* Be Bugs'  \n>>> feed['Schedule ']['events'][40]['speakers ']  \n \n[3471, 5199]\nfeed  is a dict  holding nested dicts and lists, with string and integer\nvalues.\nList the four record collections inside ""Schedule"" .\nDisplay record counts for each collection.\nNavigate through the nested dicts and lists to get the name of the last\nspeaker .\nGet serial number of that same speaker .\nEach event has a 'speakers'  list with zero or more speaker serial\nnumbers.",4789
311-Exploring JSON-Like Data with Dynamic Attributes.pdf,311-Exploring JSON-Like Data with Dynamic Attributes,"Exploring JSON-Like Data with Dynamic Attributes\nExample 23-2  is simple enough, but the syntax feed['Schedule']\n['events'][40]['name']  is cumbersome. In JavaScript, you can get\nthe same value by writing feed.Schedule.events[40].name . It’ s\neasy to implement a dict -like class that does the same in Python—there\nare plenty of implementations on the W eb.  I wrote FrozenJSON , which\nis simpler than most recipes because it supports reading only: it’ s just for\nexploring the data. FrozenJSON  is also recursive, dealing automatically\nwith nested mappings and lists.\nExample 23-3  is a demonstration of FrozenJSON  and the source code is\nin Example 23-4 .\nExample 23-3. Fr ozenJSON fr om Example 23-4  allows r eading attributes\nlike name  and calling methods like .keys()  and .items()\n    >>> import json \n    >>> raw_feed  = json.load(open('data/osconfeed.json ')) \n    >>> feed = FrozenJSON (raw_feed )  \n \n    >>> len(feed.Schedule .speakers )  \n \n    357 \n    >>> feed.keys() \n    dict_keys (['Schedule ']) \n    >>> sorted(feed.Schedule .keys())  \n \n    ['conferences ', 'events', 'speakers ', 'venues'] \n    >>> for key, value in sorted(feed.Schedule .items()): \n \n    ...     print(f'{len(value):3} {key} ') \n    ... \n      1 conferences  \n    484 events \n    357 speakers  \n     53 venues \n    >>> feed.Schedule .speakers [-1].name  \n \n    'Carina C. Zona ' \n    >>> talk = feed.Schedule .events[40] \n    >>> type(talk)  \n \n    <class 'explore0 .FrozenJSON '> \n    >>> talk.name \n    'There *Will* Be Bugs ' \n    >>> talk.speakers   \n \n    [3471, 5199] \n    >>> talk.flavor  \n \n    Traceback  (most recent call last): 4\n      ... \n    KeyError : 'flavor'\nBuild a FrozenJSON  instance from the raw_feed  made of nested\ndicts and lists.\nFrozenJSON  allows traversing nested dicts by using attribute\nnotation; here we show the length of the list of speakers.\nMethods of the underlying dicts can also be accessed, like .keys() , to\nretrieve the record collection names.\nUsing items() , we can retrieve the record collection names and their\ncontents, to display the len()  of each of them.\nA list , such as feed.Schedule.speakers , remains a list, but\nthe items inside are converted to FrozenJSON  if they are mappings.\nItem 40 in the events  list was a JSON object; now it’ s a\nFrozenJSON  instance.\nEvent records have a speakers  list with speaker serial numbers.\nT rying to read a missing attribute raises KeyError , instead of the\nusual AttributeError .\nThe keystone of the FrozenJSON  class is the __getattr__  method,\nwhich we already used in the Vector  example in “V ector T ake #3:\nDynamic Attribute Access” , to retrieve Vector  components by letter—\nv.x , v.y , v.z , etc. It’ s essential to recall that the __getattr__  special\nmethod is only invoked by the interpreter when the usual process fails to\nretrieve an attribute (i.e., when the named attribute cannot be found in the\ninstance, nor in the class or in its superclasses).\nThe last line of Example 23-3  exposes a minor issue with my code: trying\nto read a missing attribute should raise AttributeError , and not\nKeyError  as shown. When I implemented the error handling to do that,\nthe __getattr__  method became twice as long, distracting from the\nmost important logic I wanted to show . Given that users would know that a\nFrozenJSON  is built from mappings and lists, I think the KeyError  is\nnot too confusing.\nAs shown in Example 23-4 , the FrozenJSON  class has only two methods\n(__init__ , __getattr__ ) and a __data  instance attribute, so\nattempts to retrieve an attribute by any other name will trigger\n__getattr__ . This method will first look if the self.__data  dict\nhas an attribute (not a key!) by that name; this allows FrozenJSON\ninstances to handle any dict  method such as items , by delegating to\nself.__data.items() . If self.___data  doesn’ t have an attribute\nwith the given name , __getattr__  uses name  as a key to retrieve an\nitem from self.__dict , and passes that item to FrozenJSON.build .\nThis allows navigating through nested structures in the JSON data, as each\nnested mapping is converted to another FrozenJSON  instance by the\nbuild  class method.\nExample 23-4. explor e0.py: turn a JSON dataset into a Fr ozenJSON\nholding nested Fr ozenJSON objects, lists, and simple types\nfrom collections  import abc \n \n \nclass FrozenJSON : \n    """"""A read-only façade for navigating a JSON-like object  \n       using attribute notation  \n    """""" \n \n    def __init__ (self, mapping): \n        self.__data = dict(mapping)  \n \n \n    def __getattr__ (self, name):  \n \n        try: \n            return getattr(self.__data, name)  \n \n        except AttributeError : \n            return FrozenJSON .build(self.__data[name])  \n \n \n    @classmethod  \n    def build(cls, obj):  \n \n        if isinstance (obj, abc.Mapping):  \n \n            return cls(obj) \n        elif isinstance (obj, abc.MutableSequence ):  \n \n            return [cls.build(item) for item in obj] \n        else:  \n \n            return obj\nBuild a dict  from the mapping  ar gument. This ensures we got a\nmapping or something that can be converted to one.\n__getattr__  is called only when there’ s no attribute with that name .\nIf name  matches an attribute of the instance __data , return that. This\nis how calls like feed.keys()  are handled: the keys  method is an\nattribute of the __data  dict .\nOtherwise, fetch the item with the key name  from self.__data , and\nreturn the result of calling FrozenJSON.build()  on that.\nThis is an alternate constructor , a common use for the @classmethod\ndecorator .\nIf obj  is a mapping, build a FrozenJSON  with it. This is an example\nof goose typing .\nIf it is a MutableSequence , it must be a list,  so we build a list\nby passing each item in obj  recursively to .build() .\nIf it’ s not a dict  or a list , return the item as it is. It should be a str\nor an int , given the contents of the JSON file.\nNote that no caching or transformation of the original dataset is done. As\nthe dataset is traversed, the nested data structures are converted again and\nagain into FrozenJSON . That’ s OK for a dataset of this size, and for a\nscript that will only be used to explore or convert the data.5\n6",6336
312-The Invalid Attribute Name Problem.pdf,312-The Invalid Attribute Name Problem,"Any script that generates or emulates dynamic attribute names from\narbitrary sources must deal with one issue: the keys in the original data may\nnot be suitable attribute names. The next section addresses this.\nThe Invalid Attribute Name Problem\nThe FrozenJSON  code doesn’ t handle attribute names that are Python\nkeywords. For example, if you build an object like this:\n>>> student = FrozenJSON ({'name': 'Jim Bo' , 'class': 1982})\nY ou won’ t be able to read student.class  because class  is a reserved\nkeyword in Python:\n>>> student.class \n  File ""<stdin>"" , line 1 \n    student.class \n         ^ \nSyntaxError : invalid syntax\nY ou can always do this, of course:\n>>> getattr(student, 'class') \n1982\nBut the idea of FrozenJSON  is to provide convenient access to the data,\nso a better solution is checking whether a key in the mapping given to\nFrozenJSON.__init__  is a keyword, and if so, append an _  to it, so\nthe attribute can be read like this:\n>>> student.class_ \n1982\nThis can be achieved by replacing the one-liner __init__  from\nExample 23-4  with the version in Example 23-5 .\nExample 23-5. explor e1.py: append a _ to attribute names that ar e Python\nkeywor ds\n    def __init__ (self, mapping): \n        self.__data = {} \n        for key, value in mapping.items(): \n            if keyword.iskeyword (key):  \n \n                key += '_' \n            self.__data[key] = value\nThe keyword.iskeyword(…)  function is exactly what we need; to\nuse it, the keyword  module must be imported, which is not shown in\nthis snippet.\nA similar problem may arise if a key in the JSON is not a valid Python\nidentifier:\n>>> x = FrozenJSON ({'2be':'or not' }) \n>>> x.2be \n  File ""<stdin>"" , line 1 \n    x.2be \n      ^ \nSyntaxError : invalid syntax\nSuch problematic keys are easy to detect in Python 3 because the str  class\nprovides the s.isidentifier()  method, which tells you whether s  is\na valid Python identifier according to the language grammar . But turning a\nkey that is not a valid identifier into valid attribute name is not trivial. T wo\nsimple solutions would be raising an exception or replacing the invalid keys\nwith generic names like attr_0 , attr_1 , and so on. For the sake of\nsimplicity , I will not worry about this issue.\nAfter giving some thought to the dynamic attribute names, let’ s turn to\nanother essential feature of FrozenJSON : the logic of the build  class\nmethod, which is used by __getattr__  to return a dif ferent type of\nobject depending on the value of the attribute being accessed, so that nested\nstructures are converted to FrozenJSON  instances or lists of\nFrozenJSON  instances.\nInstead of a class method, the same logic could be implemented as the\n__new__  special method, as we’ll see next.",2790
313-Flexible Object Creation with __new__.pdf,313-Flexible Object Creation with __new__,"Flexible Object Creation with __new__\nW e often refer to __init__  as the constructor method, but that’ s because\nwe adopted jar gon from other languages. In Python, __init__  gets self\nas the first ar gument, therefore the object already exists when __init__\nis called by the interpreter . Also, __init__  cannot return anything. So it’ s\nreally an initializer , not a constructor .\nThe special method that Python calls to construct an instance is __new__ :\nit’ s a class method, but gets special treatment, so the @classmethod\ndecorator is not used. Python takes the instance returned by __new__  and\npasses it as the first ar gument self  of __init__ . W e rarely need to code\n__new__ , because the implementation inherited from object  suf fices for\nthe vast majority of use cases.\nIf necessary , the __new__  method can also return an instance of a dif ferent\nclass. When that happens, the interpreter does not call __init__ . In other\nwords, Python’ s logic for building an object is similar to this pseudocode:\n# pseudo-code for object construction  \ndef make(the_class , some_arg ): \n    new_object  = the_class .__new__(some_arg ) \n    if isinstance (new_object , the_class ): \n        the_class .__init__ (new_object , some_arg ) \n    return new_object  \n \n# the following statements are roughly equivalent  \nx = Foo('bar') \nx = make(Foo, 'bar')\nExample 23-6  shows a variation of FrozenJSON  where the logic of the\nformer build  class method was moved to __new__ .\nExample 23-6. explor e2.py: using new  instead of build to construct new\nobjects that may or may not be instances of Fr ozenJSON\nfrom collections  import abc \nimport keyword \n \nclass FrozenJSON : \n    """"""A read-only façade for navigating a JSON-like object  \n       using attribute notation  \n    """""" \n \n    def __new__(cls, arg):  \n \n        if isinstance (arg, abc.Mapping): \n            return super().__new__(cls)  \n \n        elif isinstance (arg, abc.MutableSequence ):  \n \n            return [cls(item) for item in arg] \n        else: \n            return arg \n \n    def __init__ (self, mapping): \n        self.__data = {} \n        for key, value in mapping.items(): \n            if keyword.iskeyword (key): \n                key += '_' \n            self.__data[key] = value \n \n    def __getattr__ (self, name): \n        if hasattr(self.__data, name): \n            return getattr(self.__data, name) \n        else: \n            return FrozenJSON (self.__data[name])  \nAs a class method, the first ar gument __new__  gets is the class itself,\nand the remaining ar guments are the same that __init__  gets, except\nfor self .\nThe default behavior is to delegate to the __new__  of a super class. In\nthis case, we are calling __new__  from the object  base class,\npassing FrozenJSON  as the only ar gument.\nThe remaining lines of __new__  are exactly as in the old build\nmethod.\nThis was where FrozenJSON.build  was called before; now we just\ncall the FrozenJSON  class, which Python handles by calling\nFrozenJSON.__new__ .",3068
314-Computed Properties.pdf,314-Computed Properties,"The __new__  method gets the class as the first ar gument because, usually ,\nthe created object will be an instance of that class. So, in\nFrozenJSON.__new__ , when the expression\nsuper().__new__(cls)  ef fectively calls\nobject.__new__(FrozenJSON) , the instance built by the object\nclass is actually an instance of FrozenJSON —i.e., the __class__\nattribute of the new instance will hold a reference to FrozenJSON —even\nthough the actual construction is performed by object.__new__ ,\nimplemented in C, in the guts of the interpreter .\nThe OSCON JSON dataset is structured in a way that is not helpful. For\nexample, the event at index 40 , titled 'There *Will* Be Bugs'  has\ntwo speakers, 3471  and 5199 . Finding the names of the speakers is\nawkward, because those are serial numbers and the\nSchedule.speakers  list is not indexed by them. T o get each speaker ,\nwe must iterate over that list until we find a record with a matching serial\nnumber . Our next task is restructuring the data, to prepare for automatic\nretrieval of linked records.\nC o m p u t e d  P r o p e r t i e s\nN O T E\nW e first saw the @property  decorator in Chapter 1 1 , section “A Hashable V ector2d” .\nIn Example 1 1-7 , I used two properties in Vector2d  just to make the x  and y\nattributes read-only . Here we will see properties that compute values, leading to a\ndiscussion of how to cache such values.\nThe records in the 'events'  list of the OSCON JSON data contain\ninteger serial numbers pointing to records in the 'speakers'  and\n'venues'  lists. For example, this is the record for a conference talk (with\nan elided description):\n{ ""serial"" : 33950, \n  ""name"": ""There *Will* Be Bugs"" , \n  ""event_type"" : ""40-minute conference session"" , \n  ""time_start"" : ""2014-07-23 14:30:00"" , \n  ""time_stop"" : ""2014-07-23 15:10:00"" , \n  ""venue_serial"" : 1449, \n  ""description"" : ""If you're pushing the envelope of  \nprogramming..."" , \n  ""website_url"" : \n""http://oscon.com/oscon2014/public/schedule/detail/33950"" , \n  ""speakers"" : [3471, 5199], \n  ""categories"" : [""Python"" ] }\nW e will implement an Event  class with venue  and speakers\nproperties to return the linked data automatically—in other words,\n“dereferencing” the serial number . Given an Event  instance, this is the\ndesired behavior:\nExample 23-7.\n    >>> event  \n  \n    <Event 'There *Will* Be Bugs'>  \n    >>> event.venue  \n  \n    <Record serial=1449>  \n    >>> event.venue.name  \n  \n    'Portland 251'  \n    >>> for spkr in event.speakers:  \n  \n    ...     print(f'{spkr.serial}: {spkr.name}')  \n    ... \n    3471: Anna Martelli Ravenscroft  \n    5199: Alex Martelli\nGiven an Event  instance…\n…reading event.venue  returns a Record  object instead of a serial\nnumber .\nNow it’ s easy to get the name of the venue .\nThe event.speakers  property returns a list of Record  instances.",2877
315-Step 1 Data-driven Attribute Creation.pdf,315-Step 1 Data-driven Attribute Creation,"As usual, we will build the code step-by-step, starting with the Record\nclass and a function to read the JSON data and return a dict  with Record\ninstances.\nStep 1: Data-driven Attribute Creation\nHere is the doctest  to guide this first step:\nExample 23-8. T est driving schedule_v1.py ( Example 23-9 )\n    >>> records = load(JSON_PATH )  \n \n    >>> speaker = records['speaker.3471 ']  \n \n    >>> speaker  \n \n    <Record serial=3471> \n    >>> speaker.name, speaker.twitter  \n \n    ('Anna Martelli Ravenscroft ', 'annaraven ')\nload  a dict  with the JSON data.\nThe keys in records  are strings built from the record type and serial.\nspeaker  is an instance of the Record  class defined in Example 23-9 .\nFields from the original JSON can be retrieved as Record  instance\nattributes.\nThe code for schedule_v1.py  is in Example 23-9 .\nExample 23-9. schedule_v1.py: r eor ganizing the OSCON schedule data\nimport json \n \nJSON_PATH  = 'data/osconfeed.json ' \n \nclass Record: \n    def __init__ (self, **kwargs): \n        self.__dict__ .update(kwargs)  \n \n \n    def __repr__ (self): \n        cls_name  = self.__class__ .__name__  \n        return f'<{cls_name} serial={self.serial!r}> '  \n \n \ndef load(path=JSON_PATH ): \n    records = {}  \n \n    with open(path) as fp: \n        raw_data  = json.load(fp)  \n \n    for collection , raw_records  in raw_data ['Schedule '].items():  \n \n        record_type  = collection [:-1]  \n \n        for raw_record  in raw_records : \n            key = f'{record_type}.{raw_record[ ""serial""]}' \n \n            records[key] = Record(**raw_record )  \n \n    return records\nThis is a common shortcut to build an instance with attributes created\nfrom keyword ar guments (detailed explanation follows).\nUse the serial  field to build the custom Record  representation\nshown in Example 23-8 .\nload  will ultimately return a dict  of Record  instances.\nParse the JSON, returning native Python objects: lists, dicts, strings,\nnumbers etc.\nIterate over the four top-level lists named 'conferences' ,\n'events' , 'speakers' , and 'venues' .\nrecord_type  is the list name without the last character , so\nspeakers  becomes speaker .\nBuild the key  in the format 'speaker.3471' .\nCreate a Record  instance and save it in records  with the key\nThe Record.__init__  method illustrates an old Python hack. Recall\nthat the __dict__  of an object is where its attributes are kept—unless\n__slots__  is declared in the class, as we saw in “Saving Memory with\n__slots__ ” . So, updating an instance __dict__  with a mapping is a\nquick way to create a bunch of attributes in that instance.7",2658
316-Step 2 Property to Retrieve a Linked Record.pdf,316-Step 2 Property to Retrieve a Linked Record,"N O T E\nDepending on the application, the Record  class may need to deal with keys that are not\nvalid attribute names, as we saw in “The Invalid Attribute Name Problem” . Dealing\nwith that issue would distract from the key idea of this example, and is not a problem in\nthe data set we are reading.\nThe definition of Record  in Example 23-9  is so simple that you may be\nwondering why I did not use it before, instead of the more complicated\nFrozenJSON . There are two reasons. First, FrozenJSON  works by\nrecursively converting the nested mappings and lists; Record  doesn’ t need\nthat because our converted dataset doesn’ t have mappings nested in\nmappings or lists. The records contain only strings, integers, lists of strings,\nand lists of integers. Second reason: FrozenJSON  provides access to the\nembedded __data  dict  attributes—which we used to invoke methods\nlike .keys() —and now we don’ t need that functionality either .\nN O T E\nThe Python standard library provides at least two classes similar to Record , where\neach instance has an arbitrary set of attributes built from keyword ar guments given to\n__init__ : multiprocessing.Namespace  and argparse.Namespace . I\nwrote the simpler Record  class to highlight the essential idea: __init__  updating\nthe instance __dict__ .\nAfter reor ganizing the schedule dataset, we can enhance the Record  class\nto automatically retrieve venue  and speaker  records referenced in an\nevent  record. W e’ll use properties to do that in the next examples.\nStep 2: Property to Retrieve a Linked Record\nThe goal of this next version is: given an event  record, reading its venue\nproperty will return a Record . This is similar to what the Django ORM\ndoes when you access a ForeignKey  field: instead of the key , you get the\nlinked model object.\nW e’ll start with the venue  property . See the partial interaction in\nExample 23-10  as an example.\nExample 23-10. Extract fr om the doctests of schedule_v2.py\n    >>> event = Record.fetch('event.33950 ')  \n \n    >>> event  \n \n    <Event 'There *Will* Be Bugs '> \n    >>> event.venue  \n \n    <Record serial=1449> \n    >>> event.venue.name  \n \n    'Portland 251 ' \n    >>> event.venue_serial   \n \n    1449\nThe Record.fetch  static method gets a Record  or an Event  from\nthe dataset.\nNote that event  is an instance of the Event  class.\nAccessing event.venue  returns a Record  instance.\nNow it’ s easy to find out the name of an event.venue .\nThe Event  instance also has a venue_serial  attribute, from the\nJSON data.\nEvent  is a subclass of Record  adding a venue  to retrieve linked\nrecords, and a specialized __repr__  method.\nThe code for this section is in the schedule_v2.py  module in the Fluent\nPython 2e  code repository . The example has nearly 60 lines, so I’ll present\nit in parts, starting with the enhanced Record  class.\nExample 23-1 1. schedule_v2.py: Record  class with a new fetch  method.\nimport inspect  \n \nimport json \n \nJSON_PATH  = 'data/osconfeed.json ' \n \nclass Record: \n \n    __index = None  \n \n \n    def __init__ (self, **kwargs): \n        self.__dict__ .update(kwargs) \n \n    def __repr__ (self): \n        cls_name  = self.__class__ .__name__  \n        return f'<{cls_name} serial={self.serial!r}> ' \n \n    @staticmethod   \n \n    def fetch(key): \n        if Record.__index is None:  \n \n            Record.__index = load() \n        return Record.__index[key]  \ninspect  will be used in load , listed in Example 23-13 .\nThe __index  private class attribute will eventually hold a reference to\nthe dict  returned by load .\nfetch  is a staticmethod  to make it explicit that its ef fect is always\nexactly the same, no matter how it’ s called.\nPopulate the Record.__index  if needed.\nUse it to retrieve the record with the given key .\nT I P\nThis is one example where the use of staticmethod  makes sense. The fetch\nmethod always acts on the Record.__index  class attribute, even if invoked as\nEvent.fetch() . It would be misleading to code it as a class method because the\ncls  first ar gument would not be used.\nNow we get to the use of a property in the Event  class, listed in\nExample 23-12 .\nExample 23-12. schedule_v2.py: the Event class\nclass Event(Record):  \n \n \n    def __repr__ (self): \n        if hasattr(self, 'name'):  \n \n            cls_name  = self.__class__ .__name__  \n            return f'<{cls_name} {self.name!r}> ' \n        else: \n            return super().__repr__ () \n \n    @property  \n    def venue(self): \n        key = f'venue.{self.venue_serial} ' \n        return self.__class__ .fetch(key)  \nEvent  extends Record .\nIf the instance has a name  attribute, it is used to produce a custom\nrepresentation. Otherwise, delegate to the __repr__  from Record .\nThe venue  property builds a key  from the venue_serial  attribute,\nand passes it to the fetch  class method, inherited from Record  (the\nreason for using self.__class__  is explained shortly).\nThe second line of the venue  method of Example 23-12 , returns\nself.__class__.fetch(key) . Why not simply call\nself.fetch(key) ? The simpler form works with the specific OSCON\ndataset because there is no event record with a 'fetch'  key . But, if an\nevent record had a key named 'fetch' , then within that specific Event\ninstance, the reference self.fetch  would retrieve the value of that field,\ninstead of the fetch  class method that Event  inherits from Record .\nThis is a subtle bug, and it could easily sneak through testing because it\ndepends on the dataset.\nW A R N I N G\nWhen creating instance attribute names from data, there is always the risk of bugs due to\nshadowing of class attributes—such as methods—or data loss through accidental\noverwriting of existing instance attributes. These problems may explain why Python\ndicts are not like JavaScript objects in the first place.\nIf the Record  class behaved more like a mapping, implementing a\ndynamic __getitem__  instead of a dynamic __getattr__ , there\nwould be no risk of bugs from overwriting or shadowing. A custom\nmapping is probably the Pythonic way to implement Record . But if I took\nthat road, we’d not be studying the tricks and traps of dynamic attribute\nprogramming.\nThe final piece of this example is the revised load  function in\nExample 23-13 .\nExample 23-13. schedule_v2.py: the load function\ndef load(path=JSON_PATH ): \n    records = {} \n    with open(path) as fp: \n        raw_data  = json.load(fp) \n    for collection , raw_records  in raw_data ['Schedule '].items(): \n        record_type  = collection [:-1]  \n \n        cls_name  = record_type .capitalize ()  \n \n        cls = globals().get(cls_name , Record)  \n \n        if inspect.isclass(cls) and issubclass (cls, Record):  \n \n            factory = cls  \n \n        else: \n            factory = Record  \n \n        for raw_record  in raw_records :  \n \n            key = f'{record_type}.{raw_record[ ""serial""]}' \n            records[key] = factory(**raw_record )  \n \n    return records\nSo far , no changes from the load  in schedule_v1.py  ( Example 23-9 ).\nCapitalize the record_type  to get a possible class name; e.g.,\n'event'  becomes 'Event' .",7240
317-Step 3 Property Overriding an Existing Attribute.pdf,317-Step 3 Property Overriding an Existing Attribute,"Get an object by that name from the module global scope; get the\nRecord  class if there’ s no such object.\nIf the object just retrieved is a class, and is a subclass of Record …\n…bind the factory  name to it. This means factory  may be any\nsubclass of Record , depending on the record_type .\nOtherwise, bind the factory  name to Record .\nThe for  loop that creates the key  and saves the records is the same as\nbefore, except that…\n…the object stored in records  is constructed by factory , which\nmay be Record  or a subclass like Event  selected according to the\nrecord_type .\nNote that the only record_type  that has a custom class is Event , but if\nclasses named Speaker  or Venue  are coded, load  will automatically\nuse those classes when building and saving records, instead of the default\nRecord  class.\nW e’ll now apply the same idea to a new speakers  property in the\nEvents  class.\nStep 3: Property Overriding an Existing Attribute\nThe name of the venue  property in Example 23-12  does not match a field\nname in the Event  records. Its data comes from a venue_serial\nattribute. In contrast, each Event  instance has speaker  attribute with a\nlist of serial numbers, and we want to expose that information as a\nspeaker  property returning a list of Record  instances. This name clash\nrequires some special attention, as Example 23-14  reveals.\nExample 23-14. schedule_v3.py: the speakers  pr operty\n    @property  \n    def speakers (self): \n        spkr_serials  = self.__dict__ ['speakers ']  \n \n        fetch = self.__class__ .fetch \n        return [fetch(f'speaker.{key} ') \n                for key in spkr_serials ]  \nThe data we want is in a speakers  attribute, but we must retrieve it\ndirectly from the instance __dict__  to avoid a recursive call to the\nspeakers  property .\nReturn a list of all records with keys corresponding to the numbers in\nspkr_serials .\nInside the speakers  method, trying to read self.speakers  will\ninvoke the property itself, quickly raising a RecursionError . However\nif we read the same data via the self.__dict__['speakers'] ,\nPython’ s usual algorithm for retrieving attributes is bypassed, the property\nis not called, and the recursion is avoided. For this reason, reading or\nwriting data directly to an object’ s __dict__  is a common Python\nmetaprogramming trick.\nW A R N I N G\nThe interpreter evaluates obj.my_attr  by first looking at the class of obj . If the\nclass has a property with the my_attr  name, that property shadows an instance\nattribute by the same name. Examples in “Properties Override Instance Attributes”  will\ndemonstrate this, and Chapter 24  will reveal that a property is implemented as a\ndescriptor—a more powerful and general abstraction.\nAs I coded the list comprehension in Example 23-14 , my programmer ’ s\nlizard brain thought “This may be expensive.” Not really , because events in\nthe OSCON dataset have few speakers, so coding anything more\ncomplicated would be premature optimization. However , caching a property\nis a common need—and there are caveats. So let’ s see how to do that in the\nnext examples.",3144
318-Step 5 Caching Properties with functools.pdf,318-Step 5 Caching Properties with functools,"Step 4: Bespoke Property Cache\nCaching properties is a common need because there is an expectation that\nan expression like event.venue  should be inexpensive.  Some form of\ncaching could become necessary if the Record.fetch  method behind\nthe Event  properties needed to query a database or a W eb API.\nIn Fluent Python, First Edition , I coded the custom caching logic for the\nspeakers  method as shown in Example 23-15 .\nExample 23-15. Custom caching logic using hasattr disables key-sharing\noptimization.\n    @property  \n    def speakers (self): \n        if not hasattr(self, '__speaker_objs '):  \n \n            spkr_serials  = self.__dict__ ['speakers '] \n            fetch = self.__class__ .fetch \n            self.__speaker_objs  = [fetch(f'speaker.{key} ') \n                    for key in spkr_serials ] \n        return self.__speaker_objs   \nIf the instance doesn’ t have an attribute named __speaker_objs ,\nfetch the speaker objects and store them there.\nReturn self.__speaker_objs .\nThe handmade caching in Example 23-15  is straightforward, but creating an\nattribute after the instance is initialized defeats the PEP 412—Key-Sharing\nDictionary  optimization, as explained in [Link to Come]. Depending on the\nsize of the dataset, the dif ference in memory usage may be important.\nA similar hand-rolled solution that works well with the key-sharing\noptimization requires coding an __init__  for the Event  class, to create\nthe necessary __speaker_objs  initialized to None , and then checking\nfor that in the speakers  method. See Example 23-16 .\nExample 23-16. Storage defined in __init__  to leverage key-sharing\noptimization.8\nclass Event(Record): \n \n    def __init__ (self, **kwargs): \n        self.__speaker_objs  = None \n        super().__init__ (**kwargs) \n \n# 15 lines omitted...  \n    @property  \n    def speakers (self): \n        if self.__speaker_objs  is None: \n            spkr_serials  = self.__dict__ ['speakers' ] \n            fetch = self.__class__ .fetch \n            self.__speaker_objs  = [fetch(f'speaker.{key}' ) \n                    for key in spkr_serials ] \n        return self.__speaker_objs\nExample 23-15  and Example 23-16  illustrate simple caching techniques that\nare fairly common in legacy Python codebases. However , in multi-threaded\nprograms handmade caches like those introduce race conditions that may\nlead to corrupted data. If two threads are reading a property that was not\npreviously cached, the first thread will need to compute the data for the\ncache attribute ( __speaker_objs  in the examples) and the second\nthread may read a cached value that is not yet complete.\nFortunately , Python 3.8 introduced @functools.cached_property\ndecorator which is thread-safe. Unfortunately , it comes with a couple of\ncaveats, explained next.\nStep 5: Caching Properties with functools\nThe functools  module provides three decorators for caching. W e saw\n@cache  and @lru_cache  in “Memoization with functools.cache”\n( Chapter 9 ). Python 3.8 introduced @cached_property .\nThe functools.cached_property  decorator caches the result of the\nmethod in an instance attribute with the same name. For example, in\nExample 23-17 , the value computed by the venue  method is stored in a\nvenue  attribute in self . After that, when client code tries to read venue ,\nthe newly created venue  instance attribute is used instead of the method.\nExample 23-17. Simple use of a @cached_property .\n    @cached_property  \n    def venue(self): \n        key = f'venue.{self.venue_serial}'  \n        return self.__class__ .fetch(key)\nIn “Step 3: Property Overriding an Existing Attribute” , we saw that a\nproperty shadows an instance attribute by the same name. If that is true,\nhow can @cached_property  work? If the property overrides the\ninstance attribute, the venue  attribute will be ignored and the venue\nmethod will always be called, computing the key  and running fetch\nevery time!\nThe answer is a bit sad: cached_property  is a misnomer . The\n@cached_property  decorator does not create a full-fledged property .\nWhile @property  creates an overriding descriptor ,\n@cached_property  creates a non-overriding descriptor . W e will study\nboth kinds of descriptors in Chapter 24 .\nFor now , let us set aside the underlying implementation and focus on the\ndif ferences between cached_property  and property  from a user\npoint of view . Raymond Hettinger explains them very well in the Python\nDocs :\nThe mechanics of cached_property()  ar e somewhat differ ent fr om\nproperty() . A r egular pr operty blocks attribute writes unless a setter\nis defined. In contrast, a cached_property  allows writes.\nThe cached_property  decorator only runs on lookups and only\nwhen an attribute of the same name doesn’ t exist. When it does run, the\ncached_property  writes to the attribute with the same name.\nSubsequent attribute r eads and writes take pr ecedence over the\ncached_property  method and it works like a normal attribute.\nThe cached value can be clear ed by deleting the attribute. This allows the\ncached_property  method to run again.9\nBack to our Event  class: the specific behavior of @cached_property\nmakes it unsuitable to decorate speakers , because that method relies on\nan existing attribute also named speakers , containing the serial numbers\nof the event speakers.\nW A R N I N G\n@cached_property  has some important limitations:\nIt cannot be used as a drop-in replacement to @property  if the decorated\nmethod already depends on an instance attribute with the same name;\nIt cannot be used in a class that defines __slots__ ;\nIt defeats the key-sharing optimization of the instance __dict__ , because it\ncreates an instance attribute after __init__ .\nDespite these limitations, @cached_property  addresses a common\nneed in a simple way , and it is thread-safe. Its Python code  is an example of\nusing a reentrant lock .\nThe @cached_property  documentation  recommends an alternative\nsolution that we can use with speakers : stacking @property  and\n@cache  decorators, as shown in Example 23-18\nExample 23-18. Stacking @property  on @cache .\n    @property   \n \n    @cache  \n \n    def speakers (self): \n        spkr_serials  = self.__dict__ ['speakers '] \n        fetch = self.__class__ .fetch \n        return [fetch(f'speaker.{key} ') \n                for key in spkr_serials ]\nThe order here is important, @property  goes on top…\n…of @cache .",6496
319-Using a Property for Attribute Validation.pdf,319-Using a Property for Attribute Validation,,0
320-LineItem Take 2 A Validating Property.pdf,320-LineItem Take 2 A Validating Property,"Recall from “Stacked decorators”  the meaning of that syntax. The top three\nlines of Example 23-18  are similar to:\nspeakers  = property (cache(speakers ))\nThe @cache  is applied to speakers , returning a new function. That\nfunction then is decorated by @property , which replaces it with a newly\nconstructed property .\nThis wraps up our discussion of read-only properties and caching\ndecorators. In the next section, we will create a read/write property .\nU s i n g  a  P r o p e r t y  f o r  A t t r i b u t e  V a l i d a t i o n\nBesides computing attribute values, properties are also used to enforce\nbusiness rules by changing a public attribute into an attribute protected by a\ngetter and setter without af fecting client code. Let’ s work through an\nextended example.\nLineItem T ake #1: Class for an Item in an Order\nImagine an app for a store that sells or ganic food in bulk, where customers\ncan order nuts, dried fruit, or cereals by weight. In that system, each order\nwould hold a sequence of line items, and each line item could be\nrepresented by a class as in Example 23-19 .\nExample 23-19. bulkfood_v1.py: the simplest LineItem class\nclass LineItem : \n \n    def __init__ (self, description , weight, price): \n        self.description  = description  \n        self.weight = weight \n        self.price = price \n \n    def subtotal (self): \n        return self.weight * self.price\nThat’ s nice and simple. Perhaps too simple. Example 23-20  shows a\nproblem.\nExample 23-20. A negative weight r esults in a negative subtotal\n    >>> raisins = LineItem ('Golden raisins' , 10, 6.95) \n    >>> raisins.subtotal () \n    69.5 \n    >>> raisins.weight = -20  # garbage in...  \n    >>> raisins.subtotal ()    # garbage out...  \n    -139.0\nThis is a toy example, but not as fanciful as you may think. Here is a true\nstory from the early days of Amazon.com:\nW e found that customers could or der a negative quantity of books! And\nwe would cr edit their cr edit car d with the price and, I assume, wait\nar ound for them to ship the books.\n— Jef f Bezos, Founder and CEO of Amazon.com\nHow do we fix this? W e could change the interface of LineItem  to use a\ngetter and a setter for the weight  attribute. That would be the Java way ,\nand it’ s not wrong.\nOn the other hand, it’ s natural to be able set the weight  of an item by just\nassigning to it; and perhaps the system is in production with other parts\nalready accessing item.weight  directly . In this case, the Python way\nwould be to replace the data attribute with a property .\nLineItem T ake #2: A V alidating Property\nImplementing a property will allow us to use a getter and a setter , but the\ninterface of LineItem  will not change (i.e., setting the weight  of a\nLineItem  will still be written as raisins.weight = 12 ).\nExample 23-21  lists the code for a read/write weight  property .\nExample 23-21. bulkfood_v2.py: a LineItem with a weight pr operty\nclass LineItem : \n \n    def __init__ (self, description , weight, price): 10\n        self.description  = description  \n        self.weight = weight  \n \n        self.price = price \n \n    def subtotal (self): \n        return self.weight * self.price \n \n    @property   \n \n    def weight(self):  \n \n        return self.__weight   \n \n \n    @weight.setter   \n \n    def weight(self, value): \n        if value > 0: \n            self.__weight  = value  \n \n        else: \n            raise ValueError ('value must be > 0 ')  \nHere the property setter is already in use, making sure that no instances\nwith negative weight  can be created.\n@property  decorates the getter method.\nThe methods that implement a property all have the name of the public\nattribute: weight .\nThe actual value is stored in a private attribute __weight .\nThe decorated getter has a .setter  attribute, which is also a\ndecorator; this ties the getter and setter together .\nIf the value is greater than zero, we set the private __weight .\nOtherwise, ValueError  is raised.\nNote how a LineItem  with an invalid weight cannot be created now:\n>>> walnuts = LineItem ('walnuts' , 0, 10.00) \nTraceback (most recent call last):  \n    ... \nValueError : value must be > 0",4235
321-A Proper Look at Properties.pdf,321-A Proper Look at Properties,"Now we have protected weight  from users providing negative values.\nAlthough buyers usually can’ t set the price of an item, a clerical error or a\nbug may create a LineItem  with a negative price . T o prevent that, we\ncould also turn price  into a property , but this would entail some repetition\nin our code.\nRemember the Paul Graham quote from Chapter 17 : “When I see patterns\nin my programs, I consider it a sign of trouble.” The cure for repetition is\nabstraction. There are two ways to abstract away property definitions: using\na property factory or a descriptor class. The descriptor class approach is\nmore flexible, and we’ll devote Chapter 24  to a full discussion of it.\nProperties are in fact implemented as descriptor classes themselves. But\nhere we will continue our exploration of properties by implementing a\nproperty factory as a function.\nBut before we can implement a property factory , we need to have a deeper\nunderstanding of properties.\nA  P r o p e r  L o o k  a t  P r o p e r t i e s\nAlthough often used as a decorator , the property  built-in is actually a\nclass. In Python, functions and classes are often interchangeable, because\nboth are callable and there is no new  operator for object instantiation, so\ninvoking a constructor is no dif ferent than invoking a factory function. And\nboth can be used as decorators, as long as they return a new callable that is\na suitable replacement of the decorated function.\nThis is the full signature of the property  constructor:\nproperty (fget=None, fset=None, fdel=None, doc=None)\nAll ar guments are optional, and if a function is not provided for one of\nthem, the corresponding operation is not allowed by the resulting property\nobject.\nThe property  type was added in Python 2.2, but the @  decorator syntax\nappeared only in Python 2.4, so for a few years, properties were defined by\npassing the accessor functions as the first two ar guments.\nThe “classic” syntax for defining properties without decorators is illustrated\nin Example 23-22 .\nExample 23-22. bulkfood_v2b.py: same as Example 23-21  but without using\ndecorators\nclass LineItem : \n \n    def __init__ (self, description , weight, price): \n        self.description  = description  \n        self.weight = weight \n        self.price = price \n \n    def subtotal (self): \n        return self.weight * self.price \n \n    def get_weight (self):  \n \n        return self.__weight  \n \n    def set_weight (self, value):  \n \n        if value > 0: \n            self.__weight  = value \n        else: \n            raise ValueError ('value must be > 0 ') \n \n    weight = property (get_weight , set_weight )  \nA plain getter .\nA plain setter .\nBuild the property  and assign it to a public class attribute.\nThe classic form is better than the decorator syntax in some situations; the\ncode of the property factory we’ll discuss shortly is one example. On the\nother hand, in a class body with many methods, the decorators make it\nexplicit which are the getters and setters, without depending on the\nconvention of using get  and set  prefixes in their names.",3135
322-Properties Override Instance Attributes.pdf,322-Properties Override Instance Attributes,"The presence of a property in a class af fects how attributes in instances of\nthat class can be found in a way that may be surprising at first. The next\nsection explains.\nProperties Override Instance Attributes\nProperties are always class attributes, but they actually manage attribute\naccess in the instances of the class.\nIn “Overriding Class Attributes”  we saw that when an instance and its class\nboth have a data attribute by the same name, the instance attribute\noverrides, or shadows, the class attribute—at least when read through that\ninstance. Example 23-23  illustrates this point.\nExample 23-23. Instance attribute shadows class data attribute\n>>> class Class:  \n \n...     data = 'the class data attr ' \n...     @property  \n...     def prop(self): \n...         return 'the prop value ' \n... \n>>> obj = Class() \n>>> vars(obj)  \n \n{} \n>>> obj.data  \n \n'the class data attr'  \n>>> obj.data = 'bar' \n \n>>> vars(obj)  \n \n{'data': 'bar'}  \n>>> obj.data  \n \n'bar' \n>>> Class.data  \n \n'the class data attr'\nDefine Class  with two class attributes: the data  data attribute and the\nprop  property .\nvars  returns the __dict__  of obj , showing it has no instance\nattributes.\nReading from obj.data  retrieves the value of Class.data .\nW riting to obj.data  creates an instance attribute.\nInspect the instance to see the instance attribute.\nNow reading from obj.data  retrieves the value of the instance\nattribute. When read from the obj  instance, the instance data  shadows\nthe class data .\nThe Class.data  attribute is intact.\nNow , let’ s try to override the prop  attribute on the obj  instance.\nResuming the previous console session, we have Example 23-24 .\nExample 23-24. Instance attribute does not shadow class pr operty\n(continued fr om Example 23-23 )\n>>> Class.prop  \n \n<property object at 0x1072b7408>  \n>>> obj.prop  \n \n'the prop value'  \n>>> obj.prop = 'foo'  \n \nTraceback (most recent call last):  \n  ... \nAttributeError : can't set attribute  \n>>> obj.__dict__ ['prop'] = 'foo'  \n \n>>> vars(obj)  \n \n{'data': 'bar', 'prop': 'foo'}  \n>>> obj.prop  \n \n'the prop value'  \n>>> Class.prop = 'baz'  \n \n>>> obj.prop  \n \n'foo'\nReading prop  directly from Class  retrieves the property object itself,\nwithout running its getter method.\nReading obj.prop  executes the property getter .\nT rying to set an instance prop  attribute fails.\nPutting 'prop'  directly in the obj.__dict__  works.\nW e can see that obj  now has two instance attributes: data  and prop .\nHowever , reading obj.prop  still runs the property getter . The\nproperty is not shadowed by an instance attribute.\nOverwriting Class.prop  destroys the property object.\nNow obj.prop  retrieves the instance attribute. Class.prop  is not a\nproperty anymore, so it no longer overrides obj.prop .\nAs a final demonstration, we’ll add a new property to Class , and see it\noverriding an instance attribute. Example 23-25  picks up where\nExample 23-24  left of f.\nExample 23-25. New class pr operty shadows existing instance attribute\n(continued fr om Example 23-24 )\n>>> obj.data  \n \n'bar' \n>>> Class.data  \n \n'the class data attr'  \n>>> Class.data = property (lambda self: 'the ""data"" prop value ')  \n \n>>> obj.data  \n \n'the ""data"" prop value'  \n>>> del Class.data  \n \n>>> obj.data  \n \n'bar'\nobj.data  retrieves the instance data  attribute.\nClass.data  retrieves the class data  attribute.\nOverwrite Class.data  with a new property .\nobj.data  is now shadowed by the Class.data  property .\nDelete the property .",3587
323-Property Documentation.pdf,323-Property Documentation,"obj.data  now reads the instance data  attribute again.\nThe main point of this section is that an expression like obj.data  does\nnot start the search for data  in obj . The search actually starts at\nobj.__class__ , and only if there is no property named data  in the\nclass, Python looks in the obj  instance itself. This applies to overriding\ndescriptors  in general, of which properties are just one example. Further\ntreatment of descriptors must wait for Chapter 24 .\nNow back to properties. Every Python code unit—modules, functions,\nclasses, methods—can have a docstring. The next topic is how to attach\ndocumentation to properties.\nProperty Documentation\nWhen tools such as the console help()  function or IDEs need to display\nthe documentation of a property , they extract the information from the\n__doc__  attribute of the property .\nIf used with the classic call syntax, property  can get the documentation\nstring as the doc  ar gument:\n    weight = property (get_weight , set_weight , doc='weight in  \nkilograms' )\nWhen property  is deployed as a decorator , the docstring of the getter\nmethod—the one with the @property  decorator itself—is used as the\ndocumentation of the property as a whole. Figure 23-1  shows the help\nscreens generated from the code in Example 23-26 .\nFigur e 23-1. Scr eenshots of the Python console when issuing the commands help(Foo.bar) and\nhelp(Foo). Sour ce code in Example 23-26 .\nExample 23-26. Documentation for a pr operty\nclass Foo: \n \n    @property  \n    def bar(self): \n        '''The bar attribute'''  \n        return self.__dict__ ['bar']",1615
324-Coding a Property Factory.pdf,324-Coding a Property Factory,"@bar.setter  \n    def bar(self, value): \n        self.__dict__ ['bar'] = value\nNow that we have these property essentials covered, let’ s go back to the\nissue of protecting both the weight  and price  attributes of LineItem\nso they only accept values greater than zero—but without implementing\ntwo nearly identical pairs of getters/setters by hand.\nC o d i n g  a  P r o p e r t y  F a c t o r y\nW e’ll create a factory to create quantity  properties—so named because\nthe managed attributes represent quantities that can’ t be negative or zero in\nthe application. Example 23-27  shows the clean look of the LineItem\nclass using two instances of quantity  properties: one for managing the\nweight  attribute, the other for price .\nExample 23-27. bulkfood_v2pr op.py: the quantity pr operty factory in use\nclass LineItem : \n    weight = quantity ('weight')  \n \n    price = quantity ('price')  \n \n \n    def __init__ (self, description , weight, price): \n        self.description  = description  \n        self.weight = weight  \n \n        self.price = price \n \n    def subtotal (self): \n        return self.weight * self.price  \nUse the factory to define the first custom property , weight , as a class\nattribute.\nThis second call builds another custom property , price .\nHere the property is already active, making sure a negative or 0\nweight  is rejected.\n\nThe properties are also in use here, retrieving the values stored in the\ninstance.\nRecall that properties are class attributes. When building each quantity\nproperty , we need to pass the name of the LineItem  attribute that will be\nmanaged by that specific property . Having to type the word weight  twice\nin this line is unfortunate:\n    weight = quantity ('weight' )\nBut avoiding that repetition is complicated because the property has no way\nof knowing which class attribute name will be bound to it. Remember: the\nright-hand side of an assignment is evaluated first, so when quantity()\nis invoked, the weight  class attribute doesn’ t even exist.\nN O T E\nImproving the quantity  property so that the user doesn’ t need to retype the attribute\nname is a nontrivial metaprogramming problem. W e’ll see a workaround in Chapter 24 ,\nbut real solutions will have to wait until Chapter 25 , because they require either a class\ndecorator or a metaclass.\nExample 23-28  lists the implementation of the quantity  property\nfactory .\nExample 23-28. bulkfood_v2pr op.py: the quantity pr operty factory\ndef quantity (storage_name ):  \n \n \n    def qty_getter (instance ):  \n \n        return instance .__dict__ [storage_name ]  \n \n \n    def qty_setter (instance , value):  \n \n        if value > 0: \n            instance .__dict__ [storage_name ] = value  \n \n        else: \n            raise ValueError ('value must be > 0 ') \n \n    return property (qty_getter , qty_setter )  \n11\nThe storage_name  ar gument determines where the data for each\nproperty is stored; for the weight , the storage name will be\n'weight' .\nThe first ar gument of the qty_getter  could be named self , but that\nwould be strange because this is not a class body; instance  refers to\nthe LineItem  instance where the attribute will be stored.\nqty_getter  references storage_name , so it will be preserved in\nthe closure of this function; the value is retrieved directly from the\ninstance.__dict__  to bypass the property and avoid an infinite\nrecursion.\nqty_setter  is defined, also taking instance  as first ar gument.\nThe value  is stored directly in the instance.__dict__ , again\nbypassing the property .\nBuild a custom property object and return it.\nThe bits of Example 23-28  that deserve careful study revolve around the\nstorage_name  variable. When you code each property in the traditional\nway , the name of the attribute where you will store a value is hardcoded in\nthe getter and setter methods. But here, the qty_getter  and\nqty_setter  functions are generic, and they depend on the\nstorage_name  variable to know where to get/set the managed attribute\nin the instance __dict__ . Each time the quantity  factory is called to\nbuild a property , the storage_name  must be set to a unique value.\nThe functions qty_getter  and qty_setter  will be wrapped by the\nproperty  object created in the last line of the factory function. Later\nwhen called to perform their duties, these functions will read the\nstorage_name  from their closures, to determine where to retrieve/store\nthe managed attribute values.\nIn Example 23-29 , I create and inspect a LineItem  instance, exposing the\nstorage attributes.\nExample 23-29. bulkfood_v2pr op.py: the quantity pr operty factory\n    >>> nutmeg = LineItem ('Moluccan nutmeg ', 8, 13.95) \n    >>> nutmeg.weight, nutmeg.price  \n \n    (8, 13.95) \n    >>> sorted(vars(nutmeg).items())  \n \n    [('description ', 'Moluccan nutmeg '), ('price', 13.95), \n('weight', 8)]\nReading the weight  and price  through the properties shadowing the\nnamesake instance attributes.\nUsing vars  to inspect the nutmeg  instance: here we see the actual\ninstance attributes used to store the values.\nNote how the properties built by our factory leverage the behavior\ndescribed in “Properties Override Instance Attributes” : the weight\nproperty overrides the weight  instance attribute so that every reference to\nself.weight  or nutmeg.weight  is handled by the property\nfunctions, and the only way to bypass the property logic is to access the\ninstance __dict__  directly .\nThe code in Example 23-29  may be a bit tricky , but it’ s concise: it’ s\nidentical in length to the decorated getter/setter pair defining just the\nweight  property in Example 23-21 . The LineItem  definition in\nExample 23-27  looks much better without the noise of the getter/setters.\nIn a real system, that same kind of validation may appear in many fields,\nacross several classes, and the quantity  factory would be placed in a\nutility module to be used over and over again. Eventually that simple\nfactory could be refactored into a more extensible descriptor class, with\nspecialized subclasses performing dif ferent validations. W e’ll do that in\nChapter 24 .\nNow let us wrap up the discussion of properties with the issue of attribute\ndeletion.",6285
325-Special Attributes that Affect Attribute Handling.pdf,325-Special Attributes that Affect Attribute Handling,"H a n d l i n g  A t t r i b u t e  D e l e t i o n\nRecall from the Python tutorial that object attributes can be deleted using\nthe del  statement:\ndel my_object .an_attribute\nIn practice, deleting attributes is not something we do every day in Python,\nand the requirement to handle it with a property is even more unusual. But\nit is supported, and I can think of a silly example to demonstrate it.\nIn a property definition, the @my_property.deleter  decorator wraps\nthe method in char ge of deleting the attribute managed by the property . As\npromised, Example 23-30  is a silly example showing how to code a\nproperty deleter .\nExample 23-30. blackknight.py: inspir ed by the Black Knight character of\n“Monty Python and the Holy Grail”\nclass BlackKnight : \n \n    def __init__ (self): \n        self.phrases = [ \n            ('an arm' , ""'Tis but a scratch."" ), \n            ('another arm' , ""It's just a flesh wound."" ), \n            ('a leg', ""I'm invincible!"" ), \n            ('another leg' , ""All right, we'll call it a draw."" ) \n        ] \n \n    @property  \n    def member(self): \n        print('next member is:' ) \n        return self.phrases[0][0] \n \n    @member.deleter  \n    def member(self): \n        member, text = self.phrases.pop(0) \n        print(f'BLACK KNIGHT (loses {member}) -- {text}' )\nThe doctests in blackknight.py  are in Example 23-31 .\nExample 23-31. blackknight.py: doctests for Example 23-30  (the Black\nKnight never concedes defeat)\n    >>> knight = BlackKnight () \n    >>> knight.member \n    next member is: \n    'an arm'  \n    >>> del knight.member \n    BLACK KNIGHT (loses an arm) -- 'Tis but a scratch.  \n    >>> del knight.member \n    BLACK KNIGHT (loses another arm) -- It's just a flesh wound.  \n    >>> del knight.member \n    BLACK KNIGHT (loses a leg) -- I'm invincible!  \n    >>> del knight.member \n    BLACK KNIGHT (loses another leg) -- All right, we'll call it a  \ndraw.\nUsing the classic call syntax instead of decorators, the fdel  ar gument\nconfigures the deleter function. For example, the member  property would\nbe coded like this in the body of the BlackKnight  class:\n    member = property (member_getter , fdel=member_deleter )\nIf you are not using a property , attribute deletion can also be handled by\nimplementing the lower -level __delattr__  special method, presented in\n“Special Methods for Attribute Handling” . Coding a silly class with\n__delattr__  is left as an exercise to the procrastinating reader .\nProperties are a powerful feature, but sometimes simpler or lower -level\nalternatives are preferable. In the final section of this chapter , we’ll review\nsome of the core APIs that Python of fers for dynamic attribute\nprogramming.\nE s s e n t i a l  A t t r i b u t e s  a n d  F u n c t i o n s  f o r\nA t t r i b u t e  H a n d l i n g\nThroughout this chapter , and even before in the book, we’ve used some of\nthe built-in functions and special methods Python provides for dealing with\ndynamic attributes. This section gives an overview of them in one place,\nbecause their documentation is scattered in the of ficial docs.",3150
326-Special Methods for Attribute Handling.pdf,326-Special Methods for Attribute Handling,"Special Attributes that Affect Attribute Handling\nThe behavior of many of the functions and special methods listed in the\nfollowing sections depend on three special attributes:\n__class__\nA reference to the object’ s class (i.e., obj.__class__  is the same as\ntype(obj) ). Python looks for special methods such as\n__getattr__  only in an object’ s class, and not in the instances\nthemselves.\n__dict__\nA mapping that stores the writable attributes of an object or class. An\nobject that has a __dict__  can have arbitrary new attributes set at any\ntime. If a class has a __slots__  attribute, then its instances may not\nhave a __dict__ . See __slots__  (next).\n__slots__\nAn attribute that may be defined in a class to limit the attributes its\ninstances can have. __slots__  is a tuple  of strings naming the\nallowed attributes.  If the '__dict__'  name is not in __slots__ ,\nthen the instances of that class will not have a __dict__  of their own,\nand only the named attributes will be allowed in them.\nBuilt-In Functions for Attribute Handling\nThese five built-in functions perform object attribute reading, writing, and\nintrospection:\ndir([object])\nLists most attributes of the object. The of ficial docs  say dir  is intended\nfor interactive use so it does not provide a comprehensive list of\nattributes, but an “interesting” set of names. dir  can inspect objects\nimplemented with or without a __dict__ . The __dict__  attribute12\nitself is not listed by dir , but the __dict__  keys are listed. Several\nspecial attributes of classes, such as __mro__ , __bases__ , and\n__name__  are not listed by dir  either . If the optional object\nar gument is not given, dir  lists the names in the current scope.\ngetattr(object, name[, default])\nGets the attribute identified by the name  string from the object . This\nmay fetch an attribute from the object’ s class or from a superclass. If no\nsuch attribute exists, getattr  raises AttributeError  or returns\nthe default  value, if given.\nhasattr(object, name)\nReturns True  if the named attribute exists in the object , or can be\nsomehow fetched through it (by inheritance, for example). The\ndocumentation  explains: “This is implemented by calling getattr(object,\nname) and seeing whether it raises an AttributeError or not.”\nsetattr(object, name, value)\nAssigns the value  to the named attribute of object , if the object\nallows it. This may create a new attribute or overwrite an existing one.\nvars([object])\nReturns the __dict__  of object ; vars  can’ t deal with instances of\nclasses that define __slots__  and don’ t have a __dict__  (contrast\nwith dir , which handles such instances). W ithout an ar gument,\nvars()  does the same as locals() : returns a dict  representing the\nlocal scope.\nSpecial Methods for Attribute Handling\nWhen implemented in a user -defined class, the special methods listed here\nhandle attribute retrieval, setting, deletion, and listing.\nAttribute access using either dot notation or the built-in functions\ngetattr , hasattr , and setattr  trigger the appropriate special\nmethods listed here. Reading and writing attributes directly in the instance\n__dict__  does not trigger these special methods—and that’ s the usual\nway to bypass them if needed.\n“Section 3.3.9. Special method lookup”  of the “Data model” chapter warns:\nFor custom classes, implicit invocations of special methods ar e only\nguaranteed to work corr ectly if defined on an object’ s type, not in the\nobject’ s instance dictionary .\nIn other words, assume that the special methods will be retrieved on the\nclass itself, even when the tar get of the action is an instance. For this\nreason, special methods are not shadowed by instance attributes with the\nsame name.\nIn the following examples, assume there is a class named Class , obj  is\nan instance of Class , and attr  is an attribute of obj .\nFor every one of these special methods, it doesn’ t matter if the attribute\naccess is done using dot notation or one of the built-in functions listed in\n“Built-In Functions for Attribute Handling” . For example, both obj.attr\nand getattr(obj, 'attr', 42)  trigger\nClass.__getattribute__(obj, 'attr') .\n__delattr__(self, name)\nAlways called when there is an attempt to delete an attribute using the\ndel  statement; e.g., del obj.attr  triggers\nClass.__delattr__(obj, 'attr') .\n__dir__(self)\nCalled when dir  is invoked on the object, to provide a listing of\nattributes; e.g., dir(obj)  triggers Class.__dir__(obj) .\n__getattr__(self, name)\nCalled only when an attempt to retrieve the named attribute fails, after\nthe obj , Class , and its superclasses are searched. The expressions\nobj.no_such_attr , getattr(obj, 'no_such_attr') , and\nhasattr(obj, 'no_such_attr')  may trigger\nClass.__getattr__(obj, 'no_such_attr') , but only if an\nattribute by that name cannot be found in obj  or in Class  and its\nsuperclasses.\n__getattribute__(self, name)\nAlways called when there is an attempt to retrieve the named attribute,\nexcept when the attribute sought is a special attribute or method. Dot\nnotation and the getattr  and hasattr  built-ins trigger this method.\n__getattr__  is only invoked after __getattribute__ , and only\nwhen __getattribute__  raises AttributeError . T o retrieve\nattributes of the instance obj  without triggering an infinite recursion,\nimplementations of __getattribute__  should use\nsuper().__getattribute__(obj, name) .\n__setattr__(self, name, value)\nAlways called when there is an attempt to set the named attribute. Dot\nnotation and the setattr  built-in trigger this method; e.g., both\nobj.attr = 42  and setattr(obj, 'attr', 42)  trigger\nClass.__setattr__(obj, 'attr', 42) .\nT I P\nIn practice, because they are unconditionally called and af fect practically every attribute\naccess, the __getattribute__  and __setattr__  special methods are harder to\nuse correctly than __getattr__ —which only handles nonexisting attribute names.\nUsing properties or descriptors is less error prone than defining these special methods.\nThis concludes our dive into properties, special methods, and other\ntechniques for coding dynamic attributes.",6201
327-Further Reading.pdf,327-Further Reading,"C h a p t e r  S u m m a r y\nW e started our coverage of dynamic attributes by showing practical\nexamples of simple classes to make it easier to deal with a JSON dataset.\nThe first example was the FrozenJSON  class that converted nested dicts\nand lists into nested FrozenJSON  instances and lists of them. The\nFrozenJSON  code demonstrated the use of the __getattr__  special\nmethod to convert data structures on the fly , whenever their attributes were\nread. The last version of FrozenJSON  showcased the use of the\n__new__  constructor method to transform a class into a flexible factory of\nobjects, not limited to instances of itself.\nW e then converted the JSON dataset to a dict  storing instances of a\nRecord  class. The first rendition of Record  was a few lines long and\nintroduced the “bunch” idiom: using\nself.__dict__.update(**kwargs)  to build arbitrary attributes\nfrom keyword ar guments passed to __init__ . The second iteration added\nthe Event  class implementing automatic retrieval of linked records\nthrough properties. Computed property values sometimes require caching,\nand we covered a few ways of doing that. After realizing that\n@functools.cached_property  does not implement the basic\nbehavior expected of methods decorated with the @property  built-in, we\nfinally settled on the use of @cached_property  in one method, and\n@functools.cache  decorated with @property  in the other method.\nCoverage of properties continued with the LineItem  class, where a\nproperty was deployed to protect a weight  attribute from negative or zero\nvalues that make no business sense. After a deeper look at property syntax\nand semantics, we created a property factory to enforce the same validation\non weight  and price , without coding multiple getters and setters. The\nproperty factory leveraged subtle concepts—such as closures and the\ninstance attribute overriding by properties—to provide an elegant generic\nsolution using the same number of lines as a single hand-coded property\ndefinition.\nFinally , we had a brief look at handling attribute deletion with properties,\nfollowed by an overview of the key special attributes, built-in functions,\nand special methods that support attribute metaprogramming in the core\nPython language.\nF u r t h e r  R e a d i n g\nThe of ficial documentation for the attribute handling and introspection\nbuilt-in functions is Chapter 2, “Built-in Functions”  of The Python Standar d\nLibrary . The related special methods and the __slots__  special attribute\nare documented in The Python Language Reference in “3.3.2. Customizing\nattribute access” . The semantics of how special methods are invoked\nbypassing instances is explained in “3.3.9. Special method lookup” . In\nChapter 4, “Built-in T ypes,” of the Python Standard Library , “4.13. Special\nAttributes”  covers __class__  and __dict__  attributes.\nPython Cookbook, 3E  by David Beazley and Brian K. Jones (O’Reilly) has\nseveral recipes covering the topics of this chapter , but I will highlight three\nthat are outstanding: “Recipe 8.8. Extending a Property in a Subclass”\naddresses the thorny issue of overriding the methods inside a property\ninherited from a superclass; “Recipe 8.15. Delegating Attribute Access”\nimplements a proxy class showcasing most special methods from “Special\nMethods for Attribute Handling”  in this book; and the awesome “Recipe\n9.21. A voiding Repetitive Property Methods,” which was the basis for the\nproperty factory function presented in Example 23-28 .\nPython in a Nutshell, 3E  (O’Reilly), by Alex Martelli, Anna Ravenscroft,\nand Steve Holden is rigorous and objective. They devote only three pages to\nproperties, but that’ s because the book follows an axiomatic presentation\nstyle: the preceding 15 pages or so provide a thorough description of the\nsemantics of Python classes from the ground up, including descriptors,\nwhich are how properties are actually implemented under the hood. So by\nthe time Martelli et.al. get to properties, they can pack a lot of insights in\nthose three pages—including that which I selected to open this chapter .\nBertrand Meyer—quoted in the Uniform Access Principle  definition in this\nchapter opening—pioneered the Design by Contract methodology , designed\nthe Eif fel language, and wrote the excellent Object-Oriented Softwar e\nConstruction, 2E  (Prentice-Hall). The book is more than 1,250 pages long,\nand I confess I did not read it all, but the first six chapters provide one of\nthe best conceptual introductions to OO analysis and design I’ve seen.\nChapter 1 1 presents Design by Contract, and Chapter 35 of fers his\nassessments of some influential OO languages: Simula, Smalltalk, CLOS\n(the Common Lisp Object System), Objective-C, C++, and Java, with brief\ncomments on some others. Only in the last page of the book he reveals that\nthe highly readable “notation” he uses as pseudocode is Eif fel.\nS O A P B O X\nMeyer ’ s Uniform Access Principle  is aesthetically appealing. As a\nprogrammer using an API, I shouldn’ t have to care whether\nproduct.price  simply fetches a data attribute or performs a\ncomputation. As a consumer and a citizen, I do care: in e-commerce\ntoday the value of product.price  often depends on who is asking,\nso it’ s certainly not a mere data attribute. In fact, it’ s common practice\nthat the price is lower if the query comes from outside the store—say ,\nfrom a price-comparison engine. This ef fectively punishes loyal\ncustomers who like to browse within a particular store. But I digress.\nThe previous digression does raise a relevant point for programming:\nalthough the Uniform Access Principle makes perfect sense in an ideal\nworld, in reality users of an API may need to know whether reading\nproduct.price  is potentially too expensive or time-consuming.\nThat’ s a problem with programming abstractions in general: they make\nit hard to reason about the runtime cost of evaluating an expression. On\nthe other hand, abstractions let users accomplish more with less code.\nIt’ s a trade-of f. As usual in matters of software engineering, W ard\nCunningham’ s original W iki  hosts insightful ar guments about the merits\nof the Uniform Access Principle .\nIn object-oriented programming languages, application or violations of\nthe Uniform Access Principle usually revolve around the syntax of\nreading public data attributes versus invoking getter/setter methods.\nSmalltalk and Ruby address this issue in a simple and elegant way: they\ndon’ t support public data attributes at all. Every instance attribute in\nthese languages is private, so every access to them must be through\nmethods. But their syntax makes this painless: in Ruby ,\nproduct.price  invokes the price  getter; in Smalltalk, it’ s simply\nproduct price .\nAt the other end of the spectrum, the Java language allows the\nprogrammer to choose among four access level modifiers.13\nThe general practice does not agree with the syntax established by the\nJava designers, though. Everybody in Java-land agrees that attributes\nshould be private , and you must spell it out every time, because it’ s\nnot the default. When all attributes are private, all access to them from\noutside the class must go through accessors. Java IDEs include\nshortcuts for generating accessor methods automatically . Unfortunately ,\nthe IDE is not so helpful when you must read the code six months later .\nIt’ s up to you to wade through a sea of do-nothing accessors to find\nthose that add value by implementing some business logic.\nAlex Martelli speaks for the majority of the Python community when he\ncalls accessors “goofy idioms” and then provides these examples that\nlook very dif ferent but do the same thing:\nsomeInstance .widgetCounter  += 1 \n# rather than...  \nsomeInstance .setWidgetCounter (someInstance .getWidgetCounter () \n+ 1)\nSometimes when designing APIs, I’ve wondered whether every method\nthat does not take an ar gument (besides self ), returns a value (other\nthan None ), and is a pure function (i.e., has no side ef fects) should be\nreplaced by a read-only property . In this chapter , the\nLineItem.subtotal  method (as in Example 23-27 ) would be a\ngood candidate to become a read-only property . Of course, this excludes\nmethods that are designed to change the object, such as\nmy_list.clear() . It would be a terrible idea to turn that into a\nproperty , so that merely accessing my_list.clear  would delete the\ncontents of the list!\nIn the Pingo.io  GPIO library (mentioned in “The __missing__\nMethod” ), much of the user -level API is based on properties. For\nexample, to read the current value of an analog pin, the user writes\npin.value , and setting a digital pin mode is written as pin.mode\n= OUT . Behind the scenes, reading an analog pin value or setting a\ndigital pin mode may involve a lot of code, depending on the specific14\nboard driver . W e decided to use properties in Pingo because we want\nthe API to be comfortable to use even in interactive environments like\niPython Notebook , and we feel pin.mode = OUT  is easier on the\neyes and on the fingers than pin.set_mode(OUT) .\nAlthough I find the Smalltalk and Ruby solution cleaner , I think the\nPython approach makes more sense than the Java one. W e are allowed\nto start simple, coding data members as public attributes, because we\nknow they can always be wrapped by properties (or descriptors, which\nwe’ll talk about in the next chapter).\n__new__  Is Better Than new\nAnother example of the Uniform Access Principle (or a variation of it)\nis the fact that function calls and object instantiation use the same\nsyntax in Python: my_obj = foo() , where foo  may be a class or\nany other callable.\nOther languages influenced by C++ syntax have a new  operator that\nmakes instantiation look dif ferent than a call. Most of the time, the user\nof an API doesn’ t care whether foo  is a function or a class. Until\nrecently , I was under the impression that property  was a function. In\nnormal usage, it makes no dif ference.\nThere are many good reasons for replacing constructors with\nfactories.  A popular motive is limiting the number of instances, by\nreturning previously built ones (as in the Singleton pattern). A related\nuse is caching expensive object construction. Also, sometimes it’ s\nconvenient to return objects of dif ferent types depending on the\nar guments given.\nCoding a constructor is simpler; providing a factory adds flexibility at\nthe expense of more code. In languages that have a new  operator , the\ndesigner of an API must decide in advance whether to stick with a\nsimple constructor or invest in factory . If the initial choice is wrong, the\ncorrection may be costly—all because new  is an operator .15\nSometimes it may also be convenient to go the other way , and replace a\nsimple function with a class.\nIn Python, classes and functions are interchangeable in many situations.\nNot only because there’ s no new  operator , but also because there is the\n__new__  special method, which can turn a class into a factory\nproducing objects of dif ferent kinds (as we saw in “Flexible Object\nCreation with __new__” ) or returning prebuilt instances instead of\ncreating a new one every time.\nThis function-class duality would be easier to leverage if PEP 8 — Style\nGuide for Python Code  did not recommend CamelCase  for class\nnames. On the other hand, dozens of classes in the standard library have\nlowercase names (e.g., property , str , defaultdict , etc.). So\nmaybe the use of lowercase class names is a feature, and not a bug. But\nhowever we look at it, the inconsistent capitalization of classes in the\nPython standard library poses a usability problem.\nAlthough calling a function is not dif ferent than calling a class, it’ s\ngood to know which is which because of another thing we can do with a\nclass: subclassing. So I personally use CamelCase  in every class that I\ncode, and I wish all classes in the Python standard library used the same\nconvention. I am looking at you, collections.OrderedDict  and\ncollections.defaultdict .\n1  Alex Martelli, Anna Ravenscroft & Steve Holden, Python in a Nutshell, 3r d Edition\n(O’Reilly), p. 123.\n2  Bertrand Meyer , Object-Oriented Softwar e Construction , 2E, p. 57.\n3  The OSCON conferences were a permanent casualty of the COVID-19 pandemic. The\noriginal 744KB JSON file I used for these examples is still online  as of December 19, 2020. A\ncopy named osconfeed.json  can be found in the 23-dyn-attr -pr op/oscon/data  directory in the\nexample code repository\n4  T wo examples are AttrDict  and addict .\n5  The expression self.__data[name]  is where a KeyError  exception may occur .\nIdeally , it should be handled and an AttributeError  raised instead, because that’ s what is\nexpected from __getattr__ . The diligent reader is invited to code the error handling as an\nexercise.\n6  The source of the data is JSON, and the only collection types in JSON data are dict  and\nlist .\n7  By the way , Bunch  is the name of the class used by Alex Martelli to share this tip in a recipe\nfrom 2001 titled “The simple but handy collector of a bunch of named stuff  class” . The\ncomments on Alex’ s recipe suggest interesting enhancements.\n8  This is actually a downside of Meyer ’ s Uniform Access Principle, which I mentioned in the\nopening of this chapter . Read the optional “Soapbox”  if you’re interested in this discussion.\n9  Source: @functools.cached_property  documentation. I know Raymond Hettinger authored\nthis explanation because he wrote it as a response to an issue I filed: bpo42781—\nfunctools.cached_property docs should explain that it is non-overriding . Hettinger is a major\ncontributor to the of ficial Python docs and standard library . He also wrote the excellent\nDescriptor HowT o Guide , a key resource for Chapter 24 .\n10  Direct quote by Jef f Bezos in the W all Str eet Journal  story “Birth of a Salesman”  (October 15,\n201 1).\n11  This code is adapted from “Recipe 9.21. A voiding Repetitive Property Methods” from Python\nCookbook, 3E  by David Beazley and Brian K. Jones (O’Reilly).\n12  Alex Martelli points out that, although __slots__  can be coded as a list , it’ s better to be\nexplicit and always use a tuple , because changing the list in the __slots__  after the class\nbody is processed has no ef fect, so it would be misleading to use a mutable sequence there.\n13  Including the no-name default that the Java T utorial  calls “package-private.”\n14  Alex Martelli, Python in a Nutshell, 2E  (O’Reilly), p. 101.\n15  The reasons I am about to mention are given in the Dr . Dobbs Journal article titled “Java’ s\nnew Considered Harmful” , by Jonathan Amsterdam and in “Consider static factory methods\ninstead of constructors” , which is Item 1 of the award-winning book Effective Java  (Addison-\nW esley) by Joshua Bloch.",14949
328-Whats new in this chapter.pdf,328-Whats new in this chapter,"Chapter 24. Attribute\nDescriptors\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 24th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nLearning about descriptors not only pr ovides access to a lar ger toolset, it\ncr eates a deeper understanding of how Python works and an\nappr eciation for the elegance of its design.\n— Raymond Hettinger , Python core developer and guru\nDescriptors are a way of reusing the same access logic in multiple\nattributes. For example, field types in ORMs such as the Django ORM and\nSQL Alchemy are descriptors, managing the flow of data from the fields in\na database record to Python object attributes and vice versa.\nA descriptor is a class that implements a dynamic protocol consisting of the\n__get__ , __set__ , and __delete__  methods. The property  class\nimplements the full descriptor protocol. As usual with dynamic protocols,\npartial implementations are OK. In fact, most descriptors we see in real\ncode implement only __get__  and __set__ , and many implement only\none of these methods.1",1547
329-LineItem Take 3 A Simple Descriptor.pdf,329-LineItem Take 3 A Simple Descriptor,"Descriptors are a distinguishing feature of Python, deployed not only at the\napplication level but also in the language infrastructure. Besides properties,\nother Python features that leverage descriptors are methods and the\nclassmethod  and staticmethod  decorators. Understanding\ndescriptors is key to Python mastery . This is what this chapter is about.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nThe Quantity  descriptor example in “LineItem T ake #4: Automatic\nStorage Attribute Names”  was dramatically simplified thanks to the\n__set_name__  special method added to the descriptor protocol in\nPython 3.6.\nI removed the property factory example formerly in “LineItem T ake #4:\nAutomatic Storage Attribute Names”  because it became irrelevant: the point\nwas to show an alternative way of solving the Quantity  problem, but\nwith the addition of __set_name__  the descriptor solution becomes\nmuch simpler .\nThe AutoStorage  class that used to appear in “LineItem T ake #5: A\nNew Descriptor T ype”  is also gone because __set_name__  made it\nobsolete.\nD e s c r i p t o r  E x a m p l e :  A t t r i b u t e  V a l i d a t i o n\nAs we saw in “Coding a Property Factory” , a property factory is a way to\navoid repetitive coding of getters and setters by applying functional\nprogramming patterns. A property factory is a higher -order function that\ncreates a parameterized set of accessor functions and builds a custom\nproperty instance from them, with closures to hold settings like the\nstorage_name . The object-oriented way of solving the same problem is\na descriptor class.\nW e’ll continue the series of LineItem  examples where we left it, in\n“Coding a Property Factory” , by refactoring the quantity  property\nfactory into a Quantity  descriptor class.\nLineItem T ake #3: A Simple Descriptor\nA class implementing a __get__ , a __set__ , or a __delete__\nmethod is a descriptor . Y ou use a descriptor by declaring instances of it as\nclass attributes of another class.\nW e’ll create a Quantity  descriptor and the LineItem  class will use two\ninstances of Quantity : one for managing the weight  attribute, the other\nfor price . A diagram helps, so take a look at Figure 24-1 .\nFigur e 24-1. UML class diagram for LineItem using a descriptor class named Quantity . Underlined\nattributes in UML ar e class attributes. Note that weight and price ar e instances of Quantity attached\nto the LineItem class, but LineItem instances also have their own weight and price attributes wher e\nthose values ar e stor ed.\nNote that the word weight  appears twice in Figure 24-1 , because there are\nreally two distinct attributes named weight : one is a class attribute of\nLineItem , the other is an instance attribute that will exist in each\nLineItem  object. This also applies to price .\nFrom now on, I will use the following definitions:\nDescriptor class\nA class implementing the descriptor protocol. That’ s Quantity  in\nFigure 24-1 .\nManaged class\nThe class where the descriptor instances are declared as class attributes\n—LineItem  in Figure 24-1 .\nDescriptor instance\nEach instance of a descriptor class, declared as a class attribute of the\nmanaged class. In Figure 24-1 , each descriptor instance is represented\nby a composition arrow with an underlined name (the underline means\nclass attribute in UML). The black diamonds touch the LineItem\nclass, which contains the descriptor instances.\nManaged instance\nOne instance of the managed class. In this example, LineItem\ninstances will be the managed instances (they are not shown in the class\ndiagram).\nStorage attribute\nAn attribute of the managed instance that will hold the value of a\nmanaged attribute for that particular instance. In Figure 24-1 , the\nLineItem  instance attributes weight  and price  will be the storage\nattributes. They are distinct from the descriptor instances, which are\nalways class attributes.\nManaged attribute\nA public attribute in the managed class that will be handled by a\ndescriptor instance, with values stored in storage attributes. In other\nwords, a descriptor instance and a storage attribute provide the\ninfrastructure for a managed attribute.\nIt’ s important to realize that Quantity  instances are class attributes of\nLineItem . This crucial point is highlighted by the mills and gizmos in\nFigure 24-2 .\nFigur e 24-2. UML class diagram annotated with MGN (Mills & Gizmos Notation): classes ar e mills\nthat pr oduce gizmos—the instances. The Quantity mill pr oduces two gizmos with r ound heads, which\nar e attached to the LineItem mill: weight and price. The LineItem mill pr oduces r ectangular gizmos\nthat have their own weight and price attributes wher e those values ar e stor ed.\nI N T R O D U C I N G  M I L L S  &  G I Z M O S  N O T A T I O N\nAfter explaining descriptors many times, I realized UML is not very\ngood at showing relationships involving classes and instances, like the\nrelationship between a managed class and the descriptor instances.  So\nI invented my own “language,” the Mills & Gizmos Notation (MGN),\nwhich I use to annotate UML diagrams.\nMGN is designed to make very clear the distinction between classes\nand instances. See Figure 24-3 . In MGN, a class is drawn as a “mill,” a\ncomplicated machine that produces gizmos. Classes/mills are always\nmachines with levers and dials. The gizmos are the instances, and they\nlook much simpler . When this book is rendered in color , gizmos have\nthe same color as the mill that made it.\nFigur e 24-3. MGN sketch showing the LineItem class making thr ee instances, and Quantity\nmaking two. One instance of Quantity is r etrieving a value stor ed in a LineItem instance.2\nFor this example, I drew LineItem  instances as rows in a tabular\ninvoice, with three cells representing the three attributes\n(description , weight , and price ). Because Quantity\ninstances are descriptors, they have a magnifying glass to __get__\nvalues and a claw to __set__  values. When we get to metaclasses,\nyou’ll thank me for these doodles.\nEnough doodling for now . Here is the code: Example 24-1  shows the\nQuantity  descriptor class, and Example 24-2  lists a new LineItem\nclass using two instances of Quantity .\nExample 24-1. bulkfood_v3.py: Quantity descriptors manage attributes in\nLineItem\nclass Quantity :  \n \n \n    def __init__ (self, storage_name ): \n        self.storage_name  = storage_name   \n \n \n    def __set__(self, instance , value):  \n \n        if value > 0: \n            instance .__dict__ [self.storage_name ] = value  \n \n        else: \n            msg = f'{self.storage_name} must be > 0 ' \n            raise ValueError (msg) \n \n    def __get__(self, instance , owner):  \n \n        return instance .__dict__ [self.storage_name ]\nDescriptor is a protocol-based feature; no subclassing is needed to\nimplement one.\nEach Quantity  instance will have a storage_name  attribute: that’ s\nthe name of the storage attribute to hold the value in the managed\ninstances.\n__set__  is called when there is an attempt to assign to the managed\nattribute. Here, self  is the descriptor instance (i.e.,\nLineItem.weight  or LineItem.price ), instance  is the\nmanaged instance (a LineItem  instance), and value  is the value\nbeing assigned.\nW e must store attribute value directly into __dict__ ; calling\nsetattr(self, self.storage_name)  would trigger the\n__set__  method again, leading to infinite recursion.\nW e need to implement __get__  because the name of the managed\nattribute may not the same as the storage_name . The owner\nar gument will be explained shortly .\nImplementing __get__  is necessary because a user could write something\nlike this:\nclass House: \n    rooms = Quantity ('number_of_rooms' )\nIn the House  class, the managed attribute is rooms , but the storage\nattribute is number_of_rooms .\nNote that __get__  receives three ar guments: self , instance , and\nowner . The owner  ar gument is a reference to the managed class (e.g.,\nLineItem ), and it’ s useful if you want the descriptor to support retrieving\na class attribute—perhaps to emulate Python’ s default behavior of retrieving\na class attribute when the name is not found in the instance.\nIf a managed attribute, such as weight , is retrieved via the class like\nLineItem.weight , the descriptor __get__  method receives None  as\nthe value for the instance  ar gument.\nT o support introspection and other metaprogramming tricks by the user , it’ s\na good practice to make __get__  return the descriptor instance when the\nmanaged attribute is accessed through the class. T o do that, we’d code\n__get__  like this:\n    def __get__(self, instance , owner): \n        if instance  is None: \n            return self \n        else: \n            return instance .__dict__ [self.storage_name ]\nExample 24-2  demonstrates the use of Quantity  in LineItem .\nExample 24-2. bulkfood_v3.py: Quantity descriptors manage attributes in\nLineItem\nclass LineItem : \n    weight = Quantity ('weight')  \n \n    price = Quantity ('price')  \n \n \n    def __init__ (self, description , weight, price):  \n \n        self.description  = description  \n        self.weight = weight \n        self.price = price \n \n    def subtotal (self): \n        return self.weight * self.price\nThe first descriptor instance will manage the weight  attribute.\nThe second descriptor instance will manage the weight  attribute.\nThe rest of the class body is as simple and clean as the original code in\nbulkfood_v1.py  ( Example 23-19 ).\nThe code in Example 24-2  works as intended, preventing the sale of truf fles\nfor $0:\n>>> truffle = LineItem ('White truffle' , 100, 0) \nTraceback (most recent call last):  \n    ... \nValueError : value must be > 03\nW A R N I N G\nWhen coding descriptor __get__  and __set__  methods, keep in mind what the\nself  and instance  ar guments mean: self  is the descriptor instance, and\ninstance  is the managed instance. Descriptors managing instance attributes should\nstore values in the managed instances. That’ s why Python provides the instance\nar gument to the descriptor methods.\nIt may be tempting, but wrong, to store the value of each managed attribute\nin the descriptor instance itself. In other words, in the __set__  method,\ninstead of coding:\n    instance .__dict__ [self.storage_name ] = value\nthe tempting but bad alternative would be:\n    self.__dict__ [self.storage_name ] = value\nT o understand why this would be wrong, think about the meaning of the\nfirst two ar guments to __set__ : self  and instance . Here, self  is\nthe descriptor instance, which is actually a class attribute of the managed\nclass. Y ou may have thousands of LineItem  instances in memory at one\ntime, but you’ll only have two instances of the descriptors: the class\nattributes LineItem.weight  and LineItem.price . So anything you\nstore in the descriptor instances themselves is actually part of a LineItem\nclass attribute, and therefore is shared among all LineItem  instances.\nA drawback of Example 24-2  is the need to repeat the names of the\nattributes when the descriptors are instantiated in the managed class body . It\nwould be nice if the LineItem  class could be declared like this:\nclass LineItem : \n    weight = Quantity () \n    price = Quantity () \n \n    # remaining methods as before",11398
330-LineItem Take 4 Automatic Storage Attribute Names.pdf,330-LineItem Take 4 Automatic Storage Attribute Names,"As it stands, Example 24-2  requires naming each Quantity  explicitly ,\nwhich is not only inconvenient but dangerous: if a programmer copy and\npasting code for gets to edit both names and writes something like price\n= Quantity('weight') , the program will misbehave badly ,\nclobbering the value of weight  whenever the price  is set.\nThe problem is that—as we saw in Chapter 6 —the right-hand side of an\nassignment is executed before the variable exists. The expression\nQuantity()  is evaluated to create a descriptor instance, and there is no\nway the code in the Quantity  class can guess the name of the variable to\nwhich the descriptor will be bound (e.g., weight  or price ).\nThankfully , the descriptor protocol now supports the aptly named\n__set_name__  special method. W e’ll see how to use it next.\nN O T E\nAutomatic naming of a descriptor storage attribute used to be a thorny issue. In Fluent\nPython, First Edition  I devoted several pages and lines of code in this chapter and the\nnext to presenting dif ferent solutions, including the use of a class decorator and then a\nmetaclasses in Chapter 25 . This was greatly simplified in Python 3.6.\nLineItem T ake #4: Automatic Storage Attribute Names\nT o avoid retyping the attribute name in the descriptor instances, we’ll\nimplement __set_name__  to create storage_name  of each\nQuantity  instance. The __set_name__  special method was added to\nthe descriptor protocol in Python 3.6. The interpreter calls\n__set_name__  on each descriptor it finds in a class  body—if the\ndescriptor implements it.\nIn Example 24-3 , the LineItem  descriptor class doesn’ t need an\n__init__ . Instead, __set_item__  saves the name of the storage\nattribute.\nExample 24-3. bulkfood_v4.py: __set_name__  sets the name for each\nQuantity descriptor instance4\nclass Quantity : \n \n    def __set_name__ (self, owner, name):  \n \n        self.storage_name  = name          \n  \n \n    def __set__(self, instance , value):   \n \n        if value > 0: \n            instance .__dict__ [self.storage_name ] = value \n        else: \n            msg = f'{self.storage_name} must be > 0 ' \n            raise ValueError (msg) \n \n    # no __get__ needed  \n  \n \nclass LineItem : \n    weight = Quantity ()  \n \n    price = Quantity () \n \n    def __init__ (self, description , weight, price): \n        self.description  = description  \n        self.weight = weight \n        self.price = price \n \n    def subtotal (self): \n        return self.weight * self.price\nself  is the descriptor instance (not the managed instance); owner  is\nthe managed class; and name  is the name of the attribute of owner  to\nwhich this descriptor instance was assigned in the class body of owner .\nThis is what the __init__  did in Example 24-2 .\nThe __set__  method here is exactly the same as in Example 24-2 .\nImplementing __get__  is not necessary because the name of the\nstorage attribute matches the name of the managed attribute. The\nexpression product.price  gets the price  attribute directly from\nthe LineItem  instance.\nNow we don’ t need to pass the managed attribute name to the\nQuantity  constructor . That was the goal for this version.",3211
331-LineItem Take 5 A New Descriptor Type.pdf,331-LineItem Take 5 A New Descriptor Type,"Looking at Example 24-3 , you may think that’ s a lot of code just for\nmanaging a couple of attributes, but it’ s important to realize that the\ndescriptor logic is now abstracted into a separate code unit: the Quantity\nclass. Usually we do not define a descriptor in the same module where it’ s\nused, but in a separate utility module designed to be used across the\napplication—even in many applications, if you are developing a framework.\nW ith this in mind, Example 24-4  better represents the typical usage of a\ndescriptor .\nExample 24-4. bulkfood_v4c.py: LineItem definition unclutter ed; the\nQuantity descriptor class now r esides in the imported model_v4c module\nimport model_v4c  as model  \n \n \n \nclass LineItem : \n    weight = model.Quantity ()  \n \n    price = model.Quantity () \n \n    def __init__ (self, description , weight, price): \n        self.description  = description  \n        self.weight = weight \n        self.price = price \n \n    def subtotal (self): \n        return self.weight * self.price\nImport the model_v4c  module where Quantity  is implemented.\nPut model.Quantity  to use.\nDjango users will notice that Example 24-4  looks a lot like a model\ndefinition. It’ s no coincidence: Django model fields are descriptors.\nBecause descriptors are implemented as classes, we can leverage\ninheritance to reuse some of the code we have for new descriptors. That’ s\nwhat we’ll do in the following section.\nLineItem T ake #5: A New Descriptor T ype\nThe imaginary or ganic food store hits a snag: somehow a line item instance\nwas created with a blank description and the order could not be fulfilled. T o\nprevent that, we’ll create a new descriptor , NonBlank . As we design\nNonBlank , we realize it will be very much like the Quantity  descriptor ,\nexcept for the validation logic.\nThis prompts a refactoring, producing Validated , an abstract class that\noverrides the __set__  method, calling a validate  method that must be\nimplemented by subclasses.\nW e’ll then rewrite Quantity  and implement NonBlank  by inheriting\nfrom Validated  and just coding the validate  methods.\nThe relationship between Validated , Quantity , and NonBlank  is an\napplication of the T emplate Method  as described in the Design Patterns\nclassic:\nA template method defines an algorithm in terms of abstract operations\nthat subclasses override to pr ovide concr ete behavior .\nIn Example 24-5 , Validated.__set__  is the template method and\nself.validate  is the abstract operation.\nExample 24-5. model_v5.py: the V alidated ABC\nimport abc \n \nclass Validated (abc.ABC): \n \n    def __set_name__ (self, owner, name): \n        self.storage_name  = name \n \n    def __set__(self, instance , value): \n        value = self.validate (self.storage_name , value)  \n \n        instance .__dict__ [self.storage_name ] = value  \n \n \n    @abc.abstractmethod  \n    def validate (self, name, value):  \n \n        """"""return validated value or raise ValueError""""""\n__set__  delegates validation to the validate  method…5\n…then uses the returned value  to update the stored value.\nvalidate  is an abstract method; this is the template method.\nAlex Martelli prefers to call this design pattern Self-Delegation , and I agree\nit’ s a more descriptive name: the first line of __set__  self-delegates to\nvalidate .\nThe concrete Validated  subclasses in this example are Quantity  and\nNonBlank , shown in Example 24-6 .\nExample 24-6. model_v5.py: Quantity and NonBlank, concr ete V alidated\nsubclasses\nclass Quantity (Validated ): \n    """"""a number greater than zero""""""  \n \n    def validate (self, name, value):  \n \n        if value <= 0: \n            raise ValueError (f'{name} must be > 0 ') \n        return value \n \n \nclass NonBlank (Validated ): \n    """"""a string with at least one non-space character""""""  \n \n    def validate (self, name, value): \n        value = value.strip() \n        if len(value) == 0: \n            raise ValueError (f'{name} cannot be blank ') \n        return value  \nUsers of model_v5.py  don’ t need to know all these details. What matters is\nthat they get to use Quantity  and NonBlank  to automate the validation\nof instance attributes. See the latest LineItem  class in Example 24-7 .\nExample 24-7. bulkfood_v5.py: LineItem using Quantity and NonBlank\ndescriptors\nimport model_v5  as model  \n \n \nclass LineItem : \n    description  = model.NonBlank ()  \n 6",4454
332-Overriding Versus Non-Overriding Descriptors.pdf,332-Overriding Versus Non-Overriding Descriptors,"weight = model.Quantity () \n    price = model.Quantity () \n \n    def __init__ (self, description , weight, price): \n        self.description  = description  \n        self.weight = weight \n        self.price = price \n \n    def subtotal (self): \n        return self.weight * self.price\nImport the model_v5  module, giving it a friendlier name.\nPut model.NonBlank  to use. The rest of the code is unchanged.\nThe LineItem  examples we’ve seen in this chapter demonstrate a typical\nuse of descriptors to manage data attributes. Descriptors like Quantity\nare called overriding descriptors because its __set__  method overrides\n(i.e., intercepts and overrules) the setting of an instance attribute by the\nsame name in the managed instance. However , there are also non-\noverriding descriptors. W e’ll explore this distinction in detail in the next\nsection.\nO v e r r i d i n g  V e r s u s  N o n - O v e r r i d i n g\nD e s c r i p t o r s\nRecall that there is an important asymmetry in the way Python handles\nattributes. Reading an attribute through an instance normally returns the\nattribute defined in the instance, but if there is no such attribute in the\ninstance, a class attribute will be retrieved. On the other hand, assigning to\nan attribute in an instance normally creates the attribute in the instance,\nwithout af fecting the class at all.\nThis asymmetry also af fects descriptors, in ef fect creating two broad\ncategories of descriptors depending on whether the __set__  method is\nimplemented. If __set__  is present, the class is an overriding descriptor;\notherwise, it is a non-overriding descriptor . These terms will make sense as\nwe study descriptor behaviors in the next examples.\nObserving the dif ferent descriptor categories requires a few classes, so we’ll\nuse the code in Example 24-8  as our testbed for the following sections.\nT I P\nEvery __get__  and __set__  method in Example 24-8  calls print_args  so their\ninvocations are displayed in a readable way . Understanding print_args  and the\nauxiliary functions cls_name  and display  is not important, so don’ t get distracted\nby them.\nExample 24-8. descriptorkinds.py: simple classes for studying descriptor\noverriding behaviors\n### auxiliary functions for display only ###  \n \ndef cls_name (obj_or_cls ): \n    cls = type(obj_or_cls ) \n    if cls is type: \n        cls = obj_or_cls  \n    return cls.__name__ .split('.')[-1] \n \ndef display(obj): \n    cls = type(obj) \n    if cls is type: \n        return '<class {}> '.format(obj.__name__ ) \n    elif cls in [type(None), int]: \n        return repr(obj) \n    else: \n        return '<{} object> '.format(cls_name (obj)) \n \ndef print_args (name, *args): \n    pseudo_args  = ', '.join(display(x) for x in args) \n    print('-> {}.__{}__({}) '.format(cls_name (args[0]), name, \npseudo_args )) \n \n \n### essential classes for this example ###  \n \nclass Overriding :  \n \n    """"""a.k.a. data descriptor or enforced descriptor""""""  \n \n    def __get__(self, instance , owner): \n        print_args ('get', self, instance , owner)  \n \n \n    def __set__(self, instance , value): \n        print_args ('set', self, instance , value) \n \n \nclass OverridingNoGet :  \n \n    """"""an overriding descriptor without ``__get__``""""""  \n \n    def __set__(self, instance , value): \n        print_args ('set', self, instance , value) \n \n \nclass NonOverriding :  \n \n    """"""a.k.a. non-data or shadowable descriptor""""""  \n \n    def __get__(self, instance , owner): \n        print_args ('get', self, instance , owner) \n \n \nclass Managed:  \n \n    over = Overriding () \n    over_no_get  = OverridingNoGet () \n    non_over  = NonOverriding () \n \n    def spam(self):  \n \n        print('-> Managed.spam({}) '.format(display(self)))\nAn overriding descriptor class with __get__  and __set__ .\nThe print_args  function is called by every descriptor method in this\nexample.\nAn overriding descriptor without a __get__  method.\nNo __set__  method here, so this is a non-overriding descriptor .\nThe managed class, using one instance of each of the descriptor classes.\nThe spam  method is here for comparison, because methods are also\ndescriptors.",4224
333-Overriding Descriptor Without __get__.pdf,333-Overriding Descriptor Without __get__,"In the following sections, we will examine the behavior of attribute reads\nand writes on the Managed  class and one instance of it, going through each\nof the dif ferent descriptors defined.\nOverriding Descriptors\nA descriptor that implements the __set__  method is an overriding\ndescriptor , because although it is a class attribute, a descriptor\nimplementing __set__  will override attempts to assign to instance\nattributes. This is how Example 24-3  was implemented. Properties are also\noverriding descriptors: if you don’ t provide a setter function, the default\n__set__  from the property  class will raise AttributeError  to\nsignal that the attribute is read-only . Given the code in Example 24-8 ,\nexperiments with an overriding descriptor can be seen in Example 24-9 .\nW A R N I N G\nPython contributors and authors use dif ferent terms when discussing these concepts. I\nadopted “overriding descriptor” from the book Python in a Nutshell . The of ficial Python\ndocumentation uses “data descriptor”, but “overriding descriptor” highlights the special\nbehavior . Overriding descriptors are also called “enforced descriptors”. Synonyms for\nnon-overriding descriptors include “non-data descriptors” or “shadowable descriptors”.\nExample 24-9. Behavior of an overriding descriptor: obj.over is an instance\nof Overriding ( Example 24-8 )\n    >>> obj = Managed()  \n \n    >>> obj.over  \n \n    -> Overriding .__get__(<Overriding  object>, <Managed object>, \n        <class Managed>) \n    >>> Managed.over  \n \n    -> Overriding .__get__(<Overriding  object>, None, <class \nManaged>) \n    >>> obj.over = 7  \n \n    -> Overriding .__set__(<Overriding  object>, <Managed object>, 7) \n    >>> obj.over  \n \n    -> Overriding .__get__(<Overriding  object>, <Managed object>, \n        <class Managed>) \n    >>> obj.__dict__ ['over'] = 8  \n \n    >>> vars(obj)  \n \n    {'over': 8} \n    >>> obj.over  \n \n    -> Overriding .__get__(<Overriding  object>, <Managed object>, \n        <class Managed>)\nCreate Managed  object for testing.\nobj.over  triggers the descriptor __get__  method, passing the\nmanaged instance obj  as the second ar gument.\nManaged.over  triggers the descriptor __get__  method, passing\nNone  as the second ar gument (instance ).\nAssigning to obj.over  triggers the descriptor __set__  method,\npassing the value 7  as the last ar gument.\nReading obj.over  still invokes the descriptor __get__  method.\nBypassing the descriptor , setting a value directly to the\nobj.__dict__ .\nV erify that the value is in the obj.__dict__ , under the over  key .\nHowever , even with an instance attribute named over , the\nManaged.over  descriptor still overrides attempts to read\nobj.over .\nOverriding Descriptor W ithout __get__\nProperties and other overriding descriptors such as Django model fields\nimplement both __set__  and __get__ , but it’ s also possible to\nimplement only __set__ , as we saw in Example 24-2 . In this case, only\nwriting is handled by the descriptor . Reading the descriptor through an\ninstance will return the descriptor object itself because there is no\n__get__  to handle that access. If a namesake instance attribute is created\nwith a new value via direct access to the instance __dict__ , the\n__set__  method will still override further attempts to set that attribute,\nbut reading that attribute will simply return the new value from the instance,\ninstead of returning the descriptor object. In other words, the instance\nattribute will shadow the descriptor , but only when reading. See\nExample 24-10 .\nExample 24-10. Overriding descriptor without __get__: obj.over_no_get is\nan instance of OverridingNoGet ( Example 24-8 )\n    >>> obj.over_no_get   \n \n    <__main__ .OverridingNoGet  object at 0x665bcc > \n    >>> Managed.over_no_get   \n \n    <__main__ .OverridingNoGet  object at 0x665bcc > \n    >>> obj.over_no_get  = 7  \n \n    -> OverridingNoGet .__set__(<OverridingNoGet  object>, <Managed \nobject>, 7) \n    >>> obj.over_no_get   \n \n    <__main__ .OverridingNoGet  object at 0x665bcc > \n    >>> obj.__dict__ ['over_no_get '] = 9  \n \n    >>> obj.over_no_get   \n \n    9 \n    >>> obj.over_no_get  = 7  \n \n    -> OverridingNoGet .__set__(<OverridingNoGet  object>, <Managed \nobject>, 7) \n    >>> obj.over_no_get   \n \n    9\nThis overriding descriptor doesn’ t have a __get__  method, so reading\nobj.over_no_get  retrieves the descriptor instance from the class.\nThe same thing happens if we retrieve the descriptor instance directly\nfrom the managed class.\nT rying to set a value to obj.over_no_get  invokes the __set__\ndescriptor method.\nBecause our __set__  doesn’ t make changes, reading\nobj.over_no_get  again retrieves the descriptor instance from the\nmanaged class.",4796
334-Methods Are Descriptors.pdf,334-Methods Are Descriptors,"Going through the instance __dict__  to set an instance attribute\nnamed over_no_get .\nNow that over_no_get  instance attribute shadows the descriptor , but\nonly for reading.\nT rying to assign a value to obj.over_no_get  still goes through the\ndescriptor set.\nBut for reading, that descriptor is shadowed as long as there is a\nnamesake instance attribute.\nNon-overriding Descriptor\nA descriptor that does not implement __set__  is a non-overriding\ndescriptor . Setting an instance attribute with the same name will shadow the\ndescriptor , rendering it inef fective for handling that attribute in that specific\ninstance. Methods and @functools.cached_property  are\nimplemented as non-overriding descriptors. Example 24-1 1  shows the\noperation of a non-overriding descriptor .\nExample 24-1 1. Behavior of a non-overriding descriptor: obj.non_over is\nan instance of non-overriding ( Example 24-8 )\n    >>> obj = Managed() \n    >>> obj.non_over   \n \n    -> NonOverriding .__get__(<NonOverriding  object>, <Managed \nobject>, \n        <class Managed>) \n    >>> obj.non_over  = 7  \n \n    >>> obj.non_over   \n \n    7 \n    >>> Managed.non_over   \n \n    -> NonOverriding .__get__(<NonOverriding  object>, None, <class \nManaged>) \n    >>> del obj.non_over   \n \n    >>> obj.non_over   \n \n    -> NonOverriding .__get__(<NonOverriding  object>, <Managed \nobject>, \n        <class Managed>)\nobj.non_over  triggers the descriptor __get__  method, passing\nobj  as the second ar gument.\nManaged.non_over  is a non-overriding descriptor , so there is no\n__set__  to interfere with this assignment.\nThe obj  now has an instance attribute named non_over , which\nshadows the namesake descriptor attribute in the Managed  class.\nThe Managed.non_over  descriptor is still there, and catches this\naccess via the class.\nIf the non_over  instance attribute is deleted…\nThen reading obj.non_over  hits the __get__  method of the\ndescriptor in the class, but note that the second ar gument is the managed\ninstance.\nIn the previous examples, we saw several assignments to an instance\nattribute with the same name as a descriptor , and dif ferent results according\nto the presence of a __set__  method in the descriptor .\nThe setting of attributes in the class cannot be controlled by descriptors\nattached to the same class. In particular , this means that the descriptor\nattributes themselves can be clobbered by assigning to the class, as the next\nsection explains.\nOverwriting a Descriptor in the Class\nRegardless of whether a descriptor is overriding or not, it can be\noverwritten by assignment to the class. This is a monkey-patching\ntechnique, but in Example 24-12  the descriptors are replaced by integers,\nwhich would ef fectively break any class that depended on the descriptors\nfor proper operation.\nExample 24-12. Any descriptor can be overwritten on the class itself\n    >>> obj = Managed()  \n \n    >>> Managed.over = 1  \n \n    >>> Managed.over_no_get  = 2 \n    >>> Managed.non_over  = 3 \n    >>> obj.over, obj.over_no_get , obj.non_over   \n \n    (1, 2, 3)\nCreate a new instance for later testing.\nOverwrite the descriptor attributes in the class.\nThe descriptors are really gone.\nExample 24-12  reveals another asymmetry regarding reading and writing\nattributes: although the reading of a class attribute can be controlled by a\ndescriptor with __get__  attached to the managed class, the writing of a\nclass attribute cannot be handled by a descriptor with __set__  attached to\nthe same class.\nT I P\nIn order to control the setting of attributes in a class, you have to attach descriptors to\nthe class of the class—in other words, the metaclass. By default, the metaclass of user -\ndefined classes is type , and you cannot add attributes to type . But in Chapter 25 ,\nwe’ll create our own metaclasses.\nLet’ s now focus on how descriptors are used to implement methods in\nPython.\nM e t h o d s  A r e  D e s c r i p t o r s\nA function within a class becomes a bound method because all user -defined\nfunctions have a __get__  method, therefore they operate as descriptors\nwhen attached to a class. Example 24-13  demonstrates reading the spam\nmethod from the Managed  class introduced in Example 24-8 .\nExample 24-13. A method is a non-overriding descriptor\n    >>> obj = Managed() \n    >>> obj.spam  \n \n    <bound method Managed.spam of <descriptorkinds .Managed object \nat 0x74c80c >> \n    >>> Managed.spam  \n \n    <function  Managed.spam at 0x734734 > \n    >>> obj.spam = 7  \n \n    >>> obj.spam \n    7\nReading from obj.spam  retrieves a bound method object.\nBut reading from Managed.spam  retrieves a function.\nAssigning a value to obj.spam  shadows the class attribute, rendering\nthe spam  method inaccessible from the obj  instance.\nFunctions do not implement __set__ , therefore they are non-overriding\ndescriptors, as the last line of Example 24-13  shows.\nThe other key takeaway from Example 24-13  is that obj.spam  and\nManaged.spam  retrieve dif ferent objects. As usual with descriptors, the\n__get__  of a function returns a reference to itself when the access\nhappens through the managed class. But when the access goes through an\ninstance, the __get__  of the function returns a bound method object: a\ncallable that wraps the function and binds the managed instance (e.g., obj )\nto the first ar gument of the function (i.e., self ), like the\nfunctools.partial  function does (as seen in “Freezing Ar guments\nwith functools.partial” ).\nFor a deeper understanding of this mechanism, take a look at Example 24-\n14 .\nExample 24-14. method_is_descriptor .py: a T ext class, derived fr om\nUserString\nimport collections  \n \n \nclass Text(collections .UserString ): \n \n    def __repr__ (self): \n        return 'Text({!r})' .format(self.data) \n \n    def reverse(self): \n        return self[::-1]\nNow let’ s investigate the Text.reverse  method. See Example 24-15 .\nExample 24-15. Experiments with a method\n    >>> word = Text('forward') \n    >>> word  \n \n    Text('forward') \n    >>> word.reverse()  \n \n    Text('drawrof') \n    >>> Text.reverse(Text('backward '))  \n \n    Text('drawkcab ') \n    >>> type(Text.reverse), type(word.reverse)  \n \n    (<class 'function '>, <class 'method'>) \n    >>> list(map(Text.reverse, ['repaid', (10, 20, 30), \nText('stressed ')]))  \n \n    ['diaper', (30, 20, 10), Text('desserts ')] \n    >>> Text.reverse.__get__(word)  \n \n    <bound method Text.reverse of Text('forward')> \n    >>> Text.reverse.__get__(None, Text)  \n \n    <function  Text.reverse at 0x101244e18 > \n    >>> word.reverse  \n \n    <bound method Text.reverse of Text('forward')> \n    >>> word.reverse.__self__   \n \n    Text('forward') \n    >>> word.reverse.__func__  is Text.reverse  \n \n    True\nThe repr  of a Text  instance looks like a Text  constructor call that\nwould make an equal instance.\nThe reverse  method returns the text spelled backward.\nA method called on the class works as a function.\nNote the dif ferent types: a function  and a method .",7108
335-Descriptor Usage Tips.pdf,335-Descriptor Usage Tips,"Text.reverse  operates as a function, even working with objects that\nare not instances of Text .\nAny function is a non-overriding descriptor . Calling its __get__  with\nan instance retrieves a method bound to that instance.\nCalling the function’ s __get__  with None  as the instance\nar gument retrieves the function itself.\nThe expression word.reverse  actually invokes\nText.reverse.__get__(word) , returning the bound method.\nThe bound method object has a __self__  attribute holding a\nreference to the instance on which the method was called.\nThe __func__  attribute of the bound method is a reference to the\noriginal function attached to the managed class.\nThe bound method object also has a __call__  method, which handles the\nactual invocation. This method calls the original function referenced in\n__func__ , passing the __self__  attribute of the method as the first\nar gument. That’ s how the implicit binding of the conventional self\nar gument works.\nThe way functions are turned into bound methods is a prime example of\nhow descriptors are used as infrastructure in the language.\nAfter this deep dive into how descriptors and methods work, let’ s go\nthrough some practical advice about their use.\nD e s c r i p t o r  U s a g e  T i p s\nThe following list addresses some practical consequences of the descriptor\ncharacteristics just described:\nUse pr operty to Keep It Simple\nThe property  built-in creates overriding descriptors implementing\nboth __set__  and __get__ , even if you do not define a setter\nmethod. The default __set__  of a property raises\nAttributeError: can't set attribute , so a property is\nthe easiest way to create a read-only attribute, avoiding the issue\ndescribed next.\nRead-only descriptors r equir e __set__\nIf you use a descriptor class to implement a read-only attribute, you\nmust remember to code both __get__  and __set__ , otherwise\nsetting a namesake attribute on an instance will shadow the descriptor .\nThe __set__  method of a read-only attribute should just raise\nAttributeError  with a suitable message.\nV alidation descriptors can work with __set__ only\nIn a descriptor designed only for validation, the __set__  method\nshould check the value  ar gument it gets, and if valid, set it directly in\nthe instance __dict__  using the descriptor instance name as key . That\nway , reading the attribute with the same name from the instance will be\nas fast as possible, because it will not require a __get__ . See the code\nfor Example 24-2 .\nCaching can be done efficiently with __get__ only\nIf you code just the __get__  method, you have a non-overriding\ndescriptor . These are useful to make some expensive computation and\nthen cache the result by setting an attribute by the same name on the\ninstance. The namesake instance attribute will shadow the descriptor , so\nsubsequent access to that attribute will fetch it directly from the instance\n__dict__  and not trigger the descriptor __get__  anymore. The\n@functools.cached_property  decorator actually produces a\nnon-overriding descriptor .\nNon-special methods can be shadowed by instance attributes7\nBecause functions and methods only implement __get__ , they are\nnon-overriding descriptors. A simple assignment like\nmy_obj.the_method = 7  means that further access to\nthe_method  through that instance will retrieve the number 7 —\nwithout af fecting the class or other instances. However , this issue does\nnot interfere with special methods. The interpreter only looks for special\nmethods in the class itself, in other words, repr(x)  is executed as\nx.__class__.__repr__(x) , so a __repr__  attribute defined in\nx  has no ef fect on repr(x) . For the same reason, the existence of an\nattribute named __getattr__  in an instance will not subvert the\nusual attribute access algorithm.\nThe fact that non-special methods can be overridden so easily in instances\nmay sound fragile and error -prone, but I personally have never been bitten\nby this in more than 20 years of Python coding. On the other hand, if you\nare doing a lot of dynamic attribute creation, where the attribute names\ncome from data you don’ t control (as we did in the earlier parts of this\nchapter), then you should be aware of this and perhaps implement some\nfiltering or escaping of the dynamic attribute names to preserve your sanity .\nN O T E\nThe FrozenJSON  class in Example 23-5  is safe from instance attribute shadowing\nmethods because its only methods are special methods and the build  class method.\nClass methods are safe as long as they are always accessed through the class, as I did\nwith FrozenJSON.build  in Example 23-5 —later replaced by __new__  in\nExample 23-6 . The Record  and Event  presented in “Computed Properties”  are also\nsafe: they implement only special methods, static methods, and properties. Properties\nare overriding descriptors, so they are not shadowed by instance attributes.\nT o close this chapter , we’ll cover two features we saw with properties that\nwe have not addressed in the context of descriptors: documentation and\nhandling attempts to delete a managed attribute.",5155
336-Descriptor docstring and Overriding Deletion.pdf,336-Descriptor docstring and Overriding Deletion,D e s c r i p t o r  d o c s t r i n g  a n d  O v e r r i d i n g  D e l e t i o n\nThe docstring of a descriptor class is used to document every instance of the\ndescriptor in the managed class. Figure 24-4  shows the help displays for the\nLineItem  class with the Quantity  and NonBlank  descriptors from\nExamples 24-6  and 24-7 .,335
337-Chapter Summary.pdf,337-Chapter Summary,"Figur e 24-4. Scr eenshots of the Python console when issuing the commands help(LineItem.weight)\nand help(LineItem)\nThat is somewhat unsatisfactory . In the case of LineItem , it would be\ngood to add, for example, the information that weight  must be in\nkilograms. That would be trivial with properties, because each property\nhandles a specific managed attribute. But with descriptors, the same\nQuantity  descriptor class is used for weight  and price .\nThe second detail we discussed with properties but have not addressed with\ndescriptors is handling attempts to delete a managed attribute. That can be\ndone by implementing a __delete__  method alongside or instead of the\nusual __get__  and/or __set__  in the descriptor class. Coding a silly\ndescriptor class with __delete__  is left as an exercise to the leisurely\nreader .8\nC h a p t e r  S u m m a r y\nThe first example of this chapter was a continuation of the LineItem\nexamples from Chapter 23 . In Example 24-2 , we replaced properties with\ndescriptors. W e saw that a descriptor is a class that provides instances that\nare deployed as attributes in the managed class. Discussing this mechanism\nrequired special terminology , introducing terms such as managed instance\nand storage attribute.\nIn “LineItem T ake #4: Automatic Storage Attribute Names” , we removed\nthe requirement that Quantity  descriptors were declared with an explicit\nstorage_name , which was redundant and error -prone. The solution was\nto implement the __set_name__  special method in Quantity , to save\nthe name of the managed property as self.storage_name .\n“LineItem T ake #5: A New Descriptor T ype”  showed how to subclass an\nabstract descriptor class to share code while building specialized descriptors\nwith some common functionality .\nW e then looked at the dif ferent behavior of descriptors providing or\nomitting the __set__  method, making the crucial distinction between\noverriding and non-overriding descriptors, a.k.a. data and non-data\ndescriptors. Through detailed testing we uncovered when descriptors are in\ncontrol and when they are shadowed, bypassed, or overwritten.\nFollowing that, we studied a particular category of non-overriding\ndescriptors: methods. Console experiments revealed how a function\nattached to a class becomes a method when accessed through an instance,\nby leveraging the descriptor protocol.\nT o conclude the chapter , “Descriptor Usage T ips”  presented practical tips,\nand “Descriptor docstring and Overriding Deletion”  provided a brief look at\nhow descriptor deletion and documentation work.",2605
338-Further Reading.pdf,338-Further Reading,"N O T E\nAs noted in “What’ s new in this chapter” , several examples in this chapter became much\nsimpler thanks to the __set_name__  special method of the descriptor protocol, added\nin Python 3.6. That’ s language evolution!\nF u r t h e r  R e a d i n g\nBesides the obligatory reference to the “Data Model” chapter , Raymond\nHettinger ’ s Descriptor HowT o Guide  is a valuable resource—part of the\nHowT o collection  in the of ficial Python documentation.\nAs usual with Python object model subjects, Martelli, Ravenscroft &\nHolden’ s Python in a Nutshell , 3E (O’Reilly) is authoritative and objective.\nMartelli also has a presentation titled Python’ s Object Model , which covers\nproperties and descriptors in depth ( slides , video ).\nW A R N I N G\nBeware that any coverage of descriptors written or recorded before PEP 487 was\nadopted in 2016 is likely to contain examples that are needlessly complicated today ,\nbecause __set_name__  was not supported in Python versions prior to 3.6.\nFor more practical examples, Python Cookbook, 3E  by David Beazley and\nBrian K. Jones (O’Reilly), has many recipes illustrating descriptors, of\nwhich I want to highlight “6.12. Reading Nested and V ariable-Sized Binary\nStructures,” “8.10. Using Lazily Computed Properties,” “8.13.\nImplementing a Data Model or T ype System,” and “9.9. Defining\nDecorators As Classes”—the latter of which addresses deep issues with the\ninteraction of function decorators, descriptors, and methods, explaining how\na function decorator implemented as a class with __call__  also needs to\nimplement __get__  if it wants to work with decorating methods as well\nas functions.\nPEP 487—Simpler customisation of class creation  introduced the\n__set_name__  special method, and it includes an example of a\nvalidating descriptor .\nS O A P B O X\nThe Design of self\n“W orse is Better” is a design philosophy described by Richard P .\nGabriel in The Rise of W orse is Better . The first priority of this\nphilosophy is “Simplicity ,” which Gabriel presents as:\nThe design must be simple, both in implementation and interface. It is\nmor e important for the implementation to be simple than the\ninterface. Simplicity is the most important consideration in a design.\nThe requirement to explicitly declare self  as a first ar gument in\nmethods is an application of “W orse is Better” in Python. The\nimplementation is simple—elegant even—at the expense of the user\ninterface: a method signature like def zfill(self, width):\ndoesn’ t visually match the invocation pobox.zfill(8) .\nModula-3 introduced that convention—and the use of the self\nidentifier—but there is a dif ference: in Modula-3, interfaces are\ndeclared separately from their implementation, and in the interface\ndeclaration the self  ar gument is omitted, so from the user ’ s\nperspective, a method appears in an interface declaration with the same\nexplicit ar guments it takes.\nOne improvement in this regard has been the error messages: for a user -\ndefined method with one ar gument besides self , if the user invokes\nobj.meth() , Python 2.7 raised TypeError: meth() takes\nexactly 2 arguments (1 given) . In Python 3 the message is\nclearer: the confusing ar gument count is not mentioned, but the missing\nar gument is named: meth() missing 1 required\npositional argument: 'x' .\nBesides the use of self  as an explicit ar gument, the requirement to\nqualify all access to instance attributes with self  is also criticized.  I\npersonally don’ t mind typing the self  qualifier: it’ s good to9\ndistinguish local variables from attributes. My issue is with the use of\nself in the def  statement. But I got used to it.\nAnyone who is unhappy about the explicit self  in Python can feel a\nlot better by considering the baf fling semantics of the implicit this  in\nJavaScript. Guido had some good reasons to make self  work as it\ndoes, and he wrote about them in “Adding Support for User -Defined\nClasses” , a post on his blog, The History of Python.\n1  Raymond Hettinger , Descriptor HowT o Guide .\n2  Classes and instances are drawn as rectangles in UML class diagrams. There are visual\ndif ferences, but instances are rarely shown in class diagrams, so developers may not recognize\nthem as such.\n3  White truf fles cost thousands of dollars per pound. Disallowing the sale of truf fles for $0.01 is\nleft as an exercise for the enterprising reader . I know a person who actually bought an $1,800\nencyclopedia of statistics for $18 because of an error in an online store (not Amazon.com in\nthis case).\n4  More precisely , __set_name__  is called by type.__new__ —the constructor of objects\nrepresenting classes. The type  built-in is actually a metaclass: the default class of user -\ndefined classes. This is hard to grasp at first, but rest assured: Chapter 25  is devoted to the\ndynamic configuration of classes, including the concept of metaclasses.\n5  Gamma et al., Design Patterns: Elements of Reusable Object-Oriented Softwar e , p. 326.\n6  Slide #50 of Alex Martelli’ s Python Design Patterns  talk. Highly recommended.\n7  Python is not consistent in such messages. T rying to change the c.real  attribute of a\ncomplex  number gets AttributeError: read-only attribute , but an attempt to\nchange c.conjugate  (a method of complex ), results in AttributeError:\n'complex' object attribute 'conjugate' is read-only .\n8  Customizing the help text for each descriptor instance is surprisingly hard. One solution\nrequires dynamically building a wrapper class for each descriptor instance.\n9  See, for example, A. M. Kuchling’ s famous Python W arts  post (archived) ; Kuchling himself\nis not so bothered by the self  qualifier , but he mentions it—probably echoing opinions from\ncomp.lang.python .",5794
339-type The Built-in Class Factory.pdf,339-type The Built-in Class Factory,"Chapter 25. Class\nMetaprogramming\nA  N O T E  F O R  E A R L Y  R E L E A S E  R E A D E R S\nW ith Early Release ebooks, you get books in their earliest form—the\nauthor ’ s raw and unedited content as they write—so you can take\nadvantage of these technologies long before the of ficial release of these\ntitles.\nThis will be the 25th chapter of the final book. Please note that the\nGitHub repo will be made active later on.\nIf you have comments about how we might improve the content and/or\nexamples in this book, or if you notice missing material within this\nchapter , please reach out to the author at fluentpython2e@ramalho.or g .\nEveryone knows that debugging is twice as har d as writing a pr ogram in\nthe first place. So if you’r e as clever as you can be when you write it, how\nwill you ever debug it?\n— Brian W . Kernighan and P . J. Plauger , The Elements of\nProgramming Style\nClass metaprogramming is the art of creating or customizing classes at\nruntime. Classes are first-class objects in Python, so a function can be used\nto create a new class at any time, without using the class  keyword. Class\ndecorators are also functions, but designed to inspect, change, and even\nreplace the decorated class with another class. Finally , metaclasses are the\nmost advanced tool for class metaprogramming: they let you create whole\nnew categories of classes with special traits, such as the abstract base\nclasses we’ve already seen.1\nMetaclasses are powerful, but hard to justify and even harder to get right.\nClass decorators solve many of the same problems and are easier to\nunderstand. Furthermore, Python 3.6 implemented PEP 487—Simpler\ncustomisation of class cr eation , providing special methods supporting tasks\nthat previously required metaclasses or class decorators.\nThis chapter presents the class metaprogramming techniques in ascending\norder of complexity .\nW A R N I N G\nThis is an exciting topic, and it’ s easy to get carried away . So I must of fer this advice:\nFor the sake of readability and maintainability , you should probably avoid the\ntechniques described in this chapter in application code.\nOn the other hand, these are the tools of the trade if you want to write the next great\nPython framework.\nW h a t ’ s  n e w  i n  t h i s  c h a p t e r\nAll the code in the Class Metapr ogramming  chapter of Fluent Python, First\nEdition  still runs correctly . However , some of the previous examples no\nlonger represent the simplest solutions, in light of new features added since\nPython 3.6.\nI replaced those examples with dif ferent ones, highlighting Python’ s new\nmetaprogramming features or adding further requirements to justify the use\nof the more advanced techniques. Some of the new examples leverage type\nhints to provide class builders similar to the @dataclass  decorator and\ntyping.NamedTuple .\n“Metaclasses in the Real world”  is a new section with some high level\nconsiderations about the applicability of metaclasses.2\nT I P\nSome of the best refactorings are removing code made redundant by newer and simpler\nways of solving the same problems. This applies to production code as well as books.\nW e’ll get started by reviewing attributes and methods defined in the Python\nData Model for all classes.\nC l a s s e s  a s  O b j e c t s\nLike most program entities in Python, classes are also objects. Every class\nhas a number of attributes defined in the Python Data Model, documented\nin “4.13. Special Attributes”  of the “Built-in T ypes” chapter in the Library\nRefer ence . Three of those attributes appeared several times in the book\nalready: __class__ , __name__ , and __mro__ . Other class standard\nattributes are:\ncls.__bases__\nThe tuple of base classes of the class.\ncls.__qualname__\nThe qualified name of a class or function, which is a dotted path from\nthe global scope of the module to the class definition. This is relevant\nwhen the class is defined inside another class. For example, in a Django\nmodel class such as Ox , there is an inner class called Meta . The\n__qualname__  of Meta  is Ox.Meta , but its __name__  is just\nMeta . The specification for this attribute is PEP-3155 — Qualified\nname for classes and functions .\ncls.__subclasses__()\nThis method returns a list of the immediate subclasses of the class. The\nimplementation uses weak references to avoid circular references\nbetween the superclass and its subclasses—which hold a strong\nreference to the superclasses in their __bases__  attribute. The\nmethod lists subclasses currently in memory .\ncls.mro()\nThe interpreter calls this method when building a class to obtain the\ntuple of superclasses that is stored in the __mro__  attribute of the\nclass. A metaclass can override this method to customize the method\nresolution order of the class under construction.\nT I P\nNone of the attributes mentioned in this section are listed by the dir(…)  function.\nNow , if a class is an object, what is the class of a class?\nt y p e :  T h e  B u i l t - i n  C l a s s  F a c t o r y\nW e usually think of type  as a function that returns the class of an object,\nbecause that’ s what type(my_object)  does: it returns\nmy_object.__class__ .\nHowever , type  is a class that creates a new class when invoked with three\nar guments.\nConsider this simple class:\nclass MyClass(MySuperClass , MyMixin): \n    x = 42 \n \n    def x2(self): \n        return self.x * 2\nUsing the type  constructor , you can create MyClass  at runtime with this\ncode:\nMyClass = type('MyClass' , (MySuperClass , MyMixin), \n               {'x': 42, 'x2': lambda self: self.x * 2})\nThat type  call is functionally equivalent to the previous class\nMyClass…  block statement.\nWhen Python reads a class  statement, it calls type  to build the class\nobject with these parameters:\nname\nThe identifier that appears after the class  keyword; e.g.: MyClass .\nbases\nThe tuple of superclasses given in parenthesis after the class identifier ,\nor (object,)  if superclasses are not mentioned in the class\nstatement.\ndict\nA mapping of attribute names to values. Callables become methods;\nother values become class attributes.\nN O T E\nThe type  constructor accepts optional keyword ar guments. That’ s an advanced feature\nnot covered in this book.\nThe type  class is a metaclass : a class that builds classes. In other words,\ninstances of the type  class are classes. The standard library provides a few\nother metaclasses, but type  is the default.\n>>> type(7) \n<class 'int'> \n>>> type(int) \n<class 'type'> \n>>> type(OSError) \n<class 'type'>",6616
340-A Class Factory Function.pdf,340-A Class Factory Function,">>> class Whatever : \n...     pass \n... \n>>> type(Whatever ) \n<class 'type'>\nW e’ll build custom metaclasses in “Metaclasses 101” .\nNext, we’ll use the type  built-in to make a function that builds classes.\nA  C l a s s  F a c t o r y  F u n c t i o n\nThe standard library has a class factory function that appears several times\nin this book: collections.namedtuple . In Chapter 5  we also saw\ncollections.NamedTuple  and @dataclass . All of these class\nbuilders leverage techniques covered in this chapter .\nW e’ll start with a super simple factory for classes of mutable objects—the\nsimplest possible replacement for @dataclass.\nSuppose I’m writing a pet shop application and I want to store data for dogs\nas simple records. But I don’ t want to write boilerplate like this:\nclass Dog: \n    def __init__ (self, name, weight, owner): \n        self.name = name \n        self.weight = weight \n        self.owner = owner\nBoring… each field name appears three times, and that boilerplate doesn’ t\neven buy us a nice repr :\n>>> rex = Dog('Rex', 30, 'Bob') \n>>> rex \n<__main__.Dog object at 0x2865bac>\nT aking a hint from collections.namedtuple , let’ s create a\nrecord_factory  that creates simple classes like Dog  on the fly .\nExample 25-1  shows how it should work.\nExample 25-1. T esting r ecor d_factory , a simple class factory\n    >>> Dog = record_factory ('Dog', 'name weight owner ')  \n \n    >>> rex = Dog('Rex', 30, 'Bob') \n    >>> rex  \n \n    Dog(name='Rex', weight=30, owner='Bob') \n    >>> name, weight, _ = rex  \n \n    >>> name, weight \n    ('Rex', 30) \n    >>> ""{2}'s dog weighs {1}kg "".format(*rex)  \n \n    ""Bob's dog weighs 30kg "" \n    >>> rex.weight = 32  \n \n    >>> rex \n    Dog(name='Rex', weight=32, owner='Bob') \n    >>> Dog.__mro__  \n \n    (<class 'factories .Dog'>, <class 'object'>)\nFactory can be called like namedtuple : class name, followed by\nattribute names separated by spaces in a single strings.\nNice repr .\nInstances are iterable, so they can be conveniently unpacked on\nassignment…\n…or when passing to functions like format .\nA record instance is mutable.\nThe newly created class inherits from object —no relationship to our\nfactory .\nThe code for record_factory  is in Example 25-2 .\nExample 25-2. r ecor d_factory .py: a simple class factory\nfrom typing import Union, Any \nfrom collections.abc  import Iterable , Iterator  \n \nFieldNames  = Union[str, Iterable [str]]  \n \n \ndef record_factory (cls_name : str, field_names : FieldNames ) -> \ntype[tuple]:  \n 3\n \n    slots = parse_identifiers (field_names )  \n \n \n    def __init__ (self, *args, **kwargs) -> None:  \n \n        attrs = dict(zip(self.__slots__ , args)) \n        attrs.update(kwargs) \n        for name, value in attrs.items(): \n            setattr(self, name, value) \n \n    def __iter__ (self) -> Iterator [Any]:  \n \n        for name in self.__slots__ : \n            yield getattr(self, name) \n \n    def __repr__ (self):  \n \n        values = ', '.join( \n            '{}={!r}'.format(*i) for i in zip(self.__slots__ , self) \n        ) \n        cls_name  = self.__class__ .__name__  \n        return f'{cls_name}({values}) ' \n \n    cls_attrs  = dict(  \n \n        __slots__ =slots, \n        __init__ =__init__ , \n        __iter__ =__iter__ , \n        __repr__ =__repr__ , \n    ) \n \n    return type(cls_name , (object,), cls_attrs )  \n \n \n \ndef parse_identifiers (names: FieldNames ) -> tuple[str, ...]: \n    if isinstance (names, str): \n        names = names.replace(',', ' ').split()  \n \n    if not all(s.isidentifier () for s in names): \n        raise ValueError ('names must all be valid identifiers ') \n    return tuple(names)\nUser can provide field names as a single string or an iterable of strings.\nAccept ar guments like the first two of collections.namedtuple ;\nreturn a type —i.e. a class—that behaves like a tuple .\nBuild a tuple of attribute names, this will be the __slots__  attribute\nof the new class.\nThis function will become the __init__  method in the new class. It\naccepts positional and/or keyword ar guments. There’ s no point in\nadding type hints to __init__ , because the actual types are Any .\nY ield the field values in the order given by __slots__ .\nProduce the nice repr , iterating over __slots__  and self .\nAssemble dictionary of class attributes.\nBuild and return the new class, calling the type  constructor .\nConvert names  separated by spaces or commas to list of str .\nIn summary , the last line of record_factory  in Example 25-2  builds a\nclass named by the value of cls_name , with object  as its single\nimmediate base class and with a namespace loaded with __slots__ ,\n__init__ , __iter__ , and __repr__ , of which the last three are\ninstance methods.\nW e could have named the __slots__  class attribute anything else, but\nthen we’d have to implement __setattr__  to validate the names of\nattributes being assigned, because for our record-like classes we want the\nset of attributes to be always the same and in the same order . However ,\nrecall that the main feature of __slots__  is saving memory when you are\ndealing with millions of instances, and using __slots__  has some\ndrawbacks, discussed in “Saving Memory with __slots__ ” .\nW A R N I N G\nInstances of classes created by record_factory  are not serializable—that is, they\ncan’ t be exported with the dump  function from the pickle  module. Solving this\nproblem is beyond the scope of this example, which aims to show the type  class in\naction in a simple use case. For the full solution, study the source code for\ncollections.namedtuple ; search for the word “pickling.”",5702
341-Introducing __init_subclass__.pdf,341-Introducing __init_subclass__,"Now let’ s see how to emulate more modern class builders like\ntyping.NamedTuple , which takes a user -defined class written as a\nclass  statement, and automatically enhances it with more functionality .\nI n t r o d u c i n g  __init_subclass__\nBoth __init_subclass__  and __set_name__  were proposed in\nPEP 487—Simpler customisation of class cr eation . W e saw the\n__set_name__  special method for descriptors for the first time in\n“LineItem T ake #4: Automatic Storage Attribute Names” . Now let’ s study\n__init_subclass__ .\nIn Chapter 5 , we saw that typing.NamedTuple  and @dataclass  let\nprogrammers use the class  statement to specify attributes for a new class,\nwhich is then enhanced by the class builder with the automatic addition of\nessential methods like __init__ , __repr__ , __eq__  etc.\nBoth of these class builders read type hints in the user ’ s class  statement\nto enhance the class. Those type hints also allow static type checkers to\nvalidate code that sets or gets those attributes. However , NamedTuple  and\n@dataclass  do not take advantage of the type hints for attribute\nvalidation at runtime. The Checked  class in next example does.\nN O T E\nIt is not possible to support every conceivable static type hint for runtime type checking,\nwhich is probably why typing.NamedTuple  and @dataclass  don’ t even try it.\nHowever , some types that are also concrete classes can be used with Checked . This\nincludes simple types often used for field contents, such as str , int , float  and\nbool , as well as lists of those types.\nExample 25-3  shows how to use Checked  to build a Movie  class.\nExample 25-3. initsub/checkedlib.py: doctest for cr eating a Movie\nsubclass of Checked .\n    >>> class Movie(Checked):  \n \n    ...     title: str  \n \n    ...     year: int \n    ...     box_office : float \n    ... \n    >>> movie = Movie(title='The Godfather ', year=1972, \nbox_office =137)  \n \n    >>> movie.title \n    'The Godfather ' \n    >>> movie  \n \n    Movie(title='The Godfather ', year=1972, box_office =137.0)\nMovie  inherits from Checked —the subject of this section.\nEach attribute is annotated with a constructor . Here I used built-in types.\nMovie  instances must be created using keyword ar guments.\nIn return, you get a nice __repr__ .\nThe constructors used as the attribute type hints may be any callable that\ntakes zero or one ar gument and returns a value suitable for the intended\nfield type, or rejects the ar gument by raising TypeError  or\nValueError .\nUsing built-in types for the annotations in Example 25-3  means the values\nmust be acceptable by the constructor of the type. For int , this means any\nx  such that int(x)  returns an int . For str , anything goes at runtime,\nbecause str(x)  works with any x  in Python.\nWhen called with no ar guments, the constructor should return a default\nvalue of its type.\nThis is standard behavior for Python’ s built-in constructors:\n>>> int(), float(), bool(), str(), list(), dict(), set() \n(0, 0.0, False, '', [], {}, set())\nIn a Checked  subclass like Movie , missing parameters create instances\nwith default values returned by the field constructors. For example:4\n5\n    >>> Movie(title='Life of Brian' ) \n    Movie(title='Life of Brian' , year=0, box_office =0.0)\nThe constructors are used for validation during instantiation and when an\nattribute is set directly on an instance:\n    >>> blockbuster  = Movie(title='Avatar' , year=2009, \nbox_office ='billions' ) \n    Traceback  (most recent call last): \n      ... \n    TypeError : 'billions'  is not compatible  with box_office :float \n    >>> movie.year = 'MCMLXXII'  \n    Traceback  (most recent call last): \n      ... \n    TypeError : 'MCMLXXII'  is not compatible  with year:int\nCHECKED  S U B C L A S S E S  A N D  S T A T I C  T Y P E\nC H E C K I N G\nIn a .py  source file with a movie  instance of Movie  as defined in Example 25-3 , Mypy\nflags this assignment as a type error:\nmovie.year = 'MCMLXXII'\nHowever , Mypy can’ t detect type errors in this constructor call:\nblockbuster  = Movie(title='Avatar' , year='MMIX')\nThat’ s because Movie  inherits Checked.__init__ , and the signature of that\nmethod must accept any keyword ar guments, to support arbitrary user -defined classes.\nOn the other hand, if you declare a Checked  subclass field with the type hint\nlist[float] , Mypy can flag assignments of lists with incompatible contents, but\nChecked  will ignore the type parameter and treat that the same as list .\nNow let’ s look at the implementation of checkedlib.py . The first class\nis the Field  descriptor:\nExample 25-4. initsub/checkedlib.py: the Field  descriptor class.\nfrom collections.abc  import Callable   \n \nfrom typing import Any, NoReturn , get_type_hints  \n \n \nclass Field: \n    def __init__ (self, name: str, constructor : Callable ) -> None:  \n \n        if not callable (constructor ) or constructor  is type(None):  \n \n            raise TypeError (f'{name!r} type hint must be callable ') \n        self.name = name \n        self.constructor  = constructor  \n \n    def __set__(self, instance : Any, value: Any) -> None: \n        if value is ...:  \n \n            value = self.constructor () \n        else: \n            try: \n                value = self.constructor (value)  \n \n            except (TypeError , ValueError ) as e:  \n \n                type_name  = self.constructor .__name__  \n                msg = f'{value!r} is not compatible with  \n{self.name}:{type_name} ' \n                raise TypeError (msg) from e \n        instance .__dict__ [self.name] = value  \nRecall that since Python 3.9, the Callable  type for annotations is the\nABC in collections.abc , and not the deprecated\ntyping.Callable .\nThis is a minimal Callable  type hint; the constructor  parameter\ntype and return type are Any , so we can omit them.\nFor runtime checking, we use the callable  built-in.  The test against\ntype(None)  is necessary because Python reads None  in a type as\nNoneType , the class of None  (therefore callable) but a useless\nconstructor that only returns None .\nIf Checked.__init__  sets the value  as ...  (the Ellipsis\nbuilt-in object), we call the constructor  with no ar guments.6\nOtherwise, call the constructor  with the given value .\nIf constructor  raises either of these exceptions, we raise\nTypeError  with a helpful message including the names of the field\nand constructor; e.g. 'MMIX' is not compatible with\nyear:int .\nIf no exceptions were raised, the value  is stored in the\ninstance.__dict__ .\nIn __set__  we need to catch TypeError  and ValueError  because\nbuilt-in constructors may raise either of them, depending on the ar gument.\nFor example: float(None)  raises TypeError , but float('A')\nraises ValueError . On the other hand, float('8')  raises no error and\nreturns 8.0 . I hereby declare that this is feature and not a bug of this toy\nexample.\nT I P\nIn “LineItem T ake #4: Automatic Storage Attribute Names”  we saw the handy\n__set_name__  special method for descriptors. W e don’ t need it in the Field  class\nbecause the descriptors are not instantiated in client source code; the user declares types\nthat are constructors, as we saw in the Movie  class ( Example 25-3 ). Instead, the\nField  descriptor instances are created at runtime by the\nChecked.__init_subclass__  method which we’ll see in Example 25-5 .\nNow let’ s focus on the Checked  class. I split it in two listing: Example 25-\n5  shows the top of the class, which includes the most important methods in\nthis example. The remaining methods are in Example 25-6 .\nExample 25-5. initsub/checkedlib.py: the most important methods of the\nChecked  class.\nclass Checked: \n    @classmethod  \n    def _fields(cls) -> dict[str, type]:  \n \n        return get_type_hints (cls) \n \n    def __init_subclass__ (subclass ) -> None:  \n \n        super().__init_subclass__ ()           \n  \n        for name, constructor  in subclass ._fields().items():   \n \n            setattr(subclass , name, Field(name, constructor ))  \n \n \n    def __init__ (self, **kwargs: Any) -> None: \n        for name in self._fields():             \n  \n            value = kwargs.pop(name, ...)       \n  \n            setattr(self, name, value)          \n  \n        if kwargs:                              \n  \n            self.__flag_unknown_attrs (*kwargs)  \nI wrote this class method to hide the use of\ntyping.get_type_hints  from the rest of the class. As explained\nin “Problems with Annotations at Runtime” , that function doesn’ t\nalways work—but it does handle the simple types the Checked  and\nField  classes are designed to handle.\n__init_subclass__  is called when a subclass of the current\nsubclass is defined. It gets that new subclass as its first ar gument—\nwhich is why I named the ar gument subclass  instead of the usual\ncls . For more on this, see “__init_subclass__  is not a typical\nclass method” .\nsuper().__init_subclass__()  should be invoked.\nIterate over each field name  and constructor …\n…creating an attribute on subclass  with that name  bound to a\nField  descriptor parameterized with name  and constructor .\nFor each name  in the class fields…\nGet the corresponding value  from kwargs  and remove it from\nkwargs . Using ... —the Ellipsis  object—as default allows us to\ndistinguish between ar guments given the value None  from ar guments\nthat were not given.\nThis setattr  call triggers Checked.__setattr__ , shown in\nExample 25-6 .\nIf there are remaining items in kwargs , their names do not match any\nof the declared fields, and __init__  will fail.\nThe error is reported by __flag_unknown_attrs , listed in\nExample 25-6 . It takes a *names  ar gument with the unknown attribute\nnames. I used a single asterisk in *kwargs  to pass its keys as a\nsequence of ar guments.\n__INIT_SUBCLASS__  I S  N O T  A  T Y P I C A L  C L A S S\nM E T H O D\nThe @classmethod  decorator is never used with\n__init_subclass__ , but that doesn’ t mean much, because the\n__new__  special method behaves as a class method even without\n@classmethod . The first ar gument that Python passes to\n__init_subclass__  is a class. However , it is never the class where\n__init_subclass__  is implemented: it is a newly defined subclass\nof that class. That’ s unlike __new__  and every other class method that\nI know about. Therefore, I think __init_subclass__  is not a class\nmethod in the usual sense, and it is misleading to name the first\nar gument cls . The __init_suclass__  documentation  names the\nar gument cls  but explains: “…called whenever the containing class is\nsubclassed. cls  is then the new subclass.”\nNow let’ s see the remaining methods of the Checked  class, continuing\nfrom Example 25-5 . Note that I prepended _  to the _fields  and\n_asdict  method names for the same reason the7\ncollections.namedtuple  API does: to reduce the chance of name\nclashes with user -defined field names.\nExample 25-6. initsub/checkedlib.py: r emaining methods of the Checked\nclass.\n    def __setattr__ (self, name: str, value: Any) -> None:  \n \n        if name in self._fields():              \n  \n            cls = self.__class__  \n            descriptor  = getattr(cls, name) \n            descriptor .__set__(self, value)     \n \n        else:                                   \n  \n            self.__flag_unknown_attrs (name) \n \n    def __flag_unknown_attrs (self, *names: str) -> NoReturn :  \n \n        plural = 's' if len(names) > 1 else '' \n        extra = ', '.join(f'{name!r} ' for name in names) \n        cls_name  = repr(self.__class__ .__name__ ) \n        raise AttributeError (f'{cls_name} object has no  \nattribute{plural} {extra} ') \n \n    def _asdict(self) -> dict[str, Any]:  \n \n        return { \n            name: getattr(self, name) \n            for name, attr in self.__class__ .__dict__ .items() \n            if isinstance (attr, Field) \n        } \n \n    def __repr__ (self) -> str:  \n \n        kwargs = ', '.join( \n            f'{key}={value!r} ' for key, value in \nself._asdict().items() \n        ) \n        return f'{self.__class__.__name__}({kwargs}) '\nIntercept all attempts to set an instance attribute. This is needed to\nprevent setting an unknown attribute.\nIf the attribute name  is known, fetch the corresponding descriptor .\nUsually we don’ t need to call the descriptor __set__  explicitly; it was\nnecessary in this case because __setattr__  intercepts all attempts to",12572
342-Enhancing Classes with a Class Decorator.pdf,342-Enhancing Classes with a Class Decorator,"set an attribute on the instance, including in the presence of an\noverriding descriptor such as Field .\nOtherwise, the attribute name  is unknown, and an exception will be\nraised by __flag_unknown_attrs .\nBuild a helpful error message listing all unexpected ar guments and raise\nAttributeError . This is a rare example of the NoReturn  special\ntype, covered in “NoReturn ” .\nCreate a dict  from the attributes of a Movie  object. I’d call this\nmethod _as_dict , but I followed the convention started by the\n_asdict  method in collections.namedtuple .\nImplementing a nice __repr__  is the main reason for having\n_asdict  in this example.\nThe Checked  example illustrates how to handle overriding descriptors\nwhen implementing __setattr__  to block arbitrary attribute setting\nafter instantiation. It is debatable whether implementing __setattr__  is\nworthwhile in this example. W ithout it, setting movie.director =\n'Greta Gerwig'  would succeed, but the director  attribute would\nnot be checked in any away , and would not appear in the __repr__  nor be\nincluded in the dict  returned by _asdict —both defined in Example 25-\n6 .\nIn r ecor d_factory .py  ( Example 25-2 ) I solved this issue using the\n__slots__  class attribute. However , this simpler solution is not viable in\nthis case, as explained next.\nWhy __init_subclass__  cannot configure __slots__\nThe __slots__  attribute is only ef fective if it is one of the entries in the\nclass namespace passed to type.__new__ . Adding __slots__  to an\nexisting class has no ef fect. Python invokes __init_subclass__  only8\nafter the class is built—by then it’ s too late to configure __slots__ . A\nclass decorator can’ t configure __slots__  either , because it is applied\neven later than __init_subclass__ . W e’ll explore these timing issues\nin “What Happens When: Import T ime V ersus Runtime” .\nT o configure __slots__  at runtime, your own code must build the class\nnamespace passed as the last ar gument of type.__new__ . T o do that,\nyou can write a class factory function, like _record_factory.py_ , or\nyou take the nuclear option and implement a metaclass. W e will see how to\ndynamically configure __slots__  in “Metaclasses 101” .\nBefore PEP 487  simplified the customisation of class creation with\n__init_subclass__  in Python 3.7, similar functionality had to be\nimplemented using a class decorator . That’ s the focus of the next section.\nE n h a n c i n g  C l a s s e s  w i t h  a  C l a s s  D e c o r a t o r\nA class decorator is a callable that behaves similarly to a function\ndecorator: it gets the decorated class as an ar gument, and must return a class\nwhich will replace the decorated class. Class decorators often return the\ndecorated class itself, after injecting more methods in it via attribute\nassignment.\nProbably the most common reason to chose a class decorator over the\nsimpler __init_subclass__  is to avoid interfering with other class\nfeatures such as inheritance and metaclasses.\nIn this section, we’ll study checkeddeco.py , which provides the same\nservice as checkedlib.py , but using a class decorator . As usual, we’ll start by\nlooking at an usage example, extracted from the doctests in checkeddeco.py .\nExample 25-7. checkeddeco.py: cr eating a Movie  class decorated with\n@checked .\n    >>> @checked  \n    ... class Movie: \n    ...     title: str \n    ...     year: int 9\n    ...     box_office : float \n    ... \n    >>> movie = Movie(title='The Godfather' , year=1972, \nbox_office =137) \n    >>> movie.title \n    'The Godfather'  \n    >>> movie \n    Movie(title='The Godfather' , year=1972, box_office =137.0)\nThe only dif ference between Example 25-7  and Example 25-3  is the way\nthe Movie  class is declared: it is decorated with @checked  instead of\nsubclassing Checked . Otherwise, the external behavior is the same,\nincluding the type validation and default value assignments shown after\nExample 25-3  in “Introducing __init_subclass__ ” .\nNow let’ s look at the implementation of checkeddeco.py . The imports and\nField  class are the same as in checkedlib.py , listed in Example 25-4 .\nThere is no other class, only functions in checkeddeco.py .\nThe logic previously implemented in __init_subclass__  is now part\nof the checked  function—the class decorator listed in Example 25-8 .\nExample 25-8. checkeddeco.py: the class decorator .\ndef checked(cls: type) -> type:  \n \n    for name, constructor  in _fields(cls).items():    \n \n        setattr(cls, name, Field(name, constructor ))  \n \n \n    cls._fields = classmethod (_fields)  # type: ignore  \n  \n \n    instance_methods  = (  \n \n        __init__ , \n        __repr__ , \n        __setattr__ , \n        _asdict, \n        __flag_unknown_attrs , \n    ) \n    for method in instance_methods :  \n \n        setattr(cls, method.__name__ , method) \n \n    return cls  \nRecall that classes are instances of type . These type hints strongly\nsuggest this is a class decorator: it takes a class, and returns a class.\n_fields  is a module-level function defined later in the module (in\nExample 25-9 ).\nReplacing each attribute returned by _fields  with a Field\ndescriptor instance is what __init_subclass__  did in Example 25-\n5 . Here there is more work to do…\nBuild a class method from _fields , and add it to the decorated class.\nThe type: ignore  comment is needed because Mypy complains\nthat type  has no _fields  attribute.\nModule-level functions that will become instance methods of the\ndecorated class.\nAdd each of the instance_methods  to cls .\nReturn the decorated cls , fulfilling the essential contract of a class\ndecorator .\nEvery top-level function in checkeddeco.py  is prefixed with an underscore,\nexcept the checked  decorator . This naming convention makes sense for a\ncouple of reasons:\n1 . checked  is part of the public interface of the checkeddeco.py\nmodule, but the other functions are not.\n2 . The functions in Example 25-9  will be injected in the decorated\nclass, and the leading _  reduces the chance of naming conflicts\nwith user -defined attributes and methods of the decorated class.\nThe rest of checkeddeco.py  is listed in Example 25-9 . Those module-level\nfunctions have the same code as the corresponding methods of the\nChecked  class of checkedlib.py . They were explained in Example 25-5\nand Example 25-6 .\nNote that the _fields  function does double duty in checkeddeco.py . It is\nused as a regular function in the first line of the checked  decorator , and it\nwill also be injected as a class method of the decorated class.\nExample 25-9. checkeddeco.py: the methods to be injected in the decorated\nclass.\ndef _fields(cls: type) -> dict[str, type]: \n    return get_type_hints (cls) \n \ndef __init__ (self: Any, **kwargs: Any) -> None: \n    for name in self._fields(): \n        value = kwargs.pop(name, ...) \n        setattr(self, name, value) \n    if kwargs: \n        self.__flag_unknown_attrs (*kwargs) \n \ndef __setattr__ (self: Any, name: str, value: Any) -> None: \n    if name in self._fields(): \n        cls = self.__class__  \n        descriptor  = getattr(cls, name) \n        descriptor .__set__(self, value) \n    else: \n        self.__flag_unknown_attrs (name) \n \ndef __flag_unknown_attrs (self: Any, *names: str) -> NoReturn : \n    plural = 's' if len(names) > 1 else '' \n    extra = ', '.join(f'{name!r}'  for name in names) \n    cls_name  = repr(self.__class__ .__name__ ) \n    raise AttributeError (f'{cls_name} has no attribute{plural}  \n{extra}' ) \n \ndef _asdict(self: Any) -> dict[str, Any]: \n    return { \n        name: getattr(self, name) \n        for name, attr in self.__class__ .__dict__ .items() \n        if isinstance (attr, Field) \n    } \n \ndef __repr__ (self: Any) -> str: \n    kwargs = ', '.join( \n        f'{key}={value!r}'  for key, value in self._asdict().items() \n    ) \n    return f'{self.__class__.__name__}({kwargs})'",7989
343-Evaluation Time Experiments.pdf,343-Evaluation Time Experiments,"The checkeddeco.py  module implements a simple but usable class\ndecorator . Python’ s @dataclass  does a lot more. It supports many\nconfiguration options, adds more methods to the decorated class, handles or\nwarns about conflicts with user -defined methods in the decorated class, and\neven traverses the __mro__  to collect user -defined attributes declared in\nthe superclasses of the decorated class. The source code  of the\ndataclasses  package in Python 3.9 is more than 1200 lines long.\nFor metaprogramming classes, we must be aware of when the Python\ninterpreter evaluates each block of code during the construction of a class.\nThis is covered next.\nW h a t  H a p p e n s  W h e n :  I m p o r t  T i m e  V e r s u s\nR u n t i m e\nPython programmers talk about “import time” versus “runtime” but the\nterms are not strictly defined and there is a gray area between them.\nAt import time, the interpreter:\n1 . Parses the source code of a .py  module in one pass from top to\nbottom. This is when SyntaxError  may occur .\n2 . Compiles the bytecode to be executed.\n3 . Executes the top-level code of the compiled module.\nIf there is an up-to-date .pyc  file available in the local __pycache__ ,\nparsing and compiling are skipped because the bytecode is ready to run.\nAlthough parsing and compiling are definitely “import time” activities,\nother things may happen at that time, because almost every statement in\nPython is executable in the sense that they potentially run user code and\nmay change the state of the user program.\nIn particular , the import  statement is not merely a declaration  but it\nactually runs all the top-level code of a module when it is imported for the10\nfirst time in the process—further imports of the same module will use a\ncache, and then the only ef fect will be binding the imported objects to\nnames in the client module. That top-level code may do anything, including\nactions typical of “runtime”, such as writing to a log or connecting to a\ndatabase.  That’ s why the border between “import time” and “runtime” is\nfuzzy: the import  statement can trigger all sorts of “runtime” behavior .\nThis is all rather abstract and subtle, so let’ s do some experiments to see\nwhat happens when.\nEvaluation T ime Experiments\nConsider an evaldemo.py  script which uses a class decorator , a descriptor ,\nand a class builder based on __init_subclass__ , all defined in a\nbuilderlib.py  module. The modules have several print  calls to show what\nhappens under the covers. Otherwise, they don’ t perform anything useful.\nThe goal of these experiments is to observe the order in which these print\ncalls happen.\nW A R N I N G\nApplying a class decorator and a class builder with __init_subclass__  together in\nsingle class is likely a sign of overengineering or desperation. This unusual combination\nis useful in these experiments to show the timing of the changes that a class decorator\nand __init_subclass__  can apply to a class.\nLet’ s start by checking out builderlib.py , split in two parts: Example 25-10\nand Example 25-1 1 .\nExample 25-10. builderlib.py: top of the module\nprint('@ builderlib module start ') \n \nclass Builder:  \n \n    print('@ Builder body ') \n \n    def __init_subclass__ (cls):  \n \n        print(f'@ Builder.__init_subclass__( {cls!r})') 11\n \n        def inner_0(self):  \n \n            print(f'@ SuperA.__init_subclass__:inner_0( {self!r} )') \n \n        cls.method_a  = inner_0 \n \n    def __init__ (self): \n        super().__init__ () \n        print(f'@ Builder.__init__( {self!r} )') \n \n \ndef deco(cls):  \n \n    print(f'@ deco({cls!r})') \n \n    def inner_1(self):  \n \n        print(f'@ deco:inner_1( {self!r} )') \n \n    cls.method_b  = inner_1 \n    return cls  \nThis is a class builder to implement…\n__init_subclass__ .\nDefine a function to be added to the subclass in the assignment below .\nA class decorator .\nFunction to be added to the decorated class.\nReturn the class received as ar gument.\nContinuing with builderlib.py …\nExample 25-1 1. builderlib.py: bottom of the module\nclass Descriptor :  \n \n    print('@ Descriptor body ') \n \n    def __init__ (self):  \n \n        print(f'@ Descriptor.__init__( {self!r} )') \n \n    def __set_name__ (self, owner, name):  \n \n        args = (self, owner, name) \n        print(f'@ Descriptor.__set_name__ {args!r} ') \n \n    def __set__(self, instance , value):  \n \n        args = (self, instance , value) \n        print(f'@ Descriptor.__set__ {args!r} ') \n \n    def __repr__ (self): \n        return '<Descriptor instance> ' \n \n \nprint('@ builderlib module end ')\nA descriptor class to demonstrate when…\n…a descriptor instance is created, and when…\n…__set_name__  will be invoked during the owner  class\nconstruction.\nLike the other methods, this __set__  doesn’ t do anything except\ndisplay its ar guments.\nIf you import builderlib.py  in the Python console, this is what you get:\n>>> import builderlib  \n@ builderlib module start  \n@ Builder body  \n@ Descriptor body  \n@ builderlib module end\nNote that the lines printed by builderlib.py  are prefixed with @ .\nNow let’ s turn to evaldemo.py , which will trigger special methods in\nbuilderlib .\nExample 25-12. evaldemo.py: script to experiment with builderlib.py .\n#!/usr/bin/env python3  \n \nfrom builderlib  import Builder, deco, Descriptor  \n \nprint('# evaldemo module start ') \n \n@deco  \n \nclass Klass(Builder):  \n \n    print('# Klass body ') \n \n    attr = Descriptor ()  \n \n \n    def __init__ (self): \n        super().__init__ () \n        print(f'# Klass.__init__( {self!r} )') \n \n    def __repr__ (self): \n        return '<Klass instance> ' \n \n \ndef main():  \n \n    obj = Klass() \n    obj.method_a () \n    obj.method_b () \n    obj.attr = 999 \n \nif __name__  == '__main__ ': \n    main() \n \nprint('# evaldemo module end ')\nApply decorator .\nSubclass Builder  to trigger its __init_subclass__ .\nInstantiate descriptor .\nThis will only be called if the module is run as the main program.\nThe print  calls in evaldemo.py  show a #  prefix. If you open the console\nagain and import evaldemo.py , this is the output:\nExample 25-13. Console experiment with evaldemo.py .\n>>> import evaldemo  \n@ builderlib module start  \n  \n@ Builder body  \n@ Descriptor body  \n@ builderlib module end  \n# evaldemo module start  \n# Klass body  \n  \n@ Descriptor.__init__(<Descriptor instance>)  \n  \n@ Descriptor.__set_name__(<Descriptor instance>,  \n      <class 'evaldemo.Klass'>, 'attr')                \n  \n@ Builder.__init_subclass__(<class 'evaldemo.Klass'>)  \n  \n@ deco(<class 'evaldemo.Klass'>)  \n  \n# evaldemo module end\nThe top 4 lines are the result of from builderlib import…  .\nThey will not appear if you didn’ t close the console after the previous\nexperiment, because builderlib.py  is already loaded.\nThis signals that Python started reading the body of Klass . At this\npoint, the class object does not exist yet.\nThe descriptor instance is created and bound to attr  in the namespace\nthat Python will pass to the default class object constructor:\ntype.__new__ .\nAt this point, Python’ s built-in type.__new__  has created the\nKlass  object and calls __set_name__  on each descriptor instance\nof descriptor classes that provide that method, passing Klass  as the\nowner  ar gument.\ntype.__new__  then calls __init_subclass__  on the superclass\nof Klass , passing Klass  as the single ar gument.\nWhen type.__new__  returns the class object, Python applies the\ndecorator . In this example, the class returned by deco  is bound to\nKlass  in the module namespace.\nThe implementation of type.__new__  is written in C. The behavior I\njust described is documented in the Cr eating the class object  section of\nPython’ s Data Model  reference.\nNote that the main()  function of evaldemo.py  ( Example 25-12 ) was not\nexecuted in the console session ( Example 25-13 ), therefore no instance of\nKlass  was created. All the action we saw was triggered by “import time”\noperations: importing builderlib  and defining Klass .\nIf you run evaldemo.py  as a script, you will see the same output as\nExample 25-13  with extra lines right before the last. The extra lines are the\nresult of running main() :\nExample 25-14. Running evaldemo.py  as a pr ogram.\n$ ./evaldemo.py  \n[... 9 lines omitted ...]  \n@ deco(<class '__main__.Klass'>)  \n  \n@ Builder.__init__(<Klass instance>)  \n  \n# Klass.__init__(<Klass instance>)  \n@ SuperA.__init_subclass__:inner_0(<Klass instance>)  \n  \n@ deco:inner_1(<Klass instance>)  \n  \n@ Descriptor.__set__(<Descriptor instance>, <Klass instance>, 999)   \n \n# evaldemo module end\nThe top 10 lines—including this one—are the same shown in\nExample 25-13 .\nT riggered by super().__init__()  in Klass.__init__ .\nT riggered by obj.method_a()  in main ; method_a  was injected\nby SuperA.__init_subclass__ .\nT riggered by obj.method_b()  in main ; method_b  was injected\nby deco .\nT riggered by obj.attr = 999  in main .\nA base class with __init_subclass__  and a class decorator are\npowerful tools, but they are limited to working with a class already built by\ntype.__new__  under the covers. In the rare occasions when you need to\nadjust the ar guments passed to type.__new__ , you need a metaclass.\nThat’ s the final destination of this chapter—and this book.",9438
344-Metaclasses 101.pdf,344-Metaclasses 101,"M e t a c l a s s e s  1 0 1\n[Metaclasses] ar e deeper magic than 99% of users should ever worry\nabout. If you wonder whether you need them, you don’ t (the people who\nactually need them know with certainty that they need them, and don’ t\nneed an explanation about why).\n— T im Peters, Inventor of the timsort algorithm and\nprolific Python contributor\nA metaclass is a class factory . In contrast with record_factory  from\nExample 25-2 , a metaclass is written as a class. In other words, a metaclass\nis class whose instances are classes. Figure 25-1  depicts a metaclass using\nthe Mills & Gizmos Notation: a mill producing another mill.12\nFigur e 25-1. A metaclass is a class that builds classes\nConsider the Python object model: classes are objects, therefore each class\nmust be an instance of some other class. By default, Python classes are\ninstances of type . In other words, type  is the metaclass for most built-in\nand user -defined classes:\n>>> str.__class__  \n<class 'type'>  \n>>> from bulkfood_v5  import LineItem  \n>>> LineItem .__class__  \n<class 'type'>  \n>>> type.__class__  \n<class 'type'>\nT o avoid infinite regress, the class of type  is type , as the last line shows.\nNote that I am not saying that str  or LineItem  are subclasses of type .\nWhat I am saying is that str  and LineItem  are instances of type . They\nall are subclasses of object . Figure 25-2  may help you confront this\nstrange reality .\nFigur e 25-2. Both diagrams ar e true. The left one emphasizes that str , type , and LineItem  ar e\nsubclasses of object. The right one makes it clear that str , object, and LineItem  ar e instances\ntype , because they ar e all classes.\nN O T E\nThe classes object  and type  have a unique relationship: object  is an instance of\ntype , and type  is a subclass of object . This relationship is “magic”: it cannot be\nexpressed in Python because either class would have to exist before the other could be\ndefined. The fact that type  is an instance of itself is also magical.\nThe next snippet shows that the class of collections.Iterable  is\nabc.ABCMeta . Note that Iterable  is an abstract class, but ABCMeta\nis a concrete class—after all, Iterable  is an instance of ABCMeta :\n>>> from collections.abc  import Iterable  \n>>> Iterable .__class__  \n<class 'abc.ABCMeta'>  \n>>> import abc \n>>> from abc import ABCMeta \n>>> ABCMeta.__class__  \n<class 'type'>\nUltimately , the class of ABCMeta  is also type . Every class is an instance\nof type , directly or indirectly , but only metaclasses are also subclasses of\ntype . That’ s the most important relationship to understand metaclasses: a\nmetaclass, such as ABCMeta , inherits from type  the power to construct\nclasses. Figure 25-3  illustrates this crucial relationship.\nFigur e 25-3. Iterable is a subclass of object and an instance of ABCMeta. Both object and ABCMeta\nar e instances of type, but the key r elationship her e is that ABCMeta is also a subclass of type,\nbecause ABCMeta is a metaclass. In this diagram, Iterable is the only abstract class.\nThe important takeaway here is that metaclasses are subclasses of type ,\nand that’ s what makes them work as class factories. A metaclass can",3222
345-A Nice Metaclass Example.pdf,345-A Nice Metaclass Example,"customize its instances by implementing special methods, as the next\nsections demonstrate.\nHow a Metaclass Customizes a Class\nT o use a metaclass, it’ s critical to understand how __new__  works on any\nclass. This was discussed in “Flexible Object Creation with __new__” .\nThe same mechanics happen at a “meta” level when a metaclass is about to\ncreate a new instance, which is a class. Consider this declaration:\nclass Klass(SuperKlass , metaclass =MetaKlass ): \n    x = 42 \n    def __init__ (self, y): \n        self.y = y\nT o process that class  statement Python calls MetaKlass.__new__\nwith these ar guments:\nmeta_cls\nthe metaclass itself ( MetaKlass ), because __new__  works as class\nmethod;\ncls_name\nthe string Klass ;\nbases\nthe single-element tuple (SuperKlass,) —with more elements in\nthe case of multiple inheritance.\ncls_dict\na mapping like {x: 42, `__init__ : <function init  at\n0x1009c4040>}\nWhen you implement MetaKlass.__new__ , you can inspect and\nchange those ar guments before passing them to super().__new__ ,\nwhich will eventually call type.__new__  to create the new class object.\nAfter super().__new__  returns, you can also apply further processing\nto the newly created class before returning it to Python. Python then calls\nSuperKlass.__init_subclass__ , passing the class you created,\nand then applies a class decorator to it, if one is present. Finally , Python\nbinds the class object to its name in the surrounding namespace—usually\nthe global namespace of a module, if the class  statement was a top-level\nstatement.\nThe most common processing made in a metaclass __new__  is to add or\nreplace items in the cls_dict —the mapping that represents the\nnamespace of the class under construction. For instance, before calling\nsuper().__new__ , you can inject methods in the class under\nconstruction by adding functions to cls_dict . However , note that adding\nmethods can also be done after the class is built, which is why we were able\nto do it using __init_subclass__  or a class decorator .\nOne attribute that you must add to the cls_dict  before type.__new__\nruns is __slots__ , as discussed in “Why __init_subclass__\ncannot configure __slots__ ” . The __new__  method of a metaclass is\nthe ideal place to configure __slots__ . The next section shows how to\ndo that.\nA Nice Metaclass Example\nThe MetaBunch  metaclass presented here is a variation of the last\nexample in chapter 4 of Python in a Nutshell, 3r d Edition , by Alex Martelli,\nAnna Ravenscroft, and Steve Holden, written to run on Python 2.7 and\n3.5.  Assuming Python 3.6 or later , I was able to further simplify the code.\nFirst, let’ s see what the Bunch  base class provides:\n    >>> class Point(Bunch): \n    ...     x = 0.0 13\n    ...     y = 0.0 \n    ...     color = 'gray' \n    ... \n    >>> Point(x=1.2, y=3, color='green') \n    Point(x=1.2, y=3, color='green') \n    >>> p = Point() \n    >>> p.x, p.y, p.color \n    (0.0, 0.0, 'gray') \n    >>> p \n    Point()\nInstead of the type hints we use to name the fields in Checked  subclasses,\nBunch  subclasses assign values to class attributes, which then become the\ndefault values of the instance attributes. The generated __repr__  omits\nthe ar guments for attributes that are equal to the defaults.\nMetaBunch —the metaclass of Bunch —generates __slots__  for the\nnew class from the class attributes declared in the user ’ s class. This blocks\nthe instantiation and later assignment of undeclared attributes:\n    >>> Point(x=1, y=2, z=3) \n    Traceback  (most recent call last): \n      ... \n    AttributeError : 'Point' object has no attribute  'z' \n    >>> p = Point(x=21) \n    >>> p.y = 42 \n    >>> p \n    Point(x=21, y=42) \n    >>> p.flavor = 'banana'  \n    Traceback  (most recent call last): \n      ... \n    AttributeError : 'Point' object has no attribute  'flavor'\nNow let’ s dive into the elegant code of Metabunch :\nExample 25-15. metabunch/fr om3.6/bunch.py: MetaBunch  metaclass and\nBunch  class.\nclass MetaBunch (type):  \n \n    def __new__(meta_cls , cls_name , bases, cls_dict ):  \n \n \n        defaults  = {}  \n \n \n        def __init__ (self, **kwargs):  \n \n            for name, default in defaults .items():  \n \n                setattr(self, name, kwargs.pop(name, default)) \n            if kwargs:  \n \n                setattr(self, *kwargs.popitem()) \n \n        def __repr__ (self):  \n \n            rep = ', '.join(f'{name}={value!r} ' \n                            for name, default in defaults .items() \n                            if (value := getattr(self, name)) != \ndefault) \n            return f'{cls_name} ({rep})' \n \n        new_dict  = dict(__slots__ =[], __init__ =__init__ , \n__repr__ =__repr__ )  \n \n \n        for name, value in cls_dict .items():  \n \n            if name.startswith ('__') and name.endswith ('__'):  \n \n                if name in new_dict : \n                    raise AttributeError (f""Can't set {name!r}  in \n{cls_name!r} "") \n                new_dict [name] = value \n            else:  \n \n                new_dict ['__slots__ '].append(name) \n                defaults [name] = value \n        return super().__new__(meta_cls , cls_name , bases, new_dict )  \n \n \n \nclass Bunch(metaclass =MetaBunch ):  \n \n    pass\nT o create a new metaclass, inherit from type .\n__new__  works as a class method, but the class is a metaclass, so I like\nto name the first ar gument meta_cls  (mcs  is a common alternative).\nThe remaining three ar guments are the same as the three-ar gument\nsignature for calling type()  directly to create a class.\ndefaults  will hold a mapping of attribute names and their default\nvalues.\nThis will be injected into the new class.",5769
346-Metaclass Evaluation Time Experiment.pdf,346-Metaclass Evaluation Time Experiment,"Read the defaults  and set the corresponding instance attribute with a\nvalue popped from kwargs  or a default.\nIf there is still any item in kwargs , it is unexpected. W e believe in\nfailing fast  as best practice, so we don’ t want to silently ignore extra\nitems. A quick and ef fective solution is to pop one item from kwargs\nand try to set it on the instance, triggering an AttributeError  on\npurpose.\n__repr__  returns a string that looks like a constructor call—e.g.\nPoint(x=3) , omitting the keyword ar guments with default values.\nInitialize namespace for the new class.\nIterate over namespace of user ’ s class…\nIf a dunder name  is found, copy the item to the new class namespace,\nunless it’ s already there. This prevents users from overwriting\n__init__ , __repr__  and other attributes set by Python, such as\n__qualname__  and __module__ .\nIf not a dunder name , append to __slots__  and save its value  in\ndefaults .\nBuild and return the new class.\nProvide a base class, so users don’ t need to see MetaBunch .\nMetaBunch  works because it is able to configure __slots__  before\ncalling super().__new__  to build the final class. As usual when\nmetaprogramming, understanding the sequence of actions is key . Let’ s do\nanother evaluation time experiment, now with a metaclass.\nMetaclass Evaluation T ime Experiment\nThis is a variation of “Evaluation T ime Experiments” , adding a metaclass to\nthe mix. The builderlib.py  module is the same as before, but the main script\nis now evaldemo_meta.py , listed in Example 25-16 .\nExample 25-16. evaldemo_meta.py: experimenting with a metaclass.\n#!/usr/bin/env python3  \n \nfrom builderlib  import Builder, deco, Descriptor  \nfrom metalib import MetaKlass   \n \n \nprint('# evaldemo_meta module start ') \n \n@deco \nclass Klass(Builder, metaclass =MetaKlass ):  \n \n    print('# Klass body ') \n \n    attr = Descriptor () \n \n    def __init__ (self): \n        super().__init__ () \n        print(f'# Klass.__init__( {self!r} )') \n \n    def __repr__ (self): \n        return '<Klass instance> ' \n \n \ndef main(): \n    obj = Klass() \n    obj.method_a () \n    obj.method_b () \n    obj.method_c ()  \n \n    obj.attr = 999 \n \n \nif __name__  == '__main__ ': \n    main() \n \nprint('# evaldemo_meta module end ')\nImport MetaKlass .\nDeclare Klass  as subclass of Builder  and instance of MetaKlass .\nMethod injected by MetaKlass .\nW A R N I N G\nIn the interest of science, Example 25-16  defies all reason and applies three dif ferent\nmetaprogramming techniques together on Klass : a decorator , a base class using\n__init_subclass__ , and a custom metaclass. If you do this in production code,\nplease don’ t blame me. Again, the goal is to observe the order in which the three\ntechniques interfere in the class construction process.\nAs in the previous evaluation time experiment, this example does nothing\nbut print messages revealing the flow of execution. Next is the code for the\ntop part of metalib.py —the rest is in Example 25-18 :\nExample 25-17. metalib.py: the NosyDict class\nprint('% metalib module start' ) \n \nimport collections  \n \nclass NosyDict (collections .UserDict ): \n    def __setitem__ (self, key, value): \n        args = (self, key, value) \n        print(f'% NosyDict.__setitem__ {args!r} ') \n        super().__setitem__ (key, value) \n \n    def __repr__ (self): \n        return '<NosyDict instance>'\nI wrote the NosyDict  class to override __setitem__  to display each\nkey  and value  as they are set. The metaclass will use a NosyDict\ninstance to hold the namespace of the class under construction, revealing\nmore of Python’ s inner workings.\nThe main attraction of metalib.py  is the metaclass in Example 25-18 . It\nimplements the __prepare__  special method, a class method that Python\nonly invokes on metaclasses. The __prepare__  method provides the\nearliest opportunity to influence the process of creating a new class.\nT I P\nWhen coding a metaclass, I find it useful to adopt this naming convention for special\nmethod ar guments:\nUse cls  instead of self  for instance methods, because the instance is a class.\nUse meta_cls  instead of cls  for class methods, because the class is a\nmetaclass. Recall that __new__  behaves as a class method even without\n@classmethod .\nExample 25-18. metalib.py: the MetaKlass\nclass MetaKlass (type): \n    print('% MetaKlass body ') \n \n    @classmethod   \n \n    def __prepare__ (meta_cls , cls_name , bases):  \n \n        args = (meta_cls , cls_name , bases) \n        print(f'% MetaKlass.__prepare__ {args!r} ') \n        return NosyDict ()  \n \n \n    def __new__(meta_cls , cls_name , bases, cls_dict ):  \n \n        args = (meta_cls , cls_name , bases, cls_dict ) \n        print(f'% MetaKlass.__new__ {args!r} ') \n        def inner_2(self): \n            print(f'% MetaKlass.__new__:inner_2( {self!r} )') \n \n        cls = super().__new__(meta_cls , cls_name , bases, \ncls_dict .data)  \n \n \n        cls.method_c  = inner_2  \n \n \n        return cls  \n \n \n    def __repr__ (cls):  \n \n        cls_name  = cls.__name__  \n        return f""<class {cls_name!r}  built by MetaKlass> "" \n \nprint('% metalib module end ')\n__prepare__  should be declared as a class method. It is not an\ninstance method because the class under construction does not exist yet\nwhen Python calls __prepare__ .\nPython calls __prepare__  on a metaclass to obtain a mapping to\nhold the namespace of the class under construction.\nReturn NosyDict  instance to be used as the namespace.\ncls_dict  is a NosyDict  instance returned by __prepare__ .\ntype.__new__  requires a real dict  as the last ar gument, so I give it\nthe data  attribute of NosyDict , inherited from UserDict .\nInject a method in the newly created class.\nAs usual, __new__  must return the object just created—in this case,\nthe new class.\nDefining __repr__  on a metaclass allows customizing the repr()\nof class objects.\nThe main use case for __prepare__  before Python 3.6 was to provide an\nOrderedDict  to hold the attributes of the class under construction, so\nthat the metaclass __new__  could process those attributes in the order in\nwhich they appear in the source code of the user ’ s class definition. Now that\ndict  preserves the insertion order , __prepare__  is rarely needed. Y ou\nwill see a creative use for it in “A Metaclass Hack with __prepare__ ” .\nImporting metalib.py  in the Python console is not very exciting. Note the\nuse of %  to prefix the lines output by this module:\n>>> import metalib \n% metalib module start  \n% MetaKlass body  \n% metalib module end\nLots of things happen if you import evaldemo_meta.py :\nExample 25-19. Console experiment with evaldemo_meta.py .\n>>> import evaldemo_meta  \n@ builderlib module start  \n@ Builder body  \n@ Descriptor body  \n@ builderlib module end  \n% metalib module start  \n% MetaKlass body  \n% metalib module end  \n# evaldemo_meta module start  \n  \n% MetaKlass.__prepare__(<class 'metalib.MetaKlass'>, 'Klass',  \n  \n                        (<class 'builderlib.Builder'>,))  \n% NosyDict.__setitem__(<NosyDict instance>, '__module__',  \n'evaldemo_meta')  \n  \n% NosyDict.__setitem__(<NosyDict instance>, '__qualname__',  \n'Klass')  \n# Klass body  \n@ Descriptor.__init__(<Descriptor instance>)  \n  \n% NosyDict.__setitem__(<NosyDict instance>, 'attr', <Descriptor  \ninstance>)  \n  \n% NosyDict.__setitem__(<NosyDict instance>, '__init__',  \n                       <function Klass.__init__ at …>)  \n  \n% NosyDict.__setitem__(<NosyDict instance>, '__repr__',  \n                       <function Klass.__repr__ at …>)  \n% NosyDict.__setitem__(<NosyDict instance>, '__classcell__', <cell  \nat …: empty>)  \n% MetaKlass.__new__(<class 'metalib.MetaKlass'>, 'Klass',  \n                    (<class 'builderlib.Builder'>,), <NosyDict  \ninstance>)  \n  \n@ Descriptor.__set_name__(<Descriptor instance>,  \n                          <class 'Klass' built by MetaKlass>,  \n'attr')  \n  \n@ Builder.__init_subclass__(<class 'Klass' built by MetaKlass>)  \n@ deco(<class 'Klass' built by MetaKlass>)  \n# evaldemo_meta module end\nThe lines before this are the result of importing builderlib.py  and\nmetalib.py .\nPython invokes __prepare__  to start processing a class  statement.\nBefore parsing the class body , Python adds the __module__  and\n__qualname__  entries to the namespace of the class under\nconstruction.\nThe descriptor instance is created…\n…and bound to attr  in the class namespace.\n__init__  and __repr__  methods are defined and added to the\nnamespace.\nOnce Python finished processing the class body , it calls\nMetaKlass.__new__ .\n__set_name__ , __init_subclass__ , and the decorator are\ninvoked in this order , after the __new__  method of the metaclass\nreturns the newly constructed class.\nIf you run evaldemo_meta.py  as script, main()  is called, and a few more\nthings happen:\nExample 25-20. Running evaldemo_meta.py  as a pr ogram.\n$ ./evaldemo_meta.py  \n[... 20 lines omitted ...]  \n@ deco(<class 'Klass' built by MetaKlass>)  \n  \n@ Builder.__init__(<Klass instance>)  \n# Klass.__init__(<Klass instance>)  \n@ SuperA.__init_subclass__:inner_0(<Klass instance>)  \n@ deco:inner_1(<Klass instance>)  \n% MetaKlass.__new__:inner_2(<Klass instance>)  \n  \n@ Descriptor.__set__(<Descriptor instance>, <Klass instance>, 999)  \n# evaldemo_meta module end\nThe top 21 lines—including this one—are the same shown in\nExample 25-19 .\nT riggered by obj.method_c()  in main ; method_c  was injected\nby MetaKlass.__new__ .\nLet’ s now go back to the idea of the Checked  class with the Field\ndescriptors implementing runtime type validation, and see how it can be",9760
347-A Metaclass solution for Checked.pdf,347-A Metaclass solution for Checked,"done with a metaclass.\nA  M e t a c l a s s  s o l u t i o n  f o r  Checked\nI don’ t want to encourage premature optimization and overengineering, so\nhere is a make-believe scenario to justify rewriting checkedlib.py  with\n__slots__ , which requires the application of a metaclass. Feel free to\nskip it.\nA  B I T  O F  S T O R Y T E L L I N G\nOur checkedlib.py  using __init_subclass__  is a company-wide\nsuccess, and our production servers have millions of instances of\nChecked  subclasses in memory at any one time.\nProfiling a proof-of-concept, we discover that using __slots__  will\nreduce the cloud hosting bill for two reasons:\nlower memory usage, as Checked  instances don’ t need their\nown __dict__ ;\nhigher performance, by removing __setattr__  which was\ncreated just to block unexpected attributes, but is triggered at\ninstantiation and for all attribute setting before\nField.__set__  is called to do its job.\nThe metaclass/checkedlib.py  module we’ll study next is a drop-in\nreplacement for initsub/checkedlib.py . The doctests embedded in them are\nidentical, as well as the checkedlib_test.py  files for pytest .\nThe complexity in checkedlib.py  is abstracted away from the user . Here is\nthe source code of a script using the package:\nfrom checkedlib  import Checked \n \nclass Movie(Checked): \n    title: str \n    year: int \n    box_office : float \n \nif __name__  == '__main__' : \n    movie = Movie(title='The Godfather' , year=1972, \nbox_office =137) \n    print(movie) \n    print(movie.title)\nThat concise Movie  class definition leverages three instances of Field\nvalidating descriptors, a __slots__  configuration, five methods inherited\nfrom Checked , and a metaclass to put it all together . The only visible part\nof checkedlib  is the Checked  base class.\nConsider Figure 25-4 . The Mills & Gizmos Notation complements the\nUML class diagram by making the relationship between classes and\ninstances more visible. For example, a Movie  class using the new\ncheckedlib.py  is an instance of CheckedMeta , and a subclass of\nChecked . Also, the title , year  and box_office  class attributes of\nMovie  are three separate instances of Field . Each Movie  instance has\nits own _title ,_year , and _box_office  attributes, to store the values\nof the corresponding fields.\nFigur e 25-4. UML class diagram annotated with MGN: the CheckedMeta  meta-mill builds the\nMovie  mill. The Field  mill builds the title , year , and box_office  descriptors which ar e\nclass atttributes of Movie . The per -instance data for the fields is stor ed in the _title , _year  and\n_box_office  instance attributes of Movie . Note the package boundary of checkedlib . The\ndeveloper of Movie  doesn’ t need to gr ok all the machinery inside checkedlib.py .\nNow let’ s study the code, starting with the Field  class, shown in\nExample 25-21 .\nThe Field  descriptor class is now a bit dif ferent. In the previous\nexamples, each Field  descriptor instance stored its value in the managed\ninstance using an attribute of the same name. For example, in the Movie\nclass, the title  descriptor stored the field value in a title  attribute in\nthe managed instance. This made it unnecessary for Field  to provide a\n__get__  method.\nHowever , when a class like Movie  uses __slots__ , it cannot have class\nattributes and instance attributes with the same name. Each descriptor\ninstance is a class attribute, and now we need separate per -instance storage\nattributes. The code uses the descriptor name prefixed with a single _ .\nTherefore Field  instances have separate name  and storage_name\nattributes, and we implement Field.__get__ .\nHere is the source code for Field , with callouts describing only the\nchanges in this version:\nExample 25-21. metaclass/checkedlib.py: the Field  descriptor with\nstorage_name  and __get__ .\nclass Field: \n    def __init__ (self, name: str, constructor : Callable ) -> None: \n        if not callable (constructor ) or constructor  is type(None): \n            raise TypeError (f'{name!r} type hint must be callable ') \n        self.name = name \n        self.storage_name  = '_' + name  \n \n        self.constructor  = constructor  \n \n    def __get__(self, instance , owner=None):  \n \n        return getattr(instance , self.storage_name )  \n \n \n    def __set__(self, instance : Any, value: Any) -> None: \n        if value is ...: \n            value = self.constructor () \n        else: \n            try: \n                value = self.constructor (value) \n            except (TypeError , ValueError ) as e: \n                type_name  = self.constructor .__name__  \n                msg = f'{value!r} is not compatible with  \n{self.name}:{type_name} ' \n                raise TypeError (msg) from e \n        setattr(instance , self.storage_name , value)  \nCompute storage_name  from the name  ar gument.\nImplement __get__ …\nUsing getattr  and the storage_name .\n__set__  now uses setattr  to set or update the managed attribute.\nNext is the code for the metaclass that drives this example.\nExample 25-22. metaclass/checkedlib.py: the CheckedMeta  metaclass.\nclass CheckedMeta (type): \n \n    def __new__(meta_cls , cls_name , bases, cls_dict ):  \n \n        if '__slots__ ' not in cls_dict :  \n \n            slots = [] \n            type_hints  = cls_dict .get('__annotations__ ', {})  \n \n            for name, constructor  in type_hints .items():   \n \n                field = Field(name, constructor )  \n \n                cls_dict [name] = field  \n \n                slots.append(field.storage_name )  \n \n \n            cls_dict ['__slots__ '] = slots  \n \n \n        return super().__new__( \n                meta_cls , cls_name , bases, cls_dict )  \n__new__  is the only method implemented in CheckedMeta .\n\nOnly enhance the class if its cls_dict  doesn’ t include __slots__ .\nIf __slots__  is already present, assume it is the Checked  base class\nand not a user -defined subclass, and build the class as is.\nT o get the type hints in prior examples we used\ntyping.get_type_hints , but that requires an existing class as the\nfirst ar gument. At this point, the class we are configuring does not exist\nyet, so we need to retrieve the __annotations__  directly from the\ncls_dict —the namespace of the class under construction, which\nPython passes as the last ar gument to the metaclass __new__ .\niterate over type_hints  to…\n…build a Field  for each annotated attribute…\n…overwrite the corresponding entry in cls_dict  with the Field\ninstance…\n…and append the storage_name  of the field in the list we’ll use to…\n…populate the __slots__  entry in cls_dict —the namespace of\nthe class under construction.\nFinally , we call super().__new__ .\nThe last part of metaclass/checkedlib.py  is the Checked  base class that\nusers of this library will subclass to enhance their classes, like Movie .\nThe code for this version of Checked  is the same as Checked  in\ninitsub/checkedlib.py  (listed in Example 25-5  and Example 25-6 ), with\nthree changes:\n1 . Added an empty __slots__  to signal to\nCheckedMeta.__new__  that this class doesn’ t require special\nprocessing.\n2 . Removed __init_subclass__ . Its job is now done by\nCheckedMeta.__new__ .\n3 . Removed __setattr__ . It became redundant because adding\n__slots__  to the user defined class prevents setting undeclared\nattributes.\nExample 25-23  is a complete listing of the final version of Checked .\nExample 25-23. metaclass/checkedlib.py: the Checked  base class.\nclass Checked(metaclass =CheckedMeta ): \n    __slots__  = ()  # skip CheckedMeta.__new__ processing  \n \n    @classmethod  \n    def _fields(cls) -> dict[str, type]: \n        return get_type_hints (cls) \n \n    def __init__ (self, **kwargs: Any) -> None: \n        for name in self._fields(): \n            value = kwargs.pop(name, ...) \n            setattr(self, name, value) \n        if kwargs: \n            self.__flag_unknown_attrs (*kwargs) \n \n    def __flag_unknown_attrs (self, *names: str) -> NoReturn : \n        plural = 's' if len(names) > 1 else '' \n        extra = ', '.join(f'{name!r}'  for name in names) \n        cls_name  = repr(self.__class__ .__name__ ) \n        raise AttributeError (f'{cls_name} object has no  \nattribute{plural} {extra}' ) \n \n    def _asdict(self) -> dict[str, Any]: \n        return { \n            name: getattr(self, name) \n            for name, attr in self.__class__ .__dict__ .items() \n            if isinstance (attr, Field) \n        } \n \n    def __repr__ (self) -> str: \n        kwargs = ', '.join( \n            f'{key}={value!r}'  for key, value in \nself._asdict().items() \n        ) \n        return f'{self.__class__.__name__}({kwargs})'",8778
348-Metaclasses in the Real world.pdf,348-Metaclasses in the Real world,,0
349-Metaclasses are Stable Language Features.pdf,349-Metaclasses are Stable Language Features,"This concludes the third rendering of a class builder with validated\ndescriptors.\nThe next section covers some general issues related to metaclasses.\nM e t a c l a s s e s  i n  t h e  R e a l  w o r l d\nMetaclasses are powerful but tricky . Before deciding to implement a\nmetaclass, consider the following points.\nModern Features Simplify or Replace Metaclasses\nOver time, several common use cases of metaclasses were made redundant\nby new language features:\nClass decorators\nSimpler to understand than metaclasses, and less likely to cause\nconflicts with base classes and metaclasses.\n__set_name__\nA voids the need for custom metaclass logic to automatically set the\nname of a descriptor .\n__init_subclass__\nProvides a way to customize class creation that is transparent to the end-\nuser and even simpler than a decorator—but may introduce conflicts in\na complex class hierarchy .\nBuilt-in dict  pr eserving key insertion or der\nEliminated the #1 reason to use __prepare__ : to provide an\nOrderedDict  to store the namespace of the class under construction.\nPython only calls __prepare__  on metaclasses, so if you needed to\nprocess the class namespace in the order it appears in the source code,\nyou had to use a metaclass before Python 3.6.14",1270
350-A Metaclass Hack with __prepare__.pdf,350-A Metaclass Hack with __prepare__,"As of 2021, every actively maintained version of CPython supports all the\nfeatures above.\nI keep advocating these features because I see too much unnecessary\ncomplexity in our profession, and metaclasses are a gateway to complexity .\nMetaclasses are Stable Language Features\nMetaclasses were introduced in Python 2.2 in 2002, together with so-called\n“new-style classes”, descriptors, and properties.\nIt is remarkable that the MetaBunch  example, first posted by Alex\nMartelli in July 2002, still works in Python 3.9—the only change being the\nway to specify the metaclass to use, which in Python 3 is done with the\nsyntax class Bunch(metaclass=MetaBunch): .\nNone of the additions I mentioned in “Modern Features Simplify or Replace\nMetaclasses”  broke existing code using metaclasses. But legacy code using\nmetaclasses can often be simplified by leveraging those features, especially\nif you can drop support to Python versions before 3.6—which are no longer\nmaintained.\nA Class Can Only Have One Metaclass\nIf your class declaration involves two or more metaclasses, you will see this\npuzzling error message:\nTypeError: metaclass conflict: the metaclass of a derived class  \nmust be a (non-strict) subclass of the metaclasses of all its  \nbases\nThis may happen even without multiple inheritance. For example, a\ndeclaration like this could trigger that TypeError :\nclass Record(abc.ABC, metaclass =PersistentMeta ): \n    pass\nW e saw that abc.ABC  is an instance of the abc.ABCMeta  metaclass. If\nthat Persistent  metaclass is not itself a subclass of abc.ABCMeta ,\nyou get a metaclass conflict.\nThere are two ways of dealing with that error:\nFind some other way of doing what you need to do, while avoiding\nat least one of the metaclasses involved.\nW rite your own PersistentABCMeta  metaclass as a subclass\nof both abc.ABCMeta  and PersistentMeta , using multiple\ninheritance, and use that as the only metaclass for Record .\nT I P\nI can imagine the solution of the metaclass with two base metaclasses implemented to\nmeet a deadline. In my experience, metaclass programming always takes longer than\nanticipated, which makes this approach risky before a hard deadline. If you do it and\nmake the deadline, the code may contain subtle bugs. Even in the absence of known\nbugs, you should consider this approach as technical debt simply because it is hard to\nunderstand and maintain.\nMetaclasses Should be Implementation Details\nBesides type , there are only six metaclasses in the entire Python 3.9\nstandard library . The better known are probably abc.ABCMeta ,\ntyping.NamedTupleMeta , and enum.EnumMeta . None of them are\nintended to appear explicitly in user code. W e may consider them\nimplementation details.\nAlthough you can do some really whacky metaprograming with\nmetaclasses, it’ s best to heed the Principle of least astonishment  so that\nmost users can indeed regard metaclasses as implementation details.\nIn recent years, some metaclasses in the Python standard library were\nreplaced by other mechanisms, without breaking the public API of their\npackages. The simplest way future-proof such APIs is to of fer a regular15\n16\nclass that users subclass to access the functionality provided by the\nmetaclass, as we’ve done in our examples.\nT o wrap up our coverage of class metaprogramming, I will share with you\nthe coolest, small example of metaclass I found as I researched this chapter .\nA  M e t a c l a s s  H a c k  w i t h  __prepare__\nWhen I updated this chapter for the Second Edition , I needed to find simple\nbut illuminating examples to replace the bulkfood  LineItem  code that no\nlonger require metaclasses since Python 3.6.\nThe simplest and most interesting metaclass idea was given to me by João\nS. O. Bueno—better known as JS in the Brazilian Python community . One\napplication of his idea is to create a class that auto-generates numeric\nconstants.\n    >>> class Flavor(AutoConst ): \n    ...     banana \n    ...     coconut \n    ...     vanilla \n    ... \n    >>> Flavor.vanilla \n    2 \n    >>> Flavor.banana, Flavor.coconut \n    (0, 1)\nY es, that code works as shown! That’ s actually a doctest in\nautoconst_demo.py .\nHere is the user -friendly AutoConst  base class and the metaclass behind\nit, implemented in autoconst.py :\nclass AutoConstMeta (type): \n    def __prepare__ (name, bases, **kwargs): \n        return WilyDict () \n \nclass AutoConst (metaclass =AutoConstMeta ): \n    pass\nThat’ s it.\nClearly the trick is in WilyDict .\nWhen Python processes the namespace of the user ’ s class and reads\nbanana , it looks that name up in the mapping provided by\n__prepare__ : an instance of WilyDict . WilyDict  implements\n__missing__ —covered in “The __missing__  Method” . The\nWilyDict  instance initially has no 'banana'  key , so the\n__missing__  method is triggered. It makes an item on the fly with the\nkey 'banana'  and the value 0 , returning that value. Python is happy with\nthat, then tries to retrieve 'coconut' . WilyDict  promptly adds that\nentry with the value 1 , returning it. The same happens with 'vanilla' ,\nwhich is then mapped to 2 .\nW e’ve seen __prepare__  and __missing__  before. The real\ninnovation is how JS put them together .\nHere is the source code for WilyDict , also from autoconst.py :\nclass WilyDict (dict): \n    def __init__ (self, *args, **kwargs): \n        super().__init__ (*args, **kwargs) \n        self.__next_value  = 0 \n \n    def __missing__ (self, key): \n        if key.startswith ('__') and key.endswith ('__'): \n            raise KeyError (key) \n        self[key] = value = self.__next_value  \n        self.__next_value  += 1 \n        return value\nWhile experimenting, I found that Python looked up __name__  in the\nnamespace of the class under construction, causing WilyDict  to add a\n__name__  entry , and increment __next_value . So I added that if\nstatement in __missing__  to raise KeyError  for keys that look like\ndunder attributes.\nThe autoconst.py  package both requires and illustrates mastery of Python’ s\ndynamic class building machinery .",6132
351-Wrapping up.pdf,351-Wrapping up,"I had a great time adding more functionality to AutoConstMeta  and\nAutoConst , but instead of sharing my experiments I will let you have fun\nplaying with JS’ s ingenious hack.\nHere are some ideas:\nMake it possible to retrieve the constant name if you have the\nvalue. For example, Flavor[2]  could return 'vanilla' . Y ou\ncan to this by implementing __getitem__  in\nAutoConstMeta . Since Python 3.9, you can implement\n__class_getitem__  in AutoConst  itself.\nSupport iteration over the class, by implementing __iter__  on\nthe metaclass. I would make the __iter__  yield the constants as\n(name, value)  pairs.\nImplement a new Enum  variant. This would be a major\nundertaking, because the enum  package is full of tricks, including\nthe EnumMeta  metaclass with hundreds of lines of code and a\nnon-trivial __prepare__  method.\nEnjoy!\nN O T E\nThe __class_getitem__  special method was added in Python 3.9 to support\ngeneric types, as part of PEP 585—T ype Hinting Generics In Standar d Collections .\nThanks to __class_getitem__ , Python’ s core developers did not have to write a\nnew metaclass for the built-in types to implement __getitem__  so that we could\nwrite generic type hints like list[int] . This is a narrow feature, but representative of\na wider use case for metaclasses: implementing operators and other special methods to\nwork at the class level, such as making the class itself iterable, just like Enum\nsubclasses.\nW r a p p i n g  u p\nMetaclasses, as well as class decorators and __init_subclass__  are\nuseful for:\nSubclass registration.\nSubclass structural validation.\nApplying decorators to many methods at once.\nObject serialization.\nObject-relational mapping.\nObject-based persistence.\nImplementing special methods at the class level.\nImplementing class features found in other languages, such as\ntraits  and aspect-oriented programming .\nClass metaprogramming can also help with performance issues in some\ncases, by performing tasks at import time that otherwise would execute\nrepeatedly at runtime.\nT o wrap up, let’ s recall Alex Martelli’ s final advice from his essay\n“W aterfowl and ABCs” :\nAnd, don’ t  define custom ABCs (or metaclasses) in pr oduction code… if\nyou feel the ur ge to do so, I’d bet it’ s likely to be a case of “all pr oblems\nlook like a nail”-syndr ome for somebody who just got a shiny new\nhammer—you (and futur e maintainers of your code) will be much\nhappier sticking with straightforwar d and simple code, eschewing such\ndepths.\nI believe Martelli’ s advice applies not only to ABCs and metaclasses, but\nalso to class hierarchies, operator overloading, function decorators,\ndescriptors, class decorators, and class builders using\n__init_subclass__ .\nThose powerful tools exist primarily to support library and framework\ndevelopment. Applications naturally should use  those tools, as provided by",2894
352-Chapter Summary.pdf,352-Chapter Summary,"the Python standard library or external packages. But implementing  them in\napplication code is often premature abstraction.\nGood frameworks ar e extracted, not invented.\n— David Heinemeier Hansson, creator of Ruby on Rails17\nC h a p t e r  S u m m a r y\nThis chapter started with an overview of attributes found in class objects,\nsuch as __qualname__  and the __subclasses__()  method. Next we\nsaw how the type  built-in can be used to construct classes at runtime.\nThe __init_subclass__  special method was introduced, with the first\niteration of a Checked  base class designed to replace attribute type hints in\nuser -defined subclasses with Field  instances that apply constructors to\nenforce the type of those attributes at runtime.\nThe same idea was implemented with a @checked  class decorator which\nadds features to user -defined classes, similar to what\n__init_subclass__  allows. W e saw that neither\n__init_subclass__  nor a class decorator can dynamically configure\n__slots__ , because they operate only after a class is created.\nThe concepts of “import time” and “runtime” were clarified with\nexperiments showing the order in which Python code in executed when\nmodules, descriptors, class decorators, and __init_subclass__  is\ninvolved.\nOur coverage of metaclasses began with an overall explanation of type  as\na metaclass, and how user defined metaclasses can implement __new__  to\ncustomize the classes it builds. W e then saw our first custom metaclass, the\nclassic MetaBunch  example using __slots__ . Next, another evaluation\ntime experiment demonstrated how the __prepare__  and __new__\nmethods of a metaclass are invoked earlier than __init_subclass__\nand class decorators, providing opportunities for deeper class\ncustomization.\nThe third iteration of a Checked  class builder with Field  descriptors and\ncustom __slots__  configuration was presented, followed by some\ngeneral considerations about metaclass usage in practice.\nFinally , we saw the AutoConst  hack invented by João S. O. Bueno, based\non the cunning idea of a metaclass with __prepare__  returning a",2118
353-Further Reading.pdf,353-Further Reading,"mapping that implements __missing__ . In less than 20 lines of code,\nautoconst.py  showcases the power of combining Python metaprograming\ntechniques\nI haven’ t yet found a language that manages to be easy for beginners,\npractical for professionals, and exciting for hackers in the way that Python\nis. Thanks, Guido van Rossum and everybody else who makes it so.\nF u r t h e r  R e a d i n g\nThe essential references for this chapter in the Python documentation are\n“3.3.3. Customizing class creation”  in the “Data Model” chapter of The\nPython Language Reference, which covers __init_subclass__  and\nmetaclasses. The type  class documentation  in the “Built-in Functions”\npage, and “4.13. Special Attributes”  of the “Built-in T ypes” chapter in the\nLibrary Refer ence  is also essential reading.\nIn the Library Refer ence , the types  module documentation  covers two\nfunctions added in Python 3.3 that simplify class metaprogramming:\ntypes.new_class  and types.prepare_class .\nClass decorators were formalized in PEP 3129—Class Decorators , written\nby Collin W inter , with the reference implementation authored by Jack\nDiederich. The PyCon 2009 talk “Class Decorators: Radically Simple”\n( video ), also by Jack Diederich, is a quick introduction to the feature.\nBesides @dataclass , an interesting—and much simpler—example of a\nclass decorator in Python’ s standard library is\nfunctools.total_ordering  that generates special methods for\nobject comparison.\nFor metaclasses, the main reference in Python’ s documentation is PEP 31 15\n—Metaclasses in Python 3000 , in which the __prepare__  special\nmethod was introduced.\nPython in a Nutshell, 3r d Edition  by Alex Martelli, Anna Ravenscroft, and\nSteve Holden is authoritative, but was written before PEP 487—Simpler\ncustomization of class cr eation  came out. The main metaclass example in\nthat book— MetaBunch —is still valid, because it can’ t be written with\nsimpler mechanisms. Brett Slatkin’ s Effective Python, Second Edition\n(Addison-W esley , 2019) has several up-to-date examples of class bulding\ntechniques, including metaclasses.\nT o learn about the origins of class metaprogramming in Python, I\nrecommend Guido van Rossum’ s paper from 2003, Unifying types and\nclasses in Python 2.2 . The text applies to modern Python as well, as it\ncovers what were then called the “new-style” class semantics—the default\nsemantics in in Python 3—including descriptors and metaclasses. One of\nthe references cited by Guido is Putting Metaclasses to W ork: a New\nDimension in Object-Oriented Pr ogramming , by Ira R. Forman and Scott\nH. Danforth (Addison-W esley , 1998), a book to which he gave 5 stars on\nAmazon.com, adding the following review:\nThis book contributed to the design for metaclasses in Python 2.2\nT oo bad this is out of print; I keep r eferring to it as the best tutorial I\nknow for the difficult subject of cooperative multiple inheritance,\nsupported by Python via the super()  function.\nIf you are keen on metaprogramming, you may wish Python had the\nultimate metaprogramming feature: syntactic macros, as of fered the Lisp\nfamily of languages and—more recently—by Elixir and Rust. Syntactic\nmacros are more powerful and less error -prone than the primitive code\nsubstitution macros in the C language. They are special functions that\nrewrite source code using custom syntax into standard code before the\ncompilation step, enabling developers to introduce new language constructs\nwithout changing the compiler . Like operator overloading, syntactic macros\ncan be abused. But as long as the community understands and manages the\ndownsides, they support powerful and user -friendly abstractions, like DSLs\n(Domain-Specific Languages). In September 2020, Python core developer\nMark Shannon posted PEP 638—Syntactic Macros  advocating just that.\nSeven months after initially published, PEP 638 is still in draft and there are\nno ongoing discussions about it. Clearly it’ s not a top priority for the Python18\ncore developers. I would like to see PEP 638 further discussed and\neventually approved. Syntactic macros would allow the Python community\nto experiment with controversial new features, such as the walrus operator\n( PEP 572 ), pattern matching ( PEP 634 ), and alternative rules for evaluating\ntype hints (PEPs 563  and 649 ) before making permanent changes to the\ncore language. Meanwhile, you can get a taste of syntactic macros with the\nMacroPy  package.\nS O A P B O X\nI will start the last soapbox in the book with a long quote from Brian\nHarvey and Matthew W right, two computer science professors from the\nUniversity of California (Berkeley and Santa Barbara). In their book,\nSimply Scheme , Harvey and W right wrote:\nTher e ar e two schools of thought about teaching computer science.\nW e might caricatur e the two views this way:\n1 . The conservative view : Computer pr ograms have become\ntoo lar ge and complex to encompass in a human mind.\nTher efor e, the job of computer science education is to teach\npeople how to discipline their work in such a way that 500\nmediocr e pr ogrammers can join together and pr oduce a\npr ogram that corr ectly meets its specification.\n2 . The radical view : Computer pr ograms have become too\nlar ge and complex to encompass in a human mind.\nTher efor e, the job of computer science education is to teach\npeople how to expand their minds so that the pr ograms can\nfit, by learning to think in a vocabulary of lar ger , mor e\npowerful, mor e flexible ideas than the obvious ones. Each\nunit of pr ogramming thought must have a big payoff in the\ncapabilities of the pr ogram.\n— Brian Harvey and Matthew W right, Preface to\nSimply Scheme\nHarvey and W right’ s exaggerated descriptions are about teaching\ncomputer science, but they also apply to programming language design.\nBy now , you should have guessed that I subscribe to the “radical” view ,\nand I believe Python was designed in that spirit.\nThe property idea is a great step forward compared to the accessors-\nfrom-the-start approach practically demanded by Java and supported by\nJava IDEs generating getters/setters with a keyboard shortcut. The main19\nadvantage of properties is to let us start our programs simply exposing\nattributes as public—in the spirit of KISS —knowing a public attribute\ncan become a property at any time without much pain. But the\ndescriptor idea goes way beyond that, providing a framework for\nabstracting away repetitive accessor logic. That framework is so\nef fective that essential Python constructs use it behind the scenes.\nAnother powerful idea is functions as first-class objects, paving the way\nto higher -order functions. T urns out the combination of descriptors and\nhigher -order functions enable the unification of functions and methods.\nA function’ s __get__  produces a method object on the fly by binding\nthe instance to the self  ar gument. This is elegant.\nFinally , we have the idea of classes as first-class objects. It’ s an\noutstanding feat of design that a beginner -friendly language provides\npowerful abstractions such as class builders, class decorators and full-\nfledged, user -defined metaclasses. Best of all: the advanced features are\nintegrated in a way that does not complicate Python’ s suitability for\ncasual programming (they actually help it, under the covers). The\nconvenience and success of frameworks such as Django and\nSQLAlchemy owes much to metaclasses. Over the years, class\nmetaprogramming in Python is becoming simpler and simpler , at least\nfor common use cases. The best language features are those that benefit\neveryone, even if some Python users are not aware of them. But they\ncan always learn and create the next great library .\nI look forward to learning about your contributions to the Python\ncommunity and ecosystem!\n1  Quote from chapter 2, Expr ession , page 10, of The Elements of Pr ogramming Style, Second\nEdition .\n2  That doesn’ t mean PEP 487 broke code that used those features. It just means that some code\nthat used class decorators or metaclasses prior to Python 3.6 can now be refactored to use plain\nclasses, resulting in simpler and possibly more ef ficient code.\n3  Thanks to my friend J. S. O. Bueno for contributing to this example.20\n4  That’ s true for any object, except when its class overrides the __str__  or __repr__\nmethods inherited from object  with broken implementations.\n5  This solution avoids using None  as a default. A voiding null values is a good idea . They are\nhard to avoid in general, but easy in some cases. In Python as well as SQL, I prefer to represent\nmissing data in a text field with an empty string instead of None  or NULL . Learning Go\nreinforced this idea: variables and struct fields of primitive types in Go are initialized by\ndefault with a “zero value”. See Zero values  in the online T our of Go  if you are curious.\n6  I believe that callable  should be made suitable for type hinting. As of May 6, 2021, this is\nan open issue .\n7  As mentioned in “What’ s a good poison pill?” , the Ellipsis  object is a convenient and safe\nsentinel value. It has been around for a long time, but recently people are finding more uses for\nit, as we see in type hints and NumPy .\n8  The subtle concept of an overriding descriptor was explained in “Overriding Descriptors” .\n9  This rationale appears in the abstract of PEP 557–Data Classes  to explain why it was\nimplemented as a class decorator .\n10  Contrast with the import  statement in Java, which is just a declaration to let the compiler\nknow that certain packages are required.\n11  I’m not saying opening a database connection just because a module is imported is a good\nidea, only pointing out it can be done.\n12  Message to comp.lang.python, subject: “Acrimony in c.l.p.” . This is another part of the same\nmessage from December 23, 2002, quoted in the Preface . The T imBot was inspired that day .\n13  The authors kindly gave me permission to use their example. MetaBunch  first appeared in a\nmessage posted by Martelli in the comp.lang.python  group on July 7, 2002, with the subject\nline a nice metaclass example (was Re: structs in python) , following a discussion about record-\nlike data structures in Python. Martelli’ s original code for Python 2.2 still runs after a single\nchange: to use a metaclass in Python 3, you must use the metaclass keyword ar gument in the\nclass declaration, e.g. Bunch(metaclass=MetaBunch) , instead of the older convention\nof adding a __metaclass__  class-level attribute.\n14  In Fluent Python, First Edition , the more advanced versions of the LineItem  class used a\nmetaclass just to set the storage name of the attributes. See the code in the metaclasses of\nbulkfood  in the First Edition  code repository\n15  If you just got dizzy considering the implications of multiple inheritance with metaclasses,\ngood for you. I’d stay way from this solution as well.\n16  I made a living writing Django code for a few years before I decided to study how Django’ s\nmodel fields were implemented. Only then I learned about descriptors and metaclasses.\n17  The phrase is widely quoted. I found an early direct quote in a post  in DHH’ s blog from 2005.\n18  I bought a used copy and found it a very challenging read.\n19  Brian Harvey and Matthew W right, Simply Scheme  (MIT Press, 1999), p. xvii. Full text\navailable at Berkeley .edu .\n20  Machine Beauty  by David Gelernter (Basic Books) opens with an intriguing discussion of\nelegance and aesthetics in works of engineering, from bridges to software. The later chapters\nare not great, but the opening is worth the price.\nAbout the Author\nLuciano Ramalho  became a web developer before the Netscape IPO in\n1995, and switched from Perl to Java to Python in 1998. He joined\nThoughtworks in 2015, where he is a Principal Consultant in the São Paulo\nof fice. He has delivered keynotes, talks and tutorials at Python events in the\nAmericas, Europe and Asia, and also presented at Go and Elixir\nconferences, focusing on language design topics. Ramalho is a fellow of the\nPython Software Foundation and cofounder of Garoa Hacker Clube, the\nfirst hackerspace in Brazil.\n1 . Preface\na . Who This Book Is For\nb . Who This Book Is Not For\nc . How This Book Is Or ganized\nd . Hands-On Approach\ne . Hardware Used for T imings\nf . Soapbox: My Personal Perspective\ng . Python Jar gon\nh . Python V ersion Covered\ni . Conventions Used in This Book\nj . Using Code Examples\nk . How to Contact Us\nl . Acknowledgments\n2 . I. Prologue\n3 . 1. The Python Data Model\na . What’ s new in this chapter\nb . A Pythonic Card Deck\nc . How Special Methods Are Used\ni . Emulating Numeric T ypes\nii . String Representation\niii . Boolean V alue of a Custom T ype\niv . Collection API\nd . Overview of Special Methods\ne . Why len Is Not a Method\nf . Chapter Summary\ng . Further Reading\n4 . II. Data Structures\n5 . 2. An Array of Sequences\na . What’ s new in this chapter\nb . Overview of Built-In Sequences\nc . List Comprehensions and Generator Expressions\ni . List Comprehensions and Readability\nii . Listcomps V ersus map and filter\niii . Cartesian Products\niv . Generator Expressions\nd . T uples Are Not Just Immutable Lists\ni . T uples as Records\nii . T uples as Immutable Lists\niii . Comparing T uple and List Methods\ne . Unpacking sequences and iterables\ni . Using * to grab excess items\nii . Unpacking with * in function calls and sequence\nliterals\niii . Nested Unpacking\nf . Pattern Matching with Sequences\ni . Pattern Matching Sequences in an Interpreter\ng . Slicing\ni . Why Slices and Range Exclude the Last Item\nii . Slice Objects\niii . Multidimensional Slicing and Ellipsis\niv . Assigning to Slices\nh . Using + and * with Sequences\ni . Building Lists of Lists\nii . Augmented Assignment with Sequences\niii . A += Assignment Puzzler\ni . list.sort versus the sorted Built-In\nj . When a List Is Not the Answer\ni . Arrays\nii . Memory V iews\niii . NumPy\niv . Deques and Other Queues\nk . Chapter Summary\nl . Further Reading\n6 . 3. Dictionaries and Sets\na . What’ s new in this chapter\nb . Modern dict Syntax\ni . dict Comprehensions\nii . Unpacking Mappings\niii . Mer ging Mappings with |\niv . Pattern Matching with Mappings\nc . Standard API of Mapping T ypes\ni . What is Hashable\nii . Overview of Common Mapping Methods\niii . Inserting or Updating Mutable V alues\nd . Automatic Handling of Missing Keys\ni . defaultdict: Another T ake on Missing Keys\nii . The __missing__ Method\ne . V ariations of dict\ni . collections.OrderedDict\nii . collections.ChainMap\niii . collections.Counter\niv . shelve.Shelf\nv . Subclassing UserDict Instead of dict\nf . Immutable Mappings\ng . Dictionary views\nh . Practical Consequences of How dict W orks\ni . Set Theory\ni . Set Literals\nii . Set Comprehensions\nj . Practical Consequences of How Sets W ork\ni . Set Operations\nk . Set operations on dict views\nl . Chapter Summary\nm . Further Reading\n7 . 4. T ext V ersus Bytes\na . What’ s new in this chapter\nb . Character Issues\nc . Byte Essentials\nd . Basic Encoders/Decoders\ne . Understanding Encode/Decode Problems\ni . Coping with UnicodeEncodeError\nii . Coping with UnicodeDecodeError\niii . SyntaxError When Loading Modules with\nUnexpected Encoding\niv . How to Discover the Encoding of a Byte\nSequence\nv . BOM: A Useful Gremlin\nf . Handling T ext Files\ni . Beware of Encoding Defaults\ng . Normalizing Unicode for Reliable Comparisons\ni . Case Folding\nii . Utility Functions for Normalized T ext Matching\niii . Extreme “Normalization”: T aking Out Diacritics\nh . Sorting Unicode T ext\ni . Sorting with the Unicode Collation Algorithm\ni . The Unicode Database\ni . Finding characters by name\nii . Numeric meaning of characters\nj . Dual-Mode str and bytes APIs\ni . str V ersus bytes in Regular Expressions\nii . str V ersus bytes in os Functions\nk . Chapter Summary\nl . Further Reading\n8 . 5. Data Class Builders\na . What’ s new in this chapter\nb . Overview of data class builders\ni . Main features\nc . Classic Named T uples\nd . T yped Named T uples\ne . T ype hints 101\ni . No runtime ef fect\nii . V ariable annotation syntax\niii . The meaning of variable annotations\nf . More about @dataclass\ni . Field options\nii . Post-init processing\niii . T yped class attributes\niv . Initialization variables that are not fields\nv . @dataclass Example: Dublin Core Resource\nRecord\ng . Data class as a code smell\ni . Data class as scaf folding\nii . Data class as intermediate representation\nh . Pattern Matching Class Instances\ni . Simple Class Patterns\nii . Keyword Class Patterns\niii . Positional Class Patterns\ni . Chapter Summary\nj . Further Reading\n9 . 6. Object References, Mutability , and Recycling\na . What’ s new in this chapter\nb . V ariables Are Not Boxes\nc . Identity , Equality , and Aliases\ni . Choosing Between == and is\nii . The Relative Immutability of T uples\nd . Copies Are Shallow by Default\ni . Deep and Shallow Copies of Arbitrary Objects\ne . Function Parameters as References\ni . Mutable T ypes as Parameter Defaults: Bad Idea\nii . Defensive Programming with Mutable\nParameters\nf . del and Garbage Collection\ng . T ricks Python Plays with Immutables\nh . Chapter Summary\ni . Further Reading\n10 . III. Functions as Objects\n1 1 . 7. Functions as First-Class Objects\na . What’ s new in this chapter\nb . T reating a Function Like an Object\nc . Higher -Order Functions\ni . Modern Replacements for map, filter , and reduce\nd . Anonymous Functions\ne . The Nine Flavors of Callable Objects\nf . User -Defined Callable T ypes\ng . From Positional to Keyword-Only Parameters\ni . Positional-only parameters\nh . Packages for Functional Programming\ni . The operator Module\nii . Freezing Ar guments with functools.partial\ni . Chapter Summary\nj . Further Reading\n12 . 8. T ype Hints in Functions\na . What’ s new in this chapter\nb . About gradual typing\nc . Gradual typing in practice\ni . Starting with Mypy\nii . Making Mypy More Strict\niii . A Default Parameter V alue\niv . Using None as a default\nd . T ypes are defined by supported operations\ne . T ypes usable in annotations\ni . The Any type\nii . Simple types and classes\niii . Optional and Union types\niv . Generic collections\nv . T uple types\nvi . Generic mappings\nvii . Abstract Base Classes\nviii . Iterable\nix . Parameterized generics and T ypeV ar\nx . Static Protocols\nxi . Callable\nxii . NoReturn\nf . Annotating positional-only and variadic parameters\ng . Flawed T yping and Strong T esting\nh . Chapter summary\ni . Further Reading\n13 . 9. Decorators and Closures\na . What’ s new in this chapter\nb . Decorators 101\nc . When Python Executes Decorators\nd . Registration decorators\ne . V ariable Scope Rules\nf . Closures\ng . The nonlocal Declaration\nh . Implementing a Simple Decorator\ni . How It W orks\ni . Decorators in the Standard Library\ni . Memoization with functools.cache\nii . Using lru_cache\niii . Single Dispatch Generic Functions\nj . Parameterized Decorators\ni . A Parameterized Registration Decorator\nii . The Parameterized Clock Decorator\niii . A class-based clock decorator\nk . Chapter Summary\nl . Further Reading\n14 . 10. Design Patterns with First-Class Functions\na . What’ s new in this chapter\nb . Case Study: Refactoring Strategy\ni . Classic Strategy\nii . Function-Oriented Strategy\niii . Choosing the Best Strategy: Simple Approach\niv . Finding Strategies in a Module\nc . Decorator -Enhanced Strategy Pattern\nd . The Command Pattern\ne . Chapter Summary\nf . Further Reading\n15 . IV . Classes and Protocols\n16 . 1 1. A Pythonic Object\na . What’ s new in this chapter\nb . Object Representations\nc . V ector Class Redux\nd . An Alternative Constructor\ne . classmethod V ersus staticmethod\nf . Formatted Displays\ng . A Hashable V ector2d\nh . Supporting Positional Patterns\ni . Complete Listing of V ector2d, version 3\nj . Private and “Protected” Attributes in Python\nk . Saving Memory with __slots__\ni . Simple Measure of __slot__ Savings\nii . Summarizing The Issues with __slots__\nl . Overriding Class Attributes\nm . Chapter Summary\nn . Further Reading\n17 . 12. W riting Special Methods for Sequences\na . What’ s new in this chapter\nb . V ector: A User -Defined Sequence T ype\nc . V ector T ake #1: V ector2d Compatible\nd . Protocols and Duck T yping\ne . V ector T ake #2: A Sliceable Sequence\ni . How Slicing W orks\nii . A Slice-A ware __getitem__\nf . V ector T ake #3: Dynamic Attribute Access\ng . V ector T ake #4: Hashing and a Faster ==\nh . V ector T ake #5: Formatting\ni . Chapter Summary\nj . Further Reading\n18 . 13. Interfaces, Protocols, and ABCs\na . The T yping Map\nb . What’ s new in this chapter\nc . T wo kinds of protocols\nd . Programming ducks\ni . Python Digs Sequences\nii . Monkey-Patching: Implementing a Protocol at\nRuntime\niii . Defensive programming and “fail fast”\ne . Goose typing\ni . Subclassing an ABC\nii . ABCs in the Standard Library\niii . Defining and Using an ABC\niv . ABC Syntax Details\nv . Subclassing an ABC\nvi . A V irtual Subclass of an ABC\nvii . Usage of register in Practice\nviii . Structural typing with ABCs\nf . Static protocols\ni . The typed double function\nii . Runtime checkable static protocols\niii . Supporting a static protocol\niv . Designing a static protocol\nv . Extending a protocol\nvi . Protocol naming conventions\nvii . The numbers ABCs and numeric protocols\ng . Chapter Summary\nh . Further Reading\n19 . 14. Inheritance: For Good or For W orse\na . What’ s new in this chapter\nb . Subclassing Built-In T ypes Is T ricky\nc . Multiple Inheritance and Method Resolution Order\nd . Multiple Inheritance in the Real W orld\ne . Coping with Multiple Inheritance\ni . 1. Distinguish Interface Inheritance from\nImplementation Inheritance\nii . 2. Make Interfaces Explicit with ABCs\niii . 3. Use Mixins for Code Reuse\niv . 4. Make Mixins Explicit by Naming\nv . 5. An ABC May Also Be a Mixin; The Reverse Is\nNot T rue\nvi . 6. Don’ t Subclass from More Than One Concrete\nClass\nvii . 7. Provide Aggregate Classes to Users\nviii . 8. “Favor Object Composition Over Class\nInheritance.”\nix . Tkinter: The Good, the Bad, and the Ugly\nf . A Modern Example: Mixins in Django Generic V iews\ng . Chapter Summary\nh . Further Reading\n20 . 15. More About T ype Hints\na . What’ s new in this chapter\nb . Overloaded signatures\ni . Max Overload\nii . T akeaways from Overloading max\nc . T ypedDict\nd . T ype Casting\ne . Reading T ype Hints at Runtime\ni . Problems with Annotations at Runtime\nii . Dealing with the Problem\nf . Implementing a generic class\ni . Basic Jar gon for Generic T ypes\ng . V ariance\ni . An Invariant Dispenser\nii . A Covariant Dispenser\niii . A Contravariant T rash Can\niv . V ariance Review\nh . Implementing a generic static protocol\ni . Chapter summary\nj . Further Reading\n21 . 16. Operator Overloading: Doing It Right\na . What’ s new in this chapter\nb . Operator Overloading 101\nc . Unary Operators\nd . Overloading + for V ector Addition\ne . Overloading * for Scalar Multiplication\nf . Using @ as an infix operator\ng . W rapping-up arithmetic operators\nh . Rich Comparison Operators\ni . Augmented Assignment Operators\nj . Chapter Summary\nk . Further Reading\n22 . 17. Iterables, Iterators, and Generators\na . What’ s new in this chapter\nb . A Sequence of W ords\nc . Why Sequences Are Iterable: The iter Function\nd . Iterables V ersus Iterators\ne . Sentence classes with __iter__\ni . Sentence T ake #2: A Classic Iterator\nii . Don’ t make the iterable an iterator for itself\niii . Sentence T ake #3: A Generator Function\niv . How a Generator W orks\nf . Lazy sentences\ni . Sentence T ake #4: Lazy Generator\nii . Sentence T ake #5: Lazy Generator Expression\ng . Generator Expressions: When to Use Them\nh . Another Example: Arithmetic Progression Generator\ni . Arithmetic Progression with itertools\ni . Generator Functions in the Standard Library\nj . Subgenerators with yield from\ni . Reinventing chain.\nii . T raversing a tree\nk . Iterable Reducing Functions\nl . A Closer Look at the iter Function\nm . Case Study: Generators in a Database Conversion Utility\nn . Generators as Coroutines\no . Generic Iterable T ypes\np . Chapter Summary\nq . Further Reading\n23 . 18. Context Managers and else Blocks\na . What’ s new in this chapter\nb . Do This, Then That: else Blocks Beyond if\nc . Context Managers and with Blocks\nd . The contextlib Utilities\ne . Using @contextmanager\nf . Pattern Matching: a Case Study\ni . Scheme Syntax\nii . The Parser\niii . An Expression Evaluator\niv . OR-patterns\ng . Chapter Summary\nh . Further Reading\n24 . 19. Classic Coroutines\na . What’ s new in this chapter\nb . How Coroutines Evolved from Generators\nc . Basic Behavior of a Generator Used as a Coroutine\nd . Example: Coroutine to Compute a Running A verage\ne . Decorators for Coroutine Priming\nf . Coroutine T ermination and Exception Handling\ng . Returning a V alue from a Coroutine\nh . Using yield from\ni . Pipelines of coroutines\ni . The Meaning of yield from\ni . Basic behavior of yield from\nii . Exception handling in yield from\nj . Use Case: Coroutines for Discrete Event Simulation\ni . About Discrete Event Simulations\nii . The T axi Fleet Simulation\nk . Generic T ype Hints for Classic Coroutines\nl . Chapter Summary\nm . Further Reading\n25 . 20. Concurrency Models in Python\na . What’ s new in this chapter\nb . A Bit of Jar gon\ni . Processes, threads, and Python’ s Infamous GIL\nc . A Concurrent Hello W orld\ni . Spinner with threading\nii . Spinner with multiprocessing\niii . Spinner with asyncio\niv . Supervisors Side-by-side\nd . The Real Impact of the GIL\ni . Quick Quiz\ne . A Homegrown Process Pool\ni . Process-based Solution\nii . Understanding the Elapsed T imes\niii . Code for the Multi-core Prime Checker\niv . Thread-based Non-solution\nf . The Big Picture\ni . System Administration\nii . Data Science\niii . Server -side W eb/Mobile Development\niv . WSGI Application servers\nv . Distributed task queues\ng . Chapter Summary\nh . Further Reading\ni . Concurrency with threads and processes\nii . The GIL\niii . Concurrency beyond the standard library\niv . Concurrency and scalability beyond Python\n26 . 21. Concurrency with Futures\na . What’ s new in this chapter\nb . Concurrent W eb Downloads\ni . A Sequential Download Script\nii . Downloading with concurrent.futures\niii . Where Are the Futures?\nc . Launching Processes with concurrent.futures\ni . Multi-core Prime Checker Redux\nd . Experimenting with Executor .map\ne . Downloads with Progress Display and Error Handling\ni . Error Handling in the flags2 Examples\nii . Using futures.as_completed\nf . Chapter Summary\ng . Further Reading\n27 . 22. Asynchronous Programming\na . What’ s New in this Chapter\nb . A few definitions\nc . Example: Probing Domains\ni . Guido’ s trick to read asynchronous code\nd . New concept: awaitable\ne . Downloading with asyncio and aiohttp\ni . The Secret of Native Coroutines: Humble\nGenerators\nii . The all-or -nothing problem\nf . Asynchronous Context Managers\ng . Enhancing the asyncio downloader\ni . Using asyncio.as_completed and a semaphore\nii . Using an Executor to A void Blocking the Event\nLoop\niii . Making Multiple Requests for Each Download\nh . W riting asyncio Servers\ni . A FastAPI W eb Service\nii . An asyncio TCP Server\ni . Asynchronous iteration and asynchronous iterables\ni . Asynchronous Generator Functions\nii . Async Comprehensions and Async Generator\nExpressions\nj . Generic Asynchronous T ypes\nk . Async beyond asyncio: Curio\nl . How Async W orks and How It Doesn’ t\ni . Running Circles Around Blocking Calls\nii . The Myth of I/O Bound Systems\niii . A voiding CPU-bound T raps\nm . Chapter Summary\nn . Further Reading\n28 . 23. Dynamic Attributes and Properties\na . What’ s new in this chapter\nb . Data W rangling with Dynamic Attributes\ni . Exploring JSON-Like Data with Dynamic\nAttributes\nii . The Invalid Attribute Name Problem\niii . Flexible Object Creation with __new__\nc . Computed Properties\ni . Step 1: Data-driven Attribute Creation\nii . Step 2: Property to Retrieve a Linked Record\niii . Step 3: Property Overriding an Existing Attribute\niv . Step 4: Bespoke Property Cache\nv . Step 5: Caching Properties with functools\nd . Using a Property for Attribute V alidation\ni . LineItem T ake #1: Class for an Item in an Order\nii . LineItem T ake #2: A V alidating Property\ne . A Proper Look at Properties\ni . Properties Override Instance Attributes\nii . Property Documentation\nf . Coding a Property Factory\ng . Handling Attribute Deletion\nh . Essential Attributes and Functions for Attribute Handling\ni . Special Attributes that Af fect Attribute Handling\nii . Built-In Functions for Attribute Handling\niii . Special Methods for Attribute Handling\ni . Chapter Summary\nj . Further Reading\n29 . 24. Attribute Descriptors\na . What’ s new in this chapter\nb . Descriptor Example: Attribute V alidation\ni . LineItem T ake #3: A Simple Descriptor\nii . LineItem T ake #4: Automatic Storage Attribute\nNames\niii . LineItem T ake #5: A New Descriptor T ype\nc . Overriding V ersus Non-Overriding Descriptors\ni . Overriding Descriptors\nii . Overriding Descriptor W ithout __get__\niii . Non-overriding Descriptor\niv . Overwriting a Descriptor in the Class\nd . Methods Are Descriptors\ne . Descriptor Usage T ips\nf . Descriptor docstring and Overriding Deletion\ng . Chapter Summary\nh . Further Reading\n30 . 25. Class Metaprogramming\na . What’ s new in this chapter\nb . Classes as Objects\nc . type: The Built-in Class Factory\nd . A Class Factory Function\ne . Introducing __init_subclass__\ni . Why __init_subclass__ cannot configure\n__slots__\nf . Enhancing Classes with a Class Decorator\ng . What Happens When: Import T ime V ersus Runtime\ni . Evaluation T ime Experiments\nh . Metaclasses 101\ni . How a Metaclass Customizes a Class\nii . A Nice Metaclass Example\niii . Metaclass Evaluation T ime Experiment\ni . A Metaclass solution for Checked\nj . Metaclasses in the Real world\ni . Modern Features Simplify or Replace\nMetaclasses\nii . Metaclasses are Stable Language Features\niii . A Class Can Only Have One Metaclass\niv . Metaclasses Should be Implementation Details\nk . A Metaclass Hack with __prepare__\nl . W rapping up\nm . Chapter Summary\nn . Further Reading",30623
